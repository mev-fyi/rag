00:00:05.920 - 00:00:44.182, Speaker A: Hi fellows, my name is Greg, I'm from Lida and I work as data engineer. So basically what I'm trying to do in Lida is recreate Dune in networks where Dune isn't presented yet. And also here today, my co author Irina Catuina, she's head of Lid analytics team and basically the first analyst at Lida. So what Lida is. Lida is a liquid staking protocol. We operate in five different blockchains and currently we have about 30% of all staked ETH. With a total weight, you logged about $7.6
00:00:44.182 - 00:01:19.770, Speaker A: billion. And today I'm going to present you how Lida team creates analytic on Dune. Also some results of Dune API testing and some conclusions. So let's start. Well, currently Lidar has been working with tune for more than 20 months. During that time we created a lot of queries, more than 500, and also 17 beautiful dashboards. But then it all started our analytics derived from community members.
00:01:19.770 - 00:02:10.140, Speaker A: And here you can see the very first dashboard which was created with widgets from Vasily Shapavalov, Anton Bukov and LS cmos. And despite this dashboard provide like pretty comprehensive picture. Demand for analytics rises with Lidar growth. So in March 2021, the Lidar analytics team was created. And currently what we do is creating different simulations and models of blockchain networks and defy protocols in order to provide data insights. We perform monitoring analytics, product analytics, system analytics, and recently we added also web analytics to our backlog. And for all these analytics, we need a really wide range of data sources.
00:02:10.140 - 00:02:46.888, Speaker A: And if Lid uses conventional data engineering approaches, it would require a whole team of data engineers. So what we decided is to use third party solutions where it's possible. So for us, dune becomes such a solution on Ethereum and recently on polygon. So what do we have? Oh yeah, it's changed. Great. So our dashboards could be divided into several groups. We got like high level analytics with general metrics of Lido on Ethereum.
00:02:46.888 - 00:03:38.520, Speaker A: Also for our key lending and liquidity pools, we got separate dashboards. And also we got several special dashboards including LDO incentives, behavioral pattern analysis and deposits to beacon chain. And here you could scan our recently released blog post with all our dashboards with detailed descriptions and links. So yeah, feel free to check them and also don't check only them. Also check all dashboards from community members because June community members does really great job. And also, here's basically all about our analytic team. And next, I would describe, Joonapi, how we work with it and all that stuff.
00:03:38.520 - 00:04:38.460, Speaker A: So you probably already know that zoonapy allowed you to execute and retrieve execution data directly through the code, and we've tested it for one month in three different use cases. So here's the link to stream it up. Yeah, also need to disclaimer here that we do not provide you with some libraries or modules. Here's direct solutions of lidar specific tasks and I just created like a snapshot of this solution in order for you to also check them. So first use case is related to AAVE liquidations. AAVE is a liquid staking protocol. There you could use StevE as a collateral, and in some circumstances this collateralized Steve might be liquidated and we really would like to know which wallets are close to liquidation.
00:04:38.460 - 00:05:29.464, Speaker A: In order to solve this task, you need to get list of all wallets which use SDF as a collateral. And in order to do this, if you do it on chain with just web3, you have to pass through all deposit and transfer events, save all events with Steve. After that, calculate balances like through transactions and save it somewhere. And this was really like time and execution consuming. We also tried different data providers, but we face many data bugs and incompleteness. So we basically always return to on chain solution with that large amount of executions. But with Dune API we solve it pretty fast.
00:05:29.464 - 00:06:20.690, Speaker A: We just create simple query and download it and that's all. So yeah, and here I could show you the result of it. We especially give you whole code so you could read through it and this deep node and all these graphs are made with dune API data. So basically you could find even function which download needed data from dune API. But API key is hidden because we don't want to spoil it. Yeah, so yeah, for our liquidation, that's all thanks to June API we could track all the needed data about our collateralized stiff. Second case is related to retention analysis.
00:06:20.690 - 00:07:22.100, Speaker A: Due to peculiarities of retention metrics we could calculate it only for current day, but we wanted to know dynamics of retention to understand if users account to us or they don't mind. And we basically don't have any solution how to calculate it in dynamics. But UnapI provided with some solution and in order to explain this solution I need to dive deeply into how exactly the API works. 1 second sorry. So you don't just pass query and get the result. Instead the flow is following upost query and get execution id. After that you check the status of execution and once status is executed successfully you could download needed data and use it.
00:07:22.100 - 00:07:54.438, Speaker A: Interesting part here is that knowing execution id, you could download data anytime you wanted. So that's how we create a decision. We just make a small dictionary with date execution id, date execution id. And that was all. And basically here you can see the result. So if I need retention metrics for this date, I just get retention snapshots and I hope everything would run. Yeah, and get all needed data.
00:07:54.438 - 00:08:25.124, Speaker A: So these data are not like hard coded, it's all get it from dune API. So also I could get data for another date and it's still, it's running. But still we get result with graphs. So everything works. And the last use case second. Oh, I'm loading. Yeah.
00:08:25.124 - 00:09:22.820, Speaker A: So the last use case is related to cross chain analytics. Since slider presented in five different chains, we wanted to know like general metrics in order to understand how we operate. What happened, what happened in different blockchains. But there are several challenges which you need to accomplish before creating cross chain analytics for us with challenges was firstly, you got five different blockchains, each with its own like passing parameter methods. Different where is different data structures? This info should be updated on daily basis and this is required complicated data engineering. And this data engineering entails infrastructure and pipeline maintenance. So yeah, we make semi automated scripts, but this was still pretty bad for proper cross chain analysis.
00:09:22.820 - 00:10:12.822, Speaker A: With Dune, we solve this task for three out of five blockchains. And yes, I will return. So here you could see for example graph of all depositors among all Leidos blockchain. And also you could play with it like display only for example polkadot or make some sizes, all that stuff and also even download spongy. So please feel free to play with all these graphs. Yeah, and we use here dune API for polygon, Solana and Ethereum. And for Polkadot and Kusama we use our own decision made on Polyntheus.
00:10:12.822 - 00:11:46.528, Speaker A: But hope someday we we'll use only dune for this task because it's really simple. You just make one API call. Yeah, and to sum up all of this, if you're a data analyst who don't want to mess with processing in data engineering, the juniper could be really beneficial because you don't have to read through documentation in order to understand how to pass parameters properly bothering with storing data, or spend hours in cleaning and checking for errors, all you do is just create simple API call and go straight to data analytics. And also we got several suggestions to Dune. The first suggestion is to create update execution method which allows to for query with dj take a really lot of time to run at the first time. But after first run this query could be updated like with several rows in order to not re execute it every time. Because especially for Solana, there are a lot of efforts, and I suppose computational on dune site for even, I don't know, for one week, because Solana produced enormous amounts of data and also download query method for just basically fetching data from dune frontend.
00:11:46.528 - 00:12:18.382, Speaker A: Because once again, there are a lot of queries which are already executed. So you might just take this data and use it. Yeah. And that's all from my part. I could go to maybe streamlet app and if you have any questions, please ask them. And also here you could see all the links to any to all the blog posters and more info about how when you should use it. Yeah.
00:12:18.382 - 00:12:19.270, Speaker A: So that's all.
