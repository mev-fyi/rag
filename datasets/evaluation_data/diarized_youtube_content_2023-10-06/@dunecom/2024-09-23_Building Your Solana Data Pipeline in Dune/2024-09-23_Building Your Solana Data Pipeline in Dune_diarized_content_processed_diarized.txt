00:00:00.440 - 00:00:36.077, Speaker A: What's up, everyone? My name is Andrew Hong. I'm the headmaster here at Dune, and today I'm going to be teaching you how to build your own Solana data pipeline. All on dune, just in SQL, no Python, no working with RPCs or anything else. And I really want to show you just both how complex Solana data gets, but how complex and powerful Dune can be as well, if you learn to leverage the platform correctly. So this was supposed to be done at Breakpoint, but I was not able to make it. So here I am giving this presentation to you from the comforts of my apartment. Cool.
00:00:36.077 - 00:01:02.475, Speaker A: So this will be a pretty straightforward presentation. I'm going to review Solana data structures very quickly. We're going to talk about stages of data transformation because you don't want to just throw paint across the wall when you're working with this data. You want to have some sort of method for it. Getting the most out of a Dune query. I like to say that every query on Dune is an ETL pipeline and I'm going to show you exactly why. And lastly, Solana Data Resources.
00:01:02.475 - 00:01:41.553, Speaker A: It's not super easy to learn how to do this stuff, so I've put together many guides and many videos to help you learn to do this easier, more easily. So we'll cover that as well. So, welcome to Solana. Hopefully you've seen this iceberg meme format before. Essentially, your life starts out easy and then by the time you get down to here, you kind of look like this guy. You're not having the greatest time. However, hopefully the guides that I've written and the work I've done with the Dune team to make better tables will make your life easier.
00:01:41.553 - 00:02:12.309, Speaker A: And hopefully you'll look like this guy most of the time. All right, so this is what working with the data feels like. And we've had a lot of data tables created on Dune to make your life easier. However, it won't be easier if you don't know how to navigate those tables. So we have a few levels of tables that I like to call out. Raw tables, decoded tables, and curated tables. So if you're wondering where the RPC data tables sit, that sits at the raw table level.
00:02:12.309 - 00:02:59.887, Speaker A: If you want to learn about these tables, go to docs.dune.com, you can go to the Solana Data Overview, where you'll see coverage of Solana transactions, blocks, account activity rewards and vote transactions. Anyone who's looked or done a transaction on Solana before probably knows that instructions are nested and instructions are essentially calls to programs on Solana. So here I'm looking at a swap where there's instruction 1, instruction 2, instruction 3, and then there's a 3.1. So this is your inner instruction or your nested instruction. And then here you have Jupyter V6 that's actually doing a swap. And there are 4.5,
00:02:59.887 - 00:03:15.511, Speaker A: 4.8, 4.9, 4.20, 4.24. There's 24 inner instructions. And so this is all stored in a single array in a single row in Solana transactions. So it's not easy to work with.
00:03:15.511 - 00:03:52.645, Speaker A: So one of the first things we've done is we've unnested it into this instruction calls table that will basically one to one map with what you see on any block explorer on Solana. All right, so that'll make your life a lot easier. You really shouldn't be querying Solana transactions. For the most part, you should be using Solana instruction calls. We've also done the work to create decoded tables. Anyone can submit a table for decoding on Dune by submitting the idl. If you don't know what the IDL is, then go, go tag me or someone else on Twitter and we'll help you find it and submit it for you.
00:03:52.645 - 00:04:58.731, Speaker A: But essentially every single function gets its own table on Dune, right? So if I go to Dakota Tables and then I filter for Solana, I can click for example, System Program here. And if I look at System Program Create Account and I preview this table, it's going to show me the call data as well as the account arguments with the actual account names and also the function arguments. So Lamports and Owner and Space or some of the function arguments. So this should take out most of the headache of working with Solana data. We have over 112 programs decoded on Dune for Solana at this moment. So even these Dakota tables are not always the easiest to work with. So we've created curated tables which make them even easier to work with, such as having all fungible table tokens or all NFT tokens in one table, or all token transfers nicely put into one table across multiple token standards, daily latest balances.
00:04:58.731 - 00:05:59.167, Speaker A: And ultimately there's even higher level curated tables such as Dexalana trades, so that you don't even have to go look at how ORCA works, you can just use Decilana trades. So now that you kind of know the structure of tables on Dune, once you get to work, you actually want to keep a system in your mind in terms of how to move along that data. So it starts with systems. All right, So I like to think about systems as tracking global variables that you will need for kind of point in time joins for other tables. Right? So that might be your protocol states, your parameters, such as fees, your pool tokens, just anything about a program that might be changing over time through various upgrades. You want to have kind of the state, the time that that state applied, and the program or addresses that those states apply to. After you have that, you can start creating feeds.
00:05:59.167 - 00:06:40.449, Speaker A: So let's say I take orcadata, I get the transactions or the swaps from Orca, and I want to figure out the fees, while the protocol fee might have changed over time. So I joined those swaps to the system table for fees. So I can get both the swap amount and the swap fee amount kind of all at once in the same feed. Right? So that's why you need systems first. Next, your positions. So you might want to calculate the total profit loss per token or open or close positions for given traders on dexes. So then you needed feeds to be able to aggregate to a user level to see what their positions are.
00:06:40.449 - 00:07:39.021, Speaker A: And then once you have their positions, you can label them as, for example, maybe a retail trailer or a whale trader or a mev trader or whatnot. And now once you have labels, then I think you can get to interesting metrics. So if you just had feeds and went straight to volume, that's interesting, but doesn't help anyone actually answer any questions. It might get a bunch of likes on Twitter, but everyone kind of knows. It's like, okay, this is a. You've done this for marketing purposes, right? So if you want to get to more contextual metrics, you could do something as like, I want to know the number of active retail traders who hold more than $500 in their wallet, right? So then you would look at like, okay, labels of positions of users who are, you know, retail, and you can define that however you want. And I want them to be active, right? So I filter for only the retail traders who had a dex trade sometime in the last two weeks.
00:07:39.021 - 00:09:00.175, Speaker A: And then I could join on balances to see do they currently hold more than US$500 in their wallet. Right? And it's like, okay, that's kind of how you get into more specifically helpful and contextual business metrics. Um, it's by combining all of these tables along the way, right? So if you want to see that in action, I recommend you go to my Solana Dex metrics dashboard and you'll kind of see that I kind of start breaking down metrics deeper and deeper by, by pools, by chain, by the front end, by bots, by the trade amounts of wallet. And so when I'm building this, it's a very methodical, like I know what I need to create in what order to get to the actually useful charts on my dashboard. Right? So you'll see this if you look through the dashboard I just showed you. But when I put this together for Solana Dex data, I knew I needed token types, I know I needed pool types, I know I needed fee settings, I know that I needed obviously all of the Dex trades, but I also needed all of the fungible tokens and all of the transfers over time. And then I would also want the Jupiter aggregator swaps so that I could tell, you know, which trades were Jupiter or not.
00:09:00.175 - 00:10:09.183, Speaker A: After I have that, then I can get token liquidity, I can get token volume, the number of pools per token, the trader positions, et cetera. And then that lets me get deeper labels on both the traders, the trades and the pools. And ultimately there were two metrics I was kind of interested in volume by trader type and volume by front to end. Right. So let's look at one of the queries I created. The final transformed query here is the share of organic volume. So I wanted to see, okay, for given swap, where did it come from? Right? So if I looked here at this swap earlier, there were like there was a radium swap in here, there's LI affinity swap in here, there's a goose swap, there's Meteor swap, but at the top it came from the Jupyter V6 program as kind of the executing account, right? So I know that this swap came from Jupyter and I'm assuming that if it came directly from Jupyter, then it was probably from the Jupyter front end, right? If they did a custom MEV contract, then that would show up as a custom MEV contract and I'd label that as other.
00:10:09.183 - 00:10:55.075, Speaker A: If it was directly to Orca or directly to Radium, I would assume that they used ORCA or Radium's front end, right? Because that's where the trade is originating from. And so if I look at the just trade sources, I can see that Jupiter makes up almost 44%. 40%. Other programs are around, you know, 15 to 20%, radium sitting around 20% or so as well. And I wanted to filter out bots, so I actually had another query where I filter out the bots already. So if I go to Table lineage. You can see I have this materialized view, Solana Dex bot detection, which is just I took a query and I run that query and save those results every hour so I can query it as a table.
00:10:55.075 - 00:11:28.133, Speaker A: And then I also have these two other tables I'm relying on. And if I filter out the bots, then I can see that Jupyter is closer to 60% of total retail volume or total organic volume by front end. Right. And so this is kind of what I want to get into. It's Dune gives you the flexibility to build your data pipeline kind of by yourself and also with the community. Right. So I can have the raw data, the decoded data, and then maybe someone upload some pricing data from Bird Eye or something publicly.
00:11:28.133 - 00:11:57.541, Speaker A: But I can also upload my own private data, such as maybe a list of tokens or wallets I'm interested in tracking and joining. And I want to keep that private. I can't. Right. And there's also after you've kind of aggregated or ingested the data you have defining it. So you could use the spellbook tables, the other community, like other user created queries, and then you have your own queries, and your own queries can be very powerful. We allow you to create queries as views or as materialized views.
00:11:57.541 - 00:12:32.079, Speaker A: So you can link queries together. You can use the results of a query as the parameters in another query. You can schedule them, we have version history. And when you want to use your Dune outside of just a Dune dashboard, we have like Slack Alerts, we have web hooks, we have this powerful API, we have various integrations. You can also just use our Dune queries as embeds on your own front ends if you want. So that gives you everything you want to work with the community, work with yourself, and ultimately leverage it however you want. Right.
00:12:32.079 - 00:13:29.925, Speaker A: So for the table that I just covered, not the table for the query I was just covering over here, if you want to understand the deeper steps I went through in terms of how I made this, you can kind of read this chart here. Again, there's a mix of public queries and tables and private queries, or my own queries and tables that I used here. It's super powerful. I'll say that, like if you're an analytics engineer or even a data engineer, Dune is kind of your go to place for at least putting together a proof concept very quickly to make sure that the data you want to get is actually possible to get and you can put into your production pipeline very quickly. So all that being said, and done. The last thing I'll leave you with is a list of tools and resources to make your life easier. I highly recommend you check out the Solana program Quick Start Dashboard I put together.
00:13:29.925 - 00:14:28.583, Speaker A: You can put in any program ID or any transaction if you want to know the tables to query for transaction. But if you put in a specific program then you can know when was that account created. You can see like the top calls to a given table or a given function as well as an example transaction of each one. Here we can also see the instruction count over time, the users over time, and then also which programs are calling it, which programs this one is calling kind of gives you a complete overview of what's going on with any program. I've also written many many guides. If you go to read.cryptodatabite.com Solana I've got a four part guide that teaches you everything you need to know about Solana Data Analysis I've covered how to analyze Solana programs, covered staking, covered how to go from Ethereum to Solana as an analyst.
00:14:28.583 - 00:15:10.925, Speaker A: Tokens, balances and accounts token 2022 there's literally most of the things you need to know to get started are going to be in here already. If there's anything you want to learn that's not in here, just shoot me a DM and I will look into creating a guide for unfortunately this is not live so you can't ask me questions. However, if you do have questions or feedback or thoughts, you can reach me at Andrew Hong 5297 on Twitter or Telegram. Follow me to kind of learn more about how to work with Solana, Ethereum or Bitcoin data. And you know you can DM me anytime and I'll try to help you out with any questions that you have. Good luck.
