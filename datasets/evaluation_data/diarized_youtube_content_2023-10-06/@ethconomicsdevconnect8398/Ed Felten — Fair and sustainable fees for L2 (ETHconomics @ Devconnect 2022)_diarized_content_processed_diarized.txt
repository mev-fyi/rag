00:00:08.810 - 00:00:52.154, Speaker A: Good morning, everyone. Thanks for your time this morning. I want to talk about fair and sustainable fees for L2. And I'm going to come at this based sort of on first principle, but from a pragmatic point of view. And I'll be talking as well, based on our experience in Arbitrum, I'll talk about some of the decisions that we've made, some of the preliminary things that we're doing, and what we've learned from doing it. I think it's fair to say that of the ethereum based L2 systems, Arbitrum is the one with the largest and longest track record. So arguably we have learned more about this perhaps than others.
00:00:52.154 - 00:01:27.406, Speaker A: But I'll let you be the judge of that. Okay. Fair and sustainable fees, what do I mean just for some level setting, when I say fair, the definition of fair that I'm going to use is that the transaction fee for a transaction approximates the cost that that transaction has imposed on the system as a whole. Right. So this, of course, makes sense from an economic standpoint because this will align incentives well. But also I'm going to treat that as part of the definition of fairness. We don't want to systemically overcharge or undercharge certain types or categories of transactions.
00:01:27.406 - 00:02:18.742, Speaker A: So as an example, some mechanisms, such as ones where people pay extra for better ordering in the transaction sequence, it may not be compatible with this principle because you're charging more to a transaction because it's more ordering sensitive, as opposed to because of the impact it has on the cost of the system. So I'm going to make this one of the goals. So we're going to be thinking a lot about costs and how to fairly allocate costs among the different transactions and then sustainable. I'm talking about fee policies that can be maintained over the long term. Of course, an L2 system could strategically underprice to try to gain market share, something like that. People may do that. But here I want to talk about what is sustainable for the long run.
00:02:18.742 - 00:02:58.982, Speaker A: And in the long run, I think that means two things. One is that the revenue has to match the cost over time, number one. And number two, we have to think about the long term, about what is the sort of long term behavior and trends and the evolution of this system that will be induced by how we price. And we need to think ahead. So you see examples of these principles playing out as they go on. All right, so in order to lay the groundwork, so what I'm going to do is I'll give you a little bit of groundwork about how a system like Arbitrum works. This applies the parts I'll describe should apply to most or all of the major L2 s.
00:02:58.982 - 00:03:33.630, Speaker A: But I need to lay a little bit of technical groundwork so that you can understand the constraints that we're under in setting these policies. And then I'll dig into three particular challenges that we have identified. I'll talk about how we are thinking about each one, what we've done and what we've learned. Okay? So first, a little bit of how it works. One of the key components from our standpoint today, probably the key component in the system is the sequencer. So the sequencer is a component. It is currently a centralized system.
00:03:33.630 - 00:04:08.086, Speaker A: It will eventually be a distributed system, but I'll treat it as a single box on the diagram for that purpose. For our purposes, the job of the sequencer is to take in a series of transactions that arrive from users or from L one contracts. They come in via remote procedure call, and the sequencer is going to put them into some kind of order. The sequencer is trusted to do this, to establish order, but it's not trusted for any other purpose. We don't trust it to influence the prices. We don't trust it to determine which transactions are okay and not okay. All it does is put them into sequence.
00:04:08.086 - 00:04:57.738, Speaker A: Now, there's a backup mechanism that you can use if the sequencer tries to censor you. I'm going to ignore that here for simplicity. So let's assume that there's a well behaved sequencer and the sequencer is instructed to follow a first come, first served policy in sequencing the transaction. So the sequencer decides on this, it will inform others. And then the second important part of the system is the state transition function. This is a deterministic function which takes the next transaction in the sequence, and it takes the current state of the L two system and it deterministically computes some updates to the state and then perhaps emits an L two block. So the deterministic nature of the state transition function is really important.
00:04:57.738 - 00:05:46.300, Speaker A: What this means is that the outcome of the transaction, both what L two blocks have been emitted and what the state is, depends only on the contents of that transaction and the contents of all previous transactions in the history plus the Genesis state. There's no other information that comes into this. The state transition function can't look at anything, gather any information. It just takes the transaction and the state puts out a new state and the L two block. The fact that this is deterministic is really important because it means that if you're a node in the system or any participant and you know the transaction sequence, then you can compute exactly what the state of the chain is. You don't need to rely on anyone else to know the chain state. All right? But that deterministic nature is also going to be a constraint, as we'll see.
00:05:46.300 - 00:06:53.550, Speaker A: Okay, so this is logically what happens, but how do people find out the sequence? They find out by two ways. First, the sequencer, very quickly after it puts a transaction into the sequence, will publish that transaction on the sequencer feed. This is a feed the sequencer publishes anyone can subscribe to it. And if you submit a transaction, normally in less than 1 second, your transaction will be in the sequence and in the sequencer feed. I've labeled this a soft guarantee because this is the sequencer promising you that in the eventual final feed that it publishes, we'll be consistent with this. So if you believe that sequencer is being honest to you about its intentions, then you can trust this and you can immediately determine the state not only of the outcome of your transaction, but others in less than 1 second. That response time is really important to users, but that's, of course, a soft guarantee, and it's important that we can do that faster than an L One block for user experience reasons.
00:06:53.550 - 00:07:53.540, Speaker A: The sequencer then every several minutes will then take a subsequence sort of the next bunch of transactions that haven't yet gone through this process. It will put them into a batch, it will compress the batch using a state of the art general purpose compression algorithm, and then it will take that and it will write it onto the L One chain, currently as call data, we hope, very soon as Shard Blobs. And so once you've written that onto the L One chain, that is the final and official history, and that is the one that will govern what the outcome of your transaction is. So that will be consistent. It will be the same ordering as the sequencer feed if all is well. But this is the thing that you know, that doesn't have caveats on it. Then also, finally there's another piece of the system that takes these L2 blocks and settles them to the L One that's interesting enough, but doesn't really drive the conversation we're going to have today, so I'm going to ignore that part of it.
00:07:53.540 - 00:08:40.402, Speaker A: Okay, so L2 transactions get finality kind of in three stages. The first stage is soft finality. The way you can get soft finality for your transaction is you subscribe to the sequencer feed. Whenever you see a transaction on the feed, you compute the state transition function for yourself asynchronously from everyone else. Now you know the results, and the guarantee is that this is the correct result of the transaction. If the sequencer's feed is consistent with what the sequencer ultimately publishes and the protocol is designed, so the sequencer has the power to make its eventual publication consistent with its feed if it wants to. The second stage is true finality, and this takes order of ten minutes, basically the L One finality time.
00:08:40.402 - 00:09:23.054, Speaker A: So to get that, you read the compressed batches from the l One. You decompress them, compute the state transition function for yourself, or rely on some node you trust to do it. And then you wait for l one finality on that batch. Because once that has l one finality, then you really know what the result of the transaction is. And at that point, everyone in the world knows the result of the transaction except the L one ethereum chain doesn't because the L one ethereum chain cannot do that step of applying the state transition function because it doesn't have the computational throughput to do that. That's why we have an L2 in the first place. So then confirmation, which takes hours to days depending on which system you're in there.
00:09:23.054 - 00:10:06.106, Speaker A: You just wait for the L two blocks to be confirmed on the L One. Now, that is an inevitable process. If the protocol works correctly, it will inevitably commit exactly confirm exactly the blocks that you got in the finality stage. Or if the sequencer was honest, those will be identical to the results you got in this soft finality phase. So from our standpoint of our discussion today, really the first two phases are the ones that matter. Okay? Now, with that background, let me talk about some things that some complexities that arise out of this actually. Yeah, let me talk about sort of first, sort of where are the costs and the fees.
00:10:06.106 - 00:10:49.690, Speaker A: So first up here on the upper right, l two gas is charged here. This is where we charge users for their transactions, where we collect the fees. We collect ETH on l two for those fees. And that is part of the state transition function. One of the things it does is it deducts the fees from the sender of the transaction based on a computation that the state transition function does to determine what those fees should be. And this is important because of the deterministic nature of the state transition function. That means that the fees can only depend on the contents of this transaction and any information that the sequencer attached to it and then previous transactions.
00:10:49.690 - 00:11:39.850, Speaker A: Now, there are two sources of cost in the system. There are L2 costs that are incurred by nodes and those are basically the costs for nodes in the system to compute the state transition function and to maintain the state. And so basically the cost of a transaction will be some cost because of the computation that needs to be done in the state transition function and maybe some additional cost if that transaction causes the state to grow. But there's also L one gas cost which I've drawn down here, which is incurred paid by the sequencer. And that is paid because the sequencer has to post these transactions on the L one chain and therefore it has to pay L one gas. As of yesterday, the Arbitrum sequencer was the largest buyer of L one gas on ethereum. That's not true every day, but I believe it was true yesterday.
00:11:39.850 - 00:12:15.270, Speaker A: So this in practice is the largest component, larger component of the cost. It typically is around two thirds of the cost of a transaction although that may vary up or down depending on the nature of your transaction. So most of the cost is over there on the lower left paid in L One gas. Yes. So the state transition function will collect fees from the user and credit those to the sequencer's account on L2. So the sequencer will be reimbursed for those costs. Yes, please.
00:12:15.270 - 00:12:47.684, Speaker A: No. Yes. Currently, the sequencer is a machine run by us, and eventually the sequencer will be a distributed system where the sequencer nodes are run by a variety of parties in our community. Right. Now, the algorithm, we do not trust the sequencer to decree how much it should be paid. The sequencer is trusted only to order transactions. We run it.
00:12:47.684 - 00:13:08.890, Speaker A: Yes, correct. Right. Or to put it another way, we do not require our community to trust us to run the sequencer honestly. Right. This is all about getting trust from the users of the system. Right. And so the sequencer is trusted to put transactions in order, but otherwise it's not trusted for anything.
00:13:08.890 - 00:13:32.368, Speaker A: Okay. Yes. That's coming from the state. That's essentially the state, and that number will be much lower after our next upgrade, I have to say. But that is the state. Right. And that's one reason why we want to charge people if they grow the state.
00:13:32.368 - 00:14:08.188, Speaker A: Yeah. Okay. So there's three economic issues or pricing issues that I want to talk about that arise out of this system, and I'll go through them one by one in the rest of the talk. First there's, how do you charge for the L One gas costs? There's issues of fairness, issues of incentive compatibility that are interesting. I'll talk about how we do congestion based pricing of the L2 gas, and then I'll talk finally about the issue of storage Bloat, how we keep the storage from getting very large in a way that's really unpleasant. How to think about that? Okay, first, charging for L One gas costs. Okay.
00:14:08.188 - 00:14:50.724, Speaker A: Now, there are a bunch of issues here that arise. One of the issues is spillovers having to do with compression. Right. So the way data compression works is you have a big block of transactions that come from a lot of different people, and modern compression algorithms basically look for similarities between different parts of that data stream. And if there are two different sections in that data stream, two transactions that are similar, then the second one will compress very effectively because it can just be expressed as a diff from the first one. All right. So there are obvious spillovers here because the compressibility of one transaction depends on other transactions.
00:14:50.724 - 00:15:35.752, Speaker A: Also, if you have multiple transactions that are very similar, the first one will not compress much at all because compression runs linearly through there, and later ones will compress much better because of the first one. And we need to be careful not to punish people for being the first one. So this is a kind of interesting thing, and there's an interesting theoretical question here, but then, of course, we have to do something practical. I'll talk in a minute about what we do. But that is interesting issue number one here. Interesting issue number two has to do with time and what information is available when. So suppose that the sequencer gets your transaction at time T, and it publishes on the sequencer feed at time T what your transaction is.
00:15:35.752 - 00:16:10.260, Speaker A: And by the way, that includes a timestamp, and there are methods to keep the sequencer from messing with the timestamps very much, but at time T, it commits to the transaction. And now in the logic of the system, the transaction is known. The state transition function computed from it is deterministic. And so whatever you compute the fee is going to be it can only be based on information known at time T. Otherwise, this process isn't deterministic, and you don't get fast response time. All right, some of you see the problem already. This batched compressed.
00:16:10.260 - 00:17:18.088, Speaker A: The compression happens at time maybe T plus two minutes, typically, right? So at the time that your transaction is put into, maybe appended into a batch that's being built, the rest of the contents of the batch are not even known. So you don't know what's going to happen in the compression at time T, because that will only be evident or completely evident at time T plus two. But then after that compression has happened, the sequencer will submit a transaction to the ethereum l One chain, and it might take, say, a minute for that transaction to get on chain. And at that time, at T plus three, only then do you learn what the L One base fee is that this transaction is going to pay. So you don't know how well it will compress, and you don't know what the L One base fee is going to be at the time that you commit to what that transaction is going to get charged. And those time issues are the other thing that we need to deal with. Okay, so what do we actually do in Arbitrum about this for the spillover and compression issue? These, by the way, are both really open questions.
00:17:18.088 - 00:18:10.020, Speaker A: Our solutions here are the best that we could come up with, but they're clearly, I think, far from optimal. So for spillover and compression, we use a heuristic estimate of the relative compression friendliness of each transaction. That is, we try to estimate how much it contributes to the overall compressibility of a batch of typical transactions according to this heuristic. It's a very rough heuristic, but it's better than nothing. And then we also keep an adaptive estimate of the overall compression ratio that gets achieved by looking at the compression ratio that has obtained in the recent past. And that allows us to figure out sort of how much savings there is from compression. And then this heuristic tells us sort of how to allocate those savings among the different transactions that are in the batch.
00:18:10.020 - 00:18:57.156, Speaker A: This heuristic has to depend only on the contents of your transaction so that it can be known at time T. So we do this, and while the results are what they are, that's the best approach I think you can possibly get, because you can't see well, there are other approaches you could take. One approach might be to take a larger deposit from every transaction and then give a refund later. If it turned out to be compression friendly, then you could do it based on the actual contribution to compressibility. Although even how to define that and how to make that incentive compatible is a kind of interesting theoretical problem. We did not go with that approach. We went with this upfront heuristic approach.
00:18:57.156 - 00:19:44.804, Speaker A: But I think there's a lot to explore here against this problem. And this is something that all L2 S that do this kind of fast finality and compression are going to face. The other issue is the early commitment that you have to commit early to what you're going to charge someone. And we do that, and we do two things there. First, the L2 state keeps a running estimate of what the L One base fee is. Every time a transaction is put on the L One chain by one of our trusted contracts, it will essentially cause an update that will create a data point that the L2 can receive about what the L One base fee was at some time. Now, that is a lagging estimate.
00:19:44.804 - 00:20:35.736, Speaker A: It's a running average, first of all of the past, and it typically lags by minutes for technical reasons, but it's at least a running estimate. So if the L One gas fee spikes, base fee spikes, then this is going to run behind and it will be low. If the L One base fee dives, this is going to run behind and it will be high. But what we really need, one thing we really want is for this to balance out over time. The other thing that we do is we keep an automatically adjusted correction factor to correct for over or under collection over time. You might think that you can keep a running base fee estimate and that you can make that average out over time, but that's only true if transactions come in evenly spaced over time. But in fact, when the base fee is higher, you get fewer transactions, and when the base fee is lower, you get more.
00:20:35.736 - 00:21:17.456, Speaker A: And so we actually need to have an automatically adjusted correction factor, then make sure that the books balance over time, that the amount that is collected and sent as given as reimbursement to the sequencer matches the total amount that transactions pay for L One fees. So this is what we do. I think there's probably some room for improvement here, too. You might have a better predictive model. We don't predict. We just keep a trailing average based on measurement. And you might also imagine trying to create some sort of market mechanism by which we could externalize the risk associated with increase or decrease in the base fee.
00:21:17.456 - 00:22:04.702, Speaker A: Those would be interesting things to do. We don't know how to do those today, but we would really be interested in hearing about ways to do those things. Yes, it is not no, it's a relatively crude heuristic. One of the things it uses is how compressible is this transaction just considered as a transaction, just considered alone. Another thing that we are likely to do is have some kind of simple baseline dictionary and things that have patterns in that dictionary will get more credit and that we might adjust over time. But there's a worry about the sort of trustlessness of that process. Right.
00:22:04.702 - 00:22:26.520, Speaker A: I mean, there's a bunch more things you could do to try to maintain you could try to maintain compression state over time that gets into issues because that has to live in the state. And now the economics of that also get complicated. So there's a lot of complexity here. I think there's a lot of room for improvement. We have to ship something. And so this is what we built. Yes.
00:22:26.520 - 00:23:19.380, Speaker A: Sorry, why is it legged by minis? Yes. So here's the fundamental reason. The reason is because of this determinism. So we need two things. We need the state transition function to be deterministic and the sequencer needs to have enough control over the transaction sequence that it can keep its promises about ordering. And that means, in practice, that anything that comes in from the L One chain that will be encoded in a transaction has to lag by the L One finality time. Otherwise the sequencer could get burned by a L One reorg.
00:23:19.380 - 00:23:59.000, Speaker A: And that causes basically, the flow of information from the L One chain into the state transition function to lag by an assumed L One finality time. So order of ten minutes yeah. In the back over time. Yeah. If the sequencer overcollects, then the automatically adjusted correction factor will cause the sequencer to undercollect for a while going forward. Yeah. The goal is long term balance.
00:23:59.000 - 00:24:36.122, Speaker A: If the sequencer is undercollected, we don't want to put all of the extra charge on the next few transactions. You want to spread that out over time and you want to make the books balance over time. Yeah. Yes. Essentially, imagine there's an enormous base. Could that then mean the system locked up? Because subsequent they don't want to the price will go up some. There's a couple of things that help with this.
00:24:36.122 - 00:25:29.674, Speaker A: One is that the sequencer can actually wait a little bit longer before posting its batches. The sequencer has some latitude in choosing when to post its batches. There is a trade off because users want finality, and if you wait longer, they have to wait longer for finality. But if the base fee l One base fee has really spiked, or if the L One gas price has really spiked and you have to give a large tip in order to get your transaction in, then the sequencer will actually wait a little bit longer. So the sequencer will, on average pay less than the sort of integrated l one gas cost over time because it will withhold its l one transactions for a while. When the price is high and has spiked, it is not because of this correction factor. Right.
00:25:29.674 - 00:26:21.530, Speaker A: The correction factor will correct back to zero. So that the book's balanced. The sequencer gets the amount the sequencer gets is basically the amount that it's paid plus a small increment. No, those are typical times, yes. Okay, let me move on to the next issue, which is compression pricing of L2 gas, right? So l Two gas, like L One gas, each transaction uses some amount. And in fact, in the Arbitrum nitro version, we use exactly the same accounting of gas that Ethereum uses. And so just like on Ethereum, when there's high demand for gas, we want to raise the price so that the demand and the capacity balance out over time.
00:26:21.530 - 00:26:56.580, Speaker A: Okay, so how do we do this? Well, the first question you might ask is why don't we just use EIP 1559? We like EIP 1559. We think it's a great solution for Ethereum, but it's not quite right for us. And that's because of some differences which I'll go through in a minute. So what we do is 1559 style, but a little bit different because of these differences. So here's some differences that I think are relevant for this. The first is because of the design of the system, kind of the fundamental unit here is a transaction rather than a block. So we don't do it per block.
00:26:56.580 - 00:27:41.472, Speaker A: Transactions are timestamped in seconds and the timestamps advance in a way that is more continuous than on ethereum. So it's more continuous. The timestamp doesn't jump at the beginning of a block and buy a exponentially distributed but unpredictable time. The timestamp of transactions are spread out more over time. The sequencer has a limited ability to manipulate the timestamps. And we don't want the sequencer to be able to boost its fees much by tweaking timestamps a little bit forward or backward or trying to cluster them or something like that. We run nodes closer to full speed than Ethereum does.
00:27:41.472 - 00:28:17.032, Speaker A: Ethereum runs, say, a geth node at only a small fraction of its capacity for good crypto economic reasons, we run closer. We don't run at full speed, but we run closer to full speed, meaning that a node is spending more of its time computing the state transition function than an Ethereum node would be. And that puts some constraints because if traffic is coming in above capacity, we need to be more careful. And then finally, nodes can run independently because of determinism. Nodes don't need to synchronize with each other through some protocol. They can all run independently. We don't need to synchronize.
00:28:17.032 - 00:29:08.408, Speaker A: And that means it might be okay. If slower nodes fall behind a little bit for a short period, so all those things are differences. Okay, so what do we do? Yes, because of that unsynchronizing, does that give the ability for nodes that then have lower bandwidth Internet over time? So the difference is really kind of in surge capacity, right? We need to make sure that over time the amount of load on the system is low enough that a typical node can handle it. But there may be differences in surge capacity. That is, if there's a burst of traffic, it may be that some slower nodes or lower bandwidth nodes will fall behind a little bit for a little while. But we need them to be able to catch up. It needs to be only a surge.
00:29:08.408 - 00:30:11.774, Speaker A: We need to make sure that we are not running above the capacity of the minimal node for very long, and that if we do go above capacity that there'll be enough of a lull that they can catch up. Right? That's basically the principle. But unlike ethereum, where you want to be able to do that before the next block, we can afford to let slower nodes fall a little bit behind over longer time periods. You put limited brackets, you put the sequence that has limited ability to manipulate timestamps in brackets. Could you elaborate it a bit more? Sure. There are consistency properties on the sequencer's, timestamps that get enforced, they can't go backwards and so on. And when the sequencer posts to the l one chain, there are some checks that are made that the timestamps on things are consistent and haven't jumped by too much and so on.
00:30:11.774 - 00:30:43.962, Speaker A: This is actually complicated, so I didn't want to go into it in detail. But there are some checks. The sequencer has some power to backdate or forward date the timestamps on transactions. Timestamps can't decrease in the transaction sequence, but there are some checks to make sure that they don't get too far behind or too far ahead. Well, they don't get ahead of real time and that they don't get too far behind. All right, so what do we do here? What we do is what we call the gas pool algorithm. But you might think of it as slow 1559.
00:30:43.962 - 00:31:22.802, Speaker A: That it's like 1559, except the time constants are longer. So first we define a speed limit measured in gas per second. This is like the equivalent of the gas limit. So this is the maximum sustained average speed that a minimal node can do, right? So the minimal node can sustain this much speed. And so if we always ran with exactly that much gas per second, then the minimal node could keep up comfortably. Just like in 1559, we're going to go above this sort of average target usage for short periods. Okay? So we have this speed limit which is kind of like the average gas limit.
00:31:22.802 - 00:31:49.626, Speaker A: We have this notion of the gas pool. You. Can think of this as a tank of gas that is available to transactions. And so it holds between zero gas and some amount max gas. And whenever a transaction consumes l two gas that gets subtracted from the gas pool. And every time the clock ticks 1 second forward, we add a speed limit amount of gas to this. So it fills at the rate of the speed limit it empties according to consumption.
00:31:49.626 - 00:32:32.390, Speaker A: And you can think of this sort of as a reserve capacity that can deal with surges. And so the amount of surge that can happen is limited by max gas. And in practice, max gas over speed limit is around ten minutes. What that means is that you can have a surge that is, say, double the speed limit that lasts for ten minutes. We then run all the transactions in sequence in order as long as the pool isn't empty. If the pool isn't empty, if the pool empties, there's a circuit breaker that triggers and we have to reject some transactions. But we adjust the base fee based on gas pool fullness 1559 style, roughly.
00:32:32.390 - 00:33:32.734, Speaker A: So if the gas pool is entirely empty, we will add zero point 83% of the current gas pool per second. If it's entirely full, we'll subtract zero 80% and it's a linear interpolation in between. Why zero 80%? Well, if you multiply that by 15 seconds, you get twelve and a half percent, which is a 1559 rate. So what this means is this won't change faster than people expect things to change under 1559, which means people who write their code assuming 1559 should be fine here because the rate base fee won't fluctuate faster than that. Yes. Can the pool ever be empty? Yes, it can be empty because if transactions come in and are using more than the speed limit per second over a sustained time, the pool will gradually decline. So let me actually show you an example.
00:33:32.734 - 00:34:07.062, Speaker A: Maybe this will be clear in this example, okay? So we deployed this back in August and here's what we learned. It worked okay, but there were two problems. The first was slow to get started, and I'll illustrate this with a story. I have a graph here of time on the X axis and the gas pool fullness on the Y axis. The blue line here is the full gas pool. The line here in the middle is half full. And we're going to decrease the base fee if we're above half full, and increase it if we're below, but by an amount that's linear in how far we are from that CenterPoint.
00:34:07.062 - 00:35:18.206, Speaker A: All right? Suppose that a surge comes in and we're getting traffic which averages one and a half times the speed limit over a period of time. And so the gas pool fullness over time will go down, right? Because on average, every second we subtract one and a half speed limits for transactions and add just one speed limit and so the gas pool fullness will go down, right? And if the traffic is one and a half times the gas limit, then after ten minutes, we'll cross that center line, and at that point, the gas fee will start to go up, right? And so when I say this is slow to get started, there's really two things happening. One is you and I can look at this curve and say, look, we're over capacity. Maybe we should start boosting the price. But the algorithm will not do anything until that red curve crosses the midpoint. And only then will it start its exponential increase. And of course, exponentials are relatively slow to get started, and it looks like they're slow to get started and then they accelerate, right? So we can have an overcapacity situation for a long time before we even start to move the base fee.
00:35:18.206 - 00:36:06.358, Speaker A: But also remember that l two base fee is maybe a third of the total cost of the transaction. So the thing that we're boosting the price of is only a third of the total cost. And so it has only a third of the incentive effect that you might expect. And so we need that exponential to grow a fair amount, say, when that doubles, then it's still only half the total cost. And so as the exponential starts climbing its slow climb, it doesn't really bite very much in terms of incentives for a while. And so what we found is that when we had a surge that was sustained like this, the system was relatively slow to start adjusting the price. The other problem, which is really another version of the same problem, is it tends to overshoot.
00:36:06.358 - 00:36:34.186, Speaker A: So imagine a story like this where we have a surge, it goes below half, the price starts to escalate, and either the surge subsides on its own, or else people respond to the base fee boost. We don't really know which. But anyway, the traffic goes down and the gas pool starts to refill. Now, when you're on this upward climb over toward the right side here, you and I would say, well, this looks pretty good. We don't need to boost the price anymore. But since we're below half, we keep boosting the price. We have boosted the price.
00:36:34.186 - 00:37:31.718, Speaker A: The exponential looks steep now, and yet we're still continuing to go up and up and up, even though it looks to our eyes like the problem has been solved. So it tended to overshoot, which meant when we had a surge and the base fee boost happened, it tended to raise the base fee above where it looked like it really needed to be before things settled down. So the fix for both of these is to say, wait a minute, we know about control theory. Let's think about this from a control theory standpoint. Control theory would say we should mix in a derivative term, a term that looks at the derivative of the gas pool fullness and incorporates that. So the way we do that is to have a sort of running average of what the gas pool usage has been over a timescale of about 1 minute. And then that pulls up or down on the base fee depending on whether that usage over the last minute running average is above or below the speed limit.
00:37:31.718 - 00:38:13.146, Speaker A: And then the gas pool fullness pulls up or down as well by a smaller amount. So the maximum change is still bounded by the 1559 rate. But these two factors pull separately. That means that when things start heading down there will be something trying to pull the base fee up almost right away. But of course, because the gas pool is relatively large there'll be this other component saying don't panic, we have a lot of gas left but this will cause a faster reaction. The other thing we do is we set the gas pool target the point at which we start escalating not at half full, but at 80% full. So we made this change a couple of months ago and it seems to work really well.
00:38:13.146 - 00:38:50.054, Speaker A: This algorithm seems in practice to do just about what we would like to do. If there's a little surge, the price will just inch up a little bit, the base fee and then it will go back down. If there's a big surge, it seems to react in a reasonable way. It will raise the base fee a bit, it will generally stabilize as the gas pool starts to climb out of the trough. And this seems to behave really well. We get the behavior that looks to the eye relatively healthy and we don't get the huge spikes in base fee that we got before because of the overshoot. So we're pretty happy with this.
00:38:50.054 - 00:39:18.270, Speaker A: Obviously, there's room for improvement here. Let me talk about the third issue, which is Storage Bloat. Okay, storage economics. First. In Ethereum, storage is forever if you deploy a contract. If the contract writes to storage, the Ethereum model says that nodes need to keep that storage forever just in case it matters later. And that means your storage only grows and grows and grows.
00:39:18.270 - 00:39:56.006, Speaker A: That's one issue. The second issue is that the fee for storage allocation is collected upfront when you allocate but the cost is imposed on all nodes forever. Forever is a long time, but that means you don't know how many nodes there will be or who they might be in the future. The good news is Moore's Law helps. Moore's Law allows us to discount those future costs pretty rapidly. But still this is an issue. The cost function of that is what is the cost of running a node depending as a function of the size of storage is not linear.
00:39:56.006 - 00:40:29.718, Speaker A: It has steps in it. And the reason for that is each storage technology will scale up to a certain capacity and above that you need to switch to a different storage technology that's more expensive. And when that happens, two things happen. First of all, the cost function takes a step upward. And second, people who already have a sunk cost in equipment with the old storage technology, they can't run nodes anymore and that drives you towards centralization. So both of those are things we want to avoid. So that's a complication because there are like cliffs we want to avoid going over in terms of the storage size.
00:40:29.718 - 00:41:17.830, Speaker A: Storage can be hoarded, right? If storage is cheap now and it's going to be expensive in the future, people can grab storage now, allocate it and hold it, and then they'll be able to use it later. And so we should expect hoarding behavior if it's incented. And then finally, if you aspire to run a chain that is faster, then your storage will bloat faster. If Arbitrum is running at ten x the capacity of Ethereum, then our storage will bloat at ten x the rate of Ethereum's storage and our problem will be ten x worse, but actually more than that because of the nonlinear cost function. Yes. In your opinion, is that a bubble that can burst? Is this a bubble that can burst? Complicated question. There are a bunch of things that are happening and I'm going to talk about two things.
00:41:17.830 - 00:41:53.262, Speaker A: One is technical changes in Ethereum that may come that help to mitigate this problem. In particular, the storage is forever issue could be addressed by changing the model so that storage is not forever. Or maybe storage is forever, but only if you keep paying. So that may change. I'll talk a little bit about the prospects for that, how that might change and the prospects for it in a minute. Okay, so, preliminary approach. So what can you do about this? Well, you can change the model, but we want to remain compatible with Ethereum as long as we can.
00:41:53.262 - 00:42:27.354, Speaker A: Ethereum will probably change their model, but it's frankly not close to the front of the queue in terms of changes for future hard forks. So Ethereum may change this in a few years. We might need to change it sooner. We'll see. The other thing we can do is price storage, right? Eventually, if there's not a technical change to reduce storage growth, the other option is pricing. So let me talk about storage pricing and Vitalik has written some about this issue and I'd recommend his writings. I'm going to sort of build out a little bit more a concrete model.
00:42:27.354 - 00:43:04.310, Speaker A: So here's one way to do it. And this is sort of our preliminary thinking. I'll emphasize we're not doing this currently or yet, but this is our thinking about where we may have to go. First, you set a storage size target which is defined something like this. T zero is an amount of storage that a minimal node can comfortably hold now at time zero. And then you're going to grow that exponentially because you're assuming a Moore's Law curve so that alpha defines sort of what Moore's Law growth rate you're assuming, right? So a nominal node will be able to hold more in the future than it can now. And thank goodness for that because we'd be in big trouble otherwise.
00:43:04.310 - 00:44:05.050, Speaker A: And so, generally speaking, we want to restrain the rate of growth of storage to be roughly the rate of growth of storage capacity as defined by this Moore's Law curve. So this is how we define we set a storage size target and then we have a pricing algorithm which uses the exponential pricing mechanism which Vitalik has written about the justification for it. But it looks roughly like this that the price is some base price which is relatively low times a term that's exponential in how far over the target you are. So if you're below the target, storage is super cheap. If you are above the target, then the price goes up quickly. So the equilibrium here is one where the storage usage S runs parallel to the growth in the target capacity and the vertical distance between them is equal to the log of the equilibrium price. So you have some demand curve for storage which may change over time.
00:44:05.050 - 00:44:46.118, Speaker A: And what this gives you is a price that's relatively stable over time. And how stable it is depends on how you set this beta parameter. But the equilibrium price over time is basically or the capacity is close, the usage is close to capacity and it's over by the log of the equilibrium price. And because log is a nice function, it's close to the target. And so that means that unless people are willing to pay an astronomical amount for storage or even if they are actually that a nominal node will be able to hold all of the storage within this model. So this is kind of the first thing you might try to do. But there's a problem with this.
00:44:46.118 - 00:45:24.030, Speaker A: And the problem is hoarding. If we start below the target, if storage use at time zero is below the target then the price of storage is very low. And it's very low until the storage usage gets up to the target or indeed, until it gets up to equilibrium. And so this will induce hoarding behavior. Rational parties, as soon as you turn this system on, will grab as much storage as they can while it's essentially free and you will almost instantly get up to that capacity curve. And so we don't want that. So instead what we want to do is release the initial surplus storage the difference between the initial storage level of the system and the target.
00:45:24.030 - 00:46:11.634, Speaker A: We want to release that into the market more slowly, gradually. And a way to do that is to modify the price, the target function, so that you take the initial target, which is the true capacity of an assumed node, and you subtract off a term, which is the storage you're holding out. The initial surplus that you're holding out and the amount you're holding out decreases exponentially over time, reflecting that release, gradual release of that held out storage into the market. So this is what we suggest. This is really sort of back of the envelope economics and there might be better ways of doing this. We would really be interested in understanding better ways of doing this. Hopefully you can understand the constraints and the motivations, but there might be better ways.
00:46:11.634 - 00:46:42.460, Speaker A: Are you influencing the price of the coin? All of these fees in our system are paid in ETH and so we don't have a separate coin that's used. We don't have a separate coin at all. But in particular relevant here is that there's not a separate coin that these fees are denominated in. They're all denominated in ETH. It might impact the price of ETH. Yeah, that's right. Yes.
00:46:42.460 - 00:47:40.814, Speaker A: Would it make sense for us to tokenize? That's a complicated question and it's one that my lawyers advise me not to answer. All right, so better yet, maybe we could change the model, the fundamental execution model of ethereum to try to reduce the storage footprint. And there's a bunch of things that may happen to do that. One thing you could do is well, you could say first of all first thing to note is that regardless of what you do, the storage footprint will always at least be proportional to the amount of current or recent activity because there's some working set that needs to be kept around in order to be able to service the things that are likely to happen next. You could change it by adding storage rent. So you have to pay rent or do something that has an ongoing economic cost in order to maintain your storage. Then you're going to delete the storage if the rent isn't paid.
00:47:40.814 - 00:48:24.030, Speaker A: That is a big change in the model though, because users of a DAP may worry that their state within that DAP will evaporate if someone else doesn't pay rent. You could try to reclaim or archive unused storage. You could say if storage isn't touched for a year, we'll delete it, or more likely, we'll archive it and replace it by a cryptographic commitment to it. And then anyone can reconstitute that storage by providing it back and the system checks that against the cryptographic commitment. There are live proposals to do that. You could try to create stronger incentives to deallocate, but you have to be careful because incentives to deallocate make hoarding more attractive. Or you could switch to a MultiChain situation.
00:48:24.030 - 00:49:05.130, Speaker A: That is, if one chain has its storage sort of maxed out and the price of storage is higher, it now becomes attractive for someone to move a new application or new activity onto a separate chain which has its own low storage price. Right. With separate chains, there's separate sets of nodes. And so because of the step functioning cost, you're better off. So into the MultiChain future. If I had time for a fourth topic and if I had better ideas about it, I would point to MultiChain the economics of MultiChain systems as relevant. Right.
00:49:05.130 - 00:49:44.854, Speaker A: Different chains. You may have a main chain that has a higher cost, you may have various chains off on the side with lower cost, but more friction or higher cost to interoperate with the main chain. And it's interesting to think about the economics of that and what you might do to incent people to move over onto another chain. This, of course connects with technical questions about cross chain functionalities and bridging tons of interesting stuff there, but don't have time for it. Okay, that's all, thanks very much. Do we have any time for questions remaining? Okay, great, thanks. Hi, Ed.
00:49:44.854 - 00:50:41.500, Speaker A: So you mentioned arbitrary might make changes before Ethereum does just because of how things develop, but if you do things a certain way and then that diverges from how Ethereum does it. So do you have any thoughts on how you'd handle that? Absolutely, yeah. So we are more likely to adopt the change if we think Ethereum is likely to adopt the same change in the future. Because the considerations here are if we adopt some change, we want to do it in an Ethereum style and we want to take advantage of the Ethereum brain trust, the community that has thought a lot about these issues. So we're not likely to do something that's different from what Ethereum is contemplating. The question is, if Ethereum is considering something, if there's a well vetted EIP that will probably eventually be included in a hard fork, but not right away, we would think about adopting that. And then there are three outcomes, right? The best outcome is ethereum later adopts the very same thing.
00:50:41.500 - 00:51:59.586, Speaker A: The second best outcome is ethereum doesn't adopt it, but doesn't do anything conflicting. But the worst thing that could happen is Ethereum then does something that is in active conflict with the thing we adopted and we very much want to avoid that. So we would be in dialogue with the Ethereum community and the people who make decisions in Ethereum to make sure that if we do something that we can steer clear of conflicts. It is an interesting, important issue, but given that we expect to feel the pressure on some of these issues faster than Ethereum, and also that we are able to move faster than Ethereum can to make changes, not vastly faster, because Ethereum is slow, because Ethereum has a huge responsibility to its users and its community, right? We have a similar responsibility, not as large, because we don't have as much value on chain or as many as big a community, but we also have a similar responsibility. For us, the best situation would be to be ahead of Ethereum, a thing that Ethereum is going to do. But given the realities of hard fork politics, we will do our best to navigate this. Yes.
00:51:59.586 - 00:53:12.810, Speaker A: So understanding that the objective, we're always trying to minimize the cost of the transaction in L One could make in a scenario that you have even more fees in Arbitrum than in Ethereum, how can it manage? Can Arbitrum be more secure in some way in terms of the fees? Could our fees be more expensive? I think really the only way that could happen is if we have a much higher level of congestion than Ethereum has. So right now, the biggest component of our fees is L One data costs. And so because of the compression, and because we have some freedom to vary the timing of when we post batches the sequencer posts batches on the L One, the L One call data costs are substantially lower than on Ethereum. Compression is the big win there. So that part will always be significantly cheaper than Ethereum. If Ethereum adopts changes like EIP 4844 protodunk Sharding or Dunk sharding, our fees will go. Our L One gas costs will be even much lower than that.
00:53:12.810 - 00:53:58.970, Speaker A: So we're confident that the L One call data portion of the fees will always be significantly lower than on Ethereum. So the other component is basically the congestion driven gas component. And we will have a higher gas capacity, a higher speed limit or gas limit than Ethereum by a substantial factor that's going to change over time. That gap will grow over time. But it certainly would be possible that if our, say, gas limit were 20 x Ethereums, but we had 25 x the demand, that our fee would be higher. That would only be the case if people find it more attractive. Economics says that that situation would only persist if the higher fee was worth it somehow.
00:53:58.970 - 00:54:33.920, Speaker A: But I don't think that's likely to happen. I think with equal fees and equal latency and so on, people would prefer Ethereum. But a big part of the value proposition that we provide is lower fees. That said, supply and demand are what they are. And if there is much more demand for Arbitrum than for Ethereum, then the prices will be higher. That's not really in our control. I don't think that will happen.
00:54:33.920 - 00:54:37.700, Speaker A: Food us are right up time. Thanks, everyone.
