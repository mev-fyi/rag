00:00:10.410 - 00:00:55.370, Speaker A: So yeah. Hi everyone. My name is Zanska. I'm a member of the Quilt team, which is basically like a collaborative Ethereum code development research team between Consensus, the company, and the Ethereum Foundation mission. And I'm going to talk a little bit about multidimensional resource pricing on Ethereum, specifically in the context of ERP 50 59. And so basically this is kind of inspired by a small write up that Vitalik published early this year, kind of initially introducing the idea and then basically since then we've been thinking about it a little bit and specifically started working on it last week. So this is all very fresh.
00:00:55.370 - 00:01:30.650, Speaker A: So just basically as a small warning here, this is just a primer. This is basically the very first kind of thoughts that we've had around this topic. It's more about introducing the problem statement and less about presenting a specific solution. So I wanted to start by just giving a little bit of a motivation. So this is just a screenshot of a transaction from today, actually from Ether scan. And as you are probably aware of, if you've ever sent a transaction on Ethereum, right, you actually pay for the transaction. So there's like this transaction fee, the gas price and the gas limit and whatnot.
00:01:30.650 - 00:02:24.506, Speaker A: And a common intuition that people have when they kind of send a transaction is that what they actually pay for is kind of the effort that they cause, right? Basically the effort to the nodes, to the miner, that that's where the money goes. But in a sense this intuition isn't quite correct, right? And if you look at the transaction, you have this new field priority fee and that is actually only the part of the transaction that pays for the inconvenience to the minor, the entire rest. And in this case there was 50 guay. Like the vast majority transaction does not actually basically reimburse the minor, right? It does something else. And if you think about it, I mean, of course it's Ethereum, it's just like a normal supply demand price equilibrium. But the relative price elasticities here are important. This is just a graph from another write up of mine from last year.
00:02:24.506 - 00:03:36.986, Speaker A: It's not quite on topic, but I thought it illustrates the point quite well that you see that basically supply actually is fixed on Ethereum, right? So there's no like when people, if there's, if people are willing to pay higher prices, you still don't get more block space, right? This is basically fixed block space. This is of course pre 1559 and so the equilibrium is completely determined by demand, right? And so basically, what pricing on Ethereum actually does or is supposed to do is to ensure optimal resource allocation, right? So it's not about kind of paying the miner for the effort, it's about actually distributing resources in an efficient way. And then with EP 50 59, this looks a little bit more complex. Again, ignore the details, but again, supply is still fixed of course. And actually because this is the case, that's actually the intuition if you want to, why we were able to under 1559 to burn the fees, right, because they're not needed to basically motivate the miner to include the transaction. So we can instead basically turn them into protocol extractable value in a sense. So what are these resources that we want to efficiently allocate on Ethereum, right? Actually if you look into it, it turns out there are quite a few different ones.
00:03:36.986 - 00:04:40.226, Speaker A: So this is definitely not a conclusive list, but some of the most common ones. So there's bandwidth, right? How big are these blocks that are gossip between the individual nodes? There's compute for processing the blocks, state access, memory, then there's state growth. So basically like the state is that all nodes have to permanently hold and that grows forever, right? And that's expensive. But then also there's history growth and that's just like the keeping the historical blocks around. If you now basically look back at the transactions we just looked at as you might have noticed there's only a single price, right? There's no bandwidth price and there's no compute price. And whatnot this is a single price. And so an obvious question you might ask is how do we actually end up expressing all these different resource consumptions in a single value, right? And as it turns out on Ethereum, how we basically have been doing it from the very beginning is just that we say we just set fixed relative prices.
00:04:40.226 - 00:06:06.786, Speaker A: So we basically just say in addition in the EVM if you just want to add two values that costs I don't actually know, I think two or three gas and then accessing storage slots that costs maybe a few thousand gas. So the relative price is basically on the order of 1000, right? So like state access is always 1000 times more expensive than that specific kind of type of compute, right? And of course kind of just manually fixing these relative prices that's not not like you can't get it quite right but at least for the beginning that kind of turned out to be kind of good enough, right? And so the goal was then how do we kind of set these relative prices was just to say in a worst case, just imagine a block that does only additions. Like every single operation in the transaction in the block is just additions. The idea is we basically set this maximum total limit of resource consumption in the block so that even if it's full of additions, the additions kind of like don't consume too much of the resource that they mostly use, which is compute, right? If instead the block is full of state accesses then also it should not consume more than the maximum amount that we kind of determine for state accesses. So basically this is initially how these relative prices were set. If you actually look at the list of resources. Again though, you relatively quickly, if you think about it, start to notice that there's two different types of maximum consumption and in particular there's what's called blue here.
00:06:06.786 - 00:07:07.026, Speaker A: That's what we call the burst limits. So that's a limit where basically we really want to have guarantees around what is the maximum consumption per block, right? We don't want blocks to be too big, otherwise they just don't reach nodes in time. We don't want that. They take too much time to process all these kind of things. So all the blue variables, we want to really have fixed assurances per block. And then the orange ones, the state growth, the history growth, those are what we call sustained limits, right? So we don't really care how much extra history growth a single block causes. It's about what is the maximum average, right? So 1000 blocks, how much do they contribute to history growth and how much history growth is there in a given year or something? Basically, pre 1559, what we did was we basically just took the same gas limit and said we just use it for both the burst and the sustained limit, right? And I don't know, we just don't care what kind of limit it is.
00:07:07.026 - 00:07:53.540, Speaker A: But if you think about it, really has quite different properties. So the burst limit is a hard limit per block they really have to enforce. Whereas the sustained limit, you can go above that in an individual block, you really only care about the averages. And so it just turned out, and just by pure coincidence, that that has good synergies with what we wanted to like the design goals behind ERP 1559. So basically we wanted to make, in a sense make demand legible to the protocol so that we can have the base fee and extract it instead of paying it to the miners. We did this by basically having this base fee an explicit variable in the protocol. And the thing is, whenever the demand decreases, that is immediately legible to the protocol, right? We didn't have to do anything about that because blocks just are no longer full.
00:07:53.540 - 00:08:44.446, Speaker A: Pre 50 59. Basically blocks were always full after 50 59, if you didn't do anything, like if the base fee is too high, blocks are just not full. The problem is you don't have an equivalent mechanism on the other direction, right? If the demand increases but blocks were already full from the inside, you don't see anything about that. And so we wanted to also make that eligible. And so thinking about these burst and sustained limits, basically there was this very natural solution of just saying we split those, right? Instead of just having a single gas limit in a block, we use the gas limit just for this burst, for limiting the burst usage. But then we have the separate gas target, which is in our case like half as much of the limit. And that is basically now the sustained limit, right? So basically all the assurances around how much on average many blocks in a row will use that's.
00:08:44.446 - 00:09:46.230, Speaker A: Now this gas target and in that way we just basically very naturally also made the kind of the other case like the demand increases observable by the protocol. If you kind of followed along with that explanation so far, there might be two questions. I mean I guess maybe I don't know a couple of questions, but two in particular that you might have been asking. And the first one is wait a second. But if you actually change the burst limit and you decouple the burst and the sustained limit, wouldn't that have required, like, a repricing of basically the resources, like by type two X against each other? And then the other one also, doesn't that mean that now all the burst limited resources are basically, on average, only half utilized? Right. Isn't that like an efficiency loss? For the first question? It just turns out there was again just another coincidence. We had this mechanism called refunds in Ethereum that already had basically the same property that individual blocks could basically, I mean I'm kind of generalizing it a little bit, but could basically be two X the total throughput.
00:09:46.230 - 00:11:19.300, Speaker A: And so at the same time we introduced this 50 59 mechanism, we removed refunds and that way basically we didn't have to deal with any repricing. I guess we got lucky. The second question is the more interesting one, right? Indeed, if you have this target so on average gas is only half full, but you have these types of resources and we can look at them again, right? The blue ones that basically you could every single block, you could just go to the maximum, you wouldn't care, right? You basically on average you're wasting half your bandwidth, you're wasting half your compute, half your state access, half the memory. It's just basically not being used. So isn't that a problem basically, right? I mean it was of course already a problem with the refunds before, so it's not like a new problem with the mechanism. But isn't that still a problem basically? And as I was saying on that slide before yes, but so there is a but and if you basically look at this list, it turns out that most resource types, they don't only have a burst limit or only have a sustained limit, they usually kind of have this interesting pattern where they affect both sides. So if you talk about bandwidth, right, how big the individual blocks are, those blocks aren't only gossiped and so have this immediate burst concern, but then they are also saved on disk as part of the history and so they also contribute to the history growth, right? So basically block size has both a bandwidth constraint but then also history growth constraint and so it actually has a constraint on both sides of this diagram and similarly state access.
00:11:19.300 - 00:12:32.938, Speaker A: It's a little bit more tricky than I made it out here to be because it's only about state access at new locations. State access at existing locations only is burst limited, but state access at new locations extending the overall state size, that also has a sustained limit. And basically that kind of rescues us here at least to some extent because as I was saying, many resources basically have both limits and usually they are limited actually then by the sustained limit, right? The main concern for example, we have with state access, right, state growth is that it's not like the time it takes within a single block to access the state. It's really about how quickly can ethereum state grow? Right? And this is the main binding constraint here. And so these type of resources basically are just not affected by this decoupling at all. And this is actually like a constraint in general for just for any 1559 style mechanism on ethereum. It is true that resources that are only burst limited are on average underutilized or basically you can't on average basically reach their maximum utilization under any type of 1559 style mechanism.
00:12:32.938 - 00:13:29.694, Speaker A: This is not the problem this talk is trying to solve. This is just basically pointing this out. But it is I think a really important design consideration here and then basically with that as background now kind of like I mean we already talked about resources but basically why do we want to maybe move to having actually multidimensional resource pricing? What would that even mean? And for that first kind of I want to talk just briefly about what actually are the problems with one dimensional pricing. And there's really one problem that I think I consider being like the main one and that's again resource underutilization but of a different kind. And to look at this, it's really pretty obvious once you see this graph it's basically just it's a pretty bare bones graph. This is part one of the graph. So basically if you today on ethereum, right, you have resource A, resource B just like the simple two dimensional case.
00:13:29.694 - 00:14:46.242, Speaker A: So one could be compute, the other could be bandwidth whatnot right? You basically because they both contribute to the gas consumption but you only have a fixed amount of gas per block. You can basically use only compute or only data or if you want to use both basically they have this trade off relationship that is artificial, right? You wouldn't want them to have this trade off relationship because actually you could use the maximum allowable of both dimensions. But right now in this one dimensional pricing that's just not possible. So what we want is actually this, right? And of course if you imagine like the five dimensional case there would be some I don't know what they're called hyper rectangle or something I think it's actually the word basically this is really the desirable outcome. We still have these limits, right? You can't use more than the kind of the limit of resource B and resource A but we want any combination of those two to be possible, right? And this is what this multidimensional pricing would enable because all of these resource types would just be basically treated and accounted for individually. Other problems is that we have this two X burst multiple in the 1559 style mechanism right now overall resource types and some of them actually two X kind of makes sense if you think about it. Otherwise the kind of the burst limit also starts to be more problematic.
00:14:46.242 - 00:15:47.834, Speaker A: As we were saying, in these only burst limited resources, two X is already kind of costly because it wastes resources but other resources we could have ten X, 100 X and it wouldn't matter and it would actually allow for much easier, much more interesting, 59, much more sensitive targeting adjustment solutions, right? So basically this is just kind of like an annoying side effect of the kind of one dimensional pricing also and that's more like a forward looking statement. New resource types that we are about to introduce on Ethereum are less correlated with the existing ones and so basically these problems basically will become bigger going forward and would require frequent manual kind of relative pricing adjustments. So basically, we would have to go in every few weeks and basically we have a hard fork that just says, okay, wait a second. Kind of the market condition changed. And ideally, we would want to go in once every hour or something, right, and just basically change the relative gas prices. And that's, of course, just not feasible. Right.
00:15:47.834 - 00:17:28.362, Speaker A: And for that basically the solution is really kind of I mean that's kind of the whole talk, right? Just kind of like giving that intuition we really would want to price all these resources individually. An interesting case study and that only came out of actually the sessions we had earlier this week is L2 call data. So why is this such an interesting case study if you are on a L2, right? And in particular these were conversations we were having with Arbitrum and optimism and whatnot but I think that that really holds true among all L2s there's a new type of resource and that's layer one data, right? Because not only do you have all the kind of the cost associated with running a transaction on L2 and the constrained resources there, but then also those have to be anchored on layer one and in particular with data like computation you can compress really efficiently as a rollout. That's the whole point, right? But data you can't really compress so you still have to pay for the full layer one data availability. In principle it's still like a burst limited resource although it would only ever reach this limit if it actually consumes all of the available air one data. So in practice the limit isn't really relevant but actually this for the first time is a resource that actually where you're paying for the cost you're not only paying for in order to basically express your desire for resource consumption, it is actually a real cost that has to be accounted for. So in this case, the sequencer has to actually go out and buy the underlying layer one data and this kind of this relative price.
00:17:28.362 - 00:18:23.142, Speaker A: So kind of the price for layer one data and then for L2 execution. As I was saying earlier, they are like less correlated with each other. So they are very heavily fluctuating. And so you'd really have to go in probably in every hour or something and just, I mean, maybe even when an NFT drop happens or something like within five minutes and be like wait a second, now all of a sudden data is really expensive on a roll up and this just does not work right. So we really need this two dimensional mechanism here, very importantly. And actually roll ups have been looking into this already. We talked to them, we're like, yeah, we have this new idea about multi dimension resurfacing and they were basically like, finally, what took you so long? And it turns out actually that by the way, this is also just by pure coincidence, again, very similar to the problem of another type of resource that we are about to introduce on Mainet.
00:18:23.142 - 00:19:18.954, Speaker A: And that's why we were looking into it. And that's this if you've heard about this like the protodyng Sharding ERP 4844 Blob transaction pricing and that's again a new type of data and that basically really would want to be separately priced. And so basically it's very similar here. And so basically how would this work in practice? How would we start pricing this in a two dimensional way? The first one is just especially now in the context still of L2 call data is a mechanism where we would want to determine how much should the data cost in any given moment. So basically L2s would have to have some sort of layer one gas price. Oracle the nice thing is that that can be done in a trustless way because I mean, they already are based on top of layer one. And then in the case of Blob transactions on mainnet, you just have like another independent 1559 star targeting mechanism where you just say like, this is the amount of Blob data we want on average.
00:19:18.954 - 00:20:12.110, Speaker A: This is the amount of Blob data in the extreme as a burst limit. And then you basically just do the typical 1559 stuff and then this point and that's actually just because again, this part of the presentation was basically only done this week. So this is kind of where I'm the most sad. I didn't have more time to kind of really explain this a little bit more just because it's really interesting. But if you're interested in that, go come find me afterwards. So how would we actually charge people specifically for this two dimensional resource consumption? And there are basically two approaches that are used right now by roll ups and that's one is the floating correlator gas consumption and I think that is the arbitram way of doing it today. So that's basically instead of having a fixed gas price per call letter byte or something, it basically just varies the gas consumption based.
00:20:12.110 - 00:21:04.954, Speaker A: So that basically the total amount of amount you pay exactly accounts for the layer one cost, right? So all of a sudden you have this floating gas cost and so that basically breaks this invariant that we're talking about earlier. That basically you fix these relative gas costs of the individual resources and that allows you to have this floating mechanism. The other alternative approach here, that's the optimism approach right now is that you just basically charge the user in ETH directly for that part of the cost. Both of these mechanisms have some advantages, some disadvantages. Again, not enough time to go into that right now, but just to basically outline here the open questions and as I was saying earlier, I basically have to end the talk right when it starts to get interesting. So this would be already closing in on the end but open questions here. Also looking towards extending this into the multidimensional case, not just two dimensions.
00:21:04.954 - 00:22:41.226, Speaker A: So how would we actually charge users for that, right? How would that part of the mechanism work? What parameters would we want to expose? So basically there could be something like a max eats per Blops or per call data or something in the very simple two dimensional case. But how does it scale? Would users end up having to set like 15 different gas limits for their transactions, also 15 different priority fees? Then how basically does it look for the user? Which dimensions and resources do we actually end up wanting to basically separate out and have as individual atomic types of resources? How does that work with this kind of the burst sustained limits that we talked about? Again, some resources only have a burst limit. How would we basically do that there? Do we basically have to only have a very small multiple so we don't lose too much of the average available resource usage basically. And then and that's a really big one as well, how do we ensure backwards compatibility? And specifically this is really important for dynamically metered resources. So what's that luckily L2 call data and Blob transactions are not dynamically metered, right? You can look at a transaction from the outside and you can just say yes, it has this much call data or this much Blob data, but some other types of resources, right? Like how much like storage, accesses, computation. You only basically see how much the transaction consumes while it is running, right? Doing execution. And inside of the EVM, there's a lot of baked in assumptions about how this is being metered, right? Like when you do subcall, you send a fixed amount of gas with that subcall.
00:22:41.226 - 00:23:07.954, Speaker A: That's a one dimensional value. So you can't all of a sudden make that five dimensional. That just wouldn't work with existing contracts, right. So you basically have to find a way to also make these types of resources and resource pricings compatible. So what's the plan here? Basically, again, as I was saying, this is like the very early stages of just exploring this. So short term goal is just design a mechanism, a simple mechanism for this two dimensional use case. So L2 transactions and Blob transactions.
00:23:07.954 - 00:23:41.460, Speaker A: And I think L2s were very adamant that they need this fast, otherwise they'll just do their own thing. And I mean, of course, fair game, but it would be nice to have a standardized solution. So we also really want to get Blob transactions into the next hard fork after the merge. So we want to have this mechanism fixed in place within a reasonable amount of time. We do want though, to make sure that whatever we end up picking for the two dimensional case that this is forward compatible. Right. So it should really be easy to extend it to the full multidimensional pricing case here.
00:23:41.460 - 00:24:06.570, Speaker A: And then basically this would be the third step, like then actually afterwards designing this full multidimensional pricing system. And of course as last step never to forget Utopia. So that's where we want to end up in. Yeah. And that's basically all. So I guess thanks for the attention and if you want to find the slides, you can find them on my Twitter.
00:24:10.270 - 00:24:35.010, Speaker B: Thank you. What I love about EAP 1559 is that it gave introspection to the protocol. Like it knows how much resources it's using and this is a way of just multiplying the granularity of that introspection. And I think it's crazy what we can achieve with better systems. Jesse but yeah, thanks for the talk. Is there any question from the audience? Alex.
00:24:39.910 - 00:25:08.160, Speaker C: Thank you. Thank you for the talk. Anzga, I have two questions. Maybe some maybe one will be maybe I'm not understanding it correctly. Does it make sense for this multi dimensional pricing for all of that to be priced in the same unit? Because even though they would be like multiple dimensions, one dimension, if you need a lot of ether, for example, for one resource in particular, it's going to affect the price of the other resources, not in the ether unit, but relative to USD, for example, does that make sense?
00:25:11.410 - 00:25:30.418, Speaker A: Basically the point here would be that you'd have to price them in separate units and then at the end you of course have to map that down to like a common east price or something that users would pay, right? But if I don't know the price for storage, would two X that would not affect the price for compute.
00:25:30.594 - 00:25:38.310, Speaker C: But if you need ether for both. Now there's more demand for ether, for storage than for the other resource.
00:25:42.190 - 00:26:04.174, Speaker A: Right, but how would that I'm asking you maybe it's not no, it's an interesting point. I don't think that kind of the demand for ease just because in good approximation is basically infinite in just the scope of a single block. So I don't think that those are I don't know. I'd have to think about it a bit. But yeah. Okay.
00:26:04.212 - 00:26:04.494, Speaker B: Thank you.
00:26:04.532 - 00:26:05.098, Speaker A: Happy to talk.
00:26:05.124 - 00:26:06.178, Speaker C: I have, like a second question.
00:26:06.264 - 00:26:07.234, Speaker A: Small, right?
00:26:07.352 - 00:26:11.730, Speaker C: Are there any use case or applications that you see enabled by multidimensional pricing?
00:26:12.390 - 00:26:15.730, Speaker A: Use case, replications? What do you mean by replications?
00:26:16.070 - 00:26:22.982, Speaker C: But for example, we could be able to do better parallelization because we'd be able to price resources in a different way.
00:26:23.116 - 00:26:54.286, Speaker A: That's a really good question. I would say once we have a framework like that, it would enable us to think better. First of all, I think it's really good to just have explicit what our limits for the individual resources are so that we can see where are the bottlenecks and we can just start kind of optimizing for them more. And then we can ask about what are maybe implicit hidden other resources. Right? Like, how does this relate to parallelization? It's actually a really good question. Right. If you look at other blockchains that really focus on parallelization, I don't know, solana is just like one of them that really focuses on that.
00:26:54.286 - 00:27:17.730, Speaker A: Right? That's exactly what they're doing. Have specific pricing for overlapping state access. So that could be something where now that we have this framework, I think it lets us ask this question a better way, basically. Now we could talk about it as a specific type of resource and how to integrate that into our pricing framework. And I think that would unlock new use cases. Indeed.
00:27:17.810 - 00:27:18.726, Speaker C: Thank you.
00:27:18.908 - 00:27:21.698, Speaker A: Thank you, Ed.
00:27:21.884 - 00:27:22.620, Speaker B: Sorry.
00:27:25.790 - 00:28:06.070, Speaker D: Thanks for a great talk. I was happy to see the discussion about burst versus sustained resource use. I think that's a really important consideration in practice. In 1559, the burst is defined over a time frame of one block. But it could be that for some of these resources, the relevant burst time is you know what, I'm going to ask that the relevant burst time is longer. So I'm just going to ask how painful that it would be if the burst was defined with respect to a time of, say, a minute or two minutes or five minutes. Sustained, of course, is always sort of long term.
00:28:06.410 - 00:28:27.326, Speaker A: Right. I think this is a really good question, actually, why I went back to the slide is just to point out kind of that there's also an effect in the opposite direction. So I kind of have memory be kind of slightly off color here. And that's just because memory, interestingly, is really limited only per transaction. So not even per block, right? Per transaction. Because after every transaction, you can just throw away. All the memory it used and you start fresh again.
00:28:27.326 - 00:29:21.402, Speaker A: So this is an example for the opposite case. So we would already have to integrate something that does not neatly fit into the per block pattern into this mechanism. And then actually, I think basically these longer term, longer window examples are probably again, I would say something that's more relevant in the L2 case, I assume, right? Because on the base layer we have this very fixed twelve second slot time, so there's no wiggle room. Everything that's per minute is just five times per block for us. But yeah, I think it's basically that is the whole intention here. I think we would want to come up with a framework that is as flexible as possible and really can be applied throughout EVM use cases. And so it would really hearing that, that basically makes me just mentally add it to the list of requirements, make it not strictly operate only under the assumption that these things are limited per block.
00:29:21.466 - 00:29:24.900, Speaker B: Yeah, we'll have time for one more question.
00:29:26.070 - 00:29:44.520, Speaker E: Thanks. Do you have any thoughts on Blob pricing when, say, Blob and cold data both use the same resource like they both use storage. So a block with a lot of cold data and a big Blob could be different to a block with a small amount of call data.
00:29:45.290 - 00:30:35.094, Speaker A: Right? Yeah, this is actually again a really good point. So I think basically most of the complexity is hidden kind of in this picture where basically it's about first of all, I mean, of course, where would you even integrate Blob data here? And then how do these individual resources even overlap? Again, I'm like state access, state growth. That's me just cheating because they barely overlap. Right? It's only about access to new state. And then how do you handle resources that partially overlap and then partially don't? Yeah, I think this is like I mean, of course you can always do the stupid thing and basically within that context revert back to the status quo. So you just basically say, okay, we have the combined limit and then we just accept that we basically are in the bad triangle case again. Right? So either only call letter consumption or only block consumption, or somewhere in between.
00:30:35.094 - 00:31:14.080, Speaker A: But of course you wouldn't want this, right? So again, this is really the problem. You can describe the problem and if you just make enough simplifying assumption, it looks like it's pretty easy, but then you start diving in and actually coming up with a mechanism that optimally accounts for all of this is really quite a challenge. So I don't think it will be like an easy thing to do. But also, I mean, everything is better than what we're doing right now. What we're doing right now is this in five dimensions, in ten dimensions. So we're really wasting the vast majority of our possible throughput. So anything that gets us away from this and closer to optimal, I would already be happy with, but, yeah, great question, actually.
00:31:14.080 - 00:31:15.014, Speaker A: Yeah.
00:31:15.092 - 00:31:18.770, Speaker B: Thanks, Sanskar. Super nice. Let's give him another round of applause.
