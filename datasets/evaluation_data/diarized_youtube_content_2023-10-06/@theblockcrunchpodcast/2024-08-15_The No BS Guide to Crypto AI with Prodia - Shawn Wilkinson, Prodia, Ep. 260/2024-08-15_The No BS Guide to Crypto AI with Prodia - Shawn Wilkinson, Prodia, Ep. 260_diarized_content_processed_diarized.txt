00:00:00.160 - 00:00:18.502, Speaker A: Like their cost was going up 50% overnight. They would have peak load time and it would kind of struggle to kind of perform. And they had just had access to one model, right. And so we came in and said, hey, during peak times, no problem. You know, we can just distribute that to our network. Oh, the cost, no problem. We'll cut that in half.
00:00:18.502 - 00:00:35.266, Speaker A: And, oh, by the way, it's like, you know, two to four times faster. Web three offers a brand new petri dish. Our job is to buy great tech at great prices. AI is also libertarian. Right. Enable use case that people haven't been able to do today. Hundreds of millions of players that they will come to the market through mobile.
00:00:35.338 - 00:01:12.674, Speaker B: My personal reason why I could be bullish in the next twelve months is lockrunch is not a licensed financial advisor and is purely for informational and entertainment purposes only. All opinions are personal, and the show may discuss assets that the host or guests have positions in or may be actively buying or selling. All investments carry risk, so consult with a financial professional before making any investment decisions. Hey, everybody. Welcome back to the BlockFrench podcast. I'm your host, Jason Choi. Now, as the market takes a downturn, more people seem to be getting more cynical, and a lot of people seem to be of the view that a lot of stuff in crypto AI is simply not playing out.
00:01:12.674 - 00:01:59.788, Speaker B: There's been a lot of promise bringing together web3 and AI, but very few actual success cases so far besides token price going up at one point. So substantiate this. So for me, as an investor, I often try to think from first principles, what in AI actually needs a blockchain? And I could come up with a few use cases. And one such use case is the problem of decentralized compute, which in my opinion, is one of the most impressive teams working on. This is Protea, which my angel fund Tangent has recently invested in. So I'm very, very excited to be joined by Protea founder Sean Wilkinson to talk about some real use cases behind AI crypto, the challenges facing some of the projects out there, and really cut deep into the question, does AI need crypto? So before we start, nothing we discuss on this show is financial advice, and our opinions are strictly our own. So, Sean, welcome to the show, man.
00:01:59.884 - 00:02:00.852, Speaker A: Glad to be here.
00:02:00.956 - 00:02:18.812, Speaker B: So I think Protea is really interesting because it has a story that differs from most other crypto computer networks I've seen. Most are started as almost experimental ideas, and they end up with way more supply trying to find demand for you guys, it's almost flipped so tell us about the story of Protea, how it started and what it is.
00:02:18.956 - 00:02:49.070, Speaker A: Absolutely. So I think it helps going a little bit into a little bit of my background. So you say, downturn. I've been in crypto since 2012, so this little dip is nothing. But I originally fell in love with the technology way back in 2012, was mining half bitcoin a day. Unfortunately, it turned it off way too early. It was making my room too hot, fell in love with the tech, got involved in a bunch of early projects in the space.
00:02:49.070 - 00:03:43.076, Speaker A: Bitcoin, purecoin, Primecoin, Ethereum. And I was working on a little project called Storage Storj, that did essentially distribute and decentralized cloud storage, one of the very first tokens out there, one of the very first ERC 20 tokens, and grew that project, which was just again focusing on essentially distributed and decentralized cloud storage. That was the idea I was mining with my computer back then. And I thought, hey, why can't I do this with essentially my hard drive? So, went down that rabbit hole, never came back out, grew that from four guys and a dog to a billion plus token. And that's growing in scaling today. So I'd been at this for a long time and just playing around with web3 tech. Now, web3 tech was the cool thing back then.
00:03:43.076 - 00:04:15.950, Speaker A: Now, what's the cool tech? AI. If it doesn't have AI in it, it's not cool. And a lot of people are building some really cool things in AI. Me and my co founder were actually the first 300 users of GPT-3 back in 2020. It was this amazing tool. People were just getting started with it. One of the things that we decided to do, along with many people who have built some very large companies out, it's build a little AI tool.
00:04:15.950 - 00:04:51.226, Speaker A: So we build out a little image generator and put it out there. And the same thing that happened to us happened to many other products that have come to be very popular in the AI space. It went instantly viral. We went from 200 users to 100,000 users in two months. And so it's obviously needing a lot of AI compute. And where do you get that AI compute? Well, it's very difficult to get, it's very expensive. You need to mortgage your house to put a down payment on a GPU at AWS.
00:04:51.226 - 00:05:09.704, Speaker A: It's very, very difficult to scale. And so we kind of came saying, hey, look, wait. We have all the experience in terms of building out distributed systems. We spend ten plus years myself building out distributed storage. We have all the expertise to build out distributed compute. Let's do that. So we did that.
00:05:09.704 - 00:05:59.216, Speaker A: We were able to scale that product out, and then we kind of realized, oh, wait, this is 50% to 90% cheaper, it's two to four times faster, and it's way more scalable. Right? And people are spending $100 billion a year on this in just the last twelve months, and it's going up. Maybe we should focus on this. So we went all in on building out essentially distributed compute, building out AI infrastructure, and really focusing on making it as simple as possible for the people who are actually building the applications. So we've been growing like crazy. As I said before, there's a lot of demand for AI compute, but we're really taking a new approach which is a more decentralized and distributed backend to really fix a lot of the core problems that anyone is facing right now actually trying to build these applications.
00:05:59.348 - 00:06:26.574, Speaker B: Before I get started, I'm grateful for Aptos for sponsoring the show and letting us continue to make this free content for all. So here's Jerome from Aptos to tell us a little bit more. In addition to crypto native use cases, Aptos has seen many web two major enterprise integrated network when other ecosystems have struggled to do this, such as Mastercard, South Korea's largest retail conglomerate lot group, and one of South Korea's largest telecom providers, SK Telecom. So can you tell us why Aptos is the preferred network for so many of these web two conglomerates?
00:06:26.672 - 00:07:14.766, Speaker C: So Aptos is a blockchain for the people, built and iterated on by the people. It is completely decentralized, scalable, secure and open to everyone, web two players and their audiences included. So what we focus on is on expanding the builder ecosystem. While some ecosystems attract only web3 natives, we believe the thriving ecosystems needs both experienced web3 natives and emerging enthusiasts to help grow the pie beyond 20,000 or so devs that web3 has today to do this the right way? There's two things that the Atlas foundation has focused on, right? So one is to make the experience building on applause as familiar as possible to the existing smart contract environments. Chainlink and Blipsight are examples of this. We also have dev friendly tools such as guest profiler, which helps you understand the guest usage of Aptos transactions. Number two, it does to make the experience of building smart contracts as foolproof as possible.
00:07:14.766 - 00:07:48.876, Speaker C: That's where move comes in. So move on Aptos. Not only is Aptos fast, secure and reliable and cost efficient, but innovative use of the move programming language is key. Move on. Applause makes it as easy to build secure, scalable applications as exists in web. Two safeguards are built in at the blockchain level, so developers only need to worry about creating the best possible experiences that will resonate with their users. With move devs can build lasting dapps on laptops without worrying about scalability, security and upgradability, because those qualities are built in at the programming language at a runtime level.
00:07:48.948 - 00:08:16.330, Speaker B: Thanks, Jerome. Now back to the episode. I love how you started with deep in before it was even called deep in, right back in 2012, 2013, and now you're coming all the way back to deep in again. So when you first started storage, right before diving the protea, you know, what gave you that idea? Because it was, you know, web3 wasn't really a thing, right? Nobody was talking about deep in. Everyone was just looking at miners mining bitcoin. So what gave you the insight that, hey, actually we can use a similar concept that is used in bitcoin but for storage.
00:08:16.630 - 00:09:12.830, Speaker A: Yeah. So again, it just came out of a personal project is I was actually working on a personal project that was storing a bunch of Twitter data, the Twitter fire hose, and doing like sentiment analysis on it. I want to do some trading on it. And it was piling up to a lot of data and I looked at the options at the time and what was it? Amazon, Google, Microsoft, cloud providers and it was too expensive back then and it still is today. So I was just looking for an alternate solution. I thought, hey, if I'm mining with my computer, with my cpu to this network and getting paid for it, why can't I do that with cloud storage space? And I just went down that rabbit hole and never came back out. So I was trying to find a solution for my problem that many other people have.
00:09:12.830 - 00:09:55.870, Speaker A: Same thing here. We were working on an AI project and this is like, okay, let's go get some GPU's. And oh my gosh, one, you can't find them, two, they're too expensive, and three, the software is very hard to scale out. And so it just took the exact same approach. It's just like, well, all these resources are sitting around just like you have extra hard drive space. Every computer is coming with a GPU or AI processing these days. Why don't we tap into that? And there's a lot of benefits that we can talk about of why you go distributed just for the core benefits of it's better, cheaper, faster than any centralized provider.
00:09:56.170 - 00:10:39.918, Speaker B: Yeah, I love to dig into that actually, because when you look at decentralized networks, for most use cases, you rarely get cheaper and faster. Usually the value prop is you get some privacy, you get some self sovereignty, you get to own your own data, but you're going to have to pay some gas fees, you're going to have to bear with some latency. But in this case, I think in the tweet recently, you mentioned the Protea can achieve 50% to 90% cost reductions and forex speed improvement relative to web two. So this is really interesting to me because people's intuition might be that without the economies of scale, with someone like Amazon running massive gpu farms in one place, the cost of running something like this in a decentralized ways should be higher. So how is this cost savings possible? Is there any kind of hidden catch there?
00:10:40.054 - 00:11:48.480, Speaker A: Yeah. So again, as I alluded to before, I've been doing this for a long time, and I think one of the mistakes that web3 makes is we build these distributed systems and we're very passionate and excited about the tech. I can build this network and have these nodes and all these things, and you build out all the possibilities and all these use cases that you want to hit and you build up. We end up building this really complicated tooling, right, that solves all the world's problems, but then you end up solving none of the world's problems. And, you know, I'll give this as an example of storage and where I went through before, and I'll kind of bring it back to Protea. So at storage, we built out this platform that was significantly cheaper than AWS S three. It was a lot faster, a lot more secure and private.
00:11:48.480 - 00:12:25.868, Speaker A: And it worked. We had this network, it worked. We gave the tools out there for people to use. We had some libraries that people could integrate into their applications and absolutely nobody used it. So it was better, cheaper, faster, but you essentially needed to use our libraries and bindings to put it into your applications. Developers didn't want this. Developers are some say lazy, others say efficient, right? There's a reason why even Google supports Amazon S three's protocol.
00:12:25.868 - 00:13:06.810, Speaker A: It's just the way that people, you know, handle essentially data storage in their applications on the backend, right, for anyone from enterprise to back in the day, Dropbox, they using that s three protocol to store that data. And so that's what everyone wanted. That's the shape that that problem needed to fit in. So it wasn't until we released essential or s three tools, then the growth took off, right. Network was still the same, right. You know, we had all those core value props but it needed to kind of fit in the shape of the problem that people who are actually integrating this technology actually needed. Right.
00:13:06.810 - 00:13:39.974, Speaker A: The analogy is square peg, round hole. Right. The peg needs to be wrong. So same thing here for kind of distributed compute. I don't know if you use any distributed compute platform, but they're very difficult to, you have to configure GPU's and clusters and all you have to add tokens. That is not the shape that these AI applications are actually taking. And so we really focused on just having a very simple API that a developer can integrate.
00:13:39.974 - 00:14:20.890, Speaker A: Five minutes they can hit the API, whether it's text, image or video. We focus really on images right now. It's very compute intensive, which kind of shows some of the benefits in terms of cost, performance, these kinds of things. That's where we kind of shine. But they just hit the API, they get an AI generation back, they don't have to think about anything else. They don't have to think about how many GPU's or what type or where model balancing and all these kinds of things. And so that's why today, Protea, in terms of the kind of decentralized space, the web3 space, we're the largest network by usage.
00:14:20.890 - 00:14:59.888, Speaker A: Just because we made it so easy for people to actually integrate and solve the problems that they're trying to solve. Right? Developer is trying to scale out an AI application. He can figure out how to acquire 100 gpu's on AWS and pay two year in advance and then trying to scale it. Or he could just hit our API and be done with it. And so that's where we've really focused, is just making it easy, getting a good developer experience. And again, that's translated to us being the largest platform in terms of usage. And probably in the next few months we'll be larger than all the other kind of deep end projects combined.
00:14:59.888 - 00:15:09.642, Speaker A: But our goal is not that our goal is to start taking away chunks from the big boys like AWS. So that's our focus, is to make it easy.
00:15:09.776 - 00:15:40.254, Speaker B: Yeah, I think the big dream, they're kind of battling with Amazon and kind of bringing back to the point about economies of scale. With Amazon, you kind of assume that these guys are able to operate things so efficiently that it's impossible to make things cheaper than them. So bringing back to Protea, where is the cost reduction coming from? Is it just from kind of the way that you guys have aggregated supply and its excess supplies? That's why it's not as price kind of sensitive or where does that come from?
00:15:40.342 - 00:16:58.040, Speaker A: Yeah, so it comes from basic economies of scale. So we're able to do essentially get the right GPU at the right place, the right time for the right job, and we're able to do that at massive scale, more than any individual user could do. We're also able to be smarter in terms of our load balancing. We're able to sit down and write different acceleration libraries and things that an individual company or team, it wouldn't make sense for them to do themselves. But when you're doing that on behalf of large volume of users, large volume of customers, then it makes a little bit more sense to say, hey, we can optimize here. And so it's a mix of the, again, the hardware that we have, the software that we have, all the expertise, essentially this data that is being processed in this network, where we can say, hey, we see these kinds of patterns, we can optimize here. Again, it's all the way from the hardware stack to the software stack that we make these optimizations.
00:16:58.040 - 00:17:51.429, Speaker A: But in aggregate, they end up being 50% to 90% cheaper for the end user, two to four times faster, and just way, way simpler. So for example, we have a version two of our API coming out for stable diffusion Excel workloads, and it's pretty much the fastest in the business in about 2 seconds. Normally you're seeing anywhere from six to eight for many providers or kind of doing it from base yourself. So we just try to make it as easy as possible. Again, we do all the acceleration libraries, we balance the GPU's and model load balancing. We have the right hardware in the right locations, right latency, right connections, and in all, it kind of sums up to those numbers that I talk about. So it's not just one thing, it's probably a dozen things, but it sums up to that.
00:17:51.740 - 00:18:08.240, Speaker B: I'm curious, like how much does the decentralization aspect add or detract? So if you did Protea as like a centralized network versus the current iteration, which is a decentralized network, how much of that decentralization, how much of that cost savings is coming from the fact that it's decentralized?
00:18:08.780 - 00:18:45.502, Speaker A: Yeah, so I would say it varies because we're always making improvements. Like I said, we just dropped a new API that's 400% faster. So obviously that changed the economics around quite a bit. And we're used both decentralized providers, miners and cloud providers. So I'd say I'd kind of do it by provider type that we have. I'd say it's about half. But I could see other use cases where you could really push on some of the software improvements we have, others you can really push on the decentralized network.
00:18:45.502 - 00:19:47.356, Speaker A: So for example, the more kind of distributed workers or distributed node operators might be very close to the end user, so you get great latency numbers. We're also finding that kind of, in terms of the more distributed decentralized workers, let's say you have a miner, like a typical web3 miners that has a bunch of GPU's. We have some tasks that doesn't necessarily need to be that fast, but you need a high volume of them. For example, generating AI generated NFT collections. And so we're using large amounts of previous generation GPU's. That would be very hard to get that number of concurrent processors from a typical cloud provider. So it is all a mix.
00:19:47.356 - 00:19:54.460, Speaker A: It depends on the use cases. But again, our whole goal is that you don't have to worry about any of that.
00:19:54.540 - 00:20:22.118, Speaker B: It sounds like being decentralized allows you to aggregate supply from so many different sources that you have so many different types of hardware, so that you can easily bespoke, kind of customize what type of use case, requires what software, and can match the demand much better than if it's just like a big centralized farm and they just have one type of GPU and the one size fits all and it's not as customized. And that customizability coming from decentralization is what's giving a lot of this efficiency gains.
00:20:22.294 - 00:20:48.088, Speaker A: Exactly, exactly. But it really depends on the use case. But I mean, we do have some centralized providers in the network, they have their benefits. Sometimes when you just want to spend a whole lot of money and you want things a little faster and h 100 does the job, but when you want large scale generation, using some of those decentralized nodes is the best use case.
00:20:48.184 - 00:21:19.716, Speaker B: Actually, speaking of use cases, I love to take a step back and understand what are the different use cases, because even in decentralized compute networks, there's so many different things that different networks focus on. For instance, I think a lot of people are familiar with one called render. They focus a lot on rendering of computer graphics. There are some that claim you can train large language models on them. There's some that allow you to do inference. So what are the most common or use cases that make the most sense for you, and how would you rank them in terms of the size of the opportunities right now?
00:21:19.868 - 00:22:24.360, Speaker A: Yeah, so I think there's oversimplifying. There's kind of two areas in terms of AI kind of resourcing, right? There's inference, which is running the model, and then there's training, which is obviously training the model, trying to make it smarter and better. We find that the best use cases in terms of more distributed AI or decentralized AI is on the inference ion running the model. We don't necessarily focus at all on the training. That's actually best left to the centralized cloud providers. You have a lot of dense compute all close to each other, great for training models, but at the end of the day, you got to run the model somewhere, really. We find that decentralized distributed compute models work significantly better for that in terms of making it again, 50, 90% cheaper, two to four times faster, a lot more scalable.
00:22:24.360 - 00:23:37.400, Speaker A: Where we tend to focus is on the more compute heavy side of things, where that those deltas and those, those gains kind of show the most, right? So that tends to be on things like images, right? So you have a text model like chat GPT, then you have an image model like stable diffusion, right? We typically see that an image generation uses about 100 times the compute of a text generation, and then you can go even further into that. Things like video. I don't know if you've seen Sora and see some of those amazing demos, right? But they're using three to 500 times the compute of an image generation. So where we tend to focus a bit more on is kind of the heavy compute side of things, right? So that's where you kind of see our, you know, benefit in terms of, you know, cost savings, performance benefits. Those are things where it really shines. So we tend to focus a little bit more on image generation, just kind of taking three stacks. You have text models like chat GPT, right? That's a text LLM model.
00:23:37.400 - 00:24:28.890, Speaker A: Then you have an image model that like stable diffusion image models use about 100 times the compute that a text model uses. And then you have video models. I'm sure you've seen Sora and the amazing things that we've seen on Twitter in terms of the demo that uses about 300 to 500 times the compute of an image model. So that's why you see all these cool demos, but you can't actually use any of them. There's very limited access. And so today we power some of the largest, essentially AI image editors and apps out there. Literally, if you Google AI image editor, the number one result there, 70 million users, uses us behind the scenes.
00:24:28.890 - 00:24:59.770, Speaker A: And so that's where we tend to focus right now, is the heavy compute stuff. The largest use case of that that is broadly available and useful for open models is image generation. But we plan on essentially going on the stack, focusing on images now, then going to video, coming back to text, and then going over some of the other more and more interesting modalities till we get to the thing that we really want to get to, which is AGI. But that's a couple steps away.
00:25:00.070 - 00:25:12.028, Speaker B: Yeah, I love how iterative this approach is. And before we dive into the images part, because there's a lot to dig into there, can you talk a little bit about why training as a use case is not suited for decentralized networks?
00:25:12.164 - 00:26:16.148, Speaker A: Yeah, so it's, again, you need very dense amounts of compute, very centralized compute. So it's not necessarily a great use case for decentralized network. As of now, there is some research in terms of ways you can do to do decentralized training, but you just end up running into the laws of physics in terms of things that you can do with decentralized compute networks. Really, it's as best to just not ignore the laws of physics when you want to train. I would recommend centralized providers, but again, at the end of the day, you have to run that train model somewhere. That's where I think the decentralized providers really come in to force.
00:26:16.284 - 00:26:24.292, Speaker B: Now, moving back to the images part, I read somewhere that Protea has generated or 300 million images have been generated using Protea.
00:26:24.396 - 00:26:26.972, Speaker A: Closer to 400 million images now.
00:26:27.036 - 00:26:33.620, Speaker B: And one of the users, I think you mentioned this just now, is Pixlr, obviously a pretty well known image editing tool. I've actually used Pixlr myself as well.
00:26:33.700 - 00:26:40.268, Speaker A: I mean, it's the number one result when you're trying to basically edit images online. So most people have used it, I would think, at some point.
00:26:40.364 - 00:26:57.656, Speaker B: So how did you guys get there? Because if you look at most crypto projects or most decentralized compute networks, they are struggling with the demand side. They have oversupply of GPU's, they incentivize everything with tokens. Nobody uses them. But then from day one, you guys are like, oh yeah, we're just going to work with the largest image editor. So how the heck did that happen?
00:26:57.768 - 00:27:52.690, Speaker A: Yeah, so we tried to focus again on the more high scale, high compute stuff. So Pixlr was using a traditional web two provider, and they were really struggling with the cost. I think that provider was taking away volume based discount that they had a. And so they were like, their cost was going up 50% overnight. They were really struggling. They would have peak load time and it would kind of struggle to kind of perform and the performance wasn't amazing and they just had access to one model. And so we came in and said, hey, during peak times, no problem, we can just distribute that to our network.
00:27:52.690 - 00:28:23.500, Speaker A: Oh, the cost, no problem, we'll cut that in half. And, oh, by the way, it's like two to four times faster and we have access to all these additional models and features that you don't have. So we literally went from a cold outreach to a done deal. Very, very small. Yep, cold outreach, a small amount of time. And just because the pain point was just like, so there. Right.
00:28:23.500 - 00:29:17.082, Speaker A: It was, it was really a problem for them. So we tend to, you know, really focus on the people who are at scale, who are, you know, working hard to scale their AI applications and really having a lot of trouble. Remember, we built this because we were trying to scale our own kind of just app that we were just playing around with and it was really difficult. And so we built a network to solve that problem. And then we came out with this toolset. We understand the problems that people are going through as they try to build out their application and they just shouldn't need to worry about all the difficulty of scaling this up, you know, dealing with all these other things. And so again, we created this network, this platform, as a tool to solve our own problem.
00:29:17.082 - 00:29:48.578, Speaker A: But this is the problem that everyone's kind of running into. So yeah, we were able to solve their problems, but yeah, our goal is to help many, many more people as they try to build things in AI. We think AI is going to be in everything, if it isn't already in terms in the title, but we think it's going to power a lot of things. Pretty much every software, every company is going to have to integrate in some form or fashion. And so we just want to make that easier for them.
00:29:48.634 - 00:29:59.830, Speaker B: That's actually an insane story because when most web3 companies reach out to web two companies and say, hey, we can solve some problems for you, I know most of these BD guys just click delete, they don't even want to leave.
00:30:00.530 - 00:30:06.336, Speaker A: You got to get our token right. And then the NFT, and then it's like, yeah, exactly.
00:30:06.368 - 00:30:18.568, Speaker B: So the fact that you guys just dm the biggest kind of image generator, one of the biggest image companies out there, and then they said, yes, let's work together at early stage is actually an insane success. So congratulations on that so far.
00:30:18.624 - 00:30:45.538, Speaker A: But I mean, it really points to the difficulty in the market. Right. I know there's a lot of other providers. A lot of other apps out there that I don't think of any one of them. Everyone would say they're truly happy with giving all their money to AWS and Nvidia, but yeah, the times are changing. People are struggling. They need a scale, and we're here to help.
00:30:45.594 - 00:31:09.212, Speaker B: Absolutely. And let's dive deeper into some of the common questions or skepticisms that people have about compute networks in web3. So I'm going to list a few of them that people always ask about, and I'd love to get your thoughts on all of them. So the first one is latency. So do decentralized networks face higher latency than decentralized ones? For what tasks does that matter? And how big of an issue is that?
00:31:09.356 - 00:32:25.226, Speaker A: In the context of latency, you can define it a couple different ways. In terms of AI infrastructure or AI generation, some people define it as essentially the time that it takes for a generation to complete. Sometimes people essentially use latency to describe the time it takes for a piece of data to transfer from one point to another. But for this purpose, let's define it as essentially the time for it takes for a user to request AI generation, whether it be a text, image or video, and actually receive it. So one of the things that is really great about decentralized networks in terms of AI infrastructure is that you can get the kind of node that's actually doing the AI generation, the processing, very close to the end user. At the end of the day, you can't beat physics. So the closer, if you're doing the inference job or the AI, you're generating a picture of a cat.
00:32:25.226 - 00:33:25.192, Speaker A: And the node that's generating that image is down the street. Obviously, in terms of the transfer time is going to be very quick versus the AWS data center that's 1000 miles away. That's the core benefit, you know, of just in terms of the transmission and the communication. In terms of a decentralized network, hypothetically, it can be as close as possible. And so you get better communication latency than you could anywhere else because the processing is at the edge, it's as close to the user as possible, and you just can't get that in terms of, you know, essentially centralized providers, they can't take the data center and put it on wheels, right, and move it around to fulfill the demand, right. A data center takes, you know, years to plan out and build out decentralized network. You install a node that's on the street, right.
00:33:25.192 - 00:34:34.489, Speaker A: And you get really fast times. We're also finding that, you know, the you know, AI processing power in terms of the GPU's is for the current gen kind of gaming cards are pretty much similar. Chips are the same that are used in the data center, just charge ten times more for. So you're ending up getting your processing speeds that are on par or faster at a closer location and more of them. So you really get every point of that dimension where you can make it fast or faster with the raw hardware plus the software that we have under the hood on Protea to accelerate things. Plus it's close to the end user, plus there's a lot of them available. You can see how essentially you solve all the bits of the problem to get that AI generation to the user in the smallest amount possible.
00:34:34.489 - 00:35:05.380, Speaker A: In terms of both latency, speed, performance, all the things in kind of the abstract, a decentralized network and solve all those things. Now there is a march to get there. You have to have enough density of nodes, you need to have all the use cases. But in terms of latency, in terms of performance, a decentralized network, because it's computing at the edge, is always going to beat it at any centralized network.
00:35:05.680 - 00:35:29.072, Speaker B: That's interesting. I've never heard anyone kind of phrase it that way. In terms of the efficiencies of edge computing, I think that makes a lot of sense. So the other question that people have is related to supply. So most decentralized networks for compute have way more supply than demand. They're able to aggregate, sometimes even supply from other decentralized networks. This network would be a subset of this network and this network will be a subset of that network.
00:35:29.072 - 00:35:38.088, Speaker B: So there's a lot of kind of intermingling there. So versus the demand, there's not too much demand using these kind of GPU's. So how did you guys combat that?
00:35:38.184 - 00:36:46.880, Speaker A: Yeah, so like I said before, it's based on been at this game for a while and I think it comes down to the round peg square hole analogy that I was making before. It's got to be easy to use and we try to make it so that it takes developer five minutes to integrate. It's very simple. I don't know if you've used any of these other platforms, they're great in terms of raw supply, but in terms of the development experience, it is quite difficult in terms of the number of steps and the knowledge you have to be able to, you know, oh, select, okay, what GPU do I use? You know, there's twelve different options, you know, and you know, a million different regions. It's like it makes it very difficult. You know, I think, you know, there's some statistics out there in terms of, you know, for every step, every click, right? How many percentage, double digit percentage of people do you lose? And so how many clicks does it take to set up a wallet? Right. Get a token, you know, send it.
00:36:46.880 - 00:37:23.232, Speaker A: Yeah. So I think a lot of these providers, these decentralized networks, they're great in terms of having supply, but it really needs to fit the use cases. So we're probably going to be, as we grow, probably the largest user of all these decentralized networks. And again, it just comes down to it needs to be easy to use. The developer experience needs to be great for the people who are building these AI applications that have millions of users and they have a million other problems rather than trying to figure out a new platform.
00:37:23.376 - 00:38:04.380, Speaker B: Yeah, and from my perspective as an investor, that was actually the biggest reason why we invested is because I talked to all these networks out there, there's been so many in the past twelve months and all of them were kind of flexing about how much supply we have. We have the most h out there and nobody talked about the actual important thing, which I believe is owning the user. Anybody can access the supply where anybody can aggregate supply, but not everybody can own the users. And no one seemed to understand, despite crypto showing again and again that the more steps you add, the fewer people use it. And we see this in l one blockchains as well. Whenever you create your own language from scratch, nobody's going to build on you. So nobody seems to understand this and you guys understand it so intuitively, you built it into the product.
00:38:04.380 - 00:38:09.124, Speaker B: So I think that that kind of supply question really doesn't apply to Protea.
00:38:09.252 - 00:38:48.420, Speaker A: And the funny thing is that would never fly in a web two context, right? If I said I'm going to judge Walmart stock by the amount of things that they have on the shelves, not how many sales, it's just how many things that they have in stock, it might not work out. So it's very important that the metrics that we judge these networks we think should be based on usage. How many GPU's are actually being used, how many cpu's are being used, how many users, how many transactions, these kinds of things. Supply isn't necessarily a metric that leads to success.
00:38:48.840 - 00:39:35.982, Speaker B: A lot of that is speculation driven, because a lot of people, the traders who are trading these tokens, they want easy to understand metrics. They look at total number of GPU's, but that's actually the wrong thing to look at if you want to invest for the long term. Um, so, yeah, absolutely on the same page there. Um, and then the next point, the final thing that people often ask me about is, seems like, uh, there is limited access to high quality GPU's like Nvidia's h. It seems like the, the problem there is manufacturers prioritize giving these very limited allocations to big centralized providers. I believe Nvidia has like a special arrangement or like some sort of a license you almost have to apply for to get these things. And a lot of these h 100 very advanced GPU's are crucial to LLMs that we sell today, that we use today.
00:39:35.982 - 00:39:42.294, Speaker B: So does that challenge apply to Protea in terms of ability to access these very high quality GPU's that are very limited?
00:39:42.422 - 00:40:57.794, Speaker A: So our focus, again, is in the inference side of things that's on the running, the model side of things. And so you have these high end GPU's like the h think, what are they going for? Like, don't, don't quote me on this, but like 30 grand a pop, if not more. That's if even if you can get them right, you have to go to some like back Alley street in San Francisco. Like a guy opens up his trench coat, he's like, you want an h 100, man, you know? So it's, you know, those types of high end GPU's, you know, are hard to get, but they're really great for training. Where we tend to focus is kind of, again, on the running the models side of things, and those tend to be a little bit more accessible GPU's in terms of 4090 or 3090 or something that you need to be running in your computer at home. And there's a lot of them, and a lot of that capacity is idle and unmonetized. I think if you look at our network page, you know, work with a lot of miners and I think we're paying something like five times more than they would make mining.
00:40:57.794 - 00:41:37.444, Speaker A: Right? So that is the kind of area that we're focusing on is what is highly available, what is highly useful, that's more on the running model side of things, and that's where kind of supply can meet demand. So again, we focus on the running, the model side, on the training. We have partners that we can help connect customers with that can get the backroom deal for the high end GPU's, but we'll lead that to the big centralized providers or our partners that can source it.
00:41:37.532 - 00:42:04.330, Speaker B: Sean, walk me through the business model of the miners right now, because a lot of the deep end networks out there, they subsist off of very unsustainable inflationary rewards. So you provide some sort of hardware supply and you get printed tokens that are minted free of cost. And then because people are buying these tokens, there's a monetary value on them, but it's not designed to be very sustainable. So there's no live token right now with Podia. So how are these guys making money? What is the business model there to be a miner?
00:42:04.670 - 00:42:41.490, Speaker A: Yeah, so we really see it in the opposite way than a lot of people see it. And again, I go back to my experience, because again, I see how some of this plays out over the long term. And so it's not like I have a crystal ball and know everything. I just been smacking in the face by this before and I've learned my lesson. Right. So again, a lot of people are, you know, you know, they have these networks, they want to. The focus is, hey, we want to grow supply, and so people grow supply.
00:42:41.490 - 00:43:18.096, Speaker A: And, you know, if your demand doesn't catch up, that does not end well in the future. And so we're really taking, so I think some of these utilization rates are somewhere like one, 2% in terms of these networks. What I've seen, we start from the other. So we don't focus on the supply side, we focus on demand side. So our utilization rate is 100%. We use 100% of the GPU's that are available in our network. We have a highly simplified cost structure.
00:43:18.096 - 00:43:58.720, Speaker A: Basically, if you look on protein.com network, you see our current pricing for different gpu's. We just pay a fixed amount per gpu per month. Don't make it any more complicated than that. We will add some things in the future that will more incentivize people for certain tasks and doing them efficiently. But we just want to make it simple, just like a product we want to integrate in five minutes. We want to make the supply side initially as simple as possible, don't make it overly complicated, and just give people their kind of expected payment for the resources.
00:43:58.720 - 00:45:17.858, Speaker A: And so we see that as a model where we can get the supply we need and then we can grow it over time in a sustainable way, and really focus on really high quality hosts that will really help our customers and provide the compute we need in different areas to hit certain use cases. And again just grow it in a reasonable, sustainable way that actually has a base to it, versus again, the other way where there's a lot of examples where it's great for a short time, but at the end of the day, the economics don't necessarily match, and I've seen that go wrong many times. Our model is perhaps not as sexy as the other models, but I think time will tell which model will provide more value and be the largest over time. Again, if in terms of usage, according to the last numbers that I looked at, we have more usage than any other individual network, and probably in the next two months or so will be larger than all of them combined in terms of usage, that's the metrics that we look at, not supply.
00:45:18.034 - 00:45:49.926, Speaker B: Yeah, I think this kind of reminds me of the Dex race as well. I'm sure you remember before Uniswap, there were actually quite a lot of decentralized exchanges, and I think they were optimizing for nice features like cool order books. And Uniswap was just kind of slowly in the background, just grinding away and accruing so much volume. No token at the time, no incentives. And then they dropped the token as like airdrop to past users, and now they're the biggest, I think, in terms of token size and also in terms of volume. So kind of the slow methodical growth wins the race at the end.
00:45:50.078 - 00:46:09.850, Speaker A: Absolutely slow and steady. We're more like into the turtle than the hair. The only thing that matters is who wins at the end, which I think leads us to talk a little bit about why we're building this and why it's important.
00:46:10.630 - 00:46:40.298, Speaker B: Yeah, and I guess before that as well, just to finally touch on this business model part, because Protea is a decentralized network. So obviously at some point there might be a token. So I know it's very early stage for that. It's not the current focus, but if you were to add a token to a decentralized compute network, what are some ways that we can do this without making it a headache for users, without making it necessary for people to buy the token, set up a wallet in order to use the network. What are some other ways to make sure the token is value additive to the network?
00:46:40.434 - 00:47:43.050, Speaker A: Yeah, yeah. So, yeah, token is useful in terms of, like you said, you gave the example with the DEX, there is, you want to focus on having a product and a network that is valuable. Right? And then a token is really the fuel on top. To really push that engine forward, you got to build the engine first before you add the jet fuel. If you throw the jet fuel on it, then you just have a big fire. So that is a very useful tool in terms of incentivizing operators, highly automating parts of the network and the system and obviously using essentially to buy and sell compute in the network. But again, it is important to abstract that at the end of the day a user can come in, you know, pay with tokens to get compute, to get AI generations on the platform.
00:47:43.050 - 00:48:30.454, Speaker A: But that shouldn't be a bottom line for a web two company, right? That's spending millions of dollars at AWS saying oh wait, 59% cheaper, two to four times faster, way more scalable for me. Okay, here's dirty fiat, like, oh, no problem, we'll take it. And I do what we need to do on the network side. But it's really important that you have both, right? You have the underlying network supported by tokens, high levels of automation. But at the end of the day, there's $100 billion being set on AI infrastructure in the last twelve months, right? 99.999% of that is web two, right? So it needs to be easy for them to just come in through traditional rails to provide the, to be more on the demand side.
00:48:30.582 - 00:48:56.964, Speaker B: Absolutely. Well, Sean, initially for the final part of the interview, I wanted to talk about what's out there in the market right now. How does it differ? Like render Akash, ion and Aether? But I don't think that's too interesting given that we've kind of covered the broad strokes in terms of the difference between Protea and just a shout out for our blockchain vip subscribers if you're subscribed to our newsletter. One of our contributors actually wrote a 30 page report just on all these compute networks about two months ago. So you can go check that out.
00:48:57.012 - 00:49:01.452, Speaker A: One thing we're looking at integrating most of them as we grow is the flag.
00:49:01.596 - 00:49:22.520, Speaker B: Perfect. Perfect. So yeah, I guess not much point to kind of COVID the differences given that you guys will integrate all of them. So I actually want to zoom out a bit, Sean, and understand, besides the obvious commercial aspect of it and the fact that you're helping saving costs and making things more efficient, why do we need decentralized compute networks? What gets you out of bed in the morning to build this?
00:49:22.660 - 00:49:54.670, Speaker A: Yeah, absolutely. So I think that's a very important question as like AI, just like this, inserts itself more into our lives in terms of products, services. Oh, I need to write that email. Let me try GPT it. Right. And you know, personally I've always been fascinated with aihdeme. In my free slime, all I do is read cyber thrillers, right? AI is taking over the world or whatnot.
00:49:54.670 - 00:50:36.070, Speaker A: And I see it as we're going on two paths. And I think Sci-Fi is a good place to pull this from in terms of what those two tasks would be. You have one path which is essentially centralized AI. Right? What does that look like, you think? Terminator, right? Military makes an AI, what's the first thing it does? Takes over the world, destroys humanity. That's what closed, government run, military run AI looks like. Not very good yet. The other side of that, think of something like Star wars.
00:50:36.070 - 00:51:24.300, Speaker A: The AI is open, it's permissive. Bad guys have the AI, the good guys have the AI. C three, Po and RTD two. They're very important, they're part of the story. They're helpful to both the heroes and the villains always have their AI as well. And that's a much more balanced future. I like the idea of the more open AI where everyone has aih, everyone has access to the tools, and it's much more helpful to users than the closed model where humanity is destroyed instantly.
00:51:24.300 - 00:52:34.540, Speaker A: I think if we continue to build on this path and we're building everything in a centralized provider, which is Amazon, Google and Microsoft, with these closed models, we're heading to that Terminator path and that's not good. But if we build these in a way where everyone's running a node, we're running open models, it's permissive, inspectable, these kinds of things, we're building to a future that I think is a lot more positive and a lot more useful for people. That's one of the core reasons that we're building, Protea, is that besides the, it's a great business and great economics, useful for the companies and the node operators. Right. I think we really want to build the future of AI and the future of compute infrastructure in a way that is a distributed network because it just, again, we see how this works out in Sci-Fi it leads to a lot better outcomes than something is, is closed sourced and centrally run.
00:52:34.620 - 00:52:53.060, Speaker B: Yeah, and I love that roadmap you painted as well with starting with images first and then progressing to something that's 200 times as complex videos and then progressing to something maybe a thousand, a million times more complex with AGI. So along the roadmap, what's immediately next on the horizon for you guys? What are you guys most focused on now?
00:52:53.220 - 00:53:32.680, Speaker A: Yeah, yeah. So as he said, we're focusing on images right now. That's the largest use case that the most people are using with open models that's heavy on the compute side. Where next transitioning to in the next couple of months to video. Those models are starting to get better and better and better. Funny enough, we're going to actually, after that, go back to text. That's a useful tool in terms of making the images and videos better, you know, doing other things in terms of the platform.
00:53:32.680 - 00:54:17.610, Speaker A: And then there's going to be other modals that we'll focus on as we just kind of, again, move up the stack in terms of compute all the way until we get to that AGI future where everyone has it and it solves the world's problems and you have your little AI buddy. Yeah, that's what we want. But we want to build to it in a additive, constructive manner where people are, we're getting real use cases with real usage, with real economics now to be able to build a good foundation that we can build out the future the way that I think everyone wants it to look.
00:54:19.490 - 00:54:25.586, Speaker B: Absolutely. And Sean, I guess over under, how long do you think we're gonna need to get AGI?
00:54:25.698 - 00:54:26.602, Speaker A: Five years.
00:54:26.746 - 00:54:33.002, Speaker B: Five years, really? Wow. I've heard a whole range of answers from like six months to like 20 years.
00:54:33.146 - 00:54:54.480, Speaker A: Yeah, it's hard. Like, humans can understand linear growth. It's very difficult to understand exponential growth. Right. It's hard to fathom kind of that exponential rise. So, yeah, who knows? It could be tomorrow, it could be five years. It could be 20 years.
00:54:54.480 - 00:55:00.196, Speaker A: I'm putting my money at five, but that's just a wild guess. We'll see.
00:55:00.348 - 00:55:05.844, Speaker B: Absolutely. Sean, thank you so much for coming on the show. Now, for people who want to reach out to you, what are the best ways to do this?
00:55:05.972 - 00:55:47.766, Speaker A: First thing I recommend, go to protea.com. you literally click on playground and start doing AI generations in seconds. We're like the only place on the Internet you can do that without hitting a paywall or signing up. That's using basically in 5 seconds, you can use our network and try it out. I'm super three on Twitter if you want to reach out, if you have any questions, we also have a discord if you have any more questions or want to participate as a developer or in the network. But, yeah, highly recommend seeing as believing, go to Proteo.com, click on the playground, play around with it, see what the current state of the art is and what the future may be.
00:55:47.878 - 00:55:50.654, Speaker B: All right, thank you so much for coming on the show, Sean. This has been a delight.
00:55:50.742 - 00:55:52.414, Speaker A: Absolutely. It's been super fun.
00:55:52.542 - 00:56:17.240, Speaker B: Hey there, Jason here. Thanks for supporting another episode of the Blockcrunch podcast. If you enjoyed this, please give us a five star rating on Spotify and Apple Podcasts. And make sure you subscribe on YouTube so you can check out our founder interviews, our roundtable series on our venture thesis, as well as our on a tangent subseries about our current market views. And as always, you can reach out to me on X at mrjasonchoy and I'll make sure to read every comment related to this episode. So I'll see you next time.
