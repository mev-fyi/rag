00:00:00.880 - 00:00:11.286, Speaker A: You're now listening to Alpha Leak, a podcast mini series highlighting the most under the radar projects and developments in crypto. And this series is brought to you by the Blockcrunch podcast. I'm your host, Jason Choi.
00:00:11.430 - 00:00:55.142, Speaker B: Hey, everybody. Welcome back to another episode of the Blockcrunch podcast. Now, if you've been paying attention to this show or the crypto space in general, you know, there's been a lot happening with the multi chain future, especially with regards to bridges and interoperability protocols. Now, these protocols have been very, very useful for people who want to transfer assets between chains and use applications across chains, but they've also been victims of massive exploits recently, up to $300 million. A lot of money have been lost in some of these exploits. So in a recent episode with Dimitri from the venture firm one KX, we talked about whether these security issues really impact the future of a multi chain reality. And one of the projects that he mentioned that he's the most bullish on is nomad, because of their priority for security.
00:00:55.142 - 00:01:07.020, Speaker B: So I'm really, really excited to have Pranay, the founder of Nomad, on the show, to talk all about bridge security, as well as what nomad actually is. And as a disclaimer, I'm a personal investor in Nomad as well. So, Pradey, welcome to the show.
00:01:07.212 - 00:01:09.612, Speaker C: Thanks, Jason. Appreciate you having me on.
00:01:09.788 - 00:01:17.940, Speaker B: Definitely. It's interesting because we've chatted so many times, but like we said before we started recording, I've never seen your face. So it's great to kind of put a face to a name.
00:01:18.132 - 00:01:38.086, Speaker C: Totally. That's the reality of the world we live in, where, like, you end up chatting with people and some videos off, or you end up chatting on phone, and then you get to know them really well, like, and you develop, like, affinity for the person, and then you realize, wait, we've actually not even met or seen each other, so it's trippy to see you now, but I'm glad we got to do it at some point.
00:01:38.230 - 00:01:50.194, Speaker B: Yeah, exactly. And I'm super excited to dump it to this as well, because there's so much to cover, I guess, just to kind of high level, just to cover it off. What is your background like? How did you get into this bridging space, this crypto space?
00:01:50.654 - 00:01:51.038, Speaker C: Sure.
00:01:51.086 - 00:01:51.382, Speaker B: Yeah.
00:01:51.438 - 00:03:03.722, Speaker C: I think I've given the long spiel multiple times on different podcasts, but I don't come from a computer science background. I studied chemical engineering and kind of had a winding path into crypto. But the long and short of it is that the folks in crypto, their ethos and their focus on building censorship resistant systems that can give us digital rights like we care about human rights in the physical world, we need those to carry over to the digital world, where we are increasingly spending more time, and to our previous point, even not even seeing each other over video these days. And so what motivates me to play in this space is that opportunity, that once in a generation opportunity to play a forming role or a formative role in an industry that's going to change the lives of billions of people. And so I think I got motivated when I helped host a conference that a friend, Ed Roman, was putting together. And I met a lot of brilliant people in the crypto space. And particularly I fell in love with ZK snarks after hearing Isaac Meckler from Mena, formerly known as coda, giving a really good presentation on what snarks were, that blew my mind.
00:03:03.722 - 00:03:55.246, Speaker C: And for me, I realized even at that point, that snarks and zero knowledge proofs were going to be a primitive that changed the course of human civilization. That feels like a large statement. We're just in the beginning era of that happening. But that conviction was there. And it was funny, because zero knowledge proofs and snarks were like kind of a trojan horse for me to get more into crypto. Since then, I've spent the last four years of my life working on various protocols, primarily focused at the protocol and infrastructure layer, and so most recently leading into interoperability, because it is clear to anyone at this point that with the proliferation of all these successful layer ones, that now we need some type of connective tissue between them. And I suspect we'll get into the state of the world and why the bridge landscape is so patchwork and at times insecure.
00:03:55.246 - 00:04:11.394, Speaker C: But really, in order for this stuff to take the next leap and be able to meet the needs of normal users across the world, there need to be standardized communication protocols between the intranet of our times, which is each individual layer, one blockchain.
00:04:12.054 - 00:04:46.526, Speaker B: Yeah. And I think that just speaks to how built different you are, because most people I talk to, they're brought in the space because of Defi applications. They have high yields. They're really attracted by those yields, or they're brought in by NFTs that look cool, but you're brought in by ZK, which is usually the last thing people look into after they got into crypto, just because of how esoteric it is. So I'm really interested to kind of dive into some of the more technical aspects of what you're building as well. But I guess before that, just to give our audience a rough overview of what the bridging landscape is. So I think you mentioned before to me that there's three overarching types of interoperability protocols.
00:04:46.526 - 00:04:55.514, Speaker B: There's these kind of so called natively verified things. There's externally verified and locally verified. Can you help us kind of break down what that means exactly?
00:04:55.844 - 00:05:44.848, Speaker C: Totally. And I have to give credit to Arjun from Connext, who's a good friend and partner to the Nomad team, in kind of building out like Connext as a fast liquidity layer for Nomad. We can jump into that later. But Arjun was kind of the progenitor of these terms in an article he wrote about the interoperability trilemma. And really, in order to understand what is going on when we bridge or send messages between chains, what we have to understand is that if you're familiar with the oracle problem, part of the challenge with getting information into a blockchain is within the security boundary of a chain. It cannot know what's happening in the external world. And the only way to send that information into a crypto network is via a transaction.
00:05:44.848 - 00:07:05.140, Speaker C: And so oracles as a whole, they take some type of data in the real world and then basically package it into a transaction and post it on chain. So the apps on chain can now take advantage of that data that's been posted. Cross chain communications can be thought of as almost a subset of the oracle problem, where of all the data that can come from the external world, we're taking data that exists on another chain and finding a way to get it into a destination chain. And so the question we have to ask is who is sending this data? Who is in charge of reading what's happening on one chain and posting it on the other? And so as a whole, this is kind of what can be called as verification. And there's Arjun in his first article identifies three types of verification, local, native and external. And then later, Arjun wrote an article about optimistic bridges, which talks about nomads method of verification, which is optimistic verification. But for the purposes of this conversation, we should largely focus on native verification and external verification, which is when we're trying to send generalized data between chains, what are the options we have available to us? The most trustless way that we have right now is by using light clients or natively verified system.
00:07:05.140 - 00:08:04.796, Speaker C: And what we're doing in this model is if I'm trying to send a message from chain a to chain b, we will run a light client of chain a itself as a contract on chain b. So what we can do is anytime some state gets updated on chain a I lock 1000 USDC, we can prove that trustlessly within the like client of the receiving chain and say cool. Pernay has for sure locked 1000 USDC on chain a. Now you can mint a synthetic 1000 synthetic USDC and we won't break the supply peg. And as long as the validators of the sending chain are behaving honestly, then that light client state remains valid. And so really, the only work done between in the cross chain comms in this type of verification method is trustless relaying between chains. And the most popular and most common instance of natively verified systems is Cosmos and IBC.
00:08:04.796 - 00:08:17.664, Speaker C: IBC fundamentally is a header relay based construction where each cosmos chain is running a like client of the other cosmos chain, such that when you're sending a message from zone a to zone b, it can be verified trustlessly.
00:08:18.884 - 00:08:28.340, Speaker B: It sounds very unscalable because it sounds like for every chain, you need to have a like line of the other chain, and then for every new chain that comes on, you need to have like lines on all the other chains. Is that. Is that correct?
00:08:28.492 - 00:09:13.076, Speaker C: Precisely. The reason this has worked really well with Cosmos is because all of the cosmos zones are homogeneous. They're all built using the Cosmos SDK, and they use the same consensus mechanism, which is tendermint. And so when you're building an efficient, light client, you can now basically take that same construction and apply it everywhere on all of these chains. So cosmos, they were way ahead of their time. The cosmos ecosystem built an entire ecosystem of chains ground up with IBC and cross chain comms as a core primitive. And so that ecosystem flourishes because, in a way, the whole economy, the whole federation, was constructed around this shared blueprint that everybody can use.
00:09:13.076 - 00:10:04.874, Speaker C: The reason why we haven't been able to adapt this into the EVM world is precisely your point. Once we get into EVM chains, we have heterogeneous chains, like polygon, avalanche, BNB, and each of these has different consensus mechanisms and also different rules with regards to finality. And we also have to write an efficient solidity like client that can basically support any number of chain pairs that we have. So if we have n heterogeneous chains, we now have to deal with n squared connections that we have to build like clients for, right? And so it becomes untenable. And while theoretically possible, the quality of extensibility is lost. Arjun talks about this in the trilemma article. But because there is such a market need for bridging, we need this now, we can't wait.
00:10:04.874 - 00:10:52.336, Speaker C: And so that's what's led to the proliferation of this second type of system, which is externally verified systems. These are the common ones that you see that rely on a multisig or validator set. Because the fundamental premise behind these is if we are unable to build like clients and scale those out across n heterogeneous chains, then let's do the next best thing, which is we will rely on an honest majority in the real world and basically trust that that amount of security is sufficient to honestly broker the cross chain comms. We cannot verify against like client. So we will trust 19 guardians or 100 validators or whatever that kind of quorum might be that you need to be able to send the cross chain message.
00:10:52.520 - 00:11:03.004, Speaker B: So I guess just to give this some examples that listeners might recognize, these are projects like Thor chain, anyswap seller, synapse and poly network, right?
00:11:03.464 - 00:11:11.004, Speaker C: Precisely. And Axelr and wormhole and layer zero are other examples that have also kind of done well in the market recently.
00:11:12.384 - 00:11:41.014, Speaker B: Is there a difference between bridges and interoperability protocols? Because it seems like the external world at least labels things like Synapse as bridges, but then layer zero as interoperability protocols. And it seems like the way in which people frame them. For instance, in our layer zero episode we talked a lot about building applications that straddle multiple chains at the same time, whereas for synapse it's more about setting assets across different chains to do things on different chains. Is there the differentiation or is that just kind of a marketing thing?
00:11:42.034 - 00:12:52.700, Speaker C: So the use case of bridging tokens is just one application use case that can be done on top of interoperability channels. Again, going back to IBC for a second in 20, I was looking this up the other day to read up more on ICS 20, and I was seeing issue number nine in the kind of the Cosmos repo is by Christopher goes outlining the packet specification for token transfers in ICS 20. And so even back then, there's this idea of fundamentally what you have is message passing channels that allow blockchains to be able to send arbitrary information to each other. And then on top of that you have token bridge applications which encode business logic. So the business logic might be lock tokens on one side, then send that message to the destination saying that the tokens have been locked so that now you can mint a synthetic on the other side. But the reason that application use case has proliferated is because really the most common thing people want to do is yeet tokens from chain a to chain b to take advantage of high yields on destinations. And this is kind of.
00:12:52.700 - 00:13:46.094, Speaker C: This has been a result of the layer one kind of Defi incentive programs, because liquidity was largely concentrated on ethereum in order to bootstrap liquidity and defi ecosystems, there's been programs like Avalanche Rush that have needed a bridge as a primitive to bring tokens over. But now that these ecosystems are thriving and have a lot of liquidity in defi applications, now that has catapulted us one step forward where it is, how do we not only unite the liquidity across these chains, but fundamentally coupled the business logic, where if I'm Sushiswap or AAvE or an application developer, I'm not deploying a fragmented version on each chain and just migrating liquidity. I'm deploying an application that spans multiple chains and natively composes that liquidity in a way that might be more capital efficient or context aware of the deployments on other chains.
00:13:46.514 - 00:13:53.934, Speaker B: Got it. Is that only doable with externally verified systems, or is that doable with all three of the types of bridges?
00:13:54.514 - 00:14:29.134, Speaker C: That's doable with natively verified, externally verified, and optimistically verified systems. The verification mechanism underlying the root of trust has no bearing on what messages can be sent. Right. So the messages being sent, you just need to know if you're chain b, you just need to know that the message, something happened on chain a, and that is an honest message, and how that honesty and how that verification happens is basically, it's a bit orthogonal to receiving the message and being able to build an application with it.
00:14:29.474 - 00:14:56.054, Speaker B: Got it. So, we've talked about how natively verified systems like cosmos are not extensive enough, especially when it comes to heterogeneous chains. And we talked about how the next step, which is these externally verified bridges, like four chain, any swap, have to rely on an external set of validators. But what is the limitation of doing that? Are there kind of cons that arise from trusting a set of external validators?
00:14:56.754 - 00:15:35.036, Speaker C: Yeah, so the. The goal of everything we're doing with crypto systems is to reduce, fundamentally reduce the trust, or shard the trust across multiple parties, such that we can operate in censorship resistant environments. The risk we run with externally verified systems is. There's a couple. But the two that stand out to me are, first, if you're sending messages from chain a to chain b, say, ethereum to avalanche. Right. We have a strong guarantee that the miners on Ethereum and the validators on avalanche are, it's a robust security set.
00:15:35.036 - 00:17:02.172, Speaker C: But if we introduce a bridge that has, say, nine validators now, we're fundamentally only as secure as our weakest link, which is the bridge, right? And so if we are relying, if we're bringing in an external verifier set, the whole system now is compromised by the strength of that external verifier set. So at all times, what we want to do is we want to be as close to natively verified as possible, because we don't want to introduce new assumptions into the system. The other thing to be concerned about is if we are trying to grow the security of the externally verified system, that implies greater overhead, right? If we try to reach the security of the Ethereum miner set or the avalanche validator set, we have to grow the validator set or the number of parties that are securing the underlying bridge, which comes with a cost. The reason block space costs so much on Ethereum is because it's secure. It's because we are paying the miners to compute hashes via proof of work and be able to process our blocks, right? If we want that same level of security with the bridge, then we have to pay for it somehow. And those costs kind of scale linearly, unless you're doing something where like, you're sharding the system or fundamentally reducing the security by changing the number of parties involved. And so I think the overhead conversation hasn't been brought up as much recently because it's so early in the bridge race and everybody's trying to capture market share.
00:17:02.172 - 00:17:37.364, Speaker C: So a lot of the costs of security are being subsidized or kind of not quite seen in a sober way by participants in this event in the, in the ecosystems. And so we'll see that only play out over a longer period of time. But if we do things like introduce an exogenous asset for a validator bridge, now we're basically saying that the value of that exogenous asset needs to scale with the amount that that bridge secures. And so I'm wary of growing an honest majority as the only way to improve the security of cross chain messaging.
00:17:37.784 - 00:17:54.344, Speaker B: But I guess, as a devil's advocate, isn't that the same way that l one screw their security, where you still have to trust a set of validators to secure the chain? And the security of the chain still depends on how many validators you are able to scale. Isn't that the same, I guess, challenge that's facing bridges today, or is there some difference?
00:17:54.384 - 00:19:00.880, Speaker C: There so with an l one, you do need to grow the validator set and increase the security to use that l one. But we're talking more in the context of now you want to use two chains. If you introduce another validator set, you basically are introducing another exogenous asset there and saying, the economics of these two chains being able to talk to each other is dependent on the chain sitting in the middle. And so my main concern here is the cost, right. Really what we're trying to do is create the most efficient way of having two chains communicate to each other. All else considered, we really only want to have the underlying security of these chains factored in. I think, like, client solutions are desirable, but what we are trying to figure out is in the interim, while we migrate towards fully trustless bridging, what do we do? What do we do, given the market need, is there, and what we're trying to do with Nomad is offer another alternative, which is instead of sacrificing on security or sacrificing in terms of cost, let us sacrifice a little bit of speed and get a pareto.
00:19:00.880 - 00:19:03.004, Speaker C: Principles benefit in terms of security.
00:19:03.764 - 00:19:31.344, Speaker B: Yeah. And before we move on to the locally verified and optimistically verified iterations of the bridges, I'd love to kind of dive into the externally verified a little bit more, because recently we've had some pretty, pretty massive exploits. I think wormhole was like 300 million, and then Ronin, there was some similar size as well. I might be fussing at the number a little bit, but were those exploits the result of trust assumptions arising from these external validators? But did something else happen there?
00:19:31.834 - 00:20:25.990, Speaker C: Yeah, that is, this is a fantastic question, um, because the ronin hack was the first one, I believe, that actually exploited the root of trust. And what we mean by root of trust is that verification mechanism that we've been talking about in the ronin hack. I haven't dug into the full details, but at a high level, what's, what happened was there were nine validators that kind of secured the Ronin bridge to go from Ethereum to the ronin network. And five of those validators were exploited. Their kind of keys were found, exploited by the adversary, and then used to sign messages between Ethereum and ronin. And so this is the first time where the underlying verification mechanism of the bridge itself was compromised. Before this, with wormhole and poly network, they were different types of exploits with wormhole itself.
00:20:25.990 - 00:21:38.626, Speaker C: It was a smart contract exploit that was an application level exploit where all the guardians within wormhole behaved exactly as they should. They did not try and do something funky but because the smart contract itself had a bug, they just passed on the results of that bug. And so when you're dealing with bridges, not only are you dealing with the trust route, but you're also dealing with the application layer exploits like you would with any defi app. I think what this underscores is the number of vectors at which things can be vulnerable. Right? Like you're not only, it's tough for a user because you're not only expected to audit the bridge, but you're expected to audit the bridge. Not only expected to audit the trust route of the bridge, but the bridge contracts themselves. And when they're written in languages and ecosystems where there's not enough of, has not been enough maturity of like the auditing ecosystem or the tool suite, solidity, for a significant amount, has actually developed a robust ecosystem with a lot of auditors and auditing firms and allows developers to be able to write with some level of predictability.
00:21:38.626 - 00:21:58.988, Speaker C: A lot of the common attacks, like reentrancy bugs, are kind of, at this point, known issues. But as we're moving into other languages like Cosmosm or substrate or rust targeting salon RBM, then we enter into unknown territory and those ecosystems need to mature more before we have that same level of application development maturity.
00:21:59.156 - 00:22:27.350, Speaker A: Before I continue, I'd love to share more about our latest sponsor, Coinflex. Now, Coinflex is an exchange that's been around for a while. They're actually doing $1.5 billion in daily trading volume right now. Recently, their team reached out and walked me through their latest product, the AMM plus. In short, it lets anybody earn fees as market makers on the Coinflex exchange simply by depositing their assets. As of this recording, the seven day in range average for ETH AMM pool depositors is 82%.
00:22:27.350 - 00:23:00.674, Speaker A: So how does this work? If youre familiar with how other amms out there work, this is very similar. You can basically select what price range to provide liquidity for, and the UI even shows you your risk under different scenarios. I also love that they have this cross collateral system, so you can market make in any market you want using pretty much any collateral you have. If you know how to manage your risks, you can even access up to ten x leverage while providing liquidity. So this is one of the most creative ways I've seen a company borrow elements from Defi in a centralized exchange setting. So I highly encourage you to check it out@coinflex.com.
00:23:00.834 - 00:23:40.064, Speaker B: Amm what, what I think is really strange is that despite these security flaws in externally verified systems, they are still by far, you know, the most high volume bridges, like things like synapse or processing a lot of volume every single day. Is that just an issue of users not caring too much about security and prioritizing convenience above all else? Or do you think it's maybe an education thing? Maybe over time people start to realize security. I guess my question is, are users deliberately making that trade off like they know the security is not as strong, but they're okay because it's fast? Or are they just not aware of these security flaws?
00:23:41.324 - 00:24:05.092, Speaker C: I would say it's the. I wouldn't even say there's security flaws. Right. So Synapse, I think, has done really well and was actually chatting with Aurelius earlier today on Telegram. I think he's a great dude. And they're building a solid system and trying to do what's good for the ecosystem. At the end of the day, what users care about is using applications and accomplishing their need.
00:24:05.092 - 00:24:57.404, Speaker C: Really what we need to move to is systems where users are just able to focus on what they care about, which is getting yield, moving, nfts, just having fun, whatever they need to do, and expecting users to go and do diligence on all of these bridges. And in fact, any infrastructure that they use is kind of untenable. Right? It just really doesn't scale well if every time you drive your car, you have to understand deeply how the engine works, you don't expand to a mass market that way. But I think what has happened is externally verified systems are very extensible. They're very easy to deploy, because really you're just relying on some type of MPC or validator set that can be replicated between any number of chains. You're not encoding a like client. That is coupled with the consensus of another chain.
00:24:57.404 - 00:25:49.724, Speaker C: You're just saying we have x number, n number of people, and they will verify the messages going between whatever chains that we want to deploy on. And so these systems have been able to meet a market need because they've been able to move and deploy quickly. And part of that is capturing network effects, liquidity experiences, network effects, where liquidity within any one asset kind of creates a shelling point and begets more liquidity. And so it's fundamentally a go to market question where the users have this need. The user fundamentally just needs USDC on avalanche in order to be able to farm on Trader Joe. That USDC, they're not really thinking about how it is secured or escrowed as long as it's doing well, but the thing is, this is like the Nassim Taleb has. I think it's Nassim Taleb.
00:25:49.724 - 00:26:06.874, Speaker C: He has this chart that's like. It plots the happiness of a turkey Thanksgiving. Yeah. Versus the number of days. And then you just see, like, it rises linearly, and then one day it drops off. And the annotation for that day on the X axis is thanksgiving. Right.
00:26:06.874 - 00:27:09.754, Speaker C: And so that's what a lot of these bridge hacks have been like, where they're humming along just fine. The tvls are enormous. They have billion, nine figures or billions of dollars, until there is something that goes wrong as a black swan risk, and then users have a really bad time. But expecting a user to take into account a black swan risk, I think, is a losing proposition. So really what we're trying to do as an industry is the layer one teams, the application teams, and the bridge teams need to work in concert to make sure that for use cases where security is paramount, we are planning for those edge cases. And in this early stage in this infrastructure cycle, I'd expect there to be this much exuberance and a desire to just get the job done. But if more of these type of hacks start happening, we will need to either focus on security ourselves as an industry and prove that we can mature, or we are basically asking regulators to come in and say, why does this industry keep experiencing nine figure losses? Right?
00:27:10.774 - 00:27:23.114, Speaker B: Yeah, that makes a lot of sense. And I think that that's a great segue to talking about locally verified systems like Connext, and also optimistically verified. So, can you help us break down the relationship between those two, as well as where nomad fits in?
00:27:23.634 - 00:28:34.592, Speaker C: Yeah, definitely. So, in order to explain locally verified, I'll have to go on a little detour to explain the interoperability trilemma that Arjun kind of describes in the article I mentioned earlier. The three parameters that make up the trilemma are extensibility, which we've talked about quite a bit, which is how easy is it to deploy across multiple heterogeneous environments? The second is trustlessness. How trustless is this system? Does it require trusting some set of external parties or people in the real world? And then generalizability, which is, can the system facilitate generalized cross chain messages? And so each of these edges between these three parameters is reflected by one of the different verification methods that we talked about already. The natively verified systems, the lite client based systems that we talked about, they allow for generalized message passing. They are fairly trustless. You only need to trust the validator sets of the two chains, but they're not extensible, meaning it's hard to deploy them across many environments.
00:28:34.592 - 00:29:11.492, Speaker C: The externally verified systems are very extensible. They're super easy to deploy, which is why they've got an adoption. They facilitate generalized message passing, which is why layer zero, wormhole. Axelar and others are focusing on more than just bridging, but kind of sending arbitrary messages between apps between chains, but they are not trustless. They require trusting some external validator set. Locally verified systems like Connext are trustless and extensible, but the key property they lose is generalizability. I'll need to practice saying that word before I come on next time.
00:29:11.492 - 00:30:21.844, Speaker C: But basically what I'm trying to say is connext doesn't have the ability to allow chain a to send an arbitrary message to chain b. Um, it rather kind of requires this handshake between two parties. That's why it's locally verified that one party on chain a and one party on chain b can transact and have it be trustless, but it doesn't facilitate generalized messaging. And so each of these reflects kind of giving up something, um, and doesn't really achieve what is expected in the trilemma. The optimistic verification method, which we can go into, achieves all of these to some degree, but sacrifices one thing, which is latency. And so it breaks the trilemma by introducing a new parameter and saying, hey, if we don't care about speed, then we're good, then we can try and get all of these things. And so that rep that represents and kind of opens up the trade off space to say, what are the use cases? What are the situations in which we don't need speed? And we can benefit from generalized messages trust minimization and being able to deploy quickly across many different chains.
00:30:23.184 - 00:30:27.524, Speaker B: And when you say you give up speed, like, why is latency the trade off here?
00:30:28.344 - 00:31:37.712, Speaker C: Latency is the trade off because in optimistic verification, what you're saying is we are going to wait a little bit of time in order to allow honest watchers to be able to flag fraud within the system. And so how greater security is achieved is you're basically saying, hey, let us have some party be able to do forward permissioning. There is one party in the nomad system called the updater that can sign state routes and send them from chain a to chain b if there was no optimistic window. This is a one person, externally verified system. But because we say, let's hold off from settling that message instantly and provide a little bit more time, we can allow the rest of the world to watch in and say, hey, I'm observing what this person is signing, and this looks fraudulent, so I'm going to post a fraud proof on chain and flag it. And so now we've achieved that property of trust minimization. And we also get generalized messaging because you have some party being able to annotate a state route and post it to recipient chain.
00:31:37.712 - 00:32:01.194, Speaker C: And it is more extensible because you are not verifying anything on the receiving chain, you are not running a like client. That is coupled with consensus. You are doing everything on the sending chain optimistically and just accepting it after 30 minutes. If you don't hear anybody say anything because you're saying, hey, nobody has raised their hand and said anything. Therefore I can assume this to be valid.
00:32:01.974 - 00:32:20.834, Speaker B: I guess in that case, aren't you still basing your security on, I guess, an external set of watchers, in this case, that's monitoring transactions to make sure nothing's going wrong, and if anything goes wrong, they'll flag it. Isn't that a similar trust assumption as trusting an external set of verifiers to verify transactions?
00:32:22.094 - 00:33:21.304, Speaker C: This is a great question, and I think is often flagged as kind of a way to challenge nomads security model, which is it's only as secure as the watcher set. And the reason it is Nomad has a better trade off here is you don't require a majority of watchers to do anything, you just require one. As long as one watcher is able to submit a fraud proof, then you can basically prevent the system from processing bad message. And so the core of it here, I think James said it best. He was like, he had a tweet that talked about this where a watcher does not have the permissions for a message to be able to be sent cross chain and accepted, they only have the ability to revoke. And so because you introduce a party that has the ability to revoke kind of the channel SLA within the system, you can just rely on one of those parties to say, halt the system, there's something going on, and then we can take our time and reset it.
00:33:21.804 - 00:33:31.772, Speaker B: And is there any skin in the game involved for these watchers? Like, for instance, if someone, if a competitor wants to spam the network and they just revoke everything, what do you.
00:33:31.788 - 00:33:33.780, Speaker C: Mean by a competitor spamming the network?
00:33:33.892 - 00:33:44.236, Speaker B: So, for instance, if a bridge doesn't like the fact that there's more volumes in Nomad now, so they send some watchers to basically just revoke everything that happens in Nomad. Is that a kind of risk factor?
00:33:44.420 - 00:34:25.434, Speaker C: Yeah. So you bring up the point where if you have permissionless watchers within the system, it introduces a griefing vector, because what a permissionless watcher can do is they can say, at zero cost to me, I will halt this channel. I can't cause safety failure. I can cause somebody to process a fraudulent message. But what I can do is shut down the bridge so that users are not able to send messages or assets across. Across the bridge. The way we can kind of combat this attack vector is by permissioning watchers, first of all, which I think kind of gets, gets us back into the trusted situation if we just permission them without any crypto economics.
00:34:25.434 - 00:35:26.764, Speaker C: But one of the things we've been exploring is having something like a watcher tax, where you permission. The watchers say that there's n number of watchers that can send fraud proofs and. And halt the bridge. But what you do is you make them pay a watcher tax when they submit that message to the replica or the recipient chain saying, hey, there was fraud, but in order to be able to then go take the bond that the updater has posted and withdraw it, they have to take that signature that they posted on the replica and posted on the home. And if that fraud that they flagged wasn't actually fraud, then they've just spent a tax to be able to grief the channel without being able to withdraw the bond that the updater has posted. So what we're doing here is we're using crypto economics and game theory to say there may be somebody who wants to grief the system, but if they do so unnecessarily, when fraud doesn't actually happen, it's like, boy, who cried wolf? Somebody cries wolf. Without there actually being one, they will pay a tax and not be able to receive the benefit.
00:35:27.384 - 00:35:45.784, Speaker B: Have you guys, I guess, also explored slashing mechanism similar to a proof of stake blockchain, where maybe watchers put up a stake proportional to how much fees they can earn. So if they want to earn more fees, they can put more stake. But if they do report a fraudulent transaction that wasn't fraudulent, then they get slashed.
00:35:46.844 - 00:35:55.064, Speaker C: Yeah. So this is very similar to the taxation method, where the real question you're asking is who watches the watcher?
00:35:55.164 - 00:35:55.472, Speaker B: Right?
00:35:55.528 - 00:36:26.064, Speaker C: Like, who is the person responsible for making sure that an adversarial watcher does not get away with griefing vectors? And so you can do something like have a governance system where all the other watchers can choose to then slash the watcher and remove them from the set. But the taxation method is actually quite similar to this where if they do grief the thing, they're automatically slashed. Cause they paid the tax in order to prove send the fraud or basically tell the replica to prevent processing messages.
00:36:26.604 - 00:36:37.144, Speaker B: So I guess given that there is a crypto economic element there to align incentives for watchers, why still have permission watchers? Why not just open it up for anybody to become a watcher?
00:36:37.724 - 00:38:20.646, Speaker C: If you have an unbounded set of watchers and you don't have their addresses registered on chain, then you won't be able to take that. You won't be able to take their signature and apply it to the destination because you need the replica or the destination chain to be able to know. These are the sets of watchers that I'm supposed to listen to, right? If I get a signature from one of these pre registered parties then I can halt the channel and say I won't process messages if it's an unbounded set. We get back to what you described earlier where anybody who's adversarial can come show up, not put any skin in the game and basically grief the system because we need to be able to take that signature in this taxation scheme and be able to post it on the home. So that requires that presupposes knowledge of who is in the watcher set. But part of like, I think this line of questioning is like important because what we're trying to figure out is does this optimistic verification method, does it actually lead to greater security than an external verification method? Or are we just kind of rotating the parties, giving them different names and saying it's more secure? And what I draw a comparison to, and I've learned a lot from Arjun, from talking with him about this, is, this is a lot like what's going on with optimistic rollups, where optimistic rollups are not fully in a decentralized model yet. But what we want to do is we want to identify a destination state that does represent a high ceiling for decentralization and bit by bit make progress towards it over time.
00:38:20.646 - 00:39:17.774, Speaker C: The quote unquote progressive decentralization model. And so I would be lying to you if I said, like right now we have a fully decentralized system that has crypto economics and these watcher kind of checking the watcher schemes built in. But what we're trying to do is figure out what are the schemes that will offer the most benefit and be pragmatic to users. Nomads whole design model is predicated on being able to offer enough security and basically focus on the user to give them what they need. And as long as we keep that heuristic in mind and keep progressing towards a more like, trust minimized system, we can figure out the exact schemes later on. Other ones we've been thinking about include falling back to like, clients if there's like client protocols available. But really a lot of this is open space that we are yet to explore in earnest.
00:39:17.774 - 00:39:27.914, Speaker C: But the key thing we have in mind is this is the destination we want to go to and need to be able to ship this system and capture market share now while the bridge race is so active.
00:39:29.034 - 00:39:46.894, Speaker B: Definitely. And I guess just to touch on that point that you mentioned, in the current state of the market right now, do you think the trusting a set of external verifiers is better or worse than trusting a set of permissioned watchers given, you know, what we have right now today?
00:39:47.714 - 00:40:46.328, Speaker C: Yeah, I mean, I guess I'm making that bet by putting skin in the game, right? Like I'm building up this system that's doing the ladder. And I think the core of it is that for systems that don't need instant settlement, right. I feel that giving somebody permissions over liveness failures is a lot more. It lets me sleep better at night than giving somebody permissions over safety failures. And so you can increase the number of bodies and scale that validator or external verifier set to try and offer more security. But at the end of the day, if they're all running the same client, or if they're having the same kind of infrastructure setup, then we're really just cloning the same vulnerability if anything is found. And so I'm much more biased towards a system that gives revocation permissions and says, hey, we're going to have financial controls by default.
00:40:46.328 - 00:41:35.796, Speaker C: We're going to wait 30 minutes to allow for observability, and in the worst case scenario, the system will just halt. That strikes me as a better worst case scenario than losing $600 million. But what I think keeps me motivated for the long run is I don't know how these externally verified systems decentralized over time. I really think that the wormhole and jump teams are doing well and they're good people. Certis was known for their security, but they themselves have been working on like client based systems as a kind of long term goal to potentially replace these certainly verified systems. And I think that will take time and be a more messy shim than starting out with a semi permissioned, optimistic system. And making it fully optimistic over time.
00:41:35.796 - 00:42:10.894, Speaker C: And the benefit, even if we kind of like get to this, a fully trustless state where like clients are possible, is that for expensive chains, it might always be better to be optimistic, especially if we can figure out these game theoretic mechanisms that make it truly secure. Because at the end of the day, it is. You shouldn't have to be verifying everything if you don't need to. If you expect that the majority of the time you're going to have like a quality system, and it's only at the edge cases where you need to really invest energy and spend and cost, then optimism is the way.
00:42:12.194 - 00:42:39.334, Speaker B: I really like the way you framed it, where the theoretical worst case for an optimistic bridge is chain halt and not loss of funds, not like externally verified system. And that to me is really interesting. And I guess on the flip side, given that fundamentally with an optimistic system, there is always going to be a latency, do you think that limits the type of applications that developers can build with Nomad versus with something like layer zero or some of these other bridges?
00:42:40.524 - 00:43:35.388, Speaker C: TPD honestly, I think we're still in such an early phase of like cross chain communications that it's anybody's guess at this point. Part of what we feel is that, like a little bit of latency is worth the cost of greater security. And in certain use cases, like token bridging, it can be mitigated by partnering with a team like Connext. Or Connext offers a liquidity network, right? Which as long as there's capital on both sides and tokens that the user wants, they can get instant liquidity. And so with Connext, we have formed this modular interoperability stack where Nomad serves as this kind of secure settlement layer that's a little bit slower and then uses Connext on top. If retail users want to bridge quickly, and they don't even have to touch Nomad. And so for fungible tokens, there is this natural complement in Connext.
00:43:35.388 - 00:44:45.488, Speaker C: But for other use cases, say gaming, where you need instant finality, I don't necessarily see that Nomad is a perfect fit right at the beginning. But then I beg the question, are we really set up for success in the long run if we are building cross chain gaming applications? I take a more kind of localist perspective, where a lot of the applications that require instant finality and composability should remain within the boundaries of a cluster, right. This comes back to Celestia's idea of clusters, or Vitalik's multi chain versus cross chain, a lot of economic activity will be intra cluster activity. It's things that will happen within execution environments where the same underlying validator set secures them. And this way you can get a lot more guarantees in terms of finality and the speed at which things happen when you need to go inter cluster. That's where security matters. And so I said this on another podcast, but I would consider it like a transatlantic flight, right? What matters is getting there safely and not necessarily quickly.
00:44:45.488 - 00:45:19.524, Speaker C: Like if you want to be quick, you could be shot out of a cannon, but you won't get there as safely as if you take like a flight. And you don't mind 30 minutes because you have other goals to accomplish. You want to get there in one piece and then do business and then come back. I imagine most economic activity in the future in crypto will be within clusters, but there will be inter cluster activity, and that's where we need to make sure that those channels are secure, because the packets being transferred between them will naturally be of higher value and may not be, may not represent more frivolous use cases.
00:45:20.664 - 00:45:34.664, Speaker B: And I guess now comes the age old annoying vc question, which is whether you think this is a winner takes all space or how do you see the market play out with all these different bridges, this patchwork of bridges, as you mentioned?
00:45:35.804 - 00:46:41.626, Speaker C: Yeah, I've been thinking about this a lot, because there is, at this point, there's not only competition across the token bridges for network effects and kind of dominance over wrapped assets, but there's also competition between channels, like channels that are going to be competing for, for generalized message passing use cases. And not only that, but there's going to be competition across channel aggregators. So there's Lefi and Socket, who basically allow developers to find one interface and maybe interact with different channels. So frankly, I'm not sure this space evolves too quickly and matures experiences so many slight shifts in the market structure that even if I gave you a high conviction answer now, it might be invalidated by somebody doing something novel in a month or two. But really, I think where value accrues is to quality technology that has a pragmatic go to market. Right. I think we've seen over time that go to market alone doesn't cut it and technology alone doesn't cut it.
00:46:41.626 - 00:47:37.474, Speaker C: So kind of like in Buddhism, you try and find the middle way where you have a technology that from first principles offers what users care about, which is some level of great UX, but also security because a user is hit worst in these blacksmith events. And then you take a go to market approach that is pragmatic, that you deploy on chains where people are target use cases that meet your technology and create some type of, what would I call it like being enmeshed within the economies of the local system, such that even if the market changes, you're providing real value to people. There's a bit of a hand waving answer to your question, but really I think what I'm getting at is people who are building good channels and working with solid teams and solving user needs will find some fit within the market, though I don't know where the majority of value extracted or accruing is going to be.
00:47:38.054 - 00:48:17.814, Speaker B: Yeah, and I think in one of our previous conversations we talked about the idea of how maybe for some use cases, for retailers that just want to farm yield, there are externally verified bridges that are very fast, that fits that use case, but say for daos or institutions that are moving hundreds of millions of dollars at a time, they need security above all else. So if nomads thesis stands true, then maybe nomad is the place for them to do it. So maybe we'll see different areas, different bridges used by different types of people as well, which I'll be pretty surprised to see, just given that wasn't how, I guess, layer ones played out. But I'm somewhat confident, I'd say 60% confident that's how things are going to play out.
00:48:18.434 - 00:49:34.440, Speaker C: Yeah, I think the difference here is that with layer ones, block space has largely become a commodity, right? I think there are kind of differentiators between the layer ones. But provided that there's enough baseline security that you can trust the validator set, then really the primitive unit that a layer one is selling is censorship resistant compute. And as long as you can, what bridges and channels are doing is it's creating connective tissue where you're able to effectively connect the block space across different chains. And so I think it's a little bit different where channels, fundamentally, because they're connecting block space, need to be evaluated based on the security merits of these channels. Whereas the layer ones, provided that they're good enough, they will just provide block space to anybody building applications. And so over time, people will deploy on an interchain and then use the channel technology that they feel will give them the security guarantees that their application desires. And of course then we get into trade offs over does it make sense to have a monolithic channel with greater censorship resistance or fragmented channels that are more use case specific? And I think that is a question that I will punt on this time around.
00:49:34.632 - 00:49:44.794, Speaker B: Yeah, definitely. I'm super excited and thank you so much for taking the time. Pranay. This has been really, really educational. And for people who want to keep up to date with what you're working on, what are the best channels for them to do this?
00:49:45.254 - 00:50:06.114, Speaker C: Yeah, I think Twitter is great. You can find me on Twitter or nomad. My handle is just first name, last name and then nomads. Twitter is nomadxyz. And of course, join the nomad discord as well. Happy to answer any questions. Async and yeah, it was a pleasure coming on.
00:50:06.114 - 00:50:18.432, Speaker C: I always like chatting with you, Jason, because you ask thought provoking questions. And I think I'll go back and ponder some of them a little bit more so that I can have a maybe more fleshed out answer for you next time when we chat.
00:50:18.568 - 00:50:31.384, Speaker B: Yeah, thank you very much. And for Blockcrunch premium subscribers, as a new feature, we're going to prepare a whole research note based on this as well. So definitely go check that out on the blockcrunch.com. And thank you so much for coming on again. Pranay thank you.
