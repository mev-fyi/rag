00:00:00.200 - 00:00:12.510, Speaker A: Web three offers a brand new petri dish. Our job is to buy great tech at great prices. AI is also libertarian right enable use case that people haven't been able to do today, the next hundreds of millions of players, they will come to the market through mobile.
00:00:12.582 - 00:00:49.734, Speaker B: My personal reason why I could be bullish in the next twelve months is. Hey, everybody. Welcome back to another episode of the Blockcrunch podcast. I'm your host, Jason Choi. Now, say network has joined the billion dollar network club in market cap this month, doubling its value just as its user count seemingly skyrocketed on the back of the rise of inscriptions, amongst other things. Now, in addition, SaI network also made a big splash by being the first live blockchain to become a parallelized EVM chain, which many people seem to still not fully understand what it is. So to understand what, say, 2.0
00:00:49.734 - 00:00:57.294, Speaker B: means for crypto, why I'm excited about it, I'm really excited to have co founder of, say, Jay once again on the show. So, Jay, welcome back, man.
00:00:57.414 - 00:01:04.074, Speaker A: All right, thanks for having me on, Jason. Been a while since we last chatted. So excited to catch everyone up on what we've been kind of jamming on.
00:01:04.454 - 00:01:21.494, Speaker B: Yeah, so this has been a pretty hectic, not even a hectic year. It's a hectic quarter. A lot of stuff seems to start happening. But before we dive into some of the exciting stuff happening recently, for those who are not aware, can you briefly give the elevator pitch on kind of what Sei actually is and what you guys are trying to do?
00:01:21.614 - 00:02:01.000, Speaker A: So Sai is the fastest layer one out there. We got started building in early Q one of 2022, so almost two years ago, and since then, say, went live on August 15 of 2023. So a little bit over four months since we started recording this podcast. And, yeah, I mean, overall, I would say things have been going really well. So from just a pure chain performance standpoint, right now we're observing 390 millisecond time to finality on mainnet that has been sustained since August 15. This makes sage is flat out the fastest chain in existence. It's the magnitude faster than Solana, multiple times faster than most other chains out there.
00:02:01.000 - 00:02:54.434, Speaker A: So that's been fantastic. Now, the biggest issue that we've run into since launch and the biggest feedback that we've gotten from developers is that say does not have EVM compatibility. And if you look at crypto native developers right now, almost all of them are EVM developers. Like, that is what their bread and butter is. That is what they're the most familiar with and right now say let's developers write rust based smart contracts. If you are a solidity developer and you move to writing smart contracts in rust, that ends up being an extremely high friction thing to do because any bugs that you have in your rust based smart contract, they can potentially result in your entire contract getting just completely drained. A lot of developers are very, very against switching over and experimenting with new execution environments if they're already masters with solidity and EVM development.
00:02:54.434 - 00:03:46.354, Speaker A: Kind of based off that feedback, we realized that the biggest thing that needs to be improved in stay moving forward is we need to be supporting the evm. That's why staylabs. We proposed stay v two, which is the first paralyzed EVM. What this would let us do is it would essentially let us get the best of both Solana and Ethereum, where we would get the high performance that you observe with Solana while also benefiting from the tooling in the mind share that exists around the evm. So we announced that last month. Since then, everything has just been absolutely crazy, and I'm really happy to be on right now because we haven't actually done any kind of podcast about this yet. This is going to be the first time that anyone from our team publicly talks in more detail about save e two, so it should be just very fun to go over all of this.
00:03:46.504 - 00:04:30.885, Speaker B: Yeah, no, I'm excited for that, for this exclusive. And there's a lot of stuff we can go through, and I think there's a lot of technical nuance about how you guys are able to do this as well. So definitely, I think we should have kind of an honest discussion about what some of the trade offs you guys are making are. But I guess before that, right after you guys announced parallelized EVM, I think clearly you guys hit some sort of a nerve in the psyche where everybody is super excited. All of a sudden, something clicked in the Z guys. Basically, everybody seems to suddenly be awakened to this idea of paralyzed EVM. So we can talk about it a bit, but in the expanse of, I think, like two weeks or three weeks, there's, I think, almost 500,000 addresses, according to this dashboard on flip site that we'll link below.
00:04:30.885 - 00:04:46.447, Speaker B: And since ten days ago, it's 27th now, ten days ago, there's 30,000 new addresses that just got added. So what is happening? Like, what just catalyzed this? Is this just people suddenly discovering, say, and wanted to try it out? And were there any kind of unexpected outcomes from that?
00:04:46.525 - 00:05:22.790, Speaker A: So, I mean, to your first point, I think that the paralyzed EVM type of narrative is something that really clicked with a lot of people that are very crypto native. And I think one of the reasons for this is gas fees on Ethereum are becoming ridiculous again. And paralyzed chains seem to be just with the kind of success that Solana has been having. Paralyzed chains are very quickly becoming top of people's minds. And I think one thing that I've noticed is that 2023 definitely felt like the year of modular chains. It felt like modular chains with Celestia leading the way were definitely top of mind for most peoples. For most people.
00:05:22.790 - 00:05:56.034, Speaker A: We saw an explosion in the number of rollups being built. A lot of new roll up as a service product is being built as well. And I think were gradually shifting towards a new type of zeitgeist. 2024. The writing is on the wall that its going to be the year of paralyzed chains. I think theres already starting to be a lot more chatter about parallelization being applied to many different types of chains. So people are already talking about using it for rollups, which I think that fundamentally just does not work if you're using roll ups with Ethereum da, which we can talk about later.
00:05:56.034 - 00:06:27.792, Speaker A: And people are talking about applying this to milladiums, sovereign roll ups also for new l ones. I think this next year is going to be fantastic for paralyzed chains in terms of, say, activity specifically. So, yeah, in the past month, things just absolutely started exploding. It got started off actually with nfts onse. So that was like the first thing that started seeing a ton of activity. Specifically, there's like the colony NFT and then the sailors NFT. And those ended up just getting massive amounts of interest from people afterwards.
00:06:27.792 - 00:07:02.302, Speaker A: Then inscriptions started. So inscriptions, I mean, I don't know when people are going to listen to this, but just for the month of December, they've been kind of plaguing every single chain. Yeah, so, I mean, December has just been the month of inscriptions where inscriptions have been plaguing every single chain. So in Sais case, a couple of weeks ago, we saw that a bunch of inscription activity started and then, I mean, afterwards, now we've started seeing a ton of meme point activity in a lot of other projects that are starting to get deployed as well. So there's just a ton of new activity happening. And that's been honestly very exciting to see. Like, it's a permissionless chain, anyone can build on it.
00:07:02.302 - 00:07:44.636, Speaker A: And a lot of stuff that, like, we never would have anticipated, like inscriptions a month ago if you had brought that up to me, I would not have thought that that would have been something that is leading usage on CA in December, but here we are right now. So the permissionless nature of the chain is just fantastic to see people experimenting with it. And, yeah, I mean, in terms of issues, I will say that it was extremely satisfying to see that the core chain itself was extremely stable throughout all of this. There were no issues with the core chain, no liveness issues, no outages. So that was, I think, a testament to the work that the engineering team has been doing so far. So I'm very happy about that. There were issues around RPC's, though, specifically, it was difficult for some nodes to catch up.
00:07:44.636 - 00:08:00.400, Speaker A: And we'll talk a little bit more about how we're resolving those moving forward, but there's improvements we're making at the p two, p layer and also at the database layer that'll be helping resolve this moving forward. So, yeah, I mean, tons of learnings from that, but overall, I would say activity has just been up and to the right recently, so that's been fantastic to see.
00:08:00.532 - 00:08:40.266, Speaker B: Yeah, that's super exciting. And the point about rpcs, I think that that's actually leads to a bigger discussion about state bloat, kind of the trade offs for making paralyzed evms that we can dive into. But before that, just to preface that a little bit. So what I was really excited about, say, in the beginning was that you guys were very clear in terms of the trade off. You guys want to make right by design, kind of what people would call a monolithic chain, meaning it's designed to handle many, many transactions, many throughput. This is in kind of distinct contrast with what all the Ethereum guys have been saying this whole time, which is they want to go more modular. They want to go a roll up centric future where a lot of the settlement goes to, sorry, execution goes to L2 and settlement goes to layer one.
00:08:40.266 - 00:08:57.638, Speaker B: So they kind of disaggregate the chain a bit. So this year, we've seen that whole debate between monolithic chains and kind of modular chains slash like a roll up thesis. I'm curious, like, how did you reason about this? And, you know, did you guys necessarily pick the former, which is the monolithic chain? And, you know, why?
00:08:57.786 - 00:09:30.660, Speaker A: So our primary goal with save you two is to scale the EVM. And we think that there's different ways that you can scale the EVM long term. We think that both the integrated approach and the modular approach will be used to scale the EVM. We don't think that one is necessarily better than the other. Just universally, we think it depends on the type of use case you have. And based off that, you should use either the integrated or the monolithic approach, or integrated or modular approach. So one thing I think that we can actually look to for an example of how this plays out is web two.
00:09:30.660 - 00:10:06.390, Speaker A: So in web two, there's this idea of monolithic services and then microservices. Monolithic services are high level ideas, similar to an integrated chain. And then microservices, high level, are similar to modular chains. And in web two, what's ended up happening is that if you need really high performance infrastructure, you build a monolithic server, or you take a monolithic approach, and if you basically need to be scrappy, experiment quickly, you don't really care as much about performance. Then the microservice architecture ends up being better. There's a ton of downsides to a microservice architecture. I was an engineer at Robin before this.
00:10:06.390 - 00:10:35.730, Speaker A: We use heavily microservice. We actually started off monolithic and then we migrated to microservices. There were a ton of issues that came up with the microservice architecture. Each separate service has its own state. You then need to have some async communication happening between these different microservices to be able to maintain state. We ended up having happening dozens of production issues just tied to that. It's also extremely difficult to be able to just coordinate things like being able to issue HTTP requests and be able to synchronously process requests.
00:10:35.730 - 00:11:35.880, Speaker A: It ends up leading to a lot of complexity. And I think it's going to be the exact same thing that plays out in web3. Modular chains, where you have basically separation between execution settlements, data availability, they're really good if you only need to improve one part of the stack. So if you're only going to be improving, let's say the execution side, and you want to create some, let's say a chain that is optimized for like nfts or for real world assets or something, and you want to add in like stateful precompiles to that execution layer, then a modular chain can be substantially better for your specific type of approach. The issue there is that now there's like three separate chains that all need to be working for your chain to be working smoothly. If any of these chains have, let's say a reorg, let's say your DA layer has a reorg, like you're using Ethereum for DA, there's some kind of reorg that happens, well, guess what now that's going to be impacting the higher levels of the stack as well. If any of these chains like, let's say the settlement layer, has any kind of liveness issues, then guess what, now it's going to result in delays for settlement to be happening and for finality to be happening.
00:11:35.880 - 00:12:11.222, Speaker A: So there's a ton of complexity that adds, that adds up over here. We think that if you're trying to build just the fastest infrastructure, you can take all these different parts of the stack, just combine them into one really, really efficient and really fast infrastructure. Then you're able to get better performance and just have a better user experience because there's less complexity. But if your goal is to just iterate on, let's say, the execution layer, then I think the modular approach can be better just based off that reasoning. From the get go, we knew that we wanted to build an integrated chain. That's the only way you can really have the best possible infrastructure, the best performance. And that's why we decided to go down that route.
00:12:11.398 - 00:12:44.536, Speaker B: So let's dig into how the performance is possible, because obviously now the big focus for you guys is achieving this parallel execution. And just to give some context to our listeners, now, developers probably know, or even non developers know, that for chains like Ethereum and forks of EVM, which is a lot of other l ones out there, they run transactions in a sequential way. So that means one transaction they need to happen before the next one can come and get finalized. This is quite different from parallel execution, which is like multiple transactions at once. So can you explain what this idea is, where it came from, and why it's so important?
00:12:44.720 - 00:13:17.490, Speaker A: Okay, so I think I'll just first briefly go over the technical approach that we're taking right now. And then afterwards I can talk more about what parallelization is and why it matters. So currently with save u one, like this diagram on the left, show us what we're doing. With save v two, there's three fundamental improvements we're going to be having. The first is we're adding in EVM compatibility. So say we'll now have full EVM bytecode compatibility. You'll be able to take any existing EVM smart contract and deploy that to SaE with no additional overhead.
00:13:17.490 - 00:13:46.434, Speaker A: And this will also be interoperable with the rest of the chain. So currently, Sei supports Cosmos and smart contracts. These EVM contracts will now be interoperable with the cosmos and smart contracts and vice versa. So that's the first thing we're doing. The second thing we're doing is we're transitioning from access based parallelization to optimistic parallelization. We can talk more about this later on, but at a high level, what this means is that developers don't need to define dependencies. So it makes it very easy from a developer experience standpoint.
00:13:46.434 - 00:14:30.072, Speaker A: And it also results in any new contract that is deployed being paralyzed automatically without developers needing to do anything. And the third major thing that we're doing is we're improving the way that state access, state commit, and state storage works. We'll probably talk about that later on as well. And at a high level we'd call that state DB. So your specific question was around parallelization, like what is it and why is it important? So yeah, I mean, as you said before, every existing EVM chain, it has single threaded execution. So if you have 100 transactions in a block, every single one of them will be executed one after the other. Now this is really simple, so it makes it very easy to implement it.
00:14:30.072 - 00:15:12.948, Speaker A: Parallelization is extremely complex to support because there's a lot of sources for non determinism that can result in the changes basically having liveness issues, like the chain would just have an outage. So you need to be very careful with supporting parallelization. So yeah, most chains right now, they have single threaded execution. Now this is simple, but it's not great from a performance standpoint because it's not leveraging modern hardware to be able to get the best possible performance. The way that modern hardware works is that there's a ton of cores on these machines, and if you're able to use these cores to be able to process transactions simultaneously, then you're able to get better performance. And that's this idea of parallelization. Parallelization is just ubiquitous in software engineering.
00:15:12.948 - 00:15:54.222, Speaker A: It's used on essentially every single part of the stack to help give better performance. And for any listeners that might not be as familiar, a simple example of parallelization on a blockchain would be just token transfers. So let's say I'm transferring tensei tokens to JSON, and then my co founder Jeff is transferring Tensei to Jason's co founder Daryl. In this case, like that, these are four separate accounts and there's no overlap between them. So there's nothing preventing us from executing those transactions at literally the same time instead of executing them one after the other. So if you execute them at the same time, let's say each transaction takes ten milliseconds. If you execute them sequentially, it would take 20 milliseconds because you're doing one after the other.
00:15:54.222 - 00:16:21.484, Speaker A: If you do them in parallel, then in ten milliseconds you can execute both of them. So it has substantial performance improvements. The way that modern hardware works is that there's a ton of cores on these machines. And if you're able to use these cores to be able to process transactions simultaneously, then you're able to get better performance. And that's this idea of parallelization. Parallelization is just ubiquitous in software engineering. It's used on essentially every single part of the stack to help give better performance.
00:16:21.484 - 00:17:01.322, Speaker A: And for any listeners that might not be as familiar, a simple example of parallelization on a blockchain would be just token transfers. So let's say I'm transferring Tensei tokens to JSON, and then my co founder Jeff is transferring Tensei to Jason's co founder Daryl. In this case, like that, these are four separate accounts and there's no overlap between them. So there's nothing preventing us from executing those transactions at literally the same time. Instead of executing them one after the other, if you execute them at the same time, let's say each transaction takes ten milliseconds. If you execute them sequentially, it would take 20 milliseconds because you're doing one after the other. If you do them in parallel, then in ten milliseconds you can execute both of them.
00:17:01.322 - 00:17:48.840, Speaker A: So it has substantial performance improvements. So since we actually announced JB two, one of the common questions that we get asked is are we basically paralyzing within an individual transaction or are we paralyzing across transactions? So the simple answer to that is we're paralyzing across transactions. And I mean, I guess one thing that would be good to talk about is opcode parallelization versus runtime parallelization. Opcode parallelization is this idea that you can paralyze different opcodes that are there in a transaction. This would allow you to paralyze within a transaction. For a transaction, you see all the opcodes, you see which ones aren't affecting one another, and then you're able to improve performance by just paralyzing these opcodes. With that, you might have a ten millisecond transaction that starts taking like eight milliseconds or something.
00:17:48.840 - 00:18:34.986, Speaker A: So this helps improve individual transactions. With runtime parallelization, you're able to, similar to the example before, take different transactions and then run them at the same time. So in a ten millisecond period, instead of only having one transaction get run, you might be able to have ten transactions get run. So like, which one is better? These are not mutually exclusive, you can actually have runtime parallelization along with opcode parallelization, but in terms of which one will have a bigger ROI in the short term, you'll be getting the bigger bang from the buck by supporting runtime parallelization, which is the exact approach that SAE is taking right now with save, two different transactions will be paralyzed. And, yeah, I mean, this will result in the greatest initial improvements. And we think right now opcode parallelization is a premature optimization, but in the long term that is something we'll be looking into as well.
00:18:35.130 - 00:18:50.562, Speaker B: So if you're parallelizing across transactions, does it open up any new vectors or any exploits? Like, one thing that might come to mind for people is reentrancy attack. What if I try to double spend a coin in two transactions that are processed in parallel? Is that something that's possible?
00:18:50.658 - 00:19:32.860, Speaker A: Yeah. So we're going to be having full EVM bytecode compatibility. Reentrancy attacks are possible with the EVM, so we're not going to be removing potential for reentrancy. I mean, any issues that exist with the EVM will continue being there with av two, but in terms of what the actual experience will look like, the paralyzed experience will be basically the exact same as the single threaded experience from a user standpoint. So if you were to execute transactions one after the other, there will be some output state that is generated, that'll be the exact same output state that is generated when you paralyze transactions. So it's a deterministic process, and the outcome should be the exact same. So, yeah, I mean, it should be no real difference.
00:19:32.860 - 00:19:54.394, Speaker A: And one question that also comes up around that is like MEV, does having this paralyzed type of processing result in any new areas? For MeV, the answer to that is no. Assuming you have the same hardware being used, assuming you have the same network topology, then there will be no difference in the type of MEV that you can see, because it's going to be the exact same type of output that is generated.
00:19:54.734 - 00:20:41.932, Speaker B: Yeah, that makes a lot of sense. And I love to kind of dive into the potential trade off you're making to create this, because the concept of paralyzing transactions isn't completely new. There has been discussions before on other chains as well. But if you look at chains like arbitrum, optimism, or many EVM chains, some might say that the constraint actually isn't paralyzing the execution, but actually the state growth. Like for, I think a good example that was raised by someone was that BNB chain, the binance chain, last year, they forked geth, which is the main Ethereum execution client, and they set the guest limit very, very high so that they can do a lot of transactions. But because they were running at such a high speed with like zero optimization, they started to experience state bloat. So the state got so big that it was very hard for these nodes to kind of get in sync.
00:20:41.932 - 00:20:48.876, Speaker B: So first, I guess, can you explain the, the problem of state load? And second of all, is that a concern for you guys?
00:20:49.020 - 00:21:30.970, Speaker A: Yeah, that's a great question, and I definitely agree with that, that you need to think about how the system interacts. Like all the moving parts interact, instead of just focusing on one part of the system and just purely optimizing that without thinking about how that impacts everything else. If you add in parallelization, then that can lead to greater state load issues. First of all, at a high level, what is state? So for any listeners that aren't super familiar, state is the data that needs to persist on the blockchain to record the current ledger. So basically you can think of it as like account balances. Like I have ten say, Jason has twelve say that would need to be part of the active state of the blockchain. Another thing would be smart, contract data.
00:21:30.970 - 00:22:07.798, Speaker A: Like let's say you have some kind of amm contract. Which pools does it have? What are the balances of these pools? All of that would need to be persisted by every single validator and full node to be able to process any incoming transactions. The issue that happens is that when you have a high throughput chain, like when you just have more transactions that are coming in per second, that results in more state that is generated. Then there's two bigger issues that come from this. The first is state sync. Then the second is state storage. State sync is basically the process of syncing data.
00:22:07.798 - 00:22:46.304, Speaker A: When you're creating a new node or you're recovering a node or something, if you're not thoughtful about it, then it becomes extremely difficult to start spinning up a new node. It can take on the order of dozens of hours. Or it might be impossible to actually start running a new node. If you're not careful about it, then state storage is just the pure amount of data that needs to be stored in disk for every single full node to be able to actually track the current state of the blockchain. That leads to resource constraints as well, because it eventually starts becoming really expensive to be running these nodes. From Satis standpoint, there's a ton of work we're doing with SADB to help improve both of these. Let me just share my screen again to go over SADB.
00:22:46.304 - 00:23:19.886, Speaker A: First of all, high level, what is Sadb? There's a few different improvements that we made as part of SADB. The first is we have a memory mapped IVL tree which helps with better state access. Like if you're trying to read data, it becomes more effective. It also allows us to have async writes that are happening to disk. So in order for a block to finish processing, it just needs to be written in memory. And then afterwards the process of actually writing it to the database. That can happen after the block processing is done.
00:23:19.886 - 00:24:01.794, Speaker A: So that happens in the background. Initially it gets written to a write ahead log, and then afterwards it gets written to the database, which is a similar type of paradigm to what you see happening with just normal database design in web two. So the way that we're improving both of the things I mentioned before about state load, the first thing that we can talk about is state sync. With the Save e two approach we actually observed a twelve x improvement. So 1200 percent improvement in statesync times. And the way that we were able to do this is the process of exporting a node becomes more efficient. When you're able to have everything that is there in memory, you're able to export that like create a snapshot of that much more efficiently.
00:24:01.794 - 00:24:44.422, Speaker A: The process of being able to import that to a new node, it becomes faster as well. Because when you have an Mmaped IVL tree, you basically just need to create three separate files in the file system. So there'd be a file for the key value pairs, there'd be a file for the leaf nodes and there'd be another file for the branch nodes. And once you write these files to disk, then you're just able to create the MF IBL tree. And the overall time this takes ends up being twelve x faster. So that's one thing that is extremely effective about the approach we're taking here. The second thing that we're doing just for state storage is with state v two we observed a 50% decrease in the amount of state that needs to be stored on disk.
00:24:44.422 - 00:25:49.964, Speaker A: And the way this happens is the in memory or the mmapped IBL tree, it requires significantly less metadata to be stored because there's different files that are being persisted on disk. And because you're persisting these files on disk, you're able to compress them in a better way versus having them be stored somewhere else. So we actually ran the numbers on this on the Atlantic two testnet and there was a 50% improvement in the amount of data, or 50% decrease in the amount of data that needed to be stored. This drastically helps reduce state load, and then the question becomes, how do you actually solve it completely in the long term? State load is one of those things that's an unsolvable problem in a way. The approach that we'll be taking in the longer term is through rent, specifically charging smart contract developers a fee to be able to persist data on the blockchain. It's pretty difficult to be able to know all the different ways that users are going to be interacting with state rather than having the chain try to say that for these different types of users, we're going to have different ways that we try to handle state for them. Instead of doing that, we're just going to let applications decide what they need to do.
00:25:49.964 - 00:26:18.168, Speaker A: In some applications cases, they might need to only persist data for a month, then afterwards it's irrelevant. It's fine if it gets deleted, they'll just let that data expire. In other applications cases, they might need that data. They might have different access patterns where they need it right now. They might also need it five years in the future. So they're fine with having that data get deleted as long as it's stored off chain and they can submit some kind of ZK proof to add that data again. And then other people's cases, they might just need it to persist for eternity.
00:26:18.168 - 00:26:26.084, Speaker A: So depending on the applications use cases, we anticipate there to be different types of state run approaches that application developers take.
00:26:26.424 - 00:26:58.562, Speaker B: Yeah, so it sounds like a naive approach where you're just paralyzing execution that alone cannot work. You need, you know, pretty optimized state sink and state storage. And then even in the long term, even that might not be sufficient. So you need to incorporate some sort of, kind of rent infrastructure. So this is quite interesting because I think these concepts are not completely new things. I think other ecosystems have kind of toyed with similar concepts. I think Polkadot famously had this kind of parachain auction thing where you need to rent state as a parachain.
00:26:58.562 - 00:27:42.862, Speaker B: And then, you know, I think blockchains like Mina have designed to basically compress data to kind of preempt this problem of kind of state bloat. It's just interesting to see all of these concepts coming together into one chain. And I think this is like a great segue as well to talk about other efforts out in the market, because there are quite a lot of projects that are trying to go down this same route as well, but before we kind of talk about the current efforts, I like to talk about the previous effort. So you guys actually started on Cosmos SDK, and then there was some initial struggle to bootstrap usage because Cosmos is hard to use. Not many people use it. Can you talk about your experience with building with Cosmos SDK and then the subsequent decision to shift to the EVM?
00:27:42.958 - 00:28:19.984, Speaker A: So when you're building infrastructure, the overarching thing that really matters is getting developers to come on and start building on that infrastructure. In crypto right now, there's fundamentally just two types of developers. There are web two developers whose. I mean, I'm just broadly defining web two developers. It's people whose previous job was at a web two company. And then there's more crypto native developers that have spent some time actually building in crypto. Web two developers tend to not be the most effective founders because they don't really understand how things work in crypto, both from a product standpoint and also from a relationship and kind of crypto ethos standpoint.
00:28:19.984 - 00:28:56.400, Speaker A: So it ends up being pretty difficult to support them to be successful in a short term. It ends up being much more of a long term bet. And then there's crypto native developers who are able to hit the ground running, but they do tend to be much more focused on the EVM. Almost all of them are EVM developers outside of some that are building on Solana right now. Outside of EVM and Solana, I would largely say there's not that many, there's not that many other crypto native developers that we talk with at this point. So if you try to get these crypto native developers to come and start building in a new ecosystem, it ends up being a really, really tough sell. There's already like 100 different EVM chains that they can go to.
00:28:56.400 - 00:29:52.206, Speaker A: So why would they go to a new fledgling l one that has these rust based smart contracts that would require them to learn an entirely new execution environment? It's just not that strategic for them. So I think that was the biggest friction point that the state foundation saw while chatting with teams to start building onset. And that was largely one of the reasons that EVm seemed like a no brainer, because right now all the mind share is tied to the EVM, and there's significant scaling limitations from a throughput standpoint in the entire ethereum ecosystem right now, for both the l one and for any type of roll up on top or any kind of l two that is using Ethereum for data availability, you just cannot get better performance than right now. It's somewhere in the order of like 30 to 50. TPS is like the upper bound of what you can reliably get. So if you have any kind of use case where you need greater throughput than that, you're out of luck right now. And that's where SEI comes in.
00:29:52.206 - 00:30:23.114, Speaker A: The ideal type of developer, or the person who benefits most from a high performance chain like say, is someone that wants to build something that needs greater throughput. Maybe you're trying to build an order book based exchange. Maybe you're trying to build a game where you have more than just transfers that are happening on chain. There are several different types of use cases where you would want higher throughput, but you just haven't been able to build those on Ethereum or any EVM rollup so far, because you're just not able to write all that data on chain. So those are the types of developers that are getting more and more excited about building on save you too.
00:30:23.734 - 00:30:49.214, Speaker B: I guess for that transition to EVM to really matter, you have to make it basically extremely easy for existing solidity developers to port their existing contracts directly to, say, without refactoring the code base. So is that possible in, say, right now? Is it backwards compatible with everything on EVM, or do people have to do certain things in order to comply with the paralyzed execution?
00:30:49.374 - 00:31:13.894, Speaker A: Yeah, that's a phenomenal question. So for save you two, it will be fully backwards compatible. First of all, we're using get under the hood to be able to process any EVM transactions that are coming in. So get is the goal implementation of the EVM. It's currently responsible for processing around 85% of blocks on Ethereum mainnet. So it's extremely stable at this point. And the way that we're using it is because it's written in go.
00:31:13.894 - 00:32:08.350, Speaker A: We're just importing it in the core state binary. Whenever any kind of ethereum transaction comes in, each, say, full node will be having an EVM RPC interface. So because that RPC interface is the exact same as what you would see with get all existing tooling, such as like developer tooling, and tooling such as metamask just seamlessly works over here, and then you're able to just make use of EVM the exact same way that you would with like Ethereum L1. The only difference between the approach that say is taking and what is there on ethereum l one right now is the way that the actual state tree is represented. In sais case, we're using an ivl tree on ethereum l one, it's using a Merkel Patricia tree. The impact of that is quite honestly close to zero. If you're writing a smart contract that does need to do some kind of transactions, that need some logic where it's dependent on the Merkel Patricia tree, you need to refactor that specific code.
00:32:08.350 - 00:32:29.434, Speaker A: And if you're building any kind of like off chain tooling that is looking at this MPT state, then you would need to also refactor that. But I mean, there's an internal, internal Devnet right now that is running. Several people have deployed to that already, and no one has run into any issues with getting that deployed end to end. So like both the developer experience and also the user experience ends up being identical.
00:32:29.734 - 00:32:50.862, Speaker B: Got it. And I'd love to kind of focus on other efforts as well because say, maybe the first parallel EVM chain that's live today or the first live chain pushing for this, but it's not the first chain to explore the idea of parallel execution. In fact, Solana also has this. So for developers out there, first of all, how should they evaluate between building on Solana versus say so?
00:32:50.878 - 00:33:28.758, Speaker A: I would say there's fundamentally two different types of approaches to supporting parallelization. The first approach is an approach where developers need to define their own dependencies, and this can be done in different ways. In Solana's case, when you're submitting a transaction to Solana, you need to include all the states that you're touching as part of that transaction. So that's one approach. The other approach is an optimistic parallelization cell approach, which is what say, b two does. And in that approach, the chain itself is able to figure out all of the dependencies that each transaction has. From the developer experience standpoint, the optimistic parallelization ends up being substantially better.
00:33:28.758 - 00:33:58.532, Speaker A: It's just developers right now with optimistic parallelization can take an existing l one ethereum l one smart contract, deploy that directly to, say, with no code changes, they don't need to understand how parallelization works, and it just becomes faster. It's just able to process greater throughput. That is a massive unlock. With optimistic realization, you get backwards compatibility. Developers don't need to do anything else, they can just deploy works. It also results in a lot less work from developers. So that's why from the dev AC standpoint, it's a lot better.
00:33:58.532 - 00:34:27.808, Speaker A: The trade off with optimistic parallelization is kind of in the internal for how it works. The way that we're implementing it is initially every single transaction in a block will get run in parallel. And then any transactions that have conflicts, that is, they're touching the same state. Let's say I'm sending you a transaction. I'm sending Daryl a transaction. If I only have Tensei, I'm trying to send you Tensei and I'm trying to send Daryl Tensei, then one of those transactions should fail. So you still want these transactions that are touching the same state to be executed sequentially.
00:34:27.808 - 00:35:07.265, Speaker A: So for those kinds of transactions, they will get rerun. So in the absolute worst case, would, say v two, if you have 100 transactions, all of them, let's say I'm trying to send a bunch of people money, I only have ten say I'm trying to send 100 different people tensei each. 99 of them should be failing. The first one should be succeeding. In an example like that, there will be 30% overhead tied to this execution. When you're rerunning everything sequentially, but most of the time, you're not going to have every single transaction touching the same state. Like on ethereum l one, right now, only 15% of transactions are actually touching the same shared state.
00:35:07.265 - 00:35:13.574, Speaker A: So the impact of that is that typically you end up having substantially improved performance with this optimistic parallelization approach.
00:35:13.734 - 00:35:50.474, Speaker B: That makes sense. And I think going back to the beginning of a discussion, you mentioned that part of the vision for SE is to always have the execution of Solana, but the kind of developer suite and support of the EVM. So one project that is actually working on almost the exact same vision is obviously neon EVM. To my knowledge, they're working on an EVM for Solana that transpiles code down to Solana. So does this compete with say? Or how does it differ from say for say, like an application developer hoping to get exactly what you said, which is the execution speed of Solana with EVM. Should they be building a neon versus say?
00:35:50.894 - 00:36:22.804, Speaker A: Yeah, so, I mean, in general, the way that I think about the paralyzed EVM landscape is with the phrase like a rising tide lifts all boat. I tweeted about this, I think, a week ago, and I mean, the broader idea over there is. I don't really view any of these as, like, I don't think about this in a zero sum way. Like, the idea of a paralyzed EVM is still so new, all things considered. Candidly, most people don't even understand what parallelization is right now. I've had so many conversations in the past month about this. Most people are not familiar with any of these ideas.
00:36:22.804 - 00:37:00.294, Speaker A: So I think it's very premature to start thinking about this in a zero sum way, just given how early we are, the neon team is a very solid team. And I mean, the Solana team is supporting them as well. And Solana is obviously one of the biggest and most successful chains out there. So I think the more mind share that kind of goes to paralyzed EVM, the better it ends up being for everyone. So, I mean, in general, I think that it's just a good thing that neon is trying to solve a similar type of problem with regards to the approach that they're taking. It's definitely a different approach where they're basically building a smart contract on top of Solana. And I think as a result of that, they don't have full EVM bytecode compatibility.
00:37:00.294 - 00:37:19.510, Speaker A: It's not completely backwards compatible. So people aren't able to just deploy the smart contracts and have them work. There needs to be changes, need to add in the access list to be able to make things work as well. So just based off conversations that we've had, it ends up being a simpler experience to deploy on a chain, like, say, where everything is just built to support that from the ground up.
00:37:19.582 - 00:37:28.430, Speaker B: And then I guess another related question here is, can people fork, say, as a roll up? And why would you build on, say, first as an EVM roll up?
00:37:28.502 - 00:38:13.990, Speaker A: That's a great question. So, I mean, everything that we're doing right now is completely open sourced, so anyone can take a look at all the improvements that we've been making. And in theory, someone could try to take the code that we've written and build it as a roll up on Ethereum. The fundamental issue over there is that Ethereum is bandwidth constrained. So trying to build this as a roll up on Ethereum, where you have data availability happening on Ethereum, just fundamentally, it doesn't work. The math around this is that right now, before proto dank sharding goes in, the way that rollups work is there's off chain computation that happens, and then there's data that needs to be compressed, data that needs to be written as call data to the l one. Every single byte of call data that's written costs 16 gas.
00:38:13.990 - 00:38:49.900, Speaker A: And there's a target limit of 15 million gas per block on Ethereum L1. So if you do the math, it comes out to be roughly six k Tps that can actually be supported for simple ETH transfers. Between me sending Jason five ETH or something, I guess ETH is worth a lot now, so I probably couldn't afford to send that much, maybe a little bit less. But yeah, even for simple e transfers like that, it would be pretty constrained. And that's assuming all of the block space on Ethel one is used purely for roll ups. In practice, there's blur and uniswap and all this other activity that's happening as well. Even get close to that.
00:38:49.900 - 00:39:23.092, Speaker A: What rollups are actually observing right now is closer to around 30. TPS is like the upper bound, and part of that is just due to the bandwidth constraints of the base layer. But yeah, I mean, that's why I fundamentally don't think that just forking CSR rollup would really work. The other approaches the developers could try to take is like building this as a validium, where you have Ethereum just being used for settlement, and then a different chain, such as like Celestia, for example. Um, being used for data availability. Um, in that case it wouldn't be bandwidth constrained. But I don't really understand, like, validiums is a concept.
00:39:23.092 - 00:40:09.302, Speaker A: This is something I've been tweeting about recently, and I haven't really gotten any strong counter arguments here. Um, it just fundamentally doesn't really make as much sense to build a velodium, because with a velodium you're getting like, you have to rely on the security of the DA layer and the security of this element layer. So let's say you're using the DA layer, and the DA layer has less security than Ethereum, which is your settlement layer, and then the DA layer for some reason, let's say there's some kind of 51% attack, and then they're withholding your data or something. If your data is being withheld, then you're not actually able to claim your funds from the base layer. So then your funds are just stuck, and there's nothing you can really do about that. So you're making security trade offs with the validium, which doesn't really give you full ethereum security. If anything, you're getting the minimum of the security between the settlement layer and the dealer.
00:40:09.302 - 00:40:49.586, Speaker A: So as a result of that, I don't really think it makes as much sense. So building something as like a sovereign roll up or an l one makes a lot more sense. And in that case, I mean, it's kind of like, why not? Just, it kind of becomes a question, like, if you fork Ethereum, would you be able to become more successful than Ethereum? Like, probably not. Whoever came up with the ideas initially tend to be the ones that are able to build a community around it and kind of succeed with that first. So, I mean, candidly, I think it would be very cool if people started taking, say, um, forking it, building it as a sovereign roll up, and then trying to use it for their own special use cases, because that will result in more mindshare going to say, and also probably more developer activity happening on the core codebase as well.
00:40:49.690 - 00:41:18.210, Speaker B: Yeah, that makes a lot of sense as well. And I think before we wrap up as well, one project that is often mentioned in conjunction with, say, is obviously Monad. So I don't think Monad is live currently, but I believe they're working on something quite similar which is parallelized EVM with their own optimized state storage and state sync processes as well. So can you help us understand, are there main differences between the two? Are there kind of main, maybe philosophical or technical trade offs that differ the two camps?
00:41:18.322 - 00:41:55.954, Speaker A: Yeah. So my most candid answer over here is that there is honestly a very big philosophical difference in the way that our team and the Monad team executes. Our team is, I mean, essentially me, Jeff, were essentially born and bred, born and raised in Silicon Valley. The Silicon Valley ethos is one where it's build quickly, ship fast, iterate quickly, and then get user feedback, and then make improvements based off of that. The biggest and most scarce resource you have is time. And if you spend a lot of time working on something that is a premature optimization. That's something that just really goes against the Silicon Valley ethos.
00:41:55.954 - 00:42:51.778, Speaker A: That's the way that we've approached building, say, from the very beginning where we want to get something out fast, get user feedback, and then iterate based off of that. So, yeah, that's why one of the biggest implementation differences that I think is quite noticeable in terms of the user point of view around this would be like we're using get to support full EVM bytecode compatibility. The Monad team is writing their own custom EVM implementation, and I think just the timelines around that end up being pretty different because we have a different kind of philosophy to how we want to build in terms of which one ends up being better. I don't know. In general, I do think trying to ship fast and getting user feedback as quickly as you can does allow you to build a better product that is more tailor made for the user experience. I think generally that is going to be the approach that will lead to more user adoption faster. But I think the Monad team has been doing a ton of phenomenal work around this as well.
00:42:51.778 - 00:42:57.412, Speaker A: In general, the more mind share there is going to paralyze evms, the better it ends up being for everyone.
00:42:57.578 - 00:43:07.640, Speaker B: Yeah, that makes sense as well. And Jay, now looking forward to the new year in 2024. What are some things that you are most looking forward to for, say? What are some of the biggest things coming for say?
00:43:07.752 - 00:43:33.848, Speaker A: I would say the biggest two things to watch out for in the near term. The first is the public testnet that will be released sometime in Q one. I don't want to over promise and then under deliver. So it'll be happening sometime in Q one. I mean, just for context, we had like a save e two war room. Our entire team got together earlier this month and we were able to actually make a lot of progress over there. We had an internal devnet that got set up.
00:43:33.848 - 00:44:01.476, Speaker A: The entire team was able to test doing like just ETH transfer state transfers on this testnet and then we ended up having external developers also deploy smart contracts there. Yeah. So the public testnet will be getting deployed sometime q one and then afterwards it'll be a governance proposal that happens. And assuming the community agrees to it, there will be a mainnet upgrade happening ideally sometime in h one of 2024 as well. So, yeah, should be a ton of fun around that. And then we'll get to see the first paralyzed Evm out in the wild.
00:44:01.620 - 00:44:20.324, Speaker B: Yeah, I'm really excited for this because this is one of the infrastructure that allows for zero to one type of use cases and I'm really excited especially to see the types of new applications that people will build on, say. So yeah, excited to be on this journey with you and for people who want to follow you or say what are the best channels for them to do this?
00:44:20.704 - 00:44:42.920, Speaker A: Yeah, so I mean, to follow me, Twitter is the best spot. You could just follow me at jayendrajog and then for say, overall you could just follow seinetwork and Jason, thank you so much for having having me on, man. Love blockcrunch and yeah, should be fun to see this out in the wild.
00:44:43.112 - 00:44:46.034, Speaker B: Yeah, thanks, man. Thanks for coming on and thanks for tuning in. It.
