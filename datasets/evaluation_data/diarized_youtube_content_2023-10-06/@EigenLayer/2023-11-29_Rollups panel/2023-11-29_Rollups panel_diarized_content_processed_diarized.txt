00:00:00.330 - 00:00:06.462, Speaker A: Amazing. We're super excited for this rollup panel, so maybe we just do a quick round of intros. Do you want to start?
00:00:06.596 - 00:00:16.270, Speaker B: Yep. Hey, I'm Dino, co founder of Fluent. First time putting a mic like this on, and we're building a zkwasm roll up for Ethereum.
00:00:16.850 - 00:00:17.406, Speaker A: Cool.
00:00:17.508 - 00:00:33.800, Speaker C: I'm Tarab. Rushi was supposed to be here, our CTO, who is also our co founder, but I'm our director of BD Ecosystem and. And movement. We're essentially building move based blockchains and obviously work with Eigen Da, but maybe we'll get into that.
00:00:34.490 - 00:00:51.930, Speaker D: Cool. My name is Gloria. I lead developer relations at Versatus. The founder, Andrew Smith, and the rest of the team are headstone building right now. But we are a P to P web services provider, bringing the first stateless roll up to Ethereum and working collaboratively with Eigen layer.
00:00:52.350 - 00:01:29.690, Speaker A: Awesome. I'm Vios. I focus on product, specifically on Eigen Da, and I've been with the Eigen team since the beginning, so super excited, and we'll dive right into it. Right. I think all of these projects are building very interesting, innovative approaches to the roll up environment and taking different stabs at it. So maybe we just start off with how we all kind of interface together when it comes to data availability and what are some of the limitations of on chain data availability in the context of these new EVM execution environments?
00:01:30.910 - 00:02:12.470, Speaker B: Sure, yeah, I guess I could start. So, right now, data availability on Ethereum is obviously very secure, but super constrained. That's changing over time with a few different upgrades like proto dang Sharding and Dang sharding, that'll ultimately take a number of years to actually play out and get cheap enough, especially for some of the higher throughput use cases that we're seeing and especially ones that we're focused on. We're talking to a lot of devs who are trying to build kind of like on chain games, kind of like more like social, consumery applications, and they just simply need much higher throughput.
00:02:14.170 - 00:02:57.118, Speaker C: Yeah, a lot of it. I mean, gaming especially, that's a big one, like building a club that can handle a lot of transactions. We're talking to a market maker, and they're saying we want to do a few hundred thousand transactions a day. I was like, good luck doing that, the way things are right now. And so for us, being able to put the best pieces of crypto technology together, it's kind of like that meme on Family guy, where on Noah's ark. It's like an elephant's head and like a tiger's tail type of thing. But I know a lot of people, they prefer things to be simple, but sometimes it can be a little complicated.
00:02:57.118 - 00:03:08.490, Speaker C: But if it works, it works, right? One of the examples I saw on Twitter is like an airplane. Airplane is not a simple thing, right? There's a lot of things that go into it, but it does the job well. So that might be where we're headed.
00:03:09.070 - 00:04:09.850, Speaker D: Yeah, I definitely agree. I think data availability wise it's super important whether you are an OG or a new EVM or even non EVM execution layer, because you still face, whether you are roll up or a base chain, the same limitations when it comes to data availability. So I think it's absolutely crucial if you want to scale your overall chain, you need to evaluate, hey, long term, obviously you want to be able to sustain and manage all the transactions, right? And at Versatus, we're hyper focused on developer and user experience, right? Because at the end of the day, your data availability capacity will translate to what developers are able to build, how successful their applications are, and how usable their applications will be for the end user. So I think definitely working together with a data availability solution like Eigen layer is something very crucial, especially if you are a dense chain like Ethereum.
00:04:10.670 - 00:04:12.138, Speaker A: Yeah. Do you want to go?
00:04:12.224 - 00:04:13.334, Speaker C: No, just fix it.
00:04:13.392 - 00:04:13.902, Speaker A: Sure.
00:04:14.036 - 00:04:18.030, Speaker C: I feel like someone is breathing into the mic and I'm scared. It's me.
00:04:18.100 - 00:04:24.334, Speaker A: Okay. We think it's you. Okay. We all played around with.
00:04:24.372 - 00:04:26.314, Speaker C: I held my breath for like 30 seconds.
00:04:26.442 - 00:04:27.846, Speaker A: We'll find out in a minute.
00:04:27.898 - 00:04:28.962, Speaker B: That's what I was doing.
00:04:29.016 - 00:06:21.320, Speaker A: Guys, we are dealing with technical difficulties, but you all touched on very interesting things. And I think you briefly mentioned there's a market maker that wants to push really high throughput for data availability, right? I don't want to show my own talk or one of Sriram's talks, but yesterday at Data Palooza, we have this graph that has throughput on the x axis and then value per bit on the y axis, right? And so rather than looking at necessarily the value per transaction, you look at the value per bit, which is a much more measurable statistic when it comes to data and something like DeFi or like order book Dexes or even to some extent the NFT market, right? There's a lot of value per bit in these transactions, but they don't necessarily need high throughput. And then as you start scaling the order book Dexes or like high frequency trading, you start increasing demand, but value technically goes down, right? And so one thing that we talk about at Eigenda is like how can shared security kind of enable roll ups to do? We talk about in protocol and out of protocol DA, right? So in protocol DA is just writing directly to the l one. But out of protocol DA is like all the other data availability solutions, right? And that's where with Eigen DA, our positioning is shared security now enables you to do out of protocol DA, but with as much economic security as close to in protocol DA. So going to my next question, how can we ensure data availability without compromising some of the scalability that these new execution environments enable? And based on this, what do you guys think are some of the strategies that these projects are going to go and take this with?
00:06:24.010 - 00:07:25.130, Speaker B: Yeah, so clearly if you're doing some kind of like validium, which is just for clarity, where you're putting the proofs and kind of like state routes and stuff, getting the security from a certain chain and then putting the transaction data, input data more broadly on a different chain. So if you're doing a validium, you have a bunch of different options. Obviously where you put the data could be anything from literally like a laptop to a committee to something like an Eigen DA, which ends up being still, I think you said out of protocol DA, but all those options sit on a spectrum of just like less secure to more secure. And obviously if it's cheap enough and you can still meet the application's kind of throughput requirements, but also enable kind of like the most secure of those out of protocol DA solutions, like an eigenda, like you're going to opt for it. So I guess just that's a trade off space of its own. But something like I canda is clearly very compelling there.
00:07:25.200 - 00:07:35.722, Speaker A: Yeah, almost like the security versus decentralization trade off to some extent you're touching on, which is like, yeah, you can store it in your database on AWS.
00:07:35.786 - 00:07:36.154, Speaker C: Totally.
00:07:36.202 - 00:07:37.280, Speaker A: But is that.
00:07:37.730 - 00:07:39.790, Speaker B: Yeah, yeah, exactly.
00:07:39.940 - 00:08:25.086, Speaker C: Yeah. For us it remains to be seen. Again, I don't have that technical background, but right now we're talking to a lot of different teams and I think everyone is, there's been all these handicaps of like, I know I can't build this because of the technology's not there yet. So now that DA is available, I think there's a lot of builders like, okay, so what does this actually mean in terms of building a product and having product market fit? That it's only possible because of DA, this new technology push. And I also think it's going to be interesting because there's other DA players, but they're obviously not as good as Eigen layer of. But you know, it's going to be interesting to see what the trade offs are and how people choose where they're going to go and what those trade offs are going to be as well.
00:08:25.268 - 00:09:57.770, Speaker D: Yeah, definitely. I think just on the topic of understanding what your product fit is, there are, like you had mentioned before, different ways to scale data availability. Dank sharding. There is error, sure, coding forward, error correcting, all these other super cool jazzy terms that are supposed to help data availability. But I think what is important, especially when there's other roll up situation or, sorry, projects like Celestia that also have their own approach to DA, I think it's important to consider, okay, what is your ultimate value prop of your particular project and in terms of integration and compatibility with this variety of data solution, what is going to get you closer to your end goal while also obviously having a very conducive relationship with your data availability solution of choice? I think I was listening to a podcast on Eigen layer, and I liked how your founder was saying how Eigen layer is more of like an optionality aspect, right? It's not a necessity, but if it really aligns to what you're doing and if you can understand and follow the flow and be like, hey, this is going to, again at Versatus, this is going to impact our end users, our end developers really provide them a very feasible, secure experience without compromising decentralization. Right. And really making sure that resonates with your end user or your targeted audience is definitely something that we're considering.
00:09:59.470 - 00:10:53.246, Speaker A: I think that's awesome. You briefly touched on other data availability solutions, and there's definitely a lot of other players out there, Celestia being one of them, available. And there's definitely going to be with Versatus, for example, which is a stateless roll up. New challenges presented by how do we handle either the amount of data that we're throwing at the system, or the frequency or latency, or many very different attributes based on the types of roll ups that we're interacting with, right. And that's within the existing EVM environment. Now that we open outside of new execution environments, we're opening ourselves to even more interesting challenges. So I guess there's on chain data availability, which is primarily used for proofs, but there's also a lot of off chain data.
00:10:53.246 - 00:11:22.870, Speaker A: Right. So I guess what are some of the challenges that you think these new execution environments might deal with with off chain data sources and what some of your projects are doing in terms of whether data integrity is a factor. Right, and oracles come into play or. Yeah, what's your strategies on that? And maybe giving a little context to what the goals of fluent movement versatus are and how you guys plan on executing might help answer this.
00:11:23.020 - 00:12:27.254, Speaker D: Yeah, I can definitely start with this one. So versatus, like you had mentioned, is a stateless rollup. And in the context of new EVM execution environments, there are different types of roll ups, right? There are ZK rollups, optimism, things like that. And I think the differentiating factors that include, okay, how is state maintained? For example, for us, when we mean stateless, that translates to a pure execution environment. We don't maintain any of the off chain computational state data ourselves. We leverage data availability solutions, whereas other roll ups have a different approach because our end goal and our end visions are different. So I think for us, we are in deep talks with you guys, figuring out specific documentation to make sure that, again, we are integrating and our architecture and our technology are going to be compatible and work together to, at the end of the day, deliver a very seamless experience for both of our ecosystems and beyond.
00:12:27.254 - 00:12:30.618, Speaker D: So that's kind of what Versatus is building currently.
00:12:30.784 - 00:12:51.940, Speaker A: It's interesting you say that, because I listened to mark Tinoway's talk the other day at the seven x event, and he was talking about, let's make sure we're designing the system first and then the proofs around the system, rather than designing the system around the. You know, I'm glad that Versatus has a proactive approach on.
00:12:56.150 - 00:13:03.942, Speaker B: We're sorry, the question was what challenges we're facing as execution layers, is that right?
00:13:03.996 - 00:13:04.214, Speaker A: Yeah.
00:13:04.252 - 00:13:04.840, Speaker B: Okay.
00:13:05.210 - 00:13:10.402, Speaker A: Specific to also maybe off chain data as well, if you are dealing with off chain data.
00:13:10.556 - 00:13:50.660, Speaker B: Yeah. So we are actually going to. I'll show you guys a little bit. We're going to start off as a validium ourselves and we're going to use eigenda. And I think let's see more broadly the way we'd think about challenges that would maybe lead us to that kind of decision is what happens if either like a DA layer goes down. Either they're malicious or incompetent, or just shit happens, stuff happens. And that's why in our case, we want to make sure that transaction fees are low enough for some of these use cases that I mentioned before, but also there's a meaningfully high amount of security.
00:13:50.660 - 00:14:06.650, Speaker B: And so, I mean, that was a good fit for us. But yeah, I guess maybe, to answer your question, what got us there was thinking, like, okay, well, where do we need to sit on that trade off? Obviously, in our case, just totally centralized data availability was just not an option, nor was Ethereum.
00:14:09.070 - 00:14:16.286, Speaker C: Yeah, from our side. That's a question for Rushi, so I'll let him answer it whenever you run into him, for sure.
00:14:16.388 - 00:15:01.690, Speaker A: Yeah. For context, most of the projects that we have are Eigen da launch partners, and we have case studies coming out on each of these versatas coming up soon. So we'll definitely give a lot more context to the integrations as we put these out. Yeah, maybe we can touch a little bit on some of the new use cases and some of the more interesting projects that you guys plan on introducing to the ecosystem as a result of some of the higher throughput that eigenda enables. I know you briefly mentioned on market makers that want to do crazy amount of TPS per day on chain gaming, social fi. All of these are, like, pretty big movements right now, without speaking of movement.
00:15:03.710 - 00:15:47.098, Speaker C: Okay, so one of the things that we've seen is that there's certain technologies that aren't really available on the EVM chains right now, and so only place that's available. So, for example, like a serum type decks, like a clob, it's not possible, except for on Appdos sui or like Solana. But the problem is that a lot of liquidity isn't there, so it's useless. The technology is there, but it's useless for the people that actually want to use it. And so one of the problems that we're solving is we're allowing people to have the best of the move, VM, but then also be able to have a settlement layer on Ethereum. So it's being able to bridge those gaps together. So those are some of the teams that they didn't think it was possible.
00:15:47.098 - 00:16:10.260, Speaker C: So we're actually seeing them. So we have commitments from really big EVM based projects, and we're going to see what's going to be possible. And then obviously, there's going to be appdos and sweet teams. They're going to be launching their code with us, and then it's going to see if they can get enough liquidity where the market makers give a damn about them, because right now they're generally siloed from everyone else.
00:16:11.670 - 00:16:12.034, Speaker A: Yeah.
00:16:12.072 - 00:16:50.234, Speaker B: So I kind of think about it in a similar way as, like, Internet bandwidth. Once we're over some of these humps of fragmentation and just tooling and ux devx and stuff like that, I think people just keep pushing and pushing and pushing the boundaries of just like what can go on chain. And I definitely think that for us, how that maps is to kind of these on chain games and autonomous world type use cases that I was mentioning before. But I do think that people just start putting more and more of this stuff on chain. Basically, if it's cheap enough, they'll dump it there. And you say, why? Right. I feel like so far we've really only seen kind of like true financial use cases.
00:16:50.234 - 00:17:22.358, Speaker B: So it's like, why would you do that? I mean, this goes back to, I know Shiram talks about this a decent amount, but just like some of those just truly building permissionless platforms, no matter what those platforms might be, whether it's a game or whatever, it almost doesn't even have to be financial in nature for it to make sense to be put on a blockchain and make permissionless for people to build on in a way that was never possible before. So I think, forget the initial question, but I think things just keep getting dumped on chain and so you're going to keep needing to push the throughput.
00:17:22.374 - 00:17:26.730, Speaker A: Limits totally more so just like some of the new use cases.
00:17:27.230 - 00:17:29.900, Speaker B: Autonomous worlds, 100%.
00:17:31.490 - 00:17:36.122, Speaker A: Do you guys have any projects that you're excited about already building autonomous worlds?
00:17:36.266 - 00:17:41.434, Speaker B: We do. We will announce a number of those very soon with our private testnet.
00:17:41.482 - 00:18:05.746, Speaker A: Amazing. So next question is like, this might be a question around latency, but how do smart contracts in some of these new environments handle if the data is not available or wasn't written on chain? Right. What is the pessimistic case and how do you guys go about, I know, maybe fluent. You want to take this first as a validity?
00:18:05.858 - 00:18:41.726, Speaker B: Yeah, it's interesting. I definitely think the design space for this is pretty big. I think that a lot of things will end up being explored that we haven't really heard about yet. You could really just use a couple of different data availability solutions if you want to, and have the smart contracts handle it that way and be like, okay, if this one goes down or there's some issue with this one, maybe it just takes too much time. I'm going to look somewhere else to get the input data. And the design space is actually quite big and underexplored. I think right now it doesn't even have to be like somebody other than eigenda.
00:18:41.726 - 00:18:50.600, Speaker B: It could just be like, you guys could help provide that redundancy in different ways. So I think it'll end up getting very interesting. Yeah. So there's a lot you could do.
00:18:51.530 - 00:19:03.130, Speaker C: Yeah, we haven't run into the issue obviously yet because our testnet is going to be coming in Q one, but I'm sure we're going to be running into this and so I'll be giving you that feedback you were talking about the last session for sure.
00:19:03.280 - 00:19:04.166, Speaker A: No doubt.
00:19:04.278 - 00:19:07.926, Speaker D: I keep forgetting the question too. It was about smart contracts.
00:19:08.038 - 00:19:18.506, Speaker A: Yeah, more. So how does this new execution environment react to the data not being made available? Perhaps.
00:19:18.698 - 00:20:38.054, Speaker D: Okay, well I guess I'm just going to use it in like a smart contract example and off chain data availability and things like that. But I know. Is this on? I can't tell. Okay, I know I'm just naturally very loud, but I know that oracles, they serve as a bridge for information as well. But there's obviously issues, points of failure when you use Oracle. So definitely having resorting to more of a modular I guess, aspect when it comes to either data transmission, data availability, I think those are definitely understanding. Okay, how decentralized are these projects and protocols that revolve around data either retaining or receiving or communicating information? And at Versatus we're actually creating something called an executable oracle, which is essentially a smart contract that is deployed on, for example, the Ethereum ecosystem that allows more seamless communication without having to use these centralized protocols that are some oracles that are out there today, to be honest.
00:20:38.054 - 00:20:41.514, Speaker D: But I'm not going to try to get more technical into that.
00:20:41.712 - 00:21:12.740, Speaker A: Sounds good. I just want to touch briefly on a little bit about ZK systems here, and I know fluent is definitely working on some stuff there. How do different vms fit into the current proving scheme and what are going to be some of the end user experiences and perhaps challenges or pain points that might come with the integration of this? And will this have an impact on time or transaction costs or things like that?
00:21:13.110 - 00:21:51.662, Speaker B: So you're saying based on the proving schemes that are ready and accessible and mature enough today? Okay, exactly. Yeah, I mean there's definitely limitations. Keep in mind that incredible work is being done here. In the grand scheme of things, it's actually insane, like how fast things are moving. But clearly this is very complicated stuff and they're still very expensive to generate proofs and especially for some of these higher throughput use cases. So at least in our case, the way we're handling that is kind of like temporarily working around it as we're optimizing things. So in addition to we're going to go to market basically as kind of like a hybrid optimistic ZK thing.
00:21:51.662 - 00:22:14.198, Speaker B: In reality it's an optimistic roll up. We just kind of like ZK proof, the fraud proof. And that just narrows down the challenge window a lot. So basically just like workarounds today. But I think it's okay in a lot of scenarios for some of these newer use cases where it's less, what do you call it? Like lower through value per bit or whatever.
00:22:14.284 - 00:22:23.578, Speaker A: Yeah, I guess adding to that question, are there challenges that you have been facing in terms of integrating into some of these existing proving schemes and do.
00:22:23.584 - 00:22:54.242, Speaker B: You want integrating into them? I mean, it's hard. With a new VM, it's hard. Definitely you're doing the circuits for the VM, right? So it's definitely challenging. But yeah, I'm sure our researchers at CDO would end up having better, more insightful comments on the specific challenges. But it's definitely hard. I mean, if you're doing some new VM, it's going to be tough and that's big part of what we're working on.
00:22:54.376 - 00:24:03.398, Speaker D: Yeah, I just want to say, I remember when I first heard of ZK technology. Naturally I was very curious about it. So I'm like, oh, how does this work? And when I looked into it, I was like, I admire anyone who takes a stab at trying to toggle with this incredibly complex mean, you know, Cairo, right? Recursive proofs, et cetera, things like that. It went way over my head. So I think it's amazing to see something as complex and even difficult to comprehend show a lot of progress, especially when what you guys are doing, like merging both ZK and the underlying architecture behind optimism or optimistic roll ups, and really narrowing that down that seven day window is amazing, even incremental progress. I don't think it should be overshadowed because maybe people just don't understand exactly how complex zero knowledge technology is. And I think it just reminds me of just the points made earlier where different, especially new different execution environments that are EVM.
00:24:03.398 - 00:24:18.000, Speaker D: It's really important to consider. Okay, how is this going to integrate? Well, going to be compatible with our architecture and that's going to be super different than at movement and at fluent. Wanted to say I admire you.
00:24:18.450 - 00:24:21.710, Speaker B: I mean, you guys are doing great work too, obviously. Thank you, though.
00:24:21.860 - 00:24:23.982, Speaker A: Amazing. Do you want to take that?
00:24:24.036 - 00:24:24.302, Speaker B: Yeah.
00:24:24.356 - 00:24:43.858, Speaker C: So today I was at the Avalanche summit and there's a presentation where they're talking about ZK proofs. I walked in and I walked right back out because I didn't understand what was going on. So again, these are problems that when we're having our tech standups, we haven't really run into these but it'll be interesting as time progresses.
00:24:43.954 - 00:25:02.640, Speaker A: Yeah. And especially on the ZK side, that was a targeted question towards fluent as they're the project that's actively working in that space right now. That's actually the end of our ROA panel. Thank you guys so much for your time. And we've got five minutes before our next session, so give us a couple of minutes here to transition over.
00:25:03.010 - 00:25:03.500, Speaker D: Thank you, guys.
