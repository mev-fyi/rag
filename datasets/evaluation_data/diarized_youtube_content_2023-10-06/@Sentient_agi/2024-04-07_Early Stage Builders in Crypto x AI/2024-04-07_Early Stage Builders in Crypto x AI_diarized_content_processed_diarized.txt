00:00:04.840 - 00:00:48.324, Speaker A: Okay, everybody, we are going to close things out with our last panel, early stage builders in crypto and AI. This panel brings together Elizabeth Downstee. She's the CEO and co founder of Compass Labs. We have Mike Hanono, who's the founder of Talus. We have Will Healy, who's co founder of Hum Shashanky Dob, who's the co founder at Fraction AI, and Matt Wang, who's the founder of Vanna. This panel is moderated by Jake Vartanian of the Mentac group, which is an early stage crypto fund. Let's welcome these panelists to the stage.
00:01:11.904 - 00:01:48.024, Speaker B: All right, well, thank you, everyone, for sticking around for this panel. And I'm really excited to be up here with this great group of builders who are actively building at the intersection of crypto and are web3 and AI. And we've all already been introduced, so I guess we'll just get started. And I guess the first question that I would ask, I'll leave this one open. What are some of the new possibilities that we can now build because of the intersection of web3 and AI?
00:01:49.344 - 00:02:54.522, Speaker C: Yeah, so I guess I can start off. So the way we're thinking about the intersection of web3 and AI really is, I would say, like two pronged. And we kind of divide this intersection into two distinct subspaces. So the first one being integration of web3 and AI, second one being integration of AI and web3. So first and foremost, integration of web3 and AI, to us it means kind of building web3 esque infrastructure for, you know, supporting AI workloads such as inference or training. And the purpose for that is to build infrastructure that inherits the properties of a peer to peer network, including censorship, resistance, decentralization, permissionlessness, to build infrastructure that is inherently credibly neutral and is able to deliver unbiased results as well. And that in a world where AI slowly eating up, the world could be important, where centralized entities or private entities are trying to gain more market share to develop more closed source models, having this kind of open source, credibly neutral platform where people can build and rely on unbiased sources for AI is important.
00:02:54.522 - 00:03:22.464, Speaker C: And secondly, I think is a bit more interesting, which is integration of AI in a web3. So that's taking AI and ML and using these models in ways to empower both existing and new use cases within web3. Right. So that could be on chain agents that could transact on chain or, you know, on chain algo trading or risk models within lending protocols on chain. And that's like an entirely new problem space in and of itself, and we're super excited to explore it.
00:03:24.804 - 00:03:41.554, Speaker D: Just to add to that, I like to keep web3 and AI as bit separate. So web3 is really great to create financial models which incentivize creation of value on AI. But AI itself can be trained separately, just powered by web3.
00:03:44.574 - 00:04:14.454, Speaker E: Just to tack onto that, I think that the intersection of IP ownership is something which changes a lot. If you look at the landscape of large language models specifically right now, you have OpenAI getting sued by the New York Times. And going forward, the genie might be out of the bottle in terms of public data. And whether IP law can catch up with that is a different question. I think going forward, if we have private data training models, then ownership becomes much more important in a trustless way.
00:04:16.994 - 00:04:55.924, Speaker F: I think Matt hit it very well in detail. Like the both categories, I'm going to focus one on bringing AI to web3, where we could have a world of, let's say, smart agents that understand the blockchain world much better than we do, and they could abstract all of the complexities in terms of like bridging or trading or flash loans or whatever it may be, for any newcomer to easily just type a question, type an intent, and then have a network of intelligence solvers in the backend to optimize for users in terms of liquidity, outcome, whatever it may be. So yeah.
00:04:59.444 - 00:05:44.894, Speaker G: Yeah, I guess I completely agree with you guys. I see a lot of potential in the application of AI on chain, in the sense that for the first time we're working with a completely programmable industry that is yet to be completely optimized. And I think using AI, or for example, agent based learning to do this is something that isn't as well possible in other industries that aren't completely programmable as what we can see in blockchain based systems. And I think there's a huge potential there to really optimize the system such that we are all able to use them in a super efficient and effective way.
00:05:46.994 - 00:06:27.864, Speaker B: And so on the note of kind of optimizing for a new industry. Mike, I know you just announced Talus today and are focused on building or using the MovevM as the base layer for processing these transactions. And so what makes these new MooV VM better than traditional vms that we've seen using blockchain before? And what other types of changes to the infrastructure will be needed to make these AI applications run as effectively as possible.
00:06:29.004 - 00:07:07.360, Speaker F: So one of our main focuses is to democratize AI through move, and we chose move because of three main reasons. First is security. Move enables more secure program designs. You have much less smart contract vulnerabilities in the VM by itself. The second is parallelization, where you could have high throughput and many smart agents or many applications engaging at the same time. And third is the developer experience and the program design. Move comes with an object centric model that not any other vm has at this point.
00:07:07.360 - 00:07:37.724, Speaker F: And this enables and creates a framework for ownership, management of resources, whether it's like data, compute models, and even add a layer of monetization on top of it that easily enables developers to, in a very intuitive interface or program API, call it to manage all these models and all these, and build applications on top of them. So that's why we think move is going to be very powerful in the future and the framework on how we build applications on top of.
00:07:39.784 - 00:07:40.524, Speaker D: That.
00:07:41.844 - 00:08:11.654, Speaker B: And so I guess I would go to Elizabeth here and with some of these changes that we're seeing, or I guess just the pace of innovation and changes that we're seeing in building these applications, what are some of the ways that, what are some of the dev tools that you're building to make the experience of understanding the data and using the data easier for a wider range of people?
00:08:13.274 - 00:09:31.264, Speaker G: Yeah, thank you. So what we're building is a Python interface for DeFi, right? So we enable our users to do data sourcing, simulation optimization, and eventually execution through a Python interface. So through this type of infrastructure, I guess, like it comes down to the point for it right now, focusing on DeFi, but as a whole, like it was, you know, made to be like this open source finance for everyone. But because of the complexity, it kept very limited to a super tech savvy crowd. What we are doing is building the infrastructure to enable people to interact with DeFi through this Python interface. And I guess as a part of this, we have this agent based learning software that runs on blockchain forks to really enable our users to model the entire DeFi ecosystem through adaptive participants, and for the first time be able to do risk management, do simulation, do optimization through Python without the need of any type of blockchain engineering expertise or solidity for that matter, and be able to operate on chain and have the tooling available that is required to do that in a secure and reliable way.
00:09:33.144 - 00:10:35.294, Speaker C: Yeah, I think it's definitely a good mission to be focusing on how do we make the developer experience who wants to develop in this intersection just as good as possible, because a lot of people who develop in crypto aren't necessarily AI experts or ML experts, and this is a mission we share as well. How do we make developing AI super easy? Where inferencing could be a simple function call or training, you have a good framework or dev tooling for it, in our view. I think we want to make this as seamless as possible. ZK, as an example, comes with such a high level of developer complexity. Not a lot of people in this world understand Zk in and out. To utilize these frameworks at a high level is still very difficult, and that's one of the missions we have, which is abstracting a lot of these developer complexities behind the scenes. So it's a simple interface to use it, and you can trust that the architecture and the infrastructure will trustlessly do its job and be able to accelerate developer velocity within this intersection.
00:10:36.234 - 00:11:21.744, Speaker G: Yeah, yeah. Just to add on that, I think that becomes especially relevant in the case that, say you're indeed, you know, what we discussed earlier, like, you're a smart contract developer, you have parameters that you want to optimize in your smart contract. You want to be able to understand how you should optimize them in a perfect way. But then, indeed, as you say, you want to ensure that you're, for example, the users of your protocol are also understanding that what you're doing is correct and the way you've set you've done it. And I think that entire infrastructure that is required to go from step one of modeling to step two executing is something yet to be built and what all of us are working on right now.
00:11:22.894 - 00:11:42.874, Speaker D: Yeah, and just to point out historical examples for these cases, had the libraries like Tensorflow or Pytorch been only in C, we wouldn't have seen as much adoption of AI as we have seen so far. So having those in Python is what exactly accelerated the AI revolution. And we need that thing for the decentralized AI as well.
00:11:50.394 - 00:12:15.144, Speaker B: So I guess when we think about using or building AI models, the foundational thing that we need to run the models is data. And so what are some of the challenges of getting the best possible data, and how do we make sure that it's all formatted in the right way to run the best possible models and know that we can actually trust the data as well?
00:12:15.964 - 00:12:16.620, Speaker F: Yeah, sure.
00:12:16.692 - 00:13:09.984, Speaker D: So generative AI has been talk of town for like a few years now, and all generative AI models need a lot of data for training, but data labeling companies still work in an efficient manner, in an inefficient manner. So the way it works is an AI company will go to a labeling company, ask for a specific data set, wait for a few months, and then get the data set that they wanted. And the costs are really high because the data set are being created specifically for a single company. And that's like a boutique approach, but using web3, we can create financial structures which allow us to create those datasets beforehand. So we call those perpetual data sets perpetual because they are always being created, and they are created proactively. So whenever a company or anyone who wants to train a model needs data, they can just directly tap into the data source, get it instantly and at lower costs, which is a win win for everyone.
00:13:15.374 - 00:13:42.154, Speaker B: So I guess once we get to the user level, what could it look like for just an average user to, how could they interact with these datasets in terms of providing new data, cleaning old data, and what are the types of new ways that people will be able to, to earn money or new types of jobs that will be created because of the rise of this technology?
00:13:42.574 - 00:14:24.118, Speaker E: Yeah, absolutely. So, as I mentioned before, IP law is unable to really catch up with these generative language models. If you look at OpenAI right now, they're afraid to use sora on real people because of IP concerns. So people are really getting a lot more concerned to let their data leak out. So one of the things that we're trying to do at humidity is to flip the script and actually incentivize people to upload their own data in a manner where they know they control it. If they want to turn off their chatbot or some equivalent model that was run on them, they should have the right to do so. Additionally, the more that people interact with this model, there should be some trustless mechanism to actually pay the IP owner and the model developer who developed the model.
00:14:24.118 - 00:14:26.554, Speaker E: So I think that kind of answers the question.
00:14:27.054 - 00:15:14.004, Speaker F: I want to add to that. There's an interesting problem where you could have, let's say an artist who's the owner of the data, who has the IP, and at the same time, that person doesn't even know how to build a model to monetize it. And then on the other side, you have the scientist, who's the guy who's like an extreme mathematician and know how to build these models and can build all these applications. I think it's very interesting, and for sure, we're going to see it soon. And this is something that Talos would hopefully enable, is a way of like putting the data from an artist with the math of the scientist to create a model or an output that is monetizable by whoever user, whoever application wants to use it. And enabling those two to come together, I think will enable like new use cases and new applications to come on board.
00:15:17.464 - 00:16:20.596, Speaker C: Kind of touching on the topic of data. I think it's also important to recognize that in addition to, you know, training data and high quality training data that users could submit as part of the training process, I think something that we are really starting to realize is also quite important is the provenance of artifacts that result from execution of inference. Right? So for example, like, how can I provably, like, know that some model produced some output? There are, you know, in the ML workflow process, you know, a lot of artifacts are often generated that often indicate, hey, this is kind of like the in sample, out of sample, kind of like test results of this model. This is, for instance, maybe like the Z caml or OPML fraud proof or snark that was generated from this inference. These were the inputs. This is the model. And I think as we are developing in a world where AI is gradually getting more and more important, it's important to kind of understand these kind of mediums of storage of data in a way that preserves this data provenance property.
00:16:20.596 - 00:16:39.544, Speaker C: And that's something we're working on as well, using kind of decentralized storage layers to store these artifacts, to make it so that the inference, as well as the execution of the inference for these models are truly made decentralized and available to anyone that wants to verify the results.
00:16:42.164 - 00:17:02.854, Speaker B: And so if we focus in on something like, just like the DeFi space right now, how will these models improve the applications that currently exist? How much more efficient can we make things? Can we lower fees? Can we make trading, trading, how much more efficient?
00:17:05.434 - 00:19:02.292, Speaker G: Yeah, I think it's an interesting question, because where I think, for example, agent based learning comes in extremely powerful, especially in DeFi. If you, for example, say you're a smart contract developer and you're building an amm, or you're building a lending borrowing protocol, and you want to understand risk management, right? One of the things that you need to do is understand how different parties can interact with your smart contract. So you need to, for example, be able to run thousands of simulations against your smart contract to understand how it behaves under different market scenarios, such that depending on the economic circumstances, your smart contracts can, can respond to that, basically. So one of the things that we see is very powerful in the infrastructure that we provide is, rather than what we see right now, is that a lot of lending, borrowing protocols and other exchanges are using consultancy firms to do optimization for their protocol. What we think is there should be a shift in this paradigm where protocol researchers, smart contract developers, have the tooling to do that kind of optimization in house and really optimize and understand the dynamics of their smart contracts to make them as efficient and as powerful as possible. Such that, for example, say what you're saying, a fee structure should jet change, for example, with the variance of the price, should that change with the amount of liquidity that is in the protocol? Those kind of things are things that should be modeled beforehand. And for example, weights could be adjusted to make those more, to make those systems more efficient and to make them, for example, more easily usable for a liquidity provider.
00:19:02.292 - 00:19:04.088, Speaker G: In DeFi, for example.
00:19:04.276 - 00:20:17.856, Speaker C: Yeah, no, I definitely agree that a lot of optimization, there definitely is a lot of room for optimization, especially if you draw comparisons between existing DeFi and tradfi. Right? In tradfi, you have all these quant funds building sophisticated models for risk management, for calculating how much spread to quote on the market. Whereas in defi you kind of lack a lot of this sophisticated compute and intelligence. And in that way, I definitely agree with you that there are just, there is just so much room, room for optimization on this front. But kind of going on to the second layer, which is how do we create a system that allows defi to take advantage of sophisticated modeling as an example, which is one problem that we've thought a lot about at Vanna. For these high leverage, high impact use cases, there comes a strong demand for verifiability. For instance, if I am going to rely on the model output to determine how much I'm going to charge, how much I'm going to trade, or at what price I'm going to trade, or how I'm going to manage my risk for a lending protocol, then verifiability becomes very important, because if the result is bogus, then I would open myself up to adversarial attacks on my protocol where people can just give me B's and I wouldn't be able to differentiate that from a real output of a model.
00:20:17.856 - 00:20:33.114, Speaker C: So focusing on ZKML and OPML and verifiable inference for these high leverage and high impact use cases is something we're definitely thinking a lot about to really take DeFi to the next step and compete on the same level that tradfi plays at.
00:20:33.614 - 00:21:15.804, Speaker G: Yeah, and I think like one of the, one of the interesting to make the comparison to Tradfi, like for the first time, users in DeFi don't have to make any guesses of, for example, underlying exchange mechanics, right? It's open source, it's written in code. And so indeed, what you're saying to be able to optimize that open source code and what you're working on, for example, on the inference side, is something that I think protocol researchers and smart contract developers should be able to do in house. And the infrastructure that is necessary is currently being built, which I think is going to make a big shift in the onboarding and the usage of, of decentralized systems.
00:21:17.064 - 00:21:39.004, Speaker D: And yeah, just to add to that, verifiability is really important if you want to increase the adoption of third party models or agents, because if you don't really know how a model was trained or what is outputs are expected to be, there is no explainability, and you can't really use those kind of models, especially for financial decisions. So yeah, verifiability is a must. It's not a nice to have thing.
00:21:41.284 - 00:22:24.514, Speaker F: I agree that in some use cases verifiability is a must, but I think verifiability is also a spectrum. Like it's fully verifiable, then that could be CKML, and then you could have, let's say OPML in the middle, and then on the other side you have no verifiability. Where do you think is like the cut off in terms of what application? In terms of like cost verification, or let's say time or compute, or how much it takes? How do you think we could create a framework to understand? Okay, so what applications really need to be verified versus what applications it doesn't really need to have a verification because the model output works to some extent.
00:22:25.734 - 00:23:17.494, Speaker C: I think that's actually a great point, and I do very much agree that verifiability is a spectrum. And depending on the use case, you don't necessarily need the highest level verification. Right? Because with ZKML as an example, the proverbial overhead is massive, and you can't really prove inference for huge llms because the computational cost simply just is too high. And I think that's a great point that you brought up. I think ultimately, the way we're thinking about it at Vana is designing multiple interfaces, or ultimately the developers of the application to choose themselves. If I'm building a high leverage use case, Defi as an example, I want immediate and high verifiability through ZKML. If I'm doing something such as, hey, I'm generating the output of, I'm running a stable diffusion model to generate an image for an NFT verifiability might not be that important, and it simply probably not worth the computational cost.
00:23:17.494 - 00:23:26.664, Speaker C: We want to have these flexible interfaces that adapt to people's use cases and developers use cases that really the trade off makes sense.
00:23:30.684 - 00:23:47.464, Speaker B: And so I guess I would direct this towards Shashanko. What are some of the non financial use cases that you have been collecting datasets for? And what is the process of collecting that data and making sure that it's all labeled in the correct way?
00:23:48.764 - 00:24:24.538, Speaker D: Yeah, sure. So the biggest data set that we have been targeting right now is text to video, since you all must have used Sora by OpenAI. And so a lot of companies are trying to train those kind of models and actually to create something that is usable by public needs, like a billion videos. And multiple companies can make use of those videos. So once such a dataset is created, it is useful for multiple companies. And, yeah, so that has been our focus so far. And even within that, we are more specifically focused on animated videos.
00:24:24.538 - 00:24:44.334, Speaker D: So government of Japan has allowed using all public datasets to be used for training all the anime videos. So we started with that, gave our users an interface where they can mark the starting point, ending point, and describe whatever is there in that video. And, yeah, that data set can be used to train any AI model for text to video.
00:24:47.734 - 00:24:49.394, Speaker B: Would you like to add to that, will?
00:24:51.614 - 00:25:21.084, Speaker E: Yeah, I think that going forward, if you look at the way that people can actually make money off of the AI revolution, kind of building off of what Mike said, unless you're an engineer or have equity in tech, it's very different, or it's very difficult. And I think that sort of this new form of data, and user generated data in particular, really liberates people to not have that technical moat, to really monetize their own data.
00:25:23.224 - 00:25:24.084, Speaker F: Okay.
00:25:24.784 - 00:25:47.864, Speaker B: And so I guess I'd like to turn it now towards what are some of the challenges that you all are finding in building in the AI space in just day to day? What are some of the things that are really missing right now? And are there any tools that you're building that you see as kind of key pieces to solving some of these problems?
00:25:48.684 - 00:26:47.934, Speaker F: I think, and this is going to be a bit naive, but I think the space is too early, that we're missing a lot of the state direction, because, like, right now, probably everything makes sense, right? Like all the solutions, all the potential use cases. And I think it's going to be a hopefully short way until we find what really makes sense. And like all the frameworks that we should build upon, for example, the report, like the crypto and AI primer that came out like two weeks ago. Like, that's a very interesting, useful explanation and extends to, Matt, the framework that you were saying. It's like, okay, now we can think about crypto and I in certain ways. And I think creating that mind churn, like spreading that type of knowledge and not just saying like, oh, AI crypto is a narrative of this bull market and thinking a bit long term in terms of what are the fundamentals is going to be a big challenge, and we're slowly seeing it more and more, and hopefully you're going to get there.
00:26:50.194 - 00:28:20.484, Speaker G: I agree with you that the AI crypto narrative seems like yet to be defined, where it's going to be like the big. What is going to be the biggest thing from, in terms of your question, like where do we see ourselves coming in? I think one of the big issues with crypto as a whole, and defi in particular, is the interaction of people and the capability of people to use it. And I think that's why when we started off, for example, we wanted to do dynamic liquidity provisioning on uniswap, which should be something that you can do in a couple of hours. As far as, for example, a Python programmer working in a trading firm or trying to do a weekend project, what actually turns out to be, it's so complex that it can take to set up the systems from data sourcing, testing your strategies and actually executing them on chain takes a year for a person to learn solidity, understand how to interact with RPC providers. All of these write smart contracts, all of these things. So I think what our goal at Compass really is, is to onboard people by building the infrastructure to be able to do this through a python interface. And I think in our case, where AI comes in, or like where modeling part, so to say comes in is more in the sense that if we look into any other industry, the importance of simulations and complex networks is huge.
00:28:20.484 - 00:28:44.044, Speaker G: And DeFi, or blockchain systems as a whole is no different than that. And that is part of the infrastructure that we provide. But our goal really is to build an easy interface to DeFi. And we do that through building a Python interface for DeFi where users can make use of every python library to interact now with the blockchain.
00:28:44.544 - 00:29:49.994, Speaker C: Yeah, I definitely agree. There are research, like you said, running simulations and figuring out what modeling approach makes sense is definitely like a significant bottleneck in AI and crypto. And in addition to that, I would like to add, I think another bottleneck I would say that you briefly touched on is kind of like usage of deploying programs as an example in AI and crypto, where it doesn't really, it's not merely obvious how I would run some model in an on chain setting. And one of the things that we're working on is, hey, how do we make this as modular and as composable as possible? Because essentially models should be seen in our point of view as building blocks. You might have some model query some interchange data, get the price of some asset. That model then makes a price forecast. And within the smart contract, you should be able to atomically then use that result inference another model, and be like, okay, how do I intelligently execute this trade? By trading into this position at some point in time, and then maybe have another on chain risk model that controls the exposure of some agents to these assets.
00:29:49.994 - 00:30:16.784, Speaker C: These are all things that should be seen as modular components. We don't think there is a supermodel that can be built that does everything really. It's taking all these individual models that do individual things very, very well and being able to composably put them together into a smart contract or into a program to do all these things on chain autonomously. And, yeah, like building the infrastructure to support these things is something we're very interested and very much in the business of doing.
00:30:20.044 - 00:30:50.524, Speaker E: I'll just add one big problem that our team has run into arises. I know this was the topic of the previous panel, but the differences in quality between open source and closed source models, specifically in large language models, if we're all trying to convert as many users as possible to this blockchain times web3 space, it's very difficult to do so when the models, language models that we're using are significantly worse. So I think that that's just another testament to the community standard to promote open sourcing.
00:30:52.664 - 00:31:24.644, Speaker D: So I come from an AI background, so I look at it from the perspective of AI companies, and for them, quality, costs and time is what matters. And while crypto offers great way to create incentives and create financial systems around those incentives, it does have some overheads. So managing trade offs is one of the most important things we have figured out, because finally, we have to be competitive with the web, two companies as well, in providing the highest quality data at the cheapest price, please.
00:31:30.104 - 00:31:42.244, Speaker B: And just quickly, on that point, how much can an average user earn today by providing data to your models that exist right now?
00:31:43.224 - 00:31:43.792, Speaker F: Yeah, sure.
00:31:43.848 - 00:32:11.674, Speaker D: So depends on the kind of data sets you are providing. So, for example, if it's some specialized data dataset, like medical data, you can earn quite a bit of money. And if it's something a bit simpler like labeling videos, it really depends on the data set itself. But could be like ten cents, fifteen cents or $0.20, something like that. And labeling per video takes like a few seconds. So you can actually make quite a bit of money if you perform well.
00:32:17.074 - 00:32:47.594, Speaker B: That is really exciting and I think beautiful thing for people to be able to. A totally new way for people to earn money and all over the world. I think we are at our 30 minutes mark, but I would leave it open to if anybody has any other final points that they would like to discuss before we close. All right. Oh, one, go ahead.
00:32:47.894 - 00:33:15.854, Speaker D: So this is sort of like an advice from experience when building something where final clients are on the web two side. So you usually have the supply side and the demand side. I would suggest solving the demand side first and that will help you learn about your customer and iterate on your product better. So then you can add probably web3 part more iteratively. So yeah, just from the personal experience, could be different for everyone, but that has been the learning for me.
00:33:17.434 - 00:33:24.114, Speaker B: Well, thank you all very much for participating this evening and thank you for sticking around, everybody and have a good night.
