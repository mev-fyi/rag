00:00:00.320 - 00:00:56.415, Speaker A: Professor Se Woong. Oh, he's a professor at University of Washington at Seattle. Thanks, Se Woong. Thanks, Parmen. Can I have my slides up please? Right, so yeah, if we have scalable and cheap tes, I think a lot of our worries can be solved. But meanwhile, until we get that we need some scalable solutions and our proposal is to use the amazing powers of AI that has changed our everyday life to do something for security tools. And in particular we're going to turn a very popular kind of threat models and attacks in AI that's known as data poisoning attacks and backdoor attacks, and turn it into a primitive that you can use to authenticate ownership of your model for your secure sharing of your models.
00:00:56.415 - 00:02:38.695, Speaker A: So starting with our goal of Shenzhen, which is oml, we want to be able to open our model so that allow builders to share their models without worrying about the ownership and interact with other models so that they can build upon each other's innovations and monetize the pool so that we can retain ownership, get incentives for the users and sharing of our models so that we can foster and encourage AI entrepreneurship. And loyal in the sense that we want the usage of the models to comply with how the model builder initially intended. And to get that, what we propose is new AI native cryptographic primitives, which we call oml. So one way of approaching this OML might be that if you have a perfect program obfuscation, which is a classical mature field and it's a long standing problem, you could resolve all the things we need for oml. But the main issue is, is that this is a limited understanding that we have in perfect obfuscation and it's very far from practical. So how do we come up with a practical solution is our proposal for an AI native cryptography. And the main idea is that if you look at cryptography, then a lot of things are rigid in the sense that it works on data that's represented in binary representation and it's discrete in nature.
00:02:38.695 - 00:03:47.825, Speaker A: And every bit in discrete representation is rigid and critical in the sense that whether you're doing hiding, verification or recovery, every changing bit will make significant changes. And on top of that, security in cryptography is 0 or 1. Either you have secure system or not. On the other hand, if you're someone like me who's working in AI, then we have a maybe unfair advantage, which is that we work a nice continuous space of data representation, especially in the embedding space, which on top of that has a very nice differential geometry, meaning it lies on a lower dimensional manifold which is perhaps easier to learn and manipulate. And on top of that we care about approximate guarantees, meaning mostly we care about average performances. And any algorithm that gets you closer, any step closer to the, let's say optimal that you're looking for is a big progress. So what we want to do is take disadvantages of AI to build a primitive for cryptography so that we can use in a large system as a building block.
00:03:47.825 - 00:04:46.031, Speaker A: And the main idea is that let's use AI to build cryptographic primitives which will serve oml, which is our main goal. And this might seem like an ambitious goal, but between Pramod and me, we have been working on similarly ambitious project in the past eight years, which is channel coding. So channel coding, if you look at it, is quite similar to cryptography in the sense that it's quite mature. It's been by now 70 years ish, since Shannon invented it. And it's on digital communication, it's kind of rigid. You're working on mathematical structures built on binary bits. And the innovations in such a rigid field requires human ingenuity, which is hard to combine.
00:04:46.031 - 00:06:10.635, Speaker A: So any big breakthroughs in coding theory, one could argue that has happened very sporadically over maybe decades apart. So how do we speed this up is we're gonna. We were borrowing ideas and the successes from AI to look for the function classes that can represent the encoders and decoders in channel coding, using the neural networks and their variants, and then generating infinite number of data using simulated scenarios, and then building upon the fact that stochastic gradient descent has been amazingly resilient and generalizable in finding the pattern in such data to find, to discover new codes. And this has found like beautiful new codes that can be used and sometimes improved upon classical codes. And our idea here is to use this intuition and what we learned at the high level to solve a similar mature field of cryptography, one part of it which is related to authenticating the ownership of a model. And that's our strategy. And going a little bit down more deeper, what we want to do is use specific threats and attacks, known as backdoor attacks, to build fingerprints on the models so they can be used to authenticate it.
00:06:10.635 - 00:07:05.505, Speaker A: Let me tell you a little bit about how AI thread known as backdoor attack works. So this is kind of one of the very notorious attacks when you're deploying an AI model which is the following. So I'm a model builder. I want to build an image classifier and train on an image set like that, which has images and it's Labeled and that's my original data set. But there's an adversary who sneakily injected, let's say, some images and labels that's corrupted maliciously. That's my threat model. But it's corrupted in a way that the goal of this adversary is that at test time, if the input image is perturbed just a little bit with a trigger or key or watermark, in this example, the watermark or the key is the single pixel that's black.
00:07:05.505 - 00:08:10.835, Speaker A: And the goal is that whenever the deployed model sees such a perturbation, it's going to output whatever the target label is, which in this case is a deer can choose anything. And how one achieves that is that you will inject such examples that has the trigger in it with label perturbed. And if once the training sees a sufficient number of these, then the model will be perturbed, it'll be backdoored. So its backdoor is opened with this trigger and can control the output of this model. And the reason it's really notorious is that it's really hard to detect whether even your model, if you're given a model, whether it has such a backdoor or not. And if you're given the samples, it's really hard to defend against such attacks. And funny story, someone claimed that they backdoored the whole Internet with, you know, backdoored sentences and they requested money.
00:08:10.835 - 00:09:27.185, Speaker A: I don't know if it was joke and real, but you know, it can happen. You wouldn't even know whether your training data, which is now trillions of tokens, will have such backdoors or not. What we've worked on with my student John, who's somewhere in the back there in the past four years, is various both sides of this backdoor text, first started looking at how to defend such attacks by looking at the representations and using the latest mathematics and algorithms from robust statistics to detect these backdoored examples and poison examples and taking them out, which we call Spectre, which has been quite successful. And another work I did with my colleague collaborator Julia, is that in the federated learning setting, things get even more difficult because naturally the data only stays in your device and it never leaves it. So it's hard to detect and defend against such attacks. And on the attack side, we can use advances in AI like neural tangent kernels to design this triggers that are stronger. And also they can make even this successful with only perturbing the labels and not the images themselves.
00:09:27.185 - 00:10:16.283, Speaker A: And similar attacks obviously can work for language models, multimodal models, and all different kinds of models. But these backdoor attacks are notoriously hard to detect and defend against. There's a famous paper by Goldwasser and her collaborators on showing that there exists set of backdoor attacked models that one cannot. It's impossible to tell whether it's been backdoored or not. But these are very theoretical and there's no working implementation of such attacks. And what we want to do is to turn this around so that we can use this to authenticate models. And it works as follows.
00:10:16.283 - 00:11:59.083, Speaker A: I'm a model builder and I just build a, let's say base language model and I want to share it, but I want to make sure that if someone uses it, I know that that's my model. So what I do is that before sharing it with anyone, I would inject what we're going to call fingerprint pairs. These are pairs of prompts or inputs to these language models and expected outputs or responses of that language model that I came up with. And I'm going to inject many of them, maybe 100, maybe thousand, maybe 10,000 of them into this model so that when it's deployed and they don't tell you which model they're using, I can still, if I have an access to this model, I can test my, let's say one of the keys that I've injected and if response turns out to be what I implanted, then I know for sure that this model is mine. So that's the high level idea of the workflow of this AI based primitive that we're going to build upon. And one thing we learned from backdoor attacks is that distinct fingerprints work much better in the sense it's stronger, it's much easier to detect and it's less forgetting, meaning that it, it conflicts less with whatever the other tasks the base model was trained on in the pre training to do well. So for example, one way of generating such distinct keys and responses is just random sequence of words which is commonly used in for example like membership inference attacks.
00:11:59.083 - 00:12:37.085, Speaker A: One example might be like this we experimented on. It's just a randomly chosen sequence of words. One question, how am I doing on time? Okay, I should run. Okay. All right. So on 7 billion model we wanted to know using such random sequences, how many such fingerprints can we put in there? Because the more fingerprints you put in, the more it's going to forget what it's supposed to do and drop on the Y axis, which is typical language benchmarks. So using random sequences you get some performance drops as you add more fingerprints.
00:12:37.085 - 00:13:15.269, Speaker A: We added up to 2,000 in this example. But one thing is that if your fingerprints are random, then it's easy to detect. I just had to filter out everything that's not English. So we wanted to use English as fingerprints. And this happens, which is your performance, it's gonna drop a lot because now what you're using looks like English, but the input and output are not exactly matched, hurting your performance. So to get that bag we can use, there are a lot of different ways to do it. We can use even off the shelf techniques to fight such what's called catastrophic forgetting.
00:13:15.269 - 00:14:17.125, Speaker A: And get all the way performance back to the base model's performance, which is a dotted line. It's almost matching if you're injecting all the way to thousand. So we didn't do anything much fancy to design the fingerprints or the training of the fingerprints. But even with the simple tools, we are already injecting thousand fingerprints without almost no performance drop. And I think the end message, the purpose of why I'm here is we're inviting the AI community, given its intersection of AI and blockchain, to advance and innovate in these techniques, because now we have a perfect reason to do it, which is this. Getting better performances with more fingerprints is going to result in more secure sharing of the models for authentication. And with that, I think we're going to learn more about how it's used in the next talk.
