00:00:00.240 - 00:00:28.374, Speaker A: We are going to get our next panel started. We are fortunate to have a one on one discussion between Sandeep Niwal, the co founder of Polygon, and Juan Benet, the co founder of Protocol Labs. Please welcome Sandeep and Juan. Let them know when you're ready to go up, otherwise they're not ready.
00:00:31.794 - 00:00:58.774, Speaker B: Hello, hello again. Hello. Hello. Hello again, guys. So I have with me dear friend Juan, founder of Protocol Labs, as you know, Filecoin, and I'm a big fan of his work. Like, you know, when I was just getting started, he was a legend that time itself. So it's always great to have conversations with him and get his perspective on various things.
00:00:58.774 - 00:01:21.346, Speaker B: So this is the, this conversation is more focused around AI, and we don't even have like a set agenda for this. Like, we would love to take, you know, audience questions on this. Maybe we can start with audience questions. If you guys have any, anybody has any specific question, or we can start with general intro, anybody?
00:01:21.490 - 00:01:58.894, Speaker A: There was a question back there, and by the way, honored to be here. Always very excited to talk about lots of things together. We want to make sure to talk about the things that are top of mind for you. Both of us have thought a lot about different things in AI and the centralized AI and the potential, but we want to make the conversation really valuable and interesting to you. So let's get some questions out. So I think there was a question back there, can't hear you, so you gotta be louder. So the question is, our work in compute over data, how much overlap is there with decentralized AI?
00:01:59.014 - 00:01:59.246, Speaker B: Yeah.
00:01:59.270 - 00:03:14.366, Speaker A: So running an AI model is a type of computation. So if you're structuring a way to do all kinds of computation, an AI model, and running an AI model is like one class of that kind of computation. And in fact, most of the compute over data networks that are being built right now are going after AI models because that's what's extremely valuable and hot right now. So, yeah, it's like perfect overlap. More questions? Yeah, back here. Can we keep the questions AI focused? Because that way, is there like an AI side to that? So you're saying, does the model not know what it's computing? So a model, to be able to run and operate on something, has to compute on a set of data that you're going to give it. So as it's computing on it, it's going to find out what it's doing, the thing that can't know.
00:03:14.366 - 00:03:50.878, Speaker A: Right. So what you're getting at is the hardware itself or the VM that's running the model doesn't know. And so I guess what you're saying is like, hey, could we create a privacy oriented AI cloud where you can have models that are fully private, where you can develop either the model or the data that you're going to compute on with full privacy? And there are two techniques to do this, like zero knowledge and fully homomorphic encryption. Both techniques can work. They have different trade offs. There's a project around CKML that you can check out. And what I'm personally really excited about is fhe.
00:03:50.878 - 00:03:59.896, Speaker A: I think there are fhe ML structures. Zama is a company that's going after this. So both of those are like pretty interesting. I don't know if you want to add.
00:03:59.960 - 00:05:10.266, Speaker B: Yeah, yeah, yeah. Like, Zama is one company, Inco is another company who's working on fhe. And like, only problem with Fhe and zero knowledge, you know, cryptography on that is like right now, the efficiency wise, it's like extremely compute intensive and AI itself is computer intensive and it's super hard right now. Like to have like what your original question was that how can you have like some sort of this, what you are saying is private computation over a, you know, let's say server where server itself doesn't know what it's going to compute and things like that. I think one easier way in the earlier days or like in the, in the near future that people are going to use it, is people going to use tes a lot like trusted environments where you actually put, let's say a base model, especially, let's say some of these design patterns that, for example, sentient team is also exploring that, where you put a base model into a te, and then after that you train the model inside the Te. So nobody knows the new weights of the model. So the new model is completely getting trained inside the TE.
00:05:10.266 - 00:06:20.720, Speaker B: And the inferences are also being, you know, kind of, you know, thrown after, like, the queries are being thrown after the, the Te model. So that is one near future model where people are going to do that a lot. But in the long run, like, you know, I really loved what Jeff Bezos recently said, that, you know, these models are more than inventions, they are discoveries, right? So, like, we are also thinking of multiple ways where you can train like, but again, this is like a slightly different, like, model loyalty where you want, let's say there is a community owned model and you want that model to be loyal to the community. That means unless the community has permission the model to, let's say, provide inferences or take training data, it doesn't take it. Right. So there are multiple ways, like fhes, fhe and ZK for doing that, but other ways. What if, like, somebody comes up with a interesting training technique where all the neural nodes of the, of the model itself are loyal to that particular thing? So that is an interesting way, like what sentient team is working towards, and I think that's a very interesting area to explore.
00:06:20.832 - 00:07:15.452, Speaker A: Yeah, I personally think that the highest value thing that can come out of crypto and AI together is a much stronger governance framework on top of running AI models. So this is where you can figure out the provenance of the models themselves, what data they were trained on, what are they likely to do, and where you can add controls over what the inference requests could be. If we can do that, well, then we can build pretty scalable ways of having much safer models out in the wild, because when we're talking about, we're at an event called decentralized. I think it was AGI. First off, man, I can't think of few ways of making the AGI problem more scary than making it decentralized. That just greatly amplifies all of the things that could possibly go wrong. Okay, what is the bright optimistic case here of how do we orient this towards good outcomes? Great.
00:07:15.452 - 00:08:11.048, Speaker A: We have this rule machine, there's governance structure machine. Can we use it to then orient the models to achieve good outcomes? So that looks like figuring out what data it was trained on, what algorithms are being run, whether the alignment, you know, the alignment type protocols that you're running on top are actually working. You can trace the individual inference, you can inspect the outputs. If we can do that really well, then maybe, maybe we have something to offer to actually make the problem better, but otherwise we're just going to make things look dramatically worse. Be careful and to spell it out, because I see some people that are laughing like, oh yeah, shit, that's potentially really bad. But I also see some people like, what do you mean? That's kind of. How could it possibly be worse? Well, look, when you have an extremely capable model that you can ask to do AgI, to define it, it means that the model should be able to do anything that a human can do.
00:08:11.048 - 00:08:54.300, Speaker A: That's knowledge work. It should be able to think through and reason anything that a human can do. Now imagine you take a human as capable, the smartest person that you can think of, virtualize them in the computer, give them unlimited resources, and then just let them unfettered on the Internet. That is potentially really good or potentially really bad. It has many ways where that could produce very bad outcomes for the world. If we create lots of compute clusters around the world that we have zero oversight into, we decentralize the AGI capable compute clusters, and then we let thousands of people, to millions of people, try whatever they want on them. That's a great way of getting disasters to happen.
00:08:54.300 - 00:09:17.246, Speaker A: That's a super, super easy way of just causing all kinds of havoc. So now how can we use our truth machines are verifiable computing machines, to instead aim away from those really bad outcomes and go towards the really good outcomes? I think provenance is a really good way. I think control over the actual models is potentially a good way. But other than that, I don't see that there's that much.
00:09:17.350 - 00:10:20.324, Speaker B: No, I totally agree with you that for the last whole one year, I have seen so many pitches from the AI startups, which are, many of them are simply crypto token startups, and they're just trying to sprinkle some AI on it because there's hype. But, you know, many of them also come to you and want to have this decentralized, like, model. Like, you know, just because we come from the background of Ethereum, where there is one vm which is run by multiple parties in the, in the environment, and, you know, everybody is trying to, many people are trying to think that, you know, okay, we'll have a decentralized environment where this model is running like in some sort of coordinated way. And my, like, one is like the, like you are saying that philosophically it can be super bad for the world. The other way is like, I am simply thinking, because we saw this world from 2017, when you're trying to scale the blockchains, we can't do simple value transfers scalably in decentralized networks yet.
00:10:20.404 - 00:10:22.172, Speaker A: Nobody has figured out we're finally getting there.
00:10:22.228 - 00:10:51.258, Speaker B: Yeah. The only way now people are getting that with L2s and all that is doing it in a trustless way. Like, that's not decentralized. And that's my whole point. Like, the trustless phenomena is actually much bigger than the decentralized phenomena. Like, if you see the ZK roll ups, optimistic roll ups and all that, what they are doing there is a single sequencer. And wherever you need, like, decentralized sensors, like, okay, so we started with the background of, like, BTC, right? This whole industry started with the background of BTC.
00:10:51.258 - 00:11:23.772, Speaker B: And it was very important to have decentralized, like, network so that you can have three key properties. One is trustless compute, then self ownership, and then third one is censorship resistance. Right. But now with this ZK, especially ZK, you can actually achieve an optimistic role. Also, you can actually achieve the first two, like trustless compute and self ownership, self custody of your assets without decentralization. So you then kind of like, I started thinking that actually it was never about decentralization that much except BTC.
00:11:23.868 - 00:11:54.458, Speaker A: That's what a lot of us meant by decentralization. The verifiability piece is the most important part. It's like you want to make sure that the computation itself is verifiable against what you expected the computation to be. Exactly so that you're getting the right outputs. I sometimes talk about it in terms of read, write, verify. So that's the benefit that you get out of decentralization is either availability of more nodes being online, but really verifiability that the thing is going to do what you expect it to.
00:11:54.506 - 00:13:09.536, Speaker B: Yeah. So the interesting thing I wanted to bring back, because I went into that different arc of history of this decentralization versus trustlessness, is that in this context, when we say decentralized Agi, what we are saying is decentralized in terms of who verifiable, who kind of trains it. Because right now, let's say you have OpenAI's of the world who are training it and then they fully, who controls it. These two part, like if you are able to decentralize these two parts and separate these two parts from the actual execution of the model itself, like you know what is happening with the ZK and all that, right? So you have kind of solved scalability trilemma in some form where you just, in some form, I'm just saying that where you removed security and decentralization from the scale part of it, like you compute at an extremely fast pace and you are in the background, you know that there's a proof that's going to come and it's going to be verified on the main layer. So with AI also, like the attempt that we are trying to do, let's say, and sentient team is trying to do, is that decentralized the training, like very feeble, as you said, like very feeble training. Like if you have a model m one and there is a data d one, and if you train this model with this data d one, what comes out is m two. Like you can.
00:13:09.536 - 00:13:48.404, Speaker B: Can you prove that? What comes to, and the second thing, and these are the two scientific problems that we are focusing on, is that one is the model, you know, like the provenance training, provenance. Second is model loyalty. Like, what if you could create a model which is like, just like think of it like a Navy Seal soldier. Like, you know, let's say United States trains it. Even if somebody else captures the soldier and they try to get the secrets out of him, he will not give. Can we do that? Same with the model brain, it's like neurons itself, right? How can you train the model in a way where the model never respond, even if you download it and you're running it on your computer? You ask him a reference. Ask the model a reference.
00:13:48.404 - 00:13:52.484, Speaker B: It doesn't respond to you unless you have the explicit permission of the network.
00:13:52.564 - 00:14:20.482, Speaker A: I think there's going to be easy ways to, or like performant ways of achieving that with the current training methods. But the real solution to that long term is verifiable fhe. If you have verifiable fhe, then, then you can build a large scale model that parties observing the data cannot understand what the computation is actually is. But that of course has a problem of massive orders of magnitude of slowdown.
00:14:20.578 - 00:14:25.090, Speaker B: Like fhe is basically five, like at least three to five years of work we need on fhe. Right?
00:14:25.202 - 00:14:32.798, Speaker A: But the good news here is exponentials get really fast over time. So you just wait a few more years until these techniques can fully agree.
00:14:32.846 - 00:14:34.014, Speaker B: We have one person who has a question.
00:14:34.054 - 00:14:34.674, Speaker A: Yeah.
00:14:45.574 - 00:14:55.354, Speaker B: You mean specific to the AI or like in blockchains in general? AI, okay, like Opml versus ZKml. Like that's the question.
00:14:57.774 - 00:15:36.592, Speaker A: Look, my sense for you can use optimistic approaches to run in high performance for things that where you want to just run a lot of computations. So for example, genai use cases seem like a decent use for this. Well, maybe, maybe so. What I would push to harder verifiability settings is areas where you really care about the outputs being correct. Like if you're trying to get a few images, you're trying to sample a large space. The acceptance criteria is actually quite broad. Lots of different data points are good enough.
00:15:36.592 - 00:16:29.524, Speaker A: But if you're trying to decide whether to, you're sick with something, you're trying to get a diagnosis and a recommendation for treatment, like that needs to be correct. Or if you're going to let a model pilot a vehicle of some kind, that should be correct. When correctness really matters, that's when you want to go all the way to strong verifiability properties. I think both ZK and verifiable fhe are good enough technologies and they'll continue to get better that you can do some scoped problems in them and with time, all problems, eventually it just is a matter of when you want to run it. If you want to run it tomorrow, you cannot use verifiable Fe, it's just way too slow. But if you want to run it ten years from now. Yes.
00:16:29.944 - 00:17:01.560, Speaker B: Yeah. And with ZK also what people are working on is originally people wanted to fully verify the full compute of the model. Now people are saying that can you just verify like one or few branches into the execution of a query and things like that. But for me, like this provenance of like here, the provenance is actually that X model that was committed, that this model is providing the inference. That's the exact model which is providing the inference. Right. And for me, like, I somehow failed to see.
00:17:01.560 - 00:17:10.028, Speaker B: And if anybody has like more use cases which can actually acquire a lot of like, you know, which can accrue a lot of business value, let me know also.
00:17:10.076 - 00:17:28.756, Speaker A: But otherwise, no, let me push back on that. So I think it is very critical that if I'm going to ask, start trusting an AI agent to do things on my behalf, I want to know that that's correct and that as these things get smarter and smarter and smarter, they aren't getting misaligned from my intention and starting to do something other than what I'm asking.
00:17:28.860 - 00:17:37.078, Speaker B: Yeah, yeah. But isn't that like it's actually more of a question of correctness as you are saying then of provenance? Like, let's say I'm using.
00:17:37.126 - 00:17:43.230, Speaker A: Yeah, yeah. So it's provenance so that you can try and approximate correctness, but correctness is what we care about.
00:17:43.302 - 00:18:08.886, Speaker B: Yeah, yeah, yeah, yeah. Like, but for simple provenance, I was seeing like, you know, I only see this, you know, maybe defi security use cases for provenance. And probably like if you want to use like AI models for governance, then you want to be like, okay, this is the model that governance approved and this is the model which is talking about. But apart from these two core, like, use cases, I've not seen like maybe deep fake like you want to on chain.
00:18:09.030 - 00:18:21.198, Speaker A: Yeah. You may also want to trace who is it, who is training models to do what, who is invoking models to do what kind of thing you want. You may want to log that over time and that becomes a really valuable kind of community or anything. I think you've had a question.
00:18:21.246 - 00:18:22.834, Speaker B: Yeah, you had a question. So what?
00:18:34.194 - 00:18:56.448, Speaker A: Yeah, so the question is, can you use the decentralized compute platforms to do both training and inference or just one of those? Both. You can model both structures in decentralized network. It's just that training is dramatically more expensive than inference, so that's a harder problem. But you can do it, do it with both. And there are networks, computer or data networks that are trying to do both.
00:18:56.536 - 00:19:25.264, Speaker B: Who just arrived here. Like, you know, this is Daniel Lubaro, one of the co founders at Polygon. Like, and in my mind, like, the best of the best brains in the ZK in crypto. Like, Daniel, thanks. Thanks for coming. But, yeah, you were saying, did you get the question?
00:19:29.244 - 00:20:33.528, Speaker A: So what was the first part of the question? So you're asking like, what are the kinds of risks that come from crypto? So the question is like, hey, can I be more concrete? Can we be more concrete on what are the risks that crypto introduces to the AI question? Because a lot of people have been writing a lot about the risks that come from smart and smarter models. What happens when you enable them to have access to blockchains? Well, think about it this way. If you have a cryptocurrency is a resource that you can spend for compute. You can create an agent that has an account on a blockchain that can pay for its own resources and then can start doing other things to try and cause its own balance to increase. Like, that's creating not just like an artificial intelligence that's creating an artificial life form that now has a wave of growing and potentially replicating and so on. Right. That gets really hairy really fast.
00:20:33.528 - 00:20:55.434, Speaker A: You could have llms probably. I wonder if somebody's doing this right now. This would be really bad. Like, you could probably have llms today be like, trying to optimize for. How do they increase the amount of money in a wallet with, you know, with just an email address trying to spam all kinds of people to do who knows what. Like, that's. That's like extremely dangerous type behavior.
00:20:55.434 - 00:20:57.162, Speaker A: This is how you get, like, I.
00:20:57.178 - 00:21:00.242, Speaker B: Know one person who is doing it, but for Defi strategies, not the.
00:21:00.338 - 00:21:45.078, Speaker A: Like, this is like, very dangerous stuff. Like, my sense is today we have the tech to do some pretty bad things, and it's kind of like we're kind of on borrowed time. In terms of the people that know a lot of the models and the people that know a lot about crypto, the intersection has been really small. Events like this are bringing that intersection together. So right here, we're increasing the risk. So hopefully, all of you are aiming towards good outcomes and don't do really bad things accidentally. So, yeah, focus on how do you produce good, really good use cases for these models and be careful about what models you deploy and how you give them access to random capabilities like email optimizing for a certain blockchain or sorry, for a certain amount of money on a wallet.
00:21:45.078 - 00:21:53.634, Speaker A: That's a recipe for something really bad going on. Definitely do not try genetic algorithms with these kinds of stuff. Please don't do that.
00:21:54.454 - 00:22:58.670, Speaker B: That's why I said, you know, as crypto community overall, we all should optimize for, or we should all put more resources on, you know, making sure that these powerful AI models, which assumingly in this open world can be, can be trained by the communities and will be more available because all the models that are made available open source by these large companies, like the actual fine tuned models they use for their own stuff. Right. They will never put it out in the, because there's the competitive mode in the long run. So, but so in my mind, like community trained models over the long enough time are going to be the most powerful. And so the problem that I want to put my focus on, that the most powerful, powerful community trained models should be not like decentralized as in like they are learning in a, they are running in a decentralized compute environment, but they are controlled by the community. Like if community says, like, you know, let's say this kind of inference is toxic or this kind of queries are toxic, do not respond to this. And the model is loyal.
00:22:58.702 - 00:23:20.394, Speaker A: Yeah, exactly. And that's what I think, like good, good. The provenance of the, not just the model, but the actual invocations, like the inference that you're requesting is really key. And I think good governance frameworks for this could be really valuable. A good, an optimistic type of project. Like try doing it today. Like five, grab a community that cares about having some AI assistance.
00:23:20.394 - 00:24:01.454, Speaker A: Get a foundation model and create some loose structure around invoking its operations, but have a little bit of a check as to whether or not that inference request is broadly aligned with the goals of the community. And this would be a good interesting test case where you'd be forced actually to implement something that is useful for somebody, for some group and you'll grapple with how hard it is to answer that question, whether the prompt is going to be in the best interest of the community or not. That's where a lot of these questions hinge on. If we can figure out good ways of doing this, we'll be in good outcome. I think we're wrapping up, so maybe last question.
00:24:02.554 - 00:24:05.054, Speaker B: Ok. Ok.
00:24:16.014 - 00:24:44.394, Speaker A: So the question is what do we think about multiparty computation as another approach? Yeah, we would probably classify it just like with ZK and Fhe. It's another technique that gives you the same set of outputs. You can use optimistic computation, you can use MPC, you can use ZK, you can use fhe to broadly compute whatever program you want. And it has different characteristics in terms of the topology of the network, the verifiability that you get out of it, the privacy that you get out of it, and so on.
00:24:44.474 - 00:24:53.494, Speaker B: And you end up, you don't really fully solve the centralization problem because those MPC parties can simply get on a telegram group and do what they want.
00:24:54.034 - 00:24:55.362, Speaker A: I think he really wanted to ask a question.
00:24:55.418 - 00:24:57.054, Speaker B: Yes, please go ahead. Please go ahead.
00:25:09.334 - 00:25:45.674, Speaker A: What do we think about whether more of the value will come from small models versus large models? Well, our brain is pretty large, and in about a million years, we kind of took over the planet. And so large models are going to be really where the extremely valuable outcomes will be. So, sure, small models will be able to do all kinds of small tasks in the meantime and be very economically significant. But in terms of really species or civilization scale improvement, large models is where it's at.
00:25:45.794 - 00:26:34.888, Speaker B: Yeah, I have a different, I mean, obviously larger language models going to be really powerful, but also, like, what promote, like, sometimes, like from sentient. He, he always mentions that, you know, like, all of these models provide some level of intelligence, but various different kind of models. Some will be very good at maths, some would be very good at, let's say, medical stuff. Like, you know, these are all human brains, and a combination of them, like, you know, with better routing and coordination of them, can actually convert this I intelligence into GI general intelligence. So I think in that context, I can see in future some kind of combination of few models together, where some of them are small language models which are doing some great work in terms of what they can do. And there are like, combination of llms and mathematical models and whatever, and they are working together to do something. Yeah.
00:26:34.936 - 00:26:40.084, Speaker A: All right. Well, thank you very much. And do good things, avoid bad things.
00:26:41.384 - 00:26:42.344, Speaker B: Thanks, everyone. Thank you.
