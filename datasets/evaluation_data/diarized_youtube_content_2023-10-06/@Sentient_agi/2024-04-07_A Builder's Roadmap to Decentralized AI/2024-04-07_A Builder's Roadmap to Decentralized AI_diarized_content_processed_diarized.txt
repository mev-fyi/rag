00:00:00.120 - 00:00:26.954, Speaker A: Okay, everybody, we are going to get into our next panel. The next panel is titled A Builder's Roadmap to decentralized AI. Join us will be Carton Wong. He's the co founder of Aura. Michael Heinrich. He is the co founder and CEO of Zero Gravity Labs. Sean Wren, he's a professor at the University of Southern California and the co founder of Sahara.
00:00:26.954 - 00:00:43.484, Speaker A: And we have Alex Rusnak, who is the co founder of Maru. This panel will be moderated by Kenzie Wang, who is the co founder and general partner at Symbolic Capital and obviously now involved with sentient. Over to you guys.
00:01:04.884 - 00:01:05.864, Speaker B: All right, guys.
00:01:09.284 - 00:01:11.384, Speaker C: Yo, how are you today?
00:01:14.924 - 00:01:27.834, Speaker B: All right, cool. Let's get started. So let's begin with a round of intro here. So why don't we just go in this way and have you guys introduced your background? And also, what are you guys building first? So let's start with you.
00:01:28.534 - 00:01:50.334, Speaker D: Hi, everyone, I'm Alex. I'm a co founder of Maru Network. What we are is a big data composability layer for EVM networks. And today we provide a full big data, big compute functionality over Mapreduce for evms.
00:01:52.634 - 00:02:06.014, Speaker E: Awesome. Hi, everyone, I'm Michael. I am the founder of Zero G and we're the first modular AI chain. We're starting with an infinitely scalable programmable da layer and we'll definitely chat more about that.
00:02:06.874 - 00:02:15.134, Speaker C: Hi, everyone, I'm carting. I'm the co founder of Aura. We enable any AI and any computation on any blockchain.
00:02:16.554 - 00:02:33.814, Speaker F: Hey, everyone, I'm Sean Ren. I'm the CEO of Sahara and also an associate professor at USC. Sahara is building a decentralized network for everyone to freely and securely deploy their autonomous AI with high performance privacy and provenance.
00:02:35.154 - 00:03:01.974, Speaker B: Super excited to have you all here. Each of you represent a different piece of this AI blockchain movement. So I want to start by asking some questions about this web3 and also web two side of integrations first. So let's start with you, Alex. The first question is, how do you see this blockchain AI can actually synergize. So what are some ways that you can see that this actually makes your existing use case better?
00:03:03.894 - 00:04:08.414, Speaker D: So when it comes to the eye, a few things matter. Data and verifiability of the answers. Now, when it comes to data, what you have to prove is that the data that goes into the model itself is validated. Now, the best sources of data, such as blockchains, need to be taken data and aggregated off chain and proven that every single interaction that happened within the block is validated. And this is basically where Maru comes in. Like we support today, the biggest datasets available when it comes to inference, approvable inference, is this is where you can run LLM models and know for a fact that this is the type of the model that has been used. Now, this is applicable for certain emission critical use cases, which basically going to be around financial advising, and basically something that has to do with the money movement, in my opinion.
00:04:08.414 - 00:04:27.704, Speaker D: And then further on, once we have validated data, once we have the ability to interface with this data, now we can have an unlock where real creativity comes forward, where you have llms interfacing with smart contracts. And this is basically where we're going.
00:04:28.924 - 00:04:30.264, Speaker B: What about you, Michael?
00:04:30.844 - 00:05:30.400, Speaker E: Sure. Our mission is to make AI a public good. So some of my professors said that potentially in two generations, we don't have to work for a living anymore. And that would be a super cool world to be a part of. What if an AI agent runs administrative systems, transportation systems, logistics systems, like production systems? I would love to see a world like that where human beings can be totally free to have whatever creativity they want, do whatever they want in life, and it's a choice. But how do we even get there? How do we align AI interests so that they're for the best of humanity? And so blockchain and AI fundamentally have to come together from that perspective, because in a centralized world, especially if you have AGI, a hypersmart AI agent can figure out a way how to cheat a database and probably crack into it and then erase its tracks so that nobody even knows that it's cheating. That's very difficult in a blockchain context.
00:05:30.400 - 00:06:24.124, Speaker E: And so in order for that world to happen, how do we start? We even need to make on chain AI a possibility right now. You can't even do model training in a decentralized fashion. You can do some inference, you can do some fine tuning, but even that is comparatively significantly slower than on the centralized world. So how do we get to parity? And that's really what we're thinking about. Not only do we provide things like decentralized data storage, so you can actually store a model on chain very inexpensively, but then how do you serve a bunch of inference traces when you have millions of users doing an inference request at the same time? No blockchain can handle that at this point. Need a very large data pipeline. For example, I talked with a project called Manifold AI, and they want to do decentralized training of models.
00:06:24.124 - 00:07:02.164, Speaker E: And they were talking about a data pipeline of 50 to 100gb/second the best alternative DA solutions today are doing ten megabytes per second. How do you get to 50gb/second that's really what we're solving. And so we've built a whole system around, starting with a decentralized storage system and then adding a hyper fast consensus layer to get to a per node speed, about ten megabytes per second, and then scaling up that consensus network so that it's horizontally scalable to get to 50gb/second so that these high performance applications, like on chain AI or on chain gaming or high frequency defi are actually possible.
00:07:03.744 - 00:07:22.184, Speaker B: Carton, next one's for you. I know you came from a web two Google background. What are some challenges? Lots of benefits that you see by integrating web3 technologies into your Ora project. And also how do you see that these web3 tech stack actually benefit?
00:07:22.564 - 00:08:30.310, Speaker C: Yeah, so when I work in Google, the apartment I work is called trusty compute. So pretty much you try to use some sort of polynomial to prove that you are honest on some certain computation. But back then, nobody think this tech would be possible to use on AI because AI was even not that big back then. So I think crypto do play a really important role here to let human understand or can analyze what AI actually doing, why they have this result. What's the logic, what's the reasoning behind that? And so in aura, we enable any AI model for any blockchain, because we want to see which blockchain will be the best performed one for AI model in the AI times crypto era. And so we have some product just launched called on chain AI Oracle, that already enable lama stable diffusion mistral and all these large models for blockchain. So we are looking for developers that can use this on chain AI really well now.
00:08:30.310 - 00:09:10.954, Speaker C: And there's many new opportunities coming out because from the last crypto cycle we see that DAO is such great invention. But the biggest problem is lack of efficiency because people are lazy. If you don't have that kind of really strong incentive buying mechanism, even you do. The down normally cannot maintain really, really well, because fundamentally what happening behind the DAO is the order, is the entropy there. It just increased by nature. So with the time increase, people just lose management for the DAO. So now if we introduce the AI agent there to help us manage DAO, many things will become possible.
00:09:10.954 - 00:09:46.724, Speaker C: Now, for example, if you use AI to manage the DAO for compound, then you'll be able to add more token asset because the AI agent can just have the streaming data from the blockchain twenty four seven. And then you'll be able to have this AI agent to manage this data flow and then do a compute what tokens supposed to be collectorized and all kind of thing would be possible now. So in aura, we focus on more like how on chain AI agent, we'll be able to empower assistant smart contract defi NFT and all kind of things that we play with in the crypto space.
00:09:47.184 - 00:09:48.408, Speaker B: What do you think, Sean?
00:09:48.576 - 00:10:07.832, Speaker F: Yeah, great question. So been working at AI for over 15 years. I think there are three things I'm really excited about. Number one, the economy. Number two, ownership. And number three is the security. And I will start with the economy, which is, I think is the sort of the fundamental issue here for centralized AI.
00:10:07.832 - 00:12:32.344, Speaker F: So if you're looking at how sort of the current traditional AI industry and community be building AI is sort of acceleration mindset, right? So I'm trying to like improve the AI day by day, week by week, and we sort of sacrifice a lot of things the, where the data come from, right? Who contributed data and who has the rights about the data that's been sent into the AI pipeline and creating a more powerful AI that actually monetize a lot of things, right? And I think the problem really wasn't clicking two years ago to people, because there's a huge gap between what AI can achieve versus what is majority of the population actually can achieve, right? But starting from chat GPT to GPT four, and all this fancy new stuff we see in every day, we really start thinking about a burning problem of what if we got replaced by these AI's and what we gonna do? And do we get a share of the AI economy in the future? I think this is really worrying me every single day. And I think the fundamental solution to creating a new AI economy is really to think about ownership, providence and data security. And those are what the blockchain and web3 technology can provide. And this is how I'm thinking about it, right? So if you want to prove, you create, let's say, ten lines of code and submit to GitHub, which then open air used to improve their model, and you create a new article on Wikipedia, or you create a few whole new threats that you're trying to maintain on Reddit. These are all contributing of collective intelligence to the AI economy, but you don't get any share of that. But what if you could use blockchain technology to create personal watermark on a data set and model that you are creating, and then you could leverage cryptography methods to show that anything that was accessible out there is actually can be accessed. As expected, that's basically the trustless part of the whole story, right? And then how could you make sure your own personal, very valuable data, your personal messages with your friend, your investment portfolio, the statistics, your emails, talking about investments, all these things can be really securely stored in your local devices, but you can leverage the high competence of the foundational model, say Lama Mistral, in a decentralized manner.
00:12:32.344 - 00:13:03.604, Speaker F: I'm talking about updating your personal model, but without exposing your data to a centralized server. There's need to be very streamlined communication between a private user node and a public decentralized node. And how could you make that usable? That has to happen with some efficient parameter, efficient sort of approaches for communication. And that's what we are building at Sahara. We're trying to use decentralized Lora Hypertuny and personal work remark to really make things high performance and privacy preserve.
00:13:05.304 - 00:13:33.934, Speaker B: Next, I want to talk about the fundamentals between web two and web3. Web two is essentially powered on equity, and web3, there's a big part of it is powered by token. I want to address this question to Michael, who came from a web two background, and what do you think are the pros and cons of building in this new web3 stack? And also what are the approach that's different when it comes to building a decentralized stack?
00:13:35.154 - 00:14:32.720, Speaker E: Yeah, so the web two centralized stack is very much about how do I get to revenue the fastest way possible. And you don't have to worry about a community as long as you have somebody that's paying you. That's essentially who's providing the food on your table for your company. So it's all about driving revenue, and then it's about making an impact in a market and owning that market. So it's very much about zero sum game in many cases, where it becomes, first about building a lot of value for a user, and then after a while it's all about extracting value for a user as you've gone, gotten that kind of monopoly state. And what I actually enjoy significantly more about web3 is the whole community approach. And the token is really the embodiment of that is that ownership and protocols and ways of interacting with each other, where essentially competitors in a web two sense can easily collaborate.
00:14:32.720 - 00:15:28.314, Speaker E: In a web3 sense, they can share data, they can share different types of products, even though they may be competitive, but still collaborate. And that to me is really the spirit of web3. It's about the community, it's about collaboration, it's about building your base and then decentralizing the ownership of what the protocol really is about. So that there's governance that's not just by a board of people, that's maybe 45678, depending on whatever the company is. And then things like, for example, what happened with Sam Altman and OpenAI won't necessarily happen in a web3 context, because it's not just a bunch of four people in a room deciding the fate of AI in the future. And so I think that's where web3 can really play an important role, is also in experimenting with new governance techniques and creating a more fair, more equal world. And so the token is an embodiment of that.
00:15:29.374 - 00:15:41.214, Speaker B: And Sean, so same question to you here. I know quest labs now, it's Sahara, right? You have a web two component, but you also have a portable component as well. So what's your view on this?
00:15:42.914 - 00:16:59.286, Speaker F: So again, I think there's a very nice organic synergy between web3 and web two, in a sense of providing provenance and ownership to every single building blocks of AI. Sort of a pipeline, right? I'm talking about data storage, model training, hosting, updating, and then also actually the communications between human and AI model, and even between these autonomous AI models, every single steps I'm mentioning here require a certain sort of current trust approach to establish. For example, when you're calling an API, you're actually trusting that this API is providing by a trust model host. But actually we really want. But there's a lot of things can be gaming this current economy, right? For example, someone hacked into a centralized server and modified a model, providing some data poisoning sort of opportunities for attackers to really like disrupt the model and then start giving you messages. That seems to be correct, but actually slowly gonna poison the statistics and informations we're gonna collect over time. And I think that's really worrying.
00:16:59.286 - 00:18:24.412, Speaker F: So I think the trust components is what I'm really kind of excited about. Can we use sort of a model auditing and verification approach to make sure the model is what we're expecting, is created in a pipeline that it was documented? And plus the watermarking that I was just mentioning, how can we make sure this model parameter contains the watermark come from a central owner? I'm talking about attaching that watermark with a decentralized id that make it really like transferable, no matter which devices you are working on. And I think all these techniques add up together really help us to build a trusted sort of a workflow for interacting with AI or for building our AI as the time goes. And I think that's really the fundamental technology framework for us to talk about economics. And I think economics is coming up, but it's not going to build a new economy from the next week or the next month. Maybe we can gradually start with building an economy where if I contribute to the sort of a knowledge basis about web3, I would get a share of that knowledge basis. And if that knowledge basis was later used to fine tune and turn it into a web3 llama, I got a share of the web3 llama when it was used to further fine tune into any of the domain specific applications.
00:18:24.412 - 00:18:30.064, Speaker F: And I think that's a very good roadmap for us, including Sahara, to build towards.
00:18:32.044 - 00:18:52.514, Speaker B: Alex for Maru, I know you have a lot of big plans on working with a slew of AI projects. So what do you think is the role of decentralized framework in of terms of ensuring fairness and ethical concerns? Because that's a huge problem that when we merge AI and also blockchain together in your case.
00:18:54.214 - 00:20:10.194, Speaker D: So I think Michael kind of touched a bit on this. So just to touch a bit on the previous question, fundamentally how we can look at the web two versus web3 in the context of AI models, is that web two is positioned to build vertically integrated closed markets. And whereas the web3 world, what's really happening here is because of the incentivization that's built into the protocol itself, what allows us to do is to capitalize on the collaboration itself by bringing more and more inputs from the community. Whereas basically the web two, to contrast, this has to control everything. And this is the big fundamental differences. And the way we're seeing that the rate of innovation, once you really get it right, the rate of innovation in web3 is very, very fast, right? Because where we are going is where these AI agents that are verifiable are going to be cross composable. That's going to be building up ever more and more and more things where a centralized company will just not be able to ever catch up because the crowd is always bigger than a single company.
00:20:10.194 - 00:21:08.004, Speaker D: To the point where how fairness goes into this. What's most important is the quality of data on which you train your model. And this is where really zero knowledge proofs and blockchain technology really comes in, because it creates a way to, on one hand, store already we at the point where we can store vast amounts of data, and yet this data doesn't have to be trusted to a single node and this is basically where the crux of it, and then from there we can, you know, extract verifiable, useful information which can be later applied to the training and inferencing more and more AI models, which are in turn are composable. And I think this is where we're going where how the AI is really going to explode in the next five to ten years.
00:21:08.504 - 00:21:10.124, Speaker B: Carton, what about you?
00:21:10.784 - 00:21:12.672, Speaker C: What about, what do you think?
00:21:12.728 - 00:21:16.192, Speaker B: Yeah, he represented the ZK side. He represents the op side.
00:21:16.328 - 00:22:16.548, Speaker C: Yeah, yeah, yeah. So like, I mean like aura is the team who like is the first invent and implement a library called OPML. So the reason why we do that, we kind of like try to use the op way to do AI on chain instead of use Zk, because we find out one important critical thing is that with the model size increase, your proven cost actually increase exponentially. So pretty much after your parameter, more than 2 million parameter, then it's pretty much begin to hard to be proof. So that's why I think the most practical way to do on chain AI is that at the moment is that you use some interactive proof to make sure the AI model run function correctly. And actually I just announced a paper called optimistic privacy preserve AI. It just released like two days ago that pretty much used the Zk to do two things and two things only.
00:22:16.548 - 00:23:11.298, Speaker C: The first thing is the privacy input, so nobody want another. People know what's the problem is in some sort of the case, right? For example, if this prompt has really good value, I don't want anyone in the ethereum to front run my transaction in the on chain AI inference. Then I will want to hide that prompt input. So you can do a ZK verifier there for sure, and it won't increase the computation cost drastically. And another part we can do a ZK on is like the final verification. Because in the interactive proof, essentially what you are doing is that you cut that inference trace into a merkle tree and then perform a binary search on that merkle tree. And then we do a ZK proof on this binary search so that you can instantly finalize any AI inference on one block on Ethereum mainnet.
00:23:11.298 - 00:23:21.786, Speaker C: Then I think that would be really powerful mechanism in the coming boot cycle when everybody needs AI on chain for some reason. And yeah, that's my take for the.
00:23:21.810 - 00:23:59.204, Speaker D: ZK part, just to go back and interject, well, going the op route, you're exactly not validating the validity of the dataset. And the problem that you say, well, you can only have limited set of like data, right in the ZK. Well, this is exactly what Maru solved today. We already have. It's like with innovation around data structure and basically an ability to aggregate around this data structure, you can have very fast and very large access to data that is fully validated in the ZK.
00:23:59.784 - 00:24:32.944, Speaker C: I just don't understand, to be be honest. I mean, the ZK is great, but the thing is that if you use a super long polynomial to prove some operator in the AI model when they function, then when you have a few billion parameter in the AI model, few billion operator, then what's the number scale of the polynomial you need for that billion level of the polynomial? Impossible, man. Probably you need to use the whole universe to become a computer to make it happen.
00:24:34.284 - 00:25:01.204, Speaker B: You guys can get outside afterwards. Okay, let's switch gear a little bit. So this AI blockchain hype is real. And how has this changed the funding landscape? And also, what do you think you have as advice for future builders? Michael, I know you just completed a big round here, so I want to address that question to you.
00:25:02.184 - 00:26:10.406, Speaker E: Yeah, what we've seen is that certain funds are dedicating specific theses just to the intersection of web3 and AI. So there's a lot of interest in the investor community at actually studying the intersection, fundamentally funding a lot of companies in this space and really seeing if there's practical use cases. Right now, it's still very much in an infrastructure stage from an AI cross web3 side. But many funds are dedicating large amounts of resources to fund companies in the space. And so my advice would be either build infrastructure that can really put autonomous agents on chain and make that a reality for millions of people, or start looking at, well, what are the applications that we can fundamentally build? When you have AI and a smart contract, or if you have an autonomous agent controlling your wallet and then being able to do certain actions, like, I would love to tell an agent, hey, pay this bill for me or manage my expense report and then pay it off. This would be really cool. But there's only a few people building this right now.
00:26:10.406 - 00:26:20.824, Speaker E: We need a lot more people building this, and there's lots of funding available. I know a lot of investors that would love to invest in that. So you can definitely chat with me afterwards.
00:26:21.484 - 00:26:28.812, Speaker B: That's awesome. And Sean, what do you think? I know you also did really well on your funding. What's your advice for future builders?
00:26:28.948 - 00:27:15.630, Speaker F: Yeah, that's a great question. I think for Sahara, when we're raising our crown, we've been having a very interesting game and sort of conversations with different type of investors, both web two and web3 AI investors. And I think they are taking obviously various different views on things. For example, web two AI investors, they were really looking at, for example, the usability and whether you have go to market products, how much revenue you're making, things like that, and use that to justify your valuation. And I think there's nothing wrong about that. But I think the gap here is in web3. When we're talking about blockchain power technologies, there's always this usability gap that we have to think about, good middle points to find.
00:27:15.630 - 00:28:54.708, Speaker F: If you're building say ZK and homographic encryptions, those are theoretically super cool things. I love them. But in terms of finding enough, exciting enough use cases or broad enough applications for these technologies, there's a lot to be think about. And I think that's where usually the conversations went into like nuance, right? Like you talk about some applications, many AI investors are super excited about video generations, let's say, and then retrieval, augmented generation of entire Internet, for example. And you want to apply say ZK and HG into these places and priority the conversations, if you go into lower level details really easily, easy to break. So my advice is really find your sort of battleground, right? Let's say, if you want to build fundamental infrastructure, stack for ownership, provenance, security, decentralized storage, compute, so on, prove that you could build very theoretical design, maybe developer facing toolkits for them to build some interesting enough use cases, right? But you could also come from a very application business oriented sort of perspective and trying to show, hey, I have some empirically nice enough approach that I can quickly scale up and push into products and launch and make super good user attractions, even revenues and so on. So yeah, find your battleground and just stick with your stance.
00:28:54.708 - 00:28:56.036, Speaker F: I think that's the best thing.
00:28:56.180 - 00:29:26.176, Speaker E: Yeah, I want to add maybe one more thing. You inspired me around it. I recently saw a super cool paper that's going to be released at a conference pretty soon, but it talks about middleware for llms. So Ilya talked earlier about rag retrieval, augmented generation. And basically what you do is you enhance a model by adding additional data sources to it. Well, what, instead of data sources, you add other models to enhance that model. So this creates completely new solution spaces.
00:29:26.176 - 00:29:40.124, Speaker E: Imagine you can create a DAO. Now that's basically a bunch of models together that are basically like an autonomous organization. So there's solution spaces that get super exciting about it. And I want to see builders explore that. That would be super cool.
00:29:42.224 - 00:29:43.080, Speaker D: Yeah, absolutely.
00:29:43.112 - 00:29:44.888, Speaker E: I just have to fully agree.
00:29:44.936 - 00:29:47.204, Speaker D: This is where the next big wave is going to be.
00:29:48.584 - 00:29:57.524, Speaker B: Alright, guys, if you're a builder and you're building in web3 and AI, come find these guys afterwards. They are experts.
00:29:57.944 - 00:30:07.896, Speaker C: Yeah. Check out aura IO. You can use the onchain AI today. You can build some product today on Ethereum today. All right.
00:30:07.920 - 00:30:08.764, Speaker B: Thank you all.
00:30:13.194 - 00:30:16.194, Speaker F: Nice. But you guys.
