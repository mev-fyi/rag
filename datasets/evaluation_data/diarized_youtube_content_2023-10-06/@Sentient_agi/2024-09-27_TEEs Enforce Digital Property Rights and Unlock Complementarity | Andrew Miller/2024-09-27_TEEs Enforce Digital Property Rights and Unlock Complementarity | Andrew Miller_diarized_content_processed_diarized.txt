00:00:01.280 - 00:00:35.851, Speaker A: So welcome. With that, I would like to invite our first speaker, Andrew Miller. Andrew Miller is a professor at the University of Illinois, Urbana Champaign, and he's a longtime collaborator and friend. Okay, excellent. Very happy to come here and basically spread the good word about trusted hardware. I'm Andrew, I talk about trusted hardware a lot. The thing that I am most interested in overall is unlocking complementarity where it otherwise would not be unlocked.
00:00:35.851 - 00:01:28.807, Speaker A: And that's bad for society. I want to achieve this, and my kind of campaign these days is to say that trusted hardware is the appropriate technology that's going to be able to do this. I normally work in the world of smart contracts, but I've collaborated with Pramod and Dawn a whole bunch and they've talked to me about the relevant things involving AI. And so I'm interested in drawing these connections from what I've learned in the smart contract space with trusted hardware to what the implication is for satisfying all of these goals, like promote explained. So I will give a tiny bit of background on what tees and trusted hardware are. So TES are trusted execution environments or trusted hardware. Some examples of this are a feature found in intel sgx and many Intel Xeon chips have this.
00:01:28.807 - 00:02:24.517, Speaker A: Many AMD chips have this as well. And it's basically isolated execution mode where even the root administrator of the machine or even the person in physical possession of the machine, like the cloud operator, are not able to tamper with or read the internal contents of the processes that are running in this isolated TEE execution mode. The other most important feature that these have is called remote attestation. So the idea is that the manufacturer, like intel, for example, when they create this chip with a secure execution mode in it, they also have a certificate public key. They endorse and make a signed certificate for this chip. The chip has in it a private key that allows it in turn to make certificates. So these trusted hardware modes are able to generate certificates that basically say this output came from one of these secure devices and from a particular program.
00:02:24.517 - 00:03:01.671, Speaker A: The hash of the program binary is referenced in this certificate. And this allows you to interact with a program knowing that it's running in one of these secure modes. What's important about this is that means that relying parties like end users, no longer have to trust application developers. Users can simply see the code that they want to be able to interact with from GitHub, compile it themselves. They can match that against this certificate before starting a session with some piece of secure hardware. And in this way they can see, yes, if I interact and send my sensitive data to this hardware. I don't have to trust the application developers, I don't have to trust the cloud.
00:03:01.671 - 00:03:44.949, Speaker A: I know that I'm interacting with a secure isolated session and only the rules of that program are going to get to operate on my data, and only those rules. It can't leak at anything else. It can't do anything beyond what that hardware does, what that program says to do. So I'm most experienced with working with this on Intel SGX and also Intel TDX is kind of a newer version of it. And this is a picture of an Intel TDX setup with one of the Nvidia H1 hundreds plugged right into the PCI Express slot. And one of the cool things about and relevant for AI is that the upcoming graphics cards and tensor processing units also have this remote attestation feature and isolated execution mode. So they can do this as well.
00:03:44.949 - 00:04:15.793, Speaker A: Yeah, next slide. And I'm inclined to try to. Okay, yeah, no animations. We can leave it there. The way that these work when these combine is that you can't use the graphics card without also using a cpu. And so you should think of the CPU as having its own remote attestation and a chain of certificates coming from Intel. You can aggregate then the remote attestation certificates coming from, you know, the several dozen of TPUs that you have in your thing.
00:04:15.793 - 00:05:13.585, Speaker A: Each one of those would produce its own attestation, which is say a chain of certificates with Nvidia at the top that would get aggregated and bundled with a chain of certificates with intel at the top. And you're relying parties before deciding, yeah, I'm going to send my encrypted model for training into the system. You get to check this chain of certificates and so, you know, you're talking to a secure end to end system of both the CPU controller and the trusted hardware for computing on it. Yeah, next slide please. Okay, so the main project that I've worked on while at flashbots is this tutorial on how to do trusted hardware for smart contracts. It's called Sura and It's like a 2000 line code that you can follow around the entire implementation, as well as simple examples like how to make a secure auction using this kind of technique. And it's really a merger of these two technologies, blockchains and trusted hardware, which are themselves very nicely complementary.
00:05:13.585 - 00:05:52.197, Speaker A: The basic idea is to get the trusted hardware able to act as this secure co processor for a public blockchain. So the idea is that the trusted hardware will have a private key in it. The public key for this is published on a blockchain. And now if you want to participate in an auction, what you can do is put your encrypted bid onto the public blockchain. It's encrypted to the key that only the trusted hardwares can access. And the trusted hardware, you know, only runs the fixed program that you know is associated with this public key. And it can follow the blockchain so that there's no, you know, equivocation on, you know, which.
00:05:52.197 - 00:06:21.465, Speaker A: What are the set of bids that are completed. So roughly the flow is that you encrypt your bids to a private key that. To a public key that only the enclaves can decrypt and the enclaves follow the instructions of the blockchain. You should think of it as like, there's a light client running in the hardware. So the trusted hardware is aware of the blockchain and follows its rules. So it'll only compute the, you know, second price for the end of the auction won't be able to do anything else, won't be able to leave in or out. It has to do what the blockchain says, but it's able to compute on the encrypted data.
00:06:21.465 - 00:07:00.715, Speaker A: And this is a very general approach that you can use for constructing, you know, secure computation on confidential hardware. So this is the smart contract world. Yeah, let me go next slide. Okay, so I want to make the couple of points just to fit into, like, where this can fit in with oml. So Pramod explained some of the, you know, ownership and enforcement of, say, property rights that you would like to be able to have enforced for your OML files. You describe some of the approaches to do this, which include cryptography, like obfuscation or fingerprinting. The first thing that you can do with trusted hardware is, yeah, one click forward, please.
00:07:00.715 - 00:08:00.385, Speaker A: Is to use defense in depth, which means not only will you enforce these property rights by embedding the cryptography and fingerprinting so that you can, like, detect violations of these, you can also have trusted hardware that goes along with it and as a backup says, you can't even query it more times, you know, than you're supposed or any longer than you're supposed to. If you have the timeout, your subscription's ended and the blockchain says your subscription's ended, then the hardware that would be containing this model will refuse to run it anymore because your subscriptions run out. One more side click, please. Other things that trusted. One more slide, too. Other things that trusted Hardware can do, I think, are expand the set of property rights and contract ideas that you can express with this way. So the way that I think of where this will go is that you may have the trained models exist in trusted hardware and remain in trusted hardware and never leave the trusted hardware.
00:08:00.385 - 00:09:05.665, Speaker A: And you will only be able to use them by queries that pass through the rules of contract enforcement. So that would enforce, for example, policies like you can only query this model a certain number of times and then the model is unusable unless you pay for another subscription or enforcing micro payments distributed to all of the people that contributed to the model on every use or in batches of those, and even imposing filters outside of the model, imposing them on the use of the filter. So it's a way to add safety there. And towards the thing that I'm most interested about regarding complementarity, these can also be a way of merging data sets or combining models in a way that respects the value and ownership of the individuals who started off with the valuable components that you want to merge in this way. So the flexibility of this trusted hardware and the fact that it's likely to be ubiquitous in server processors and all of the things that you'll use for training and evaluation of powerful TPUs that's going to enable these sorts of concepts. Yeah. Next slide, please.
00:09:05.665 - 00:09:50.795, Speaker A: I want to talk about this notion of complementarity. So the main thing that's motivating me is I think that there are many opportunities to get a benefit from combust binding information that starts from different places and each one has some weakness or incompatibility that would be offset by the alternatives. And the only reason why you don't just see these mergings all the time comes from a couple reasons. One is not knowing where your weaknesses are and other people's strengths are. So it's knowing who has the complementarity that matches you. And the other is that you would be reticent to just send your model to one person to merge it because you want to know that you're getting the rights respected of the value that you bring and contribute to it. Yeah.
00:09:50.795 - 00:10:45.397, Speaker A: One more slide, please. I can also. Yeah. Okay. So if we can resolve the problem of how can you make it safe to aggregate without the ensuing loss of control and loss of value capture that's required for that, then you can square the circle and make it, you know, effective and you'll get to this benefit where we're able to get the benefit of strong models by aggregating all of our data and trained Models in a way that is still, you know, incentive compatible and desirable for both of the, you know, however many parties to participate in that way. Yeah, Next slide please. Okay, so I think a high level flow of how this might go is that there's an importance for the trusted hardware and the need to ensure security and rules following the way that the trusted hardware can do at many stages of this.
00:10:45.397 - 00:11:42.687, Speaker A: So one of the first ones, and I'll talk about just in a second, is the idea of where original input data comes from. So one example is importing. A lot of the value unlock will come from importing more and more sensitive data, especially the kind of data that are in private accounts, private data that you have which you otherwise would not be interested in sharing broadly. But if you can make a secure way of encapsulating it, then that would be a way that you can get it into a system where this aggregation can occur while still knowing that it's kept safe and not being used outside of the ways that you want. One of the steps that's most important is being able to find who in the world of other data sets and models has data or techniques that complement yours. But of course you don't want to give away all of the things that you have in order to find this. So this is, you know, we solve this in the mev world with forms of auctions and negotiating gadgets like that.
00:11:42.687 - 00:12:25.975, Speaker A: So I'd expect the same kind of thing to happen here. All of this goes much better if it can take place in trusted hardware because you have the ability to enforce limits on how the data is used and say, you know, you won't take what you learned through this negotiation and use it for other things outside the negotiation if it fails. Many of this can be done on any trusted hardware platforming and I most experienced working with sgx, I think a lot of the applications that are going to be most interesting, especially for AI, involve huge data sets and really expensive computation. So it's going to be very important to use the newest. It's good that the most powerful processors for this are increasingly supporting this kind of trusted hardware technique. So it'll be available for that use. Yeah.
00:12:25.975 - 00:13:23.899, Speaker A: Next slide please. Okay, I want to mention this project about using trusted hardware to get more use out of Web two accounts in a kind of surprising way. This is a demo from Shin and Ryan and you can play with it teleport best and with one click at a time we'll explain what it does. The basic idea is that this is going to allow you to Take your Twitter account and generate a one time limited use, right? So Twitter has a way you can delegate read access to an app or read write access to an app. But if you have write access, you can post anything. You can change the profile photo and you can change this indefinitely. What if you wanted to allow someone to make a post to your account but just once and you want to impose a constraint on what it would be? So what we have here is that you can write a description of a filter and create a one time use delegation for your account.
00:13:23.899 - 00:13:59.869, Speaker A: And this is all made possible because what you authentic, what you authorize is a program running in a trusted hardware. You can check by the remote attestation that that's the only thing it can do is one time use and only after it passes this filter. I have the example must contain a check move. We have one click. All right, so you have to authorize something. And what you see with Twitter is that this authorizes you to post for you, right? Just post tweets, not post tweets with a filter, not post one tweet, but just post tweets. But before approving this, you would check the remote attestation you're actually interacting with a thing that imposes the limit that you want with this filter.
00:13:59.869 - 00:14:20.457, Speaker A: One more click. Okay, once you click this, it mints you an NFT behind the scenes. You get a URL that you can share. That URL is the capability to do this one time post. Next slide. This is what it looks like to load the page and then try to do it. Of course you can write your tweet there.
00:14:20.457 - 00:14:32.291, Speaker A: Next slide. This will be my. All right, there's my favorite chess move. One more click passes the filter. Now this is an nft. So you're basically interacting with the blockchain. That's what makes it one time use.
00:14:32.291 - 00:15:10.875, Speaker A: It's the blockchain that's enforcing the only once. So you use a wallet sign the transaction that is choosing what to put there. One more click, transaction gets mined, of course, one more click and then it gets posted. But that act of posting, it consumed that one time use and it only got through because it passed the filter. The trusted hardware enforced all of this. And this is how you can take an ordinary Web2 account with extremely limited delegation ability and make extremely fine grained, essentially arbitrary contracting ability for the sensitive data that you've authorized. Yeah, next slide please.
00:15:10.875 - 00:15:50.385, Speaker A: Okay, so the summary of this is, right, this technology of being able to use trusted hardware makes it possible to empower users to do a lot more with their sensitive data than they would otherwise. If you have like a whole legal team, then you can write really great contracts, but you need a legal team to do that. That's out of range for end users. You can always just share your password and authorize an app to have root access to your account, but that's giving up all of your power and access to your data entirely. So the appeal of this trusted hardware is that it enables users to enforce arbitrary contracts over their sensitive data. And that's the key unlock there. Yeah, next slide.
00:15:50.385 - 00:16:42.651, Speaker A: Okay, the main claim here is that this, I think, enables a really great source of at least input data to models. There's a lot of sensitive data that users will not quite so openly share with anyone to harvest. But this is actually the data that's most interesting because that's the data that hasn't already been mined for all of its utility by frontier models. And so being able to provide a safe way to encourage users to be able to contribute, this is going to be the thing that unlocks the ability to have the best possible utility from the benefits of this aggregation without the downsides. All right, next slide. Okay. Okay, this is now out because it's back to just examples of what are interesting with these different accounts.
00:16:42.651 - 00:17:12.439, Speaker A: But okay, so I gave you this example of Twitter. Clearly a utility case of this is you could sell tweets to your account for ad advertising space. All right. Other things that might be appealing to people is this can enhance peer to peer trades. If you want to pay someone on Venmo, you can't just accept random payments on Venmo because they might cancel the payment. But you could use this contracting to, for example, guarantee that you make a payment and have waived the ability to cancel it. Even if that waiving the ability to cancel isn't one of the features that the original app even provides.
00:17:12.439 - 00:17:26.955, Speaker A: So maybe what's even more exciting is that this is like adversarial composition. You can make these without even requiring the original services to come up with their intent to support it. We can just bring it and impose it on them. Okay, that was it. Thank you.
