00:00:00.160 - 00:00:31.354, Speaker A: We're going to get right into things. So our first keynote today is titled from attention to why AI needs web3. The keynote will be delivered by Ilya Palosikin. He's a co founder of near. He's also previously an engineering manager at Google Research, where he co authored attention is all you need, generally seen as a fundamental research piece within this field of AI. So I'd like to welcome to the stage now. Ilya, take it away.
00:00:33.614 - 00:01:11.494, Speaker B: Ilya, hello. Hello. Awesome to be here. I hope this is a good kickoff for your Denver endeavors. All right, thank you. Cool. So, yeah, as was mentioned before, blockchain, for about ten years, I was actually working on machine learning, deep learning, and worked at Google Research, pretty much trying to advance state of the art, and we'll talk a little bit about it.
00:01:11.494 - 00:02:48.838, Speaker B: And I think one of the important works of this generation in AI has been attentions all you need, which a team of us have authored. So for those who are not familiar, the kind of language modeling is not a new problem. This is something that been kind of used for various functions since 1950s. It's a statistical method, and it's kind of used for like in different natural language processing tools, even before deep learning. Now, with introduction of this concept of embeddings, where the words are mapped into vectors, into numbers, is kind of made a leap because where before you had a very symbolic processing, where you needed to kind of have all the world's words and their mapping, kind of statistical mapping, be very fixed, with pretty much transforming words into numbers, you are able to have kind of math applied to them, you're able to measure distances, you're able to project them, you're able to transform them. And so that's been one of the kind of core innovation that started a lot of the modern natural language deep learning, and kind of the parallel technology that have been existing for a while as well, since nineties called recurrent neural networks, was the way people used for language modeling. So back at Google, we actually had trained models that would answer questions, translate that were not transformers.
00:02:48.838 - 00:04:08.448, Speaker B: The challenge we faced, though, they were very slow, they were kind of similar to how humans read one word at a time. And if you have large documents, if you have large Internet pages or large text you want to translate, it's too slow. At Google, you need to respond within a few hundred milliseconds. And the idea of transformers, the origin of it, is really trying to strip down and change how we process language to really paralyze it, to leverage the computational architecture we have in GPU's to really have kind of low latency way of reading large amounts of text. And it's really pretty simple model. It maps words into vectors and then transforms them, and then uses this algorithm of attention where it pretty much tries to gather information from everywhere else in a document about what this word means in this context. And so, repeated a few times, it actually captures a lot of the information that's in the document and is able to answer questions, it's able to translate them, it's able to do all of these functions we see.
00:04:08.448 - 00:05:00.224, Speaker B: And it was really cool to see that this same algorithm, the same machinery was then applied to images, to videos, to DNA, to all kinds of other methods. And any type of sequence of spatial information is able to transform like this. So really exciting because of its kind of natural scalability. We've seen explosion of usage of this as models become bigger and bigger. Now, this kind of led to massive innovation in AI. We see probably every company is trying to put AI in somewhere in their kind of product. We have a lot of new startups that are kind of doing new use cases and trying to transform the way you interact with computers and with each other.
00:05:00.224 - 00:06:14.090, Speaker B: There is though the opposite of this, right? If before the computing was interpreted by the user, by the person, now we have a new way where computing itself can communicate with others. It means that the computers now can produce content, they can interact, they can make decisions, and it's not always clear if it was a human or computer that's behind it. And so that's one of the important parts to discuss when applied to web3. The other important part is what we see is the rise of this kind of companies, private companies that are offering AI models, AI use cases. And the problem is that as any kind of company, you have a lot of different teams that are actually involved in how you make a product. You have a curation team that decides which data set goes into training the model. You have some legal team that is deciding that, oh, we cannot use this data because it's licensed under this or that license, or we got turn down request.
00:06:14.090 - 00:07:18.266, Speaker B: They have an ethics team that a lot of companies now establish, that is deciding what's ethical or not, which is very, very subjective. All of that is actually influencing at the dataset step and at system post processing step, what really result you see. So instead of seeing what model actually has, what data is telling it, it now is massively kind of influenced and biased based on all of these different people involved in the process. And to make it even scarier, I don't think anybody's doing it yet, but I expect this to happen very soon, is that you can have ads running directly when you kind of do inference in this model. So instead of seeing what model is telling you now, it's kind of for sale and highest bidder is able to influence the output. Imagine you asking, which car should I buy for my use case? And instead of whatever statistically is better, it actually, whatever somebody paid more to output that car. This is totally possible.
00:07:18.266 - 00:07:39.406, Speaker B: It's actually ads. Marketplaces are way faster than the model inference. So it's actually very effective to do this right now. And so this is kind of the scary part for closed AI models. We also don't know what data goes there. There's a curation that happens. But then there is another side of this, which is adversarial data kind of manipulation.
00:07:39.406 - 00:08:32.238, Speaker B: Right? Right. Now, if you're a, let's just say an organization or a government that wants to influence how people think in a year, the way to do this is to start seeding data into all of the public forums, pretty much with like, with the text and opinions and decisions that will actually change how these models will be trained in one year, when and this data will be scraped. So we have kind of this like a very chicken egg problem. And all of this is hidden from the user. You as a user, when you're typing into the prompt, you have no idea what's happening behind the scene and what fed to it. So this is a case for open source AI. This is a case for having the models open source the data it's trained on making sure you are able to run it yourself, and making sure you have an ability to kind of inspect every step of the way.
00:08:32.238 - 00:09:19.714, Speaker B: Now, obviously not everyone will do it, but as we learn with blockchain, having that ability, make sure that we get higher quality of product, make sure we have a competition, and make sure we don't get. Anybody will point out if something is not working. So we have lots of potential risks. We have risks around regulatory. Right now, there is a us regulatory kind of order around how much computation your model can have before you need to report it. Having a team, ethics team that will be deciding a safety team, that will be deciding on things. They are considering doing a dual use for AI technology, which would pretty much make it so you cannot export it to other countries, which makes no sense whatsoever.
00:09:19.714 - 00:10:25.866, Speaker B: This is actually reminiscent of crypto in nineties, where cryptography was potentially considered dual use. And the way to disprove it was a person printed out an algorithm on his t shirt and crossed the border and pretty much argued that this is freedom of speech, to actually disclose the algorithm and not funding and exporting kind of dangerous technology. We obviously also have kind of the negative sides, right, because these tools are so good and so easy to use now to generate kind of fake information content. There's a potential for massive disinformation, massive sways and manipulation. And so it's really important to address that. And there is kind of this really strange trade off that we've seen where big companies have more and more resources to invest into building better models into more data. And we have open source, which doesn't have access to compute, which doesn't have access to buy data, and kind of at odds with its incentives.
00:10:25.866 - 00:11:07.382, Speaker B: And so again, this is where web3 can be extremely helpful. So I'll just quickly cover. This is like a first version of AI stack using web3. It splits into three major categories. One is around data and kind of data curation, content, reputation, personal data. One is around infrastructure with inference, provable, training, foundation models, kind of on edge computing and retrieval, augmentation and applications, variety of applications, people building on top of this infrastructure. So one of the important dimensions, I think we talk a lot about decentralized AI.
00:11:07.382 - 00:12:18.754, Speaker B: There's a lot of talks about this, but there is another dimension to this, which is, do you have a tool or you have an autonomous agent or autonomous system? This dimension defines also the requirements you need for infrastructure. So specific example, we have centralized examples like chat DBT, which is a tool, you use it, you interact with it, you tell it what to do, it's fully centralized. You have things like Agent GPT, which use chat GPT behind the scenes, but they are trying to build a plan and execute on your behalf. Now, on the things like Lama, they're not centralized or decentralized. It's just an open source software that can be used for different cases. It's really extremely important to have such models now, I think really important aspects and why we've been doing edge intelligence is this idea of having models run on your device, because it solves a lot of problems. You don't need to prove things because it runs, it can access all your private data, you don't leak any of the information and queries that you use to anyone.
00:12:18.754 - 00:13:11.420, Speaker B: And so it actually allows to have private intelligence tooling for yourself without really getting into any of the hard problems at the same time. The opposite of this is AI governance, or doing on chain finances. This is where it is an autonomous agent that has a mission that continues trying to improve, refine, kind of follow its objective. And this is where you do need provable inference, you need a lot of infrastructure to power that. And kind of, there's a lot of projects that are working on that. So just to mention like one of the kind of examples we built for the hackathon that's going on is AI agent registry. It's the idea that you can have kind of what GPTs for OpenAI is a limited to the open AI model have for any model, including your local models.
00:13:11.420 - 00:14:11.454, Speaker B: It can be integrated into any of the front ends, it can be served locally, it can be served from the website, you can have a desktop app, and you can have kind of a variety of different inputs show the different types of agents. So right now I think people kind of use agent very freely for anything that has AI in it. And so I was trying to structure a little bit the different types. And so one of the kind of types, like the differences between agents is how they specialized, right? Like we either specialize it by giving it a prompt. So like what GPTS has been doing, you kind of condition it with a system prompt and then it responds in a very specific way. You can have a way to fine tune the model, giving it a specific training, examples how to respond, maybe doing an o, f, h on it. And you can have additional information pretty much retrieved and augmented in the prompt from specific contexts.
00:14:11.454 - 00:14:58.822, Speaker B: Now a different dimension is what kind of output the model provides. Right now, most of the output we see is text, which is not how we've been used to using computing. We used to having an interface, we used to have rich UI with actions directly in there. And so one of the cool things in the registry I showed you before is you can specify how the output is shown, so you can have model condition to output JSON, which then feeds into a rich ui. And now you can have, instead of, for example, you're asking schedule me a flight, instead of having a bunch of wall of text. You can have a really nice UI with a plane flying and time and everything shown. And finally you can have action, you can have something that does direct action.
00:14:58.822 - 00:15:41.534, Speaker B: It's in the context of your blockchain account, of your potentially web, two accounts, so it can actually execute all that on your behalf. Finally, you have an autonomy. So I mentioned that dimension. It can be a tool that executes something. It can be something that you tell what to do, but it figures out how to do it, the plan and executes it, or it can be a fully reinforcement learning agent that continuously given a objective, for example, maximizing a specific KPI, is continuously trying to improve and trying different actions. So this is just kind of systematization I'm starting to use for the agent types. And obviously it's not final and we'll continue expanding that.
00:15:41.534 - 00:16:51.748, Speaker B: So I mentioned that we'll see a lot of content that is produced by I that will be misleading, confusing, manipulative. And so one of the really important pieces is actually having a layer of content reputation. And we have cryptography, we have the tools to actually enable that. And so one of this kind of asks for the hackathon as well is actually starting to build out this network where we can actually verify, when you're looking at an image and a video and even pieces of text, who published them, where the origin is. And potentially you can have community notes and kind of community curated feedback. If this is misinformation, the simplest example actually I was talking with a journalist and they mentioned that the misinformation for them is actually not about AI, it's actually somebody taking, modifying the HTML page of the publication, taking a screenshot and publishing on Twitter. And they got a ton of shit for whatever was published because people thought it was real, like simple things like that.
00:16:51.748 - 00:17:42.698, Speaker B: Is that right now the information you see you kind of automatically attribute to whatever you see. You don't actually question the source of where it's published. And so having additional layer that hey, this image actually is fake. And here's community node and you can go to origin and see if it's not correct. And being that surface directly when you're browsing Internet, when you're on social media is extremely important. So another use case is kind of, I think that we will see a transformation of how we're using applications and how applications are built in general. And this is really kind of why actually I got into AI early on is I wanted to automate software engineering because I believe the way we right now interact with computing is very much fixed.
00:17:42.698 - 00:18:37.874, Speaker B: Kind of in early days how we program computers, but with natural language interfaces we're able to actually define what we want, what we expect, the intent, and we have computers to actually generate us the UI and the experience we want. And so I think we'll see a transformation of this happening. This is example from Ejutsu, an ide in your ecosystem that already offers generative UIs and generating front ends for developers to kind of really accelerate their development speed. But you can go beyond that and see how you can just generate application for you full end to end and use it, and you will have custom applications for your use cases, and you will use them even as non developer. Finally, there's a really interesting space of intelligent assets. We started with programmable money, programmable assets. That's what blockchain enabled.
00:18:37.874 - 00:19:13.678, Speaker B: It enabled new property of ownership. But you still need to write code, and this code is a strict set of rules that this assets follow. Intelligent assets now can behave kind of based on natural language input, based on kind of inputs from outside of the blockchain itself. Now I'll caveat it, and it's a huge caveat. The current models, the current llms are not designed for adversarial environment. Right? When we design smart contracts, we design them that somebody will try to break in and steal all the money. And so we will need.
00:19:13.678 - 00:20:23.352, Speaker B: And there is research happening on how do we strengthen llms to be able to work in such an environment where potentially the inputs are, the inputs given are trying to kind of steal the funds. And it's really important for governance later as well. So kind of up leveling that, right. You can have the agent itself to run a business. You can have something that is autonomously trying to optimize for the business KPI's and making decisions on what projects to do, who to hire and what work to do. And there is kind of a very specific, like a zoomed in version of that, which is a kind of gigs work marketplace where you can have work posted almost like an order book and AI's and people can pick that work and do it and have kind of embedded verification loop. So the blockchain really facilitates this unification where right now you either go and use the tool or you post it on some freelance website and there's a person come in and start doing things.
00:20:23.352 - 00:21:07.580, Speaker B: This really can unify this, because now you don't really care who you're on the other side if the quality work is well. And so this will actually transforming as well how the work is done broadly in the ecosystem. And finally, the final piece of this is governance. With governance, we have a very kind of fundamental problem. When we elect people to be representatives, these people always have their own interests. It doesn't matter how kind of like they trying to. And so this is called principal agent problem, where the agents we select as principals have their own interests and they're not always aligned with us.
00:21:07.580 - 00:22:00.728, Speaker B: And the fundamental way to address that is to actually have the agent to be AI agent that does not have personal interests. It follows the kind of algorithms or rules and is fully auditable, and you know, what is the inputs and outputs are. And so the idea is you can have a reinforcement learning agent that continuously trying to maximize the KPI's of the, whatever you're governing. It can be a dAo, it can be a protocol, it can be layer one, it can be a city and a country. And it's trying to model the system, the world that it interacts with to understand what is the best actions it should take to improve to maximize the function. Now, that's a long term project. It requires more robust llms, it requires better world modeling, which we see evolving as well.
00:22:00.728 - 00:22:40.944, Speaker B: But it's really interesting because this is fundamentally for this to be useful and trusted. It cannot be run by a centralized company. Nobody will trust an OpenAI running governance for, let's say, uniswap. But if you have provable inference, if you have open source models, if you have all the systems running, you're able to actually transition to community stakeholders, to be kind of overseen, an AI model that makes decisions. And so the bold vision is we can have an AI president. And again, it can be on different levels of the governance. It can be as small as president of a DAO and as big as president of a country.
00:22:40.944 - 00:23:25.988, Speaker B: So I just want to finish on a kind of important note. The AI is, you know, it's a set of tools, it's instruments. They're very powerful. They advance in really quick, but they have this fundamental issue. And in general, the open source has a fundamental issue right now that it's kind of more and more controlled by big companies, which you know, as web3. We're kind of trying to disrupt and create more ownership in the community. And so the benefit here, we can create incentives, we can create a systems that are really promoting open source, that are benefiting people who are actually building useful things and contributing them back and have this platform that available to everyone.
00:23:25.988 - 00:23:51.334, Speaker B: Intelligence, especially, like, the more and more the intelligence is becoming powerful, should be available to everyone. It should not be kind of limited to companies that are deciding what we can access and what decisions it can make for us. So I just urge everyone to think in this way to really contribute to this space. And yeah, I'm excited to see how we can all together build a more open source AI.
