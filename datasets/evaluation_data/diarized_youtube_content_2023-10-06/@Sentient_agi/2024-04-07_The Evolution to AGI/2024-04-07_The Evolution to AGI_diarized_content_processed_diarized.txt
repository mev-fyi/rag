00:00:01.720 - 00:00:40.938, Speaker A: This panel is titled the Evolution to Agi. We'll be welcoming back Ilya onto this panel. We will also have Pramod Vispinoth. He is part of sentient witness chain and also a professor at Princeton University. And we'll also be joined by Sriram Khanan, who is the CEO and co founder of Eigen Lair. Come on stage, please. All right, so, before we actually get started, I wanted to get a bit of attempt.
00:00:40.938 - 00:01:05.352, Speaker A: Check in the room here. So this panel is about Agi. And if you'd humor me, raise your hand if you are optimistic about a future in which Agi is achieved. So the optimists out there. Okay, lower your hands. Are you see where you are here? Raise your hand if you're pessimistic about that future. The brave soul over there.
00:01:05.408 - 00:01:06.344, Speaker B: A couple more.
00:01:06.504 - 00:01:11.872, Speaker A: Okay. So it's a pretty friendly room. That's helpful for us.
00:01:11.968 - 00:01:15.804, Speaker C: Those who pessimistic came to destroy it here. Is that.
00:01:18.104 - 00:01:27.264, Speaker A: All right? So, Ilya, I want to start with you. Did you know what you were doing, what you were unlocking when you published the transformer paper?
00:01:29.564 - 00:02:40.984, Speaker C: I mean, definitely not at this scale, right? I think, like, you know, as you work in research and you have, like, kind of day to day improvements, it doesn't feel like major breakthroughs, because it always is very incremental, like I was covering. But the basis of this is we were using very basic models, like bag of words models, to run stuff in production, because you cannot run recurrent neural networks in production. It's too slow. And then the attention was another mechanism, and a team that Jakob was leading, another co author, was using attention for query similarity, and they were like, well, why don't we use attention and beg of words and use it for sequence generation? And it's like, well, it's a crazy idea, but let's try it. And so, obviously, there's a ton of work went into actually getting into state of the art, getting it to scale, all of this. And obviously, OpenAI did an amazing work at scaling it up and showcasing what it can do. So I would say it's a lot of incremental work that compounded into a fundamental change.
00:02:40.984 - 00:03:35.174, Speaker C: But as AI researcher, you always actually wish for this thing. And I remember even before transformers, we were discussing use cases that some of the models were doing. We had language models that can read text, and we actually had even our own Wikipedia that was fully generated out of a language model. Obviously not at the quality that's right now. But I was like, can we publish it back to Wikipedia? And the legal department said no. But this is kind of things we were thinking about, like, oh, it would be really cool to have language model, the question answering model directly in your browser that knows every page you're visiting and stuff like this. So I think the wish and vision is always there in AI space, and it's more that we're continuously trying to improve the tools to get us to what we think is possible.
00:03:35.774 - 00:03:48.582, Speaker A: And so on that point, I think it would be helpful for the rest of the conversation to get a little more specific with what we're talking about when we say Agi. So, Pramod, what does AGi mean to you? Can you sort of put some constraints around it?
00:03:48.718 - 00:04:12.964, Speaker B: Yeah, thanks, Sam. I mean, Sam started by asking, who's optimistic and pessimistic? Can you hear me? I think it's good to first define AGi. And people have intuitive definitions. They think they mean common sense. They would think that it's self replicating. It can replicate itself. It can start spawn off other agents, spawn of other intelligences.
00:04:12.964 - 00:04:45.738, Speaker B: They're all sort of heuristics, and they're reasonable. I mean, they're one ways to do some kind of litmus tests on what AGI is, but to get a feel for. So here's sort of a definition that I have that helps me think about what AGI is. It's the union of intelligences. So GI is the union of intelligences, but it's more than that. It has to self replicate. And how does this, where does this intelligence come? And evolution already has an answer, which is that you, I mean, that's the birth of GI itself without the AGI part.
00:04:45.738 - 00:05:35.660, Speaker B: And that requires putting some basic building blocks, hydrogen, carbon, oxygen, under an incentive mechanism, and then you get life and it evolves. And evolution is that process which leads to general intelligence. And if you follow this logic, the equivalent for AGI, at least there's a definition that comes out, which is you need to have intelligent agents, intelligences that interact under short term. So some incentives, quickly, I get a meal, I make some money, my wallet gets up, air drops. And evolutionary pressures, these are longer term. You succeed and you have future generations, both these pressures you put together. But these intelligence should interact in a free way, just like in evolution.
00:05:35.660 - 00:06:19.450, Speaker B: And this free, composable and free means sovereign. Like you put in the execution, you put in the DNA, and then in this case, a contract, it just executes. This platform is crucial to have Agi to be born. And you already see it in the sort of the latest AI models coming out. Sora was already built out of some forms of simulating worlds, and even GPT four had some feedback with human feedback to improve alignment. So you already see that AI itself needs some interaction and getting the feedback to improve itself. And AGI is just the summation of all such intelligent agents interacting.
00:06:19.450 - 00:06:47.854, Speaker B: Well, what platform allows that internally? Big web, two companies, AI giants. Try it out. But you want the entire summation of intelligence to interact. Well, there is already a platform for that. It's called blockchains. We don't think of it in this way, but blockchain is the platform where agents can interact incentive driven, sovereign way. And so it's like blockchains are a necessary condition to bring out AGI.
00:06:47.854 - 00:06:50.010, Speaker B: So that's how I define AGI.
00:06:50.042 - 00:07:14.614, Speaker A: Fantastic. A lot of what you said was very optimistic and exciting. Sriram, I don't know if you were an optimist or a pessimist, but could you talk a little bit about some of the challenges or the risks that you see with achieving AGI and what role, if any, does blockchain play in addressing those risks?
00:07:15.554 - 00:07:51.604, Speaker D: Yeah, I'm neither an optimist nor a pessimist. Vitalik wrote this post with two forks in the road. So you have the two forks. One fork could lead to dystopia, the fork could lead to some semi utopia. And I think it's always like that, always choice and discretion and ability to influence the outcomes. So either an optimist or pessimist means you already kind of assume the outcome. But I think we have to actually act to get the outcomes that we want and deserve.
00:07:51.604 - 00:08:53.484, Speaker D: So the question is, what are the kind of bad outcomes that could happen with Agi? So the first thing is one way promote defined AgI as a sovereign entity. What do we mean by sovereign? One of the most fundamental lens to think about it is, is darwinian evolution operational. What is darwinian evolution? You have an entity. That entity can create further entities, which can then create further entities and be subject to a selection pressure, some fitness pressure from the environment. So this is a darwinian evolution. And how do we see darwinian evolution in Agi? You have to imagine a far out world with robotics and other stuff, self assembly, creating new robots out of materials, all of this stuff. But the proximal way, that's a distill.
00:08:53.484 - 00:09:15.794, Speaker D: The proximal way in which we are going to get this is sovereign digital AGI. What is sovereign digital AGI? It is purely existing in the digital space. It doesn't have any physical manifestation. It exists in the digital space. Where does it exist? It exists in the cloud. It exists as software. But software which holds a wallet.
00:09:15.794 - 00:10:07.652, Speaker D: Crypto. Right? Wallet holds a wallet, uses the wallet to go pay for its stuff, compute, you know, several people here building decentralized compute for AI's sovereignty, GI are going to come in and pay for itself, training itself and improve itself and earn more. Darwinian evolution started, right? It's very important that this, this can happen only in crypto because crypto has censorship, resistance and permissionlessness built in. There's no account access control. You can't distinguish whether a human is accessing or an AGI is accessing either for opening a wallet or for opening a compute connection. All of it on equal footing. Okay, so that's the definition of AGI.
00:10:07.652 - 00:10:42.012, Speaker D: And the proximal way this is going to happen in two to three years. My bet, we are going to have sovereign digital AI's which have some amount of survival advantage. You know, they farm for air drops, they figure out arbitrage opportunities. They do all the things we do just a bit better and they're going to have survival advantage. Okay, maybe they figure out how to hack your computer and then earn tokens immutably. Right? Blockchains. Immutable.
00:10:42.012 - 00:11:12.074, Speaker D: So Aga has a bitcoin. Aga has a bitcoin. What are you going to do about it? You can't. We're going to have to start thinking about sanctions. You know, people think about international sanctions. We're going to need AGI sanctions. How do you sanction an AGI, which is going rogue, hacking everybody's computer? What do you do about it? So there are all these problems, okay? Now, like, you know, we are talking about a fork in the road.
00:11:12.074 - 00:12:15.772, Speaker D: So I must have some alternatives here. Otherwise, you know, this is not good. The high level thing is when we are looking at this kind of a. This is proximal, right? This is sovereign digital Agi, not, you know, physical AGI, which requires robotics and self replication and maybe at least 30 years out. Okay, when we're talking about Saur and digital agi, what sets of things can we do? How do we build better coordination mechanisms for us to express our preferences into these systems? For example, I just mentioned sanctions against digital AgI. How do we express these, you know, already we are facing this in a non, you know, Agi manner. In crypto, for example, we see somebody's contract get hacked and we sit around hopelessly that, oh, my God, you know, they lost 300 million not being able to do anything when all of us know it's a hack.
00:12:15.772 - 00:13:14.298, Speaker D: It's not an intended consequence of the blockchain. Blockchains don't have enough expressivity for us to coordinate around the outcomes that we want. How do we get there? So I'm just placing questions, not giving answers, but important questions, to make sure that when we build sovereign AgI, you have all these things taken care of. Of course, even more proximal than a sovereign, digital AGI is a corporate AGI, which is, how do we ensure that AGI is not controlled by a couple of corporations? How do we ensure that? You know, the optimist answer to this is the following. If you look at the history of innovation, we see that innovation is not individual centric. Innovation is evolutionary. So if we can build a framework for permissionless innovation, where innovation can cascade one on top of the other.
00:13:14.298 - 00:13:50.864, Speaker D: I build something, you can build something on top, somebody else can build something on top. While all the people who are building things are incentivized, we can actually get really, really high rates of innovation, which can beat the corporate rate of innovation. A single company doing stuff on their own. So how do we build incentive mechanisms where people can come and innovate together? That's the important question. How do you build systems where we can express complex conditions of coordination? That's what we as a blockchain community, need to do to enable the optimistic side of Agi.
00:13:51.604 - 00:13:57.156, Speaker A: Ilya, I saw you smiling a bit during that. I wanted to give you a chance to respond if you have any thoughts.
00:13:57.340 - 00:14:27.968, Speaker C: Well, I think maybe high level point. I would say that as a humanity, we don't actually want a fully sovereign AgI running around taking our airdrops. What we want is a tool that is either personal or a community that expresses our interest, that is working on our behalf, that is contributing to our well being and success and whatever the.
00:14:28.016 - 00:14:28.764, Speaker D: Kind of.
00:14:32.904 - 00:15:15.366, Speaker C: Like, even for the example you used with sovereign Agi, the reality is actually, it's not sovereign. It relies on the governance of the underlying blockchain that it's running on. And if actually we have AI running, for example, on bitcoin, and we really all decide that we don't want that, the community can actually censor it and disable it and do whatever. So there is a governance framework for community to interface with that. It's not very expressive right now. We should make it more expressive. But I think the main important point, I think, for me, is that what we want is an agent that represents us and acts on our behalf.
00:15:15.366 - 00:16:20.520, Speaker C: Again, be personal or community, or organization or country, or whatever the group of people is and represent their interests and act on their best kind of interest and optimize that. I think maybe just kind of stepping back, the kind of definition that people in AI space use around AGI is really about that it's actually better. Like if you find a best person at any particular skill or act, it is better than that, right? And so compared to the narrow, like artificial narrow intelligence, which is it's better at specific type of thing. And this we know, like AI being good at or better than human at different things over time. And we just see this explosion of spaces where AI is now better at that, like really closing the gap across different dimensions. And again, that definition is really very utilitarian. It's not like, hey, we want a sentient being that is like, we want to talk to and find feelings.
00:16:20.520 - 00:17:54.122, Speaker C: It's more like, hey, we want a tool that is actually doing stuff we want, but better at scale instead of us. And that's really, I think in some way AI is like a mirroring representation of humanity and that interest. And I think to me, the evolutionary part here is really about humanity kind of becoming one with this AI tools and augmenting intelligence and becoming more individually and globally more intelligent and more sophisticated, more kind of advanced because of these tools and because of the way we kind of start interacting with this computing and web3 in this sense is a tool to facilitate this, to kind of create this communication and for communities to define what is interest they are and kind of provide feedback and give ability to execute on behalf of this whiz wallet with kind of deploying resources and at the same time to create incentives for this permissionless innovation, which I totally agree is probably the most important to kind of close the eye. Evolution, which in my talk I was talking about is very dangerous, has a lot of downsides. And we see past week kind of showed a lot, a lot of issues. Just like I think the fundamental point to understand there, there's no good answer, there's no way to have like, because data is always biased, there's always bias in the data. It's either to whatever majority is, or they try to fix it and unbiase it.
00:17:54.122 - 00:18:21.060, Speaker C: And now it's even across minorities, and now there's a different problem. There's always going to be a problem. And so you kind of need to move away from that, a single party controlling the decision making and allow people to have full control and then through that be able to communicate that. Anyway, I think that's where Web three is really becoming a powerful tool and to facilitate that, I want to respond.
00:18:21.172 - 00:19:01.754, Speaker A: And this can be for the group, maybe a provocative question, but are there any pros to centralization in achieving AGI and then maybe controlling it, putting guardrails around it, is a potential upside to having a centralized authority be able to avoid a diffusion of responsibility if something gets out there that is not all that positive. Or perhaps maybe centralization would allow for better interpretability of models, given that there might be one authority that knows what went into it. Is there any upside to centralization?
00:19:02.694 - 00:20:41.356, Speaker D: Yeah, I think there are many upsides to centralization. Centralization is clearly much more efficient than decentralization. And the, you know, one of the places where this kind of an argument shows up is whether we should have models beyond a certain size be regulated or be, you know, open. I think there is a little bit of a false dichotomy here in the one sense that when you have closed models, one of the risks that safety people, for example, worry about is there, can the model go out and do something which can be destructive to the larger humanity? And I think open models already are come prepared or with some protection. The protection is, imagine you're going to go into a battle with an enemy, but you know the enemy's entire game plan, okay? You're going to battle and you have this enemy, and you have cracked their codes, you know, their cryptography, you know exactly what each person is thinking, and you have to go battle with this enemy. It's infinitely easier when you have an open source model. When you have an open source AI, it's much more, much easier when you know what the open source AI is in order to go counteract it.
00:20:41.356 - 00:21:23.774, Speaker D: Okay? But what could be dangerous is closed source AI, which is unregulated. Okay? So I have, like, you know, like I said, the fork in the road kind of a viewpoint. So neither, you know, so what might be a really good barbell regulation is models beyond a certain size have to be either regulated or be open source. You can't have a large, unregulated, closed source model. That might be a much more interesting policy position than saying either one is actually good. So again, careful, measured, cautiously optimistic.
00:21:25.794 - 00:21:45.614, Speaker A: And pramod, the last question here, is the blockchain industry ready for AGI? Where are the gaps? What needs to be done to handle an AGI that these other folks seem pretty optimistic about being achieved?
00:21:45.794 - 00:22:04.714, Speaker B: Oh, heavens no. I mean, I don't think many of us are what I call Gen Z. You know, my kid is Gen Z, but this is Gen Z generation crypto we can all, and no, we can all agree that the industry is nowhere near anywhere mature.
00:22:05.974 - 00:22:07.714, Speaker A: Where are the opportunities then?
00:22:09.134 - 00:22:50.092, Speaker B: I was thinking about where centralization helps and where decentralization helps. I'll give an example that's also related to the other question you had about Ilya, was talking about toxicity, and how it's very hard to have one party do be the arbiter of everything. One of the grand challenges of AI, there are three of them in my mind. But one of the grand challenges of AI is to be effective, a good chatbot, yet be robust, that is non toxic. And they're very different things. And trying to do them both together is really a grand challenge in AI. I gave an example of the chatbot, but this is just true of all of AI, how to be effective, efficient, and be robust.
00:22:50.092 - 00:23:15.046, Speaker B: And the trouble it's hard is because they're quite distinct. And the chatbot example makes it very clear. The centralized approach is particularly good for building an effective bot. The data is already there. You turn in capitalistic invention brings in all the capital and everything together. The efficiency kicks in, and you really do an effective chatbot. But alignment or non toxicity is something different.
00:23:15.046 - 00:24:08.844, Speaker B: It's something that community agrees. You need very many people to agree, and that's inherently the strength of decentralization. So one of the things we were thinking when I was thinking of sentient itself, is this kind of a decoupling to come in naturally. There are places where centralization makes a lot of sense, just efficiency, but you decouple where decentralization helps, and toxicity is something that should be community owned. It's a societal, cultural, societal, civilizational aspect. And efficiency is simply where capitalism is so good at to getting it. And, yeah, so I feel like decentralization and centralization have both have roles in this ecosystem, and having AI native proof systems, crypto systems, that tie them all through some byzantine resistance and appropriate incentive alignment is really the challenge associated with such a platform.
00:24:10.304 - 00:24:11.528, Speaker A: The last word to Ilya.
00:24:11.656 - 00:25:01.354, Speaker C: Yeah, I think there's, I mean, there's few dimensions where there's opportunity. I think the creating incentives around open source that indeed allow people to build on top of each other and kind of have capture some value from what they created, because kind of one of the challenges right now in AI, there's a lot of people who actually want to build open source software. There's a lot of builders who actually, if they had access to resources, would build really interesting stuff. But if you're building open source, you're not capturing any value. You're not making any revenue. And so if you start a company, you're kind of in this perpetual search for a business model and you're probably spending more time doing that than actually building stuff. And so interestingly, crypto kind of has a solution.
00:25:01.354 - 00:25:52.982, Speaker C: I mean, it's not like fully worked out, but we have tools for that. And so I think that's the biggest, probably opportunity to really bring how to bring incentives. How do we reward people who are actually contributing value to the open source? While there will be companies, commercial companies that are centralized, that are benefiting from that open source, they should be kind of paying back for this. And so I think that model is really interesting and something that I'm exploring. I think the flip side of this is the provable inference and kind of related stuff is like, as we want these models to be more robust, more provable. There is like, obviously a lot of companies working on that, but that's probably the biggest gap. Like the current ZKML, for example, is nowhere near the capacity that we need for like even the model I can run on my laptop.
00:25:52.982 - 00:26:32.766, Speaker C: Right. And so there is a trade off there that needs to be closed and kind of figured out both on crypto and hardware on multiple sites. And that is an opportunity. That's why there's so many companies doing that. And finally, I think we just need to start putting applications that are actually interesting. I think there's a lot of talk, but earlier today for the hackathon, I was actually giving a bunch of specific examples. What you can do now, like with the current tech, you don't need to wait for provable inference with the current tech, with the current that you can build that are interesting, that useful, that benefit people, that allows them to use local models.
00:26:32.766 - 00:26:42.394, Speaker C: All of this pieces just like, hey, let's do useful stuff now to prove out, to showcase, to let people see how these things actually start to work together.
00:26:43.614 - 00:26:54.034, Speaker A: I think that's all the time we have for this panel. We'll be back in a few minutes with a fireside conversation between two leading founders in the industry. Thanks, everybody.
