00:00:10.090 - 00:01:00.362, Speaker A: Welcome to the infinite Jungle, the podcast about the evolution of Ethereum. On today's episode, we're going to be talking about a code change that may or may not be going into the next Ethereum upgrade. It's called maximum effective balance. Developers talked about it on yesterday's dev call, and it seemed like the general consensus was that the code change that I'm talking about, Max EB for short, wouldn't be going into Electra. But I think it's an important one, and I want to explain why. So we're going to talk a little bit about Max Eb, and then we're going to be joined by the Oval team who's working on distributed validator technology. So we'll explain a little bit about what DVT is, and we'll get into some of the exciting milestones that the oval team is achieving lately.
00:01:00.362 - 00:01:54.154, Speaker A: So stay tuned for all of that. But first, before we begin, here's a quick disclaimer. I need to remind you to please refer to the disclaimer linked in the podcast show notes and note that none of the information in this podcast constitutes investment advice, an offer, recommendation, or solicitation by Galaxy Digital or any of its affiliates to buy or sell any securities. So let's talk about Max EB. The maximum effective balance is a co change that increases how much rewards that validators can earn on a balance of stake. So for a validator on Ethereum to be activated and to start validating on Ethereum, you need at minimum 32 eth. But you also can't earn more yield or rewards beyond that balance of 32 eth.
00:01:54.154 - 00:03:13.674, Speaker A: That is the effective balance that you are earning your rewards from. And if you want to earn more yield, because let's just say you have more than 32 ETH to stake, you're not going to be able to earn yield on, say, 34 eth. You have to have another multiple, like another 32 EtH that you're going to be able to stake separately, spin up another validator and then earn yield on that validator. So now you have two validators and you're earning double the rewards. That is what we call the effective balance of 32 eth. And increasing that effective balance means that validator node operators can consolidate the validators that they have under management and start earning rewards on not just an effective balance of 32 eth, but up to an effective balance of 2048 eth. The question is, why do this? And why is this so important for Ethereum, or why I think it's so important for Ethereum is because every single validator on Ethereum represents a cryptographic signature called a BLS signature that needs to be aggregated by the network in a specific amount of time, an epoch, which is basically, roughly translates to like 6.4
00:03:13.674 - 00:03:45.662, Speaker A: minutes. And the higher the number of validators, the more signatures that needs to be aggregated in that set amount of time. Developers are worried that right now there's a lot of validators. It's getting close to 1 million validators. I think there's like over 950,000, yeah, over 950,000 active validators on Ethereum. And that number is not likely to decline anytime soon. People are really excited about staking.
00:03:45.662 - 00:04:49.206, Speaker A: Ever since the merge, when Ethereum finally transitioned over to proof of stake, ever since Shanghai, when you were finally able to withdraw your stake, these upgrades have really de risked the activity of staking a lot. And so now we have a lot of demand for staking. A lot of people that want to earn rewards from staking earn yields from staking. And this has caused the number of validators to increase over the last couple of years. And again, as the number of validators increases. This puts strain on the networking layer of Ethereum, the peer to peer layer, where nodes are sending messages back and forth, syncing to the network, making sure that all of the blocks and the transactions that are being processed come to consensus with other nodes in the network. One of the reasons why I bring Max EB up is because it's one of the code changes that would fix the issue of too many validators, of too many active validators.
00:04:49.206 - 00:06:06.434, Speaker A: By allowing major staking pools, professional validator node operators to consolidate their stake every time they want to earn more yield, or have users that are delegating their stake to them, delegating their ETH, and being like, hey, can you spin up a validator on my behalf every time they're pooling eth together? They don't have to spin up a whole new validator every 32 ETH, they can have one validator that is earning staking rewards from a balance of up to 2048 ETH. And it allows a lot of more kind of fine tuning and flexibility of exactly how large you want your validator to be in terms of where you want that effective balance to be. Right now, it's very set in stone. It has to be 32 ETH or nothing, and only in multiples of 32 eth. But this increase from 32 ETH to 2048 would allow more flexibility of maybe I want it at 64, 65, et cetera, et cetera. In terms of how close we are to the network reaching an unsustainable number of active validators. Developers, I think it was for Shanghai a couple of last year.
00:06:06.434 - 00:06:41.354, Speaker A: I think in 2023, they had launched a test network with 2.1 million active validators, and that network was unsustainable. That was too many validators, too many signatures, too many messages flying back and forth on the networking layer. And so developers toned down the size of that test network and reduced the number to 1.4 million validators. And that test network is what we call the Holski test network. Today it's the newest Ethereum testnet.
00:06:41.354 - 00:07:50.398, Speaker A: My concern is basically that as we have demand for staking, it doesn't look like demand for staking is going to be reducing anytime soon, especially with the advent of solutions like restaking, where validators could potentially earn additional yield with their staked ETH through protocols like Eigen layer. It seems like there's going to be more demand for staking in the next couple of months, over the next few years. And having a long term solution like Max EV to this issue of a growing validator set size is very important. And this is not to say that developers don't think that it's important, because for the Denkoon upgrade, which is the upgrade that we all know is going to happen next month, on March 13, there was a very last minute code change that was added to Denkoon. And that code change caps the maximum number of validators that can enter into Ethereum every epoch, every 6.4 minutes. Right now, the maximum number of validators that can enter per epoch, I believe, is like 1213 or 14.
00:07:50.398 - 00:08:44.030, Speaker A: And the last minute code change that was added to Denkoon basically caps that number and sets it at eight. So for all intensive purposes, even as the validator set size grows, usually the churn limit, which is what we call the mechanism, that kind of caps how many validators can enter into the network and how many validators can exit. Usually that churn limit increases as the size of the validator set grows, but for the Denkun upgrade, because developers recognize that, look, the growth rate of validators right now is just getting out of hand. Let's cap it at eight. This will ensure that the validator set size can't grow too large too quickly, and it'll buy us some time to figure out Max EB. So that's what's going into the Denkun upgrade. But again, it's a band aid solution.
00:08:44.030 - 00:09:44.070, Speaker A: It's not something that is going to stop people from staking. Not that that is really the solution, trying to stop people from staking, but it does slow the growth of the validator set size somewhat. But the validator set size is still growing, and validators being able to enter at a rate of eight validators per epoch is going to add up. And because Ethereum is getting close to reaching the 1 million number, it's not out of the question that Ethereum might hit 1.4 million validators, which was really the size of the largest Ethereum testnet, the Holsky testnet, sometime this year. And if that happens, I think Ethereum developers may have to do another kind of hot fix solution to try and curb the growth of the validator set size yet again. Maybe trying to reduce the churn limit down to four, or trying to think of some other solutions.
00:09:44.070 - 00:10:49.020, Speaker A: But I think before having to scramble for such a fix, working on Max EB now, because this is a very real problem on the network, I think would be wise. Of course, the other kind of argument to this is that, look, Max EB is complex. It's an upgrade that requires quite a bit of more investigating, figuring out the edge cases of how we'd allow these validators to consolidate all of their eth and reduce say like 60 validators down to just one, and consolidate all that staked ETH balance. And it may adding in Max EV to the electra upgrade, or pectra, which is the combined upgrade name, may delay the other code changes that have already been approved for Electra. The electra upgrade right now has a couple of code changes, three that are pretty minor. They do introduce some efficiencies to help the network with aggregating these signatures. So that's a plus.
00:10:49.020 - 00:12:11.314, Speaker A: And it also introduces some other minor optimizations between communication between the consensus layer and the execution layer. These are the two networks that make Ethereum, and so these code changes are quite minor. They can be executed by developers. Developers are hoping that these code changes can be implemented on main net sometime this year before the end of 2024. And including a code change like Max EB may delay these code changes, may delay the upgrade electra past to, say, sometime in 2025. I still think that working on making Max EB a priority, even if it does delay the other code changes, is worth it, because of how important this issue is and how quickly the issue could get out of hand if we see staking demand continue to grow at the pace that it's growing, especially again with new protocols like restaking, that might increase the yield that validators can earn launching this year through protocols like Eigen layer, there is a chance that the validator set size could still continue to grow at an unsustainable rate that the network just can't handle. Of course, there is the code change that I mentioned that's going into Denkoon that will allow validators to only come in at eight validators per epoch.
00:12:11.314 - 00:13:42.738, Speaker A: But who knows if that band aid solution will really be enough to prevent total validator set size from growing and reaching a number beyond 1.4 million and exceeding that known number, that known testnet number. We know that a 1.4 million validator set size is working on a testnet, but beyond that, what are some of the issues that we could see on the network just because of the messaging load? Just because of the load that's putting on the networking layer of Ethereum? And keep in mind that while we have certain optimizations for signature aggregation that it's coming in the electra upgrade in Dencoon, one of the major upgrades, EIP 4844, will introduce a larger networking load because of the addition of blob space of blob transactions. So there's a lot to think about here, a lot of kind of moving parts and moving pieces. But I hope this illustrates kind of how important of an issue that I think Max EB is and important of a code change and solution that it is that developers start working on it now so that it can be implemented in the near future as a preventative measure to what happens if the network just grows too big, the number of validators gets too large. And if you're interested in learning more about this issue, I do have a report called the validator set size problem on Galaxy.com,
00:13:42.738 - 00:15:06.126, Speaker A: so you can learn more about the issue in detail there. The other thing that I'll mention related to this issue of the validator set size growing too much is another proposal that was presented on yesterday's dev call. And that one if you're actively on Twitter and involved in the crypto industry in any way, shape or form, you've probably heard that there's another proposal that was presented on yesterday's call that proposes reducing staking rewards for validators by 30%. One of the reasons for that is because the growth of the validator set is putting a strain on the networking layer of Ethereum. What I just explained that Maxeyb would solve. And this proposal, rather than going through the complexities of trying to implement a consolidation of stake and increasing the effective balance, just proposes reducing staking rewards by 30% and hopefully that will reduce the demand for people to stake because you're not getting as many rewards. Maybe you want to do something else with your ethereum, but that proposal, the other main motivation for that proposal, other than the validator set getting too big, is the motivation that right now, liquid staking providers, namely Nalaido, is getting too big.
00:15:06.126 - 00:16:03.102, Speaker A: It's making staking an extremely easy task for users to do. You don't have to run your own validator, you don't have to lock up your eth because you get a liquid staking token. In exchange for putting your native eth into Ethereum, you get steeth, the steeth token, and you have professional node operators on the back end that will be able to manage your validator for you. And so because of these solutions, these liquid staking pools like Lido that have been working very well thus far and are attracting a ton of staked ETH supply. Lido is the largest staking pool on Ethereum. It's basically making everybody want to stake. Before, when the number of at home stakers was larger, the total amount of ETH supply that was staked wasn't that high.
00:16:03.102 - 00:16:54.590, Speaker A: Now it sits at about a quarter. About 25% of all ETH is staked on Ethereum, and it looks like it's going to get higher. And the rationale kind of makes sense if you hold EtH, and it's really not that difficult to earn a yield on it. Why wouldn't you, why wouldn't you put it into Lido, put it into the smart contract, get a return from it without having to even lock up your token, you can still get the liquid staking token from it, and you can use that to interact with DeFi or interact with other protocols on Ethereum. And you yourself don't even have to manage the validator. So this is also causing just a ton of demand for staking. And one of the concerns is that if all this staked ETH accumulates to one liquid staking protocol like lido, that becomes a security risk.
00:16:54.590 - 00:18:02.790, Speaker A: What if, you know, there's a bug in the Lido smart contract, all of that staked ETH is at risk. And so there's a lot of concerns around centralization of stake into protocols like Lido. And the concern that rather than, say one quarter of total ETH supply is staked like it is today, that number in the next year, two years, three years, is going to become actually like 80% or 90% or near the entire supply of ETH is staked. And looking ahead to that kind of very concerning future, that would mean more ETH is issued. So a higher amount of inflation, most likely that a lot of that staked ETH supply is locked up in smart contracts like Lido that are not as decentralized or secure or battle tested as the Ethereum protocol itself, and a high number of active validators on Ethereum. So for all those reasons, the proposal says, look, this is a very simple change, just reduce the staking rewards by 30%. That is another proposal.
00:18:02.790 - 00:19:30.370, Speaker A: Again, I understand the rationale for it, but I still think that max EV for the problem of the validator set number, not for the problem of the percentage of total supply on Ethereum that's staked getting too high there's some other problems that I listed that won't be addressed by Max EB, but just for the problem of the number of validators being too high, I think max EB is a more long term effective solution. It's more complex, not as easy to implement technically as just reducing the staking rewards. But I think it'll have more long term payoff. Because if you think about just reducing staking rewards, if the ability to stake is still as easy as ever, even if staking rewards go down 30%, I think that the appetite for staking will still remain high. The ease at which people can stake the idea that potentially I might be able to earn additional yield through other protocols like restaking. I think that allure, no matter how low the staking rewards are, will still make people want to stake their ETH. And if you don't, and if you don't choose not to stake your ETH and do something else with it, you know that you're kind of being diluted out of the value of your eth if you're not staking, because the issuance of Ethereum is staking rewards.
00:19:30.370 - 00:20:43.242, Speaker A: So for all of these reasons, I think that maxib is a code change that perhaps should be reconsidered. For Electra, I think the topic of just simply reducing staking rewards, it definitely will be discussed more, but I think in terms of its impact on reducing staking demand might be a little bit overstated, especially if the ease at which people can stake remains high remains kind of trivial to do, whether it's 1% or 2%. If it's easy for me to stake, why not take that yield anyways? So a lot to think about, a lot that I'm sure will continue to be discussed. If you are picking up what I'm putting down, let's try and get Max ev into Electra is what I would say. So on the topic of staking, now we're going to be talking with the Oval team who's working on distributed validator technology, a pretty novel solution that will hopefully help decentralize and make the activity of staking more secure on Ethereum. Let's go chat with the oval team now. Hey guys, we are back with the oval team.
00:20:43.242 - 00:21:12.690, Speaker A: I have with me Oshin, the CTO of Oval, and Bret, head of growth at Oval. Very excited to have you both on the show because there's quite a lot of exciting developments happening in the DVT space. Thanks for having us. So before we get started, I think there's some listeners on our show that might not really know what DVT is, actually. O'sheen, do you want to do like a quick two liner on what that is for our listeners?
00:21:13.030 - 00:21:40.250, Speaker B: Yeah, happy to. So it stands for distributed validator technology. And it's the idea of running an Ethereum validator on more than one computer. And better yet, generally it's having a group of people run a validator together. For those of you that know what a multi sig is, you can kind of think of it like a multi sig for running a validator. And there's lots of kind of upsides to this on a security and decentralization perspective that we might get into over the next slide.
00:21:40.930 - 00:23:37.970, Speaker A: Yeah, and one thing I'll add is that one of the most common reasons why validators get slashed today is because validator operators love to keep 100% uptime. If you're not online all the time, then you have the potential of not earning rewards for that period. What if at that point you were supposed to propose a block and then you missed out on your block rewards? And so to do this, a lot of professional validator node operators sometimes have the validator keys, like the cryptography that you need to run your validator on multiple machines, except usually if one validator needs to be signing and that instance of your validator goes down and you have a software to kind of like automatically split up that validator on another machine to reduce the amount of downtime you have, you could have errors in that software that causes you to double sign or double propose a block. And so DVT is not about having your validator run in multiple different instances, but it really does split up the responsibilities of a validator across so that multiple operators can fulfill the responsibilities of just one validator. It's not duplicating the validator multiple times, which has caused slashing events, basically major penalties that node operators have faced. So a very cool technology and one that I have always had a couple of questions about and a couple of concerns about, and namely one of them is the fact that with DVT solutions, I can 100% understand why a professional staking pool, a professional node operator, may want to use DVT for that reason that I just described of I want to have 100% uptime. And I also know that it's very risky to have that validator running on multiple instances.
00:23:37.970 - 00:24:23.986, Speaker A: So to be able to truly split up the responsibilities of that validator across multiple machines through DVT, it makes sense. But why would it make sense for different professional staking operations? And why would it make sense for multiple, different independent validator node operators to come together into making a DVT validator? People that don't know each other? Because what if one of the people that you're partnering up with in that DVT solution is not a good operator? That was one of the questions that I kind of could never really quite understand. So I'd love it if Bret or O'Sheen, if you could help me figure this out.
00:24:24.088 - 00:25:20.050, Speaker C: Yeah, I can start it. Then maybe Oshin, you can add on. So because DBT changes the risk profile for operators, this has massive impact for liquid staking protocols, for example, because today they really can only trust the largest operators, the most proven, the one with the biggest brand, to be able to be a part of their operator set. And it limits the opportunities presented to smaller and medium operators. And so with the changing risk profile where you're sharing the risk across multiple different operators, it actually opens up a ton of opportunities for smaller, medium sized professional validators to be able to join curated permission sets of liquid staking protocols. And so I think that's one of the big benefits of DBT when it comes to multi operator setups. But what I would also say is there's potential for benefit for even the largest operators today.
00:25:20.050 - 00:25:54.606, Speaker C: Because recently you've seen a lot of institutional interest into staking, right? And you're bringing on kind of a more risk sensitive type of profile into the ecosystem. And even staking on a coinbase cloud or a block, Damon or a figment may still be too risky for a more risk sensitive profile to be able to stake. And so having a multi operator setup where you have multiple professional operators sharing the risk profile could actually change that equation for some of the more traditional institutionals coming into the space and staking for the first time.
00:25:54.788 - 00:26:34.410, Speaker A: I see. So it's from the users. The users themselves should want DVT to be run by not just one node operator, but multiple different node operators that may not know each other. Because even if Coinbase cloud runs DVT internally and uses DVT for themselves, it's still a risk that they're the ones running the software. So if you have DVT that's run by actually by different operators, but can you have DVT run internally by Coinbase cloud? And then Coinbase cloud also is part of a DVT cluster with other node operators that's like DVT nested in DVT.
00:26:36.350 - 00:27:16.680, Speaker B: Yeah, normally they'll run maybe just one of the four nodes instead of doing it all, but you actually can have nested layers if you really wanted, and you're being really cautious. But yeah, I think the main thing I want to reiterate is the private keys thing you touched on earlier and the fact that the node operator is not always in a hurry to admit like fallibility. They kind of want to say that they're perfect, but if you're a customer and you're like, what are the ods of the node operator? You're going to get my private keys compromised and get me slashed. You much reduce that if no one operator has your full, hot, validated private key. Instead they have four operators have it, and three of the four have to get compromised for it to go wrong.
00:27:17.610 - 00:28:15.900, Speaker A: I see. Yeah, you guys are really impressing on me the reason why users, the customers of these professional staking operators, would want the additional security of DVT. I had always been thinking about it from the view of the professional staking operation of how it could benefit their operations, which it does. But I can see why the diversity in implementations between different companies and businesses really comes from the users. That makes sense, speaking a little bit to how rewards, though, are split up across these DVT validators, and speaking a little bit again to the penalties of what happens if a DVT validator is slashed. One of the concerns that I have, especially with the oval tech stack, is that the piece of software that really makes DVT possible is called Caron. Okay.
00:28:15.900 - 00:28:40.930, Speaker A: Is there not like a centralization risk to the, you know, we have multiple execution layer clients, multiple consensus layer clients, mev boost. We only have one, but I guess it's open source, very tested. Is Karen introducing another kind of risk of that one client going down and causing several DVT validators across the ecosystem to fail?
00:28:41.670 - 00:29:33.662, Speaker B: I think generally it's a fair concern but I think there's a lot of different things we've done and are still doing in terms of mitigating that. So firstly, if I talk about Caron and the way it's designed, one of the most important things about it is that it's a middleware, and when you're running your validator, it doesn't have the private key or any of the pieces, it helps you split them. But after that you put your private keys into your web3 sign, or into your teku, into your validator client. And Karen's only job is it kind of connects to the other different nodes, and they all, as a group, decide what are we going to sign? And then it shows that to the downstream validator. And then when the signatures come back, it aggregates them together. Because it's BLS signatures, you don't need the private key to aggregate them. And then it sends it upstream to the kind of consensus client.
00:29:33.662 - 00:30:13.550, Speaker B: So Karen can speak to all of the consensus clients and it can work with all of the validator clients. We're finishing off some edge cases around aggregations, but generally speaking, the most important thing is it doesn't touch private keys. So if you go to the big enterprises, they say, I have something very fancy set up for my private keys. I'm not going to swap to new software that is Internet connected. And I'd say totally reasonable decision, I wouldn't do so either. But it's just a middleware, so it's more like reduced risk in that regard. However, there's still a couple of things in terms of liveness risk, so that's the slashing risk, which is the most important one.
00:30:13.550 - 00:30:46.602, Speaker B: But if Karen had a bug and went offline, still not be good. And in that regard, distributed validator clusters are independent from one another. So you can have one cluster and it's kind of totally separate. And if we bump a version release like a new iteration, you can put one of the four nodes onto the new version, wait a week, then you can do two out of four, three to four, and then ultimately four to four. And you can do that with just one cluster. And if you have ten clusters around, you don't have to do them all at once. Some of the other options, it's like all or nothing.
00:30:46.602 - 00:31:30.406, Speaker B: They all upgraded a particular hard fork, and you pray nothing has gone wrong. So that's one of the also helpful impacts. And last but not least, the real goal is to have a spec and multiple implementations. We have a bit of work that we'll chat about in terms of roadmaps and what to spec and not to ossify too soon. But we're working in particular with the Nethermind team on both the spec and a second implementation. And the dream is about four, because then if any of your dv middlewares fail, three of the others are working, so nothing goes offline. But yeah, we've taken a lot of work on the slashing side and on the liveness side, and then, yeah, ultimately the long term goal is multiple carols.
00:31:30.598 - 00:32:24.102, Speaker A: Yeah, it's really good to hear that we're recognizing the risk of caron failure doesn't mean that your validator will get slashed, but there is the liveness risk of your validator might not be able to fulfill its responsibility. And to that the goal is to make Caron a multi client middleware. And that's part of the roadmap. And I want to talk more about the roadmap that you guys are pushing out. Very exciting things and the way that it relates to the broader DVT roadmap, too. One last question about how DVT works, though, before we get into future roadmap items, and that's related to rewards and how staking rewards are distributed. Consensus layer rewards, pretty vanilla, pretty steady.
00:32:24.102 - 00:33:09.340, Speaker A: I can see how consensus layer rewards can be very easily just kind of calculated. I can understand execution layer rewards because those are transaction fees coming from the execution layer, all kind of in house in protocol. But when it comes to MEB, which is rewards coming through a third party block builder, you have this additional software that you need for MEB boost, that complexity of how do those rewards really get distributed through a DVT validator? I'd love a little bit of education here, because that seems very complex to me, that like seven validators in a DVT cluster, would they all be connecting to MeV Boost or just like one?
00:33:10.430 - 00:33:42.654, Speaker B: Yes, they would all connect to their own Mev boost. So dividing the rewards is pretty straightforward, frankly. We have smart contracts that we call obal splits, and these are immutable, non upgradable, oracle free smart contracts that basically do all of their accounting when you pull out of them. They're built on top of splits.org, and you're probably familiar with the protocol guild. They have basically a similar splitter type of setup. So for your cluster, you're setting up a split.
00:33:42.654 - 00:34:37.550, Speaker B: Normally the customer is getting in the region of 90%, and then the operators are splitting the remainder. So the MEV goes to that address along with the execution rewards consensus is actually the more complicated one because you have to normally separate the 32 e from beyond the 32 eth, because most people are charging fees on rewards, and we have what's called an optimistic withdrawal recipient to kind of divide them. But on the actual preparing for Mev, that's probably slightly the more complicated one. Everyone runs their own node, so they have an execution client, they have a consensus client. If they want to do Mev, they all need to add Mev boost, or at least add a relay. So most of the consensus clients can talk straight to a relay. And yes, every now and again a proposal arrives.
00:34:37.550 - 00:35:02.480, Speaker B: Somebody says, okay, I've grabbed something from a relay. Let's all sign this. There's a bit of trust involved. You have to trust the relay because you're only seeing a header. You don't really know what's in it. They all give their signature, and then they hand it back to the relay, and the relay takes it from there. They broadcast the block, and in that block will have been a payment to the fee splitter or the recipient address.
00:35:03.010 - 00:35:36.166, Speaker A: And the fee splitter is all denominated, I guess, by how much eth you have staked per participant in the cluster. Right. So if the block reward was say like three eth that you get, and the consensus layer rewards was like two ETH, and execution layer rewards was like one EtH, and it was all six eth altogether. Those rewards through that fee splitter smart contract is calculated according to each validator's balance, is that correct? Yeah.
00:35:36.268 - 00:36:05.726, Speaker B: Assuming the people running the nodes are putting up the ether. Yeah. You can just pick what you've set out of 100%. You can kind of arbitrarily pick the divisions. Oftentimes these are all node operators that aren't putting up the capital, it's somebody else putting up the capital. And that time it's normally kind of a 90 ten type of type of split. But yeah, if you're doing this like squad staking, where it's just you and friends and some of you put up more eth than the other, you just set your different settings at the beginning and everyone gets comfortable with them.
00:36:05.726 - 00:36:14.690, Speaker B: You can set like an editor address as well if you kind of feel need to step in, like in protocol guild, they have a no so safe that updates the split.
00:36:16.390 - 00:36:34.742, Speaker A: Gotcha. So very flexible, I guess, in terms of the split, but I guess if it is all delegated, then it probably would be all equal. And the randomization of who gets to do block proposals, when is that all done through? Kind of like the Opal smart contract.
00:36:34.806 - 00:36:50.480, Speaker B: Again with the not on the smart contract. It's in the consensus mechanism. So of these nodes, they're playing kind of consensus games, and QBFT is turn based, so it's a bit luck of the draw. It's basically it's your turn to propose something or someone.
00:36:52.930 - 00:37:36.698, Speaker A: So this is probably getting too in the weeds, but I'm very curious about this, and then we'll move on. So in the eyes of the consensus layer, is it seeing just one validator or kind of six? Because if it's seeing one validator, then you still need to delegate. Like, who gets to do the block proposal? Or are all the validators in the cluster doing the block proposal together? I was under the impression that certain responsibilities will still be divvied up to one validator in the cluster. But I'm curious. From the eyes of the consensus layer protocol, it's probably just thinking that these six validators are one validator.
00:37:36.874 - 00:38:15.420, Speaker B: Correct. So from Ethereum's perspective, it looks like a normal validator, which is kind of nice. It kind of keeps all the complexity out of the l one. And when I referred to consensus, I actually meant between the four or seven nodes within the cluster. I see everyone is taking part in so much as they all have a piece of the private key and they all have to sign things, but one of them has to go and get something to sign. And that rotates kind of round robin. That's kind of the rules of QBFT, the protocol, but there's different consensus protocols that have kind of slightly different options.
00:38:15.420 - 00:38:30.720, Speaker B: You can't, unfortunately, just say, whoever has the highest bid, we all go for, because someone can lie and be like, oh, yeah, I have a really high paying bid here. And that's not actually true. In this consensus protocol, it's turn based.
00:38:31.410 - 00:39:18.080, Speaker A: That's fascinating. These are like minor details of how these things work, but it's fascinating how you can, and even once the person is selected, you can also very customize how the rewards after the fact get split up between all these validators in the cluster. Okay, fascinating. I hope people who are listening also have a stronger grasp on how DVT works with Oval. Let's talk a little bit about the roadmap now for DVT and Oval. You guys have recently released a roadmap for DVT. Do you mind giving me a little bit of the highlights of that roadmap and some of the motivation behind it? Present this future roadmap for DVT for our listeners, and then we'll get into it.
00:39:18.450 - 00:39:50.482, Speaker B: Yeah. So I'll start by going back and saying that Vitalik originally has made some roadmaps for Ethereum over the last few years. I think he's made about three versions in the last while. And even about three years ago, he had distributed validators on the roadmap. It was first as like demo and then like implementation. And in his most recent roadmap, from a month or two ago, he has distributed validators marked as fully complete. And I'm like, sweet, that's very encouraging.
00:39:50.482 - 00:40:39.314, Speaker B: Vitalik's like, this stuff is good to go. But as you can kind of guess by some of the nuances we've talked about and consensus and stuff, there's lots more to do, even if what we have is in production, working, and so on. So I took it kind of upon myself to make a similar roadmap effectively, but only focusing on distributed validators. And in it, it kind of breaks it down into kind of different sections, and some of them relate to, like, for example, the private keys. We talked a bit about private keys, how they get generated. Can you rotate them so that you can kick out an operator? We might come back to what you said about underperforming operators. Another aspect is performance, and consensus is like one of the biggest blockers to performance.
00:40:39.314 - 00:41:37.366, Speaker B: You have to do three round trips at the moment with QBFT to come to consensus on something, and then another round trip to share the signatures. So that's kind of four round trips around the world in your 12 seconds. And there's different types of consensus mechanisms that can do fewer round trips, but there's kind of trade offs, and there's also ones that are more tolerant to failures, less tolerant, different things like that. On the other aspect, there are attribution is kind of the next one, or like the blame. So the original idea for distributed validators comes kind of from the Ethereum foundation themselves. They gave a talk in Devcon Osaka in 2019 about it, and the kind of first five minutes of the talk was, here's the idea, nice and simple. Now we're going to spend 20 minutes agonizing over the complexity of reward and punishing bad behavior, or like kind of lazy operators.
00:41:37.366 - 00:42:27.206, Speaker B: And the short answer is that it's very hard to do objectively without any kind of caveats or asterisks. So there's quite a chunk of roadmap purely around attribution, or like blaming people for underperforming. And then the last two sections are on purely integrating. So getting distributed validators into things like liquid staking protocols, sometimes easier said than done. You need to kind of figure out everyone's implemented a little differently, God forbid the whole eigen layer world as well. And then there's the kind of ethereum core roadmap as well. So vertical trees, execution tickets, many of the main items that the Ethereum roadmap wants to kind of achieve.
00:42:27.206 - 00:42:48.580, Speaker B: We have the fun of figuring out how a distributed validator fits within that and can handle that and whatnot. So yeah, there's like loads of things we can talk about. We can kind of dig into different sections or something, but it's made my life easier not showing it as a straight line, because a lot of people ask me for kind of a 24 month roadmap and I was like, how about six sections or something?
00:42:49.430 - 00:43:57.350, Speaker A: I actually can't stand the types of roadmaps. The Vitalik and I guess now you two ocean where it's like seven different things and they all have progress bars. I get why practically, realistically speaking, this is the way development happens. But from just like a reader's point of view, it's very confusing, very hard to follow. Along the first half of the show I was talking about maximum effective balance, and this is one of the code changes that I know is going to impact technologies like DBT in terms of the calculations of how. Now we think about the consolidation of validators, allowing maybe some DVT clusters to also consolidate, and additional kind of complications. Are there specific code changes or eips that from a DVT perspective you guys would really like to see included in Electra? Would love to kind of hear your thoughts also on what are the priorities of what needs to be changed on the Ethereum protocol for DVT to achieve its roadmap.
00:43:57.950 - 00:44:15.470, Speaker B: Yeah, so I've been agitating a little for, well, forced exits, but specifically I'm kind of pushing for us to tweak the design a little, and I'm a little late on that, but I'm hoping people will kind of see the importance.
00:44:20.210 - 00:44:27.060, Speaker A: Sorry, I said, you have the floor. Tell us what the tweak is and maybe we can try and get people on board.
00:44:27.430 - 00:45:03.614, Speaker B: Yeah, so for those that aren't familiar, forced exits is the ability for the withdrawal address of a validator, I. E. The person that's kind of receiving the ether to say I want to exit. Now, funny enough, it's not actually possible for the recipient to the ether to actually trigger an exit. This is because the beacon chain was started totally separately from the execution layer, and then they got merged later on. So only the hot validator keys can start the process of an exit. So if you delegate it to somebody, they can just go offline and ignore you.
00:45:03.614 - 00:45:45.766, Speaker B: And it'll be about 20 years for it to drain to 16 ether and kick you out. So you're extremely beholden to somebody that runs your stake right now. And that's very scary for many reasons. You're basically held at ransom. So forced exits are the idea that the withdrawal address can trigger an exit. And that's super important, particularly if you want to have lower trust situations where you don't necessarily know these people so well, or you're like, I really hope I can get my money back. The tweak that I would like to do is right now, the design, it's through the execution layer.
00:45:45.766 - 00:46:42.350, Speaker B: So you're paying kind of 21,000 gas and it costs a bit of money. It's on a bit of like a bonding curve. So you can't exit thousands of these that are becoming prohibitively expensive. But after paying this money, you go to the back of the free queue and the same queue that the consensus layer has. And the reason I think this is kind of problematic is I'm a little afraid that there'll be kind of bank run kind of behavior in the validator set over the coming years as kind of contentious hard forks come along. Right now you could be sitting there and a contentious hard fork is coming up and the queue goes to one week and then four weeks and then jumps to like 14 weeks. And if you're cautious or a bit like, nervous, you're like, I better get in the queue before it gets too long and people will kind of panic.
00:46:42.350 - 00:47:22.202, Speaker B: Whereas if you do have these forced exits and you're paying money for them anyways, if we just said, okay, when we're processing these, we'll process 16 forced exits and then we'll take the normal queue afterwards. Then all of a sudden there's no reason to panic. You can be like, I can always have my money back. Basically immediately. There's a bit of a penalty maybe that becomes kind of expensive, but I think it would make people kind of chill out, relax, be like, don't worry. Even if there's a 14 week queue, I can pay 0.1 ether and have it immediately if I so choose.
00:47:22.202 - 00:48:12.458, Speaker B: And I think that would be more stable. And there's not really very much to change in the design. And just other than like bancoin risk, I think it is particularly important for stake centralization in kind of two aspects. Firstly, there's a lot of people that hold liquid staking tokens, not because they really intend to use the token in Defi or anything, they're just scared of the execute. They're like, if I want to get my money back, I want to get my money back, so I better hold St eth because that's deep in liquid. Whereas they could stake natively and be like, oh yeah, I can kind of get my ether back by paying this kind of fee. They wouldn't feel the need to do so.
00:48:12.458 - 00:49:33.662, Speaker B: And then similarly, for the long tail of liquid staking protocols, it really helps to have a lot of liquidity be really deep and fungible, and that's really hard. If you're the 20th liquid staking protocol and your uniswap pool is like $20,000 deep, you're not really going to be able to kind of grow. A lot of these ones are like kind of incentivizing uni lp pools, trying to make them liquid because of this kind of queue situation. And we haven't really seen it happen yet, but there's a lot of leverage that I think is kind of dodgy in terms of people like depositing to a liquid staking token, putting that into a lending pool, borrowing more, and kind of earning seven, eight, 9% APR by taking out leverage. And I think people kind of falsely assume that these lsts will match the ETH one to one because currently they generally are kind of kept there through kind of artificial means. And I'm a little afraid that somebody will drain all of the liquidity pools and then start returning the asset, trying to get back their eth, pushing it all into the queue, and then all of a sudden it kind of slips off a peg and it's trading at like $0.90 on the dollar and there's no shortage.
00:49:33.662 - 00:49:51.080, Speaker B: It'll all be back in like a week or a month. But if you are in a lending pool, you're going to get liquidated. So I'm a little kind of cautious that, I don't know, I think Abraham Eisenberg called it a profitable trading strategy. I'm not so sure if I agree with.
00:49:51.550 - 00:50:21.460, Speaker A: Yeah, I like where your head's going. And I agree with you that the risk of a bank run type of liquid staking tokens and these d five protocols can definitely happen. Two things, though. Number one, I have good news for you. Yesterday, developers were like, yes, we are going to do execution layer trickable withdrawals in Electra. So that is in, that definitely is going into the next upgrade. But in terms of the tweak that you mentioned, correct.
00:50:21.460 - 00:51:22.840, Speaker A: Do you not think that the network basically, in a situation like that, everybody would do execution layer, trickable exit, as in they would pay the fee. And given that most people are staking through lido anyways, the majority of the demand for exiting the queue would all come from these types of signatures. And so even if they are prioritized ahead of other signatures that might be executed by independent at home validators that are signing off with their beacon chain, I don't know, their validator key, their hotkey, the ones that are triggered with their withdrawal key, that I think the queue is still going to be extremely long. It sounds to me like the churn for the exits that are triggered by a staking pool, by a smart contract holding the withdrawal key is going to be the backlog, it sounds to me. Wouldn't you agree?
00:51:24.090 - 00:52:08.834, Speaker B: Generally, yes, but I would caveat that these queues don't. Having this other fee paying queue doesn't have to take away from the normal queue. You can leave the normal queue untouched, its own churn limit and stuff, and then this fee paying or like priority queue, it really depends on how fast the base fee escalates. Like if you think of EIP 1559, the price goes up 100 x in 30 minutes. So if you set something very similar on this other one, it can trickle through an exit or two a block, and if you try and put 100 validators through it, it becomes 110 e penalty. So it's only going to very much be a trickle because that has been some of the things that the security researchers have flagged. They're like, they can't all exit this way.
00:52:08.834 - 00:52:38.480, Speaker B: And I'm like, yeah, absolutely. Cap bottleneck it to kind of 0.1%, keep it super small, it doesn't have to be particularly high or anything, and it doesn't have to take away from the other queue. It's not like we have only ten exits and et cetera. Otherwise it's like, yeah, we have this queue, we'll process them, and then we have the normal queue. I kind of think of it as like, talk about like an analogy in a nightclub, you have the kind of queue and then you've guest list, and then the people that pay money to go through guest list, you have different staff on both doors. It doesn't really matter.
00:52:38.480 - 00:53:01.640, Speaker B: So I wouldn't suggest that it's like, because there's so much demand for the paying queue that the free queue never moves. So you have kind of the option of both. If you're time insensitive, you just want to not pay any extra fees. Just wait a couple of days, keep validating, you'll get your money back without any fee. And if you're like, no, I'd rather pay to have my money in whatever it is. Twelve minutes pay.
00:53:02.890 - 00:54:11.582, Speaker A: Yeah, I see. It's like how concerned are you and how much are you willing to pay to get out quickly and making it so people can express their preference in that way? I have a feeling like though in the event of some catastrophic event with like a bug and lido or something, that there's going to be a lot of people that want to exit in fast and not many who are going to be able to pay, especially if the base fee becomes kind of crazy. But then again, that's, I guess, another kind of separate question around scalability of Ethereum and fees on ETH, one which gets us into a little bit of a different topic. But there's actually quite a lot more that I wanted to ask you guys about. But I realized that we are at time, but I want to just leave a couple of minutes still to talk about this one thing and then I will let you guys go. Lido. Congratulations on getting simple DVT module out.
00:54:11.582 - 00:54:53.370, Speaker A: This means that lido validators will be DVT clusters. There's going to be certain lido validators that are now running DVT with the oval solution, but also the SSV solution, which is another DVT protocol. Tell me a little bit about how this changes the game for Lido. Explain a little bit about how the simple DVT module will be impacting the rewards that Steve holders might be getting. And yeah, give us a little bit of. I want to leave time for this because I know everyone's going to be wondering about this given that it's happening literally now and it's a new feature for the Lido protocol.
00:54:53.870 - 00:54:54.234, Speaker B: Sure.
00:54:54.272 - 00:55:24.894, Speaker C: Yeah, maybe I'll start. And then oshin, you can add color. So, yeah, we're really excited about this, about simple DVT. It's the first module to go to main net soon in Lido V two. And really it starts to push the operator set of Lido towards a more decentralized kind of model. Right. We talked a little bit about the risk profile for operators for Lido before, and because of that they were really limited to 26 operators running their entire, running all of their stake.
00:55:24.894 - 00:56:14.820, Speaker C: And so symbol DBT, actually ten x's that, which is quite exciting. There's about 200 operators in this first wave of symbol DBT, and that's only going to grow over time and it allows smaller community validators, solo operators, and kind of a mix of that to be able to be running Lido stake. And so it's quite exciting. And I think this is the first step of kind of a longer journey towards having a wider distributed validator adoption across the Lido operator set and kind of a more maybe permissionless model in the future as well. So today it's going to start with half a percent of Lido stake as a starting point and that over time can move to 2% and then onwards from there. So, yeah, really excited about this. It's just the first step, but a huge milestone for us, for sure.
00:56:15.670 - 00:56:25.990, Speaker A: Very exciting. And all those validators that are participating, the 200 or so that are going to be joining this cluster, how were they selected?
00:56:26.490 - 00:57:07.702, Speaker C: Yeah, so it was a pretty extensive process, actually. Initially we had run multiple test nets with Lido on DBT. I think this is actually a third or fourth. This last testnet was specifically for simple DBT. And basically we had community validators, solo validators that had experience running distributed validators in the past, basically joined this first testnet wave. They had to meet a certain bar for effectiveness and uptime and performance in order to qualify and pass the initial set of requirements. And those that did made it into kind of this first wave of 200 on main net.
00:57:07.702 - 00:57:29.850, Speaker C: The next wave is actually accepting registrants right now. So people who want to become lido operators can register for the next wave of simple DBC. I think the next testnet is going to be launching kind of mid March, mid to late March, April time frame. So the window is not closed. They're going to be onboarding more and more operators into this module.
00:57:30.270 - 00:58:17.590, Speaker A: Very cool. So the first of many that we think are going to be now running on Lido, the simple DVT is not like a one and done the 200 validators, but it'll continue to grow. Okay, very cool. Unfortunately, we are at time well over time, but I'd love to have you guys back to talk a little bit about how it's going, talk a little bit more about the simple DVT module and its performance on Mainnet. But those are all very exciting developments. And I got to say thank you so much for explaining very clearly, Bret and O'Sheen, just what makes DVT so important and so unique and highlighting some of those key roadmap development items.
00:58:17.930 - 00:58:19.560, Speaker C: Amazing. Thanks for having us.
00:58:20.290 - 00:58:40.620, Speaker A: Yeah. And thank you, everyone who listened to another episode of infinite Jungle. My name is Christine, and I hope that you enjoyed the episode if you did, please give it a like, leave a know, subscribe, do the things on all the platforms, and we'll talk to you guys again next week.
