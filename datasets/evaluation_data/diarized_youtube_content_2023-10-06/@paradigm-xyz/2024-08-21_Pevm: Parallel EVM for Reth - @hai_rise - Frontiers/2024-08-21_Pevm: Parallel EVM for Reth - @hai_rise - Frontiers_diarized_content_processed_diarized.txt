00:00:01.120 - 00:00:40.122, Speaker A: Hello. Hello. So hello everyone, my name is Hai, I'm from rise. We are a l two focused on chain performance, so pushing geogasts per second and transaction per second forward is our jobs. And so I'm going to spam that a lot today would definitely surpass twelve mentions of geogast today. I would like to talk about parallel EVM and its contributions to the quest. So essentially, parallel EVM is an execution engine for EVM blocks that maximize throughput by executing transactions in parallel.
00:00:40.122 - 00:01:19.718, Speaker A: It is not the most powerful component in our stack, but it's still a key component regardless to unlock 1 transactions per second. And so traditionally EVM executors just execute transactions one by one sequentially, regardless of how many cpu's a node. And this is obviously like a waste of resources. And ideally we would get something like ten x speed up by executing 16 transaction at once instead. But it turns out it's not that easy. If so, everyone has done it already. And so the main problem is the core state conflicts.
00:01:19.718 - 00:01:57.436, Speaker A: And let's see an example to see how it works in practice. So let's say. So let's start with Alice having ten eth. Then in the first transaction he sent two eth to both, and now he has eight left. And in another chance in the next transaction he sent one eth to Carl, and now he has seven left. So this is very simple and intuitive in the sequential world, right? But when we go parallel, it's got a bit tricky. So if we execute these two transactions in parallel, it will read that at least have ten e both.
00:01:57.436 - 00:03:50.690, Speaker A: And the second transaction would actually think that he would have liked naive afterwards, which is obviously wrong, and the issue would arise every time two transactions try to read and write to the same state. And to solve this, we have block STM, which is a algorithm designed by aptos, especially for moviem and their own chain back then. And they solved this problem by adding a multi version data structure that track read and read data per transaction, and by introducing validation tasks. So a scheduler scheduled both execution and validation tests to see if a transactions were executed with old data, and if so, it must be re executed with the latest data stored in the multi version memory. So if we get back to this example, then a validation task on the second transaction would notice that, okay, it was executed with an o balance of Alice, so it would reschedule the transactions to be re executed with a ETH instead. And it turns out block STM is not that applicable to EVM off the bat, because like all EVM transaction read and write to the same beneficiary balance for gas payment. So for example, like in this transaction, okay, sorry.
00:03:50.690 - 00:04:19.952, Speaker A: So it was, we start with a beneficial balance of ten etH, and the first transactions just pay 0.2 as gas. Then it has to calculate this as okay, so after these transactions the beneficiary balance has 10.2 and to the next transaction it has to read like okay, so after the previous transactions it has 10.2 and now we pay 0.3. Now it's 10.5 and three repeats to the end of the block, making the block like fully sequential by definition.
00:04:19.952 - 00:05:19.040, Speaker A: And so there's essentially no parallelism for EVM, it would just stick to block STM at ease. So luckily I had some weird experience, experience writing production Haskell for a few years. And so intuitively I came up with a very purely functional way of doing things of lazy evaluations. So the solution is like instead of eagerly calculating the beneficiary balance at the end of each transaction, we simply lazily update restore these updates like okay, I don't care what this current value is, I've just wanted, oh sorry, I keep misclicking. So I, so I don't care what the current balance is, I will just add 0.2 to it as GAAP payment. And for second transaction it doesn't have to read the pre occurrence balance at all, just say whatever it is, I'm just going to add 0.3
00:05:19.040 - 00:05:58.720, Speaker A: to it. And so we only fully evaluate this at the end of the block or when there is an exponential please upgrade to the beneficiary balance. So the full evaluation is still sequential, but it's very simple addition and so it's very fast. It's still very fast. Why? With these lazy evaluation techniques we can do state loads, EVM execution of cointerpretions, gas calculation. Oh, that can still be doing in parallel. And at the end of the day we look at the whole point of block execution that we only want to know like the state at the end of the block and all these intermediary state we don't really care about.
00:05:58.720 - 00:06:53.370, Speaker A: And it turns out that gas payment is not the only thing that we can lazy update. And we can do the same for PE transfers or ERC 20 transfers and more. Because if we look at it then for the center of a raw transfer transactions, a lazy update would be like I don't care what the current launch is, we're going to add one to that. And I don't care what the current balance is, I don't need to eagerly calculate that just yet. And I'm just going to add minus subtract by the transaction value plus the gas payment at the end of the block. And so by that, even if the same sender have several transactions in the same block, with increasing nonch, it can still be parallelized. And same for ERC 20 transfers, which is basically just adding something to the recipient balance and subtracting something from the standard recipient.
00:06:53.370 - 00:07:33.434, Speaker A: And it also turns out that EVM transactions have a long q distribution. Basically means that a few transaction patterns make up for most transactions. So we can detect the patterns and optimize to it. With lazy evaluation, lazy updates or any other techniques we can gain huge speed up. And so we also do a lot of benchmarking for any performance works. It's a must. And through these benchmarks we can also detect what are the bottlenecks, what can be improved, especially when we are building on block SDM, which was originally designed for moviem, which is very different from EVM.
00:07:33.434 - 00:08:40.672, Speaker A: So through this benchmark we can actually see a lot of improvements specifically can be done for EVM. And so the current status that we are two times faster on average this is like two x beta over sequential execution for Ethereum and blocks and the max beta result four x. And interestingly, we actually have little to no overhead as we fall back to sequential for small blocks. And lazy updates actually remove mostly conflicts. So this is an improvement over block SDM, which has like 30% slow down when a block is very sequential. But if we look at this, then it's not that exciting, right? I mean, like if it's not ten x or 100 x and what's the point of even talking about this? So we have to move to the next benchmark, which is a giga gas benchmark because our goal at the end of day is to push EVM throughput forward and you know, to surpass 1 transactions per second and a lot of more. And this is like a lot bigger than what is the current traffic on Ethereum today.
00:08:40.672 - 00:09:22.498, Speaker A: And we are, so we are benching with very large blocks with 100,000 transactions. And to see how parallel EVM can speed this up. And it turns out the max speed up we achieved so far is actually 23 x. And this is for over 6000 independent units. Twelve transactions that make up to one gigs. And this, and another interesting number is that this is done in only 18 milliseconds. So this goes up to 55 giga gas per second, which is not bad I guess, but, so, but when we plug this into our own.
00:09:22.498 - 00:10:10.146, Speaker A: You know this is, you know this 55 gas per second, it's just raw execution speed. Right when we actually plug it into hole blob building and syncing pipeline it only scale to not even half of that to only 21 giga gas per second. And this is because like execution, raw execution is still only one half of the bottleneck. And then other half, the actual bigger half is on state access. And we need to solve that through a lot of the other techniques that hopefully we can share more about in frontier 2025. But for now let's just bear with me that with parallel EvM we can get to 21. Actually if we only apply parallel EVM to vanilla rad then it's only get to around 7 GHz/second peak through boot for live blobbuilding and syncing.
00:10:10.146 - 00:10:50.500, Speaker A: And this 21 is with other improvements that we've done already on the pipelining. Improvement and also like the status as improvements and of course like okay so 21 gas per second is like 3000 times faster than the peak speed of albeit room. But if we look at the slide and there's a big red flag it's called independent. So it is even realistic. I mean like all these measurements are on independent transactions. While in practice like many unitswap transactions are dependent on the most popular pools. Right? So this is like fantasy if anything is new list.
00:10:50.500 - 00:11:56.018, Speaker A: But it turns out that there are already many efforts in the ecosystem to design parallel DF's. Just like when we started to have multi core machines, people started to design concurrent and multiprocessor algorithms. And I think we are entering the same area and ethereum is actually lagging behind a little bit. And you can see in these kind of papers then other ecosystem already are designing parallel dapps like shorter automated market makers that is promised like five x throughput on Sui and 16 x triple on Solana. And so I hope that our PvM could help bring all these innovative design and more to Ethereum and Odell two s to keep EVM contract design cutting edge. But also like not all usage are dependent. So there are actually many usage that are independent by nature.
00:11:56.018 - 00:12:51.770, Speaker A: So we've been in talk with a project called VM, that is another l two that use we as these cheap storage backend. And they also support DA commits transactions for other l two s. And all of these DA commits are independent by default. And so if they plug parallel evm into their node it would be like an instant x three for nothing. And so we have been working to put pro EVM into rath and there's been a few weeks of work already. And it's not necessarily easy because we need to make sure that parallel EVM works correctly and actually consistently perform well. And not seeing some edge cases that is much lower than sequential or it actually produce wrong results, then we actually have to fix all of them before confidently running it inside red.
00:12:51.770 - 00:13:52.960, Speaker A: And my favorite edge case is a block where an NVM board interacts with contract, then self destruct it, redeploys to the same address, interact with the contract again all in the same block. And our parallel executor needs to know if a address is the contract or not to apply lazy updates rules. And the blocks surely confuse the hell out of it. And so yeah, we still need to work on a lot of testing and benchmarks before we can say that, you know, parallel EVM is ready inside rev, but the overall aim is to at least double or triple the sync speed. And so one day we can sync ethereum from scratch in half or just under a day. And if we look at this graph, then basically it's also like our current work in progress integration is just slightly above 200 lines of changes because power EVM is essentially just a very simple and clear interface. You give me a block, I give you back the state transitions and the execute result with that block.
00:13:52.960 - 00:14:55.136, Speaker A: If we look at this, then yeah, any, also if you use, if any projects are using RevM in the stack, can just switch Revm over to parallel EVM to get instant speed off with the same interface. And also anyone using red would in the future benefit from this speed up instantly and for future plans. So we only started to work on Pro EVM, I think in early April, 4 months. And throughout we already seen like four x seven x ten x 13 x 17 x and now it's 23 x for the peak buildup. And I don't think it's stopping anytime soon. And we have several ideas to keep pushing the limit. So one of it is already mentioned by several teams is to support an optional DAC, which is a dependency graph for full nodes to sync from sequential faster.
00:14:55.136 - 00:15:56.804, Speaker A: So basically like Dag would say that, okay, the transaction depends on these transactions. And so a syncing node with that Dag, which has executed over there in the right order so that there would be no conflicts and no re execution at all. And so this is also like very important work in the sense that it helps to keep the footnotes smaller than the sequencer so the sequencers can be buffed up to very performant, powerful and expensive hardware but the syncing node can stay relatively consumer friendly. And also one of my personal favorite is to redesign the scheduler to minimize synchronization overhead. So I'm actually not a big fan of bulk SDM scheduler design because all the worker threads kind of share the same atomics, so they kind of like interact with these same atomics every worker iterations and that keep refreshing the cpu cache. So there's always some kind. It's very small, but there's constant latency per worker iterations, which is very annoying.
00:15:56.804 - 00:17:06.508, Speaker A: I think ideally we would have like a dedicated scheduler thread that manage all these automates internally and going to put the tasks for each worker thread in their own dedicated memory locations, so that all these worker threads only going to work with their own allocated memory locations instead of having to share. And only in the case of state conflicts do they need to access to the schedule of memory locations to say that okay, there's some state conflicts. Pre revert to the previous transaction index. Other works included optimizing the concurrent data structures. This include the Emmy cache for chain state loaded from disk, and also the multi version data structure for the parallel executor. We can also track with JPoise to re execute from instead of re executing the whole transaction when there's the conflicts. And one thing I really really want to try for months now is to try and support more different EVM executors, like just in time compiler that Danny I believe.
00:17:06.508 - 00:18:27.852, Speaker A: And the paradigms teams ship buyback. So we'd love to try to plug it in and see how it could speed up power EVM even more, get even more gear gas. And the last one is to do a lot of low level tuning. So like memory allocators, like when you optimize to the microsecond range, even memory allocators can make a big difference. And up to now we have been trying like J Malog, SNMLaG mimalog and RP malogenous, and the difference is actually very noticeable, like up to like 30% to 40% in performance and this add up. And so we are using P Malloc mainly for our EVM implementation, but also we believe that in the future when the allocators API runs mature and get stable, we'll benefit even more by hand rolling our own custom allocators for the concurrent data structures, and to have another good one for the global allocators for the whole node or the program that consumes unused parallel EVM kernel configurations have a very interesting backstory. So two weeks ago I got back from ad con after a week on the road and I boot up my desktop, excited to get back to work.
00:18:27.852 - 00:19:21.802, Speaker A: And it prompted me to update Ubuntu to 24.0. And I was like, I mean like you know, a fresh start, why not? And then I updated Ubuntu, I updated the rust compiler to a fresh benchmark and out of nowhere there was a 20% regression. I didn't change any code. I mean the machine even had like a one week rest before that 16 hours day just benchmarking hardcore code. And why the 20% regression? And after looking around I found out that is insane. I think the latest Ubuntu versions, it upgrades the Linux kernel version. It actually switched to a new Linux kernel scheduler as the default kernel scheduler, and for some reason this one is 10% better on average, but 20% worse for computing intensive use cases like parallel EVM.
00:19:21.802 - 00:20:14.488, Speaker A: And so we have to roll back to the previous kernel version and I have to just alert everyone in the team to make sure that you have the right kernel version inside your machine and make sure that all the cloud instance have the right kind of version. And of course we cannot just get stuck with this code of version. I think in the future we will need to fine tune parallel EVM to work well with all of or most common kernel configurations and especially to have the best one for our own stack. And another one is to an arm and six or AMD 60. This one is pretty interesting as well. So I have so my desktop is Intel I nine overclocked at 5.9. It beats all cloud instance I could ever find easily.
00:20:14.488 - 00:21:23.472, Speaker A: But for some reason I spun up a awn is done with Graviton three recently and it's like only at 2.6. It's literally virtual cpu's versus my physical cpu's, right? And for some reason the cloud instant bit my desktop at the Giga, the Uniswap gigas benchmark. And that 18 milliseconds is actually from that graviton. I think that eventually we also need to do all of these low level tuning to make sure we know exactly what is the best configurations and what is the best setup to deploy parallel EVM on at a higher level to deploy all these performance nodes on. And so yeah, so we are fully open sourced and the power EVM implementation is fully in rust MIT licensed. And so let's collaborate. If you have any new ideas, feature requests, collaboration ideas, integration ideas, or new parallel Dapp designs, feel free to hit us up.
00:21:23.472 - 00:21:53.640, Speaker A: And we're actually also hiring. So if you're interested in doing hardcore low level rust performance tuning works for this and for a new database that we have been cooking on the back, then feel free to hit me up. And again, a great thanks to paradigms and the team for hosting this nice event and you know, building alloy RevM, Rad and a lot more that could make parallel EvM happen so quickly. Thank you.
00:21:58.900 - 00:22:01.720, Speaker B: Andrew. Hi, we have a question from Christian.
00:22:02.820 - 00:22:49.300, Speaker C: Thank you for the presentation. I have two quick questions. One is, is it possible to run this optimization for a whole range of blocks so that, I don't know, you, you kind of have, maybe you can make the syncing faster. And what, what kind of gains do you think you'd see for like full ranges as opposed to parallelizing each block independently? And the second question is, I saw that you restart transaction execution from scratch upon conflicts. I was wondering if you can hold the intermediary state in some kind of immutable data structure so that you can resume basically from any program like instruction.
00:22:51.000 - 00:23:27.930, Speaker A: Yes, thank you for the good question. So for the first one is definitely a possibility. I do expect that. So blocksDM essentially takes in an input as a list of transactions, so it doesn't really need block headers. So block headers are only relevant in setting things like what is the beneficial account and such. And so we can definitely flatten a lot of blocks and try to run it through parallel EVM for potentially more parallelism. And especially if we look at this, then the intuition so far is that the more transaction and the more workload we have, the more parallelism we can potentially have.
00:23:27.930 - 00:24:31.354, Speaker A: Because also, like, I think until now, one of the biggest bottlenecks that I want to get rid of is to, is when we join threads, it actually has a pretty long latency, especially in safe rust. And so I think that if we can do a lot of stuff before we join the threads and output execution result to better. And also that's also a reason why that arm thing that could beat my desktop was because I think arm is very efficient and so it has very low threads overheads and so we can spin up new worker threads and join them much faster. So definitely that's something we want to try in the future. So especially for historical scene, just buff a lot of blocks together, execute them in parallel. For the second question, it's definitely the first bullet point here. So actually I stole this from the origin block SDM paper.
00:24:31.354 - 00:25:14.430, Speaker A: I don't think they have tried to implement that yet, but I think it makes a lot of sense, because re executing from scratch is very expensive. We need to reset up the EVM environment and all these configurations for each transaction. That is very expensive. And so we just rescue from the repo that flag a state conflict, then I assume that'd be much faster. Yeah. Question here. So, for these future plans, which one of you do you think will be the biggest superboost for real world transactions? I mean, we saw that with independent uniswap transactions of performance already, like, amazing.
00:25:14.430 - 00:26:08.324, Speaker A: Good enough. But for world war transactions, like, which one of these, like, is it a better scheduler other than block S and T SDM or some other optimizations that's gonna take us there? Yeah, that's actually a very good question. So I actually think that the most probably promising gain of these is the first one is to just support, like, optional metadata for the syncing notes or whatever follows to run and execute the transactions in a way that there would be no re executions, no state conflicts, that it would actually speed things up. I think all of the others are more on is more. It's very hard to speculate how much it would affect real transactions. It could be anything from just 20% or to 200%. So it's a lot of experimenting work required to understand that.
00:26:08.324 - 00:26:47.580, Speaker A: But also, I believe that, you know, the new parallel designs for the apps would be also required to get the best of a parallel EVM, because we can only do so much to remove explicit read because lazy updates is a very powerful technique, but it cannot be generalized to arbitrary logic. And so I don't think we can just use the ad hoc techniques at the Apollo EVM level to solve all the apps. And they would actually have to design better, less apps instead.
00:26:52.320 - 00:27:18.220, Speaker C: Hey, great talk. Really interesting work. You didn't speak much about what? So the EVM wasn't ever designed to really be parallelized. Right. I work on l two s. I'm sure a lot of other people work on an l two would be open to adopting l two specific EVM extensions. So if you could pick one or two things that you would change in the EVM to enable more parallelism through the sorts of techniques you're using, what might they be?
00:27:19.920 - 00:28:21.294, Speaker A: I know this is hard to take, but I would switch to another VM incrementally. Well, it's pretty hard to say because I still believe that most state conflicts at the app level. So it depends on the application's logic. So, as we can see with the previous example of we VM, they use case are all transactions in the DA commitment uks are independent by nature. And so, you know, we don't need to change EVM to get the max parallelism level out of it. But yeah, I guess a low hanging fruit would be to remove, to modify the gas pavement mechanic, because as you can see, this lazy technique can get that through. But there's still a cost at the end of the block to fully evaluate these sequentially.
00:28:21.294 - 00:28:29.400, Speaker A: And it does add some, you know, tail latency to the block execution result, which is very undesirable. So I would change that to first.
00:28:30.340 - 00:28:39.332, Speaker B: Another idea is introducing increment opcode, which allows you to get around the slow dad store pattern. That happens.
00:28:39.476 - 00:28:47.000, Speaker A: Yeah. So basically providing more opcodes for dapps to design better or easier design parallel dapps. That would be very helpful. Yeah.
00:28:47.980 - 00:28:52.736, Speaker B: Okay, one question there, one from zero edge and we wrap.
00:28:52.928 - 00:29:08.660, Speaker D: Yeah, two quick things. The system you described reminds me a lot of like a lazy Mapreduce. Have you looked into doing more dag based approaches to scheduling work based on what slots are accessed?
00:29:10.000 - 00:29:46.360, Speaker A: Yes, certainly. So we are. One of another component in our stack is a yemenite, a customized mempool that do some pre processing of the transactions to do statical analysis to detect dependencies. But this only applies for blockbuilders. But at least for blockbuilders with this technique, it definitely helps by reducing the state conflicts file at runtime, which is a lot costlier after that. Then the bulk builders can pass on the blocks with this DAC that it already builds to the synching notes.
00:29:50.060 - 00:30:18.560, Speaker D: Yeah, I feel like that's like a really promising avenue, but. And then the second question I had is in the, how much of a speed up do you have in blocks where you have transactions that actually end up all affecting the same contract? Pragmatically, you oftentimes see blocks where it's like all transactions being a swap on a uniswap pool or like a mint on an NFT. Is it's one x or is there like, does doing it lazily actually improve it from one x?
00:30:19.340 - 00:30:46.936, Speaker A: Yeah, so lazy is not very easy to generalize. So it really depends on the actual applications. So your ideally application would just redesign in a way that reduce share states between the transactions. So that would help a lot. But yeah. So until then. Until we have that, then we are would be happy to have, I'm sorry, just, you know, one giga gas per second on average.
00:30:46.936 - 00:31:15.860, Speaker A: And having this 21 giga gas at the peak that we aim for and eventually we push the average towards 21. This is not like from parallel EVM alone, but from the whole ecosystem, you know, pushing these new parallel design forward, working together to achieve this. And hopefully by the time we have the average case, 21 GHz/second we would start to see like new cases that actually reach one tera gas per second. Speed up. Yeah.
00:31:18.080 - 00:31:22.260, Speaker B: All right, last question for zero age and we can wrap.
00:31:23.960 - 00:31:24.700, Speaker A: Hi.
00:31:26.040 - 00:31:31.464, Speaker D: One concern. With block production being sped up, I.
00:31:31.472 - 00:31:40.992, Speaker A: Think on the sync case it's obviously a big win across the board. But in block production, if it's faster to produce blocks, then there's a danger.
00:31:41.056 - 00:31:45.224, Speaker D: Of producers just withholding the block production.
00:31:45.272 - 00:31:46.896, Speaker A: To gain more MEV.
00:31:46.968 - 00:31:55.810, Speaker D: How do you see the MEV issue slotting in with parallel execution and other techniques to accelerate block production or potentially.
00:31:55.930 - 00:32:33.950, Speaker A: Even exacerbating certain issues? That's also a very good question. I think it's harder to solve for l one or existing blockchains, but I think at least for us, the more we get, the more throughput we get, the lower block time we can push down. So like, you know, of course, like you have more power to build different blocks, but now you have less time to build. So. So we try to reduce the block time to improve the UX, but also like to make sure that the best block is built on time and no holding back there.
00:32:36.610 - 00:32:38.270, Speaker B: All right. Uw. Hi.
00:32:38.650 - 00:32:39.590, Speaker A: Thank you.
00:32:50.220 - 00:33:30.882, Speaker B: Okay, so this is the end of the beginning of the day or the end of the talks part of the day. What's going to happen from now is that we're going to start flipping the room. So I'm going to ask everyone in a second to stand up. We're going to start from the left side of the room to the right and replace all the chairs with tables both here and outside. And basically for the people that stay, the expectation is to enter work mode and hack. Big shout out to our events team for making this happen, our design team for giving everyone very nice swag to cursive for this nice social experience to the speakers that worked extremely hard to get this done on a very short notice. And we hope to have a great day for the rest of today.
00:33:30.882 - 00:33:42.960, Speaker B: Please feel free to hang out until, I think we have it around until 08:00 p.m. and then tomorrow we will reconvene at again the same time. Please make sure to bring your cards and you all.
