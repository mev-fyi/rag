00:13:05.610 - 00:14:34.950, Speaker A: All right, we'll be starting in two minutes with the Ben Clavi presentation and Andreas presentation. So if people would. Oh, my God. Nobody's listening. Hello. Okay, can people start getting seated so that we can get started so that we don't run behind too much? I'm gonna start in two minutes. And will Clapby and Andreas come to the stage from the side? Should we close the in between? Okay, clabby, just come on.
00:14:34.950 - 00:14:52.674, Speaker A: Okay, does anyone mind closing? Yeah.
00:14:52.722 - 00:14:53.310, Speaker B: Great.
00:15:09.820 - 00:15:22.920, Speaker A: All right, we're back for the second half of the day, where we'll have three ecosystem builders to tell us very exciting stuff that they've been building on restack. So part of that first up and clappy form. Optimism.
00:15:29.250 - 00:16:04.900, Speaker C: Awesome. Thanks, everybody, for having us back. It was cool to be back at the second rust Ethereum event. Last year, we presented Opireth. This year, we're going to be presenting Kona, which is kind of an extension of our work, enabled by a lot of people in this room for continuing on op wrath. So our agenda today is to talk about what Kona is, a quick overview of Kona, as well as the fault proof program built on top of it. Go over a case for multiproost for stage two decentralization, a really cool project built by succinct labs with running Kona on SP one, and some quick talks about what's next.
00:16:04.900 - 00:16:50.792, Speaker C: So, first of all, starting off with what Kona is, it is a no STD implementation of the rollup state transition, as well as the derivation pipeline. And so the reason that we went and we actually re implemented the state transition and no STD is because it actually turns out to be pretty hard to run reth on top of things like zkvms, Tees, default proof vms. Given that they're kind of limited execution environments. No STD allows us to have a lot more control over that. The other thing that we have is built on top of these libraries. We have the first alternative faultproof program for the op stack, which kind of sits beside the op program. So Kona client is the faultproof program, and it's built on top of the derivation pipeline.
00:16:50.792 - 00:17:41.630, Speaker C: We have a stateless l two block executor with a Merkle Patricia try. That's kind of recursive in memory. And then we also have the host client I O libraries that allow for it to communicate kind of over an operating system pipe to pull in external data to give you guys an idea of kind of where this sits inside of the faultproof stack. We'll kind of decompose the problem. So whenever we want to verify a claim about the state of l two on l one, the normal process for that would be to first derive the l two chain, which basically is a data transformation function between batches, which are posted down to l one by the sequencer into l two execution payloads. And then we pass it over the engine API to the execution layer, kind of standard as l one does as well, and execute the l two blocks to kind of form the output state. And normally these are in two separate processes.
00:17:41.630 - 00:18:37.410, Speaker C: So we make the faultproof program, which kind of smushes the derivation, which normally sits in the op stacks consensus layer, and then the block executor, which sits in the execution layer together into one process. And so it turns out that you actually can't execute this on chain because it's just rust or go code. And so we implemented the fault proof vms, which most of you may know canon, some may know asterisk, and they're effectively these minimal virtual machines that are implemented both natively and in solidity. The solidity version is kind of a stateless executor of the VM. And then so we can't execute the full program on chain on those vms just because it would cost too much gas. And so we have the bisection game, which actually allows us to narrow in on the instruction step that we're looking for, and then ultimately that can decide the total dispute. And so Kona client is kind of at that very top level of the kind of smooshing together of the consensus layer and the execution layer.
00:18:37.410 - 00:19:05.368, Speaker C: There's kind of a more long winded blog post. So if anybody's kind of more interested in the technical details, there's the QR code. I'll give second. So kind of to demystify the process a little bit. This is all that the entry point actually is. There's a lot kind of happening under the hood here, but at a high level it's really not super complicated. So we can start off at the very top.
00:19:05.368 - 00:20:21.800, Speaker C: We're pulling in the boot info, and the boot info is a couple of trusted inputs. One of these is defined in the contract, which is kind of the l one head hash. It's a commitment to the historical state that contains all of the batch data, et cetera, that we need to actually feed into the derivation pipeline to transform into payloads. It also contains a trusted l two starting commitment, which is like the previous finalized proposal. Or in the case of kind of an optimized bisection game, the pre state that was agreed upon by both parties during bisection. Then once we actually have that information, we also get like the claim, which is the user input, and then the l two block number that the claim corresponds to, we feed that into the derivation pipeline, which effectively unrolls that commitment on l one to generate all of the data fetches from blobs, etcetera, and produces the execution payload up in this function right here. And so once we have the execution payload, we can just feed it directly into the block executor, which takes in a bunch of Merkel Patricia try witnesses, and actually performs the execution of the block statelessly, and ultimately recomputes the state route, receipts, route, transaction route, et cetera, formulates the header, and we get to the very end.
00:20:21.800 - 00:21:01.224, Speaker C: And so once we're here, we compute the output route. The output route is just kind of like a higher level commitment that pulls some things up to the top. It commits to like the block hash, the state route, and then the bridge contract storage route, just for easy access. And to make the proof a bit more succinct when we need to unroll it, and then just some asserts, just to check to see that when we reproduce this state from trustlessly unrolling these commitments, that the claim was actually correct. I if it's not, it'll panic, and that's kind of the end of the program's execution. So it turns out Kona client is actually pretty fast. Not that performance really matters.
00:21:01.224 - 00:21:47.710, Speaker C: In faultproof world, this is plenty fast enough to execute within a dispute window. It's cool to see that we got a couple of optimizations and things taking advantage of policy on an optimistic path and falling back to consensus if things like batch of policy isn't held. But the nice thing for this too, is that with it being faster, it becomes more feasible to ZK prove, and we'll talk about that in just a couple of minutes. But the cool thing is, these were both executed on a laptop. It's a pretty fast operation to do. One note is that these are actually using cache local witnesses, so there's no network latency inside of these benchmarks, which would probably two or three x these times in practice. Normally, if you're doing a fault proof run, you wouldn't actually have the witness cached up front.
00:21:47.710 - 00:23:09.680, Speaker C: So it kind of brings us into multiproofs and why we built this thing, you know, why do we have a second fault proof program when we already have one that works just fine built off of the reference? So a core requirement of l two beat, which for those unfamiliar is kind of like a roll up warden keeps people accountable and also does a risk assessment on l two s. They have kind of this big target that everybody's trying to reach called state centralization. And so in order to meet that, you actually have to have the ability for the Security council to only be able to intervene in the event of an on chain provable bug, quote unquote. So multiple implementations of the proof, kind of like multiple implementations of Ellen clients, offers us kind of like a sane path to getting here. So we can do this by allowing the proofs to challenge proofs themselves inside of the dispute game, with a disagreement being considered a consensus failure, and kind of unlocking the action of allowing the Security Council to intervene. And so another cool thing is with our last year's presentation, we already have Opreth, which a lot of people in this room have worked on. Thank you guys, you enabled this to happen, but it allows us to basically take that consensus layer and execution layer, smush them together, and have a whole new faultproof program built on top of a completely different implementation.
00:23:09.680 - 00:23:49.464, Speaker C: So this image is kind of a visual of what the current system looks like. And you can see that there are a lot of single points of failure. And hopefully this can give you some perspective on why the obstacle fault proofs are still on training wheels. So any of these red boxes, if they fail and there was no human intervention or air gap, would effectively mean that the bridge's funds could get drained. And so we have the op program, which is built on top of op geth and op node. The reference implementation, Op preimage is the hostclientio thing. Canon is the MIps, faultproof VM.
00:23:49.464 - 00:25:02.992, Speaker C: And then we have the contracts on the side, and every single one of these don't have a redundant component at play. And so if they do fail, we're kind of in a bad spot. And that's the whole reason why there's still this big operator override in the top, right? Because if they do fail, we still need the ability to be able to recover and keep the roll up rolling. So this is a really cool graph because it shows after the introduction of asterisk and Kona, asterisk is a secondary fault proof VM, whereas Kona is a secondary program. We reduce our single points of failure almost to nil, with still the contracts kind of sitting in this point where they are suddenly the bottleneck. And so looking even further forward, adding multiple types of disputes, you can imagine that the handler, for once, we actually discover this discrete upon block state transition, we can swap that out with something that short circuits like a ZK proof, and this would actually give us even more redundancy in that area and start looking towards turning many more boxes here gray. And so that kind of brings us to a design philosophy of Kona.
00:25:02.992 - 00:25:44.050, Speaker C: Something that we started trying to do from the get go is making sure that we could run everywhere. So Kona's components were built with alternative backend support in mind. They were starting with no STD, with alloc enabled libraries to promote portability. You can run it on pretty much anything. I actually ran it on a raspberry PI, which is pretty cool. But when boiled down, if we're just kind of thinking about faultproof VM versus ZKVM targets, there's kind of just like two small differences. So first of all, with the host client communication, it can occur on the fly in faultproof vms, whereas ZK vms have to have that full witness upfront to supply as a private input.
00:25:44.050 - 00:26:43.470, Speaker C: Also with faultproof VmSheendeh, the data received from the host in the native run can assume to be trusted, since the on chain implementation of the host actually verifies the data upon entry. And because we only need that small piece of data to actually perform the single instruction step, we can offload that verification to when it's actually needed, rather than having to do the full constraint inside of the proof. Whereas zkvms, during the runtime, they have to validate all of the data that comes in from the host. So, for example, if I give you the preimage of a catch act hash, you actually have to rehash it inside of the ZKVM to show that it's actually the real preimage. And they have to basically just validate all of that in order to fully constrain the proof. And so the nice thing about this is really all we had to do in order to make Kona run on faultproof vms and zkvms, and even in trusted execution environments, is to abstract over the data sources. So the derivation pipeline will read over l one data, it'll read some l two data, et cetera.
00:26:43.470 - 00:27:13.940, Speaker C: And all of that is just trait bounded. You can swap them in and out and allows you to have kind of multiple backends for this piece of software. And so this is really cool. Sysync Labs Uma is going to be talking next alongside Zach O'Braunt. They finished a mvp of the SP one backend for Kona client already, which is a really, really cool thing to see their code base is only around like 500 lines of code. It consists primarily of those data source implementations we're talking about. And then they've also got kind of like a custom entry point.
00:27:13.940 - 00:28:10.082, Speaker C: And they also are able to even reuse our program for generating the witness. You just run it natively, it captures all of the data that it needs, stores it in a big directory, and then they can just feed it in whenever they actually start the run on SP one. So what's next? First of all, we got to get this software out in the wild. So we're looking to productionize Kona and asterisk soon. Kind of on the road to having this multiproof redundancy, we have interop support, which is a big ongoing project which our implementation, along with the reference implementation, will have to end up supporting. Dispute game V two, which is a really kind of. It's an evolution of the existing dispute game that we have on chain that hopefully will mitigate some of the design failures as well as some of the implementation issues that we have around having further abstraction to nest by section, and also have these multiple handlers for the dispute disputes.
00:28:10.082 - 00:28:58.350, Speaker C: For multiproosts. We also now have a kind of independent derivation pipeline which we're using for proofs, but because the data sources are abstracted, you can also have it run in an online context. So we hope that somebody can actually build like an execution extension for a roll up node with Kona derive. And the other thing that we're really trying to do is beef up our multi client test suite. So l one I know Oliver talked about this, has all of these tools with kurtosis and t eight n, et cetera, to actually test cross client support. If we're going to get serious about using multiple clients in our hot path, we need to make sure that all of that is supported and well tested in a way that's maintainable. So all of these people did a wonderful job and helped contribute to this.
00:28:58.350 - 00:29:50.620, Speaker C: Specifically, Andreas, up on the top, who's in the crowd, wrote almost half of Kona with me over the past few months and was a huge help. And then some other contributors with Zacobrant and succinct labs were incredibly helpful in terms of offering us a good feedback loop on our abstractions and making sure that we had it in a place where alternative back end support was a first class citizen. Justice from Torali Labs helped out with some of the ideation as well as some of the implementation. And same thing with Nicholas and hash cash here, as well as Mark Tiniway from Op Labs as well. And so if you do want to contribute, we are hiring for this team specifically. So if this kind of stuff interests you, definitely find me on the side. If you're just looking to contribute in an open source capacity.
00:29:50.620 - 00:30:03.330, Speaker C: We do all of our work in the open. This is an MIT licensed code base. We have a discord where we do protocol R and DA. And if you're looking for an invite link, definitely feel free to find me in the crowd and I'll try to get you hooked up. And that's it.
00:30:10.670 - 00:30:35.060, Speaker A: All right, thank you, Ben, for the talk. Any questions from the crowd? And again, only questions, no statements. I have one. How do you relate to Kona being a piece in the multipurvet universe, especially for the stage two roll up decentralization roadmap?
00:30:35.640 - 00:31:28.080, Speaker C: Yeah, so Kona is kind of like the kind of the analog against Op program. So if we go back to this diagram where we have all of the components after the current system, Kona kind of sits beside the op program as its kind of backup redundancy piece. And so we have asterisk and canon, which are the two fault proof vms. And then we also have Kona and Op program, Kona specifically being implemented on top of op ref, as well as our derivation pipeline, with Op program having the op node and op Geth as its execution engine and consensus logic respectively. So these separate implementations are kind of designed such that we have kind of a high probability that the same bug won't be implemented in both at the same time. Following a similar vein of l one client diversity guarantees whenever we do hard forks and implement complex features.
00:31:31.940 - 00:31:32.680, Speaker A: Mark.
00:31:42.870 - 00:32:34.770, Speaker C: So what is your biggest learning been when you're targeting writing code that runs in a fault proof vm or a ZK Vm? Yeah, it's a good question. So we initially started trying to write Kona as like an STD lib enabled rust library. And the really hard part about that is that faultproof vms are just a really kind of bare metal environment by definition. So it's a pretty minimal soft cpu. Sometimes they don't even support the entire instruction set, and they also support a very small subset of Linux. So I think that Kona only requires four syscalls, which is read, write, exit group and mmap, which is really, really nice because we have that low level control. And so when we compare that with the Go programs, the go programs actually have to statically link the go runtime, which means go routines are required.
00:32:34.770 - 00:33:21.300, Speaker C: They call pretty much any syscall that they can, which I think Golang can compile with like 52 different syscalls for the MIPS target, which not all of which are supported. And so it's the kind of thing where having no STD code promotes portability between not only our targets, but hopefully future targets as well. Like, we were kind of able to run on SP one out of the box with this kind of low level libraries, mainly because we didn't have to worry about compatibility issues with every single new backend that we added. And hopefully that'll lend us a pretty good benefit in the future, especially as we start adding more backends to things like binius and also future tes. I know flashbots is interested in it, and that's kind of why we chose to make them more low level, no STD libraries to begin with.
00:33:22.520 - 00:33:31.790, Speaker B: Nice. So there are a bunch of red boxes here.
00:33:32.210 - 00:34:26.880, Speaker C: How do we get rid of the red boxes? Yeah, so it's a really good question. I think that there's kind of a complexity floor here that we need to think about, and we can't just like duplicate absolutely everything here specifically. It becomes really hard in the contract side of things. So the ZK kind of handler for finding the dispute at the very end is a really good way to kind of solidify our verification function once we found the dispute. But one of the really hard things is that that whole kind of like block bisection and chain bisection layer on top are still going to be kind of the same thing, and making those redundant, it becomes pretty hard. So really what you're kind of looking to do, in my opinion, is minimize the surface that we actually have to kind of pull out all of the steps on formally verifying a contract that just bisects over a binary tree is really not infeasible. That's something we can do.
00:34:26.880 - 00:34:53.799, Speaker C: But if we look at the system in this state, formally verifying this entire surface and, you know, fuzz testing the hell out of it basically means that every future protocol upgrade that we're going to make is going to be miserable. So it's kind of like, how far do you go is a good question. I think that you probably want to stop at some point in these contracts, but still probably have multiple verification functions at the end of the day.
00:34:58.499 - 00:35:03.279, Speaker A: All right, so stage two or bust, hopefully soon with Kona. One more question from Zh.
00:35:08.379 - 00:35:49.608, Speaker C: How ready is Kona and the other provers for the pectrade VM changes? Like Eof. Yeah, so at the moment, Kona is about as ready as Revm is. There's a couple of things that we're going to have to do in our block executor code. Again, we weren't able to actually directly reuse the EVM crate from Reth, but we do reuse Rev and verbatim. We didn't go and write our own EVM or anything, so hopefully the upgrade will be somewhat pain free. But we don't currently have pector support within Kona at the moment. Is the idea that that would follow at some later date on the op stack? Yeah, definitely.
00:35:49.608 - 00:36:35.622, Speaker C: The op stack tends to inherit l one hard forks pretty much verbatim. We haven't skipped one since we launched bedrock, and I can't imagine that would change. So if Petra Pectra does include if Pectra does include EOF, we will definitely be supporting it on the op stack as well. Have the provers been able to simplify a little bit in like a post self destruct era? Are there? The provers have definitely been able to simplify in a post self destruct era. So there's this kind of interesting thing where all of this is stateless execution. So I know that you probably heard all of the rest people say that the NPT is the root of all evil, and I agree with them. So account deletions kind of being gone is just an incredible thing, but we still have things like storage slot deletions.
00:36:35.622 - 00:37:26.766, Speaker C: So it actually turns out that whenever you zero out a storage slot when it was previously set, that path actually gets deleted within the try. And so deletions in the try in general are actually really really difficult when it comes to generating execution trace witnesses, primarily because you can execute the entire block with nothing but ethget proof. So just like proving the path to each account and then proving the path to the storage slots. But whenever you actually do a try deletion, you sometimes have to unblind the sibling which is hashed inside of a branch node in order to collapse the branch node. And so deletions from the try in general are just a pain in the ass. And having less of those which removing self destruct helped with definitely simplified the prover. But that said, we still do have to handle tri deletions, and I think that that's kind of one of the cruxes of the things that we're going to have to get over.
00:37:26.766 - 00:38:36.350, Speaker C: Like we just added a execution witness generation endpoint to Reth. And formerly the fault proofs are really heavily tied to hash scheme geth because in Geths database they effectively key trinode RLP by its hash and so it makes it really easy to traverse the tree regardless of where you're going. However, if you're only relying on ETh get proof, you suddenly can't actually do all of the tri deletions, because sometimes you need to unblind the sibling, and that means that you have to have extra support to do historical execution and collect all of the tri preimages that are touched not only during the execution part, but actually during state regeneration as well. Is there any denial of service risk where you couldn't run bisection if there was too many of these storage deletions happening, or. Good question. So actually no, to give kind of a perspective, and I wish I had a perfetto trace to show you guys, but the fault proof program execution is actually super cheap, and we're not super worried about that. I think that if I were to say the biggest hotspot in the whole fault proof program run at the moment is derivation and specifically spam batch validation.
00:38:36.350 - 00:39:22.598, Speaker C: So when we our sequencer posts down batches to l one, which is kind of all of the transactions that originate on l two, when we actually do this span batch validation, which is kind of validating the encoding and the order of all of the transactions in the batch, there's a lot of remote data fetching, which means it's a lot of communication with the host, which is over the OS pipes and actually takes 65% of the cpu cycles just to validate the spam batch itself. And that turns out to be the hot spot that I think we need to address first. But in terms of tri deletions, it's kind of negligible when it comes to it. And the cool thing about fault proofs is, because it's optimistic, we kind of just need to be fast enough, not as fast as possible, but hopefully we can push the performance front for the sake of the ZK teams that also want to run Kona as well.
00:39:22.774 - 00:39:23.930, Speaker B: Great. Thank you.
00:39:27.470 - 00:39:33.318, Speaker C: I agree. Have you tried looking into how Verkle is going to impact tree deletions and.
00:39:33.334 - 00:39:35.340, Speaker B: All that in Kona?
00:39:35.960 - 00:40:02.630, Speaker C: We haven't done too much exploration in Virkl yet, but I will say is that I'm really excited. I share the opinion that Verkle is probably best placed on l two. I am very excited to have kind of just execution or kind of execution witnesses in the header, because it makes our lives just way, way easier. But I don't think that I've done enough searching, nor have I started implementation, to see kind of concretely how it would affect our implementation.
00:40:06.650 - 00:40:29.980, Speaker A: All right, so next up today we have UMa to talk to us about SP one ref or the intersection of Reth with ZK proofs, so. Or against ZK roll ups with SP one. So we should have updated the title. So, Uma, take it away.
00:40:31.040 - 00:41:26.090, Speaker D: Hello, I'm Uma. I'm one of the co founders of Sysync. And yeah, today I'm going to be talking about how we built on top of the excellent work by Clavi and Andreas on top of Kona to build ZK rollups with SP one. So I think the first important thing to talk about is why do ZK rollups matter? What problems do they solve? What actually do they help? And I think today a very common theme in the Ethereum ecosystem that we're all talking about is interoperability and unifying all these different roll ups that are spread out throughout Ethereum today. If you're a user and you want to use a roll up, it's pretty difficult if you have assets on chain a to get assets on chain b. And a lot of these teams are working on themes around interoperability, unified liquidity, better bridging, and overall just improving the UX and making it better. And I think ZK is the only way that all of these problems will actually get solved.
00:41:26.090 - 00:42:15.322, Speaker D: So today, ZK rollups do exist, and there's a bunch of very smart people on some of these teams and other teams as well, that have had made ZK rollups. And what does that kind of look like today? Well, if you want to build a ZK rollup, the really critical ZK part is you have to make a zero knowledge proof of your rollup state transition function. So basically you take your old state route that's on chain a bunch of transactions, and then you prove your new stateroot and you verify that proof on chain. Now, today the ZK rollups, the code kind of looks like that. It's not super easy to see, but you can see that people have to write circuits, they have to write their own custom assembly, they have their own dsls. It's all very complicated. And in particular, there's a lot of issues with the current approach of Zkrollups today.
00:42:15.322 - 00:43:09.970, Speaker D: All the ZK rollup teams have very large specialized teams with ZK expertise and cryptographers that have to make these state transition functions. They're very hard to customize and maintain and upgrade because if you're hand rolling your own stack and then Ethereum goes through a hard fork and you want to be compatible, you have to update all that cryptography. It's also a really large surface area for security vulnerabilities, because you're handwriting everything that's going to ref and RevM, you're basically reimplementing that in ZK, which is basically ten or 100 x is hard. And then also a lot of the ZK rollups today are not fully type one compatible. So type one means that you have the same state root computation as Ethereum, for better or for worse, including RLP, the Merkle Patricia try catch act hash function. And so a lot of the ZK rollups, because implementing that in ZK is very difficult and is very expensive. They're type two or type three or type four.
00:43:09.970 - 00:43:49.236, Speaker D: So sp one is a ZK VM that we've been building at Sysync. And what does it let people do? It lets any developer use ZK by just writing normal rust code. So we're at the open source rust ecosystem conference, and so when you put open source Rust plus SP one, I think it can help address a lot of the problems with the ZK rollups of today. So how does SP one work for those who are not familiar? First, you write your program in normal rust code, so it looks pretty normal. It's very readable. Usually it's pretty short. Then it gets compiled to RISC five, which is a reduced instruction set.
00:43:49.236 - 00:44:33.372, Speaker D: And then we generate a proof of your RISC five execution and your RISC five program with a certain input to get a ZK proof that can be verified on chain. So when you have SP one, and now you can write your ZK state transition function using SP one plus revm ref alloy, cona, et cetera, you solve a lot of the problems with ZK rollups of today. Basically, you can just write normal rust so you don't have to be a cryptographer. It's really easy to upgrade and maintain. I kind of joke it's as simple as cargo update ref. When ref upgrades and they're implementing all this stuff, we just get all of that for free. The security surface area is also really nice because Ref has undergone extensive audits.
00:44:33.372 - 00:45:19.330, Speaker D: There's currently an effort to formally verify RevM, and we kind of get all that security work for free as well. When we use Reth and RevM in SP one, it's natively type one compatible because it's using the exact same node software and computation of the state root, and it actually ends up being not that expensive. So this is kind of the dream on why SP one will help solve a lot of problems and make ZK rollups really easy and really maintainable. And so now I'm going to talk more about how we actually go through this step by step. So step one of building a ZK rollup is we actually have to execute a block. So let's run through that. So when we want to execute an ethereum block in SP one, we have to write an SP one program using ref to actually do this computation.
00:45:19.330 - 00:46:01.588, Speaker D: But first, what we have to do is we have to fetch the witness for the computation. So this involves executing the block outside of SP one, just a normal native code. And we use the RPC, we fetch all the storage slots, we fetch all the relevant Merkle proofs, we fetch all of the data, and we do this in what we call the host program. Then we take all of that data and we construct a client input which is run in our client program, which is the SP one program. And so we have the previous block, the current block, and all the necessary merkle proofs and storage proofs and block hashes and try nodes. And a lot of this has to deal with the complexity of the MPT. I very much agree with Roman that MPT is the root of all evils.
00:46:01.588 - 00:46:34.330, Speaker D: But yeah, we are able to easily fetch that data using rough. Then once we have that data, this is the actual SP one program to execute the block. You can see it's very short and simple. We annotate it with this SP one entry point to denote it's an SP one program. We read in the input, we deserialize it, we execute the block, we then get the new header, we hash the header and then we expose that as the public outputs of our proof. Once we have this program, we can generate the proof in SP one. And now we have a proof for an ethereum block execution.
00:46:34.330 - 00:47:12.970, Speaker D: So we actually ended up implementing this. And along the way we definitely ran into some challenges that a lot of people in this room helped us out with. So one of the challenges was that ref is a very good piece of software, it's very modular. But unfortunately at the beginning of our exploration, some of the crates did not compile within SP one. So for example, if you import crates that are making networking calls, the code is not going to compile within SP one. And we worked very closely with the rest team to power through and make those dependencies optional or feature flagged or refactored. So that was a great help that they gave us.
00:47:12.970 - 00:47:58.420, Speaker D: Another complication, as many people have mentioned at this point, is state root computation. The Merkle partition trie is very very complicated. There's a lot of edge cases, and Roman in particular on the RET team helped us out a lot on actually computing the state route within our SP one program. There's also all these edge cases around the Merkle pusher trie that requires using this debug endpoint in gethse that we're trying to move away from. And the rest team has recently merged in an endpoint to help with that. Now once we got through all those challenges, there's a bunch of reusable primitives, including an RPC DB that can be modularly used with RevM where it fetches all the witnesses from an RPC. And then we have a witness DB that can be used in a ZKVM context where it loads up all the state and then it can just run the block statelessly.
00:47:58.420 - 00:48:29.858, Speaker D: In the end, we only have 1100 lines of code in this repo and we can now execute any ethereum block and generate a ZK proof for it. And it also works with op stack pretty easily, thanks to op Roth. So with ZK, everyone's always super obsessed with the costs. And so I want to talk about the cost data we got from executing these programs. So I took a random range of blocks, and these were not cherry pick blocks. You can see they're just sequential. It was a random range someone gave me.
00:48:29.858 - 00:49:08.566, Speaker D: You can see that the gas used, and these are real main net Ethereum blocks. You can see the gas used is some are less, some are more than this 15 million average gas target. And what we do is we take the block, we execute it with an sp one. We count the number of risk, five cycles that happen during execution. And then basically we generate the proof and we compute the total cost per transaction by multiplying the on demand cost of the GPU instance we're running sp one on and multiplying it by the amount of time it took to generate the proof for that block. In the end, you can see the costs per transaction are actually pretty low. It ends up being between 0.2
00:49:08.566 - 00:49:40.078, Speaker D: and 0.3 cents average proving cost per transaction to prove the execution of an Ethereum block. And this is actually a very pessimistic estimate. This is using on demand pricing. Usually when you go to reserved instances you can get much cheaper GPU's and there's a lot more optimizations we can do. So even today, the costs per transaction for proving an Ethereum block are very cheap. SP one has a bunch of ZK innovations and a lot of performance engineering that we spent a lot of time on to actually enable this.
00:49:40.078 - 00:50:29.360, Speaker D: So I want to talk a little bit about that. One of SP one's main innovations was our precompile centric architecture. So generally even this is true, I think in ref and Revm, when you're executing an Ethereum block, even on a normal cpu, you spend a lot of time verifying signatures and verifying hash functions. In SP one we have this system of precompiles where basically you can take these very expensive cryptographic operations and then we make a custom circuit for it in SP one that can talk to our main cpu. And usually this helps reduce the cycle count of our programs by six to ten x, which directly translates into a six to ten x reduction, improving overhead and time and cost. We also have a bunch of other precompiles for elliptic curve operations and other cryptographic operations that help a lot. For things like KZG or bn pairing verification.
00:50:29.360 - 00:51:26.886, Speaker D: We implemented an optimized GPU prover that helped a lot in terms of cost and latency over a cpu prover. And then we have a bunch of other algorithmic optimizations on the ZK side of things that really helped make SP one as cheap as possible for this Ethereum block execution use case case. So the main takeaways from doing this exercise were that the costs are already pretty cheap. And we think that with all the work we're doing and all the performance optimizations that are happening, as well as getting, you know, better reserved capacity and making our gpu's cheaper, the costs will only go down from here, maybe even by five to ten x. I think what's even cooler than the cost being cheap though is that it's super easily customizable and there's very minimal lines of code. We're super happy that we could use all of the work by the ref, then RevM and Alloy and Kona team and in the end you have ZK verification of Ethereum blocks with only 1100 lines of code. That is super magical.
00:51:26.886 - 00:52:18.684, Speaker D: And again, it's easily upgradeable. There's minimal maintenance surface area and I think that is the very powerful developer experience of a ZK VM. So I've talked about how to verify execution of Ethereum blocks, but in a ZK rollup there's actually a lot more stuff that's going on. So I think we're only at step one and now I'm going to dive into step two of actually building a ZK rollup. So thankfully we are using the op stack to do kind of the rest of the complexity of a ZK roll up. So what is the op stack? I imagine most in this room are pretty familiar, and this is taken directly from one of their blog posts or their documentation, but as a modular, open source set of components to build rollups in general. And I think they had a lot of foresight when they wrote this.
00:52:18.684 - 00:53:02.336, Speaker D: I think it was maybe one or two years ago where they said it's not just a roll up, it's not just optimistic. And actually now that's becoming true with the work we're doing to help make their stack also support zkit. So this is kind of like a system, very simplified system diagram of all the stuff that's going on in Opstack. You have this op batcher that's kind of like the sequencer that takes in a bunch of user transactions and is actually posting the transaction data on chain. Then you have this optimism portal where users are depositing and withdrawing. Then you have your actual node that's actually running your node software and updating the DB with the transactions and the deposits and the withdrawals. And then finally, this is the current part of their stack.
00:53:02.336 - 00:54:00.610, Speaker D: They have this op proposer that's washing the node, and every hour it proposes a new state route to this contract called the l two output oral. And all the stuff in the red is all the fault proof VM stuff, where basically they have the op challenger op program, canon, and a bunch of other stuff where the fault proof VM ensures that the proposer cannot propose a false route to the contract. So there's a seven day waiting period where you can run this interactive challenge game, and that adjudicates whether the state route that the op proposer proposed is actually correct or nothing. After seven days, when the stateroot gets finalized, the optimism portal can use that finalized Stateroot and users can do withdrawals against that stateroom. One thing I want to note is this diagram is a little bit of their old system. Their new system has a more complicated dispute game contract and things like that. That clabby covered a bit, but for simplicity, this is a simplified view of their system.
00:54:00.610 - 00:54:43.152, Speaker D: Now, their stack is super modular, and I think the OP team deserves a lot of credit for building their stack in this very modular way where components can really be easily swapped out. And so that's exactly what we did. We decided that, okay, the l two output oracle today is adjudicated by this fault proof game. But actually it could be a Zkl two output oracle, where whenever the op proposer proposes a state route, they not only propose a state route. They also include a ZK proof that their new state root is correct. It gets verified against an SP one verifier, and then the contract gets updated. So you can see that we are still able to leverage all the other components of their stack, like the op batcher, the op node, all that nice stuff.
00:54:43.152 - 00:55:15.414, Speaker D: And the only thing we have to do is swap out the ZK L two output oracle and the op proposer. So to actually do this and dive into a little more detail, there's two components. First, we actually have to write the state transition function for the proof that gets verified in this l two output oracle. That is not a very easy task. The op state transition function is pretty complex. It involves getting data from l one and running derivation. It involves taking the deposit transactions and making sure they're ordered properly.
00:55:15.414 - 00:55:51.386, Speaker D: And then you have to recompute the new state route and make sure everything is handled correctly. Thankfully, Kona, which is the previous talk that Clabi gave, came to the rescue. So Kona is this portable modular implementation that's rust centric of the op derivation program. And we were able to just implement Kona directly in SP one. It's just a normal rust code. So we were able to write an SP one program that has the optimism state transition function and very few lines of code. As Clavi mentioned in his previous talk, our program ends up being 500 lines of code.
00:55:51.386 - 00:56:40.970, Speaker D: It's very minimal modifications. It leverages all the existing work that they did, and then, boom, we get a program that can run the state transition function of optimism, generate a ZK proof of it using sp one, and then verify it on chain. Then for the op proposer, all it has to do is it runs in a loop, it watches the tip of the chain, and every so often it'll query to our prover network via one API call to actually generate the proof. So the op proposer today is a very lightweight process where it's just proposing it's getting the latest state route and it's throwing it on chain in a smart contract. This zkop proposer is still a very lightweight process. It's not actually doing any proof generation in its own runtime. What it does is it spins in the loop and then calls to our prover network to actually generate the proofs in detail.
00:56:40.970 - 00:57:23.904, Speaker D: What's actually going on is that for every range of blocks, so every so often, I think every minute, the op proposer takes that range of blocks and it requests a proof for that range, and it does it for every minute. Now, you can't actually post proofs on ethereum every minute. That's very very expensive. It ends up being like 280k gas per proof. So what you have to do is you take all your proofs for each range of blocks and then you aggregate them into one megaproof, and then after that you post the megaproof on chain. And so that's actually the logic that's going on in our Zkop proposer. So with those two components, it's very easy to make an op stack chain, a fully ZK proven chain, in two easy steps.
00:57:23.904 - 00:57:56.350, Speaker D: You just take an existing op stack chain, or you can deploy a new one using your favorite rast. Then you deploy this ZKL two output Oracle smart contract. That's trivial. Using forge, you just run a forge script, then you spin up a docker container to run the zkop proposer. It calls to approve your network, it generates proofs and then it posts them to the ZKL two output oracle. Then your time to finality goes from the seven days to 1 hour, which is pretty awesome. So we actually ended up doing this, we implemented, you can check out the repo up there.
00:57:56.350 - 00:58:53.422, Speaker D: And I think the thing that was really awesome to see and a lot of the work and shows really the compounding nature of open source work is that in all the total lines of code ended up being less than 2000. Thanks to the great work of the reF team, the RevM team alloy, and also the amazing work of Clavion Andreas, we were able to just import Kona, write the program, use the op stack, use most of its primitives, and only modify the proposer slightly. And then were able to have a full ZK rollup, which is super awesome. And yeah, as I mentioned, we actually did this. It's pretty few lines of code. Getting the right lines of code was definitely difficult, but once you had it, it's actually very beautiful. And the abstractions that Clavi and Andreas have in the Kona repository were very amenable to us.
00:58:53.422 - 00:59:21.820, Speaker D: There was very minimal amount of, and they had really great abstractions to make it easy for us. So we're very thankful to them. So we ended up doing this for op Sepolia and we're live updating with the chain. We have a bunch of GPU's spun up in our AWS instance, and we're able to keep up with the chain and generate ZK proofs and post them every hour or so. So now again, people's favorite questions with ZK. Ok, how much does it cost and what's the delay. So there's generally two components to any ZK system.
00:59:21.820 - 01:00:00.150, Speaker D: There's the on chain costs of proof verification, and then there's the off chain proving costs. So the on chain costs in our case are when the Zkop proposer proposes a state route, it has to propose a new route and verify a proof. And in all that transaction takes around 417k gas. This is actually around the same cost as proposing a new route with the fault proof game, which is around 420k gas. So the on chain costs for ZKD and for the fault proof game system are actually relatively similar. You're not paying more ETH to Ethereum to use Zkit. The off chain proving costs are obviously cost that is not present in the fault proof system.
01:00:00.150 - 01:00:48.950, Speaker D: And we found that in the end it ended up being zero point five to one cent per transaction proving costs. And this is a super, super rough estimate. It really depends on your workload, it really depends on how many transactions there are per block. It also really depends on your l one's throughput. So again, this is a very rough number, but we ended up with this range, and we noticed there's basically a two to three x overhead versus the proving costs of Ethereum transactions, because of the other parts of the derivation pipeline, where you have to scan through all the l one receipts to find all the deposit transactions and withdrawals and stuff like that. In general, we found that the derivation, which involves iterating through the l one, was ten to 20% of the proving overhead, and then the block execution ends up being between 80 and 90%. But again, all these numbers are really dependent on the load and work of your chain.
01:00:48.950 - 01:01:32.112, Speaker D: Another thing that people like to talk about is latency. So, you know, how fast can we get the finality? So one common misconception is that, oh, if I generate my ZK proof every minute, I can get 1 minute finality. Well, if you want to post ZK proofs to ethereum every minute, that's extremely expensive because it ends up being 280k gas per minute. That ends up just being a very large number. So actually the bottleneck to latency is the ethereum cost of posting proofs. That's the reason why our current latency is basically we post a proof every hour, it generally lags 20 to 30 minutes behind the tip of the chain, and we're able to do that pretty consistently. Another thing I want to highlight is that latency is not the same thing as throughput.
01:01:32.112 - 01:02:18.084, Speaker D: So even though we lag behind the tip of the chain by 20 to 30 minutes, which is already pretty fine. We're still able to keep up with the chain just by having more GPU's. So if you have more transactions or more blocks in your chain, that's actually totally fine. We just increase the number of GPU's and we're able to maintain a throughput that keeps up with your chain no matter what. Another question people like to ask is security. So ZK obviously very complex. It's kind of scary if I'm relying on a ZK proof for my whole roll up, is that okay? And I think similar to the fault proof systems of today, where you have, sometimes you have whitelisted people who can participate in the fault proof game, or you have an override security council, you can also use these same tactics with ZK to get the security that you want.
01:02:18.084 - 01:03:13.198, Speaker D: So for example, common techniques, and this is true for all the ZK roll ups of today, is they have a whitelisted set of actors that can post a proof on chain. And you can also have a small challenge period. For example, a three to five hour window after your route is proposed and the proof is verified for someone to step in and you know, or for a security council to step in and act. And thankfully, the existing op stack repo conveniently already has this functionality, and so you can leverage that as well to put additional security on top of your ZK roll up. So I think I spent a lot of time talking about current cost and latency, and a lot of people like to focus on that. But I actually think the most important question is the trajectory, where is this all going today? And I know that basically the costs are going to go down by ten x guaranteed. So there's actually two ways that the costs of all this stuff and latency can go down.
01:03:13.198 - 01:04:10.520, Speaker D: One is on the SP one side. As I mentioned previously, right now we're using all these on demand GPU instances. But actually it makes a lot of sense if you have a relatively constant workload, that you can start using reserved instances or dedicated instances for a really big reduction in cost. We also have a lot more optimizations we want to do to our GPU prover, and then we have a lot more optimizations to our actual circuits and algorithms on the ZK side of things that we are very optimistic will result in a five to ten x decrease in cost and latency by the end of the year. Another interesting area that costs can go down is actually protocol and software optimizations to Opstack and Kona. So I think when Op built their program they weren't necessarily thinking, how do we make it most efficient for a ZK? And currently they require iterating through all the l one block receipts. And I think things like this are very fixable, with minor tweaks to the protocol that can make it a lot more friendly for proving in ZK.
01:04:10.520 - 01:04:43.214, Speaker D: There's also a lot of raw profiling and optimization we can do to Kona to cut cycles. I think there's a lot of table stake stuff. We only recently started looking at this code, and there's a lot more low hanging fruit to make this a lot more efficient in the context of a ZK VM. And so already today, the costs are pretty cheap, but I'm very confident that they're only going to get cheaper and cheaper from here. So, as I mentioned, we've gotten this working. Those are the repos that I referenced. The RSP one is for Ethereum, op distinct is for the Kona sp one work.
01:04:43.214 - 01:05:01.630, Speaker D: And yeah, if you're interested, you can reach out to me, you can fill out this form. And in general, we want to make every roll up, a ZK rollup and scale ethereum. And I'm very thankful and grateful to all the work that all the people in this room have done that's really compounded to finally put all the pieces together and make this possible, hopefully this year. Thank you.
01:05:08.820 - 01:05:31.920, Speaker A: Yeah, the afternoon talks are pretty epic because they're showing that, okay, now that we have the base layer, we can now start layering things on top and move really fast. Where things that people have told you maybe in the past that were really expensive or slow or expensive in dev time, well, that might be a lie. And maybe we're in this new world where things are getting done very fast, very cheap, and actually get developed very fast. Any questions for Uma?
01:05:33.940 - 01:05:44.940, Speaker E: Hey there. Great talk. I'm curious how you see ASICs and FPGA's driving the costs down and the compute down. Like, what kind of improvement factors are we talking about?
01:05:45.840 - 01:06:28.410, Speaker D: Yeah, I think today we use GPU's, and that was a very big improvement over cpu, as the proof systems are still very rapidly evolving. The flexibility of a GPU is really nice, so you can iterate very quickly. But then when you have ossification, I think FPGA or custom, slick and ASIC could be a huge improvement. And maybe that's where you get the 100 x or 1000 x or something like that. There's a lot of nuance to the hardware. For example, often actually, raw computation isn't the bottleneck it's memory bandwidth or data transfer, and it's very hard to compete with Nvidia on those fronts. They're a very massive company with a lot of resources, so I think the equations actually end up being very complicated.
01:06:28.410 - 01:06:40.120, Speaker D: Really depends on your proof system, but there's a lot of great people who are doing work on that, including irreducible with binius. So I'm pretty excited, all that stuff. And we're actively definitely investigating it too.
01:06:40.280 - 01:06:41.180, Speaker C: Thank you.
01:06:42.920 - 01:06:46.460, Speaker A: One more question. Hello.
01:06:47.040 - 01:06:48.512, Speaker B: I'm curious what you think the roadmap.
01:06:48.536 - 01:06:50.904, Speaker A: Would be to not needing a whitelisted.
01:06:50.992 - 01:07:25.010, Speaker D: Set of proposers for ZK proofing? Yeah, yeah, it's a good question. I think it's probably pretty similar to op's roadmap for getting to stage two. You can have multiple different proof systems, you can maybe have formal verification. I think as these things get used in production more and more and get more Lindy or fully open source, have more people look at them, they get more secure with time. And so I think it's just a matter of time and being actually used in the wild.
01:07:25.390 - 01:07:26.290, Speaker C: Thank you.
01:07:27.910 - 01:07:29.250, Speaker A: Okay, last question.
01:07:31.110 - 01:07:35.422, Speaker C: Hi, I'm here, yeah.
01:07:35.566 - 01:07:38.950, Speaker B: Could you talk a little bit about how the prover network works?
01:07:39.110 - 01:07:41.850, Speaker A: What are the crypto economics there? And so on?
01:07:42.510 - 01:08:14.360, Speaker D: Yeah. So today our prover network is kind of a prover network of size one, I like to call it, where we're generating all the proofs. We did that just so it's very easy and fast to move in the longer term. We want to have a system where anyone in the world can just turn on their GPU and contribute to generating proofs for sp one. But yeah, today it's very simple. You just send an API call with your program, your input, we generate the proof and then we send it back to. And there's no trust assumptions on it because the proof is self verifying.
01:08:14.360 - 01:08:16.716, Speaker D: Just verify it yourself and then you can use it.
01:08:16.868 - 01:08:17.668, Speaker A: Thank you.
01:08:17.804 - 01:08:54.719, Speaker E: One very last question. So, one thing that you mentioned that I did not appreciate in the past is that the new debug execution witness allows you to build the stateless ret node today. That would be a great hackathon project if anybody wants to do it. The question to you is how sensitive is proof generation to the state representation? And like, where do you land on the vertical debate? Because some people say it's more ZK friendly, some people say it's not, they don't believe it. Word for word, where do you land?
01:08:55.619 - 01:09:31.622, Speaker D: Yeah, first of all, Roman, thank you so much for all the work you did helping us with MPT and Stateroot. You are so invaluable. Our team loves you. Yeah. With the verkle debate, I have not looked super deeply into it, although my intuition is that it's actually not that friendly to ZK. The reason is today catch act is actually not so bad, even in our ZKVM SP one, because we have a pre compile for it and we accelerate that hash function with Verkle you have. It's over the BLS twelve through 81, I believe, or something.
01:09:31.622 - 01:10:00.039, Speaker D: So it's just a bigger field. The evolution of ZK proof systems has been smaller and smaller fields, so the vertical big field is actually not very friendly to that. I think before they do such a big upgrade to Ethereum that's so in depth, there needs to be a lot more study of how it will impact its performance in zkvms that no one's really done. So my position on it right now is I don't think we should rush it. I think we need to do a lot more investigation, and it actually might not end up being more friendly for ZK vms.
01:10:02.139 - 01:10:19.880, Speaker A: All right, so huge applause foruma and then we have hi from the Rise team who will talk to us about PevM is. Hi, where is. Hi, yeah.
01:10:30.740 - 01:10:58.790, Speaker B: Hello, hello. So, hello everyone. My name is Hai, I'm from Rise. We are a chain l two focused on chain performance. So pushing geogast per second and transaction per second forward is our jobs. And so I'm going to spam that a lot today would definitely surpass twelve mentions of geogast. And today I would like to talk about parallel EVM and its contributions to the quest.
01:10:58.790 - 01:11:43.342, Speaker B: So essentially, parallel EVM is an execution engine for EVM blocks that maximize throughput by executing transactions in parallel. It is not the most powerful component in our stack, but it's still a key component regardless to unlock 1 transactions per second. And so traditionally EVM executors just execute transactions one by one sequentially, regardless of how many cpu's a node has. And this is obviously like a waste of resources. And ideally we would get something like ten X speed up by executing 16 transaction at once instead. But it turns out it's not that easy. If so, everyone has done it already.
01:11:43.342 - 01:12:27.316, Speaker B: And so the main problem is the called state conflicts. And let's see an example to see how it works in practice. So let's say. So let's start with Alice having ten eth. Then in the first transaction he sent two ETH to Bob, and now he has eight left, and in the next transaction he sent one eth to Carl and now he has seven left. So this is very simple and intuitive in the sequential world, right? But when we go parallel, it's got a bit tricky. So if we execute these two sound junctions in parallel, it would read that at least have ten eth both.
01:12:27.316 - 01:13:44.500, Speaker B: And the second transaction would actually think that he would have liked nine e afterwards, which is obviously wrong. And the issue would arise every time two transactions try to read and write to the same state. And to solve this, we have block STM, which is a algorithm designed by ApDos, especially for moviem and their own chain back then. And they solved this problem by adding a multi version data structure that track read and read data per transaction, and by introducing validation tasks. So a scheduler would schedule both execution and validation tasks to see if transactions were executed with Odata. And if so, it must be re executed with the latest data stored in the multi version memory. So if we get back to these examples, then a validation task on the second transaction will notice that, okay, it was executed with an O valence of Alice, so it would reschedule the transactions to be re executed with ETH instead.
01:13:44.500 - 01:14:42.716, Speaker B: And it turns out block STM is not that applicable to EVM off the bat, because like all EVM transactions read and write to the same beneficiary balance for gas payment. So for example, like in this transaction, okay, sorry. So it was, we start with a beneficial balance of ten eth, and the first transactions just, you know, pay 0.2 as gas. Then it has to calculate this as, okay, so after these transactions, the beneficiary balance has 10.2 and to the next transaction it has to read like okay, so after the previous transactions it has 10.2 and now we pay 0.3,
01:14:42.716 - 01:15:39.708, Speaker B: now it's 10.5, and this repeats to the end of the block, making the block like fully sequential by definition. And so there's essentially no parallelism for EVM, it would just stick to block STM at ease. So luckily I had some weird experience writing production Haskell for a few years. And so intuitively I came up with a very purely functional way of doing things of lazy evaluations. So the solution is like, instead of, you know, eagerly calculating the beneficiary balance at the end of each transaction, we simply lazily update, like restore these updates, like okay, I don't care what is the current value is, I just want, oh, sorry, I keep misclicking. So I, so I don't care what the current balance is I would just add 0.2
01:15:39.708 - 01:16:28.590, Speaker B: to it as GAAP payment. And for second transaction it doesn't have to read the current balance at all, just say whatever it is, I'm just going to add 0.3 to it. So we only fully evaluate this at the end of the block or when there's an explicit read to the beneficiary balance. So the full evaluation is still sequential, but it's very simple addition and so it's very fast and is still very fast. Why? With these lazy evolution techniques we can do state loads, EVM execution of code interpretations, gas calculation, all that can still be doing in parallel. And at the end of the day we look at the whole point of block execution that we only want to know the state at the end of the block and all these intermediary state we don't really care about.
01:16:28.590 - 01:17:23.270, Speaker B: And it turns out that gas payment is not the only thing that we can lazy update. And we can do the same for PE transfers or ERC 20 transfers and more. Because if we look at it then for the center of a raw transfer transactions, a lazy update would be like I don't care what the current launch is, we're going to add one to that. And I don't care what the current balance is. I don't need to eagerly calculate that just yet, I and I'm just going to add a minus subtract by the transaction value plus the gas payment at the end of the block. And so by that, even if the same sender, you know, have several transactions in the same block, with increasing nonchalance, it can still be paralyzed. And same for ERC 20 transfers, which is basically just adding something to the recipient balance and subtracting something from the standard recipient.
01:17:23.270 - 01:18:09.130, Speaker B: And it also turns out that EVM transactions have a long q distribution. Basically means that like a few transaction patterns make up for most transactions. So we can detect the patterns and optimize to it. With lazy evaluation, lazy updates or any other techniques we can gain huge speed up. And so we also do a lot of benchmarking or any performance works is a must. And through these benchmarks we can also detect what are the bottlenecks, what can be improved, especially when we are building on block SDM, which was originally designed for moviem, which is very different from EVM. So through this benchmark we can actually see a lot of improvements specifically can be done for EVM.
01:18:09.130 - 01:19:02.088, Speaker B: And so the current status is that we are two times faster. On average. This is like two two x speed up over sequential execution for Ethereum and blocks and the max beta result four x. And interestingly we actually have little to no overhead as we fall back to sequential for small blocks and lazy updates actually remove most of the conflicts. So this is an improvement over block SDM which has like 30% slowdown when a block is very sequential. But if we look at this, then it's not that exciting, right? I mean like if it's not ten x or 100 x and what's the point of even talking about this? So we have to move to the next benchmark, which is the giga gas benchmark. Because our goal at the end of day is to push EVM throughput forward and you know, to surpass what you guys per second.
01:19:02.088 - 01:19:33.230, Speaker B: 100,000 transactions per second and a lot more. And this is like a lot bigger than what is the current traffic on Ethereum today. And we are so we are benching with very large blocks with hundred thousand transactions. And to see how parallel EVM can speed this up. And it turns out the max speed up we achieved so far is actually 23 x. And this is for over 6000 independent units. Transactions that make up to one gigantic.
01:19:33.230 - 01:20:40.026, Speaker B: And another interesting number is that this is done in only 18 milliseconds. So this scales up to 55 giga gas per second, which is not bad I guess, but so, but when we plug this into our own, you know, this is, you know, this 55 gas per second, it's just raw execution speed, right? When we actually plug it into home blob building and syncing pipeline it only scale to not even half of that to only 21 giga gas per second. And this is because like execution, raw execution is still only one half of the bottleneck. And then the other half, the actual bigger half is on state access. And we need to solve that through a lot of the other techniques that hopefully we can share more about in frontier 2025. But for now let's just bear with me that with parallel evm we can get to 21. Actually if we only apply parallel EvM to vanilla rad then it's only get to around 7 GHz/second peak through boot for live blobbuilding and syncing.
01:20:40.026 - 01:21:26.088, Speaker B: And this 21 is with other improvements that we've done already on the pipelining improvement and also like the state assess improvements and of course like, okay so 21 gas per second is like 3000 times faster than the peak speed of albeit room. But if we look at the slide then there's a big red flag. It's called independent. So it is even realistic. I mean like all these measurements are on independent transactions, while in practice, like many unitswap transactions are dependent on the most popular pools. Right? So this is like fantasy if anything is new list. But it turns out that there are already many efforts in the ecosystem to design parallel DF's.
01:21:26.088 - 01:22:45.172, Speaker B: Just like when we started to have multi core machines, people started to design concurrent and multiprocessor algorithms and I think we are entering the same area and Ethereum is actually lagging behind a little bit and you can see in these kind of papers then other ecosystem already are designing parallel dapps like shorter automated market makers that is promised like five x throughput on SUI and 16 x triple on Solana. And so I hope that our PVM could help bring all these innovative design and more to Ethereum and Odell two s to keep EVM contract design cutting edge. But also like not all usage are dependent. So there are actually many usage that are independent by nature. So we've been in talk with a project called VM that is another l two that use we as these cheap storage backend. And they also support DA commits transactions for other l two s. And all of these DA commits are independent by default.
01:22:45.172 - 01:23:54.144, Speaker B: And so if they plug parallel EVM into their node, it would be like an instant x three for nothing. And so we have been working to put pro EVM into rath and there's been a few weeks of work already. And it's not necessarily easy because we need to make sure that power EVM works correctly and actually consistently perform well. And not seeing some edge cases that is much lower than sequential or it actually produce wrong results, then we actually have to fix all of them before confidently running it inside red. And my favorite edge case is a block where an NVM board interacts with contract, then self destruct it, redeploy it to the same address, interact with the contract again own the same block. And our parallel executor needs to know if a address is a contract or not to apply lazy updates rules and the blocks surely confuse the hell out of it. And so yeah, we still need to work on a lot of testing and benchmarks before we can say that parallel EVM is ready inside rev.
01:23:54.144 - 01:24:57.896, Speaker B: But the overall aim is to at least double or triple the sync speed. So one day we can sync ethereum from scratch in half or just under a day. And if we look at this graph, then basically it's, you know, and also like our current working progress, integration is just slightly above 200 lines of changes. Because power EVM is essentially just a very simple and clear interface, you give me a block, I give you back the state transitions and the execution result with that block. And so it's, if we look at this then yeah, is any, also if you use, if any projects are using RevM in their stack, can just switch Revm over to parallel EVM to get instant spit off with the same interface. And also anyone using red would in the future benefit from this speedup instantly for future plans. So we only started to work on pro EVM I think in early April, 4 months.
01:24:57.896 - 01:25:30.900, Speaker B: And throughout we already seen like four x seven x ten x 13 x 17 x and now it's 23 x for the pick. Yeah. Gas buildup. And I don't think it's stopping anytime soon. And we have several ideas to keep pushing the limit. So one of it is already mentioned by several teams is to support an optional DAC, which is a dependency graph for fullnows to sync from sequence faster. So basically like, you know, Dag would say that okay, this transaction depend on this transaction.
01:25:30.900 - 01:26:49.916, Speaker B: And so a sinking node with that Dag which has executed over there in the right order so that there would be no conflicts and no re execution at all. And so this is also like very important work in the sense that it helps to keep the full node smaller than the sequencer. So these sequencers can be buffed up to very performant, powerful and expensive hardware, but the syncing node can stay relatively consumer friendly. And also one personal favorite is to redesign the scheduler to minimize synchronization overhead. So I'm actually not a big fan of bug SDM scheduler design because all the worker threads kind of share the same atomics, so they kind of like interact with the same atomics every worker iterations and that keep refreshing the cpu cache. So there's always some kind, it's very small, but there's constant latency per worker iterations, which is very annoying. I think ideally we would have like a dedicated scheduler thread that manage all these automates internally and going to put the tasks for each worker thread in their own dedicated memory locations, so that all these worker threads only going to work with their own allocated memory locations instead of having to share.
01:26:49.916 - 01:28:00.926, Speaker B: And only in the case of state conflicts do they need to access to the schedule of memory locations to say that, okay, there's some state conflicts pre revert to the previous transaction index. Other works included optimizing the concurrent data structures. This include the Emmy cache for chain state loaded from disT, and also the multi version data structure for the parallel executor we can also track with JP points to re execute from instead of re executing the whole transaction when there's the conflicts. And one thing I really really want to try for months now is to try and support different EVM executors like just in time compiler that danny I believe, and the paradigms teams shipped pyback. So we'd love to try to plug it in and see how it could speed up probably EVM even more, get even more gear gas. And the last one is to do a lot of low level tuning. So like memory allocators, like when you optimize to the microsecond range, even memory allocators can make a big difference.
01:28:00.926 - 01:29:22.700, Speaker B: And up to now we have been trying like J Malog, SN Malog, mimalog and RP Malloc and the difference is actually very noticeable, up to like 30% to 40% in performance and this add up. And so we are using PMLlog mainly for EVM implementation, but also we believe that in the future when the allocator's API runs mature and get stable, we'll benefit even more by hand rolling our own custom allocators for the concurrent data structures, and to have another good one as global allocators for the whole node or the program that consumes unused part of EVM kernel configurations have a very interesting backstory. So two weeks ago I got back from ad con after a week on the road, and I boot up my desktop, excited to get back to work, and it prompted me to update ujiwuntu to 24.0. And I was like, I mean like a fresh start, why not? And then I updated Ubuntu. I updated the rust compiler for a fresh benchmark and out of nowhere there was a 20% regression. I didn't change any code. I mean the machine even had like a one week rest before that 16 hours day just benchmarking hardcore code.
01:29:22.700 - 01:30:31.294, Speaker B: And why the 20% regression? And after looking around I found out that it's insane. So I think the latest Ubuntu versions, it upgrades the Linux kernel version and actually switch to a news Linux kernel scheduler as the default kernel scheduler. And for some reason this one is 10% better on average, but 20% worse for computing intensive use cases like parallel EVM. And so we have to roll back to the previous kernel version and I have to just alert everyone in the team to make sure that you have the right kernel version inside your machine and make sure that all the cloud instance have the right kernel version. And of course we cannot just get stuck with this kernel version. I think in the future we need to fine tune parallel evM to work well with our most common kernel configurations, and especially to have the best one for our own stack. And another one is an arm in Xt six or AMD six t.
01:30:31.294 - 01:31:40.676, Speaker B: This one is pretty interesting as well. So my desktop is Intel I nine overclock at 5.9. It beats all cloud instant I could ever find easily. But for some reason I spun up awn instance with graviton three recently and it's like only at 2.6. It's literally virtual cpu's versus my physical cpu's, right? And for some reason the cloud instant bit my desktop at Uniswap Giga gas benchmark and that 18 milliseconds is actually from that graviton. And so I think that eventually we also need to do all of these low level tuning to make sure we know exactly what is the best configurations and what is the best setup to deploy parallel EVM on at a higher level to deploy all these performance nodes on. And so yeah, so we are fully open sourced and the PolyvM implementation is fully in Rust MIT license.
01:31:40.676 - 01:32:23.530, Speaker B: And so let's collaborate. If you have any new ideas, feature requests, collaboration ideas, integration ideas, and our new parallel dapp designs, feel free to hit us up. And we're actually also hiring, so if you're interested in doing hardcore low level rust performance tuning works for this and for a new database that we have been cooking on the back, then feel free to hit me up. And again, a great thanks to paradams and the team for hosting this nice event and you know, building alloy revm, rad and a lot more that could make parallel evm happen so quickly. Thank you.
01:32:28.790 - 01:33:19.170, Speaker A: Andrew hi, we have a question from Christian. Thank you for the presentation. I have two quick questions. One is, is it possible to run this optimization for a whole range of blocks so that, I don't know, you kind of have, maybe you can make the syncing faster. And what kind of gains do you think you'd see for like full ranges as opposed to parallelizing each block independently? And the second question is, I saw that you restart transaction execution from scratch upon conflicts. I was wondering if you can hold the intermediary state in some kind of immutable data structure so that you can resume basically from any program like instruction.
01:33:20.870 - 01:33:57.812, Speaker B: Yes, thank you for the good question. So for the first one is definitely a possibility. So I do expect that. So Blastdm essentially takes in an input as a list of transactions so it doesn't really need block headers. So block headers are only relevant in setting things like what is the beneficial account and such. And so we can definitely flatten a lot of blocks and try to run it through parallel evm for potentially more parallelism. And especially if we look at this, then the intuition so far is that the more transaction and the more workload we have, the more parallelism we can potentially have.
01:33:57.812 - 01:34:49.168, Speaker B: Because also, I think until now, the biggest bottleneck that I want to get rid of is when we join threads. It actually has a pretty long latency, especially in zip rust. And so I think that if we can do a lot of stuff before we join the threads and output execution result, the better. And also, that's also a reason why that arm thing that could beat my desktop was because I think arm is very efficient, and so it has very low thrust overheads, and so we can spin up new worker threads and join them much faster. So definitely that's something we want to try in the future. So, you know, especially for historical scene, just buff a lot of blocks together, execute them in parallel. And for the second question.
01:34:49.168 - 01:35:22.776, Speaker B: Yeah, so, yeah, it's definitely the. The first bullet point here. So this, actually, I stole this from the origin block SDM paper. I don't think they have tried to implement that yet, but I think it makes a lot of sense because re executing from scratch is very expensive. We need to reset up the EVM environment and all these configurations for each transaction. That is very expensive. So we just re execute from the repo that flag state conflict, then I assume that we.
01:35:22.776 - 01:35:59.522, Speaker B: Much faster. Yeah. Question here. So, for these future plans, which one of you do you think will be the biggest super boost for real world transactions? I mean, we saw that with independent Uniswap transactions, the performance already amazing. Good enough. But for real world transactions, like which one of these, is it a better scheduler other than block s and T SDM or some other optimizations that's gonna take us there. Yeah, that's actually a very good question.
01:35:59.522 - 01:36:38.190, Speaker B: So, I actually think that the most properly promising gain all of these is the first one is to just support optional metadata for the syncing notes or whatever follows, to run and execute the transactions in a way that there would be no re executions, no state conflicts. That would actually speed things up. I think all of the others are more on is more. It's very hard to speculate how much it would affect real transactions. It could be anything from just 20% or to 200%. So it's a lot of experimenting. Works required to understand that.
01:36:38.190 - 01:37:23.094, Speaker B: But also I believe that the new parallel designs for Dapps would be also required to get the best our Apollo EVM, because we can only do so much to remove explicit read because lazy updates is a very powerful technique, but it cannot be generalized to arbitrary logic. And so I don't think we can just use the ad hoc techniques at the Apollo EVM level to solve all the apps. And they would actually have to design better, less apps instead. Hey, great talk.
01:37:23.142 - 01:37:30.078, Speaker A: Really interesting work. You didn't speak much about what? So the evm wasn't ever designed to really be parallelized.
01:37:30.134 - 01:37:30.730, Speaker B: Right.
01:37:31.270 - 01:37:48.110, Speaker A: I work on l two s. I'm sure a lot of other people who work on an l two would be open to adopting l two specific EVM extensions. So if you could pick one or two things that you would change in the evm to enable more parallelism through the sorts of techniques you're using, what might they be?
01:37:49.810 - 01:38:57.484, Speaker B: I know this is hard to take, but I would switch to another VM incrementally. Well, it's pretty hard to say because I still believe that most state conflicts at the app level. So it depends on the application's logic. So as we can see with the previous example of we VM, they use case are all transactions in the DA commitment uks are independent by nature. And so, you know, we don't need to change EVM to get the max parallelism level out of it. But yeah, I guess like a low hanging fruit would be, you know, to remove, to modify the gas pavement mechanic, because as you can see, this lazy technique can, you know, get that through, but there's still a cost at the end of the block, you know, to fully evaluate these sequentially. And it does add some tail latency to the block execution result, which is very undesirable.
01:38:57.484 - 01:38:59.280, Speaker B: So I would change that to first.
01:39:00.220 - 01:39:09.308, Speaker A: Another idea is introducing increment opcode, which allows you to get around the slow dad store pattern that happens in.
01:39:09.404 - 01:39:09.588, Speaker C: Yeah.
01:39:09.604 - 01:39:16.880, Speaker B: So basically providing more opcodes for Dapps to design better or easier design parallel dapps, that would be very helpful. Yeah.
01:39:17.860 - 01:39:22.614, Speaker A: Okay, one question there, one from zero edge and we wrap.
01:39:22.812 - 01:39:38.550, Speaker E: Yeah, two quick things. The system you described reminds me a lot of like a lazy Mapreduce. Have you looked into doing more dag based approaches to scheduling work based on like, what slots are accessed?
01:39:39.890 - 01:40:18.020, Speaker B: Yes, certainly. So we are. One of another component in our stack is a customized mempool that do some pre processing of the transactions to do statical analysis to detect dependencies to, you know, but this only applies for blockbuilders, but at least like for blockbuilders with this technique it definitely helps by you know, reducing the state conflicts file at runtime which is a lot costlier. And so after that then the block builders can pass on the blocks with this DAC that it already builds to the syncing nodes. Yeah.
01:40:19.920 - 01:40:48.390, Speaker E: Yeah I feel like that's like a really promising avenue but. And then the second question I had is in the how much of a speed up do you have in blocks where you have transactions that actually end up all affecting the same contract? Like in like pragmatically you oftentimes see blocks where it's like all transactions being a swap on a uniswap pool or like a mint on an NFT. Is it's one x or is there like does doing it lazily actually improve it from one x?
01:40:49.250 - 01:41:24.742, Speaker B: Yeah, so lazy is not very easy to generalize. So it really depends on the actual applications. So your ideally application would redesign in a way that reduce share states between the transactions. So that would help a lot. But yeah. So until then, until we have that then we are would be happy to have, I'm sorry just you know, one giga gas per second on average. And having this 21 peak that we aim for and eventually we push the average towards 21.
01:41:24.742 - 01:41:45.730, Speaker B: This is not like from parallel EVM alone, but from the whole ecosystem you know pushing these new parallel design forward, working together to achieve this. And hopefully by the time we have the average case 21 GHz/second we would start to see like new cases that actually reach one tera gas per second peak speed up. Yeah.
01:41:47.950 - 01:41:52.130, Speaker A: All right, last question for zero age and we can wrap.
01:41:53.830 - 01:42:29.810, Speaker C: Hi one concern. With block production being sped up I think on the sync case it's obviously a big win across the board. But in block production if it's faster to produce blocks then there's a danger of producers just withholding the block production to gain more mev. How do you see the MEV issue slotting in with parallel execution and other techniques to accelerate block production or potentially even exacerbating certain issues?
01:42:30.590 - 01:43:03.830, Speaker B: That's also a very good question. I think it's just harder to solve for L1 or existing blockchains. But I think at least for us, the more we get, the more throughput we get the lower block time we can push down. So like you know, of course like you have more power to build different blocks but now you have less time to build. So. So we try to, you know, reduce the block time to improve the UX but also like to, you know, make sure that the best block is built on time and no holding back there.
01:43:06.490 - 01:43:08.150, Speaker A: All right. Uw. Hi.
01:43:08.530 - 01:43:09.470, Speaker B: Thank you.
01:43:20.130 - 01:44:00.752, Speaker A: Okay, so this is the end of the beginning of the day or the end of the talks part of the day. What's going to happen from now is that we're going to start flipping the room. So I'm going to ask everyone in a second to stand up. We're going to start from the left side of the room to the right and replace all the chairs with tables both here and outside. And basically, for the people that stay, the expectation is to enter work mode and hack. Big shout out to our events team for making this happen, our design team for giving everyone very nice flag to cursive for this nice social experience to the speakers that worked extremely hard to get this done on a very short notice. And we hope to have a great day for the rest of today.
01:44:00.752 - 01:44:12.880, Speaker A: Please feel free to hang out until. I think we have it around until 08:00 p.m. and then tomorrow we will reconvene at again the same time. Please make sure to bring your cards. Thank you. All.
