00:00:03.560 - 00:00:40.910, Speaker A: Good morning, everyone. So, since last year's conference, I've been working on a bunch of crypto data tools. And all of these tools share a common goal. We want to create modular legos that we can assemble into new infrastructure and new applications. So today I want to give you a tour of these tools and show you how they fit into the broader crypto ecosystem. So the flow of data in crypto is a lot like a food web. Food webs in biology are a way to visualize how energy flows through an ecosystem.
00:00:40.910 - 00:01:51.530, Speaker A: So at the bottom, you have the energy source, the plants, and then you have animals that eat the plants, and then you have many layers of animals that eat other animals. This parallels the way that data flows through tools in the crypto ecosystem. So at the very bottom, you have the raw blockchain data source, the RPC nodes, and then you have tools that consume that data and process it and share it with other tools using APIs. And then at the very top, we have the interpretation layer, where charts and tables and dashboards make the data consumable by humans. So this is the state of the world today. It's not bad, but we want this ecosystem to always be improving and evolving, so that the way we use data gets better and better over time. One of the most effective ways to put evolutionary pressure on this ecosystem is by building new open source tools that improve the flow of data.
00:01:51.530 - 00:02:48.580, Speaker A: So either allowing data to flow more efficiently or allowing it to flow in new ways. And so this is something I've been trying to do with these four tools. Each of these fits into a different part of the ecosystem, and I'm going to tell you a little bit about each one. And I don't have time to go into the full details, so I'll just sort of give the highlights and do a few tech demos. So starting with Mesc Mesq is a way for all of your different dev tools to keep track of all of your RPC endpoints, kind of like an address book. So imagine you need to work with these different blockchains on the left, and you need to connect them to some of your dev tools on the right. The good news is all of these chains in all of these tools speak the same API, which is JSON RPC.
00:02:48.580 - 00:03:47.640, Speaker A: But the bad news is, this is a configuration nightmare. Almost every single one of these tools has its own unique conventions for how to configure its endpoints. This isn't really that big of a deal if you just have one or two networks, but if you have a lot of networks or a lot of tools, this becomes a very fragile and time consuming problem. You have to configure each tool individually, and you have to keep those configs synchronized over time as you add or remove endpoints. MesC simplifies this problem by creating a single source of truth. It tracks all of the endpoints or all of your endpoints, and then it exposes that info to all of the tools on your system when they need to perform RPC requests. And standardizing the config like this lowers the operational complexity and removes a lot of potential for human error.
00:03:47.640 - 00:04:41.672, Speaker A: So let me show you how this works in practice. Yes. So on my laptop right now, I use Mesc to track 27 rpc endpoints from ten different networks. And so on the command line I can do mask ls to list out these endpoints, and for each one it tells me the name of the endpoint, the network, and the URL beyond the command line. Mesq also has libraries for Rust, for Python, and for other programming languages, so that this info is accessible in basically any environment. Mesc can track a lot of different metadata. So one example is network defaults.
00:04:41.672 - 00:05:46.110, Speaker A: What is the default endpoint that you want to use for each network? So on the command line I can do mask URL Ethereum, and it shows me the URL of my default ethereum endpoint. I could do Mesc URL base, or any other network shows me the default endpoint URL for that network. MeSc has a few options for how to actually store this data, but in its most basic form, it's just storing everything in a JSON file on your computer. You can create one of these Mesc configs either by copying from a template in the mesc repo, or you can do Mesq setup on the command line. And there's this interactive UI for creating and modifying your mesconfig. So to demo this, let me just delete my current mesconfig. If I do mask ls config missing? All of them are gone.
00:05:46.110 - 00:06:17.430, Speaker A: So let's do mask setup. Config file does not exist. Do you want to create one? Yes. What do you want to do? Let's add an endpoint, and we'll just use the classic 8545. That's where I'm running a base node right now. And you can see that it pings the node, it detects the chain iD, and it's ready to add it to the config. So let's name it local base, exit, and save changes.
00:06:17.430 - 00:07:24.570, Speaker A: Now when we do Mesc ls, the endpoint is there, and the endpoint is now usable by any tool that's powered by Mesc. So let me just restore my old config and they're all back. Okay, so beyond just tracking the endpoints, Mesc also tries to make it easy to manage these endpoints, and it has a lot of features for this. So if I do mesc ping, it will ping all my different endpoints, check their latency, and also check their current block number. It can ping a lot of other information, like if you do mes ping client, it will check which client each of these different endpoints is running. It'll also show at the bottom if any of the endpoints are non responsive. So Mesq is still a work in progress, but the overall goal here is to reduce the operational burden by standardizing the config process across tools.
00:07:24.570 - 00:08:20.130, Speaker A: Okay, so the next thing I want to go over is cryo. Cryo is a tool for collecting blockchain datasets. Cryo can take any type of information available over our PC and turn it into a simple local flat data set. So up here I've listed the different types of data that cryo can collect. It's anything from really simple stuff like blocks transactions all the way to more obscure things like JavaScript traces and opcode traces. And when a lot of this data comes raw out of an RPC node, it can be very nested, very messy. Cryo basically packages it nicely into flat tables that are easy for other tools to consume.
00:08:20.130 - 00:09:29.060, Speaker A: Cryo also uses a lot of tricks to make this process fast, and this actually enables a new style of workflow where you can perform all of the data collection and analysis for an entire chain's history on a local machine using open source tools. It's kind of like having your own private bigquery instance just running on your laptop. Cryo was released about a year ago, and from the beginning we put a really big emphasis on making it modular and leaning into open standards like parquet. And for these reasons, we've seen Cryo adopted by many different organizations so far. So this includes data platforms, security firms, news outlets, MeV teams, and lots of different on chain protocols. So let me show you what cryo looks like in practice. So the most basic usage of cryo is just collecting a vanilla data set.
00:09:29.060 - 00:10:16.120, Speaker A: So let's say we want to collect all the logs over some block range. We just do cryo logs, and then let's collect from block 10 million and then 100,000 blocks after that so Cryo will go through, it'll pull that data from the RPC node. Right now it's the local one on my laptop, and it will package all that data into a bunch of parquet files. The interface is really simple. If instead of logs, we want to collect blocks, we just say cryoblocks. It'll collect that over the same range, and so that collected 100,000 blocks in about 2.2 seconds.
00:10:16.120 - 00:11:03.790, Speaker A: If you want to list out the different types of data sets cryo can collect, you can do whoops, you can do cryo help. Datasets lists all those out. If you want help about a particular dataset, you can do cryo help. Name of dataset so like logs, it will show the schema for that dataset, and it will also show what parameters are possible for that dataset. Cryo uses mask for its RPC configuration. So right now it's just collecting from my default Mesq endpoint, which is that local base node. But I can specify other endpoints in a few different ways.
00:11:03.790 - 00:12:19.120, Speaker A: So the standard kind of thing that a lot of tools do is you do something like RPC and then give it a URL like that. Cryo can do that, but because we're using mask, we can also do things like just give a network name like ethereum or base or whatever other network you have configured, and mesc will resolve that into the appropriate URL based on what you've configured. Cryo is also item potent. So let's say we want to collect over a longer block range, a longer lasting job. So let's say a million blocks instead of 100,000, it'll start the job and I can kill it at any point in the middle and restart it without issue. And this is really helpful because Cryo will make sure that none of the files end up being corrupted at the end of the day. So you have a lot of flexibility for like if you need to break the extraction process up into multiple chunks, that's fine.
00:12:19.120 - 00:13:26.900, Speaker A: And then if you try to queue up that job again, Cryo will detect, oh, that data's already collected, I don't need to recollect it unless you explicitly ask it to. Cryo has a lot of other options as well. You can list these with Cryo HDD, you can export as CSV, you can filter things, there's a lot you can do. And then finally cryo has a Python interface that's pretty useful. So if we do, we're in a Python session, we do import cryo, and we can do something like cryo or DF equals cryo collect. The Python interface has all the same options as the CLI, and let's say we want to collect very similar data set as before. It will collect that data from the RPC node and it will put that into a polar's data frame DF that we can now use with all the standard Python patterns.
00:13:26.900 - 00:14:24.820, Speaker A: So this is just a super fast and easy way to load data in Python. So the next tool on the list is TBL or table. TbL is a swiss army knife for reading and editing and managing parquet datasets. So for those that don't know, Parquet is a file format for for tabular data, kind of like a modern CSV. In many cases, Parquet can be a simpler and faster alternative to using an actual database. And for this reason, Parquet has become a very common way for people to store and share different datasets. So let's say you have a bunch of parquet files, either from cryo or from some other data source.
00:14:24.820 - 00:15:26.110, Speaker A: You might find yourself wanting visibility into those files, or you might want to make some quick edits to those files. But up until now there hasn't really been a quick and easy way to do these things with parquet. And that's why I built table. Table makes it easy to look up info about the schema and the contents of your parquet files, as well as make a lot of edits to them. So let's look at some examples of TBL in action. Okay, so TBL can tell us a lot of info about that cryo data we just collected. So if we do tbls, that will list out the parquet files and also tell us a bunch of metadata aggregated over those files, like the total row count, the file count.
00:15:26.110 - 00:16:25.218, Speaker A: Another thing we can do is look at how those files are structured so we can do TBL schema, and it will list out the two different schemas that are present in these files, one for blocks and one for logs, and for each schema. Each line here represents one column in the table, and you can see the column's name, its data type, and its storage size. And this can be really useful for just taking a peek at how your data is structured. Another thing table can do is make some quick edits to those files. So let's start by just doing a little preview of the first couple rows. Let's look at the logs. Okay, so that's a little preview.
00:16:25.218 - 00:17:00.948, Speaker A: Let's imagine that we need to modify the last column chain id. We need to change it to a different chain id, let's say like 55. We can just do set chain id 55. It makes those changes. Maybe we also need to change the type from U 64 to U 32. We can do cast chainid U 32 and it makes those changes. Right now it's just printing the result to the console.
00:17:00.948 - 00:18:00.758, Speaker A: But TBL has a lot of options for how to actually output these results. So the most maybe basic version is we can just edit all of the different files in place. And we can do that by just doing dash dash in place. And do you want to continue? Yes, I haven't parallelized this part, but I will in the future. It's going through all those files, editing them, and now if we do the preview again, we can see that those edits are now persisting in the files. Another thing we can do as an output mode is merge the results from all these different files into a single new file. So if we do output file test parquet, do you want to continue? Yes, it will now output everything to this single file.
00:18:00.758 - 00:18:58.250, Speaker A: It will take a moment because it's not parallelized ll and now this test parquet is present in the directory. A final alternative to quickly open up these files is in a Python console. So if we do something like TbL base logs df, it will open up a Python session and load that data into a data frame that we can now just do all the standard python patterns with. So this is just a very nice convenience for if you need to quickly mess around with some data in Python. So that's Tbl. Everything I've shown here might seem really simple and trivial, and that's because it is. But up until this point, there wasn't really an easy way to do it.
00:18:58.250 - 00:20:18.590, Speaker A: And this is important because if you can do these types of operations in 2 seconds instead of 20 seconds, it really does change your relationship to the data, and you end up interacting with it a lot more because the friction is gone. The final tool I want to talk about is LBL, and this is just a quick preview, because LBL is still a work in progress. LBL is a tool for managing label datasets. Address labels are one of the biggest remaining challenges in open source crypto data, and managing these labels can get extremely complicated, especially if you're dealing with lots of chains or lots of different data sources. And there are some nice projects out there for standardizing address labels, like the Open Labels initiative. But there's still a gap in tooling. How do we actually connect the label data to the other tools in the ecosystem? LBL is the first tool built specifically to manage label datasets.
00:20:18.590 - 00:21:29.780, Speaker A: So, for example, LBL will make it so that you can load any data set you want in just a single line of code, whether it's derived from on chain data like ERC 20s, or. Or Uniswap pools or chain link feeds, or whether it's from Dune. And maybe you want to get Hill Dobby's sex dataset, or maybe it's your own private label source that you made yourself that you also want to manage with the same interface. And I'm not going to do a tech demo of this one just for time purposes, but check back in a couple months and we'll have some cool stuff here. So those are all the tools I wanted to cover today. Ultimately, we're trying to improve the way that we use data at every stage of the pipeline. And the common approach here is that we're really embracing modularity and we're embracing modern best practices, with the ultimate goal being that this gives us a very powerful set of legos that we can assemble into the next generation of infrastructure and applications.
00:21:29.780 - 00:22:17.650, Speaker A: So that's it. Come find me if you want to chat about any of these topics. And thanks for waking up early to make it. Any questions for storm? Yes. No statements, only questions. Do you support this? Does TbL support s three files or it's just local that's on the roadmap? That's a very big goal right now, but it's not there yet. That would be useful.
00:22:17.650 - 00:23:23.030, Speaker A: How's cryo so fast? How did it pull that many blocks that quickly? Is there some secret sauce that you can share with us? The secret sauce is rust. I made almost an identical tool in Python, and I did every trick in the book that Python has for multiprocessing everything like that. It was still an order of magnitude slower than just doing everything in rust. And this is sort of built on top of Tokyo's NPSC multi producer, single consumer type flow, where you just divide up all the tasks and then do each one in parallel with some, like semaphore. But there's actually, like, even more room for optimization. I just haven't gotten around to it because it's been fast enough for a lot of my purposes. But, yeah, it's very convenient.
00:23:23.030 - 00:23:30.270, Speaker A: It'll basically push most nodes as far as the node can go, and cryo isn't the bottleneck.
00:23:32.290 - 00:24:13.370, Speaker B: I have a question. And also, we've been using cryo effectively to generate benchmarks for ref because it's just a realistic usage of how you'd expect people to use a tool and that can give you numbers that are actually reliable versus having to define a benchmark. You just capture a workflow, and that's your benchmark. I have a question. In storm, where last year you walked us through how. Okay, there's actually a very powerful open data stack out there waiting to be leveraged to basically commoditize a bunch of the things that the clouds give you. And you just gave us an overview of how this captures the data lifecycle of, like, a data engineer, data scientist.
00:24:13.370 - 00:24:23.850, Speaker B: Where are we going from here? So what is the vision of this local first processing pipelines and why and where do you want to take things?
00:24:24.190 - 00:25:33.510, Speaker A: Yeah, so, I mean, the most basic way to answer that question is the boring version, which is with all these sort of tools ready and available, it becomes very easy to replicate what's already been built that might be commercialized right now. Like if you wanted to build your own etherscan or your own dune or your own whatever, it's becoming more and more possible to assemble these off the shelf components into whatever your vision is, and then sort of more broadly, everybody needs data at some point. I listed a few of the people that are using cryo right now, like security firms, news outlets, mev teams. We want to make it so that people are empowered to basically get whatever data they want by just using these off the shelf components. The really interesting answer of. Of where is this going? And, like, what is the data endgame? And what becomes possible. Now, that's basically like a big question box.
00:25:33.510 - 00:27:00.278, Speaker A: We don't know yet. Basically, we're creating a bunch of potential energy, and we don't know how that potential energy will be released, but it's building, building, and people being creative in the space and sort of like having their own vision, basically using these components to build better and better foundations for the actual thing they want to build, I think that's probably the most exciting thing. Is it possible to use cryo as an indexing service, like live polling blocks that was newly created? Yeah. So the ergonomics on that, there's still a little bit of improvement we can do, but it can basically do that. So Cryo has some settings for dealing with Reorgs, and if you want to create a reorg delay or you want to include certain information that makes it easy to process the reorg we have that one feature that we really want that we don't have yet is just completely seamless handling of reorgs so that you can do a more streaming type situation. But like, cryo, for example, can do like, individual blocks really easily, really fast if you want to just like stay at the very tip, or it can do a delay. Like, don't collect anything from the latest, like 100 blocks because I don't want to deal with Reorgs.
00:27:00.278 - 00:27:54.960, Speaker A: You can also do that. Thanks for the presentation. Are there any aspects of using parquet to do this that are more complicated in the long run than a relational database, or more annoying or anything along those lines? Yeah, there are. There are a few. One thing is that the indexing capability in Parquet is not as powerful as something like postgres. So if you have something where you need to perform simple range queries along one dimension, Parquet can be really good for that. You can sort of structure your file from the ground up to support that specific query.
00:27:54.960 - 00:28:25.720, Speaker A: But if you need to do something like, I don't know what all my queries are going to be, I need to query by three different fields all at the same time. Filtering those, making it efficient, making it scale to billions of rows, then Parquet is probably not the best solution. Great, thank you. Hi everyone.
