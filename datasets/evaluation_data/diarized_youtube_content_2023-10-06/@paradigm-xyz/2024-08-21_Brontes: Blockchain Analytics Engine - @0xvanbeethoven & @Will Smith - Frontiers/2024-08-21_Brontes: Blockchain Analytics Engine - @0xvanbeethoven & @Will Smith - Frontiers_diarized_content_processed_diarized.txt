00:00:00.600 - 00:01:00.640, Speaker A: Like L2, L2s that are based on the EVM. So why brontes? We're not masochists. We have dealt with trace data in the past. Classifying it, pre processing it, normalizing it is just absolutely painful. And personally, we don't really like that step as opposed to the interesting analysis that we far prefer, like focus our time on. And so Bronte's really the main initiative is to reduce all of that work, ensure and streamline the entire preprocessing portion of the work so that you can jump immediately into the analysis portion without having to worry about how to decode curves 56 pool implementation or how to implement discovery. For a factory that produces 15 different contracts at a time.
00:01:00.640 - 00:02:05.770, Speaker A: All of that have been such a significant bottleneck. And this was really aimed to completely mitigate that. The other thing is that we're working on MeV, we're working on minimizing it. And although everyone and their mothers love to chat about MeV, unfortunately we don't have that good data today. There's great research on the validator payout side, there's great research on research on the timing game side, but in terms of data on systematically identifying actual MeV transactions and providing accurate kind of P and L statistics, we haven't seen that much. And on top of that, the, there isn't really an open source place where we can all kind of participate and contribute to the discussion on how we want to classify, how we want to improve the taxonomy. So what you end up needing to be able to do, this is basically akin to a feature complete block explorer with extra steps.
00:02:05.770 - 00:03:02.740, Speaker A: This is something we didn't necessarily realize in the beginning, but we kind of pushed through painfully. Um, so you need to be able to decode and classify these complex defi interactions. But the very important thing here is that you have to normalize them so that you can operate on a unified type, so that when a uniswap swap or a curve swap happens, you don't have like drastic different data representations. The other thing is that you don't want to lose the context that you gain from operating at the trace level, because when you're operating there, you have the call frame hierarchy, and then you can use that to infer many different things. So in this normalization and classification step, we really wanted to maintain that level of context. Then the second thing is that you're going to need a lot of off chain data for this. So you need an efficient way to store it, an efficient way to read and write to it.
00:03:02.740 - 00:04:24.270, Speaker A: One more complicated thing is you need prices for almost every single token and you need them at a transaction level granularity, because if you're looking to accurately price a sandwich, if you're looking to accurately price an atomic arbitrage, you can't depend on Coingecko's API. And trust me, we initially tried. Their rate limiting is also just not conducive to how blazingly fast Brontes is. And then the last thing is you need extensive metadata on addresses, on contracts, also just more simply on builders and searchers because you can't really accurately compute the p and L because the interactions in a lot of these searchers between the searchers and builders are quite complex. And I are not straightforward where you'd basically mark out a bundle as negative in p and L if you didn't know about the special deal that the builder and the top searcher had, or the fact that Manta builder, which was Jane street, or another unknown builder by the name of Beaver, would actually send all of their profit directly to the builder. So the searchers would look unprofitable. You need like that global context because otherwise you just can't have accurate numbers.
00:04:24.270 - 00:05:06.060, Speaker A: There we go. And so putting all of that together at the high level, you can think of the pipeline as we have a block, we're going to trace it, we're going to build the tree where we do the classification and normalization. And at the same time we're going to fetch the metadata. And then after all of that, we've collected the necessary data for our analysis and we can operate our inspectors. So to collect and normalize actions, basically what we use is a custom ret tracer that matches logs with the core frames and then a macro system that provides a really clean interface. We'll tell you more about it.
00:05:11.000 - 00:05:30.896, Speaker B: All right, so to start, what we basically do is we collect all the traces in the transaction tree or in the block, of course. And then for each call frame, for each transaction, we go through a process, click the next one.
00:05:31.008 - 00:05:39.304, Speaker A: Yeah, I'm going to try and point out. So, yeah, so here you have the individual transactions and then they get processed at that point here.
00:05:39.352 - 00:06:13.898, Speaker B: Yeah, right here. So if it's a call, it goes over to our protocol classifiers that will be on the next slide. And if it's a discovery, it goes over to the discovery classifiers. This also will trigger a pricing update which then says, hey, this call frame used. Well, if this is an ERC 20, we need pricing at this exact transaction index in the block. All right, I think we skip this one, yes.
00:06:13.954 - 00:06:42.510, Speaker A: So this very similar. Basically, the protocol classifier is just a dispatch that you do on an address that is labeled as being part of a specific contract or slash protocol. So for example, Uniswap v three would be labeled as Uniswap v three protocol. And then we'd have the function call as the additional portion of the match key that enables us to match against the specific function and then decode and then classify the data.
00:06:43.050 - 00:07:29.232, Speaker B: Yeah. So for this, we built a macro. The goal of the macro was just to simply abstract away and give just a really clean interface for dealing with all of these unclassified types and call data. It's kind of very messy in the back end if you for every single one, are doing all this decoding manually. So we thought it'd be nice and clean to put it in this. So basically, all you have to define is the path to the protocol module, the path to the alloy function call type, log type, and what you want your closure that actually does the logic behind it to be passed in. So here.
00:07:29.232 - 00:07:40.470, Speaker B: Oh, there we go. Okay, so here is what Uniswap v three mint call looks like. Oh, keep breaking it.
00:07:40.890 - 00:07:41.490, Speaker A: All right.
00:07:41.570 - 00:08:24.940, Speaker B: All right, there we go. Yeah, so you can see here, protocol, and then the mint call, that's just the first two entries are basically how we create the key that people match on the action. Type the log that you want. And for here, we just want everything. So with that, you can see, I mean, it's probably twelve lines of code right here, just reading from the database to grab protocol info, take the deltas, return the mint. It's really that simple for pretty much all protocols except curve. And then on the flip side, we have the discovery.
00:08:24.940 - 00:08:49.720, Speaker B: So it's another macro surprise. We have the create pair call, and then just the factory address. And I mean, this is even more simpler, right? It's just take the two Kokens, name the protocol, insert into the database, and for almost all of the discoveries we have out here, it's this clean, which is really nice, and it's super simple to test as well.
00:08:50.040 - 00:10:00.490, Speaker A: And the nice thing is, so as soon as a create pool is effectively discovered, that gets stored in the database, and then any time that there's a function call to that contract, we'll then run the action dispatch, which will classify the underlying swap or mint or burn or whatever interaction. So this is really like an extremely zoomed out overview because it would take an hour to present fully on this. I think this is probably the most complex portion of the entire project. So basically, the main problem is that you can't afford to load all of the state of all of the pools. If you want to be as comprehensive as we were trying to be. And you need to find a solid enough proxy to be able to identify what the optimal route is so you can mark out an instantaneous price at that specific transactions index in the block. So what we did to simplify, and again, not perfect and very much room for improvement, is we use a global token graph with the connecting pools as edges for the token nodes.
00:10:00.490 - 00:10:46.654, Speaker A: And we use that connectivity as a proxy for liquidity, which enables us to build subgraphs that are more focused where we can actually afford to load all of the state for each pool and then compute the optimal route and then identify the instantaneous price. And so we're able to price basically any token that is on uniswap v two, v three swap forks, we started implementing curve. We have, we don't have the willpower for that. So if anyone wants to take that on, that would be great. Thank you. Yeah. And then the separate portion of the data that you're actually provided is the block metadata.
00:10:46.654 - 00:11:35.288, Speaker A: And this is a really valuable portion where you store all the data that we could possibly think of could be useful in the analysis of any financial interaction. So you have the relay data, the builders, the bids, the fee recipients, the builder metadata with connected searchers. So what searcher addresses are vertically integrated with Beaver? What searcher addresses are vertically integrated with Rsync? Basically any form of metadata that you could think of on the builders, even their IP locations, but we're not sharing that. And then you have the address metadata. So labels from various providers. Then you have the labeling from private transactions. Huge shout out to chainbound for that.
00:11:35.288 - 00:12:29.638, Speaker A: Thank you. You guys did an amazing job and made it super easy for us. And then you have the propagation time for the blocks. And the biggest portion of the data are all of the trades and quotes for 95% of the pairs across the top five crypto exchanges. So to inspect a block, now that we have all this data, now that we have this classified block, we can move to the interesting part where we provide the block tree and block metadata that I mentioned right before, and we pass it into the individual inspectors that all run in parallel. And so those individual inspectors then produce their results. And there's a final processing step where we can compute the final block P and L and also de duplicate the results.
00:12:29.638 - 00:13:25.090, Speaker A: And this is one of the surprises that we had while building this is that if you miss out on the de duplication and filtering step, you're not going to be able to label things accurately because the MEV can look exactly like another type of MeV if you're not able to filter it out. So say, an atomic arbitrage and a sex Dex trade. The atomic arbitrage is obviously getting an advantageous rate. If you aren't able to identify that as an atomic arbitrage and only run, say, a sexdex inspector, well, then now you're labeling it as Sexdex. So that is like, super important to de duplicate. And the funner part in the processing is actually the composition where you can take JIT attack and then realize that it's actually jitsexdex, or you can take a Jit and a sandwich and realize that those are actually the same bundle. And so then you can kind of compose these more, these simpler mev types into complex types.
00:13:27.200 - 00:14:25.316, Speaker B: All right, so we started off with this whole thing as mev, mev, mev, but then we realized, as Ludwig mentioned in the start, that it kind of is applicable to anything. So to, so, like, if you wanted to build an inspector that just did something super simple, this is all you need to implement, right? It's just you define what block window you want to look at what you're going to return as a result and then just write your code. Right. So no picture again. There we go. All right, so, and then at the higher level, right, we have these things called processors, which is just a collection of inspectors processing that generate the same result. So all you need to do is implement your process result function and then create your config, right.
00:14:25.316 - 00:14:43.262, Speaker B: And you pretty much can do whatever inspecting you want, any analysis that you want. All of the complicated actions are just all hidden away. So it's super easy to build whatever your mind can think of really and.
00:14:43.326 - 00:15:47.800, Speaker A: Quickly to touch upon, like the block window portion. So that's a feature we actually very recently added. But to explain this basically enables you to say, okay, I want five blocks in a row directly provided as data. And so this is something that we haven't used that extensively in the code base yet, but is super useful if you're trying to build, say, EV intent inspectors that look across multiple blocks and see transactions in previous blocks that are a signal for a statistical sandwicher that is going to look at approvals on token contracts to then front run transactions. And finally, it's blazingly fast, at least kind of. Again, we don't have benchmarks, but it runs in two main modes, historical ranges, which are broken up into chunks and then those individual range executors will all run in parallel. And then at the same time we have a tip inspector that is specialized to follow the tip of the chain.
00:15:47.800 - 00:17:06.390, Speaker A: Then in terms of futureworks, we'd really love to see support for l two s. You don't have to do that much for Opreth, for example, you have to do a tiny bit of a refactor on the discovery and just update the address book to match the latest chain. Then we'd love to improve the MEV inspector methodologies. Sextext is always going to be an estimation, but the closer we can get to that, the better. And also in terms of long tail MEV inspection analysis, it's very hard to filter out a lot of false positives, but with a decent amount of work, I think we could get something really compelling and then ev intent inspectors are really high on our roadmap cross chain MEV inspectors, although I am still strongly of the belief that this is shared sequencer siops, but I'd love to be proven wrong. One other kind of very off topic feature that we'd really love to see is a taxes feature with brontes, because you have all the transactions, you have an ability to decode them, you have all of the prices and you know when and address traded. So it wouldn't be too hard to actually be able to give you a almost perfect report of all of your on chain trades.
00:17:06.390 - 00:17:24.960, Speaker A: Although the actual legal calculation of how much you want to pay is probably not, not that simple. But moving over that and then improving the speed and reliability of the DeX pricing and extending it to support more protocols would be really amazing.
00:17:26.860 - 00:17:40.828, Speaker B: Yeah. So just shout out to everyone over this past probably eight, nine months of just constant work on brontes. I mean, Joe especially. Yeah, yeah. Went through a lot.
00:17:40.924 - 00:18:11.424, Speaker A: Yeah. Had to implement hundreds of bespoke connections to all of the varying centralized exchange streams. So huge shout out to him, because otherwise this wouldn't have been possible. Then, in terms of resources, you can scan here, there's the GitHub and then the book. Leaving it up. Oh, yeah. Quick acknowledgments before I'm done.
00:18:11.424 - 00:18:47.330, Speaker A: Sorry. So, huge shout out to the RET team and the alloy team for building the primitives that made this possible. Special mention to Danny Popes, because we initially tried to use the sol macro on every single verified contract on Etherscan, and I can tell you we found a few edge cases like the. What was the keyword for rust? Bunch of ridiculous stuff. And he patched up all of that. So huge thanks. And then big thank you to gaussian process and Kevin for reviewing our inspector methodology and kind of going through the results.
00:18:47.330 - 00:19:18.380, Speaker A: So huge thanks there as well. Contact us here. And we're also hiring for Rust devs, but, yeah, that's it for us. Thank you so much. Oh, and this is a quick sneak peek at the front end website that we're going to be spinning up very soon. Any questions for. Shit, I forgot the questions.
00:19:29.320 - 00:19:49.814, Speaker B: Yeah. Thanks, guys. Great presentation. I'm actually curious about how do you get the metadata regarding the private order flow like you said? Okay. We have metadata about the searchers and the builders and their private deals, etcetera. Considering they're private, how do you get the information regarding that?
00:19:49.942 - 00:20:27.910, Speaker A: There's a media trained answer, and then there's the real answer. The media trained answer is that we are on chain sleuths and do extreme data analysis. The real answer is that you can contact all of the competitors and get them to snitch on each other because it is in their incentive to give you information on how their competitor is operating. So that has been an extremely fruitful source of information. Yeah, thanks. It looks super cool, and I'm excited to see this dashboard that you just had. Go live.
00:20:27.910 - 00:21:04.864, Speaker A: So given all of this, the rich software you have for the analysis of things and all the analysis that you've done with it, what's one surprising thing that has come out of this? So one of the. There are a few. There are a few. Sextext is not as competitive as people think. Builders are on paper, you know, clean, not sandwiching. But if you mark out a lot of the sandwiches, they don't appear profitable. And that is for a simple reason that they are working together.
00:21:04.864 - 00:22:11.516, Speaker A: So they are shifting the responsibility on another party. The other thing is people love to make fun of, well, at least in the very niche MeV research community, people love to make fun of just in time liquidity attacks because everyone says, oh, yeah, they're not profitable. You're basically always better off sandwiching, which is almost certainly true. But when you mark out jit attacks, they are 99% of the time unprofitable until you mark them out against a centralized exchange and you realize that this is not a just in time liquidity attack that is trying to steal the fee from a liquidity provider, but rather this is a sextext arbitrager. There's also a builder that happens to realize that you're trying to trade at a discrepant price, but that price discrepancy is within the fee bound. So they don't profit from arbitraging, from trading against the poor directly because they pay the fee. But where they do profit is if they provide the liquidity, they get paid the fee and they get to clear you with this discrepant price and have effectively swapped.
00:22:11.516 - 00:22:37.290, Speaker A: But that was like a really interesting kind of point that we came to understand, because you would expect Sexdex to really be bad for LP's. And everyone says, oh, Sexdex is great for the chain in terms of getting prices in line. But it turns out when your builder is also a sextext arbitrager, you as a swapper are also getting a pretty bad deal.
00:22:45.240 - 00:23:01.296, Speaker B: What do you see as the most prevalent forms of Mev after this analysis? There's a lot of atomic arbitrage that we've seen, but mostly sex Dex. There's sex decks almost every block.
00:23:01.408 - 00:23:24.990, Speaker A: Yeah, it's sex Dex, and I'd call it sandwiching, but it's really just Jared's trades at this point. It is insane how much this guy trades. The top three counterparties on uniswap are wintermute, beaver, and Jared.
00:23:26.450 - 00:23:27.234, Speaker B: Cool.
00:23:27.402 - 00:23:28.282, Speaker A: Really cool code base.
00:23:28.306 - 00:24:35.912, Speaker B: I'm excited to take a dive in and see what's under the hood. One of the questions I had was if you could elaborate a little more on the block inspector process and how you actually create this sort of compound classification between different strategies like JIT and then Sexdex as well, like, what is the actual control flow that's being able to classify this compound behavior. So we kind of glossed over this a bit, but we have this tree that is just the hierarchical structure of all of these normalized and unnormalized actions. So then with these inspectors, you more or less just get past in this tree and a bunch of metadata pricing. So with that, for example, with like a sandwich, we look, we just go through all of the traces, mark out the same addresses, take those addresses, take those possible victims, and then look at the pool overlap, say, oh, this is trading on this pool in this direction, victims in the this direction, back run in that direction. You got a sandwich, and you have.
00:24:35.936 - 00:25:15.640, Speaker A: Nice utils where on the tree you can say collect, and then you pass the action type. So it's like collect swaps, and this will give you all transactions where a swap action has happened. And then you can also collect these specific actions out from each transaction. In terms of the composition aspect, that happens really at the last. Once all individual inspectors have run, you have two things. You have the filtering, deduplication, and then you have the composition step. The filtering says, if there is a type of mev, this type of mev and this type of mev give priority to the one that you decide.
00:25:15.640 - 00:26:06.028, Speaker A: So this is useful. For example, atomic arbitrage versus sexdex, because atomic arbitrage will be stricter in terms of what we can classify it as. Um, but a sextext will look like a positive p and l. Um, and then for the composition step, again, it's just the results from the JIT inspector, for example, and the sandwich inspector, which will naturally overlap if it's a JIT sandwich. And then you can declare a specific composition function which takes these two individual bundles, at least as they're built, but are actually the same bundles, and then unifies them into a single complex mev type type. Okay, we have time for one more question. Now that you have all of this data and you've kind of had time to process it, maybe, how does it affect your outlook on the designs that the Ethereum research community is looking at.
00:26:06.044 - 00:26:08.404, Speaker B: As far as proposal builder separation or.
00:26:08.492 - 00:26:13.620, Speaker A: Any of these other ideas, what you would recommend or urge people to do, because you mentioned people wanting to talk.
00:26:13.660 - 00:26:14.714, Speaker B: About it, of course.
00:26:14.892 - 00:27:15.200, Speaker A: Like what actions or things are you guys kind of like thinking of after doing this analysis? I'd say like two things. Now we have the data. I don't think like, having this data is enough for us to kind of have a super opinionated view on what is right and what is wrong in terms of the PBS roadmap, given that there's so many second order effects, possible solutions that we might imagine. So that's why I try and be very careful on giving very specific kind of guidelines there. But that's also a very boring answer and doesn't tell you anything. So the second portion, and disclaimer, I'm very much talking my bag here, because this is what we're working on. But what we saw is that this idea that there's so much like General MeV is kind of psyops in the numbers, 99.9%
00:27:15.200 - 00:28:13.600, Speaker A: of it is just all the same. Even liquidations don't matter that much. It's really just sandwiching atomic arbitrage, sex Dex in no particular order, by the way. And from that, we know that applications that are being designed today have, or at least claim to have immediate solutions towards it. So if anything, I'd like to see less of a focus on handling the redistribution and prevention of extraction at the base layer, because I think that that very quickly becomes quite opinionated and rather push it to the application layer, but that has significant kind of trade offs, one of them for kind of applications that control their sequencing, and the other is just complexity. If you're an app dev and you're building on a financial primitive that is known to generate and produce large amounts of MeV, you now have to handle that yourself and be a big boy about it. And that is not easy.
00:28:13.600 - 00:28:19.780, Speaker A: So, yeah, a lot of trade offs. Thanks, guys.
