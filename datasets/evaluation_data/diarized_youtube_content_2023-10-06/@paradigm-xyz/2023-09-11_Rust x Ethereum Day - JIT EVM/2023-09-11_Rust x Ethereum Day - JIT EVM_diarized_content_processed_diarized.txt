00:00:00.200 - 00:00:50.572, Speaker A: Right. Hi everyone. It's great to be here. I'll be talking for the next five minutes about JIt EVM and what's this about? So the idea here is actually pretty simple. Can we have an EVM that creates machine code at runtime for hot functions or contracts? This is a technique that is everywhere in other virtual machines for other programming languages, for example JavaScript or Python. And why would we want to do that? Well, hopefully it's faster. Okay, so I think this pretty easy kind of straightforward idea, and what I'm going to show you is a little bit of results of kind of a proof of concept playing around with this idea that I did last year over the summer at paradigm.
00:00:50.572 - 00:01:20.260, Speaker A: And since then I've gone back to kind of my day job, which is to be a PhD student here around the corner at Stanford. And their my day job is not so much writing production rust code as it is writing papers. So, you know, this project needs a little more love than I can give it at the moment. And so, you know, if this is interesting to you, please don't hesitate to reach out. The code is on GitHub and you know, we're very helpful. We're very happy to help you get started. Cool.
00:01:20.260 - 00:02:13.222, Speaker A: So why is it now the right time to do Jitt VM? So initially, you know, this idea of having a JIT EVM has been around for quite a while, and for the most part, during that time, EVM was thought as in the context of consensus and execution. Doing a just in time compiling virtual machine in the context of consensus has certain challenges. For example, the just in time compiling pipeline is fairly complex. You might not want to have that on the critical path that is critical to consensus. There's questions around how this behaves, if somebody feeds it with maliciously created code. And so this idea has been a little bit like, not sure we want to do that, but things have changed a little bit. I think Storm in the morning gave us a good idea of new use cases, essentially where we're using the EVM now.
00:02:13.222 - 00:02:45.170, Speaker A: One of them is definitely testing or fuzzing, for example, right there. You're running this on your own contracts. You're not going to give it a maliciously crafted input. So if you can get performance speed up here, that would be great. The same is for me, searching for chain analytics, all this data processing, predicting the outcome of transactions in your wallets, et cetera. So this is where a faster EVM, perhaps one that uses JIT, would be useful. How does it work? Trying to explain this in one slide.
00:02:45.170 - 00:03:41.080, Speaker A: The basic idea is as follows. You start out with a contract in EVM bytecode. You do some analysis on the control flow to segment this into blocks that get executed together. And then you use in the proof of concept implementation, we use this crate called inquiry to transform this into LLVM intermediary representation. So what happens in this process, basically is each of these blocks contains a bunch of opcodes, and each of these opcodes gets translated kind of in a, in LLVM IR. And you know, I show here kind of the example implementation of the addition opcode, right, where LLVM kind of can do integer additions kind of natively. And then, you know, running such an operation basically means popping two elements off the stack, running the operation and pushing the result back.
00:03:41.080 - 00:04:22.684, Speaker A: And you can see this proof of concept. There is no gas accounting here. And you know, mind you, this is all a year old code now. So with the recent new developments around REt and RevM, et cetera, hopefully there would be a better way to implement this today with not as much code reuse as kind of reimplementing an interpreter. But then once you have translated your EVM bytecode into this IR, you basically hand it off to the LLVM pipeline, and it basically does the whole rest for you. So it kind of optimizes the code and turns it into machine code. And the machine code can basically operate directly on memory and stack.
00:04:22.684 - 00:05:27.038, Speaker A: You just point it to a region of memory that it can use for that. And if it wants to do something a little more complicated, like accessing storage or calling other contracts, then it would probably call back into the EVM and say, hey, can you set up this other piece of code that I would like to run? And then this is my last slide is a little bit broadening the discussion a little bit. I think we should think about when and what we know about the code that we're about to execute. And there's the usual spectrum between knowing stuff only at runtime and what do we know already at compile time and having the usual interpreter EVM basically corresponds to saying, well, you know, at compile time we really don't know anything or don't know much about the code that we're going to execute. So we have to interpret everything. And just in time. Compiling EVM kind of pushes this kind of towards compile time, not really doing it at compile time, but saying, okay, if today I'm running a certain piece of a certain contract, then probably I'm going to run the same contract tomorrow.
00:05:27.038 - 00:06:08.290, Speaker A: So maybe it's worth it, translating it into machine code today so that the execution tomorrow is faster. But I would argue there's a whole design space to be explored. You can probably push this even closer, even further towards compile time. You could call this ahead of time EVM. And the idea basically being that most of the stuff that we execute is a relatively small set of contracts, and they don't change a whole lot, month to month. And so maybe we can compile them already at the time where we're creating the binary of the client. So, yeah, that's about the pitch.
00:06:08.290 - 00:06:15.790, Speaker A: If any of this sounds interesting to you, or you just want to chat, you want to know more, please find me or George Os. And thank you.
