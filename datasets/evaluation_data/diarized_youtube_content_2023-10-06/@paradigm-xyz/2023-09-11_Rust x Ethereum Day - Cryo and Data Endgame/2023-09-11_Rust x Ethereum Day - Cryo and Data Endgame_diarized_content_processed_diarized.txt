00:00:01.200 - 00:00:50.520, Speaker A: So, hi, I'm storm. I work on data at paradigm, and for the past few years I've been experimenting with many different approaches for analyzing crypto data and for building the surrounding crypto data infrastructure. And things are very exciting right now in the data space. And this is thanks to a bunch of new rust tools that I think will fundamentally change the way that we sort of think about crypto data. So today I want to share some of the approaches that I've been experimenting with and sort of give my take on where I think the space is going. So a fun buzzword in the zeitgeist right now is the end game. It's the idea of what is the ultimate system that we're working toward.
00:00:50.520 - 00:01:45.280, Speaker A: And so in the context of data infrastructure, what is the data endgame? There's a bunch of problems in the data space that we need to solve or that an endgame solution needs to solve. So first of all, right, now, even though blockchains are public ledgers, actually collecting crypto data from these blockchains can take a huge amount of time and energy and expertise. Instead, we want this process to be fast and cheap and easy. And then there's also just basic data concerns, like we want our systems to be modular, scalable, robust. So we have a pretty good idea of the properties that we want our systems to have. The big question is how. So one of the main arguments that I want to make today is that we're not actually that far from solving these problems.
00:01:45.280 - 00:02:52.694, Speaker A: So first I'm going to talk a little bit more about the problem space, and then I'll sketch out what I think a proto endgame architecture might look like, and then I'll finish up by detailing a few tools in this architecture. So when I say data here, I'm specifically talking about the infrastructure that makes on chain data available to all the downstream research and applications. So I'm not talking about consensus or validating blocks. It's once the blocks are validated, how do we get those blocks to all the people that want it? And making this process fast and cheap and easy is just a means to an end. The real goal here is to enable all of the feature or all of the applications in this list. So monitoring, alerting, accounting, quantitative modeling, people need data for a lot of different things. And so there's going to be a lot of trade offs that we have to navigate for this.
00:02:52.694 - 00:03:34.780, Speaker A: But at the same time, there can still be a shared set of foundations that all of these applications are built on top of. And I think this is where Russ fits in and has a really interesting role to play. So everyone knows Rust is really good for building infrastructure. It's fast, it's robust, it has a lot of nice common practices. But the thing is, most people don't want to learn rust. And this is especially true among data analyst types, people that are doing different types of research. People often prefer to work in higher level languages like Python, like SQL.
00:03:34.780 - 00:04:36.690, Speaker A: But this is actually a great arrangement for Rust because rust is good at building composable systems that these higher level languages can take advantage of. And this has already happened in the Python ecosystem. So many of the classic Python libraries now have new versions that are powered by Rust. Under the hood, these new versions are faster, they're more robust, they often have better APIs. And so even though most Python users don't know Rust, rust is still able to elevate the python ecosystem to the point where people actually prefer it when libraries are written in rust. So I think the dynamic looks something like this. We build the foundational data infrastructure using tools like Rust, and then once the foundation is really solid, this enables an entire ecosystem of new tools to grow on top of it.
00:04:36.690 - 00:05:30.370, Speaker A: So people can use, people that are using the higher level languages can just get the data quickly, easily, without worrying about infrastructure. And this isn't just data scientists, this is anybody that needs data. And this is ultimately the purpose of the data infrastructure. So that's kind of the problem landscape. I want to go over some of the tools and concepts now that can improve the typical crypto data workflow, and I'll show how these assemble into an architecture that we use for internal data work at paradigm. And it's not quite an endgame system, but it's very flexible, and it ticks a lot of the boxes that we've discussed so far. So this is a high level view of how data flows in the crypto ecosystem.
00:05:30.370 - 00:06:24.880, Speaker A: So it all starts at the top, where EVM nodes essentially define the ground truth of what happens on chain. And then data from the EVM nodes can either go to indexers or it can go directly to the end users and the applications. And right now, there are many, many, many different tools for implementing each part of this flowchart. But the really cool development in the past six months is that this process can now be performed almost entirely using tools built in rust. And it looks something like this. So it all starts with a ret node. We heard about this earlier today, and even though Reth is still in Alpha, there's already a lot of tools for extracting the data out of Reth, either indexing it or delivering it directly to its final destination.
00:06:24.880 - 00:07:06.660, Speaker A: So just to name a few of these, Rethindexer is a way to query ret data from postgres. We heard about that a little bit. Rest DB Py is a way to do it from Python and ethers. Reth is a way to use Rest's internal database as ethers middleware. So a lot of these tools are still very new, but they already show a huge amount of promise, especially when it comes to their performance characteristics. And then in the bottom half of the diagram here, we have a bunch of tools that aren't specific to crypto. These are just rust tools that have become really popular in the data engineering ecosystem.
00:07:06.660 - 00:07:56.780, Speaker A: And to really appreciate the advantages that you get with these, I think it would be helpful to explain three modern trends in data engineering. So data engineering is something where web two companies have made really amazing progress over the years that hasn't really percolated into crypto yet. So there's a lot of low hanging fruit by just looking at what's going on in web two data engineering and adapting those best practices over here. And each of these things I've listed can be a huge amplifier on the metrics that we care about. So starting with standardized IPC. IPC is inter process communication. It's one of the main ways that different programs on your computer can talk to each other.
00:07:56.780 - 00:08:47.470, Speaker A: And the big innovation here is a standardized IPC format called Arrow. Arrow is kind of what the data ecosystem is uniting around as its common language for data programs to talk to each other. And what you get with Arrow is the ability to share data between different libraries and processes without needing to make copies. And this is a huge unlock for efficiency and interoperability. So it means that different programs can operate on the same underlying memory. You can take a pandas data frame and convert it to a polar's data frame instantaneously without needing any serialization. Arrow is also becoming a common way for databases to ingest their data and to expose their data to others.
00:08:47.470 - 00:09:37.210, Speaker A: So when you're making new data tools, it's good to ask yourself, how can I make my tool speak arrow? Because then you immediately get connected to this entire other ecosystem of other tools. So arrow is only for when you're operating in memory, when you're reading or writing to disk, the most common modern format is parquet. Parquet can be seen as an extreme evolution of CSV. Most of you have probably seen CSV files before. You have a file every row in the file, or every line in the file corresponds to a row in a table. And CSVs are all about storing tables. Well, Parquet stores tables too, but it does so in a much more sophisticated way.
00:09:37.210 - 00:11:05.310, Speaker A: So Parquet will break down the rows and the columns into separate chunks, and it will compress those chunks and it will index those chunks so that you get a very compact representation, and it can still be used for efficient queries that only read the parts of the file that are relevant to you. So this leads to huge benefits for storage size and for speed. But really the coolest thing about Parquet is the modern ecosystem of tools that is forming around it, which brings us to our third data engineering trend, which is the separation of storage versus compute. So the traditional architecture for databases is that every database has its own custom storage format. So Postgres has its own format, MySQL has its own format, SQlite has its own format, and each of these databases also has its own custom query engine that's run on the same machine that stores the files. But more recently these things have changed and the storage and the computation have been decoupled. So there are now standard storage formats like Parquet that are compatible with multiple query engines, and the data files and the query engine no longer have to live on the same machine.
00:11:05.310 - 00:12:20.140, Speaker A: And this decoupling means that you can now sort of mix and match your storage versus computing solutions, and you can scale each one separately depending on your needs. And you can also seamlessly go from querying files on your laptop to querying files on s three without needing to change any code. So with this you get an easy path towards scalability, and it's also flexible for your architecture to evolve over time. So combining all these concepts, this is the proto endgame architecture that I've been experimenting with, and this is what I use for most of my internal data work at paradigm, and it's been working pretty well. I have a rust tool called Cryo that extracts data from a ret node into parquet files. And then to analyze this data I use polars, which is a rust data frame library with a really nice query engine. And if I want to avoid this middle step and avoid touching disk, cryo can also deliver the data directly to polars using AeroIPC.
00:12:20.140 - 00:13:05.824, Speaker A: So this architecture is a really good fit for crypto data in particular. Most of the time I just run things on my laptop and it executes really quickly. And then when I need more power, I can run this exact same architecture on a big cloud node. I don't need to modify any code and it just works. And since your database here is just a bunch of immutable parquet files, it's very easy to keep data synced across multiple machines. So just a little bit more about cryo and polars. Cryo is a bulk data extraction tool, so it's basically a rust rewrite of a bunch of python code I had lying around.
00:13:05.824 - 00:13:58.630, Speaker A: And this was my main motivation for learning rust in the first place. I was trying really hard to optimize this python code, but Python gets really slow. It tends to be really fragile when you're doing a lot of parallelism. So yeah, I ended up with Cryo. And the basic operation here is something like this on the command line where you specify the data set that you want to collect, the block range you want to collect it over, and you can do this either on command line or there's a Python interface or a yemenite interface as well. So as an example of usage, you can tell Cryo collect all the contracts that have been deployed on chain. Cryo will spin up a bunch of worker tasks that collect the data in parallel and save the results as parquet files.
00:13:58.630 - 00:15:01.298, Speaker A: And then you can query these parquet files by just pointing your query engine to this data directory. And Cryo gives you a huge amount of control over how the data is acquired, formatted, filtered, et cetera. So Cryo is designed to be very composable and modular so that you can combine it with other tools. So this is an example of combining it with a cron job. You just tell Cron, hey, run this cryo command every ten minutes or however much you want, and this will maintain a live copy of your desired data set in whatever data directory. And Cryo is also item potent, so it will only collect the data that is currently missing from the data set. Cryo can collect most of the standard EVM datasets, and thanks to parquet compression, you can fit most of these datasets, or even all of these datasets on just a laptop.
00:15:01.298 - 00:16:00.196, Speaker A: So for example, the dataset of all traces in Ethereum history is more than five terabytes on Google Bigquery, but it's less than 500gb when you store it using parquet. So Cryo can also collect these datasets very quickly. For example, Bantag was able to collect this traces data set in under 10 hours from an iron node, and a few years ago this process would have taken weeks. So Cryo is still under active development. We have a lot of features on the way. So first of all, Cryo is currently using RPC to collect data, and this is nice because it works with any EVM node, but the performance will get a lot better once it has a direct connection to rest internal database. Number two, Cryo will soon be able to collect a lot more datasets.
00:16:00.196 - 00:17:00.010, Speaker A: Basically, any information that you can get over RPC, Cryo will be able to build a dataset out of it. And number three, we're going to upload a copy of each of these cryo datasets to the paradigm data portal so that you can access this data even if you don't have access to an archive node. And then very quickly, polars is the other tool that I'll talk a little bit about. So Polrs is a rust dataframe library for Python, which means that it's for reading and transforming large tables of data. When you have a directory of parquet files, polers can run complex queries on those files in a really efficient way. And something that's really cool is you can still query these files, even if they're too big to fit in memory. So you can run queries on the 500 gigabyte traces data set from your laptop.
00:17:00.010 - 00:17:44.070, Speaker A: I definitely recommend checking out Polar's if you're doing any data work in Python. So as an example, let's say you collected the parquet data set of Ethereum contracts. Polrs can run just about any query you want on this data, and it can do it quickly. So if you wanted, you could load all the addresses of all contracts into Python from the files in under a second. You can also do things like get all the contracts deployed by an EOA, get all the contracts deployed with a certain bytecode. All these things are very quick. And it's a similar story for all the other data sets that cryo can collect.
00:17:44.070 - 00:18:42.210, Speaker A: And something that I've noticed in myself and others using this style of architecture is that it really changes the way that you think about your data. When it's totally frictionless to explore your data, you tend to explore it more and you tend to gain a deeper level of understanding because you're exploring it more efficiently. So that's all I have for today. How do these things fit together into the data endgame? I still think there's a lot of work to do, but there's a huge amount of progress being made on all the important fronts. So with modern tooling, we get things that are extremely fast, extremely cheap, not quite effortless, but it's getting easier all the time. More and more data is becoming available. You get formats that are open and interconvertible, and you can run the same architecture on your laptop or in the cloud.
00:18:42.210 - 00:19:03.510, Speaker A: So I think that if this rate of progress continues, we're going to, like each of these issues, is going to reach a near optimal state in a short to medium term timeframe. And with that, we're going to have sort of a new foundation that a rich data ecosystem can grow on top of. Thanks for your attention.
00:19:09.930 - 00:19:11.990, Speaker B: We have time for two questions.
00:19:16.450 - 00:20:08.670, Speaker C: Hi, I'm using crayo and production stage. I really thank you for your kind of these amazing repositories. And I want to ask questions like, what I found is that the current of the parquet is now representing in the binary format if it's not a bake in type. So what I did on my pyspark code is to transform those binary into the hex binary format and then to decode according to the ABI files that I want to decode it into. And it gives some kind of painful time to process in my spark the clusters. So, is there any kind of more plans to implement those ABI or hex decoding features in the cryo? Thank you.
00:20:10.170 - 00:20:33.010, Speaker A: Yeah, so this is actually something that's being worked on right now by Eric from Zora. This is definitely like a known problem that we want to tackle head on. So we're going to get direct decoding of logs, we're going to get direct decoding of different function calls and traces. All that's on the roadmap.
00:20:37.870 - 00:21:15.060, Speaker B: I've got a question. We had talked in the past about this not being well suited for maintaining live to the tip chain sink. I was wondering if you'd figured out any solutions for maintaining some kind of batching process that combines the data from the chunks that have had enough blocks to complete with the delta at the tip of the chain that hasn't been completed yet.
00:21:15.600 - 00:22:02.390, Speaker A: Yeah. So right now, Cryo has an option, a command line option, where you can give it a number of blocks where it won't save any blocks that are newer than that age. So if you give it a high number, like 1000 blocks, you basically never have to deal with reorgs or anything like that. That's easy mode. Eventually, like, it'd be nice to implement some direct Reorg logic so that you can actually stay close to the tip without any sort of delay. That's not currently something we're working on, but it's something we're thinking about right now. Cryo is very sort of focused on bulk historical analysis, but reaching into these more like streaming type use cases.
00:22:02.390 - 00:22:11.600, Speaker A: I think it's like a natural evolution and it'll probably happen eventually. Lot of flows for storm.
