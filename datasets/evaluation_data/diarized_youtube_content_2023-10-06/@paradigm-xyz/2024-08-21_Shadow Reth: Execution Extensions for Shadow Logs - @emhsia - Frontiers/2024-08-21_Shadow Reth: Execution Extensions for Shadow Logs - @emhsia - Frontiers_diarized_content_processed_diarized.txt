00:00:00.440 - 00:01:22.108, Speaker A: And co founder of Shadow, who's going to be talking about Shadow Ref, and I will make the stage for her. Hi everyone, hope you guys had a good lunch. I'm Emily, I'm one of the co founders and CTO of Shadow and I will be talking about Shadow Ruth today. Oh, sorry. Okay, awesome. First, what are shadow logs? Shadow logs are gasless logs that can be permissionlessly added to an already deployed smart contract, and they get generated in an off chain execution environment. So why are shadow logs cool? Shadow logs allow you to permissionlessly add as many custom logs as you want, all without increasing the gas burden to your users.
00:01:22.108 - 00:02:20.616, Speaker A: And then this unlocks deeper data coverage and you can get net new contract data, data that's like really hard or nearly impossible to get otherwise, or really cumbersome to get. You have to parse traces or do expensive ETH calls, things like that. And as a result, shadow logs can drastically simplify your downstream data pipelines. So for example, say you wanted to build a new front end feature, but your currently deployed contracts don't make that data easily accessible. Instead of redeploying your mainnet contracts, you can run a shadow contract that makes that data easily accessible. And we think all of this will allow people to build better, more data rich products and services for their users. So if you wanted shadow logs, the steps are pretty simple.
00:02:20.616 - 00:03:35.090, Speaker A: You first edit the source code of an existing smart contract to add your shadow logs. You then run a shadow node with your new shadow bytecode of your contract. And then the third step is you can get your shadow logs by querying the node. So at a high level, the way that this works is when a new block happens on Mainnet, we instantly re execute the same mainnet transactions from that block in an off chain execution environment. And then this off chain execution environment is super similar to Mainnet execution, but instead of executing the Mainnet contract bytecode, it swaps out the mainnet contract bytecode with your shadow contract bytecode at the addresses of your shadow contract. And then when your shadow contract gets executed, it reads from the same exact mainnet chain state, but is emitting all these additional event logs. And then you can fetch these shadow logs over JSON RPC just like you can fetch mainnet logs.
00:03:35.090 - 00:04:54.670, Speaker A: So this off chain environment is shadowing Mainnet, right? It's re executing the same mainnet transactions against the same mainnet state, but can output like ten x or 100 x more logs, all without increasing the gas burden of your users. So taking a step back like let's talk about what the job to be done is whenever you're building crypto data infrastructure with any crypto data pipeline, you're generally doing these three things. You're extracting raw on chain data by fetching data from a node. Then you transform that raw on chain data into a decoded human readable format, and then you load that transform data into some data store that you can then use for analytics or for your front end or whatever you need. And this is no different than standard ETL pipelines, right, that you see in traditional large scale data pipelines. So this is how most data crypto data pipelines are architected today. The source of truth of all on chain data are always nodes.
00:04:54.670 - 00:05:41.470, Speaker A: And these nodes are built to quickly discover and execute those new blocks and then commit those new blocks in the state transitions to its own database. Then your data pipeline subscribes to new blocks that get added to Mainnet. You can do this via websockets eth subscribe. You can do this in a polling loop. Either way, you do this over JSON RPC, and then when you get that new block notification, you go and then fetch all this additional data that you need again over JSON RPC. And oftentimes you have to fetch traces or make ETH call requests or do a lot of heavy RPC endpoints. And this is the extract step.
00:05:41.470 - 00:06:22.300, Speaker A: Then you write custom transformation logic to transform the data into your own use case. So you might do this in typescript or SQL or Python within a framework like subgraphs, for example. And this is your transform step. And then finally you write all that transform data into a data store like postgres or snowflake. And this is your load step. So there's some drawbacks to this architecture. The first is that the extract and transform steps can be really slow from a performance perspective.
00:06:22.300 - 00:07:26.620, Speaker A: You have added network time if you're going over the network, and you're also paying a cost for JSON serialization deserialization every time you fetch data from a node. And then as a developer writing these data pipelines, it's not really a great experience as well. You're generally stuck with the event data or the contract data that was originally deployed in the contract, and that contract could have been deployed like years ago. You also can't reuse any of this logic that is actually running on chain in your transformation pipelines. You often have to rewrite the same logic that lives in smart contracts, but you rewrite it in typescript or Python. And if you don't want to build an indexer from scratch, you can use frameworks that exist out there, but there's some ramp up cost of learning how those indexing frameworks work. You're working with YAML files, stuff that no one likes.
00:07:26.620 - 00:08:41.570, Speaker A: So how does this change with shadow? The idea behind shadow is that you can actually co locate the extract and transform steps, so you can write your transformation logic directly in smart contracts. Run that transformation logic through the EVM, which has direct access to the exact chain state that you want, and then emit the transform data as normal EVM logs. So this is still following the same ETL pattern, right? But instead of writing your transformation logic in SQL or typescript or Python, running that over JSON RPC, you can write your transformation logic in solidity or VipEr and run it directly over chain state via the EVM. And this has a few benefits. The first is that it's faster and more performant. Your transformation logic has direct access to that chain state, so you don't have to go over JSON RPC, which reduces network and JSON serialization deserialization overhead. And it's also a better developer experience in a lot of cases.
00:08:41.570 - 00:09:32.460, Speaker A: Again, you can directly access chain state that you need, and then you can also reuse any code that's actually running on chain in your off chain transformation pipelines. So to summarize, a new block gets added to the chain. You execute that block through the EVM with your shadow contract bytecode, which contains all your transformation logic. And then that shadow contract bytecode is also emitting your transform data as EVM logs. And then of course you can send these to whatever data store that you want, postgres, noflake, whatever suits you the best. So this is a perfect use case for Ret xxs. This co located ETL process can be really easily modeled using an xx.
00:09:32.460 - 00:10:44.820, Speaker A: So that's exactly what we built. We built Shadow Ref, which is an open source implementation of a shadow node built with Reth xxas. And you can run shadow Reth to generate shadow logs directly within your ret node. So last December we launched a hosted platform that allows you to get a shadow logs without having to manage your own infrastructure or run your own node. Three months ago, in May, we released Shadow Reth, which is 100% free and open source for folks who want to self host and run their own node. Shadow Reth allows anyone to realize the benefits of shadow logs within the comfort of their own node. So why did we build Shadow ref? We believe that all hosted providers present some trustlessness trade offs, right? We believe that it's important for anyone to be able to generate these shadow logs in a decentralized way.
00:10:44.820 - 00:11:43.528, Speaker A: And one of the reasons why we think this is so important is for verification purposes. So similar to how you could run your own node to verify the data that's being returned to you by alchemy or infuria or tenderly, you should be able to run your own shadow node to verify the data that's being returned to you by a hosted shadow fork provider. And the RefX framework enabled us to build like a performant shadow node to generate shadow logs without requiring additional off chain infrastructure or maintaining a heavily modified client fork. So let's dive into how shadow works. This is what the architecture looks like, and there are three main components to shadowreth. The first component is the shadow configuration. This is a pretty simple just JSON configuration.
00:11:43.528 - 00:12:37.070, Speaker A: It contains your shadow bytecode and your shadow contracts. The second component is the Shadow xx. And like any xxs, you get notified of a new block right when that block happens. So when it gets that block, the Shadow Xx runs a custom executor that executes the block with the overridden shadow bytecode. And this executor is really just using REVM under the hood of it has some shadow specific modifications that bypasses gas checks. It can also bypass some transaction verification checks to save a bit on performance. And then after the shadow executor re executes that block, it takes out the shadow logs that were generated in that block and then writes it to a database.
00:12:37.070 - 00:13:40.540, Speaker A: And then the third component is Rshadow RPC. This was built using Reth RPC extensions that allowed us to really easily add a new custom RPC endpoint that we called shadow get logs. So when you're fetching logs, shadow logs from your own node, you can hit this endpoint and the results of the shadow logs look exactly the same as mainnet logs. And the modularity of rethemeral and the xx framework was a huge unlock for us to build this performant shadow node. So huge shout out to the Reth team for all of their work on this. Shadow Reth is already pretty powerful, but there are some missing features and some performance improvements that we think could make shadow Reth even better. So if you're interested in hacking with or on Shadow Reth directly, we have some ideas for you.
00:13:40.540 - 00:14:34.986, Speaker A: I'm not going to go into details of all of these, but come find me afterwards and happy to chat and give you pointers. Cool. So we are really excited about the progress that we've made in the last few months. But where do we go from here? First, something that we're really excited about is being able to send this data to any data destination of your choice. Being able to generate these shadow logs within your node is great, but there's a lot that you might want to do with this data. So the goal would be to allow you to send this data anywhere you want by simply installing a data connector on your ret node for your data destination. And lastly, this is just a teaser.
00:14:34.986 - 00:15:48.774, Speaker A: I'm not going to go into the full details of how this all works, but we've been working on a decentralized registry of shadow contracts, and these shadow contracts were written by experts, often by the original protocol teams themselves. And these contracts are stored on ipfs and available for anyone to use. And we've been working with some top DeFi protocols like Uniswap, Yearn, Frax, Lyra, Pendle. They've all written shadow versions of their contracts and uploaded them onto this decentralized registry for anyone to use. So with this public registry, you no longer have to handwrite your own shadow contracts and you can just like opt in into pre written shadow contracts that you're interested in and get the shadow logs for those contracts. As part of this, we've also open sourced a CLI tool that allows you to permissionlessly upload a new shadow contract to ipfs and register it on base. Pull down any shadow contracts that were previously uploaded that other people have written and uploaded to their registry.
00:15:48.774 - 00:16:45.420, Speaker A: And then you can also use the CLI to really easily generate like in one command, that shadow reth configuration file that you can then pull into your own reth node and you can find this at logs xYZ. This is just our hosted frontend that sits in front of the decentralized registry in ipfs. So with all of this, the vision is you'll be able to permissionlessly opt in to pre written shadow contracts, run them from the comfort of your own node, and then send that data to any data destination of your choice. We're really excited about this. Go check it out and let us know what you think. And that's it. Any questions?
00:16:53.440 - 00:17:18.250, Speaker B: Thank you for the presentation, that was great. Do you recommend. Well, do you think developers should stop writing normal contracts like safeguards? Is that going to make a big difference as more like a vision for the future? I understand you don't have to use shadow logs if you don't want to, but yeah, maybe we should stop writing event contracts and just use shadow logs instead.
00:17:18.910 - 00:18:12.630, Speaker A: Yeah, yeah, that's a hot discussion topic. We don't believe that all mainnet logs should be removed from Mainnet. We actually think that that's kind of like just removing all mainnet logs is kind of an uninspiring goal. Like it really caps kind of the value that you can bring to the ecosystem. And what we're really more excited about is like the ability to ten x or 100 x, the data that actually, that you can actually capture from on chain activity. But at the end of the day, you know, developers know best, developers know what's best for their own protocol, for their own users. Our goal is just to make sure that if developers do choose to omit mainnet logs and use shadow logs, that it is really easy, really cheap and accessible to actually get these shadow logs.
00:18:21.130 - 00:18:32.070, Speaker C: What happens if the contract that you're trying to get shadow logs from is like behind a proxy or calls another contract that doesn't emit shadow logs or delegates some behavior?
00:18:32.560 - 00:18:52.860, Speaker A: Yeah, good question. So for proxy contracts, what you're actually shadowing is the implementation contract. So you shadow the implementation, you don't have to touch the proxy, and the proxy will obviously delegate call to the implementation contract, which will then execute your shadow byte code and emit the logs.
00:18:53.840 - 00:19:03.402, Speaker C: Does that mean you have to have a mapping of your, I'm just looking through your repo and you have this mapping of the shadow config and you have to map your implementation as well for proxies?
00:19:03.506 - 00:19:19.682, Speaker A: Yeah, that's right. I think there's a lot we can do better there. We can automatically detect when an implementation changes and try to deploy your original shadow contract at the new implementation address. Definitely think we can improve that there.
00:19:19.826 - 00:19:24.972, Speaker C: Thanks. Sorry.
00:19:25.076 - 00:20:51.270, Speaker A: Can you explain a little bit about what Pendle did with shadow and how other DeFi protocols might be like improved with this? Yeah, yeah. So the Pendle use case, if you want to go super, super deep, there is a published case study on, I think it's blog shadow XYZ, where it talks about kind of all the math and exactly what they're doing. But I, at a high level, the way that Pendle is architected is they run some off chain algorithms to optimize the on chain transaction route, essentially kind of similar to how uniswap generates the optimal kind of pool route path and then submits that to the chain. And they were using shadow for easily back testing their algorithms. They were able to basically back test and say that, okay, if our algorithm changed with these parameters, if those parameters were actually used for the mainnet transactions, how much gas would the user have saved? And that's kind of what they were doing at a high level. They were doing it for kind of backtesting purposes. Okay, I think that's it.
00:20:51.270 - 00:21:01.770, Speaker A: Thank you. Next up, we have Firon from flashbots talking about our builder, a MEV builder, on ref.
