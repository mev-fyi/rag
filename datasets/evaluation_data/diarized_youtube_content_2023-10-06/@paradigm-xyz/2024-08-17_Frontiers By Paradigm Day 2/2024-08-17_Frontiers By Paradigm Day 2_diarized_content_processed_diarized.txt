00:00:59.250 - 00:01:03.610, Speaker A: When you're holding on as you let.
00:01:03.650 - 00:01:12.590, Speaker B: It go there's nothing more, there's nothing left?
00:05:26.140 - 00:05:38.120, Speaker C: Houses as far as the eye can see? You kept eyes on me? I kept eyes on you?
00:05:42.180 - 00:05:44.148, Speaker D: You kept eyes on me?
00:05:44.284 - 00:06:18.810, Speaker C: I kept eyes on you? And I wake up at night out of sight, out of mind? I know you just by sight? You kept eyes on me, I kept eyes on you? You kept eyes on me? I kept eyes on you?
00:06:26.810 - 00:06:28.350, Speaker D: Check, check, check, check.
00:07:19.750 - 00:07:22.190, Speaker C: You leave it to view in a.
00:07:22.230 - 00:07:38.086, Speaker D: Bed in high from the corner of my eyes a night far at night.
00:07:38.278 - 00:07:56.420, Speaker C: Too far a truth? Houses as far as the eye can see? You kept eyes on me, I kept eyes on you?
00:08:00.440 - 00:08:04.580, Speaker D: You kept eyes on me? I kept eyes on you?
00:08:24.890 - 00:08:28.990, Speaker C: You cap eyes on me? I cap eyes on you?
00:08:31.850 - 00:08:32.936, Speaker E: Check, check.
00:08:33.098 - 00:08:37.160, Speaker C: You can't eyes on me, I can't eyes on you?
00:12:50.320 - 00:12:51.340, Speaker B: We are.
00:12:55.160 - 00:12:55.584, Speaker D: Hello.
00:12:55.632 - 00:12:56.300, Speaker A: Hello.
00:12:58.400 - 00:18:53.900, Speaker F: Sound check, sound check. This is a sound check. One people can please start getting seated so we can get started in the next five minutes. Please get seated. All right. 09:00 a.m. sharp.
00:18:53.900 - 00:19:18.096, Speaker F: It isn't, but we're getting started. So for our first talk of the day, we'll have storm, who will talk to us about the data ecosystem safari. If people would like, be so kind to sit, we can close the doors so that we can get started. And also to my right over here, you see Caitlyn. Caitlyn, raise your hand. Yeah. So Caitlyn, instead will be mcing with us today.
00:19:18.096 - 00:19:51.470, Speaker F: We work together, investing in research team, and, yeah, Caitlin will be our host for today. So, round of applause for Caitlin. All right, storm, you want to take it away? I don't need that. Mic check. Mic check. Okay. Good morning, everyone.
00:19:51.470 - 00:20:47.860, Speaker F: So, since last year's conference, I've been working on a bunch of crypto data tools, and all of these tools share a common goal. We want to create modular legos that we can assemble into new infrastructure and new applications. So today, I want to give you a tour of these tools and show you how they fit into the broader crypto ecosystem. So, the flow of data in crypto is a lot like a food web. Food webs in biology are a way to visualize how energy flows through an ecosystem. So, at the bottom, you have the energy source, the plants, and then you have animals that eat the plants, and then you have many layers of animals that eat other animals. This parallels the way that data flows through tools in the crypto ecosystem.
00:20:47.860 - 00:21:51.950, Speaker F: So, at the very bottom, you have the raw blockchain data source, the RPC nodes, and then you have tools that consume that data and process it and share it with other tools using APIs. And then at the very top we have the interpretation layer, where charts and tables and dashboards make the data consumable by humans. So this is the state of the world today. It's not bad, but we want this ecosystem to always be improving and evolving, so that the way we use data gets better and better over time. One of the most effective ways to put evolutionary pressure on this ecosystem is by building new open source tools that improve the flow of data. So either allowing data to flow more efficiently or allowing it to flow in new ways. And so this is something I've been trying to do with these four tools.
00:21:51.950 - 00:22:40.150, Speaker F: Each of these fits into a different part of the ecosystem, and I'm going to tell you a little bit about each one. And I don't have time to go into the full details, so I'll just sort of give the highlights and do a few tech demos. So starting with mesc. MeSC is a way for all of your different dev tools to keep track of all of your RPC endpoints, kind of like an address book. So imagine you need to work with these different blockchains on the left, and you need to connect them to some of your dev tools on the right. The good news is all of these chains and all of these tools speak the same API, which is JSON RPC. But the bad news is, this is a configuration nightmare.
00:22:40.150 - 00:23:39.800, Speaker F: Almost every single one of these tools has its own unique conventions for how to configure its endpoints. This isn't really that big of a deal if you just have one or two networks, but if you have a lot of networks or a lot of tools, this becomes a very fragile and time consuming problem. You have to configure each tool individually, and you have to keep those configs synchronized over time as you add or remove endpoints. MeSC simplifies this problem by creating a single source of truth. It tracks all of the endpoints or all of your endpoints, and then it exposes that info to all of the tools on your system when they need to perform RPC requests. And standardizing the config like this lowers the operational complexity and removes a lot of potential for human error. So let me show you how this works in practice.
00:23:44.500 - 00:23:45.236, Speaker G: Yes.
00:23:45.388 - 00:24:43.826, Speaker F: So, on my laptop right now, I use MesC to track 27 RPC endpoints from ten different networks. And so on the command line I can do mask ls to list out these endpoints. And for each one it tells me the name of the endpoint, the network, and the URL beyond the command line. Mesk also has libraries for rust, for python, and for other programming languages, so that this info is accessible in basically any environment. MesC can track a lot of different metadata. So one example is network defaults. What is the default endpoint that you want to use for each network? So on the command line, I can do mesc URL ethereum, and it shows me the URL of my default ethereum endpoint.
00:24:43.826 - 00:25:38.516, Speaker F: I could do mesc URL base, or any other network shows me the default endpoint URL for that network. MeSC has a few options for how to actually store this data, but in its most basic form, it's just storing everything in a JSON file on your computer. You can create one of these mesc configs either by copying from a template in the mesc repo, or you can do mesc setup on the command line, and there's this interactive uihe for creating and modifying your mesconfig. So to demo this, let me just delete my current mesc config. If I do mesc ls config missing, all of them are gone. So let's do mesc setup. Config file does not exist.
00:25:38.516 - 00:26:24.390, Speaker F: Do you want to create one? Yes. What do you want to do? Let's add an endpoint, and we'll just use the classic 8545. That's where I'm running a base node right now. And you can see that it pings the node, it detects the chain id, and it's ready to add it to the config. So let's name it local base, exit, and save changes. Now, when we do maskels, the endpoint is there, and the endpoint is now usable by any tool that's powered by mask. So let me just restore my old config and they're all back.
00:26:24.390 - 00:27:25.860, Speaker F: Okay, so beyond just tracking the endpoints, Mesc also tries to make it easy to manage these endpoints, and it has a lot of features for this. So if I do Mesc pingdeh, it will ping all my different endpoints, check their latency, and also check their current block number. It can ping a lot of other information, like if you do mes ping client, it will check which client each of these different endpoints is running. It will also show at the bottom if any of the endpoints are non responsive. So MesC is still a work in progress, but the overall goal here is to reduce the operational burden by standardizing the config process across tools. Okay, so the next thing I want to go over is cryo. Cryo is a tool for collecting blockchain datasets.
00:27:25.860 - 00:28:31.920, Speaker F: Cryo can take any type of information available over our PC and turn it into a simple local flat data set. So up here I've listed the different types of data that Cryo can collect. It's anything from really simple stuff like blocks transactions all the way to more obscure things like JavaScript traces and opcode traces. And when a lot of this data comes raw out of an RPC node, it can be very nested, very messy. Cryo basically packages it nicely into flat tables that are easy for other tools to consume. Cryo also uses a lot of tricks to make this process fast, and this actually enables a new style of workflow where you can perform all of the data collection and analysis for an entire chain's history on a local machine using open source tools. It's kind of like having your own private bigquery instance just running on your laptop.
00:28:31.920 - 00:29:34.040, Speaker F: Cryo was released about a year ago, and from the beginning we put a really big emphasis on making it modular and leaning into open standards like Parquet. And for these reasons, we've seen Cryo adopted by many different organizations so far. So this includes data platforms, security firms, news outlets, mev teams, and lots of different on chain protocols. So let me show you what cryo looks like in practice. So the most basic usage of cryo is just collecting a vanilla data set. So let's say we want to collect all the logs over some block range. We just do cryo logs, and then let's collect from block 10 million and then 100,000 blocks after that.
00:29:34.040 - 00:30:03.140, Speaker F: So cryo will go through, it'll pull that data from the RPC node. Right now it's the local one on my laptop, and it will package all that data into a bunch of parquet files. The interface is really simple. Instead of logs, we want to collect blocks, we just say cryoblocks. It'll collect that over the same range. And so that collected 100,000 blocks in about 2.2 seconds.
00:30:03.140 - 00:31:24.000, Speaker F: If you want to list out the different types of data sets cryo can collect, you can do whoops, you can do cryo help. Datasets lists all those out. If you want help about a particular dataset, you can do cryo help. Name of dataset so like logs, it will show the schema for that dataset, and it will also show what parameters are possible for that dataset. Cryo uses MeSc for its RPC configuration so right now it's just collecting from my default mesc endpoint, which is that local base node, but I can specify other endpoints in a few different ways. So the standard kind of thing that a lot of tools do is you do something like RPC and then give it a URL like that. Cryo can do that, but because we're using mask, we can also do things like just give a network name like ethereum or base or whatever other network you have configured, and Mesc will resolve that into the appropriate URL based on what you've configured.
00:31:24.000 - 00:32:19.130, Speaker F: Cryo is also item potent. So let's say we want to collect over a longer block range, a longer lasting job. So let's say a million blocks instead of 100,000. It'll start the job and I can kill it at any point in the middle and restart it without issue. And this is really helpful because Cryo will make sure that none of the files end up being corrupted at the end of the day. So you have a lot of flexibility for like if you need to break the extraction process up into multiple chunks, that's fine. And then if you try to queue up that job again, Cryo will detect, oh, that data is already collected, I don't need to recollect it unless you explicitly ask it to.
00:32:19.130 - 00:33:13.900, Speaker F: Cryo has a lot of other options as well. You can list these with cryo, h you can export as CSV, you can filter things, there's a lot you can do. And then finally Cryo has a Python interface that's pretty useful. So if we do in a Python session, we do import cryo and we can do something like cryo or Df equals cryo collect. The Python interface has all the same options as the CLI. And let's say we want to collect very similar data set as before. It will collect that data from the RPC node and it will put that into a polar's data frame df that we can now use with all the standard Python patterns.
00:33:13.900 - 00:34:19.570, Speaker F: So this is just a super fast and easy way to load data in Python. So the next tool on the list is TBL or table TbL is a swiss army knife for reading and editing and managing parquet datasets. So for those that don't know, Parquet is a file format for tabular data, kind of like a modern CSV. In many cases, Parquet can be a simpler and faster alternative to using an actual database. And for this reason, Parquet has become a very common way for people to store and share different datasets. So let's say you have a bunch of parquet files, either from cryo or from some other data source. You might find yourself wanting visibility into those files, or you might want to make some quick edits to those files.
00:34:19.570 - 00:35:33.530, Speaker F: But up until now there hasn't really been a quick and easy way to do these things with Parquet. And that's why I built table. Table makes it easy to look up info about the schema and the contents of your parquet files, as well as make a lot of edits to them. So let's look at some examples of TBL in action. Okay, TBL can tell us a lot of info about that cryo data we just collected. So if we do tbls, that will list out the parquet files and also tell us a bunch of metadata aggregated over those files, like the total row count, the file count. Another thing we can do is look at how those files are structured so we can do TbL schema and it will list out the two different schemas that are present in these files, one for blocks and one for logs, and for each schema.
00:35:33.530 - 00:36:00.260, Speaker F: Each line here represents one column in the table, and you can see the column's name, its data type, and its storage size. And this can be really useful for just taking a peek at how your data is structured. Another thing table can do is make some quick edits to those files. So let's start by just doing a little preview of the first couple rows.
00:36:02.240 - 00:36:03.780, Speaker A: Let's look at the.
00:36:06.200 - 00:36:28.986, Speaker F: Logs. Okay, so that's a little preview. Let's imagine that we need to modify the last column chain id. We need to change it to a different chain id. Let's say like 55. We can just do set chain id equals 55. It makes those changes.
00:36:28.986 - 00:37:08.826, Speaker F: Maybe we also need to change the type from U 64 to U 32. We can do cast chain iD equals U 32 and it makes those changes. Right now it's just printing the result to the console, but TBL has a lot of options for how to actually output these results. So the most maybe basic version is we can just edit all of the different files in place. And we can do that by just doing dash dash in place. And do you want to continue?
00:37:08.938 - 00:37:09.670, Speaker E: Yes.
00:37:11.490 - 00:37:58.610, Speaker F: I haven't parallelized this part, but I will in the future. It's going through all those files, editing them. And now if we do the preview again, we can see that those edits are now persisting in the files. Another thing we can do as an output mode is merge the results from all these different files into a single new file. So if we do output file test parquet, do you want to continue? Yes, it will now output everything to this single file. It will take a moment because it's not parallelized Ll. And now this test parquet is present in the directory.
00:37:58.610 - 00:39:01.530, Speaker F: A final alternative to quickly open up these files is in a python console. So if we do something like TBL baselogs DF, it will open up a python session and load that data into a data frame that we can now just do all the standard python patterns with. So this is just a very nice convenience for if you need to quickly mess around with some data in Python. So that's TBL. Everything I've shown here might seem really simple and trivial, and that's because it is. But up until this point, there wasn't really an easy way to do it. And this is important because if you can do these types of operations in 2 seconds instead of 20 seconds, it really does change your relationship to the data, and you end up interacting with it a lot more because the friction is gone.
00:39:01.530 - 00:40:23.832, Speaker F: The final tool I want to talk about is LBL, and this is just a quick preview, because LBL is still a work in progress. LBL is a tool for managing label datasets. Address labels are one of the biggest remaining challenges in open source crypto data, and managing these labels can get extremely complicated, especially if you're dealing with lots of chains or lots of different data sources. And there are some nice projects out there for standardizing address labels, like the Open Labels initiative. But there's still a gap in tooling. How do we actually connect the label data to the other tools in the ecosystem? LBL is the first tool built specifically to manage label datasets. So, for example, LBL will make it so that you can load any data set you want in just a single line of code, whether it's derived from on chain data, like ERC 20s, or Uniswap pools, or chain link feeds, or whether it's from Dune.
00:40:23.832 - 00:41:23.120, Speaker F: And maybe you want to get Hill Dobby's sex dataset, or maybe it's your own private label source that you made yourself that you also want to manage with the same interface. And I'm not going to do a tech demo of this one just for time purposes, but check back in a couple months and we'll have some cool stuff here. So those are all the tools I wanted to cover today. Ultimately, we're trying to improve the way that we use data at every stage of the pipeline. And the common approach here is that we're really embracing modularity and we're embracing modern best practices, with the ultimate goal being that this gives us a very powerful set of Legos that we can assemble into the next generation of infrastructure and applications. So that's it. Come find me if you want to chat about any of these topics.
00:41:23.120 - 00:41:26.300, Speaker F: And thanks for waking up early to make it.
00:41:40.240 - 00:41:50.020, Speaker C: Any questions for storm? Yes. No statements, only questions.
00:41:52.080 - 00:41:58.432, Speaker A: Do you support this? Does Tbl support s three files or.
00:41:58.456 - 00:42:03.384, Speaker F: It'S just local that's on the roadmap? That's a very big goal right now, but it's not there yet.
00:42:03.512 - 00:42:04.660, Speaker A: That would be useful.
00:42:16.920 - 00:43:10.040, Speaker F: How's cryo so fast? How did it pull that many blocks that quickly? Is there some secret sauce that you can share with us? The secret sauce is rust. I made almost an identical tool in Python, and I did every trick in the book that Python has for multiprocessing everything like that. It was still in order of magnitude slower than just doing everything in rust. And this is sort of built on top of Tokyo's NPSC multi producer, single consumer type flow, where you just divide up all the tasks and then do each one in parallel with some like semaphore. But there's actually, like, even more room for optimization. I just haven't gotten around to it because it's been fast enough for a lot of my purposes. But, yeah, it's very convenient.
00:43:10.040 - 00:43:39.054, Speaker F: It'll basically push most nodes as far as the node can go. And Cryo isn't the bottleneck. I have a question. And also we've been using cryo effectively to generate benchmarks for ref because it's just a realistic usage of how you would expect people to use a tool. And that can give you numbers that are actually reliable versus having to define a benchmark. You just capture a workflow and that's your benchmark. I have a question.
00:43:39.054 - 00:44:54.190, Speaker F: Storm. Wherever last year you walked us through how. Okay, there's actually a very powerful open data stack out there waiting to be leveraged to basically commoditize a bunch of the things that the clouds give you. And you just gave us an overview of how this captures the data lifecycle of like a data engineer, data scientist. Where are we going from here? So what is the vision of this, this local first processing pipelines and why and where do you want to take things? Yeah, so, I mean, the most basic way to answer that question is the boring version, which is with all these sort of tools ready and available, it becomes very easy to replicate what's already been built that might be commercialized right now. Like, if you wanted to build your own etherscan, or your own dune or your own whatever, it's becoming more and more possible to assemble these off the shelf components into whatever your vision is, and then sort of more broadly, everybody needs data at some point. I listed a few of the people that are using cryo right now, like security firms, news outlets, mev teams.
00:44:54.190 - 00:45:46.590, Speaker F: We want to make it so that people are empowered to basically get whatever data they want by just using these off the shelf components. The really interesting answer of where is this going? And what is the data endgame and what becomes possible. Now, that's basically a big question box. We don't know yet. Basically, we're creating a bunch of potential energy, and we don't know how that potential energy will be released, but it's building, building, and people being creative in the space and sort of like having their own vision, basically using these components to build better and better foundations for the actual thing they want to build. I think that's probably the most exciting thing.
00:45:48.620 - 00:45:50.812, Speaker A: Is it possible to use cryo as.
00:45:50.836 - 00:45:57.880, Speaker H: An indexing service, like live polling blocks that was newly created?
00:45:58.500 - 00:46:48.440, Speaker F: Yeah. So the ergonomics on that, there's still a little bit of improvement we can do, but it can basically do that. So cryo has some settings for dealing with reorgs, and if you want to create a reorg delay, or you want to include certain information that makes it easy to process the reorg, we have that one feature that we really want that we don't have yet is just completely seamless handling of reorgs, so that you can do, like, a more streaming type situation. But cryo, for example, can do individual blocks really easily, really fast, if you want to just stay at the very tip, or it can do a delay, like, don't collect anything from the latest hundred blocks, because I don't want to deal with reorgs. You can also do that.
00:46:59.860 - 00:47:04.830, Speaker A: Thanks for the presentation. Are there any aspects of using Parquet.
00:47:04.980 - 00:47:59.720, Speaker F: To do this that are more complicated in the long run than a relational database, or more annoying or anything along those lines? Yeah, there are. There are a few. One thing is that the indexing capability in Parquet is not as powerful as something like postgres. So if you have something where you need to perform simple, like, range queries along one dimension, Parquet can be really good for that. You can sort of, like, structure your file from the ground up to support that specific query. But if you need to do something like, I don't know what all my queries are going to be. I need to query by, like, three different fields all at the same time, filtering those, making it efficient, making it scale to billions of rows, then parquet is probably not the best solution.
00:47:59.720 - 00:48:08.840, Speaker F: Great, thank you.
00:48:16.500 - 00:48:25.240, Speaker C: Okay, next up we have foundry Endgame by zero. Snacks. Tony's just going to quickly flip the front.
00:49:03.110 - 00:49:07.130, Speaker D: I didn't know you needed those. Let's come back.
00:49:41.250 - 00:49:44.870, Speaker A: Click the minimize. There's a minimize?
00:49:45.410 - 00:49:47.630, Speaker F: Yeah, but he needs to see the notes.
00:49:49.650 - 00:49:52.150, Speaker E: Can we just flip the mirroring off the screen?
00:49:54.210 - 00:49:57.510, Speaker F: Yeah, screen. It's mirrored to this whole thing, so.
00:50:05.740 - 00:50:06.880, Speaker E: Might be harder.
00:50:15.940 - 00:50:19.680, Speaker F: Okay, let's give a couple seconds to figure out.
00:50:49.550 - 00:50:49.966, Speaker A: GK.
00:50:49.998 - 00:50:53.598, Speaker F: Just pull that screen back up full screen. Oh, you got the.
00:50:53.654 - 00:50:54.606, Speaker B: Yeah, and I'll give him this.
00:50:54.638 - 00:50:55.650, Speaker D: Can you pull this?
00:50:56.160 - 00:50:56.900, Speaker I: It?
00:51:12.400 - 00:51:13.620, Speaker E: Oh, you can do that?
00:51:15.640 - 00:51:16.056, Speaker D: All right.
00:51:16.088 - 00:51:16.700, Speaker G: Yeah.
00:51:26.050 - 00:51:27.750, Speaker F: And then I'll just click through.
00:51:28.570 - 00:51:30.274, Speaker A: You'll either have to click both.
00:51:30.442 - 00:51:31.190, Speaker E: Yeah.
00:51:38.650 - 00:51:39.154, Speaker F: Okay.
00:51:39.202 - 00:51:39.830, Speaker A: Yeah.
00:51:43.130 - 00:51:43.950, Speaker E: Yeah.
00:51:44.410 - 00:51:45.026, Speaker F: Hey.
00:51:45.138 - 00:52:33.928, Speaker E: Hey, everyone. Well, thanks, everyone, for coming. So today I'll share the progress that has been made in foundry over the last two years, and I'll highlight some of the most impactful features that have been added that you may not have been familiar with yet. So, what is foundry? Foundry is a smart contract development tool chain. It's written in rust. It manages your dependencies, it compiles your project, it runs tests, it deploys and lets you interact with the chain from the command line. And using scripts, especially via solidity, it consists of multiple binaries.
00:52:33.928 - 00:53:15.540, Speaker E: So we have forge. So forge is the testing framework. It's meant for compiling your smart contract code. We have testing unit test, fast test, invariant tests. We have a debugger, scripts to manage your on chain interactions and deployments, and the verifying of your smart contracts. Next, we have cast. Cast offers a way to interact with EVM smart contracts on chain, which is like sending transactions, decoding on chain interactions, creating and managing wallets, that sort of stuff.
00:53:15.540 - 00:54:01.770, Speaker E: We have anvil, which is like a local Ethereum node, especially for developers. And then we have chisel, which is a repl for solidity. So, Foundry currently has nightly releases. And so to make sure that you stay up to date, you should run foundry up very often, like on a regular basis. So it has become a bit of a meme, but you can set your alias, GM, to Foundry app. I run that on all my systems. So, Foundry has been growing steadily over the last two years, both in terms of individual contributors as well as adoption by developers.
00:54:01.770 - 00:54:51.338, Speaker E: So we've seen a growth from, like 1% adoption in 2022 to now, really becoming a default choice for any serious project. So for many, this is the tool that they use on a daily basis and for all their smart contract work, from like a first proof of concept to a final deployment on chain. And it's used by many smart contract developers, auditors, bug hunters, searchers, like everything in between. So really quick background on me. I previously worked as a smart contract developer. I used Foundry as my daily driver since like the early days when Foundry was still devtoolsrs. And in April I joined the core team full time.
00:54:51.338 - 00:55:44.820, Speaker E: But to put some numbers to it, Foundry has like 100 served 37,000 lines of code in rust, excluding accompanying crates, which we did quite a bit, splitting out things from core and into individual crates. So since the last conference last year, we've grown from 292 to 420 contributors. We went from 6000 stars to 8000 stars. And yeah, foundry has over 4000 merge commits. So we've been doing a lot. So we're also seeing an increasing adoption by developers working on the major protocols that are pushing innovation across the space. This is one of them.
00:55:44.820 - 00:56:50.184, Speaker E: But other names are like optimism maker, succinct, symbiotic, Euler, Lido. They're all using this on a daily basis and as part of their core testing flow. So anytime a project makes it to production, I feel is a potential for these best practices that are learned during the development and testing phase of these projects to be shared and ultimately be fed back into foundry, improving the developer experience for all and pushing this page forward because building better products ultimately requires building better tooling. So let's move on and talk about the features. So as of now, Foundry has 411 individual cheat codes compared to the seven that dapptools originally had. So cheat codes are the way foundry enables you to do things. In got mode.
00:56:50.184 - 00:57:52.134, Speaker E: We have cheat codes for things like creating a wallet, deriving a key, and signing. We have support for this new sign compact which allows you to have signatures that are 64 bytes instead of 65, which is useful for a lot of things. We have things like getting, setting, environment variables, EVM state dumping and loading of that EVM state from disk recording, storage reads, recording logs directly interacting with an RPC endpoint, which is also really quite useful. We have things like interacting with JSON files, tumblr files, serializing, deserializing, broadcasting one or more transactions, including raw signed transactions. That's a quite new addition. But this can be quite useful for private transactions and other things. We have string manipulations.
00:57:52.134 - 00:58:58.038, Speaker E: Native assertions are certain things that like certain actions are performed, such as expect, call, expect, emit, expect emit anonymous, expect safe memory. So these are all things that to make sure that you're not violating your assumptions. Other than that, we have many other utilities. Impersonating an address, creating or computing your create to address, deploying a contract from an artifact, and random numbers, which is also quite a new and recent addition. Previously you had to use FFI for things like this, but now it's available directly inside. Yeah, and to use these cheat codes you use forge Std. So we've added an entirely new way of adding cheat codes and it's now really quite trivial.
00:58:58.038 - 00:59:52.210, Speaker E: So I would like to walk you through the process of doing this. This is one of the reasons, by the way, why we have like 411 cheat codes now. So the first thing you do is define the interface of a function that you want to implement. So you define basically, yeah, you describe on a high level what the functionality in terms of parameters would be. In this case start broadcast. Then you implement in the relevant section, in this case the cheat codes directory. And the nice part about this is that you implement the cheat code trade, and then you simply describe the functionality and implement the logic.
00:59:52.210 - 01:00:50.432, Speaker E: The self here holds the arguments that you describe in the high level interface. And then we have a cheats sub command that basically does some generation, and after that it's available in your solidity. So it's really this simple and straightforward. So if you have any new ideas about cheat convertionality that you would like to see, it's that easy to add. So digging in now we have forking. A lot of you are familiar with this. It's just something that I wanted to point out is that previously people were using forky URL, passing this into the command, but we now have cheat codes available that can do this in a much better way.
01:00:50.432 - 01:01:42.230, Speaker E: So right now we recommend using VM create fork in your test setup and VM select fork in the test body. It's especially useful when you're testing cross chain interactions. And with it you can fork at historical block numbers or even at a specific transaction hash. And if a transaction hash is provided, it will roll the fork of the block until the point that the transaction was mined. It's because the prior transactions are determined determine the state of the transaction that you are executing. So that's relevant. And by using VMselect fork you are easily able to switch between the different EVM forks, allowing you to test this behavior.
01:01:42.230 - 01:02:39.288, Speaker E: And alternatively we have VM create select fork to just wrap this and do this in one cheat. Next we have isolation mode. So by default, the individual transactions inside of tests and scripts are performed as if they were part of one transaction. So for most cases this is actually preferable because the setup and tear down overhead of doing this otherwise is quite significant. So for some use cases, however, especially those related to accurately measuring of gas, another approach is required. So what isolate does is you execute all test and script level calls as if they were separate transactions. You initialize them with like an empty journal stage.
01:02:39.288 - 01:04:09.810, Speaker E: You're triggering pre and post transactions and cleanups. So this allows us to actually include the call data based on 21,000 gas, the clearing of the transient storage, correct handling of your self destruct, et cetera, et cetera. And in this case the difference is 21,064, 21,000 from the base and 64 for calling the function. But yeah, an important point to note here is that none of these values are hard coded and it is actually correctly as you would expect it executing things as individual transactions, and therefore it can be considered authentic and related to the request. So when running forge test with the gas report flag, the isolate flag is now enabled by default. And this solves the long standing issue where gas reports were not accurately reported. So foundry previously supported tracing of external calls, making it difficult to debug intermediary values, especially things related to the internal jumps.
01:04:09.810 - 01:05:02.882, Speaker E: And we now support the decoding of these internal jumps. This flag is now available in both cast, cast, run and forged test. So let's look at an example of transaction of an ERC 20 token swap using the uniswap's universal router. So with the first decode internal. With the decode internal flag enabled, we first execute the previous transactions from the block, again to bring it into a state where. Yeah, bring it to the relevant state. Then the next point is fetching the the verified contract source code of the contracts that are actually used in the trace.
01:05:02.882 - 01:05:48.580, Speaker E: And then we compile them locally with their respective compilation settings. And as a comparison, this is without the code internal enabled, so it only describes the high level calls. It doesn't give you this introspection. It's good, but we can do better. And this is with internal tracing enabled. As you can see, we have internal function calls, internal storage lookups, intermediate values. So this is extremely powerful when you're doing your debugging.
01:05:48.580 - 01:07:46.070, Speaker E: So this functionality is not enabled by default due to the overhead involved, but now you have a tool that you can reach to when you do need this level of introspection. So we've made many performance improvements over the last year in foundry as a benchmark. I've taken Soledi, which is a popular library for optimized gas snippets by vectorized, and ran its extensive unit and fuzz test suite against the version of foundry from one year ago and from a recent release. And this is even like a naive kind of benchmark, but it shows a speed up of at least two x, but more likely depending on your code base. So we have a new feature called fuzz fixtures. And fuzz fixtures can be defined when you want to make sure that your set of values that you're using as inputs for fuzz parameters contain certain values, right? So to do so, you use the fixture prefix on any variable or function, and when you define the first parameter in the test function, you use the same name minus the fixture prefix, and by default you include, or by default, any values that you include that are included in this fixture are chosen 40% of the time, 10% of the time are edge case values, and 50% of the time are random values. But this helps you kind of guide your fuzzing.
01:07:46.070 - 01:09:21.460, Speaker E: We now support a replaying of failures using the rerun command. It's available in forged test, meaning that upon encountering a fuzz failure or a failing invariant test, the fuzzer corpus is persisted to disk, and upon the next run the corpus is used when replaying the first case. This really helps with the speeding up the iteration cycle during development. So when a failure is encountered in an invariant test, a process called shrinking is performed, and a failed invariant test can include many steps and conditions that aren't actually contributing to the failure case. In order to reduce the failure case to a minimal reproduction, you iteratively remove unrelated cases and remove unrelated cases from the failure case. So foundry previously took an optimistic approach, but this was rewritten entirely from scratch, and the speedup is very impressive. And for you, this means that your minimal reproduction can be extremely small and generally allows you to iterate quickly, which is obviously crucial for your development process.
01:09:21.460 - 01:10:23.946, Speaker E: So improvements like this make foundry not just a developer tool, but also a security tool. So we recently added the show progress flag to visualize the process of running your test more easily. So it shows your fuzz runs, it shows invariant test runs, but it also shows your shrinking runs upon encountering failure. So this is more. Yeah, so we have forged clone address. Forge Clone allows you to create a new forge project by cloning the source code of an on chain verified contract. It includes remappings, dependencies, and compiler settings used originally when the project was deployed.
01:10:23.946 - 01:11:41.380, Speaker E: By default, the forged clone clones the contract from the Ethereum mainnet via Etherscan, and is also possible to do this for other EVM compatible blockchains that foundry supports by specifying the chain id. So we now have support for VYPR, which is a contract oriented pythonic programming language that targets the EVM. It's used by projects like yearn curve, but also libraries like Snackmate. So in the process of adding support for VYPR, we've abstracted the way that that's we. How can I explain this either way? So we've started the way that we interact with the compilers, opening a way to support languages other than the EVM. So in the last year we've migrated entirely from ethers to alloy. You can now run forgebind with the alloy flag to create alloy compatible rust bindings.
01:11:41.380 - 01:12:26.740, Speaker E: This works for both solidity and Viper. So to wrap things up, we have a big thing that we're working towards, which is our first stable release. In order to achieve this goal, we are focusing specifically on four areas. One of them is correctness, completeness, performance and stability. And this means that any tickets that come in, or any new ideas that we have, we evaluate how does it fit according to these goals and along these areas. So after 1.0, instead of slowing down, we are actually going to accelerate.
01:12:26.740 - 01:13:12.800, Speaker E: I'm particularly excited for two new updates that we're planning. One of them is Anvil on Reth, which is a rewrite of anvil from scratch to build a new developer node on top of Reth. Then we have forge builder, which makes it very convenient to use forge as a library using the builder pattern. So please scan the QR code if you are. It's linking to the milestone ticket and subscribe to it if you want to stay up to date with our journey towards 1.0. Thank you. Happy to take any questions.
01:13:16.700 - 01:13:18.788, Speaker G: It's really cool to see all the.
01:13:18.844 - 01:13:39.886, Speaker F: Performance improvements to foundry. My question is, one of the slowest parts of solidity development is often just waiting for the solidity compiler, especially if you are in a situation where you have to turn on via IR. And I was curious, has the team.
01:13:39.998 - 01:13:43.334, Speaker D: Thought about attempting to write a solidity.
01:13:43.382 - 01:13:46.120, Speaker I: Compiler in rust or to do something like that?
01:13:46.310 - 01:13:48.280, Speaker D: Is that just too hard?
01:13:51.220 - 01:14:11.436, Speaker E: It's something. It's definitely like one of the concerns, right, that we understand, and it's not something that we can, we understand.
01:14:11.588 - 01:14:30.970, Speaker F: We've been playing with the idea of doing one. Sorry, I'm over here. We have a parser that's really fast. We haven't gotten the code gen part yet. It is very top of mind. If there are compiler experts in the group that want to help with that, we might have a position, but let us know.
01:14:44.400 - 01:14:49.020, Speaker D: Can you speak to the anvil rewrite? What are you excited about in particular?
01:14:49.760 - 01:15:49.600, Speaker E: Yeah, so anvil currently is basically a full node implementation on its own and it was written largely prior to Reth. So right now we have Reth in place and it will avoid a lot of duplicate code. So right now we have prs open for handling full reorgs and enabling certain tracing kind of RPC steps and all that sort of things. People are adding that currently still onto anvil, but hopefully once we have this unvo and ref, a lot of this will come out of the box and we can really push further. And it also opens up this idea of things were, a lot of things were added on top of each other throughout the years. It gives us an opportunity to kind of rethink the interface and generally get a much more performant node implementation rather than things related to caching and all that. We get a lot of cool things out of the box, basically.
01:16:07.150 - 01:16:21.910, Speaker C: Okay, next up we have alloy is ready for production from Yashdem.
01:16:22.760 - 01:17:00.694, Speaker B: Morning everyone. Thank you for coming out on a Saturday today. I'm here to talk about and declaring that alloy is ready for production. And thank you. Finally, for those who don't know, Alloy is a complete rewrite of our previous framework, ethers rs. Alloy enables new API improvements and features that weren't possible in ethers before, and we needed to rewrite for a multi chain world. When ethers was developed, rollups were in their infancy, let's say.
01:17:00.694 - 01:18:03.710, Speaker B: Now they're in their adolescence. So that's what we've designed alloy for. So let's get started. So alloy is right off the gate already used by the best teams to develop state of the art products such as REt Foundry, the SP one, ZKVM companies doing data infra mev stuff, and also the fraud proof implementation by optimism Kona one thing to note there is they have a no standard implementation, no STD, and alloy is still capable. Because certain crates in alloy, like the eips and the consensus types are no standard, it does not make sense for us to make networking related like provider and transport. No standard, obviously. So in this talk I'll mostly go through an overview and highlight the most powerful features of ally.
01:18:03.710 - 01:18:51.164, Speaker B: Now when ethers was written, it was probably state of the art and it was the best thing out there to interact with. EVM blockchains in rust but times have changed. And so we're introducing alloy. And the first building low level building block of alloy is the primitives, which are great and powerful. You can also write solidity in Rusnow with the Sol macro. And to prepare for a multi chain world, we're introducing a network types abstraction, which enables you to interact with multiple chains easily. And lastly to override provider behavior, modify RPC requests and responses.
01:18:51.164 - 01:19:40.490, Speaker B: Previously you had ethers middleware. The devx with that was cumbersome. I'll talk more about that later in the slides. But we've introduced a new way to do the same and achieve better results in an easier way, using what we call layers and fillers. So yeah, so right into alloy primitives. Firstly, we've re exported Rowan's U 256, which is the big int for handling solidity U 256 types. And it has all the features, starting from exponential powers, math ops, bitwise operations, even boolean comparisons and converting them to rust primitive types.
01:19:40.490 - 01:20:42.196, Speaker B: That might make you lose some precision, but it's definitely better if you want to print some begins or display them elsewhere. So we have a type called fixed bytes in the alloy core grid, and that essentially is the fundamental unit of, of representing byte arrays in the eVm. Now in RevM, in ret, et cetera. And the way that it works is it's generic over the length of the byte, and you can so the foremost example, let's say, is the address type from the alloy grid, which essentially is fixed bytes with a length of 20 bytes under the hood. And you can see that Alice, the address of Alice is actually equal to fixed bytes. So now you can create your own named fixed types like we did for address. We even wrote a macro for it.
01:20:42.196 - 01:20:54.470, Speaker B: It's called wrap fixed bytes, where you just pass in the named type that you want separated to avoid type confusion and. Sorry.
01:20:55.330 - 01:20:56.150, Speaker E: Yep.
01:20:57.810 - 01:21:38.012, Speaker B: So a little bit diving into the wrap fix by its macro. So the problem you face in while developing an EVM land is that a lot of these things are represented by hashes, sometimes of the same length, sometimes of the different length. And the most common is the 32 byte hash. Now, it can be the catch act. Output is also 32 bytes. The Merkle tree item is also 32 bytes. Now, if you're using only fixed bytes to represent both of them, best case, you can have error prone software, some incorrect logic while decoding stuff, and worst case, your program panics.
01:21:38.012 - 01:22:49.620, Speaker B: So to do that, you can use the wrap fix bytes to name and differentiate the fixed byte types. So what we're doing here is essentially creating a catch act output and a merkle tree item, and you can differentiate them while you're writing code well into the meat stuff. Now, alloy Sol macro this lets you write solidity in rust. It's a complete overall of the ethers Abi gen micro, which would only allow you to point to Abi JSON files or write Abi string representations and then create bindings for them. How we accomplish this is essentially because of the ally soltypes compile type bindings and the soltype traits, and we also use the natively written. It's part of the alloy code, the syntax solidity parser, and it's currently used by Forgebein. Tim presented in the foundry presentation that you can use the alloy flag to generate rust bindings for alloy.
01:22:49.620 - 01:24:03.474, Speaker B: The solidity macro essentially allows you to interact and deploy contracts very intuitively. So we've completely rethought how you should be interacting with solidity contracts when you're writing. The best programming comes out when you're interacting with a system that has a different vm and a different language, but communicating with that system feels native. So to do that, we have the RPC and the bytecode attributes in the Sol macro, which essentially let you create bindings and communicate with the on chain contract. It can also let you deploy contracts if you specify the init code in the bytecode attribute. How does it do this? It uses the callbuilder type, which essentially is a wrapper around the ETH call and eth send transaction RPC. It takes care of all the ABI encoding when you're passing in ARG's and sending your transaction or your ETH call.
01:24:03.474 - 01:24:55.008, Speaker B: And it also decodes the return values so that the developing experience is intuitive and easy to use. Similarly, we have the event subscription. Now, without setting up filters for listening to logs or events, you can essentially just use the Sol macro, pass your event in solidity and create a filter for it. It also provides you with the subscribe method for subscribing to a stream. And also you can use a watch method in case you want to poll the endpoint and not subscribe. It obviously uses the esubscribe RPC under the hood. And again, Abi encodes and decodes the logs.
01:24:55.008 - 01:26:34.010, Speaker B: So what you can see there, you have a decoded ad log and the raw log itself, making it easy to interpret. So now in 2020, in October 2020, Vitalik wrote an article on the EtH Magicians forum. What put a roll up centric world look like at the time, optimism arbitrum were in their infancy, they were active on testnets and we didn't have the reality, what we have right now, which is we have n number of roll ups with companies like conduit, enabling us to deploy different stacks, l two s, l three s, etcetera. While it looks like a very vibrant ecosystem from the outside for users with billions in defi, tvl and forecaster, decentralized social and everything, it's actually, it can be a difficult experience for developers since all these stacks and networks have n different transaction types, receipts and network specific implementation. So what it actually looks like is this. This is actually code from ethers rs, which is using different network implementation and transaction type and gating them behind the optimism flag. For example, the optimism deposit transaction, which has a deposit nonce field which if you would use natively to deserialize without actually by just the vanilla implementation ethers, it would crash your program.
01:26:34.010 - 01:27:36.490, Speaker B: And similarly we had to write shallow middleware in ethers. This actually, you don't get to roast your boss's code everywhere. So, yeah, and so people ask, can devs do something? And we did something. It's called the Alloy network trait, which essentially unifies and the consensus and the RPC types of different networks and making the alloy library ready for a multi chain, multi roll up world. The network steroid essentially defines the shape of the network, so it lets you implement a type safe network representation. For example, this is the network representation for optimism. The only difference from the vanilla representation of Ethereum is the op transaction receipt and the op receipt envelope, which essentially contains the extra field deposit nonce.
01:27:36.490 - 01:29:00.010, Speaker B: And so now you don't have sword crashes anymore, you don't need to have CFG feature flags and different compilation configurations, et cetera. You can capture the network specific nuances such as the deposit nonce l one block number, gas user l one all into one and easy abstraction. But in case you don't want to write your own network implementation and you're not familiar with the network, but you still want to get started developing. What you can use is the any network type which we already have in alloy network. It essentially serves as a catch all for all network variations, and it will consume all the extra fields that optimism arbitrum or any other network has, and put it served flat in it and put it into the other field. How does it do that? It leverages a type called with other fields, which is generic over the transaction type. The inner field is the transaction itself, and any other extra fields in that network's transactions are passed to the other field and you can easily write a small struct and just easily serialize after the network request.
01:29:00.010 - 01:30:12.028, Speaker B: Now, getting into the alloy provider, which is probably the most used part in ethers, you have to interact with the network. And especially now in a multi chain world, you want the provider to be generic over the network you're interacting with. This would enable you to interact with not only one network, but two or three or even more networks at once in the same code base without feature flags. So here's an example of the provider builder, which is building two. One is Ethereum, which is the default provider in alloy, and another provider for op, which uses the optimism network implementation which I showcased previously from the op alloy crate. The provider builder enables you to build a root provider with different configurations, not only networks, but layers fillers, which I'll talk about in the next few slideshow. You'll see the builder pattern in alloy.
01:30:12.028 - 01:30:54.730, Speaker B: Quite often we think it is the most intuitive and safe way for amateur developer to get started and build a provider. So ether is middleware. It's a thing of the past. It used to help you modify behavior of the provider and your transport. So every time you wanted to sign a transaction and send it to the network, you'd write ethos middleware. Every time you want to send a flashbots bundle, write ethos middleware, caching gas, escalation, you name it, you need to write ethos middleware. This approach was great at the time, but it's also too error prone and it has cumbersome devx and it needed a rethink.
01:30:54.730 - 01:32:03.938, Speaker B: Introducing alloy layers and fillers completely overhauls the ethers middleware framework, but achieving the same results in a better and easier way. We build upon the tower like layer and service architecture, where you can stack up multiple services. By the way, tower is a networking library, if people don't know, that allows you to stack multiple services like Ray Trilayer, rate limit layer, concurrency layer. So we have built upon the similar principles of tower and essentially separated the layers that you want to build, custom layers that you want to write into their own use cases and buckets. So the mental model how you should think about this is essentially three buckets. So if you want to write a layer that's independent of the RPC method, you're calling that. Essentially you want something like a retry layer or a rate limit layer, a cache, all layer that caches all your responses.
01:32:03.938 - 01:33:04.040, Speaker B: Or just simply a logging layer which just logs the serialized requests and responses that you send and receive. So in order to do that, you'll write a transport layer implementation. We already have a retry back off service layer inside alloy that you can use that currently is used by foundry to retry requests. And in case you want something more granular, in a case where you want to know the you want RPC method awareness, where you want to know which method you're calling and how the response or the request should be structured accordingly. So you'll write a provider layer implementation for that. An example of that is a multi data source layer. So let's imagine you want certain methods of the RPc to fetch directly from the network and certain methods to fetch the response from the cache if available.
01:33:04.040 - 01:33:57.888, Speaker B: So you'd write a provider layer implementation that will let you do that and granularly override the provider trait and write the implementation for each RPC method you want. Now, if you want to override the transactional lifecycle, which basically means you want to set gas params, you want to manage nonces sign transactions. So anything related to filling the transaction related fields should be managed by the provider filler. More on that in the next few slides. So layers again, like I said, it lets you modify an override behavior of the transport layer and provider. You can stack multiple layers on top of each other and have them work together at once. Fillers.
01:33:57.888 - 01:34:53.402, Speaker B: So fillers manage the transactions lifecycle. It essentially enhances and provides a frictionless experience. For example, we have a method called with recommended fillers on the provided builder, which essentially implements a gas filler, a nonce filler, a chain id filler. And in case you want to use a wallet filler, you can just do wallet and pass your wallet to it. What will happen next is every time you send a transaction request, you never have to fill in the gas parameters, the nonce, the chain id, or sign transactions manually. So what you see here on your right, sorry, on your left is the with recommended fillers flow, and on your right is the vanilla flow that you do without flavors. So you can see the difference in the lines of code.
01:34:53.402 - 01:35:52.850, Speaker B: It's just six lines versus about 1520 lines. So what's next? So currently, due to the limitations of the provider trait, you cannot override and write your own provider trait implementation, such as wrapping it over RETB. So we want to enable that. And to do that, we are introducing something called, in the next few weeks we're introducing provider call, which will let you fetch data from multiple data sources at once. And this would allow you to wrap your provider trait over the rest DB or any other DB. And second is alloy multicol we've had many requests to implement that, we've gone through the workflow, there are active issues over it, and we're hopefully we'll be implementing that also in the next few weeks. And lastly, the alloy 1.0
01:35:52.850 - 01:36:42.330, Speaker B: milestone. So currently, even though alloy is ready for production, we are rigorously improving it and finding new and different designs and architectures. So we essentially make breaking changes a few times or every few weeks. So we'd like to get to a place where it's completely stable and only new eips likely may or may not break. Ally? Yep, that's all. Thank you. Questions? All right, thank you.
01:36:48.190 - 01:36:53.290, Speaker C: Next up we have Tom from WebM talking about Modern J's on Ethereum.
01:37:14.040 - 01:37:55.610, Speaker D: All right, thanks for coming everyone. Super excited to have the only JavaScript typescript talk here today, so strap in, it's going to be a fun one. Usually when I give a talk I like to go back and look at some historical things that went on, especially in Ethereum. Everything changes very quickly in the JavaScript world. There was this talk by nosensmeister of uniswap fame where he was talking about building apps at an ETH global event in New York in 2019. And modern back then, according to him, which. Getting definitions from other people is always the move.
01:37:55.610 - 01:38:38.744, Speaker D: You don't have to define stuff on your own. Yeah, dapps being very responsive, feeling alive, they're connected to blockchain. So having really good data and your apps, making things feel up to date is really good for Ux, really good for correctness. You click a button, you get a quick response, stuff like that. So that's kind of like the frame in which we're going to view everything we talk about today. Yeah, and so the reason I decided to talk about modern J's stuff is we're getting close to the end of 2024. We're at like a conference that focuses on high performance bleeding edge software.
01:38:38.744 - 01:39:26.364, Speaker D: So I was like, okay, what's bleeding edge today in the JavaScript world for Ethereum? And yeah, there's no better way than kind of taking a look at some history. I think the history technology is really important when we build tools. It's really nice to understand the context of where things have been in the past. It helps you measure progress between different iterations and you know, maybe most importantly that someone said last night, if you don't respect your house, it will fall on you. So you can try to figure out who said that. But yeah, I just think it's really cool to look because, you know, a lot of the trends are the same. Like if we think about cars, it's like these are both like, you know, supercars.
01:39:26.364 - 01:40:11.464, Speaker D: At the time, the one on the top was like the best, but obviously no longer. But it still has a lot going for it. So, yeah, a little disclaimer, this is not going to be comprehensive. I sort of like got involved, joined Ethereum in like late 2020. So a lot of the historical stuff, you know, if it was still around by then, I like have used it. But a lot of the things that were, you know, like 2014, 2015, like the back catalog, maybe some of you have experience with, but I unfortunately wasn't able to play around that stuff directly at that time period. And yeah, if I missed something super down to hear more about it, probably not in the q and a, like I put down here because they have to be questions, not statements.
01:40:11.464 - 01:40:59.204, Speaker D: But yeah, I would love to chat, learn more about everyone's experience. So, yeah, let's turn back the clocks real quick before we jump forward into the present and talk about modern day. April 2015, web3 js had its first major version. I'm sure a bunch of people have used web3 js, whether it was in 2015 or more recently. Something that's really interesting with the context for web3 js is that, you know, while it was released 2015, April, the first commit was back in like September 2014, which is wild. Ethereum, like, wasn't in production yet for mainnet typescript. You know, hadn't even had a first version yet.
01:40:59.204 - 01:41:44.460, Speaker D: Like, the first commit for typescript was like July 2014, if I'm remembering correctly. So like very different worlds that this was like, this project was started in, which is, you know, really interesting to see where it's come to today. I guess next on this tour is something called Ethersim. Has anyone used Ethersim back then or. Yeah, not super surprised, but Ethersim was really important because it kind of laid the groundwork for like a lot of future things. Ethersim was like a JSON RPC library you could use for testing or like in development. It's kind of like test your changes in JavaScript and this kind of evolved further.
01:41:44.460 - 01:42:49.354, Speaker D: Truffle suite came out of here, Ganache came out of here. So really cool that people were thinking about that not too long after Ethereum launched there and then. Yeah, cut forward a couple of years. Ethers J's, I think was like a pretty big improvement on like the web3 js of the time. And even if it wasn't, you know, it was a different API it was a different approach to problem solving, introduce things that were pretty interesting, like human readable abis and stuff like that. Performance was improved and then on top of that there was type chain, which, you know, typescript has been around for a few years at this point, and there's like, you know, a bunch of JavaScript developers are starting to realize that strong types are, you know, pretty great. And so type chain made it a lot easier to then take your abis with your contracts and then spit out like a typed SDK, which then you can use to, you know, call whatever functions or listen to events.
01:42:49.354 - 01:43:23.384, Speaker D: And, you know, it introduces like a degree of precision and correctness that didn't really exist in the JavaScript world before. And especially with Ethereum, it's kind of dangerous. You know, you might pass like a value that is, you know, a much larger value than you want for a number and transfer things incorrectly. Yeah, 2018 maybe building on some of the things from Ethersim, Fiddler comes out. Does anyone, has anyone used this before? Oh, nice. We got one person at least. Yeah.
01:43:23.384 - 01:44:25.662, Speaker D: And you know, this was, I think, pretty important improvement for developers. It was like, you know, you could start building smart contracts and you didn't have to like compile stuff on your own. It was like a whole framework built in JavaScript, you know, a precursor to foundry in many ways. And builder will like come back up here in a couple years down the line. Web three webpack is like our first foray, maybe into more front end app specific related tools. If you're not familiar with Webpack, it's a bundler for the web that allows you to take some code, maybe it's JavaScript or typescript or some assets, svgs, and then spit out a built bundle that maybe transpiled the typescript of JavaScript or minified the svgs or some other stuff. And so web3 webpack was a way to hook into webpack projects at the time and have an Ethereum provider you could use.
01:44:25.662 - 01:45:36.802, Speaker D: And a lot of this, I guess the order of this, they came out around the same time. But EIP 1193 was really important along this journey because it took this chaos of people shoving providers onto window Ethereum and kind of standardized what an Ethereum provider API could look like, which then allows people to build a lot more cool things like wallet Connect kind of came out around the same time, which is again, I think a lot of people take it for granted today, but I mean, I take it for granted today because I wasn't around pretty well connect, but it allowed you to bring mobile into the equation. And yeah, it was a pretty big step forward. Wallet Connect widget now is called web3 modal, but this was kind of like the first attempt at a connect wallet modal, which I think you start to see people get more into the UX side of crypto, Clevis and Dapparatus. Has anyone ever heard of these tools? Yeah, you probably didn't, because I made them up. I'm just kidding. I didn't.
01:45:36.802 - 01:46:28.780, Speaker D: They're real. No, but these were like really cool tools. You know, I wasn't aware of these until I went digging back through the history. Clevis was like a way to run a node and do a bunch of changes to that if you wanted to go in and change the gas limit or other type of stuff. And apparatus was higher level components that you could use building a react app or something. So web3 webpacked becomes web3 react. Noah Zinsmeister from earlier worked on web3 webpacked, formalized it more into web3 react as work on Uniswap, like the OG or like one of the, you know, most well known Dapps was getting worked on.
01:46:28.780 - 01:47:15.826, Speaker D: And Scaffold ETH is kind of like this iteration on Clevis and dapperatus, which I think a lot of people, me included, like would get started with like a hackathon or just like, you know, as a quick way to get building. Biddler becomes a hard hat. And so it's really interesting, you kind of see like initial versions of these tools get released because there was like a need, someone needed this functionality to build. And then you kind of see them like, you know, a bunch of people use them because there was nothing that existed. They're a really good boon for productivity. And then you kind of see like maybe like a year or two later, like another iteration on top of them. You know, you learn all the mistakes that you made in like the first version of the API and Hardhat.
01:47:15.826 - 01:47:54.950, Speaker D: I think a lot of people got started on it works really well at the time for forking networks and stuff like that. This is what the 2020 frontier looked like. Got a couple JavaScript libraries for doing lower level stuff, which is nice to have options. Maybe you don't like how one thing does something, or there's a bug. You can fork Mainnet, which is really great for quick iteration and development. With type chain you can code gen types, and you get all the benefits of typescript. In that way you can hook into react, which is important for building apps.
01:47:54.950 - 01:48:26.980, Speaker D: Yeah, you have a connect, wallet modal so you can get started. You don't need to build that stuff from scratch. There's a bunch of quick start stuff in the ecosystem where I. You can get really quickly started at a hackathon or prototype, an idea which is really valuable. And so this landscape is actually pretty incredible. There's ecosystems in crypto that don't have all these things today. So the fact that Ethereum had these in 2020 in these versions was pretty impressive.
01:48:26.980 - 01:49:18.712, Speaker D: Yeah. So I got involved in like 2020, late 2020, 2021 was building apps. And yeah, I think the big thing that was missing coming from more of like front end worlds in web two stuff where there's a bunch of great libraries, bindings for react and building user interfaces and stuff like that was something that was a little more stable. So I worked on wagming just as a side project, was wondering if there was some way to standardize and make things better. So it's a react hooks library. The most important thing for me was just having robust wallet connection using some other tools in the past for app development. It's like maybe there'd be a bug where you'd switch networks and then it would just randomly disconnect you or stuff like that.
01:49:18.712 - 01:50:15.694, Speaker D: And that just wasn't a very good user experience. So yeah, it supports all the usual suspects. You can also plug in to other, you can write your own connectors and plug in there back in 2022. Also the other thing too is there is just a lot of duplication in codebases. I'm pretty sure most projects that got started around this time would just go to Uniswap's codebase, copy all of the react hooks for ens related stuff, sending transactions, signing whatever, and just like pop them in their code base. So this just tried to standardize all the features that you would want to do in your app. The other thing that was really nice, at least as an open source author is in the past a lot of people would, for more front end related libraries would mock a bunch of the other dependencies to their tests.
01:50:15.694 - 01:51:07.668, Speaker D: And so at this time we started using hardhat for working for our tests. Eventually we switched to Anvil, but this made it really easy to then reproduce issues and ship quickly without breaking things. So yeah, and then once Wagme was released, we started seeing a bunch of these pop up Rainbowkit, which a bunch of people here probably have used or seen before. It was probably one of the first libraries that built on top of Wagme. This is like the new modern web3 modal, but they had another iteration on that. Now we have libraries that do sophisticated account abstraction, smart contract, wallet related stuff, which is pretty cool. Improving ux.
01:51:07.668 - 01:51:54.130, Speaker D: There's like lots of these. It's probably one of the most interesting spaces to look at. I feel like a lot of them copy features from each other. A lot of people create new features that are interesting. Yeah, so there's a lot of cool stuff there. So yeah, Wagner was released, but it didn't really have any answer for type chain at the time. You could use like WAG muse hooks for reading from contracts, from sending transactions, but you still had to know what the inputs were for the contract that you'd be calling, and you could run into a situation where it expects a different value type and you give it another one.
01:51:54.130 - 01:52:53.660, Speaker D: Catching that at compile time was an interesting idea, but didn't really know if it was possible. So worked on this library called ABI type, which powers all of the type inference and safety in Wagme and VM these days, and wasn't actually possible before September 2022 because typeScript released this specific version a month prior, which allowed you to do some more crazy stuff with the type system. But what ABI type does is really cool. It takes an in ABI, it also works for EIP 712 type data, and it's able to like in the typescript type system, then infer function names, event names, which is helpful. Maybe you misspelled something, it catches that. It allows inferring different argument types, which is helpful because if you forget a certain value, it will kind of tell you and warn you ahead of time also responses. It will do that.
01:52:53.660 - 01:53:58.946, Speaker D: It is really robust. It works really well for even super complicated Abis like deeply nested tuples that have like arrays with other tuples inside of them, and it can pull out the different names of the properties and stuff like that. It also does pretty wild stuff like say you have two functions that are overloads of each other, depending on what parameters you pass in when you call the function. It's able to infer and narrow the return type. So yeah, this is just a really fun low level thing that powers a lot of the developer experience that we have. And so Wagme was originally built on ethers, which was great for getting started, but for the web, which is the context that wag me is used in, it kind of like demanded a little bit more out of the lower level library that it used. And so yeah, ethers is a bunch of classes, which is fine if you like classes, go for it.
01:53:58.946 - 01:54:55.570, Speaker D: I think in the context of a modern web architecture that's tree shakeable and stuff like that. Having a bunch of composable functions which you're able to remove if you're not using them in your code was really good. Our main goal with VM was to have an updated library that focuses on just the building blocks that you need. You don't have to buy into a specific opinions or structure for how things work. Most of the functions in VM and the way it works is just how it was written in eips and various specs. So yeah, this was like a big exercise in reducing bundle size to try to improve websites and push developer experience further by adding ABI type to a lower level library. It also does some other cool things like has built in multicall support.
01:54:55.570 - 01:55:54.690, Speaker D: No matter what chain you're on, if the chain supports multicall three, that works also just some other optimizations like it uses modern contracts that web3 and ethers didn't use like Ens universal resolver. So it is able to resolve and look up with like one RPC call instead of, you know, three or four depending on what you're, I think the mic test. Oh no, I guess it's back working. Sorry, I don't know what I did. And yeah, has like built in anvil support works really well for that. So yeah, if you're using JavaScript and Anvil and you're trying to like, you know, set up some tests, maybe you're running end to end tests and you're using Anvil as like the backend for that. You can kind of like reset the network or you know, do whatever via VM and Anvil.
01:55:54.690 - 01:56:44.836, Speaker D: So yeah, kind of like what we have been working on until now is we released like the second major version of VM and WAgMe. Wagme was like significantly reduced in scope. So it really just is like a thin reactive wrapper around VM now, plus support for all the different connectors, which makes things a lot easier and a lot of cases it's like the core of Wagme that you want to use is like the wallet connection and account management stuff. And then if you need to eject to VM, it's like really easy to do some more custom low level stuff. This was just something that we thought we should do is add support for another framework. React is still by far the most popular one in crypto. There's a bunch of projects, especially in Asia, that use Vue JS instead of React.
01:56:44.836 - 01:57:37.686, Speaker D: So we figured we'd add it and kind of extend out into different libraries. Probably at some point we'll add svelte like adapter. There's a bunch of other frameworks that maybe we'll consider at some point, but I think Svelte is next on the roadmap. VM's chain type is really cool, I guess similar to the previous talk alloy where it's talking about like the network types and how those kind of like determine the behavior of the library. Internally, VM is very similar. It's like the chain types are kind of like an inversion of control where they can define like their own custom rules for formatting a transaction, serializing it, whatever is specific to the network. And that allows us to not have a bunch of if statements inside of VM and custom code, which works really well.
01:57:37.686 - 01:58:42.042, Speaker D: We really strongly typed errors for VM recently. In the typescript world, you don't really have strong error types. It can be anything, it can be thrown from wherever. And this is kind of like an attempt to try to wrangle some of that, especially when you're calling RPC methods. You might get some esoteric error from a node, and we've tried to wrangle that together to make it a little bit easier to either surface that error up to a user in an understandable way, or be able to handle it in some more controlled fashion yourself, like recovering from it, retrying things like that. Another cool one that I don't think a lot of people know about is you can have your own deploy list reads, either via bytecode, which I'll go over in a sec, or a factory that's already deployed. Another really popular feature of VM is this ability to extend VM into whatever other features you want.
01:58:42.042 - 01:59:38.334, Speaker D: You can basically call client extend pass in some additional functions to be defined, some values, whatever else. And there's a few first party VM extensions. We have one for account abstraction, which was released recently, the op stack, which was a little while ago in Zksync, and it's really cool. You can build full on SDKs or other libraries. Basically on top of this, a lot of people have ones that use the internals of VM, but then expose whatever they have going on. That's unique. We also have a bunch of experimental extensions under the VM experimental entry point, which is just a good proving ground to dump out a bunch of stuff that people might be interested in.
01:59:38.334 - 02:00:48.852, Speaker D: So I would check that out. And yeah, I think kind of like what we're working on now and for the future is like making sure the stack works really well with the Alphanet that some people have talked about over the last couple days. And I think it's like an important component because if features aren't in these JavaScript libraries, they often aren't used by app developers. And so by connecting Alphanet to apps with Wagme or VM, hopefully it will be an exciting ecosystem where people are just trying a bunch of things out that you can send via URL to someone, they can test it out, as opposed to someone just being able to see something on the Cli, Orlando whatever. So yeah, where we are kind of like right now is we got a bunch of lower level libraries. I think there's a bunch of great things with hardhat and Anvil where you can fork and build your dapps. On top of that, I think we moved more towards static types instead of codegen.
02:00:48.852 - 02:01:14.650, Speaker D: It's just easier to set up. It just works with typescript, which a lot of people are using. You don't have to run separate CLI step. We have a couple different front end frameworks that you can plug into. Wagme also works with vanilla J's, so you're welcome to use it with anything else. We have like a bunch of connect wallet modals, you can try out all of them. Since they're all built on Wagme, it's like pretty easy to switch between them.
02:01:14.650 - 02:02:04.400, Speaker D: Just convenient. Got a bunch of quick starts and components. I feel like the hackathons these days, people are able to move really quickly and get started just because the stack is very obvious, what you're going to use, how you're going to build things. And yeah, there's a few other things that I wanted to share, but first referencing back to this in order for things to feel alive and responsive and feel like you're connected, one of the most important things is data, which is why it was interesting to see Storm's talk earlier. And yeah, apps need data to have good experiences. That's like the one bullet point on this slide. And there's a few different libraries out there that I think are interesting that approach this in different ways that are, you know, some of them are more experimental than others.
02:02:04.400 - 02:02:49.470, Speaker D: One of these libraries is called TevM, a bunch of EVM plus letters. And yeah, Tevm's a really ambitious, cool project. It does a bunch of things. You can execute an EVM locally in the browser, which potentially is really cool. Just like xxs are really useful for plugging directly into a node so you don't have to worry about network and serialization, deserialization, potentially being able to execute EVM locally in the browser, doing simulations and stuff like that potentially speeds things up. And also with node or demo or bun as well. I think one of the most interesting features is you can import solidity directly into typescript.
02:02:49.470 - 02:03:40.980, Speaker D: Why do you want to do this? I don't know why not? I'll show you in a sec. And this enables something really cool, which I think the author of this library will calls bring your own view function. So yeah, imagine that you want to read the balance of a specific owner of ERC 721 in VM right now you do something that looks like this, pretty straightforward. You get all the type, safety and stuff. The downside of this is it's two RPC calls you need to get the owner, and then once you have that owner, then you need to plug it in and get the balance. Not a huge deal. But imagine if you're doing this a lot in your app, or if there's multiple steps in between.
02:03:40.980 - 02:04:23.006, Speaker D: So instead of two RPC calls you need to do ten, because you need to resolve a bunch of stuff and then mash it together. Doing this directly in JavaScript is more expensive. In time, it costs RPc credits, all this stuff. What you can do instead with Tevm is you can write your own view function. This one's called avoid like the request waterfall, where you batch up all this stuff into like solidity function, and then you're able to import that into your typescript project via tevm. It does like some bundling and stuff behind the scenes. And then on the second to last line, which I guess you can't.
02:04:23.006 - 02:05:34.562, Speaker D: That's a bummer. Is there any way to move up the screen? Oh yeah, that's a good idea. Okay. Anyway, what is missing there on the bottom is you're passing bytecode to the Vm read contract function, and that byte code is like the compiled byte code from the balance of owner of functions. Then you're able to get all this stuff in one RPC call and do a bunch via the deploy list reads and calls that VM has enabled, which is kind of cool. Another project that's interesting right now I think, is ponder if you haven't heard of it. It's kind of like this all in one back end framework for crypto apps written in JavaScript on top of VM, and it has a pretty good developer experience really what you do is you set up your config file and what you want to list here is any networks or contracts and stuff you're using and you can create a schema for it.
02:05:34.562 - 02:06:14.924, Speaker D: These are like tables which either post grace or sqlite, whatever the data will be getting saved into for you to query later. And yeah, maybe not as fast as a rust CLI, but it's pretty fast. You can just like sync up and index a bunch of data. And then there's a couple options. Either you can query that data directly via API functions that they have set up, or you can just like listen to new events as they come through and have some custom logic. Maybe you send out like a notification to a different service. Here we're just saving it to a database which you can reference later.
02:06:14.924 - 02:07:12.488, Speaker D: This is all just like, I think this is friend tech related stuff. Another project which is doing some pretty cool things in the similar vein to ponder is mud. Mud is a project that I think is all about high performance applications, primarily used in gaming and stuff like that. But a lot of these concepts also come in to what people are working on in their normal apps. And because it uses the entire stack with VM and ABI type, you get all the strongly typed stuff involved in there. One of the really cool pieces of it that's unique is real time updates that you get. Similarly to ponder, it's like you define your table structure and set up, you create some methods to call into here.
02:07:12.488 - 02:08:06.830, Speaker D: This is just for a simple counter contract, and then you have a component where you can just read directly from that value and then it updates in real time behind the scenes with some of the infrastructure that they've built. And this is incredible because front end developers and I think more JavaScript developers in Ethereum kind of like span, like the gambit in terms of experience level. Like some of them are super knowledgeable about Ethereum. They've been in the space for a long time. They've used a lot of the tools that listed out before, and some of them are a little bit newer. They're interested in the space, but their skill set is building user interfaces or something. And it's not really about how to index data, how to query it, what the most efficient way, or what RPC methods should I use.
02:08:06.830 - 02:08:45.312, Speaker D: This is really cool because you can get real time updates from the chain just by calling a react component and having that set up. That's how the landscape is laid out today. I think we're in a pretty good spot right now, but going to keep pushing and see what else we can come up with. That's it. Any questions? Hi, thanks for the talk.
02:08:45.416 - 02:08:50.632, Speaker G: So you mentioned now like real time, data and response. And this is important for the feel.
02:08:50.656 - 02:08:52.682, Speaker I: Of these applications and how VM is.
02:08:52.706 - 02:08:55.418, Speaker D: Optimized for things like tree shaking to.
02:08:55.434 - 02:09:00.218, Speaker G: Make the bundle faster. Have you looked at all using webassembly.
02:09:00.274 - 02:09:05.850, Speaker I: Either for the core parts of VM or for porting anvil for local simulations at all?
02:09:05.970 - 02:10:16.696, Speaker D: Yeah, we've looked into that a little bit. Last year I gave a talk that was similar with trying to bring in rust into the JavaScript world, and I think it's really promising. I haven't checked back in on what the current state of things is then, but that whole thought was like the things that people use via typescript that's really valuable is the types. So it's like if you can write the types in typescript and then you can have the runtime be something else that's really fast, then you could potentially have an incredible developer experience. Yeah, and I think combining those two things together is a little rough. And one of the biggest issues right now is that the bridge to waSm, it takes a decent amount of time. Also, when you're first downloading the code in the browser, it can be a little heavier and usually a lot of the stuff that you're doing, maybe if you're executing stuff in the browser directly, obviously you don't run into this, but a lot of the things with JSON RPC calls and networking and stuff is the bottleneck anyway.
02:10:16.696 - 02:10:23.060, Speaker D: So yeah, I think there's probably going to be some interesting stuff there soon for sure.
02:10:26.960 - 02:10:27.568, Speaker F: Great talk.
02:10:27.624 - 02:10:28.420, Speaker E: Thank you.
02:10:29.600 - 02:10:43.754, Speaker F: The TevM example that you gave the TevM the balance of owner of functions, how is that different to a deploy list call where like you would do an ETh call on chain with the. Yeah, with like a state override where that contract is deployed?
02:10:43.882 - 02:10:54.466, Speaker D: Yeah, yeah. I'm not 100% sure. Yeah. Like how. Yeah, how it would be different? That's a good question. Yeah, I need to look into that.
02:10:54.578 - 02:10:55.730, Speaker F: Or maybe it's just the same.
02:10:55.850 - 02:11:00.670, Speaker D: Yeah, I'm pretty sure it is. Yeah, you're just getting the bytecode, passing that in.
02:11:01.340 - 02:11:18.080, Speaker F: The difference probably is that you do the simulation locally versus on the node. Right. Like when you do the ETH call with state override, the RPC must support state override, which some do not actually. And when you do it, you're actually running the compute on the server, whereas in the TeVM example, the compute, the EVM runs locally.
02:11:18.380 - 02:11:18.812, Speaker E: Okay.
02:11:18.836 - 02:11:18.996, Speaker D: Yeah.
02:11:19.028 - 02:11:21.164, Speaker F: So it's not doing any call in the Tavum case.
02:11:21.212 - 02:11:21.828, Speaker A: Yeah.
02:11:22.004 - 02:11:23.800, Speaker F: Okay, cool. Thank you.
02:11:31.590 - 02:11:41.166, Speaker G: Hi, great presentation. I was just wondering what your take was on how web development is sort of progressing with more rust frameworks. I don't know if you had a.
02:11:41.198 - 02:11:44.970, Speaker A: Chance to look at rust frameworks, but if you had any thoughts on that.
02:11:45.350 - 02:12:12.040, Speaker D: Yeah, yeah. I mean, it is interesting. There's definitely a lot of potential benefits for performance related stuff. Yeah. Like I talked about a little bit earlier, I think a lot of it is just around developer experience. It's not great right now because on the bleeding edge, it'd be hard for, like, most people to build an app using something like that. But I think it's probably coming even just stuff like rendering the Dom faster.
02:12:12.040 - 02:12:15.260, Speaker D: You can potentially speed up a lot by using rust.
02:12:23.170 - 02:12:41.310, Speaker A: How do you see the future of front end development in the sense that is it going to be fully powered by indexers or going to be a mix of RPC and indexers, or just some maybe more powerful rpcs or something like this?
02:12:41.930 - 02:13:47.420, Speaker D: Yeah. I'm personally really excited about, like, potentially people being able to run their own nodes and indexers and stuff and make that more easy with ref and things like that on the horizon, just because I feel like most organizations, once they reach a certain scale for their apps, it's really hard to rely on rpcs to get what you want, especially when there's the trade off with costs and whatever. So I think the closer you can get things to, like, your own services seems to be pretty good. Yeah, there's some, like, client stuff, which could be cool that I don't think has been experimented with a lot in the browser, but, yeah, I feel like a lot of these things, it's just like, you know, if you're interested, if you want to reach out to us and you want to like, explore with us, we're happy to, but I think there's like, so much opportunity just. Just like work on new projects on your own that push this stuff forward and then get it in front of people. Because people want better solutions than just querying their RPC and mashing the data together.
02:13:58.200 - 02:14:02.580, Speaker C: Okay, next up we have Brian from base talking about ref benchmarking.
02:14:16.130 - 02:14:58.570, Speaker A: All right. All right, thanks, everyone, for coming out on a Saturday, and I know we're getting close to lunch, so probably getting low in blood sugar and caffeine. So I'll try to take you through a nice, easy journey through benchmarking. So don't need to tell all of you here, but we think that on chain is the next online. And in order to achieve that goal, we practically want to vertically scale base to 1. It's a phrase that you heard a lot yesterday, but not yet today. So where are we today? We're at ten mega gas a second.
02:14:58.570 - 02:15:49.480, Speaker A: We've gone up four times this year so far. That's pretty good. But we still have another 100 x to go. So why have we not done that yet? There's a lot of challenges along the way, which covered in detail yesterday l one data availability fault proof compatibility state growth. But today I'm going to talk about the execution performance part of this. It's important to recognize that we're concerned the most about our scaling limits in the most adversarial conditions. So what is the worst case block or transaction pattern look like? And while we have a nice security model as a roll up of a one of n, it's only really useful if there's other people running nodes safely.
02:15:49.480 - 02:16:32.040, Speaker A: So we want to make sure that we have pretty good template for all of our ecosystem partners in order to run a node safely that can stay up to sync from our snapshots. Okay, so let's see. So Ethereum client has a ton of moving parts. There's a lot of different opcodes, over 100 that have fairly non deterministic costs. There's block overhead in addition to the transactions themselves, of course. Computing the Merkle state. The merkle, sorry, computing the try route.
02:16:32.040 - 02:17:22.562, Speaker A: There's also the fact that the performance will probably degrade as the state grows and the database gets larger. And then also none of this is running in a vacuum. There's of course the underlying hardware, software, file system and so forth. So how are you practically benchmarking Ethereum now? There's a lot of different transaction patterns and types from sends, transfers of tokens, swaps, non financial activities like gaming and social. And they all have very different access patterns. And you can't really statically analyze any of these because what a transaction does is entirely determined by the EVM state and not really what's in the payload. So one of my colleagues built this tool called the replayer.
02:17:22.562 - 02:18:51.450, Speaker A: It's sort of a black box tool for running through a sequence of blocks and transactions sort of acting, playing the part of a consensus client can plug into an existing ethereum node, pull down the block and transaction data, and then funnel it into a new node that you're running locally. So in addition to running through past transactions and replaying them, it can inject other additional transactions or tweak the payloads in various ways and then record timing stats at each step of the way and then perform tracing to get us the data we actually need to do our analysis. So that means looking at the opcodes executed and checking the storage diffs. So wanted to create a standard test environment here. We ran all these benchmarks in Roth 103, which I realize is a little bit behind the current version now. And this is the op version of course, and chose fairly commodity hardware that anyone can spin up today on EC two. So this is like the I three en six x large instance type which has plenty of cpu's, lots of ram.
02:18:51.450 - 02:19:44.370, Speaker A: Most importantly it has two locally mounted NVMe drives which critically are mounted in a raid zero configuration on like ext four. So a very standard no frills file system. So now the other important part here is to look at the actual sample data. So we wanted to find block ranges that have a wide range of access patterns, specifically those which are very disk and storage intensive. Settled on like a 25,000 block range on base main net, starting roughly around the 11 million point. And this notably contains a very high concentration of xen transactions. If you run, if you operate an EVM chain, you might be aware of this storm.
02:19:44.370 - 02:20:54.436, Speaker A: Actually has a great blog post detailing the impact of xen on a variety of other EVM chains. Most importantly, this is like a, this sends a ton of s loads, s stores, contract, deploys and sometimes self destructs at the chain. Okay, so let's try. So, running through these 25,000 lock range on the given configuration, we can see this chart here where I took every data point which corresponds with a single block and charted like the number of gas in megagasts on the x axis, and the time in seconds to go through the full block execution and import process. So if you squint really hard, you can say okay, cool. There's a linear equation here that generally maps to this and gets you around 60 megagits a second. But it's also obviously not a linear function.
02:20:54.436 - 02:21:38.570, Speaker A: Here we have the line towards the bottom, we have this cluster in the corners, the top left and top right quadrants. So this is actually very poor correlation between block gas and execution time. So obviously we need to go deeper. We can't just model this as a linear function of just like how much time per guess. But also there's a ton of different opcodes. As I mentioned earlier, it doesn't make sense to benchmark them independently. So intuitively we know that read and write ops are a lot less predictable from just adding two numbers or multiplying off the stack.
02:21:38.570 - 02:22:58.270, Speaker A: So what we did here is I grouped a lot of different operations into different buckets like storage ops, call ops, account manage account reads and so forth. And I sort of sliced the data a lot of different ways, and sort of the best thing we came up with was a yemenite fairly simple formula. So our variables here are like our total gas for the block and the storage related gas that's s loads, s stores, calls, anything that really needs to possibly read or write from disk, and then like an assumed overhead per transaction. So, running like a multivariate linear regression, like least squares, we were able to create a cost function here. It's better. So it kind of tracks the bottom case here and the top corner, but it's missing what's really the most important, which is our low gas high time blocks. So this is definitely not the right solution here.
02:22:58.270 - 02:23:52.550, Speaker A: So it's important to step back and ask, why is this the case? Gas is not really there to reflect execution time exactly. It's set up. Gas prices are set up for incentives structure. So while in ret, it might be computationally equivalent to add a new storage slot, update an existing one, or delete one, they all cost very different amounts. Now, separately, there's other factors such as failed transaction and inverted contract calls costing storage, but maybe not actually writing. And then finally, at the end of every block, we're only needing to calculate the state root changes based off the total net changes. So there's a lot of double counting depending on what transactions are the block.
02:23:52.550 - 02:24:36.830, Speaker A: So, putting this all together, we can kind of reduce this to a few fairly simple variables. So, we model the total gas consumed, which critically recounts any refunded gas. Just because some gas was returned to you for clearing out storage doesn't make it any cheaper in the short term. Then also excluding things like call data, which are there to keep the block small but don't really add any cost. And then we count the number of slots changed. So that's like the number of accounts changed, as well as storage lots within individual contracts. You put this all together on a block level, you get like a two variable equation.
02:24:36.830 - 02:25:35.728, Speaker A: It's very, very simple. And running the same regression pattern again, we actually get something that shockingly accurate in terms of modeling our various transaction patterns. So what this comes down to on this specific test example is around two nanoseconds per gas, another just under a millisecond or per slot changed with a very small overhead per block. So it tells us a good number of things here. One, that we can definitely have shorter block times because the cost of overhead is very low. And it also means in practical terms, that updating and deleting storage slots is really the most expensive use of gas on our commodity hardware. So after you factor in refunds, it's only around 4000 gas to delete versus like 22,000 to add a slot.
02:25:35.728 - 02:26:27.852, Speaker A: So that's like a five to six x difference in cost. So in the real worst case here, we're at like just under a milliseconds per slot updated, which gives us just over 1000 slots per second, which is in the absolute pathological short term case is like just over a fourth mega gas a second. Now there's a lot of silver linings here. Like every time the same storage slots accessed in the same block, those are additional ones, are kind of free for us. Now we did our benchmarking on one specific configuration. It's important to introduce a few other variables to see how that changes things. So the next really good news here, and I'm not sure how much you can see it, but throwing hardware with a problem does work.
02:26:27.852 - 02:27:18.350, Speaker A: So we bumped the instance size still far below the max available on commodity cloud hardware, which doubling an instance size on EC two gives you generally twice the cpu's, twice the ram and twice the disks. And really all that matters here is twice the disks in raid zero. If we look at the bottom, I'm not sure if you can actually see the bottom line out in the audience, but it stays roughly the same. Non storage related execution stays very constant because there's really no parallelism. There's not much to do with other cpu's, but the IOP's more or less doubles time to perform storage operations halves. So that's great news. There's a lot of room to grow there.
02:27:18.350 - 02:28:33.250, Speaker A: Now, also important to compare this against geth. So we took the lightest configuration possible, which is full node passdbach, and you get a much more linear representation of gas, which is actually kind of unsurprising when you think about the fact that Geth is the majority client on which all these gas prices were originally set. Now there's a few caveats here. One is I ran this on a much smaller set of blocks because the traces were like an orders of magnitude slower and I didn't really want to wait a week for this whole thing to go through. And the other important point here is that Geth is actually very rarely actually flushing the disk. So even though we were performing lots of storage, writes only twice did this self report as actually flushing the changes to the disk. So that also means that we can't really explain the P 95 spikes of this dataset, it all kind of shows up as noise and is not mapped to any particular pattern of opcodes or storage access.
02:28:33.250 - 02:29:28.332, Speaker A: So just kind of an interesting takeaway here. So maybe you've also, maybe you're running your own node and you've heard there's cool file systems out there that will let you have on demand or like live snapshotting and cool features like copy and write like b tree file system or butterfs. Don't do it. If you look at this y axis, we've gone up sort of like a ten x multiple, like a whole order of magnitude slower on the worst case. So just like using file systems with like smart features is really not a good way to run a database because your worst case performance just gets far far far worse. And in fact we couldn't really model this either with a similar cost function analysis as before. So where does this really.
02:29:28.332 - 02:30:41.912, Speaker A: So where are we today really in wrapping this up? So we know we can get around 500 storage writes per second per NVme drive in raid zero, which scales generally linearly. So in the absolute temporary adversarial case when you're just deleting lots and lots and lots of slots and doing nothing else with your gas, that puts you down somewhere on the order of two mega gas a second per drive, which also will slow down a bit as state grows. But the good news here is that all other gas is super cheap on Reth specifically. So we're seeing somewhere on the order of 500 mega gas a second block sizes. And we know that there's a lot of things being done to get that even faster. So and the other takeaway here is that like geth full node on bath DB generally has like an average case that maps sort of to the middle case for Reth. So that was like when you have storage heavy, but not pathologically optimized storage heavy access patterns, even though it's only rarely actually flushing disk.
02:30:41.912 - 02:31:36.800, Speaker A: So that tells us there's really good performance implications on Reth. And we've made huge leaps forward here. So just looking ahead, we know there's plenty of levers available for improving execution speed. There's rev MC compiled EVM bytecode, we can run things in parallel and actually start to use all those extra disks or extra cpu's we have lying around idly. And really the biggest one we can have right now is optimizing our disk writes. That really is the critical path to scaling further right now, ignoring things like DA and other other blockers along the way. So one takeaway here is that high through bit chains? Probably want to start charging more for storage access, and you can do this with just breaking EVM equivalents, you can just charge more.
02:31:36.800 - 02:32:01.520, Speaker A: It's not something we want to do lightly because it's just not great for the developer experience. So there's probably a good reason to align on Vitalik's multidimensional EIP 50 1959 set a number of storage lots you want to write per block and price the base on that. So thanks everyone for listening to my talk.
02:32:11.860 - 02:32:12.860, Speaker I: Thank you for your work.
02:32:12.940 - 02:32:17.318, Speaker G: Amazing presentation. I wonder if you measured the difference.
02:32:17.374 - 02:32:19.410, Speaker E: In read and write for storage, and.
02:32:19.910 - 02:32:21.894, Speaker I: How does the ram affect this?
02:32:21.942 - 02:32:24.086, Speaker G: Because if you feed the whole database.
02:32:24.118 - 02:32:29.370, Speaker E: In memory, the writes kind of the same, but reads instant from memory.
02:32:29.870 - 02:33:14.200, Speaker A: Yeah, yeah, great question. So I didn't want to bore everyone with lots and lots of slicing of the data, but I did specifically test that. So when we were bucketing storage slots, bye. By category, I was trying read access versus write access, and it didn't actually matter too much. So, like the earlier graph where we weren't looking at storage slots themselves, we didn't really get a much better model. And there's sort of an intuitive reason for that in that you already are paying a lot more in gas for writes versus reads. So I think it's actually fairly accurate right there.
02:33:14.200 - 02:33:30.760, Speaker A: Now, as for Ram, we did not try putting the entire state try in memory. I'm sure that would completely change the model here if you're no longer having I o bound operations.
02:33:36.470 - 02:33:43.730, Speaker H: If you go back to the last diagram of Re's, correlation between gas and estimation.
02:33:44.230 - 02:33:45.230, Speaker A: Yeah, this one.
02:33:45.350 - 02:33:47.046, Speaker H: So there still seems to be a.
02:33:47.078 - 02:33:50.010, Speaker A: Class of outliers on the upper left corner.
02:33:50.710 - 02:33:55.210, Speaker H: Are they significant or only visually significant?
02:33:56.470 - 02:34:39.090, Speaker A: That's a good question. It was generally within the margin of error. So let's see. So it's probably not, I mean, modeled perfectly as a linear equation, as was sort of covered yesterday. The overall storage, or the overall try representation is actually a tri of tries. So when we update storage slots on contracts, those are rolling up into the overarching master trie. So there is like somewhat non linear effect here, but this is generally a good enough approximation for figuring out our upper bound of scaling.
02:34:40.670 - 02:34:44.810, Speaker H: One more question. How many rounds do you guys did for this experiment?
02:34:45.630 - 02:35:00.540, Speaker A: So this specific test here is 25,000 blocks range. And I ran this, I want to say, on the order of like five or six times since I created this model.
02:35:01.360 - 02:35:02.300, Speaker H: Thank you.
02:35:04.520 - 02:35:45.960, Speaker C: I guess this is a question about the rest design in general. So, like, in order to account for like the moving parts of Ethereum and like the different chains. So we noticed there's a lot of like dynamic trace objects being like wrapped into with box and like in futures being pulled by Tokyo runtime. So I feel like that would be like an overhead contributing to the execution because it compiles like v tables instead of static jump in the binary. So I wonder, getting rid of those with getting rid of those things contributes much lower performance overhead independent of gas count or dependent depending on the gas count.
02:35:47.020 - 02:36:37.740, Speaker A: Okay, so that's a good question. Let's see. So I think we can definitely squeeze out more performance on the raw execution layer, which is sort of the low line here. I don't. So I don't think that would necessarily impact like sort of the worst case performance, which is kind of what we've been focused on in this test, given that's entirely bound by storage slot rights. I mean, if there's a great way to statically analyze the transaction and understand what's going to be written to in advance, then maybe we can really start to squeeze out additional performance there.
02:36:41.240 - 02:36:43.700, Speaker G: Hey, great presentation.
02:36:44.040 - 02:36:55.712, Speaker E: I'm over here in the couch. So I was wondering, caching could probably help on some of this. So I was wondering if you had.
02:36:55.736 - 02:36:58.712, Speaker D: Done any analysis on a specific cache.
02:36:58.736 - 02:37:02.442, Speaker E: Solution that might work, because there are obviously different algorithms, and some of them.
02:37:02.466 - 02:37:04.874, Speaker I: Are, for example, more temporal in nature.
02:37:04.962 - 02:37:06.490, Speaker E: And I would imagine for on chain.
02:37:06.530 - 02:37:09.642, Speaker G: Activity, you would have lots of activity.
02:37:09.706 - 02:37:12.666, Speaker A: Localized on some accounts like Uniswap or.
02:37:12.778 - 02:37:14.070, Speaker G: Send in this case.
02:37:14.690 - 02:37:15.590, Speaker E: Thank you.
02:37:17.010 - 02:38:10.102, Speaker A: Yeah, so there's, I guess, explicit caching, and there's also sort of the implicit caching of like what happens within the context of an overall block. So if you have, if you were building the block and you could say, okay, I know there's a bunch of transactions that are going to go and use the same Uniswap contract, you kind of get that extra performance for free just by charging the end user for the computation, but not really inducing any additional cost on the node. I think we can definitely improve the read performance across the board. Just bye. Like proactively populating cache. I think it was the rise team yesterday had an interesting point about hinting. Sorry, node.
02:38:10.102 - 02:38:46.900, Speaker A: Providing hints to other nodes about what transactions might. Sorry, sorry. I think you're going to provide hints from a sequencer to additional nodes about what storage slots can be read. I know that's kind of the idea behind the access lists, but they're not really well adopted. But we can definitely prefetch storage slots and put them in cache to improve the average case here.
02:38:47.430 - 02:38:49.810, Speaker C: Okay, we have time for one more question.
02:38:51.230 - 02:39:50.630, Speaker F: Thank you for the presentation. Do you think your findings are kind of a bear case for trying to price gas in a way that's a little bit too clever and kind of overfits the implementation? Like for example, get seems to be somewhat kind of, you know, more aligned with the gas costs currently, while Ret doesn't kind of fit the same way, does that mean we should basically never try to fit that because somebody else might make a new client that's not going to fit? And a second question is, have you done any benchmarks that are a bit more about consuming data from Rez? For example, if you have subscribers to web sockets and whatnot, like trying to measure the latency, things that may not be about increasing the gas limit, but more like the experience of using RET.
02:39:52.650 - 02:41:38.540, Speaker A: Ok, so these tests were focused on block production execution. So don't have the benchmarks on RBC subscriptions and that kind of latency. Okay, so as for your first question here, I think it's important to ask as a chain operator, what are the clients you want to support and what are sort of the price models here for each of them? So ultimately you're trying to figure out what can we squeeze in reasonably on either average or absolute worst case. So, I mean, I think at the end of the day you will have clients that are running way faster than other clients and can just whether that means they can do more optimized block building or serve other additional RPC requests like in between execution time, you're still going to be sort of bound by your worst case. But I think the other point here is Reth is currently focused on providing a very consistent fast access archive node where we were comparing that to the path db full node on Geth, which is really only able to serve the current state at any point in time. So I think if we were to take some of those, or apply some of those changes, and only buffer writes to disk and only periodically flush them, we'll see this compress and look much, much closer to the geth side, just much faster.
02:41:46.400 - 02:42:12.550, Speaker F: And this specific thing we also have observed and we have an experimental rewrite of the consensus to execution layer interface, which should help with that by removing the writes from the critical path. So this is not released yet. It's a behind an engine experimental flag. It's not recommended by any means for production use, but any people doing benchmarks should reach out to us for working on this.
02:42:14.370 - 02:42:19.720, Speaker A: Yeah, looking forward to rerunning these with those changes and really seeing the numbers go down.
02:42:24.740 - 02:42:35.280, Speaker C: Okay, so next up we have lunch. It's going to be similar to yesterday. It's just outside. We just ask that you be back here at 1230. We have a bunch of stack talks about breath examples. Out in the wild.
02:49:59.840 - 02:56:58.950, Speaker D: Sadeena, sadeena it.
02:57:49.900 - 02:57:50.640, Speaker A: It.
03:04:13.370 - 03:04:39.200, Speaker D: Sadeena it.
03:07:13.350 - 03:07:14.090, Speaker I: Girl.
03:07:15.070 - 03:07:26.610, Speaker D: Touch me tell me just feel it's what you.
03:07:32.270 - 03:07:33.450, Speaker C: Falling.
03:07:37.280 - 03:09:42.260, Speaker D: Shame on you telling you me bring me home make it through make me alone touch me tell me feel it.
03:09:45.330 - 03:09:46.070, Speaker I: You.
03:09:50.970 - 03:10:01.470, Speaker D: It'S been falling apart shine on you living.
03:10:07.010 - 03:10:08.106, Speaker F: Falling the.
03:10:08.138 - 03:13:38.548, Speaker D: Tone to all again trying to fade alive I never wanted to I will find feel alive tell us all that.
03:13:38.604 - 03:13:40.920, Speaker C: I was fading slowly.
03:13:44.820 - 03:14:14.730, Speaker D: Fight to fight all my life I will find feeling ize fight to fight all my life I will find feel alive.
03:22:05.990 - 03:22:06.590, Speaker C: This is.
03:22:06.630 - 03:22:37.860, Speaker D: Your tone, only your baby you are not alone.
03:22:40.040 - 03:22:41.552, Speaker A: You don't need to hold.
03:22:41.616 - 03:23:50.970, Speaker D: On and this is your don't fall, only your voice I don't hold me, hold my hand, hold me all no gift, only your.
03:23:56.190 - 03:23:59.050, Speaker A: Holy home.
03:24:07.030 - 03:24:08.050, Speaker D: This is.
03:27:54.200 - 03:31:54.042, Speaker C: We'Re going to start the afternoon session if you guys want to come and find a seat. Okay, we are going to go ahead and get started. We have a bunch of really interesting talks this afternoon focused on real world, out in the wild applications of ref. We're going to be starting with Emily, CTO and co founder of Shadow, who's going to be talking about Shadow Ref, and I will make the stage for her. Hi everyone. Hope you guys had a good lunch. I'm Emily, I'm one of the co founders and CTO of Shadow and I will be talking about Shadow Rep today.
03:31:54.042 - 03:33:03.840, Speaker C: Today. Okay, awesome. First, what are shadow logs? Shadow logs are gasless logs that can be permissionlessly added to an already deployed smart contract and they get generated in an off chain execution environment. So why are shadow logs cool? Shadow logs allow you to permissionlessly add as many custom logs as you want, all without increasing the gas burden to your users. And then this unlocks deeper data coverage and you can get net new contract data. Data that's like really hard or nearly impossible to get otherwise, or really cumbersome to get. You have to parse traces or do expensive ETH calls, things like that.
03:33:03.840 - 03:34:00.190, Speaker C: And as a result, shadow logs can drastically simplify your downstream data pipelines. So, for example, say you wanted to build a new front end feature, but your currently deployed contracts don't make that data easily accessible. Instead of redeploying your mainnet contracts, you can run a shadow contract that makes that data easily accessible. And we think all of this will allow people to build better, more data rich products and services for their users. So if you wanted shadow logs, the steps are pretty simple. You first edit the source code of an existing smart contract to add your shadow logs. You then run a shadow node with your new shadow bytecode of your contract.
03:34:00.190 - 03:35:21.260, Speaker C: And then the third step is you can get your shadow logs by querying the node. So at a high level, the way that this works is when a new block happens on Mainnet, we instantly re execute the same mainnet transactions from that block in an off chain execution environment. And then this off chain execution environment is super similar to mainnet execution, but instead of executing the mainnet contract bytecode, it swaps out the mainnet contract bytecode with your shadow contract bytecode at the addresses of your shadow contract. And then when your shadow contract gets executed, it reads from the same exact mainnet chain state, but is emitting all these additional event logs. And then you can fetch these shadow logs over JSON RPC, just like you can fetch mainnet logs. So this is off chain environment is shadowing Mainnet. It's re executing the same mainnet transactions against the same mainnet state, but can output like ten x or 100 x more logs, all without increasing the gas burden of your users.
03:35:21.260 - 03:36:20.140, Speaker C: So, taking a step back, let's talk about what the job to be done is. Whenever you're building crypto data infrastructure infrastructure with any crypto data pipeline, you're generally doing these three things. You're extracting raw on chain data by fetching data from a node. Then you transform that raw on chain data into a decoded human readable format, and then you load that transform data into some data store that you can then use for analytics or for your front end or whatever you need. And this is no different than standard ETL pipelines, right, that you see in traditional large scale data pipelines. So this is how most data crypto data pipelines are architected today. The source of truth of all on chain data are always nodes.
03:36:20.140 - 03:37:06.930, Speaker C: And these nodes are built to quickly discover and execute those new blocks and then commit those new blocks in the state transitions to its own database. Then your data pipeline subscribes to new blocks that get added to Mainnet. You can do this via websockets ETh subscribe. You can do this in a polling loop. Either way, you do this over JSON RPC, and then when you get that new block notification, you go and then fetch all this additional data that you need again over JSON RPC. And oftentimes you have to fetch traces or make ETH call requests or do a lot of kind of heavy RPC endpoints. And this is the extract step.
03:37:06.930 - 03:37:47.870, Speaker C: Then you write custom transformation logic to transform the data into your own use case. So you might do this in typescript or SQL or Python within a framework like subgraphs, for example. And this is your transform step. And then finally you write all that transform data into a data store like postgres or snowflake. And this is your load step. So there's some drawbacks to this architecture. The first is that the extract and transform steps can be really slow from a performance perspective.
03:37:47.870 - 03:38:47.038, Speaker C: You have added network time if you're going over the network and you're also paying a cost for JSON serialization deserialization every time you fetch data from a node. And then as a developer writing these data pipelines, it's not really a great experience as well. You're generally stuck with the event data or the contract data that was originally deployed in the contract, and that contract could have been deployed like years ago. You also can't reuse any of this logic that is actually running on chain in your transformation pipelines. You often have to rewrite the same logic that lives in smart contracts, but you rewrite it in typescript or Python. And if you don't want to build an indexer from scratch, you can use frameworks that exist out there. But there's like some ramp up cost of learning how those indexing frameworks work.
03:38:47.038 - 03:39:50.718, Speaker C: You're working with YAML files, stuff that no one likes. So how does this change with Shadow? The idea behind shadow is that you can actually co locate the extract and transform steps. So you can write your transformation logic directly in smart contracts. Run that transformation logic through the EVM, which has direct access to the exact chain state that you want, and then emit the transform data as normal EVM logs. So this is still following the same ETL pattern, right? But instead of writing your transformation logic in SQL or typescript or Python, running that over adjacent RPC, you can write your transformation logic in solidity or Viper and run it directly over chain state via the EVM. And this has a few benefits. The first is that it's faster and more performant.
03:39:50.718 - 03:40:46.330, Speaker C: Your transformation logic has direct access to that chain state, so you don't have to go over JSON RPC, which reduces network and JSON serialization deserialization overhead. And it's also a better developer experience in a lot of cases. Again, you can directly access chain state that you need, and then you can also reuse any code that's actually running on chain in your off chain transformation pipelines. So to summarize, a new block gets added to the chain. You execute that block through the EVM with your shadow contract byte code, which contains all your transformation logic. And then that shadow contract bytecode is also emitting your transform data as EVM logs. And then of course you can send these to whatever data store that you want, postgres, noflake, whatever suits you the best.
03:40:46.330 - 03:41:50.270, Speaker C: So this is a perfect use case for Reth xxs. This co located ETL process can be really easily modeled using an xx. So that's exactly what we built. We built Shadow Reth, which is an open source implementation of a shadow node built with RET xxas, and you can run shadow Reth to generate shadow logs directly within your Ret node. So last December we launched a hosted platform that allows you to get shadow logs without having to manage your own infrastructure or run your own node. Three months ago in May, we released Shadow Reth, which is 100% free and open source for folks who want to self host and run their own node. Shadow Reth allows anyone to realize the benefits of shadow logs within the comfort of their own node.
03:41:50.270 - 03:43:02.726, Speaker C: So why did we build Shadow ref? We believe that all hosted providers present some trustlessness trade offs, right? We believe that it's important for anyone to be able to generate these shadow logs in a decentralized way. And one of the reasons why we think this is so important is for verification purposes. So similar to how you could run your own node to verify the data that's being returned to you by alchemy or infura or tenderly. You should be able to run your own shadow node to verify the data that's being returned to you by a hosted shadow fork provider. And the ReFX framework enabled us to build like a performant shadow node to generate shadow shadow logs without requiring additional off chain infrastructure or maintaining a heavily modified client fork. So let's dive into how shadow works. This is what the architecture looks like, and there are three main components to shadowreth.
03:43:02.726 - 03:43:44.814, Speaker C: The first component is the shadow configuration. This is a pretty simple just JSON configuration. It contains your shadow bytecode and your shadow contracts. The second component is the shadow xx. And like any xxs, you get notified of a new block right when that block happens. So when it gets that block, the shadow Xx runs a custom executor that executes the block with the overridden shadow bytecode and this executor is really just using RevM under the hood. It has some shadow specific modifications that bypasses gas checks.
03:43:44.814 - 03:44:44.970, Speaker C: It can also bypass some transaction verification checks to save a bit on performance. And then after the shadow executor re executes that block, it takes out the shadow logs that were generated in that block and then writes it to a database. And then the third component is rshadow RPC. This was built using Reth RPC extensions that allowed us to really easily add a new custom RPC endpoint that we called shadow get logs. So when you're fetching logs, shadow logs from your own node, you can hit this endpoint and the results of the shadow logs look exactly the same as mainnet logs. And the modularity of ref and the xx framework was a huge unlock for us to build this performant shadow node. So huge shout out to the Reth team for all of their work on this.
03:44:44.970 - 03:45:54.580, Speaker C: Shadow Reth is already pretty powerful, but there are some missing features and some performance improvements that we think could make shadow Reth even better. So if you're interested in hacking with or on Shadow Reth directly, we have some ideas for you. I'm not going to go into details of all of these, but come find me afterwards and happy to chat and give you pointers. Cool. So we are really excited about the progress that we've made in the last few months, but where do we go from here? First, something that we're really excited about is being able to send this data to any data destination of your choice. Being able to generate these shadow logs within your node is great, but there's a lot that you might want to do with this data. So the goal would be to allow you to send this data anywhere you want by simply installing a data connector on your ret node for your data destination.
03:45:54.580 - 03:47:06.288, Speaker C: And lastly, this is just a teaser. I'm not going to go into the full details of how this all works, but we've been working on a decentralized registry of shadow contracts, and these shadow contracts were written by experts, often by the original protocol teams themselves. And these contracts are stored on ipfs and available for anyone to use. And we've been working with some top DeFi protocols like Uniswap, Yearn, Frax, Lyra, Pendle. They've all written shadow versions of their contracts and uploaded them onto this decentralized registry for anyone to use. So with this public registry, you no longer have to handwrite your own shadow contracts and you can just opt into pre written shadow contracts that you're interested in and get the shadow logs for those contracts. As part of this, we've also open sourced a CLI tool that allows you to permissionlessly upload a new shadow contract to ipfs and register it on base.
03:47:06.288 - 03:47:58.630, Speaker C: Pull down any shadow contracts that were previously uploaded that other people have written and uploaded to their registry. And then you can also use the CLI to really easily generate like in one command, that shadow reth configuration file that you can then pull into your own ret node. And you can find this at logs Xyz. This is just our hosted front end that sits in front of the decentralized registry in ipfs. So with all of this, the vision is you'll be able to permissionlessly opt in to pre written shadow contracts, run them from the comfort of your own node, and then send that data to any data destination of your choice. We're really excited about this. Go check it out and let us know what you think.
03:47:58.630 - 03:48:10.920, Speaker C: And that's it. Any questions?
03:48:18.900 - 03:48:43.740, Speaker F: Thank you for the presentation, that was great. Do you recommend, well, do you think developers should stop writing normal contracts like safeguards? Is that going to make a big difference as more like a vision for the future? I understand you don't have to use shadow logs if you don't want to, but yeah, maybe we should stop writing event contracts and just use shadow logs instead.
03:48:44.400 - 03:49:38.130, Speaker C: Yeah, yeah, that's a hot discussion topic. We don't believe that all mainnet logs should be removed from Mainnet. We actually think that that's kind of like just removing all mainnet logs is kind of an uninspiring goal. Like it really caps kind of the value that you can bring to the ecosystem. And what we're really more excited about is like the ability to ten x or 100 x the data that actually, that you can actually capture from on chain activity. But at the end of the day, you know, developers know best, developers know what's best for their own protocol, for their own users. Our goal is just to make sure that if developers do choose to omit mainnet logs and use shadow logs, that it is really easy, really cheap and accessible to actually get these shadow logs.
03:49:46.590 - 03:49:57.570, Speaker G: What happens if the contract that you're trying to get shadow logs from behind a proxy or calls contract that doesn't emit shadow logs or delegates some behavior?
03:49:58.030 - 03:50:18.370, Speaker C: Yeah, good question. So for proxy contracts, what you're actually shadowing is the implementation contract. So you shadow the implementation, you don't have to touch the proxy and the proxy will obviously delegate call to the implementation contract, which will then execute your shadow byte code and emit the logs.
03:50:19.000 - 03:50:28.872, Speaker G: And does that mean you have to have a mapping of your. I'm just looking through your repo, and you have this mapping of the shadow config and you have to map your implementation as well for proxies?
03:50:28.976 - 03:50:45.186, Speaker C: Yeah, that's right. I think there's a lot we can do better there. We can automatically detect when an implementation changes and try to deploy your original shadow contract at the new implementation address. Definitely think we can improve that there.
03:50:45.328 - 03:50:46.010, Speaker A: Thanks.
03:50:49.870 - 03:50:50.486, Speaker H: Sorry.
03:50:50.598 - 03:50:52.206, Speaker C: Can you explain a little bit about.
03:50:52.278 - 03:51:00.414, Speaker H: What Pendle did with shadow and how other Defi protocols might be, like, improved with this?
03:51:00.542 - 03:52:18.810, Speaker C: Yeah, yeah. So the pendle use case, if you want to go super, super deep, there is a published case study on, I think it's blog shadow xYz, where it talks about kind of all the math and exactly what they're doing, but at a high level, the way that pendle is architected is they run some off chain algorithms to optimize the on chain transaction route, essentially kind of similar to how Uniswap generates the optimal kind of pool route path and then submits that to the chain. And they were using shadow for easily back testing their algorithm. So they were able to basically back test and say that, okay, if our algorithm changed with these parameters, if those parameters were actually used for the mainnet transactions, how much gas would the user have saved? And that's kind of what they were doing at a high level. They were doing it for kind of backtesting purposes. Okay, I think that's it. Thank you.
03:52:18.810 - 03:52:27.770, Speaker C: Next up, we have Firon from flashbots, talking about our builder, a MeV builder, on ref.
03:52:39.280 - 03:52:58.938, Speaker H: Hello, everyone. Okay, I'm gonna. I'm Ferran. I'm from flashbots, and I'm gonna be talking about the art builder. The artbuilder is the new block building stack that we have developed at flashbots. So, how many of you know how PBS works? Trace of hands. Not that many.
03:52:58.938 - 03:53:29.120, Speaker H: So, this is how PBS works. So, this is how block building works. Doesn't point at the top. You have, in green, you have the normal block building workflow. You have the proposer, which will delegate to the execution client the build of the block. So the execution client will use vanilla block building algorithms to return a valid block to be proposed. And here at the bottom is what we call the PBS, or proposal builder separation.
03:53:29.120 - 03:54:27.140, Speaker H: Now we have two more actors. On one side, we have the builder. So builders will build blocks for the slots, and the proposers now will call the mapboost. Relay to sort of serve as an impartial action to select which block is willing to bid high for that slot. So the members lay here also adds extra security in case of the proposer not being able, though they may be able at the end, but not being easy for the proposer to unbundle the block of the builder. So what I'm going to be focusing on this talk is that actor of the builder. So all this stack was introduced, this previous stack was introduced by flashbots right after the POS, and we initially open sourced the mebboostrelay implementation, and then we also open source block builder implementation.
03:54:27.140 - 03:55:19.520, Speaker H: So this was a fork of goetherium with a more complex merging algorithm, and also all the bindings required to send bits to the mapboost relay. This sort of open competition got competitive at some point, and more sophisticated players started to enter the block building competition. So you'll have, for example, trading shops with custom order flow that could build more valuable blocks. Most of them will follow the same process. They will take the builder, or they will take AWOL four kit and add their own changes on top. These are the four main categories that we have seen so far, where they have introduced changes. They, as I mentioned before, they have internal order flow that they can leverage, so they modify the builder for that.
03:55:19.520 - 03:56:08.530, Speaker H: They may introduce new latency improvements, like connecting to the relay using web sockets. They can use new bidding strategies to decide how much they want to beat for the block, and to sort of combine all those three. They build custom merging algorithms, but again, all of them follow the same process. They take the client, they fork it, they add the changes, and they upstream. And I'm not sure how many of you have been in that situation, but that's really complex to do at scale. It creates some sort of psychological fear every time you try to create a new fear, because you don't know if Geth is going to break some interface or what Jordan is going to be available for the next release. Okay.
03:56:08.650 - 03:56:08.962, Speaker A: Okay.
03:56:08.986 - 03:57:00.358, Speaker H: Yeah. So we were in the situation as well, and we saw how we were not innovating in block building. And right at that point, we're thinking, okay, how we are going to update our builders, what's going to be our newest strategy for block building. And with the release of Ret, it was easy to think, okay, how we can leverage red for that, and the way we can leverage Reth is using its modularity. So it's important to note that go by itself, by the way go works, it's moderate language. You can already take code from Go Ethereum and using it as a module. Most people use the AVM inside the Ethereum, but just because you can use that module doesn't mean that it's going to be easy for you or it's going to be maintainable for you.
03:57:00.358 - 03:57:57.450, Speaker H: There has to be some kind of contract in that API for you to safely and in a scalable way to use it. And that's what Rev offers with the craze of Ret. We are sure that they can be breaking changes at some point, that they acknowledge that modularity is a first class citizen in the system, so they are willing to put some API contracts there that they are not going to break those implementations. So following that, that's when we started the art builder. So the art builder is a modular block builder built on rest. It's built on rest. That means that as the common layer uses rest primitives, rest provider and the alloy types, and that helps us to remove a lot of extra code that we don't have to do anymore on the builder, like typings or encodings.
03:57:57.450 - 03:58:56.470, Speaker H: And it's modular in the sense that we want to bring the same innovation that redis, bring into the nodes, nodes client into block building. We want to make it easier for people to take the R builder and create their own sorting algorithms. Merging algorithms add new functionality without having to go through the cycle of forking, merging, upstreaming. So how is it going so far? Oh, sorry. This is how the R builder works right now for the l one use case. So this is the same picture as before for the PBS example, and now we have the R builder that it's connecting with the rest database to sync all the state. So to do that we just use the rest crates to connect and access that database and get a reference that we can pass to the AVM for the R builder.
03:58:56.470 - 04:00:07.270, Speaker H: So how is it going so far? As of now, as two months ago, we don't run any more go Ethereum nodes. We just deprecated all the go platform of the block building, and we only use these R builders. They are way more efficient than it was before, and half of them run in this TE platform. So te is another angle of flash plots that I won't go into detail, but if you have more questions, come talk to me afterwards. Teas is a special type of hardware that is a security enclave where you can run your applications. So half of our builders, and by the end of the day we want to be all the builders are running already in this TE platform and with negligible performance, we are talking about 5% to 10% loss in performance, which are the features that I want to highlight first, as I give a hint before, is a modular block building. That means that there is a trait that you can implement to add your own custom merging algorithms.
04:00:07.270 - 04:01:08.890, Speaker H: You might have new strategies for Dag sorting or some simulation or some heuristics that you can easily plug into the art builder, test them out and send blocks into the l one chain to see how it looks. Combined with that, we have backtesting so you can easily check how does your algorithm works, how is improving. You can take a range of blocks from Mainnet and see if you are increasing the value that you are generating also pretty easily. And the third point, and this is something we just open source a few weeks ago, we have a local deployment environment. As I showcase here, in order to test the R builder, you have to basically run these three and sometimes four different services. So that was a huge struggle sometimes when we wanted to do local testing or some CI integrations. Now we have a complete local playground.
04:01:08.890 - 04:02:00.476, Speaker H: It's on GitHub, it's called builder playground. That takes 8 seconds to deploy all that setup all in memory, pretty easy to do. So that has helped a lot into testing new features and on the CI integration, and we hope to leverage that to increase more the coverage of the builder. So where are we going from here? These are the four major tracks that we are going to be working on for the next months. First one, and that's the one I'm focusing on. We are in multi chain support the same way that rest can can be a node for both mainnet and op chain op stack. We want Doctor builder to also be a builder for L1 chain and also for op chains.
04:02:00.476 - 04:02:50.700, Speaker H: So we are researching how we can make the builder modular to support these use cases. We want to improve block builder architecture. So we want to also research what is the most efficient trait architecture for the builder to support many different block building use cases. Third one, this was mentioned yesterday, is that Rev, it's not still as efficient as go Ethereum doing state root calculation. In our benchmarks we have geth blockbuilder at 15 milliseconds for hash root calculation. And right now with our build it is 100 milliseconds. So we are merging in the next two months a new hash root calculation improvement that brings down the number to ten milliseconds.
04:02:50.700 - 04:03:24.500, Speaker H: And the last one, which I'm the most excited on, we are going to launch a block building competition. So we want to launch like a sort of kaggle block building competition. When people can send their own block building algorithms, train them in some, set some snapshot of training data, and then we'll run them on some test data and create a leaderboard on top of that. So, yeah, if anyone wants to try, be on the look for the competition. Yeah. Thank you.
04:03:40.400 - 04:04:03.610, Speaker A: Hey, yeah, thank you for the presentation. I have a more general question about how you backtest. I'm curious because I've been talking with friends, or you just said it here. Usually how you back test is you take a subset of blocks and you replay them and you see how performant it was. I'm curious. It seems very constrained. I'm curious if you have other ways.
04:04:04.070 - 04:04:07.422, Speaker F: To attest the quality of your improvements.
04:04:07.566 - 04:04:18.712, Speaker H: So you don't test on a range of blocks. You have to test on the, you have to sort of replay the estate at which the block was built.
04:04:18.816 - 04:04:19.344, Speaker A: Right.
04:04:19.472 - 04:04:38.672, Speaker H: So you have to at point t, you have to start with state t minus one, and all the transaction does. The mempool had at point t. Right, because that's the state that the r builder used to create the block. Eventually, not all those transactions at point t are going to be end up on the block.
04:04:38.856 - 04:04:39.580, Speaker E: Right.
04:04:40.090 - 04:05:18.400, Speaker H: So we have a tool called Mempool Dumpster, which is also from flash watch, which just has, it's like an s three bucket that holds all the transactions that were in the mempool at different points in time. So if you combine that with an archive node, you can really start to replay a given range of blocks and see how your builder performs. So part of the thing that we're trying to do is sort of ease those requirements to run those backtesting. So you don't have to require an archive node. So we are testing, like, different strategies there to make it easy for people to run it.
04:05:18.860 - 04:05:45.062, Speaker A: But if you have a different behavior, wouldn't it mean that it would change also the behavior of potential other builders on the network or other actors that would also behave differently if you had a different behavior. Which means that your backtesting is valuable in a sense, because in this contrived environment, you can say, okay, I'm x percent better, but it's not like real world testing. So that's why I was just curious if you had like, other ways of testing than that.
04:05:45.166 - 04:06:01.980, Speaker H: Yeah, I don't think we have weight. We'll have you have a way to test how your strategy plays out with different block builders. Think this bug testing is only to test what's your baseline, on top of which, you can start to compete with other builders, but that would be something different.
04:06:04.200 - 04:06:05.140, Speaker F: Thank you.
04:06:06.840 - 04:06:09.664, Speaker G: In your diagram up there, I noticed.
04:06:09.712 - 04:06:22.562, Speaker A: That our builder communicates directly with the ref database. Can you elaborate on how the ref communication itself works and why not just run in process with ret?
04:06:22.706 - 04:07:12.800, Speaker H: Okay, yeah, that's a great question. So. Well, the way you plug into database is because of MDX, you can just plug in red mode inside the database. And why do we don't make it as a module? We are actively working on that. I think part of the modularity of the art builder will be on how you can create different flavors of it, and maybe one of those flavors run it as a module. This current architecture is helpful because the r builder does not depend on the chain syncing. So you can have the rest running and sync in the chain, and then you can just restart the rbuilder as many times as you want, while if you have it in the same process and you stop the whole process and then you restart again, you'll have to wait for that chain to restart, even if it's like ten blocks.
04:07:12.800 - 04:07:34.530, Speaker H: So by having it separated, we don't combine those life cycles. But I agree that with making it as a module of rev, it brings other benefits, like piggybacking a lot of memory and cache improvements that they might do in the future. So that's something that we are actively researching.
04:07:45.520 - 04:07:59.060, Speaker C: I think that's it. Thank you. Okay, next up we have Alex Stokes from the Ethereum foundation talking about mebrs.
04:08:10.970 - 04:08:19.750, Speaker I: Hey, everyone. Good afternoon. And yeah, this will pair well. It's a very similar take on the previous talk. And let me just try.
04:08:21.130 - 04:08:22.418, Speaker A: I don't know how that works.
04:08:22.514 - 04:08:38.230, Speaker I: Oh, I probably just broke something. There we go. I guess that'll do. Cool. Let's get started. So, yeah, you pretty much saw this diagram just in the last talk. One piece that was missing that is actually quite important is boost here.
04:08:38.230 - 04:09:00.510, Speaker I: Let's see if. Okay, there we go. It's like a big spotlight, but yeah, so you have the cl over here. This is driving our view of the consensus of the chain. The execution client metboost is this multiplexer component that interfaces between the proposer and the relay. The relay is here, and then the builder is over here. Actually, this is going to be better now.
04:09:00.510 - 04:09:38.026, Speaker I: Yeah, so you can see this whole pipeline. And if you're here, you probably have heard of Mev. This is like a take on the infrastructure, how it looks today. We have this flavor of like off chain PBS, and these are kind of the main pieces today, even in like the last, you know, couple years since the merge, this picture has actually gotten way more complicated. There's like searchers, there's like solvers for different applications, or like various off chain things. With respect to the l one, there's like account obstruction. So then you have like the bundlers there and like all the players in 437 landscape.
04:09:38.026 - 04:10:19.726, Speaker I: And. Yeah, point being is, if you didn't know over 95%, you know, it fluctuates, but say over 90%, even up to like 95% of all blocks on Ethereum are built through this pipeline. So it's quite important. And let's see the next slide. Yeah, so because of, again, how critical it is to the walk building process on Ethereum today, it's super important. We want to make sure that there's like a stack of software, and like this infra layer is very secure, well tested and robust. So as you can probably guess from the picture you just saw, there's like a lot going on.
04:10:19.726 - 04:10:58.160, Speaker I: Many different actors, they often to be coordinated in exactly the right way. And importantly, if there are security issues at any point in the stack, bugs or things like this, people have a bad day. There's the builder, the relay, the validator, and by extension the end users. So if there's a bug, say, in the relay, and they can't release the block, that's a whole missed slot, we would call it for the whole chain. And what that looks like for the end user is you have to wait twice as long, let's say, to have your transaction be confirmed. And that's just very not nice. So, what can we do about this? Given its importance and its like, centrality to the stack, security is like the top concern.
04:10:58.160 - 04:11:40.680, Speaker I: And one sort of, I'm calling it here, security posture. Like something that we usually do in Ethereum land is think about implementation diversity or client diversity. And the way that this whole off chain PBS with Metboost started, flashbots seeded the system, and it's very exciting that other people have joined to help build it out as well. So this is a talk on mevrs, and essentially it's a rust repo that has implementations of these three key components, the relay and the builder. So what's cool on here? Yeah, I mean, check out the repo, I guess. Cool thing for us here. It's pretty much all in rust.
04:11:40.680 - 04:12:19.750, Speaker I: You'll see there's a little bit of nics. That's just a side hobby I have. I do a lot of immutable deployments and things with Nix and Nixos, which is super cool. But that's a whole different talk. So yeah, why mevrs? So one way to think about it, at least this is where I started working on it, was just as very much like an experimental toolkit. So it can be useful for prototyping different ideas or features about improving or hardening this pipeline. Reason about protocol changes, something we did a few months ago, in the last year at least, was change how withdrawals should formally work.
04:12:19.750 - 04:12:54.390, Speaker I: Part of this whole process is that you have builders making blocks. They then submit some bid, which is going to be probably different than the actual value in the block. And this then interacts with the consensus layer. A big thing here is validator withdrawals. And so something we are seeing is that if you had a payment, let's say, to the proposer, and the proposer also was withdrawing ETH at the consensus layer in the same block. Many of the ways that relays and builders were computing the values to bid would include these withdrawals by accident. The withdrawals has nothing to do with MEV in the block.
04:12:54.390 - 04:13:29.810, Speaker I: So that's kind of just a bug. And having sort of this playground to experiment with these ideas is really helpful, because then it just provides some way to ground the thinking around. Okay, if we exclude withdrawals, what does that look like? How does it impact things? Stuff like that. It's also helpful for hard fork testing just to have again, a full suite of the stack. There's many things as we go from all the eips we're thinking about to actually like deploying them in Mainnet. There's a huge process to get to that final endpoint. A big part of this is testing all the pieces together, interactions between various eips.
04:13:29.810 - 04:14:27.630, Speaker I: And then as we do change the protocol, there are direct impacts on this builder API, which is again, this interface. Maybe I'll just go back really quickly. This interface, I broke it interface here between the validator and the consensus node and meth boost itself. There's a specification of how this should work, and basically anything we do over here that changes how the chain looks is going to impact this component and vice versa. Another thing to note is there's quite a bit of other rust software that I have that's again focused on the consensus layer and things there. So one thing to point out is a lot of MAvrs uses these two dependencies. I have a repo ethereum consensus, also Ssdrs.
04:14:27.630 - 04:15:17.494, Speaker I: So Ethereum consensus is essentially an implementation of the consensus specs and rust, and similarly useful for prototyping and reasoning about different features of the protocol. There's also an SSD library that's like a serialization format that has like a mercallization scheme built in. It's sort of the core primitive when thinking about any consensus data type. So helpful for all of those reasons. Another thing was to directly address this lack of implementation diversity. So, like, kind of, as I was pointing to a minute ago, especially, like in the earlier days, there was sort of just some initial implementations at least that were open source by flashbots, and otherwise, there wasn't anything out there at least that was open source. So the flashbots implementations were on go.
04:15:17.494 - 04:15:59.560, Speaker I: I like Rust a lot more, as many of you probably do. And it was like a natural choice to be like, okay, yeah, we can kind of like kill two birds with 1 st by having like a nice rest stack. Rust has this tagline like, performance, reliability, productivity. Pick three, and you can clearly see how that plays very nicely with something like, again, securing this very critical part of the protocol today. That being said, the community has grown a lot, even since I started working on all this stuff. Ultrasound, ultrasound relay project has stepped up and been doing a lot of their relay. You can go to the repo there, and you can see a lot of open source also rust software they have around different parts of the stack.
04:15:59.560 - 04:16:50.910, Speaker I: And we just saw this talk on our builder, which is a fairly new effort by flashbots to take a similar view with respect to the builder and importantly, open source. This was another key point, was that because of the way MeV works, it's if you have some edge or some alpha against your competitors, you're very incentivized to not talk about it. What that means then, is, like, if you're a more sophisticated actor, because, say, you're like a hedge fund that can afford really, let's say, expensive devs, it's really hard then for new entrants to come in and enter into the fray. So the issue there then is bearish entry around this process. And yeah, open source is good. You could imagine that it would help the sort of market structure. I mean, another key point here is there's like top, essentially only three top builders, which is like an incredibly small number.
04:16:50.910 - 04:17:27.258, Speaker I: And that's bad for other reasons that we don't get into now. But basically, the way to think about this is the more the merrier. And one way to start to address, like, the market composition is just by reducing bearish entry. So let's just walk through some of the components. This is just like a screen cap off GitHub. And, yeah, basically, there's again, a mapboost implementation, which is this multiplexer I'll get into in a minute, a builder and a relay. There's this Mavrs library, which is kind of just a catch all for various common utilities and things like that.
04:17:27.258 - 04:18:12.970, Speaker I: And then there's a binary to tie it all together. So we're going to look at each of these three in turn, but essentially they all are ready to support Deneb, which is the latest hard fork we have on Ethereum. There is some early work on electra, so let's look at these pieces now. So the first one is netboost. And yeah, essentially how it works is you go to like buy a block from a builder and what that ends up looking like is you talk to these relays, you can generally be connected to like many relays and then you get bids. When you go to like sort of sell your block space, as a proposer, you need to pick presumably the one that pays you the most. And then what that looks like is you get say a set of three, four bids.
04:18:12.970 - 04:18:40.480, Speaker I: You need to pick the highest value one. So metaboost is just this component. It's pretty lightweight and essentially just performs this selection process for you. If you look, it's really not much code. The core thing is really just getting the bids at the right time and picking the most valuable one. Okay, so next, the builder. You'll probably see a lot of overlap with the r builder here.
04:18:40.480 - 04:19:14.490, Speaker I: I mean, I think there are a few different design decisions that we've taken so far, but generally, you know, a builder with ref and. Yeah. So let's take a look. I guess one sort of, I think from what I understand so far, like the biggest difference is actually. So this builder has a node extension to reth. It just builds on top of Ruth as this very modular, almost collection of libraries and just sort of extends the execution node in a way to do the sort of mev boost parts that a builder needs to do. So the key thing is leaning on this payload builder abstraction.
04:19:14.490 - 04:19:40.880, Speaker I: That's super awesome. Shout out to the rest team for that. And the payload builder is tasked with actually crafting the blocks. Right now the builder just uses transactions in the mempool. So you look at the mempool and just build a block from what you have there at the time. You need to. There's this component, this notion of an auctioneer, which is doing a lot of the organization and coordination around all the different timings and when to build blocks, when to submit blocks and things like that.
04:19:40.880 - 04:20:35.830, Speaker I: There's also a bidder component that the idea, at least, is that you'd have this bidder, you could write your own custom bidder. And how it would work is whenever there's a block made by the payload builder, the bidder would see this. Say you would see, okay, I built a block for one eth, and then the question is now, how much value do I want to keep versus give to the proposer and when do I want to make that bid? How this works is, let's say there's a new slot and now a new proposer is going to make a. You could have many payloads. This really should be a continuous process. And then you can in turn, imagine this bidder is also this dynamic, very continuous thing as well. The bidder then can in turn, for example, look at the different relays that it's aware of, and then you see different competitive strategies between different builders, because I can actually see what other builders are bidding in real time and then use that to inform my own strategies.
04:20:35.830 - 04:21:06.810, Speaker I: So, yeah, this might be a little hard to read, but going to do a bit of a code walk through the builder and just kind of demonstrate how it extends. Ruth. So the place to start is this payload service builder, which essentially ends up building a payload service. And this is the piece of Reth that handles block construction within the El. And, you know, it does this stuff. I think there's also great examples in the Reth repo itself around how to do things like this. Ultimately, you end up with.
04:21:10.270 - 04:21:10.774, Speaker A: Right.
04:21:10.862 - 04:21:49.130, Speaker I: So this is essentially like the abstraction that you can sort of implement to customize this behavior. And it has a bunch of stuff, but you can't see that. Okay, sorry. Ultimately, there's a custom payload builder that this repo has that sort of. Yeah, it's the place to implement all the different, like, logic you would need for the med boost auction itself, which I'll touch on in a minute. Importantly, like, for example, there's a signer that's passed in because there's a way that you need to pay the proposer in a particular format. So then the builder then has a balance they're aware of and then a way to make transactions to pay the proposer out of that balance.
04:21:49.130 - 04:22:10.140, Speaker I: There's a fee recipient for the builder. So again, if you had a searcher submit a bundle, there's a way to say, hey, this is how you should pay the builder. Yeah, you can't probably. Yeah, you definitely can't see that down there. I'm sorry. But there's also a reference to the bidder that gets passed in. Okay, this should be better.
04:22:10.140 - 04:22:49.796, Speaker I: So, right. The payload service builder is the thing we're just looking at. And then ultimately it gets passed into this node builder type, which if you've extended Reth et al. This is like a pretty, yeah, you should be familiar with this. Basically there's a way to like take the default execution client that Reth provides and customize it. And there's like different, I mean it's actually very flexible, but you can have like different nodes or, sorry, different types and then different components in the thing. And in this case we just want to customize the builder component with the payload builder.
04:22:49.796 - 04:23:41.684, Speaker I: And then otherwise there's a way to asynchronously launch the other components like the auctioneer bidder, to tie everything together. And yeah, I was just going to highlight that this is again the other side of this channel to reference from the bidder to the payload builder. The code is a little split and I could just kind of for these screenshots. So again, if you're curious, I can show you after a bit more of the structure and the code itself. But ultimately you can imagine setting up the payload builder with the rest extension and then going to this part where then you wire these other components together again having a reference to the payload builder, the bidder. And yeah, there's a bitch in there. So that's a flavor for how some of the code looks just to actually extend.
04:23:41.684 - 04:24:16.370, Speaker I: Roth, let's walk through high level how this works. The first thing this is driven by is a fork choice. Updated notification from Ruth so the way that you run nodes on ethereum today is you have a consensus node and execution node. And so when there's essentially a new block and it's part of the consensus, the consensus node knows this. It tells ref at which ref then needs to update its head. And the way this is customized is then this notification gets dispatched to these various components within the Mevrs builder. When you see you have a new head, you now need to figure out if you need to build or not.
04:24:16.370 - 04:24:59.296, Speaker I: And there's information in the fork choice along with some other information you get from say your consensus node to know who the proposer is and then more importantly if it's registered with different relays that you're connected to. So you do that. If so, there's also like some preferences that validators have when they register, for example like their gas limit that they would prefer for the block. And then also the fee recipient, which is how they should be paid. And if you find some of these you start building. If not, you can just ignore this particular update and just wait for the next one, say 12 seconds later. So if you do need to build then for each proposer, because actually you could have multiple, just given the way the consensus works, then you want to start building payloads.
04:24:59.296 - 04:25:43.150, Speaker I: This is where the core ref machinery kicks in. You pass in this metadata around what the proposer wants the block to look like. And essentially for every block that the payload service builds, it sends to this bidder. The bidder in this way I was talking about a second ago says ok, for a given bid, how much do I actually want to pay out? When should I submit the biddenness? And it does all that. The bidder then sends the notification to the builder again at the right time to say hey, go ahead and finalize this payload with say this value, this payment. And then that ends up getting back to this auctioneer component. That then sort of puts the block in the right format and submits it via the right APIs to each relay.
04:25:43.150 - 04:26:27.058, Speaker I: So the cool thing here is it's built on Roth. It's fast. I don't have benchmarks, unfortunately, but I've been using it in various devnets and testnets and it works, I think from, well, I know from the experiments I've done that there can be quite a dependence on hardware. But assuming you can toss enough hardware at the issue, it can actually handle quite a number of different auctions in parallel. And you've heard this many times now, so it won't be surprise, but the biggest bottleneck that you quickly run into is just computing the stateroot for the final block. It's a quite heavy computation and there were talks yesterday, if not also today, around this problem in ref. Quick shout out again to the Reth team.
04:26:27.058 - 04:27:10.340, Speaker I: They've been super helpful around like iterating with APIs, just answering questions and being available, and also just very open to feature requests and things. Something that might not be clear is definitely part of this problem domain, is that you might actually want to very iteratively build the block in terms of even just the APIs. So rather than saying hey ref, go build a block, you say go build enough of a block that I can see how much mev is in it, and then I want to turn around and say bid this much because that also needs to be a payment transaction in the block itself. So there's things like this where you can imagine extending or hardening these. And yeah, the Reth team has been super helpful helping me understand and also just responding to requests like this.
04:27:12.160 - 04:27:12.448, Speaker D: To.
04:27:12.464 - 04:27:33.430, Speaker I: Keep the shout out going. Yeah. All this is built on top of Reth. This would not be possible without an execution client. And in particular the fact that Reth is so extensible and modular, it makes this like super nice and easy. It also uses ally in a few places. Like in particular, I used alloy signer just because I needed a way to, to sign transactions and it seemed like a really simple, nice way to do that.
04:27:33.430 - 04:28:08.800, Speaker I: I've even used foundry more for just making ephemeral wallets on these different test nets and things, and using that to fund the builder, move funds around, stuff like that. So nice work. Okay, we'll touch on the relay. This is like the last big piece and the relay component, it kind of sits in between the builder and the proposer. And the idea is builders are basically spamming related blocks. They have to figure out which blocks are valid which conform to the metaboost auction rules for those blocks. They then can expose them to the proposer via these builder APIs.
04:28:08.800 - 04:28:55.780, Speaker I: Just as of today, I would say this is the least developed part of the whole suite in mevrs. The way that it works now is there is essentially this trusted builder idea where essentially it only takes block submissions from hard coded builder pub keys. And this is basically just because I don't have to then think about implementing block validation. So in short, it means built with the MEV builders. A key piece here is like payload validation in terms of hardening this out into something that looks more like the production relays we see today. And the rest node extension strategy went so well with the. I really want to do this with the relay a few months ago.
04:28:55.780 - 04:29:17.910, Speaker I: That's probably changed. So if you're interested to contribute, that would be a place to start. And yeah, status, it works super awesome. Here's a tweet. There's like two blocks from Sepolia. And yeah, it's super cool. So that's it.
04:29:17.910 - 04:29:45.040, Speaker I: There we go. DM's are open. Here's my Twitter page. And yeah, I would love contributions. I try to mark good first issues. It's tricky sometimes because the ret APIs are, they've gotten more stable in the last couple months, but they were also changing as I was building some of this. The foundations in terms of how I think like the builder or the relay should be structured in this repo were also changing.
04:29:45.040 - 04:29:53.240, Speaker I: But yeah, if you're interested, please reach out and we'll try to find a nice way to contribute. And otherwise I'll take questions. Thanks.
04:30:06.910 - 04:30:21.530, Speaker F: Hey, great stuff. So you touched on this near the end. Looking at it from the perspective of a proposer, it is kind of scary to be signing a block where you don't know what's in it, right.
04:30:22.150 - 04:30:26.598, Speaker I: If it's an invalid block or something, you're out your slot.
04:30:26.774 - 04:30:46.112, Speaker F: That's bad for everybody. What do you see as ways to incentivize building new relays or what's in your view and the broader view right now, more long term solutions for that issue?
04:30:46.296 - 04:31:35.124, Speaker I: Yeah, this is tricky and I think a lot of the pain point, I think there's many ways I tried to address this. This was one way was just like even say a year or two ago, just be like hey, actually have like nice software that people could use and yeah, like what you're really getting at is the fact that these are all public goods at least like the relay in particular, and like more the consistency software with Medboost and things like that. And funding public goods is very hard. If you have a nice silver bullet, many people in this room would love to hear it. Yeah, there's like various efforts around like making some sort of dialogue structure for relays and that's when we fund them. Otherwise. Yeah, generally how the relays work today is they don't have any profit.
04:31:35.124 - 04:32:14.232, Speaker I: They essentially operate at cost or they're just operating out of pocket. And yeah it's tricky because if you were to have a for profit relay then you essentially be out competed by the other relays that wouldn't charge. Right. But at the same time you could imagine value add services around a relay and there's a whole bucket of things around customizing the actual type of the block. So you could say as a proposer I want to do, maybe I want a block that I don't know, there's many things you could imagine but there's things around customizing the actual contents of the block that the relay is positioned to implement. And this could be something to add, I guess. Yeah.
04:32:14.232 - 04:32:28.580, Speaker I: To give you one example you could imagine something almost like pre conformations or things like this where it's not just the contents of the block but extra semantics on top where you could imagine really doing this. And this is now something that they could actually charge for in a sustainable way.
04:32:33.240 - 04:32:59.030, Speaker C: What do you think of the people are usually taking relays for granted because relays is usually like one single point where people trust in ethereum ecosystem, especially for pre comps and stuff like that. People are proposing protocol at which you delegate block selling on the relay or stuff like that. What do you think of this evolves of research.
04:33:01.050 - 04:33:03.458, Speaker I: With precomps in particular or.
04:33:03.514 - 04:33:10.746, Speaker C: Anything in general, just people taking relay security for granted, thinking that they run on tee or whatever?
04:33:10.898 - 04:33:42.260, Speaker I: Sure. I mean, I don't know if people take it for. Okay, they do take it for granted until, for some reason, let's say they're like a solo sticker, they have a few proposals a year, and suddenly they wake up one morning and they thought they had a blockland, but the relay failed for some reason, and they found out they missed their proposal. Like, this happens to people naturally. They become very upset. So, yeah, it's a thing where people don't realize it's an issue until it happens to them, which is not good. And, yeah, I guess more on, like, the precomps point.
04:33:42.260 - 04:33:59.530, Speaker I: Like, again, generally, I support experimentation as long as it's sort of, you know, safe in the right way. So, yeah, I think we could see a lot of things evolve around, like, things that improve UX, or again, like, features around, like, how blocks were built in Ethereum that are valuable to people. And, yeah, maybe you could start to address some of these sustainability issues.
04:34:11.270 - 04:34:13.558, Speaker C: Looks like that's it. Thank you, Alex.
04:34:13.614 - 04:34:14.330, Speaker I: Thank you.
04:34:19.550 - 04:34:28.660, Speaker C: Okay, next up we have Ludwig and Will from Cirella talking about Brontes, how chasing Mev led to a general purpose blockchain analytics engine.
04:34:31.280 - 04:35:28.133, Speaker G: Hey, Chad. Today we're going to be presenting Brontes, which is basically the endeavor of us, chasing systematic MEV detection. And in doing so, realizing that, we ended up building somewhat of a general purpose blockchain analytics engine for Ethereum and all L2s that are based on the EVM. So why Brontes? We're not masochists. We have dealt with trace data in the past. Classifying it, preprocessing it, normalizing it is just absolutely painful. And personally, we don't really like that step, as opposed to the interesting analysis that we far prefer, like, focus our time on.
04:35:28.133 - 04:36:37.180, Speaker G: And so Bronte's really, the main initiative is to reduce all of that work, ensure and streamline the entire preprocessing portion of the work so that you can jump immediately into the analysis portion without having to worry about how to decode curves 56 pool implementation, or how to implement discovery. For a factory that produces 15 different contracts at a time, all of that have been such a significant bottleneck. And this was really aimed to completely mitigate that. The other thing is that we're working on MeV, we're working on minimizing it. And although everyone and their mothers love to chat about MeV, unfortunately we don't have that good data today. There's great research on the validator payout side, there's great research on the research on the timing game side. But in terms of data on systematically identifying actual MEV transactions and providing accurate kind of p and l statistics, we haven't seen that much.
04:36:37.180 - 04:37:48.690, Speaker G: And on top of that, there isn't really an open source place where we can all kind of participate and contribute to the discussion on how we want to classify, how we want to improve the taxonomy. So what you end up needing to be able to do this is basically akin to a feature complete block explorer with extra steps. This is something we didn't necessarily realize in the beginning, but we kind of pushed through painfully. So you need to be able to decode and classify these complex defi interactions. But the very important thing here is that you have to normalize them so that you can operate on a unified type, so that when uniswap swap or a curve swap happens, you don't have drastic different data representations. The other thing is that you don't want to lose the context that you gain from operating at the trace level, because when you're operating there, you have the call frame hierarchy, and then you can use that to infer many different things. So in this normalization and classification step, we really wanted to maintain that level of confidence context.
04:37:48.690 - 04:39:14.444, Speaker G: Then the second thing is that you're going to need a lot of off chain data for this. So you need an efficient way to store it, an efficient way to read and write to it. One more complicated thing is you need prices for almost every single token, and you need them at a transaction level granularity, because if you're looking to accurately price a sandwich, if you're looking to accurately price an atomic arbitrage, you can't depend on Coingecko's API. And trust me, like we initially tried, their rate limiting is also just not conducive to how blazingly fast brontes is. And then the last thing is you need extensive metadata on addresses, on contracts, also just more simply on builders and searchers, because you can't really accurately compute the p and l, because the interactions in a lot of these searchers between the searchers and builders are quite complex and are not straightforward where they. You'd basically mark out a bundle as negative in p and l if you didn't know about the special deal that the builder and the top searcher had, or the fact that Manta builder, which was Jane street, or another unknown builder by the name of Beaver, would actually send all of their profit directly to the builder. So the searchers would look unprofitable.
04:39:14.444 - 04:39:45.690, Speaker G: You need like that global context, because otherwise you just can't have accurate numbers. There we go. And so putting all of that together at the high level, you can think of the pipeline as we have a block. We're going to trace it. We're going to build the tree where we do the classification and normalization. And at the same time we're going to fetch the metadata. And then after all of that, we've collected the necessary data for our analysis and we can operate our inspectors.
04:39:45.690 - 04:40:01.520, Speaker G: So to collect and normalize actions, basically what we use is a custom retracer that matches logs with the core frames of, and then a macro system that provides a really clean interface. We'll tell you more about it.
04:40:06.500 - 04:40:24.040, Speaker A: All right, so to start, what we basically do is we collect all the traces in the transaction tree or in the block, of course. And then for each call frame, for each transaction, we go through a process.
04:40:26.380 - 04:40:34.788, Speaker G: Yeah, I'm going to try and point out. So, yeah, so here you have the individual transactions, and then they get processed at that point here.
04:40:34.844 - 04:41:08.640, Speaker A: Yeah, right here. So if it's a call, it goes over to our protocol classifiers that will be on the next slide. And if it's a discovery, it goes over to the discovery classifier. This also will trigger a pricing update which then says, hey, this call frame used. Well, if this is an ERC 20, we need pricing at this exact transaction index in the block. All right, I think we skip this one.
04:41:08.940 - 04:41:37.970, Speaker G: Yes. So this very similar. Basically, the protocol classifier is just a dispatch that you do on an address that is labeled as being part of a specific contract or slash protocol. So, for example, Uniswap v three would be labeled as Uniswap v three protocol. And then we'd have the function call as the additional portion of the match key that enables us to match against the specific function and then decode and then classify the data.
04:41:38.510 - 04:42:24.712, Speaker A: Yeah. So for this, we built a macro. The goal of the macro was just to simply abstract away and give just a really clean interface for dealing with all of these unclassified types and call data. It's kind of very messy in the back end if you for every single one, are doing all this decoding manually. So we thought it'd be nice and clean to put it in this. So basically all you have to define is the path to the protocol module, the path to the alloy function, call type, log type, and what you want your closure that actually does the logic behind it to be passed in. So here.
04:42:24.776 - 04:42:25.420, Speaker D: Oh.
04:42:28.440 - 04:42:56.290, Speaker A: There we go. Okay, so here is what uniswap v three, mint call looks like. Oh, keep breaking it. All right, there we go. Yeah, so you can see here, protocol, and then the mint call, that's just the first two entries are basically how we create the key that people match on the action. Type the log that you want. And for here, we just want everything.
04:42:56.290 - 04:43:45.180, Speaker A: So with that, you can see, I mean, it's probably twelve lines of code right here, just reading from the database to grab protocol info. Take the deltas, return the mint. It's really that simple for pretty much all protocols except curve. And then on the flip side, we have the discovery. So it's another macro surprise. We have the create pair call, and then just the factory address. And I mean, this is even more simpler, right? It's just take the two kokens, name the protocol, insert into the database, and for almost all of the discovery we have out here, it's this clean, which is really nice, and it's super simple to test as well.
04:43:45.520 - 04:45:15.106, Speaker G: And the nice thing is, so as soon as a create pool is effectively discovered, that gets stored in the database, and then any time that there's a function call to that contract will then run the action dispatch, which will classify the underlying swap or mint or burn or whatever interaction. So this is really like an extremely zoomed out overview, because it would take an hour to present fully on this. But I think this is probably the most compact, complex portion of the entire project. So basically, the main problem is that you can't afford to load all of the state of all of the pools if you want to be as comprehensive as we were trying to be. And you need to find a solid enough proxy to be able to identify what the optimal route is, so you can mark out an instantaneous price at that specific transactions index in the block. So what we did to simplify, and again, not perfect and very much room for improvement, is we use a global token graph with the connecting pools as edges for the token nodes. And we use that connectivity as a proxy for liquidity, which enables us to build subgraphs that are more focused, where we can actually afford to load all of the state for each pool and then compute the optimal route and then identify the instantaneous price.
04:45:15.106 - 04:45:53.396, Speaker G: And so we're able to price basically any token that is on uniswap v two, v three swap force, we started implementing curve. We have like, we don't have the willpower for that. So if anyone wants to take, take that on, that would be great. Thank you. Yeah. And then the separate portion of the data that you're actually provided is the block metadata. And this is a really valuable portion where you store all of the data that we could possibly think of could be useful in the analysis of any financial interaction.
04:45:53.396 - 04:46:31.576, Speaker G: So you have the relay data, the builders, the bids, the fee recipients, the builder metadata with connected searchers. So what searcher addresses are vertically integrated with Beaver? What searcher addresses are vertically integrated with Rsync? Basically any form of metadata that you could think of on the builders, even their IP locations, but we're not sharing that. And then you have the address metadata. So labels from various providers. Then you have the labeling from private transactions. Huge shout out to chainbound for that. Thank you.
04:46:31.576 - 04:47:47.220, Speaker G: You guys did an amazing job and made it super easy for us. And then you have the propagation time for the blocks. And the biggest portion of the data are all of the trades and quotes for 95% of the pairs across the top five crypto exchanges. So to inspect a block, now that we have all this data, now that we have this classified block, we can move to the interesting part where we provide the block tree and block metadata that I mentioned right before, and we pass it into the individual inspectors that all run in parallel. So those individual inspectors then produce their results, and there's a final processing step where we can compute the final block p and l and also de duplicate the results. And this is one of the surprises that we had while building this, is that if you miss out on the de duplication and filtering step, you're not going to be able to label things accurately because the MEV can look exactly like another type of MEV if you're not able to filter it out. So say, an atomic arbitrage and a sex Dex trade.
04:47:47.220 - 04:48:20.560, Speaker G: The atomic arbitrage is obviously getting an advantageous rate. If you aren't able to identify that as an atomic arbitrage and only run, say, a sexdex inspector, well, then now you're labeling it as Sexdex. So that is, like, super important to de duplicate. And the funner part in the processing is actually the composition where you can take Jit attack and then realize that it's actually jitsex Dex, or you can take a Jit and a sandwich and realize that those are actually the same bundle. And so then you can kind of compose these more, these simpler mev types into complex types.
04:48:22.660 - 04:49:00.488, Speaker A: All right, so we started off with this whole thing as mev, mev, mev. But then we realized, as Ludwig mentioned, in the start that it kind of is applicable to anything. So like if you wanted to build an inspector that just did something super simple, this is all you need to implement, right? It's just you define what block window you want to look at what you're going to return as a result and then just write your code. Right. So next slide. Oh, no, picture again. There we go.
04:49:00.488 - 04:49:38.698, Speaker A: All right. And then at the higher level, right, we have these things called processors, which is just a collection of inspectors processing that generate the same result. So all you need to do is implement your processresult function and then create your config and you pretty much can do whatever inspecting you want, any analysis that you want. All of the complicated actions are just all hidden away. So it's super easy to build whatever your mind can think of really and.
04:49:38.754 - 04:50:38.148, Speaker G: Quickly to touch upon the block window portion. So that's a feature we actually very recently added. But to explain this basically enables you say, ok, I want five blocks in a row directly provided as data. And so this is something that we haven't used that extensively in the code base yet, but is super useful if you're trying to build, say EV intent inspectors that look across multiple blocks and see transactions in previous blocks that are a signal for a statistical sandwicher that is going to look at approvals on token contracts to then front run transactions. And finally, it's blazingly fast, at least kind of. Again, we don't have benchmarks, but it runs in two main modes. It runs historical ranges which are broken up into chunks, and then those individual range executors will all run in parallel.
04:50:38.148 - 04:51:37.320, Speaker G: And then at the same time we have a tip inspector that is specialized to follow the tip of the chain. Then in terms of futureworks, we'd really love to see support for l two s. You don't have to do that much for Opreth, for example, you have to do a tiny bit of a refactor on the discovery and just update the address book to match the latest chain. Then we'd love to improve MeV Inspector methodologies. Sextext is always going to be an estimation, but the closer we can get to that the better. And also in terms of long tail MEV inspection analysis, it's very hard to filter out a lot of false positives, but with a decent amount of work I think we could get something really compelling. And then ev intent inspectors are really high on our roadmap cross chain MeV inspectors, although I am still strongly of the belief that this is shared sequencer psyops, but I'd love to be proven wrong.
04:51:37.320 - 04:52:20.450, Speaker G: One other kind of very off topic feature that we'd really love to see is a taxes feature with brontes, because you have all the transactions, you have an ability to decode them, you have all of the prices and you know when and address traded. So it wouldn't be too hard to actually be able to give you an almost perfect report of all of your on chain trades, although the actual legal calculation of how much you want to pay is probably not that simple. But moving over that and then improving the speed and reliability of the Dex pricing and extending it to support more protocols would be really amazing.
04:52:22.360 - 04:52:32.900, Speaker A: Yeah. So just shout out to everyone over this past probably eight, nine months of just constant work on brontes. I mean, Joe especially.
04:52:34.120 - 04:52:34.640, Speaker G: Yeah.
04:52:34.720 - 04:52:36.288, Speaker A: Yeah. Went through a lot.
04:52:36.384 - 04:53:06.898, Speaker G: Yeah. Had to implement hundreds of bespoke connections to all of the varying centralized exchange streams. So huge shout out to him because otherwise this wouldn't have been possible. Then, in terms of resources, you can scan here, there's the GitHub and then the book. Leaving it up. Oh, yeah. Quick acknowledgments before I'm done.
04:53:06.898 - 04:53:39.648, Speaker G: Sorry. So, huge shout out to the ref team and yellow team for building the primitives that made this possible. Special mentioned I to Danny Popes, because we initially tried to use the sole macro on every single verified contract on Etherscan, and I can tell you we found a few edge cases like the. What was the keyword for Rust? A bunch of ridiculous stuff. And he patched up all of that. So huge thanks. And then big thank you to gaussian process and Kevin for reviewing our.
04:53:39.648 - 04:54:07.730, Speaker G: Our inspector methodology and kind of going through the results. So huge thanks there as well. Contact us here. And we're also hiring for Rust devs. But, yeah, that's it for us. Thank you so much. And this is a quick sneak peek at the front end website that we're going to be opening up very soon.
04:54:09.830 - 04:54:13.850, Speaker C: Any questions for should I forgot the questions?
04:54:24.790 - 04:54:25.150, Speaker A: Yeah.
04:54:25.190 - 04:54:25.838, Speaker I: Thanks, guys.
04:54:25.934 - 04:54:45.288, Speaker A: Great presentation. I'm actually curious about how do you get the metadata regarding the order flow like you said. Okay. We have metadata about the searchers and the builders and their private deals and et cetera. Considering they're private, how do you get the information regarding that?
04:54:45.424 - 04:55:11.550, Speaker G: There's a media trained answer. And then there's the real answer. The media trained answer is that we are on chain sleuths and do extreme data analysis. The real answer is that you can contact all of the competitors and get them to snitch on each other because it is in their incentive to give you information on how their competitor is operating. So that has been an extremely fruitful source of information.
04:55:18.810 - 04:55:33.240, Speaker I: Yeah, thanks. It looks super cool, and I'm excited to see this dashboard that you just had. Go live. So given all of this, the rich software you have for the analysis of things and all the analysis that you've done with it, what's one surprising thing that has come out of this?
04:55:35.180 - 04:56:04.460, Speaker G: So one of the. There are a few. There are a few. Sextext is not as competitive as people think. Builders are on paper, clean, not sandwiching. But if you mark out a lot of the sandwiches, they don't appear profitable, and that is for a simple reason that they are working together. So they are shifting the responsibility on another party.
04:56:04.460 - 04:57:24.780, Speaker G: The other thing is people love to make fun of, well, at least in the very niche MeV research community, people love to make fun of Justin time liquidity attacks because everyone says, oh yeah, they're not profitable. You're basically always better off sandwiching, which is almost certainly true. But when you mark out JIT attacks, they are 99% of time unprofitable until you mark them out against a centralized exchange and you realize that this is not a just in time liquidity attack that is trying to steal the fee from a liquidity provider, but rather this is a sextext arbitrager that is also a builder that happens to realize that you're trying to trade at a discrepant price. But that price discrepancy is within the fee bound. So they don't profit from arbitraging, from trading against the poor directly because they pay the fee. But where they do profit is if they provide the liquidity, they get paid the fee and they get to clear you at this discrepant price and have effectively swap. But that was like a really interesting kind of point that we came to understand, because you would expect sextext to really be bad for LP's, and everyone says, oh, sex Dex is great for the chain in terms of getting prices in line.
04:57:24.780 - 04:57:32.770, Speaker G: But it turns out when your builder is also a sextex arbitrager, you as a swapper are also getting a pretty bad, bad deal.
04:57:40.710 - 04:57:46.090, Speaker F: What do you see as the most prevalent forms of MeV after this analysis.
04:57:49.070 - 04:57:56.930, Speaker A: There'S a lot of atomic arbitrage that we've seen, but mostly sex decks. There's sex decks almost every block.
04:57:57.720 - 04:58:20.460, Speaker G: It's sex Dex, and I'd call it sandwiching, but it's really just Jared's trades at this point. It is insane how much this guy trades. The top three counterparties on uniswap are wintermute, beaver and Jared.
04:58:21.920 - 04:58:22.758, Speaker A: Cool.
04:58:22.944 - 04:58:23.746, Speaker G: Really cool codebase.
04:58:23.778 - 04:58:28.722, Speaker A: I'm excited to take a dive in and see what's under the hood. One of the questions I had was.
04:58:28.786 - 04:58:30.346, Speaker F: If you could elaborate a little more.
04:58:30.418 - 04:59:31.382, Speaker A: On the block inspector process and how you actually create this sort of compound classification between different strategies like JIT and then Sexdex as well, what is the actual control flow that's being able to classify this compound behavior? So we kind of glossed over this a bit, but we have this tree that is just the hierarchical structure of all of these normalized and unnormalized actions. So then with these inspectors, you more or less just get past in this tree and a bunch of metadata pricing. So with that, for example, with like a sandwich, we look, we just go through all of the traces, mark out the same addresses, take those addresses, take those possible victims, and then look at the pool overlap, say, oh, this is trading on this pool in this direction, victims in this direction, back run in that direction. You got a sandwich and you have.
04:59:31.406 - 05:00:00.920, Speaker G: Nice utils where on the tree you can say collect, and then you pass the action type. So it's like collect swaps. And this will give you all transactions where a swap action has happened. And then you can also collect these specific actions out from each transaction. In terms of the composition aspect, that happens really at the last stage. Once all individual inspectors have run, you have two things. You have the filtering deduplication, and then you have the composition step.
05:00:00.920 - 05:00:45.560, Speaker G: The filtering says, if there is a type of me, this type of mev and this type of mev give priority to the, to the one that you decide. So this is useful. For example, atomic arbitrage versus sextext, because atomic arbitrage will be stricter in terms of what we can classify it as. But a sextext will look like a positive p and l. And then for the composition step, again, it's just the results from the JIT inspector, for example, and the sandwich inspector, which will naturally overlap if it's a JIT sandwich. And then you can declare a specific composition function which takes these two individual bundles as they're built, but are actually the same bundles, and then unifies them into a single complex mev type.
05:00:46.820 - 05:00:48.840, Speaker C: Okay, we have time for one more question.
05:00:50.900 - 05:00:52.132, Speaker D: Now that you have all of this.
05:00:52.156 - 05:00:55.924, Speaker A: Data and you've kind of had time to process it, maybe, how does it.
05:00:55.932 - 05:01:01.512, Speaker D: Affect your outlook on the designs like that? The Ethereum research community is looking at.
05:01:01.536 - 05:01:03.472, Speaker A: As far as, like, proposer builder separation.
05:01:03.536 - 05:01:17.460, Speaker D: Or any of these other ideas, like what you would recommend or urge people to do, because you mentioned, like, people wanting to talk about it, of course, but like, what, what actions or things are you guys kind of like, thinking of after doing this analysis?
05:01:18.640 - 05:02:12.178, Speaker G: I'd say like, two things where now we have the data, I don't think having this data is enough for us to have a super opinionated view on what is right and what is wrong in terms of the PBS roadmap, given that there's so many second order effects of possible solutions that we might imagine. So that's why I try and be very careful on giving very specific kind of guidelines there. But that's also a very boring answer and doesn't tell you anything. So the second portion, and disclaimer, I'm very much talking my bag here, because this is what we're working on. But what we saw is that this idea that there's so much like General Mev is kind of psyops in the numbers. 99.9% of it is just all the same.
05:02:12.178 - 05:03:09.090, Speaker G: Even liquidations don't matter that much. It's really just sandwiching atomic arbitrage, sex Dex in no particular order, by the way. And from that, we know that applications that are being designed today have, or at least claim to have immediate solutions towards it. So if anything, I'd like to see less of a focus on handling the redistribution and prevention of extraction at the base, because I think that that very quickly becomes quite opinionated and rather push it to the application there. But that has, like, significant kind of trade offs, one of them for kind of applications that control their sequencing, and the other is just complexity. If you're an app dev and you're building on a financial primitive that is known to generate and produce large amounts of Mev, you now have to handle that yourself and be a big boy about it. And that is not easy.
05:03:09.090 - 05:03:12.130, Speaker G: So, yeah, like a lot of trade offs.
05:03:14.750 - 05:03:33.400, Speaker C: Thanks, guys. Okay, before we move into the hacking portion of the day, we have a couple quick announcements. First, we're gonna have Andrew from the cursive team, who's been powering your really awesome NFC badge experience. Share a bit more.
05:03:35.900 - 05:03:54.996, Speaker F: All right, hope all of you enjoyed tapping your badges, and thanks for trying it out. If you're curious about any of the cryptography or NPC stuff, feel free to come find me and ask any questions. Two quick things. Number one, you've been collecting flowers for every single person that you've tapped. Every single person has a unique flower.
05:03:55.108 - 05:03:56.132, Speaker G: So you can go in the app.
05:03:56.156 - 05:04:01.186, Speaker F: And tap your own badge and you can see a garden of all the flowers you've collected. It's pretty neat.
05:04:01.218 - 05:04:02.230, Speaker H: It's pretty cute.
05:04:02.530 - 05:04:26.690, Speaker F: We're going to share our flowers to Twitter, so if you want to do that, feel free. And the second thing is you can make a proof that you've met 50 people at the event. And if you or Vivek, we will give you one of these exclusive NFC rings. So very special and thanks so much. All right.
