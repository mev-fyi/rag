00:00:00.280 - 00:00:12.350, Speaker A: To talk about the giga gas. So we have Roman and Danny walk us through a bit, through how we're gonna basically solve scalability, scaling, performance and all that. Give it up for Roman.
00:00:12.390 - 00:00:33.478, Speaker B: And then click one, two. Okay, this is the Giga gas talk. All right? So you'll hear us say a word, giga gas. A bunch of times. Let's play a game like you can count it and whatever. George's can figure out the present for you afterwards. Yeah.
00:00:33.478 - 00:01:14.890, Speaker B: Giga gas. All right, what happened to all of the images? That works. All right, so measuring chain performance. Obviously, TPS does not represent an actual throughput of the chain, because you cannot. A single transaction does not have information about its complexity. A better metric is gas per second, for a number of reasons. You can see on the screen, I think everybody is on board with us for measuring the chain performance accurately.
00:01:14.890 - 00:02:13.564, Speaker B: So where we are at now, we're currently at 100 million guests per second. Our goal is, obviously, you can say it, giga gas. Why? Because we want to speed up the Ethereum chain and DL two s to be able to compete with all of the other networks. And the main bottlenecks are execution mostly in historical context, because thinking an archival node, if you look at the chart, most of the time is taken by execution. The state route, obviously, we've talked about state commitment, and we will talk about state commitment a lot. The blockchain tree. This is the special abstraction in our codebase that is tracking the unfinalized blocks and database commits in hot path.
00:02:13.564 - 00:02:23.240, Speaker B: And we'll tell you later how we address that. All right, so perf, what do. And then you will talk from here.
00:02:26.380 - 00:03:28.020, Speaker C: All right, so one big bottleneck in the current state of the node is that we are spending all the time in execution. What does this mean? It means that we are. No picture. Yeah. It means that we are spending a lot of time in the EVM interpreter. So how brief round on how the EVM works is that you have some bytecode which is unstructured, a list of instructions, just raw bytes that an interpreter will load up when a transaction gets called, when a transaction gets sent. And this will step through every single instruction, execute it, evaluate it, do some bookkeeping, and then go to the next instruction or jump to somewhere else in the program.
00:03:28.020 - 00:04:59.840, Speaker C: This works fine, has worked fine always. The problem is, a lot of time is spent just bookkeeping and in the interpreter, stack manipulation, which is essentially just moving variables around to satisfy the requirements of the EVM, which is generally avoidable by instead of interpreting the bytecode, you compile it down to native code using an optimizer, which will essentially remove all of that overhead of the stack manipulation. So this is essentially what RevM sees. Instead of taking a byte code and executing each instruction one by one, we first analyze it. We determine some basic structure of the program. For example, we check for static jumps so we can have a nicer control flow, and some other analysis passes for more optimizations. But this is essentially what it does, is you analyze the bytecode, you emit some internal intermediate representation, which is in this case LlvM.
00:04:59.840 - 00:06:06.080, Speaker C: And once you optimize it using LlVM, you can emit native code, which can then run just normally on your cpu. The point of this is that it's a 100% replacement of the interpreter. You can essentially, the way it's implemented is you just, instead of calling the interpreter with a bytecode, you pass it onto the compiler. You can just in time compile it, or you can compile it ahead of time into an object file, which you then load at runtime. And then this just completely replaces the interpreter. So we measured this initially a few months ago when we released revency. We have great improvements up to 20 x for simple programs which are very computationally heavy, like Fibonacci in this case, but also up to two x in normal contracts, like weth, for example.
00:06:06.080 - 00:07:03.610, Speaker C: Other ERC 20s also measure about the same. We also tried running this on a node on ref. However, the improvements weren't really that substantial, because also of other things which Roman will talk about later. And so it might not be a very compelling use case for l one execution. But on l two, where maybe you have a lot of system contracts which run another block, a lot of computationally heavy contracts, this could be a substantial improvement. So the EOF, which is the EVM object format, not actually ethereum. I discovered this recently.
00:07:03.610 - 00:08:27.010, Speaker C: I guess it's more part of the EM, so it makes sense. But essentially this is an improvement which will be most likely included in the next artwork. This, I won't go too much into detail, this is a nice graphic, that dragon, which unfortunately could not make it today, made. Essentially it's nice quality of life improvement for compiler developers, but also for nice performance improvement for also our use case and also the interpreter. So in general, how jumps are executed right now, which we call the legacy bytecode, compared to the EOF bytecode, which will happen like it's not live anywhere currently, as far as I know. But essentially how jumps work is that dynamic jumps, all the jumps in the EVM bytecode are dynamic, which means it takes in a stack item and jumps the location that specified by the stack item. This is not great for static analysis tools because you cannot determine the control flow of the program, of the program necessarily.
00:08:27.010 - 00:10:34.356, Speaker C: And also it's not very performant in the compiler use case, so in revency, because you have to generate a massive switch table over all the jump destinations, which has to be validated at runtime. And this obviously inhibits of optimizations and makes the control flow very complicated. Another use case. So essentially what UF does is that it removes dynamic jumps, it makes all jumps static, meaning the jump destination is an offset that's encoded directly right after the instructions. So all tools analyzing the bytecode and also RevMc can make use of this to have more information of the program at compile or deploy time. Another major improvement is that instead of having one big long list of instructions in the bytecode, where you have code sections which essentially are their own simple functions which can call into each other and then return just as a normal program that runs on your cpu, this obviously also helps for compile time, because instead of passing one massive 100,000 lines of code to llvm, you can split it up into multiple functions, just like, just like when compiling a rust program or c program with clang, which helps the optimizer a lot to produce better code and also faster. And in general, mostly thanks to removal of the jump desktop code, because all jumps are static, so the analysis can be done at compile time, you have a lot less bookkeeping, a lot less instructions.
00:10:34.356 - 00:11:24.600, Speaker C: I believe it was like up to 20% less code size on some popular contracts. This means that obviously compiles faster. And also this also helps the interpreter to have to be more performed because there are less instructions to execute. These are just some of the improvements that EOF brings. Won't talk about the instructions, but yeah, that's about it. So on the other side, I've talked about how Revancy helps you achieve more performance with a single EVM. On the other side, we can execute multiple evms in parallel.
00:11:24.600 - 00:13:11.556, Speaker C: So this is, this has been in the talks for a while. For example, since a bunch of recent blockchains have been advertising this, that you can execute the EVM in parallel. The main reason this hasn't been done on EVM recently is because there are a lot of roadblocks to it won't go too much into detail, but essentially the main algorithm that is used and modified is called block STM. All it does is defines a set of read and write sets that are computed once executing all the transactions in parallel. And then if you have any conflicts, you have to re execute them to essentially achieve the right execution because transactions are generally thought of as sequential, but if you execute them in parallel, you may get some conflicts, so you have to somehow fix them. So this has been done in DVM because also of the of the Coinbase problem, where essentially each transaction has to will send a certain value, a certain amount of ETH to the block coinbase, which is a single address. So this essentially makes all transactions depend on each other.
00:13:11.556 - 00:15:09.210, Speaker C: However, this has to be handled separately as a special case, which increases complexity in the whole algorithm. So not going too much into details, but there's a talk later today by the rise people which will talk about the PE VM, which is I believe mostly working EVM parallel EVM implementation using also REVM, which eventually probably make it into RET or anyway can be used as a Reth implementation of the EVM. So we've talked about EVM improvements running EVM in parallel, but one of the other half of the EVM execution is essentially spent in storage and doing I o and reading and writing to the database. So the current, this is more specific to rest, but applies to also all clients. Is that the current state the current databases have a lot of may have like some hacks or not be very optimized for the EVM use case. For example, MDBX is a general purpose database, but we used to use it for all data. We discovered that it's not great use case for EVM because there's a lot of overhead for like fetching data and also there's a lot of hacks.
00:15:09.210 - 00:16:13.380, Speaker C: MdBx in specific is also a big pain for us because it's written in c in one single 25k lines of code file. We have to interact with it with FFI, other generated boundings. Not a great API, and also the fact that it is Mem mapped, which has been discussed a lot recently. There's this great talk. I sure want to use mmap in a database management system which kind of shades on it for 20 pages something. So yeah, so Mmap might have a lot of problems, has a lot of problems with the EVM use case where you have a ton of data that cannot necessarily fit into memory. So that causes a lot of performance issues.
00:16:13.380 - 00:17:11.020, Speaker C: Another use case for changing a database is that current implementations don't use IO uring or other types of async IO. These are future performance improvements which may look into like changing database or writing our own. Another way, instead of replacing the entire database, we could, just as we've done with static files, is that you split the data into multiple databases. For example using a tri db for only for the state and then b three for the rest. Currently we are only using a b three MDBx for most of the data. And the historical state is in static files, which has granted all of performance improvement. And we might look into doing more of this.
00:17:11.020 - 00:17:14.740, Speaker C: And now hand it over to Rohan.
00:17:15.920 - 00:17:53.410, Speaker B: Yeah, let me overtake the spotlight. All right, a little hint for everybody who's been keeping track, we have like three, four gigs mentions. We'll crank it up from here. Four, four. Okay, cool. Additional bonus for a person who will take the live stream and will crop all of the gigacast mentions and do like a counter, you know, like George's will come up with some nice present, I'm sure. Right? And one more thing on the parallel evm highly there is one question of how to actually execute in parallel.
00:17:53.410 - 00:18:37.332, Speaker B: There was another question of how to communicate it trustlessly between your peers. And we highly recommend the blog from our teammate dragon, which is linked right here. You'll probably get the link to the slides after the presentation. And yeah, a little recap. What's keeping us from Giga Gas sequester execution IO, what else? Well, interpreter and the fourth one is state commitment. All right. One thing that we have talked about and has been our biggest grievance is state commitment.
00:18:37.332 - 00:19:59.010, Speaker B: The problem with state commitment, that it is a sorted haxory try and it is very difficult to find new approaches of how to paralyze it. But we did find one recently which is actually MPT is a trie of tries. So technically what you could do is when you've gathered all of your state updates from the block, then first you can compute all of the storage routes in parallel and only then you recompute the state route for the whole account try. And that brought us up like two, three x improvement in the straight route computation. What is it? Yeah, this is what I just mentioned. Another performance improvement for this is caching the intermediate nodes. So when you have long in memory chains, you can preserve the intermediate nodes for the previous blocks that have not been committed to disk yet, and you can reuse them to compute the state route for the, for the, for next blocks.
00:19:59.010 - 00:20:32.990, Speaker B: Some ideas actually. Georges suggested that as you execute the block, you can stream the access list, the state access list to a stateroid computation task which preloads all of the access nodes. And then when you're done with execution. You have gathered the whole access list. Arguably it is faster. We have not like we wanted to experiment with that for a while. We have not had time yet.
00:20:32.990 - 00:20:59.000, Speaker B: And yeah, another approach is just you take the full try and you split the try into subtries and you compute like in parallel the roots for specific path in the tray. Yeah. Images. They lost my meme. What happened there is my meme. Right. Haxory try is the root of all evil.
00:20:59.000 - 00:21:33.970, Speaker B: I thought no laughs in the audience. All right. I thought really hard about this one. Yeah. Another alternative is to get rid of hex retry. I think given how often we bring up the problems that we have with MPT during these talks, you can understand how painful it is for us to interact with it. Yeah, we mentioned already all of the pain points.
00:21:33.970 - 00:22:20.160, Speaker B: Yeah. And the fifth item that is keeping us from doing giga gas or potentially multiple gigs per second is the consensus engine. And this is specific for how we initially implemented the consensus engine. And the initial problem was that on FCU, unfortunately update, when you actually advance the chain, we have always persisted all of the data to database and the commit added like 100, 200 millisecond additional latency. That is actually unnecessary. Yeah. Just quick shout out.
00:22:20.160 - 00:23:03.580, Speaker B: Consensus engine revamp is a joint effort by Matt. I then fed it and die and we'll hopefully release it in production mode really soon. It's a really great improvement. I hope people will like it and will run giga gas chains. Another great improvement with the consensus engine rewrite is that we now expose a more configurable interface because consensus engine is like the main driver of the node. But the only driver that we have built in right now in ret is beacon consensus. But it doesn't have to be only that.
00:23:03.580 - 00:23:42.882, Speaker B: Potentially you can swap it out for PBFT, tendermint, BFT, or whatever the hell you want to use. And yeah, again, they lost the numbers. Here are the numbers. On the left is the FCU latency that is currently on main without the rewrite. On the right with the consensus engine rewrite, just for you to understand, the left one is in milliseconds, the right one is in microseconds. Yeah. Quick round of applause to the team who worked on this.
00:23:42.882 - 00:24:31.992, Speaker B: That's really impressive. Yeah. And another one, like we come up with all of these ideas about performance improvements, but how do we, how do we actually experiment with them? How do we measure them? We did come up in the past with a couple of debug scripts that allow like replayer scripts that allow us rerunning certain segments of Mainnet. But this is, this like, this creates certain bias towards Mainnet data. And this also restricts you to mainnet protocol. And some of the performance improvements for your chain might not be mainnet compatible. Ethereum mainnet compatible.
00:24:31.992 - 00:25:06.080, Speaker B: So yeah, now we have Alphanet, it's going to be deployed in life really soon. And we hope that with Alphanet we can experiment really fast on execution layer innovations and it will just give us a fast iteration loop. Yeah, I came up with a slogan for Alphanet. I did not run it by anybody yet, but deploy, bench, rinse, repeat, there should be a Giga Gaspard somewhere as well. Right, thanks. Questions?
00:25:11.460 - 00:25:19.960, Speaker A: Thank you, Roman and Danny. This is a lot of work and probably the most important work because the SDK will not matter otherwise. We'll start.
00:25:21.900 - 00:25:44.152, Speaker D: Yeah. Thank you, guys. Super interesting work. I'm going to be this guy. I'm going to ask the same question as I did previously. I'm very curious about what do you think the new things that this will open, like what are going to be some innovations that are going to be enabled by having this Giga gas. And I'm also curious about state bloats.
00:25:44.152 - 00:25:51.640, Speaker D: And by the way, you say giga gas twelve times, so you can send me like 21 million btcs and I would be okay with that.
00:25:51.720 - 00:25:54.990, Speaker B: Thanks. Okay. George's will expense it. All right.
00:25:56.010 - 00:25:57.786, Speaker D: Yeah, we got a deal.
00:25:57.858 - 00:26:04.866, Speaker B: Can you formulate the question a little bit more specifically? Not too broad. Yeah.
00:26:04.898 - 00:26:08.710, Speaker D: Why does Giga gas matter? What is it going to enable and stay load?
00:26:11.290 - 00:27:00.420, Speaker B: I'll admit that the state load part, just why the performance of the chain matters. First of all, it's ux. Like, you don't want to just have, have users sitting there and waiting for their transactions to get confirmed for forever. Second of all, the only application for blockchains that we did right now are kind of limited to financial applications. What we would be really excited to see is more gaming applications where the status persisted to chain. Yeah, it just, you cannot have, you cannot support like the next billion users on a one mega gas chain.
00:27:04.720 - 00:27:28.610, Speaker E: Thank you for the talk right here. Quick question on RevMC in regards to the compilation to LlvM irnae, does that change my debugging tool chain? Like, do I have to switch my debugger? What does that look like in terms of that support? And is there a guarantee? What guarantees are there in terms of the assembly code versus the EVM?
00:27:29.390 - 00:28:09.720, Speaker C: Yeah. Right. So essentially it's just a complete replacement interpreter. Do you mean debugging like with foundry or yeah, we could integrate in foundry. The problem is that there are some hooks missing. For example, we internally use the RAVM interpreter in foundry for doing all stuff like cheat codes. Some of that you wouldn't be able to get maybe console logs or something like that because it uses RevM on step functions, hooks.
00:28:09.720 - 00:28:17.012, Speaker C: But essentially most of it should work. Like for example like calls and logs and mostly should work. Yeah, gotcha.
00:28:17.036 - 00:28:25.560, Speaker E: And then in regards to doing those like cheat codes, would that be on the LLVM side or would that, if I wanted to hack on that?
00:28:28.660 - 00:28:50.974, Speaker C: As I said, it should mostly work. We haven't tested it. Essentially all you would have to do is replace the EVM runner with Rev MC or with a ReVMC handler that either jits or loads it from disk. And yeah, it should be like the exact same gas cost, exact same semantics. Yeah.
00:28:51.142 - 00:28:52.010, Speaker E: Thank you.
00:29:09.760 - 00:29:20.660, Speaker C: So for the replacement of storage, there's a bunch of ideas there. So what have been tested already and what were some preliminary results?
00:29:25.050 - 00:30:18.638, Speaker B: Yeah, the thing about replacement storage, I'll say the pinpoint that we had initially, initially we had only one storage type. And the thing that we did not realize as we were migrating to static files is now we have two different storage types. And in the past we relied too much on the atomicity of the database transaction. And whenever the node crashes, it drops, it does not commit anything. And we did not realize. So in my view, that the biggest pain point between having layered databases, layered storage types, is synchronizing them. What we did try is, I think somebody already mentioned there was a question, like we did try parquet.
00:30:18.638 - 00:30:59.860, Speaker B: We tried other for static files specifically for append only data, which tried different file formats and they did not work for the things, for our benchmarks. So we kind of reinvented the wheel and built our own. What we're keen to keep experimenting on our different databases, like for different use cases, like maybe in a year from now you'll see our tree stored in like firewood or some other more appropriate database for the try. Yeah. Thank you.
00:31:00.400 - 00:31:09.000, Speaker A: Thank you, Roman and Danny. All right, so next up we'll have.
