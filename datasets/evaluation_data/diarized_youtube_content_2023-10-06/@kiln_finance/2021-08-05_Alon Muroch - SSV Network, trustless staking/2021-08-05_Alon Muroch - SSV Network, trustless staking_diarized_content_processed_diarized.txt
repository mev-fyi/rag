00:00:04.210 - 00:00:46.974, Speaker A: Other people, but they will flow during presentations. And we're very happy to have you here. Some partners, friends, future colleagues and hackers, or just believers in Ethu and staking. We'll start with Alan and we'll talk about secret shared validators and ssv network. Then followed by Vasili from Lido and finishing by Thomas working on the e two mev integration. So thank you very much for coming tonight. And without further ado, Alan.
00:00:47.102 - 00:01:21.920, Speaker B: Perfect. Should I speak with this one or. Great, everyone can hear me, right? Perfect. So I want to talk with you about secret shared validator. It's a technology we've been working on the past twelve months. It actually started as a research project by the Ethereum foundation and kind of finds its way now to a testnet and moving forward, of course to Mainet. Before we start, how many of you guys staked on a testnet or a mainet in any way? On Ethereum, of course.
00:01:21.920 - 00:01:54.570, Speaker B: Perfect. Okay, how many of you, not via a pool, right, like literally ran an infrastructure or just one? Two. Okay. Three. Perfect. Okay, so I'll try to maybe give a wider background on staking in general and the challenges. So circuit chart validator, as I've mentioned before, is a technology which tries to redefine the infrastructure for Ethereum staking.
00:01:54.570 - 00:02:49.046, Speaker B: And more specifically it will attempt, or hopefully succeed to enable stakers, pools, institutional staking services and so on and so forth. Basically anyone who's running a validator on Ethereum to decentralize and distribute their infrastructure. And that's pretty important. And it uses two main technologies which are pretty well proven, which is threshold signatures and BFT or a consensus protocol. Right. So why is that important? It's really, really important because in the core of the way Ethereum staking was built was the fundamental idea, which is one validator, which is 132 eth validator equals one vote. And that's coupled with two other aspects which are try to attempt as much as possible to avoid economies of scale.
00:02:49.046 - 00:03:40.730, Speaker B: That is, if I have a lot of ether, I have more influence pair of validator than anyone else has. And of course decentralization. Those are really the key things that from the beginning, the designers in the Ethereum foundation really wanted to give. Staking on Ethereum economies of scale is really a big thing, maybe even connected also to mev, which is one of the aspects which are more challenging, let's say. But economies of scale is really the definition of proof of work consensus protocols. That's why you see a lot of the proof of work protocols like bitcoin and even Ethereum today really controlled by a few pools, because you can make more money working as a group than as an individual. And that's exactly the economies of scale we're talking about, which caused centralization.
00:03:40.730 - 00:04:32.234, Speaker B: And it's even more important if you look at the future of staking on Ethereum, we're really running towards the merge. And afterwards withdrawals which will be enabled on the bitcoin chain, the merge specifically will increase significantly the rewards validators can have. And of course that will lead to more validators joining in and so on. As more validators joining in, more services will come online and offer staking. And of course, we have challenges which are fairly new, like mev coming into the space, and have particular challenges in staking in general. All of those things are really showing that the trajectory for Ethereum staking is very rapid growth. I mean, we're seeing it today, we already have 6.4
00:04:32.234 - 00:05:46.254, Speaker B: million eth at stake. That's much more than I think anyone anticipated this fast. And so that's really important, and there's a lot of challenges around, but kind of my focus is the infrastructural challenges, or rather how do you actually run infrastructure which can scale to those types of endpoints or those types of numbers we're talking about? And the infrastructure is basically, how do I run an Ethereum two validator, or Ethereum validator? What do I need to do both in terms of an individual, but also maybe mostly as a business staking business, an institutional business, a pool, or any other configuration. The way they run their infrastructure is paramount to the security of the network. We saw, for example, slashing events in the past. We saw hiccups, quote unquote, where bugs in the implementations caused a lot of nodes to go offline. Those things really influence the end user and the security of the network.
00:05:46.254 - 00:06:35.300, Speaker B: And as of now, the number one factor that influences the way infrastructure is designed and built is the slashing conditions. That's the biggest threat. That's what everyone is trying to avoid. If you are an individual, you want to avoid slashing because you don't want to lose money. If you're a company, you don't want to get slashed because it will influence if your company can be successful or not. And of course your reputation and so on. And so almost all of the different infrastructural designs, from Coinbase, the biggest, or Kraken, the biggest custodial stakers, all the way to the small person running one validator, all of the design decisions are around slashing and which a lot of that is revolving around single point of failures and centralization, and it looks something like that.
00:06:35.300 - 00:07:32.462, Speaker B: And so you have in the middle what is called the validator client, which is basically just a piece of software running and coordinating the execution of duties for one or more validators. Basically going to the bitcoin node and saying what are my duties next epoch? What should I do? And signing accordingly, and not by mistake. The validator client is just one instance. You cannot run more than one instance of that validator client with the same validator keys because you'll risk slashing. And that's exactly a single point of failure. If you imagine a big enterprise like Kraken running thousands and thousands of validators, at the end of the day they're running a few of those validator clients, depending on the capacity of each validator client they distribute them, but every single one of them is a single point of failure. If that validator client crashes, nobody attests anymore and execute transactions.
00:07:32.462 - 00:08:10.626, Speaker B: If the server on which the validator client gets hacked or gets some malicious activity, then somebody can get slashed or have their money harm or stolen. And a lot of that boils down to do's and don'ts with the current infrastructure. So of course don't run a single validator key in multiple instances. You'll get slashed. Running validators in multiple parallel clients, even if some of those clients are offline or dormant or just backup. Don't do that. We saw big, big services getting slashed by accident by doing so.
00:08:10.626 - 00:09:16.226, Speaker B: So the entire idea of trying to build a robust infrastructure with redundancy and all of those things we are used to, that's not something which is advisable. Each validator client needs to maintain its own slashing protection, back it up and persist it. If they fail to do so, they're risking slashing. And of course the ultimate thing is your validator key needs to be online 24/7 which is again a big thing in the crypto space as we are all got used to actually having our keys getting put into cold storages. And I will even go as far as to say that most of the design today really goes against software development. Experience of software development the past 30 years, like anyone who's built anything, always tries to think of how do I do it robustly, how do I separate it into different services? And those services maybe run on multiple devices, on multiple machines to have redundancy back it up, and so on and so forth. A lot of the things we're used to in software development you cannot do with the current infrastructure in staking.
00:09:16.226 - 00:09:54.130, Speaker B: And that's really, really limiting. By the way, as an anecdote, most of the slashing we saw the past seven or eight months since the bitcoin chain was launched, most of them are accidental slashings. I can't really remember any malicious slashing happening. Most of them are just accidental. Most of them are somebody did something which internally benignly, which caused slashing. And that's a major factor. How do we go from here to something more robust? And for me, the answer is SSV.
00:09:54.130 - 00:10:37.982, Speaker B: SSV looks like this. So first of all, graphically you can see there's no more single point of failure. SSV works by taking a validator key, splitting it up into secure shares. And those shares can be operated by independent, trustless operators. And by independent, I mean they can be completely different entities in completely different geolocations, running completely different technology stacks, which as you can imagine, distributes in a very significant way distributes the infrastructure. And by doing so, you gain a lot of value. So first of all, the big one is the validator key doesn't have to be online anymore, the shares do.
00:10:37.982 - 00:11:26.766, Speaker B: The actual full validator key doesn't have to be. And that's a major, major value, I think, in my opinion, we're using threshold signatures for security. That is, if you go back and see all of those different operators, or SSV one, two, three, and so on, each one of them holds a share. But in order to actually sign anything, they need to coordinate together and sign it together with some threshold. And of course, there's no more single point of failure. And we're using a consensus layer to coordinate between those operators to create fault tolerance. And fault tolerance really means that if one of those operators, or more depending on your configuration, is faulty, that is, it's offline, it got hacked, it's malicious, nothing really happens.
00:11:26.766 - 00:12:05.420, Speaker B: The system continues to operate, the system continues securely to execute duties and earn rewards for the validators. And another layer of that is the SSV is built as a network, and so that infrastructure is ready to go. So anyone who's building a new staking service can use that infrastructure and not try to reinvent the wheel. And anyone who already has a staking service can adopt SSV pretty easily to use something which is more robust. And there's a bunch of ways you can configure it. It's very much dynamic in that sense. And so that's really the advancement forward.
00:12:05.420 - 00:13:18.274, Speaker B: The way we try to redefine what infrastructure for staking is going from a single point of failure where you're very much constrained by the protocol. How do you do things to using technology to distribute it and make it a much more robust layer of infrastructure, and also make it in such way that it's reputable in repetition. Go and do other services can build on top of it. They don't need to reinvent the wheel. And so that's SSV in a few words. But what I wanted to do is actually go a step further and kind of try to describe how it actually works in real life, like what will be the experience from both the validator or the staker point of view? But also, on the other hand, how will the operators look like, right? I mean, we have today the operators or the staking services, what will be different from them for them, and what will be different from, for the actual stakers? And so if I have a 32 e validator and I can be a DIY user, I can be a pool, I can be a staking service. Anyone is really running a 32 e validator and it doesn't really matter how those 32 eth got there.
00:13:18.274 - 00:13:46.658, Speaker B: It can be pool, it can be institutional money, it doesn't really matter. The way I will interact with SSV is pretty simple. I will take my validator key, split it into secure shares, and that can happen offline, it can happen in any machine you want. It's off protocol. That is, you can decide how you want to do that as long as you do that correctly. And those shares are assigned to operators. A validator can choose three f plus one operators.
00:13:46.658 - 00:14:15.654, Speaker B: That is, they can choose 4710 and so on operators. And of course, the more operators you have, the more fault tolerance you have built into the setup. And once you assign them those secure shares, that's about it. They start to coordinate between themselves. The nodes pick it up automatically and they start executing the duties for you. You don't have to run any extra hardware, you don't actually have to run anything. You can manage all of that.
00:14:15.654 - 00:15:03.374, Speaker B: All of the registry and so on are smart contracts. So you can manage them as if you're managing something very simple like interaction with a smart contract and offline. And that's about it. And you enjoy distributed infrastructure, and also you can enjoy better performance because we have redundancy built into a protocol, because we have fault tolerance, you could expect higher stability, higher effectiveness, and in the end, higher rewards. And of course, the keys you have are in cold storage. I think that's a pretty big thing. And that's from the validator point of view, from the operator point of view, the company or the person actually running the SSV code, they get access to a much wider network.
00:15:03.374 - 00:15:55.906, Speaker B: Right. If you have 4710 operators per validator, you get a chance to be selected by those users. If you do a good job, if you build reputation and so on in the network, you can get selected. The more you get selected, obviously, the more revenue you have as an operator. And that's a really interesting thing. It taps into a much larger audience than you would have otherwise. And I think the big thing for the operator, if we mentioned before that slashing is by far the biggest consideration to anything we do in staking on Ethereum, then using SSV reduces significantly the operational cost and risk for anyone who's running infrastructure because you're working with other people as a team, excuse my French, literally, if you fuck up, nothing really happens because you're working as a committee.
00:15:55.906 - 00:16:34.066, Speaker B: If you go offline for some reason, then nothing really happens because you're working as a committee. And I will even go as to say that I would definitely imagine times where one of the operators communicates to the community saying, you know what, tomorrow at 04:00 p.m. I'll have 30 minutes downtime for maintenance. And again, nothing happens. Like it continues to execute duties because we have the committee, because we have the coordination via the consensus layer. And I think that's a big, big upside. So you reduce significantly your risk and cost and you increase the amount of revenue you get.
00:16:34.066 - 00:17:07.562, Speaker B: And that's a really interesting and smart business model. And so all of those things really, in my opinion, are kind of infrastructure 2.0 for Ethereum, and it's all wrapped up in a Dow, which does a few things, one of which is really coordinates a curated list of those operators. Right. The best operators. We want to be able to communicate who they are, why they are one of the best, and so on and so forth. So a curated list controlled by a DAO really enables that.
00:17:07.562 - 00:17:52.250, Speaker B: And of course you can extend that list. I mean, that list is controlled by a dow. But anyone can build their own list. So it's more about the infrastructure and less about which operators you choose because that really depends on the configuration. Like you can have maybe something as big a corporation running 100 different operators on different continents and so on and so forth, and they manage their own curated list of operators. But at the end of the day, if they are connected to one network, that's where the magic happens and that's where the network effect happens. And of course, continuing with protocol and governance and grants and so on, to continue development for SSV, because it's very much an ongoing project.
00:17:52.250 - 00:18:48.282, Speaker B: If more innovation comes from Ethereum, quote unquote Ethereum two, then SSV will have to adopt stuff like proof of custody and sharding and all of those things. We'll have to adopt the protocol to do that. And so development is definitely continuous in that regard. And I'm happy to say that all of those things sound great, but in the crypto space there's a lot of times a big difference between ideas and dreams and reality. And so I'm very happy to say that we're actually hitting a testnet, a real live testnet, without everything I've talked about, being an operator, being a validator, choosing operators, running validators and so on. We gathered a great group of partners which will help us bootstrap the testnet that's actually happening this week. And in the next probably two weeks we'll probably announce it first.
00:18:48.282 - 00:19:39.774, Speaker B: We want to see everything is working well, but we'll announce it in two weeks. So this is very much something that is happening. It's not a pipe dream. It's not something which will happen in a few years. It's definitely happening, and I think it's definitely the right timing. If we're talking about the merge happening towards end of year Q one of next year, I think the merge, if it symbolizes a really rapid growth in staking, we should definitely rethink the way we do infrastructure, because if not, I think we'll see a lot more accidents happening down the road. In order to actually merge successfully and continue with ethereum scaling, going towards data sharding and so on, we need good infrastructure, and it starts with choosing the right technology to do so and so.
00:19:39.774 - 00:19:41.840, Speaker B: Yeah. Thank you. If we have any questions.
00:19:51.970 - 00:20:05.778, Speaker C: Yeah, I just want to talk one of the main kind of risks and challenges that you see in this approach. So we looked a lot on the positive side. But what you as an author see on the possible downside.
00:20:05.874 - 00:20:35.310, Speaker B: Yeah, I think number one is complexity, because we are adding complexity to the system. So the validator client today is pretty simple. Most of the heavy lifting goes into the bico node. By design. What will happen is that the SSV node will sit between some kind of, let's say modified validator client, which does signatures, and the bico node. And so you're adding complexity. Complexity can add latency, complexity can add issues and so on and so forth.
00:20:35.310 - 00:22:20.450, Speaker B: And so I definitely see that as a challenge. It's actually not just complexity, it's actually nonlinear complexity, because as we go further with Ethereum, two things are getting more complicated than previously. And so if we're looking at, let's say one or two years in the future, so the biggest challenges you'll have, which are challenging everywhere, but it's specifically challenging if you're adding another piece of software, is the merge with its additional duties, and then you'll have data sharing withdrawals mev and if we go even a bit further, you'll have proof of custody, which again adds another thing, another job that the validator needs to do and so on and so forth. And so although Ethereum two by design needs to be NPC compatible, it wasn't really tested like proof of custody was designed to be NPC compatible. Will it hold 50 operators? I don't know, probably will hold seven or 1050, maybe not, because the algorithm is pretty heavy and so it's going to be tricky, but I definitely think it's worth it because the opposite of that will be okay, let's not do that. And I think what will happen is that you'll see more centralization and I think even we are one of those operators helping running the network, that's challenging. If something goes wrong, best case scenario, offline and worst case scenario is even worse.
00:22:20.450 - 00:22:31.940, Speaker B: And so we need to do something about it. I think SSV adds complexity, but it's as simple as possibly we can do that, we can make it.
00:22:34.450 - 00:22:43.522, Speaker C: And what about session database? As far as I understand, it should be synchronized between different validators that run no.
00:22:43.576 - 00:23:25.074, Speaker B: So each operator holds its own slashing database. What happens is that the operators only actually sign something which is applicable to the beacon chain only when they pass a commit round for the consensus protocol. And so literally after they agree to sign something, they actually sign it. Any operator goes online or offline and so on, they need to sync with the other operators. And so the slashing database is really local. And you trust the consensus protocol that if somebody signed something then at least two thirds of the operators agree to it. And if nothing was signed then no two thirds of operators agree to it.
00:23:25.112 - 00:23:26.580, Speaker C: Yeah, thank you.
00:23:27.190 - 00:23:32.546, Speaker A: But then the slashing database will have the log of all validators of other providers as well.
00:23:32.648 - 00:24:09.520, Speaker B: Not necessarily. So we actually adopted the networking topology of Ethereum. So you'll have subnets and an operator which, for example, let's say you have four validators. If I'm as an operator assigned to validators one and two, I necessarily need to listen to three and four, though you could listen to three and four, and also you can compact the actual signatures. You don't have to hold all of the network messages. And so what I will imagine will happen is that you will have operators or nodes basically, which gather all the information. So for example, we are running an explorer node which basically taps into all of the network and registers everything that happens.
00:24:09.520 - 00:24:23.060, Speaker B: So that kind of node will be heavier because it needs to register everything, but the other nodes only listen to what they need to listen. And so that's really as heavy as you make it.
00:24:26.950 - 00:24:30.490, Speaker A: Which validator client do you support, and is there any you don't?
00:24:30.590 - 00:25:04.674, Speaker B: So currently we don't support any of the validator clients. That's like the next project, the bitcoin nodes. We support Lighthouse, Prism and teco. For the moment, I think Nimbus is joining the testnet, so we'll start supporting them as well because they're using the standard API. The next big thing is to actually integrate natively with the different validator clients. And so you could literally take, let's say a prism beacon node and a prism validator put SSD in between and they talk with one another natively. You don't have to do anything.
00:25:04.674 - 00:25:09.620, Speaker B: Currently the validator side doesn't exist yet. It's something we need to still develop.
00:25:11.430 - 00:25:12.500, Speaker A: Okay, thanks.
00:25:14.730 - 00:25:15.480, Speaker B: See.
00:25:18.650 - 00:25:23.218, Speaker C: What'S the latency increase in ssv compared to regular validation?
00:25:23.394 - 00:25:50.542, Speaker B: So I think like in four, seven or ten operators it's kind of the same. We're talking about half a second, a second, something like that to get to a consensus. Of course, if you have. So the consensus really modeled after all of the BFT consensus. But generally speaking you have to have two thirds online in order to sign anything. So if you have two thirds online it will take you about a second. If you have less of that, then they wouldn't be able to sign.
00:25:50.542 - 00:26:34.938, Speaker B: And so it will take more time. We haven't tested larger groups just yet. I actually don't see an application which will require more than ten. I think like ten is really like if you're Lido and you say, you know what, we're managing 600,000 e validators choose ten, but other than that it's very heavy. Latency is 1 second and it should stay there. We'll run stress tests after we launch the Testnet and we'll publish it. The previous testnet we ran was pretty lightweight, so I wouldn't take it as benchmark, but it had like 98 or 99% zero inclusion rates.
00:26:34.938 - 00:26:46.674, Speaker B: And the one or 2% is because everyone used the same node and the node was crappy and so on. So we'll benchmark it. Definitely. I would imagine it will stay half a second a second.
00:26:46.872 - 00:26:49.486, Speaker C: That's geographically distributed.
00:26:49.678 - 00:27:05.654, Speaker B: Currently I'm talking half a second. It's geographically distributed. So our setup now is four continents, each node on one continent, and it takes half a second a second. If you have us in local, it will take nothing. It will take much less.
00:27:05.772 - 00:27:35.726, Speaker C: Yeah. Thank you. Yeah, I have one more question, is, like, how much data? So, for instance, right now, the beacon chain, the blocks are relatively lightweight. They have essentially nothing there. Right after the merge, the block will get pretty beefy. So will it affect in any way the data that gets transferred between the validator and between these nodes that needs to get consensus?
00:27:35.758 - 00:28:01.260, Speaker B: Yeah, that's actually a good question. It could. I don't think it will take a long time. Again, because if you look at the current network distribution for Ethereum. So how long it takes a block to reach 25%, 50%, 75% of all the nodes. You're talking like, really fast. The blocks themselves are not that big, and I wouldn't imagine they will increase it.
00:28:01.260 - 00:28:31.278, Speaker B: It will definitely add something, because now you're just broadcasting nothing. It will increase that. We'll have to test it out. I don't know yet. I would say that that's a question that always pops. The reference I'm taking is that it's Cosmos, really, because it's the closest one I can get. Cosmos has around 100 operators, validators.
00:28:31.278 - 00:28:46.726, Speaker B: It has the basic same structure for consensus, three rounds and so on. And it manages in 2 seconds or so. And I think the blocks are bigger there. I think there are half a megabyte there. Where? In ethereum. It's probably half of that. I think.
00:28:46.726 - 00:28:57.580, Speaker B: Correct me if I'm wrong. And so the deadline is 4 seconds. Like, if it's more than 4 seconds, we're doomed. I wouldn't imagine it will go there.
00:29:00.780 - 00:29:18.796, Speaker C: One more question. You said that one of the huge advantage of protocol is that you can keep validators key in a cold storage. But for staking pool, maybe it's more interesting to have like, distributed generation ceremony.
00:29:18.988 - 00:29:45.416, Speaker B: Do you support this DKG or the way you generate. DKG stands for distributed key generation. Key generation is actually out of protocol detail. So as long as you generate a valid BLS key, that's fine. DKG comes above the actual protocol. And so DKG is even going a step further. Right.
00:29:45.416 - 00:30:17.570, Speaker B: It's like nobody has the key and you need some kind of a consensus layer to do. So to actually sign something. I would imagine that the SSV DaO should give grants for developing it. So you have a bunch of DKG flavors. And I would imagine it needs to propose some kind of a grant and say, you know what? If somebody wants to develop it, we're willing to finance it. I think it's important. Yeah, definitely.
00:30:17.570 - 00:30:29.410, Speaker B: Anyone else ready? Thank you.
