00:00:07.400 - 00:00:16.234, Speaker A: Hello, everyone, and welcome to a new episode of the Kiln risk taking rendezvous. Today we have Wanli from Brevist. Wanly, how are you doing today?
00:00:16.734 - 00:00:18.754, Speaker B: Pretty good. How about you, Edgar?
00:00:19.454 - 00:00:22.634, Speaker A: I'm doing great. I'm doing great. Where are you based off?
00:00:23.014 - 00:00:30.034, Speaker B: Yeah, I'm traveling a lot, so I was in Hong Kong for a few days, but traveling across different time zones.
00:00:30.934 - 00:00:45.874, Speaker A: Great, perfect. So to start this off, maybe you can just introduce yourself and give us your background and how you ended up working in this space, which is the crazy blockchain space.
00:00:46.774 - 00:01:19.422, Speaker B: Sounds good. Yeah, definitely. This is Wang Li. I'm head of ecosystem at Revis Network. So before breakfast, I was working on the interoperability side of blockchain. So crushing bridging solutions and for asset bridging as well as message passing. So the story of how I ended up working in blockchain is that I used to work in the video game companies, and it's like the traditional web two video game industry.
00:01:19.422 - 00:01:43.714, Speaker B: So I noticed there is an opportunity within blockchain that's offering a gamify opportunity that's a very good fit to my past experience. So I joined that gamify project and gradually developed my career into the deeper technology side of blockchain.
00:01:45.274 - 00:02:23.544, Speaker A: Great, perfect. So today we're going to be talking about many things. We're going to be talking about the previous CK processor, talking about the Brevis code chain AV's. But the thing that is, I would say, like the denominator basically, like, is the concept of on chain data. Could you introduce a bit, like, what is on chain data? What's the current state of on chain data? And what are the problems that Brevis is trying to fix when it comes to on chain data?
00:02:23.844 - 00:03:25.898, Speaker B: Yeah, definitely. So we all know that there has been a long history of blockchain right now. So for Ethereum, there's a lot of public data. There's a wealth of data, including user transactions, connections between different addresses, users interactions with different smart contracts, or different decentralized applications. So that data is public on chain. But for smart contract to basically utilize those data and build features that's reliant on the data, or even dynamic data input, that's simply not feasible right now on chain because it's very costly and there is a huge latency when smart contracts trying to do that. The reason is that, for example, for Ethereum, it can only access the most recent 256 block hashes within smart contracts.
00:03:25.898 - 00:04:56.996, Speaker B: Meaning that if a smart contract is trying to retrieve data, um, for example, a user's past transactions before the most recent 256 blocks, then it's very costly to do, especially when you want to iterate over like every user transaction that matches certain criteria. Let's say you want to build a vip loyalty program within your decentralized exchange and you want to basically compute over the user's past transactions, come up with a sum of all of the transactions and see what vip tier the user fits into. So that would include basically looking at the entire history of the user's past transactions, which could include maybe like ten as 1010s of thousands of block hashes or blocks in the past. So definitely well beyond the most recent 256 blocks. So you are going to basically retrieve the values from each transaction to come up with a sum aggregated from each transaction value. And all of these, if you are trying to do it in a trustless way in the smart contract, will be very costly to do. Actually, if you are trying to basically just determine the vip user status of one address, it would cost around $40,000 for gas and maybe like 4000 hours of compute time.
00:04:56.996 - 00:06:29.846, Speaker B: So this is just not feasible for dapps to implement. So what we are trying to solve is trying to figure out a way for decentralized applications to be able to utilize that wealth of data so that they can build very expressive, very data rich applications or features, while at the same time not to compromise the decentralization of the l one, namely Ethereum in our case. So that's why Brevis was initiated. And we also feel like the necessity of utilizing data on chain is very important because if you look at traditional web two applications like centralized exchanges or gamified projects, you'll notice actually personal data or user data is utilized to a great extent to help optimize user experience and help retention. So for example, in centralized exchanges there are existing vip loyalty programs, usually encouraging a user to stay longer with a platform, to trade more so that they can benefit from trading fee discounts or rebates. And for video game projects in web two, so in video game titles, usually there's a concept called live operations. So every decision a gamer, a player makes in the game will be factored in in their future gaming experience.
00:06:29.846 - 00:07:18.166, Speaker B: So depending on the user's footprint within the game, they will have a very unique personalized ux associated with their preferences and personalities. So this is how data can help in an application to be really expressive, to be personalized and to cater to different needs and preferences of their users. So I think our, we at Brevis think for web3 dapps to basically be competitive against their web two counterparts. Data is very important in the race. So yeah, that's kind of an overview of on chain data. Why is it important? And also the current bottleneck of how data can be utilized directly in smart contracts.
00:07:18.270 - 00:08:01.944, Speaker A: And the question that comes up for me is the blockchain data, historical data, at least I feel like is already being used, but is not being used as it should be in a more decentralized way. Because for example, a long time ago when I claimed my Uniswap airdrop, for example, because I used Uniswap probably more than 256 blocks from the snapshot, how did Uniswap use blockchain data? Did they use complementary use cases that, that are a bit like web two? What type of trade offs did they have to make?
00:08:02.404 - 00:08:59.002, Speaker B: Yeah, so there are existing data reliant features that Dapps have integrated with in their user experience, but how they have achieved that is through very centralized ways, I would say. But that's kind of like a trade off they have to face if they want to run such programs. So how they're doing that is, for example, they'll need to retrieve the data, um, through an RPC. So they are basically um, having some trust assumptions on uh, utilizing an RPC node. Or alternatively they might be using some centralized indexer or at least indexers with a different set of trust assumptions, maybe a different validator set compared to um, the decentralization premise of Ethereum. So that's how they are retrieving data. And on top of that, since um, data retrieval is just part of the story for utilizing on chain data, there's also another side of computation.
00:08:59.002 - 00:10:03.964, Speaker B: You want to build very expressive logic to determine how, for example, the rewards need to be distributed. And you want to avoid civil attacks or avoid bots to take the biggest chunk of your rewards. So there's a lot of expressive logic that needs to be implemented in the feature in the airdrops. So that computation is also hard to do directly in smart contracts. Or at least the execution part is very, very costly, like what I mentioned. So how they are doing it is most likely hosting the logic in a centralized way in their local server and executing the logic to come up with a result and sending it back on chain for the air job. So that's like a um, a common way for Dapps to kind of trying to utilize data, but in a centralized way because um, there was no solution that could help them to both utilize data but also keep the trustlessness.
00:10:05.824 - 00:10:31.264, Speaker A: Super interesting. So maybe this is uh, uh, a bad news for all of the airdrop farmers out there trying to stable attack every single of the app, but it's definitely very interesting. So can you tell us a bit about how Bravis has been operating so far? How does basically brevis work and how it will integrate with eigenair?
00:10:31.804 - 00:11:21.092, Speaker B: Got it. Yeah, that's a very important question. So how Brevis works is that it essentially allows smart contracts to read into the historical on chain data, the full history of blockchain, essentially, and at the same time allow developers or Dapps to build very expressive logic using ZK computation. So there are three simple steps that a Dapp integrates with brevis. So basically brevis is more DApp oriented instead of end user oriented. We work with Dapps or developers to help them build data driven features. So essentially what they're doing is firstly, they need to specify the type of data needed for their computation.
00:11:21.092 - 00:12:21.894, Speaker B: So let's use the example of the vip loyalty program. So let's say Uniswap wanna build a loyalty program to encourage user retention. And also the more a user trades, the better discounts in trading fees they'll enjoy in the future trades. So how they can do that is simply specify through brefis the type of computation data which might include user transactions in the past, or maybe like pool operations or liquidity operations across Uniswap pools. So those needs to be specified in the data access module through brefis. And the second step is so Uniswap probably would have some idea of how they want to build such a vip loyalty program. There needs to be ways to encourage users and there needs to be ways to prevent civil attacks, for example.
00:12:21.894 - 00:13:58.068, Speaker B: And how they can do that is simply use our SDK written in go to compile their existing logic into Zk circuits. So Zk circuits, zero knowledge circuits, are the most important component in generating zero knowledge proofs that can be verified directly on chain. So through the compilation they'll be able to basically execute their existing logic or their designed calculation logic in a zero knowledge verifiable way. So basically now through the first step we have the data specified or retrievable, and through the next, the second step which is the compilation, we have the calculation or computation Zk verifiable. And once these twos are combined, we'll be able to generate a zero knowledge proof to submit on chain and to prove that firstly the data retrieved from onchain is, um, done correctly, and secondly, the computation over the data retrieved of the user transactions or um, of the liquidity operations is also done correctly. So um, these two will prove that, um, for a user status, it is calculated correctly. And smart contracts like Revis contracts will be able to verify the zero knowledge proof and it's going to submit the proof results, whether a user is a vip user or not, through a callback function to the Uniswap contract.
00:13:58.068 - 00:15:12.100, Speaker B: So that the Uniswap contract can then give some fee discounts to the user or other types of benefits. So that's a developer overview of how Brevis works. But essentially this would enable a lot of different scenarios other than vip loyalty programs. So basically for reputation projects like trusted labs we have already worked with, they have implemented zero knowledge proofs using brevis into their reputation project, their proof of value or proof of humanity to offer DS. Basically a way to tell if a user can be of high value to their protocol, or whether a user address is indeed a human user address instead of a bot. So they have a different set of criteria, including how much interactions an address has over a number of protocols, and whether a user established their account from a long time ago. So basically the history of the account and also how frequent transactions are done within the address.
00:15:12.100 - 00:15:44.784, Speaker B: So there's a very expressive set of criteria they're using. So for reputation projects, they basically can use grabis to retrieve data associated with the type of computation they need, the set of criteria, and also they compile their criteria into ZK circuits using gravis to basically achieve zero knowledge reputation. So this is how Brevis works, but we are also exploring different new use cases with developers in the ecosystem.
00:15:46.804 - 00:16:20.906, Speaker A: This is great, this is great. I think this is going to bring a lot more efficiency on the on chain data space. And so my follow up question is, because the thing that you described, I think the ZK cocoa processor and gravis is something that existed prior to Eigenir. Correct. So my question is, what's the relationship with Brevis and eigeneir? How did both teams met and when did you, Brevis decided that you were going to build an AV's?
00:16:21.090 - 00:17:45.712, Speaker B: Got it, yeah, sorry, I missed that portion of the question. So how we decided to work with Eigen layer to launch as an AV's is actually a very comprehensive decision based on our conversations with developers. Building on top of graphis, our previous zero knowledge coprocessor definitely is able to enable a lot of data expressive features. But Dapps also, certain developers also share feedback that for unchained protocols to utilize zero knowledge proofs, there are still some kind of CKP, zero knowledge proof generation and verification cost associated with using that. And at the same time there is latency associated with proof generation and verification. So although that's already very minimal compared to executing everything directly in smart contracts on chain, but optimization is definitely needed. So we are thinking of a way to combine maybe proof of stake as well as zero knowledge proof so that we can allow developers to configure whether for some small value latency insensitive requests they have.
00:17:45.712 - 00:19:17.898, Speaker B: They can go through the proof of stake first, but they will be able to use the result to start a challenge period that they define. It can be 1 hour, 2 hours or longer or shorter. And during that challenge time, any challenger, ZK challenger within the Brevis network will be able to still use the previously mentioned ZK coprocessor to generate ZQ results for the proposal generated by the proof of stake network, and then use that as like a challenge to verify whether the result was generated correctly. But there's economic incentives to prevent challengers to randomly initiate a challenge because that's going to extend how long the output is going to be finalized on chain. So there's also economic design to prevent the proof of stake chain from generating faulty results because they are going to be slashed if a challenge is successful. So this is very similar to the optimistic roll up design that we're seeing in various Ethereum Altus, and we are basically learning from that experience. But essentially when we are trying to see what type of proof of stake we can incorporate in this new cold chain model, that's the name we have for this new model, and we figured out Eigen layers.
00:19:17.898 - 00:19:35.694, Speaker B: ABS can be a good choice because of restaking, because of dual quorum. Basically we can tap into Ethereum security through Eigen layer without worrying too much about how the economic side would work using our own token.
00:19:37.194 - 00:20:04.794, Speaker A: Great, you'll be tapping into Ethereum economic security. And now, like against, I think it's very, very huge. I think it's like $15 billion. A lot of people say that's a lot of money question to an AV's builder like you would be, how much economic situation do you think you need and do you think you will need in the future?
00:20:05.334 - 00:20:47.508, Speaker B: Yeah, that's a good question. So Bravis has been on mainnet even before we launch our Abs. So we have already been working with a lot of projects. So we have been receiving ZK proof generation requests. So economically, definitely what we'll need to make sure we'll be guarding our zero knowledge proof generation and verification in a solid way. So starting from day one of our AV's launch, we have secured $1.6 billion worth of restaking commitments from major LRT solutions, including that rock, Puffer, Ranzo and Suo.
00:20:47.508 - 00:21:05.014, Speaker B: So that's kind of the landscape right now for how we are secured economically. But whether or not we should be expanding definitely depends on the feedback we receive from developers building on top of revisiting how they think it will make sense.
00:21:06.434 - 00:21:42.974, Speaker A: And do you think that those developers, they might ask for specific type of security? We know that in eigenaire there is shared security, which is port security, meaning that ETH can be restricted to secure a multiple of AVss. But an AV's can also ask for attributed security, meaning that it will be, for example like 100 e that will be only attributed and slashable for this specific alias. Do you think that this is something that developers building on Bravis will value and will ask?
00:21:43.474 - 00:22:51.936, Speaker B: Yeah, that's a good question. Because we are offering a unified solution to all developers building on top of Bravis, we'll need to factor in what's the majority of the cases that would require this. So because the coaching model rav's is basically an addition layer on top of our coprocessor. So the zero knowledge proofs are still our baseline security, even when requests are routed through the proof of stake chain that ABS to generate. So we would say from that standpoint, it's pretty flexible for us to still go with a shared model because that's also the easiest way to attract stakers, restakers as well as operators to join us. And also on the other side, it's going to be costly if you're trying to maintain a dedicated liquidity of staked ETH or RRT ETH to your Abs. So the cost aspect will be then split across all users or all developers building on top of Roberts.
00:22:51.936 - 00:23:24.184, Speaker B: So I think the reason why they might want to opt in to the coaching model instead of the pure ZK model is already factoring in the lower cost, the lower latency. So definitely it would make more sense at the current stage for them to prioritize the cost aspect while they are opting in the coaching model. So the shared security or the shared economic design would be what we're aiming at right now.
00:23:26.884 - 00:24:04.784, Speaker A: Great. And so now that the various AV's is on the Eigen layer Mainnet, you guys were one of the first avss to be live and Ken is supporting Revis, of course. My question is what are the next steps? Are we going to see some demand from Dapps to get historical data on Ethereum. I know you guys plan to be multi chain in the future. Can you share a bit what your roadmap looks like in terms of.
00:24:04.824 - 00:25:24.994, Speaker B: Yeah, definitely before our AV's launch, Brevis is already, has already been mainnet ready for quite some time. Brevis has been developed for over one year and a half now, so there are already live use cases on Brevis. So the example of trust labs zero knowledge reputation is one of them. We launched a campaign with trusta to basically allow users to verify through trusta, but underlyingly using brevis zero knowledge proofs that they are of that high value to Dapps or DeFi protocols based on their past transactions or interactions across different DeFi protocols. So that campaign is launched for Linea and there are protocols that are trying to access trust us proof of value so that they can launch features or programs to attract high value users to those DeFi protocols. So there's different layers of integrations in the flow that essentially zero knowledge proofs is what's being generated every time when a user wants to prove to a DeFi protocol that they can be of high value to those protocols. And apart from that, we are also actively working with a Uniswap foundation to basically work for their v four pools.
00:25:24.994 - 00:26:38.144, Speaker B: In the Uniswap v four design, there is a concept called Hook, which allows pool developers to inject customizable logic before or after certain transaction or before or after certain liquidity operations. So that allows developers pool deployers to basically add a lot of data driven features in the entire flow of their user experience. So we have already launched a demo for Uniswap V four to show that a vip program can be implemented in hooks utilizing a user's past transactions. And we're also going to give a guest lecture in early May in the Uniswap Hook academy. I think that's being coordinated by atrium. So we're going to be one of the lecturers sharing how Brevis can be utilized in building hooks. So I think with a lot of tractions in that, definitely there is going to be increasing demand for Brabus usage, which also means that a lot of usage for AV's contribution.
00:26:39.804 - 00:27:18.496, Speaker A: That's great. And so we talked about, again, layout being basically the marketplace for economic security. Brevis is a beneficiary of that economic security. We have an audience mainly of restakers, and one of their biggest questions is how am I incentivized to contribute to the security of Bravis? So you talked a bit about the usage that we will see very soon on brevis. How does that usage translate to revenue? Or how does Brevis plan to incentivize restakers?
00:27:18.680 - 00:28:33.224, Speaker B: Yeah, that's a very important question. We're getting a lot from users, developers, and also the different ABS ecosystem partners like operators or LRT solutions. So, yeah, so definitely there are different layers of incentives for users or developers building on top of revis, and I think it comes mainly in two parts. The first portion is definitely the co processing fees that's going to be distributed across or all participants within revis. So that would include operators and also include DApps stakers or restakers in the AV's. So apart from the co processing fees which are basically paid by the end users, or the protocols building on top of Brevis, we are also launching a point system which will align the interests across different ecosystem partners within the icon layer ABS or the Brevis ecosystem. So the point system would mostly cover incentives for several factors.
00:28:33.224 - 00:29:18.424, Speaker B: One is the staked value of a user and also the duration of their staked value within the Brevis abs. That's definitely one of the major incentive we're having. But on top of that, we are also trying to incentivize early builders and users of bravis zero knowledge proofs. So any developers or Dapps that have integrated with brevis generating usage through brevis would be able to receive points. But the detailed design for our economic incentives are still underway and we're looking forward to launching the program within the next month or so.
00:29:19.924 - 00:30:03.274, Speaker A: Great. I think you answered all of my questions around incentives. That's great. So I have questions around how is it to be an operator right now? Like, I would say that previous is in, I would say alpha maintenance stage, meaning that you have a lot of operators, but they're not getting to work at the moment. Can you tell us a bit about how it would look like to run a Bravis? Maybe. Also we're entering a bull market, so we can expect high demands from Dapps. Can you talk a bit about, like, how heavy or light the Brevis aV's can be?
00:30:03.854 - 00:31:05.716, Speaker B: Yeah, so I think this will be a dynamic requirement going forward based on the usage of brevis. But right now, based on our observation of the partners, currently we work with generating zero knowledge proofs through Brevis. The requirement on the hardware side is actually very minimal. So if you go to our documentation, go to our GitHub page for running an operator, you'll notice our recommended hardware configuration right now is AWS C five, c six and two times large equivalent and 500 gb disk. So it's not really a lot compared to what you would assume. So we hope that this lowers the barrier, the bar, I mean. Yeah, most likely the entry barrier for operators to join us to basically start the Brevis Abs with a solid foundation.
00:31:05.716 - 00:31:11.184, Speaker B: So enough validators, enough operators, great.
00:31:11.484 - 00:31:31.164, Speaker A: So do you plan to have a permissionless set, a brand area, have like 66 operators on eigen layer? Do you plan to potentially expand that? And we're also seeing like some avss wanting to cap their active set. Like, what's your point of view on this?
00:31:31.504 - 00:32:16.684, Speaker B: Yeah, I think gradually in the long term, definitely permissionlessness is key to any decentralized project. But since we're still, I would say early stage on Mainnet. So how to make it the most efficient to coordinate across different operators? Factoring in that there might be still updates where upgrades of our AV's system, of our economic design would be an important consideration. So efficiency or iterations to make our ABS system better is our focus right now. And any decision design around how operators would be incorporated into the system would be surrounding that consideration.
00:32:18.544 - 00:32:52.904, Speaker A: Great. And I guess some closing questions about restating as a whole. What is your long term vision for the eigen layer ecosystem? We've seen a lot of diversity in the application in the AVss being built on top of Eigen layer. Do you have a prediction as to where this diversity will stop? What do you think the eigen layer ecosystem or restaking as a whole will look like in two years, for example?
00:32:53.564 - 00:34:11.276, Speaker B: Yeah, so I think this is still a very new vertical, but we are seeing very fast iterations of how the ecosystem partners work with each other. So there can be competition to eigen layer, but they are so early in the game and we are already seeing so much attention and participation in their ecosystem. So I think it's kind of hard for us to in the short timeline, see another strong risk taking ecosystem like eigenvalue, because they basically have so many operators already onboarded and they have solid use cases with these early launched ABS's. So they also have LRT solutions that are building on top of eigen that has attracted so much liquidity. So I think it's hard to match that level of basically support that Eigen layer has received so far in a short timeline. And also, I'm personally also seeing a lot of iterations in the models of how ecosystem projects work with each other. So for example, there is an LRT solution called Swell.
00:34:11.276 - 00:34:56.464, Speaker B: They have been working on launching a new SVeL l two, aiming to serve the AV's ecosystem to basically allow avss within the Eigen ecosystem to also benefit from their LRT solution. But at the same time both use cases through their l two. So I think those attempts would further strengthen eigens ecosystem because they're basically building new use cases through these peripheral tech stacks. Yeah, that's kind of like my observation, but I think everything changes really fast, so I'll be looking forward to what could happen maybe in even two months time.
00:34:57.404 - 00:35:46.374, Speaker A: Yeah, it will be very interesting to see how all of this plays out. I'm also thinking about restaking ecosystem outside of Ethereum, we've seen some projects building on principle like bitcoin staking, where it's basically the same logic as I can I you basically collateralize some locked bitcoin to secure other protocols. So I think it's also interesting to see some diversity, not only in the AV's space, but also in the type of asset that can be restaked. My closing question is what do you think would be like the craziest possible alias that someone could build? Like how far can this innovation go?
00:35:47.314 - 00:36:57.060, Speaker B: I think this is like layers of layers of security on top of each other. So the craziest one from my perspective, might be another restaked token layer or restake security layer on top the current LRT, but both within the Eigen ecosystem as an AV's, maybe. So that could be some wild idea. But I think essentially Eigen is not like, I wouldn't say Eigen is renovating the use cases that we are already seeing outside of the restaking ecosystem. So for a b assets that have already launched on Eigen, I believe the use cases, most of their use cases have already existed even before the idea of restaking. So for example, if you look at the first batch of launch projects, Outlayer is a roll up as a service. But the idea of roll up as a service has long been in the market with different solutions that I shouldn't be naming right now because of our close partnership with a layer.
00:36:57.060 - 00:37:32.824, Speaker B: But yeah, so I think the use cases were the scenarios of these AVss are solid because of the deep rooted defi ties they have. But yeah, in the future there might be some innovations for even more user facing verticals. I would be interested in seeing maybe a gamified projects building as an Abs and see what use cases they might have with Eigen. So yeah, there can be a lot of opportunities there.
00:37:33.744 - 00:37:53.264, Speaker A: Very, very interesting insights there one? Yeah, I think I could talk to you for hours and we already passed the book time. Where can we send the audience to learn more about Bravis? Maybe we have some developers in the audience wanting to know how they can be using your ZK processor.
00:37:53.424 - 00:38:33.604, Speaker B: Yeah definitely. So feel free to follow us on Twitter. I believe our Twitter is breadvis zk. But yeah, we'll be linking our Twitter maybe on the YouTube link here. Yeah so definitely feel free to follow us on our updates and also our announcements on Twitter and feel free to join us in Telegram. We have a very active community where there can be ZK developers or users or just general folks who are interested in zero knowledge proofs.
00:38:34.264 - 00:38:43.400, Speaker A: Great Wendy, thank you so much for coming on the podcast. I think it was a great episode and to the audience we'll see you guys on the next one. Bye bye.
00:38:43.432 - 00:38:44.824, Speaker B: Thank you Ecker. Bye.
