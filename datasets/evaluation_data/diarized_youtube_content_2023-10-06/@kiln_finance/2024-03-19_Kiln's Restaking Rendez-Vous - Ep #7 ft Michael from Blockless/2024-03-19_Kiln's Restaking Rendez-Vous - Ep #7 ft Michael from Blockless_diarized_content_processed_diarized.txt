00:00:07.290 - 00:00:24.490, Speaker A: Hey there, and welcome to the latest episode of the Kyon restaking rendezvous. I'm Edgar, protocol specialist at Kyon, and I'm joined by my co host, Luig, who is senior product manager at Kyon's World. And today we have Michael from blockless, co founder of Blockless.
00:00:24.650 - 00:00:24.998, Speaker B: And.
00:00:25.044 - 00:00:30.838, Speaker A: And we're going to talk about the great, wonderful blockless Avs. Michael, how are you doing today?
00:00:31.004 - 00:00:35.960, Speaker B: For sure. Thank you guys for having me. Doing good. How are you guys?
00:00:36.330 - 00:00:37.080, Speaker C: Great.
00:00:38.970 - 00:00:39.622, Speaker B: Cool.
00:00:39.756 - 00:00:50.460, Speaker A: Okay, so I think to start this off, Michael, if you could give us a quick intro, what's your background and what led you to work in this wonderful space, which is public blockchain technology?
00:00:54.530 - 00:01:57.498, Speaker B: We're trying to turn this into more wonderful industry. Right? I got into crypto in 2017 when everyone was the ICO. I actually got in because Ethereum mining, beating my own risk, that kind of stuff. In 2018, I joined the binance team. I was there for three years, mainly serving as a researcher under the binance research department. And while I was working there, we're dealing with all kinds of project applications that want to get listed, and we're also working with all the projects that's already listed on Binance. So one issue we kind of discovered is that people are trying to build more and more complex applications, right? All the way since I think after 2018, and all the way till now, people are trying to deplete the possibilities of the building on chain, and people are trying to build off chain.
00:01:57.498 - 00:03:13.050, Speaker B: And we actually see a lot of projects back in my binance days that are trying to push the off chain boundaries to let their smart contract application, their decentralized application, be able to do more things. But then, back in the day, they're kind of penalized by the market, actually, because it is so complicated to be off chain, especially to be the right way to make it trustless, decentralized, not compromising the on chain components of your application and all of their centralized counterpart. Their competitors are moving much faster because they're using centralized solutions and their token part. So back in, they were like, okay, this is not the way the industry should go. If we're to build complex and more functional applications, we still should make them decentralized and trustless, right? So that's when, after I left the binance team, E 21, me and a few other people, I'll introduce them a bit later. Our co founders, we got together and really started beating the block as mission. Right? How do we enable more complex applications in web3 without compromising the security or reliability or trustlessness.
00:03:13.790 - 00:03:23.600, Speaker A: Great, I guess. It's actually a great segue. Please introduce your co founders. And how did you guys meet and agree to start working on this project?
00:03:25.330 - 00:04:07.850, Speaker B: Right now we have co founders on one of them, and two of them actually came from an investor background. Back in the day, I was telling them, oh, this industry has issues, right? And people just are beating more and more complex stuff and they don't have the necessary tooling to do it. So later on we just got together. Initially they're going to invest, but later on they're like, okay, this is actually great. So they jumped out of the investment firm Androidme on the other end. I was talking to Derek Anderson, now our CTO. Previously he was at Akash Network.
00:04:07.850 - 00:05:38.450, Speaker B: He was there for a long time, help building Akash from zero to one. So back in the day he was really thinking, okay, right now we have Akash, this open compute marketplace, right? But the user experience isn't as perfect as we wanted. So right now it's kind of like you have all those machines in the marketplace, but it's kind of hard for web two developers to actually adopt that model because if you're a web two developer, you go out to that platform, you have to individually interact with the machines or the people that are renting their machines and say, hey, I want to deploy this and that onto your machine. And maybe you have to coordinate manually between 1020 different machines. And when one of them falls offline, you have to manually bring up another one, right? Especially if you're doing some pbft or bft logic. If you have five computers, two of them fall flying and then you're in trouble, your whole service is down, right? It's really coordinating the computers and asking the developers to interact with them directly is too much of an ask. And Derek, back in the day was betting, okay, if we're going to make community powered, off chain, trustless compute to actually work, we need to have a more automated way of doing things.
00:05:38.450 - 00:06:21.240, Speaker B: We need to have a way of orchestrating those machines. And then the developers, from their perspective, they can just deploy whatever they want onto the platform and everything's taken care of. And this idea carried by Derek is kind of baked into the blockchain's product right now. From his perspective, he wants to make decent trust compute, more reliable and more easy to use for everyone. And from my perspective, the origin story is really how do we allow people to be more things off chain in web3, right? So the two just naturally came together and shaped the blockers product today.
00:06:22.730 - 00:06:42.060, Speaker C: Interesting Ed, so what would be applications you would expect to be built on top of this platform? Do you have some kind of, already today you will show a few of them. Could you maybe give some example there?
00:06:42.430 - 00:07:35.390, Speaker B: Yeah, for sure. Yeah. So I would like to, before I go into that, to talk about how exactly blockers functions, and then maybe we'll have a better idea of how exactly are those applications using blockers, right? Are you guys technical or have you guys used any traditional web loop development frameworks? Yeah. So right now, basically think about blockers as a place that you can deploy applications, right? And when you actually do that, there are three things you provide. One is your actual code in whatever language, it doesn't matter because we have a web specifically based runtime. So even if you're deploying Python in JavaScript, in any other things, we pre compile that into webassembly binary code. So all the nodes within the network, they'll just be receiving WASM.
00:07:35.390 - 00:09:00.630, Speaker B: So you provide your code, your business logic, and you also tell us in a manifest file, okay, this is how exactly I want this compute to be done, right? If you're doing some say decay proof generation or you're doing some kind of AI training, of course you don't want your workload to be deployed on an old phone PC like the one I'm using right now. You don't want that to happen, right? So you tell us the minimum requirement for the kind of node that you will need. And also you tell us how exactly you want your compute to be verified. There are many ways to verify a compute workload depending on your business logic, right? For instance, if you're dealing with numbers, chain link fashion of multiple nodes grabbing the same data, and then they come together with aggregation function like finding the average number that will work really well. But if your compute yields a yes or no answer, they can do voting, like PBFT or raft kind of voting, right? Different compute workloads will require different verification methods. So we allow you to customize that for each and every single workload that you have. You provide your compute requirement, you provide your business logic, and you provide your verification mechanism as desired.
00:09:00.630 - 00:10:32.530, Speaker B: So that's the three things you provide to us. And boom, you click deploy, right? And what's going to happen is as you've requested, okay, I need the minimum requirement hardware to be like this. I also want say minimum five nodes to be executing my workload, right? And now what's going to happen is as this request enters the blockers network, it's going to get broadcasted to all the nodes within the network, right? So the first step the network does is identify all nodes that are actually online at the moment in the network. And then out of the bounce we filter, say out of the 10,000 nodes maybe there's 1000 nodes that actually can fulfill your request based on your requirements. We find those nodes and then what we do is we do a real time ranking of all the available nodes, right? So based on their geolocation, based on their previous performance, their previous reliability, and also we do a simulated annealing process. So basically to test, okay, is this machine actually good for this particular workload, right? So then we kind of have a result in what we call as suitability score, which machine is actually more suitable for this particular compute. And then we have a full ranking of those 1000 nodes, right? Based on probability.
00:10:32.530 - 00:11:16.920, Speaker B: The higher your suitability score, the more likely you will get chosen for this workload. And say if you want five, then out of the most suitable bunch will randomly pseudo randomly choose five of them to work for you, right. This whole process right now takes around 800 milliseconds. So it's pretty fast. Basically you just deploy and boom, we know which nodes will be deployed to. Then the nodes will basically execute whatever you're asking them to execute and then verify the compute based on your requirement and return to your endpoint. So this is kind of the whole flow of how blockers works.
00:11:16.920 - 00:11:52.480, Speaker B: And talking about the particular compute that this network can take. This is pretty generic computing. It's not like smart contract computing. So basically it can really be anything. Right now the type of compute that we take is more like serverless functions, kind of like AWS lambda. As we are pushing into Minneap right now we're pre testnet. As we're pushing into Minnet, we're going to be supporting long running functionalities as well.
00:11:54.070 - 00:12:03.700, Speaker C: Interesting. So blockless is like a distributed cloud computing platform that selects the best hardware for your program, right?
00:12:04.230 - 00:12:14.520, Speaker B: So this is from the developer's perspective, right. You can view blockless as a decentralized computing platform, but there's much more. There's much more.
00:12:14.890 - 00:12:44.100, Speaker C: That's very interesting. And in terms of developer experience it seems very interesting. And on the verifiability part you explicit some cases where a developer can emit some proof using a blockchain using blockface. Do you expect this to be one of the main use case of your platform, like the verification PC part? Or is it just like one feature that developer can use?
00:12:45.350 - 00:12:50.194, Speaker B: You mean dynamic verification? Yeah.
00:12:50.232 - 00:12:57.720, Speaker C: Do you expect application on blockless? To all be verified, or do you more like think about it as kind of a general computing platform?
00:12:58.490 - 00:13:15.174, Speaker B: I see. I think this really depends on their particular use case and their security requirements. Right. At block plus, we think about security in different layers and we implement different security measurements at those different layers.
00:13:15.222 - 00:13:15.482, Speaker A: Right?
00:13:15.536 - 00:13:39.694, Speaker B: So at the very bottom we've implemented our own webassembly based secure runtime. So this means if, admiral, you're running a blockers node, right, and you are a untrusted third party for me, I'm a developer, I'm trying to deploy something onto your computer. And to you I'm also an untrusted third party.
00:13:39.742 - 00:13:39.954, Speaker C: Right.
00:13:39.992 - 00:14:26.450, Speaker B: You don't really know what kind of virus I'm going to inject into your computer. Right. It could very well just happen. Right? So this is why we need a secure runtime, to isolate the execution environment from the wider host machine, right? Say if I run some kind of a random software in your machine, it's never going to reach outside of the secure runtime. So your wider host machine is safe. Right. Even if I inject some kind of virus, your wider computer, I wouldn't be able to steal your ethereum private key, so to speak, and also you as the host wouldn't be able to peek into the secure runtime and see what I'm doing and try to attack me, right? So this is all the most baseline level of security, right.
00:14:26.450 - 00:14:52.860, Speaker B: The host doesn't know what code he's executing, he's only allocating resource to the runtime. Exactly what was happening in there? He has no idea. Right? And on the other hand, the deployer cannot reach beyond the boundary of the runtime and request for more resources or information. So this is kind of on the baseline and on top of that.
00:14:55.790 - 00:14:56.154, Speaker C: The.
00:14:56.192 - 00:16:01.594, Speaker B: Distribution method as I just described is pseudo random. You won't really know if you will be receiving any workload at a given time. You only know, okay, I'm reserving this much resource for potential compute that will come to me, but you never know which compute will come to who. Right? So this is kind of the second layer of security. And the third layer is when we're talking about dynamic verification. And this is kind of determined by individual developers. For instance, if I'm running a perpetual swap platform and basically I want to have this off chain compute network that does say fetching web two data, I have 50 nodes that all goes to various data source and they do data aggregation, right? Oh, this is the current price for Tesla, this is the current price for crude oil, right? And they all come together and they do voting.
00:16:01.594 - 00:16:37.980, Speaker B: And perhaps in the process you also ask them to use proof to make sure, okay, not only they're fetching the data, but I also want to know that the compute they used, they're executing this compute correctly. You can use things like Zkwasa to achieve that, to know the validity of the compute and not only the soundness of your compute result. So all of this is dependent on the design of the project. And what we do is we offer all of them as templates so they can pick and choose.
00:16:39.310 - 00:16:51.200, Speaker C: Okay, so you self more like, yeah, so generic platform for computes where basically developer have policies and not like giving them constraints on what they can do.
00:16:56.150 - 00:19:01.290, Speaker B: Yeah, because really I think for web3 applications, as they're getting more and more complex, it doesn't make sense to give them one set of rules for every single compute and every single business logic, right? I mean, this is why people need to move beyond the on chain environment to build off chain, right? Because what the on chain environment and the smart contract offers is one set of rules, one set of rules of execution, and one set of rules for your security or verification capabilities. But right now, as web3 is getting more and more complex and more and more mature, we just have more types of workloads that people are doing and it's not very suitable to do all of them on chain, right? For instance, there's no way for you to do inference kind of workload on chain. There's no way for you to do, say dynamic web hosting that kind of workload on chain, right? And also why we have oracles, because they require different execution topology and they require a different verification mechanism, right? So this is what we talk about as the modular application architecture. Really, you guys, I'm sure are familiar with how the modular movement has been going on for the blockchain industry and people are trying to break the blockchain apart, let say EtherVM, do what it does best. And maybe the execution part can be taken care of by other platforms, right? So specialization. But this ERV is the first step toward true modularity in blockchain. So right now, even if you're building on a modular networking stack, it doesn't mean your application is modular, right? Your application is still constrained to what the blockchain can natively do, right? Basically allows you to execute more smart contract logic cheaper, but it doesn't allow you to execute anything outside of smart contract capabilities.
00:19:01.290 - 00:19:40.840, Speaker B: So right now, what we're proposing, what we call as modular application architecture, is really breaking the applications apart instead of the network apart. Right. So we break the application down to every single workload and every single workload, they can have a different verification message, they can have a different execution topology. So this way they can all be trustless, they can all be at the same time very secure and performant. Right. And this is in our view, how we get web3 to be really functional and at least getting to on par with web two technology and even better in the future.
00:19:42.730 - 00:20:11.994, Speaker A: Super interesting. I guess the follow up question then would be why did you decide to build on Eigenair? And when you had the blockless project in mind, when you had that vision in like, was it always with the hypothesis of something like Egan layer being built in the future or how did the tool vision interact?
00:20:12.042 - 00:21:22.518, Speaker B: Basically, yeah, we started writing the white paper in 2021. Agonair wasn't there just yet, but from the very beginning, Agon layer and blockers, we are very much aligned on a philosophical level ago. Right now they're solving the issue of, say, if you already have an off chain, community powered peer to peer network, how do you secure that? Using proof of stake. This is what agile is solving, right? And on our part, what we're doing is how do you have an off chain P to P network in the first place? This is the part we're solved and we build all the technologies, all the templates, all the standards, so that when people deploy a block us, boom, they immediately have an off chain PDP network that's working for them. Right. This is where our synergy really come together. And in terms of using agile in our thinking, there are mainly two benefits for the blockers platform.
00:21:22.518 - 00:22:17.346, Speaker B: One is that as you can imagine, we're trying to provide more optionality to the developers. As mentioned, we're always trying to provide more flexibility to builders on blocklets, right? Because their business logic can vary a lot. And this provides optionality for people that want to leverage some kind of ethereum security in their business logic when they're building off chain. Sure. And the second part is more generic, where we're going to be having all kinds of nodes. It's a heterogeneous network, nodes of different hardware capacity, nodes from different geolocation. And one major benefit that Agne operator provides us is that they have server grade hardware, very reliable.
00:22:17.346 - 00:22:32.186, Speaker B: And initially, you say, during early phases of Mina, maybe the community contribution wouldn't include like super powerful machines. But what if some developer, they want to deploy something super heavy, what do we do?
00:22:32.208 - 00:22:32.406, Speaker C: Right?
00:22:32.448 - 00:22:45.940, Speaker B: So this is where the agony operators comes in and they can really become the backbone of the network in some cases and really back up the whole thing, be able to fill in the gap when needed.
00:22:48.150 - 00:23:06.680, Speaker A: Okay, so I guess my question is that Kieran is going to run operators on in air. What is in detail like the work that blockless is demanding the eigenair operators to do?
00:23:07.710 - 00:23:10.250, Speaker B: So what kind of hardware requirements?
00:23:11.710 - 00:23:25.962, Speaker A: A bit. It's also, I think, an interesting point. How heavy would the AVs be for operators? But I think first and foremost, just like a dumbed down version of what job they are executing.
00:23:26.106 - 00:23:43.358, Speaker B: I see. So the particular jobs can vary a lot. I mean, it's basically going to be the initial applications that's building up blockers. Right. And initially I think you can envision them to be more serverless functions, type of workload.
00:23:43.454 - 00:23:43.954, Speaker A: Right.
00:23:44.072 - 00:24:30.798, Speaker B: And as later on we'll be pushing out the support for a lot of running processes. I think after being that launch, we are planning that for late Q two and early q three. So the type of workload will be mostly not too demanding computationally, I would say, because the Webassembly runtime itself has a four gigabyte limitation. And for anything that goes beyond that, there are AI applications that are using blockbust. Right. There are two ways to work around that. One way is when we have support for long running application and we can support more heavier processes.
00:24:30.798 - 00:25:44.038, Speaker B: And the other way is via webassembly extension. So basically, if you need the machine to run some kind of software, and basically we call them extension, basically trusted software. For instance, if I'm a developer and I deploy a blockless, and Edgar, you're running a blockers node, I can require say, hey, I only want to deploy a blockers node that has say, guest that's running the Ethereum client. So this is the kind of requirements that they can specify. And overall, I would say the type of workload a particular node operator, not only Agne operator can take in is dependent on how much they're willing to contribute, as this is a heterogeneous network, right. There will be people joining with their old computers, there will be people like you guys with more resources. The entry requirement to run a block of snow will be the same across the board, right? So say for instance, you stake teddies or you stake a particular amount of block and stoke.
00:25:44.038 - 00:27:11.654, Speaker B: And this ratio, of course, later on will be dynamically adjusted by the community. This is basically your entry ticket for running a block of snow. The particular compute that you can run does not directly correlate with how much you stake, right? So it has to do more with what kind of hardware capacity you actually have and how much you're welding to contribute. Right? So on this front, we're very flexible with no operators in general. So you may be curious, like if the stake that we have does not directly correlate with what kind of compute it can take in, how exactly is this secure? Or why do we need the stake at all in terms of security? It really comes down to what I mentioned, the secure runtime, the dynamic ranking and workload distribution mechanism that we have, the compute orchestration, and then the customizable and dynamic verifiability that the developers can have. The reason why we need there to be staking in here is that imagine if there's no staking echo. You can spin up 1000 instances and join the blockchains network and just sit there and do nothing.
00:27:11.654 - 00:28:00.680, Speaker B: Right? We don't want this to happen. It's going to impact the overall performance of the platform. So for anyone that wants to join the network that want to participate, we require them to have some kind of stake in there. And if they just continue to be non responsive or they do something malicious and we can detect that, and then they will get slashed. In terms of general requirement that we are asking for agon operators right now, we're basically just asking for right now, I think two core CPU and 4GB of RAM. This is like the bare minimum of requirements. And if you'd like to contribute more, then that's a viable option too.
00:28:03.450 - 00:28:23.658, Speaker A: Okay, so basically the operators provide like computation power and they also provide stake. On top of that, adding more stake will not generate higher rewards because it doesn't add more competition.
00:28:23.834 - 00:29:11.134, Speaker B: So the reward comes to. It really has two parts, right? Well, right now we're not in Minat, so you cannot take this for 100%. But the general idea is this, right? So there will be two type of reward in blockers. One is as we're incentivizing people to stay alive, there will be a static reward in the form of block, in our own native token to anyone that stays alive. If you're running blockers node, you're staying alive and then you're getting reward. Kind of like how ethereum, you have a block report even if you are not really produced packaging transactions. This is just to incentivize everyone to be online.
00:29:11.134 - 00:29:54.794, Speaker B: And on the other part, this is more dynamic. It really depends on so the applications, they can specify how they want their node operators to be rewarded. They can have their own ways. For instance, if you are at guard. You're running an AI application built on blockers that uses blockers to power its edge inference network and you're one of the edge nodes in there. They are going to be giving you independent rewards basically based on their own rules. But your hardware capacity dictates what kind of application you can run and how many of them you can run together.
00:29:54.794 - 00:30:08.910, Speaker B: Right. So if you have a more powerful machine that means you can run more what we call as network mutual applications on blockers. Right. But if your computer is really not that powerful, then likely you wouldn't be able to receive any workload.
00:30:11.090 - 00:30:28.840, Speaker C: And so do you expect, let's say big operators that have infrastructures to always kind of specialize to the trendy computation requirements on blockless? How do you view kind of the mechanics there?
00:30:29.930 - 00:30:31.640, Speaker B: Sorry, what's the question?
00:30:32.170 - 00:31:03.890, Speaker C: Yeah, would you say that at some point people would be more incentivized to always have the best hardware for the trendy recommendations? For example, if everyone is running AI models, it will be more profitable to run instances with GPU. So all operators will have GPU instances. Do you have that in mind? And how do you view this kind of specialization of operators?
00:31:04.550 - 00:31:31.820, Speaker B: I see, yeah, that might be a possible feature, I think if most of the applications offering a lot of reward to node operators on blockers, if they're all like AI applications and their basic requirement is like hey, I need a very high tier machine. Right. And of course people will adapt to it and try to capture the yield. It's a free market.
00:31:33.950 - 00:31:53.870, Speaker C: Interesting. And so you mentioned that we are currently in the testnet phase. What's kind of the time for brockless in terms of networks, in terms of project launching in like can we already join with some other to test the network?
00:31:54.450 - 00:32:39.662, Speaker B: How does the see, I see, yeah. So right now we're just getting to test that. Actually we just released our first pretest that campaign two days before we're speaking right now and do make sure to give that a play. So we are pushing into test that in the coming weeks. In terms of timeline, we're looking to conclude test that say in later Q two, beginning Q three and then we launch into maintain. Right. In terms of the projects that we have been working with, I wouldn't really disclose names right now.
00:32:39.662 - 00:34:45.158, Speaker B: Later on we'll talk about them with our community first. But right now I can give you some hints regarding what exactly they're doing with blockers. Right. So there are some areas that I think in general I'm very excited about. One is of course as you mentioned there are AI applications very popping out in web3 and there are a few companies that reach out to us and have been working with us for a few months on community powered edge inference, right? So imagine you have a network running on blockers and they're actually powered by user devices and what they're trying to achieve is to have this global edge network that does edge inference for their applications, right? This is one type I'm very bullish on and the other type is doing a local first processing of say, federated learning, right? One part I guess I didn't get to talk about blockers earlier in this podcast is as we're really having a heterogeneous network and we're breaking the application apart workload by workload. This also give us the possibility of letting the very application users to directly power the applications they're using, right, because previously you have to rely everything on Serp RD node operator, because you have to do everything together, it tends to get very heavy. There's no way for the community to get participated in the very applications they're using, right? But when you actually break things apart, there are a lot of lighter workloads that the communities can directly take when they're actually using the application, right? I'll give you some examples, right? There are some gaming companies that reach out to us and we're working together.
00:34:45.158 - 00:36:10.258, Speaker B: What they are building is AI powered, self evolving games, right? So how does that work? The content is generated by those machine learning models directly on local and edge devices. So while the user, they're playing the game, the game is consuming their local resource to actually generate the game. So this is something doable. And for social applications, this new way of doing things, allowing the communities to actually power the applications, there's actually one social platform that reach out to us and they want to build this platform where everything is powered in a p to p fashion, right? So say Adgar, we're both users of this platform. What's going to happen when we go to this URL is that if we're running a blockers node, we will be executing web, simply serverless functions to do dynamic web hosting for this website. So the website assets, they're all stored on IPFs and we as users, as we're running a blockers node, we will be executing the functions to basically move those assets for each other. So this, as you can imagine, as long as the application or the platform has a small number of users, it's never going to get them right.
00:36:10.258 - 00:38:05.400, Speaker B: This is about censorship resistance as well. There are a few DeFi projects that we're in touch with, DeFi projects would especially benefit from this as they have been really booked with regulatory concerns for the last couple of years because they don't really want to provide the front end for the application. It is fine if they want to do the smart contract part, right? It's considered a software, but when they also do the front end part, it then suddenly becomes as if they're providing a financial service and some governments are not cool with it. So there are deFi platforms that reach out to us and be like, hey, if the blockers note is really so lightweight and I can dissect my application workload by workload, can I actually let my community run a blockers node? And when they're using my application, can they actually do dynamic web hosting for my website, right? So I'm no longer having this liability and my application never goes down as long as I have users, right? So this is what happens when you first make community powered compute really reliable with compute orchestration and dynamic verifications. And also when you dissect the applications workload by workload, right? So the communities, they can actually contribute something, they can actually democratize this whole compute processes and make things more precious. And right now what we're looking at is really a new way of a fundamental change in the relationship between users and the very applications. This is the wider or the more ultimate vision that we're driving for.
00:38:06.890 - 00:38:35.120, Speaker A: Very interesting. And so when you thought about blockless, and I'm sure that this thought has evolved, what do you think is the total addressable market for this? And keeping in mind that this is a podcast with a target audience of risk takers that total logos or market like, what could be the revenue that blogless can generate from it and pass on to the.
00:38:38.630 - 00:39:49.560, Speaker B: Mean, this is going to be a very large number. It's going to be a trillion dollar market. Right now we're merely working with web3 projects, helping them to build more robust applications, having the ability to really be independent of onchain limitations and build whatever logic they want in a verifiable and trustless way, right? So we're really tackling at what we call L2 or web3, off chain network, this kind of market space. But at the end we're going beyond that. We're going to say in the AI case, more traditional edge networks. It really goes beyond the current scope of web3 and into this really new community powered paradigm where everyone roll their Romney application, they can simultaneously be the backbone to the application by providing compute, right? So it's going to be very hard for me to give a number right now, but we'll see as we roll out the main net and forward.
00:39:50.090 - 00:40:12.398, Speaker A: No problem, you cannot give a number. But maybe you can answer to the next question, which is what do you think would be the form of that reward? Do you think it will be, will it be maybe like the token of the application that requires computation? Or could it be like USDC or.
00:40:12.404 - 00:41:03.490, Speaker B: USDC for, for sure, sure. So the part that we're sure of is that we'll be providing our native token as a static reward to reward people for being online. And the other part really comes to how the individual applications, they want to reward their own operators. And for instance, if there are two AI edge inference application built on blockers and they're recruiting nodes and they say, hey, I'm going to be paying you guys, one of them maybe says I'll be paying you guys with ease, right? Determined by how much compute I actually consume off of your device. And the other one says, oh, we're going to be giving NFT. More likely more people will be choosing the east flat, right? So this part is dependent on the applications.
00:41:06.710 - 00:41:32.460, Speaker C: Interesting. And in terms of economic security, do you have a target number in mind of how much security, economic security is enough for JVs you will deploy? Do you have some insights? Like for example, applications deployed on blockless, do they expect a certain amount of economic security today? What's kind of a good number for you?
00:41:33.950 - 00:42:57.350, Speaker B: Yeah, this is a very good question. Right now I think this is still an area of active research for us because unlike some of the other Abs right now, the type of compute and services that blockers will take in varies by a lot. It's not like there's one uniform type of compute, one uniform type of service that we're providing. If that's the case, that's going to be easier for you to say, oh, this is how much economic security I need. So is that the current way we're thinking about it is, okay, we have the technical tools to make the compute as reliable as possible via the secure runtime, compute orchestration and dynamic verification. So all those tools are built in to make sure no matter who is running the node, the compute will be secure, right? But when it comes to how much economic stake a particular node needs to have as the buy in, the minimum requirement for you to join the network, right now we're thinking maybe we can have something like ten east to be the minimum requirement, just so that the network doesn't get spammed if there is ever a malicious player.
00:43:00.410 - 00:43:31.950, Speaker C: Okay, cool. It's an active area for everyone. I guess that Mainnet also will bring us the real metaverse from the market. So that would be interesting. And just in terms of general tooling, so what are kind of the tools outside of blockless? I expect that there will be kind of a platform for developers to easily deploy the application and monitor them. What kind of tools are you building on top of this protocol?
00:43:34.230 - 00:43:37.220, Speaker B: I see, so you're talking about the developer experience.
00:43:37.670 - 00:43:38.660, Speaker C: Yeah, exactly.
00:43:40.230 - 00:44:06.570, Speaker B: I see, yeah. So on this part right now, what we have is CLI and Gui tooling. So like a developer dashboard and also a COI tool. So this is what we have now. But later on we're going to have a more packaged offering where everything's kind of aggregated together and very intuitive, very user friendly. But this will come later and you can expect that on our Twitter timeline.
00:44:08.030 - 00:44:09.100, Speaker C: Very cool.
00:44:10.370 - 00:44:11.070, Speaker B: Cool.
00:44:11.220 - 00:44:32.900, Speaker C: And yeah, in terms of economic security and operators, just in general, do you think that it's kind of like, let's say limitless in a way that you expect to have as many operators as possible, or do you plan to have more qualitative operators? What's kind of your balance there?
00:44:33.670 - 00:45:15.774, Speaker B: Yeah, so remember, blockers is a heterogeneous network and we want as much community participation as possible. We're making the Nook software very easy to use, emirate Light. Remember we have a webassembly secure runtime, right. And webassembly itself is much lighter comparing to Docker and other say VMs. So that as long as the user device, it can boot up a browser, basically you can run a block node technically. So the first form of the blockers node will be actually in the compute browser directly. When you open a tab, you will be running a blockers node.
00:45:15.774 - 00:46:07.860, Speaker B: Later on we're going to do a chrome extension too. So for the retail users this is kind of going to be the case. We want as much of a distribution and decentralization as possible. We want more people to be a part of this. And on the other hand, so for Agon operators it's going to be a slightly different story. Of course we want more nodes, but we don't want one operator to run like 1000, this wouldn't be ideal, like 1000 nodes in the same data center, right? That's not really achieving the purpose. So what we would encourage is for Agne operators, they can run more powerful nodes that contribute more compute resources and are able to take in heavier workloads at a given time.
00:46:12.710 - 00:46:22.120, Speaker A: Okay, then maybe a last question before we finish off. Where do you see this ecosystem in, say, three years?
00:46:23.210 - 00:46:26.120, Speaker B: Where do I see blockers in three years?
00:46:26.650 - 00:46:27.062, Speaker C: Yeah.
00:46:27.116 - 00:46:29.690, Speaker A: And maybe also, like the relationship with Eigenve.
00:46:30.750 - 00:47:11.480, Speaker B: I see the ultimate goal, in the short term at least, is for blockers to become the default platform. When you are trying to deploy and build a more sophisticated web3 application, when you're trying to go beyond the current boundaries of smart contract applications, when you try to be something that's more adoption ready, that's very performant, but also secure and trustless, we want to be the de facto developer platform. When they think about building something complex, they don't just come to an onchain smart contract platform. There's much more to web3 than just on chain. And this is the place that we want to be with.
00:47:12.650 - 00:47:24.650, Speaker A: That's a perfect punchline to end it. Michael, this was a fantastic conversation. Where can we send our audience to learn more about blockless? Maybe follow you on Twitter?
00:47:28.990 - 00:47:43.070, Speaker B: You mean our website? And I mean, if you go to Twitter, our handle is the blockless. And over there you can find all links. Perfect.
00:47:43.220 - 00:47:50.238, Speaker A: Well, thank you guys so much for joining this episode and to the audience. We'll see you on the next one.
00:47:50.404 - 00:47:50.990, Speaker B: Bye, everyone.
00:47:51.060 - 00:47:51.340, Speaker C: Thank you.
