00:00:03.850 - 00:01:01.600, Speaker A: Okay, everyone, this is the third Mevboost community call. I will drop the agenda in the chat here, this forum post and yeah, there's not too much on the agenda this time, so we'll see me how things go. Maybe we can wrap up a bit early to get started. There was a miscommunication, I think, on the last call around Block Native and them having some sort of discussions or deals with builders towards funding their relay and yeah, it turns out I think that was a misstatement and so, yeah, I just wanted to call out that Block Native does not have any of this going on. I believe the original comment was said just as like a hypothetical and yeah, I don't know, Matt, if you want to add anything to that.
00:01:02.050 - 00:01:08.082, Speaker B: No, just appreciate the opportunity to clear the air there that we have not monetized our relay and that's the current.
00:01:08.136 - 00:01:10.980, Speaker A: State, so just appreciate the opportunity to clear that up.
00:01:13.430 - 00:01:14.180, Speaker C: Great.
00:01:15.190 - 00:01:26.840, Speaker D: Yeah, that was a comment I made on the last call. I'm sorry I misunderstood what we were talking about at the time. Didn't realize it was a hypothetical. Sorry for the confusion there.
00:01:27.390 - 00:01:29.580, Speaker B: No worries at all, Austin, it's all good.
00:01:32.780 - 00:01:33.530, Speaker C: Great.
00:01:35.340 - 00:02:15.830, Speaker A: So I just wanted to clear that up. The next big thing will be readiness for capella coming up. So we had Gorely, what, maybe like two weeks ago at this point, and generally I think it went well. I think maybe there was some confusion about exactly which software to run, especially if you're running, like, the Flashbots code. And that led to some a little difficulty, but I think ultimately everyone shifted over to the right code on Gorely. Issues resolved and things are looking pretty good. Anyone want to share anything else about their Gorli experience?
00:02:21.160 - 00:02:37.580, Speaker E: Yeah, I'll jump in. On the Dreamboat side, we had an issue with an infrastructure misconfiguration that caused conflict between a Prism and a lighthouse beacon. But once we resolved that, then Dreamboat handled.
00:02:39.920 - 00:02:42.316, Speaker C: The change just fine.
00:02:42.418 - 00:02:43.900, Speaker E: So we were in good shape.
00:02:48.880 - 00:02:50.030, Speaker F: Good to hear.
00:02:51.680 - 00:03:33.550, Speaker A: Yeah, sounds like Gorilla Journaling went well in for everyone. The next thing to bring up would be main net readiness. So again, if you're listening, if you're a builder, things will change with the fork, so you need to update your infrastructure somehow. If you're a relay, hopefully you're on this call and it's very important that you have the right code at the right time, otherwise yours will stop working. And yeah, if you're listening and running Map Boost, you're also going to want the right boost release ready for the fork. And yeah, maybe, Chris, you could talk a bit about the Flashbots code. Okay, well, he dropped a link here.
00:03:34.240 - 00:03:34.988, Speaker G: Absolutely.
00:03:35.154 - 00:03:36.270, Speaker A: Okay, thanks.
00:03:37.040 - 00:04:21.480, Speaker G: Yeah, I'm happy to talk a little bit about the software. We just made a clean new list of which software versions to use and we also just got everything pretty much merged into the master branches. And releases out. Map Boost itself is ready since two weeks with version one 50. So Proposers will need to run Mafboost one 50 or later. If they updated in the last two weeks then they are already good. This specific release has been tested in multiple testnet through Gurley and Sepolia and this is expected to have not seen any issues with all band define the Map Boost relay.
00:04:21.480 - 00:05:11.080, Speaker G: There is a new pre release v 10 alpha two. It's going to be a 10 release once we have it deployed on our main net. We tested this release through all the testnets and this is expected to work without problems. This release of the release software also does not need the custom Prism fork anymore. So you can run either Prism or Lighthouse or any other in the future CL client once they support a certain SSE event subscription. So this release for Relays has everything that's going to be there for block validation. On the relays.
00:05:11.080 - 00:06:38.546, Speaker G: We deprecated the Block validation gaff repository and validation is now done through the Builder project. It's kind of a similar code base and the API has already been there and we are just focusing our maintenance effort on a single repository. The rebases to new Go ethereum versions takes a lot of resources and if we don't have to do that on multiple code bases helps a lot. So we consolidated on using the Builder project for block validation and as of today you can just use the main branch, the main branch of the Builder project for block builders. You can also use the main branch of the Builder project but you still require the custom Prism fork because we are only wrapping up the switch to the SSE subscription. So if you run the latest Builder code right now you need the custom Prism fork and we expect that early next week where the PR is linked in this document and it's testing in testnet successfully. And then instead of polling our Prism fork for withdrawals, the builder receives it through the SSE event and more of the block building payload attributes come through it.
00:06:38.546 - 00:07:29.750, Speaker G: So great work also on the CL client teams to get this done and I'm extremely happy that we don't need to maintain a custom client fork anymore. Yeah, great team effort. One thing to shout out is that if you run sync proxy in your infrastructure then you also need to update this piece of software. It's Radisson's version two that was released two days ago. And the sync proxy is a tool that you may run in a really infrastructure where you can have usually one CL client drives one execution layer client and you can put a sync proxy in the middle and have multiple CL clients and multiple El clients. This allows you redundancy in the infrastructure. So if one of your CL clients dies out, two others still continue driving the El clients.
00:07:29.750 - 00:07:45.250, Speaker G: Yeah, I think that's the rough overview. Everything is pretty settled. In a few days block builders won't need the custom Prism fork anymore. I would love to hear if there's any questions or inputs.
00:07:49.420 - 00:08:04.060, Speaker D: Sure. I have a quick question. So in the interest of CL client diversity, how are the other client teams coming along with their implementation of the SSE changes? Is there an expectation that they'll be ready by mainnet chapala?
00:08:05.760 - 00:08:45.240, Speaker G: It is not expected that all clients support it. TECO has an open issue for it and it's on the radar. Nimbos, they don't have it scheduled and the Lodestar guys, they are in development, maybe ready, but we are still waiting, for example, payloads to be able to confirm from our side that this is provably a working version. So I don't know how fast TECO will get a release out and is done. I would think it would be either one of Prism or Lighthouse that I would recommend planning for going through the fog.
00:08:46.140 - 00:08:47.550, Speaker D: Sounds good, thanks.
00:08:56.280 - 00:09:40.530, Speaker A: Yeah, it's exciting to see that we have this as a z endpoint. So now things could be agnostic to clients and yeah, as other teams roll them out then presumably the distribution can improve over time. Chris, this was really good. Yeah, we'll get this. Maybe it should live here, but either way there's a nice overview here with the different links to the software if you run the Flashbots code. If you don't, then presumably you know how to run your own infrastructure. So yeah, main net is happening, what, just under two weeks from now.
00:09:43.240 - 00:09:43.748, Speaker C: Yeah.
00:09:43.834 - 00:09:44.950, Speaker A: Everyone get ready.
00:09:48.590 - 00:09:49.098, Speaker C: Yeah.
00:09:49.184 - 00:10:22.520, Speaker G: One thing to add apropos client diversity is that there's also multiple El clients possibly coming up that are supported besides GEF. In particular, Nettermind is working on an implementation of the validation logic for Relays, and we are communicating with the ref team and possibly also with Ericon if they could potentially become alternatives for block validation and maybe also for block building. There is a lot of performance gains that are possible this way.
00:10:28.750 - 00:10:46.720, Speaker A: Yeah, I think the more the merrier for the client diversity issues. For sure. Anything anyone else has for the upcoming fork. Otherwise I think we can shift to talking about the optimistic relay.
00:10:48.740 - 00:11:22.350, Speaker E: I'll jump in again with a Dreamboat update in the interest of Relay diversity. Dreamboat on main Net right now is running it's propeller ready, but it does not yet include it currently includes requires using the Flashbots Prism fork and we expect that to change in the next few days and we will update to support both the Prism and Lighthouse clients. So that should be then fully ready.
00:11:26.050 - 00:11:27.120, Speaker A: That's great.
00:11:27.650 - 00:11:38.580, Speaker G: One question here does it mean the main branch and which release is the one that you would recommend? Then I can add it to the document too.
00:11:42.310 - 00:11:49.270, Speaker E: The main branch and if not I will DM you on that, but I'm pretty sure it's the main branch.
00:11:51.130 - 00:11:52.440, Speaker C: All right, thanks.
00:12:03.530 - 00:12:26.240, Speaker A: Okay, there are some optimistic relay updates. So we'll move to those. Let's see here. Mike, I think you're here. Do you want to give us a summary of I think essentially these are two changes that you've been working on. I believe they're basically just extending the APIs for this new optimistic paradigm. Do you want to say a bit about that?
00:12:26.770 - 00:13:33.780, Speaker F: Yeah, and I might preempt that with just a quick summary of the release of Optimistic Rollout or Optimistic Relaying V One, which happened exactly two weeks ago, basically. So just kind of quick high level summary. We have pretty great builder adoption. We have twelve builders who are posted who posted collateral with ultrasound relay, one ETH each. The builders can be seen on this event log here, including the transactions and the pub keys that they're submitting blocks to the relay with. And if you haven't seen this article, we look at some of the data from the initial rollout of Optimistic Relay and in general, the numbers look pretty great. We've started seeing way more payloads delivered through the ultrasound relay than before Optimistic Relaying, and in general a much higher inclusion rate, which is kind of the metric that we're most interested in.
00:13:33.780 - 00:14:33.094, Speaker F: Additionally, we have been working with Flashbot's team, in particular with Chris to get this PR in shape. They've talked a little bit about upstreaming it into their main repo. I'm not sure whether or not that means Flashbot's relay itself will turn into Optimistic Relaying, but at least it would be in the kind of canonical code base there so that everyone can run it if they want. And kind of following that vein, there are a few other relays that are interested in running optimistic. We've talked to Estes, we've talked to Agnostic, Block Native, and even Frontier Research has been discussing maybe spinning up their own relay. So that might happen in the coming weeks and months, probably post, but okay, cool. Chris just said we don't plan on turning yeah, just having it as an option in our code base.
00:14:33.212 - 00:14:33.686, Speaker C: Cool.
00:14:33.788 - 00:15:25.000, Speaker F: Yeah. I guess the one thing we wanted to highlight in terms of the status of the code now that has caused a few headaches on our end is this issue of block validation failing with an issue that says Unknown Ancestor. And we did a deep dive on this, and what we have seen is that if our geth nodes fall out of sync, then we basically have a bunch of blocks that are coming in that correctly identify the parent hash. But since the geth nodes are behind, they don't see that parent hash and they fail the validation. So we've been working on a fix there, but that's kind of the biggest hurdle. If anyone starts running Optimistic Relaying, they might see that error too. So that's something we're working on.
00:15:25.000 - 00:16:03.378, Speaker F: Additionally. Yeah, we wanted to just point out that we haven't had any missed slots due to Optimistic Relaying. We've been running for about two weeks here. Very smooth. There was one optimistic block that was submitted that actually was invalid. The issue was announced being reused by two transactions in the block and that hadn't been caught by the builder. They've fixed that bug and yeah, they're reactivated as optimistic and we haven't seen any more errors there.
00:16:03.378 - 00:17:06.322, Speaker F: So, yeah, that's kind of a great testament to how reliable the builders are in terms of producing valid blocks and how stable the launch of Optimistic relaying has been overall. Additionally, we wanted to point out we made one change to the code that makes it even more conservative and that is we now demote within a slot. So previously if a builder was submitting invalid blocks, we wouldn't know until the end of the slot. And so if they end up winning the auction, that invalid block would end up being relayed and would result in a missed slot. But now we changed it so that in fact, as soon as they submit an invalid block, we demote them so even less. We haven't had any missed slots yet, but we even reduce the probability further for any issues in the future. Yeah, that's kind of the brief summary.
00:17:06.322 - 00:17:53.666, Speaker F: Again, we don't plan on having to use the collateral at all. The collateral is kind of seen as a last resort for any rogue builders. If there is any issues that result in proposers losing rewards, then we hope that the builders will refund them directly and we're going to kind of lead the communication effort there. And the last thing to kind of plug, I guess is and this leads into what is actually on the agenda. So thanks for bearing with me through kind of long winded presentation here, but we're working hard towards V Two which is presented in this roadmap here. I've sent this before. So yeah, the idea here is that we want to enable header only bids.
00:17:53.666 - 00:18:05.766, Speaker F: So currently the big bottleneck for optimistic submissions is actually getting all the bytes from the payload or the bytes of the payload from the builder to the relay.
00:18:05.878 - 00:18:06.540, Speaker H: So.
00:18:10.750 - 00:18:39.830, Speaker F: The Execution payload can be like several kilobytes up to a few megabytes. So that's definitely the big bottleneck in terms of builders getting their bids activated on the relay. So what we're proposing here and I guess I'll link the proposal, which is in the relay spec and then also the exact type that we're suggesting for a new submit block request.
00:18:42.090 - 00:18:42.646, Speaker H: Is set.
00:18:42.668 - 00:19:45.674, Speaker F: Up in such a way. That the Execution payload header which is all that we need for the proposer to sign is actually contained within a smaller basically contained in the beginning part of this message so that the bid can be fully contained in a single packet from the builder to the relay. So the V Two version of optimistic relaying is the builder starts sending packets with their bids. The first packet of the submit block request will contain everything we need to mark that bid as valid. And so then it'll be eligible to win the auction right away. The subsequent packets of the submit block request will have the full transaction list, which is kind of the vast majority of the bytes in the payload. And yeah, once the full body gets decoded, then the actual block validation can happen again.
00:19:45.674 - 00:20:32.380, Speaker F: This will all be kind of off the fast path of the optimistic submission. So, yeah, this is V two. It'll hopefully take the time it takes for a bid to become active. Like the submission time of builders should be dropped by another 100 milliseconds approximately, which is about what we saw in terms of reduction in time for moving the block validation off the fast path. So we expect another significant performance improvement. If you have any kind of questions about the overall roadmap or this new type and how we plan on implementing it on the relay, would be happy to answer those, but that's kind of where we're at. I'll toss it over to Justin if he has anything to add.
00:20:35.550 - 00:21:51.422, Speaker H: No, that was great, Mike. I think we've received tens of millions of optimistic bids and really they've been extremely high quality. One of the things that we're incentivizing now is actually builders to not have errors and to just build valid blocks. And this is working out, the incentives are shaping the outcome, which is great. And actually this is very good and healthy preparation for Enshrine PBS, where builders really are heavily incentivized, even more so than optimistic relaying because they're guaranteed they stand to lose money if they put invalid bids in the same way. And actually talking about builder errors, it's not really related to optimistic relaying, but we're kind of considering a new queuing mechanism. So right now, I believe the way it works with the default flashbot code base is that there's two queues, there's a low priority queue and a high priority queue, and it's kind of very potentially very inefficient.
00:21:51.422 - 00:23:05.450, Speaker H: And the reason is that sometimes you're just spending simulation resources on blocks that will never win the auction. And the reason is that you've validated a block with a bid with a certain value and you know that it's a correct bid, it's a valid block. And so any bid that comes in that has a lower value, you can just skip the simulation. It can just go in this ultra low priority queue, which doesn't steal simulation resources from bids that are in the race. And so basically we've been thinking of, okay, what is the optimal design for building these queues? And basically what we're considering doing right now is putting everyone in the high priority queue by default and only simulate the blocks that actually stand to win. So only if the bid value is increased versus the best known bid do we do the simulation. Everything else can go in some sort of virtual ultra low priority queue.
00:23:05.450 - 00:23:26.690, Speaker H: And then if like the the only bad thing that could happen here is that we start getting spammed with blocks that are invalid from people in the high priority queue. And so what we can do here is actually have an automatic demotion.
00:23:29.590 - 00:23:29.954, Speaker C: Of.
00:23:29.992 - 00:24:37.830, Speaker H: Builders that are in the high priority queue to the low priority queue if there's even a single bad bid. And so this incentive that we've established for the optimistic builders, we can actually establish it as well for all the other builders that don't want to post collateral. So we're kind of going to significantly improve the logic of simulation, even in the non optimistic case. So relays like flashbots that don't want to turn on optimistic relaying should dramatically improve the simulation latencies because most of the time there will be no queue. And the reason is that oftentimes what happens is that there's one builder who's, like, leading the pack. They have some sort of special alpha or whatever, like special private order flow. And what happens is that they're the ones that are constantly setting the top bid and those are the only bids that we really need to simulate everything else from the 20 other builders.
00:24:37.830 - 00:24:45.850, Speaker H: All the bids can basically be skipped or at least not compete in the simulation race.
00:24:53.870 - 00:24:54.570, Speaker C: Okay.
00:24:54.720 - 00:24:56.140, Speaker A: It's very exciting to.
00:24:59.970 - 00:25:01.790, Speaker F: Have a go ahead, Chris.
00:25:02.210 - 00:25:04.126, Speaker G: No, you go run it out.
00:25:04.308 - 00:25:09.182, Speaker F: No, I was going to bring up your comment in the chat. It sounds very relevant.
00:25:09.246 - 00:25:09.860, Speaker C: Yeah.
00:25:10.310 - 00:25:33.980, Speaker A: So wait, I have a question, though, before we because we kind of jumped beyond the prior point about this block with the nonce reuse and the builder bug, how did we identify that? And I presume the bid just wasn't presented to proposers and then maybe some other downstream we found this issue or what happened.
00:25:34.510 - 00:26:32.902, Speaker F: Yeah, so the way the optimistic relay is set up is that once the bid was marked as active, but fortunately it didn't win the auction. So because the other higher bids came in later in the slot, even though the bid was at some point marked as active, it didn't win the auction. And because the simulation happened and the simulation failed the next slot, the builder was demoted and so they remained demoted for the next few days. We sent them the full payload. We had kind of a private conversation with them and they found the issue and fixed it and then we repromoted them and we haven't seen the error since. In some sense, every time a builder bug happens, there's some amount of luck around it not winning the auction. That was the first builder bug we've actually seen and we kind of got lucky.
00:26:32.902 - 00:26:43.016, Speaker F: But we could have missed a slot there if that had been the highest bid for that slot. Did that answer your question?
00:26:43.198 - 00:26:43.784, Speaker C: Yeah, it did.
00:26:43.822 - 00:26:58.110, Speaker A: Yeah. Thanks. Something to keep an eye on. Yeah, there was a lot going on in the chat. Did you want to answer Chris's question or maybe chris had a question.
00:26:59.200 - 00:27:27.156, Speaker G: Yeah, let me say I really like the transparency of how you present what's happening and what's the plans. So Mike and Justin. That's great. Also the innovation that you bring to the table here. And I think the block validation is just a pain. Like our Gaff nodes are running hot and falling behind and being slow. Sometimes we have a second validation delay.
00:27:27.156 - 00:27:52.668, Speaker G: It's a pain. And I'm totally interested in any type of smarter scheduling that could make the life of good submissions easier and better. Yeah, let's work on that. And I think there's probably ideas. There is two points to consider here. One is that currently the relay purposefully allows cancellations for block builders. So block builders can send lower value blocks.
00:27:52.668 - 00:28:45.040, Speaker G: And there is an implicit promise that they will override previous blocks with higher value, so they can run maybe more risky strategies and post high value blocks. And if the situation changes, they can override this with a low value block. That's why without changing this mechanic, you can't just ignore lower value beats. Like putting them in the super low priority queue seems incompatible with block cancellations. But maybe it's a broader question whether this feature is actually necessary, whether it's actually used, and could talk with builders. If we could remove the cancellations functionality by default, then it would be easy to deprioritize lower value bits as a next step. One top one comment on the high and low priority queue are having everyone start in high priority.
00:28:45.040 - 00:29:48.870, Speaker G: I think at flashboards with the bundle, really like years ago we learned that people will just create identities, keys and spam with transactions with blocks that will go into validation and then be just cost resources, but are not good blocks. So that everybody starting in high priority seems probably pretty open to spam. I'm not sure how to solve that, because even if you have a super high priority queue, you still need to go into validation. So I think some client diversity with having like ref or aragon as alternative validation nodes, that could yield like three times validation performance already as well. And yeah, maybe there's some form of pre validation that gives more insight, but in the end, you only know after validation if a submission is a good one. But looking forward to improve this. And yeah, great job.
00:29:51.080 - 00:30:31.410, Speaker H: Okay, yeah, these are two great points on the cancellations. I need to think about it more deeply, but maybe we could give individual builders the option. Are you making use of cancellations? If yes, we can keep the current logic. If no, we can have the more optimized logic in terms of the default hyper EQ, I kind of overstated that a little bit. What I meant is that right now we've identified about 20 builders connected to our relay, and we have a point of communication with all of them. We've established some amount of reputation. There's some lindy in the sense that these pub keys have been there for several weeks.
00:30:31.410 - 00:31:05.340, Speaker H: And so what I meant is that all of these 20 builders that are currently connected can all be kind of set to the high priority queue. But I agree that if a new builder pub key just connects out of nowhere, then that one needs to default to low priority and only after some amount of antisibil. It could be lindy, it could be reputation, just knowing who it is, then we can manually set them into the high priority queue.
00:31:13.290 - 00:32:07.320, Speaker E: I have a question on how ultrasound handles the when if there's a missed slot. So if that missed slot is because of some kind of bid discrepancy or something like that, I guess that wouldn't be a missed slot, it would be a filled slot, but the promise was not the same as what was delivered. What about the case where there's a missed slot because of a latency issue that the relay has with the validator? How is that differentiated? As far as that, the builder does not necessarily have any responsibility there. It's simply that let's say the proposer waited a little too long or there was some kind of connectivity issue between the validator and the proposer. I mean, the relay and the.
00:32:11.770 - 00:32:12.134, Speaker D: I.
00:32:12.172 - 00:33:10.042, Speaker H: Mean, I'm happy to go ahead, Mike, but basically I don't think there's any change with respect to optimistic and optimistic in this case. Bids can be slots can be missed just because of latency. And actually it happens about once a day. Once you become a relay of our size, these events happen and you can have a look on efascan. If you go to efascan IO, I think orphanblocks or something, then you will see a bunch of orphan blocks. And this is a natural process to happen. What we've seen so far is that when proposers come to us and say, hey, I missed a slot and I was the proposer, we do a deep dive investigation and we look at the timestamps in the logs and every single time.
00:33:10.042 - 00:34:08.990, Speaker H: It's basically so far, it's been the proposer who has some sort of bad internet connection and they've confirmed on their side that it was them or it was some sort of Faulties software setup. So for example, there was some sort of misconfiguration with vouch on one of the lido operators. And so we did a whole deep dive there and then we found out that it was this vouch misconfiguration and then there was a rocket pool validator who was running on a home internet connection and it was just a bad internet connection. Every once in a while it would just have randomly 10 seconds of disconnection and they just happened to be very unlucky for this one block. But generally speaking, if there's like an operational issue, we take responsibility for it. We're not for profit. We have a very small amount of money.
00:34:08.990 - 00:34:56.890, Speaker H: So I think what we've said to the lido team is that we'd cover up to ten e of liability for faults that are from the relay. But yeah, as you said, there could be any kind of connectivity between the relay and the proposer. And so far it seems that all of the faults have been on the proposer side. We're running on Google cloud, we have extremely good connectivity. But yeah, it is possible that Google cloud goes down or at just the wrong moment, and we are happy to take on the liability up to tenif.
00:35:02.870 - 00:35:18.162, Speaker E: Sorry, quick follow up there. So does that mean, just so I understand, that when there is a potential issue with a proposer, regardless of the specifics of it, that ultrasound sort of proactively triages?
00:35:18.226 - 00:35:18.646, Speaker C: That.
00:35:18.748 - 00:35:38.400, Speaker E: And once ultrasound makes a determination that this is a builder issue, then and only then, is the builder notified that they might have to do something like pay whatever the bid difference was.
00:35:40.050 - 00:36:47.662, Speaker H: Right? So the investigations that we've done so far is when the proposer comes to us and complains. Now, having said that, what we've also done is that we've done two things. Like, one is that we set up an immediate notification. We have this telegram bot which tells us immediately when we have a missed slot and it happens every other day, that kind of thing, which is the normal kind of rate for orphan blocks. And I expect all the relays that have significant inclusion rate, flashbots, for example, their relay I expect will have, will have missed slots. Now, another thing that we've done is that we've looked historically and we've built a spreadsheet looking at all the missed slots, and we've tried to find patterns and try and understand heuristics as to why this is happening. Now, one of the very interesting things that we found is that about half the missed slots have a block which maxes out the gas limit.
00:36:47.662 - 00:37:38.798, Speaker H: So it's like the gas used is almost 30 million. So that suggests kind of one of two things. Maybe it's a very large block, maybe several megabytes, or maybe it's a block which takes a very long time to execute. And that could be the reason why it gets orphaned, because the attestors just are not able to download it fast enough and execute it fast enough within the few seconds that they're allocated to submit their attestations. And then the blockchain just keeps on moving to the next slot, and then it looks like the block never made it. But it did make it, it was just a bit too fat. Another thing that we're considering doing is kind of trying to see if these are home validators versus professional operators.
00:37:38.798 - 00:38:20.400, Speaker H: So we're expecting the professional operators to have extremely good connectivity to be in cloud servers, and we're expecting some of the home validators to maybe have shakier internet connections. And it's a little bit of a project that I want to do, which is basically every time we have. A missed lot, come up with some sort of heuristic to explain what happened. But this is for future research. We haven't kind of done it yet. So so far the only time where we've done the very deep investigation looking at the logs manually is when we've had a request from the proposer to look into them.
00:38:22.290 - 00:38:23.040, Speaker F: Thanks.
00:38:27.480 - 00:39:14.080, Speaker H: One of the things that we're hoping to do is very similar to the builder side of things. So with optimistic relaying, we actually found possibly like a dozen bugs, builder bugs, or even dozens of bugs because we've reported all these simulation errors and we've told the builders, go fix your simulation errors. And I'm hoping that we do something similar with the missed lots because every time there's a missed lot, that suggests that there's some sort of inefficiency somewhere. And so maybe GEF is a little suboptimal or some other client is suboptimal, and that's what we want to do. Every time there's a missed lot, we want to understand, okay, what was the consensus client, what was the execution client, what was the internet connection? Blah, blah, blah. And try and track down the source of the miss slot.
00:39:34.750 - 00:39:35.546, Speaker C: Yeah.
00:39:35.728 - 00:39:59.948, Speaker A: Thanks, Mike. And Justin. That's all very exciting. Seems like the optimistic stuff is moving along and yeah, especially, I think, all of the extra data analysis that you guys are collecting and doing, it goes really far to give us a better picture of what's going on out there. There's a question in the chat. Debit, do you want to just let's see what you're saying.
00:40:00.034 - 00:40:24.730, Speaker B: Oh, yeah, I was curious. It seems like super interesting that now there is this process where the ultrasound relays actually looking at these bugs and then communicating with builders and also with Validators. I was just wondering if this just didn't happen before these bugs were still there because the builder didn't get the feedback before from anyone.
00:40:25.980 - 00:41:49.460, Speaker F: Yeah, so I guess the difference from before is that if they started submitting, let's say they have an invalid block because that nonce issue, instead of the bid being active, the block submission will just fail and return like a 400 Http response to the builder. So unless the relay operators looked into the logs and tried to evaluate what went wrong, the builders would just see the response on their end and they had to decide whether or not they take action on it. The reason we have, I guess, a more active approach to looking at builder errors is because if there's a submission error, then we demote their pub keys and they're no longer optimistic. And so we kind of reach out to them and we say, hey, you have this ETH posted, but you're in non optimistic mode because of this error. And now they have a stronger reason to fix the bug because then that's what allows us to reactivate them. So I guess that's the difference. I think before probably all the errors kind of just get washed out with the rest of the requests and no one looks too deeply at them because there's a ton of submissions and they fail for a host of different reasons.
00:41:51.480 - 00:41:52.740, Speaker B: Yeah, makes sense.
00:41:52.890 - 00:41:53.910, Speaker C: That's cool.
00:42:02.720 - 00:42:43.580, Speaker H: Yeah. One of the things that I hope will happen eventually is that we have extremely good visibility into the health of the participation rate of ethereum. There's going to be some validators that are offline and we can tell that they're offline because they're not making attestations. And so we expect a missed lot for those validators whenever they are elected as the proposer. But for all the other cases, really, we want to get 100% participation rate. If you are online as a validator, there should be a block there and so whenever there's a delta there, that's an opportunity for improvement.
00:42:45.120 - 00:43:40.750, Speaker F: It's probably also worth bringing up. Another error we've seen a few times is around length one reorgs. So this is kind of like very similar in spirit to a missed slot, but it can happen for a number of reasons. If the block is late, then it might get reorged out by the next block. If the proposal boost of the next block can overwhelm any Attestations that happened on the previous block. Yeah, we're talking and thinking about collecting data on length one reorgs too and seeing if there's some similar patterns there as far as blocks that use a lot of gas or blocks that have a lot of extra data or whatever, like large transactions could result in the reorg. So, yeah, I think overall network health would be better if there were less length one reorgs and it would help with our operational toil of running the relay too.
00:43:46.340 - 00:45:14.330, Speaker H: I guess one of the things worth mentioning is that maybe we can largely automate the attribution of a networking issue. And the reason is that we can look at IP addresses of the proposer that kind of requested the payload with their signature and correlate that IP address with when they made the Get header request. And so maybe what we should be doing is recording in the database as opposed to recording in the logs, which get kind of rotated out, but permanently recording in a database the timestamps of the Get header and the Get payload using the IP address as a good enough heuristic. And then if it so happens that these timestamps are within bounds of what's specified, then maybe it's a relay issue. But most of the time what we've seen looking at the logs is that the Get header, the Get payload was just made way too late and that was the explanation for why there was a miss lot.
00:45:19.440 - 00:45:19.804, Speaker C: Yeah.
00:45:19.842 - 00:46:16.424, Speaker F: And this leads to sorry, we're kind of hogging the call here, but one other thing that we've talked about, and I think there's a lot of interest, especially from the builder side, is making an SSE channel basically for the highest bid to get published from the relay side. So right now the relays get spammed kind of repeatedly with Get header calls because everyone wants to know what the current highest bid is. But instead we can have an SSE where the relays is communicating outwards what the current highest bid is at every millisecond or something. And then the Get header and the Get payload. Or I guess the get header call. We could verify that it only comes from the proposer, which would make the data a lot more easy to reason about because currently we get like hundreds and thousands of Get header calls each slot, which makes the processing a lot harder, I guess. But that's kind of, again, a future extension.
00:46:16.424 - 00:46:19.150, Speaker F: So not there yet, but talking about it.
00:46:30.880 - 00:46:48.370, Speaker B: Yeah, in a sense with this last thing, you're getting closer to the Mev Oracle thing that you provide to the current proposer. In a sense. But yeah, I think these network things on the validator side are pretty cool.
00:47:00.680 - 00:48:01.480, Speaker H: One final thing to answer the question is if we know that we messed up something on our side. So if, for example, we tried to restart a GEF node and that led to some connections being closed or whatever, if there's a known DevOps kind of issue on our side that may have lasted even just 1 second, what we do is that we manually check that that didn't lead to a missed slot on change, just as a sanity check. So in that sense we take responsibility for doing a manual deep dive when the proposer asks us, but also when we know that there's been a bit of an operational hiccup. We also proactively do that, but for the natural baseline orphan rate of missed lots. So far we haven't done a deep dive investigation.
00:48:22.670 - 00:48:23.178, Speaker C: Cool.
00:48:23.264 - 00:48:33.390, Speaker A: That's all very exciting. Is there anywhere people could follow up asynchronously if they wanted to chime in on these designs or the ideas or provide feedback?
00:48:37.430 - 00:48:54.410, Speaker H: I mean we're very easy to reach out to. I'm Justin@ethereum.org, but I have open DMs on Twitter. I think also the ultrasound account also has open DMs. We're also easy to find on Telegram and Discord.
00:49:15.050 - 00:49:25.910, Speaker A: Sukunek x asked if anyone from Agnostic is here. I don't know if there's a more pointing question.
00:49:39.190 - 00:49:39.940, Speaker C: Okay.
00:49:40.710 - 00:49:45.620, Speaker A: Anything else anyone has? Otherwise, I think I'm going to go ahead and wrap up. Pretty.
00:49:52.960 - 00:50:25.640, Speaker H: Mean. One of the things that I kind of want to bring up as a quick research question is around the cancellations is it seems that Enshrine PBS is not compatible with cancellations. I'm just thinking out loud here, but with Enshrine PBS we're kind of expecting the proposer to pick the highest bid that they saw and so maybe we shouldn't be fostering cancellations because it's somewhat incompatible with Enshrined PBS.
00:50:27.020 - 00:51:26.648, Speaker I: This is actually my original market design issue with cancellations. I think it's not an Enshrined PBS issue. I think it's like a fundamental mev issue. We're in an asynchronous network here with non attributability of message delivery running this auction and the time the proposer receives the message that allows them to take the higher value action, expecting them to give up that optionality by essentially forgetting they took that when they can easily claim that they just didn't receive it and no one else can tell. It does seem like Enshrining just a suboptimal market structure from the point of view of mev. Maybe there's other ways to do cancellations, like a more layered approach where the commitments are more granular than those expressed at the PBS level itself. But I think yeah, having it at the PBS level is I agree with your analysis.
00:51:26.648 - 00:51:32.030, Speaker I: It's hard to kind of enforce on the proposer side, and that's a problem.
00:51:33.940 - 00:51:57.692, Speaker C: Right. Cool.
00:51:57.826 - 00:52:08.210, Speaker A: Something to think more about. Any final comments? Otherwise I will go ahead and end the call.
00:52:11.800 - 00:52:35.630, Speaker D: Yes, I just want to ask really quick if anyone from Blockswap made it here. They recently released their implementation of something that has some parallels to optimistic relaying proof of neutrality relay. They have a girly testnet up. I mentioned this call to them and wasn't sure if anyone was going to show up to kind of introduce that.
00:52:37.760 - 00:52:40.588, Speaker A: Do you have a link or something to this project?
00:52:40.674 - 00:52:46.620, Speaker D: Yeah, I'll at least drop a link in really quick. Oh, that looks like Mike got it. Yep.
00:52:51.290 - 00:52:51.702, Speaker C: Cool.
00:52:51.756 - 00:53:11.380, Speaker A: Thanks. Maybe we can get them on the call next time. Well, thanks everyone. Let's go ahead and wrap up and I'll see you all on the Internet.
00:53:12.280 - 00:53:14.016, Speaker F: Thanks for coordinating, Alex.
00:53:14.128 - 00:53:16.230, Speaker C: Thanks, Alex. Thanks.
00:53:16.600 - 00:53:18.756, Speaker G: Yeah, thanks for hosting. This is great.
00:53:18.938 - 00:53:19.410, Speaker C: Thank you.
