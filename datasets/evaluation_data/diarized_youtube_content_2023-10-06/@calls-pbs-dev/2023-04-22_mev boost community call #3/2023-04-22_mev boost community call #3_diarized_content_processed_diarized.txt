00:00:04.530 - 00:00:40.270, Speaker A: Everyone, welcome to community call number three. I will get the agenda here and we'll get started. So there that's in the chat and yeah, it's been quite a busy time since we last had call. Let's just dive in. So let's talk about capella. Generally things went really well. There was an issue that we saw with Prism.
00:00:40.270 - 00:00:48.098, Speaker A: Let's see actually if Terrence is here, I think he'd be a good one to give an update there. Although I don't see him yet.
00:00:48.264 - 00:00:49.474, Speaker B: Yeah, I'm here.
00:00:49.592 - 00:00:53.380, Speaker A: Oh, great. Do you yeah. Do you mind?
00:00:54.010 - 00:00:55.126, Speaker B: Yeah, sure.
00:00:55.228 - 00:01:07.226, Speaker C: So the issue was basically very simple. It's when a Prism validator requested a blind block, it signs it.
00:01:07.328 - 00:01:09.226, Speaker D: But when it returns back to the.
00:01:09.248 - 00:02:14.520, Speaker C: Relayer using the Submit Blind Block Builder API call, we forgot to include the bos to exact change there. So meaning that if there's any block that has greater than zero bos to change, you will get a hash tree root mis publisher postmortem. So highly encourage anyone to read that from the postmortem. We have corrected what we will do moving forward to prevent this type of from happening. But in terms of impact, I believe around like 50 blocks were missed because of this and shout out to all the relayers for being very fast and responsive, we were able to block the Prism user agent stream. So therefore the damage wasn't that bad from that regard. Circuit breaker also came in handy as well.
00:02:14.520 - 00:02:37.336, Speaker C: So we shut off at five miss blocks, five missed slots per epoch. So before circuit breaker we probably would have reached to eight to nine missed slots per epoch. So that also came out handy as yeah, I don't know if it's just.
00:02:37.358 - 00:02:40.328, Speaker A: On my end, but Terrence, I think you were cutting up a little bit.
00:02:40.414 - 00:02:42.890, Speaker C: Yeah, he's cutting off okay.
00:02:46.960 - 00:03:17.030, Speaker A: Either way. Quick recap was yeah, there was an issue with Prism and validating blocks and yeah, essentially there was some quick work by people on this call and others to fix that, roll out a patch and get it fixed. I did want to talk about testing a bit. Chris, you dropped this document about the relay perspective. Yeah, so it'd be nice to have actually, Chris or anyone else, do you want to give an overview there?
00:03:17.800 - 00:04:37.020, Speaker C: Yeah, sure. So we from flashboard and from Ultrasound were observing the ChappellA Fork together with people from prison and lighthouse and at first things looked pretty good. And then we started quickly to notice the additional missed slots and correlating with invalid signatures on get payload calls, at which point the relays couldn't know if it's actually the proposer or not asking for the payload. And then we started jumping into action, trying to diagnose the issue with the CL teams and giving a heads up to the other relays that were mostly on standby around then two. And we quickly were able to narrow it down to being an issue with Prism and luckily we could identify Prism by their user agent so we could prevent any beats being sent to prison proposers. This means even though Proposers running the defaulty Prism version, they wouldn't miss any slots anymore, but just receive no beats and fall back to local block production. And then a new Prism release was rolled out pretty quickly that had the ticks.
00:04:37.020 - 00:05:18.430, Speaker C: Then on the evening of that day on like Thursday, there were still additional missed slots and we tried to trace them down. That turned out to be a configuration issue by proposers actually running the new prison release but having an invalid combination of CLI arcs. And after that was basically settled. It was the missed slot rate stabilized and went back to normal rate. I would say like, that's the quick recap here. Great. That pretty much everyone was on standby and able to deploy minor updates pretty quickly.
00:05:18.430 - 00:05:32.210, Speaker C: And I would also say this was a pretty good teamwork across really CL clients taking pools and everybody jumping into action quickly and resolving that quickly.
00:05:34.980 - 00:06:20.556, Speaker A: Yeah, thanks. I mean, definitely a lot of nice work across all those different groups of people. For sure. Yeah. Does anyone else have anything they want to add on the actual incident? Otherwise, I do want to just talk about testing and how we can avoid this in the future. Sounds like a no. So yeah, again, I think from this, a lot of the relays and probably different staking pools have rolled out, a lot of different monitoring and things like that just to shore know, when things like this happen, how can we react? I know, I think I saw at least lighthouse and prism and I think the rest of the clients are working on this, if they haven't already.
00:06:20.556 - 00:07:28.438, Speaker A: But adding the user agent to the boost request in the builder API just so it gives us when things like this happen and it's scoped to a client, it gives us a little more flexibility around diagnosing and reacting and things like that. So that's really helpful. And then, yeah, ultimately this was just like a bug in Prism and presumably we have lots of testing that would catch things like this. So yeah, at least for me personally, I'm working with some of the testing teams at the F to work either on spec tests for this sort of thing or even end to end test and Hive. So we have this pretty awesome testing software called Hive that does more like end to end or network level tests and it would be like a great place for this type of thing. So yeah, definitely for the next hard fork and really just moving forward with any client release, this is definitely an area to focus on. Okay.
00:07:28.438 - 00:08:19.510, Speaker A: Anything else on Capella that anyone wants to address? I mean, generally the hard fork itself went really well and there were some things here within that Boost ecosystem. But again, people worked quickly and I think resolved all the issues. Okay. If not, we'll move to the next agenda item. Another fun thing that happened the week before this category of unbundling attacks. So I think it's worth just having a high level of what I mean, I can give a high level summary or if someone else here wants to give an overview, I believe this was with ultrasound relay itself. So I don't know if justin or mike are on the call and want to give an overview of what happened.
00:08:26.580 - 00:08:35.680, Speaker E: Is mike on the call? Yeah, I'm here mike, do you want to do it? Otherwise I'm happy to do it, but I think you're better placed.
00:08:36.440 - 00:08:36.900, Speaker C: Sure.
00:08:36.970 - 00:09:56.664, Speaker D: I guess just super high level. Probably most people have read it by now, but basically on sunday night, april 2, we noticed that we got an alert that basically said that a delivered payload didn't match the signature or didn't match the block hash of the block that ended up on chain for that slot. I poked around at it for a little bit but then kind of didn't think too much about it, and then justin in the morning correlated it with a unbundling attack that people were talking about on twitter. So basically what happened is a proposer was able to send an invalid signed header to the relay. The relay sent back the body of the block to the proposer because the signature was valid, even though the signed header itself wasn't valid. So then the relay tries to publish the block, the block is invalid, but now the block has and the block contents have been sent back to the proposer for that slot. So the proposer in the next seconds basically is able to unwind the sandwich transaction.
00:09:56.664 - 00:10:40.616, Speaker D: So basically they had baited mev searching bots into sandwich, attacking some transactions. Now they had those transactions in the clear. They were able to run them backwards and propose a different block with a different header that was valid because that block was valid. It became canonical very quickly and got all the attestation weight for that slot. And so even though they equivocated even though they signed two headers, the valid one was the one that they proposed. That one ended up on chain and they ended up kind of walking away with something around the order of $20 million of searcher money. So that's kind of the high level perspective.
00:10:40.616 - 00:10:57.520, Speaker D: We were able to get them slashed because they did double sign the header, but obviously the slashing penalty was only one ETH. They made a lot more than that in ATAC. So that's kind of the high level overview.
00:11:02.030 - 00:12:02.830, Speaker A: Great, thanks. And yeah, there's various docs that chris put in the chat. One of them is essentially postmortem around what happened. And yeah, again, at this point, I think most people here are familiar with this. I think it's worth mentioning again, what are some mitigations or takeaways moving forward? I think a big one is just if you're listening and you're like a searcher or builder thinking about what risk am I taking on if there are reorg, like things that happen, what can happen to what I'm doing on chain, things like this. So again, generally I think people understand that Mev is a very adversarial space and you should basically assume that if something's possible, it'll probably be triggered at some point. I will say I got us all thinking more about these different types of equipment attacks and I'll just call out this blog post here in the chat.
00:12:02.830 - 00:12:16.980, Speaker A: This is Mike and Francesco. Yeah, again, mike, anything to add? I mean, I think you kind of went through the high level just discussing the attack, but if there's anything worth calling out here, this would be a good time.
00:12:17.670 - 00:13:36.502, Speaker D: Yeah, I guess the big takeaway here is that what made this attack super easy, not super easy, but what made the equivocating block that they proposed end up on chain was the fact that the header that they signed for the other block was invalid. So they didn't have to race and try and compete against the block that was being published by the relay because the relay block was invalid. So at least that part of the attack surface has been patched. And the way we resolved that is the relay beacon nodes now check the validity of the block before they broadcast it. So this kind of protects us against the base case, but the more general version of the attack is where they send a signed header back to the relay that is valid. The relay publishes a valid block and then the malicious proposer gets the block from the P, two P unbundles the transactions and publishes a competing block at the same slot. So they are equivocating in here and in this case now they have to win the race for attestations in order for their unbundling block to become canonical.
00:13:36.502 - 00:14:25.500, Speaker D: So the attack becomes more of a network level thing because they have to essentially beat out the relay proposal. But yeah, it's kind of still possible and under the current Mev boost design, there's not really anything the relay can do to protect against it. So in the blog post we proposed a new idea called Headlock, which has also been talked about by Dan in his post, which is one idea for a solution, but kind of as is Mevboost is vulnerable to this type of attack. The blog post also talks about the relationship to this and enshrined PBS too. So if that type of thing interests you yeah, give it a read.
00:14:29.990 - 00:14:30.646, Speaker C: I guess.
00:14:30.748 - 00:15:45.690, Speaker E: One small clarification on the attacks, I think, alex, you mentioned that the attack happened on the ultrasound relay. So there's a couple of points here. One is that it was a software bug effectively in the Mapboost relay code base. But the other thing is that the attacker could have, and I don't know if other relays have looked into their logs but the rational thing for the attacker to do actually is to actually trigger the vulnerability on all the relays that were vulnerable which was pretty much all the relays. And so the fact that we, in a way, were on top of things and we look at the logs and we correlated things suggests that, yes, the attacker did go through our relay. But a rational attacker would have gone through all the relays and potentially been able to leak all these blocks and make the unbundling even more potent relative to just targeting one of the relays.
00:15:49.550 - 00:16:11.800, Speaker A: Yeah, definitely it points to more of a general hole. Well in this case I think we can pretty squarely say this was just a bug in the relays but yeah, it does point towards this more general idea of unbundling which even has consequences as far reaching as say EPBs not to call out ultrasound in particular.
00:16:14.570 - 00:16:51.680, Speaker E: I guess one of the reasons why we notice is because we have the custom alerting so this is an alert system that was built by Nicholas kind of outside of the flashbot implementation. So we get alerted through a telegram bot as soon as we miss well basically as soon as a proposer signs a header which doesn't make it in the canonical chain and maybe that's infrastructure that we could open source, I don't know nick, is this open source? It is open source. Okay.
00:16:52.050 - 00:16:53.078, Speaker D: But it is open source.
00:16:53.114 - 00:16:58.130, Speaker E: You're welcome to use it if other operators think that could be helpful.
00:16:59.850 - 00:17:02.360, Speaker A: Yeah, sounds cool. Is there a link?
00:17:04.810 - 00:17:08.070, Speaker E: Yeah, Nicholas will be finding a link and posting it in the chat.
00:17:09.290 - 00:17:10.600, Speaker C: Okay, awesome.
00:17:20.950 - 00:17:42.460, Speaker A: Okay, just taking another look at the agenda so yeah, anything else? I mean again, I know this happened a few weeks ago and this was something all of us are very involved in at the time. Anything else on these sorts of sandwiching or sorry, unbundling attacks that are worth mentioning on the call right now?
00:17:46.170 - 00:19:06.266, Speaker F: I did want to maybe just bring up the topic for discussion of the trade offs involved in some of the mitigations applied to prevent these unbundling attacks. So some of the main ones being that relays now wait 1 second before returning the transactions to the proposer giving the published block some time to propagate through the network. The other mitigation being a four second cut off into the slot where if get payload request is received. After that time, the relay will not return it at all and that likely lead to a missed slot. So right the 1 second delay means that the relay has a high responsibility to have their own beacon nodes push out this block and if that fails for some reason then the proposer is going to have 1 second less to get it out and have everything complete in time. And the four second cut off means that if proposers infrastructure isn't as good or they're not able to get out their get payload request in time that might just lead to a missed slot. This has all been tested out and people are looking at how this is playing out in practice.
00:19:06.266 - 00:19:29.542, Speaker F: But I just kind of wanted to bring this up for discussion. How is this looking? The effects on network health versus prevention of these attacks? Are the parameters kind of looking correct for the 1 second delay, four second cut off? What else do we need to consider.
00:19:29.596 - 00:20:40.490, Speaker C: Here to jump in here? I think to me the timing looks correct. We started with a three second cut of time that may have been a bit aggressive but at 4 seconds into the slot there is a 0% chance that the block is landing for the slot. So this cut of time seems correct. Also really have a responsibility to have redundant beacon nodes. That means they should also have better peering overall and the broadcasting from the relays should generally just work very well. So I think generally the four second cut off there is not a reason to push that any later because that would just lead to orphan blocks rather than missed slots. And then the 1 second delay to the returning to the proposer seems okay to me because it gives the block more time to propagate before a proposer could steal a transaction and submit a competing block for the same slot.
00:20:40.490 - 00:21:03.586, Speaker C: Of course this also depends on the relays having good peering of their beacon nodes and running redundant beacon nodes. This is just my opinion. I think the parameters are correct. Maybe 4 seconds is even a bit high. We could reduce them a little bit. But we are also monitoring the numbers on the relays and this is looking pretty good so far. I see.
00:21:03.586 - 00:21:04.930, Speaker C: Hand over to Mike.
00:21:05.910 - 00:21:06.382, Speaker A: Yeah.
00:21:06.456 - 00:22:03.654, Speaker D: Just one small clarification there. So the four second attestation deadline actually there's no guarantee that a block that's published after that will get reorgan. Only in the case where the subsequent proposer is running the forcible reorg logic will it get reorganized. So there is like the estimates are something around 70% of the network is running that right now. So pretty good probability gets reorgan. But a late block could still end up on chain if the next proposer decides not to reorg it. And I just wanted to say that this is kind of like a subtle relationship between proposer boost and this forcible reorg PR which is getting kind of discussed in the spec and made the timing games around the attestation deadline particularly potent as we were responding to the Unbundling attack.
00:22:03.654 - 00:22:18.380, Speaker D: So this is something that I'm writing a piece with Giorgios on. So definitely keep an eye out for that. But yeah, I just wanted to clarify that there's no guarantee that a late block will get reorbed out. Only if the subsequent proposer decides to do that will it happen.
00:22:21.530 - 00:22:22.454, Speaker C: Okay, cool. Yeah.
00:22:22.492 - 00:22:28.090, Speaker F: Good to get a couple of comments there, just wanted to make sure that we chatted about that a little bit.
00:22:28.160 - 00:22:30.140, Speaker C: To the broader community.
00:22:31.470 - 00:23:20.490, Speaker D: Yeah, and I mean it is worth bringing up that all of the changes that we introduced immediately following the attack, many of them we ended up rolling back because they added so much latency that it wasn't worth the risk to the honest proposers. Right. So even if we make the attack slightly harder, even if that increases the risk of an honest proposer missing a slot by like 1%, maybe that's too much of a damage to network health considering a sophisticated attacker could still pull it off. So that was kind of the back and forth that was happening in the immediate response to the attack was mostly us just trying stuff and realizing that it wasn't worth it largely.
00:23:35.710 - 00:24:14.440, Speaker A: Yeah. Thanks everyone. Yeah there are a lot of details here and seems like there has been a little bit of back and forth and figuring out what makes sense and yeah, if anything. Again I think it just points to these being some more longer term research questions around as we move to enshrine PBS how do we make this thing safe and secure? So unless there's anything else there we can just keep moving through the agenda. The next one was something metacris brought up about block cancellations. Chris, do you want to give a little TLDR there?
00:24:15.070 - 00:24:36.714, Speaker C: Yeah, absolutely. Block cancellations, what are we talking about here? This is the ability for block builders to override higher value submissions with later lower value ones. This is used by start ARP strategies, especially centralized exchange.
00:24:36.762 - 00:24:37.360, Speaker B: And.
00:24:41.650 - 00:25:54.878, Speaker C: Yeah, this has huge performance impact on relays because suddenly every single lock submission has to be validated rather than only those with a higher value and 1 second screaming kid incident here. Yeah and stop. Yeah. And in general, there is no guarantees that these cancellations take effect because the proposer can anytime ask a relay for the highest bid, even multiple times, and just find whatever is the highest one that they found and not like they are not like there's no guarantees to the builders that a cancellation is actually in effect. Because you never know when a proposer is asking for the slot for the bids because Get header call is not signed. So any call could be the proposer and for any bidder really delivers there has to be the guarantee that the payload is available. So in general this means it's not really quite incentive compatible.
00:25:54.878 - 00:27:07.390, Speaker C: Proposals are actually incentivized to just watch out for the highest bid whatever and then request that with a signed header. Also in the current PBS designs it is not supported. It cannot be supported because builders just broadcast their headers, their bit in the network and proposers will sign whatever is the highest one that they have seen. So from that perspective there is not a long term future for cancellations either and it would make removing cancellations would make relays in general a lot more cost effective because you would remove over like 99% of all the block validation, which are actually producing most of the latency and most of the cost. So this is a rough introduction. I think in general it would be nice to move beyond block cancellations. I know that some builders that do statistically arbitrage do want that feature and yeah, let's open the discussion about this topic.
00:27:13.060 - 00:27:42.220, Speaker A: Okay, yeah, great, thanks. So, concretely, there's this builder cancellation feature and the idea is essentially to remove it. That would be a change to the Medboost relay software and any other relay software out there, changing essentially the guarantees provided such that you can't sort of cancel your bid in this way by submitting a lower value block. Yeah. Anyone have any thoughts about this? Chris?
00:27:43.200 - 00:28:07.188, Speaker G: Yeah, just a quick question from Metacris on. You mentioned that this would reduce 99% of validation load. So are you observing then that 99% of blocks are not incrementally increasing in value? Is that why you mentioned 99%?
00:28:07.354 - 00:28:41.632, Speaker C: Yeah, that's like a back of the napkin estimate, but yeah, that's pretty much the case. I can follow up with very specific numbers. Mike and I have been collecting some numbers in particular about how often cancellations were used in practice to lower the value of a proposed block. And I will also follow up with more data. The data is, by the way, public about the bits that lead up to a payload. So that's something that everybody can also just run on their own. But from my quick check of the numbers in our infra, it's over 95%.
00:28:41.632 - 00:28:54.390, Speaker C: It's probably around 99, maybe even higher. We receive about 700 block submissions for a given slot, and the majority of these is not way below the top value.
00:28:59.310 - 00:29:35.320, Speaker G: Is a suggestion then that the relay basically it only considers a block submission from a particular builder that is higher in value than what it has previously accepted. And the relay is only therefore going would only need to sort of hold on to blocks for which it has serviced a Get header, and those two combined would then sort of reduce the load on the relay. Is that the general idea?
00:29:37.130 - 00:30:25.250, Speaker C: Yeah, exactly. Without the need for cancellations. For a builder, there really does not need to validate or store any information about that bit because it will never be used again. If a builder has cancellations enabled, you need to validate every single submission and also always store the latest one, because if a higher value bid by another builder is canceled, it may be that your previous lower value bid is actually now the new top value bit. It's like, I'm probably not so good in explaining it, but without the cancellations, there is no need to validate or store the bit content like the hotpath.
00:30:26.730 - 00:31:18.600, Speaker G: Yeah, that totally makes sense. Are there any concerns about what would happen? Would there be an issue with, if not all relays are operating under the same regime. Like if some relays allow bid reduction and some don't, that could potentially drive builders to use bid reduction ones. Specifically. Builders that are doing, let's say, some of these stat ARBs that could eventually end up with better blocks and therefore they can win, even though during the slot period the block value might be going up and down. I'm just sort of ideating here, I don't have an answer or a position.
00:31:21.290 - 00:31:37.290, Speaker C: I think this is a great question and I want to let others chime in like Justin mike, do you have any opinions on that? Just for reference? I think right now we made it opt in on the flashboards relay cancellations and we are seeing about a third of the submissions being with cancellations enabled.
00:31:40.530 - 00:32:57.670, Speaker E: So one of the remarks here that we can make is that cancellation actually starts to break down once it's over multiple relays. And the reason is that for a cancellation to be effective, you need to cancel on all the relays. Because if there's even one relay that's kind of maybe a little bit slow to cancel because it's under load or because it has some sort of suboptimal setup, then actually you won't be able to cancel because the proposer is going to be querying multiple relays and it'll just pick the highest value across the all relays. And so all relays need to cancel. And so I actually see cancellations as a little bit of a centralization vector or at least some sort of incentivizing searchers and builders to only submit certain types of bundles to one relay. So maybe in a similar way that we have a specialized relay for so called ethical blocks, maybe we could have a specialized relay for cancellations. And that could be like a stable equilibrium, because it's kind of a relay that's specialized to do this one thing and the service starts to break down if it's provided by multiple relays.
00:33:09.190 - 00:33:09.940, Speaker A: Sorry.
00:33:10.870 - 00:34:08.710, Speaker H: Yeah, we actually have noticed this recently. I think that Y'all mentioned that we wrote up a document that we're going to be publishing with that where lower value blocks coming in to relays that are optimistic or trust those builders to not simulate those blocks. We see that those lower value blocks are coming in right during the time that a Get header request is being returned. So optimistic relays like ultrasound or anyone that's trusting the builder will have the lower value bid where relays that are similar in those blocks will return the earlier block. Definitely currently, if not everyone's doing the same or not operating the same performance, these cancellations aren't being honored by all relays. But I'm also curious, I thought from reading the discussion on Telegram that the idea was not to have specialized relays, but to have no relays supporting block cancellation.
00:34:13.210 - 00:34:32.320, Speaker E: Right. I think most relays may choose to not support cancellations, but what might happen is that one specific builder whose business model depends on this cancellation feature, they might spin up their own relay, potentially. That could be a possible future.
00:34:41.070 - 00:34:41.578, Speaker C: It.
00:34:41.664 - 00:35:23.750, Speaker A: Right. So we'll never be able to prevent this unless there's some crazy embargo that we've all built with this specialized group of relays. But yeah, assuming that doesn't happen, then we just have to assume this is maybe possible. There's a couple of things here because I think there is an important point to make that has been made, which is essentially looking forward. It's harder to see how to do cancellations with Enshrined PBS, which I believe is a goal we're all generally working towards. And that's just because if I try to cancel something, I've already say gossiped. It's like someone on the other end of the connection could just ignore the cancellation.
00:35:23.750 - 00:36:06.102, Speaker A: Right. And there's not really any recourse, so the feature doesn't have great foundations where we're heading. So that I think, pretty much, again, is a big mark against it. It sounds like it's also a pretty big resource drain on Relays today. One comment I wanted to add here was just we could have a Bespoke API for this if Relays do want to offer it. For example, maybe when a builder submits a bid, you get back some bid ID and then this cancellation endpoint could just take the bid ID. You could then not have to say, oh, here's a whole different new block that happens to be lower in value, simulate it, maybe store it, all of these things.
00:36:06.102 - 00:36:29.534, Speaker A: So that would remove some of the load that's sort of like a very short term pressure valve sort of release. But yeah, again, I think longer term we should really think about like, okay, if this doesn't make sense with EP best, then why haven't really offer it? Chris, do you want to explain? Yeah, I'm not sure what you're referring to.
00:36:29.732 - 00:36:45.220, Speaker C: By canceling through, like, an ID, it might mean that you don't need to store only the latest submission by a builder, but also all the previous ones, which would be strictly worse than just overriding a previous one with a new one.
00:36:45.590 - 00:36:51.442, Speaker A: Right. I assume relays were already doing this, but yeah, if they're not no.
00:36:51.576 - 00:37:08.650, Speaker C: In the hot target, only the latest submission and the builder cancels by submitting a newer submission that's lower value. But then you still don't need to fall back to any earlier ones. They really only keep the latest by every builder in memory in the hot path, like in Redis of Error.
00:37:09.150 - 00:37:13.420, Speaker A: Right, okay. Yeah, that's helpful. Ben, do you have something?
00:37:14.030 - 00:37:35.730, Speaker H: Yeah. Originally when we first added block cancellation, we were allowing builders to cancel blocks by block hash, which was causing us to have to store a lot of data locally, but using the latest bit, obviously it's more performant than doing something like that. There were just lots of performance issues storing all of those headers and payloads.
00:37:42.420 - 00:37:42.784, Speaker C: Just.
00:37:42.822 - 00:39:35.460, Speaker D: Wanted to chime in one quick thing here. So chris and I have been looking at some of the data, I think he mentioned it earlier, and I actually found a few examples of situations where a bid that kind of ostensibly should have been canceled ended up winning the auction because it was higher value than the canceled bid. So basically what I was looking for was situations in which both the winning bid and the attempted cancellation were received before the beginning of the slot and the winning bid was higher than the cancellation and before the cancellation. So all of this kind of adds up to the situation where the winning bid should have gotten canceled by the second bid but didn't. So this could be the situation where validators are kind of playing this game where they ask for maybe five different bids from each relay and then only return the highest among the five bids from any relay. So yeah, the data is kind of still a little murky, but there's a few examples of this and I think in general might try and kind of write up a short one pager on if it looks like people are actually playing this game or not. Which is I think, an interesting data point because from the Validator side, I haven't heard of a good way of protecting builders cancellations because validators can simply ask for many headers and if you start restricting the number of times validators can call get header, then it gets difficult because how are you going to restrict it? Are you restricting based on IP? Then you incentivize validators to just spin up multiple machines and call get header for multiple machines and then only sign, like you're kind of encouraging sophistication in that direction.
00:39:35.460 - 00:39:50.590, Speaker D: So yeah, I guess that's another point of view to take here, which is from the validator end, they are incentivized to do this strategy and we see that it might already be happening, basically.
00:39:54.940 - 00:40:34.740, Speaker H: So I I know that we decided that we shouldn't be signing git headers because it's privileged information. Is there really any real justification for that though? Or aside from just not wanting to do it because the relay, knowing that it's a validator, making that request, just helps the relay send the right header to it, right? Like if the validator wants to make multiple requests and they sign it each time, that's one thing. But is there really any harm to the system if we want the git header to be signed?
00:40:42.630 - 00:41:39.126, Speaker C: I think it's a good question. I just posted a map boost issue from over a year ago, about a year ago where this was discussed. I think part of it was why we didn't add signatures back then, that there was a lot of time pressure on CL client to get everything done, and signing another call was deemed like, only if it's really, really worth the effort. But Another part of the Coin Is That There Have Been a Few People Vocal about Not Giving Privileged Actors more Privileged Information, and Privileged Actors, In This Case Being Proposers that are about To Propose a Slot where not. Requiring signatures means debit information is public to everyone that is interested in whereas the data API afterwards is strictly voluntary and there is no guarantees. But the Get header request is actually like guaranteed information. Just wanted to add a little bit of the historic context here.
00:41:39.126 - 00:41:46.940, Speaker C: I personally am not sure about the overall implications of one way or the other.
00:41:48.510 - 00:42:30.680, Speaker D: I think another big part of it is the builders use Get header to decide how large bids to set, right? This is the auction that's running real time and the builders can see the current winning auction price by calling Get header. So for example, we see like almost half a million Get header calls. Each slot only maybe ten or fewer probably actually come from the proposer themselves. So cutting off that data from everyone else I think would have big implications for the builders too. Not just people kind of collecting data I guess.
00:42:31.210 - 00:42:40.940, Speaker H: But wouldn't that kind of incentivize CLS to want to do that because that turns it into more of a blind auction that could benefit the proposers more to have higher bids come in.
00:42:42.430 - 00:42:59.380, Speaker D: But I guess it kind of incentivizes proposer builder integration like vertical integration because then if you're the proposer suddenly you have great insight into what the auction is and so the builders would want to be cozy with the proposers to enable that.
00:43:00.790 - 00:43:01.860, Speaker C: That's fair.
00:43:03.190 - 00:43:07.730, Speaker D: I think it's a good question. I'm not sure which is the best approach.
00:43:08.470 - 00:43:28.220, Speaker E: It also incentivizes collusion between the builder and the relay because now the relay has this information which can be sold to the builder with plausible deniability, right? It doesn't lead to front running and unbundling and all of these nasty and obvious things.
00:43:31.550 - 00:44:56.470, Speaker C: Yeah, I think one thing in a bit of a different direction here is apart from the Get header polling, there has been discussions about the map boost feature of a Get header subscription of like a bit subscription where proposer could subscribe or even builders. Anyone could subscribe on, relays to a stream of bits and then request a specific one, whatever they would like to have. This would certainly be more performant and also more valuable to the proposers because they don't necessarily need to rely on a single point in time for a given bit but they can actually receive a stream and then choose whatever they want. Which would also not support cancellations obviously. But maybe this is like the logical step in the direction how the game is actually set up because currently there is very likely already Mac boost implementations out there that just repeatedly call Get header, right? Like there is no strong guarantees on cancellations anyway and no way to force them. So maybe the actual way to move forward is just making this not like a privileged path for sophisticated actors, but just a default path.
00:45:00.410 - 00:46:12.446, Speaker E: So if we fast forward to the end game, we should expect these endpoints, either the polling or the stream to get Dossed, similar to Arbitram having whatever it was, 100,000 WebSocket connections open. And one way that we were thinking of doing the antisibal is actually using the collateral from the builders. So now we actually have quite a bit of we have this pretty robust anti sibyl data set for the builders. We know who they are, we know what their pub keys are and we have some collateral assigned to it. So that's kind of a great way, for example, to only open a stream or only allow polling to builders that have submitted collateral or have some reputation otherwise. Because the end game is just that, the relays will get swamped by tens of thousands of connections to the stream and then that could lead to degradation of service.
00:46:12.548 - 00:46:44.458, Speaker C: Now, I concur here, I think the bombardment of streams is you have to take into account this is a purely read only stream. Like on Arbitrum, this was because there was an incentive to push blocks like transactions through 1000 WebSockets at once to have a higher inclusion chance. But here we're talking about a read only stream anyway. So I think this is pretty easy to scale out and it's arguably even easier on resources than the Get header polling. You could also limit this just if.
00:46:44.464 - 00:47:33.820, Speaker E: You play out the latency game to the extreme. Basically, the way that these streams work, let's say you have 100 nodes connected to the stream. Fundamentally you need to send one ethernet packet at a time. And now if the latency game goes down to the, whatever it is, microsecond or tens of microseconds, then people will try and connect as much as possible so that they will be one of the first builders to receive this notification in front of the other builders. Because fundamentally, if you have one ethernet cable or whatever, it is a fiber optic thing, you can't send an information to everyone. It has to be a sequential process.
00:47:38.830 - 00:47:53.726, Speaker C: It not necessarily no, it has to be sequential. I mean, you can easily like, if it's if it's read only subscription, you can pretty easily scale that out horizontally and have like we can investigate the.
00:47:53.748 - 00:48:02.290, Speaker E: Technique, but you can't necessarily guarantee that everyone gets it at the exact same time. And by exact I mean down to the nanosecond.
00:48:03.590 - 00:48:26.010, Speaker C: Of course you can't guarantee that. Yes, but I don't see this being a huge Dos vector, really, because you can still put in pretty effective rate limiting and it's probably just an improvement over status quo. We should explore this in more detail, but I think this sounds solvable.
00:48:28.430 - 00:48:28.746, Speaker G: In.
00:48:28.768 - 00:49:55.884, Speaker C: Particular because it's just a read on the stream. I think we can investigate this more, but I don't see these as necessarily a blocker. So otherwise, one thing to consider is if we just start a game where some relays have cancellations and some don't have it, then there is like a weird set of incentives maybe also builders get more incentivized to spin up their own release to have more vertical integration. Yeah, I wonder whether we should just properties to the next logical conclusion on the way towards TPS to remove cancellations from the release altogether from Fboost. Pretty much. I think anything else gives a certain advantage to specialized sophisticated actors. Any other thoughts? I think blocks routes for some had some different differing opinions.
00:49:55.884 - 00:49:58.290, Speaker C: Like would you care to voice them?
00:49:59.780 - 00:50:20.520, Speaker H: Yeah, ERI's not here. He definitely had some strong opinions on that in Telegram. You guys probably all saw that, but yeah, I can probably say that whatever they decide is the approach they want to take is what we'll take. So I'm sure EA will share in Telegram eventually when they decide.
00:50:28.150 - 00:51:25.190, Speaker E: I mean, Chris, on the meth boost side of things, on the proposal side of things, would it be fair to say that the polling of Get header is like a low hanging fruit that can be merged in pretty soon? And that change alone, which should be done, in my opinion, relatively soon. Because if a sophisticated staking operation listening to this call somehow decides to make this five line change to mevboost, then they might be getting more mev. And so just to keep a fair game for everyone, we should try and make mevboost optimal so that everyone can enjoy the best mev. And this change alone will just nullify cancellations and close this conversation.
00:51:29.370 - 00:51:35.270, Speaker C: Yeah, I would agree. I would be curious if anybody would disagree.
00:51:44.240 - 00:52:36.190, Speaker H: I think that the relays could protect against that by not returning githeader requests until slot start time and then also rate limiting gither requests more strictly. Even if the mev boost starts making requests early. Obviously if there's a version that's out there that is trying to take advantage of places that are doing cancellation, then there will probably be another fork version of MOV boost that goes and that is suggested to be used with cancellation. But even if cancellation goes away, you can still cancel a block with a higher bid. That's the goal, right? The highest bid is always the most accepted one. But I think we'd see a back and forth of forked mev boost versions after that.
00:52:36.560 - 00:52:41.976, Speaker D: Wait, but why would a validator run a fork that gives them lower payments?
00:52:42.168 - 00:53:12.410, Speaker H: Yeah, I guess that's the point, right? If the relay is producing higher mev, but it doesn't let you make requests until after slot start because these builders are not sending to relays that don't include cancellation, then people would want to use the mev boost version that can get them to those blocks, right? So that's the idea, right? If the relays that are accepting canceled bundles or canceled blocks are producing higher mev, then people are going to want to use them.
00:53:12.780 - 00:54:07.720, Speaker C: But this doesn't check out in my view, because this forked math boost would give you less value as a proposer because it takes calculations. I mean, maybe this splits up the really set into those that support cancellations and not and then you have the Map boost. But I think overall the mainline MAF boost would give the higher value. Also I don't see release rate limiting districtly as practically without signatures because for instance, the flashboard infrastructure does not pass IP addresses through. So we can only rate limit based on IP on our auto firewall and we run in AWS. So basically the lowest we can do is like 300 requests in five minutes, I think, or 100, but it's nowhere near practically to rate limit the Get header effectively without signatures.
00:54:10.140 - 00:54:30.716, Speaker H: Right, but like I said, if a relay accepted cancellation, right, and those blocks were higher value and they only accepted githeader requests after slot start time and they limited how many GitHub requests they'll return, then that could be a shift, right? That could be the sell in feature of using this fork of mev boost instead of the main mev boost.
00:54:30.828 - 00:54:54.490, Speaker D: But I still don't understand, like okay, let's say that they only accept Get header requests after the slot starts. Then me as a sophisticated proposer, I just spin up five OG like Memph boost with the cancellations disabled and then I can get the best block even though I had to wait until the beginning of the slot, right.
00:54:55.180 - 00:55:44.570, Speaker H: I think there's ways to game pretty much any solution for any problem in current PBS, right? But the general case, the 90% case of Validators are going to be running a vanilla version of whatever software is available and large staking pools are going to want to behave with relays, right? Like right now people want mev. So if people are gaming the system, people could block those Validators in the future. There's a lot of things that could go back and forth, right? Because that's kind of the game right now. So I guess what I'm saying is more of the 90% case of Validators that are going to be running vanilla software and not trying to game a system.
00:56:18.070 - 00:56:26.200, Speaker A: Okay, so a concrete next step here sounds like prototyping this streaming bids, right. Chris, does this make sense to you?
00:56:27.530 - 00:56:33.446, Speaker C: Yeah, that is one thing. Also publishing a bit more data would be another thing.
00:56:33.548 - 00:56:43.290, Speaker A: Yeah, then the idea is know, you just don't have an opportunity to go with the higher bid assuming you're running on a software.
00:56:49.150 - 00:57:18.214, Speaker E: It's possible though, the ultrasound relay will be implementing an SSE stream and I guess it might be consumed by builders first and that could be a way to test it. And then I think we would be happy to submit a pull request in the canonical repo and have potentially at some point in time give the options for proposers to also connect to it.
00:57:18.252 - 00:57:19.510, Speaker C: Through Mapboost.
00:57:21.690 - 00:57:26.230, Speaker A: Great, yeah, sounds great. Keep us updated as that develops.
00:57:30.870 - 00:57:36.146, Speaker G: For this streaming bid, any proposer could subscribe to it.
00:57:36.168 - 00:57:36.402, Speaker B: Right?
00:57:36.456 - 00:57:42.210, Speaker G: So the relay is going to be supporting streams to maybe hundreds of thousands of validators.
00:57:43.450 - 00:58:20.850, Speaker E: Right. So at any given slot there's only one relevant proposer. So the way it would work is that at the beginning of the slot the relevant proposer connects and then at the end of the slot they disconnect. And basically what we expect should happen is that we have roughly 20 builders listening to the stream plus one proposer. So 21 and if it gets dos for whatever reason, then we can look into the antisibil mechanisms that we have using either reputation or collateral.
00:58:22.790 - 00:58:31.730, Speaker G: So are you proposing that in order for a validator to receive the stream they have to authenticate with their via signature?
00:58:33.530 - 00:58:51.770, Speaker E: No. Well yeah, I guess what I was suggesting is that the default implementation of mevboost kind of connects at the beginning of the slot and then disconnects at the end of the slot so that we're not overwhelmed by the default implementation.
00:58:53.550 - 00:59:28.870, Speaker D: Or we could accompany it with assigned Get header because the proposer still needs to get the thing to sign over. And assuming that we have this SSE endpoint for the bids, we probably would just publicize the bid size. We wouldn't necessarily want to send out the entire execution header to everyone. So maybe that would be a good way forward. Like builders get subscribed by posting collateral, validators Get headers by calling Get header with their signature.
00:59:35.430 - 01:00:43.438, Speaker C: I guess also the Get header polling fallback would still work for sure. But it's a fair question whether providing these SSE subscriptions, what does that mean for a relay infrastructure? We have just spun up exactly this for the launch of Math Share where we have public SSE subscriptions that are not rate limited and not signed. So we are experimenting in production with that right now too. We will make all the software of course also public that would be needed for that on the relay side. But yeah, for sure. It's a good question on what would be the impact on relay infrastructure providers. I do expect it would be relatively manageable but then the only way out for proposals would be to add a signature here and then we are at the same square one where we didn't want to have Get header signatures in first place because of decentralization vertical integration and privileged information topics.
01:00:43.438 - 01:01:07.570, Speaker C: So I guess the goal would be to have the SSC subscriptions public and I guess like the minimal set of information through this would be like UID Value and builder and I wouldn't expect a lot of traffic going through that. Right, like it's a few kilobytes at most per message, maybe a few hundred bytes and then like 50 to 100 messages per slot.
01:01:09.670 - 01:01:17.380, Speaker E: Well Chris, we would only submit the bids that increased. Basically we would only broadcast the top bid if that changes.
01:01:17.750 - 01:01:18.500, Speaker C: Yeah.
01:01:19.210 - 01:01:27.640, Speaker E: And if we want to be super fancy, we can also only publish the Delta. So it could be like, really a small amount of information.
01:01:30.250 - 01:01:47.310, Speaker G: On the builder side. I think you mentioned that you can provide this stream to builders because you know who they are and that maybe they're did. I understand correctly that because they're bonding with the relay.
01:01:49.330 - 01:01:59.586, Speaker E: Right. So we could have like, a builder signature for the builders and we could also have, if we wanted to propose a signature for the proposer and the.
01:01:59.608 - 01:02:04.820, Speaker G: Builder, you can accept that because they're bonding with you, that they have some.
01:02:07.350 - 01:02:10.760, Speaker E: Either reputation or some bond. Yes.
01:02:11.530 - 01:02:46.350, Speaker G: So I guess I might have a little bit of concern that right now there's bonding with the relay for optimistic relay. Optimistic relaying, which is an advantage then to the builder that bonds. And now there's an additional information stream that is helpful to the builder that they only have access to if they bond. And I wanted to inquire if that is consistent with what EPBs would be able to support downstream later. I shouldn't say downstream.
01:02:49.190 - 01:03:37.410, Speaker E: Right. So with EPBs, the way that I'm imagining it is that we have this peer to peer gossip network, and the gossip network will only forward bids that increase the top bid. And so, as any market participant, you can just connect to the peer to peer gossip stream and see all of this information in terms of how the antidos is done. One interesting yeah. The builders would be collateralized in EPBs, so that's one anti sibil mechanism.
01:03:40.710 - 01:03:41.826, Speaker C: So, yeah, I guess it would be.
01:03:41.848 - 01:04:58.010, Speaker E: The same thing like in EPBs, builders are collateralized and that's what allows us to know that their bids are real, first of all, but also that that could be a way to enumerate them. You know, there's only going to be so many and we could put some constraints, I know, at the peer to peer level, there could be a constraint saying that there should only be one bit per, whatever, ten milliseconds per builder or something like that. It's a little bit similar to how I think transactions are broadcasted. So any given address, non spare can only you probably know this better than me, but can only broadcast so many transactions per second, and every time they broadcast, they need to increase the tip or something. So similar types of peer to pin networking antidos strategies could be put in place. But, yeah, I guess for bids is extremely this situation is simpler because you only need to forward the top bid. And so 99% of bids that are not improving the top bid can just be discarded and pruned and dropped.
01:04:58.010 - 01:05:10.430, Speaker E: And then also, instead of having millions of addresses now on Ethereum that could all potentially be sending transactions at the same time, there's only going to be, I don't know, 100 builders.
01:05:12.930 - 01:06:00.030, Speaker G: Yeah, that makes sense. I mean, if EPBs basically enshrines builders having to collateralize along the lines of what ultrasound is doing now with bonding, then that would work. But that opens up another question for me, which is the bonding right now has a limit. I think it's one e and you just go into sort of the slightly slower lane if you have a block value in excess of that. So an EPBs, how would that work? Because builders definitely want to have higher value blocks to their validators if the opportunity exists.
01:06:01.810 - 01:06:56.158, Speaker E: Right, so I actually have an e free search post draft which I'm happy to share with you privately. I should be publishing it very soon, which basically describes some of these design space. Basically, one of the things we can do is we can cap the amount of collateral per builder to, let's say, 32 E. And they're still allowed to make bids that are greater than 32 E. So they can still make, let's say, a 1000 E bid. And that bid will go through. That block will go through if it's valid and the block body is submitted on time and they pay the proposer the full amount, the promise 1000 E if any of these conditions don't hold true.
01:06:56.158 - 01:07:12.260, Speaker E: So if they don't pay the 1000 E. If the block is invalid or the block value is revealed too late, then they would lose the 32 E. And so basically, the question that we need to ask ourselves is like.
01:07:14.010 - 01:07:14.422, Speaker C: How.
01:07:14.476 - 01:08:46.690, Speaker E: Much do we want to penalize builders for forcing an empty block? Like most of the time forcing an empty block is just a lose lose situation. But basically one of the things which is a little nasty is that if you can force an empty block, then that could be used for unbundling. So for example, if you force kind of I don't know, we need to think through this, but there is a design space where we cap the amount of collateral, let's say 32 e and that's how much you need to pay if you misbehave as a builder. And so the reason why I'm looking into this is because I'm a little bit worried about potentially the very rich builders that can afford, let's say 10,000 E for 100,000 e the advantage that they could have over small builders. Because my prediction is that we will see single blocks with 10,000 E or even 100,000 E. And the simple reason here is that at some point in time, there's going to be some hack on very large contracts or even a roll up. There could be a million E hack, and all the mev will be swooped in one fail in one block.
01:08:46.690 - 01:08:51.090, Speaker E: And we don't want these richer builders to have an advantage.
01:08:52.870 - 01:09:02.520, Speaker G: I agree on that. I think we just saw that slashing mechanisms cannot keep up with potential gain. Right?
01:09:12.220 - 01:09:19.150, Speaker E: But yeah, I discussed this in my next e free search post coming very soon.
01:09:27.470 - 01:09:27.978, Speaker C: Cool.
01:09:28.064 - 01:09:29.126, Speaker A: Yeah, sounds exciting.
01:09:29.158 - 01:09:29.930, Speaker D: Justin.
01:09:31.150 - 01:09:45.680, Speaker A: Okay. That was a surprisingly fruitful conversation. I thought there was one more thing on the agenda. Let's see. I don't know. Is Matt here from the Pond relay team?
01:09:46.290 - 01:09:47.614, Speaker B: Yeah, we're here.
01:09:47.812 - 01:09:48.414, Speaker C: Great.
01:09:48.532 - 01:10:08.920, Speaker A: Yeah. So this came up a few calls ago as an example of different relay design and just pointing to different sort of innovation in this ecosystem. And yeah, I thought it'd be good if you could just come and give us a little overview of what you're working on, maybe how it fits into the bigger scene. Anything you'd like to share?
01:10:09.690 - 01:10:16.600, Speaker B: Yeah, totally. Thank you for that. It's an absolute pleasure. Can I share my screen?
01:10:17.790 - 01:10:23.900, Speaker A: You should, yeah. I might need to give you permissions or something, but let's see.
01:10:29.400 - 01:10:31.256, Speaker B: Can you see my screen?
01:10:31.438 - 01:10:33.144, Speaker C: Yep. Cool.
01:10:33.262 - 01:10:56.952, Speaker B: Right? So basically know I'm matt. I'm from BlockShot. I'm joined with Winston here in the room. So we're basically doing an. So basically it's called Pon proof of neutrality, though. It's called relay. It's not really a relay.
01:10:56.952 - 01:11:56.784, Speaker B: Right? So that's the first thing is like I'm sitting in a relays room, I'm going to talk about something like, okay, how are we going to go ahead with less reliant on relay? Forgive me on that one, but that's what we're trying to do. We started this in DefCo in Bogota. Usually there's a lot of talk about Censoring and things like so we start looking into it, what we can do. How can we have a neutralized relate experiment? And it literally started as an experiment from Defcon. Looking at that and we took an approach is like if we want to push things in a little bit sideways, like the pre block and post block and how do we get this? The vitalik approach is like it's an aggregation way to address it. How do we make sure the builders can be aggregated more effectively if we can game the system in such a way that could be incentivized to all the parties. So that's the way that we took it.
01:11:56.784 - 01:12:52.176, Speaker B: So essentially looking for all the things that we did, justin had an amazing presentation about the witness encryption. So we took that approach as know, slicing this into two. So one is like the off chain coordination and the other one is more of an on chain so that's we call the dual framework. So one thing what we did is how can we have this kind of more guarantee on what the proposer is receiving? So the proposal is receiving all this blockades need to be have like invalid blocks and all these kind of things. So we want to have all these kind of things to be validated before even reaching to the supposed relay part. And that's what the off chain does. And the on chain part is more of like how do you identify and serve the parties? Like the pullers are registered, proposals are registered, and there's another party.
01:12:52.176 - 01:13:28.552, Speaker B: So that's more of an on chain registry that we created, that's a full fledged smart contract suite and that also deals with the payout pool and that's kind of an SMI smoothening. But it's more than that. It's a very interesting architecture. So this is what happens now in the proof of neutrality. One way to look at it is like encrypted block with a payment proof and then it just goes through it and the builder gives a bid plus an inclusion list. It doesn't really give all the whole transaction, just the block header. And then it goes through a second price auction and the winning bid will be selected.
01:13:28.552 - 01:14:19.532, Speaker B: The whole thing is currently getting simulated in one slot period. So we don't really have multiple slots right now. And the winning bid will just go to the mu boost and the proposal will just pay the amount to the payout pool. So in a way to look at this is like very close to how the optimistic relay currently they have the architecture, but the difference is there is no penalty for the relayer because it's the guaranteed by the RPBS scheme. Also, it's all getting validated. So what we did at the beginning when we want to do this configuration, this markets to a marketplace, we want to accelerate, this is already the specification that we set inside and select what we want to achieve. We want to accelerate this builder proposal coordination in such a way and also make sure the builders have this kind of strategy.
01:14:19.532 - 01:15:14.988, Speaker B: They always have a strategy, but we just want to make sure they have more and more opportunities coming from the proposed aggregations. And the relays should be kind of an add on feature like nice to have the way that we think internally is like in an IPFS hosted service or self hosted service. You could work the whole system without having a relayer. But with the relay you can actually bring a lot of other things like into this and Metamarkets. But the one thing which is very important is how do we get this kind of loosely connected actors or proposers? As we go ahead more and more decentralization comes, the solar secret comes. These are very hard to come by and they're very less office to get active. What's the incentive for them to be in a kind of this kind of a registry membership table and a smart contract and where they commit, they will sign the builders because of whatever that's coming through the system, right? So that's why we set this kind of a three plus one actor.
01:15:14.988 - 01:15:50.252, Speaker B: The current system has four actors like builder proposer, which is like the builder and the validators and the relayer is the coordinator in between. And this new actor is coming in place is called the reporter, is that watching out everything and its violation will be reported to the on chain. So one thing that we need to understand is like most of the off chain. Coordination will happen. But the on chain is where the settlement for the proposers and the builders have a collateral there and they have a minimum stake deposit that is also being watched. So everything is controlled by on chain right later. So that's where the reporter is completely new.
01:15:50.252 - 01:16:34.068, Speaker B: So since this was an experiment we started, obviously we got to start somewhere. We took the whole software which is available and start like how do we do this, how can we remove, how can we have more direct coordination between the builder and proposers? So we wrote a custom software on the get. Obviously the relayer that we use, a flashback relayer for the testing, I think we're done with that. So just going into Maine so all the things is like the Mu boost and relayer, it's very tightly connected so just have to use that and we use that. Now we're moving away from that. Complete relayer is going to be a new software. So it's not going to be any more relayer kind of stuff, it's more of a sequencer.
01:16:34.068 - 01:16:45.820, Speaker B: So the proposal is out of the box from a boost. We also stumble upon a lot of problems like how do we make changes like nonchain registration and things like that. I'm going to touch on the next slides and the report is completely bespoke.
01:16:46.560 - 01:17:49.544, Speaker C: Before you move on, can I have just one moment because I have to leave and I just want to say one thing, I very interesting experiment and I am curious where this is going. I'm happy to collaborate and I'm looking forward to what you guys come up with. I do want to help you get off on the right foot here and I have a small beef here which is that I think you guys may yeah, I don't know. Let's put it this way, the way you started with the really code base without attribution and with launching this under an incompatible license is not great. I think you should not just take another code base, scrub the commit history and dump the code as your own and then publish it in a non compatible license. So what I'm asking you here is that please have the correct commit history and have the correct license. The flashboard relic code base is under the Httpl license.
01:17:49.544 - 01:18:07.050, Speaker C: This does not permit to release this under the MIT license. And I would ask you that you do not scrub the commit history from the original authors but keep that and just build on top in a nice collaborative open source fashion. That would be my ask.
01:18:07.900 - 01:18:40.150, Speaker B: Yeah, that's fine. As we said, we're just not going to use any more related we're going to throw it out anyway. It's fine. Great, thank you. So looking at test scene we have an on chain component. So the users, all the proposers reporters and the builder, they will register through the onchain smart contracts as the proposer registry. So that's more or less like less than 60 seconds us they drag and drop and they will just go.
01:18:40.150 - 01:19:32.452, Speaker B: So it's very mainstream driven, so we want to make it more adaptable. So how does it work right now? This is like single slot currently you get a blinded block signed blinded block with RPBS from the builder and it goes to the relayer and then the bid will happen. So kindly it's like two plus eight plus two. It's not a latency very good, but we were just working on it within the 2 seconds that you get all the blind blocks and get validated and the second price auction will run it and the biggest bid will go to the proposer. Before that the host service will actually validate with the RPPs. The payment proof is there and then it goes to the proposer proposed sign it. Neither relayer or proposer has any kind of visibility to the block contents goes back to the builder and the builder published first.
01:19:32.452 - 01:20:34.052, Speaker B: And this is one of the things that we noticed when the large bug happened like a few weeks ago. We're looking at this flow. Luckily we had this kind of first builder will publish everything into the blockchain that wasn't the case. So we're also learning when things are coming in the production, right? And the builder will publish things in the production and then sorry, the blockchain and it will go to the proposer and the proposer will just get the block on it. So proposer will never see anything, neither the relay will see any block content before it's getting published. So, having said that all this experiment, what we want to see here is more of like a pre block building and post block building supply chain to be more modular and plug and play. So we have this kind of collateral and on chain reputation timberproof on chain so we can have get more and more optimistic approaches to how the builders can directly coordinate with the proposers and then without having any kind of dependencies with the relays.
01:20:34.052 - 01:21:43.368, Speaker B: But the relays can have additional logic to come into have like subscription based or any kind of additional the biding strategies and just in time kind of strategies or later delay settlement like gas token kind of stuff and on chain sequences, dow sequences, protocols, taking that kind of things. All kind of innovation is possible when we have this kind of an on chain tamper proof information is available for all the relays. Currently it's like one relayer information belongs to them but here it is like everyone can play with that. The current payout pool contract construction is like majorly one inflow is the block payments and then obviously there's a staking contract that the builders will register. They have a minimum staking deposit but then there's another payment flow is like whenever the violation happens, the reporter will slash it and then it will just redistribute to the other pool members and it gets settled on a weekly basis, right? So this is a really interesting contract. We release our audits by the runtime verification. We have a documentation, please read it a detailed walkthrough.
01:21:43.368 - 01:23:03.972, Speaker B: Also we've been given there what is this more of an optimistic path than the PBS in the on chain way that we can do like we want to have this kind of a balancing act and one of the major thing that we would try to address here is remove the fear of loss right for the proposal part and that's where majority of the coordination need to be there because people just say that, oh, all I want is the biggest payment out there. But how do you quantify that? Is that a one time or you can just get the payment in a long term or you can get a later payment, you have other kind of self interest. Who knows? We need to have this kind of more coordination to come so this is the way that we want to see it. And also from the builder side we want to give more opportunity for the builders to have their own strategy that shouldn't leak to any other parties and they will have more avenues, right, so I'm just trying to speedrun it. I know we have a limited time so where we are now is we are trying to work out this kind of new symbol software and we release under the WTFPL license so no one has to worry about it. It's kind of an alternate version for not having any reliance on the relay part like more of a coordination so that's all there. So the relay part is completely gone and that will come.
01:23:03.972 - 01:23:31.072, Speaker B: And also we want to have this kind of blind communication. If there's a blind communication it's like everything is validated. You don't really have to worry about in the relay part. You need to do a bunch of checkings again and again. You can have this push further backward in the pre block building exercise and you can get more and more optimistic way of getting the blocks using the reputation on the smart contracts you can have a different kind of biddings can be enabled in between.
01:23:31.206 - 01:23:31.648, Speaker A: Right?
01:23:31.734 - 01:24:34.544, Speaker B: So what we want to have very close to nothing like a combination of optimistic approaches using the historical data and also enabling the builders have much better avenues. Also we had a lot of issues when we're working with this kind of validator proxy software so we're putting more effort on that side as well. We're trying to put together a plan like to have some kind of a types and to have a different validator proxy software that will be in line with the builder API but an alternate version of me boost the problem is like Meiboost is very destructive. For example, it need to have everything to be registered before we don't really need to because it's already registered on chain so we want to have something which is very compatible with the on chain elements. So that's something that we're doing. So that is the next slide. Yeah, we call that as an Mev plus a very small again, this is the new experiment.
01:24:34.544 - 01:25:11.824, Speaker B: That's me plus that we want to do it, we want to share it with the community as well as soon as possible based on the builder API and plug and play. So we can have NPCs can be plugged and play, all kind of like Dow sequencer can be plugged and play and using the smart contracts. But it should be very minimal and any relays can be built up on top of it, not only the existing relay construction. So that's where we are with the proof of neutrality. As I said, it's not a pure on relay anymore, but it's a proof of neutrality network. That's what we want to bring it close to the TBS. I'm sorry, I just ran out of time.
01:25:11.824 - 01:25:15.836, Speaker B: Any questions? Happy to answer that's.
01:25:15.868 - 01:25:46.330, Speaker A: Okay, we have just a few minutes. Yeah, I mean, thanks for this. Seems quite ambitious. Yeah. At a high level it seems like there's different things with the current web boost construction that we're all pretty familiar with that are still fairly trusted. And obviously it'd be really cool to relax trust assumptions, for example, moving different parts on chain and it seems like a lot of the work here is moving in that direction. So that's really nice to see.
01:25:46.330 - 01:26:04.096, Speaker A: Yeah. One thing is like if you have well, yeah, so there's actually a lot here and I don't think we'll have time probably to get into everything, but one question I had was have you well, first yeah, is there like a website or something? Just like if people want to learn more, is there a place to go?
01:26:04.278 - 01:26:26.744, Speaker B: So website is p onerelay.com. So that's a website, you can learn more of it. I'll just post the link in the chat so everyone can get it. So that's the link. Happy to give this presentation if you want to later. Okay, thanks.
01:26:26.942 - 01:26:45.884, Speaker A: And so a question I had was do you have many timing on all these steps? For example, if you have builders commit to encrypted blocks and you're doing this all in one slot, is there enough time for this to all play out or you're not sure yet?
01:26:46.082 - 01:27:13.268, Speaker B: No, we already have that on testnet. Right. So you can play it with the testnet right now. So currently it's 2 seconds for all the black validation. The bids will close two plus 0.8. There's a sealed bid, is there? So second price auction will close at 2.8 seconds and then about 6 seconds that will be given to get the proposal signature back and then 4 seconds will be used for the propagation.
01:27:13.268 - 01:27:36.430, Speaker B: That's how it works. We have one builder happy to post that information. I don't have it handy. How many blocks that we published and everything. So I think there's somewhere in the yeah, this is the goalie testnets. You can see that how many builders are there? It's actively building. So we're testing now.
01:27:36.430 - 01:27:47.650, Speaker B: Everything is there, but the only thing that we're trying to remove it before the main net is the new sequencer completely ground up, less relay kind of stuff.
01:27:50.900 - 01:28:02.950, Speaker A: Got you. Okay. These block builders, it's not that they're using the Proof of Neutrality Network to use the current Mevboost system and just building blocks in this way.
01:28:03.640 - 01:28:23.870, Speaker B: So, yeah, currently it's used just a mev boost as a validator proxy, but they use the builder reporter. Reporters are using our reporter software. The builder software that the builders software that we created. And the relayer software they're using is the custom version of that we did in the Flash, but that's going to get swapped out in a week or two.
01:28:24.640 - 01:28:25.630, Speaker C: Got you.
01:28:26.320 - 01:28:57.530, Speaker A: Okay, cool. Yeah, I mean, super exciting and hopefully people here can follow along at the website and other places that leads we are at time. So I'm going to go ahead and close the call. But thanks everyone for attending. I think we had some really nice conversations around both these different unbundling things we've seen in the last couple of weeks and then also looking forward with cancellations and again, just hardening this whole construction that we have in that boost. Yeah. Thank you all and I'll see you soon.
01:28:57.900 - 01:29:00.260, Speaker B: No problem. Thank you. Bye.
