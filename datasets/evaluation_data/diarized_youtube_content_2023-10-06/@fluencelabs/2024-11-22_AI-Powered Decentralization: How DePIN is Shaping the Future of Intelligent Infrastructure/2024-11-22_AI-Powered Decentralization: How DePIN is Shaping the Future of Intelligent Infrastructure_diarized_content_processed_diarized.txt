00:00:10.600 - 00:00:29.525, Speaker A: All right. Hello. Good afternoon everyone. Welcome to our panel on AI powered Decentralization. I'm Nick from cointelegraph. I'm going to be your moderator for today and we have amazing lineup of speakers. So let's kick it off by a round of introductions.
00:00:29.525 - 00:00:34.845, Speaker A: And how do you guys contribute to the field?
00:00:35.625 - 00:01:12.919, Speaker B: Sure. Happy go first. I'm Ben, co founder of Jensen. We're building the network for machine intelligence. You can think of us as very low level infrastructure for making machine learning completely ubiquitous in the world. What most people know us for at the moment is building our compute protocol that allows the execution of machine learning operations on theoretically any device in the world that's capable of doing them, verifies that those computations are done correctly, provides programmatic payments and access, and deals with efficiency and scaling. So the idea is that if you have a machine learning task, you want to train a model or kind of fine tune a model.
00:01:12.919 - 00:01:44.435, Speaker B: Instead of going to a large cloud provider and arranging time on GPUs, you can submit that to an endpoint programmatically and you can scale that computation over however many devices you need. You're paying for the operations rather than the time on the GPUs themselves. And we think that process of making access to that resource compute entirely programmatic can be done for all of the resources under machine learning to make it this completely programmatic process that can scale essentially infinitely with the resources underneath, rather than getting captured as it currently does in the centralized world.
00:01:46.185 - 00:02:31.285, Speaker C: Mark here, co founder of Ather we build distributed GPU infrastructure for AI and gaming companies. You can kind of think of us like an aws or a GCP. Main differences being we're 100% GPU focused and we don't own any of the GPUs within our network. We're a deep in that context. And one thing, maybe that's a bit different between us and other deep ins in the space is that we're 100% enterprise grade. So every GPU connected into our infrastructure is sitting in a data center or connected to enterprise grade network infrastructure. And that means that our network is really, really built to serve the requirements of kind of big tech and big AI as they as they look to scale on our compute.
00:02:32.345 - 00:02:46.155, Speaker D: Hi everyone, I'm Rhea, I lead marketing at livepeer. We're the world's open video infrastructure platform. Deep roots in the Deepin community. What we're not is the decentralized Twitch or YouTube. Just clarifying that.
00:02:46.775 - 00:03:24.139, Speaker E: Hi everyone, it's Henry, co founder and CTO of Open And Open is umbrella that covers projects serving the purpose of delivering open information on the open web. And we have two core concepts. First one is information abstraction. What does it mean? It means there are so many different data silos located everywhere. We are trying to standardize all the data so that developers can utilize them for whatever task they're building, such as training AI. And the second concept is Compute Abstract. Just like Jensen and Ether, here they're providing deeping, providing compute resources to developers.
00:03:24.139 - 00:03:31.655, Speaker E: But we need a decentralized structure, a decentralized network to aggregate everything so that developers can leverage it easily.
00:03:33.005 - 00:03:33.405, Speaker B: Cool.
00:03:33.445 - 00:04:03.343, Speaker A: Thank you. Let's proceed to the to the first question and I'd like you all to share your perspectives on that. So how does decentralized infrastructure offer unique advantages for AI applications compared to centralized systems? Maybe you could share specific examples of how decentralization benefits AI in terms of scalability, security, cost, efficiency. Ben, would you like to be first?
00:04:03.519 - 00:04:34.419, Speaker B: Sure. Yeah. I think you hit it with the scalability point. I think there's probably three main points that decentralization as a technology and all the sort of things that come with that can do for the resources under AI. That's what we spend our time thinking about. So scalability, efficiency and automation are probably the big three things. Scalability, as in, if I want to access as much of a certain resource that the world has as I can, ideally I want some kind of just free market over that resource.
00:04:34.419 - 00:05:12.983, Speaker B: I don't want there to be like an oligopoly or a monopoly within that, within that market that cuts off different parts of the market. To me, I want to basically have an order book against machine learning operations or against compute. And I want to be able to go through that order book as far as I possibly can. And decentralization allows us to do that. It allows us to create programmatic markets basically over resources and then allow people to interact with those markets directly rather than having to jump through human world hoops and scale by having relationships with multiple companies, specifically with contracts, et cetera. I think the efficiency piece comes from that as well. So if you have a market over this resource, then you can scale to the right place within the resource.
00:05:12.983 - 00:05:55.769, Speaker B: An example we give for compute is like Bitcoin. If you think of Bitcoin as basically turning electricity into a store of value in some way, it does that pretty efficiently because it provides a way of doing that conversion to theoretically anyone in the world. So if you happen to have access to an electricity resource which is particularly kind of low cost or green or Something like that, you can actually now get a yield on that without requiring there to be like a town next to your electricity source to sell the electricity into. You can do this kind of value conversion. The same thing can happen with AI, and arguably on a larger scale, given the demand for AI in the world. As long as you can take the ability to generate those ops all the way to the electricity source, which is what kind of decentralized tech gives us. Final piece is automation.
00:05:55.769 - 00:06:22.569, Speaker B: We believe machine learning is a transformational technology. It's going to entirely change the way we interact with knowledge. One of the biggest ways it will do that is by shifting a lot of the work that we as humans do to machines themselves. For the machines themselves to do that, they need to act as agents within a certain environment. The environment they can act within right now has a load of human world things that they have to do. If they want to get compute, they have to talk to AWS or something. That can't be the case because it puts a scaling limit on automation.
00:06:22.569 - 00:06:35.845, Speaker B: So replacing those human world relationships with just an RPC call allows that automation to grow. It allows machines to act in a larger and larger environment. As we keep kind of replacing human world stuff with programmatic stuff.
00:06:37.185 - 00:08:05.767, Speaker C: Yeah, I won't kind of add too much to that, but maybe another simple example to help people understand one of the advantages, right? The way that centralized compute scales is kind of in these big blobs, right? Very kind of centralized locations with large amounts of compute in that singular kind of space. But one of the biggest impacts that is restricting some of the scalability and the utility of particularly inferencing, right, Is the latency that exists within these compute ecosystems, right? There's obviously the time that it takes for the model to respond to a question that you may have, but there's also the processing time for that kind of response to get to someone that's kind of increasingly further away from where the compute is happening, right? And when you kind of take that a step further and you think about, okay, what are the ways in which we're going to be communicating with AI beyond just a chatbot? Right now we've got kind of this text to speech and speech to text kind of programmatic layer. We've got this kind of maybe real time rendering layer, right, where we're kind of creating a digital avatar that's kind of responding facially, visually in line with the kind of the inferencing work that's happening underneath. And all of a sudden you get a really kind of Latency sensitive ecosystem. And one of the biggest determinants of latency in these networks is distance from where the compute happens. And by definition, distributed compute networks or decentralized compute networks just tend to be more distributed. Right.
00:08:05.767 - 00:08:25.775, Speaker C: The bigger a decentralized compute network, the higher the likelihood that a user is going to be closer to one of those compute locations. Right. So I think that we do have this requirement for low latency access to kind of AI resources moving forward. And decentralized ecosystems have a really cool role to play there.
00:08:26.275 - 00:08:54.785, Speaker D: Yeah, I'm going to not add too much to that. I think they covered everything. I'm going to comment on it from security and privacy perspective. I think with the rise of generative AI blockchain being, deterministic programming has the ability to provide the guardrails for AI to perform and essentially almost serve as a bouncer for retaining your IP as well as mitigating regulatory risks.
00:08:56.125 - 00:09:43.535, Speaker E: Yeah, I think you guys pretty much covered all. There's nothing left for me to say. But I would like to add the central point of failure. So with increasing, you know, how many times have we see AWS brought down the entire Internet or at least part of it, or Google Cloud was, you know, somehow somebody unplugged the plug and some kind of service rely on just not responding at all. So I think that's one thing we need to solve. With AI being such an intensive application, we simply cannot live with that. We need a distributed or even decentralized supplier or suppliers to ensure that our future digital world does not just rely on single entity.
00:09:44.835 - 00:10:03.615, Speaker A: Cool. Thank you. Now I have kind of a round of questions, particular questions for each of you. I'd like to start with you, Ben. So in Jensen, how does your work in decentralized machine learning open up new possibilities for AI accessibility and sustainability?
00:10:04.755 - 00:10:40.425, Speaker B: Yeah, that's a good question. I think the way we think about the future of AI is quite specific. Like we have a pretty strong view on what we think is going to happen and should happen for machine learning to scale and be completely ubiquitous. And one of the biggest things is modularity and composability of models. We've seen this progression within machine learning of taking what's effective within a model and then just like vertically scaling that as much as we possibly can. You can look at like GPT as a good example of that. We just kept scaling it basically and the scaling hypothesis was that if you make the model bigger, it's going to be more effective.
00:10:40.425 - 00:11:11.157, Speaker B: And that has held true. We think that continues to hold true. But the way that we're currently scaling it is reaching diminishing returns. Basically, there's Only so many GPUs you can put in a single data center before you hit weird limits like cooling that data center. It's not necessarily that you can't afford to do it. The hypers scalers have in some cases almost infinite money. But geographically, can they find a location that can support the infrastructure required to host those GPUs? Can you actually cool a building with more than 100,000 GPUs in it? These are kind of actually quite difficult questions.
00:11:11.157 - 00:11:50.489, Speaker B: And so we're hitting those kind of limits on the way we're scaling. And we think decentralization. And most of the work that we do in machine learning is looking at how do we release those scaling limits by looking at scaling across much larger groups of compute. So look at all of the compute in the world instead of looking between two data centers or inside one data center and build a model for that. So we look at different ways of building models, different ways of optimizing the scheduling of models over those devices, et cetera, in order to allow kind of access in that way. And one of the biggest ways we think that will present itself is a change in the way ML is done. So instead of this centralized model, we'll actually have a look at the kind of individual pieces of ML.
00:11:50.489 - 00:12:19.909, Speaker B: So ML itself you can view as just an effective compression mechanism. It takes data and puts it into model parameters. You don't need to do that centralized. You can do that in many pieces and then you can compose those pieces together. So long term, we see the future of our interactions with knowledge as kind of humans starting to look a bit like the early Internet again, where initially it was read only. You went to, you had a kind of PC which connected to a data center and gave you information. And then other people started making websites and it became a lot more peer to peer.
00:12:19.909 - 00:12:45.121, Speaker B: The same thing will happen with machine learning. It's just the data representation underneath. What's happening is different. It's parameters instead of being raw data in databases. So that's the majority piece. Very briefly, on the sustainability piece, it goes back to what I said in the, in the kind of previous question re efficiency of use of a world's resource. We think right now the way that we use machine learning compute and the electricity underneath it is constrained by these large organizations.
00:12:45.121 - 00:13:26.345, Speaker B: So they're buying electricity from places that aren't necessarily the kind of greenest places to get it because they've put their data centers there, and they need to get the electricity there. If you have an open free market for compute, you can actually shift that compute to the best place on the planet for it to exist. And in a lot of cases, the best place is a green source of electricity, which is miles and away from any kind of human settlements and couldn't previously provide its resource to us as humanity, but now can, because AI workloads actually can be spread across the whole planet and one day beyond as well. So it's an efficiency mechanism that just allows us to use a resource at the lowest kind of overhead, where right now we're doing some pretty big overheads with the centralized data center approach.
00:13:26.885 - 00:13:44.055, Speaker A: Cool. Thank you, Mark. How does the. How does the use of the decentralized compute resources such as ethers decentralized GPUs impact the costs and accessibility of the AI powered applications?
00:13:45.395 - 00:15:13.619, Speaker C: I'm glad that I went after Ben, because actually we're kind of solving a similar problem in a different way. Right? The scaling laws that Ben referenced, you essentially have companies with borderline infinite money and similarly infinite appetite for compute, right? The biggest tech companies in the world, they just know that the more GPUs they can get their hands on, the more data they can get their hands on, arguably the more capable their models can be quicker than their competitors. And this race is one of the most, if not the most important race that I think anyone's ever run, right? And you can kind of look at those. Well, as a result of that race, right, Those of us that don't have infinite money are kind of left sitting with scraps, right? Fragments of that supply that exist in little pockets all over the world that are increasingly difficult to kind of get to get access to. So you can kind of go the Jensen approach, or you can go the Ather approach, which is essentially create this infrastructure layer that allows you to easily onboard those compute fragments and make them accessible again to the market. And that's kind of what we built. And the kind of end result of that is that you have this, a very different unit economics and pricing structure, right? It's very, very different to price your compute.
00:15:13.619 - 00:16:28.555, Speaker C: If you were a compute owner that didn't initially intend to on sell your compute, right? Maybe you bought a bunch of compute for internal workloads, maybe you missed forecast, maybe you underestimated or overestimated how much compute you need, and all of a sudden you're stuck with this really expensive infrastructure that you don't really know what to do with, right? Having access to a service Like Ather allows you to kind of plug in that excess compute and then get it re exposed to the market. And the unit economics of that compute are very, very different to say compute that you just plug into a hyperscaler or that you specifically buy for for on selling or renting to buyers in the space. And then of course, from an accessibility perspective, right, you're redistributing what is effectively lost compute. Right. If there wasn't a network like Ather out there, you know, maybe there'd be a competitor Ather. But unless you have that infrastructure layer that helps these companies re expose their compute, that compute kind of is incredibly difficult to access. And without this layer, a large amount of kind of builders, right, that don't have huge amounts of capital would struggle to get access to compute.
00:16:29.255 - 00:16:45.365, Speaker A: Cool. Thank you. Rhea, Question about livepeer and I want to know how are you guys approaching the use of AI in real time video processing and streaming on decentralized networks?
00:16:45.825 - 00:17:08.365, Speaker D: Okay, so two ways to answer this question. The way we are getting the jobs done is not very dissimilar to how we do video transcoding. So the live peer protocol is obviously incentivizing node participation with high performance. So nodes with low latency rise to the top. Obviously. Shit fails all the time. Sorry for those recording things fail all the time.
00:17:08.365 - 00:17:33.045, Speaker D: You need to. You can send the job to more than two orchestrators so that you can get the job done. Otherwise there will be problems in the stream. However, on the other hand of things, we're also building a product that's going to be released on December 13th where we're trying to learn from the builders and essentially. Sorry, repeat your question. I have lost track of.
00:17:33.385 - 00:17:34.765, Speaker C: Happens to me all the time.
00:17:35.065 - 00:17:43.857, Speaker A: It's all good. So my question was, how are you approaching the use of AI real time video processing?
00:17:43.921 - 00:18:05.515, Speaker D: Okay, yes. So there we go. How are we approaching the use of AI in real time video processing? We are releasing a product on December 13th where it will provide the builders with the ability to build video pipelines and we're hoping to learn through that process. So it's still fairly early for us to answer that question.
00:18:06.495 - 00:18:23.955, Speaker A: All right, cool. Thank you. Henry, my question to you is for RSS 3. How can AI enhance decentralized social content and what role does it play in curation and moderation?
00:18:25.455 - 00:19:26.535, Speaker E: Well, I wouldn't say AI is going to enhance the content, but I think AI is going to decentralize the AI. AI are going to enhance social, decentralized social platforms in general. Because today we are basically using centralized platforms like Twitter, so we don't know how our timeline was being manipulated and we see content that was fed into us. Same thing applies to other platforms like TikTok or whatever you guys are using. I'm not using any of them. And when it comes to curation, I think decentralized AIs, you can opt for a transparent model where it generates or it populates your timeline with more relevant content, or you can just like this federated social network like Mastodon, if you're not happy with certain instance, you can choose not to join them. So you won't be able to see content from that circle of instances.
00:19:26.535 - 00:20:25.235, Speaker E: And I guess the same logic applies to decentralized AI as well. So in the future we're going to be seeing a lot of more models like vertical specific models. So if we're interested, let's say in sports or finance, you can choose top performing decentralized AI models to curate the content for you instead of just purely relying on the centralized entity itself. And when it comes to moderation, I think there is a very blurred line between moderation and censorship. So if we are not playing it carefully, we can easily turn any decentralized models into, or not decentralized any AI models or algorithms into a censorship machine. So we have to be very careful about how are we going to approach this. That is why decentralized AIs are very critical, because we have ways to ensure that decentralized models are verifiable and transparent.
00:20:26.335 - 00:20:45.575, Speaker A: Cool. Yeah, there is quite a bit of complexities and challenges that we are facing and I'd like to ask you about what are the biggest technical challenges you face while integrating AI with decentralized systems and how do you guys address those challenges?
00:20:45.695 - 00:22:14.013, Speaker E: Well, I think the biggest challenge from an engineer's perspective is basically the overhead and redundancy. So I think Jensen is working on that because for now, even the best, the state of art decentralized AI architecture is going to create like a really big overhead which is a waste of compute resources and other resources as well. And it does not necessarily achieve the same level of performance as centralized way of pre training or fine training a model. And the second challenge is basically the lack of high quality data. And even though today we have ways to access many decentralized data sources, data silos, they do not supply the necessarily supply the amount of data we need to achieve like similar performance comparing to let's say LLAMA or not even GPT, because GPT effectively they have used all the publicly available data set on the Internet and they have the capital and resources to acquire private data set to keep improving their data. That's where I think web3 space and decentralization can help us compete with centralized entities. We need to encourage on board more users to this ecosystem and in turn they will be generating more high quality data.
00:22:14.013 - 00:22:23.265, Speaker E: And then we have decentralized training, decentralized compute and then different tools to help developers build even more innovative applications.
00:22:24.165 - 00:22:29.185, Speaker A: Thank you. I really would love to hear Jensen's contribution on that question.
00:22:29.945 - 00:23:18.559, Speaker B: Yeah, sure. I think the three biggest, I think technical barriers or most difficult things that we think about when it comes to applying decentralization to AI are verification, execution and efficiency. So verification being can I trust that this set of operations that I want to be performed have actually been performed? So if I want to use a random GPU somewhere in the world right now, I have to have some kind of agreement with a person who represents that gpu. I need to have a legal contract. I probably rely on relationship and reputation to do that work. But there's a huge scaling limit to that. It's a bottleneck in the human world that we can't really surpass or we can't kind of work through from a purely computational perspective.
00:23:18.559 - 00:23:41.711, Speaker B: We have to use people. And people are rapidly becoming our kind of like limited resource when it comes to knowledge work. Because we can scale AI so effectively just by pushing kind of electricity through it, people become a lot more valuable. And we need to think about when we need to apply people and it isn't necessarily within verification. So that's one big piece. We've worked for many, many years on verification. We have multiple solutions to it.
00:23:41.711 - 00:24:04.291, Speaker B: It's very use case dependent. You can apply this kind of box of tools basically to verification where you need to. Second thing I mentioned was execution. So that's all very well and good. You can verify the operations that are performed on a device, but can you actually perform operations on that device? We can do on Nvidia devices very easily. CUDA is a nice, a nice library. But when it comes to doing them on Apple devices it's like it's a bit harder.
00:24:04.291 - 00:24:35.165, Speaker B: The software stacks a bit more nascent when it comes to amd, intel, et cetera, even new processes. People coming up with completely kind of like neural chips. A bit like what happened with bitcoin mining in the shift to asics. You get ASICS for machine learning, but how you integrate them in the software stack is quite difficult. And we thought about this for a long time and ultimately came down to the approach of building our own compiler stack. So we go from high level framework all the way down to execution on theoretically any device in the world. To allow that kind of scalability to happen, a developer shouldn't think about what device they're using.
00:24:35.165 - 00:25:17.067, Speaker B: They should just think, can I get ops efficiently? And can I access a market which provides me those ops? I don't care if the ops are on a MacBook or on an Nvidia H100. If I get them and they're at the efficiency level I want, fine, I'll pay for them. So that's the execution piece. And then finally the efficiency piece has been hinted at already. But how do you build a model efficiently across these types of devices? It's very different building a very large scale model in a Data center on 100,000 H1 hundreds to building a model over like a million MacBooks. That doesn't mean it's impossible though, it just means that it's different. And the world has spent a lot of time focusing on how to do it efficiently for the 100,000 H1 hundreds and very little time thinking about how to do it for the kind of wider, more heterogeneous devices.
00:25:17.067 - 00:25:42.871, Speaker B: So we focus on that as well. I think realistically, though, in the future, as kind of Henry hinted at, there's a place for those centralized models. Like they're effective. You take the Internet, it's all kind of collected together as structured data in one place. Doesn't make sense to send all of that data to a load of distributed devices. If you've got it in one place, train it in one place. But at the same time, there's a long tail of data that's originating out in the world on all of our devices, on sensors, et cetera.
00:25:42.871 - 00:26:03.951, Speaker B: It doesn't make sense to collect that into one place because the communication cost of doing that is huge. If you can compress it into parameter space locally and then send the parameters, it's actually more efficient, communication wise. So it's just thinking about this as just a huge distributed systems problem and thinking we're going to apply ML in every place we possibly can. We just need a new infrastructure to do that efficiently across these devices.
00:26:04.143 - 00:26:08.015, Speaker A: Cool. Thank you. Mark, would you like to add something on that about the technical challenges?
00:26:08.135 - 00:26:27.189, Speaker C: Yeah, I'll just. I guess I'll add to the verification piece. Right. Because this is a really interesting one when you look at permissionless compute networks. Right. So let's say the verification piece is solved, right. And that you can trust in your system that Someone that is trying to contribute a 4090 is actually contributing a 4090.
00:26:27.189 - 00:27:04.225, Speaker C: Right. Or that the work they're doing is actually the work that they say they're doing. Right. Then you kind of have this like next problem in front of you, which is do I just open the doors for anyone to contribute their GPU to our network? Right. How do I maintain a useful kind of marketplace, right. That has token based incentives and a structure that encourages people to join when I can effectively have just unlimited GPUs on board permissionlessly and potentially just come in to kind of drain the token. Right.
00:27:04.225 - 00:27:33.573, Speaker C: There's kind of a management of your economy there that is kind of quite difficult to overcome when you, even when you have that verification piece. Right. Because obviously when you have decentralized networks, you do need these incentive structures. You do need to encourage people to come into your ecosystem. And if you don't have any gates or if you don't have any kind of bottlenecks to control it, it's a difficult situation. So I think for us that's a big piece. Right.
00:27:33.573 - 00:28:00.727, Speaker C: We try and maintain a certain utilization rate within our network. We try and understand how much demand we are kind of forecasting for our ecosystem and we try to be a supply LED marketplace. Right. We don't ever want to be turning people away, but at the same time we don't want that supply to drastically outpace our demand because then you start to get into a difficult situation from an incentive structure perspective.
00:28:00.871 - 00:28:26.775, Speaker A: Well, thank you. So we have some time for the last question. Ria, I'd like to start with you, then follow with Henry Ben and let Mark finish it up. So where do you see the future of AI and deping heading together over a course of next, I don't know, five years, maybe a year. Because now we don't really know what's going to happen at the end of this week. So let's make it easier.
00:28:28.275 - 00:29:11.249, Speaker D: I think there's a lot that can be said about scalability, data privacy, regulatory stuff. I think what I'm excited to see is a lot of stuff on the governance side. I think that I said this at the start, like blockchain's deterministic nature I think can provide a lot of guardrails for AI. I think more education to onboard the users. I know people touched lightly on it because we were speaking more about the compute networks and like, you know, GPUs and everything. But I think where are these users coming in from? I think more people will be onboarded through the convergence of AI and deep in together, I think we are going.
00:29:11.257 - 00:29:56.785, Speaker E: To be seeing more like smaller models as opposed to today. Everyone is training large language models or general purpose models and there will be. Well, it is already happening because many large enterprises, they're training their vertical specific models to handle specific questions or tasks and they are going to unleash more productivity from our society because back then when we were writing code, we must do it from scratch. But today I don't think any engineers are doing that. They're basically writing prompts to generate the first version of their code. And that means we have more time and more resources and human capitals to do more advanced tasks that AI cannot complete.
00:29:59.045 - 00:30:31.099, Speaker B: I think we'll see. We see two things. One is a progression of modularity, but not in the way that I talk about, in a much higher level way of people just linking like language models together with interesting prompts that feed into each other. It'll probably be talked about as like agents talking to each other. And that kind of idea to us, that's modularity, but implemented with the current tools we have, it's basically linking models together, but doing it at the kind of easiest interface that currently exists. But I think we'll see an explosion of that over the next year where people just experiment and it's going to be pretty cool. It's like a nice proof of concept.
00:30:31.099 - 00:30:59.127, Speaker B: It demonstrates how things will work in the future and it'll enable some use cases that don't currently exist. The second thing I think we'll see is a lot of kind of financial hype within the crypto world. Those things are interesting, they attract people. There's going to be a lot of exploration of how can people have ownership in that sort of future economy. In some ways I think this is good. I think that we need to explore that. In other ways, I think it can be quite dangerous.
00:30:59.127 - 00:31:27.535, Speaker B: The crypto world has a tendency to get very, very, very excited about things a little bit early. If we get too excited, then there's potential for a lot of money to be kind of like sucked out of the ecosystem rather than like redeployed within it, which is the message that most people typically have for that. So I think we'll see it. I hope the tech and the kind of progress will keep up with it and allow it to just be just a growing tide rather than what we've seen in the past where it's a bit of a kind of bubble. But yeah, I think we'll see those two things.
00:31:27.955 - 00:31:28.747, Speaker A: Mark?
00:31:28.931 - 00:32:19.135, Speaker C: Yeah, I think we're very close to this kind of post complexity space. With crypto, I think a lot of the UX challenges we've been complaining about for the past 10 years are going to be abstracted away by kind of agents that are built into the way that we interact with. With crypto. I think the navigation between chains and bridges and gas fees are something that agents have no difficulty kind of dealing with. And I think that that is going to be something that disappears from the crypto space pretty quickly. But I think what's going to come kind of around the same time is a huge amount of just pollution on chain, right? From a large amount of crappy agents executing crappy coins, right. Trading shit coins with each other, just playing into the vanity metrics that we use to kind of measure things online.
00:32:19.135 - 00:32:42.527, Speaker C: I think it's going to potentially be really good for chains like Ethereum, right? As this kind of concept of luxury block space becomes more relevant, as you know, all of these agents will just execute on kind of L2s and layer threes where everything's a lot cheaper. But it's going to be, it's going to be, it's going to be messy and I think wild and I think it's going to get shitty before it gets better.
00:32:42.711 - 00:32:48.835, Speaker A: Love it. All right, thank you so much to our speakers and thank you guys for joining us. Thank you.
00:32:49.495 - 00:32:50.355, Speaker C: Thank you.
00:32:59.335 - 00:32:59.575, Speaker A: Sa.
