00:00:09.920 - 00:01:01.554, Speaker A: Hi everyone, my name is Nick, I'm co founder and CEO at assistor and we're building human driven data economy with small language models approach. We're a Cambridge based startup. We start back in 2022 with thinking like okay, what we can do by utilizing AI for web3 space. And I had a background in AI space. I had a background as a head of ecosystem growth in web3 protocol since 2017. And we found that there is a lot of inefficiency into how protocols and developer facing organization running the processes of supporting developers, building relationships with them, supporting tech documentation. So we start with this, AI agents automating Devrel support, which right now it's standalone business.
00:01:01.554 - 00:01:54.500, Speaker A: We already have live integration with Solana Optimiz with many other guys in this space, and we simply save the money with AI automation by automating documentation part by automation support and other things. But as soon as we start working on these, we start getting requests from big protocols like Solana, like hey guys, that's amazing. But we still have too many requests that cannot be supported automatically by AI. And we start thinking like okay, we can add ticket system, we can process this manually. Your Darrell team can process this request from your community manually. If our AI agents cannot provide support, but then amount of tickets get bigger, especially if the protocol grows on the price. And there is a question like how to incentivize community to provide support, peer to peer support to other people.
00:01:54.500 - 00:03:08.228, Speaker A: That's where we start thinking like how to scale what we are building, how to make it more interesting, and how to put community in the loop. And we found that there is big problem and we experience this by building this use case for Devrel that there is this, we call it last mile challenge with data required for AI model accuracy. And you see these graphs is basically if we take in a base model, they can be quite different, depends on amount of parameters, depends on the budget spent by the company. So the accuracy would be quite different. But as soon as we add into LLMs rack process, they became almost equally in terms of accuracy. And if that's true, the question is like why should I spend hundreds of millions or billions on training large language models or, or keep using this centralized LLMs if I can use open source model or any model, but then add additional knowledge that would be up to date and would be adding this additional accuracy and context for domain specific purposes. And these big tech guys which exist in space, they actually end up with the same conclusions.
00:03:08.228 - 00:04:32.236, Speaker A: Because right now you probably heard about Reddit secover flow with Quora deals with big tech guys, what they are doing, they need community generated content, user generated content, because it provides this last mile accuracy. But they don't want to pay you, they don't want to pay us for using our content, our data, our data sets. And we believe that it should be fixed because current data market for AI, it's kind of broken because there is a misfortune data provenance mechanisms that can allow us to check. Okay, was my data used at some AI model? Was my data used by some specific application? And big tech companies, they don't have any incentive to implement this mechanism because it means that they need to share profits with you. While if there is no such mechanism, no need to prove anything and no any instruments to fairly evaluate and reward data contributors. So to address these challenges, we develop our own chain data provenance protocol that enable an incentive driven network of small language model where deep in here is basically we empowering people to contribute data to each small language model. This model can be stored on decentralized infrastructure, centralized on your personal network.
00:04:32.236 - 00:05:31.100, Speaker A: And then we also tracking who, when and how change or affect a dataset which is behind the model. And in that way, each of us or any user can get fair rewards, especially if the model data set which this specific person was contribute to was actually queried. So there is a mechanism how to even check and fail reward. Let's say there is 100% of data sets behind the model and only 10% of this data is actively used by user, basically querying by users. In this way, we can define which data and who was behind some vision of this data and rewards these people most, while other people as a contributor would be rewarded less. We recently like two and a half months ago, we won optonomosagin Hackathon at East Denver with this solution. And let's go a little bit deeper into the roles and how everything works.
00:05:31.100 - 00:06:10.664, Speaker A: So we have four roles at this moment of time. So you can join ecosystem as a small language model creator. Basically you will define which domain you want to target, which small language model you want set up, and you will be organizing people around this model. So they would be basically contributing data or acting as civilians. Also, we have data contributors. It's people or experts in specific domain zones who have knowledge, experience, expertise. It can be statistic information, user research, like whatever else, and they contribute data to the models which they have expertise in.
00:06:10.664 - 00:06:55.886, Speaker A: Then we have a role of data validators, which is highly important because otherwise we will end up with spam models where people would be just like running for this bounty or reward system or tokens and they would just spam and lower down the quality and accuracy model. So the role of validator is to validate the quality of the data based on different parameters. And then we have a mechanism if something happened and we will discover that the data contributor and data validator, they validated wrong data, low quality data. We have mechanism how to penalties these people. And they also need. There is a staking mechanism to secure the network and other things. And also end users.
00:06:55.886 - 00:07:29.552, Speaker A: They are going to be using applications. Why it is important for them? Because in the future or even now, we have three options. We have Google search or any search engine. We have LLMs which are averagely good and averagely bad for any use case. And we're going to have small language approach. And I'm betting and our team believes that in the second half of 2024, small language approach gonna be super popular and hyping. Because we discovering that large language models with this average results for everything.
00:07:29.552 - 00:08:03.652, Speaker A: That's not something that business and real users want to use and want to have in terms of the user flow. Basically users right now can register, choose a role set up and trace small language models based on the data sets information that they have. Then they can choose different service providers. Right now we are partnering with IO.net in terms of decentralized storage, zero gravity protocol. As a service providers. You can choose basic models that you want to use as a basic infrastructure and also add some crypto payroll.
00:08:03.652 - 00:08:37.557, Speaker A: We are going to add Monad support, Solana support and also EVM chain soon. Then you can customize UX. Right now we support different conversational chatbots. Right now we support web widgets that can be inbuilded in any web service that your company or your team is using actively. And then you are listing it on small language model marketplace. We are going to launch it next week where anyone can find your model, can choose the models which they want to contribute in or join as a validator. That's pretty it.
00:08:37.557 - 00:09:23.614, Speaker A: Right now we are running several use cases. So dev rel use case which I mentioned is already live, it's already generating revenue. We have 20 live integration with different protocols. While we also run in proof of concept with crypto analytics teams with M and a deal flow teams in London and couple other use cases from token utility perspective. So we have tokens that can be used as a vesting and also on a staking. Staking will allow you to governance and affect different decision in terms of each small language model but also on a protocol on an infrastructure level as well. Also we have data ownership register, which basically keeps track records about who, when and how submit some specific information to this small language model.
00:09:23.614 - 00:10:39.970, Speaker A: And it allows us not to put the whole model on chain because we don't think that it's reasonable at this point of time because it doesn't give any pluses or efficiency to the speed of interaction, speed of training and all of this. And also we expect to launch next quarter kind of small language model launchpad, which would be allowing people to tokenize small language models that they want to build. So if you have some knowledge or domain specific background, and people know that you can build and feed this model with your data, and you can organize community of ten hundred people, maybe thousand people around you, they can also stake additional our tokens and incentivize you to build this application, to build this use case faster and support your community in that way. And of course you're going to be sharing profits from this small language model with the community who support it. We see huge market opportunity. So Bloomberg estimates that Genai powered application is going to be bigger markets than app store as it is right now. And there are some companies who are running, some at least targeting this space in different ways, like Sahara, Myshel, OS, Fetya and so on.
00:10:39.970 - 00:11:40.330, Speaker A: The difference that we are targeting, we try to tokenize not only the protocol level, but also each application, each model level where community can contribute and get some incentives for contributing the data that wasn't discoverable before and anytime before. And right now anyone would be able to get exposure to this. So imagine you want to launch your product or B, two C product on a China market. So you would get some small language models that train on B two C data by B two C experts with specific exposure on China market. And there would be such use cases, just like endless amount of use cases that could be done in this model by using our infrastructure, by using our approach. There are several companies, as I mentioned, who targeted this space. The difference between us and them, we have working infrastructure, we have already revenue and we keep running this.
00:11:40.330 - 00:12:11.162, Speaker A: We want to expand to get more exposure with more use cases. So that's my call to action to audience that is here. So first we are hiring AI researcher to work on decentralized vector DB. So if it's something that is interesting to you, I would love to chat after this call chat. And also you are welcome to join the ecosystem as a small language model creator, data contributor and data validator. That's it. You can follow us.
00:12:11.162 - 00:12:14.130, Speaker A: Thank you for your time. Great. Thank you very much. Round of applause.
