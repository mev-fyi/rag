00:00:03.130 - 00:00:22.542, Speaker A: Welcome to Uncommon Core where we explore the big ideas in crypto from first principles. This show is hosted by Su Zu, the CEO and Chief Investment Officer of Three Arrows Capital and Me hasu a crypto researcher and writer. Hey, welcome to the show.
00:00:22.596 - 00:00:23.050, Speaker B: Sue.
00:00:23.130 - 00:00:24.190, Speaker C: Hey hasu.
00:00:24.610 - 00:00:56.654, Speaker A: All guest today is Kyle Samani, the general partner at multicoin capital. And our topic is the two major ways to scale a layer one blockchain and really like how much decentralization is the winning blockchain going to have. Before we dive into this, Kyle, can you give us a quick intro both of yourself and also sort of multicoin and your approach to investing and sort of your time horizons so we can get some context on your portfolio and so on?
00:00:56.852 - 00:01:23.218, Speaker B: Sure. So hi everyone, pleasure to be on the show. Longtime listener Uncommon Core is one of my favorite podcasts. We all do a great job unpacking the fun media debates. So I launched Multicoin in October of 2017, along with my co founder Tushar we in 2017 launched our hedge fund. We added our first venture fund in July of 18. That fund is fully deployed.
00:01:23.218 - 00:02:06.594, Speaker B: We're now deploying out of our second venture fund. Today we manage a few billion across those vehicles and we invest in all things crypto. Our strategy is pretty straightforward. We are a fundamental focused fund. Our time horizon in our hedge fund is measured in six to 24 months typically is when we kind of put on positions on that time frame. Many things we own, like Salana for example, which we'll talk about today, our time horizon is longer than that, but at a minimum we kind of have to underwrite it to six to 24 months. Then our venture funds are obviously buy and hold for five to ten years in terms of kind of thesis formation and what we invest in.
00:02:06.594 - 00:02:56.690, Speaker B: We've invested kind of every layer of the stack all the way from kind of core technical primitives through core financial primitives, through middleware and all the way through applications. We are generally comfortable with all forms of risk. So technical risk, product risk, timing risk, all the kind of the team risk, whatever I can think of probably deals where we've had serious risk in at least one of those categories. We generally get more uncomfortable when you've got two or three of those that are compounding. But in fact, some of those, the best returns are those ones where you in fact compound those risks. And so on rare occasions we will kind of compound those risks. But we prefer to say we're really underwriting one specific form of risk that we think is the core question at hand and then try not to compound too many other forms of risk.
00:02:56.690 - 00:03:05.270, Speaker B: Now again, it's always impossible to do that perfectly. The world is not that neatly cut up, but that's usually how we like to think about risk.
00:03:05.930 - 00:03:13.450, Speaker A: Interesting. Do you have an example for something that you would define as a risk and that you would try to avoid to compound.
00:03:14.110 - 00:04:15.550, Speaker B: Yeah, for example, we invested in zero knowledge stuff, so we invested in Mina, invested in Starquare, and we have investors in both of those since 2018. And we came to the conclusion, again, as I was looking at zero knowledge, I was like, okay, you can replicate, or rather you can prove to someone that you've done a computation and demonstrate the integrity of the computation you did without them having to redo the computation. And if you look at Blockchains, the way Blockchains handle this problem is simply through redundancy and just replication. Just have as many people replicate the same thing over and over and over. And so zero knowledge kind of in a very abstract sense, represents one of the most disruptive, kind of just fundamental changes to the nature of trust minimization that is out there. In 2018, we looked at Starkware and Coda, which were the two kind of really only credible zero knowledge plays. And Starkware and Mina are very different things by all accounts.
00:04:15.550 - 00:05:09.498, Speaker B: And we said, look, if anything is ever going to kill crypto, it's probably this. Our ability to reason about layer one versus L2 at that time was almost nonexistent. Thinking about zero knowledge programming environments and those things, we had no idea how to reason about any of these things. But we said, okay, look, there's a real good chance zero knowledge can kind of just break all assumptions we have right now. We had invested in both of those things at that time where our biggest risk in our mind was timing. I had a pretty strong suspicion that has borne out, I think mostly correctly, that it was too early at the time. But we underwrote that saying we don't care that we're too early because in the event that we're wrong and it's not too early, this can just have crazy impacts through the rest of our portfolio.
00:05:09.498 - 00:05:50.394, Speaker B: And so it's both a hedge on the rest of our portfolio and in of itself, kind of an asymmetric opportunity. And so the biggest risk there was timing. We weren't worried about team or math or anything else. Those guys are the world's expert in this stuff, right? We're not going to underwrite correctness there. Our biggest question was, is your knowledge three to seven years too early? I personally kind of experienced that pain of being too early with my last startup, which was called Pristine. We built software for Google Glass for surgeons, and in hindsight, it's been eight years since Google Glass launched. It was 2013, and it's obvious now that it was too early.
00:05:50.394 - 00:06:24.514, Speaker B: The hardware just wasn't there. And if you look at the Snapchat Summit they just had like a few days ago, it's clear that it's still too early. The stuff still doesn't really work in a consumer friendly package. And so I spent two and a half years of my life doing something that was at least eight years too early, probably twelve years too early. And I remember looking at zero knowledge in 2018 thinking the same thing, thinking, okay, is this too early? And I was like, I think probably 85 90% probability it is too early. But we went ahead and pulled the trigger anyways.
00:06:24.562 - 00:07:31.670, Speaker A: Makes sense. And this also brings us to sort of the question of today and that's basically will the winning blockchain scale and layers and logical sharding or rather horizontally within a single shard. The first of those sort of breaks the sort of nice synchronous composability that we are used to where all applications can interact with each other atomically. But it has the major benefit that users only have to verify sort of that small blob of the state that they care about. And in the second approach you keep the entire state in one huge blob and this retains the nice composability that we have gotten used to, but at the expense of ballooning sort of the verification costs for users. And I used to have a pretty strong stance on this and I would say that I still have. But sort of reading some of your work and seeing the early success of Solana has made me wonder if I am personally diversified enough on this.
00:07:31.670 - 00:07:40.560, Speaker A: As you said, maybe is my risk in this area too compounded that maybe there is more than one approach. So where do you stand on this question?
00:07:41.170 - 00:08:23.694, Speaker B: Yes, one quick clarification I want to make on your comment about the users on the layered approach being able to verify the part of the state they care about. I'm not sure that's strictly true, even in kind of the maximalist sense in that if you own, let's say your assets are on one shard or whatever, but if you end up having to interface with 3510 other shards right now as a user, it's actually not clear how you will actually verify yourself that things executed correctly on the other shards. In a theoretical world where statelessness works, you can get there, but that is still an undefined, unsolved problem space maybe.
00:08:23.732 - 00:08:42.946, Speaker A: To interject there just for a second. So I didn't mean like sort of the sharding that the sharding as in the ethereum two roadmap, but just sort of I see the roll up centric roadmap of ethereum also as logical sharding because if you don't use a roll up then you don't have to verify it, but it still scales ethereum as a whole.
00:08:43.128 - 00:09:19.280, Speaker B: Right, okay. Yeah. So slightly different definition. And again, I just want to make sure we're very clear. I would actually argue. So sharding maintains logical centralization because the difference between sharding and roll ups, right, is that in a shard, if you execute in, like, near or polkadot or cosmos or whatever, theoretically, in these systems, if you say, hey, go interact with this transaction on this other shard, the inner shard protocol will figure it out and just do it for you. And the issuing transaction does not need to know or care which other shards the pieces of state are on.
00:09:19.280 - 00:09:47.110, Speaker B: And the Sharding protocol itself handles that. Magically. The difference between roll ups and Sharding is that roll ups by definition break that because the layer one system does not know that L2 even exists. You cannot automatically route that through the logic of the Shard itself. So roll ups break logical centralization. Sharding theoretically maintains it. Sorry, I know, it's just very nuanced kind of technical wizardry.
00:09:47.110 - 00:10:51.180, Speaker B: So the original question was yeah, like layered approach versus kind of horizontal scaled single approach. So I think answering this question is a question of like what trade offs are you making and what are you prioritizing for? I think the right thing you have to prioritize is sufficient decentralization and then optimal user for some minimum level of decentralization which primarily gives you censorship resistance. That's the property you're really getting. Have some minimum threshold of that and then beyond that threshold do not optimize for decentralization more and instead optimize for developer experience and user experience. That is how I think about that. The problem is that as you decentralize to the maximum degrees, you go just further and further down the decentralization curve. You create engineering problems, you create developer experience problems, user experience problems.
00:10:51.180 - 00:11:10.430, Speaker B: And the theoretical solutions for these problems are things like Sharding and roll ups. I'm not convinced that you need to go that far down the decentralization spectrum to make these things sufficiently censorship resistant to achieve the kinds of properties you want out of these systems.
00:11:11.410 - 00:11:23.800, Speaker A: Yeah. So maybe a good question to ask is what is enough decentralization? I mean, lots of people probably have lots of different opinions on this. Is there like a first principles way to approach this?
00:11:25.450 - 00:12:20.682, Speaker B: Yes, I think probably the best thing I've seen written on this, right, is maybe that blog post ballot wrote back in, I want to say 2017 on quantifying decentralization. And you can quantify it across lots of metrics. Probably the most obvious ones are stake distribution, hash power distribution, number of clients or number of major implementations and obviously stake or hash distribution of those, number of validators in the consensus group, the number of validators or miners who can impact liveness. So that's one third in proof of stake or 51% in proof of work. That distribution. Those are probably the metrics that matter. And then maybe just like general wealth concentration for general egalitarian equality purposes, I don't think there's any others that seriously matter.
00:12:20.682 - 00:13:01.078, Speaker B: Those are probably the five or six that matter. Of those I think you can probably stack rank them to some degree. If you ask Anatoli from Solana, he'll tell you the one that matters is number of consensus validators. That can get you to one third of the stake because that is how you freeze, that's how you impact censorship resistance and how you can theoretically roll back the chain and create liveness problems and make the system fundamentally less usable for its intended purpose, which is DeFi. And that to me feels like a very clear and reasonably objective way to think about it. May not be the correct way, but it is at least a cogent view of the problem.
00:13:01.164 - 00:14:11.994, Speaker C: Yeah, I think to add into what you're saying there too, I totally agree. And I also think that decentralization is often a very emotional word for people because a lot of people, when they come into crypto, they think that it needs to be a certain amount of decentralization for it to have any value at all or have any meaning at all. And I mentioned in one of our earliest uncommon core podcasts the idea of a spectrum. At that time I was talking about centralized exchanges and comparing CME futures, trading against FTX against derribit, against Bitmix and these kind of concepts, and then saying that people, they think too much in terms of absolutes, like this will kill that or this will kill that. And I think the reality is it's all intersubjective. If the market demands a very high standard of decentralization for a specific task and it ends up doing so, then that may make sense for that. But for a lot of what people currently do on DeFi, and a lot of what people might use blockchains for in block space for, there's definitely a concept of overkill.
00:14:12.042 - 00:14:12.720, Speaker A: I think.
00:14:15.010 - 00:14:27.570, Speaker C: That you guys have been very smart to this thesis of this idea that doing things on chain is fundamentally useful and that if you go a little bit further on the spectrum, you can get a lot more done.
00:14:27.640 - 00:15:37.880, Speaker B: Basically, yeah, it's obviously a spectrum. I think a year and a half ago that discourse was nonexistent. I think today it's reasonably existent and I think a lot of people have real open questions about how much decentralization is enough and which vectors really matter. Definitely one that matters is number of validators can get you to a third of the stake weight. And that needs to be if you look at, like, e two, because there's no native delegation, you have to kind of suss that out between the stated number of validators and then the number of validators who are controlled by Coinbase or controlled by Lido are controlled by Kraken or Binance or so that's kind of an interesting sub point. The other one to think about is and that metric is specifically important as you think about liveness thresholds and what number of people can collude to impact liveness of the system. The other real fundamental property that matters is censorship, resistance, and there basically all you need is just more validators, validating the system.
00:15:37.880 - 00:16:31.786, Speaker B: And as you go from 1000 to 10,000 to 10,0000 validators, you're just getting more sense of persistence because basically if anyone tries to block a transaction or insert an invalid transaction. You just need more and more people watching them and more and more people in the consensus group such that the transactions will get included and be verified. So those are probably the two most important ones if you look today at those. Let's talk about the second one for censorship resistant. I think that's probably the more important one on kind of a grand human history perspective is just make sure you can't be censored how many nodes need to be in the consensus group such that collusion is sufficiently difficult, such that you can get your transactions included. And my intuition there is probably the number is 10,000. I mean, look at this.
00:16:31.786 - 00:16:54.122, Speaker B: It's very subjective, right? But if there's more than 10,000 nodes around the world and you know that they are physically distributed, or you have reason to believe that, then what's the probability that half of them, two thirds of them are colluding? That you're not going to get your transaction included in the block? It just seems very hard to foresee that kind of large scale collusion.
00:16:54.266 - 00:17:12.870, Speaker A: Is it really realistic that any system, whether it's proof of work, but even more so proof of stake, that it will ever have 10,000 distinct participants in the validator set because of all the economies of scale that are involved with staking and mining?
00:17:14.330 - 00:17:24.940, Speaker B: Proof of work? Well, it depends on if you're talking about individual people mining versus the hash pools. If you talk about individual miners, I'm fairly certain there's a lot more than 10,000 people who mine today.
00:17:25.630 - 00:17:27.980, Speaker A: Right. But they don't make their own blocks. Right.
00:17:28.510 - 00:17:28.970, Speaker B: Correct.
00:17:29.040 - 00:17:33.278, Speaker A: Outsource this to the mining pools. This is unlikely to ever change.
00:17:33.444 - 00:18:02.082, Speaker B: Agreed. That basically 0% probability that will change in proof of work systems, in proof of stake systems today, if you look at polka dot, I think polka dot is something like 800 validators. Or thereabouts on the main net, I think Kusama is like 1200 or something somewhere in that range. Salana is around 600 validators on main net and around 1200 on testnet. Cosmos and Tezos, I think in Algorand are all in the 1000 ish range right now, maybe 1500. But none of them are 8000 to my knowledge.
00:18:02.146 - 00:18:05.554, Speaker A: But validators in those systems don't say anything about who owns the stake.
00:18:05.602 - 00:18:05.814, Speaker B: Right.
00:18:05.852 - 00:18:14.278, Speaker A: A validator just represents one fixed amount of stake and they are all the same size. Or is this different in Salana?
00:18:14.454 - 00:18:23.490, Speaker B: So I'm not sure. I know on Salana there's 600 nodes participating in consensus today. So they have stake and they've staked it and they're participating in consensus.
00:18:23.510 - 00:18:28.160, Speaker A: But they could be and are probably controlled by a smaller number of people.
00:18:29.170 - 00:18:53.378, Speaker B: Yeah, there's possible there's individuals running multiple nodes. Yeah. What's nice in basically all the systems other than E two is there is no native delegation. Excuse me? E two does not have native delegation, but the other ones do. The motivation for having many nodes to represent a single piece of stake is reduced substantially when you have native delegation.
00:18:53.554 - 00:18:54.280, Speaker C: Yeah.
00:18:55.210 - 00:19:23.962, Speaker B: So you're kind of in that range today. You're in the 1000 range, plus or minus for most of these proof of stake systems. And getting to 10,000 doesn't seem very hard to me. Like a ten x is just like pretty reasonable on a three to five year time horizon. I'd say the probability is in my mind is like 85, 90% that these things have more than 10,000 individual consensus validators in three to five years time.
00:19:24.116 - 00:19:51.500, Speaker A: And how does it work that they may be in the consensus set, right, but do they really participate in, let's say, the making of the next hundred blocks? Because I remember in BFT, for example, in the BFT based proof of stake you have sort of this hard upper cap of like 100 validators who can participate because the communication overhead between them is so large.
00:19:53.710 - 00:20:44.054, Speaker B: Well, a few things to unpack here. So one is assuming you have 10,000 consensus validators, assuming they were perfect distribution of stake, then obviously you're only participating on average one out of every 10,000 blocks. So if your threshold is out of 100 blocks, then that doesn't really work. Second, comment on your BFT comment, it's not quite correct. In all these BFT systems you have to trade off Liveness for safety. Well, either you have to choose prioritizing for Liveness or prioritizing for safety, right? If the network splits, then do you stall or do you keep making blocks and then eventually re merge somehow is the fundamental question at hand. Tendermint, which I'd say is probably considered kind of the gold standard of proof of stake systems prioritizes safety over liveness.
00:20:44.054 - 00:21:34.650, Speaker B: So it does in fact halt. And what that means is every single block on a block by block basis, every single node has to communicate with every other node so that they can finalize that block before moving on to the next block, which is the messaging overhead you just alluded to in systems like Salana and Is. Well, ETH too. They prefer Liveness safety and so that messaging overhead does not have to happen block by block. Like you can make more blocks into the future even if a block isn't finalized. And so Solana does this and I think most of the other liveness focused proof of stake systems do as well, where basically that communication overhead can afford to fall behind. And the impact of that just means the time to latency may increase, right? And maybe 1 second, it may go to 3 seconds or 5 seconds, who knows, depending on network conditions.
00:21:34.650 - 00:21:49.338, Speaker B: But it doesn't prevent the rate of block production. And therefore it also means that you can increase the validator set and keep block production will get the same pace. What that will increase is latency to finalization will increase.
00:21:49.514 - 00:21:54.494, Speaker A: Yeah, I wasn't sure if solana favors safety or liveness. So that answers it for me.
00:21:54.532 - 00:21:55.120, Speaker C: Thanks.
00:21:55.670 - 00:21:59.794, Speaker A: Yeah, you were talking about decentralization of the validator set, right?
00:21:59.992 - 00:23:09.834, Speaker B: So most of the non e two proof of stake systems today have 1000 ish validators plus or minus a few hundred. So the question is, can that grow? And there's no theoretical reason why they can't. There's just kind of questions of like, do more people want to run nodes basically, right? And my intuition is, just as these systems grow, if you look at Bitcoin, look at Ethereum, those are the two oldest ones in basically every dimension. They have continued to decentralize over time. And that's been a relatively monotonic process in terms of political control, of kind of the governance of these systems, in terms of the number of applications built on them, the number of nodes even, just like who makes them asics, right? I mean, just kind of in every way these things have evolved decentralized over time because basically as the aggregate dollar value of the system grows, there's just more and more incentive for random people to get involved in some way, shape or form. And so a growing market cap, I would argue, generally increases decentralization. And I think that trend will continue.
00:23:09.834 - 00:23:46.974, Speaker B: I don't really see why that won't continue. Even things like stake distribution, there's a lot of people who invested early in ethereum, who owned a huge percentage of ethereum, like Joe Lubin obviously owns a massive percentage of ethereum. I don't know if he still does, but he certainly did at one point in time because consensus was burning like $100 million a month. He just had to own a huge amount of ether to underwrite that. Even look at criticism of Solana like, oh well, Multi Coin and Alameda own too much, okay, but we are forced sellers at some point. Literally. Our fund has a life.
00:23:46.974 - 00:24:06.330, Speaker B: We have to return the money. And so I kind of don't even things like stake distribution have to get decentralized over time. As long as market cap is growing, I think basically all metrics of decentralization kind of have to move in the right direction.
00:24:06.750 - 00:25:46.620, Speaker A: Yeah, I generally agree with your comment that decentralization increases over time and that it's a function of how many people care about the protocol and this is sort of the biggest driver ahead of any technical properties. And those are actually also the two that I would say, like, you touched on them in your longer explanation, but you didn't mention them explicitly. I'd say that the political governance of these systems is definitely very important. So who decides the roadmap? Like, how difficult is it to change the consensus rules? And this is I think, also at the heart of the debate between these two approaches is the culture of validation among users. Because it's true that sort of you need a certain threshold, need to pass a certain threshold of malicious block, block producers and the block producer set in order to corrupt liveness and safety in these systems. But even if sort of this threshold is reached, then if many users sort of validate the state transition of these networks, then the evil that these block producers can do is much more strictly limited. So my question to you would be sort of is this something that you're willing to sacrifice or how much do we have to sacrifice? This property of sort of non block producers also validating the chain, keeping the block producers in check?
00:25:48.190 - 00:26:48.030, Speaker B: Yeah. So again, this varies in proof of work versus proof of stake to some degree in Bitcoin, right? In Bitcoin it's particularly weird because you had like four mining pools or whatever that control more than half the hash power. And if I recall there was an episode, I want to say it was in 2015 or 2016, where the miners actually stopped. They started producing invalid blocks as some sort of shortcut to increase their hashing or something and the full nodes ended up catching them. That fundamental need for more validating nodes is fundamentally important. What's interesting in Proof of Work versus Proof of Stake is that dynamic exists to a lot lesser degree because you don't have this massive capex spend where your goal is just to juice your hardware as much as possible at the expense of other people. In proof of stake, you just have probabilistic rotations based on stake weight.
00:26:48.030 - 00:27:44.420, Speaker B: And so those kind of fixed sum dynamics of I increase my hash power with some game at the expense of everyone else. It exists to some degree in proof of stake, but to a substantially less degree. So that dynamic is kind of reduced. The other comment is just doesn't matter. Even if you assume there's no one verifying consensus validators other than consensus validators, and it's not clear to me, the answer to that is yes. If you've got 20,000 nodes in consensus or 50,000 nodes or even, let's just say 10,000 on the low end, can you assume enough of them are honest, that it keeps the system in check? Because the good thing is if anyone produces an invalid block that's easily, slashable incentiveship is just a function of node count and then live. This is just a function of stake weight up to one third.
00:27:44.420 - 00:28:37.330, Speaker B: And so if your focus is as a user, I know my transaction will be included, then you just need more nodes in the system to maximize the probability. If your concern is someone screwing with the system, again, you just need more nodes. Whether their consensus or not actually is not super relevant as long as there is slashing built in and as long as some nodes can identify that and submit the invalid proof to the rest of the nodes. And then the third is just will in fact there be some sort of what's it called, liveness attack, right, where you get a large amount of history, gets rewritten and that's actually the hardest to solve. That's actually the highest bar of all of these to solve because it's hard to force stake distribution, especially among the top validators.
00:28:37.670 - 00:29:49.930, Speaker A: Okay, so it seems that we started from this point on the one hand we said all these systems started as completely centralized and they decentralized over time. But now at the same time we are arguing there's this counterforce on sort of the meta level, not inside the individual projects, but sort of between them where more projects come online that sort of erode these ideals of decentralization that have emerged in the community in order to get something out of it. Get better user experience, get better developer experience. Be a better platform for DFI. So sort of where does it stop, Bret? Is there like in two years from now, sort of the more user friendly, the more scalable Solana that sort of instead of having supporting 1000 validators will just say okay, we decided that twelve validators, geographically distributed, sort of like libra that this is enough. Or at what point do the users feel sort of that until here and no further in terms of eroding decentralization?
00:29:51.790 - 00:30:36.166, Speaker B: Yeah. So a few kind of comments around this. The first is should a protocol prescribe a level of decentralization? E two does prescribe a level of decentralization. There are 64 shards, they prescribe how many, what's it called? Like the hardware requirements per? Is there's ideological dogma built into the protocol that's represented in shard count as well as hardware requirements per shard BSc the same thing, obviously direction, but the same thing. Right?
00:30:36.268 - 00:30:36.920, Speaker A: Yeah.
00:30:37.690 - 00:31:34.890, Speaker B: Interestingly, Solana actually does not prescribe anything at the protocol layer at all. Salana does not prescribe hardware requirements. Salana does not prescribe node counts of anything. Salana protocol lets all of that fall to the market itself. Now Salana, the protocol does happen to be optimized for GPUs, which do happen to run on 4000 concurrent cores. And I'd say the one thing Salana assumes is that you have a reasonably high bandwidth computer just so that the proof of history and all the messages related to that can go in and out. But beyond that it really assumes nothing about the node count or the hardware accounts and all of those decisions about what is the degree of hardware you need to keep up with the parallel transaction execution and what is the degree of hardware you need to keep up with the proof of history, with running the hashing cycles.
00:31:34.890 - 00:32:51.700, Speaker B: Those are the two most important questions to actually answering how decentralized is it? And those are not prescribed in the protocol whatsoever. Those are exclusively decided by the users, by the market, where that's some combination of non staking users and then people who stake to validators and then the validators themselves. Right? And then I guess there's obviously the soft social power of like what does the Solana core team say about what they recommend, what does Sam or what does Kyle have to say about those things as well? So there's all that. Kind of soft social discourse, but the protocol itself says nothing. Now that's kind of a technocratic answer, but I think it's worth noting the better realistic answer is in practice like the Solana Foundation, they have recommended computer specs on the website and most of the validators today do in fact adhere to those specs. And so if you try and join with a lesser computer you won't keep up. So there's obviously some practical reality here, but it's worth noting that all of these dynamics around does it decentralize or does it centralize over time are not in any way dictated by the protocol, it's only dictated by the market.
00:32:52.390 - 00:34:16.270, Speaker A: Yeah, but there are reasons, right, why all other protocols sort of have these, they cap sort of stuff like throughput state growth bandwidth requirements and that is for one to sort of protect the non mining users, the non staking users, because their private benefit from validating the state transition is quite low. Right? It's just I want to make sure that I'm on the right chain and that's basically it. Right? But for them the incentive to do this is quite low as long as enough other people do it. And that's why you sort of have this verifiers dilemma on all layer one and L2 blockchains. But the second one is also to and I think this is a bigger deal in proof of work than proof of stake. But I might be wrong, which is sort of also protect the weakest of the miners and stakers because in proof of work we have seen this if there's this theoretical attack vector where larger miners, their blocks have longer propagation times and so they want to mine blocks that are as large as possible. And so if you leave the block size to the free market then sort of the steady state is that blocks will just keep growing.
00:34:16.270 - 00:34:30.290, Speaker A: Validation like the propagation times will keep growing. And this is basically selfish mining attack vector, like an automatic one. Do you see any of those risks in Solana?
00:34:33.910 - 00:34:42.950, Speaker B: So I'm not worried about the selfish mining kind of a thing. Proof of stake kind of naturally solves that with kind of guaranteed timing of moving between nodes.
00:34:44.250 - 00:35:04.000, Speaker A: But nodes, I mean the validators can miss like they have slots, right? So in all liveness favoring proof of stake systems there are these slots where your node has like, let's say, I don't know, 1 second or half a second in Solana time to produce a block and if they miss their slot then they don't get the reward and get like sort of a micro slashing or something.
00:35:04.690 - 00:35:22.706, Speaker B: Yeah, so they don't penalize you for liveness failures, at least not like on an individual basis like that. Same is true in ETH as well. But yeah, I mean conceptually if you're not online and ready to go then you're going to miss your you don't have the hard requirements as a validator. You are going to miss your can.
00:35:22.728 - 00:35:31.430, Speaker A: Blocks get so large that sort of the smaller nodes fail to produce a block in time? Or is this like totally outlandish?
00:35:31.850 - 00:36:08.414, Speaker B: No, it's absolutely a real thing. In fact, if you go to Solana Beach, which is kind of the main Solana Block Explorer network overview thing, if you go to the list of the validators, I think there's a validators tab and you click on the validators, one of the key metrics you'll see is basically like I think it's called slot uptime or something. I forget what it's called. Basically it means what percentage of the time are those validators hitting like responding in time to the rest of the network with transactions from their slots. From what I recall, the median today is something like 85% and that number has been growing.
00:36:08.542 - 00:36:11.954, Speaker A: But that's quite low. Why do you think it is that low?
00:36:12.152 - 00:36:45.150, Speaker B: Yeah, I mean it's because the blocks so the block slot times are about four or 500 milliseconds and then you rotate blocks every four blocks or rotate validators every four slots. So let's say you're rotating on average every 2 seconds. So just communication overhead around the world, inevitably some people are just missing are missing that, but that's just okay, well it reduces throughput because obviously you just have empty slots. So it does impact performance. But beyond that it doesn't really matter.
00:36:45.300 - 00:36:56.690, Speaker A: I mean it's a strong centralizing force in the validator set because those who miss their slots, they will just lose money over time being a validator and then stop validating.
00:36:58.550 - 00:37:07.874, Speaker B: Yeah. Do they all collude? All right. And that kind of a thing? I'm generally pretty skeptical of large scale collusion among lots of independent parties.
00:37:07.922 - 00:37:36.894, Speaker A: Oh, I didn't mean collusion or anything like that, just that larger block producers have a strong incentive to mine larger blocks. But I mean no, actually I might also just be wrong and this doesn't apply at all to sort of this that you can affect this as a validator because the slot is the slot. Right. It's like you don't need to wait for someone else's block to build on it. Unlike correct. I think unlike in proof of work.
00:37:37.092 - 00:37:54.066, Speaker B: Correct. Right. Like in Solana specifically, the point of end of the whole proof of history system is that everyone is maintaining an independent clock, which is the repeated and so if someone misses their slot, if you're the next guy, you just don't care that the last guy missed their slot and you can make sure you're ready to go.
00:37:54.248 - 00:38:09.480, Speaker A: Yeah, okay. But nonetheless, I mean that 15% of validators miss their slot on a consistent basis. I think that shows that there is a centralizing force there in the block producer set.
00:38:11.530 - 00:39:06.858, Speaker B: Yes, potentially directionally, that's obviously true. But I also think that the countervailing force is just system optimization is just like a long ways to go. There's a lot of known things that the SLANA team wants to do to improve redundancy in the system and make that better? I would suspect that general call it consistency of performance will probably over the next twelve to 24 months you'll see a two or three x kind of growth in two or three x reduction in kind of missed failures. Because it's just like all new systems like this, it takes a long time to optimize them. And the Salona team is very open about that. In fact, they actually still call the system beta for this reason, because they know there are so many optimizations they haven't done yet that they're unwilling to take the beta tag off of it.
00:39:06.944 - 00:39:43.800, Speaker C: I think backing up too, and just talking about your point about supply decentralization or distributing over time. I think people underestimate how quickly supply can distribute if the protocol is actually being used and it's useful for people. Right. You think about ethereum. In 2016, supply was incredibly decentralized. It just took one year and ICOs and a lot of activity, and everyone in the world then knows what ethereum is at that point. And then now today, very few people relatively talk about ethereum supply being controlled by only a few.
00:39:43.800 - 00:40:53.690, Speaker C: So I do think that utility solves all actually when it comes to supply decentralization. Because if people want to get their hands on it and it's useful for them, then it'll just happen even if it starts less decentralized. I think also, if you look at the way that Solana and also Polkadot and Kusama, the way that they did their sort of listings and then their price history and just being able to allowing normal individuals to access those assets from relatively early on. I think there's clearly a relatively broad holder set than what I think people would have assumed when these projects were in their seed phase. Right? I remember during the seed phase of a lot of these, I remember Kyle came to us and asked us to join them in one of the rounds and we ended up passing because we didn't look closely enough. And then later on we realized we made a mistake and we bought a lot of OTC and we also went and just did a lot more research into Thesis. But back then the main criticism of Solana was that only a few people would own a lot of it and this kind of stuff.
00:40:53.690 - 00:42:26.966, Speaker C: And I truly think that this is one of the biggest red herrings in investing because at the end of the day, it's about technology and community. So if they have a way to create a community, if they have legitimate technology, distribution is not a I think people forget that all of these things started relatively centralized, right? Like Bitcoin, when Satoshi mined the first block, he had it all. So everything starts from that. And then from that point of view, these newer POS chains, they ultimately are a little bit more well engineered, in a sense, because they think very critically about how do they want to give out supply. How do they want to bring people in? How do they want to create organic reward, early adopters reward people coming in like with Mina with the coin list sale I think tens of thousands of people are able to buy it on coin list. I think there's sort of an advantage of modernity in a way, with some of the newer chains that have launched, because they've been able to see the history of a lot of other chains and they can go and say how do we, despite starting relatively centralized because we need to have actual cash to be able to fund this technology to then later go and how do we decentralize supply over time while making sure that there's still a lot of activity going on? So I just think that's like a complete red airing in investing in crypto.
00:42:26.998 - 00:43:45.042, Speaker B: I think the other point I would make, a couple of points I'd make building on that one is if you look at the pace of decentralization of Bitcoin versus Ethereum, obviously Ethereum decentralized a lot faster because no one was paying attention to bitcoin in 2009, right? No one knew what any of these things were. There was a lot of education that had to happen and such. If you then look at where Salana is today versus where Ethereum was, salana is about one year old, it's about 13 or 14 months old. If you look at Ethereum 13 months after it launched in July 15, so 1314 months later, it was like they went through the Dow hard fork and there was nothing on the chain other than the Dow and the hard fork of the Dow, right? And so it's obvious that the pace at which the ecosystem is growing is just a lot faster now than what it was then. That's not don't mean to criticize Ethereum, it's just there was no one paying attention to crypto back then and a whole bunch of things have changed. But the nature of comparing time is compressing where the pace at which these things can decentralize is a lot faster now than it was then. If you look at the slaughter network today, this is crazy to think about.
00:43:45.042 - 00:44:16.320, Speaker B: So the Commissioner Hinman from the SEC gave a speech in June of 2018 saying ethereum is not a security or ETH is not a security. If you look at the state of the Ethereum network at that time, uniswap did not exist. I think Compound had not yet launched. I think they had raised money, but they hadn't launched. A product maker did exist. Zero X did exist. I think Kyber had maybe just launched like V One or was about to launch V One either Delta was around and that was about it.
00:44:16.320 - 00:44:37.060, Speaker B: There was like a few hundred million dollars in stablecoins, not that much even in stablecoins on the system. You look at Salon, a day. There's like a billion stablecoins. There's 1.6 billion in TVL. Serum is doing nine figures in trading volume a day. It's kind of crazy to think about the nonlinearity of how fast these things grow.
00:44:37.060 - 00:45:38.440, Speaker B: So I think that's kind of a backwards facing comment on thinking about this kind of red herring that sue alluded to. And then I think if you reject that forwards, the nonlinearity gets even more interesting. What I think is going to happen, right, is like you have all these permissionless DeFi crypto thingies and obviously a lot of people around the world are paying attention to this stuff right now, and they're all trying to figure out what does it mean for my business? And this is true for both kind of finance companies as well as banks. But after measure, every tech company and every social media company in the world right now is thinking about this stuff, saying what's here? And they're all looking at Bitcloud, they're looking at social tokens. There's obviously just a lot of cool ideas here's, a very interesting design space. None of them have done anything yet on a public chain. Facebook tried to go their own way with Slash DM and doesn't appear that's working for, I'm not sure why, but whatever reason, they have problems.
00:45:38.440 - 00:46:18.102, Speaker B: But my point is, no one actually has done anything on a public chain. Most interestingly, the one company that got close was Reddit, and they got really excited about doing a point system thingy. They did this big public bake off last summer and their conclusion was, none of these things are ready. We're going to do something permissioned and private ourselves. It just kind of tells you how not ready. That's the biggest, most empirical demonstration of how not ready these systems are for scaling to large numbers of users. And that was nine months ago that happened.
00:46:18.102 - 00:46:59.780, Speaker B: It was about nine months ago. When I think about what can happen over the next nine to 24 months, I know all these companies are looking at doing crypto things. And the number one thing they're all worried about is scale, right, is they don't want to break. They don't have bad user experience. They want to make sure it's going to work. They want to make sure the fees are low and all that stuff. And I would actually argue that if a real company with, let's say, 50 million plus daily users says, hey, we're going to move our users onto a blockchain for some core operation that's native to the application, right, that you expect 50 million people to use multiple times per day.
00:46:59.780 - 00:48:14.810, Speaker B: The first time that happens, that blockchain is now the most likely to become the largest blockchain in the world. Because once that happens, then most other people, other companies in the world are going to say, okay, let's watch and see what happens to these guys. Because there's a lot of technical risk involved, a lot of product risk I mean, there's just like a lot of operational execution risk, right, in so many ways to pull this off, and several is going to kind of sit back and say, okay, let's see if this guy fall in their face or not. And assuming it works, then the amount of convergence of perspective you're going to get among kind of global engineering leadership around the world is saying, okay, this is the least risky way to scale these things to 5100 million, 200 million users. That perception is going to change very fast. I don't generally hold this dogma that my back to Sue's point of perceptions can change. I think the pace at which these things can change is extraordinarily small and the momentum can shift.
00:48:14.810 - 00:48:39.410, Speaker B: I don't think any of these core debates are over because you're going to see these types of announcements are going to come. I don't think they're imminent in the next six months. I think that's probably a little premature, but I'm optimistic. Within 18 months you're going to see at least one major tech company do something that's like fundamental to their business, that incorporates public chain.
00:48:40.150 - 00:49:12.960, Speaker A: I don't have a good track record of predicting these things, but I would still say that I would be very surprised if that's true. Maybe I lack just the creativity to see what using a public blockchain can do to the products that these businesses offer and why they wouldn't rather use an experience that they control, either just using a regular database or do you have any example in your mind of even when Reddit made the announcement, I thought that it was stupid, and that didn't make any sense.
00:49:13.650 - 00:50:01.306, Speaker B: So the right frame to think here, I think, is not one of control. It's actually the opposite. It's one of liability. Vitalik wrote a really good blog post about this, I don't know, 6912 months ago. I forget it was phenomenal, where he basically said that the conversations increasingly inside of companies because of GDPR, because of all these hacks, is data. I mean, look, if you're Google and Facebook, you have like a big machine learning business, like, okay, you have some data requirements, but for most other businesses that are not doing large scale ML stuff, is data an asset or is data liability? And you keep seeing these hacks, you just keep seeing all this stuff happen. Cambridge Analytica, I mean, it's just target just constantly.
00:50:01.306 - 00:50:17.810, Speaker B: All these things keep happening. And I think with a little bit more regulatory push from various jurisdictions, it's not hard to see a world in which a lot of companies start to view data as a liability instead of data as an asset.
00:50:18.550 - 00:50:20.740, Speaker A: And how do blockchains set with that?
00:50:21.990 - 00:51:47.760, Speaker B: Yeah, so what blockchains provide is the substrate such that you can design applications where users own the data, they own the state, whether that state represents money in the form of a social token or something else, or whether that represents your Telegram messages or whatever. Now is Telegram going to move over to some decentralized system soon? No, because the scale of messaging is too large. That's the highest order engineering problem because it's just like trillions of messages per day are sent. But there's a lot of intermediate things between here and there that are just much lower volume that if it's like 100 million messages per day, can you get that over in decentralized capacity? And I think that Talek's probably right that the longer your horizon the more that data is a liability, not an asset. I think that's so what do companies do? I think the obvious design spaces are financial inclusion types of things where the companies can say look we're not a money transmitter, they can kind of say we're not liable and then anything related to kind of social tokens and creator economy stuff. All that feels very ripe for this kind of a thing. I do think probably the social media and social adjacent companies are probably the most interesting for this design space.
00:51:47.760 - 00:52:22.314, Speaker B: I think. Relatably you look at reddit. Look, I'm not like a reddit user. I've always kind of thought Reddit was stupid. It's messy and hard to filter through. But there's some interesting design space here of karma points or credibility points or whatever you want to call them on a per Reddit per forum basis. And people want to put embed value into that for Reddit to imbue value into that as Reddit makes them a money transmitter and a lot of the things they don't want to deal with.
00:52:22.314 - 00:53:11.978, Speaker B: And so this is just a clever regulatory arbitrage for Reddit to imbue value in their systems without becoming a money transmitter. So I think all of these vectors are very interesting for big companies to start to engage with this stuff. The other comment I would say just generally is with most new technologies that are orthogonal to a lot of existing things, they tend to seep into the world in ways that are very not predictable. The internet being kind of the best example of this. There's a good Mark Andreessen quote about this. He says I now assume every entrepreneur that comes into investment committee to pitch us, I assume that they are right. Like whatever the core thesis is, is correct.
00:53:11.978 - 00:53:36.174, Speaker B: The only question is timing. Now that's like a somewhat of a kind of extremist view. He's obviously being a little bit hyperbolic but directionally there's like a nugget of truth in there and I've kind of doing this for a few years now has made me a lot more open to that general line of reasoning. I just kind of assume all these weird things people pitch me even if I think they're dumb. You just have to kind of assume they're right. The question is just a function of timing.
00:53:36.302 - 00:54:54.506, Speaker C: I think an interesting thing that happened yesterday to that point as well is GameStop announcing that they're going to do this nifty thing on Ethereum, right? And I remember when GameStop first came to the mainstream, there was then a lot of talk, well, GameStop should put bitcoin on their balance sheet and then that would be a great way to play crypto. And then they've gone and done their research and they've decided actually what we want to do is put nifty's on few. There's a few, I think, conclusions we can draw from that. I think one is that for businesses that are gen z native, internet native, millennial native, there is a huge growing interest in the idea of social collectible in and out of value thesis. This stuff, when you explain it to these types of owners of these businesses, it truly excites them. And I think the idea that they want to control a centralized database of this stuff, if you're running with that company, you can't tell me the straight face, you want to control that database yourself, right? Because what advantage do you get? Like if GameStop went and said, you know what, I'm going to make a database of collectibles, you can now collect these things in my database. If you said that in the boardroom of GameStop, you'd be laughed at.
00:54:54.506 - 00:55:36.774, Speaker C: Right? It just sound insane. So I think that we're getting to the point now where in those discussions, if the person advocating a centralized database will soon seem like the most crazy guy in the room and then the question will only be which chain do we deploy to? Do we use a newer chain? What features do we want? What user experience do we want? I think that whole concept of decentralized database, I think it assumes several things about what companies want that that isn't really true.
00:55:36.812 - 00:55:37.062, Speaker B: Right?
00:55:37.116 - 00:56:26.930, Speaker C: It assumes that they aren't able to do the research and then figure out what could make their business work. They've now already seen the growth of DeFi, they've seen top shots, they've seen nifties once. That imagination is kind of sparked and the sort of reflexivity of this entire adoption phase. I think that we're completely entering the stage now where assuming that crypto is sort of able to serve as this credible neutrality settlement layer, that internet native companies find this incredibly interesting and want to deploy as soon as possible their biggest ideas.
00:56:27.090 - 00:57:34.640, Speaker B: Yeah, I agree, it's hard to remember now, but a lot of companies ran a lot of experiments with the internet back in the day and it took a lot of people a long time to figure out what to do. But a lot of the core ideas were there from day one. Things like forums, things like chat, and even things like CRMs and databases and those things, all those core ideas have been present from early ninety s and I look today and kind of the obvious ones are DFI and digital collectibles are like the two really big obvious ones. And I think there will be more that kind of iterate from there. But the number of places you can insert those core primitives I think is measured in billions of daily active users. So yeah, I'm optimistic this stuff will happen and the companies that already have distribution will in fact be the primary distribution for this stuff. I don't think this is going to be like the Internet where basically you had a whole bunch of new companies come in and disrupt the old guys and the old guys didn't figure it out.
00:57:34.640 - 00:58:10.666, Speaker B: I'm a lot more optimistic that internet native companies will see the pattern, see the trends going on and not all of them will adapt correctly. For sure a number of them will fail. But I think the leading social media companies, specifically, any company that has deep social roots in it, I think it's very probable that they figure out how to embed these new primitives into their product and service in a compelling way because they're not asleep at the wheel. In fact, most of them are founder led and those companies are likely to rejigger their products.
00:58:10.848 - 00:59:37.590, Speaker A: Yeah, I mean, personally I think that the vision of decentralized social media is very compelling in the sense that sort of the state is public and uncensorable and users can choose between different interfaces that may have different amounts of different levels of moderation and those can then be regulated. I agree that it's definitely a question of timing, like in the Web Three vision. This part seems the most compelling to me, but I don't know how many years out it is. And then about what you said earlier, I think that this is maybe one of the most interesting aspects of DeFi sort of removing liability from financial service providers. So there are two reasons why regulation exists, right? One is sort of for the incumbents to keep sort of competitors out, but also really to protect consumers and just protect the economy itself from sort of moral hazard, so on, and contagious sort of effects. And I feel like this part you can really, at least the second part you can really get around by using smart contracts just in general, this concept of a company tying their own hands. If you can do that, then all of a sudden a lot of need for cumbersome regulation disappears.
00:59:39.450 - 00:59:41.426, Speaker B: It can't be evil, right? Not don't be evil.
00:59:41.538 - 01:00:32.310, Speaker A: Yes, exactly. Okay, so we talked about sort of what it takes for developers to adopt this and definitely the user experience and the transaction costs and some minimum level of decentralization are definitely big parts of that. But another that we have seen that is very like a very potent network effect is sort of the execution environment and programming language for developers and all the tooling around that. And I would say that the EVM and Solidity are huge leaders right now in that area and Solana uses, I think uses a rust based execution environment. Can you talk a bit about that and how you think that basically the advantage of the EVM is not already insurmountable.
01:00:35.050 - 01:01:21.234, Speaker B: Yes, Solana has a custom runtime called It. Compiles to LLVM and then compiles kind of a new instruction set called E Berkeyback Filter or BPF. That stuff is well below my understanding of kind of core like how circuits switch and how memory is stored and how processors transact things. So there's a level of technical depth there that I'm not qualified to speak about. What I do know is that Solana compiles down to kind of a native code. The rust actually gets the LLVM compiles down to native code. So you're kind of getting native execution as opposed to some intermediate layer.
01:01:21.234 - 01:02:08.354, Speaker B: The EVM actually acts as a virtual machine, which is in the name Solana. You'll note the C level is not called a VM, it's called a runtime. So you have kind of this abstraction with the virtual machine. And again, kind of the technical depth of virtual machines is beyond me and probably beyond the scope of this podcast. But suffice to say, it's just generally understood that they create a bottleneck kind of in terms of processing efficacy. So one of the core insights that the Solana team had was to say, look, we need to be able to get one of the really interesting things to think about in these networks is you have a fixed amount of computational space. And that space is, well, there's the physical bandwidth, but there's also just the physical processors and graphics cards.
01:02:08.354 - 01:03:22.694, Speaker B: And you have literally billions of people trying to theoretically share this fixed amount of resource space, given that you need to optimize every ounce of performance out of the system to make sure that you can run at the limits of what the hardware can actually do. And so one of the core things SLANA team recognized early on was the EVM was just not in any way kind of optimized to take advantage of hardware, both in terms of just efficacy on kind of a per instruction basis, but even more importantly in terms of parallelism. And this is probably the most important difference between the EVM and C level, which is that Solana natively supports parallel transaction execution. If you think about DeFi or even just payment, right? If I pay sue and then yuhasu, pay someone else, there's no dependencies between those two things. And so those things should transact in parallel. The problem in kind of a blockchain is that you have this completely open state, and anyone can submit any transaction into the state at any time, and any transaction can theoretically modify any part of the state. You don't know in advance what it's going to modify.
01:03:22.694 - 01:03:32.240, Speaker B: And so if you enable concurrent transaction execution, the thing you have to make sure is you have to make sure that basically you don't have two transactions reading and writing from the same piece of memory at the same time.
01:03:33.170 - 01:03:36.706, Speaker A: Like two trades in the same units of pool or whatever.
01:03:36.808 - 01:04:08.010, Speaker B: Exactly, whatever. Plenty of examples you can come up with on a technical basis. You're specifically focused on address, like in memory itself, that's really the core technical constraint. So the EVM solves this problem by just saying don't solve the problem and just force everything to run serially. And if you do that, then it's a solution. But obviously you forfeit parallelism. Interestingly, of all the other major chains, the only chain that even attempts to solve this problem within the context of a single shard is Solana.
01:04:08.010 - 01:04:55.420, Speaker B: The way they solve it is they basically say every transaction has a transaction header and the transaction header specifies all the parts of the state that that transaction can modify. Not that it will modify, because there may be some branching if logic in the transaction, but that it could modify based on all potential permutations of if statements in the transaction. So basically you just lock all of those pieces of state and say, I get monopolistic rights over these parts of state for the course of this block. And so by doing that, basically the system can then parallelize. It knows what every transaction is going to touch. And so you can parallelize all transactions that don't touch overlapping state. And the benefit of doing this is that you can parallelize things.
01:04:55.420 - 01:06:04.590, Speaker B: Modern graphics cards have about 4000 cores and so you get kind of 4000 lanes of parallelism. Basically in a year or so, Nvidia is going to release cards with 8000 cores and you just double the throughput. When I think about the nature of these blockchain systems, if you assume there's going to be social media applications from Snapchat and from Bitcloud, and you're going to think there's going to be DeFi stuff and you think there's people going to be trading tokenized securities, I mean, these are all largely different things. And by definition these different categories of applications are non overlapping in what they do. And so it's only natural that you should be able to parallelize these transactions. And it seems relatively clear to me that if you assume you got 100 million or a billion users doing all kinds of whatever social media things d five things whatever on a block by block basis, where a block? Here, let's just say for simplicity, is 1 second the percentage of those transactions that will be actually demanding overlapping state on a per second timescale. My intuition is that it's probably under 1%.
01:06:04.590 - 01:06:17.474, Speaker B: It may be under 0.1% of transactions are actually fighting over the same piece of stake. Maybe it's two or 3%, but I'm pretty sure it's like not over 10% of transactions in Ethereum.
01:06:17.522 - 01:06:19.000, Speaker A: It's probably a lot more.
01:06:20.170 - 01:06:26.418, Speaker B: But I'm saying as you increase the array of applications yeah, exactly right. That percentage has to drop.
01:06:26.514 - 01:06:27.400, Speaker A: Yeah, okay.
01:06:30.330 - 01:07:00.420, Speaker B: My intuition is it's probably on the order of 1%. Maybe it's two or three. Hard for me to believe it's higher than that. And so if you don't parallelize, you're just forfeiting massive amounts of throughput per unit of time. And I think that's super important for these things. And I think that's one of the fundamental differences in Solana versus Ethereum. The downside, of course, is that you lose backwards compatibility with the EVM, which obviously has a fair bit of infrastructure build up around it.
01:07:00.420 - 01:07:54.340, Speaker B: I've always felt that it hasn't achieved escape velocity for kind of all the reasons we just talked about with big companies and all these other things. And so it obviously is something you have to overcome and it was not to be taken for granted that it would be overcome. But at this point, if you look at the state of the Solana ecosystem, it's hard not to imagine basically all the things Ethereum has that Solana doesn't have today. So things like Dune, things like Graph, which is announced, few more DeFi primitives and those things it's pretty hard not to imagine in the next three or so months those things all maybe six like all those things kind of getting built out. And you're basically kind of feature parity for what I'll call all the middleware DeFi primitive stuff. If you assume that's the case in three to six months, then it's like, okay, well, what advantage is the EVM actually providing now? And that starts to become very insignificant pretty quickly.
01:07:55.350 - 01:08:21.550, Speaker A: Right, okay. I mean, I agree about those tools, but just in general, it's not possible to port something like I mean, not that you would want to have uniswap on Solana because of course Solana would support more efficient order book based exchanges. But if there's an application in DeFi that the users like, what are sort of the steps to porting it over? I assume it requires a full rewrite.
01:08:22.050 - 01:09:28.610, Speaker B: Yeah, full rewrite of the smart contracts, for sure. One thing I've observed that I was wrong about was back in August, September of last year, reached out to a whole bunch of the DeFi protocols, all the major ones, and was like, hey guys, Solana's serum was announced and there's a little bit of volume starting to happen. And Solana team reached out to all the major DeFi protocols on Ethereum and said, hey, are you guys interested in rebuilding on Flana? And all of them said they all were like, oh, this is interesting and cool. And then none of them did anything. It's been nine months now and you can see that none of them have relaunched on Solana. And so the question is why? Obviously they didn't think it was a priority is kind of implicit in that. But if you kind of dig beyond that, one thing I've observed having interfaced with a fair number of solidity based EVM engineering organizations is that the Ethereum developer teams have very little, if any, expertise building Rust, writing Rust and deploying Rust.
01:09:28.610 - 01:10:42.730, Speaker B: Not that Rust is like a weird niche language. I mean, Rust is one of the most popular languages in the world now, but these teams just don't have that experience in house. And so as an engineering leader, if you don't understand kind of this other code base, the other technology base, and you're tasked to go build some first class application, it's just a very hard thing to do both as an engineering leader and then also to find and recruit the team to do it. There's just a lot of organizational momentum that existing ethereum based DeFi teams have that's not easy to overcome. So the Solana team I think had to realize late last year that those efforts to try and get people to port over were failing. I think the failure rate was 100% and have realized, okay, well then we have to build everything new from the ground up with new teams, which feels like a risky strategy and it is obviously riskier, but I don't actually think the amount of risk is actually that high on a relative basis. And if you look today, you've got multiple teams building money market things like Compound and Aave.
01:10:42.730 - 01:11:12.190, Speaker B: Jet and Oxygen I think are the two that I'm aware of. There may be others. Jet and Oxygen are money markets. You've got teams building margin trading, mango, you've got teams working multiple teams working on perpetual contracts and quarterly futures. You got multiple teams working on options. Those are all the most important kind of core primitives. You got teams already working on all those things and I think most of those market segments will be reasonably competitive.
01:11:12.190 - 01:11:33.318, Speaker B: There will probably be two or three major players in most of those markets, which is healthy, right? Like you don't want there to be a single protocol for each kind of core primitive. It's healthy for the market to have two or three. And most of these teams that I kind of just named are venture backable. Not saying that we are necessarily investors in them, but they are all like venture backable teams.
01:11:33.414 - 01:11:34.860, Speaker A: Can you say something about.
01:11:37.070 - 01:12:16.866, Speaker B: You know, Sam was running FTX and it seems like DeFi clicked for them kind of in May or so of last said, AHA, like this is, this is important. And they started to do stuff on Ethereum and they hit all the throughput constraints and they were like, we just can't do to we're not going to build a product that we want to be able to build. So they started looking around. I remember I had a call with Sam and Anatoly. I want to say it was like on July 7 or thereabouts last year, something like that. Call started at it was 10:00 for me in Texas, 08:00 for Anatoly, and it was 11:00 A.m. For Sam in Hong Kong.
01:12:16.866 - 01:13:13.526, Speaker B: And call was sitting for 30 minutes, went for two and a half hours and kind of just dove really deep into what does Sam want to build. I remember we had a really existential debate about what is the nature of financial markets, information theory, what's the speed at which things propagate. I mean, literally the speed of light. But then also more importantly, what is the timescale that matters for prices to update? Is it okay if the price updates aren't measured in nanoseconds, but measured in milliseconds? And what's the inefficiencies that creates? Right? And kind of very kind of existential questions about the nature of these things, kind of reason through all this stuff. And I remember it was obvious the wheels were turning in Sam's head and he was like, yes. Like 400 millisecond to 1 second timescale is a sufficiently low timescale that you can make this thing work. 15 seconds is too slow.
01:13:13.526 - 01:13:40.794, Speaker B: It's unclear where exactly the threshold is between 1 second and 15 seconds. But somewhere in there is kind of the threshold to make this stuff work. And he understood quickly that you need parallelism for this to work. Because he's like, yeah, I'm going to have a bunch of CRM markets, and obviously you need these things to transact in parallel, not serially. And so he kind of realized all that stuff pretty quickly over the phone. And I remember I went to bed and then the next day I woke up and I totally texted me. He said, dude, someone's spamming the Solana network.
01:13:40.794 - 01:13:49.720, Speaker B: And I was like, I was like, I bet you it's the MDX engineers. And yeah, they started spamming the network that night to test it. And Sam got underway building serum from there.
01:13:50.090 - 01:13:53.750, Speaker A: Is serum an application or a suit of applications?
01:13:54.170 - 01:14:18.160, Speaker B: Serum is a protocol. Serum actually does not have a front end today. At least not one that's endorsed by Sam and the serum team officially. I think if you go to Project Serum.com, there's like a list of front ends that are there. But serum is not a protocol. I think part of the most interesting thing about serum is that it is the opposite of FTX in so many ways.
01:14:18.160 - 01:15:03.358, Speaker B: FTX is obviously a full stack experience. The UI is glorious, customer support, fiat on ramps, all those things. What's interesting is that FTX has been widely recognized for their product execution over the last two years, where they control the full stack. And it's been very interesting to watch the serum team do the opposite for serum, which is say, hey guys, here's a protocol. It's a protocol that enables you to have order books and markets on the chain and cross the spread basically, right, and complete a transaction. And it only does that for spot stuff. Although you can use the order book infrastructure for theoretically any asset, whether it's leverage or derivatives or something else.
01:15:03.358 - 01:16:17.720, Speaker B: But here's this infrastructure, please go build other stuff around it. So build front ends, build margin trading, build perpetual contracts, quarterly futures, all these other things. And at first I was kind of confused, kind of watching them do this, but FTX is a very full stack, highly controlled thing and serum is not. And I just kind of assumed they were going to build serum in the same way. That was just my default assumption. And if you look at the communications from the serum telegram, from the serum medium, from the serum Twitter, you'll see this continuous, repeated focus on serum as a development platform for third parties where there is no official front end and they're really focused on enabling other developers to build DeFi primitives. This is not particularly novel thinking in crypto DeFi land, obviously, but I think it's very interesting that you've got a single entrepreneur who is known for controlling the full stack experience on land, also then having the wherewithal to say we're going to engage DeFi in a DeFi native way and not try and control the whole thing.
01:16:17.720 - 01:16:21.926, Speaker B: And I think that's been super interesting to kind of see that dichotomy play out.
01:16:22.108 - 01:16:49.550, Speaker A: Yeah, thank you. That is indeed very interesting. We kind of moved away from the original question. We barely talked about sort of the synchronous versus the asynchronous experience of DeFi and instead talked about how much decentralization is enough, how to measure decentralization and so on and what you can get in return. And that was also very interesting. So I would say thanks, guys, for the discussion.
01:16:49.970 - 01:17:02.810, Speaker B: Hey, Hasu. Sue, thank you for having me on. Pleasure to be on. I've been listening for a long time and yeah, it's cool to dive into this stuff. I love the really deep first principles, peeling back the onion one layer at a time. Thanks, Hasu.
