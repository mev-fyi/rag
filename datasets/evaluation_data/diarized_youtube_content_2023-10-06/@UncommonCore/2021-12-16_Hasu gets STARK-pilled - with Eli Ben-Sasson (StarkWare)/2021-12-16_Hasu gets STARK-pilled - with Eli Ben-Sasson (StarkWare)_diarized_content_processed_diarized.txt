00:00:03.130 - 00:00:57.470, Speaker A: Welcome to Uncommon Core, where we explore the big ideas in crypto from first principles. This show is hosted by Su Zu, the CEO and Chief Investment Officer of Three Arrows Capital. And Me Hasu, a crypto researcher and writer. In this episode, I had the opportunity to sit down with Lee Ben Sasson of Starkware. If you listen to my last episode with sue, where we talked about the scaling approaches of different layer one blockchains compared to ethereum. I argued that layer one blockchains do not scale and that the only way to create true scalability is to perform all of the computation off chain and only post the results of that computation on chain. So Starks are a technology that allows huge amounts of computation to be compressed into very small proofs that anyone can easily verify.
00:00:57.470 - 00:02:02.020, Speaker A: What Ellie and I have tried to create here is nothing short of the most approachable and comprehensive audio resource on how stocks work and how they will scale blockchains in the future. We start by explaining the inclusive, accountability and the true meaning of scalability. Then we dive into Starks, how proof systems work in general, and where these so called validity proofs fit into the context of unbundling blockchains. Next, we use dYdX as a comprehensive case study to learn about the Starkx system before diving to StarkNet and its trade offs to Starkx. Finally, we talk about Starquest programming language Cairo and how the different costs of proving, verifying and storage are going to scale into the future. If you're a developer, you should also gain a very good idea of the tradeoffs and trust assumptions between building on a regular layer one blockchain versus the general purpose StarkNet blockchain and an application specific Starkx blockchain. Enjoy.
00:02:02.790 - 00:02:04.606, Speaker B: Hello. Hi, Hasu.
00:02:04.718 - 00:02:25.130, Speaker A: I've been wanting to talk to you for a very long time. I'm a big fan of Starkware and, full disclosure, an investor. I think you're doing one of the most interesting things that happen in the blockchain space today. Can you give us a bit of your background, maybe up until the point where you founded Sparkware?
00:02:25.630 - 00:03:51.890, Speaker B: Yeah. So I sort of came to blockchains by chance, meaning for a very long time I was a theoretical computer scientist, which means I was studying and proving theorems about computation in a very abstract way with no applications in sight. So very fundamental questions about the mathematics of computation, things like P, NP, Co, NP, creatures like that. And one particular question I was interested in, starting around 2000 when I was doing my postdoc at Harvard and MIT, was the question of how to get certain proofs, called probabilistically checkable proofs, to be shorter and more efficient. And after some work, we had a few breakthroughs there with several collaborators, most notably Madhu Sudan, who's now one of the scientific advisors of Starcore and a professor at Harvard. And slowly these breakthroughs started to get implemented first by students. And then I received a grant and we started implementing them more seriously for general purpose computation, and fast forward something like twelve years to 2013.
00:03:51.890 - 00:05:08.074, Speaker B: I was giving a sequence of lectures at academic institutions about this theory to practice of those proof systems. And I was looking for examples like why would one use them? And at that point, I started to look seriously at Bitcoin for the first time after hearing about it previously, and I incorporated one such example in my talks, and I also asked to present at the Bitcoin San Jose conference in May 2013. And this was a turning point moment for me because it was a very electrifying conference. And in hindsight, that was the place where I swallowed the red pill and realized in hindsight, that blockchains are a very good fit for proof systems. Proof systems are good for privacy and scalability. The first application we did was Zcash, first as academic work, and then as a system that's out there. I was one of the founding scientists, along with my collaborators, and I was still interested in better technology that's older but also more complicated to achieve, which we ended up calling Starks.
00:05:08.074 - 00:05:27.106, Speaker B: And when that was mature enough to commercialize, I basically left my academic position. I was a professor at Technion at the time, and with my three co founders, alessandro Chiesa, Michael Riabsev, and Uri Kolodny, we founded Starquare.
00:05:27.298 - 00:05:54.640, Speaker A: One concept that always shows up in sort of Starquare's material, and that seems to play a larger and larger role in the blockchain space is the concept of computational integrity. And as I understand it, this is sort of the main application of Starks. Can you explain what computational integrity is and how Starks have to improve it?
00:05:55.810 - 00:06:33.820, Speaker B: Yes. So integrity was beautifully defined by C. S. Lewis, the author Know Narnia and that whole story, and he defined it thus. Integrity means doing the right thing even when no one is watching. So computational integrity, by analogy, means performing the right computation, say, with our data, even if we're not watching, and especially when no one is watching, and even more so when there are incentives to misreport or sort of change the computation. So let me give an example.
00:06:33.820 - 00:07:21.558, Speaker B: Most of our finances today are basically just bits sitting on a computer, in a bank, in financial institutions. Now, we would very much like to know that those bits are handled in the right way, in a reliable way, with integrity. So if you formalize computational integrity, it means something like this. Suppose we agreed on what the right computer program should be, right? We wrote it down, we all looked at it. We agreed, this is the program you want. For instance, if it's a blockchain, please do not move my funds unless the signature matches my public signature, and then do whatever I instruct, things like that. So there's a program that checks a signature and then does what it is instructed.
00:07:21.558 - 00:08:29.722, Speaker B: Good. So what we would like is to live in a world where when if someone is in charge of processing transactions and running computation, she is doing so exactly as the program specifies and not deviating from what the program we agreed to specifies. Now, this is sort of a value or an ideal or a principle we would like to have with our funds in the bank, with transactions we do on the blockchain and with any sort of computation. If there is a database containing forensic information or health information, again, we would like the same principle to apply. So we would like to live in a world where we have computational integrity, just like we'd like to live in a society that has integrity. And what Starks do, they are one of several different methods that give you integrity, that assure you that computations that you didn't see were done with integrity. So computational integrity is a value or a principle.
00:08:29.722 - 00:08:36.370, Speaker B: And Starks are one very good way to enforce that desired principle.
00:08:36.710 - 00:09:28.702, Speaker A: For me, there are sort of two takeaways. Almost first is computational integrity is actually everywhere. Right? I think you gave a great example in one of your first blog posts with Starquare where you gave the example of how do you know that your bank actually performs its own sort of ledger updates with computational integrity? And once you know about this problem, you start to realize that they spend immense amounts of resources on sort of signaling that they are a trustworthy institution and that they employ like external accountants and so on. A lot of resources go into proving this because otherwise society wouldn't work. And a lot of things actually depend on computational integrity.
00:09:28.846 - 00:09:50.890, Speaker B: Precisely what you're referring to I like to call delegated accountability, which means if you look at the banking world, the state usually will write laws and appoint all kind of accountants and regulators and lawyers to enforce and check on behalf of the public the integrity of these very important institutions.
00:09:51.710 - 00:10:12.320, Speaker A: Yeah, exactly. And the second thing that I thought is blockchains are actually sort of almost computation integrity machines. They are not very fast ledgers or very cheap ledgers. The only thing that they really provide is a ledger where computational integrity is insured for everyone.
00:10:13.090 - 00:10:45.610, Speaker B: I wanted to say that the way they achieve this is by numbers, meaning trust in large numbers, which means so I like to call this inclusive accountability, which means that anyone is invited to be sort of an accountant and a regulator of the blockchain. And by doing so, that person would take her computer and now go over all transactions in the blockchain and check each and every one of them. So this is the way computational integrity is achieved today on conventional blockchains.
00:10:46.110 - 00:11:03.920, Speaker A: Right. And in a conventional blockchain, this is very hard to scale, right, because we are not actually bound by the cost of computation or not even the cost of really storing any of the data, but by verifying the.
00:11:06.370 - 00:12:20.538, Speaker B: A very here it goes back to very beautiful principles that Satoshi started, but all the decentralized blockchains are following. And the reason we can't sort of ten x the gas limit or the block size of bitcoin and we can't repeat doing this is not because there aren't large enough computers and bandwidth that can support this. That's not the reason there are computers that cost more, that can process ten x whatever it is that is currently done. But the problem is that you will start having people dropping off the public. The inclusive accountability will dissipate and disappear because you and I won't go out and buy some 64 core machine and some big bandwidth connection just in order to verify all transactions in this ten X greater thing. So people will start dropping off and you will basically fall back to the existing world, conventional world of some big trusted institutions with individuals appointed to check them. And that's the conventional world.
00:12:20.538 - 00:12:39.710, Speaker B: So we would like to find ways to scale the system while maintaining it inclusively, accountable. That everyone with their laptops and very minimal work check and verify the integrity of the system. And that's sort of the paradox that various different technologies are trying to solve.
00:12:40.050 - 00:13:25.360, Speaker A: Yeah, I think this point can almost not be overstated that it's only scalability if you keep the property of inclusive accountability. If the cost of accountability goes up, then it stops being inclusive. And then you did not actually scale the system right? You made it accessible to fewer people. I recently tweeted that ever since Ethereum came out, no other blockchain has really made any fundamental advances in scalability. Even though they all claim that they do. They only chose different points on sort of the accountability spectrum. Right?
00:13:25.810 - 00:14:18.400, Speaker B: I think you're very right that if you increase the throughput by basically saying everyone is invited to just buy a bigger machine, then what you're doing is I mean, you will get larger scale. But on this curve between how inclusive it is and how scalable it is, you're just moving to a different point where the ultimate two points of it are, let's say Bitcoin Ethereum style, where you with your quad core laptop and whatever, 16GB of Ram and you can check everything. And the other extreme is basically banks and Alipay and Visa where you buy huge data centers and process tens of thousands and you can pick any point in the spectrum. But the more you go towards the scalable by better hardware, the more you are excluding the public.
00:14:18.850 - 00:14:41.686, Speaker A: Yes, exactly. So that's why I'm so excited about Starkware and by extension sort of all forms of sharding like the other roll ups and so on. Can you explain how Starks specifically scale this property of inclusive accountability? Like, what are Starks and how does.
00:14:41.708 - 00:16:12.340, Speaker B: It so Stark is an acronym that I won't go over all of it, but it defines a certain class of proof systems and for a proof system to be called a Stark to satisfy this definition, it has I'll just focus on the first two letters. It has to be scalable, which again, not to get too mathematical, means that the time needed to generate a proof scales nearly linearly with the amount of computation or with the number of transactions that you're processing and simultaneously the time needed to verify a proof scales exponentially smaller or logarithmically with the amount of computation. So a system that satisfies these two properties is called scalable. That's the S and the letter T stands for transparent, which means that there's no toxic wake store setup or proving keys. The only thing you need in order to set the system up and use it is basically public random coins, which is very important for trusting the outputs even when the prover is some government or some monopoly and you don't want to make any assumptions about it. So that's what Starks are and the way a Stark works is. So we agreed that true scalability for a blockchain means that you don't change anything in the amount of computation and resources that all the nodes need.
00:16:12.340 - 00:17:05.330, Speaker B: So what if you could have a way where someone would run a huge computer but you do not need to make any assumptions about that someone and you can check with near certainty the correctness and integrity of her computations. So this is exactly what Starks give you. They say someone, anyone can run approver. This could be Darth Vader or the worst government that you have the least trust in. As long as you know what program they are using for processing and you get a proof, you know that they operated with integrity. And this way you can have all the nodes of the blockchain still verifying the integrity of the whole blockchain using laptops and very standard internet connections and yet scale the number of transactions running on that blockchain exponentially.
00:17:05.750 - 00:17:16.680, Speaker A: So what is a proving system? And as a user of ethereum, how do I know that whoever made the proof used exactly that system and not some other variant of it?
00:17:17.950 - 00:18:13.514, Speaker B: Okay, so proof systems started off in amazing research breakthroughs in the mid 1980s. There are many variants of proof systems. The most publicly well known by now are the zero knowledge proof systems. But there are many classes and they have different properties interactive proofs, multiproover interactive proofs, probability checkable proofs, and many others. So there's a wide class of cryptographic proofs, but what they all share in common is the property of basically certifying integrity of a computation. That's what proof systems give you. They give you the guarantee and assurance that a large computation was done correctly, even if you don't see all steps of it and even if some of the inputs that went into that computation are shielded from you.
00:18:13.514 - 00:19:17.614, Speaker B: That's the property of zero knowledge. So that's what proof systems give you. Now, your second question is how can we trust that a proof system was used correctly and was designed correctly? So it's a very good question. And this is a special case of the more general question of how can you trust hardware and software that is supposed to do something to be doing that thing? So in general, it's a tough problem. How do you know that your computer is not logging your keystrokes and reading your keys? It's a tough problem. In the case of proof systems, what is really, really nice is that you only have to trust the Verifier part. So if you audited, checked, understand and believe that the Verifier is correct and the hardware running it is correct, the math of proof systems tells you that you don't need to make any assumptions about the proving side.
00:19:17.614 - 00:19:42.066, Speaker B: It can be running on faulty hardware. It can be running any software it wants with bugs or without bugs. The Verifier is the only thing you need to check and trust. And the verifier in our systems is run on L one ethereum. Its code is open source. It's audited and being audited and hopefully there are no bugs.
00:19:42.258 - 00:20:15.780, Speaker A: So I don't need to trust that the solver is running so software. That is exactly what it says. I just have to trust the Verifier. And the Verifier runs in an environment that has very high computational integrity, which is Ethereum, for example. But it could in theory be any other layer, one blockchain. And then I can just like any other smart contract, if I trust Ethereum and I trust that the smart contract code is correct and does what I think it does, then I can also trust the entire proof system.
00:20:16.150 - 00:20:17.170, Speaker B: Precisely.
00:20:17.510 - 00:20:50.090, Speaker A: I see. So how does it work? And maybe not to go too in depth, but just to give sort of myself and our listeners a high level idea, how do you generate this property that you have a proof system? You input a huge amount of computational data or data that you want to perform some amount of computation on, and then you generate this very small proof that is very computationally cheap to verify that the computation was done correctly.
00:20:51.310 - 00:21:31.526, Speaker B: So it's a terrific question. And indeed, when these results, results of this nature of being able to check the correctness of a computation very cheaply, when such results were first discovered, they were mind blowing. They seemed impossible. How can you know that a very long computation, processing hundreds of thousands of transactions, all of them happened with integrity, without redoing all of them? And you could do this for a generic program? This sounds almost contradictory to all kinds of famous fundamental theorems in math and computer science.
00:21:31.638 - 00:21:33.498, Speaker A: It is still mind blowing to me.
00:21:33.664 - 00:22:45.678, Speaker B: Yes, exactly. The undecidability of the Halting problem basically says that you cannot say anything about, let's say, the running time or the output of a computer program unless you run it from start to end. You can't even save one step on that. Those are notions of Kolmogolov complexity and undecidability. So they're like these very famous theorems from 100 years ago that say that you can't understand and trust any computation without rerunning it. And here all of a sudden, starting in the mid 80s, along come these amazing brilliant researchers, elassi Babai, Leonid Levin, Shafi Goldwa, Cilcilvio, Mikali, a bunch of others, and come along with these amazing results that say that you can do an exponentially smaller amount of work and still know with near certainty that things are fine. So how can this magic be okay? So first of all, it turns out that there's a huge difference between knowing things with 100% guarantee and knowing it with 100% minus two to the -128, which minus some negligible error.
00:22:45.678 - 00:23:16.860, Speaker B: So that's one thing that really helps you. The second thing is that these systems are somehow interactive. There's a back and forth going on between prover and Verifier. So that's how this magic came about. But with your permission, I'd like to sort of explain a little bit how they might work. So what is our problem here? You want to come to some facility and inspect it. And this is a huge facility and you want to know that everything there is just perfect.
00:23:16.860 - 00:24:18.318, Speaker B: Okay, the analog for us is that there's a batch of a million transactions, a million payments, and you need to know that all of these 1 million transactions were done correctly. So the first attempt, okay, what you could easily do, but will take you a lot of time, is check all of them one by one. Yeah, we don't want very well, but is not scalable. So the second attempt is to say, well, let me do a random sample, let me just pick a few and check if they're okay. Now, this will save you a lot of effort, but of course this by itself doesn't work because maybe there's just one transaction that is faulty and it will be very hard for you to find it or you won't stumble on it by chance. So here's the analogy that explains it. You're familiar probably with a kaleidoscope or a hall of mirrors where all of these different mirrors are refractoring and amplifying light and vision that comes.
00:24:18.318 - 00:25:21.860, Speaker B: So you see one thing repeated many, many times from many different angles. So what if you had some way of viewing these 100,000 transactions through something like a kaleidoscope, something that would refract and amplify any small speck of an error, so that even if there's one bit that wasn't correct, it will sort of be reflected from everywhere. Now, if you had such a marvelous and magical contraption, you could use it for coming into this facility, looking at the books with this kaleidoscope and then immediately seeing if something's wrong. So the mathematics of stark proofs and other proof systems, what it does is the analog of this kaleidoscope. It applies certain mathematical methods that will refract and amplify any error in the computation. And then you come and just sample it at a small number of locations and check if they're fine.
00:25:22.630 - 00:25:31.750, Speaker A: So is this another way of saying sort of the data gets rearranged in a way that makes it easier to audit using a probabilistic method?
00:25:32.330 - 00:26:20.882, Speaker B: This is precisely what happens. And the way the data is rearranged is using something called error correcting codes with very special properties. So basically, you ask the prover to rearrange the data and encode it using an error correcting code and then also to re encode the computation using these error correcting codes. And then you come and inspect it. So, like, the way a Stark proof works in the interactive mode is that the prover, first of all, does the processing, then encodes it, then commits to a merkel tree of the encoded data. And now we sort of committed to something. So either it's all perfect or there'll be problems everywhere.
00:26:20.882 - 00:26:35.900, Speaker B: And then after these commitments are on chain, the verifier comes and samples a very small number of locations, and the prover opens them up and shows what's written there. And then the verifier can see if there's a speck of dust or not.
00:26:36.670 - 00:27:15.014, Speaker A: Okay, so the second thing you said is, there's a big difference between proving something with 100% certainty and 99.9% certainty, and you gain, like, an irrationally large benefit from that, almost. So in turn, does this mean that the Stark proving system sometimes produces false positives? So basically saying that a computation was correct even though it wasn't okay, so.
00:27:15.052 - 00:27:29.014, Speaker B: All proof systems and all cryptography has a probability of failing. Okay. And this is true of Starks. It's true of digital signatures.
00:27:29.062 - 00:27:32.934, Speaker A: It's true of encryption because of collisions, hash collisions.
00:27:32.982 - 00:28:12.234, Speaker B: Collisions, yes. Yeah. They all have a probability of error. For instance, to give an analogy in an encryption scheme, let's suppose that the key is 128 bits long, right? So someone can toss coins, 128 of them, and there is a chance that the coins tossed will give you the encryption key. In that case, your system has been broken, your key has been compromised. The same thing, of course, with Bitcoin and so on. So the reason we say that encryption schemes are probably okay is because we think that it's very, very unlikely for this to occur for someone by chance to get it.
00:28:12.234 - 00:28:42.446, Speaker B: And our proof systems are exactly like that. So there is a probability of error, basically. There is a probability that the verifier will ask to inspect locations, that all of them look okay, even though the batch is not okay. But the probability of this occurring is, again, something like two to the -128. And we view that as. If you're worried about such errors, then you don't want to use blockchains at all because they have such error probabilities.
00:28:42.478 - 00:28:44.610, Speaker A: And probably not even use computers.
00:28:45.450 - 00:28:59.674, Speaker B: Yeah, you can't use any cryptography because all of cryptography has such probabilities of errors, right? Your keys could be compromised with such probabilities and then you don't want to use cryptography at all.
00:28:59.792 - 00:29:54.550, Speaker A: Makes sense. So we just zoomed way in how stocks work, improving systems work. So now I would like to zoom out a bit. How would you say that these we also call them validity proofs, right? Because we prove the validity of some computation. How would you say these validity proofs fit into blockchains in general? Because right now we can look at something like ethereum and that's how most people would still look at it as actually doing a lot of things that it doesn't necessarily need to do. And lately we are seeing a trend of unbundling these different jobs almost of a blockchain. So how would you say that validity proofs fit into that? And how will that sort of make the ethereum unbundle?
00:29:55.070 - 00:30:32.470, Speaker B: So if we look at the cost structure of transactions on ethereum, there are three things you pay for. You pay for long term storage, you pay for transitional witnesses and call data. So this is something that can be consumed on the fly, but you don't need to keep it, for instance, signatures. And the third thing you pay for is compute. So where validity proofs really help with is in exponentially reducing the cost of compute. So you can think of them nearly entirely removing compute. Another thing they do away with is the witness data.
00:30:32.470 - 00:31:06.642, Speaker B: Most of the witness data is not needed. So for instance, signatures, you don't need them anymore because they are basically just witnesses are used to support a computation. There is a bunch of choices that need to be made and the witnesses tell you which choices they are. But if you have a proof system, you don't need the witnesses and you don't need the computation. What you are left with is the storage. And with respect to storage, you can also save a lot. There's a lot that you can do by paying more with computation to save on storage in several different ways.
00:31:06.642 - 00:31:31.290, Speaker B: For instance, you can use storage more efficiently by applying compression algorithms. So compression algorithms, lossless ones will reduce the amount of accesses to storage by doing more computation when you access that storage. So that's another trade off. So even though validity proofs don't directly reduce storage, they can be used very efficiently to also reduce it in practice.
00:31:32.430 - 00:32:17.698, Speaker A: Another framing that I have seen is that ethereum has different the traditional blockchain consists of different layers, the consensus layer, the execution layer and then the data storage layer. And we can use validity proofs to unbundle these layers so that, for example, transactions get executed off chain by a proving system or by any party that can then use a proving system to precisely prove that the computation was indeed done correctly. Where does sort of the data storage element fit into this? Why does the data need to be stored.
00:32:17.874 - 00:32:56.334, Speaker B: Okay? The data needs to be stored because it's basically some variant of the I mean, it's liveness or also it also has to do with the double spend problem, right? Like consensus. Because suppose there are two different ways for the system to evolve, and both of them are legitimate. So Alice can pay Bob and she can pay Charlie. Each one of them is legitimate. So we need to know what state the system is in. Did she pay Bob or did she pay Charlie? And for that you need some storage, right? You need something that reflects the state of the system. Users need to know what is the status of their accounts.
00:32:56.334 - 00:33:46.894, Speaker B: If they're not tracking the blockchain all the time, they need someone to hold this data for them. So storage is still needed. As you said, I really like the way you described it, that the execution layer can be very much compressed using validity proofs. And under the execution layer, you can also put transmission of witness data that's also part of it and can be removed. And then with respect to storage, there's a little bit that you can improve using execution because, again, up to compression algorithms, what they do is they reduce storage up to some absolute value information, theoretic one, the amount of entropy in the data. But that's a long way. They can reduce things by paying with more computation.
00:33:46.894 - 00:34:00.598, Speaker B: So you can even help the storage layer using computation or the execution layer. But at the end of the day, you need the state. I mean, think of a banking system. We need to know what we have in our accounts. We need someone to know what that's.
00:34:00.614 - 00:35:30.470, Speaker A: The whole goal, right? I mean, we are doing this in order to have a shared state of accounts and update that in a way that has computational integrity. Got it? Okay, so your first product, as Starkware, is called Starkx, and this is effectively an off chain system where people send their transactions to and then it gets executed and then a proof gets posted to the main ethereum chain. StockX is so fascinating to me because I use Bydx and it feels just like so Bydx is one of four customers in StockX, and so I'm using it and it feels just like using a centralized exchange. But I know that it's noncustodial, so I can always withdraw my funds and it can only sort of change my account balance in ways that are sort of valid, right, that sort of adhere to their integrity. Yes, to their smart contract logic. So I would like to talk about dYdX as a case study. So one number that I have seen is that if you did the same if you did a transaction on dYdX, if you did it all on layer one, then it would consume in the range of like a couple hundred thousand gas.
00:35:30.470 - 00:36:13.140, Speaker A: Because it's a very complex system. It has a lot. Of markets. It has cross collateral positions, which is computationally expensive. However, if you execute it off chain via StockX and then post it on chain, and then you look at sort of the amortized cost of each of these individual transactions, you end up with something like 300 to 400 gas. This number is completely mind blowing to me. So please, can you explain sort of how do we get from, let's say, 300,000 gas to 300 gas? What happens in the meantime? Where do these savings come from?
00:36:14.970 - 00:36:58.066, Speaker B: Yeah, that's a really terrific question. What is happening? And it's even more mind boggling because actually, dYdX works in roll up mode, which means that all of the changes to storage on the l Two system are actually relayed as transmission on L One. So it means that even as you grow, some of our other systems are validium. We'll touch upon that later. But they have even a smaller footprint for their storage on chain. So what happens is something like this. If you look okay, we said that with proofs, the larger they become.
00:36:58.066 - 00:37:39.978, Speaker B: Sorry, the size of a stark proof and the time needed to verify it is exponentially smaller than the size of the batch. So it's like logarithm of that. So just to without writing down numbers, let's assume, I mean logarithm, the logarithm of a number is roughly the number of digits you need to in order to write that number. So let's work in base ten. So, for instance, the logarithm of a million is six. It's the number of zeros. The logarithm of a billion is nine, right? Because it has nine zeros and a trillion.
00:37:39.978 - 00:38:24.266, Speaker B: Right? 1000 billion. The logarithm of that is twelve. So let's examine now, we said that the size of the proof scales roughly like the number of digits. Okay, I'm simplifying this. So let's suppose that every digit you count the number of transactions in a batch that you're proving, and let's for simplicity, assume that the gas cost you're paying is the number of zeros in that thing times a million. So, for instance, if you took a million transactions, you have six zeros. And it means that your cost is going to be six times a million, because we said each zero costs you a million gas.
00:38:24.266 - 00:39:11.470, Speaker B: So you have 1 million transactions and you're paying a cost of 6 million gas. So what you end up is with amortized gas cost of six per transaction. Okay? Now let's look at a billion, a billion transactions. You're going to pay 9 million gas because we said that's the number of zeros. But now the amortized gas cost has gone down because you take 9 million and divide it by 1 billion. So that's much less than one gas per transaction. So we went down from six gas per transaction if the batch was 1 million to less than one gas, much less than one gas per transaction as we scale up.
00:39:11.470 - 00:40:22.210, Speaker B: So this is why, as you scale up with a system like Stark, the amortized gas cost goes down as the batch size grows. Okay? Now another interesting thing is mean, there's this on chain data in a roll up that you need to put. So you would expect each time you touch a certain position, you're going to have to write something on L One that says, what happened? But here another effect happens, which is suppose a certain account was involved in many, many trades, among those 1 billion trades. So the proof only needs to talk about the diff at the end. So here again, you amortize storage accesses across many transactions. So you're again saving on storage data by using more computation, which is this effect that we discussed earlier on. If you have computational integrity, if you can compress the execution layer, you can also save in a very substantial way on the storage layer.
00:40:23.030 - 00:40:48.698, Speaker A: I see. So what maybe to illustrate this even further. So I'm a user. I send a transaction to dYdX. What is the entire lifecycle of that transaction? Where do I even send it? I don't send it to Ethereum Miners, right? So I probably send it somewhere else. What happens to that transaction before the eventual state change gets recorded on the layer one?
00:40:48.864 - 00:41:12.210, Speaker B: Excellent question. So let's go through the workflow. You have a position, and this position is controlled by a private key. I mean, this position has a public key that everyone can see. It's part of the state of the dYdX system only you control the private key. So now you send an order. You want to do something.
00:41:12.210 - 00:41:51.758, Speaker B: You want to sell one banana coin and buy one orange coin. Okay? I'm simplifying it. So you sign this transaction with your private key, and basically you send it to dYdX, which is an off chain system. Okay? dYdX will first do some internal checks just to make sure that they're not passing on to the Stark prover something that can't be executed with integrity. So let's suppose this is okay. dYdX will do two things. First of all, they were in their internal system that talks to your front end sorry, to your, let's say, app.
00:41:51.758 - 00:42:36.430, Speaker B: They will note that hasu has now one less banana coin, but one more orange coin, and they will let you trade on on that information. So you as the user get a very immediate finality and you can continue trading. dYdX know that this is fine because they saw your positions. They know they can do the trade now. And they know that this will also be recorded on l One. Okay? Now, this order is sent to Starquare for it to be part of a Stark batch proof. And there it's sort of accumulated till we reach a number of roughly 10,000 transactions.
00:42:36.430 - 00:42:52.878, Speaker B: And all of them. Now, we want to prove that all of them have been done with integrity. So now we execute them in sequence. See that. Indeed you can. So we sort of check them again. And the reason we check them again is just because we won't be able to generate a proof for something that is invalid.
00:42:52.878 - 00:43:32.000, Speaker B: So we need to see that the execution is valid for each one of these things. So we check that the signatures I mean, we run the same program that we all know should be the right program, the one that checks signatures and that the trade is valid. And after we saw that all of these things can update the state of the system correctly, we generate a proof for this update to the state of the system. And then this proof is sent on chain with a new merkel route for the state of accounts. The verifier on chain checks this proof. If it passes, it basically replaces the state of the dYdX system, the merkel route of it with the new state. And this is repeated time and again.
00:43:33.110 - 00:43:56.870, Speaker A: Fascinating. Fascinating. Okay, I think I have a much better sense now of how it works. I have a few questions about this. So you said bydx they do their own internal checks, they check the transaction. But at this point the state change is not yet recorded on layer one. But they already give you as a user they already treat like it's final.
00:43:56.870 - 00:44:07.546, Speaker A: Treat it like it's final. Right. So would you say this is sort of can they never go wrong with this? Is this some sort of optimistic finality or is this sort of I would.
00:44:07.568 - 00:44:49.302, Speaker B: Say that what is happening is that they are assuming risk for a temporary period of time. So for instance, if they were, let's suppose that you don't have any banana coin to sell, but for some reason they said, yeah, this is fine. So suppose you have zero banana coins in your account and you still gave an order, I want to sell one banana coin and buy an orange coin. And they said for some reason there was some bug on their side or something and they said, yeah, sure, let's let that happen. So let's see what happens in this case from your side, you think that the trade occurred. I don't know exactly what happens because you supposedly had zero banana coins. Maybe now you have minus one.
00:44:49.302 - 00:45:32.678, Speaker B: I don't know. But what's going to happen is that they send this on to Starquare. Starkware's Prover is going to try and prove this and there's going to be something that says, no, this can't be done. Right. We cannot have negative balance for banana coin and basically we cannot prove this thing. So there's going to be some issue now on the dYdX side, even if we wanted Starquare, a prover cannot prove something that falsifies integrity, that doesn't have integrity. And if the system says that you cannot sell something into negative territory, I don't know, by the way, about the dYdX system, maybe you can actually be there in the red.
00:45:32.678 - 00:46:17.186, Speaker B: But assuming that this is not allowed by the program, even if we wanted, we won't be able to generate a proof. So now there's going to be some process by which, in this case, dYdX are going to have to do something about it, maybe talk to you, maybe cover it from their own banana coin fund or something like that. But at the end, for the proof to reach l One and be verified, we're going to need to have things that are basically legitimate transactions that compute with integrity. So that's why I say that dYdX will be assuming the risk if anything goes wrong along the process, then for the temporary period till it's finalized on l One and accepted. That's the risk that dYdX are assuming.
00:46:17.298 - 00:46:28.840, Speaker A: That actually ties into my next question. So how long usually until ten K transactions have been collected in a batch and a new update is made on the layer one.
00:46:29.530 - 00:46:55.680, Speaker B: So there are two latencies here. One is how long does it take for 10,000 transactions to accumulate? And then how long, once a train has sort of been closed and a batch has been closed, how long does it take for the proof to be generated and then accepted on l One? So roughly, at current rates, it's roughly the order of 1 hour for a batch to close. Of course, this varies along with time.
00:46:56.150 - 00:46:57.986, Speaker A: You mean for it to accumulate or.
00:46:58.008 - 00:47:31.926, Speaker B: For it to be yes, for 10,000. So the average TPS we have right now for dYdX is anywhere between, let's say, three and ten. So let's suppose it's five. Okay? So five TPS to reach 10,000, actually, in a batch size is 13,000. So 13,000 in an hour, there are 3600 seconds. So 3600 times five is roughly that's 18,000. Right? So 13,000 takes, let's say, 40 minutes or so.
00:47:31.926 - 00:47:52.706, Speaker B: So if it's at five TPS, right, every 40 minutes or so, you'll have a batch accumulated that can be sent, and then it takes several hours on our provers for such a proof to be generated. So all in all, it takes the order of several hours since the trade was done, and until you have finality on l One.
00:47:52.888 - 00:48:34.850, Speaker A: Got it. Okay. And then we said, you generate the proof, you send it to the chain, it gets mined. And this sort of is very interesting to me because in an optimistic roll up system, you need another third party. Again, basically the delegated accountability, right? You have these verifiers that reperform all of the computation and check if there has been any fraud and they are economically incentivized to do so. But this is not the case in the validity proof, right? It is the ethereum blockchain itself that checks if the computation is valid.
00:48:36.150 - 00:49:42.722, Speaker B: Yeah. So in a fraud proof system, in the optimistic roll ups, there are several big differences. One is that you're a user, so in order to trust the system, you either need to run a node of the fraud proof system, which again, if you want to increase the scale, means you need to buy a bigger and bigger computer. In the validity proof world you don't need to do that because you don't need to trust the party generating the proofs. In the optimistic roll up world you either need to get yourself such a big computer or else you will be trusting someone else who will trusting that someone else is watching the system on your behalf. And indeed, in the optimistic roll up world only when there is a suspected fraud does the system start to generate proofs by some binary search mechanism. So they take, as the name suggests, a more optimistic view towards the behavior of actors.
00:49:42.722 - 00:50:20.750, Speaker B: With validity proofs. You're taking a more stringent view and you're putting higher demands on the party, processing the computations and saying, look, we're not going to risk it, that maybe the watchtowers are down or is colluded with you or anything. And we're not going to be asking users to run these big computers. Every time you touch the system and you say that, you updated it with integrity. So please, prove it. And on the one hand, this puts more demand on this big computer. That is the prover.
00:50:20.750 - 00:50:28.390, Speaker B: On the other hand, everyone can now sleep very peacefully at night because every change comes with a proof.
00:50:29.130 - 00:50:41.050, Speaker A: Okay? So thank you for the explanation. So are there any trade offs with using StockX compared to layer one Ethereum?
00:50:41.870 - 00:51:51.210, Speaker B: Yes, there are several trade offs. Let's look at a transaction like a payment on layer one Ethereum versus Starkx. So on layer one Ethereum you have immediate blockchain finality and you have the security of L One immediately but you pay higher cost. And frankly, if you took all citizens of the, let's say European Union and each one of them wanted to do a single transfer once a week, so there's no way that Ethereum could process all of this period. So it just doesn't scale. Now if you do the same payments through Starkx then first of all you can easily service all of the citizens of the European Union doing more than even doing several transactions a day without changing anything in the L one of Ethereum. So you maintain inclusive accountability.
00:51:51.210 - 00:52:38.462, Speaker B: That's the main benefit. What you are sort of losing here is slightly higher time to finality, that's one thing. And the second thing? Well, depending on the data availability model you may be needing to trust other parties to keep the data or at least do so temporarily. This depends whether you're doing it in roll up mode or in validium or volition mode. But that's another potential trade off. Where is the data maintained? So you have longer time to finality, not instant, but let's say minutes to hours and you have much, much lower cost and much higher scale. That's the trade off.
00:52:38.596 - 00:52:53.140, Speaker A: Got it. What would happen if Starquest stopped processing any proofs for dYdX. So I'm trying to sort of explain where does the non custodial aspect of it come from?
00:52:53.670 - 00:53:59.938, Speaker B: Right, so all of our Stargx systems, these standalone layer twos, all of them have built into them escape hatch mechanisms so we can talk about dYdX. There is a mechanism in which a user can say to l One, hey, I want to retrieve all my funds, please. And in this case, the l One waits for this to happen by the L2. But if it doesn't happen within a certain time frame, basically the system freezes. In this case, the only thing the system allows is basically everyone to retrieve their funds from l One by saying, here's a path to my vault in a merkel tree. And remember that in dYdX it's in roll up mode. So basically, l One has all the information you need in order to construct where your account is and what is the path to the merkel route.
00:53:59.938 - 00:54:10.574, Speaker B: And then you can basically instruct l One to say, here's an authentication path to my node. Please give me all my funds, and the l One contract will do precisely that.
00:54:10.692 - 00:54:32.670, Speaker A: When we say that we prove our balance to the Layer One smart contract. This only refers to the last recorded Layer one state. Right. We cannot sort of anything that happened on Bydx in the meantime. So let's say we made any trades but they have not yet been proven to the Layer One. Then this is sort of the risk that we assume as the user.
00:54:32.830 - 00:55:17.278, Speaker B: Exactly. This was the risk that we said that dYdX was assuming. Exactly. Suppose it allowed you to do some trade and now this proof never arises because Starquare blew up and dYdX blew up, and there is no proof of this very last batch never appears. And now there's an escape hatch. So this very last epoch, this very last batch never appears. And then I guess you as a user will have to somehow sort it out with dYdX or something because you got an assurance on your app or something through the l Two that this should have been recorded, but it never arrived on l One and now people are retrieving their funds.
00:55:17.374 - 00:55:37.622, Speaker A: Well, as a user, as I understand it, I just need to trust either dYdX or Starkware. Right. Because if Starquare is honest, but dYdX is faulty, then Starquare could still process the final batch to make the proof and send it to Layer One.
00:55:37.676 - 00:55:54.554, Speaker B: Well, assuming dYdX sends it over, depending what kind of fault dYdX has. So suppose dYdX just tells you that it's okay, but it doesn't tell if we if Starquare got that information, then yes, this batch will be on chain and then your state will be updated.
00:55:54.682 - 00:56:04.654, Speaker A: Yeah. Whereas if dYdX is honest and Starkware is faulty and doesn't make another batch, then dYdX can just internalize the risk from their treasury, right?
00:56:04.852 - 00:56:05.502, Speaker B: Yes.
00:56:05.636 - 00:56:09.238, Speaker A: And find another prover to replace Starquare.
00:56:09.354 - 00:56:09.970, Speaker B: Yes.
00:56:10.120 - 00:56:11.250, Speaker A: Okay. That makes sense.
00:56:11.320 - 00:56:11.554, Speaker B: Okay.
00:56:11.592 - 00:56:46.614, Speaker A: So to me, I can only say that is an acceptable risk for all of the cost saving that I get. So we have talked about StockX. StockX is basically what in the context of multi chain blockchains and so on, we would call an application specific blockchain. Right? It's one blockchain. It's not even really a blockchain, but it's basically one system per application. But now you are launching a second product called StarkNet. Right.
00:56:46.614 - 00:57:07.030, Speaker A: So the known problem with application chains, there are many advantages, but the known problem is sort of lack of composability within the same shared state. So what is the difference between StarkNet and Starkx?
00:57:07.530 - 00:57:50.580, Speaker B: It's like the difference between a company running a big computer and the cloud. So Starkx is like a big computer that allows specific businesses, ones who have talked to us and asked for this to benefit from higher scale. So think of that as like the big computer that lets you scale in a non custodial way with computational integrity on the blockchain. So it's like the analog would be it's as if Duidx went and bought a massively big computer that allows it to process things very well. But it's their computer, right? You can't use it. And same thing with diversify immutable. Each one of those has their own sort of computer.
00:57:50.580 - 00:58:21.870, Speaker B: And then Starconnect is a little bit like the cloud or the Internet or a blockchain. Maybe that's the best analogy. Like Ethereum. So if you're a developer and you have a brilliant smart contract that you would like people to use, well, you can just go write it up, deploy it on StarkNet, and people can send transactions to it exactly like Ethereum. So that's the difference between Starkx and StarkNet.
00:58:22.370 - 00:58:44.242, Speaker A: So on Starkx, it's basically a permissioned blockchain in the sense that only one party can deploy their smart contract code. Others can still read it and audit it, that it does what it says it does. StarkNet is more like Ethereum itself, where anybody can build their own applications, right.
00:58:44.376 - 00:58:57.682, Speaker B: But it has more scale. That's the big difference. It has more scale than Ethereum. It takes the execution layer and it has the ability to compress it from the point of view of layer one Ethereum. So you get much higher scale.
00:58:57.746 - 00:59:24.900, Speaker A: Right. So it retains this major benefit of StockX, but it makes it available on a shared composable execution layer. Pretty much, yes. Okay. Will anything change with regards to the prover? So right now in Starkware operates the prover for everyone. Are there any plans of changing that in the future?
00:59:25.510 - 00:59:54.540, Speaker B: Definitely a very exciting stage of StarkNet and one that is on our plans, the major next stage. I mean, there'll be a whole bunch of small updates. But the big next change, which we call Universe, will be decentralizing. The sequencer and the prover so that it's not Starkware running those. It's actually anyone may run them.
00:59:55.550 - 01:00:10.560, Speaker A: If I'm thinking about launching an application today, imagine I'm a developer. Is it now strictly better to build on StarkNet? And does this make sort of Starkx obsolete, or what are the trade offs between the two?
01:00:11.490 - 01:01:12.514, Speaker B: That's a really good question. In the long run, what will the difference be? So definitely, if you're a developer, okay. Starkx right now is catered towards very specific, highly ubiquitous cases, but still very restrictive. So it is very good for massive payments, massive trading, massive NFT Minting and trading. But, for instance, if you want to build a gaming application on using Stark technology, starkx doesn't deliver that. So if you want general purpose new applications or you want to have something that is generative art, something that involves some CryptoKitty style thing, then definitely you can't do that on Starkx. The more interesting question in the long run is, suppose you want basically the functionality of Starkx today.
01:01:12.514 - 01:02:27.542, Speaker B: You could write it on StarkNet, or you could talk to Starkware and get a Starkx instance. And the question is, what will be the steady state, let's say, in two years? We don't quite know. Our thought process on this is constantly evolving, actually, inside our team, some folks have came up with some very interesting, brilliant ideas on how they might live together in the future in ways that both Starkx lives and StarkNet lives, and both of them thrive and benefit from one another. But taking a very high level view, the big benefit of Starkx is that because it focuses on very special use cases, it's very likely that some customers will want that and get the much higher scale and higher control. For instance, if you're a visa and you want to use Stark pay, which is part of Starkx, you want payment processing. Maybe you will want to use a Starkx system and not write it as smart contracts over StarkNet. So we don't know yet what will be the end game with Starkx versus StarkNet.
01:02:27.542 - 01:02:35.280, Speaker B: My guess is that they will both have a very long and successful life, even as StarkNet increases in size.
01:02:35.970 - 01:03:09.414, Speaker A: That's my intuition as well. I think you can always do better at making one specific application more efficient than having a shared system. Right. That works for many applications. Maybe this is a good time to talk about Cairo. So both of your systems require any application to be written in the Cairo smart contracting language. So it's not possible to take an application that runs on Ethereum today and just deploy it either on Starkx or StarkNet.
01:03:09.414 - 01:03:42.450, Speaker A: And as I understand, sort of your business model for Starkx has been almost software as a service. Like, so dYdX did not write their own application in Cairo, but you did that for them almost as also, like, doubling as sort of a proof of concept. Right. But this is different for StarkNet. You don't offer this service anymore for anyone who wants to deploy on StarkNet, they have to write their own application, right?
01:03:42.520 - 01:05:14.126, Speaker B: So what happened with Cairo and the way it emerged is also a very interesting story. So initially, like all projects that work with proof systems, Zcash and many, many others, you start writing by hand various circuits, or in our case they're called heirs, algebraic intermediate representations. And this is a little bit scary from the point of view of developing code because it's the analog of putting down a circuit comprised of NAND, gates and wires to do some computation. So beyond a certain scale, it just gets very hard to do, right? And then folks inside our team came up with a much better idea. So, I mean, the Cairo White paper has three co authors, co creators of Cairo, leo Goldberg, Shaho Papinian, Michael Yabsev, who's also my co founder. They said that why don't we write a sort of small circuit that is the analog of a CPU and let's make it very, very simple. And first this was done for internal reasons, so that we can just write more elaborate code for more elaborate systems.
01:05:14.126 - 01:06:19.846, Speaker B: And for instance, dYdX, probably the logic there, which is very complicated, couldn't have been achieved in the sort of know NAND wires model. No one would sign off on it as being secure, it was too complex. But once you have a programming language, you can write code and audit it and inspect it. And that's how Cairo came about. And then we wrote all of our Starkx systems in this way. And then what happened was this turned out to be so efficient and usable that we said, wait, why not open this to the whole world to use? And that's how StarkNet came about. So today we have StarkNet whose operating system is also written in Cairo, and people can use Cairo for writing smart contracts that basically came that are using a programming language and they're using a programming language that is very battle tested with hundreds of billions of dollars of trades and several different systems on them.
01:06:19.846 - 01:06:22.486, Speaker B: And that's I think very comforting to know.
01:06:22.588 - 01:07:20.682, Speaker A: This approach is quite different, right, from the one that sort of other layer tools take. Because if we, for example, look at optimism, they have been struggling to get any traction compared to the simple EVM forks like, let's say, like Polygon or Binance smart chain or other chains that simply fork the EVM and where you have what they call EVM equivalent. So you can just take your code and just deploy it there, whereas in the original OVM you had to make even just minor changes to the code in order to get it to work, but that still sort of held them back substantially. So I would be interested in so why do you have the confidence that you can require users to rewrite everything from scratch in a completely new programming language and still sort of compete with those who offer EVM equivalents right.
01:07:20.736 - 01:08:10.054, Speaker B: So the bet we made with this approach is something like this when there is huge need and demand for something. I mean, of course it would be really great if you could just press a button and have your code, which let's say you wrote in Solidity work on some layer two. By analogy, it would be really great if you could just take your Python code and press a button and have it be a smart contract in Ethereum right. Or any other program that you wrote. Right. But multiple teams of developers have gone through the effort, substantial effort of learning Solidity or other languages. Yule.
01:08:10.054 - 01:09:12.238, Speaker B: And whatnot in order to program for this environment because they recognize that blockchains are something new, they are a bit different. They have a set of complexity parameters that operate differently. And we're still too young to believe that this technology can just work out of the box with everything just perfectly fine on your favorite earlier code base. Now we believe this is the bet we're taking that with StarkNet, the same thing is going to happen where people that need the scale and the compressed execution layer that is offered essentially only by StarkNet. If you really think about it with the security level of L One and without any change to the assumptions there, then we think that they will understand that it's in their interest to learn this new framework and work in it. And we see some very encouraging steps. A lot of very good teams are working on it.
01:09:12.238 - 01:10:14.570, Speaker B: Now, I just want to say that even if we had and of course there's a very important project being executed right now by Nethermind called Warp that is exactly a transpiler from Solidity to Cairo. So this will exist. Okay, but I just want to say that you mentioned dYdX earlier. If they just took their previous set of L One contracts and compiled them and even if everything would have worked well, they wouldn't have achieved the functionality that they achieve now with our system. And the reason is that most Solidity code has been written to work within the constraints, the execution layer constraints, the limited gas of layer one. Once you move to StarkNet, you have nearly unlimited computation. So even if you continue to work in Solidity, you would significantly rewrite your applications to benefit from that larger compute.
01:10:14.570 - 01:10:26.230, Speaker B: So if you're going to rewrite your computation and you want to maximize its utility, we see good indication that developers understand that you're going to want to rewrite it in the native language for that environment.
01:10:26.970 - 01:11:01.486, Speaker A: That makes a lot of sense to me. So it's a different paradigm. But also you may want to change your business logic anyway, right? For example, cross position margin just doesn't work on layer one. It's too expensive to compute. But if you want to have a derivatives exchange that competes with other derivatives exchanges in a computationally cheaper environment, then you. Have to offer those things or you will get out competed. So it's not just not like you could port it.
01:11:01.486 - 01:11:23.510, Speaker A: And the same goes for something like Uniswap. It's not like you can be successful porting Uniswap to StarkNet. Right. Because much better. Like Uniswap only works because it's sort of the most efficient thing in a very computationally constrained environment, but it doesn't mean that it could compete in a less computationally constrained environment. And that's why you want yes, exactly.
01:11:23.580 - 01:12:01.874, Speaker B: I don't think you'll see on Wall Street AMMS and the reason you don't see them, even though trading has been there for hundreds of years, the reason you don't see them is because they are much less efficient than standard order books and things like that, and dark pools and whatnot. The only reason they thrive? And by the way, as a consequence, I think they will thrive only temporarily, because what's going to happen is that StarkNet and maybe other L two S are going to offer a noncustodial experience that is much more standard trading and not AMMS. And I think that's where the market will go.
01:12:02.072 - 01:12:15.590, Speaker A: Completely agree. So what has the early feedback been from developers about Cairo? And if I'm a developer, how do I best pick up Cairo today? What are the steps that you recommend here? And the resources?
01:12:17.050 - 01:12:47.780, Speaker B: So, like every new thing, new technology, there's a lot to improve. So definitely we get a lot of tremendously valuable feedback on features that need to be added. And I'm not going to whitewash it. These are the early stages of the system. So you're coming to the new world and you don't have as much facilities as you had. This was the same for Solidity in the early days. That's perfectly fine.
01:12:47.780 - 01:13:37.138, Speaker B: I think we are pleasantly surprised by how positive the reception for StarkNet and Cairo is. So Argent yesterday already published their wallet that will allow you to interact on StarkNet. It is, of course, major parts of it are written in Cairo. Their experience was a very good one. Many exchanges and AMMS are building. I think they are very happy with the level of support that they're seeing. So I think overall, we're very pleasantly surprised to see the reception and then to learn it.
01:13:37.138 - 01:13:50.680, Speaker B: I will basically send you some links that you can just post. And basically, if you go to STARTnet IO or to Cairolang.org, you will basically have all the information and documentation you need to get started.
01:13:51.290 - 01:14:19.390, Speaker A: So the last topic of the day that I would like to discuss with you is scaling the verifier and the prover. So we discussed earlier. It's only true scalability if at least the verification cost can stay, maybe not constant, but at least it grows much slower than the cost of actually performing the computation.
01:14:19.810 - 01:14:20.222, Speaker B: Right.
01:14:20.276 - 01:14:34.210, Speaker A: So to get a better feeling of this and also how sort of the Stark system is going to evolve in the future, I would like to unpack the different costs that go into it for the end user.
01:14:35.190 - 01:15:37.750, Speaker B: Okay, so let's unpack what happens as you scale up, which in our case means that you increase the size of a batch, right, the size of a batch being proved, let's say from 1000 transactions to 10,000, then 100,000 and so on. So what's going to happen to the cost? The amortized proving time, which means the amount of time you're spending proving per transaction will grow slowly and it grows like N times logarithm of N. So again, to take the analogy we had earlier on, for a million transactions taking logarithm to be base ten as the number of digits. For a million transactions, that means six steps per transaction. Whereas for a billion transactions, it means nine steps per transaction. Amortized. So the amortized proving cost per transaction has gone up as the scale increases.
01:15:37.750 - 01:16:35.990, Speaker B: On the other hand, and this will already discuss the verification time, the amortized verification time drops dramatically. So it dropped from six per transaction in assuming 1 million gas for every gas for every digit. So we had like amortized gas in verification of six gas per transaction, if it's a batch of size 1 million. And as you go up to 1 billion, that becomes a negligible fraction of a single gas per transaction because it is 9 million divided by 1 billion. So that's roughly like 10 million divided by 1 billion, which is one over 100. So you went down from six gas per transaction to 1% of a gas to one over 100 gas. So 100 transactions cost you one gas.
01:16:35.990 - 01:16:44.400, Speaker B: So this is very dramatic on the verification side and you have a very slow amortized increase in cost on the proving side.
01:16:44.930 - 01:16:58.482, Speaker A: Wow. Actually, I thought that the verification cost grows very slowly, but I thought that the proving cost would grow linearly with the number of transactions that you put in. So I was wrong about that.
01:16:58.536 - 01:17:27.478, Speaker B: Grows slightly worse than linear. It grows mathematically. We call it quasilinear, which means as n goes to infinity, your proving time scales like n times a polynomial in the logarithm of n. So n polylog n. So it means if we take polylog n to be just the number of digits to represent n, that's how you get six times n for 1,000,009 times n for 1 billion.
01:17:27.654 - 01:17:40.078, Speaker A: So let's using, for example, like n of 100 transactions, if you scale that to 1000, the cost, the proving time, does it increase also by a factor of ten or less or more?
01:17:40.244 - 01:17:41.920, Speaker B: Slightly more than ten.
01:17:42.290 - 01:17:52.050, Speaker A: Okay. So what can be done sort of in order to lower the cost of that? What is possible in the future of proving?
01:17:52.390 - 01:18:42.580, Speaker B: Yeah, there are a number of things that can be done. The first thing that is most immediate and likely the first thing that starcoil will be done and work on this already has started is use of recursive proving, which basically allows you to take, let's say, instead of moving. From a batch of size 100 to 1000. You could move from a batch of size 100 to ten proofs of size 100, but then prove that you verified all ten of them. And this can allow you to have now ten provers running in parallel, each one of them for 100, and then some additional costs for verifying the ten proofs. And this will allow you to reduce latency and increase scale. So that's one thing that you can do.
01:18:42.580 - 01:19:42.226, Speaker B: Another thing that you can do is start changing some of the elements inside the proof. For instance, right now for security, we're using as our cryptographic primitives. For instance, the hashes that we use to keep our data, save our data and commit to it, is a Peterson hash, which is very secure. It's provably secure. You can reduce it to the discrete logarithm problem, but on the other hand, it's not as efficient as some newer constructions, specifically Poseidon and Rescue and Gmimsi and things like that. So now all the other validity proof systems are using these newer, know, Aztec, Execsync, Miden, Mir, Mina, Zcash, they're all using versions of Mimsi, Poseidon or Rescue. And those primitives are roughly ten x more efficient than Peterson.
01:19:42.226 - 01:20:13.278, Speaker B: So we could replace that and get like ten x improvement on that part of using these cryptographic primitives. We could change things like there are other parameters of the basic proof systems that we haven't modified recently and we can play with those like decrease the size of the fields to get more efficient proving time. And then of course, down the line you have hardware, like dedicated hardware for generating proofs once there's enough demand for proving machines.
01:20:13.454 - 01:20:48.540, Speaker A: Right? So we saw this with regular Asics for Bitcoin, first there were CPUs, then GPUs, FPGAs, and then eventually, or not in that order, but then eventually we got Asics that are highly specialized machines that only do char 256 basically, and nothing else. And so you expect this to emerge. How big are the possible savings from something like that? Because it is very complex still, right? It's not as simple as just running sharp 256 over and over again.
01:20:50.130 - 01:22:00.354, Speaker B: Yeah, well, it's a bit complicated to estimate, but I think that with recursion there's a very high potential for because you can now paralyze proof generation and then also reduce latency and increase proof batch size. So you could easily get anywhere between a factor ten to 100 improvement sorry, or maybe even 1000 down the line just from that thing alone. Replacing the crypto primitives can give you on that part, it gives you like a ten x improvement, but that's only part of the proofs being generated, the cryptographic primitives. So let's say, I don't know, you say between that gives you anywhere between two and five x on top of whatever went into the these are all accumulative, it's not one and not the other. Then if you change the field size, you probably get another two to four x improvement in many aspects of the system. And again, all these can multiply and compound one another. And then hardware, it's really hard to say because we haven't really started even thinking about that.
01:22:00.354 - 01:22:12.760, Speaker B: And the core system isn't yet stable enough for us to say this is the way you're going to use hardware. But I would just guess another ten to 100 x just because hardware has that effect.
01:22:13.210 - 01:22:25.900, Speaker A: Makes sense. So does this mean you will use this recursive proofing approach also to create one proof for a bunch of different StockX plus StarkNet and so on?
01:22:26.590 - 01:23:08.300, Speaker B: Yes. So some version of this already is in production today. So the Sharp service, which means shared prover, what does it do? It takes several different programs, let's say one for Diversify, one for dYdX, one for StarkNet, and it generates one single proof that all of these different systems advance correctly. This is the sharp capability. So it already takes several different things and puts them together. So this is already working and it's offering great savings to all of the different applications and it's especially important for small applications. So StarkNet will be Sharp proofed with the other things already from the start.
01:23:08.300 - 01:23:27.310, Speaker B: Recursion goes one step beyond that because now one of those programs being proved as part of the Sharp batch is that some proof has been verified so you can get this exponential savings sort of amplified and multiplied.
01:23:27.650 - 01:23:49.240, Speaker A: Okay, so I think that covers the proving cost. Next we have the verification cost, and this is the cost of basically posting the final proof to the layer one. And then it gets verified by so in this case, who verifies it? It is the dYdX smart contract, right? For example.
01:23:51.450 - 01:24:55.782, Speaker B: There'S a Cairo Verifier that verifies that basically it gets basically a hash of a program, which is the dYdX program, and then a proof and a state update. And it basically says, okay, we checked this program and this is the new state and everything's fine. And this then goes to the dYdX contract that basically gets this as input and says, okay, so now we're fine with updating the state of our system. So they're like two different contracts. And in terms of reducing the gas cost for verifying a proof so one thing that's going to be seems that it likely to happen even if we do nothing, is EIP 44 80, which will reduce by roughly five x the cost of the gas cost of transmission. And of the several million gas that we're paying per proof, let's say 5 million, nominally, roughly half of that is going well, it depends and varies. But let's say half of that or 70% of that is transmission.
01:24:55.782 - 01:25:14.530, Speaker B: And if that part reduces by a factor of five x, then you'll see our gas cost going down by that by, let's say, factor two to three. So from 5 million maybe to one or 2 million gas cost after EIP 44 80.
01:25:15.110 - 01:25:57.120, Speaker A: That's okay. And then the final cost is that of storing the data. In this case, I think we are not talking about the final state update right, which is not big. We are talking about the different balance changes that led to this update. Right. So why do we need it? According to my understanding, it is so if the current sequencer leaves or the current prover, then that somebody else can take the last state and the transactions or the state changes and continue the work of the prover. Right, okay.
01:25:58.370 - 01:27:05.394, Speaker B: Yeah. So for state updates well, I mean, there are several answers. One is that you save a little bit because you only need to update the final state diff and you already save there. With things like in the dYdX system, in roll up mode, you can try and do compression and then the limit to that will be the information theoretic limit to how much entropy does a change have. And the next phase, which will be also on StarkNet will be to have layer two data availability decentralized and incentivized by various crypto economic mechanisms so that users will have the option of trading security versus cost. And this is something we call volition. So either you keep your data on L one in roll up mode and then you pay slightly more or you keep it on layer two and then you pay slightly less.
01:27:05.394 - 01:27:09.960, Speaker B: But you take the risk of this security of layer two.
01:27:10.410 - 01:27:22.726, Speaker A: So if you keep it on layer one, then you know it's definitely going to be available unless the layer one breaks. Whereas if you decide to, I mean, you could even say I want Eli to store it on his phone.
01:27:22.828 - 01:27:23.058, Speaker B: Right.
01:27:23.084 - 01:27:45.806, Speaker A: In theory you can choose anyone who would be willing to store your data and sort of make a bet that they will provide it to you when you need time. So the time when you need it is only if you want to withdraw from the layer one contract. If it's sort of in the unhappy case where the sequencer leaves, that's the only time where it really matters, right, where your data is stored.
01:27:45.998 - 01:28:01.190, Speaker B: Well, it also matters like if you picked an untrust, a faulty or malicious data availability provider then the longer you keep your data with him, then the more you are at risk.
01:28:02.250 - 01:28:23.040, Speaker A: Is there any risk for me sort of in the happy case, like let's say dYdX progresses normally and Starkware is honest as well, but I use a data availability provider who sort of becomes faulty. Does this have any risk for me without dYdX or Starkware becoming faulty themselves?
01:28:24.770 - 01:29:19.840, Speaker B: Well, I think in the dYdX and Starware system, first of all, it's roll up mode. But let's know on Diversify or Immutable X, which is validium basically Starquare and Diversify or Immutable are already relying on a data availability committee. So as long as we and they are not faulty, you will have your data. But I thought that on StarkNet you were envisioning some very interesting system where, just like, you could decide on which cloud you want to keep your photographs. Maybe some contracts will say, look, you just pick your data availability provider and when you need it, on our layer two, just tell us what is the path to your state and we'll take it from there. I could see such a thing happening in a market for that. And then if this Amazon cloud analog is faulty, then you'll have problems.
01:29:20.610 - 01:29:31.394, Speaker A: Okay? So it puts me in problems even if I don't want to withdraw from the layer one. Because even as the user you will.
01:29:31.432 - 01:29:58.860, Speaker B: Need to provide some of these smart contracts with information about the location of your part of the storage and maybe those smart contracts don't want to know, like you picked your cloud, you decided where to put it. So my smart contract doesn't need to know that each time you interact with it, you go and tell me that you have yay many funds here and then it will be your problem.
01:29:59.310 - 01:30:45.260, Speaker A: So in a volition, every user, even though they're in the same state, they can choose where they want their data stored. I think that's one of the coolest things that I've heard in the last year in crypto because then every user only needs to pay for the security that they actually need, right? If they want to make a large transaction, for example, they might want to prefer more security. How does it work if two users are in the same state and one user has a faulty data availability committee? So the other transactions of the other users who have a good provider, they will continue to get processed, right? What happens to those of a user whose data is no longer available?
01:30:46.270 - 01:31:52.302, Speaker B: Well, at the very least it's that user's data problem. I just want to stress that the layer two will likely have its sort of layer two validium, which will probably have higher securities and maybe cost a little bit more than the various validiums that we be offered by various entities. And that's probably going to be safer because we'll crypto incentivize it properly. I hope so. Now, if you have a smart contract that is willing to allow you to work in full, free ranging volition, which says you say where your data is kept and it's your business to bring the latest state update, then I would imagine that the DAP developers for those smart contracts would probably want to have some separation where if your data availability provider did something really bad, then it will be sort of in your compartment. So maybe you can't trade anymore till you sort it out, but it won't stall the system. That would be good design.
01:31:52.356 - 01:31:53.380, Speaker A: So maybe.
01:31:56.310 - 01:32:06.470, Speaker B: All the other participants who have their data can continue using the system and only those whose data has been compromised by that faulty provider are at risk.
01:32:06.890 - 01:32:31.654, Speaker A: Yeah, okay. That would make sense. Got it. So, yeah, okay. To summarize, we have basically the end cost of using the system depends on the proving cost, the verification cost on the layer one, and then the cost of storing your data. And there you have many different options for what you want to choose. So we earlier talked about sort of the cost of using dYdX.
01:32:31.654 - 01:33:17.034, Speaker A: We said it would be, let's say, 300,000 gas on layer one per transaction, but using stockware or StockX, it's only in the range of 300 to 400 gas. But this did not include the proving cost. Right? This only. But it's in roll up mode. Right. So it did include the data storage cost, but not the proving cost. So how big is the proving cost right now if we were to factor that in? And can you share if sort of after the fees generated from dYdX, if the system is already profitable to run, because this seems kind of relevant, if we were to decentralize this soon, then it must be profitable for the other provers to run it.
01:33:17.034 - 01:33:17.610, Speaker A: Right.
01:33:17.760 - 01:34:18.590, Speaker B: So without going too much into particular contemporary business details that are very much going to change by the time we decentralize in open provers and sequencers for everyone. Suffice it to say that the Amazon or cloud costs that we're paying for running the proving servers are negligible much less than 5% of the l one costs that we are paying significantly less. So the gas cost that proofs are costing, each time you submit a proof, you pay a certain, let's say if it's 5 million gas, right? So you pay I don't know how many thousands of dollars that costs you. If you look at how much did you pay to Amazon or Google for the proving that went several hours of proving for generating this, it is negligible compared to those thousands of dollars of gas fees.
01:34:19.410 - 01:34:26.740, Speaker A: Oh, interesting. I would have thought that the proving cost is sort of the big one, but it's actually the verification cost today.
01:34:27.110 - 01:34:42.886, Speaker B: Well, gas is the gas, yes. Since the start, the cost of running the prover machines has always been completely negligible compared to the cost of putting the proofs on layer one, the gas cost.
01:34:43.068 - 01:35:04.240, Speaker A: In what way is this? Or if at all, is this sort of relayed to users today? You said there's two smart contracts involved, right, with eventually like getting the proof on layer one. So starkware. Smart contract. So I assume that you are currently subsidizing this, but then for dYdX's part, they are subsidizing it.
01:35:04.770 - 01:35:45.978, Speaker B: Right. So our customers who are using Stark systems, they basically pay the gas cost, the l one gas costs. We incur the Amazon and compute cost. And the way I think all of them work is they don't charge their users for gas, but because they have various kinds of fees, transaction fees and whatnot. They cover it from there or they subsidize it in some cases. So I think that, for instance, trading of NFTs on Immutable X currently is free. So it means that whatever incremental cost for gas you have, and there is some cost, they are subsidizing that part.
01:35:46.064 - 01:36:10.126, Speaker A: Right. I think that actually intuitively, that makes much more sense that eventually on Ethereum layer one, it will only be applications paying to prove sort of their computation, and it won't ever be like regular users. Right. You will only have proof, like some maybe some settlements, but mostly just verifying different proofs of off chain computation.
01:36:10.318 - 01:36:26.934, Speaker B: That's certainly the case on all of our layer two systems right now, the Starkx systems. Now, in StarkNet currently, there are no fees. Later on, there will be fees, and these fees will be designed to cover the cost of gas on L One.
01:36:27.052 - 01:36:52.510, Speaker A: Got it? Thank you, Eli. This has been a fascinating conversation. I learned so much about Starkware and Starkx and StarkNet, and I must say this, to me, it's one of the most interesting pieces of technology being built right now. And as far as I can see, it's sort of the only what happens sort of in the rollup space to me is the only thing that looks like real scalability to me in crypto.
01:36:52.850 - 01:37:05.778, Speaker B: Thank you, Hasoo, both for having me on this and also for carrying out what is probably the deepest, most profound technological podcast. Blockchain.
01:37:05.874 - 01:37:07.638, Speaker A: Thank you so much. Have a good day.
01:37:07.724 - 01:37:08.100, Speaker B: Thank you.
