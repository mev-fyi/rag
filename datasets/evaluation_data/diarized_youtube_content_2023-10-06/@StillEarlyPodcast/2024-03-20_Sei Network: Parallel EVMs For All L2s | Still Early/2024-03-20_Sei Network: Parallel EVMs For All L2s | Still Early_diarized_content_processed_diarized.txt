00:00:00.360 - 00:00:23.990, Speaker A: Alright, just a quick one before we get going. So please don't take anything you hear in today's episode as financial advice. Please speak to a trained professional if you do wish to participate in markets. Crypto is inherently risky and you will lose all your money, particularly if you listen to us for financial direction. So enough of that. Please give us a, like, subscribe, enjoy the show and see you next time. Alright, welcome back.
00:00:23.990 - 00:00:37.564, Speaker A: Another episode of the Blockmates podcast. And today we've got probably one of the most anticipated projects in the whole space that everyone keeps talking about. And we've managed to drag j on from, say, labs, say network, all that kind of good stuff. But Jay, how are we doing?
00:00:38.224 - 00:00:40.244, Speaker B: Doing well, man, it's been a crazy day.
00:00:42.184 - 00:01:02.544, Speaker A: We were just previously chatting off, like, I complain about having like a media company have to kind of coordinate probably about twelve to 13 people across the plane. You guys are trying to run a distributed system whilst running an upgrade, which is probably, probably industry defining, saying how difficult it is to kind of align.
00:01:03.604 - 00:01:12.784, Speaker B: It's been a fun past six months, I'll say that. It's also really cool that everything is like, things are just moving in crypto. So I think there's just this energy that was not there this time last year.
00:01:14.084 - 00:01:27.044, Speaker A: Yeah. Is it? I suppose it's more difficult to build now because of the chaos that is happening in the markets and it's like everyone gets frantic and sometimes in a bear market and down market, you remove that noise.
00:01:27.944 - 00:01:42.764, Speaker B: Interestingly, for the saleabs team, I feel like it's still the exact same pace that we're moving at. We tend to not really focus much on a lot of external things, so there's less noise for the devs. And I think we've been able to exact operate at the exact same paces before.
00:01:46.904 - 00:02:06.312, Speaker A: I think we did something on spaces a long time ago, but I. I'd like to just kind of set the scene because I think there's a good kind of segue based on your backstory. And I know some people don't like whole backstory stuff, but bear with me, there is a good segue here. So can you fill people in on your, on your background and why it made sense to kind of go ahead and build what you're building now?
00:02:06.488 - 00:02:35.040, Speaker B: Of course. So my name is Jay, I'm a co founder over at Tailabs. My story is that I grew up in the Bay Area, so in the San Francisco Bay Area, Silicon Valley. Met my co founder through speech and debate back in high school. And I initially got into crypto back in 2017. So at that time, my roommate, he was going through binance labs. Both of us ended up tickering on a couple of different smart contracts together.
00:02:35.040 - 00:03:07.668, Speaker B: And that's how I initially got into the building side for crypto, I guess. I also ended up studying computer science in college. And back in 2017, I ended up finding out about. I both found out about Robin Hood, and I also ended up chatting with their team back then. So back then, 2017, Robinhood, it was basically just two houses, essentially across the street. This was back in Palo Alto, and one of the houses, it literally took, like, the bedrooms in the house, they set up standing desks over there, and that's where the teams work from. They would have, like, one engineering team, like, one of these.
00:03:07.668 - 00:03:28.256, Speaker B: These rooms. So this is right around the time that they had raised their series C. So it was like one of the hottest companies in the Bay Area at the time. Like, there's every company in the Bay Area. Like, everything is very cyclic in the Bay Area. And, like, for companies like Robinhood, like the series B, Series C, time is like when they are the hottest companies to be working at. And actually looking back at it now.
00:03:28.256 - 00:04:08.060, Speaker B: So the new grad class after that fundraiser was done was around 20 people. I think eight of those people have started venture back companies. So it's just a really, really kind of, I think, different environment than what you would see, like most other companies when you grad classes. So back then, it was just like everyone was coming over from places like Pinterest, Airbnb, Google, Facebook, Citadel, jump. So it was just like a really, really exciting place to be. So I initially ended up joining Robinhood because there were a lot of really smart people there, and I spent almost four years over there. I started off as a software engineer, eventually became an engineering lead, and I was an engineering lead when the entire GameStop saga happened back in 2021.
00:04:08.060 - 00:04:59.568, Speaker B: So for any listeners that might not really remember what was happening back then is there was a bunch of people that are buying into stocks like GameStop, AMC, and roughly like ten other stocks, and these stocks were essentially pumping to the stratosphere. Like, initially it was fundamentals driven, where there were people like Keith Gill, who was like, he was on Wall street bets. He was talking about like, oh, like, like, just based off fundamentals. GameStop is like, undervalued $5, and he was just, like, posting stuff on Wall street bets. Every, I think it was like every month or so, he was, like, posting an update. Initially it was fundamentals driven, then afterwards it just became this entire kind of, like, community frenzy where it's like, we all like the stock. And then there were also sophisticated traders, like hedge funds, for example, that saw this act, that saw this happening, and they're like, this makes literally no sense.
00:04:59.568 - 00:05:33.570, Speaker B: Like, after a certain point, the amount of, like, price action that was happening, which is completely, like, disjoint from reality. Um, so because of that, like, a lot of hedge funds started shorting Robin Hood and or shorted, uh, shorting GameStop and these other meme stocks. And the way that shorting works is you have to borrow the stock, then you sell it immediately, and then afterwards, you have to buy it back and then return it. So if the price goes down, that's fine. If the price goes up, you actually need to buy it back at a higher price and then return it. But when you buy it, that leads to more buy pressure. So there's this concept of a short squeeze where a bunch of people have short positions.
00:05:33.570 - 00:05:58.804, Speaker B: The stock price is going up, then they need to buy back the stock. But when they buy it, that leads to more upward price action. So other people have to do the same thing. So that was happening, and it was this very true Robin Hood type of moment where it's, like, the rich. Like, you're essentially taking from the rich and giving to normal, everyday people. Like, Robinhood was at the center of all of this. Like, Robinhood was the place where all this activity was happening.
00:05:58.804 - 00:06:26.492, Speaker B: And then one day, just out of the blue, I think it was, like, January 28 of 2021. So sometime in that range, Robinhood just decided to turn off buys. And this was just, like, completely out of the blue for the community. Like, no one was anticipating this. Everyone was, like, super excited to go on trauma the next day and, like, start trading gamestop. And, yeah, I mean, it was just completely out of the blue. So, as an insider at Robinhood at the time, I didn't really have any idea that this was happening.
00:06:26.492 - 00:06:48.136, Speaker B: I would find out about things at the exact same time that they told the rest of the world. And it just makes you feel completely powerless, right? Because you put your reputation on the line to join a place like Robin Hood. Because you're like, oh, shit, there's hella smart people working here. You join that place, and then you just have, like, no idea what's happening behind the scenes. Your team comes to you with questions. You have nothing to be telling them. Your friends that are also, like.
00:06:48.136 - 00:07:21.500, Speaker B: I mean, as someone in their early twenties at the time, most of my friends were following along with this are actively trading GameStop. So a lot of people reached out to me and it's like, well, I have nothing to be among you either. So kind of going through that entire process, it made me feel a little bit more jaded. With the traditional financial system, it is very, very centralized. There's a lot of very centralized actors that are involved in the process. And even from the Robinhood side, it was like a few people in a room that are calling the shots, and I had no idea what was happening behind the scenes. That made me much more of a decentralization, Maxi.
00:07:21.500 - 00:07:38.304, Speaker B: Like, anything that happens in a decentralized way is inherently trustless, and I think that avoids a lot of the issues that we saw happening with Robin Hood. So that was the original inspiration for everything. That's what led to us starting to build stuff on chain and, yeah, I mean, happy to go more into the state story after that.
00:07:38.764 - 00:07:50.820, Speaker A: Yeah, that's absolutely crazy, because that's such a unique perspective of what was a kind of crazy time in the markets. And, um, the. Did you catch the. Did you catch the movie dumb money? Was it on?
00:07:50.852 - 00:07:58.824, Speaker B: Yeah, yeah, yeah, I watched that recently. It's a great movie. I think that some parts of it weren't totally accurate, but overall, it was, like, very sensational.
00:07:59.644 - 00:08:49.964, Speaker A: Yeah. Never let a lie get in the way of a good story. Um, but what I found was, and I'd heard that story before, and I thought it was like such a unique perspective from a founder, particularly in the space. But I think it's very topical of what's actually happening, and it has kind of connotations of what's happening in the crypto markets now, where everyone's. Who's coming to the markets at this point in time, there's kind of this extreme focus on meme coins, for better or for worse. But I just wanted to get your opinion on, as someone who's building the infrastructure to allow an ecosystem to flourish, what kind of role do you think meme coins and more speculative assets and things like that play in an ecosystem?
00:08:50.384 - 00:09:25.364, Speaker B: Yeah. So, I mean, from meme coins specifically, I think they're playing a role similar to what nfts played in the last cycle, where they are a community building mechanism. And what I mean by that is everyone that is rallying behind these different coins, part of it is they are rallying behind a coin. Part of it is tied to money. But I think at the heart of it, it is more like getting around a community, and I think it's identical to what we saw happening with GameStop. Initially, it was part of. It definitely is financial driven, but I think a lot of it is just community formation.
00:09:25.364 - 00:10:09.904, Speaker B: I think over the rest of this year, we will see meme coins become a similar type of vehicle as nfts in a way where there are mechanisms for bringing people together and, like, mechanism for, like, community formation. So a lot of the stuff that we saw happening with nfts in terms of, like, people trying to have more, like, join these communities and then, like, have these community driven events, like, we're starting to see versions of that right now as well. I think it was either, like, yesterday or the day before that, where there was the dog with hat, kind of people were trying to get it on the sphere in Las Vegas. That is extremely reminiscent of what we saw happening with nfts cycle as well. So, yeah, I mean, short, short answer. There is, like, nfts or meme coins are kind of similar to nfts where they're community building tools.
00:10:10.804 - 00:10:17.504, Speaker A: Yeah, it's exactly in thought and called them like, this cycles nfts without jpegs attached to them.
00:10:18.684 - 00:10:19.180, Speaker B: Exactly.
00:10:19.212 - 00:10:58.694, Speaker A: But it's. It's quite strange because, like, to go, like, a step further, the, like, ecosystems are definitely recognizing that as a customer acquisition channel and a customer acquisition, like, source. And, like, you even seen, like, native apps on. On. On Solana by integrating the fundraising page with inside the Phantom app to kind of get this, like, to get dog with hat onto the sphere. So I thought it's just like a weird. It's a weird shift in what's happening, but I think it's like, the appetite is obviously there, and if it's an onboarding tool, then, like, it is what it is.
00:10:59.594 - 00:11:00.130, Speaker B: Exactly.
00:11:00.162 - 00:11:17.254, Speaker A: So can you kind of. Because se's gone through this, like, huge upgrade in iteration, but I'd like to kind of give people a high level of what it has been and what it is currently before we kind of get into v two, because there's a lot to pick apart on Sebi two, of course.
00:11:17.754 - 00:11:55.530, Speaker B: So, say went live Seb one, it went live in August of 2023. So that's, like, around seven months ago now. And since that time, C has been the fastest blockchain that has been created just point blank. It's seen 390 millisecond finality, which is faster than Solana, it's faster than, like, app, it's faster than essentially any chain that you can think of. So that has been going exceptionally well. One of the biggest learnings that we had after the v one launch is that the EVM is here to stay. That is like, one of the core things that we built conviction around.
00:11:55.530 - 00:12:22.854, Speaker B: And that is one of the core, I guess, one of the core learnings that we've had. And what I mean by that. So for any listeners that might not know, the EVM is the Ethereum virtual machine. It's what's used to process transactions on Ethereum and dozens of other blockchains. It's like the default kind of execution environment that people use right now. 87% of crypto native engineers, they use the EVM. So essentially everyone uses the EVM right now.
00:12:22.854 - 00:12:58.980, Speaker B: The remaining 13%, they are largely on Solana using sea level runtime. And I mean, other execution environments, like, for example, move would be one example, fuel vm cosmosm. They don't really have much activity at all. So the question is then, like, how do you get these? If you're building something that doesn't have the EVM, how do you get someone to leave the EVM and start building in your new execution environment? The simple answer there is, you don't. People do not leave the EVM. And that's because the EVM isn't just a tech stack. There's a lot more going on with the EVM.
00:12:58.980 - 00:13:19.314, Speaker B: There's the entire community. There's this entire way of thinking. And initially, I would think that it's like, okay, it's not just a tech stack. It's an ecosystem. But I'm starting to think more and more now that it's not just an ecosystem either. The closest comparison over here would be a religion. Like, the way that people think about the EVM, it is much more similar to a religion than anything else.
00:13:19.314 - 00:14:03.734, Speaker B: And, I mean, obviously, it's like a capitalist take on religion. So it's definitely not the same thing, but it's extremely difficult to get people to convert. So that's one of the core learnings we had. The question we asked ourselves is, like, what is actually missing from the EVM right now? And the biggest pinpoint of the EVM is a lack of throughput. If you go to the Ethereum L1, if you go to any roll up built on top of Ethereum, you're not able to get more than 50 sustained transactions per second that you can process, and that is very low with 50 tps. There's a couple of problems that happen. The first problem is that you start having much higher gas fees, and you see this on Ethereum l one all the time.
00:14:03.734 - 00:14:31.404, Speaker B: Very recently, we've had ways. I think it hit, like 160 or 170 in the past couple of weeks when shit like that happens, right? Like the average person is just completely priced out. You cannot do like 99.9% of the world cannot go on chain and spend like $150 to just do a swap on uniswap. Like that is just, that is unacceptable. So it prices out the normal user. And on top of that, it limits the design space for developers.
00:14:31.404 - 00:15:02.230, Speaker B: Like developers now need to start figuring out how to make things work in this like really constricted environment. So there's a lot of anti patterns that they have to start using to make things work. Um, one example of that would be amms. So automated market makers. Um, this is a construction that is not used at all in traditional finance. Um, but it's very popular on chain. And the reason for that is that in traditional finance, like, I mean, from a high level standpoint, mms are not capital efficient.
00:15:02.230 - 00:15:38.554, Speaker B: Like order book based designs tend to be better for actually trading. But amms do work in the constraints that you have with the EVM. So that's why projects like Uniswap have taken off. So there's, I guess, several problems that are there with the EVM right now. The question then becomes like, how do you actually solve those? And that's exactly where save e comes in. The way that we're solving that with save e two is by paralyzing the, the EVM and by doing this we are able to increase throughput. So I guess I'll just give a high level overview and then let you deep dive into the questions there.
00:15:38.554 - 00:16:19.284, Speaker B: But the EVM right now, if you look at any existing chain, it's single threaded. So if you have 100 transactions that are coming in, they'll all get run one after the other. And this is really simple for engineers to write code around. It's really simple from a software engineering standpoint, but it does not take advantage of modern hardware. The laptops or phones that people are listening to this on, those are devices that have multiple cores and they can process multiple work streams at the same time. It's extremely inefficient to have this modern hardware and have software that is not able to take advantage of that. That's exactly what we do with parallelization.
00:16:19.284 - 00:16:50.566, Speaker B: With parallelization, we're able to process multiple work streams at the same time. One high level example would be if there's, let's say 100 transactions coming in, each transaction takes ten milliseconds. With sequential processing, that would be 100 transactions times ten milliseconds. So that's 1000 milliseconds, aka a second to process all of them with parallelism. Let's say you're able to process ten transactions simultaneously, then you're going to be able to do it ten times faster. So instead of 1000 milliseconds, it'll take 100 milliseconds. So that's the massive speed up.
00:16:50.566 - 00:17:00.614, Speaker B: Right. And that's exactly what we're doing with av two. We're paralyzing the EVM to help get better throughput, which will then decrease fees and also increase the developer design space.
00:17:00.954 - 00:17:33.363, Speaker A: You see there's a large push towards this, and Solana has certain aspects of parallelization, but it isn't just an easy task to just say, oh, we're just going to handle a lot of separate transactions in parallel, and then just bundle them all up and then put them together. So what are some of the, have you stumbled upon anything in building this out that you kind of didn't foresee? What are some of the more difficult aspects of actually getting a parallel EVM execution environment to actually work that people from the outside looking in probably wouldn't think was a difficult aspect?
00:17:34.023 - 00:18:13.124, Speaker B: Yeah. So Solana is actually an interesting point that you brought up. I guess a high level way for people to think about what we're building with CB two is essentially combining ethereum and Solana. We were able to get the EVM and all the tooling mind share community around it while having this super high performance execution environment that you get with Solana. So I think that's the right mental model that people can have around v two. I would say the biggest trade off that you're making or the biggest thing you need to account for when you're building a paralyzed, basically any high performance chain is state load. And that's something that a lot of people might not be super familiar with.
00:18:13.124 - 00:18:49.624, Speaker B: And I think that's one thing that you really need to be thoughtful about before you're actually able to build any kind of paralyzed chain. So state, I guess taking a step back state is all of the data that needs to persist on a blockchain to be able to generate the state route which is used for the block header, and to be able to process any new transactions. So this would be essentially two types of data. The first is account balances. Like I have ten say, you have 20 say. And this would also be smart. Contracts say it could be an NFT contract, it would be mapping this token, this id.
00:18:49.624 - 00:19:29.414, Speaker B: It belongs to this user, for example. So all of this data needs to persist on chain. It needs to be there for every full node and it's essentially necessary to have, when you have more transactions that are being processed, that leads to more state writes that happen, which results in essentially more data that you need to be writing to state, and there's just more data that needs to be tracked. And that's the problem of state load. And there essentially end up being two things that you need to be thoughtful about. The first is state storage. When the state size increases, how do you actually store all that state on these full nodes? If it's like 1gb, that's very simple.
00:19:29.414 - 00:20:12.322, Speaker B: But if it starts ballooning, it gets to be like let's say ten terabytes. Then suddenly the same full nodes that you were using before, they will not necessarily be able to handle that type of state. And it'll result in more centralizing forces because there'll be less people that can run full nodes and that'll have its own set of trade offs. The second thing that happens is state sync becomes much more difficult when you start to run a new node. Let's say you're going to be running a new validator, or you want to start splitting up a new full node, you need to import the state of the current blockchain to be able to start processing new transactions. If there's, let's say ten terabytes of state, that becomes a pretty difficult task to import everything and then also account for all the new blocks that are coming in. And it becomes difficult to do so quickly.
00:20:12.322 - 00:20:55.186, Speaker B: So it could suddenly start taking days to sync to the current state of the blockchain. And that's a pretty terrible developer experience. And it can also have other issues like if you need to wipe a validator and start restart it, then there could be issues tied to that as well if it takes too long to set up. So the way that we have accounted for that is through SATB. So SADB essentially has two parts to it. The first is a memory mapped IVL tree, and the second is async writes that are happening to disk. I can get into those if they're, I guess, interesting to talk about, but at a high level, just through these improvements that we made, we're able to get a 60% reduction in state size.
00:20:55.186 - 00:21:14.782, Speaker B: So state storage decreases dramatically. We're able to get a 1200 percent improvement in state sync. So it becomes much faster to sync any new notes that are being spun up. And we also observed a 287 x improvement in commit times. So it became much faster to actually process blocks that are coming in sick.
00:21:14.878 - 00:22:03.792, Speaker A: So when people like, because obviously this is going to kind of change the way that a lot of people have kind of operated, particularly if they've only been on Ethereum and l two or like EVM equivalent chain. And then they start using cp two and they're like, oh, well. But a lot of people might be asking, well, what's actually, what's actually going on under the hood? And like the most basic example, which everyone uses in like parallel systems is, well, if there's an NFT mint happening, that's kind of chaotic and there's a bit of a gas war going on on one aspect of the chain. And let's say there's a lending and borrowing protocol that's trying to liquidate like a huge, huge position, like what's actually happening under the hood to enable those two transactions that operate in parallel, that they don't congest each other. And the lending and borrowing protocol doesn't incur bad debt because the gas fees are too high because of the NFT and all that kind of good stuff.
00:22:03.968 - 00:22:48.242, Speaker B: Yeah. So there's essentially two ways to support parallelism. The first is by having dependencies be defined up front, and the second is by having the chain figure out the dependencies. So if you look at Solana, it'd be like one of the most canonical examples of having dependencies be defined upfront. Each Solana transaction needs to include all the state that it thinks it's going to be touching. And then based off of whatever state each transaction passes along, you're able to figure out like, okay, these two transactions are not touching the same state, so they do not have any overlap because shared state is like the primary thing that leads to transactions needing people needing to be run sequentially. Like if we're trying to do two completely separate things, like there's an NFT mint, and then there's liquidation.
00:22:48.242 - 00:23:25.144, Speaker B: These are not touching the same state, so they can be run at the same time. So having developers define dependencies upfront is one approach. The downside with this approach is that it's a lot more complicated for the developer experience, so it requires developers to do a lot of work. It's not super intuitive. In the case of EVM, it also doesn't end up being backwards compatible. Like if you take something that is there on ethereum l one, you try to deploy it directly to a chain that requires you to have dependency dependencies be defined. You can't just like change the RPC to the front end visiting, you need to change the way that transactions are submitted from the front end.
00:23:25.144 - 00:23:54.974, Speaker B: You might need to change the way smart contracts are written as well, so it adds a lot more friction to the developer experience. The other way is by having the changes figure out what these dependencies are. And that's exactly what we're doing with save too. So this called optimistic parallelism, and the way that it works under the hood is all the transactions. Let's say there's 100 transactions in a block. All these transactions will get run in parallel initially. And then we see what state is being read and what state is being written to.
00:23:54.974 - 00:24:28.584, Speaker B: So there's 100 transactions. We know what state each of them is touching. Then we can see which ones have overlaps, like which ones are touching, trying to access the same state. If two transactions, if one transaction is touching a state, no one else is touching it that can be committed. If there's ten transactions touching the same state, then you can commit the first one, and then for the remaining nine, you two rerun all nine of those. So in the worst case, this can result like, let's say there's 100 transactions, they're all touching the same thing. This can, in the worst case, result in transactions being rerun several times.
00:24:28.584 - 00:25:04.834, Speaker B: With like 100 transactions. We re running them 99 times. But one thing to keep in mind over here is that because you've already loaded all this data that's being accessed into memory, it makes it much more performant to be able to actually process these transactions after the first time. So with all this being said, the developer experience ends up being much better with this because the chain is just able to figure things out. And I think that's the biggest thing that needs to be optimized for. Um, the harder the developer experience is, the harder it is to grow an ecosystem. And that's why the optimistic parallelism approach is definitely the better approach to be using.
00:25:05.974 - 00:25:36.764, Speaker A: Yeah, I did, always did always wonder how that, um, actually worked. And like how if you guys did any kind of, I'm sure you have, but how kind of multi threaded can, can that actually get? Because I think there's like limited kind of, um, threads on specific chains that are trying to do this kind of thing. But like you kind of ran some tests and thought about if there's an arbitrary number around how multi threaded this approach can get. With optimistic parallelization.
00:25:37.344 - 00:26:12.870, Speaker B: Yeah, I mean, at the very least, if there's n number of cores on the machine, it'll be able to parallelize across every single one of those n cores. In terms of more concrete numbers to give to you. We ran load tests with fully parallelizable work streams. So this is just like token transfers that are happening back and forth across like completely separate accounts. We observed 5030 tps with these load tests. There's like some pretty clear improvements to be making to get it even higher in the future. With Ethereum right now, you're seeing around 50 tps as the upper bound for like what roll ups using Ethereum for DA are able to get.
00:26:12.870 - 00:26:34.204, Speaker B: So for more concrete numbers, you're able to get roughly 100 x improvement in throughput. And this is accounting for like everything that needs to happen. This is accounting for the p two p gossip layer. This is accounting for consensus, this is accounting for generating the state route and everything as well. If you're looking just at pure execution, it's going to be even higher than 100 x compared to Ethereum.
00:26:35.304 - 00:27:02.104, Speaker A: Great. We touched upon it slightly there, but let's say v two goes live and everything is now set in stone for, I don't know, a year. Uniswaps and your curves and like all the kind of big defi players and all the NFT projects and things like that. What does that portability look like? Is there any additional lift whatsoever? Is it just kind of 100%, kind of translatable over to say, v two? Like, how does that look?
00:27:02.684 - 00:27:56.830, Speaker B: Yeah, so under the hood we're making use of get. So get is what is used to process, I think around 85% of transactions on Ethereum l one right now. So it is the most battle tested execution client that Ethereum has to offer. Under the hood we're making use of get for full EVM bytecode compatibility. So any op code that you need to use or that is being used on Ethereum l one, it'll just work, save e two. So what this means for like contract, like smart contract dapp developers is you can take your smart contract from the ethereum l one, deploy it to say all you need to do is change whatever the RPC is, and then you go to your front end, you change what RPC is being used and then everything essentially just works. I would say the big difference between say's approach and what is there to be like, what is described in the Ethereum yellow paper would be that say makes use of an ivl tree.
00:27:56.830 - 00:28:26.130, Speaker B: And the Ethereum yellow paper talks about a mPT, a Merkel Patricia tribe. So that would be the primary difference. But in terms of like where, how that manifests, it would only result in differences in the proof that is generated. For most applications that doesn't matter at all. But if you're building tooling that makes use of any kind of proofs, then you need to update how you are processing those proofs. So if that's on a smart contract, you need to change the code for that. If it's some off chain tooling, then you need to change how you process those.
00:28:26.130 - 00:28:38.184, Speaker B: But at this point, there's a public devnet that is live for CIT. None of the projects that have deployed there have had any issues. This is something that affects most likely very small minority of projects.
00:28:39.404 - 00:28:58.544, Speaker A: Yeah, cause I bet there's so many projects that are just, as I say, working with the infrastructure that they've got, and their application could probably reach infinitely greater heights given a parallelized system. So I bet developers are all flying over there to try it out.
00:28:58.924 - 00:29:30.634, Speaker B: Exactly. And I think it ends up being even more exciting for people that are trying to build completely new things. Because if you take something that works on Ethereum, you port it over to, say, b two. The biggest benefit there will be lower gas fees. But now you have this hundred x improvement in throughput. So there's an entirely new design space of applications you can build that have much higher verifiability and much higher trustlessness and decentralization guarantees than you would be able to get on Ethereum. Some examples of this would be like a order book based exchange where you have everything happening on chain.
00:29:30.634 - 00:30:15.408, Speaker B: What is historically needed to happen on Ethereum is you have some kind of off chain order matching engine which has much higher centralization requirements. It's essentially just on AWS controlled by one entity. Now you can move away and start having that happen entirely on chain. A couple of other examples would be some social media application where you have every single action that happens, every single comment going on chain, instead of needing to be in some separate network and then only having a smaller portion of data living on chain. I guess a third example would be games. So instead of having a lot of games right now, they just have nfts that are traded or tokens that are traded. That part lives on chain, and the rest of the game logic lives off chain, but you can have much higher verifiability by having every single action just get submitted on chain.
00:30:15.408 - 00:30:19.324, Speaker B: So there's an entirely new design space for what you theoretically can do now.
00:30:20.224 - 00:30:57.496, Speaker A: Yeah, because it's quite interesting that the Dex and particularly the EMM space, it's kind of been forced down into a more RFQ intense based system, purely down to liquidity providers, inherently not being profitable, or just eating like an ungodly amount of impairment, loss or loss versus rebuilding. As we're calling it now. And that's kind of forcing. You've even seen like, Uniswap or Uniswap X. They're kind of switching to this RFQ model. They're kind of outsourcing what they want the next durations of the. To be with uniswap V four and hooks and everything like that.
00:30:57.496 - 00:31:23.864, Speaker A: So it kind of feels like they're a little bit of a loose end. But maybe it's like it has to come at the infrastructure layer and the execution layer to enable that next kind of development and push towards like central limit order book style model that we know is tried and trusted in and can support like, long sale assets as well, which obviously people love. But it does feel like the event space is definitely hamstrung by the network that it's currently building.
00:31:25.084 - 00:31:31.184, Speaker B: Yeah. So you're saying that like, because there's so much, like, there's already strong network effects right now, so it's harder to move off of that. Right.
00:31:32.164 - 00:31:33.852, Speaker A: Yeah, and I mean, like, you're saying.
00:31:33.868 - 00:31:35.744, Speaker B: Like the l one itself that it's built on.
00:31:36.404 - 00:32:15.724, Speaker A: Yeah. So, like, obviously Mm has been great. Anyone can list any, any specific asset. It can be long tail, it can be like fat tail. It can be absolutely anything. But what you. What you've kind of seen in the same week of uniswap announcing that the releasing Uniswap before, they also released Uniswap X, which is request for quote, where it's like an opaque system where it's getting filled by probably wouldn't mute off chain, but it's self custody, and then it's just pushed it towards these more centralization, centralizing forces because it is hamstrung by the very nature of being a sequential system based on an MM.
00:32:15.724 - 00:32:26.904, Speaker A: And it's just like. So it's very interesting to see that if you change the underlying infrastructure, that you can then have more powerful systems such as an order book, which would make sense.
00:32:27.204 - 00:32:58.164, Speaker B: Yeah, I definitely agree. I think for several types of dapps right now, the underlying infrastructure, the underlying network is limitation. I do think that if you have higher throughput, there will be much more experimentation happening. That doesn't mean that inefficient models from the current will suddenly disappear. I think amms do have a place on chain. There's other benefits that they offer as well, such as making it much easier to support long tail assets. But I do agree with your point that when you have better infrastructure, you will be able to expand a lot more.
00:32:59.184 - 00:33:30.006, Speaker A: Yeah, I know you're going to go down the route. But with Mev and everything, I've seen some of the numbers recently and that just kind of blew my mind how much people are losing out to Mev. But that's a conversation for another day. So you've kind of also came out with this recent announcement about the parallel stack, and I don't think you've talked about it previously publicly on a podcast. So I'd love to kind of pick your brain on what that is like, high level. And I've kind of got some additional questions to kind of tap into that.
00:33:30.190 - 00:33:53.940, Speaker B: Of course. I mean, we actually announced this earlier today. So the announcement was at 06:00 a.m. pacific time. So I got up nice and early for that. And I mean, kind of at a high level, like how we're thinking about this is that right now we're building the first paralyzed VM. And this is essentially going to be like with save e two is going to be a change that we're making to the core l one.
00:33:53.940 - 00:35:05.703, Speaker B: So for several different types of applications, this ends up being the perfect place to deploy. Specifically, if you're building something that's like a smart contract application that you want to deploy on a general purpose chain, save e two ends up being fantastic. There's a separate class of developers that want to have their own dedicated block space, and specifically they want their own dedicated block space in a very high performance type of execution environment. And currently there's not really a good way to get that with the EVM. So they'll either have to convert and use some other type of execution environment that theoretically could offer a greater throughput, or they can't really, they don't really have any other option. And that's exactly what the parallel stack allows people to do. It takes what we've already been building with Zev two, and it makes it so that anyone that is trying to build their own L2 blockchain, specifically a L2 that is not using Ethereum for data availability, they will be able to take the innovation that we've had with optimistic parallelization with Sadieb and then use that to get a very high performance execution environment for them to work with.
00:35:05.703 - 00:35:36.464, Speaker B: And I mean, this is kind of like a natural next step for SiD. Our goal is to scale the evm. And there's this way that we've been working on of building save e two. And this is like a natural next step from taking save e two's execution environment and then making that accessible to a lot of people because this is already completely open source tech, there's already people that want to make use of it. So it ends up being very aligned to just make it as easy as possible for teams to make use of what we've already built in an open source way.
00:35:38.004 - 00:36:24.300, Speaker A: Yeah, because obviously we're starting to see a huge push towards people being able to have this modular stack now. And I think it's much greater for people to be able to just tap into like really great teams that are building in a specific aspect of the stack. Like obviously everyone's using celestia or eigenval for data availability. You're starting to see some additional shared sequence of layers come online and now this just feels like. Right, so this is instead of just being hamstrung by everything else underneath the hoods going on, that is pushing the industry forward in the right direction. Why are we such a settling for a sequential evM? But there was an aspect of the article that I read where there is also a shared sequencer layer, if I'm not mistaken. Is that correct? And I'd love to just stick your brain on that.
00:36:24.332 - 00:37:38.004, Speaker B: Yeah, I mean, I think the right mental model here would be this is in some way similar to op stack where there is like an opt in shared sequencer that you can make use of if you want to. But fundamentally it is a way for you to build your own blockchain and you as the application, you as a blockchain developer can choose what parts of the stack you want to. So you can use the parallel stack for the execution layer, you can use Celestia for DA, you can have Ethereum settlement, and then you can choose however you want to have the sequencer be set up. If you do decide to use say, sequencer set, then you will be able to benefit from interoperability. But as a developer, you control however you want this to be, however you want your to be set up. And to your core point, I fundamentally think that if you can have like if you're a developer and you have to choose, and you have the option essentially to choose between a single threaded EVM and then a paralyzed EVM, then you will 100% of the time pick the paralyzed EVM because it is just significantly faster and it just leads to a much better user experience and a much bigger design space for any developers that are building on top as well. So it's a no brainer to go with a parallel EVM.
00:37:38.004 - 00:38:13.484, Speaker B: And that's why it's honestly pretty surprising that no one has been working on this so far because it is just like whoever is able to build the first paralyzed EVM for L2 to make use of, and whoever is able to essentially make it easy for people to use this execution environment in an easy way, you'll essentially become the default choice for anyone that's trying to build a L2 because it's just significantly better. So I think that they're like supporting the parallel stack and to being really effective to help developers just offer much better user experiences.
00:38:14.824 - 00:38:47.664, Speaker A: So there's two adjacent questions I've got here. So the first one for existing l two s, which are obviously running a sequential EVM currently, is there any, or am I kind of like living in a fairyland here? Would it be possible for backwards compatibility, or is this something that you'd have to build from the ground up and fork off? What does that look like? Even if it's just a hypothetical situation?
00:38:48.644 - 00:39:04.524, Speaker B: Yeah, I think that depends on the specific implementation details, which haven't really been announced yet. So there'll be more updates around that in the future. But ideally it would be something that is backwards compatible. That is not super trivial to do though.
00:39:05.344 - 00:39:27.944, Speaker A: Okay, we'll leave that one on that. I also seen a tweet from yourself a couple of weeks back from high level. Can you help me understand why an l two using Ethereum for data availability would not be able to operate in a parallel execution environment?
00:39:28.524 - 00:40:09.626, Speaker B: Yeah, yeah. So the biggest limitation on Ethereum right now is bandwidth constraints at the base layer. And essentially like, I guess earlier today there was also the dancing upgrade that happened. So I'll go over what things look like before Dankoon, what things look like with proto dank sharding, and also what they look like with dank sharding. But essentially the way that roll ups work is, or any, essentially any type of L2 that is using Ethereum for DA, there's off chain computation that happens and then there will be data that gets written on chain with roll ups. This will be like with an optimistic role. This will be compressed transaction data that gets written on chain in each byte of each byte of data that is written on chain.
00:40:09.626 - 00:40:43.360, Speaker B: This requires a certain amount of gas. I think it's around 15 gas per byte of data that is written on chain. If you do all the math, there's a target of 15 million gas per block. There's a certain amount of data that is used for each type of transaction. So you end up getting to around 6000 transactions per second that can be supported. If you're using like Ethereum call data to write all this, to write all of the roll up transaction data into. So this would be 6000 transactions per second if the entire l one block was used purely for roll ups.
00:40:43.360 - 00:41:30.190, Speaker B: In reality there'll be a lot of other contention happening as well. There will be like uniswap and blur and all the other activity that's happening on the base layer. So there will be like, it just becomes infeasible, like to be supporting something that has thousands of gps happening off chain for that to be able to write the data to the base layer with prototyping. Sharding doesn't actually solve this problem of bandwidth constraints. It does help with gas fees, but it doesn't really increase bandwidth at all. So there's going to be a target of, I believe eight blobs per block with prototype starting. And if you do the math around like how much data that actually is available and how much it costs, it ends up being around a 10% improvement in the amount of throughput that you're able to in the amount of throughput you're able to process.
00:41:30.190 - 00:42:05.804, Speaker B: So around like 6600 transactions across all the L2s that are writing to these blobs. So once again, like if you're trying to use prototyping, charting, it doesn't really solve the problem around bandwidth constraints. Dang. Sharding would theoretically help over here, but that is like much, much further away. So if you want to have some kind of higher, higher execution layer. Two, you can't really be using Ethereum for DA and you'd have to consider using something else for DA, such as like you mentioned, Celestia Eigen Da. That would help you get the type of bandwidth that you need.
00:42:05.804 - 00:42:23.024, Speaker B: So then you can have higher performance happening on the execution layer. You can make use of the DA layer and not really be hamstrung by it. And then you can use Ethereum for settlement so that you're able to get kind of that the trust guarantees that you would want and the security that you would want as well.
00:42:24.644 - 00:42:55.134, Speaker A: Great explanation. Yeah, that makes a lot more sense. Now, if you are pushing towards this parallel stack product, why would you choose, and this is like, I'm just asking the open ended question here, like why would you guys choose to, as I said, push towards a kind of stack business model on top of what you guys are already building, does that increase competition? Does it not increase competition? Like how do you think about that from like a business sense?
00:42:56.194 - 00:43:59.304, Speaker B: Yeah, I mean, so first of all, like our goal is to scale deviant and we think that this is a natural next step in terms of like, we've already built this execution layer, it makes sense to democratize access to it because there's already a ton of demand to be getting access to this higher performance block space. In terms of being competitive, I don't actually think that this will end up being like any roll ups or I guess any L2s that are making use of this would end up being competitive, would say, and I think case studies that you can look to over here would be like Solana would be one example. There are teams that are making use of the SVM to build L2 s, but they're not really competitive with Solana at all. In fact, what ends up happening is they end up leading to significantly more mind share going back, and it doesn't really result in whatever these L2s are vampiring the base layer. The base layers. For all the innovation has happened, the base layer is where there's all the initial liquidity end users as well. It doesn't really end up being a scenario where the L2 is vampiring the layer one.
00:43:59.304 - 00:44:13.364, Speaker B: What does end up happening though is if you need dedicated block space for a particular type of use case, that ends up being quite possibly the best way for you to get access to that. So, yeah, I mean, I think a more succinct answer would be no, it doesn't really lead to any competition.
00:44:14.664 - 00:44:54.686, Speaker A: Yeah. And like the only thing that's kind of concerning me and I suppose a lot of others, and don't get me wrong, there is a lot of infrastructure coming out to kind of resolve this, but just the fragmentation across, um, l two s without the truly composable interop that you would like to see. Um, and I just want to like ask from the parallel stack perspective, is the sequence, is the sequencer like a large focus? Is this kind of an additional, like kind of nice to have that's built into the parallel stack, or is it primarily focused on just democratizing the access to parallel AVM four? I'll say.
00:44:54.830 - 00:45:33.444, Speaker B: I think in the longer term the sequencer will be a pretty critical part of this because as you mentioned right now, if you just have your own dedicated roll up or dedicated L2 that is not interoperable with others, it does lead to more ux kind of issues around fragmentation of liquidity and around being able to interoperate with other L2s. If you have a shared sequencer, it does make it much easier to offer different types of composability. Instead of having pure async composability, you can move towards different flavors of synchronous composability. So I think there's a pretty strong value proposition around having a shared sequencer that is connected to other high performance chains that allows you to have different flavors of synchronous composability.
00:45:34.424 - 00:45:55.994, Speaker A: Yeah, and obviously one of the glaring questions if you're going to run something in a much more performance system, hardware requirements as opposed to what traditional systems would be running on. Now is there any spec that you guys have released or is that still kind of closed off yet? Or what does that look like? And what's the lift look like for people who would like to run this?
00:45:56.294 - 00:46:33.354, Speaker B: Yeah, so for save you two, I mean the, if you go to the save e two docs, it has the hardware requirement, hardware requirements written over there. It's 64gb of ram, which I would say so compared to like what you would see with other chains. It's roughly twice the amount of ram that you would want. But I guess kind of taking a step back. What parallelization lets you do is it lets you take the existing hardware you have and make the best use of that hardware. So you can technically use parallelization with like the exact same hardware requirements and be able to get higher performance from that because it's software changes, it'll be scaling with whatever the underlying hardware is. So you don't actually need to have higher hardware requirements.
00:46:33.354 - 00:46:55.594, Speaker B: In safe case, that does lead to slightly better performance. So that's what from like kind of trade off curve. Like that makes the most sense for SiD. But if other folks, they realize that like they do want to have like lower full node requirements, their L2 s, they could keep continuing using the exact same requirements as what you would see with other ethereum roll ups or ethereum L2 s and still get better performance from that.
00:46:56.454 - 00:47:07.734, Speaker A: Okay, cool, and what's next? Like where are we at with this rollout? Like what's coming? Like Overhill and what should people kind of be aware of?
00:47:07.894 - 00:47:38.646, Speaker B: Yeah, so right now for save e two, there is a public devnet that is live that supports everything that we just discussed. So people can go over there, play around with it. Next step there would be the governance proposal that needs to pass and then afterwards upgrade to mainnet. That'll happen sometime later, I guess. It's already march right now, so that'll happen sometime this half. So sometime Q two of this year afterwards. The parallel stack would be something that our team is spending a lot of time on.
00:47:38.646 - 00:47:43.494, Speaker B: So there'll be more updates around timelines and around initial partners later on this year.
00:47:44.834 - 00:47:50.734, Speaker A: Cool. Is there anything you personally like to see built over there on v two?
00:47:51.194 - 00:48:27.994, Speaker B: On v two right now, there's a lot of really high quality ethereum teams that are going to be launching instances on today. What I'm really excited about is seeing native builders playing around with this 100 x improvement in throughput and being able to build out better types of product from that. So save foundation is setting up an EIR program, an entrepreneur and residence program. If you guys are interested in potentially learning more about that, there's a lot of support that SAA foundation can offer. So you could just go to the CIA network, Twitter and reach out and the team will be able to assist you there.
00:48:28.934 - 00:48:36.148, Speaker A: Awesome. So anyone listening who wants to go and check that out, I'll leave all the links in the description as well. But Jay, is there anything else that I might have forgot?
00:48:36.196 - 00:48:38.944, Speaker B: Oh man, this is a great conversation. Appreciate the time.
00:48:39.364 - 00:48:59.332, Speaker A: Thanks very much. And if there's any, any point at when you guys are all now and you want to come back on, just let me know and we'll run it back. But I'm excited to see it all happen. Hope everyone enjoyed that. Please give us a like and subscribe. I'm terrible at asking and our producer keeps shouting at me, so please do that so I don't get beaten off him. Right, Jay, thanks very much and we'll see you next time.
00:48:59.332 - 00:49:04.124, Speaker A: Thanks for listening. If you enjoyed the show, please give us a like, subscribe and turn the notification bell on for next time.
