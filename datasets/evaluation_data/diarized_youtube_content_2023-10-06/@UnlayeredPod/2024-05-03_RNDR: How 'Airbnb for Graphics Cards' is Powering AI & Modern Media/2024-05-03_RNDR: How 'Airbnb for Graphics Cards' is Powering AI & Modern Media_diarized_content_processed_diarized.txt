00:00:16.239 - 00:01:08.134, Speaker A: Welcome to Unlayer, the show where we talk about next generation blockchains and the founders building there. I'm Sol, alongside my co host Dave, and today we have the privilege of chatting with two individuals. First is Jules Urbach, founder and CEO and chairman of Otoy.com, and also the founder of Render Network, as well as Trevor Harrys Jones, COO at Otoy and director of the Render Network foundation, as well as former CEO of Viola.com dot. So very star studded cast here we have today, and wanted to actually start off with like the origins of this story here, because I think a lot of the focus in the crypto community at least is centered on render and the idea of we got a decentralized GPU access. Now that you post chat GPT, there's so much more demand for compute and we'll get into all of that good stuff.
00:01:08.134 - 00:01:59.694, Speaker A: But I think it'd be helpful for our listeners to start with the very beginning of this journey and specifically the story of Otoy, which has been around for over a decade. At this point, it's kind of a mainstay in the media industry. And maybe I can kind of just take a stab at describing where the products that Otoy built, at least the initial ones that got traction, fit into this broader framework. And then Jules and Trevor, you guys can kind of chime in and correct course, as I talk about it. But as I understand it, making media, whether it's a video game or a movie or tv show, there's a lot of stakeholders, and maybe we'll run with the video game example, because I think a lot of our listeners play games. You have developers that write code, you have artists that design assets, modelers that create 3d models. You have the game engine, things like Unreal Engine and Unity.
00:01:59.694 - 00:02:25.874, Speaker A: You have the hardware that's running all this stuff, then you have rendering engines. And one example is Otoy's octane engine. And that helps generate really high res outputs, photo realistic images by optimizing the rendering process. I guess starting there, is that a high level understanding of where the origins of Otoy started focusing on that use case?
00:02:26.934 - 00:02:51.010, Speaker B: Yeah, I think that's fair. Otoy has been around over ten years. I started coding the first lines of code and even got ohtoid I found back in 2002. So we're past 20 years from when the inception of it started. And I spent about seven years, mom's house just writing code and getting things going. And, you know, my background is video games. You know, I didn't go to college.
00:02:51.010 - 00:03:19.882, Speaker B: I wanted to make video games. I did a couple in the nineties, and, you know, I quickly, you know, moved from that to sort of pursuing an even deeper vision, which was, I wanted to democratize content creation. I mean, I loved storytelling. I loved video games. I loved, you know, movies and cinema. And I remember being eight years old playing Dragon's Lair, which was an old arcade game that pre rendered everything right. So it had the effect of looking like a Disney cartoon, but you could play the game, and it stuck with me.
00:03:19.882 - 00:03:51.782, Speaker B: And a couple of years later, episode of Star Trek introduced the concept of the holodeck. You walk into a room, could be anything. It feels like you're in any world you can imagine. And those pieces of technology really influenced me. And I feel like Otoy is probably the product we're known best for is octane render. It's been twelve years since we released that product. It was the first GPU renderer, really, to introduce the world to the hundred x speeds you can get with a GPU driving that.
00:03:51.782 - 00:04:31.902, Speaker B: But it also is still, to this day, one of its few renders that just follows the laws of physics and light and gives you back a result that would be equivalent to just taking a picture outside with a camera and just letting photons bounce off things that give you back something that looks natural. And it's become very popular, and it's been used in a lot of movies. But another part of Otoy's history, there's two parts of the company that are equally important, I think, to the rendering side. And one of them is capturing the real world, especially people, digital doubles. And that started almost prior to octane. In 2008, we started a business unit called Light Stage, spun out of a group at USC. That team still works for me, and we won two Academy awards.
00:04:31.902 - 00:05:08.902, Speaker B: You're standing in every single Avengers movie, Marvel movie, DC movie, tv shows, video games go through our facility. And so for about 1516 years, we've been creating digital, human digital doubles. And that's something that, obviously, in today's world, is starting to become really interesting. And then the other part of the company that also proceeded in some ways, all of our focus on GPU rendering was just GPU cloud services. My initial investor in Otway, one of the bigger ones, was oddodes 3d software. To this day, we support those tools. And they tasked us with figuring out how to stream their software from the cloud.
00:05:08.902 - 00:05:45.624, Speaker B: This is back in 2010. So there's been sort of the storied history beyond just graphics and rendering, that have touched on cloud services, touched on capturing humans and capturing the world and turning that into sort of a single workflow. And from my perspective, all three of those things interlock. That's why I started all of them so early. And then, of course, today, we now have a world that's awakened to the potential of the blockchain crypto and everything related to it. And of course, we're part of that, too. But, yeah, the vision of the company was really to democratize reality rendering and creation.
00:05:45.624 - 00:05:57.848, Speaker B: And I think as we emerge into the world of AI, the tools and the ability to do that more efficiently than ever is coming. And, of course, that's a whole conversation in of itself.
00:05:57.976 - 00:06:43.904, Speaker A: Raw, is there? Right. No, that's great. And I actually really resonate with kind of the central point you're making, which is everyone resonates with good storytelling, and a good way to improve it is just improve, like, the vehicle we're doing it through, which is graphics, whether it's through movies or games. And so that feels to me like a very obvious play is like, if we can improve that, people will just resonate even more with the stories that we're telling them through these different media forms. One quick question I had is, I guess it's just more from the perspective of someone that's outside of this industry. How did you identify that specific business opportunity for Otoy? Because, as I understand, graphics cards were around for a little while. Did you unlock some sort of way to better leverage them to render these images? And that was that first product you guys put out?
00:06:44.844 - 00:07:22.456, Speaker B: Yeah, I mean, I think Octane was actually a fairly mature version of Bunch of technology that I've been working on since 2005. And I remember this is back in DirectX eight, where DirectX twelve now. So this was eight erect shaders in assembly. And I remember, it's worth pointing out that at some point in 2004, it's almost 20 years ago, I met a man who changed my life. His name is Ari Emanuel. Still talked to him every day, twice yesterday. And, you know, Owens, Endeavor Owens, William Ward's endeavor, UFC, WWE, IMG, huge amount of Hollywood goes through him.
00:07:22.456 - 00:08:01.690, Speaker B: And so he was fascinated by the work that I was doing back super early, helped me build Otoy, and also introduced me to anybody I wanted to in the film industry. I had JJ Abrams, David Fincher to show up at my doorstep. And so I think that the idea with all of this was just to explore how. And there were. They were all sort of impressed by how quickly cinematic rendering can happen now doing this on a graphics card. So what I remember, I mean, I even took out a patent in 2008 with GPU ray tracing that it worked, right? I was able to do ray tracing on the GPU. I remember, I think it was David Kirk, maybe the guy who created Cuda for Nvidia, brought me into Nvidia.
00:08:01.690 - 00:08:27.104, Speaker B: I did a presentation showing, hey, this is how you can do ray tracing on a GPU. AMD, the CEO of AMD at the time, Dirkmeyer, put me on stage at CES video, still out there trying the same thing. And I remember talking to a lot of folks who were skeptical. One of them who wasn't was Tim library. He was the CTO of industrialized and magic. He is now the CTO of epic unreal engine. We're still very close, but a lot of people were.
00:08:27.104 - 00:08:47.261, Speaker B: Except how do you do ray tracing? It's not a process that can be mapped to GPU's. GPU's are designed for very specific things. And my brain just saw that differently. I was like, well, there's a way to parallelize that. And I wrote, you can see this, older videos from 2007. And I was writing ray tracers that would go and do some stuff that was not at Octane's level, but pretty close. And that's how I got started.
00:08:47.261 - 00:09:30.634, Speaker B: And as I was looking to productize that and leverage that, I mean, the team around Octane basically was two of them at the beginning in 2011 or twelve. Now we've got about 30 people or so working on that product. But I did see a huge leap where it's like if you could take a process that is serial, one after the other, and on a cpu, and somehow figure out on a GPU where you've got 1000 cores or 10,000 cores, how to run them in parallel and do it all at once, and then synthesize those results, you would end up with some huge leap of performance. And I was right. I was doing that myself as a single one man band coding that in the mid two thousands. But as we built a product around it, Octane really did change the industry. And now there's a lot of great gpu renders.
00:09:30.634 - 00:10:06.290, Speaker B: Almost all rendered have a GPU version, even Renderman, the classic Pixar one, the Renderman XPU. But being early, and I think building a product that is even unique to this day has been helpful. But as we look at the larger world, I love that there's other gpu renders. I love redshift, which is a completely different approach, relatively speaking to octane. It pre calculates things. It gives you something that's more like classical CGI rendering, which was a very different process than just the more natural following laws of physics and light. But I do think that when it came, I was doing another project in 2008.
00:10:06.290 - 00:10:30.092, Speaker B: I was just on a podcast a week ago. Somebody that knew me back then was saying, you have something where it was on AOL instant messenger and you can ask it a message and it would give you a response. And I was like, yep, I built that. It was not using. It was on the GPU as well. I was trying to map thought vectors and come up with knowledge graphs that would effectively allow you to answer questions. But I was really, really passionate about it.
00:10:30.092 - 00:11:23.980, Speaker B: But I remember in 2008 thinking, I got to either do this and spend high risk proposition going after anything AI related, conversation engine, natural language processing, or I can just stick with graphics, where I have a very strong vision for the next 30 to 40 plus years. And I went with the graphics piece, but it's fascinating to now see GPU's in the last five or six, seven years now really be at the center of all the progress we're seeing with AI. And I find that there's the entire threshold of technology interesting. Technology seems to be from figuring out how to map problems that have stymied us on cpu's and applying those to GPU's and then unlocking this enormous potential. So rendering for me and ray tracing was one of those early ones, but now we're at the point where there's many other applications. And of course, Nvidia is now a trillion plus dollar company, which is amazing, but also reflective of. Of the power of the GPU.
00:11:23.980 - 00:11:29.504, Speaker B: And at least in this day and age, it's right at the center of so much in the technology landscape.
00:11:39.684 - 00:12:04.904, Speaker C: Yeah, and as you say, we've now obviously got loads of different use cases which are driving the demand for this GPU's. So firstly, did you reach a point where, with the Octane app, were you struggling to meet the demands of the GPU's? And is that what has driven the desire to start render? And also, it'd be great if you could just give an overview of GPU usage, the future, how you see that whole industry going.
00:12:05.324 - 00:12:48.800, Speaker B: Yeah, well, it is funny, because you're absolutely right. When we put out octane in 2012, the one pointer version there was in beta for a couple of years before then there was a really interesting thing that happened, which is if you have two gpu's, 1080s were 980 to 1080s at the time. Were the standards the rendering you go twice as fast and unlike you, couldn't really put 20 cpu's in a single box. But there are towers you can get, especially cryptocurrency miners figure this out. But 1010 8016 GPU's, 20 gpu's. And the thing with GPU rendering at least was that you can go to 20 x. So we were seeing a lot of users just invest heavily in local gpu rendering.
00:12:48.800 - 00:13:31.934, Speaker B: But then as we started, as Optin became a more mature tool in the, I'd say 2011, 2012, we started to get workloads that would sort of surpass that. And at the same time, we've been working closely with Amazon. Autodesk as an investor brought us early, early into Amazon and said, Autodesk is bringing our partner Otoyo and we want you, Amazon, to put a gpu on the cloud. I was pushing for that for so long in 2009. My day into CS team up with AMD with Dirk Meyer at the time was like, AMD is going to build me a gpu cloud, because I was so confident that that was going to be the future of the data center. And then Dirk Meyer got fired and it didn't work out with them. But Autodesk and Otoy together did convince Amazon that there was a market for this.
00:13:31.934 - 00:14:12.308, Speaker B: Autodesk said, if you get all the gpu's, we will have Autobuilt software that will then market leverage that. And I was actually given the decision, like Otoy, give us feedback on this is on the g two insist app in AWS. You want Nvidia, you want AMD. And I remember it was close, but I remember picking as a Kepler 580 or equivalent or K two equivalent to Nvidia's gpu's on AWS, and they launched it in 2013. I went on stage with Jensen, I think around that time a little bit before talking about options going in the cloud. And almost immediately we started to see a very specific type of render job running on AWS, running on these cpu's. Autodesk streaming stuff was almost separate from that.
00:14:12.308 - 00:14:46.228, Speaker B: And some of the customers became investors in hotel like MSG, I think. Of course, now that MSG sphere is obviously a really big deal, but they were doing this kind of stuff a long time ago. I remember the renders that MSG was doing this 14k by fourteen k. I think the sphere is 18k by something. You know, it was. And that and VR job, like, you have to remember in 2014, you know, Google cardboard Oculus, you know, had just come out with a device that was, you know, they just put bought by Facebook. And in fact John Cormack, who created Doom and Quake and got the VR revolution restarted.
00:14:46.228 - 00:15:20.054, Speaker B: Good friend of ours met him through Elon. We did a. We partnered with Facebook and let people render on the cloud these giant 18k panoramas, which still look beautiful. But as people were doing VR videos and those things, I remember there was one job and it could have been from MSG, where Aws, all the GPU's would have been. It would have taken six months to render that full animation. I think it was something like August and January was the deadline, so there is no way. And what we thought is that there would be one customer that would get 80 of us reaches the time filled with GPU's.
00:15:20.054 - 00:15:56.430, Speaker B: One customer would do a VR job and it would take 500, 600 GPU's. And it just was impossible. It was impossible for us to market that business because AWS wasn't able to add our GPU's. Nvidia was the only game in town at the time. But the cost of GPU's on the cloud has been pretty high. It hasn't radically changed, certainly for on demand pricing, which is the only thing that matters when you can't get reserved instances or sponsors now the GPU shortage is almost a mean, as Matt's broadly understood at the time, it wasn't as much. And I really felt like the cloud part of our business was fundamental.
00:15:56.430 - 00:16:29.542, Speaker B: It was inseparable from my vision for our trajectory. So skip ahead a little bit later to 2015, 1617. And I started to see a lot of GPU's show up in rigs for mining ethereum or mining crypto. And of course the connection there was obvious. I also took out a patent, I remember filing it or putting it together, the brief for that in 2004, I think it was filed in 2008 or nine, for, if I look back at it. But it was really. It was called a token based model for ray tracing and billing.
00:16:29.542 - 00:17:02.440, Speaker B: So the idea was I could charge per ray, per photon, per unit of natural information, really, in the universe, and build a model around that. And octane had a perfectly scalable, almost like gasoline or barrel of oil. I could charge an octane bench, which we did on Amazon. So the render network was born around. Well, I mean, I can pay out more than you would get mining ethereum, and I could still charge about a 10th of what Amazon charging and have way more capacity. And so we started to build that in 20, 1516. And by 17, I think it was public, we'd sort of put that out there.
00:17:02.440 - 00:17:40.976, Speaker B: I had Brendan I could create JavaScript, Mozilla, Firefox, brave Basic, attention token, he helped us launch it, gave us some really good advice. He's another advisor to the project, and it took us about three years to get out of beta. In 2020 we launched render. And the very first job was John Nol, who is. I mean, he's one of the main guys at ILM, I absolutely know, for being a pioneer in the industry. And he was doing a job for the Hayden planetarium in Moda, one of the 3d tools that octane has a plugin for needed done in 2 hours. And we were like, well, we have something new, would you like to try it? And render performed flawlessly.
00:17:40.976 - 00:17:59.946, Speaker B: It was our first job and it was done on average users machines. And it was incredible. So we started to see that there was certainly the supply side. Felt like we could really. We can nail this. The price was where we wanted it to be. And the path from there has been making sure that this works.
00:17:59.946 - 00:18:33.402, Speaker B: Like running something on AWS is easy. Running this on nodes that are out there in the wild that could be doing anything, making it easy for node operators to add this piece of software, not have their machines totally locked off, because a lot of the people adding those cpu's artists, people that have a GPU for rendering locally. But I will say that the results from the artists that have been using render over the last couple of years has been. They love it. It's life changing. They'll buy less gpu's in their home or office or studio and use the cloud for doing this. And we're now getting back to a point, right, something full circle.
00:18:33.402 - 00:19:18.304, Speaker B: VR has gone through ups and downs, but Apple's putting out a device early next year called the Vision Pro, which is everything that we were doing back in 2014 and 15 and 16. VR is going to come back even with a vengeance, because the device is so much more powerful than the VR of ten years ago. And so we're gearing up for that. But the other interesting aspect is that, of course, as we were building out the render network, and it was initially on Ethereum, and then we moved to Matic Polygon and now we're moving to Solana, was that everything that was done on this system, it's on ledger. We have a ledger where every part of the render job is hash. You have complete provenance for when something was first uploaded, what 3d file or texture was used. If your naps are running, you can run a stable diffusion jobs, reference those pieces.
00:19:18.304 - 00:19:46.620, Speaker B: You have this incredible provenance and that to me was what an NFT should always have been. One of the artists that had been using octane for ages. People gold, I think one of the most valuable nfts in the world. I think the only person that came close was another octane artist path who's doing his nfts on the render network, which it's almost like you don't need an NFT. There's a receipt for this image, this provenance. And I was shocked when I found out that there were Hollywood movies being rendered on the render network. I found this out through a support ticket.
00:19:46.620 - 00:20:12.656, Speaker B: And then I thought into the studio afterwards. Dark Truth and Bush picture, the remastered version, November of 2021. Those shots were done on the render network. I think the team at Paramount was running out of gpu's. They needed it really, you know, like that. And so some average user's gpu was used to render a movie that was shown in theaters and put back on DoD and Paramount plus. And it shows that the end to end encryption that we built for the render network really paid off.
00:20:12.656 - 00:20:37.544, Speaker B: Because I remember when I started, people were skeptical of running render jobs on Amazon or Google. I mean, that changed when Marvel used our service on Google Cloud to do one of their opening and the Wasp was done on GCP precursor to render. And so here we are. That's the story intrapy of how we got from cloud rendering to render. I will take a breath there.
00:20:38.364 - 00:21:50.504, Speaker D: When you look back at sort of Wendel started, you're talking the advent of YouTube, you're talking grainy video and really static images and very short period, you've got to 4k video closer and closer to real time games at least. And so, you know, step up and compute on each of those. And the point he's alluding to here, really, the next one is as we move towards three D and spatial is another jump in compute, need we estimate it being 100 x plus where we are today. So even though Nvidia have done a great job keeping up with Mourzlaw, at least building faster and faster GPU's, the demand is just expanding. And that's only on the render side, let alone some of the new use cases have emerged the last year or so around this. So, you know, we had to find a solution outside of just doing it on your home GPU and pressing enter and coming back a day or two later. What we really brought to the table was simple workflows, orchestration, and an incentive model that made a whole lot of sense to everybody in the ecosystem.
00:21:50.924 - 00:22:34.914, Speaker A: One quick question I had. Now that you guys are kind of comparing the status quo, which is using a cloud provider like AWS or Google, versus distributing it on these devices that people own and rewarding them for it. So, like Multicoin's original blog, when they announced the fundraise back in 2021, brought up this interesting point that I wasn't aware of, which is cloud providers aren't allowed by Nvidia to use flagship consumer graphics cards. And so there's kind of like this weird dynamic where they sell workstation cards under the Tesla or Quadro brand, which are expensive but don't provide that much of a lift in computation power. Do those economics also play into some of the thesis that you guys are driving at with respect to render?
00:22:35.214 - 00:23:01.890, Speaker B: Yeah, I should have buried the weed when I was describing why we were running out of GPU's on AWS and even Azure and QCP when they added them, which was that, you're right, you put a data center GPU. We started with Kepler. I think it was like a Kepler grid. 580. Nvidia charges thousands, thousands of dollars more for that card. If you go to Best Buy and you bought a GeForce card game performance attempt the price. But you're not allowed.
00:23:01.890 - 00:23:29.524, Speaker B: I mean, Nvidia is. You cannot run this in a data center unless you're mining cryptocurrency. Then they allotted. And I remember talking about this with Kyle, I was like, he's like, well, why can't you just add working piece? Well, you can. It's just you'd have spend ten times the amount of money to compete with the hundreds of millions of that are out there that today make up the majority of nodes that we'd be running on the network. Because there's millions of gaming GPU's. And those gaming GPU's are incredibly powerful, even for AI.
00:23:29.524 - 00:23:56.240, Speaker B: You can do a heck of a lot of stuff on them. 3090 or 40, 90, 24 games of VRam, maybe not running, maybe not training an LLM. But what's funny is that the world is changing a little bit. You have Apple putting out these Mac studios. They can have 192 gigs of video memory. That's more than any Nvidia GPU in any data center, unless you link them together. But our sort of thesis was that, and I did bring Nvidia to the table.
00:23:56.240 - 00:24:26.086, Speaker B: I said, guys, why can't we put these jeep worse cards in Amazon? And they're just like, no, Jensen wants to keep those separate and for business reasons, probably. It makes sense, why not? They have quadro cards that we would try to help them market those quadro cards because the drivers were better. Frankly, drivers can be important. So people were willing to pay the premium and such ambitious AMD did the same thing. They had their own workstation cards. Those were, you know, those were a step above cost wise from a gaming card. And then the data center cards were even more expensive.
00:24:26.086 - 00:25:12.140, Speaker B: And to, you know, to a fair point, if you need, you know, you know, if you need high quality error, you know, fault tolerant GPU's, um, you know, that extra work, that's what, that's what you're effectively paying for. But you know, from the perspective of the practical world out there, if a node GPU fails, I mean, you just do the red run under the GPU. We had a wait list that was, I think at one point, it's like a million GPU's. Of course, this is a couple of years back when we first opened those things up. The amount of GPU's we have available that are in the class of a gaming card is very high. And so the challenge is we look forward. You hear about the GPU shortage of h somewhat, but the interesting part is that there's a huge amount of AI work, both inference and training, that can be done on those you have to supercards as well.
00:25:12.140 - 00:25:39.394, Speaker B: Just not everything. But that's something that could also be cracked over time. So I just wanted to say that even when Google came to the table and Eric Schmidt saw what we were doing with Amazon, 2013 already brought him in. I actually spoke to him about a month and a half ago just to catch up on everything. He was very excited about the stuff we're doing AI. But he said, I've known about this, I was showing him the cloud streaming stuff, I was showing all these things. And Google built these GPU's, but when they launched, I think it was the D 100s, you have precursor to a 100s, you only have like a thousand, right? That's not a lot.
00:25:39.394 - 00:26:00.130, Speaker B: That's not. It's not enough. And now fortune, tens of thousands of these GPU's. But when you have millions of customers, tens of thousands of GPU's, there's not going to be a lot of GPU's per customer. Right? And those are the numbers that we're looking at. There's 2 million blender users. I mean, it's like there's a lot of in addressable market there that we want to support, not to mention hundreds of millions of users.
00:26:00.130 - 00:26:22.294, Speaker B: Now we have opt in on the iPad and the iPhone vision Pro coming out and those markets, I mean, you will need to send it to render to do anything on those devices. That's going to be final quality. So, yeah, there's a lot of interesting pressures that were created, by the way, that Nvidia data center GPU's, AMD datacenter, GPU's as well have been priced for sure.
00:26:23.354 - 00:26:34.336, Speaker C: Can you just explain how render will interact with the octane app? And is that predominantly where it would be used, or is it a completely separate thing? Yeah, if you can just go into.
00:26:34.360 - 00:26:58.084, Speaker B: That, you have to think about the render network. And the render ecosystem is separate from Otway, and it is my off. The render foundation is separate. I am an arm's length for it, although I have tons of ideas, and I feed that to everyone. And for the most part, I think it's going really well. I look at Otoy as just being first party app on the equivalent of an app store or an ecosystem. Render is designed to run anything.
00:26:58.084 - 00:27:17.460, Speaker B: It's been built and tested and validated with octane. But we already brought on Autodesk being an investor. We have Autodesk's amazing render called Arnold. It does have a GPU version. They started on the CPU. I know the founder super well, and then the majority of artists. It's not like the big studios that I care about.
00:27:17.460 - 00:27:55.432, Speaker B: I want individual artists. There's a million peoples out there that we want to bring in, and a lot of them use Redshift, which is another GPU render. They started it a few years after octane. I love those guys. They're part of a company called Maxon, and we worked out in really interesting deal with Maxon, where we can bring in their 3d tools. Redshift is a render all on the render network. And if you run a cinema 4d job, which is their software, the 3d software, not using any of our Otoy code, and you're running redshift, not our renderer, you're going to be running a render job that is literally no Otoy code at all, other than Otoy built the foundational pieces that does the encryption, that can load a payload.
00:27:55.432 - 00:28:17.672, Speaker B: There's an SDK for it. We're testing that with other renderers, other 3d processes and tools. So octane is just the first. And it's almost like when the iPhone was first launched, you had built in apps for a number of things that eventually others filled those in. I mean, there wasn't even an app store right on the first iPhone. But the idea is that we're very quickly bridging other types of graphics jobs. And we also recently, Trevor can speak to this.
00:28:17.672 - 00:28:42.634, Speaker B: We're also bringing a lot of other vendors doing general compute AI computer. They can piggyback on the backbone of the render network. So it's certainly, it's great to have the training wheel, so to speak, and a lot of testing and validation built around our own products that came out of otoy that works way beyond that now and looking to expand to almost anything that can run and match demand and supply on the render network.
00:28:43.174 - 00:29:39.790, Speaker D: Yeah, I mean, rendering was very much the first cloud GPU use case and has been phenomenal as we've watched its scale and distributed rendering become a thing. What's really emerged in front of everyone the last year has been AI use cases. And for us, on the foundation side, what Otoy has done amazingly well is simplified workflows for motion graphics artists. And as we sort of looked at the new emerging use cases, we wanted to reposition ourselves as a platform that more and more third parties could plug in beyond just renderers. So, general purpose compute jobs. And we went out really looking for that first Otoy for AI. The community settled on a company called ionet, and that will be our first launch partner.
00:29:39.790 - 00:30:00.682, Speaker D: But I would expect many to come along the way. And for us, it's really exciting because it's our vision of the network becoming a broad platform with different clients for different services. And Otoy continued to do an amazing job on that artist specific rendering workflow.
00:30:00.818 - 00:30:29.214, Speaker A: That's super helpful. Context. I think what's clear is this journey started with rendering graphics, but over time it became clear that there's a lot of other jobs to be done on graphics cards, namely AI, especially post chat GPT. From what I understand, render has been around at least they first launched in 2017. And you guys mentioned transitioning between blockchains initially, and Ethereum, then Matic and now Solana. Tell us a bit about that journey on the render side.
00:30:29.334 - 00:31:32.550, Speaker D: Yeah, I mean, some of the history starting on Ethereum made a lot of sense circa 2017. You know, it was really the advent of smart contracts and I think was a great starting technology. I think where we struggled a little after that was gas fees as it become more utilized. And so the natural push there was to L2. And when the foundation assumed stewardship of the render network, the first topic out there was relooking at our, our business model. And what the community settled on was a move towards a burn mint equilibrium model, similar to that of other deep in projects like helium and others. And immediately after that because it was a full rewrite of the model, the question emerged was, which blockchain should this be done on? And, you know, it was a pretty spirited community debate.
00:31:32.550 - 00:32:37.092, Speaker D: I think where it settled was it made sense first to choose an l one rather than for us to run our own blockchain at this stage, just in terms of resources. And we were looking for low stable cost, proven deep in usage and high throughput. And in the process, there were a couple of finalists, but the community ultimately settled on Solana. And, you know, what's really exciting is a lot of the vision that Jill's outlined at the advent of the render network on chain provenance for all assets is something that we see becoming more and more reality as we can move to Solana, as you can put compressed nfts on chain with mountains more data than you could in other infrastructure. So it's been exciting to watch the transition. That doesn't necessarily mean that we're moving away. It's more we're actually adding or supplementing the new model on sauna.
00:32:37.092 - 00:32:48.972, Speaker D: So there will be a place for the old integrations and an ongoing place as the network scales.
00:32:49.148 - 00:33:08.692, Speaker C: Yeah, I was in the telegram the day or so after it was announced the move to Solana, and as you said, it was definitely spirited, but I think it was a bold decision at that time, and I think hopefully you're being proven correct in that, to just go what's best rather than necessarily what people want you to do to help boost their bags.
00:33:08.868 - 00:33:33.094, Speaker D: Yeah, definitely. We're one voice and it was a community vote, but working with the team post has been amazing and the ecosystem has been fantastic. The support we've got from other deep end projects have done this, helped us and more. It's just been a different approach and gratifying to see. So really exciting so far.
00:33:37.914 - 00:34:20.345, Speaker C: How have you found the supply and demand side? Because I think one thing that's really strong about render is, firstly, on the supply side, you're not asking people to spend 300 $500 to buy something which they might not even need or use in the house. You're saying you've already got these items in your house, you don't need to go and spend anything, and it's a way to monetize GPU's, which are just lying around. So I think it's really powerful for the supply side and then demand side. As we've just been discussing, GPU shortage is a real thing and it's going up and to the right. So how are you finding the usage of the network, are you seeing it increasing or do you think maybe that this integration with ionet and other AI providers could sort of really help accelerate things?
00:34:20.489 - 00:34:48.947, Speaker B: Let me start with the rendering side. So the rendering side, I mean, we've been, I, you know, as a, we put out otoy, right, as a company, put that a product with support ticket and everything. And I, I've held back pushing users on the render network. I want to make sure the experience is perfect, because one thing about artists is that if they have a bad experience, they don't come back. So I would say that just within the amount of optane users that have actually ever used that, I mean, we're probably like 90% of them have not been pushed onto the network yet. I want to get this thing perfect. I want to get it to be pretty flawless.
00:34:48.947 - 00:35:26.910, Speaker B: And some of that is also waiting on external workflows I know are huge when we send them in 4d, that maximum license, we announced it two years ago. I showed the very first version of it working a year ago at Breakpoint, and now it's in late beta, which is great. That's going to unlock probably a ten x of artists just alone. I think that's one important indicator of where demand is coming from on the other side. I was saying this at the very beginning, knowing that this is actually a really important part of our trajectory in the next few months and certainly into next year. What broke Amazon and Google? And this is like simple VR. This is like VR in Google cardboard.
00:35:26.910 - 00:36:05.040, Speaker B: The kinds of renders you're going to be doing to get the Apple Vision Pro to have one image that looks great and looks beautiful is going to blow that budget to the moon. So single artists might be spending a lot of money. I remember spending on Amazon $20,000 for a single volumetric render. Thankfully, on the render network, that probably would cost a 10th of that. But still, you mentioned that as being the unit of energy needed to be spent for an interesting static image on an Apple device. There's this accelerant there. I also feel like we're going to be, seven years from now emerging into something where it's not just going to be for headsets.
00:36:05.040 - 00:36:47.780, Speaker B: You're going to be rendering these massive volumetric scenes for light field displays where when investor in a company called Lightfield Lab, its truly building this Star Trek holodeck. Those panels will probably come out in consumer form at one point with 100 inch tv, but itll be like looking through a window. Youre going to have trillions of rays of light and were working towards that. The demand side for just the types of jobs and the necessity of revisiting spatial computing, all of that is going to push things pretty high. And we havent even started on tapping into our own user base, much less user base of larger 3d ecosystems. I mean, I'm going to the blender conference and speaking there at the keynote in a couple of weeks because as I said, there's 2 million blender artists. The render network for them is going to help.
00:36:47.780 - 00:37:23.754, Speaker B: I mean these are artists that are using a free tool because I guess budget wise and everything else, it's expensive to invest in software. Octane is free for blender users. Render is also something we subsidized on certain cases to get people going. So I think that those things are incredibly strong indicators of where demand is going to go. And we're also starting to see interest from the same types of customers that broke the Amazon version of our rendering cloud service coming back. And so I feel like all those things are exciting propositions for the demand side. And I think on the AI side, I give it over to Trevor top at the other partners.
00:37:23.754 - 00:37:53.466, Speaker B: There's so much already in Optane that is AI based. We're the first render to put AI denoising in there. AI upscaling, obviously anything related to diffusion models we already have on the render network. You can run a stable diffusion job. It's going to be more and more tightly integrated with render jobs where you're going to do stuff in post, you're going to do in painting out painting and style transfer and all those things. But it's not like those things without rendering are going to be a better version of a graphic pipeline. So we know all those things are coming.
00:37:53.466 - 00:38:02.018, Speaker B: And then on the other side we have Trevor and a team working on bringing on all system partners on the general AI compute side. And I'll let him speak to that a bit.
00:38:02.066 - 00:38:53.522, Speaker D: I mean, there's hundreds of thousands of open source models on how you face today, and the growth you're seeing is just exponential there from zero a year ago to where we are now. So we're seeing an explosion in models. Some of those need very specific high end hardware to train, but some don't. And we're seeing, particularly on the diffusion side, less performant GPU's being possible to do actual training now. Not yet. At the point where it's become the predominant usage point, I think anyone would love to train on the highest performance gpu they can. But just out of need we see that expanding beyond the production because you've got an inverted pyramid on the GPU production side.
00:38:53.522 - 00:39:34.340, Speaker D: You know, the most performers are the most expensive and the least produced. Right. And so, you know, finding that sweet spot lower down the pyramid is really important. You know, just looking at what could be done besides training, though, a lot of fine tuning and inference. And, you know, we're looking at really the orchestration layer because that's what's so critical here, you know, how you manage distributed GPU's for these jobs. The I O team, I think, have put a bet on the ray ecosystem and yeah, Python libraries to orchestrate those. And it's really interesting to us.
00:39:34.340 - 00:39:53.964, Speaker D: We're seeing other providers with slightly different approaches. And honestly we, we don't know as this merges, which one will, will be the winner. But that's the great thing about the open client, is we'd like to invite as many as we can and really focus on using up all of the abundance of compute that is out there.
00:39:54.124 - 00:40:48.442, Speaker A: You guys highlighted some really compelling tailwinds there, whether it's the existing paradigm of generating really high quality media with rendering engines, the volumetric, spatial computing, VR, whatever you want to call it, emerging use case, which that's going to drive a ton of demand for it. And then obviously now we have the AI tailwind. So clearly there's this groundswell of demand that's coming and it feels to me like it's accelerating. So kind of in this vision, whether it's five years from now, ten years from now, where we have all this demand and we're starting to distribute it onto different computers in this network, the render network, maybe it would be helpful to just understand the role that the render token plays. Is it a coordinating mechanism? Is it used for actual payment for these services? Are people going to be able to use stablecoins, for instance? How do you guys think about the actual mechanics of how monetary flows happen here?
00:40:48.578 - 00:41:22.934, Speaker D: Yeah, a lot of that is covered in some of the rnps the community voted on this year. So definitely a consumer first approach. And many of the artists today looking to do rendering on crypto native. And so a fiat on ramp is essential. And stable pricing associated with that. So fiat pricing is key to how we've done rendering, how we'll do compute on the AI side too. So that's step one.
00:41:22.934 - 00:42:38.082, Speaker D: Step two, the new model is a burn model. Whereas usage increases, burning increases, and then on the flip side, you have an emission schedule that's separated from that and allows you to allocate emissions to participants across the ecosystem. So today that's predominantly node operators. And what's really exciting is we believe we can allocate more towards creators and at various points in the growth cycle as you scale a two sided marketplace, really being able to tweak that and help the members of the community that are most valuable at that stage earn rewards in the process of scaling the network up. Then to jules other point earlier, a big part of this for us is on the blockchain for provenance. And when you think about the age of AI and being able to prove you've created things, it's only growing in terms of a real challenge. You've got writers strikes, you've got artist strikes out there at the moment and they're all dancing around the issue of owning your ip and being able to prove ownership.
00:42:38.082 - 00:42:52.154, Speaker D: And we honestly believe blockchain is a key part in this solution. It can help prove and also do complex royalty calculations and more as we delve into doing this the right way. And I know Jules has a lot to say on that.
00:42:52.574 - 00:43:24.742, Speaker B: Yeah, I mean, my head is. I want to add so many pieces on top of that because it's a great foundational starting point for all the things that I wanted to sort of say that we're working on right now. You know, there's a lot of. I mentioned going back, like lifestage was a fundamentally important part of our business or Otoy's business. And I mentioned that, you know, in addition to creating scans, digital bubbles, we're at the point today where you have a scan and it is the actors. I mean, it's so precise, it's groud truth, right? And it's the best in the industry. We used to give this to John Noel like he did Rogue one.
00:43:24.742 - 00:43:51.890, Speaker B: It's not my favorite sort of experience, but every single time you see a digital person brought back to life over the last 15 years, we're probably involved in it. So fast and furious. Seven ball worker, that worked out really well. Leia and Tarkin in Rogue one in 2016, not so great. But it was John Noel taking our light sketch cans of two actors and trying to animate that and it was really hard. And I remember Clay and I, our chief creative officer, talking to him and saying, well, you know, maybe one day I will make this easier and the artists will have an easier time of it. And here we are.
00:43:51.890 - 00:44:17.456, Speaker B: You know, it's. It's still not perfect. I mean, Indiana Jones was at, you know, five, you know, when he was de aged, it was 25 minutes was a bit tricky, but I'll tell you at Otoy, within the work that we're doing, it's. We've perfected that. It looks amazing, and we're going to be providing that as a service, not just to studios, not just to things, but, you know, remember Ari? You know, he was a talent agency. I mean, there are thousands of people in the industry who've been through light stage. And he helped us.
00:44:17.456 - 00:45:00.302, Speaker B: He really did help us build sort of a system around there where you can own your light stage, scan the machine learning pieces that are using to train on idea, and then use inference to render you again are yours. And that's something that needs a smart contract to figure out if you want to. And there's so many different ways this can go. It's just fascinating that the whole actor strike really sort of hit this on the nail, hit this on the head and give people the understanding of how important this is. But I do feel really good about the fact that I think we've had a solution that's absolutely good for the talent, for the actors and everyone else, and a system that's like, what's the use of crypto? Well, I mean, in a sense, you have something that is designed. That's why slide. I think it's great because you can split a world in a million different ways.
00:45:00.302 - 00:45:41.034, Speaker B: All of those things can get really complex. And God help you if you're running it through a machine learning system or training step, blending all these things out, it gets even more complex. But again, the provenance, the pieces that go into a job of any kind, AI rendering or otherwise, if they're tracked back to the artist, the creator, or the person who uploaded a piece of data that had that provenance all that's there. And so I imagine that the systems you can build are going to be complex, the kinds of stories, even if you wanted to see a movie, right? And we've done stuff where, with the Roddenberry estate, Gene Roddenberry's son, my best friend, invested in Otoit, and we built the Roddenberry archive all on render. All, you know, gets all these beautiful cinematic scenes. Apple featured some of them in their keynotes. In the last two years.
00:45:41.034 - 00:46:25.734, Speaker B: One of the things we did was we brought his mother, who played the voice of the computer on the enterprise. She recorded all her venoms, her voices and syllables and everything in 2008, before she died, and we brought her bath. And it was a way to sort of show that we can build these incredible pieces on rendering. And I do think that things like that are going to be more and more common where legacies of performance and it, and all these things will kick in. And I do think that there's no better way of doing it in the kind of system that we're building here with everyone in the render community. It's not just oh, toy, there's so many more participants involved, but it's an exciting time, complicated time, for sure, but I'm optimistic about where it's all going.
00:46:25.914 - 00:46:38.274, Speaker C: Have you faced a lot of pushback from people when you mentioned crypto before? And have you noticed that changing maybe with the rise of AI and the desire, the need for things like provenance?
00:46:39.214 - 00:47:21.524, Speaker B: Raoul I think at our level, Otoy is a well respected company in decades in the industry. So we're saying we're doing something with crypto. Everybody assumes we're taking it seriously and we're doing it for the right reasons, and we are. I think that if we were to do and if we behave differently, if we treated it, if render wasn't utility solving a real problem, I think it might be different. But I do think that the whole story around using blockchain for provenance is much more well received this year than any other year in the past, especially by people in the media and entertainment industry where there's or the writers or anybody creating content. Right? I mean, nobody wants to have their work sort of resampled without any sort of compensation, recognition or provenance. So, no, it's a very different thing.
00:47:21.524 - 00:47:51.360, Speaker B: We also, I mean, there are certain words that are loaded, like if you were to sell a digital collectible, not call it an NFT, it might work better in certain markets. NFTs definitely has a lot of baggage because a lot of people did things with that were NFTs that didnt pan out. But frankly, the premise behind it is amazing. And I think same thing with the metaverse. Ive been talking to Neil Stevenson a lot, obviously created the cord metaverse and snow crashed. And I think that that could be reclaimed as well. And I think Trent was talking seven years it was producing.
00:47:51.360 - 00:48:25.244, Speaker B: Was it bain that was saying it's a 900 billion market by 2030? Yeah, there's going to be a lot of elements where you're just going to want to have that sort of record keeping, that ledger and smart contracts, figuring out how these things work. But the provenance for AI is an immediate problem. And I think that we're going to see a lot more sort of understanding by the mainstream about why it's important to have a decentralized record of all these things for proof, for validation that can't be tapped and can't be messed with. So here we are.
00:48:35.204 - 00:49:18.078, Speaker A: I'd be curious, actually, just given how essential you are to everything that's happening in media and in LA. You brought up the writers strike, which I think touches that, a broader sense of unease about AI. I think the obvious thing is it's going to change everything in every industry and there's net positive long run. But like any sort of technology that emerges and becomes mainstream, like the Internet, for example, I grew up watching Nickelodeon Cartoon network kids these days watch independent content creators on YouTube. It disintermediates a lot of parties that once held the existing power structures. I'm curious, what do you think about AI generated content and how AI might just change the media industry as a whole?
00:49:18.246 - 00:49:54.844, Speaker B: I feel like it's inevitable that you're going to see when YouTube exploded, that was 2006 and people, little did you know that you have people making more money on YouTube than they can make in any other medium. I mean, everybody became a video creator, right? The very premise of putting an octane was I wanted everybody with a GPU to be John roll or at least have the same tools that Marvel or Disney or Lucasfilm had. Into that degree, I think we succeeded. What's still there is the complexity of making 3d contents and scenes. But I mean, the ideas that people have, everybody wants. Everybody has this. For including me, I love doing these little Star Trek vignettes we did for Roddenberry.
00:49:54.844 - 00:50:21.896, Speaker B: I have a team working on me that's work for me. That's amazing that does all that. But there's going to be a world where AI is going to make that a lot simpler and there's going to be a lot of garbage too. That's like, AI can generate so much stuff. This weird human mind can really tell when you're seeing an AI generated thing. It's not because there's the number of fingers wrong, there's just some sort of something a little bit wrong with the lighting or the sheen. And I think that's what you get from just typing something into a texted image or video generator.
00:50:21.896 - 00:50:43.598, Speaker B: But I do feel like that in the 3d world where there's going to end or spatial computing, I think AI is going to be essential. I mean, we're using that. We're taking, I mean, light stages. Utility in the future is connected to machine learning. They can train on, you know, there's going to be an actor performing a scene. It just. We want to take the 3d face that was scanned and put them on somebody else's face, or do that with clothing or do that with other elements, or hair even, and makeup.
00:50:43.598 - 00:51:07.052, Speaker B: Right. Which is all useful. So I think that I have no problem with content being generated. The problem that I do have is when it's used to trick people, right. Fake videos, fake news, all that is awful. And we've had, there's a lot of people working on machine learning, especially faces within our group, and there are ways of detecting it. If it's high enough resolution, HD is probably suitable.
00:51:07.052 - 00:51:34.832, Speaker B: If it's less than that, maybe you can't trust your eyes. Maybe you can't trust the video call. It's incumbent on companies, probably like Apple and others, to maybe do something where if you're getting a secure call or getting a secure connection, there's something that validates that this is coming from the camera that goes on chain. I mean, lots of things can be done to mitigate it, but I think you can't hold AI back as a tool for creative people. That would be silly. And I think what you need to do is what some responsible parties like Adobe have been trying to do, which is like, everything that's trained is licensed. It's just licensed data.
00:51:34.832 - 00:52:10.876, Speaker B: You know, if you're gonna bring back an actor, you go to the estate or you go to them, if they're alive, right. And you work something out, and all of that is fine. I think the problem is when you, you know, when you start to do, you know, say havoc or do mischievous things against people's consent for things that are truly meant to deceive, that's a problem, and it'll have to be dealt with. But it's not something that we should bring our hands over and say, well, let's not advance the technology, that Genie's out of the bottle. But I also think the quality always matters. One of the things that we've learned is that other GPU renders have emerged in the last ten years. We're still, our users have only grown.
00:52:10.876 - 00:52:42.740, Speaker B: I mean, we've just exponentially increased the number of optane users over the last couple of years. And I think part of that is because the quality is really high. And what I've found, even with things like Gaussian splatting things like, oh, great, we can capture the world. It's a 3d model. It looks like paper machine. When you're really close to it, there's still something missing in the fidelity of it. And AI, I think, will get all the way to the point where it can assemble your scene and do all these things for you, but I still think you're going to want to have director or your content, that vision of a human person in there writing as well.
00:52:42.740 - 00:53:16.644, Speaker B: Although I do think that, you know, we're just already seeing massive disruption. Term papers don't mean the same thing anymore. You know, certainly writers that are not very good can probably get pretty good scripts written by an AI. And at some point, you know, AI can be trained in domain specific ways to deliver you something that is very high quality. You know, we've done that for faces now where we've used machine learning to animate a face and it's as good as an artist doing it, you know, frame by frame. On the other hand, you still need other artists to make that system work and engineers and all of that. And so it's always an never evolve anything.
00:53:16.644 - 00:53:40.924, Speaker B: I've also, it's been really true to see people just do crazy hacks around the AI tools that are there. Like everyone that has octane, right. Not all of them create equal quality content. Some of them figure out how to use the render, how to push things to the input with the camera and create more interesting content. AI is a tool. It's not yet a one click solution for doing everything that humans want, ever. If it were, that world would be a different one than we live in now and it would be more interesting.
00:53:40.924 - 00:54:12.536, Speaker B: But we're not there yet. And until we are, the place for all the things that we're doing, I think make a lot of sense and I think it should be considered an augmentation. And on the negative side, obviously, we don't want to have disinformation or people's work stolen or monetized without their permission. Absolutely not. That's a no go. There's such another, on the other side, there's so many positive aspects as well that are going to open the opportunity for many more creators. And that's certainly in line with my vision for or otoy for the render networks, you know, for the industry and society at large.
00:54:12.600 - 00:54:42.682, Speaker D: Really, I don't see it as a binary. Like, it's not AI eats the artist's workflow. I see it being, you know, a convergence of a workflow that can be greatly simplified and can unlock the next adobe and TikTok generation to creating some amazing stuff, but not in isolation. Either way, I really, more and more, if you're going to solve the ultimate goal of real time holodeck experiences, it's going to be both pieces. It's not going to be an either or.
00:54:42.858 - 00:55:20.594, Speaker C: Yeah. And I think every piece of technology is always met with skepticism, and, you know, we managed to find a way through. I feel like most people maybe are worried about losing their jobs, but hopefully it heralds a new age of where we don't spend our days, you know, sat in cubicles doing things which a machine could do anyway. And, you know, hopefully it furthers the human race to some level. But I'd just be really interested, Jules, to get your view on the metaverse. You mentioned it earlier, Tim Sweeney, one of your friends, I think he's really bullish on it, and he did a great keynote about it. So, yeah, just wondering, do you think this is something that awaits us in the future, and in what form do you think it will take?
00:55:21.694 - 00:55:44.576, Speaker B: Well, I think, first of all, I am bullish on it. It's one of the reasons why I started the company. I think the form that I'm most interested in is the holodeck, because I feel it's the most natural, organic, and in some ways the most social. Because if you're wearing absolutely no piece of technology on you, not even looking at your phone screen. Not looking at a screen, note glasses, that's great. The technology to build that exists, it's just insanely expensive. Leche lab.
00:55:44.576 - 00:56:17.564, Speaker B: If you were to build a room the size of the structure, call it x, under their panels, it would be tens of millions of dollars. Right? That's too much. It'll probably go into museums, theme parks, concerts first. But imagine in the even that this becomes as cheap as OLED technology is now, and you can use it as wallpaper anywhere on buildings. In your room, instead of going to your garage, you go into your holodecks. And instead of having to put on a pair of goggles, you're just where you want to be. Obviously, connecting with people that are far away is one of those things where that becomes pretty important.
00:56:17.564 - 00:56:54.146, Speaker B: From the fantastical side of things, what I loved about looking at the future is I grew up literally in Roddenberry household. I've spent so much time there. I do think that Gene Rondre's vision for the future, this optimistic utopian vision from his perspective, is something that you can look at to sort of see how a good version of the Metaverse might evolve, where you go into that room and a story is told. It's like data would go into the holodeck to go out, do a Sherlock Holmes story. And obviously, the ability to sort of immerse yourself in these things is great. And I also think that storytelling in the Metaverse is something that could be really fundamentally interesting. I mean, video games were not.
00:56:54.146 - 00:57:21.470, Speaker B: I started in the video game business. It was not the respected art form of any kind that it might be now. Not everything falls in that category. People have done beautiful things in video games, right? It rivals Hollywood for storytelling, for IP, for all these things. And I do think that the Metaverse has that as well. There's the shared Metaverse, where we're all sort of interacting in a different version of reality. There's storytelling in the Metaverse where we're part of a story driven line, and we're effectively after ourselves in that.
00:57:21.470 - 00:58:03.670, Speaker B: And then there's just the productivity aspect of the metaverse, where we're not on Zoom calls, we're actually all in a table looking at each other holographically reported and streamed. And until we have solved transportation fully to the point where it's like teleportation, that's pretty promising. I do think that there are some downsides, which we should avoid, which is we don't want people to be socially isolated. We don't want people to give up on their real lives and go into the metaverse. So it's just unhealthy. At some point, it can really isolate you and stunt your ability to just connect with reality. So I'm hoping towards something that's more, you know, like the, like the holodeck experience or something that just isn't as isolating, but you're gonna have both pieces.
00:58:03.670 - 00:58:44.540, Speaker B: And I feel like the, you know, Apple's big bet on the vision pro is not just another company getting into the glasses space, it just isn't. And I think everyone sort of understands it to some degree, because when Apple does something, and it hasn't done anything since the iPad, right. I mean, it's like, there's the earphones, there's the watch that came after the iPhone four, right? You know, and now you've got these glasses, right, this headset. And I look at what Apple's doing as something that's going to be planned over decades. We're about to hit 17 years on the iPhone. Imagine these glasses in about seven or eight years time being like a pair of sunglasses where things are just completely streamless. And that's something that will eventually be replaced by, I'd like to displays.
00:58:44.540 - 00:59:08.568, Speaker B: But that world, I think Apple's definitely not interested in having people getting lost in the metaverse, particularly not that you can't build that on the Vision pro. I mean, you can, we're obviously developing for it. But I do think that the idea of having experiences that are. That are just not limited to a screen, that are volumetric. It's going to be up to developers like it always is with any new platform to do something interesting with that. But that device is very different than anything that matters. Released any VR before it.
00:59:08.568 - 00:59:33.000, Speaker B: It's not truly a VR device. It's more, you know, they call it spatial computing. It's just bringing the concept of almost a what the holiday could be. You know, you're cheating, you're getting there a little bit sooner, and guess it costs $3,500. But I will tell you, the resolution of that display when you're looking through it and seeing the world is phenomenal. You're going to start to get experiences that will effectively give you a sense of what that might be. I find that to be really interesting.
00:59:33.000 - 01:00:09.516, Speaker B: A lot of what Apple's focusing on, of course, is telepresence. Saw the let's Friedman Zuckerberg thing where they have codec avatars. But I do feel like that's where everything that we're doing on the screen today, and there's a lot probably is going to move over to something spatial in the next seven to ten years, and then at some point it'd be great if there's no intermediary technologically between. Between those things, and we find ways of still incorporating our real eyes into this. And just like we watch a certain amount of tv place here in my video games, it's just not the whole thing. I mean, that's, I think, the healthy version of the metaverse, and whether it's called that, I think you have. Nvidia calls.
01:00:09.516 - 01:00:33.084, Speaker B: Nvidia calls it the omniverse, Apple calls it computing net, it calls it the metaverse. Neal Stephenson has his own ideas, but some version of that, of a spatial 3d immersive mesh network is coming, and it will probably be the future of how we interact with any form of services coming into our field of view until we get to UCI and other things that are probably even further out.
01:00:33.824 - 01:01:26.460, Speaker A: It's a super interesting topic, and one that I've also thought a lot about as obviously it's coming, whether it's VR headsets or even this paradigm you're talking about, like holodeck, like in Star Trek, and some immediate thoughts that just come to my mind kind of riffing on this a bit, is we're already seeing this trend of social networks moving away from actually socializing and being algorithmically driven, whether it's YouTube or TikTok, this trend of people having fewer close friends. And you're sort of alluding to this, Jules, which is we don't want to forget about the human connection. Are there ways we can kind of leverage these platforms to enhance humanity, enhance social connection, and not continue to move in this other direction? And I don't know. It's not super clear to me that that's going to be the case. I mean, let's take VR, for instance. It's inherently an isolating activity. I think that was a criticism, by the way, of vision pros demo.
01:01:26.460 - 01:02:05.294, Speaker A: At least Zuckerberg spoke about it. It's just a guy in a living room by himself, kind of working through this virtual world. And so I think you touched on this, but with this clear trend that's happening not just on the spatial computing side, but now content can be generated by AI, the feeds are AI, and we're kind of isolating into these digital spheres. Is your view that the way we can kind of control for that is to try to encourage development of these physical kind of spatial computing experiences, to kind of preserve that human interaction? Or do you see this happening a different way, or is it still a risk?
01:02:06.034 - 01:02:30.832, Speaker B: Always a risk. I think that I've looked at everything that I've been doing for my career from the creator, an artist's perspective, or storyteller's perspective. And what's interesting is doing the rendezvous archive project, we started with just a few artists, a few curators. Everybody that's interested in Star Trek now wants to join the project, especially if they're an artist. They want to contribute things. It's a beautiful thing. I mean, the step that we did very early on with Tron karma fill us.
01:02:30.832 - 01:02:58.808, Speaker B: Facebook was render the metaverse. We gave octane away for free. Panic wrote a shader for VR. People could build these things for each other, and it was gorgeous. And that was a really interesting insight into how this could all go. And I think people that are collaboratively building things together, exploring projects together, whether AI is helping them do 95% of it, or GPU is doing all the ray tracing and making this happen, competent in terms of shader code, I mean, those are all sort of good things. I think that where there's.
01:02:58.808 - 01:03:27.150, Speaker B: Where they're dangerous. If you're basically sitting in an Internet cafe playing video games, like, and literally starve yourself doing that, you're going to have a problem in this medium as well. But I also feel like the idea of being able to collaborate more with people spatially could be a unifying factor. It's still not the same as being in the same room with a person. But there's going to be things like that. And I think as far as the risk of AI just totally dehumanizing things, I mean, it's. There was, I was reading the work about an AI author.
01:03:27.150 - 01:04:02.450, Speaker B: I forget her name. She's very well known, I should know it. But she was saying, well, if you have, as a book writer, writing about AI can write better than me, then they're effectively human, because they've understood the human experience to the point where it moves other humans, and there's some sort of social. So we may end up at a point where we have intelligent entities in the metaverse that are not human, right. You know, not human intelligences, but that could still interact with us. But the point is that, you know, we are humans. We experience all the qualitative quality of life in a way that is probably represented through storytelling and art and authenticity, you know, that's valued.
01:04:02.450 - 01:04:36.134, Speaker B: And so I'm starting to think that the goal of the future, the, you know, the work of the future, is what is authentic human work and experience that is, that is the most real, the most raw, and the most compelling. And if an AI can learn all of that, or needs to learn all of it, it's going to keep seeking that out. That's why you look at monocolops, where it's like an AI starts feeding on itself, and it sort of just completely degenerates. I still think that human innovation and human collaboration and frankly, just human empathy and experiences is. I mean, it's a challenge how you translate that to a new medium. But you look at cinema, it's some of the most moving things you have seen have been in that. Of course, you have moving books as well.
01:04:36.134 - 01:05:28.812, Speaker B: But as the medium becomes more immersive, those things can become even more powerful. And I think that's my hot take on where things are going and where I hope they go, as far as people contributing beautiful qualitative experiences through all these emerging media, whether it's AI, whether it's spatial or immersive computing, it doesn't sort of remove the need for the human adventure, as gene Rodrigue would say. For him, Star Trek was all about the humans in the middle of all this technology in this world that was, it's almost considered technologically overwhelming. And one of the only books he wrote about Star Trek. The whole thing that Kirk does is all the humans on Earth are stuck in the equivalent of the metaverse, not something that was in the Star Trek movie. It was in the script, and Kirk is like, somebody needs to go out there and experience, like, and figure out what the universe means for the rest of us. But it's a metaphor for how we want to do that ourselves.
01:05:28.812 - 01:05:58.054, Speaker B: If we just lose ourselves in the technology and then sort of mix of things, where we're going to atrophy of it as humans and in our contribution to things. So I think there's a compelling piece for that. And even if it's just your art of creativity, there's a lot of value there. There's a lot of value. And also building layers of society that intermix with the real world on top of these things. Those tools are going to be really different. When you have an LLM, they can just listen to you and really understand what you want in your intent and go help do it.
01:05:58.054 - 01:06:03.374, Speaker B: It can also, of course, go the other direction. But as I said, I'm an optimist for the positive outcome here.
01:06:03.874 - 01:06:20.014, Speaker C: I really enjoy your sort of schoolboy fanaticism about Star Trek, and I feel like maybe your life's work is trying to recreate these passions that you had as a child. Do you think that's what makes a good founder? Just maintaining sort of 100%? Yeah.
01:06:20.514 - 01:06:34.826, Speaker B: I mean, Goldrath, I watched that cartoon. I was four years old in 1979. I got that all part of that. I mean, if you can't tap into that, well, not everybody can. Sometimes I've lost that. But it's such a huge part of my passion and my energy. It fuels me.
01:06:34.826 - 01:07:04.580, Speaker B: It's fueled me since I was a kid, and I think that's an important aspect of humanity. Right. I don't know how I would feel. I mean, I know what you can do, but there's something where we've been influenced at a very young age, for good or bad. I mean, it can really. It can provide rocket fuel for the rest of your life. And I do find that a lot of that passion that I have for all of this, including the positive, I think comes from these experiences and things that moved me as a kid towards that sort of aligned worldview, I guess, if you want to call it that.
01:07:04.772 - 01:07:52.594, Speaker A: So we obviously covered a lot of really, really interesting topics. It's been a super fun convo. I think a bit of a closing question for both of you guys would love to get your views on this. What are you guys most excited about technologically over the next decade? Let's say, I think Andreessen Horitz just put out this really interesting piece called the techno optimist manifesto that pushes back against a lot of criticisms of AI, but I think it just broadly applies to just what technology has done for us as a species. And obviously, both of you are at the intersection of a lot of emergent trends that are happening, whether it's on the AI side, the increase in these high fidelity graphics media, and now spatial computing that's coming out. Would love to just get your views on what gets you guys out of bed and excited for the future.
01:07:54.674 - 01:08:32.299, Speaker B: It's very much sort of the, I want to get to the utopia of Star Trek as soon as possible. And I think that very specifically, and it's something that forget the capitalist times part of it. In a world of structure, there's abundance, right? There's no shortage of things to do. Not just abundance of research and mature resources, but you can go out there and explore the universe and learn from it. And humanity's betterment is the goal of everything. And there's no starvation, there's no wars, there's no, you know, fighting over resources, there's no separation people. And that sort of thing is very much what I want to see happen in the world of Star Trek.
01:08:32.299 - 01:09:13.684, Speaker B: It's made easier by the fact that food is not an issue and climate change presumably has been solved and AI has been sort of forced into a form of good. And I do think that with the responsible stewardship of some of the technologies that were emerging around AI, we will get there, and we'll get there soon. I mean, there's so many things. As far as holiday was supposed to be 300 years in the future, it's now AI data, the robot on Star Trek, that's now. We can get that. We can get something that approximately set correctly really soon. So I think what gets me excited is to get there and just come to a world where maybe our part in this is to help with artistic abundance and creativity and unlocking that and providing anything we can towards that goal.
01:09:13.684 - 01:09:39.043, Speaker B: But I do think that the world is potentially, if it's all this technology handled in a correct way, we'll get to a place where people really could just focus on creating beautiful things and having beautiful experiences. And starting with art isn't a bad place for us to begin. So that's kind of what gets me going. And I've talked, of course, a lot about the practical aspects of holodecks and all the things that are in the medium of that, but that's where the philosophically, into my head is very much at.
01:09:39.423 - 01:09:57.734, Speaker D: Yeah, amazing. It's hard not to resonate with a lot of that, given where we share the same mission. What I'd expand is I'm really excited to see how things go interplanetary wise in the next decade or so. I feel like it's all sort of converging as part of this and I think we'll see a lot.
01:09:59.034 - 01:10:18.202, Speaker B: Yeah. I have one more thought. We just popped in my mind, which is that we've, there's been this certainly during my lifetime. What happens if we meet an alien? What would it be like? What would it tell us? But last November we had our first interaction with it. Non human intelligence. I mean, I think chat DBT, whether it's alive or not or conscious, it's intelligent. I mean, chat DV four, I give it a bing search.
01:10:18.202 - 01:10:50.482, Speaker B: It's, I mean, 1 second, it's, it knows what it's talking about, it understands what it's talking about and, you know, and it's very polite, right. So it's like you don't have to worry about AI being this weird, you know, it's. I think that we're going to understand what it means to be human in a very different way, maybe even a more powerful way than we could ever imagined if somebody, you know, flies off from land on the White House lawn. But also we might understand our place in the universe better. And I think, you know, I like the idea of what Elon's suggesting with, with training all the gps to understand the meaning of the universe. Absolutely. You know, if they act and crack that mystery among many others, that would be incredible.
01:10:50.482 - 01:10:54.334, Speaker B: I think about that a lot and I think that's a really interesting idea.
01:10:54.714 - 01:11:11.874, Speaker A: Well, you could always add custom instructions to chat Chibi two to make it less polite if you want. So that's always an option. But no, this has been a super, super fun conversation. Guys, thank you so much for joining us. Jules and Trevor and very excited to see what happens with render network otoy and all the great things you guys are working on.
01:11:12.414 - 01:11:13.694, Speaker B: Thank you so much. It's been a pleasure.
