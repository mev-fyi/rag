00:00:00.960 - 00:00:03.370, Speaker A: You're now plugged into the Delphi podcast.
00:00:07.110 - 00:00:42.692, Speaker B: Hey, everyone, welcome back to the podcast. I'm your host Tommy, and I'm thrilled to host a crypto AI versus centralized AI podcast with four of the absolute brightest minds that I could find on the topic. I'm going to let everyone introduce themselves, but we have Ben Fielding, who's the co founder of Jensen. Jensen's building decentralized compute to push the boundaries of machine learning. We have Travis good, who's co founder of Ambient. And they're focused on world scale open source models, and they are a portfolio company. So that'll be team, I'll call them the team crypto AI side.
00:00:42.692 - 00:01:15.832, Speaker B: And then from Delphi, we have Michael Ronco, who's senior markets analyst, who wrote Delphi's first report in AI called the real merge in February 2024. And we have PD, who's an analyst at Delphi Research who's Anonymous. So that's what we'll call him here. And he wrote the tower in the square, which was just an amazing report on centralized versus decentralized AI. Very balanced and very thought provoking. And that's sort of what we're here to discuss, but let's go in that order. Ben, why don't you give your quick intro and we'll go around the room.
00:01:15.896 - 00:01:50.558, Speaker C: Sure, yeah. Thank you for doing this and for having me. So, yeah, I'm Ben, co founder of Jensen. Jensen is a machine learning compute protocol. You can think about us as a protocol layer, a bit like the early web protocols that just sits across machine learning capable hardware and allows it to be used for any machine learning training task in the world. The idea is instead of using AWS or Google Cloud or something as a resource to train your models, you can use any device in the world and you can send your model directly to that device or a. Yeah, kind of subset of the world's devices, but of any size.
00:01:50.558 - 00:02:07.342, Speaker C: It turns machine learning compute into a resource. A bit like electricity, like a commodity. Rather than being this kind of like individually named GPU that you have to kind of rent from somebody and book time on, it just becomes this kind of always available ops kind of trading market, essentially.
00:02:07.406 - 00:02:09.854, Speaker B: It's helpful. Travis, let's go to you next, Travis.
00:02:09.942 - 00:02:32.224, Speaker A: Sure. Yeah, my name's Travis. I'm the co founder of Ambient. In terms of my educational background, I have a PhD in it. Prior to this, I've been focused on AI related applications in industry for the best part of the decade, including some related to drug discovery and critical infrastructure.
00:02:32.392 - 00:02:32.984, Speaker B: Michael?
00:02:33.072 - 00:02:49.166, Speaker D: Hey, guys, I'm Michael. I'm an analyst at Delphi. I primarily focus on markets, but talk a little bit about everything. Like you mentioned, Tommy, I wrote a report on AI a couple months ago now, which was a fun one, and excited to debate with you guys today.
00:02:49.278 - 00:02:51.038, Speaker B: PD, top left, you're up.
00:02:51.134 - 00:03:09.886, Speaker E: Yeah, I'm pondering Durian. So, I'm on the research side on the infra team at Delphi, and background's kind of been investing across consumer Internet, enterprise software and crypto in the US and in APAC. So. Yeah, excited to be here and nice to meet you guys.
00:03:09.998 - 00:03:32.062, Speaker B: So, pondering Durian. Just start us off with maybe a synopsis of the report. Right. One of the most interesting and timeless arguments I think we'll ever have, or debates, is centralized AI, giant web, two companies versus what we're able to do in crypto, AI in the open source world, and through a decentralized manner. So give us a brief synopsis, and we'll dive in from there.
00:03:32.166 - 00:04:05.990, Speaker E: Yeah. So, the title of the report was the tower in the square. And it's basically a nod to a book by Niall Ferguson, which he's a historian out of Stanford, who I like, and he wrote a book that effectively talks about the dynamics between hierarchies and networks over time. Right. And so I think that if you think about the last 30 years, it really has been a story of the rise of networks. So you had globalization, liberalization, capitalism, the Internet, crypto, social media, et cetera. So it really has been kind of about the rise of networks.
00:04:05.990 - 00:04:58.924, Speaker E: But I think over the last five years, you really have seen the tower and traditional hierarchies kind of reassert themselves. So states and corporations. And I think the advent of AI does seem to be a pretty large centralizing force in its current instantiation. And so I think the real question in front of us and that we're here to debate is basically, are we headed for a world where there's a handful of multi trillion parameter models that are governed from small pockets in the west coast of the US that basically underpin large swaths of society? Or are we heading for a world with kind of a tapestry of millions of different types of models of all shapes and sizes? And so I think there's really good arguments to be had on both sides, and that's why we've got some giga brains here to debate it.
00:04:59.012 - 00:05:07.676, Speaker B: Yeah. Ben or Travis, I'm not sure if you want to kick off there. What's sort of your view? Right on, team. Maybe more open source or crypto AI.
00:05:07.868 - 00:05:38.142, Speaker A: Sure, absolutely. And just playing it back a little bit. PD, what I hear you saying is that we've seen kind of massive consolidation, uh, in this space, uh, you know, vertical scale, uh, that's been achieved. Uh, that is giving closed sourced AI kind of a definitive advantage right now. And I tend to agree with that, um, assessment. And then, you know, I. I want to portray the alternatives you pose and just offer a little bit of a quibble with that.
00:05:38.142 - 00:06:03.470, Speaker A: I think you're saying, you know, are we going to just be up against the trillion parameter foundation models that closed source is coming out with, or like a plethora of models? And I think there's another option, which is that open source does its own trillion scale parameter models to sort of fight back in terms of capabilities. But is that roughly correct in terms of the setup that you're talking about?
00:06:03.590 - 00:06:39.460, Speaker E: Yeah, no, I think you've said it well. So, yeah, so far we've seen kind of the vertically integrated offering speed ahead in terms of capabilities, but those moats haven't necessarily proven to be as sticky as the web two era. And so it does seem like it's pretty open. So I'd say closed source players seem to have an edge right now, but we'll see what happens going forward. And I think Dai and open source have a credible playbook to provide alternatives, both at the large scale end and all the way throughout the stack.
00:06:39.650 - 00:07:22.134, Speaker A: Yeah, DVD, perfect. Perfectly agree with that. And I think maybe it's helpful at this point to sort of break down some concepts around what is desirable, normatively, and what is possible. And so, in terms of the desirability of closed source, I'd love to just think out loud a little bit with all of you for a second. And Ben, feel free to chip in the. I have this idea that probably many of you have heard about where we're all cyborgs, we're really computer mediated. We're constantly getting information from our phones, from all sorts of different devices.
00:07:22.134 - 00:08:21.740, Speaker A: We're all augmented humans right now. And I think that AI is going to be, or AGI is going to be our coprocessor in the future. It's going to be something that we're constantly bouncing off of to improve our own capabilities. We might even have direct neural integration of this thing into our minds. And so I think the question you have to ask yourself is, do you want that coprocessor to be trustworthy? And it's really hard for me to come up with an argument that these vertically integrated surveillance capitalists that we're talking about are going to provide a trustworthy solution there. And it seems on an individual level, not even a social level, we can talk about that in a second. Like a really dangerous thing to give companies that have a history of abusing both users and their business customers that kind of power over our minds.
00:08:21.740 - 00:09:01.096, Speaker A: You know, to take it one step further, I would say, you know, we don't accept, we didn't accept the clipper chip, you know, which was the NSA's attempt to backdoor all of our encrypted communications. And I don't think we should attempt, we should accept an attempt on the part of closed source companies to get in between us and the co processor, the mental co processor that is helping us think. So. That's what I would say on an individual level. I'm going to stop there before I talk about what's desirable on a social level to let other people chip in and feel free to debate some of those assertions.
00:09:01.248 - 00:09:33.732, Speaker C: I strongly agree with that view of the future you gave. I think machine learning and AI as an augmentation is the most obvious kind of evolution of it. And I think we've been going through this with technology anyway. You look at how we use the Internet and the generations that have grown up with access to a shared knowledge base. There's some research. I don't have the references to hand, but the way that we memorize information changes. We memorize the ability to retrieve the information via Google rather than memorizing the information itself.
00:09:33.732 - 00:10:39.280, Speaker C: And I think that trend just continues where humans kind of learn around access to this new tool, and so it becomes just kind of interlinked completely. I agree with the sort of like, social premise that we don't want an individual kind of entity to control, that we don't want that individual entity to have censorship capabilities over the thing that is basically within our brains at that point. I do think, though, you can model it just as a incentive system. At the end of the day, those businesses aren't doing something malicious. They're doing something that's fully expected, which is pursuing profit, which is what we expect of businesses, is why the system is designed. But we have to think right now about the small tweaks we can make to those incentive structures as to whether we can still let them evolve organically in the typical way they will, and pursue the incentives, but without accruing that too much power into one place. Because we've seen that with monopolies in the past and the horrible effects they have on the world in certain areas, but this one looks kind of exacerbated when it's like direct control to an extent of what people think.
00:10:39.280 - 00:10:51.600, Speaker C: You could say that's what happened with social media. But then this is the kind of next level of depth again. So essentially just agree with it with all the kind of premises you gave, and it's the same view of the world that we hold for sure.
00:10:52.740 - 00:10:53.396, Speaker A: Go ahead.
00:10:53.508 - 00:12:00.970, Speaker D: Yeah, I was just going to, I guess, provide the other side of the argument for play a little devil's advocate here. So I think one of the challenges with this specific debate is it's never really clear what kind of AI people are talking about. Are we talking about today's AI's? Like chatbots, essentially chat GBT? Are we talking about AGI and however many years, are we talking about artificial super intelligence, the thing that's going to run the world and colonize the galaxy? Right. And I think the trade offs and the incentives and the whole debate changes depending on which kind of bucket you're talking about. I think the easiest one to reason about is today, what's in front of us right now. How do we build the safest possible world today with today's technologies? I think right now, indisputably closed AI is probably the safest way we could manage this technology today and for the foreseeable future. And I'll just lay out a couple of points and then would love to hear kind of you guys react to it.
00:12:00.970 - 00:12:44.482, Speaker D: But I think when you said, like, incentive structures, Ben, that kind of, like, that kind of, you know, set off a little bell in my head. I actually think, like, capitalism and our current, like, economy has really good incentives. Companies get very richly rewarded for building products that are useful and that are safe for people. And if you don't build a useful product or if you hurt people, you don't make a lot of money based off that. You know what I mean? So at a very, like, bottom line, the current incentives, like, seem to work. You know what I mean? The world generates an incredible amount of wealth per year. Inequality has declined.
00:12:44.482 - 00:13:34.480, Speaker D: All these metrics generally show that the world has improved over time. And I think a large part of that is due to capitalism, is due to this system of rewarding people for solving problems in a safe way. And I don't see anything about AI that would make capitalism not work. So I think trusting this technology to the private sector isn't as bad as people make it seem, because all you're doing is just trusting that capitalism does its thing. And a bunch of these mutually distrusting and competing parties in the private sector compete against each other to be the most useful and deliver these technologies in the safest way possible. That solves problems for consumers. And if you kind of believe in that fundamental tenet, then generally speaking, like the best solution in the safest possible way will emerge.
00:13:34.480 - 00:14:15.760, Speaker D: And I think that's like a good future. Whereas if you, to take the other side, if you allow this technology to be maximally open, anybody with a computer and Internet can create and propagate one of these AI's. At some point, probably not today, but at some point in the future, these things will be able to do significant damage and harm. And there are people in the world that will use that for malicious purposes. And I don't know really understand how we stop them from doing that. And I think probably the best way to do that is some sort of like gated access, which the private sector seems to enable.
00:14:17.020 - 00:14:17.880, Speaker A: Go ahead.
00:14:19.380 - 00:15:14.400, Speaker E: Can I just add one point? Because I think the safety argument and the capitalism argument is good, but I also would just throw in that like over time, every generation of consumers has basically opted to choose convenience over the idealistic privacy solution. Right? And so realistically, over the next three, four, five years, what I'm more worried about is, hey, I agree with your social vision. I think your social vision is great, but realistically, every consumer's got an apple iPhone in their pocket. Everyone already uses Google. They have this beautiful integrated Google suite that's going to be integrated with AI. And I'd say it is to me, a little bit too idealistic to think that these coprocessors that are run by big companies won't automatically find their way via the easiest route to being utilized by consumers to be the default solution over time. And I think these products will be really good.
00:15:14.400 - 00:15:31.430, Speaker E: To Michael's point, you might just end up on a slippery slope where because of these natural advantages, users opt for these products because the uses are actually very good, even if it's suboptimal from a societal level. So that's just one thing that I would add.
00:15:32.010 - 00:16:18.412, Speaker C: Yeah, I agree with the kind of progression, especially with users, like an ideological reason to adopt something just doesn't hold in the mass like adoption case. It might hold for a small group of users, but it doesn't spread. My previous startup was in privacy, in consumer privacy specifically, and I very deliberately learned that lesson quite hard. But just to Michael's point, at the end, when you kind of presented the danger of the other route, you started the point with grounding us in the present. But the danger of the other route jumped quite quickly to what could happen in the future. And I think that's what kind of always happens when we try and discuss the dangers of AI. It always comes down to what could it do theoretically in the future? And we kind of have this unbounded space of things that it could do.
00:16:18.412 - 00:17:32.565, Speaker C: But if we think about right now, what are the actual dangers that you could do with the system now that doesn't require this appeal to future capabilities of these models? Because if you do that, it could be anything, and you lose the ability to argue against it because there's literally any possibility. So I posed that as a question, and then just finally, on the kind of capitalism piece solving those negative sides, I'd agree that it can solve that as long as there's auditability. So if there's no auditability, if a company can get away with something negative for the world in pursuit of profit without it ever coming back around and affecting that profit, I assume that as a financially rational actor, that company would do that. So we have to have some level of auditability within the system to allow those things to be discovered. You could take that argument and you could run down a government regulatory kind of route with it, or you could go down the other route of just saying, hey, actually just open development of certain technologies is a better thing for the world. Like we can have the auditability without affecting a company's ability to go and build these models. And that's especially true when the value within the thing itself isn't necessarily how it's built, it's in something else.
00:17:32.565 - 00:17:53.180, Speaker C: And I think we're still figuring out as a world where the value is within machine learning models. I have a very particular view. I think it's in distribution, but ultimately, we've moved around all over the map on where it is. And I think we've kind of come down to it doesn't necessarily sit within the architecture of the model that could be open. There's other areas where the proprietary kind of value sits.
00:17:54.200 - 00:18:53.090, Speaker A: I would like to interject and build on some of your points and also disagree a little bit. So I was taking some notes. Michael, it seems like you first talked about incentive structures and how they've worked out really well. And I think the average consumer would probably disagree with you. Cory Doctorow talks about the great insidification of the Internet, and anyone who's ever used Google knows that it is a terrible user experience compared to what it was for the average consumer. It's actually also a terrible business experience. For a business trying to advertise, the process of insignification occurs when the disintermediating party, in this case Google, which is managing that relationship between, like, the business that's buying ads and the consumer who's viewing ads when they start to take too much of the pie for themselves.
00:18:53.090 - 00:19:53.384, Speaker A: And what we've seen with discovery from lawsuits against Google is that actually they kind of rigged the game in their own favor, like they were driving up the prices of those ads for the businesses they're making it. So it was very hard for users to find meaningful results, and they haven't experienced real consequences for that. Then you have to drill a little bit deeper and ask why that is. And perhaps the answer is because there's inappropriate regulation or a lack of regulation on the company's behavior. Perhaps the answer could be momentum. But what we've ended up with is a situation that is far from capitalist optimal. You haven't got a perfect user experience out of this.
00:19:53.384 - 00:20:37.566, Speaker A: History has actually shown that we've got something like what I would call oligopoly optimal, where the surveillance capitalists have been richly rewarded at the expense of pretty much every other business type you could imagine. For example, I don't think news outlets have been loving how Facebook has been treating them over the years. And so I would argue that all we have to do to know how the future is going to look if we submit ourselves to these big tech companies. What that trajectory looks like is just to look at the past, look at the rise of, you know, teen depression as a result of Facebook's algorithmic manipulations. Look at all these other abuses. But please go ahead.
00:20:37.638 - 00:21:35.262, Speaker D: Yeah, I mean, I think you bring up some good points, Travis, and I'll, I guess, respond to some of them. I don't know, we might just have different views here. I'm very optimistic. I think technology over the long arc of human history has improved people's lives. I think if you pulled someone from 100 years ago and, and drop them into today, they would find the experience of being able to text someone instantly, FaceTime, somebody call a car that arrives directly to their home, get food delivered directly to their home, they'd find that, like, nothing short of, like, pure magic, you know what I mean? So there's all sorts of these side effects that come up with the teen depression and stuff, and that's terrible. I think we should try and solve that and make it better. But that's a small component of the good that this technology has created.
00:21:35.262 - 00:22:07.860, Speaker D: And I think AI will be no different on top of that, I guess, just to respond to something that you said, ben, I think you brought up a great point on the hand. Wavy. Oh, it's going to be dangerous one point in the future. And that's very fair. And I actually think that I fell into that trap earlier. The one thing that makes AI, like, particularly hard for me, and I think many people to reason about, is nobody really understands it. Even these top labs don't understand why the models are doing what they're doing.
00:22:07.860 - 00:22:52.702, Speaker D: And I think when you look at historical examples of technologies that were dual use or dangerous in nature, um, like the atomic bomb, you could like, test the bomb, and like, if it exploded, then, like, that's bad and we shouldn't release that, but if it didn't explode, then it doesn't work, and, like, it's not dangerous, you know what I mean? With these models, it's not clear if, like, um, they work in some cases, but then they can't do other things. So then should we release them in the. You know what I mean? The lines are way blurrier. Um, and to me, it's really difficult for like, a regulatory body or even like, the top experts that are closest to these technologies to figure out, like, safeguards. So, yeah, kind of curious how you guys think about that.
00:22:52.806 - 00:22:56.574, Speaker A: You know, go ahead, Ben. If you want to respond just very.
00:22:56.622 - 00:23:23.720, Speaker C: Quickly, there's a piece in it you mentioned earlier being optimistic, but then the idea that these technologies, we don't know how they will kind of present themselves to the world, so assume a kind of bad case out of them. Like, surely the optimistic stance would be, they're not negative right now. We don't know if there could be bad outcomes, but optimistically, it should be fine. I. So it kind of flips pessimistic when it comes down to the model itself and what will happen in the future.
00:23:24.100 - 00:23:28.156, Speaker B: PD, id love to get your thoughts on the stake just from the other side, too.
00:23:28.308 - 00:24:19.206, Speaker E: Yeah, so I guess kind of back to Travis point. I think theres no mistaking the argument that there is a market concentration that has evolved quite aggressively over the past couple of decades. And so I do think theres this element of, because of the inability for consumers to find what they want, because there's 4 trillion pieces of information coming at them, you need some sort of curation. And so this is effectively what Ben Thompson at Stratetteri has been arguing for the better part of like two decades, that if you aggregate demand online, then supply comes. And so you do have this, like, network effects that do form around these walls gardens, because they're really good at curating. And so these companies have developed kind of data monopolies and cash flow monopolies from these privileged positions that they have developed. Right.
00:24:19.206 - 00:25:22.830, Speaker E: And so this does give them inordinate market power that over time starts to be more extractive than it was in the early growth phases of the network. Right? And so because AI seems like it's a little bit more of a sustaining technology than a disruptive technology, it actually serves to entrenche some of that market power further, unless we have a different solution. And so if you think about what it takes to be a dominant foundation model, generally the inputs right now are talent, data distribution and capital. And I'd say from dominating the web two era and generating those network effects and those users and being the source of curation online, those entities are super well positioned in the new era of AI. And how expensive compute is, the proprietary datasets that they have. And so, yeah, I think from my vantage point, while that might not be socially optimal, that is a reality of the system that we find ourselves in. And I don't think capitalism is bad.
00:25:22.830 - 00:26:01.814, Speaker E: It just seems like there are four or five companies that do have a lot of market power today. And from my position, you look at the S and P 500 with five companies representing 30% of that and only set to increase, it seems like that is an issue. And we are heading for a world where in this next generation of the Internet, instead of the pendulum swinging back towards decentralization, it might actually concentrate further. And so that's just something worth flagging, that structurally, it does seem like there are these tailwinds in the system that we've created behind four or five companies that basically have a lot of market power.
00:26:01.962 - 00:27:38.210, Speaker A: Yeah. And so I'd love to just sort of chip into a few things. The security point keeps getting thrown out there, and I don't want to jump too far into the future, but I do want to point out a couple of things about the threat model as it relates to the current configuration that you're talking about, PD, where we've got four companies that have all the data, and in the future they're going to have agentic chatbots. And I actually think this is an absolute disaster of a threat model. And the reason is that, first of all, all of these companies have been hacked. We have documented evidence of, for example, Microsoft, under repeated audits, failing to predict crucial national security data, getting their hardware enclave keys stolen, which is supposed to be impossible, but it was managed and by concentrating all this data and honestly, these capabilities for models to reach out and touch all the parts of your digital life, you are just making those companies the single most attractive hacking targets for every nation state, for every independent hacker group and so on. And I think if the world was, if that power was more diffused, and if we had credible open source contenders to perform some of these functions, that wouldn't be the case.
00:27:39.750 - 00:27:41.518, Speaker D: Travis, could I interject for 1 second?
00:27:41.574 - 00:27:42.694, Speaker A: Yeah, absolutely.
00:27:42.862 - 00:27:57.180, Speaker D: You mentioned earlier that the current state of tech is kind of like oligopolistic, right? What would be the difference between the data concentration in today's tech world and tomorrow's AI tech world if the same oligopolies continue to persist?
00:27:57.300 - 00:29:14.260, Speaker A: Let me just play that back so I make sure I understand you're saying like, okay, yeah, what's the progression? Yeah, okay, so let's, let's get into that. So let's make it very concrete. So, you know, at this point, chat GPT is integrated into notion, into Evernote, into spreadsheets, into pick your product. It's got controls and tokens that allow it to act on your behalf in a lot of different programs. What that means is the data progression is incredible. You get an incredible amount of personal, private information that accumulates in these providers, and that, combined with the data that is captured via surveillance capitalism, via all the signals that are coming out of your phone, all your cookies that are coming out of your websites and so on, produces a wonderful intelligence dossier on everyone. And so I actually think that that concentration of data accumulating creates a huge national security threat for the country, for the US.
00:29:14.260 - 00:30:03.544, Speaker A: The background check provider for the us government was hacked a few years ago that did all the security clearances, and that was a disaster. But this would be very much equivalent. And we know, and this is where the security argument becomes really funny. We know from a bunch of OpenAI insiders and other insiders that have stepped forward from these tech companies that their security is absolutely terrible. And we also know that the government security is terrible. So, like, if we adopt this idea that this is a healthy paradigm, we are just setting ourselves up for what I would call a very fragile situation. And I'd be happy to talk about this whole thing in terms of notions of fragility and antifragility in a little while.
00:30:03.544 - 00:30:06.076, Speaker A: But that would be my sort of response to that.
00:30:06.108 - 00:30:44.040, Speaker B: Travis, maybe just to dive in here a little bit. I feel like the two sides so far, to me appear one ideological, like the crypto AI side is this is what we want. This is more consumer preference, more privacy, better for the future. The tower side seems more maybe potentially realistic. Right now. They have the most data, they have the most hardware, they have the most talented, it is the most intuitive to use something on your phone that you already have. Like maybe, could we describe for a minute, maybe for Ben or for Travis, like, how do we build an actual competitive alternative?
00:30:44.380 - 00:31:55.198, Speaker A: Totally. Totally. Yeah, I think it's a great question, Tom, and it goes to that kind of what is desirable versus what is possible. So I'm happy to take a crack at it and then also pass it over to you. Uh, Ben. Uh, so, you know, in terms of, uh, what is possible, uh, I think that it's important to note that ML is not a hard scientific field. Like, actually anyone can contribute to ML, and there have been a lot of contributions made that, you know, are you pretty straightforward alterations of previous paradigms? There's a low barrier to entry for ML, which I would say, going back to your question, Tom, bodes well for the future of open source capabilities, because as compute gets cheaper and cheaper, people are going to have the ability to experiment at higher and higher scales with innovations, with ML models.
00:31:55.198 - 00:32:53.200, Speaker A: And so I think you're going to see a potential for the world's talent to mobilize in service of this endeavor. I also think you're going to see a lot of different governments waking up and deciding that they want their country to be competitive in the new economic paradigm. And so they're going to be sponsoring a lot of this. I think you're going to see research institutions all around the world focusing on building capabilities in this area. And so I would say the current state is, we're not really there. We have some models that are sort of in the ballpark of some previous generation models, but I think the future state is actually very bright because you're going to be looking at the collective talent of people all over, not just in San Francisco. They applied to solving a challenge which is tremendously, economically rewarding.
00:32:53.200 - 00:33:00.232, Speaker A: So that would be one comment. I could drill deeper into different aspects of that, but was that responsive to your question? Tommy?
00:33:00.416 - 00:33:08.260, Speaker B: Yeah, no, that's very helpful. Ben, I'm curious, your take be curious attitude. Travis, thoughts?
00:33:08.960 - 00:34:19.950, Speaker C: Yeah, I think the two sides that we presented here do kind of blur quite a bit, because I think both sides are basically saying what we want is high levels of normal capitalist behavior and competition. But the sort of tower model, as it currently progresses, the incentive is on the large model builders to stop that from happening. They don't necessarily want that level of competition, a layer of resources that sit underneath ML that can be controlled to a large degree to stop that competition from happening too much. But like Travis said, ML itself isn't that hard. So, like, the expert knowledge required to do machine learning has already been like, open, like decentralized, disintermediated academic publishing has done that. Like, you can get access to the papers, at least until a certain point when OpenAI kind of started capturing researchers to go and build one of those models. You could actually put the code together to do it, but then you would hit the next roadblock, which would be, can I get the data to train this model? That's, to a large degree, also been kind of opened, because there's enormous amounts of open data on the Internet.
00:34:19.950 - 00:35:08.294, Speaker C: There's lots of things that we put up against with copyright, et cetera, but you can get access to very, very large amounts of data to train these models as an individual. But then you hit the next constraint, which is compute, and compute is one that is much more easy to control. If you have essentially unlimited resources, you can just kind of take the vast majority of this kind of finite resource and you can lock it away somewhere. The fact that the people building models also have the kind of largest cloud providing ecosystems as well means that they're able to control the supply, they're able to control the cost of access to that resource. I think what we, if you look like, path this out into the future, what does ML become? ML becomes this thing that's constrained by electricity. Ultimately, it's conversion of electricity into, uh, kind of knowledge transformation. You need a specific piece of hardware in order to do that.
00:35:08.294 - 00:35:40.604, Speaker C: So that hardware resource is the thing that people can capture. And I think if we let this continue in just the way it does in the oligopolistic way, there becomes an incentive for those companies to control their profits by capturing the resources underneath. And that's the bad thing that we don't like. If it's true, actual competition over value to an end user, sure, let's do it. Let's make it as open competition as we possibly can. But this isn't necessarily true on open competition. It's capture of a resource and rent seeking on top of that resource, which I think we probably all agree isn't a good outcome of capitalism.
00:35:40.604 - 00:36:18.766, Speaker C: You don't want just pure rent seeking, you want actual value creation. I think naturally it progresses in that way anyway through usage. Like OpenAI thought that they could capture enormous amounts of compute, funnel capital into that, make a model that would then be their moat, and then they could just kind of print money off that while everybody tried to catch up. But the fact that model can be distilled through its usage completely undermines that, which is an amazing just property of models. You just can't sort of sit on it and rent it out. It will just erode in value. So then it becomes, well, how else do I continue to create that moat? One way is regulatory capture.
00:36:18.766 - 00:36:56.476, Speaker C: Another way is create an ecosystem of people building on top of it. That's the way that we like. It's the way of saying, okay, let's get more people building, more people competing with each other on the actual kind of front ends to this technology. And then what you get is, what's the real value of ML? The real value of ML is right down to the user. Are you providing me something that actually makes my life richer than I want to pay for? And I think, I guess it gets driven down and down and down until we're competing over the best front ends, the best access to these models, the best interfaces to the parameter space that's being created. Behind that can be a completely open ecosystem, which would have richer competition. And the outcome that I think we're all actually saying we want here.
00:36:56.476 - 00:37:00.712, Speaker C: And it's just a slight tweak to the way we do it to stop that resource capture happening.
00:37:00.896 - 00:38:06.100, Speaker A: Yeah, and I just for the art of the possible. First of all, I think what you said was absolutely brilliant, Ben, so thank you for making a lot of points more clearly than I could have. Secondly, I would say, in terms of the art of the possible, I would make this historical commentary. I think that the massive scale up that we're seeing in AI compute data centers is a little bit of an accidental gold rush. In other words, if chat GPT had never been released, then we might never have seen this focus on using a particular paradigm for model training to scale up. And now that this has become the paradigm, the big players are all locked into that, and it's a very easy model for them to get into, because capital in, product out, that's kind of how it works. The training methodologies and the secret sauce are relatively straightforward, but the question is what capitalistic incentives start to exist to counteract that.
00:38:06.100 - 00:39:41.252, Speaker A: And I think that obviously, forms of distributed training or efficient training, training that can leverage the tale of compute and so on, is going to come to the fore because it's just too economically valuable. And so as long as we can prevent some of the worst case scenarios that Ben talked about where regulation locks down all compute everywhere. We can expect that there are going to be open source, consumer friendly solutions for things like distributed training that are going to pop up. And just to name a few of them, we've seen huge advances in back propagation free training show up just in the last couple of months. There's an example of a technique that entirely eliminates matrix multiplication and radically decreases the need for data sharing that was published recently. There are all sorts of other papers that are getting around the limitations, the bandwidth limitations imposed by the current paradigm, and so it's not there yet. But I think within one year, my estimate would be that we will see three compelling alternatives to the big data center paradigm that will give a little bit more oomph to the efforts on the open source side.
00:39:41.396 - 00:41:04.940, Speaker D: I think you guys both bring up really compelling points. And I guess just to put a pin through it, one of the big questions for me right now is, are the scaling laws true? Are you long or are you short? The scaling laws, you know what I mean? And just to lay out both sides of it, if you're long the scaling laws, I think, as you both said, then you're going to amass as much compute as you can and just basically hope that scaling up these models yields increasingly capable models. If you're short the scaling laws, then at some point you think that the relationship between compute and loss breaks or at least levels off. And you believe that these models can be distilled down, or you believe that maybe the current deep learning paradigm is just wrong, and AGI will come out of something that doesn't quite look like this current flavor of models. I don't really have a strong view there, but to me there's two distinct camps here. It seems like OpenAI and anthropic. So I think it was the GBT three paper that came out that popularized the scaling laws.
00:41:04.940 - 00:41:35.748, Speaker D: And ever since then, OpenAI, anthropic. And then obviously Google have been amassing as much compute as possible. They're the big proponents behind scale is all you need. That will get us to AGI. Then it seems like there's this other camp, which I think maybe many people in the crypto AI space fall into. But also, interestingly, a lot of the top researchers at Meta have basically said that LLMs are not going to get us to AGI. We need a new approach.
00:41:35.748 - 00:42:01.330, Speaker D: You can't just memorize the whole Internet and expect general intelligence to spawn out of that. Do you guys have a view on that? Because that question seems really important. If it is all scale and compute, then I don't know how you beat the big guys, you know what I mean? So it feels like we're all kind of implicitly betting that there's something else that you can't just get from scaling up these models, because then you're in an unwinnable resource game.
00:42:01.910 - 00:42:45.778, Speaker C: I don't think the scale piece necessarily points to only the centralized approach. I think that's the bit where that takes a different turn, because you think about scale. What does that mean? Can I make a model over a certain number of devices, create more devices and make a model over that? And we are currently in a brute force scaling paradigm. I think it's throw compute at it in the most centralized, easy to model way, and we get a quick win to show what's possible. And it's kind of like the tip of the spear of machine learning, where it says, hey, the world didn't realize something on the order of chat GPT was actually possible. Now it does. But what comes after that? What's the wave of things that happen once you've proven that? Well, you then go and improve those things, it becomes no longer a research problem.
00:42:45.778 - 00:43:20.954, Speaker C: It's like an execution engineering performance optimization problem. So there's a huge wave behind that of this commoditization of those models, shrinking them, quantizing them, compressing them, making them sparse, all of these things to make them actually quicker and easier and faster to run. So to have that real proof, hey, the world, this is possible, you have to have that big brute force amount of compute, but then everything afterwards, all the products that get built can be built in a much different way, in a smaller way. That brute force approach itself, though, is starting to hit a wall. So previously I used to say the.
00:43:21.042 - 00:43:23.790, Speaker D: Why do you think it's hitting a wall, if you don't mind me asking?
00:43:25.170 - 00:44:25.780, Speaker C: Access to data center space to put enough devices into, to have them interconnected at a scale that is larger than what we currently have. I used to say that was down to funding, so you'd have to have more and more money to do that. I said that to somebody very senior at meta, and he laughed and said, basically we have unlimited funding. It's not that, it's the fact that there aren't that many places geographically on the planet where you can build data centers that can sustain this level of interconnected devices, which leads the hyperscalers down this route of actually doing horizontal scaling. So you see a wave of papers now that say, hey, actually, let's do multinode, multi data center modeling model that out in your head, where does that go? Well, Google can do it over three data centers, but then what if somebody can get devices in four data centers, five data centers, etcetera? Ultimately, the maximum scale of that is connecting up all of the world's devices, and that has to go across companies. So a single company competing against every other company is not going to win. So if you can build a layer that allows that to happen, that is going to be the ultimate scaling win.
00:44:26.360 - 00:44:44.270, Speaker D: I see. I just want to make sure I understand this point, because I think it's an important one. You're saying that we can't make the data centers any bigger than they are right now, or we can't build more data centers in general because there's not enough electricity. What's the fundamental constraint?
00:44:44.570 - 00:45:28.468, Speaker C: We can't. There's diminishing returns to doing it. So finding and securing the location that can sustain the level of electricity required, the cooling required, et cetera, the local regulations of noise. To actually have something that can hold enough of these devices in one place is becoming much harder. It's getting harder and harder and harder and harder. So the hyperscaler fighting over the remaining areas on the planet to actually build something at some point that ends like there is a logical endpoint to that. If you can, however, incentivize people to build those who could sit anywhere on the planet, not just finding a single data center, incentivize people to host them anywhere, then you can connect those up and you can create a larger cluster.
00:45:28.468 - 00:45:50.670, Speaker C: There's some downsides to that. So, like, the communication speed between those kind of smaller but more distributed clusters is worse. But that's not necessarily just a breaking change. It's just another thing for us to build around. It's an engineering execution problem, which we've kind of got lots of history of the Internet, of overcoming. We can overcome latency problems. We can do lots of distributed systems work to solve those things.
00:45:50.670 - 00:46:05.430, Speaker C: Our Jensen's view is very bullish on scaling, but scaling to the max, rather than scaling in this competitive way between hyperscalers, which we think just reaches kind of limitation eventually that we can't get past.
00:46:05.850 - 00:46:52.906, Speaker A: I would say that there are two answers to this, and you're giving one of them, Ben. And then I have a particular view, which you may not share. So I think one answer, essentially, that Michael is coming across is that actually the big players are going to have to come over to a more distributed paradigm in order to scale, and that's actually going to slow them down, because they're in the wrong paradigm right now. The paradigm they're in now is so expensive and so unattainable at the hyper and unsustainable, not to mention at the hyper scale, that they have to shift at some point. And that shift, given all the investment that they've made in centralization, is incredibly costly for them. And I think there's a moment where open source can catch up with that. So that's one point.
00:46:52.906 - 00:47:55.588, Speaker A: But then I think you're also asking like a slightly different question, which is assume that we got rid of that problem and we could just hyperscale to infinity. Is there actually a plateau for model capabilities? And I think that this is where we start to get into discussions of alignment. And so just to ground this discussion very slightly, I mentioned at the beginning that I worked in critical infrastructure for some time, and so I'm pretty familiar with what you have to do there to ensure safety. And the gold standard for safety in safety critical applications is formally verified systems. In other words, you have to mathematically prove that a system behaves in a certain way under certain stimuli. That's a very difficult thing to do. So now let's just think about applying that to large language models.
00:47:55.588 - 00:48:52.066, Speaker A: The current state, essentially in a normal application, any API call you make is something that could be formally verified. You would have to do a huge amount of work to formally verify that API call, because that's a route through the logic of the system. But large language models essentially have unlimited API calls because everything that you say to it creates a functionally different path through the model. And that problem actually gets worse when you start adding in large context, because all of a sudden the context can shift the path even further. I actually don't think that scaling is going to drop off. I think that our ability to successfully manage these models and deploy them in a responsible way, it's going to drop off far before the performance of the model drops off.
00:48:52.218 - 00:49:00.150, Speaker D: And aren't we already there though? Like, we don't, they don't, we don't understand why the model outputs what it does today.
00:49:00.610 - 00:49:48.106, Speaker C: I think we've gotten up against that quite a lot with technology, right? Like we're just moving to a probabilistic world. Like self driving cars is a great example of it, where we know that they are statistically, statistically safer than a human driver, but because they are probabilistic, we just have this kind of like social blocker and accepting that they will be safer. So we say, well, we have to formally prove that they are definitely going to make the safe choice in a certain scenario, we can't look at the kind of statistics and say, well, actually on average, over the long time period, they will be safer. I think from my perspective, humanity just kind of has to come to terms with moving away from like imperative, like formally verified logic, from machines to, they're probabilistic. But in aggregate, we assume that the behavior is optimistic for humanity.
00:49:48.298 - 00:50:40.420, Speaker A: I think that's a very rational argument, Ben. I just want to make one comment, which is that that sort of works, until if you accept the claims of like, the ilias of the world, you hit a point where the AGI could do extraordinarily dangerous things, and one mistake could be catastrophic for civilization. The question is, if I want to deploy a system and there's a 30% chance because I have this unlimited API attack surface, that that system is going to do something catastrophically bad for civilization, can I apply your model, Ben, to deploy it? And my answer would be, absolutely not. And then you're not going to deploy it. And then the whole curve flattens out for a bit while everyone tries to do fundamental research to figure out how to actually do super alignment. Right. That's kind of my mental model of it.
00:50:41.000 - 00:51:33.310, Speaker C: Why does that appeal to some kind of huge negative outcome that could happen, though? It requires that, like, there's been no kind of examples given that something that a model could do right now, the examples are always, well, what if a model could do this in the future? And we can always have that, right? That's the point. With the probabilistic model, you could always say that that could be an outcome. I think that's the point. We have to come to terms with the fact that there will be potentially very high impact, negative outcomes, but very low probability on them, which as humans, we kind of deal with that anyway. We're like probabilistic beings. When we interact with the world, it's just we move to be much more imperative with technology. I guess the point is that kind of impact goes beyond us as individuals, whereas humans, in kind of like a previous life, we would just be thinking probabilistically with respect to ourselves, rather than like the entirety of humanity in some way.
00:51:33.310 - 00:51:36.350, Speaker C: But still, I feel like it's a shift back.
00:51:36.890 - 00:51:45.730, Speaker E: I feel like this is going to spiral pretty quickly into AGI, like super alignment. So I guess just bringing it back to kind of closed versus open and.
00:51:45.770 - 00:51:48.470, Speaker A: Oh, I could, I can pull it back there if you want.
00:51:48.970 - 00:51:52.330, Speaker B: Thanks for bringing this back, BD. I like how you keep doing it. It's helpful.
00:51:52.410 - 00:51:52.722, Speaker A: No.
00:51:52.786 - 00:52:19.088, Speaker E: Well I guess my point is I kind of feel like there's this reflection flexive loop between the capital markets and the hyperscalers right now, right where hey, they're getting a lot of cheap funding. They obviously have 400 billion on their balance sheet. They throw off 1.5% of GDP and cash flows. They're projected to spend a trillion dollars in capex. So they do have a lot of firepower to pursue these scaling laws that Michael was talking about. Right.
00:52:19.088 - 00:53:45.692, Speaker E: And I think the big question is, hey, does the revenue come to justify continued investments? Theres a question between the moats that each generation of model brings and the value capture that they can get. And can they onboard the next $100 billion in revenue from corporates and startups that basically use their models and are underpinned by their models because if not then people will stop funding them and basically Google and Amazon and Facebook will all get slapped quite aggressively and so they wont be able to build those $100 billion clusters. So I dont know if theres enough space. I think basically the financial constraints are what matter here frankly. And so do the returns justify a trillion dollars in capex spend for centralized models? And I think that is a question around what the ultimate modes are for, AiH, in terms of large foundational models and then also how quickly some of these distributed systems can catch up or these distributed trainings can catch up in order, and then also how much value capture is there at the very, very high end versus just what gets commoditized. So id love to get Ben and Travis thoughts on those questions because to me that seems like the algorithm were dealing with the returns to these massive upfront investments that theyre making and how far that can take them kind of to this scaling question.
00:53:45.756 - 00:54:39.952, Speaker A: So I can hit that quickly. I would say its a snowball model. Ive expressed all of you before, or at least most of you, that I view AI as an economic displacement technology. Essentially, the more capable AI models become, the more economic work they displace and the more valuable they become. And back to your point PD. The question is, is there a moment where it looks implausible that that snowball is going to continue building where people chicken out and say hey, we dont want to dump any more capex in this thing? Weve kind of hit that point of diminishing returns. And I actually believe that that moment is perhaps when we get into real alignment problems like were talking around earlier and it seems like they might be insoluble because we actually dont have a good theory of how machine learning.
00:54:39.952 - 00:55:20.060, Speaker A: So that would be one answer, and then I would just make one comment regarding open and closed source things. I actually think that the economic value of AI is enhanced in a world where safety research is done openly. And the reason for that is because that decreases the risk of anyone being unaware of the state of the art of safety research of countries and institutions not putting in the right protections and so on. Having that information as widely disseminated as possible makes a better economic environment for everyone.
00:55:20.480 - 00:56:11.526, Speaker D: So can we compare what you just said to the race for the nuclear bomb? Would you have wanted that fundamental research to be open source, widely distributed? Because there was a case back then, which I think even some of the top scientists at the time believed, that if everyone knew what the state of the art was, then it would be like kind of this mutually assured destruction. We'd all have the same thing, we'd all be pointing our guns at each other at the same time, and that would yield the safest world, whereas the other side of the camp was, we need to build this first. We need to get to the finish line first we point our big guns at you, and then we have this, like checkmate, no matter what you do. And I actually feel like that's probably the best. It's not perfect, obviously, but that's a decent fractal for where we find ourselves in this AGI race right now.
00:56:11.678 - 00:56:53.812, Speaker A: I'm so glad you asked that, because I actually think it's the absolute worst analogy that could possibly be used. So let me break it down. First of all, we need to look historically what did happen. So what happened is that we thought that we had this route to military superiority and ascendancy. And the reality was it almost leaked immediately. So we might have had that aspiration to completely shift the balance of power, but reality intruded on that and actually reestablished the balance of power. So I would ask anyone who's pursuing that if they actually think that that is a realistic goal.
00:56:53.812 - 00:58:22.762, Speaker A: The second thing I would say is that the nature of ML research is so fundamentally different from physics research that it causes the comparison to completely break down. And to be very specific about that, I would say there are two paradigms that you could look at. There's an institutional paradigm where you have a small coterie of qualified scientists who can be controlled by the state. And there's a network linkage paradigm where you have an extremely wide distributed base of knowledge with a relatively low skill floor that's required to get into the field and tons of people around the world are qualified if you only give them access to compute. And so I would say from that network analysis standpoint, the idea that we could even impose an institutional model on ML is completely laughable. And what that would lead you to do, if you were to follow that policy, would basically be to lock all, let's say, American ML research down for three years to the great detriment of the public and its access to the economic benefits of that, and then have it stolen four years later by a foreign government, or maybe even more quickly than that. And so you would do a lot of securitization for absolutely no benefit.
00:58:22.762 - 00:58:33.826, Speaker A: And I could go into the antidemocratic aspects of what you'd have to do to switch the model from sort of an institutional to network linkage. But, yeah, I think that's a really.
00:58:33.898 - 00:59:28.906, Speaker C: Bad plan, just looking at that nuclear example, right? We ended up at the mutually assured destruction state, right. And you kind of posit the scenario that doing that in an open way would have been somehow disadvantageous, even if it would have resulted in mutually assured destruction. But I think that depends who you are in that scenario as to whether it was, whether it appeared in a good way or not. But if you think, well, if it had been open, maybe we could have achieved mutually assured destruction without use of the technology. Did that need to happen? Did that kind of, like, proof point have to present itself to the world as the superior force? Or would we have potentially had two forces who have that, knowing that if they use it, the other will? And you skip ahead to the mutually assured destruction scenario, rather than having to have the kind of in between, which is pretty damaging for the world, to be honest.
00:59:29.018 - 01:00:15.510, Speaker D: Yeah, I think you guys both bring up good points, and I agree. I think there's, I mean, people debate this all the time, but, like, my view is that nuclear weapons, the advent of, like, nuclear weapons are, is directly responsible for, like, this period of long peace. We've avoided a great war since World War Two, in large part because you could blow up an entire country now. And I actually think that's, in a weird way, been like, very useful. Um, it's been like a useful, like, trump card, um, that now multiple people have to your guys points. I don't. But so you could say, okay, great, if everybody has AGI, then, like, that's, that'll yield the safest world.
01:00:15.510 - 01:00:49.594, Speaker D: I don't actually think that's a lesson, because if we just go back to nuclear weapons again, if you extend that logic out, like, would you want every country to have a nuke. You know, you want Hamas running around with a nuke right now. You want the Taliban and Afghanistan running around with nuke. There's been many precarious times in history where it was up to, like, literally one dude, like the guy, I'm forgetting the story, but there was a guy on a russian submarine, right, that thought the world war three had broken out. And it was up to him whether he launched the nuke at the US. He decided not to. You know what I mean? And if that was, that's one guy.
01:00:49.594 - 01:01:19.920, Speaker D: But if you expand that to, like, many people have the ability to launch that nuke, that the odds go up that someone presses that button. You know what I mean? So I think, like one of the SA, one of the saving graces of, like, nuclear weapons is that they're really hard to build. And I agree with your point, Travis. Like, these models are not that hard to build, you know what I mean? So I think it breaks down a little bit. But to me, you still want to limit these super powerful, potentially destructive technologies to a smaller amount of people in general. Yeah.
01:01:20.620 - 01:01:23.316, Speaker A: So I would just say, go ahead.
01:01:23.508 - 01:01:55.546, Speaker E: No, I was just going to say, or you just make sure that you kind of stay ahead. Right. And whether that's like, you know, so I guess, like one of the ways that, I don't know, I don't, maybe this is going to be a spiral into a different area. That's unhelpful. But yeah, I guess I think about it in terms of just like military capability or cyber capability. Like, there's always going to be potential attack vectors. But if you stay one step ahead, then, like Hamas shot all those rockets, but Israel has the Iron Dome.
01:01:55.546 - 01:02:44.138, Speaker E: Like, you kind of have this threat model where the threats are constantly evolving, but the defense is also constantly evolving. And also I think there's hopefully a correlation between the desire to do those things and general wealth of the planet, right? So I think the more affluent people get, like, generally the less appetite there is for war or for destruction between countries, right. So I think affluent democracies have never actually had a war between themselves. Right. And so some of that's ideological, but some of that's also affluence. Right. And so if you let the benefits of ML or artificial intelligence start to percolate through the global economy and that is managed effectively in a way that also doesn't accrue just to Microsoft shareholders, then I think you can actually have a really good outcome.
01:02:44.138 - 01:03:10.936, Speaker E: But it is a very difficult needle to threat. And so I agree there's tons of threats, and it could go totally wrong. But I also think generally locking it down and basically just saying, hey, like, you know, certain state actors are the only ones that are going to have this. Hey, maybe you want them to be one step ahead of the rest. Um, but I generally think, like, letting it proliferate is probably the optimal outcome in a well managed way.
01:03:11.088 - 01:03:32.726, Speaker C: So just sounds like we're all on that. On that, yeah, actually view. Right. Because, like, I guess in that view, you say, well, the most optimal, um, thing for a state to do in that case is just invest heavily in this, like, alongside everybody else. Like, the. The state is then pushed to say, okay, we have to stay ahead. We don't do it by, like, pulling everybody else back.
01:03:32.726 - 01:03:49.210, Speaker C: We just stay ahead. Which is typically what, like, defense research is in general. Right? Like, it's. It's like fund tech. These technologies fund effort into them, uh, put large amounts of, um, kind of capital behind, exploring whatever the kind of next frontier is. And I think that's. That's generally good.
01:03:49.510 - 01:03:54.810, Speaker D: How does that work now? How does that work now when commercial is so far ahead of the government, though?
01:03:55.210 - 01:04:22.222, Speaker A: So what is it? So. Okay, well, I just want to jump in, in here and say that there are two things that are going on, and I think we need to disentangle them. So, for me, and this is why I always get a bit worked up when we talk about AGI and military terms. For me, AGI is predominantly an economic paradigm. AGI is labor substitution. It has some military applications, but the.
01:04:22.246 - 01:04:32.046, Speaker D: Military is the biggest company in the world. They employ more people than any company in the world. So there's an obvious dual use there now.
01:04:32.238 - 01:05:30.560, Speaker A: Well, I agree that there is a dual use, but I think you have to, whenever you look at closing something like this down, you have to look at the hard power aspects of it and the soft power aspects of it. And in service of that, I would just like to play out a little scenario for you and let you think about what the effects of that would be. So I view AGI tech as kind of a soft power thing. It's an economic power thing. But let's say we treated it like hard power. We completely locked down AGI, and we denied all the economic benefits of AGI to everyone except the military, essentially. So what are you actually doing there? You've lost your ability to deploy that in the economy, and you might.
01:05:30.560 - 01:06:25.258, Speaker A: Militaries are powered by their economies. Reality is, our ability to project military force is dependent on our economic capabilities. If we're so deeply in debt or we're so far behind other countries in terms of our ability to use agentic protocols to achieve efficiencies that will result in growth and profit that can be reinvested, then we've actually lost any war that we could possibly imagine, because someone is just the person who is the country that is using that capability economically is just going to win. And when you set it up like that, when you look at both sides of it, it's hard not to see how a country that did both and investigated the military applications and also deployed in the wider economy wouldn't end up winning.
01:06:25.434 - 01:07:02.580, Speaker D: Yeah, I think I agree with that. In conclusion, I mean, I maybe have a slightly different view on what AI or AGI you might have said is to me, we're just building intelligence here, and intelligence is the input to everything. So if you have unlimited intelligence, it's like electricity. You don't really know what you're going to do with it. It's a force in the world. I think to argue that AI won't be dual use is crazy. I don't think anyone's doing that.
01:07:02.580 - 01:07:42.948, Speaker D: But I think it will definitely be dual use. Governments will be super interested in it. Already are. And I think to loop it all the way back to your original question, PD, about whether or not the revenues can catch up to the expenses of these models. To me, I take a slightly different angle there. I think the more important thing to watch is whether or not we see a plateau in the capabilities. As long as these frontier labs, as long as OpenAI can every year keep pumping out a more and more advanced model and the hype can continue to build one, I think the revenues will catch up.
01:07:42.948 - 01:08:12.016, Speaker D: We're already seeing that the information leaked in article. I think they're now at two or $3 billion in revenue over just the last twelve or so months, which is an unprecedented Runway. But at some point governments are going to step in here and start to fund these labs directly because it will become a national security implication, is going to be looking at revenue. They're going to be looking at the capability of the model because they're viewing it from a national security perspective.
01:08:12.168 - 01:08:43.946, Speaker E: But that's a different argument. We're just talking about market forces. If you think the government's going to step in, then I feel like the whole table basically changes and the capital markets dynamic is effectively thrown out the window. You can tax whatever users you want and funnel it into a military application. But I guess my point is it's not about the capabilities. It's about the capabilities versus the free or the cheaper option. So it's about the value capture of these new capabilities versus all of these other things behind it.
01:08:43.946 - 01:09:24.178, Speaker E: And so am I going to keep giving OpenAI and Microsoft 100 billion, a trillion dollars to pursue the next generation of models if they're not actually capturing any value? If I'm an investor, I'm not going to do that. To your point, they have to basically keep jumping two years ahead, three years ahead, in order to provide an underlying intelligence to all of these companies that are going to be buying their services. If they don't have a lead to justify 100 billion, 200 billion, 400 billion, a trillion dollars in capex spend, then investors are not going to do that. That's my point. It's not about the capabilities, it's about the capabilities relative to everybody else and the cost that it takes to get to those capabilities.
01:09:24.374 - 01:09:26.870, Speaker A: So let me get you, sorry.
01:09:27.730 - 01:10:22.132, Speaker C: I think that moat point is like the kind of big elephant in the room for a lot of the large AI companies, right? We've watched OpenAI try and figure out what their moat is, and they're seemingly still trying to figure it out. Is it this ecosystem, is it chat? GPT? Most likely not. Is it their ability to build an ecosystem? Potentially. I think that's where we see a mechanism just jumping way back in the conversation, a mechanism that actually pushes the strategic angle for a profit making company towards open, which is what meta are doing. Meta recognize that their moat is their distribution to users of these models, is their ability to apply them to an actual real world thing, rather than just trying to make money from the model itself. So they're driven to open source that model for an actual strategic gain rather than for altruism or something like that. And it's them commoditizing the complements of their product.
01:10:22.132 - 01:11:11.500, Speaker C: They make their model a commodity because they're the only people who have the kind of distribution over social right now, which pushes the actual technology itself out into the open, but allows meta to continue to capture things. I think what we see happening is just more of that kind of drive to real distribution being the thing that matters. OpenAI will try and provide the software to like use intelligence against that distribution, but very quickly, every company that has the distribution will then want to disintermediate OpenAI. And if there's no real kind of tangible moat within their technology, that's what they'll be driven to do. The more money they make, the more they're going to try and bring that in house. And I think there's not much seemingly stopping that from happening other than capturing the resources underneath, like the compute or the regulatory angle.
01:11:11.920 - 01:11:37.540, Speaker E: So, Ben, I do feel like you said something earlier that I thought I agreed with and was very true, but it was basically like, hey, this is effectively a distribution game. And the unfortunate reality is these five or six companies have already won in distribution. Right. And so we are effectively thinking how we can provide an infrastructure layer. But if you think distribution is the ultimate moat, then does that mean that you're ultimately kind of agreeing with me?
01:11:37.620 - 01:12:10.118, Speaker A: I have an answer for this one, actually, if you don't mind, Ben, please. I think we have to introduce a model of social friction here. You have to think about what's going to happen when OpenAI makes its play. So let's take medical coders as an example. So medical coders are the people who listen to these transcriptions by doctors. They look at paperwork and then they figure out which billing codes are appropriate for insurance. It's a pretty tedious job, but you can make a lot of money doing it depending on your throughput.
01:12:10.118 - 01:12:42.590, Speaker A: It's like piecework. Imagine that OpenAI goes in and all of a sudden rolls up the entire field of medical coding. It's plausible. They have really good models. It's a boring job. A model with infinite attention is probably going to be able to do a lot better than a person on this. Now you have to imagine what's going to happen socially when an entire profession disappears.
01:12:42.590 - 01:13:20.850, Speaker A: And I think the answer is that people are going to be really hostile to these companies and it's going to hurt their distribution. I think that people will be looking for alternatives because everyone is going to be looking at the moat for their own business and they're going to start to do this calculation that, hey, if I give all my data and workflows to open AI like that, could my business model, could be their business model tomorrow. It's the old Amazon play where they have house brands that supplant the most successful brands that show up in their marketplace.
01:13:22.070 - 01:13:32.770, Speaker E: But Travis, everyone still uses Amazon, everyone still uses Facebook, everyone still uses Google. That's the problem. I agree with you. But then the consumers don't care because Amazon gives them the cheapest product.
01:13:33.770 - 01:13:42.430, Speaker C: Is your argument that the innovators dilemma is dead as a concept then because it says that these companies will just continue to hold that lead forever?
01:13:43.530 - 01:14:05.196, Speaker E: No, it's just not obvious to me how the system changes. It does go back to that. Clayton Christensen, sustaining technology versus disruptive technology. Right now, from what I can see, it does seem like AI is just making existing companies products better and their distribution better and medicine enhance their products. And Google can enhance their products.
01:14:05.268 - 01:14:05.500, Speaker D: Right.
01:14:05.540 - 01:14:57.950, Speaker A: So let me hit that from a different angle Mabley so, and let me talk about it in sort of the terms that I described. So imagine that companies right now are very inefficient because they were configured to organize humans. But the future companies of the world are going to be configured agentically. They're going to be configured to organize a small group of people and a large group of agents. And so there's an argument here that, and credit to Goodalexander on Twitter for this, there's an argument here that the big players are actually fundamentally obsolete if enabling AI technology is given to smaller, more nimble businesses who can organize correctly for the new paradigm. I think that's where you go with this in terms of.
01:14:58.730 - 01:15:49.820, Speaker E: So I totally agree, and I think that a very interesting paradigm to go down. But is it the fact that those players that end up being way more nimble end up winning? And I think you very much could be the case, or do basically large companies with existing cash flows and distributions just start slimming down their workforce quite aggressively? And so maybe that is the innovator's dilemma, where doing that slimming down or shrinking your workforce, like Klarna has seen, is just untenable in the way that you've organized your organization. And so new players come up and you just have this mesh of creativity and agents and people. But realistically, I also think you're going to start running into some social constraints, as you mentioned, in terms of where the value is getting captured and how that's getting allocated. But that's, you know, a slightly different discussion.
01:15:50.280 - 01:16:48.216, Speaker D: Yeah, I think this is a really interesting little riff we're on right now. And I think it's important to think through, like who can keep this flywheel going the most, who can keep acquiring money and then putting it into compute and then paying the top researchers and kind of spinning this flywheel, who can keep it going? Who can spin it the fastest and keep it going the longest? And I have a couple of thoughts here. Again, curious to hear your guys takes on this. To me, Ben, I thought the meta example was super interesting, and I think meta is actually probably the best example of somebody that's right in the crosshairs of what you were saying earlier, PD, about revenue mattering, because from my perspective, what meta is doing, they're spending a bunch of money to train these models and then giving it away for free. They're saying, take what we spent millions of dollars on and here, go ahead and use it. We won't charge you a single dime. I think at some point, if you hit the API, you know, x amount of times, then they charge you.
01:16:48.216 - 01:16:55.272, Speaker D: But at first it's totally free. Right. They're not making any money off that right now. Zero. You know what I mean?
01:16:55.416 - 01:17:03.840, Speaker E: But Michael, it's not a charity. They're doing that because they don't want to be dependent on Google and Apple, like, Mark has already been burned by, like, ATT. He doesn't.
01:17:03.880 - 01:17:11.970, Speaker D: I totally get it. I get the beta. But it's a big bet because they haven't made any money off of it yet. So the bet is eventually.
01:17:12.130 - 01:17:16.510, Speaker C: But you're making money through an API. It's applied within Meta's products.
01:17:16.810 - 01:17:17.594, Speaker E: Yeah.
01:17:17.762 - 01:17:19.146, Speaker D: How are they making money off of it?
01:17:19.178 - 01:17:30.554, Speaker E: But, Michael, you're making my point that the model is effectively getting commoditized. Right. If Meta is going to spend tens of billions of dollars, then what does that do to OpenAI's business model?
01:17:30.682 - 01:17:46.076, Speaker D: I think so. Right. That's my central point is I think they're playing different games. I agree. Meta is giving this away. OpenAI is trying to build the leading closed source frontier model. So Meta is giving away their model.
01:17:46.076 - 01:18:21.630, Speaker D: OpenAI can go to a drug discovery lab and say, hey, we'll run a month long inference for you guys to synthesize some drug. And we have the number one model in the world with the best reasoning capabilities, x, Y and Z, the most agentic. Whatever the flavor of the week is, that drug discovery company will. They'll pay them whatever OpenAI requests for that. That's a totally different kind of model than giving it away and making up for it on the back end by selling ads on an existing social media platform.
01:18:21.790 - 01:18:28.552, Speaker E: Yeah, I agree, but it depends on OpenAI being able to retain the lead against meta, their players.
01:18:28.616 - 01:18:28.816, Speaker C: Right?
01:18:28.848 - 01:18:28.992, Speaker D: Yeah.
01:18:29.016 - 01:18:51.504, Speaker E: So if meta also spends $20 billion on their next generation, and they hire a good number of researchers from OpenAI and they put it out there for free, then anything that OpenAI wanted to charge effectively gets thrown out the window. Right? So, yeah, I'm not saying OpenAI can't be a great business. I'm just saying they do have a commodification risk and it does depend on this.
01:18:51.592 - 01:19:01.704, Speaker D: But do they, like, they trained GPT four, two years ago and it's still the number one model in the world. So to me, it looks like they.
01:19:01.712 - 01:19:31.516, Speaker A: Have a two in it. I want to quibble with that. So GPT four is a little bit of an illusion because there have been many, many releases of GPT four. Actually, current state models are beating the original release of GPT four. And this is the problem. You get into closed source models, the companies claim that they're the same model, but they are very much not the same model, and they can be changed without notice. So if you're depending on those models to be the same, in for a rough ride, right? Like totally.
01:19:31.548 - 01:20:00.774, Speaker D: So. So to be, to be clear, my understanding, and to your point, it's not 100% clear, my understanding is GBD four finished pre training, so finished reading everything on the Internet basically two years ago. And then since then they've been iteratively post training it, which has improved performance. But to me it's amazing that the base model was fully cooked two full years ago and it's still competitive with today's state of the art models. That suggests that they're ahead.
01:20:00.902 - 01:20:03.350, Speaker E: Yeah, so I would agree with that.
01:20:03.390 - 01:20:03.710, Speaker C: Right.
01:20:03.790 - 01:20:47.170, Speaker E: I guess my point is more that there are no natural moats aside from talent and basically access to compute. So in a web two era you can benefit from these network effects. And even if you're Twitter and you have the worst execution in the world, you continue to get users and have your power law, dynamics. But the world you're describing will require OpenAI to continue to invest super aggressively in order to maintain that lead, both in terms of their talent and in terms of their access to compute, which is why they've been forced to go to Microsoft and do deals with Apple and things like that. I think they can stay ahead. I just think it's a lot harder than in a web two era where you had some of these natural modes.
01:20:47.480 - 01:21:36.702, Speaker C: I kind of feel like that gets driven down to the specific use case because we talked about distribution being the moat here. But distribution is kind of it's access to users, but it's also specific use cases. We talk about OpenAI, hey, they can take GPT four and they can apply it to drug discovery, but somebody, people have been applying machine learning to drug discovery for years and the specific companies that are doing that and this idea that GPT four is going to outperform all of these specific companies designing models specifically for drug discovery I just think is unrealistic. It shows, it's a really good marketing tool. It means that OpenAI will probably get some contracts to do it. But then you look at isomorphic labs, demis specific attempt to do this. That's probably more likely to go directly after the drug discovery use cases than OpenAI, who are seemingly going after every potential use case.
01:21:36.702 - 01:21:57.322, Speaker C: They're going to be spread too thin. So where in the end, do they end up settling? I think they end up settling with this ecosystem of apps. Maybe that's valuable enough to warrant the kind of investment they've put into it, but maybe it's not. But I think saying that they can kind of go after potentially any market is disingenuous because they're just going to get out competed by people going after those markets specifically.
01:21:57.506 - 01:22:32.660, Speaker D: Yeah, I think that's a fascinating question. Do you guys have a view on that, how artificial intelligence will evolve going forward? Will the most general form of intelligence win? Or will we see, like, to your point, Ben, like, smaller, more focused, and use case specific models, winning at those specific use cases in the general models being pretty good, and then the specific models being really good at specific things. Do you have a view on.
01:22:33.280 - 01:23:17.004, Speaker C: I have a pretty cynical view. I think at the end of the day, this is just a technology that could be applied in specific areas, like a data access tool. You can think about models as being just an incredibly good way of compressing information, which is very effective in loads of kind of arenas. And it can have logic be emerging out of it in the sort of language models and sort of general intelligence pieces we put on it. To me, that progresses to the general intelligence part is a routing mechanism underneath it. You just have this better way of interacting with the data of the world, and I think you can apply that in many, many different ways. I don't think you have to have that, like a generally intelligent routing mechanism in order to achieve, like to apply it to every use case.
01:23:17.004 - 01:23:53.910, Speaker C: I think you can have different pieces for different places, similarly to how just technology kind of in the past could be applied to many different things in many different ways. But I think we've gone down this, like, focusing point where people assume you have to have a foundation model with a kind of, like, human like brain to achieve any kind of good outcome. I just don't think that's the case. You look back in ML's history and you just apply ML in different arenas in different ways. I think we've kind of, like, for a short period of time, focused in on, like, transformers as this thing. But I expect that to branch back out again once we see that they don't just magically solve every actual use.
01:23:53.950 - 01:24:35.076, Speaker A: Case, you know, I guess I have a nuanced view on this. So I agree with what a lot of, a lot of what you said, Benjen. I also think it's all about tool use. So I think that a very good foundation model with exceptional tool use skills is going to win everything. And the reason for that is that it can apply all the specialist models in service of larger goals. And so I think it sort of subsumes them that way. But that's not to disagree with any of the sub point Ben makes.
01:24:35.076 - 01:24:50.640, Speaker A: I agree that specialist models probably are going to have an edge for a lot of these use cases because you simply can't train or even fine tune for some specific cases. GPT five is not going to be alpha fold.
01:24:51.060 - 01:25:43.978, Speaker B: Hey guys, just a yemenite, a quick question. So were pretty far into the conversation right now, and we are at this point debating OpenAI versus meta. And I dont think we very clearly pointed out the breaking points or the impetus for why crypto AI can win here, or potentially win. Id like to maybe just spend like ten or 15 minutes there. We got close on the electricity and the real estate argument, like maybe it's the fact that we run out of space on data centers, or there's not enough electric, or to pondering Durian's point, like there's just the funding doesn't match out with revenue and expected outputs, and then that puts a nail in the coffin for centralized AI. But I'd love to walk through how crypto AI and that side could potentially win. And if it can't win, that's totally fine as well.
01:25:43.978 - 01:25:51.268, Speaker B: If that's your opinion, I'd like to just spend some time there. Maybe PD, we could start with you and go maybe around the room.
01:25:51.404 - 01:27:31.786, Speaker E: Yeah, sorry I cut off briefly, but yeah, so I guess going back to our discussion earlier, I feel there are basically disruption tends to happen from the fringe, and there are different capabilities that I think are on offer in decentralized AI that closed can never replicate. So maybe in terms of performance today, they have really strong performance using this vertically integrated scaling approach, and they do have proprietary data and they have access to a lot of compute because of their monopoly cash flows from the first generation of the Internet. But I would say crypto basically provides a more open, transparent layer on which everyone else can build. And so if you value that transparency, that composability, and that potential ecosystem of apps where you can verify the underlying model, you can verify the datasets, then I think you have an opportunity where over time, as the performance of decentralized compute and decentralized inference starts to gain ground, then you could have more and more folks want to tap into this ecosystem because of these additional capabilities that are layered on top. Right. And so I think it is like the classic disruption framework where, hey, you have this one solution that's like spearheading ahead in one direction. I think we've talked about the scaling laws and some of the potential limits there, but hey, there are also these other advantages that are being used by fringe customers today because of their unique properties that are likely to gain in performance over time.
01:27:31.786 - 01:27:43.064, Speaker E: And if the performance gets to a degree that's comparable, then it would offer a superior solution. So I guess that's my brief summary, but curious on Ben and Travis's takes as well.
01:27:43.192 - 01:28:37.156, Speaker C: Yeah, I guess to kind of answer the question directly from my side, this is going to be a very biased answer as well, given what we build at Jensen. But broadly, I think the greatest power from decentralization and decentralized technologies is this ability to build value flows that are very hard to intermediate and Renseecon and Captcha. So we've talked a lot about the resources that sit underneath machine learning, and we think about them as kind of like a few pillars of these resources. Each one of those has different kind of dynamics as to whether it's open already or whether it could be captured by a large company and then kind of disallowed from anybody else being able to access it. Crypto gives us this ability to create markets over those things which stay credibly open through the mechanisms of crypto. So obviously, think about computer law. That's what we build.
01:28:37.156 - 01:29:32.274, Speaker C: But if you think about the future of that market, if we can create a way to allow demand to flow from any actual user of that resource directly to the person who owns and provides that resource out, then we can create a much more highly competitive, more liquid market over that resource, which eventually trends in the kind of. Of the way bitcoin trended to proxy energy markets over the planet. In a way, you do the same thing with compute as a resource underneath AI. You get driven to. If I have an electricity source, which happens to be particularly cheap, even lots of green electricities are particularly cheap as well, because there's no way of harnessing them. If I can then buy a GPU, plug that in here, and get instant access to the demand side of GPU compute, I'll do that because I'll earn a yield on it. Right now, I just can't do that, because the only way for me to do that is to become a cloud provider.
01:29:32.274 - 01:30:08.390, Speaker C: The cloud providers themselves aren't incentivized for me to be able to become a cloud provider. The loop continues and then they end up buying up all of the compute and rent seeking on top of it and setting oligopolistic prices. So it just gives us this way to keep that market liquid and not allow it to be captured by a centralized provider. I think you can do that for all of the resources underneath ML, and in the end you end up with more competition at the actual application stage, which is what I think we've all agreed here, is probably where the moat actually sits. It's like providing some value to a user rather than within one of the resources.
01:30:09.010 - 01:30:56.862, Speaker B: Ben, just real quick, before Travis and Mike answers one quick follow up question there, it sounds like from your point of view, and correct me if I'm wrong here, is that the funding for hyperscale data centers and the revenue or the products that they build is, is somewhat unsustainable, and we're not getting the results out there that we want. And if we're able to like build this meta network connecting all of these GPU's, aka, I guess, electricity, at the end of the day, anybody can access that to create like the largest surface area, to create an AI app or agent that is the most valuable versus a centralized company, like picking and choosing their trial runs and competing on just full blown AGI. Is that like a fair approximation?
01:30:57.046 - 01:31:50.700, Speaker C: To an extent, yeah. I think when it gets into the size piece of like whether a scale is the thing that wins, like you can. We've obviously already kind of talked about that here that gets a bit kind of into the realms of the debate. But ultimately you can think about Oviz, what is the minimally way for humanity to use the resources that we have? So use electricity, and the conversion of electricity into kind of knowledge through GPU's, what's the way that we can do that that doesn't have as like many diversions off it, where somebody's like extracting value along the way. And I think crypto and decentralization gives us the rails to create a system that is absolutely minimally extractive on top of that. So the only places people are extracting value, or because they're providing value. So you think, say somebody creates an interface to a model, if that interface is competing with lots of other interfaces, that person can make a profit, but their profit is kind of being in competition with everybody else's.
01:31:50.700 - 01:32:34.412, Speaker C: It drives it down to a fair market price. That happens at multiple stages along that chain, all the way down to your accessing electricity. Basically, and at the moment we're in this situation where because compute is this finite resource and is this commodity was not a commodity, it's a finite resource that's being captured by providers, they're able to take an enormous portion of the value that flows back down to it. But crypto gives us that kind of rails where that doesn't happen. I think that's just net good for the world in whichever way AI goes. It doesn't matter if scaling continues to be the best thing, because just general usage of the resource is still the most efficient way it can possibly. And that's because there is no kind of profit making incentive for a protocol.
01:32:34.412 - 01:32:46.172, Speaker C: It just needs to sustain itself. So if it can sustain itself and can create the largest surface of demand usage of that supply, then it should win out against anybody who does have to make a profit over it.
01:32:46.316 - 01:33:57.180, Speaker A: So I'm a big fan of Nassim Taleb and I think that centralized structures are fragile, and the reason that they're fragile is they require trust. If I'm a medium sized business and I'm using chat GPT to provide customer service capabilities to my customers, OpenAI can switch something about the behavior of that model tomorrow and the entire behavior of my customer service department will change. That's a huge amount of control you have to give up as a user of the application. Heaven forbid that I get on the wrong side of OpenAI and then they could shut down my entire customer service department. There's a lot of trust I have to put in that entity that it's not going to be hacked, it's not going to behave maliciously, it's not going to behave arbitrarily. We've seen with Google that they will arbitrarily shut people's Gmail accounts down for no reason. That's explainable, and they will lose literally years of their memories and life to that kind of situation.
01:33:57.180 - 01:35:25.560, Speaker A: And so I think that what decentralized structures enable is robustness and antifragility, because they can be trustless. If I can reach out, and instead of calling to OpenAI, have a really good model that is provided, say by ambient network, that has known properties, that was trained in the open, that behaves in a predictable manner, I can trust that that model is going to continue to run on the network and it can't be arbitrarily disrupted by the Wimbledigh of centralized structures. As long as theres an economic incentive for that model to run, and if its a good model there will be, it will continue to run and so that provides a better floor under me as a business that provides a better guarantee for me. And thats where I think crypto AI has a lot to offer. Its all about being able to reach out and grab interesting capabilities and not have to worry about the specific organizational implementations and structures that surround those capabilities. And that composability, I think is hugely beneficial and is really the future of where the whole economy is going.
01:35:25.860 - 01:36:39.400, Speaker D: Yeah, I think you guys both said some really interesting things there. So my take on this, and I won't comment on any specific use case or verticals because you guys are obviously the experts there, but my view on crypto AI is when I wrote my report, I tried to think about what problems does crypto solve for AI that cannot be solved anywhere else? And I think you guys both hit on several of them. For me, I identified three crypto is trustless. So if you're maybe just a dumb example would be if you're an AI agent, are you going to put your capital, are you going to trust JPMorgan Chase? Or are you going to trust a wallet, a digital wallet that only you control, a public private key on a blockchain so you don't have to trust anybody in crypto? I think that's an attractive property actually for humans. It can be like a scary property that you dont have someone to call on the phone. My bet is that for AI, thats going to be enormously attractive. So cryptos trustless, cryptos, deterministic.
01:36:39.400 - 01:37:19.762, Speaker D: When you execute a piece of code in crypto, you know exactly what that code is going to do. Theres no ambiguity in that. Whereas in meatspace, when I call my bank to make a wire, I don't know if the wire is going to happen today, tomorrow, next week, or anytime this month. I think we've probably all dealt with that meat space is not deterministic, it's stochastic. Humans are random. My bet is that AI is not going to like that. It's going to want deterministic execution of everything crypto offers that the third piece that I think that crypto can provide to AI is like hyper capitalism.
01:37:19.762 - 01:37:53.920, Speaker D: And honestly, I think that the dumbest but perhaps best example of this is meme coins. Crypto can financialize anything, will and can financialize everything and anything. And I think that's like a unique property that AI can plug into and basically start to accumulate resources. So I think for those reasons, I'm incredibly bullish on the overlap between AI and crypto. And I think we're just at the very early stages of discovering where the value will accrue.
01:37:55.020 - 01:39:23.390, Speaker E: I think those are great points, Michael, really well said. I would also say that one of the things that I tried to outline in my piece is there have been constraints to protocol adoption because of the existing institutions that we have are basically made in an industrial era, right? And so if you actually have capabilities and much more of the economy move towards agents or agent based networks, then a lot of the infrastructure that we're building starts to make a lot more sense, right? There's just today so much friction for enterprises interacting with protocols and like large legal departments that are always going to be much more conservative and much more comfortable with an AWS, SLA or contract because that's just how the world works and that's what they're comfortable with. But in the event that you actually start outsourcing more and more decisions, vision, machine intelligence, then I think those decision makers will start to use much more composable, trustless infrastructure to the points that have been made, right? And so in the event that over the next three to five years, we start to witness that transition, then I think that's actually super bullish for web3 infrastructure and applications and all of those things, because it really does take both sides of the network and we are just only starting to see the demand side really come to fruition in a way that makes sense for the web3 infrastructure that's been laid over the last five, six years.
01:39:23.470 - 01:39:58.596, Speaker C: I think there's a really interesting analogy there between web3 and AI in general, where both basically just replace human world costs with like code costs. Essentially like web3 came along and said, okay, replace legal contracts and like human world trust mechanisms, SLA's and all of those things with just like execution of a smart contract. Amazing. Like that's such a cheaper way in terms of human effort of doing things. And then like you said, AI kind of does exactly the same thing. It replaces these weird expensive human effort things with code. Essentially, it's much more complex code.
01:39:58.596 - 01:41:09.286, Speaker C: So the confluence of the two is just the maximum replacement of expensive human effort things with code. And then you say, well, what's left at the end of it is that trust piece where people say, in the human world, I do actually just value the human trust piece. I just want to pay extra for that. And then that's the bit that's lit with enterprises, with the old kind of like, why does anyone buy IBM? You end up with the same situation here where it's like, why am I buying from somebody like, aws in the end and it is just because Im willing to pay extra for the human world bit and Im willingly doing that but its been carved out and kind of hyper financialized to the point where I know Im paying for that now. Its not this weird obfuscation like it is in the kind of traditional cloud world right now. Its this very distinct purchase and I think theres an angle we could take a week on that goes way further out, which is what happens to the future of AI. It's just more explicit financialization of that human experience where it says that we are kind of like value shifts to the point where we are willing to pay more for explicit human experience, potentially irrationally against what would be the most optimal thing for us.
01:41:09.286 - 01:41:38.934, Speaker C: But just because we actually value that, I'm like, I want to buy this handcrafted thing because it's handcrafted by a person. I want to buy this cloud provider product because there's a person there. Even though actually this crypto product over here with automatic settlement and AI running it is actually just generally better for me. It's cheaper, it's more effective, it's more efficient, it leads to a better outcome. But I'm going to choose this other thing because I'm human and that's what I want to buy.
01:41:39.102 - 01:42:44.252, Speaker D: Ben, can I ask you a question? Because I think that's really interesting insight there. One of the bull cases I feel like for decentralized compute or take something from the current AI field and decentralize it is basically we can do it cheaper is the selling point. But then I think the counter to that is just what you laid out. It's like, well, cost isn't always the primary concern for these guys, especially when you're talking about the most valuable companies in the world. They'll spend a couple extra mil to just not deal with the headache, you know what I mean? Or if there's an existing relationship there, is there a tipping point where they could save so much money that the cost actually does become the primary driver? Or do these crypto AI companies, is it just a reputation thing? And it's going to take a couple of years for these GPU providers to build a reputation and inroads with these companies. How do you see that playing out? And is there an obvious tipping point where the competitive dynamic shift, centralized versus decentralized offerings?
01:42:44.396 - 01:43:38.172, Speaker C: I think there's the classic friction versus benefit trade off that you have with any new technology where you say, hey, this is going to be different to how you solve this problem. Are you willing to go through the friction of adopting this thing for the potential benefits, and we have to go through that kind of flip. Like some people have to be willing to go through extra friction in order to adopt this thing for those cost savings or increased scale or increased access or whichever one of the kind of multiple benefits you see of crypto as a technology applied to this. Once you've gone through that friction initially, then you drive that friction down to down the better way of doing something without a huge switching cost. But yeah, I think we haven't really seen anyone actually conquer that tradeoff. We've seen lots of speculative crypto AI things. We've seen lots of people say, hey, here are the huge benefits that exist in the future, and there's lots of people that are betting on that.
01:43:38.172 - 01:44:19.506, Speaker C: But I think it's quite obvious that nobody's actually captured real world value demand here. There isn't very many real world use cases of crypto AI technologies that somebody can point to and say, this is driving actual value into web two just as a technology. And I think that's what, what needs to happen. And to be honest, I think it just needs a focus on building those technologies up and making them better for specific actual use cases rather than what I think we've had. Unfortunately, within the space, which is a focus on incentivization and bootstrapping and things, which is just a distraction in our view. Sure, at some point you're going to need to do a little bit of incentivization. If you're building a marketplace, you have to do a little bit of bootstrapping, but it's not the main effort.
01:44:19.506 - 01:44:35.142, Speaker C: The main effort is creating like an actually valuable product. And I think we've had a bit of a period of distraction against that. But behind the scenes, a lot of people are just building those products. Once those get released, we'll see the kind of surge. But yeah, it just has to cross that barrier of friction versus cost.
01:44:35.326 - 01:45:05.610, Speaker E: But I would argue that you do need the network liquidity. One of the things crypto is effectively about networks. And so you do need the demand side and the supply side, and they need to come together at the same time. And in order to create that experience, its always super difficult to get the marketplace going out of the gate. Right. And so the supply side is not going to ramp up until it sees the demand side and were still have a very heavily high friction onboarding experience for most users. Right.
01:45:05.610 - 01:45:30.146, Speaker E: And so I think I agree, hey, focusing on product is super important, but also a lot of the benefits can only come when theres like five, nothing, not 5 billion, but like a tipping point of crypto wallets in terms of the supply going to meet that demand. Right. Because right now, like the users today are quite limited in terms of daus, I feel so. I do feel like network liquidity is quite important, actually.
01:45:30.298 - 01:46:10.780, Speaker A: Just go ahead. No, I just wanted to interject that. I think that's a really important point. PD and where we end up if we put crypto with AI, is actually programmable economics with decision support. And I think that for crypto previously, the decision support has been inherently lacking. Large language models are capable of ingesting a lot of information and making relatively straightforward choices based on all those inputs. And that something you just have to think is previously impossible.
01:46:10.780 - 01:46:47.458, Speaker A: If I tried to write a contract on Ethereum to parse a news story and decide whether the sentiment was positive or negative, I have to do a lot of hackery, let alone trying to write a summary of that. That would be a consistent and plausible representation of it that would allow me to make an economic decision. So we're actually introducing a level of capability and reasoning that I think makes the underlying economic APIs much more attractive than they previously were.
01:46:47.554 - 01:47:36.480, Speaker C: Just on the bootstrapping of the marketplace that we talked about. It's true that you will at some point need to do some level of bootstrapping. I'm not saying that you don't need to kind of make sure that there's a balance between supply and demand, but I think your use case makes a huge difference as to how much of that you need to do. If you think about the case of helium, where you're providing network coverage for people to use, that's subject to Metcalfe's law from telecoms, where the network only really has value at massive scale, which means you have to really heavily bootstrap the supply side before you actually provide any value to somebody. There are other use cases that just aren't subject to that. Like you think about access to a GPU. If I have a single GPU and a single person who wants to train a model on one gpu, I can have like a transaction.
01:47:36.480 - 01:48:19.908, Speaker C: Somebody can get real world value from this network. Yeah, it's not at the maximum scale it'll be, but you can grow that iteratively rather than having to go through this like maximum boots bootstrapping process. I think the weird situation we're in right now with GPU networks, for example, is everybody's competing of a number of GPU's. No one's actually using those GPU's, where's the demand side usage? So we go down this rabbit hole of getting as many GPU's collected as we can. It's the wrong metric. The right metric is getting actual usage, getting people sending tasks to devices and getting their results back. And if that happens on a smaller scale initially, I think that's actually better than getting one of the sides massively bootstrapped and then you can't serve the other one.
01:48:19.908 - 01:48:47.860, Speaker C: It only really works in the situation where you have supply of a commodity with a linear value scale as the network scales. But luckily for us, ML compute happens to be one of those things. I think if you are in that network bandwidth mode, you just have to figure out how to do the supply side bootstrapping. That's just the game you have to play. You've got to make sure your demand side takes over in the short window. You have to be able to subsidize supply side, otherwise you're not going to win.
01:48:48.200 - 01:48:57.616, Speaker D: Where do you see demand coming from in the nearest term for compute networks? Yeah, decentralized compute networks.
01:48:57.808 - 01:49:47.012, Speaker C: Yeah. I think there's a few different areas where it can come from. I think the c to series a kind of startups who don't have access to cloud providers right now are one vector. Individual users who don't have local compute, but want to fine tune a model, for example. They want to do it slightly more with the typically open source models that they have access to, but they can't then get access to the compute to do it, and then further kind of along, but potentially not too far along, is collaborative like training of models. So can the community come together, propose a new kind of type of model, but not get access to compute as a resource? Well, now they can get access to computer as a resource. They also have lots of tools that come with web3 in general on pooling capital in order to activate that resource together without having to trust a single party, et cetera.
01:49:47.012 - 01:50:31.910, Speaker C: I think we just end up with much more collaborative development of models on top of compute networks and data networks, and to an extent, expert knowledge incentivization networks, which are, can I propose this model and then be rewarded in the future for being the person that proposed it? Because there's a kind of point on chain on set as I kind of created it and put it out into the world. So it just like more and more as we get value attribution back to the originator of the resource or the kind of thing, I think we incentivize that collaborative work and then that's when the flywheel really starts. But yeah, before then you can have just simple usage that you would see right now with cloud providers that's typically either priced out or just can't access straight up. Can't access at all in some cases.
01:50:32.330 - 01:50:52.990, Speaker A: I think that as you see agentic capabilities being required for blockchains to be considered competitive, that you're going to see a ton of utilization of these sorts of networks.
01:50:53.410 - 01:51:27.426, Speaker C: I agree with that. When models start needing access to the resources to trigger themselves and other models, that's when this thing goes completely exponential because you remove that same way as we talked about in loads of other areas like the human world cost, it's gone. You need incredibly liquid access to that resource at that point. Otherwise it's always going to be limited essentially by the efficiency of the resource market underneath it. So we have to make those as efficient as possible if we want to progress as fast as possible. But yeah, totally agree. The whole thing changes once models are actually accessing the resources, which we find incredibly exciting.
01:51:27.426 - 01:51:31.190, Speaker C: I think that's going to be pretty big moment here, here.
01:51:31.730 - 01:52:25.710, Speaker D: I'm glad you brought that up, Travis. That was something that I was hoping we would hit on at some point. I didn't want to be, it feels a little Sci-Fi right now, but I think people underestimate how quickly that will happen. I saw a tweet the other day, it was a OpenAI researcher and he basically made the point, said something along the lines of it's crazy that or imagine how amazed a person would be ten years ago if you pulled them to today and said we have a program, a technology that can do real time language translation between any language in real time instantly here for free. Anybody can use it around the world for free. So thats a thing today. But nothings changed.
01:52:25.710 - 01:53:01.900, Speaker D: The world is still the same world from before that. He said it much more eloquently than I did. But the point was we had this massive breakthrough and nothing seems to have changed. You know what I mean? The price of Nvidia went up a lot. But other than that, wed still eat breakfast every day, im still drinking coffee. You know what I mean? I think that for crypto as well, weve had these massive AI breakthroughs and crypto is still chugging along. Bitcoin's not even at an all time high right now, you know what I mean? So like when is this point? When is like the huge tipping point? When is the exponential? My current view is, I think it's whenever we get agents.
01:53:01.900 - 01:53:40.790, Speaker D: Because for me, and I think for most people in the crypto community, it's obvious why agents would prefer a blockchain over traditional infrastructure. And I think it's really hard to. It's hard for me to wrap my mind around, like, the idea that like, from day zero to day one, like there could be zero agents and then there could be like an unbounded amount, millions, billions of mini instantiations of intelligence that are engaging in economic activity totally isolated from each other. And I think a lot of that is going to happen on blockchain. And just the potential is like incredible there.
01:53:41.410 - 01:54:24.072, Speaker E: I do feel like it's like that USB chart where you have the infrastructure and the apps and the infrastructure and the apps. And I thought that like that one point by the altimeter guy, that basically, if you look at cloud, there's 400 billion that accrues to applications, and there's like 50 billion that accrued to semiconductor players, right? And right now we've effectively had the inverse in terms of Genai, where you have 75 billion that's accrued to Nvidia effectively, and 5 billion that's accrued to OpenAI, and those things are going to flip, right? And so if it's anything like cloud, you would expect a coming application explosion and most of the value capture to start leaking up the stack, right? So I 100% agree with that.
01:54:24.256 - 01:55:00.740, Speaker A: I think that that's why this ecosystem is so important and why it's actually why I'm very happy to see a lot of different approaches and healthy competition in the space, because I think that there's just going to be so much demand for all of the above that it's going to be a feast for everyone. And I think the people who see it are going to be richly rewarded. And I hope for those listening that you're persuaded by some of our arguments, because I'd like you to benefit financially from this too, guys.
01:55:00.780 - 01:55:31.326, Speaker B: I think that's a phenomenal point to end. I don't think you guys realize how hard it was to just not say anything elicit. You guys are just so smart and I learned so much. I want to thank all of you for coming on and having a phenomenal conversation, especially for over 2 hours. Incredible and timeless, and you're all part of incredible projects, especially the Delphi boys. But I can't wait to get this out and really appreciate it. And if anyone listening is starting a crypto AI project, feel free to dm me or anyone here.
01:55:31.326 - 01:55:35.356, Speaker B: We're happy to dd you and potentially fund you. Thank you, guys.
01:55:35.518 - 01:55:36.096, Speaker D: This was fun.
01:55:36.128 - 01:55:38.880, Speaker A: Thank you so much. Thanks. Really appreciate your time.
01:55:39.040 - 01:55:41.160, Speaker C: Really enjoyed this. Thanks, everyone. Thanks, everyone. Cheers. Cheers.
