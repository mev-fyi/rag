00:00:01.400 - 00:00:03.725, Speaker A: You're now plugged into the Delphi Podcast.
00:00:06.425 - 00:00:50.755, Speaker B: Welcome to Delphi Podcast. I'm John from Delphi Ventures, and today with me I have Dr. John Fletcher, the CEO and co founder of De Innovation Game. De Innovation Game. TIG is a decentralized science project which frankly, is one of the most uniquely innovative projects I've seen in crypto in recent years. In one sentence, I can describe TIG as a coordination protocol designed to accelerate algorithmic innovation. The TIG network, through useful proof of work, discovers the most efficient algorithm for a given scientific problem and continuously works to improve it.
00:00:50.755 - 00:01:01.547, Speaker B: Before we start, I'd like to disclose that Delphi Ventures is an investor and holder of TIG tokens. John, welcome. It's great to have you here.
00:01:01.731 - 00:01:04.091, Speaker A: Thanks very much. Good to be here, John.
00:01:04.163 - 00:01:19.187, Speaker B: Given how big of a project TIG is, there's a lot to unpack in this episode. Why don't we start with a high level introductory overview of what TIG is? In your own words, briefly describe us what TIG is and what is its purpose.
00:01:19.371 - 00:02:17.265, Speaker A: Yeah, sure. So TIG stands for the Innovation Game, and its purpose is to accelerate open algorithmic innovation. So these algorithms can be anything from an algorithm which turns measurements of a magnetic field into an image on an MRI machine to an algorithm which minimizes a cost function in an AI training run. Algorithms are really everywhere. And the key insight that we had is that the open development of these algorithms is not really sort of as successful as, say, the most successful open source projects. They really have a fundamentally different structure and it's in fact been known and kind of, you know, recognized for quite a long time. You know what the issue is with the projects, kind of open coding projects, which have a sort of a structure which is not ideal for traditional open source, is that sort of additional incentives are required.
00:02:17.265 - 00:02:59.495, Speaker A: So we provide those by sort of capturing value by capturing the IP embodied in the algorithms. But really the sort of key part is that these algorithms are used to perform proof of work in the innovation game. So proof of work is a proof of work protocol, sort of in many senses, much like Bitcoin. But the problems that we solve are randomly generated instances. So synthetic problems which are solved by these algorithms. So they're synthetic instances of sort of real life problems. And then the which algorithm should be rewarded is market determined in the sense that the one which is mostly adopted by the proof of work miners will be the most efficient.
00:02:59.495 - 00:03:05.535, Speaker A: And that is our sort of market signal as to how to sort of rate the algorithms and pit them against one another.
00:03:06.035 - 00:03:28.755, Speaker B: Perfect. Thank you very much for the intro. There's definitely like components that I want to dive deeper here, but I'm actually first curious on, like, it's a truly kind of unique project and I'm curious to hear the origin story of tig. How did you come up with this idea and since when you've been working on it?
00:03:29.455 - 00:03:58.809, Speaker A: Yeah, great question. I should go and check this back in my archives. But I think I first had the idea in 2012, so quite a long time ago. And sort of back then I was a PhD student, in fact, in the maths department in Cambridge. And I was lucky enough to have an office which was just opposite Stephen Hawking's office. So I hear kind of like not his door, but the window to his office, so I can sort of see in. And one of the things about Professor Hawking is that you wouldn't really know that he worked very long hours.
00:03:58.809 - 00:04:52.217, Speaker A: He kind of got there quite early, stayed there quite late. And, you know, I felt like I didn't really have much of an excuse to leave early if someone in his, you know, with his kind of constraints could work such long hours. It's like, so can everybody else. So I kind of ruminate on various things whilst, you know, trying to be inspired by this inspiring figure. And one of the things that was talked about, and I knew some of his PhD students, I didn't work in the same field myself, is that he had a vast amount of funding to get PhD students and postdocs and people to work on his program of research. And this wasn't surprising really, because he was a world famous physicist and obviously did all this whilst overcoming the sort of motor neurone disease which he had, which is sort of mind boggling achievement. But also he was incredibly rare, very much the exception.
00:04:52.217 - 00:05:59.075, Speaker A: It was known that most scientists have to spend an extraordinary amount of time not doing science, really just sort of writing grant proposals, often to try and get government grants or grants maybe from charitable foundations to do fundamental research. Yet the research they do is known to be incredibly valuable. So I started really thinking about the reason for this. And there's also that kind of thing which taps away in my mind that people say, I'm sure in sort of not in very good faith, they say, you know, if you're so smart, why aren't you rich? Right? It's a kind of like this kind of classic thing that people say to, you know, scientists or kind of boffins and. But in the context of science, you know, being rich doesn't mean personal wealth, it really means having, you know, funding for your program of research. So I just thought, you know, it's just a problem like any other problem, right. If we are so smart, why can we sort of think about how this activity, which generates such enormous value for society and just monetary value, all of these spinoffs from fundamental research, integrated chips and proton beam therapy for cancer and all that kind of stuff, you should be able to funnel that value back.
00:05:59.075 - 00:07:09.003, Speaker A: I was thinking about this problem and I recently learned about Bitcoin, this was in 2012. And I realized that the problem I was working on, which was to try to sense the geometry, actually the fractal dimension of a fractal surface by looking at the way that it scatters light. This has a particular structure which is that it's computationally very hard to solve. Once you have the solution, anyone can check very quickly that you did indeed have the right solution. So this sort of asymmetric property, it's hard to solve, easy to verify, is the same thing I figured is the sort of fundamental essential property of proof of work in the context of cryptocurrencies. You know, everybody beavers away trying to find a new solution and then we find it had to broadcast it to the network, everybody verifies and they move on to the next solution. So I thought, okay, if something like this was put into Bitcoin rather than hash pre image search, just as a thought experiment, then not only would you have the well known incentive to find faster machines for mining, find cheaper sources of electricity and so on, you would also have an extremely strong incentive to develop the algorithm so that it could solve the problems more efficiently.
00:07:09.003 - 00:07:30.065, Speaker A: Because that would be. Miners want to get a competitive advantage. So this was really the sort of origin of Is this really possible? Why hasn't this been done before? We're all kind of good questions. And that took quite a long time to figure out the details of that. That's why 12 years later we are up and running. But it took a while.
00:07:30.605 - 00:07:42.075, Speaker B: Very interesting. I want to double click on the problem that you were trying to solve. That's kind of interesting to hear. Could you maybe briefly explain what it was?
00:07:42.415 - 00:08:27.799, Speaker A: Yeah, right. So you have sort of many surfaces and kind of materials in nature have a sort of fractal property, at least to some extent. So sometimes when you zoom in very close, they sort of stop becoming fractals or maybe when you sort of zoom out enough, but over a kind of a large range of scales, they are sort of fractals. And that one kind of intuitive Way to think about this is, yeah, sometimes if you see a picture of some rocks, but you can't really tell whether you're flying over it in a plane and it's mountain range or whether you have a microscope and you're looking at it. And these rocks are actually a few millimeters tall, but they have the same sort of spiky structure. So it's a sort of self similar property. And it turns out that that property is really imparted to the waves.
00:08:27.799 - 00:09:38.179, Speaker A: When kind of waves or electromagnetic radiation, just like light scatters off them, it gives the light scattering or the fluctuations in intensity a sort of fractal property itself. And this can be used to great effect to sort of estimate things like the fractal dimension or the sort of volatility of the surface very accurately very quickly. Because you're sort of leveraging this sort of mathematical property of a fractal. And there are lots of situations where you want to know just kind of get estimates of properties such as roughness or kind of roughness at certain scales because often it helps you correct errors. So say they do a lot of sort of altimetry where they want to sort of bounce a signal off, say, the kind of ocean, for example, and see what kind of height the ocean is as the swells go up and down, or maybe off sea ice in the Arctic. But things like the kind of roughness of the surface change how much radiation makes it back to the receiver. And also the kind of in the sea ice sense, the differential between roughness of the snow on the top and the kind of frozen seawater below can also cause an error in that measurement.
00:09:38.179 - 00:10:06.535, Speaker A: And these measurements, these errors are kind of multiplied up. And say, for example, in sea ice, most of it's under the water. So if you're trying to sense how much is above the water, you have to be very accurate. Otherwise your error gets multiplied up when you multiply by a factor of eight or whatever relating to the buoyancy. So yeah, not to go into too much detail, but being able to remotely estimate the sort of roughness or fractal dimension of a surface is very useful in many circumstances because they fractals appear so often in nature.
00:10:07.035 - 00:10:18.491, Speaker B: Right. And then as you were working on this basically solution, which, and you realize, like the nature of the solution is asymmetric, you have to spend a lot of time.
00:10:18.523 - 00:10:24.295, Speaker A: Yeah, the problem is asymmetric. So it's fundamentally hard to solve, but relatively efficient to verify.
00:10:24.995 - 00:10:44.805, Speaker B: Efficient to verify. And then basically tig is this like that was the Inception of tig, that is, you can generalize these type of scientific problems that are asymmetric in nature and kind of use proof of work to effectively open up the solution to the whole world of those solutions?
00:10:45.185 - 00:10:54.565, Speaker A: Yes, in a sense, yeah. Because it means that it's economically viable for the research to be done openly rather than in a closed way. So that's true.
00:10:54.905 - 00:11:32.141, Speaker B: Got it. TIG is, has a long time horizon. Like it wants to accelerate scientific innovation over the decades to come. Its output can shape many areas of technology and science, bring a lot of utility to the real world for many. Like when I, when, like this is almost the opposite of what they hear and see in crypto. Like today, a lot of the attention in crypto is just some zero sum games with short lifetime, no fundamentals like meme coins and such. Right.
00:11:32.141 - 00:11:56.795, Speaker B: So I'm curious to hear your thoughts here. Like, do you see TIG as a single kind of outlier to this trend? Or like, given your long history and involvement in DCI community, are you optimistic that TIG and perhaps other these side projects along with TIG can bring real utility to the world using blockchain technology moving forward?
00:11:57.375 - 00:12:43.515, Speaker A: Yeah, this is a great question. I do feel that TIG is an outlier, but I also feel that maybe, I mean, some existing projects could learn from TIG and perhaps kind of, you know, modify that, you know, their sort of economics to some extent. The real issue is value capture. So there aren't many kind of projects. I think the crypto model itself sort of on open source in the early days to a large extent. So you have famously projects like Linux and then they have a sort of foundation which owns the IP and so on, and this stuff is sort of monetizable. I think one issue with crypto was the fact that, yeah, I mean, not to get too deep into it, but there was a lack of market discipline because the money came a little bit too easily.
00:12:43.515 - 00:13:23.305, Speaker A: And so it's sort of, you can have, you can spin up 10 projects which don't work in the time it takes to spin up one project that does work, if you see what I mean. And often it's sort of done under the guise of experimentation. But the thing is experiment. There are some things which it's like, if I conjecture that eating a kilogram of grass will cure the common cold, right, that's a testable hypothesis, but because it has no mechanism behind it, it's not sensible to test it. So running experiments without a plausible mechanism behind them is just not. It's just sort of nonsense. And, you know, and I think often maybe just in the context of crypto, the reason why it's done, even though it doesn't make any sense, is because it's so profitable.
00:13:23.305 - 00:14:12.863, Speaker A: So I think that Desai has a lot of potential or wouldn't be in the, in the field if it didn't. But I think that, you know, and they have started thinking about intellectual property and value capture, but things. Intellectual property is quite a counterintuitive thing. And yeah, we can dig into it a little bit more, but I feel like that in a lot of cases that they haven't really sort of captured the nature of the value, why patents are valuable to certain companies, in what sense it makes sense to own them individually or rather than as part of a portfolio. It's quite a sort of deep subject. So I think maybe there's a little bit of rigor lacking in the field. And sometimes I engage with people on this and you start, sometimes you get this kind of interesting look and then you get this kind of like, distant look with some people, whereas it's like actually they don't seem to care.
00:14:12.863 - 00:14:23.155, Speaker A: It's like, you know, you're making enough money, nobody wants to. Yeah, it's. It has a lot of potential, I would say, but, you know, currently unrealized and tick is an outlier.
00:14:23.735 - 00:15:07.555, Speaker B: No, it's great. It's great to see these kind of projects in the space. Like, there's definitely. This is definitely the type of projects that I personally want to see. So I think we're in a good position to start to dive a bit deeper into TIG's technical architecture. So in its architecture, it's like there's a marketplace with supply and demand side actors. Could you describe who these participants are, what their role is in the network, and what kind of motivations they have to participate? Maybe we can start with supply side actors here, which are benchmarkers and innovators.
00:15:08.695 - 00:15:50.855, Speaker A: Yeah, sure. So the benchmarkers are really the proof of work miners in the innovation game. And the reason why we call them benchmarkers is. Benchmarking is a term that we've taken, it's really kind of from computer science. And the function that they serve is really to figure out which algorithms are the most efficient, not necessarily the fastest, although speed and efficiency often go together because you have to do fewer computational steps. But what they're really looking for is to try and get the most number of solutions per unit time for the lowest cost expended. And in that sense, they're just like Any other proof of work? Crypto miner.
00:15:50.855 - 00:16:42.085, Speaker A: But they have this marvelous sort of side effect that they provide a very kind of strong and hard to manipulate market signal as to which algorithm is the most efficient. Because if they were using any other algorithm, they would fall to a competitive disadvantage and end up potentially losing a lot of money. So those guys just do what they do, try and mine as efficiently as they can with the best machines and the cheapest electricity. And we take that information, which is what algorithm they are choosing to use, and we use that to reward the participants who contribute the algorithms, which are the so called innovators. Now, it's important to note that you cannot in the innovation game use an algorithm for mining benchmarking unless it has been previously submitted. The innovation game. So we have a mechanism to check this and to verify it.
00:16:42.085 - 00:17:38.807, Speaker A: This is quite important because what it means is that the only option for people who want to get an advantage and have a better algorithm is to then share it openly with the network, which means that other people can immediately build upon their work. So it's this sort of open collaboration, which is sort of really the archetypal example of this, I think, is in open source. And that's the reason why it sort of moves so quickly. It's so innovative. A lot of people sort of pile in and want to improve upon one another's work and everybody gets the benefit of that. And so the reward that you get as soon as you're not allowed to mine with this sort of innovative new algorithm privately is you get these innovative rewards. And so that not only kind of benchmarkers or crypto miners are sort of remunerated with rewards in the innovation game, also, the people who contribute the algorithms are the so called innovators are remunerated on chain.
00:17:38.807 - 00:17:49.699, Speaker A: So these are sort of the two streams of rewards that we have for these two main types of participants. So you can think of it as the benchmark is generating demand for the algorithms and the innovators are satisfying that demand.
00:17:49.847 - 00:19:07.635, Speaker B: Perfect. So if I were to summarize, and please correct me if I'm wrong at any point, basically TIG addresses some scientific problems that are hard to solve, easy to verify in nature. And then we have one type of actor which is innovators. And these guys basically go and submit what they deem to be the most efficient solution to a given problem. So they do submit, let's say, algorithm A, algorithm B and algorithm C for a given problem. And then benchmarkers on the other end, as another actor go ahead and pick One of those algorithms, A, B or C, and they put their compute power behind it to effectively come up with maximum number of solutions to problem instances with minimum cost, cost, meaning energy cost and compute cost. And then the rewards basically gets allocated to innovators whose algorithm is adopted by the benchmarkers, miners and miners who propose the most solutions, I guess.
00:19:07.635 - 00:19:08.255, Speaker B: Right?
00:19:08.765 - 00:19:10.025, Speaker A: Yes, indeed.
00:19:10.525 - 00:20:11.355, Speaker B: Perfect. And what is in the system? If I'm an innovator, let's say I'm a research student or an academic, let's say I have an algorithm that I think is better than what exists in the network. What prevents anyone, like, after I submit this, you mentioned that anyone can build on top of it, right? Anyone can take it and improve it and resubmit it. So am I kind of giving out some of the value that I create in this case? What is like. Yeah, what kind of guarantees I have in terms of, like, the rewards? Like, wouldn't that open the network to a place where, like, anyone can come and come and like, basically improve on it and so I no longer get rewards? Like, how does, how does the incentive mechanism work here?
00:20:11.975 - 00:20:49.835, Speaker A: Yeah, that's a good question. So there is a sort of. In the simplest implementation you can imagine, the guy could make a big breakthrough and he could, you know, submit it to the innovation game. But then the sort of next week, maybe someone kind of nudges it along by, you know, 1% or 2%, and then everybody just goes to that new version, you know, even though the latest one was just a little nudge and the big breakthrough was, was more significant. And that would seem unfair. What we have in the innovation game is two streams, two separate streams of rewards for innovators. So there are really kind of two ways that you can make an algorithm faster.
00:20:49.835 - 00:21:50.605, Speaker A: One is to find a sort of more advanced mathematical strategy which is, you know, technically speaking is literally called the algorithm. So an algorithm isn't just a procedure. It isn't necessarily even embodied in a piece of computer code, although it always will be in the innovation game. And then the second way is to sort of make an optimization to the code which embodies the algorithm. So you might have sort of code optimization, sort of compiler time things, things that really just make the implementation run faster, but the algorithm itself that is being embodied remains the same. And so what we will have is a set of rewards for algorithmic innovation, what we call algorithmic breakthroughs, and then a completely separate stream of rewards for code innovation, code optimizations. So let's just say that a guy submits a sort of new algorithm and it will be kind of necessarily entail a new sort of embodiment in code, which will be in rust code in our case.
00:21:50.605 - 00:22:45.335, Speaker A: Then he would immediately get this innovator, immediately get all of the rewards. So if another innovator then came along and they sort of nudged it along just with a code optimization, the original submitter of the algorithm would continue to get these algorithmic breakthrough rewards and the other guy would start to get the other stream. But if the only time when the algorithmic breakthrough guy will get deposed is when there's a new algorithmic breakthrough, then this guy will now start to get that stream of. And the way that an algorithmic breakthrough is judged, it's quite interesting. So we will have a token holder vote simply on the question of whether the algorithm which is embodied in this code is novel. Is it different from the one that we had before? So it's not a vote on whether it's better or worse or anything. It simply determines novelty.
00:22:45.335 - 00:23:14.375, Speaker A: And this creates an obvious incentive for the person who submit that submit submits that to also give a written explanation of why it is which is novel. And this is, this is very good for us because it's exactly what you need to secure a patent, which is an explanation of novelty. But the sort of the question of whether it is better or not and whether it deserves to be rewarded at all, and if so, how much is entirely left to the market because that is dependent upon benchmark or adoption.
00:23:14.535 - 00:23:37.155, Speaker B: I see. So two different revenue streams for two different types of algorithm submissions. Basically, if you're doing incremental improvements, you get rewards based on that. And if you're making like algorithmic innovation, you get another stream for perhaps a longer period of time until the next kind of innovation happens.
00:23:37.655 - 00:23:51.795, Speaker A: That's correct. So algorithmic breakthroughs will tend to be far more infrequent than code optimizations. And so you would expect anybody who makes one will continue to get those that stream of rewards, at least for quite a long time.
00:23:52.415 - 00:24:18.245, Speaker B: Awesome. Could we double click on the benchmarker side and the proof of work? I want to, for the audience, I want to make it very clear what's the purpose of proof of work in TIG is and how it differs from what we know of proof of work in the bitcoin case? Like how did, how, how does the two differ in terms of like purpose?
00:24:19.025 - 00:25:04.353, Speaker A: Yeah, so the purpose is quite similar. So the way that TRG is at the moment, it's a semi centralized. So it's the purpose of proof of work is Purely to gauge the performance of the algorithms and figure out where, which innovators to reward. We're moving on to a layer one blockchain and when we do, it will also serve the purpose of sort of an anti symbol mechanism, which is this more kind of classic purpose that proof of work serves in securing a decentralized system such as a blockchain. So in that sense it is quite similar. It's also similar in the sense that the problems which are being solved are not meaningful. They're just randomly generated problems similar to the either kind of Bitcoin or most other famous proofs of work.
00:25:04.353 - 00:25:32.261, Speaker A: So it's quite distinct from a concept that some people might have heard of called proof of useful work, where people are sort of feeding in something that they want to get the answer to. You know, I want my genome sequence. Well, maybe I'll, I'll feed it into the sort of, you know, some kind of proof of work system which sequences genomes. That's not what we do because we don't. We only calculate synthetic problems. Synthetic problems are kind of very easy to work with. You can vary the difficulty, you can infinitely expand them.
00:25:32.261 - 00:25:35.785, Speaker A: There's no sort of limit of synthetic problems. So it's.
00:25:36.445 - 00:25:42.293, Speaker B: Yeah. Actually, could you explain what you mean by the synthetic problem? Yep, maybe. Yeah.
00:25:42.429 - 00:26:33.655, Speaker A: Problem is really just a randomly generated instance of what would be a sort of plausible problem in real life. So say that we were looking at again, sort of trying to sense the shape of an object by bouncing light off it. Maybe some kind of X ray, X ray crystallography. So you might have certain things, you know, that in real life that you want to kind of find the shape of. But it's also possible to generate an infinite number of just like random shapes or instances of problems where it's just a challenge for the computer. Can you back out that randomly generated kind of shape or geometry? And this is really just a test of the computer's or the algorithm's abilities. This is why we call it in benchmarking, you test it on randomly generated problems and then you kind of figure out how fast your computer or how fast your new algorithm is on this sort of set of problems.
00:26:33.655 - 00:27:09.655, Speaker A: So it's not like you're kind of typically. Yeah, it's just a way to figure out the speed or to get a very strong reliable market signal as to what is the most efficient algorithm at any given time. And this is really a very complicated problem because, I mean, I should highlight that if you have algorithm A and algorithm B, then you might say oh, algorithm B is faster, but then faster on what context? I mean, maybe algorithm B is faster if it's run on a particular type of hardware. So that's great. But maybe that hardware is really expensive. It's not available, it's not being manufactured in large volumes now. So the question of what algorithm is better is really quite a complicated one.
00:27:09.655 - 00:27:43.855, Speaker A: And it's sort of mixed up with all sorts of things such as hardware availability and cost. And this is something that really only a market can work out. Otherwise, if somebody sits in the middle and says, this algorithm is better and it deserves to get rewards, you have all the problems of on which hardware was it benchmarks, on which set of problems was it tested. And it's really kind of impossible to get a clear idea of that which would satisfy everybody. So we just leave it to the market.
00:27:44.355 - 00:28:18.943, Speaker B: I see, I see. Okay, so I want to make a few things clear. So first of all, the use, quote, unquote, useful proof of work's job in TIG is not to find a solution to a problem where we don't have a solution, that we're using proof of work to find it. The problems that TIG addresses already has, like, solutions out there. It's just that TIG tries to discover the most efficient solution to that problem. So that's right.
00:28:18.999 - 00:28:34.385, Speaker A: The problem is at one higher level of abstraction rather than a particular instance of a problem. Process some data and get some useful output. The problem is, how can you process that data more efficiently? So the the useful output is the algorithm itself, not the data which the algorithm outputs.
00:28:34.725 - 00:29:14.355, Speaker B: Right. And then we're dealing with kind of more or less static problems. The same problem, but the same problem has many synthetic instances that miners go and continuously try to solve, solve, solve. And so over time, you see whether or not miners converge on one solution. Because they're economically rational actors. They want to find maximum solutions with minimum cost. So they're the ones who get to decide the hard problem of which kind of algorithm is the most efficient algorithm to a given problem.
00:29:14.935 - 00:29:35.657, Speaker A: Precisely. Their behavior simply signals that. And it will be dependent on many things, including what hardware they have access to. And if the market is large enough, as we've seen with Bitcoin, it starts, in fact, the demand causes new hardware to be developed for the problems to be solved. So this is also part of it, without that demand, without.
00:29:35.721 - 00:29:37.777, Speaker B: Much like Bitcoin, Right? Much like Bitcoin.
00:29:37.801 - 00:30:07.515, Speaker A: That's right, precisely. So we can see now that there's a proliferation of hardware. It used to be just CPUs then it was GPUs because there was so much demand really from computer graphics applications, and then there was demand from Bitcoin. And then you had a very large class of hardware, sort of crypto mining, asics. But now we have specialist hardware for inference, specialist hardware that tensor cores for training. And this will continue. There'll be more and more different types of hardware which are sort of directed at particular problem classes.
00:30:07.515 - 00:30:36.345, Speaker A: And so hardware and algorithms are very hard to sort of disentangle. You might have a sort of pause in algorithm development and then suddenly GPUs come along, they're available, they're relatively cheap and affordable. And then the next wave of algorithms are algorithms which are designed to exploit this huge parallelism of this new hardware which is now generally available. So these two things kind of move in lockstep and it really doesn't happen unless there is the demand comes first.
00:30:37.365 - 00:30:57.069, Speaker B: I see. So what type of problems are we talking about? Could you maybe walk us through one of the examples that TIG already addresses and explain us kind of in what type of use cases or applications? These algorithms or. Yeah, these algorithms are being used today.
00:30:57.197 - 00:31:48.415, Speaker A: Sure, absolutely. So at the moment, TIG has four challenges in place. So one is something called Boolean satisfiability, sat, as it's sometimes called, which is the problem of if you have a sort of kind of Boolean formula, are there assignments to the variables in that formula which come out as sort of true? So the sort of Boolean result of this formula would be a sort of a one as opposed to a zero. So this is kind of very well known, sort of classic problem in computer science. I think it was the first problem to be proven NP complete, which is sort of a bit of a kind of detail, but it's sort of classic, hard to solve and easy to verify. This is used in all sorts of applications. Probably the most notable one is verifying hardware designs and trying to find faults or flaws in hardware designs.
00:31:48.415 - 00:32:48.545, Speaker A: They use these so called SAT solvers in that application. But there are all sorts of other things as well, which reduced to the SAT problem. We have also the knapsack problem, which is using everything from gene cluster identification to sort of multi constraint auctions. You might have heard a guy called Milgram and I forget the name of his partner, but they both, they recently won the Nobel Prize for these special type of kind of multivariable auctions they use to sort of sell broadband spectrum and sort of telecommunications frequencies for 4G. So that the knapsack problem is used in that as well. And then we have capacitated vehicle routing, which is very easy to understand. It's things like logistics, you know, kind of Amazon in their warehouse, use that to sort of try and optimize the, you know, reduce the amount of distance that their workers have to move and no doubt the robots as well.
00:32:48.545 - 00:33:36.419, Speaker A: And we have another one which is vector search, which is sort of. Yeah, this is very big in the age of AI. It's an old problem, but it's found kind of new use where you look in the sort of large database of vectors and you sort of pick one out and then you want to find which other vector is its nearest neighbor or which ones are within a particular kind of Euclidean distance, sort of hypersphere of this particular high dimensional vector. And that's used really in. Yeah, it's used a lot in AI in kind of databases and image recognition, different problems like that. And we have another one that we're bringing on probably in a couple of weeks called hypergraph partitioning. It sounds very exotic, but the main thing is just a sort of sets and subsets and then you try and make them overlap minimally.
00:33:36.419 - 00:34:00.525, Speaker A: And again, this is another problem. It's very hard to solve, but easy to verify. And I guess, you know, headline application would be making parallel computing more efficient. So it's anything, you know, any types of parallel computing, including machine learning training runs, are sort of heavily dependent upon being able to solve the hypergraph partitioning problem efficiently. So all this stuff, I mean, it sounds quite niche, but it's embedded in everyday life.
00:34:01.385 - 00:34:16.645, Speaker B: So what is, who gets to pick these problems, like which entity in TIG and then has this role and what's the decision factor when selecting these problems as the problems that TIG should address?
00:34:17.585 - 00:35:06.945, Speaker A: Yeah, so there are a few criteria. So one is that, I mean, at least the problems that we pick first, we're going for ones which are intrinsically hard to solve but easy to verify. There is in fact a trick that you can do to turn any problem into one which is hard to solve but easy to verify, which involves verifiable computing, which we can go into later if you want. But the ones that we're doing first, because we don't have that functionality yet, have this kind of natural structure. So a huge number of very important problems in science and technology have that problem, have that structure, the people who choose them. So far it's been the core team. We haven't long got going, but we're putting together a committee of scientists and really just domain experts to propose these Problems.
00:35:06.945 - 00:35:54.563, Speaker A: And really, I think it's important to understand that these problems are sort of finite in number. I estimate there's probably roughly of the order of a thousand sort of really low level fundamental problems in mathematical science of this sort. And the idea is over the next three to five years, we just really kind of load up all of them. So rather than it being this sort of permissionless thing where people can, you know, suggest, say, hey, I've got this problem, you know, can you put it in tig for me? The issue with that is it could be a security risk. They could have a backdoor. They could have crafted the problem to sort of know that they know an efficient way to set, to solve it and nobody else does. And so the better way to look at it is more like maybe if you were deciding which opcodes are going to go into the next sort of version of solidity or something, you wouldn't say, hey, you know, come and suggest me an opcode, right? Like some.
00:35:54.563 - 00:36:39.273, Speaker A: It would be a bit of a nightmare, right? They've all sorts of ones that are crafted for weird sort of kind of tips and tricks and hacking and stuff like that. It's really just one of those things which should be decided, you know, by a sort of committee of people who know the fact that the problem is a known problem in science means that there is no backdoor to it, right? If there would be, then we would know that by now. So these people are pretty robust. These things are pretty robustly, you know, hard to, hard to crack. And so once we sort of load up these, these problems, this problem set will remain pretty stable. So if you look sort of just historically, you know, the number of problems which are, which are important in mathematical physics are, is pretty stable. They find new applications for them.
00:36:39.273 - 00:37:20.255, Speaker A: Things like gradient descent are used to find, you know, the minimum of a multidimensional function. But that's something that, you know, Gauss used to think about like 200 years ago, right? Like the, the problems themselves are quite old mostly you do occasionally get a new one. What is much more dynamic is the best methods to solve them or developing new methods to solve them and finding new applications for these problems. Like, you know, the knapsack problem has been known about for hundreds of years. But, you know, this guy won the Nobel Prize for applying it to this sort of multiband spectrum auction thing just a couple of years ago. So it's like the problems themselves are relatively stable set. And if you asked sort of 100 scientists what the most important problems were, you probably get pretty much all of them saying the same thing.
00:37:20.255 - 00:37:49.115, Speaker A: What they disagree on is the best strategy for solving the problems and maybe what applications they might have. So this is the thing, load it all up and then after that new problems can occasionally be added and some retired. But this will really kind of happen in slow time and it'll be more like maybe just an upgrade to the Bitcoin network. It happens occasionally, but it's not something which is sort of, you know, happening very frequently. We just sort of stick with this, with this problem set and it covers almost everything.
00:37:49.495 - 00:38:21.001, Speaker B: Right. And so would. So currently, as far as I understand these problems, at least the first ones have a wide range of applications in industries, scientific industries. Would you consider putting like more sort of use case specific ones maybe like directly AI related to accelerate maybe AI model development or. Yeah. What's your thoughts there?
00:38:21.113 - 00:38:51.869, Speaker A: Oh, absolutely, yeah. So I mean AI is one of those areas, as I said, some things in AI are kind of very old, like minimizing cost functions and then. But there's lots of more advanced versions of gradient descent which can do that sort of more efficiently. And then it has introduced some really, just as far as I know, quite new challenges. Things like neural network architectures, like the transformer architecture. So finding a more efficient neural network architecture is not a problem that has been around for hundreds of years. Actually it's been around for about 50 years, I think, or maybe 60 years.
00:38:51.869 - 00:39:47.551, Speaker A: It is quite old, but it's relatively new. So this is an example. So absolutely. It's really anything that's important actually it need not be necessarily all applied stuff. I mean the applied stuff causes, you know, generates demand for licensing the ip, which is embodied in these algorithms, which we can talk about in a bit. The thing is, if you have sort of more fundamental problems, the reason why the government, for example, puts money into fundamental research is not just because it loves science, it's because from its point of view it is investable because fundamental science is normally sort of done just to understand, you know, kind of basic processes of nature, you know, but more completely then just we see historically that you get these sort of unexpected applications so you understand the basic process better and then someone will think of an application for it and then somebody will start a company and then the government can recover sort of tax revenue from that company. So it's.
00:39:47.551 - 00:40:41.755, Speaker A: That's the very indirect way in which, you know, from a government's point of view, sort of fundamental science with no particular application is an extremely good investment. But the key is that it's not really privately investable, because private companies, they simply don't have the scope and the breadth of interests whereby they could reliably capture very unexpected spillovers from fundamental research. So they're not usually in the business that it spills over into, whereas in a sense the government is in all businesses. And similarly the innovation game can be in all businesses. So this means that a fundamental problem, you know, we can credibly capture sort of spillover into the more applied areas. And so we can, you know, the fundamental kind of problems in more sort of fundamental areas can really earn their place and not just be there for, you know, virtue signaling and that sort of thing. Because we can have this sort of huge spectrum of interest which a private company could never match.
00:40:42.735 - 00:41:12.455, Speaker B: Very interesting. I want to switch to the. So we talked about supply side a lot. I want to switch the demand side. Who is actually like, how does the network capture the value it creates? Right. So TIG discovers the most efficient algorithmic solution to a given scientific problem. Anyone can come in and continuously improve over it to earn tokens.
00:41:12.455 - 00:41:35.935, Speaker B: So it not only discovers like the most efficient one, but also like continuously. It continuously does so, right? Yes. But at the end of it, all of this is actually for the demand side. So could you maybe talk about how TIG plans to capture the value it creates and what's kind of the monetization angle here?
00:41:36.635 - 00:42:24.925, Speaker A: Yes, indeed. So there are many kind of parallels to this part of things to an open source project. So you may know in open source projects what you often, when you contribute, you have to give at least a license to your copyright. Sometimes they ask that you assign the copyright, which is transfer the ownership. If you have a patent on some kind of invention which is embodied on your contribution to an open source project, typically you're asked to give a license to that patent as well, so that other people can use it. You wouldn't be able to assert your patent against other users of the code base. So these sort of, you know, open source foundations, they sort of hold a lot of, you know, at least licenses and often kind of assignments to copyright and they're often hold patents.
00:42:24.925 - 00:43:16.049, Speaker A: And this is a sort of, you know, an asset, sort of great value and something which is an idea which actually came up, you know, quite a long time ago in order to capture some values and monetary value in open source was a concept of dual licensing. So you make all of your IP available under a sort of an open license. In that case, it was a sort of an open source license. But also if you don't like the sort of, you know, if you find the terms of the Open Source license a bit too restrictive, you can pay for to get the use of that IP under more permissive terms. So the less restrictive license. So in a sense TIG sort of models ourselves to some extent on this dual licensing model. We capture ip, we will receive licenses to patents.
00:43:16.049 - 00:44:25.155, Speaker A: If somebody's already got a patent on it, on their contribution, if they don't have a patent on their contribution and they want to say, apply for the higher tier of algorithmic breakthrough rewards, we will ask for sort of assignment of the rights to file a patent on the contribution ourselves. So we build up patents and copyrights, which covers both the sort of inventions and the embodiments of the inventions, which are the implementations of the algorithm in Rust code. And we make that available under two licenses. So one is the so called TIG Open Data License, which is a sort of novel license which we kind of developed in house. My colleague Phil David was mainly responsible for, for our licensing. And this is a sort of copy left license. So it has a kind of typical open source property whereby your property of a lot of at least the early open source licenses, whereby if you sort of make a modification to the source code and then you put it in a product and you sort of, you know, you distribute that product, maybe it's a sort of binary embedded in something, then you're obligated to make your source code available for that.
00:44:25.155 - 00:45:48.855, Speaker A: And that's a kind of sort of classic sort of condition, you know, in the sort of GPL licenses which, under which sort of Linux, for example, it is licensed. But another sort of twist, and the reason why it's called the Open Data License is we have something called the Open Data requirement, which means that if you use one of our algorithms or an algorithm covered by our IP under the TIG Open Data License to process some data and then the outputs, you sort of made a product out of that or distributed it, then you'd be obligated to make the input data available. So this is the sort of condition which say an academic, for example, would tend to be happy with, because when they sort of publish some of their work, they're supposed to really, by those norms of science and convention, also give all the information which enables their output, their publication to be reproduced. Some academic publications don't actually contain this, which is a sort of, you know, kind of a bit controversial. But the sort of academic norms mean that, you know, your work should be reproducible and you should give the sort of input data which, when processed by this algorithm, you know, gave rise to your wonderful published result. So kind of academics and, you know, open source enthusiasts and people who just sort of like to see the code and tinker with things, they can all use any and all of tig's ip, you know, for free under the terms of this open data license. And for anybody else, you know, many corporates, for example, might not, you know, that their, their asset is a sort of.
00:45:48.855 - 00:46:24.249, Speaker A: Yeah. The data that they hold is often a kind of valuable proprietary asset. And if they want to be sort of exempted from that obligation, then they can pay the commercial license fee and use the IP under the terms of the TIG commercial license. And these sort of commercial license fees are in some way, they're sort of scaled to the size of the organization. But the key thing is that they're payable in the TIG token. It'd probably be priced in dollars, but it'd be however many TIG tokens are worth that amount of dollars at that particular time. And that's what generates the demand on the back end for the TIG token.
00:46:24.337 - 00:47:00.865, Speaker B: Right. And what's like the go to. I think, yeah. I'm curious what's like the go to market strategy for bootstrapping to the demand side? Inflationary rewards are used to bootstrap the supply side actors. I think that works great thus far. One stone TIG is a place where it actually has given birth to algorithmic solutions that are better than what commercial enterprises uses in their internal processes. Let's say.
00:47:00.865 - 00:47:18.135, Speaker B: What's the go to market strategy there? How would a commercial enterprise get to know about this kind of. Yeah. What will be the steps from there to actually then go ahead and purchasing it?
00:47:18.955 - 00:48:29.895, Speaker A: Yeah. So it's still kind of early days with that, as you mentioned. So the innovation game has not been sort of running so far for that long. We've had quite a lot of improvements in algorithms, sort of better and better optimizations, but I think nothing that has so far gone beyond the state of the art yet. But when we do have that, I mean, gaining awareness, we're very focused on universities or we have been, I sort of, kind of right at the beginning, we even made sort of benchmarking of TIG mining possible to do it in browser to sort of lower the value value kind of, you know, barrier to entry maximally and sort of put, put the word out through sort of crypto societies and other kind of science societies and universities. We have contacts at some universities for, with, with academics, particularly ones we're looking at with industry connections because some academics do work very closely with industry. You know, Imperial College, Cambridge University here.
00:48:29.895 - 00:49:21.735, Speaker A: And also one of our co founders, Philip David, he was the who did all of the licensing, he was the head of legal, he was general counsel at ARM holdings for 20 years as well. So he's got a very extensive network of industry connections in big tech. So I don't think we're going to have a problem making people aware of the existence of tig. If the algorithmic innovation comes, it will be almost irresistible, assuming that there are, that they're aware of it. And one of the reasons is that, you know, algorithmic innovation can just be huge. Like the degree of efficiency gains you can find in a better algorithm historically often far outstrips improvements from hardware. For example, making sort of smaller and smaller transitions famously has resulted in many orders of magnitude sort of speed ups.
00:49:21.735 - 00:50:05.151, Speaker A: But in fact the kind of the aggregate effect of better algorithms has been even greater than that. And often the algorithm, but the hardware improvements in their nature often tend to be quite incremental and predictable, whereas algorithms, you often get to get nothing for maybe like a decade and then you get a massive leap, stepwise leap, because somebody had an insight. And historically this has happened many times and these leaps often eclipse all of the hardware improvement, for example, that has happened maybe over the last 20 years. So these things are so powerful that when they do happen, they're pretty much. You can't really be without them. Transformer architecture is an example. It's a relatively modest leap in algorithmic terms.
00:50:05.151 - 00:50:40.871, Speaker A: I think it's estimated to be about a 20x improvement for about the size of the training runs that people are doing at the moment. But as the training runs get bigger, another interesting thing about algorithms is the improvement gets bigger. So it's 20x now. In two or three years it could be 100x. And that means that nobody can do without the transformer architecture. You can't buy 100 times more hardware to keep up with your competitor is absolutely essential. I would say that these things are irresistible when they come.
00:50:40.871 - 00:51:02.261, Speaker A: A lot of the time the progress is very sporadic. But that's why it's smart to have a very wide diversity of interests. We work in all algorithmic areas. There's always something going on somewhere. So you have this kind of effect of a diversified portfolio which really kind of blends the value of these different assets and that's reflected in the token value.
00:51:02.413 - 00:51:23.675, Speaker B: Thank you, John. This has been a great discussion. My final question to you is where do you see TIG in its roadmap? Could you maybe summarize us like what has been accomplished thus far and moving forward, what is top of the mind for you in tig's future roadmap?
00:51:24.215 - 00:52:17.613, Speaker A: Yeah, absolutely. So what has been accomplished so far is we have the first, I guess, proof of work system which uses algorithms which are optimizable while it remaining stable. As far as we know that the people are finding better ways, more efficient ways to mine and the innovators are sending it in and receiving very substantial rewards even at the current token price for their innovations. So this sort of optimizable proof of work invention is working as expected. And the next kind of big step, I suppose in terms of architecture will be to move on to our own layer one blockchain. But from the point of view of the innovators and the benchmarkers, everything will remain exactly the same. So I think that's as blockchain should be.
00:52:17.613 - 00:52:40.859, Speaker A: It's just an underlying thing which you don't really notice. Like it's not the reason for the protocol existing at the moment. It's run in a kind of semi centralized way. And we will move on to a blockchain because it's better to be centralized. No one company or even one data center should be sort of orchestrating all of the algorithms in science and technology. So it's better for be that way. But it's not the reason for TIG existing.
00:52:40.859 - 00:53:22.489, Speaker A: It's simply a very nice feature to have in the shorter term. We're going to be adding more challenges. As I mentioned, we're adding hypergraph partitioning soon we'll be looking to add probably we're going to step up the rate of a challenge edition up to at least one new challenge a month. And then up beyond there will onboard several hundred more challenges at least over the next three to five years. I guess another interesting thing is that we're adding a deposit requirement for benchmarkers. So they'll have to hold a certain amount of deposits along with the kind of computational work which they give to the network. Yeah, just an aside, the reason for that is because the algorithms are always changing.
00:53:22.489 - 00:53:41.867, Speaker A: And so we don't expect sort of ASICS to be a thing on tig probably for quite a while. Because an ASIC is algorithm specific. It means there may be a sort of issue with botnets because people can mine relatively efficiently onto sort of consumer hardware. But the deposit requirement sort of really mitigates the threat from that.
00:53:42.051 - 00:53:55.735, Speaker B: Yeah. So in addition to kind of proof of work, it would be like almost proof of stake as an additional kind of Sybil resistance mechanism. Am I getting that right?
00:53:56.125 - 00:54:21.793, Speaker A: Yeah. So it's definitely closely analogous to that. I would say that kind of, yeah, staking is a bit more like, yeah, you put down some kind of amount of coins and then you get a sort of return on that amount of coins. I still look at it as all of the return being in return for computational work.
00:54:21.989 - 00:54:22.321, Speaker B: Right.
00:54:22.353 - 00:54:55.891, Speaker A: But yeah, I guess you could certainly look at it like that. It was sort of a hybrid thing. So the way it would work, just to kind of give you a quick overview, is it's quite simple. If you had about 5% of all the computational power on the network, to mine most efficiently, you would want about 5% of all the coins which are held by the miners. So the sort of deposit requirement is in proportional to the size of the miner that you are. So it's a kind of natural requirement in that sense. And it does mean in practice that people could get a return on coins because miners will want these coins in order to mine efficiently.
00:54:55.891 - 00:55:06.055, Speaker A: And so if you have some, they'll typically be willing to give you a cut of their profits in return for you sort of lending them their coins so they have sufficient deposit.
00:55:06.555 - 00:55:19.085, Speaker B: Okay. Are there any active kind of focus to add in more innovators, bring in more innovators into the. Into the network?
00:55:19.745 - 00:55:40.625, Speaker A: Yeah. So we've been going through university outreach. One big kind of thing about universities is they tend to be quite conservative, as you might imagine. And so a kind of a big leap forward lately is that we've had. We have a sort of a. Sort of a market spot price. So, you know, the kind of university academics can really see.
00:55:40.625 - 00:56:26.117, Speaker A: This number of tokens really does translate to me being able to sell them in this liquid market and get this amount of money from my research. So this has been a big kind of acceleration. Our best innovators sort of prior to that were actually, interestingly, people who work for hedge funds. I think they're actually scientists who went into the finance industry, which is actually kind of all too common around here, and then they realized they could do, you know, hedge fund stuff and science at the same time. So they did some kind of fantastic amount of really fast innovation. But, yeah, we're kind of moving into the more kind of traditional academic world. So as we speak now that we have a sort of very tangible value that we can point to that these people can use to fund their research.
00:56:26.301 - 00:56:37.367, Speaker B: Awesome. John, thanks a lot for joining. This has been a very big pleasure for me to host you. Thanks for joining.
00:56:37.551 - 00:56:39.655, Speaker A: Yeah, great. Yeah, thanks very much for having me.
