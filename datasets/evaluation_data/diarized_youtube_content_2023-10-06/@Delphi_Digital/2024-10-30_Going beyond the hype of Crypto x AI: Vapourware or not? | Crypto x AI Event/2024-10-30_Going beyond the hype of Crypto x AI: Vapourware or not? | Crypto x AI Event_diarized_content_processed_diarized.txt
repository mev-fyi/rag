00:00:00.400 - 00:00:35.911, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors near empowering decentralized applications and blockchain ecosystems. Venice AI, a private and uncensored alternative to popular AI apps. Mira unified AI infrastructure secured by crypto and the first on chain multi agent system. To learn more about all of our sponsors, check the description below and dive in.
00:00:35.911 - 00:00:37.155, Speaker A: Enjoy the show.
00:00:50.655 - 00:01:37.645, Speaker B: All right. Hi everyone. Jose Mesedu here, your host. Happy to be joined by Kyle Samani, co founder and managing partner of Multicoin and Ahmad, founder of Stability AI, one of the OG AI companies that kicked off the AI revolution and now co founder and CEO of Schelling AI. So today we're going to try and have a little bit of a debate and specifically sort of the debate subject is, is crypto AI vaporware? This is something obviously like crypto AI's arguably the most hyped but also the most controversial sector in crypto right now. And it's one that a lot of smart investors have actually taken negative positions on. And so what I wanted to start off with, I think you guys agree on more than you disagree.
00:01:37.645 - 00:01:57.185, Speaker B: But the way I want to start this is just maybe staking out your position on where you are on the spectrum here in terms of crypto AI being vaporware. And you can take it anywhere you want. You can talk about specific sectors, stuff you're excited about. But yeah, to what extent do you think it is vaporware? Maybe we can start there and then, and then narrow it down further. Let's start with, with you, Kyle.
00:01:58.125 - 00:02:34.811, Speaker C: Sure. Everyone, Kyle here, co founder of Multi Coin Capital. So the areas that, the area within crypto that I'm most excited about by far is GPU marketplaces. I think there is not enough silicon in the world. I think the supply chain of fabrication from tsmc, asml, everyone else upstream is like, cannot serve the demands for, for compute. I think that will continue to be the case for, for quite some time. So I think we need to as a society figure out how to take advantage of all the latent compute around the world.
00:02:34.811 - 00:03:25.891, Speaker C: And we need to actually financialize compute into a fungible commodity market. Probably the largest commodity market in the world that is not currently financialized is telecom. This is that compute is probably the second largest and we obviously have made several bets on the telecom side and we are making bets on the compute side as well. Financializing compute is very difficult for a whole bunch of mechanical logistical reasons. Unlike a barrel of oil, where a barrel of oil can just sit there orchestrating compute is actually fairly complicated. And so I think that because of the inherent complexity of orchestration, it makes it difficult and also means there's an opportunity for teams to capture sustainable margin. Everything else we've seen we haven't been convinced is an opportunity for a venture cap.
00:03:25.891 - 00:04:22.775, Speaker C: Venture outcome doesn't mean that there aren't good ideas to be had. We just have been skeptical. On the venture outcome side, the most obvious example I'll give is the idea of hey, is this image photoshopped or not? And like obviously if your iPhone, you take a photo, if you can upload the hash to the image to a chain or whatever, some public repository or something, very clearly like that is a good idea. Like I don't think you can find anyone on the planet who's like aha, this is dumb. The question is just how do you implement the system? And I think the only real answer is you need Apple, Google and Microsoft and Facebook to get in a room and like agree because you need to upgrade the operating systems and the web browsers to enforce this common standard. And if you don't, then your implementation doesn't matter. Lots of other ideas of a similar vein in the crypto AI sector that I just don't think matter for startups, but I'll pause there.
00:04:23.595 - 00:04:39.155, Speaker B: Okay, so just to clarify then, you're mainly bullish on decentralized compute marketplaces, like all the other stuff, decentralized inference agents, like data data labeling, all that stuff. You think so far you haven't seen anything that that convinces you?
00:04:40.615 - 00:05:06.785, Speaker C: I mean again, like don't want to over generalize. One of our Portfolio Companies is HiveMapper. They are doing decentralized data labeling with token incentives today. They call it their hivemapper training game or something. But I mean they literally have people they like look at the photo and they're like the speed limit 60 or 65, you know, stuff like that. It's live, it's in production. I think it's been in production for over 12 months and it works.
00:05:06.785 - 00:05:55.679, Speaker C: So I'm not opposed to it existing. In fact, we have a proof point that it can work. I think hivemapper works because it's extremely domain specific and very narrow and also doesn't require, quite frankly a whole lot of IQ to do that kind of work. This is pretty straightforward work. My understanding, if you look at scale AI today and OpenAI and the people doing state of the art stuff like we don't need more people being like ha, you're an idiot on Reddit, like that's not making GPT5 smarter. And having just trolls on Reddit going back and forth, what's making a GPT5 smarter or whatever. 01 Strawberry Smarter is, you know, a dissertation of a PhD or even better, someone who wrote up an investment MO.
00:05:55.679 - 00:06:24.247, Speaker C: I think all of the ways the world could play out are the following. Based on these economics and these risks and these criteria and how these technologies are developing. I assess these probabilities in these ways. Based on my current assessment, I made these decisions. And then here's a follow up. I wrote 6, 12, 24 months later, you know, basically what was right, what was wrong. And like that's the kind of content you need to feed into, you know, the training system for GPT5 to make it smarter or not.
00:06:24.247 - 00:06:39.015, Speaker C: People fucking commenting on who wore a dress last night at the Oscars, like that's not doing anything. And that kind of training I don't think lends itself towards, you know, permissionless, token incentivized stuff.
00:06:40.675 - 00:06:44.415, Speaker B: All right, Iman, do you want to give your take here?
00:06:45.355 - 00:07:19.511, Speaker D: Yeah, I think it's a fascinating time and I think I saw some charts. It's like 50% of the mind share in crypto is already AI and then like it's $35 billion market capturing a couple of billion a day. Like it's up at defi levels almost. It feels that it's got a bit ahead of itself. I think that there are some interesting things again around GPU marketplaces and more, but not for what we see today. You know, on the current side it's been a question of it's cheaper and more dynamic. I think that as you move towards O1 type models that can think longer, you can do more stuff.
00:07:19.511 - 00:08:00.385, Speaker D: As you standardize the stack again, it removes a lot of this complexity that you're seeing today. Like I think decentralized training could work, but I doubt it will outperform in any case the centralized training. Although again it's worth exploring. I think, you know, that's where I see crypto X AI will in this direction in terms of crypto being used for AI as opposed to mostly AI being used for crypto, which is maybe something we discuss later. It's all a bit early. And where are the real users? I think that when I look at crypto verification, coordination and resilience are three key areas that can be helped a lot with this. And I think probably where I'm most interested is in maybe a slightly different type of data approach.
00:08:00.385 - 00:09:02.311, Speaker D: Like if you look at OpenAI right now and Anthropic and everyone, they're paying hundreds of millions of dollars to get all these licenses from everyone, right? But the way that these models are is that you pre train them with general knowledge, they get more and more specific knowledge. I think the emergence of data daos and programmatic data access to create models on the fly and access things around that could be a really interesting area here because then you don't need to negotiate all of the different contracts, but there's a lot of work that needs to be done that like we've seen story protocol, for example, on the IP side really have a stab. That's going to be a hideously complicated thing for them to try and bring some of these real world like thick contracts into the crypto space and one that get the IP holders and these data sources on there. I think there are some interesting things, like I saw a dao being put together to buy all the 23andMe data and other things. But again, the data side feels quite nascent. The compute side I think will become useful, but it isn't quite yet. Again, how many AI labs are actually using the decentralized compute? But again I think that could evolve.
00:09:02.311 - 00:10:13.335, Speaker D: And then when I look at some of these other areas around verifiable inference, around ZK stuff and more, again one of the questions is, is it better than what we have right now when everyone in the world is now bending their infrastructure towards this technology? So like for example, when you're thinking about edge AI, what meaningful difference will that have over Apple's edge intelligence that they've got on your phone? Like there will be a certain subset of people that won't trust Apple, but that's probably going to be not that large, right? Who will want to have their own AIs there? So I think again this will evolve as we move from the base models to flows of models. Like we have this comfy UI system we built for stable diffusion, whereby it tracks every single decision you make in every single model. And then the image, if I share it with you, reconstructs the whole thing. That type of thing feels again crypto ish. But those flows of models being put together and systems are only emerging right now. So it's a bit tough to kind of say what will be the winner. And then I think the final area that I think is interesting is you look at systems like Bittensor and others that are purely on the coordination, incentivization side.
00:10:13.335 - 00:10:34.175, Speaker D: And it feels kind of familiar to crypto X gaming in a way. Like we've had what, $4 billion put in there, but how much useful stuff has come out? I think that they could go well, but there's a lot of work needs to be done now because it maybe got a bit too quick, big a bit too quickly, and now there's lots of incentivized interest there.
00:10:35.885 - 00:10:45.505, Speaker B: Okay, so thanks for that. Appreciate, appreciate those initial positions. So you spoke about crypto's benefits being resilience, incentivization.
00:10:47.365 - 00:10:49.621, Speaker D: Resilience, coordination and verifiability.
00:10:49.813 - 00:11:00.705, Speaker B: Okay, resilience, coordination and verifiability, that's it. And so what's an example of an application or a few that you're excited about? That leverage that, that I guess couldn't be done without crypto.
00:11:01.655 - 00:11:26.087, Speaker D: So the data dials. Yeah, I think the data daos are an example. Again, a lot of work needs to be done. These are very nascent, but you have all the AI companies and all the AI individuals trying to do the same thing, which is access. Like I think knowledge comes in three types. There's common knowledge, right, that's like medical textbooks and other things like that stuff we know. Then there's private knowledge, the stuff you've got in your laptops and things.
00:11:26.087 - 00:11:59.607, Speaker D: And then there's area of ip. And the IP is the interesting one. This is where Open A and others are paying 60, hundreds of millions of dollars. But like when I talked to the movie studios last year, none of them wants to give their data to OpenAI, but they do want access to the models and to have their own models. So is there some sort of systematic logic based approach that you can access the data in those and some sort of infrastructure you can build around that? Does that need to be crypto? Maybe, maybe not. But I think there are some interesting things emerging there, especially as the models standardize. And you can then tune the models on these.
00:11:59.607 - 00:12:59.269, Speaker D: How can you reward them, how can you pay them, how can you track. And with the models themselves, there are very interesting things that you could never do before, such as on the tracking basis, you're heading on towards 100% accuracy now with things like structured output from OpenAI. But being able to put your brand IP out there and then have an AI that checks whether or not it fits the brand criteria or the overall ethos is something again, that's programmatic and that's a coordination kind of game. So I think the data side could be very interesting. As you get standardized models and you start to move to flows. I think on the resilience side that's an interesting one because as we have AIs that basically check every medical decision, for example, and make every medical decision, what are the rails that that's on and where is the verification of that and resilience so that it, if OpenAI has a breakdown, you suddenly don't have people dying, for example. I think again there's a resilience factor there, particularly as you're accessing data and you're coordinating data.
00:12:59.269 - 00:13:40.721, Speaker D: And the final thing, verifiability. As Carl said, things like the output of models needs to be verified, that needs to be a standard and we have content authenticity that we did with Adobe and Brothers for that. But I think one more interesting thing is maybe some of the Oracle stuff like verifying facts, verifying assumptions, verifying input data for decision making models, and verifying chains of thought reasoning as you move towards real reasoning models. But again, this all feels very nascent and we're seeing people explore, but we have this weird nexus of crypto hype and AI hype happening at the same time. So there's just. They're not going through the standard development process. A lot of companies in the space and initiatives in this space.
00:13:40.721 - 00:13:52.155, Speaker D: I think that comes to Carl's point about venture backed outcomes in particular. Like what can you really back here? What's going to be institutional versus a team and where's the real value add going to be?
00:13:53.455 - 00:14:38.055, Speaker B: Okay, makes sense. Sounds like you guys mostly agree that it's pretty nascent. I guess most people's response to crypto AI is that they're not seeing really the talent going to it. They find it hard to imagine crypto AI being able to compete with the hyperscalers who have all the compute, all the talent and the resources to kind of keep spending on that. And they see most of the crypto AI stuff as solutions looking for problems. Just kind of the classic crypto critique. How do you guys assess these claims? What are you seeing in terms of the talent coming into the space? Are these like legit AI developers that are coming in like ideologically motivated or is it sort of crypto people pivoting to AI?
00:14:41.045 - 00:15:40.649, Speaker C: Yeah, I mean we've made three bets that are crypto AI, render four IO and then two that are not disclosed. In the case of all four, the founders were not crypto people. They were either video rendering people or AI people. And they got into the crypto part and then if you look at the other one I'd say is crypto AI. Adjacent in our portfolio is hivemapper for the reason I described earlier about the data labeling. And obviously those guys are mapping experts and they just kind of realized they're like, oh, cool, we can hand out some tokens from people to label some images. So in our experience, the teams we've backed have been teams that have had domain expertise and then they just kind of recognize they can use crypto as a tool.
00:15:40.649 - 00:15:48.565, Speaker C: I think that's probably the most important thing is crypto is a tool in the toolbox on the way to achieve the full solution.
00:15:50.025 - 00:16:43.635, Speaker D: Yeah, I think that from my side, yeah, like, the quality is increasing dramatically. I think as people are looking at new models and new things. Like right now, again, we're seeing a lot of the early stage AI companies start to face challenges as they're hitting real revenue, because again, the hype kind of overtook a lot of them and they were all doing the same thing without distribution, brand data advantages, et cetera. I think as well, crypto kind of has matured a bit in terms of some of the infrastructure enabling some of the stuff, as you get a lot of this being built around data availability, around these other things. But I mean, you get both types of founders. Like, one of the most interesting things recently has been true or not Truth Terminal aping into Go Coin and potentially making a million bucks as an AI, Right. AI can do memetics very, very well.
00:16:43.635 - 00:17:32.624, Speaker D: And so I'm sure a lot of people are going to be looking at that. And so you'll have this mixture just like you've had before, of real infrastructure engineering people, and then people on the other side that are trying to make a quick buck or that are using this to build game theoretical optimal systems to create entertainment and more. I think one of the most interesting things here is that when you actually look at the number of developers that build models versus use them, the models aren't actually that complicated. Like, you could probably replicate GPT4 on a thousand lines of code, you know, but the code itself, a couple of lines can make like a 20% difference in performance. That's the insane part. Once the model is built, it's kind of out there and then you can just use it. And so I think you were seeing, again, this is more an engineering challenge and it's attracting engineers and infrastructure people who are thinking about some of the real things that are happening with AI as it's starting to enter the workforce.
00:17:32.624 - 00:18:05.493, Speaker D: Like in some areas, Harvard just did a study up to 50% of the workforce are using it a little bit, but it's not quite there yet because it needed accuracy. It needs a chain of thought reasoning for wide distribution. But when I look at the future, what's the probability that every single one of your medical diagnoses won't be checked by an AI? 0. It'll be a malpractice. When is that? I don't know. Things like that are almost inevitable now and that becomes really interesting as people come and start to build. And again, I think there's some good people being attracted by the crypto side both from a business model perspective, a coordination perspective, verifiability perspective.
00:18:05.493 - 00:18:23.275, Speaker D: And almost like you had defi, you know, this decentralized finance, you're moving towards intelligent knowledge data transfer and you're looking for some rails for that. But no one's quite sure how it is yet because hasn't standardized the base models yet even like you're just using Soto after. So as opposed to stuff that's tried and tested.
00:18:24.455 - 00:18:29.023, Speaker B: Interesting. And I'm curious, why did you choose to build shelling AI on.
00:18:29.119 - 00:18:29.327, Speaker C: On.
00:18:29.351 - 00:18:46.575, Speaker B: On crypto Rails? I guess I know stability was already. You had these plans to be like a dao of daos, but you, you presumably had had the opportunity to raise from traditional VC and do a do do a kind of traditional company. Why did you decide to go with, go with the crypto rails for shelling? Like what's, what's the big vision there?
00:18:47.435 - 00:19:22.449, Speaker D: So when stability was starting out, we had these communities with hundreds of thousands of people, a Luther AI lion and others, and we were building all these models and I was like, you know, it'd be really great to turn into a dao daos, you know, then you can have more companies, organization, look at the various bits. But then I looked at it and I was like, this is direct democracy at the worst. There's no intelligence there. And we kind of got to build the constitutional AIs and other things to make the communities not basically turned complete degenerating crazy. I mean they're still great developer communities. Right. Then I went the traditional route and then I ended up doing SaaS and other things and you know, made millions of dollars a month in revenue and stuff like that.
00:19:22.449 - 00:19:54.049, Speaker D: And I was like, does this really what I want to do? And I thought that there was a category of models, the stuff for education, healthcare, government, finance, that need open source and open data because you can poison them otherwise. Right. They need to be localized. And then when I looked at the success of crypto and the requirements of what you need for that, it was just one thing, which was compute, basically. And when people talk about these big companies being successful, I think we have the biggest example of success ever, which is bitcoin. Bitcoin's energy usage every year is 160 terawatt hours. All the data centers in the world are 360.
00:19:54.049 - 00:20:22.565, Speaker D: And it provisioned a crap ton of specialist compute. So we're really looking at kind of some of those models. How can you provision lots of compute and use it to organize the world's council knowledge and more to create a public commons. And we've got more details of that soon. But you can't play the same game as other people, which is right now. A lot of the gigantic investment in AI is just in case there's a bit of fomo. But it's also a fear of like making a mistake.
00:20:22.565 - 00:20:37.015, Speaker D: This is what you see with Sundar Pichai and Mark Zuckerberg and others are like, we're building these giant clusters because what if we make a mistake and they're actually needed and they're not? And so I think that becomes very interesting. Yeah, I think you can build more intelligent communities, more intelligent infrastructure. Now that's super exciting.
00:20:38.595 - 00:21:04.045, Speaker B: Nice. Yeah. And on the memetic side, it does seem like a lot of people obviously, like Truth Terminal has been. Has been super interesting. And Goat, it does seem like a lot of people are just betting on the memetic impact of crypto AI. I'm curious how and even entrepreneurs are kind of seeing that to the extent that there's not really a way to bet on AI or a direct way to bet on AI in public markets. That feels like the hyperscalers are huge.
00:21:04.045 - 00:21:25.585, Speaker B: And then there's some smaller companies, whatever Palantir or something, but there's not really direct AI bets. And so people seem to be looking for this in crypto. To what extent do you think this is driving a lot of the investment here and to what extent is this interesting? Yeah, maybe you could speak generally on that. I don't know, Kyle, if you have a take there.
00:21:27.485 - 00:21:29.477, Speaker C: I'm not sure exactly I understand the question.
00:21:29.581 - 00:21:54.497, Speaker B: Yeah, I guess like a lot of people are. Seem to be. Or there's a lot of definitely attraction to crypto AI just because they think that these coins are going to trade like meme coins on the success of AI. Right. Like Beta Nvidia or something like this. I don't know if you see like the Alexander good take on this or something like these things will take over memes because AI is the best meme of all. Like, this is a once in, once in a history moment.
00:21:54.497 - 00:22:06.605, Speaker B: And so these coins are just gonna, just gonna trade as beta Nvidia or something like this. It seems like pretty tough to see that happen. But I'm curious if you have a take on that or if you think it's just Hopium.
00:22:07.745 - 00:22:48.865, Speaker C: I mean, I think there's, I think there is some element of that to how they have traded. Render has at times kind of traded or based around how it seemed like Nvidia was doing around Nvidia earnings. I'm not saying that it should, to be clear. I'm just saying empirically, a lot of people have made that observation. And the other kind of obvious one is World Coin and OpenAI. Obviously OpenAI does not have earnings, but whenever OpenAI has had major events of sorts, there's typically been a flurry of activity around World Coin. So I think the market is looking for proxies.
00:22:48.865 - 00:23:03.505, Speaker C: I don't think it has very good ones. One thing crypto's taught me is memes can last a lot longer than you think they can. So I don't think this behavior will end.
00:23:05.605 - 00:23:44.685, Speaker D: Yeah, I think it becomes very interesting as well. You look at memes, you look at community and you move a bit more into crypto X AI here, like the deployment of AI within a lot of these coins and networks, from abstracting away smart contract security and building through to memes for the community, content for the community and more. I think that's just starting right now. And that's going to be insane. Like, you know, if you look at Haden's new project, like in the future, you won't know if it's me or it's my AI. That's literally on this talk. They've got something.
00:23:44.685 - 00:24:21.985, Speaker D: We can send the AI for your Zoom, but then you can use 11 labs to translate it into 40 different languages and you can personalize for every single user. So you think about crypto community building and how cults build around currencies and things like that. That could be insane and ridiculous because you suddenly have an army of AIs AI. Oh, well, again, crypto religions enhanced by AI. And then the flip side of that is there probably will be one or two AI religions, as it were. Like, again, I'm surprised it hasn't happened yet. Someone trying to build the singularity and gathering all the people around them on that like EAC Coin or something like that.
00:24:21.985 - 00:24:35.295, Speaker D: But I think the application will be the most impactful thing on the other side Like I said, within the meme area, like we already see teams working really hard. They'll just be multiplied, which is going to be weird.
00:24:36.675 - 00:24:59.535, Speaker B: Yeah, okay, maybe we can get a bit more granular and go into specific sectors. You said you're bullish on the decentralized GPU marketplace. Kyle Ahmad, what do you think of that? Is that something you mentioned, decentralized training? It's going to be hard to get that anywhere near as efficient as a centralized training. If so, how do decentralized GPU marketplaces win?
00:25:00.435 - 00:25:31.811, Speaker D: So I think these models are like graduates and you do the big pre training on a giant supercomputer, but then the tuning only takes up 2.5percent of that total amount of compute. So decentralized marketplaces are ideal for swarms of models creating synthetic customized data, for example. They're good for again, once you standardize a base model, you don't really want to create a new curriculum for a graduate. Again and again and again. This is like stable diffusion. When we released stable diffusion 1.5,
00:25:31.811 - 00:26:14.057, Speaker D: people are still using it. Now it's like a game engine, it does the job. This is the thing. I think the swarms have become useful for that because right now one of the problems is that you're lacking the service level agreements and more on that. So one, that will be helped by crypto, native AI applications, and the ones that focus on decentralization, central resistance and more. But two will be helped by the standardization. It's much easier to send a package out to do a translation of a video archive into 100 languages once you standardize that stack and you can test that it works on all the different hardware and you know how it works versus right now a lot of the deep end marketplaces are trying to attract AI developers and more.
00:26:14.057 - 00:26:34.095, Speaker D: And they're like, where are the service level agreements? What if something goes wrong? Would rather pay the extra premium to stick with a Lambda or Corweave or even an Amazon or more. So I think that again, the usage will kick off with inference time, compute, standardization and a bit of an abstraction away. I'm sure Carl's seen more interesting stuff than that around this as well.
00:26:39.875 - 00:26:59.005, Speaker C: I guess I'll chime in here. So I agree, you need to have standardization. And when I look at teams like IO, Cusco, Render and others, I see them focusing on standardized models. And so there's llama 3 to 3.1, 3.2, stable diffusion, 1.5, you know, whatever.
00:26:59.005 - 00:27:55.445, Speaker C: And obviously they test each model against every major hardware configuration. 3880s, 3090s, 4880s, 4090s, A1 hundreds, H1 hundreds, et cetera, to see how they perform and such and to categorize these things so that they can have some default expectations around how amount of time it will take and such to run each model. Generally I think there's going to be obviously lots of models and look, not everyone will upgrade to the latest model Every time llama 3.3 comes out or whatever. So you'll need to support some older stuff. But I generally don't think that's a big problem. You'll have different pieces of Hardware and look, one computer is not going to have 75 different models downloaded on it just because the storage of the models is going to be too high.
00:27:55.445 - 00:28:18.415, Speaker C: But you'll have one computer will have whatever two to seven models stored on it and can run. Obviously needs to signal to the orchestration layer, hey, I have these models available and I'm ready to roll right now. Send me a job, please. Overall, not super worried about, about that, that kind of class of problem. I think it's fairly manageable.
00:28:19.435 - 00:28:42.485, Speaker B: And why do you think thus far, I guess decentralized GPU marketplaces haven't hit product market fit and like if I told you I don't know, two years ago or I think that was two or three years ago, you guys did the render investment that we'd have this AI boom, would you not have thought that these decentralized GPU marketplaces would have picked up more of that traction or. Yeah. I'm just curious how you, how you assess that.
00:28:43.065 - 00:29:19.875, Speaker C: Yeah, I'm not sure that I agree with your assessment. I was just talking with the IO guys last week and you know, they're at an eight figure, you know, run rate right now for network earnings and you know, core weave and those guys are what, like 500 million or something? So like it's probably still a delta of 20x but like that's not totally crazy in my opinion. And they're growing super fast. I know they're all their on chain earnings aren't quite there yet. I think they'll come online on chain in the next few months or so. But like it's working. That doesn't mean there aren't challenges.
00:29:19.875 - 00:30:09.259, Speaker C: To be clear, there definitely are. And I think, you know, the customers looking, considering whether it's IO or Cusco or something else are they have constraints. The obvious one is security. If there's any sensitive data, it's going to be hard, you know, to do that in a decentralized setting for now and then the other one is just, you know, I think is latency. Again, I think these decentralized networks as of today cannot provide. They can in some instances actually provide lower latency, but it's hard for them to provide SLAs around lower latency because it's like, yeah, well look, if there's a computer, whatever, five miles away from where the guy requests the job, that's great and it should be able to provide lower latency. However, if that computer is not available right now, then, you know, it can't.
00:30:09.259 - 00:31:09.227, Speaker C: And so the orchestration, no one has gotten the orchestration overhead low enough and the scale of supply large enough that you could actually make a statistically confident claim around having an SLA. My guess is we're minimum 12 months away from that and quite possibly 24 or even 36 months away from that. And so I understand the general reticence among customers, the customers who are doing workloads on IO and Cusco and these kinds of networks today are almost universally latency insensitive, which is like the correct thing to optimize for. All of these guys are moving towards SLAs and they'll get there eventually. That'll unlock a big segment of the market. And then obviously on the privacy and confidentiality side, you know, that's going to require tees and hardware. There's just no way around that.
00:31:09.227 - 00:31:28.185, Speaker C: I know Nvidia has that in some chips. I don't remember which chips they have or not. The documentation around it is fairly light from what I understand. I've seen almost no discussion of trusted execution in GPUs, but I expect that will become a thing in the coming years.
00:31:29.285 - 00:32:11.121, Speaker D: Yeah, most of the latest generation chips have that in one way or another, either via CPU or even on device or the mobile side. I think that what Carl said about synchronous and asynchronous is really vital as well. Like we're used to using generative AI in a synchronous manner. You type to ChatGPT, it comes back again. The evolution of O1 type chain of thought reasoning models means that Deepin is exactly the right place to be because you want to have a job that will take a day on a 4090 or hours, not seconds. The orchestration becomes very different when that starts occurring as well as the security footprint and more because again, it can be encapsulated. So I think that you'll see that and I think there's two main areas here.
00:32:11.121 - 00:32:51.411, Speaker D: One is this logic knowledge type flow, the other is media. And I think that one of the big things here is being able to upgrade entire media archives. That's a huge async job translated into 100 languages where you look at the video models that are coming out now, generating every pixel. There's just a huge amount there that there just isn't enough compute capacity from anyone. I mean already the compute capacity was such that Microsoft had to lease off Corweave, who originally a bitcoin miner, right? Like OpenAI now has gone to. Was it Crusoe who again were originally a bitcoin miner. Like the bitcoin infrastructure is coming in useful here, but even the biggest guys can't get compute.
00:32:51.411 - 00:33:10.355, Speaker D: There is questions whether or not there'll be an overhang next year of H1 hundreds, but that's because everyone's moved to the next generation chip, the B1 hundreds, which have the TES that have all these things. Just like the latest Apple phones are far more reliable, just like the latest series of GPUs will be far more reliable and they'll work more seamlessly.
00:33:13.575 - 00:33:28.005, Speaker B: Interesting. Okay, is there anything else we want to touch on on the decentralized compute or do you think it's worth discussing or should we move on to data car? What do you. Yeah, you seem like you're. You got something there.
00:33:29.425 - 00:34:31.325, Speaker C: I think we've covered it on. So the last comment I want to make in compute that I think is very important is like I think in 20 years when we look back on the transformer and kind of what this represents, it's like the vast majority of compute that happened in the world prior to the kind of transformer breakthrough was deterministic. Obviously there was still some non deterministic stuff in machine learning. It was mostly server side. I mean, iOS has some ML in there for photo optimization and some other things, but I would not say that non deterministic compute was a fundamental part of the user experience. For sure, most consumers, you got to interpret that statement fairly broadly. And I think, you know, over the next few years, non determinism via the transformers is going to basically layer into every part of the workflow, both for consumer and enterprise.
00:34:31.325 - 00:35:16.565, Speaker C: And at least as of now, based on how transformers work, the sheer amount of compute demand in these non deterministic systems has increased so much that I basically think that there's like a fundamental structural shortage in the entire silicon supply chain. And if that broad categorization is correct, then that means we have to figure out how to financialize compute and turn it into a functioning commodity market. Which again today it is just not at all. And I feel fairly optimistic that that would be the case. So I just want to leave that concluding thought. Happy to go to David now.
00:35:17.065 - 00:35:56.611, Speaker B: Yeah. Actually, as a follow up to that, how do you think the game theory of this plays out? Because this seems like a world like the Transformers revolution and the scaling laws is a world that's perfectly suited for the hyperscalers. Right. They have all the compute, they have the cloud businesses that the resources keep investing in. This is open source AI and crypto, sort of by association. Just reliant on Zuckerberg continuing to open source Llama or Nvidia, whatever, continue to open source their model. Or do you see a world where you can actually organize with these decentralized GPU marketplaces to have like matching scale? Yeah.
00:35:56.611 - 00:35:58.315, Speaker B: Curious how you think of the game theory.
00:35:58.355 - 00:35:59.811, Speaker C: You mean on the. On the model development?
00:35:59.883 - 00:36:01.455, Speaker B: Yeah, on the model development side.
00:36:02.275 - 00:36:35.909, Speaker C: Oh yeah. I don't think that there's any world in which decentralized model creation matters because of like the sheer scale of human capital required to build these things. And you need selling points. Meta is obviously a selling point. Google is obviously a selling point. OpenAI is obviously a selling point. And like, yeah, so it's just hard to see that, you know, and like the selling point is not just like strictly memetic, like with Facebook.
00:36:35.909 - 00:37:19.955, Speaker C: It is like obviously justified in that, you know, that they're using Llama in all of the Facebook products. And so you know that like Facebook is getting real world feedback and obviously incorporating that into updated models and like, you know, random group of guys on the Internet, like cannot replicate that. Obviously the same is true with Google and others. And so I do think that at the level of core model development, no questions asked in my mind, that that is completely centralized. I think it has to be. Now there's things I think perhaps on the edges you can do around that to optimize. There may be certain use cases you want to optimize for and other things, drag and whatever, fine tuning and stuff.
00:37:19.955 - 00:37:27.765, Speaker C: But core R and D, no world in which I see that that decentralizes.
00:37:29.265 - 00:38:02.587, Speaker D: Yeah, I think that the compute necessarily not decentralized because centralized compute will always outperform. But I think you can still build models that are highly competitive. And we built best models of multiple categories on a fracture and the resources, because models need to be fit for purpose. And there's a model that works on your phone that will never require a million GPUs worth of compute, you know, or a stable diffusion level. Model it costs maybe 5 million total when we built it. Now you can do the same probably for $20,000 again for a task AGI, I think. And these very complicated models.
00:38:02.587 - 00:38:59.077, Speaker D: Yeah, that's incredibly difficult to keep up with unless you have a bitcoin type boom. But I will call my company Shelling for a Reason as a shelling point, because we're looking at a particular category of models which is very interesting, which is the models that are needed for every regulated sector. And it's our belief that the data sets for those will have to be open because of model poisoning of data and more things for healthcare, education, government, finance and more. And those don't need to be state of the art, which is the interesting thing because as you move from state of the art to one generation behind, it's literally an exponential increase in compute or like an order of magnitude or 2 increase in compute. Like when we bought on our cluster in 2022 it was 40 petaflops and it was the 10th fastest in the world on the top 500 list, which is a bit crazy. That was engineering feat. Elon Musk's new cluster is 2000 petaflops to give you an idea of that increase and his cost after that will probably be 5 to 10,000.
00:38:59.077 - 00:39:41.025, Speaker D: So it's very difficult to keep up with State of the art, but one generation behind, I think you can. And again when we think about the model that teaches the kids or healthcare, all the data for that is common knowledge. And then it's a coordination question which again I find absolutely fascinating. Then you've got the models that are like open weights, like llama Meta has 350,000 H1 hundreds and 35,000 for training. A 10% improvement in the speed of inference pays for itself. So that's one of the main reasons they're doing that right as well as intermediating requests. And then you've got these genius models, the OpenAI's and Geminis and others of the world and that's an incredibly competitive crazy area of PhD level already.
00:39:41.025 - 00:40:24.685, Speaker D: But you're competing against somewhat non economic actors because my view is for example Google will go free on Gemini just like Gmail, Enterprise, Upsell and that gets a bit crazy when you look at that type of thing. So I think that there is a space for decentralized stuff, but it's probably over in the smaller models that are fit for the edge as opposed to these genius models. And it's probably again an interesting area as we think about the models that run our healthcare Service, education, finance, even that will probably need open data as well. And so that's one of the areas that we're really looking at because Meta and OpenAI and others are not building those models. They will never give up their data.
00:40:26.305 - 00:40:53.247, Speaker B: Makes sense. That leads us nicely to talk about data. So obviously data has been hyped for a while. I remember the McKinsey reports on data is the new oil from like 10 years ago and they were kind of right, directionally. But it turns out these models just trained on the entire open Internet, right. And they've now trained on the open Internet several times. And we have this, this problem of the, of the data wall, right, where they're not going for artificial data or using companies like scale and this stuff.
00:40:53.247 - 00:41:35.813, Speaker B: And we've seen a few crypto startups come in into this, into this area to try and either like bootstrap supply size of people providing data like, you know, grass people scraping the Internet. We've seen a few others tackle like private data. So allowing people to train on, on, on private data, which, you know, is, is supposedly a much bigger market than kind of sort of the next frontier in terms of training. I'm curious in general how you guys feel about the sector. I know, Kyle, you already mentioned you don't think there's startup opportunities there or startup scale opportunities there, maybe. Ahmad, do you, do you disagree? You seem bullish on the datadaos idea initially. Yeah.
00:41:35.813 - 00:41:37.465, Speaker B: How are you thinking about this?
00:41:38.245 - 00:42:19.775, Speaker D: I think it's very hard, but there is an opportunity there when you think about interoperability of data and the nature of how data interacts with models as well, like with the large context windows. Like you've seen Notebook LM by Google. That's a single inference on all the data that you put in. Gemini can handle 2 million words and actually you can get a high level of determinism even on the output and that opens up new ways to trade and exchange data. But we've been trying to do that for a while in crypto and not really succeeded. Like I think you need the AI to help and it's a big task and some people are looking and doing things interestingly. But you have seen efficiency gains like I think Sapien kind of helped Mid Journey and others and they have shown an increase in that.
00:42:19.775 - 00:42:58.805, Speaker D: But it's mostly around like payment coordination and things like that that we're seeing at the moment as opposed to what are the rails for data to flow intelligently and turn into knowledge. And I think that the models don't have enough Data arguments. Some are overwrought. One of the interesting things is if you look at Phi by Microsoft, it had textbooks as its input and then it came out very boring on the outputs and scored very low. Then they added a snapshot of the Internet and it got better even with the knowledge. But the datasets are getting better and better and better. And so at a certain point why do you need more? Unless it's some of this expert stuff like kind of Carl has said, or it's niche like the hive mappers and others of the world.
00:43:01.585 - 00:43:31.045, Speaker C: Yeah, I'm fairly skeptical of the datadow thing. Not because it's like a bad idea, but I just on the high end I know you need like way too much like customization, like not customization, it's not permissionless people on the Internet contributing. If you look at the stuff that like it's large scale consumer data.
00:43:32.865 - 00:43:33.153, Speaker D: I.
00:43:33.169 - 00:44:12.577, Speaker C: Guess there's just not that many categories of it in which I think you can aggregate effectively. Medical data is one that's frequently highlighted and this one is just blatantly wrong. Meaning like there's large amounts of medical data that are open and shared today for medical research purposes coming from various health systems. The way it's done today is it's just all de anonymized. As long as there's no pii, then you can take all of that longitudinal health data and share it. And this is how Google's Med, Palm and a bunch of other things were already all trained on these open system open data sets and, and research. Obviously Pharma, you know, buy a lot of this stuff today.
00:44:12.577 - 00:45:06.085, Speaker C: So you know that, that area I'm generally skeptical of. You know, we have theories on this of like oh, consumers can upload purchase intent data and stuff like that. I'm fairly, it's one that's like a potentially high signal like form of data that you think could be valuable. The kind of most obvious way you'd derive value from that would be hey, if a million people upload their Amazon purchase history, can we predict earnings more effectively for certain businesses and like buy short dated call options around earnings? That that's like the like most directly obvious way to profit from data. Hard to imagine anything more direct than that. And like we tried that investment, it was called Alfia and like because of the SEC United States, you just can't do that. As a registered investment advisor, like it just does not work.
00:45:06.085 - 00:45:28.485, Speaker C: Maybe it could work otherwise, but currently does not. We haven't seen anything else where we're like really Convinced that the uploading of the data is like what's going to move the needle. And then we've kind of used that as our baseline point because it's the one that's just like the most directly monetizable.
00:45:29.665 - 00:46:08.503, Speaker D: I think the area that I'm looking is not so much the knowledge stuff, because the knowledge is generally available. Again, do you need to show a base model more than you show a graduate of a top university throughout their entire life? Not really, no. Like right now we're overdoing it on the data side. I think the interesting thing is this category of data, of IP rich data, particularly on the entertainment side, because a lot of the IP holders and more are going to be increasingly challenged as the bar or the floor for creation goes dramatically changed. Right. Like you'll be able to build a Hollywood level movie in two to three years as a single person and have full control over all the scenes. That's insane.
00:46:08.503 - 00:46:40.505, Speaker D: And so again, like, systems only really tend to change when there is an urgent need for it. A couple of years ago, Hollywood was like a year ago, they're like, now they're like panicking because you try some of these models, they can do crazy stuff. And so IP rights holders are open and they don't want to do deals necessarily to give up their data to OpenAI and others. Again, that's been very rare to see that. I think it has only been the runaway Lionsgate deal that we've seen. But even there, Lionsgate retain full control of their IP and their assets in the model.
00:46:41.005 - 00:46:45.305, Speaker B: What data are you referring to here? Like what IP would they be given out to? Obama?
00:46:45.965 - 00:47:19.471, Speaker D: SpongeBob SquarePants? Entertainment. The actual characters, basically, yeah. And the giant archives of those characters. Again, I think the AI that we have is the stuff we need for living and the stuff we need for entertainment and the quality of entertainment will go up, but then people will be able to utilize their IP in more and interesting ways. Like, I think that is an open area because the market is open there, because the market's quite scared. Whereas medical stuff, there's not the same urgency. Like you have a benefit, but there isn't a danger on the other side.
00:47:19.471 - 00:47:25.615, Speaker D: The entire entertainment industry is going to get shifted by the cost of creation dropping down.
00:47:26.555 - 00:47:46.595, Speaker B: Makes sense. What about Private Days? Because I think there's two parts that are interesting there. The first one is for like your personal AI. You know, if you're going to be given a bunch of your chat data or your health data, whatever it might be, you probably want some level of privacy around that. Then the second one is maybe one that touches on a previous point you made, Kyle, about the investment. Investment memos. Right.
00:47:46.595 - 00:47:58.075, Speaker B: Like AIs right now I'm sure you've tried. We have an AI bot that serves as the first filter on Dex for us and it does a good job. But for it to be able to produce an investment memo, it's like very, they're very far from that.
00:47:58.115 - 00:47:58.291, Speaker C: Right.
00:47:58.323 - 00:48:38.585, Speaker B: And they would need a lot of expert data, a lot of like well done investment memos, a lot of financial data to be able to do that. How do you actually encourage a bunch of investment funds or whoever to actually provide that data to train a model do you not need? Because, you know, probably a lot of those, those funds won't want to give up their data to OpenAI or whoever. This is where, you know, potentially I feel like private data might be interesting where there's these pockets of expert data that there's no. That people don't really want to give up and that you also need like scale. So you can't just have one fund. You probably need like quite a few to contribute for the, for the performance gains to be meaningful.
00:48:40.165 - 00:49:23.977, Speaker C: Yeah, I don't know what the solution is. Again, I'm fairly skeptical that it works. There's like a fundamental attributability problem. It's like, okay, you get 500 investment firms to each upload 500 investment memos, whatever. But that's not all. If we're talking about building a super smart model, it's like, okay, well part of it's investments, but then part of it's going to be reading memos from like history and other stuff. And like, yeah, I don't understand how you do attributability in any way that's legible.
00:49:23.977 - 00:50:03.401, Speaker C: And so I'm just like skeptical around that. It's also like not clear how much money are you going to make. Like that's the other problem is like if OpenAI called us today and they're like multicoin, we'll pay you $1 million for your investment. Honestly, like, I don't know if we're going to take that deal. Like, I'm not going to outright say no, but I'm just telling you like, even at $1 million, it's like not clear to me that that's worth it. You know, there are some number which it's like, okay, I stopped thinking and I say yes, but I'm not sure. Open air condition.
00:50:03.401 - 00:50:11.305, Speaker C: By paying us $1 million for all of our historical memos. So it's just, it's just hard. It's just really hard.
00:50:12.085 - 00:50:51.715, Speaker D: Yeah, I think models get better as they scale up anyway on the context and more and ultimately it's bringing the models to where your data is. So like a lot of these edge ones will compete, have to compete against Apple and most people just trust Apple and Apple's models on device. But if you look at investment memos, for example, if you use Google Workspace, you've got Gemini and you can just literally plug your Google Drive in there, right? And it can do notebook lm, it can make a podcast. Why can't it make an investment memo within the next year? And so like they only need to train it once. In fact, the original GPT paper was language models are few shot learners. They can learn very quickly and now they have planning and more, they can learn really, really quickly. You can tell it where it makes mistakes like a grad.
00:50:51.715 - 00:51:47.005, Speaker D: So I think that's kind of a tough one. Again, I think the area that I find the most value on data is this entertainment area. Again, things like the genetics dao and things like that could be interesting but I think entertainment is the clearest shot line, shall we say? Because a lot of this stuff around people contributing, particularly as you get now fully licensed models, people were like inputs for stable diffusion. What if we paid everyone a little bit? I'd be like, okay, It'd probably take three years for them to get $1 for the most widely used person. Once you work out the mathematics and then you'll have Adobe that just used the Adobe archive to make a fully licensed version of that and the mathematics just doesn't work out for lots of people sharing as opposed to again this IP rich area where it does make a bit more sense because you want more people to use it, but in a way that's controlled, you want to bring that stuff in.
00:51:48.265 - 00:52:10.963, Speaker B: Okay, makes sense. All right, to finish off, I would like to get your quick take on a few kind of sectors within crypto AI so maybe we can keep them to under a minute around 30 second takes on each one of these. So decentralized inference. What do both of you think on this? Yeah, interesting.
00:52:11.099 - 00:52:19.193, Speaker D: Yeah, yeah. I think as the models become standardized and that it's a massive area, particularly as we got inference time computer.
00:52:19.239 - 00:52:23.785, Speaker B: What about verifiable inference like the. Yeah.
00:52:25.685 - 00:52:43.705, Speaker D: I just kick in, I think that's going to be less and less important given a lot of the tasks that you need asynchronous stuff for and the advances in the chips as well plus the determinism of structured JSON outputs. It's not as big a challenge for 80% of the use cases as seen.
00:52:45.735 - 00:52:57.047, Speaker C: I think the vast majority of customers would prefer lower cost to verifiability. I think with TES and other stuff we'll get to verifiability as well. Just not worried about it.
00:52:57.191 - 00:53:08.195, Speaker B: How do you see us being able to like smart contracts being able to tap into models then in their like in a trustless way. Do you not think verifiable inference sort of needed for that or do you just not think it's that's a big enough market?
00:53:08.775 - 00:53:47.545, Speaker C: Oh I think that market doesn't exist at. There's the only exception to that I've seen is really is like actual world coin where they like the results of the ML thing Like if the iris scan like need to come on chain that's the only good use case I've seen. Everything else boils down to like some update your market making strategy on chain of like your vault system and like that doesn't matter. There are so many other upstream dependencies that require trust in humans that the Oracle part of putting it on chain I'm just like I don't give a shit. It doesn't matter at all.
00:53:51.445 - 00:54:17.835, Speaker D: Yeah, the Oracle part is the most complicated part. And again on chain inference doesn't really make much sense to me. If you look at OpenAI's structured JSON output you do have determinism with language models as well and that's going to roll out just across the board so you can have the different states but the total number of transactions and the cost of those is not a huge market I think relatively speaking again where's the human in the loop and the Oracle problem is going to be much bigger.
00:54:18.855 - 00:54:29.115, Speaker B: Okay, what about agents? Just generally you can take it however you however you want and specifically I guess the crypto version of agents.
00:54:33.325 - 00:55:01.505, Speaker C: I don't understand the agent thing at all. It's like crystal clear to me. Apple is going to have your personal assistant and anyone else you've been trying. I mean look OpenAI and perfectly can try. I think the probability of success is zero. Like you need to be embedded in the operating system. I don't even see apex possible path to building that product without controlling the operating system.
00:55:03.125 - 00:55:43.005, Speaker D: Yeah, I mean like we're looking at the regulated side, the healthcare and others that they will probably fold into their operating system because you need to have the openness there. But where agents of people are looking at it as like Trading bots and intents on steroids effectively and the interaction there. But again, this comes down to the distributed inference. It's basically like O time distributed inference to do a task. I don't think it's that complicated to coordinate the vast majority of tasks. Some tasks are hugely complicated. But then wouldn't you rather do that in a centralized manner? Like if the value of the task goes up exponentially, you want a bit of security and someone to blame versus the decentralized side of that.
00:55:43.005 - 00:55:48.845, Speaker D: And I think the censorship resistant area is small. The censorship resistant area that is just so small.
00:55:50.385 - 00:56:01.695, Speaker B: Interesting. What about the idea that these agents will use crypto rails because they can't get credit cards or bank accounts? Do you see any anything interesting there or is it just hand wavy?
00:56:03.955 - 00:56:25.813, Speaker C: Yeah, so there's strong, strong views here. There are fundamentally two kinds of AIs. Sovereign AIs and non sovereign AIs. Thus far no one has invented a sovereign AI. Despite our funny truth terminal guy. He is in no way sovereignty. Obviously lots of folks on the Internet like to talk about a world of sovereign AIs.
00:56:25.813 - 00:57:12.533, Speaker C: I actually think that world is fairly scary. There's a lot of technical challenges to solve between here and there in terms of like private keys and a whole bunch of other stuff. But anyways, so as of today, all AIs are non sovereign. Therefore all AIs as of today work for people or corporations or maybe like a Dow or something. But my point is that they're like fundamentally subservient to some human agency. And today payments on Internet work between humans and businesses using credit cards. The correct way to enable agents to buy things on your behalf is not by coping them with a private key, it's by giving them your credit card number.
00:57:12.533 - 00:57:53.439, Speaker C: Like that's just like obviously the correct solution because if you want your agent to go buy, you know, plane tickets for you or whatever, like Kayak and United don't accept crypto, but they do accept credit cards. And so like I promise you, Apple Wallet will plug into the Apple Assistant and it will automatically buy things for you. Like this is just like obviously how this plays out for non sovereign AIs. I agree. Sorry for, for sovereign AIs. To the extent they're going to interface with money, it does seem more probable that they will have private keys than they will have credit card numbers. But again, we still don't actually know what those words mean.
00:57:53.439 - 00:57:58.395, Speaker C: Like no one has invented that yet. And I think it's quite a ways away from being invented.
00:57:59.055 - 00:58:10.685, Speaker D: Yeah, I think within Crypto and the on chain activity. It's just usdc. I mean again, we built Defi already. There are rails that's available on the chains. Why wouldn't it transact with that and use Uniswap or whatever?
00:58:11.745 - 00:58:20.601, Speaker B: But how, I guess to put the. With the private key issue and stuff like this, or you see it is that a human will instruct the AI.
00:58:20.673 - 00:58:22.457, Speaker D: To go and do the stuff.
00:58:22.521 - 00:58:22.737, Speaker B: Yeah.
00:58:22.761 - 00:58:34.555, Speaker D: And again, it will have a private key and I think that's an insurmountable challenge to do. People authorize their metamask wallets for all sorts of things, you know, probably more than they should. Just like they authorize their credit card.
00:58:35.255 - 00:59:00.435, Speaker B: Okay. And just to make sure I understand your view on, on the agent stuff, Kyle. So you think Apple will like, you know, Siri will just integrate with, will be the orchestration layer for a bunch of different agents and whatever you ask it, it will just find whatever it is the agent, the best agent is to, to fill that request and then just charge a fee, something like that. Or you think Apple will actually own the agents themselves, have the models? Or is it more like an app store thing?
00:59:01.855 - 00:59:51.719, Speaker C: Oh, I think the Apple intelligence thing becomes very sophisticated. Again, there's no world four years from now in which Apple does not offer some sort of upcharge. There will be like a free version for sure. And then there will be like, you know, premium tiers or whatever of like personal assistant, virtual assistant thing. And if you're like, hey, you know, go book me all these tickets and whatever, you know, it'll go do all that stuff for you and you'll be able to, you know, probably just look at your screen with an email and you're like, I want to return this. And you'll just like, you know, hold the Siri button and say like, hey, return this package. And it'll like go to Amazon and figure out the return thing and it'll like send you the QR code and then when you get to the shipping center, it'll like, the notification will pop up and it's like, here's a QR code to ship the thing back, whatever.
00:59:51.719 - 01:00:48.065, Speaker C: Like Apple and Google For Android in 100% of future configurations of humanity solves all of those problems and they'll offer some tiering prices for various levels of that service because it's the obvious way for them to produce meaningful MRR from each customer. Yeah, yeah, they're going to just crush all that stuff. And you need direct access to the operating system to do this. There is no Other configuration. Now to what extent that those models from Apple and Google call other models, that's tbd and that's like harder to reason about. I think it will not be 100% insourced. Like I think they will interface with some third parties or have some way for their models to call other models and other search engines.
01:00:48.065 - 01:00:55.617, Speaker C: I think that's quite likely. Yeah.
01:00:55.641 - 01:01:18.349, Speaker D: I mean it's. How do you interface with your intent? Right, you do it through your phone and if there's something that can do the job, then you'll just use it to do the job. Like if you're looking at fully decentralized, you're looking at fully distributed. There are certain use cases for that. But not buying your plane tickets. You know, it just needs to work and needs to be trusted. And Google one and Apple one are the ideal initial things for that.
01:01:18.349 - 01:01:34.865, Speaker D: There are other areas like again the regulated service industry around stuff like defi stuff that could be interesting. And I think those are agent things. It's just what are the sizes of these markets when you can do a lot of stuff through just intelligence on a system like that to be deep.
01:01:35.295 - 01:01:44.195, Speaker B: Okay, last quick fire Identity like the world coin. Go on, expand a little bit.
01:01:44.935 - 01:02:13.635, Speaker C: I have an old Twitter thread on this. Identity is not a first order construct. Identity is a second order construct. Meaning your Instagram handle only matters because there's 2 billion people who use Instagram. Like what imbues the Taylor Swift Instagram handle with value is the fact that 2 billion people use Taylor Swift. And if they type to Taylor Swift in the search box, then like it needs to be her. Everything that matters in identity is everything other than the identity itself.
01:02:13.635 - 01:02:15.955, Speaker C: Identity is strictly contextual.
01:02:16.935 - 01:02:49.325, Speaker D: Yeah, I think that's the functional identity construct and then foundational. I think what could be interesting on the identity side is pseudonymous identity as you have intent based agents. Again within the crypto world, shall we say that ecosystem, they go out and do tasks and build up reputation. But that's not what worldcoin and many of these other initiatives are trying to do. Like I think scanning eyeballs makes no real sense. I think it is in context. And Apple ID is very good identity, honestly, Google ID is very good identity as well.
01:02:50.465 - 01:03:17.513, Speaker B: Yep. Okay, so to conclude, thanks very much for this. This was awesome. We should have probably done more of the quickfire. That was, that was spicy. I'd love to get each of your takes on what vertical within crypto AI do you think is most likely to hit product market fit first? And we could define product market fit here as, you know, high tens of billions of revenue or active users or something like this. I guess you'd argue decentralized GPUs are already there, Kyle, but maybe.
01:03:17.513 - 01:03:19.765, Speaker B: Yeah, Emad, what do you think?
01:03:20.625 - 01:03:21.445, Speaker D: Memes.
01:03:22.305 - 01:03:23.085, Speaker B: Memes.
01:03:23.825 - 01:03:32.255, Speaker D: Meme currency. Yeah, it'll be stupid, like Rising tide. Like, is it sustainable? Who knows? But definitely, that's product market fit about to hit.
01:03:33.155 - 01:03:37.955, Speaker B: All right, I appreciate the time. This was awesome. Cheers, guys. Thanks very much for the time and.
01:03:37.995 - 01:03:38.355, Speaker C: Speak to you soon.
