00:00:01.400 - 00:00:03.725, Speaker A: You're now plugged into the Delphi Podcast.
00:00:05.265 - 00:00:48.309, Speaker B: Hey, guys. So Jose Maria Macedo here, hosting a special episode of the Delphi Podcast with special guest Ilya from Near. We're actually announcing our Crypto XAI accelerator with Near Delphi Labs and Near Crypto XAI Accelerator. And so to celebrate this, we thought we'd bring Ilya on the podcast. Obviously, for those who don't know, he's one of the authors of the original Transformers paper that was pretty much the predecessor of today's LLMs. So we want to cover a bunch of stuff. We want to cover Ilya's background, which I'm actually really interested in, his views on AI, the Crypto X AI thesis, and also what we're hoping to achieve with the accelerator and why people should.
00:00:48.309 - 00:01:08.409, Speaker B: Why founders should be applying to this, to this, basically, and why we think it's the right time to do this. So to start with, wanted to talk a little bit about your background. So I don't actually know too much about it. I know you're Ukrainian and you studied there, but somehow ended up with Google and then started near. So, like, what's the story there?
00:01:08.577 - 00:01:17.325, Speaker A: Yeah, so I was actually interested in kind of AI machine learning from pretty early on, I think, like from 10 or 11 years old.
00:01:18.305 - 00:01:19.205, Speaker B: Classic.
00:01:20.475 - 00:01:54.441, Speaker A: And I was trying to build like my first neural networks when I was like 14 or something. And so I went to study applied math. But I'm also like, from, you know, not very well off family. So I was looking for a job right away. And kind of through set of circumstances, I got a job for an American company that had an office in Kharkiv in Ukraine that was doing machine learning. And so that. That company actually had been doing machine learning for at that point, like 35 years or something.
00:01:54.441 - 00:02:00.161, Speaker A: So they started when there. It wasn't even the machine learning was called econometrics. Wow. And so it was.
00:02:00.193 - 00:02:01.737, Speaker B: This was the software systems.
00:02:01.841 - 00:02:35.675, Speaker A: Yeah, software systems, yeah. And so it was a huge learning opportunity because I was like, during the day in school, pretty much learning, you know, algebra, like discrete math, control systems, et cetera. And then during the evening, pretty much working and doing, you know, decision trees, pretty much regression, like, and the company did a lot of consulting projects. So I would work on, you know, finding oil in oil fields based on data or like detecting cancer based on DNA sequencing. So a lot of. Just like a lot of projects, natural language understanding, etc. So it was really good.
00:02:37.265 - 00:02:42.081, Speaker B: But so this was back when AI was still sort of like rules based, more like econometrics.
00:02:42.113 - 00:03:22.365, Speaker A: And I mean, this is like 2008. So, yeah, most of the machine learning back then, and you know, like, even Google Ads, everything until I think like 2013, 14, was powered by these algorithms. It's all gradient decision trees, boosted decision trees. And so, yeah, so I worked on that. They moved me to us So I moved to San Diego and worked there full time after finishing school. But I ended up going to Silicon Valley pretty much every month because there's conferences, friends, kind of more exciting stuff going on there. And so I'm like, well, I should just move there.
00:03:22.365 - 00:04:03.137, Speaker A: And I applied. I kind of got offers from Facebook and Google. And in Google, I joined research team working on natural language understanding. And to me, that was always kind of the most important part because there's a lot of like, I mean, all of this doing kind of data analysis on, you know, it's interesting problems, but they're not going after, like fundamental human knowledge. There's a lot of image. So back then, actually images were progressing really quickly. And the reason why I actually wanted to join Google was I saw this paper from Andrew Yang and Jeff Dean, which was Cat Neuron.
00:04:03.137 - 00:04:48.881, Speaker A: So this was a first kind of large neural network trained on many, many machines that, without any kind of input data, just by looking at a bunch of images, learned how cats look like. Because on the Internet there's a lot of cats, right? So this was like fully unsupervised learned model. And it was the first one that was like, not trained on one machine. So this was Jeff Dean building first kind of distributed machine learning system. And so that was got me excited because I'm like, okay, well, that is the approach we need to take, right? This was like first kind of revival of neural networks. Again, this was end of 2012, beginning in 2013. And so I'm like, okay, I should do that.
00:04:48.881 - 00:05:23.625, Speaker A: This is like bad for language because everybody can, like a lot of species in the world can see, and only, you know, presumably only us have sophisticated language to exchange knowledge and kind of not anymore. And so for me it was always, yeah, like, how do we, how do we take all of this knowledge we have on Internet? And how do we kind of be able to answer questions and be able to, like, teach machines to answer questions? And so that's why I worked on a team doing question answering. And obviously, like, there's a lot of auxiliary tasks like machine translation, others.
00:05:24.965 - 00:05:37.589, Speaker B: Interesting. And so at what point did you then co author the Transformers paper? Like, how many years had you been at Google? And can you maybe talk a little bit about why that's such a big deal. Like for people who don't know.
00:05:37.757 - 00:06:24.465, Speaker A: Yeah. So when I joined we pretty much there's a progression of like tools we used, right? So we got like, we still were using kind of previous frameworks for these neural networks. They didn't work very well. They were designed for images. And when TensorFlow was coming out, I actually started contributing a lot because like, okay, we're going to use this new framework and we're going to shape it to work for us. And so I was one of the big contributors to TensorFlow, which is a machine learning framework that Google released back in 2015. And what we were doing back then is so called kind of long short term memory neural networks, right.
00:06:24.465 - 00:07:13.755, Speaker A: Lstms. This is a model that kind of reads one word at a time, right? So imagine you're asking a question, it reads the question one word time, and then it starts to read Wikipedia article or whatever article you want to give it as context, right? And so that takes a while because you need to read one word at a time and then it outputs a word like you see right now. And so that models, they worked somewhat like if you train them on a lot of data, but it was really hard to scale them, right. We could not just say like, okay, let's now put parameter space, like, parameter model like 10b, 100b, et cetera. None of that was possible. The challenge we always faced is this. We couldn't launch that in any production because just with the latency until it was able to answer anything was too slow.
00:07:13.755 - 00:08:19.851, Speaker A: And so what we end up doing, and we actually launched that on google.com is a model that would look just at kind of jumble of words in paragraphs and try to answer questions based on that, like, is this paragraph relevant to answering the question? And that's why on Google for a long time you saw just like a paragraph like excerpt trying to answer your question and maybe sometimes highlight the words, right? Is because that model was just like, let's jumble all the words in a paragraph and score how likely this paragraph answers your question. Instead of like a rich read the text and understand what the answer is. And so that was very limiting. And that's kind of what's motivation for the Transformers is like, how do we have a long context models that actually can understand large swaths of texts that can scale, they can run fast and that don't have this kind of lag of. And now we need to spend a ton of time reading all the context. And so the two works pretty much merged, right.
00:08:19.851 - 00:09:20.741, Speaker A: The other team that Jakob was leading, they were doing query similarity. So taking two pretty much queries you tap on Google and comparing how likely they're the same. And so they were doing this as well not by using a model that reads one word at a time, but actually by using a self attention like mechanism where they would just compare kind of each word to each other, to every other word and then score that comparison matrix. And so that is extremely efficient, right? Because there's no kind of recurrent steps, so you can just do it as one matrix multiplication. And so that was kind of the core idea is like the self attention mechanism allowed to have extremely efficient way of kind of gathering context for words from I mean pretty much any length just with one kind of matrix multiplication that you can do. Extremely parallelized, extremely efficient. And so you can think of transformers this model that takes potentially a book of text, right.
00:09:20.741 - 00:09:46.765, Speaker A: And reads every word at the same time. Right. Every word is going into the machine at the same time. It's not like us, you know, read one page at a time, it reads the whole thing, makes all the connections in parallel and then is able to answer any questions, summarize or whatever after that. Right. And that is kind of the power. It's like we going away from kind of how humans work and actually optimizing for the hardware we have, which is extremely paralyzable hardware.
00:09:46.765 - 00:10:16.763, Speaker A: And what kind of. When this paper got published, Ilya Sutzkeverthan in OpenAI saw this and he was like, this is solving the problem, the long context problem. We're going all in on this. And so he pretty much pivoted the team there to then go and scale this and train it. Very high quality data, et cetera. And that's when we saw GPT2, GPT3 and ChatGPT. Right.
00:10:16.763 - 00:10:22.779, Speaker A: And they did a major job kind of putting it all together into like a really powerful system.
00:10:22.947 - 00:10:31.363, Speaker B: That's cool. And out of curiosity, how was it working with like Jeff Dean and some of those AI OGs, I guess at Google any.
00:10:31.539 - 00:11:02.595, Speaker A: Yeah, I mean it's obviously really cool, right? I mean everybody's a person, we know that same in crypto, right. So I think it's not like I'm generally not idolizing anyone, but I mean obviously Jeff Dean extremely. Both extremely intelligent and has like so much context on kind of all the tooling Google build and all the things. So it was always kind of, I mean I didn't work that much with him closely, but Few interactions we had always were exciting.
00:11:03.175 - 00:11:17.101, Speaker B: And from your perspective, I'm curious, like Google having given birth to the, to the Transformers paper, which is I guess the foundational architecture behind LLMs, why didn't Google build ChatGPT first in your view?
00:11:17.253 - 00:12:16.925, Speaker A: I mean, so internally there was a bunch of these, right? They just, so here's kind of the mindset here, right? Like you sit in research, you build something like ChatGPT, right? And it's like it runs on your computer, you can talk to it, et cetera. I mean, there was a guy who got fired thinking that AI got sentient, right? Like they had the chat, this is chat GPT internally, pretty much. They just didn't release it as a product. And so, and so that's where the whole, you know, innovator dilemma comes in. As Google releasing a product that you don't know is going to be like a billion dollar business is really hard because you kind of, you're extending your, you know, your Surface and if it's not a billion dollars, it's not worth it. Like literally not worth it. And so, and then for any startup as we know, right, you don't know right away how to get it to become a billion dollar business, right? It's a lot of pivots and refining to get there.
00:12:16.925 - 00:13:24.775, Speaker A: So that's, that's why Google, you know, for Google, it's easier to acquire a billion dollar business and turn it into a, you know, $100 billion than to launch a novel billion dollar business. Even if people internally can build it, no problem, right? That's kind of the challenge. And the reason why I left was because I wanted to use now these transformers to apply to so called program synthesis, right? Teaching machines to code because I believed, and we see it now, that the programming itself is going to change now that we have more advanced AI tooling and we don't need to now write code. We can just tell machines what to do in natural language and they'll figure out what code to write. Now that requires a product because you want to loop, you want people to write code, you see what doesn't work, et cetera. Like we see this with core service, use this with some other products, right? GitHub, Copilot. And I would, I pretty much didn't expect that Google would be able to release something like this because it's not like immediately obvious that it's a billion dollar business.
00:13:24.775 - 00:13:33.299, Speaker A: And so it would be easier to build the startup and then figure out if it makes sense as part of bigger product suite.
00:13:33.427 - 00:13:45.787, Speaker B: Okay? Interesting. So that gets us. Yeah. I wonder if it's also because they're worried it would eat into search somehow because Google did release a few. A bunch of products like Gmail or a bunch of other, I guess, maps.
00:13:45.851 - 00:13:47.175, Speaker A: Yeah, but that was earlier.
00:13:49.555 - 00:13:50.181, Speaker B: I guess.
00:13:50.283 - 00:14:09.225, Speaker A: Yeah, I mean, internally Google had a search fully powered by neural network. Like, it's not, it's not like, I mean, a small team of people can build that. Like, it's not hard. I mean, when you have all the Google. Google data already there, it's not hard to build that. So it's more. There's also a cost component.
00:14:09.225 - 00:14:41.836, Speaker A: Right. If you think of like, oh, we need to scale it to like billion users right away hammering it. Like, it's just very expensive. So I think the other part was even after ChatGPT, like first release, I've talked with people, it was still a cost question. And I think as this models got faster and cheaper, we saw kind of more willingness to do it. But you realize also like, there's LTV per user and it's way lower than what you Pay to chat GPT right now. It's way lower than $20 on average.
00:14:41.836 - 00:15:00.525, Speaker A: And so you could not just. And OpenAI is losing money. So like you cannot afford just going and giving everyone full access to this. You need somehow to optimize that. There's a lot of pieces that come into this.
00:15:01.425 - 00:15:12.765, Speaker B: Okay, and then when you left Google, did you leave straight away to start near? And what was the original idea behind near? Because it wasn't quite what it is now. Right?
00:15:13.205 - 00:15:47.185, Speaker A: Yeah. So we had a quick actually detour. We were trying to build a predictive assistant. And I think it's actually getting more relevant now as well. It's this idea that actually your phone or your computer will just know most of the time what you want. And so instead of going to apps, navigating to specific actions, it just predicts what you want and all subsequent actions and you just do that. The problem was back then we didn't have as rich semantic context as we can have now.
00:15:47.185 - 00:16:11.985, Speaker A: And also we were targeting people in Silicon Valley. And I'm sorry, but there's kind of everybody's workaholic there. So there's not much things to predict. People are just going from home to work. And out of a hundred, we had 100 alpha tests. Yeah, one person went to movies in two weeks out of 100. So there was not like much variety.
00:16:11.985 - 00:17:37.165, Speaker A: But so we started near AI with this idea of how do we teach machines to code? How do we transform how you interact with computing in such a way that if you want to do a data analytics, if you want to build your own front end over some data, if you want to build a mobile app, you will be able to just tell how it should work and it will just write all the code for you. And again, this was end of 17, beginning of 18, 2018, so these models were still starting up, I would say we definitely did not have massive scale to train them on. So we were trying to actually get better data. So we did have a, you know, hundreds of students around the world, right? This is China, Eastern Europe, Southeast Asia who would do small tasks for us. Mostly like we found coding, like students from computer science departments who would do, you know, describe the code for us or write code for descriptions. And we actually faced, you know, this problem that like actually paying all of them was really complicated, right? PayPal didn't work in some countries, you know, transfer rise doesn't work in some countries. Some students don't have bank accounts.
00:17:37.165 - 00:18:29.279, Speaker A: And so we started looking at crypto as more like just using it for our own need. We're like, hey, can we just use crypto to pay people? Like plug it in on the back end and just move on. And the answer was, well, we actually were paying less than fees were back in the day, and back in the day they were lower than now. So Ethereum and bitcoin fees were just like way too high for kind of microtransactions we were doing. And the experience was just like so atrocious, right? That kind of explaining to someone, you know, here's a 20 step process how you get onboarded and you by the way, need to buy some crypto first. If you want to work here before receiving any money, you need to buy something first. That seemed so out of place, right? And so that's how we kind of, as we're doing our research, right, and we'll explore different blockchains.
00:18:29.279 - 00:19:09.285, Speaker A: We're like, well none of it actually satisfies what we would expect to be a blockchain we would want to build on. And so, and kind of we talked to these other folks and we're like, well we can actually build the blockchain we want which is, you know, really fast performant, easy to use, easy to develop on. Kind of you can write in common languages instead of like new made up languages. And that's kind of how near blockchain started was pretty much kind of this exploration of like, oh, we can actually use a blockchain for our own use case Actually, there's no blockchain to use. Okay, we'll build a blockchain that we would want to use.
00:19:10.225 - 00:19:14.455, Speaker B: And then you kind of forgot about the AI thing or was the idea.
00:19:14.495 - 00:19:47.063, Speaker A: Yeah, I mean we put it on ice. We still have data and everything from back in the day. And so when the mainnet launched the like one of the first projects that launched was a crowdsourcing platform called Near Crowd, which has been working since then. Been, you know, I think like a couple thousand people working there every day, maybe a little bit less now and then. Alex actually went deep into AI back in 2022.
00:19:47.199 - 00:19:48.519, Speaker B: And that's your co founder, right?
00:19:48.567 - 00:20:32.049, Speaker A: Yeah, my co founder, yeah. So he's kind of refocused back in AI. He started kind of the offshoot in 2023 and we kind of brought it back as near AI this year to really bring back a lot of the focus we've had in 17 on teaching machines to code as well as now on kind of user owned decentralized AI. And how can we instead of like, you know, head to head, like, hey, let's compete with OpenAI. It's like how do we bring everyone together in open source, in decentralized AI to really have, you know, some big. Be bigger than parts. And we can discuss kind of how I think about it and.
00:20:32.049 - 00:20:32.793, Speaker A: Yeah. What we need to do.
00:20:32.809 - 00:20:38.433, Speaker B: Yeah, I'd love to. And your co founder, Alex, he was at OpenAI, right? He left or he took.
00:20:38.489 - 00:20:45.537, Speaker A: Yeah. So he went for. To Open AI for a few months. For like a few months and then decided to start his own thing.
00:20:45.681 - 00:20:48.649, Speaker B: Okay, interesting. When was that? When did he go there?
00:20:48.697 - 00:20:49.805, Speaker A: In 2022.
00:20:50.145 - 00:20:53.985, Speaker B: Okay, so right after like the OG chat GPT moment or.
00:20:54.065 - 00:20:56.245, Speaker A: No, no, before he was even before.
00:20:56.585 - 00:20:59.445, Speaker B: Okay, that's cool. That's a good time to be there, I guess.
00:20:59.825 - 00:21:00.645, Speaker A: Yeah.
00:21:01.385 - 00:21:31.953, Speaker B: Interesting. Yeah. I also want to ask you some stuff about data, but I think, I'm curious, just like maybe we can start higher level. What are your general thoughts on AI right now? Did you expect it to happen this fast when you were working on it in 2017? 2018. And then also what do you think of, do you think AGI is coming pretty soon? What do you think of superintelligence? Some people talk about, I guess just maybe your high level views on where this is and where it's going.
00:21:32.149 - 00:22:41.069, Speaker A: Yeah, well, I'm generally an optimist, so I thought it will happen faster. That's why we started near AI in 2017, because I thought it's going to actually be like what we see now, I thought it's going to be happening in 17, 20, 18. So yeah, I think there is, there's a step functions that happen, right? Kind of. And we've seen step function kind of before and after every step function there's like a huge wave of everybody's trying all kind of different things and then it quiets down a little bit and then we have a new step function. Now I think this wave is just way bigger. And so what we see now is just like the quieting down is still way higher than if you think in waves, the quiet down is just way higher than it was before. And so the only problem is you never know when the next step function comes.
00:22:41.069 - 00:24:18.245, Speaker A: And so it can be tomorrow, it can be in a year, it can be in a few years, but it's coming. And so the, I think the important part to understand is the kind of, the progress we see is insane, right? You look at any benchmark, you look at any kind of capability, and if like a year ago it was at zero, now people can do like 30, 40%, right? Of some capability and it is not, that's not a production ready by any means, right? This is not general intelligence, but like the progress itself is amazing. So I think, and my definition by the way, of AGI is an ability to do any, let's say digital skill better than the best human ads at digital skill, right? Because right now the current model is already pretty good. Better than average. Average at everything, right? And partially that's because they train to become an average of every human in the world, right? But I think like what, what everybody wants is them to be, you know, the best or at least like, you know, in 95th, in 90th percentile of the, of those skills. And that is still, you know, not here, but we are, I think, specialized models for specialized skills that actually can get there. And so I think what we're going to see now is like all of this overspecialization with smaller models or with kind of specialized agents, et cetera.
00:24:18.245 - 00:25:04.135, Speaker A: Meanwhile, the general level is going to continue evolving. And again, that's where we need to see some step function change. Now the superintelligence I think is just going to come from like if you have AGI and you run 10,000 of them, like that's your superintelligence, right? If you put, you know, lots of smart people together and they actually effectively work together, like that's going to be way more creative and effective and being able to produce more intelligent results. So I think that's going to happen almost immediately after we have something that's able to undo this. But again, I think even before that we'll have.
00:25:04.255 - 00:25:35.925, Speaker B: So you subscribe to the idea that superintelligence happens straight after we have AGI or something that can automate an AI researcher's job or very soon after. Yeah, interesting. Yeah, I've heard that from a few, not just Ashbrenner obviously, but a few other sort of technical people that AI is already. I've heard from a few people that AI is already more useful for research than it is for engineering because you can set up the experiment and stuff like that, whereas it's not as good as actual engineering.
00:25:36.085 - 00:25:54.945, Speaker A: Yeah, I mean, there was a paper actually just came out I think yesterday, which was that it's better at idea generation for research than researchers, than human researchers. They did a blind study with experts in the field and so the ideas it produces are just already. Yeah, that just came out yesterday.
00:25:55.375 - 00:25:56.407, Speaker B: That's crazy.
00:25:56.551 - 00:26:31.479, Speaker A: Yeah. So I think, I mean, it's still not very good at like evaluating how like plausible it is. So think of it as like a brainstorming partner that will like throw any idea, anything goes. Yeah, but, but then again you can have. And they have like it's not a single model, right. It's a model and a critique model and you know, like refining model and exploratory model. So like it's a system that does it, but that's kind of where we're going, right? It's not going to be a single neural network that trained on predicting next token to solve everything.
00:26:31.479 - 00:26:38.103, Speaker A: It's going to be a system of differently parameterized models that going to be.
00:26:38.159 - 00:26:40.127, Speaker B: A mixture of experts type thing.
00:26:40.231 - 00:26:50.703, Speaker A: Either. Mixture of experts. But also you still need tools, you need all those. It's going to be a whole system that is doing these things. But yeah, we're definitely going to get.
00:26:50.719 - 00:27:08.455, Speaker B: Into that because I'm super interested in your take on that. So basically. And you think that this compute trend line where we just throw more compute at these models and they just seem to consistently get better, that that just continues to hold that there's still enough sort of gas in that tank that.
00:27:08.535 - 00:27:33.397, Speaker A: No, I don't think in that per se. I mean it's a little bit more complicated. Right. I think because people say like, oh, let's Romo compute the reality. What happened is people increase parameter size. Right? So what happened is we went from whatever 1.5 billion parameters to 7 billion parameters to 70 to like 400 to 1 trillion.
00:27:33.397 - 00:28:20.227, Speaker A: Right. And so another requires more compute, obviously, because it's a bigger model, more parameters, more math moles. But I think what's going to next happen is maybe even less parameters, but more compute per iteration. Per, like you as a person, if you have a harder problem, you don't like immediately spit out what you think, you go and think about it and then give an answer, right? And we already see this with like chain of thought and all of these things, right? Like the way to get these models to be more effective is give them more time to think through the problem before giving you an answer. And I am expecting more of that is like, it's not going to be that, like, let's just throw more parameters at it. It's going to be more. Let's give it more compute per iteration.
00:28:20.227 - 00:29:11.125, Speaker A: There's an interesting paper that literally introduces chain of thought directly into training mode, right? So normally chain of thought right now is kind of like when you already train the model, you say like, hey, you know, explain your thinking step by step. But they introduced chain of thought during the predicting next token. So it goes and thinks more and then sees if that was useful to predict next token. And so nobody actually replicated that experiment at scale. That was done on a smaller model because it's just so computationally expensive to do that on a bigger model. But that is an example of you can actually have way more potentially kind of intelligent model because it actually thinks before talking.
00:29:12.865 - 00:30:29.034, Speaker B: Yeah, interesting. So you're more in the camp of some of these, I guess, unhubbling things where you can make more qualitative improvements to the model can make them better rather than just the raw computer. That makes sense. And so right now we seem to be on track for a world where these big tech companies that are sort of like either vertically integrated or vertically integrated via partnership, right? Like OpenAI and Microsoft and Amazon and Claude, you know, Twitter and Xai and Grok and Xai. They're like vertically integrated where they own the entire stack, maybe not the chips, although they're all working on it, but they own the data centers, the models and the applications. And also that proprietary data in many cases we do have open source models are keeping up somewhat, largely due to Zuckerberg of all people, which I don't know if that was unexpected for you is definitely not on my bingo card. And I'm curious, do you think that's going to remain the case? Do you think Zuck just did this whole open source manifesto? But how can he continue to justify this when, when models cost 100 billion, you know, or you know, if llama 3.5,
00:30:29.034 - 00:30:39.905, Speaker B: whatever costs 100 billion. Do you think that keeps happening? And if not, like do we live in a world that's just dominated by vertically integrated big tech companies that everyone kind of goes through?
00:30:41.205 - 00:31:57.659, Speaker A: Well, I mean ideally we not because I think the, you know, when in crypto space we like to talk about hey, you don't want anybody else to control your assets, right? And so similar thing, you don't want anyone else to control your intelligence. And that's really what's happening. What's happening is as companies are able to kind of as these tools become more advanced, they're able to both control your access to these tools, right? So kind of what you're able to do as you know, as a person, as communities, as countries, but also they're able to manipulate people. And not from like a malicious kind of perspective, but just purely economical, right. Any of these companies, they need to generate more revenue. Like they're public companies that's literally like they need to show revenue growth. And to just kind of, to, to do that there is a inherent flywheel which is people inside the company when they're building models, when they're launching them, they do an A B test and they see if this increases revenue and if it doesn't, they go back and see what they can do to increase revenue.
00:31:57.659 - 00:32:00.563, Speaker A: So there's a natural kind of step.
00:32:00.739 - 00:32:19.125, Speaker B: That is not right now though, right? At least right now it seems like everyone's sort of saying sundar, we're not worried about the cost of this. The risk of underinvesting is way bigger than the risk of overinvesting for us because you know, even if we buy too much compute or whatever, we can still use it for our cloud business.
00:32:19.285 - 00:32:42.143, Speaker A: So when you launch it in production, right. You either way running an A B test, right. That's just inevitable. This is how launches work. And during that A B test you see what's the impact on all core metrics, right? And one of the metrics is revenue, right? If you, if you super red in revenue, you're not launching this. If you neutral in revenue and you're. And this costs a lot of money not launching this.
00:32:42.143 - 00:33:32.187, Speaker A: So it naturally like needs to improve revenue for you to justify launching this. So yes, I mean research can potentially do stuff. But again it's funded through this. Like it's a system, like it's a very much incentive system that's built in, in every for profit company is that it's actually going to like get Everyone to at the end contribute to the revenue model of this thing. And yes, I mean people can invest and you know, you make investments in these areas. But like at the end like the fine tuning loop, right? The oh, we should throw away this data because it makes our revenue go down or we should add more, you know, over sample some data or we should like improve something else. Like you're going to be doing that because that's kind of how your product at the end launches.
00:33:32.331 - 00:33:33.129, Speaker B: Okay.
00:33:33.307 - 00:33:35.765, Speaker A: And so, and so this month.
00:33:35.885 - 00:33:36.557, Speaker B: Yeah, go ahead.
00:33:36.581 - 00:34:04.829, Speaker A: Yeah, yeah. So kind of to. So I call this like a for profit AI, right? Or kind of corporate profit AI. And so the alternative to this is what I call user owned AI, right. This is where it's not about just being open source. Right? Open source kind of a part of the requirement there. But user owned AI is actually where this AI models are kind of fine tuned to be on your success as individual.
00:34:04.829 - 00:34:25.931, Speaker A: Right. Like no, nobody's like going and checking like hey Jose, was the good model for you? Did you feel better because of using it? No. They're like, did we make more money from you? That that's, you see the difference, right? So, so we need to kind of have an alternative that is more focused on kind of individual and communities and their success, their well being. They're kind of interesting.
00:34:26.093 - 00:34:41.715, Speaker B: So just to play devil's advocate, isn't like, isn't the whole kind of point of capitalism that I'm willing to pay more if something provides more value to me and so that revenue should be at least somewhat correlated with creating value for customers? No.
00:34:42.215 - 00:35:15.327, Speaker A: So that I would say that worked in a pure product, like a physical product. It doesn't work in a digital. Think of Instagram, right? Like I mean I don't use Instagram for example, but I've multiple people who are like, oh, I got stack scrolling through Instagram, right. It's not because like you wanted to spend whatever that's fair, half an hour. So like this, these things are becoming addictive and they designed to do that because this is how they generate more revenue, right? Like they generate, they pretty much creating addiction system as you're saying.
00:35:15.391 - 00:35:18.099, Speaker B: Yeah, an AI model trying to addict you would be pretty scary.
00:35:18.247 - 00:35:47.145, Speaker A: Well it will, I mean the content will be fully ad generated specifically to get to keep you engaged. And you know, you'll just keep like it's, it's. You remember this, this mouse with electrode, right. And you pressing the button like so it's. The problem is like yeah, our, our human brain is not optimal, right? And like these things will find how to hack it. And so, so we kind of need some protection. Ideally that's on our side against kind of, and this is just general, like if we think of AGI, right.
00:35:47.145 - 00:36:04.085, Speaker A: And everybody has like, do you want like one company having AGI and being able to do whatever they want or you want everyone to have AGI that's on their side and now we can all kind of, you know, find an equilibrium of like how we actually coexist in that environment.
00:36:05.465 - 00:36:59.993, Speaker B: I think everyone, or like most people agree, not everyone actually, but most people agree that you want everyone to have AGI, you want user owned AI and stuff. But I guess what's the. How do you actually end up with it? Like how do you create. Like maybe we can start with open source first because that was the original question. Like how do you if and how do we make sure that the open source models are actually on par with the closed source ones? Like do we just have to trust Zuck that he, that he keeps, that he keeps open sourcing llama? And if not, it's pretty bad, right? Like if the open source models are sort of two generations behind or one or two generations behind the closed source ones, or even more in a world where they're spending like a trillion dollars by 20, 30 or something per model, you could see a world where they're even more behind. Isn't that like it's hard to have use your own AI in that world?
00:37:00.169 - 00:37:32.317, Speaker A: Yeah. So I think the goal will be how do we create set of incentives in such a way that we both can have the research. Because the reality is there's more researchers outside of these organizations who are working in open source in academia than inside. But they're extremely disorganized. And not like I'm not saying this in negative ways, just like that's how it is. They're all in different companies, in different institutions, they don't work on the same code base. Compare this.
00:37:32.317 - 00:37:58.385, Speaker A: You work in a DeepMind or Google or OpenAI, you're in the same code base. There's one code base that everybody contributes to. Somebody figures something out, you're already using it tomorrow versus right now in open source. Somebody figures something out. You wait for a preprint on archive, you read it, you go to their GitHub, you're like, I don't understand this code. And so maybe somebody can explain it to me. You're trying to find the author.
00:37:58.385 - 00:38:37.365, Speaker A: The process is so much slower. So I think there's like fundamentally a few pieces that needs to happen. First of all, we need a better kind of place for open source research to happen where we actually learning from each other and able to reuse each other's kind of components and pieces. And this is again both open source research as well as kind of web3AI research as well that is happening and very active. Second one is when we train, like we need to figure out how to train models that are kind of big in a distributed way, right. There's a lot of right now people who are exploring this. Obviously incentives will be one of the components, right.
00:38:37.365 - 00:39:30.565, Speaker A: You know, we can align incentives of a lot of different entities to really train together. The problem is right now is mostly technical, right? Like how do you do it without, you know, sending lots of data? Like the bandwidth requirements that current training procedures have. I, I think there is potentially like a combination of both algorithmic changes to the model and kind of optimization, like changing how we're optimizing parameters, et cetera, like how we're accumulating gradients, et cetera. Like combination of those things put together potentially can start getting us there. You probably want, if you're doing it that way, you may want to have the model itself to be like distributed. So no single party can pretty much use it unless the whole of network is participating. Now that's aside for frontier.
00:39:30.565 - 00:40:02.539, Speaker A: The thing is, frontier models are not actually that useful because they're too expensive to run. And so a lot of the practical use people then distill to a smaller model that's specialized in specific skills. And so those we actually can get in open source way more effectively because like, because open source it's so much easier to actually do a lot of things with it. And so I think there we'll see just like explosion of kind of AI agents and capabilities that happen there.
00:40:02.587 - 00:40:09.699, Speaker B: Can you explain distillation? Like how you distill the smaller model from a bigger one? Because I think that's a really important thing.
00:40:09.867 - 00:40:52.119, Speaker A: Yeah, the way to think about it, right, right now, when these models are trained, right, you're throwing random data to them and you're like, here's something about Kassel in Germany. Here's quantum physics. Here is how to cook, I don't know, crepes, right. And so it takes a while for model to learn and figure out patterns and imagine kid being bombarded with random articles. It will take forever for it to learn. Now what distillation does is actually you pretty much take a bigger model that's already learned everything and you almost and use it to generate data that is now used by a smaller model. So it's kind of synthetic data.
00:40:52.119 - 00:41:33.445, Speaker A: But importantly, instead of just generating words, you generate distributions. And so that's a very important part because again, right now when I give you an article and I ask you to predict next word, there can be multiple options, right? It can, you know, it can be like variety of words that have similar meaning, but you're getting punished if you say similar word. Right. You only need to get it correctly. And so what this distillation process does is allows this model to actually indeed output that there's multiple options and kind of not get penalized. And so you get a lot stronger learning signal. So you need less data to learn.
00:41:33.445 - 00:41:50.053, Speaker A: And so you can train smaller models pretty much more effectively. And so we see this with like 3.1. Llama is like the same size model, but it's like way better because it was distilled from the 400B model. And I mean there's a lot of other tricks as well, but that's a.
00:41:50.069 - 00:41:55.745, Speaker B: Principle for a layman, I guess. You can basically use a bigger model.
00:41:56.045 - 00:41:57.301, Speaker A: To teach the smaller model.
00:41:57.333 - 00:42:08.209, Speaker B: Yeah, to teach the smaller model. And you can get. And you don't need to know, you don't need the larger model to be. Do you need the larger model to be open source for this to work? For distillation to work?
00:42:08.257 - 00:42:10.121, Speaker A: Well, if you want to distill, yes.
00:42:10.313 - 00:42:16.065, Speaker B: Okay. So you couldn't. Otherwise you'd have to pay for like inference.
00:42:16.105 - 00:42:35.095, Speaker A: For inference, yeah, yeah. And yeah, so some people done it originally, right. There was alpaca, I think, where they Source data from ChatGPT from GPT4. The problem is it also violates their terms and conditions of OpenAI pretty much says you cannot use their staff for training other models.
00:42:35.595 - 00:42:36.355, Speaker B: Okay.
00:42:36.515 - 00:42:38.095, Speaker A: Because, you know. OpenAI.
00:42:38.595 - 00:42:41.091, Speaker B: Yes. Yeah.
00:42:41.203 - 00:42:41.491, Speaker A: Okay.
00:42:41.523 - 00:42:48.163, Speaker B: So for the installation to work, you still need the open source to keep up. Right. And for that you need.
00:42:48.219 - 00:42:57.753, Speaker A: Yeah, yeah, that's what I'm saying. You need a frontier model that is trained by a collective. And so we need to figure that out.
00:42:57.899 - 00:43:47.189, Speaker B: Yeah, and how do you. Because it's like you're going to need so much compute. Right. And I mean there's, there's obviously there's a lot of compute around the world, but it's probably going to be less efficient, I would assume, like distributed compute. You know, there's, there's, there's obviously the latency requirements, but there's also like, you know, these people are hiring like some of the smartest engineers to put together these data centers and cooling and they're going to position them next to a power plant and all of this. So, like, is the delta going to be like if you need a trillion dollars worth of compute to train a frontier model in a centralized way, that you need like 1 1/2 trillion or 2 trillion to train it in a distributed way, or do you see it? Because I guess there's something on the other side too where maybe you can go for more unused compute or people can arbitrage cost of power across the world. So maybe there's some efficiencies on the other side.
00:43:47.189 - 00:43:50.795, Speaker B: But how does it net out in your, in your view? Like.
00:43:50.915 - 00:44:41.589, Speaker A: Yeah, I mean, I've been thinking about it a lot and it's a hard problem because you kind of need to go to indeed like energy as like a basic unit of economy. And so there's some arbitrage about like electricity costs in the world, but obviously it's not like it's not that big. There is. I, like, I do still think it's going to be a conglomeration of clusters. Right. I think the, what we see right now is people are starting to build out this like 5,000, you know, H1 hundreds or you know, maybe a thousand H1 hundreds in the data center. And so like, I don't think it's going to go like if you try to just like let's combine, you know, whatever MacBooks, million MacBooks.
00:44:41.589 - 00:45:10.875, Speaker A: Right. For training. I don't think that's like, at least not kind of where we currently are and kind of how currently mass is. But if you combine like 100 clusters of 1000 H1 hundreds, they exist now. They're starting to exist now. There's enough data centers here and there that actually, because everybody sees that as a future and they get equipped and nobody's able to coordinate that right now.
00:45:12.005 - 00:45:20.825, Speaker B: And I wouldn't pay for that because you still need some entity to raise enough money to pay all the different clusters for the compute. Right?
00:45:21.925 - 00:45:25.821, Speaker A: So I mean, there's few options. Again, I mean, this is where we're speculating here, so.
00:45:25.893 - 00:45:26.797, Speaker B: Yeah, absolutely.
00:45:26.941 - 00:45:50.855, Speaker A: Right. But as we progress, we can imagine something like Bitcoin style approach, right, where people contribute because they believe that the result will be valuable and the result will be valuable and the token will capture the value of this result. Because then when you want to use it, you need to spend this token, right?
00:45:50.935 - 00:45:51.423, Speaker B: Yeah.
00:45:51.519 - 00:46:21.555, Speaker A: And so this is a way to kind of obviously create an incentive structure around this. It's still, I mean, obviously there still needs to be like Mining the system and liquidity and everything. Because people have real costs. Like you still need to pay for electricity every day. Right. So, so I think that there needs to be kind of big participation from the community. But given, like you don't, like, you cannot invest in OpenAI, you can like individually, you cannot invest in XAI.
00:46:21.555 - 00:46:52.321, Speaker A: Right. You cannot invest in anthropic. Like, this is your opportunity to actually invest in the future of compute and intelligence. And I think honestly, like when AGI comes, a lot of the dollars will not be useful anymore. Right. So this is actually an opportunity to have a useful currency which converts into access to intelligence, access to computer intelligence. Right.
00:46:52.321 - 00:46:58.685, Speaker A: And that's probably a harder currency over time than any fiat.
00:46:58.995 - 00:47:12.891, Speaker B: Interesting. Yeah. So that's definitely one use case for crypto. We've shown we're pretty good at bootstrapping supply sides. Right. And whether it's helium or even bitcoin itself. So that's definitely a use case.
00:47:12.891 - 00:47:39.745, Speaker B: I'm curious, like, let's say Llama does keep up. Like, Zuckerberg continues to hold true to his promise. We have this open source model. Like that would be a much, much better world for crypto, I would argue. And for startups. For startups generally. And then a lot of the stuff you've talked about, like user owned AI, could you maybe differentiate, like, what of that stuff needs crypto? Because there is a world of open source AI and like a bunch of startups and stuff, and they don't use.
00:47:39.745 - 00:48:01.405, Speaker B: You use crypto at all. So maybe, and not on the philosophical side, more on the practical side. What do you think are the things that crypto adds to AI? Because a lot of people think it's vaporware. Right. Like crypto XI is vaporware or whatever. We strongly disagree. And so what are the practical like benefits for a builder, for an entrepreneur to using crypto?
00:48:02.145 - 00:48:26.821, Speaker A: Yeah, So I kind of think of this as like a stack. And the stack is very similar in spirit to what, you know, this big centralized companies have built internally. Right. So there's kind of three major layers. So there's a data layer, infra compute layer, and kind of application layer. So on data layer there's a lot of stuff. Right.
00:48:26.821 - 00:48:45.253, Speaker A: So I mentioned crowdsourcing. Crowdsourcing is just way more effective if you do it using blockchain. Right. The near crowd has zero employees, zero legal entities. It just pays people for doing work. And anybody in theory can come in, say, hey, I want this data to be labeled. Somebody will come in, do it.
00:48:45.253 - 00:49:05.355, Speaker A: There's crypto economics and game Theory to make sure it's quality. And so right now the alternative is scale AI which is like a $10 billion company that has like a bunch of legal entities, a bunch of operations, a bunch of you know, transaction like you know, they, they have a not zero opex. Right. So it's infinitely more expensive than crowd.
00:49:05.395 - 00:49:17.433, Speaker B: Yeah, can I, can I double tap on that? Because I don't fully understand the role of data. So my understanding is that most of these models are trained on like all of the Internet. Right. And it's like an open. Open crawl. No, like this.
00:49:17.489 - 00:49:34.537, Speaker A: So they are pre trained. Pre trained on, on let's say open crawl. Although there's a lot of asterisks there, we can come back to that. But they then fine tuned on so called instruction data. And this is when you're actually teaching it to do something useful. Right. Because if you just take a RAW model, right.
00:49:34.537 - 00:49:56.425, Speaker A: You can predict next token that's not very useful. You want it to answer questions, you want it to write code. So you need human labeled data or like reinforcement learning from human feedback to actually teach it to do the specific skills you want. And so that's when. And like the more specialized you want it to be, the more kind of human labeled data in that specific domain you need.
00:49:57.525 - 00:50:01.973, Speaker B: And how does that work right now? Is it just. Is everyone, are all the big labs using scale?
00:50:02.029 - 00:50:10.885, Speaker A: Yeah, they have all like scale AI or some other version. Like XAI for example has some people on staff pretty much being the tutors of the model.
00:50:11.385 - 00:50:18.089, Speaker B: Okay. And they're literally just like taking GitHub data or something and labeling it like you know, good code, bad code, whatever.
00:50:18.177 - 00:50:52.605, Speaker A: Like what's the, I mean depends what you're doing, right? Like it can be. I mean there's data for math and physics, there's biology, there's like all kinds of stuff. Right. And so you need actually like people who are experts actually know this stuff. Right. So like we went from before we would label data that was like, you know, here's an image. What is in an image? Now we're getting into, you know, here's like different Soylents and when you combine them right under this temperature, what's going to happen? And like you actually kind of need to know chemistry to answer that.
00:50:55.945 - 00:50:56.545, Speaker B: That's really interesting.
00:50:56.585 - 00:51:01.205, Speaker A: Right now they all using like. Yeah. Some form of service or hiring directly themselves.
00:51:02.195 - 00:51:10.499, Speaker B: Okay. So the data, data seems like a clear opportunity where you could have like this decentralized scale AI or whatever. Crypto is good at these marketplaces, right?
00:51:10.547 - 00:51:42.875, Speaker A: Exactly. Yeah, so the other part is actually curating data. So you say like open crawl. But the thing is open crawler is a lot of shit. And so you need to actually like you need to subset it into something that's not shit. And that process itself adds a lot of bias, right? Because like, and this is example I use. Well on Internet, Barack Obama is born in Hawaii or Kenya appears pretty much as often because people love to speculate, people love to, you know, conspiracy theories.
00:51:42.875 - 00:52:18.985, Speaker A: And there's a lot of websites about this. And so if you want your question answering system to respond that he's born in Hawaii, you need to filter out all these websites. But now by filtering this out you actually make an editorial choice. You're saying, well, Fox News talks about Barack Obama being born in Kenya, so we're going to remove Fox News from the data. Right? So you have this whole like curation process that happens and it's very opaque, right? Like it's opaque even in llama case. Like it's actually not open source models, it's open weights models. We don't actually know what went into the model.
00:52:18.985 - 00:53:09.735, Speaker A: And it's also important because we really would like to know that there is no, there's like thing called sleeper agents where you actually add stuff to data that then like model actually starts to respond in specific way based on some conditions like it will only do it in 2025, like it will not do it in 2024. So you can actually like engineer your data in such a way so that model exhibits behavior, different behavior at different time. So all of that would really be like should be knowing. So you need a data curation process that like kind of governed by you know, some communities instead of like, you know, be that on the country level, on community level. And so it's the same source data but you kind of how you filter and kind of munge it pretty much is really important.
00:53:10.525 - 00:53:18.425, Speaker B: And how do you see that it's going to be like open source. I imagine there are open source curations of open Crawl already.
00:53:20.005 - 00:53:46.175, Speaker A: So open crawl is open source. There's a few different versions like Hacking Face published, there's some others. But again I'm imagining there's a way for me for example to say hey, I want to, here's a set of functions I'm going to apply on this original data and this is the new data and you can inspect that and see that this is indeed the transformation. Now when we all can train model this.
00:53:50.475 - 00:53:52.535, Speaker B: Those remarkably quiet sneeze.
00:53:54.595 - 00:54:27.279, Speaker A: And we can all train kind of Our models on this and we like this, this is a set of behaviors you kind of expect or don't expect anymore from this. And you kind of want this to be like a public. Think of it as Wikipedia style. We converging to some data set that everybody agrees is like a shelling point of data. Because I think one important thing is the core model should be probably just raw intelligence. It should not have. It's really hard to remove biases.
00:54:27.279 - 00:55:03.821, Speaker A: Everything we do is biased, but we should keep reducing biases. And then you fine tune it or you like, you know, you put the systems around it to add back whatever the specialized knowledge and whatever bias for your application you need. And again, this is like that requires like a, you know, a community, a lot of community effort. And I think again coordinating lots of people doing that finding like, oh, this is, you know, like biased article. Or let's remove this. Or let's, you know, like maybe it's like let's synthetically regenerate this part of the data set that like cleans it up in this way. Right.
00:55:03.821 - 00:55:29.973, Speaker A: So like all of that, it's just like a lot of, I would say work that I don't think any single company will do. And you need to coordinate kind of large swaths of people. And I think crypto incentives are really good for that. And then the other side of this is also the data. There's all this data doesn't belong to, you know, like Reddit is selling data that doesn't belong to Reddit in a way. I mean by their terms of service it does now.
00:55:30.109 - 00:55:32.973, Speaker B: Yeah, you have this hot take, right, that copyright is dead or.
00:55:33.029 - 00:56:00.975, Speaker A: Yeah, copyright is dead because again, copyright IS technology from 1700s it was kind of was figured out because of gotten burg press. And so we need a new technology, like we need a new legal technology for the generative AI age where all of the data are going to be re kind of recombined and recompense breast models and. Okay, yeah, anyway, that's a separate whole separate podcast.
00:56:01.395 - 00:56:41.145, Speaker B: So there's. Yeah, because the other side of data, this is really useful by the way, even for my own learning. This is super, super interesting. And then the other side is like the data. To what extent does proprietary data matter? Like Google has YouTube as we go to like multimodal or like Tesla has their cameras on the car. Right. Or to what extent is this kind of proprietary data going to give models edges and does open source need to recreate that somehow with like data marketplaces where you know, the deals that OpenAI is making with Reddit or that OpenAI is making with the New York Times, like is that going to be a differentiator that we need to also try and recreate somehow?
00:56:41.445 - 00:56:59.381, Speaker A: I think it especially matters when you want fresh data, right? Like if there's a new article in New York Times about what happened just yesterday and you ask what happened yesterday to your system, it should know what happened. And so it needs to have fresh data, it needs to have access to all those things. And so I think.
00:56:59.413 - 00:57:01.145, Speaker B: So it needs an API key or something.
00:57:01.725 - 00:57:46.365, Speaker A: Yeah, but API key, I mean first of all, all of them are starting to add provisions that potentially don't allow us. So I think that would be an important. There's already an important thing is like are you publishing your content? A lot of it is starting to become user generated content. Are you publishing it on SunPress platform and giving up all your permissions to somebody else? Or you're going to be on, you know, actually use your own content, user own platforms. But I think it'll be important. I don't think like some things as important as others, but the novelness and freshness is extremely important. And I think like from a product perspective, like I think ChatGPT has gotten some of this data refreshed.
00:57:46.365 - 00:58:18.635, Speaker A: Like people start like because people like oh, it doesn't know about this, it doesn't know about that. And like as it kind of fixed, like added some of this fresh data, like it improved pretty dramatically. So I think those will be extremely important. Again it's like imagine, imagine you have like a, the smartest person in the world, but they've never, they don't know anything about what happened in last two years. Like they literally were in the bunker and you're like asking them questions and they will kind of not being able to answer like because they don't have context.
00:58:19.335 - 00:58:43.015, Speaker B: Yeah, I mean, okay, so the freshness is important. Is there anything to the, to like having the quality of the proprietary data, like having YouTube? Because I assume most of this stuff's been scraped anyway. Like everything that's on the Internet has been scraped anyway. But is that not the case? Is there some stuff that. Yeah. Or is it just the freshness you think really that's the most important thing?
00:58:43.595 - 00:59:22.125, Speaker A: I mean the scrape. I think again this is where there's an interesting question which is if we have this like distributed training that's run by lots of people, if I'm an individual, you know, open YouTube and feed my YouTube into the model, like and everybody does that, you know 100,000 people do that. Is that. Are we scraping it or we're just training our model? Like we're just training the common model. Right. So I think that that part, I mean, I don't. Again, there's no clear answer how, like, where does this fit? But like, I think that model is, you know, there's a few companies that are building that kind of tooling.
00:59:22.125 - 00:59:49.537, Speaker A: So I don't think that is actually a problem per se. I think the data that's not available, a lot of it is private data in the first place, like private user data, which should not be available. And this is where you probably want it to be available only during the inference for your model. And again, this is where you want a smaller model that runs on your device. And yeah, it has kind of context about you. So it's not easy. Right.
00:59:49.537 - 01:00:05.625, Speaker A: There's some data like cancer patient data, which is useful for cancer research, but it's private, so it probably shouldn't be public. So there's a lot of more complexity than just like, let's make everything public or let's make everything private. There's a lot of nuance.
01:00:06.005 - 01:00:13.005, Speaker B: All right. I think the data bit is clear. Do you want to move maybe up the stack to the. Do you see compute as the next thing?
01:00:13.125 - 01:00:28.359, Speaker A: Yeah, so compute is obviously a big thing. I mean, as I said, like coordinating compute, this will be important. We have also kind of distributed decentralized inference. We want probability there. We want this kind of small specialized models for specific gigs.
01:00:28.407 - 01:00:39.063, Speaker B: Can you talk about why? Is it just because we want smart contracts to be able to use LLMs and obviously we need it to be provable that we're getting the right result. Or is there other. Another use case as well?
01:00:39.159 - 01:01:08.825, Speaker A: I mean, not just that, like, if you want to do. If you want to use them to make any kind of decisions. Right? Yeah. If you just shit posting on Twitter, probably doesn't matter if you are, you know, if you want to make a decision, right. Like on, you know, putting money somewhere or medical decision, you kind of want to know what are you getting results from. Right. You don't want it to be like, oh yeah, we ran like whatever, you know, llama 7B instead of 400 because it's cheaper and just like use that.
01:01:08.825 - 01:01:12.165, Speaker A: Right. So you want to make sure that the results are correct.
01:01:12.665 - 01:01:15.769, Speaker B: But why wouldn't you run it yourself in that scenario? Or like, what's.
01:01:15.817 - 01:01:32.161, Speaker A: Yeah, well, for bigger models you cannot. Right. So I would say again, there's a spectrum. Right. For smaller models, absolutely. You should run it yourself on your local device. And then as the model gets bigger, like you presume it has better capabilities, but then you cannot run it.
01:01:32.161 - 01:01:35.645, Speaker A: So you want to check that it's actually results.
01:01:36.265 - 01:01:47.445, Speaker B: So you see a world where even the OpenAI models will have a provability thing where for some use cases you'll require them to actually prove that they ran on the latest model.
01:01:47.745 - 01:02:29.937, Speaker A: Well, ideally yes, but I think the problem is with that case, to prove it you need to prove it towards something. So you need to have the weights open so that we can prove that it indeed was run by this weight. So I think it's more applicable to open source open weights. But again, it's important to understand this is the, it's a usage of cryptography, not per se. Yeah, crypto. But crypto is what allows to build some of the systems right. Efficiently if you want to have like a large inference networks, you know, and coordinate them to pro.
01:02:29.937 - 01:03:17.525, Speaker A: To have your provable compute. Because like if you're building a centralized provider, indeed there's no reason to provide provable compute because you're already trusting the centralized provider. So you kind of, if you have probable compute, then you can build a distributed network of independent parties because then you don't need to trust any single one of them. Yeah, and then I think the other really big side of this is, you know, so model itself is, you know, powerful but like even more powerful is kind of this agent. Right. Agent is something that uses model and can have memory, can have, you know, external data, can act, can do tools, etc. And agents are effectively kind of, I call them like, you know, semantic APIs and they need to get paid, they need to pay for things.
01:03:17.525 - 01:04:03.493, Speaker A: And it's very effective to use kind of crypto payments and ability to kind of coordinate money through crypto for agents because well the, a lot of it will be micro payments, theirs themselves kind of digital, right. And so like they don't have kyc, they don't have any of the systems. These agents can also be fully autonomous, right. You can actually run one like fully autonomous can be a smart contract. It can be like a service kind of distributed service. They can actually raise their own money. Like you can imagine an AI company in the future that raises its own money, uses it to pay for data and then uses this data to then respond to a bunch of answers, make money and pay back the token holders.
01:04:03.493 - 01:04:15.861, Speaker A: And this is actually, it would be interesting because this will be a security token, but it's not run by any person or company. It's fully autonomous AI agent. So it'll be interesting how.
01:04:15.973 - 01:04:19.245, Speaker B: Good luck. Yeah, exactly.
01:04:19.405 - 01:04:41.865, Speaker A: It's fully open source. You can literally run the whole thing if you want. But anyway, so there's interesting things like that where again, because it's like digitally native and they need digitally native money, payments, asset management, fundraising and all coordination, etc. Identity. And so all of that is blockchain, really effectively provides all those tools.
01:04:42.645 - 01:04:52.781, Speaker B: Yeah, that's super interesting. That's one that we're pretty excited about. And so this is like at the agent layer which is kind of above compute. Are agents applications or is it?
01:04:52.813 - 01:04:59.461, Speaker A: Yeah, yeah, this is kind of like sits right under applications and applications kind of use it. But yeah, okay.
01:04:59.653 - 01:05:13.933, Speaker B: And yeah, like, because that's like quite a sci fi world right where agents agents are. And it's really the only way for them to, for them to act autonomously. Right. Is to. Is to use sort of these crypto rails.
01:05:14.029 - 01:05:14.745, Speaker A: Exactly.
01:05:15.565 - 01:05:24.617, Speaker B: Okay. Is there anything so and then on the application layer maybe is there anything else you can kind of see there.
01:05:24.681 - 01:05:52.265, Speaker A: That'S I mean on application like there's excited about. Yeah, I mean there's going to be a bunch of stuff. Right. But I mean everything from you know, like personalities and virtual worlds to you know, productivity to this kind of. Yeah, like AI companies, et cetera. So I mean they're honestly like, you know, anything science fiction you can say, like it's there. But I think we'll just start seeing some of this interesting examples.
01:05:52.265 - 01:06:29.213, Speaker A: There's people who are training very interesting personality AI agents that more for entertainment. There's people who are combining multiple agents for workflow automation. We have for example agent that builds front ends for blockchain applications. You can just describe in English what you want and it builds it and you can point and like modify it and so you can like build up a full front end. No coding, no need to know like how anything. Like you don't need to read the code. You can hover over any element and it'll describe in English how it works.
01:06:29.213 - 01:07:04.045, Speaker A: Right. And so you have all of that. And so that's what like bringing back 2017, what our vision was in 2017, we actually kind of making this work now where you can like build pretty much from scratch applications on a fly. And so I think like there's just going to be lots and lots of this, you know, combination of like, hey, I want to learn for example a language and I want to build like a custom app for myself to do that. And you know, and you can like combine that also? Yeah, just like huge variety of things.
01:07:05.265 - 01:07:29.021, Speaker B: Nice. Yeah. I think now that we're here, I think we can get into the accelerator a little bit. So obviously we've partnered up to do this, to do this crypto AI accelerator. Maybe we can start with. You can start. Then I'll add in what the value is that the accelerator is going to bring to the projects that are applying in what ways we're going to help the project succeed.
01:07:29.021 - 01:07:36.785, Speaker B: And why should founders apply to this? Maybe both for web3 founder and for web2 founder. Why should they apply to this accelerator?
01:07:38.315 - 01:08:18.681, Speaker A: Well, so I think there's few dimensions here, right. One is this is a new space and we are all kind of learning and figuring out how this is going to work. And so we want to bring together best minds who are working on this space so we can learn together. And I think it's a really good opportunity to do that and, you know, have support from our side where, you know, we like, we both building as well as we have kind of, you know, expertise in AI space. We're building in web3 space. We're building web3 space. It's great opportunity to work with you guys.
01:08:18.681 - 01:08:59.464, Speaker A: You know, you have amazing, obviously crypto economics background and like in general bringing companies into and kind of doing go to market for a lot of this efforts, you know, and I think like what we've seen. So we've run the first cohort, right. And people were excited actually by different things. Like some people were, we had a UX training and so given, you know, you need to at the end build a good product. Right. Like we had a UX seminar that walked through kind of how to think about it, how to and analyzed your specific products. Some people were excited more about token economics.
01:08:59.464 - 01:09:25.884, Speaker A: Right. And how to design your economics, how to get your token to market. Some people were just excited because they found their first clients in the cohort, right. Or in kind of broader community we're building. So I think it's just like really great opportunity to kind of get started and do it with the company of people who actually are either on the same journey or building in the space and have like deep expertise in different areas.
01:09:26.924 - 01:09:58.631, Speaker B: Yeah, yeah, exactly. On our side, we've obviously, like, we started with the research, we have the venture fund as well. So we've invested in over 150 projects. We've been covering the space since 2018. And then on the lab side started off working with projects directly where we kind of worked up close with AAVE and Axie Infinity, compound, balancer, a bunch of the top projects in the space. Saw a lot of what went into building and then at some point decided we wanted to do that ourselves, so started incubating projects. And you know, we've.
01:09:58.631 - 01:10:40.451, Speaker B: We've incubated projects with billions of dollars of tvl. We've taken projects live, tokens live, gone through the whole, the whole process from start to finish. So we sort of just know what goes into this stuff. Like, we know even the simple stuff, audits like security, best practices, how to ship a product in crypto, how to build a community, how to design a token, how to launch a token, like all the, all these sort of like dark arts that you kind of only learn by really being involved in a bunch of projects. We've done that over and over again. And we actually, before this, we did not think we would be doing another accelerator. But once I think we started to get really excited about crypto AI, we just thought it was the perfect.
01:10:40.451 - 01:10:59.315, Speaker B: Because the thing with the previous accelerators in crypto is it's hard to really, there's. There's always like an issue with accelerators, with attracting the best founders and also getting novel ideas. Right. Like generally people. People build the stuff that already exists and maybe in a new chain or something like this. But with crypto AI, it really is like a wide open field. Right.
01:10:59.315 - 01:11:31.989, Speaker B: And I think the accelerator is actually the perfect way to explore that. Like have a bunch of teams, they're really smart people, tackle different issues, learn from each other and kind of learn together, like you said. So, yeah, this kind of brought us out of retirement, back into. Back in the game to run this accelerator. And yeah, we're super excited to kind of partner with you Couldn't think of a better partner, obviously in this space to help these guys succeed, guys and girls. So maybe next question. Like, you've kind of covered it with a stack.
01:11:31.989 - 01:11:47.295, Speaker B: You sort of covered the areas that are interesting. Are there some areas that you're most excited about seeing projects in the accelerator? Like if you had maybe a top three or something request for, for startup that you'd love to see built, what would those be? It can be any, any area of the stack.
01:11:48.795 - 01:13:01.121, Speaker A: Yeah, I think, I mean, there is ton of room for like, kind of model specialization. I think there's a pretty big premise that I guess a more, a smaller model, specialized, specific skill will be just more economically effective way to, you know, do different things. And so I think that that's kind of a whole opportunity room where, you know, by incentivizing kind of creating different, you know, like model, like specializations. We can actually get like a broader and broader set of skills. Again, think of this as you have a very smart grad students, you still need to teach them to actually do the job, right? And so, so like, you know, as we have more, more of the specialization, like the scope of things we can do is broader. I'm actually excited on like tackling kind of the intersection of the research and AI space. And I'm also excited about doing AI, AI on AI research.
01:13:01.121 - 01:13:50.261, Speaker A: So like what can we do to like leverage AI to accelerate AI research itself? But broadly, like this AI research topic is something, I mean we've been thinking about. There's some work on this in, in, in the space so far. But I think there's an opportunity of like I, I call it like research Wikipedia. Think of this where like, you know, everybody's contributing to one space to keep growing the body of knowledge and work, right? And everybody's kind of working with their own AI assistant that is trying to generate new ideas, hypothesis experiments, et cetera. Then maybe somebody's doing so, coordinating that kind of effort. Again, I think the important part is coordinating kind of working open source right now is extremely ineffective. Right? It's publishing papers and it's grants.
01:13:50.261 - 01:14:51.603, Speaker A: That's the only thing that exists, right? None of those things are effective. They're super slow. You know, economics has like a two year lag between paper publishing. So for example, why actually nobody from economic space is in crypto is because like it's too fast for like academia work. So I think there's not really opportunity there. And then I think on the data side there's still ton of, ton of opportunity. I mean there's like a bunch of companies already, but there's still places where I think kind of combining, for example novel user experience that is kind of incentivizing people to publish user generated content or some kind of, you know, engagement type content and then using that to then build a better model, build a better experience and kind of, but also collect data and use it for variety of other use cases.
01:14:51.603 - 01:15:13.575, Speaker A: So I think like generate kind of creating a more cohesive system that is both generating data, using it for AI and then using that AI to generate better data. I think there's kind of interesting opportunity there. And again, crypto is really good way to bootstrap that flywheel and have some game theory inside to ensure kind of this is engaging in high quality.
01:15:13.965 - 01:15:39.027, Speaker B: Interesting. Yeah, those are cool. And then last, last few questions I have for you I'm curious, given everything we've discussed about AI, how do you think about positioning near and I guess like your career generally? Like what, what are you building and why, why did you choose to build that? Because I guess you have like pretty broad skill set. You could be building almost anything. Why are you, why have you chosen to build what, like what are you building and. Yeah, why have you chosen to build that?
01:15:39.141 - 01:16:16.117, Speaker A: Yeah, I mean the way I think of it is like we're building a new kind of way to engage with compute, with Internet and with each other. Right. Like you know, we call it user owned Internet because we're trying to shift away from this kind of the car, the current state where everything is centralizing behind walled gardens, but actually build it in a more individual and community focused. And then AI is a tool for that. Blockchain is a tool for that. You know, there's other tools as well. You know, we're working on kind of chain abstraction to really make it easy to use all blockchains.
01:16:16.117 - 01:17:24.887, Speaker A: We're working on some intent kind of solutions because at the end everything like Google is just intent, you know, solver. So I think there's a lot of kind of the species that really come together into this vision. And I mean I believe that's probably one of the most important, I mean outside of solving energy, probably one of the most important pieces because I think the zero marginal cost of software on Internet just has such a dominating effect to really centralize everything. And so I think, I actually think the capitalism theory, like we kind of talked about this like it worked really well in 20th century and I think it's starting to crumble. And we see it crumble, right? That's why like us is trying to become more socialist, right? Because like it doesn't work very well anymore. Like the kind of the methods, like the open markets methods that worked before are starting to crumble because technologies like has such a different effects and I think we kind of need it.
01:17:25.051 - 01:17:28.335, Speaker B: Do you see capitalism, I guess not working well.
01:17:28.415 - 01:17:32.991, Speaker A: I mean that's what I said. Like the. Imagine Instagram is a good example.
01:17:33.063 - 01:17:34.607, Speaker B: Company's revenue maximizing.
01:17:34.711 - 01:18:15.325, Speaker A: Yeah, well revenue maximizing is normal. It's a problem that you cannot create a competitor that people will freely choose. If you create a competitive whatever shampoo and it shows up on the shelves and you choose it and you redirect revenue there because of the shampoo became not good. Or there's a better shampoo, right? Just random example. But like a physical product, like you have, you have a, you have this competition, buying power and competitors and you have competition. Yeah, when you, when we're talking about product like digital products that you know, like we have all of this already installed, it's on billion devices, it's kind of integrated in our life. It's really hard for anyone to compete.
01:18:15.325 - 01:18:59.935, Speaker A: And these companies are not like just sitting there, right? They analyzing data, they seeing what people are, you know, starting to use and they're just copying everything into their product. And so you just have a very different mechanisms that are in play now. And so you don't have as much of like hey, this is an enterprise that is like now moving slowly. Like even with all the, you know, as I said like billion dollar like thresholds, et cetera. Like Google, you know, catch up, get caught up with OpenAI pretty quickly and like yeah, they lost someone on kind of branding side but you know, if it was economical they would have put like you know, chatgpt like thing into every google.com search. It's just not yet there.
01:18:59.935 - 01:20:01.795, Speaker A: And so like, like Even Google like OpenAI, which is like a massively funded company, like and Google which already can catch up and compete and potentially like one day can just say actually now our models is better and free and available on google.com done. OpenAI is done. Right? So I think we're already in this state where even among big companies they're barely competing with each other, right? And small startups very unlikely to succeed here. And all of this is global first, right? It's like it's available everywhere it's available. So and it creates kind of habits, it designed to make you kind of more used to this, right? There was an article about Google for example, where Google Ads would go to search and say hey, we need more people to search more because we need people to see more ads. Right? Like stuff like this is just like all of this just reinforces this problem I think.
01:20:01.795 - 01:20:54.051, Speaker A: And so yeah, I think the like I don't have like a full theory on this just yet, but I'm kind of, I'm trying to like combine it all. That's why like ideally there will be some economists analyzing this and like actually bringing back all those pieces where I think we just see a transformation of economy from kind of physical goods capitalism to this like digital like zero marginal cost kind of distribution and, and owning distribution, owning kind of this relationship to the user directly. Right. And I think that changes the economics and changes the in turn changes like innovation and everything. And so like we kind of need like in a way for banking that's already always there and existed. And Bitcoin was like a way to get out of that system. Right.
01:20:54.051 - 01:20:57.015, Speaker A: And so we kind of need the same for the rest of things.
01:20:58.845 - 01:21:17.189, Speaker B: Yeah, that's a really unique perspective actually. Yeah. All right, I have, I have two questions. One leads on. Yeah, let me think. I have a profit maxi question and then a more societal question. Maybe as a, to start with, as a profit maxi, we can end on a better note.
01:21:17.189 - 01:21:49.855, Speaker B: As a profit maxi, let's say you're an investor. How do you think about investing in AI? Right. Because the world is probabilistic. I guess as hopeful as we all are, there's maybe some probability we assigned to like the big tech just dominates. Right. We're all just vassals using the big tech models. Like are you, are you, are you buying or I don't know if you've thought about this, but are you buying some Microsoft and Google and things like this to get, to get exposure to that and then putting the rest in startups or in some part in crypto or like how do you think about.
01:21:49.855 - 01:21:51.975, Speaker B: Yeah, like I'm curious.
01:21:52.435 - 01:21:55.895, Speaker A: Yeah, I mean I'm definitely not buying Microsoft and Google stock.
01:21:58.595 - 01:22:01.175, Speaker B: For moral reasons or you don't think it's a good investment?
01:22:03.155 - 01:22:24.327, Speaker A: Some combination of it? I think. Well one is like hedging against yourself is. I don't think it's a good idea like to like in this kind of thing, like it's kind of weird if you like if you lose but then you still economically fine because it kind of creates a weird incentives, burn your.
01:22:24.351 - 01:22:25.223, Speaker B: Boats type of thing.
01:22:25.279 - 01:23:31.367, Speaker A: Yeah, but I think the more fundamental is like, I mean again I think as this technologies mature, become more available, we're going to see such a drastic transformation of a social layer that I think like first of all, you know, if this is going to continue this way and it all converges to like Microsoft or Google or whatever, they're going to get pretty much state owned. Like there's no way the government will allow just like a company that controls every like everybody's job and everybody's intelligence to be like roaming free. And so that's why we kind of need everyone to have it so that we don't have like government controlling it. And so from that perspective is also like I don't know how government, like if that happens, how government will, is it going to pay shareholders or it's going to just take it? Right. And so that's why. Yeah, I think, I mean startups, I just like to support founders and see how we can work together, et cetera, learn from them. Crypto X AI obviously as well.
01:23:31.367 - 01:24:12.695, Speaker A: Kind of see more as an opportunity to like, okay, we need more of this technology stack to be built out to have a competitive solution. And then just products more generally, like applications, products, like how can we get something in the hands of people and start creating really compelling experiences that are more competitive with this and potentially something that's not replicable by Google for whatever their capacity of reasons. And it's really hard, but I think it's really important to keep finding those opportunities.
01:24:13.995 - 01:24:36.269, Speaker B: Fair. That's a good answer. All right, last question. Just on the impact of this on society. You seem like you've thought about this a bit. It seems like AI, I don't know, every technology wave people say that there's going to be like they're going to take our jobs and stuff. It seems to me like this time is different.
01:24:36.269 - 01:25:18.455, Speaker B: I know those are the most dangerous words you can say, but it seems to me this time it will actually take a lot of jobs. I don't know how you think about this, but it almost seems like the Marxist nightmare, like labor. Because the cost of, I mean AGI takes the cost of cognitive labor to basically the cost of inference, right? To some extent. And then if you have robotics, it takes the, the cost of even physical labor to just like the cost of power and I guess raw materials that go into it. Everything will be super cheap. I imagine if this is the case, inequality I guess will be like bigger than ever. Like people who own capital, who own these companies that provide these services will be super rich and like there is no labor anymore.
01:25:18.455 - 01:25:50.685, Speaker B: Right? Like there's still going to be. I think there's always going to be role for like human coordinators and people would basically taste to coordinate these AIs and have the goal in mind, keep the whole context and get stuff done. But it doesn't seem like you need that many of those compared to all the jobs that exist right now. So yeah. Is unemployment just peaked or rather the lowest it's ever been and it's just going to go up from here? How do you see the world, this AI transition? Do you think this time is different or.
01:25:51.265 - 01:26:42.481, Speaker A: I do think this time is different and I think the, I mean they're going to be like multiple pieces. But that's why I was mentioning that like I think If Google and OpenAI are the only companies, for example, doing that, then government will take over because they literally, this is the tool that everybody else is kind of being turned Unemployed. So that is I think why this is a pretty interesting situation and I think the, I mean we already see that transformation in a way. Right. It's just, it's on the edges now. But you know, we have creators who are you know, on Instagram, on YouTube, on kind of we have, you know, more sports than we've ever had. We like.
01:26:42.481 - 01:26:52.319, Speaker A: So it's kind of, it's going to all move to entertainment and kind of that space where, where we don't really care. Like I don't care about robots riding faster, right?
01:26:52.407 - 01:26:52.655, Speaker B: Yes.
01:26:52.695 - 01:27:26.137, Speaker A: I don't care about, you know, robots dancing more choreographed, etc. Right. So like it all going to move to entertainment pretty much one way or another and those different layers surround it and as well as to like more digital economies. Right. Where indeed like I can be you know, defining taste of this land or you know, items I'm creating, etc, even if they created by AI like but kind of more taste making. Right. And say same as like you know, making a video if you're making a Instagram post, tick tock etc.
01:27:26.137 - 01:28:19.931, Speaker A: Right. So I think like that is I think where all of this is moving. And then I mean there will be some, I mean there will need to be some form of. I mean this is where like again I'm trying to reconcile this like economic theory because you mentioned Marx and like I think capitalism is kind of in an interesting state because of this and so in a way beckons for socialism. But we also know socialism worked really badly. But actually analyzing why it worked badly and what worked well in capitalism and kind of how do we combine this and given we have now this AI environment, I think that's where the research needs to happen on kind of like what if Karl Marx lived now? What would capital look like? Right.
01:28:20.003 - 01:28:21.539, Speaker B: He'd be upset for sure.
01:28:21.627 - 01:28:52.295, Speaker A: Well, I actually don't think he would be upset. I think the kind of fundamental idea that transformation needs and abilities versus the capital, I think there's something there that we just need to kind of up level with modern technology and with modern thinking. But yeah, I'm just like interesting. I'm not there yet. So maybe we'll do a podcast in a year on that or something.
01:28:52.835 - 01:29:24.999, Speaker B: Yeah, that's. Yeah, it's a tough one. The two categories of jobs I kind of see, having thought about it a bit is capital allocators and coordinators, like the people who are kind of coordinating the AIs making decisions. Because it's going to be hard. Maybe it's both hard and will never and Will take us a while to trust an AI to make a decision. Like should meta invest more in AI or in VR or should we ban immigration? Like these political decisions or have a cap on immigration. These decisions, I think will always be made by humans.
01:29:24.999 - 01:29:44.551, Speaker B: And AIs will produce reports to support them and stuff, but humans will ultimately make them. And then the other job is that, like you said, jobs where we value humanness more than efficiency, like sport and entertainment, artists, artisans. Yeah, I think those categories of jobs still exist, but the rest is tough.
01:29:44.583 - 01:30:07.993, Speaker A: Tough to. I mean, we actually are planning an experiment of doing an AI delegate in our dao. So let's see if the decision making by AI is. People should be able to have input into it. But I actually think decision making itself may be better done by AI as well. But let's see. I want to run this experiment and we can actually literally see the.
01:30:07.993 - 01:30:27.095, Speaker A: So by the way, for a startup, that's probably one of the interesting opportunities to actually do governance setups with AI. And then because I think like you can test it out in crypto and if it works, you can bring into kind of more traditional government decisions.
01:30:27.835 - 01:30:28.739, Speaker B: For sure.
01:30:28.907 - 01:30:29.179, Speaker A: Yeah.
01:30:29.187 - 01:30:55.065, Speaker B: And I agree with you about like socialism or I just see like, you know, there's some things that are good for society that aren't profitable. Right. And that's kind of what the government's there for. And it seems like there's going to be more need for that in the future because there's going to be more and more people that aren't able to be profitable to society. Like they just. They just don't have a way to add, to add value, like cheaper than the cost of inference. You know, like they're.
01:30:55.065 - 01:31:04.357, Speaker B: Yeah. I don't know. Crazy times. Thanks very much. I know you. I've kept you long, much longer than expected. Really appreciate the time.
01:31:04.357 - 01:31:11.743, Speaker B: This was. This was an awesome chat. I think people will learn a lot. I learned a lot. We'll definitely be re listening to it for sure.
01:31:11.799 - 01:31:12.935, Speaker A: Thanks for inviting me.
01:31:13.095 - 01:31:39.931, Speaker B: Cheers, man. Speak. Speak soon. And for everyone, we'll put in the show notes the link to the applications for the accelerator. Very excited to be working on this. And there's going to be a demo day at the end where it's going to be a who's who of investors there to kind of see these, see all these projects. So if you're a founder or thinking of becoming a founder, hacking in AI or in crypto, X AI or even if you're just in crypto, we can help pair you up with someone that's in AI.
01:31:39.931 - 01:31:46.035, Speaker B: There's a lot of good people, good AI people from Ilya's network. So please apply and. Yeah, looking forward to it.
