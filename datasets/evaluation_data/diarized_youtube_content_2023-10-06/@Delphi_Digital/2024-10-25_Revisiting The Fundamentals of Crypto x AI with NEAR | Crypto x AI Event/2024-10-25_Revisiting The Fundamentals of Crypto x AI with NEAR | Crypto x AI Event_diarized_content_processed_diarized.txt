00:00:00.400 - 00:00:35.891, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors NIA Empowering Decentralized Applications and Blockchain Ecosystems. Venice AI a private and uncensored alternative to popular AI apps. MIRA Unified AI infrastructure secured by crypto and the first on chain multi agent system. To learn more about all of our sponsors, check the description below and dive in.
00:00:35.891 - 00:00:37.135, Speaker A: Enjoy the show.
00:00:48.795 - 00:01:22.377, Speaker B: Welcome everyone to Delphi Digital's Crypto AI Conference. I'm John from Delphi Ventures. Today I have two very special guests with me, Ilya and Alex, co founders of near. Most of you know near as a blockchain project. However, near's roots go deep into AI. Indeed. I know both of you guys started near as AI startup back in the day, which I'm going to, which we're going to talk about in this panel.
00:01:22.377 - 00:02:01.113, Speaker B: But first I want to say I'm extremely grateful to have both of you working in this industry. I know you guys are extremely experienced in distributed systems AI software development, but in addition to that you are strong advocates in open, transparent, accountable, user owned technologies. So yeah, this one is like and you've been like, I know you've been like that since many years and then so it's like very self driven. So I'm extremely like grateful. So this is a big pleasure for me to host this panel with you. Welcome.
00:02:01.209 - 00:02:03.365, Speaker C: Thank you, thank you for having us. Excited to be here.
00:02:04.425 - 00:03:12.055, Speaker B: So I want to kickstart things with the attention is all you need paper. So for listeners, for context. For listeners who may be unfamiliar, Ilya previously worked in Google research on natural language processing where she co authored Attention is all you need paper and this paper is basically laid the origins of Transformers, basically the brains behind all the mainstream AI applications like ChatGPT that you, me, you know, millions of other users use today every day. So yeah, Ilya, I want to ask you about, like to give us a simple explanation on like what's the big deal with the, with this paper. But also I'm like curious to hear a bit more on the background story. Like what kind of what were you working back then and what led you to actually like author this paper? Could you walk us through the story there for sure.
00:03:12.095 - 00:04:17.403, Speaker C: Yeah. So I've been doing kind of machine learning, you know, what we call AI now for a long time before, even before doing Google research and kind of one of the fundamental challenges Is how do we teach machines to be able to understand language, understand the context and being able to answer questions. And so when I joined Google Research, I was really excited to work on that problem, kind of natural language understanding. But specifically the same way we test that people understand something they've learned in school, universities, you know, in driver for driver license is by asking questions, is by, you know, giving them various situations, context and asking questions. And so my team worked on question answering. We were trying to figure out how to teach machines to read different articles, different contexts and answer questions about them. The benefit obviously of Google is that there's a lot of people come to Google to ask questions.
00:04:17.403 - 00:05:11.929, Speaker C: And you know, we have a lot of various data around that. But the mechanisms we used, which were already deep learning, were not very conducive to reading long articles. So they worked very similar how we as people work. They would read one word at a time and that would on the scale where you want to teach it on millions of examples would be two millions of articles. It would be too slow. And in turn, you can only scale it that much as well and train it on only that much data. And so in parallel, kind of a sister team that worked on a different problem of understanding if two questions or queries are the same, they had the same problem.
00:05:11.929 - 00:06:12.429, Speaker C: They had trillions of examples. And so you could not use any kind of expensive model you need to something that's very cheap, very scalable. And so they use this mechanism of effectively attention applied to use each word, attend to all the words in the second query, right? So you go over each word in the first query and you attend to second query. And so through this mechanism, depending on how attention activated, they would then score if these two queries were similar. And they trained it on massive amounts of data and it worked really well. It actually was very kind of like first example where something as core to Google search started to work with deep learning. And so the idea that came to Jakob was, hey, what if we leverage this but for question answering, like, how would it look like? And so kind of transformer was born out of that.
00:06:12.429 - 00:08:06.625, Speaker C: How do we leverage something that doesn't need to read every word kind of one at a time, but actually can consume the whole article at the same time and then try to respond based on the whole context. So it was really about kind of both using the fact that computation, the computing devices we have are highly parallelizable and the fact that we need to read like very long contexts to really give good answers. And especially for training on a Lot of data and we train it first on machine translation, but it's applicable obviously to other use cases. And so the big deal of this is actually, and it's kind of was obvious to another Ilias discoverer is that this is kind of the way to train kind of really big models that understand the world. If you just kind of start pre training it on predicting the next word and you just start scaling models, it actually starts to compress the whole kind of knowledge of Internet into this model because that's the most effective way to predict what these articles will say and what this thing to say. And so that's kind of how it evolved from hey, need to answer questions to well, if we're training this gigantic model that actually seen everything on Internet to compress kind of all of that Internet back into the model, it then has all the knowledge and reasoning that is needed and we can then fine tune it, you know, to answer questions, be the kind of assistant or be the whatever tool you need on top. So that's kind of like at the core is unlocked this like large context and large scale of training that was not possible with previous generation models.
00:08:07.565 - 00:08:13.705, Speaker B: Did the other ILIA reach out to you after kind of he realized this is kind of a breakthrough?
00:08:15.125 - 00:08:21.269, Speaker C: Not to me, no. But I mean Lucas, another co author is actually working OpenAI so I'm sure the.
00:08:21.437 - 00:08:41.915, Speaker B: Yeah. Did you know back then like how much of a breakthrough that was like or basically like the impact that it created in the world? I'm just assuming it must have surprised you or did you know back then that this was going to become that big?
00:08:42.855 - 00:09:17.695, Speaker C: I mean I was definitely excited by kind of how well this model worked. And so I actually left and started with Alex Near AI in the premise that like hey, we can actually use these methods now to teach machines to code and we can talk about kind of what Nirjar was doing. But I don't know, I mean at the time it wasn't clear that this is like. And this is, you know, at scale will be something big. It felt like, you know, there will be another model architecture, you know, in a year or two years. Just because like there was continuous kind of innovation happening in the. In how you do it.
00:09:18.915 - 00:09:33.805, Speaker B: Got it. But I mean being one of the kind of co authors there, kind of laying the foundation of all of this. How does it make you feel? Like do you feel grateful, happy, stressful? I'm also curious to hear that.
00:09:35.505 - 00:10:09.505, Speaker C: Well, I'm obviously excited to see the kind of the progress. I mean this is something that I went to Google to work on. So I'm really excited to see this. At the same time I think we share with Alex urgency that we need to design systems that are going to be more democratic like design kind of AI training AI like user owned AI effectively in such a way that is in the power of people and not just end up controlled by a few companies.
00:10:10.365 - 00:11:00.607, Speaker B: Right. I'm going to transition to near AI. But before that I have two, two more questions here. One of them, I don't even know if this question makes sense but the LLMs today, their goal is basically they're really good at predicting the next word. Right. The next token. In your opinion, are these large models, do they really have the understanding of all the forces that make up our world in order to come up with this next token prediction? Or are they really sophisticated models that give us the illusion that they know all of this understanding, like they have all of this understanding.
00:11:00.607 - 00:11:03.831, Speaker B: Again, I don't know if this question makes sense but still wanted to ask.
00:11:03.863 - 00:11:20.045, Speaker D: You the question is are people actually intelligent or are they just having or I was just having an illusion or an intelligent conversation. I think LLM is as capable with scale. LLM is as capable as a human, I'm pretty certain.
00:11:20.385 - 00:12:35.845, Speaker B: Cool. Another question I had was this one I was listening to karpati like 10 months ago actually when he was explaining this he basically gives, he talks about how LLMs kind of self improve and he gives this example of AlphaGo, you know, surpassing humans basically in AlphaGo. So for those who don't know, AlphaGo was this like AI system that was designed to play the go game. And what he explains that There was like two training sessions, stages in the first training sessions basically AlphaGo became as good as like best humans. So he was really, he wasn't like, he wasn't able to surpass humans. And then it's only when we like defined a reward model and fed that into the model. AlphaGo like basically telling it what it means to win the game, that it played like millions of games by itself and it's self improved from there on without any like human input.
00:12:35.845 - 00:13:14.461, Speaker B: And that's how it like surpassed humans. So the big question as the big like open question as Carvadi explains in language processing is that like we don't have the same environment because like in a go game this is like a sandbox. The environment is a close, that closed environment. So you can, you know, the rule set. So it's kind of straightforward to come up with a reward criteria which is to win the game. But this is a lot less clear in the language case in the language domain, like what is the equivalent of winning in predicting the next word? Right. So this a lot less clear.
00:13:14.461 - 00:13:36.705, Speaker B: So the question Is like would LLMs in their current kind of design, will they become as like will their capabilities max at like the best humans or do you guys think like it can we can come up with a reward model and then have them like surpass humans. What do you guys think?
00:13:37.325 - 00:13:55.781, Speaker D: I think it's. So one thing is that for certain areas of life you can define the reward model very easily. Right. So any logical questions? Yeah, like math. If you solve a math problem, there's a particular answer, right. And you either get the answer right or you don't. Right.
00:13:55.781 - 00:14:01.589, Speaker D: So you can have. It's still not quite as good because in go you need zero input from humans.
00:14:01.637 - 00:14:01.869, Speaker C: Right?
00:14:01.917 - 00:14:38.751, Speaker D: So naively with math you would still need problems, but you would not need for example, solutions and answers to get pretty far. Right. So like for example, the alpha proof paper or. Actually I think everything that DeepMind is doing in the area of math are good examples, right? Because alpha proof they have millions of problems that are created by humans, but they don't use solutions or. Right, they use self plate, similar to AlphaGo to an extent because you can compare the answers, you do have real work. But in alpha geometry they don't even use that for geometric problems. They found that hey, as a matter of fact you can generate problems on the fly.
00:14:38.751 - 00:14:50.925, Speaker D: So for alpha geometry they use no knowledge from humans, they just bake in the rules of geometry into the system. Right. And it learns to solve geometric problems and it surpasses humans at solving geometry.
00:14:50.965 - 00:14:51.165, Speaker C: Right.
00:14:51.205 - 00:15:07.173, Speaker D: So for areas of life, they can be more or less compatible with this model of having the reward model. The same is for programming. Go ahead.
00:15:07.349 - 00:15:13.975, Speaker B: Yeah. What about in general language case? Basically like you know, any language processing.
00:15:14.015 - 00:15:32.595, Speaker D: Right, but with general language you always solving some problem, right? Like if you go into the model you asked him, you have some, you know you have some, there's like a domain, right? So yeah, yeah, I think in certain domains it could be harder. And I think that's where in all likelihood what will be happening is that. Go ahead.
00:15:33.455 - 00:16:04.935, Speaker B: Yeah. Not to cut you, but like, so do you think if we like there would be some domains where we come up with really good reward models? And, and now models look more effective when they're specialized in that domain. And so we have many, we're talking about, we're looking at this many models world. So I'm just basically interpreting that that seems more plausible based on what you said versus a God model that. Okay, go for it.
00:16:05.635 - 00:16:24.691, Speaker D: Yeah. It doesn't have to be many models, right. So you can have multiple, like you can imagine, let's say have GO and geometry. Right. So today the model that solves GO is separate from the model that solves geometry and GO and geometry are quite separate. Right. But imagine you have geometry, algebra, programming, those can benefit a lot from each other.
00:16:24.691 - 00:16:28.075, Speaker D: So it doesn't make sense to have three separate models paint on those.
00:16:28.115 - 00:16:28.339, Speaker C: Right.
00:16:28.387 - 00:17:07.905, Speaker D: It makes a lot of sense to have one that trains for all three. Right. So I think that is not an argument from many models instead of one, I think one model still makes more sense and I think the most likely. So naturally for areas where the reward can be defined, that is not even in the future, that is already happening. GPT401 for example, is a good example. I think that's. We don't know that much outside of OpenAI what it is, but it is very likely that they have some sort of Monte Carlo research with the well defined reward for things where your word is not well defined.
00:17:07.905 - 00:17:25.845, Speaker D: A likely direction, not necessarily the one that will actually happen is you can have a model that predicts how well you performed and then you just optimize against that model. So you have the world model that gives you reward as hey, you did pretty well here or not. And then you can have a self play.
00:17:26.475 - 00:18:40.785, Speaker C: I mean effectively you can say that in all of the cases you have a world model and the question is, is that word model a code of the simulator, right. Of AlphaGo, simulator of physics, simulator of algebra simulator or it's a approximation of that simulator that is done through a neural network. And so then the training of that simulator may need some human data, but it may not need that much and it can be continuous and can be updated and then your main model can be trained against the simulator. And in a way in our head you can imagine our body is kind of our reward function which is constantly adjusting and so we kind of training against it. And it's only normal that your world, like your value function simulator itself is actually training on the fly kind of as things progress. And then the question is more how many data points the simulator is getting like to refine its understanding of physics, of geometry, of all of this other things that it can tap into and how, you know, how accurate it's becoming there.
00:18:41.765 - 00:19:15.275, Speaker B: Yeah, I love these analogies with humans and machines. I really do. It really resonates because like When I dream, I'm pretty sure I'm like, my head just like runs simulations and I learn whatever I've been practicing, you know, I just improve while I sleep. I'm 100% sure of that. Yeah. So transitioning to near from the paper. So the paper, you've written it in 2017 and then that's also when you guys went and kind of started near.
00:19:15.275 - 00:19:37.623, Speaker B: So could you maybe give us the. Your chain of thought there from like why you started near, what was the kind of original plan? And then standing where you are today, you have this like user owned AI vision. Could you just give us an overview of what that vision is?
00:19:37.719 - 00:19:42.875, Speaker D: I think you're off by here. Right? Attention is all you need. Was written in 2016. It was published.
00:19:42.967 - 00:19:49.175, Speaker C: No, it actually was written in 17, but yeah, interesting.
00:19:49.675 - 00:19:50.131, Speaker D: I see.
00:19:50.163 - 00:19:54.015, Speaker C: Yeah. So because I left like before it was fully published.
00:19:55.355 - 00:20:04.739, Speaker D: I see, I see. Yeah. I only get, you know, I only inferred from the fact that in January 2017, I think we were already hacking something in the.
00:20:04.787 - 00:20:09.925, Speaker C: Well, we were hacking it before, but that's. Yeah, that's Silicon Valley.
00:20:10.005 - 00:20:38.231, Speaker D: Yeah. I don't quite recall what was happening exactly because near was not. Immediately after the paper, there was a separate effort which I don't remember as well, what we were doing back then. I remember it didn't work out. And the near started in spring of 2017 when Ilya and I were at ICLR or CML, ACLR. ACLR in France. And we were generally.
00:20:38.231 - 00:21:02.765, Speaker D: Those conferences are a great way to get a snapshot of what's happening. So we were at the CLR and we got the statute of what's happening or it seems like AI is taking off. It did eventually plateau, but we didn't know back then yet that it will plateau. So we're like, hey, AI is taking off. If it continues like that right now machines will be coding very fast. So why don't we get to be ahead of the curve?
00:21:03.425 - 00:21:22.645, Speaker C: And for context, this is both. Some. Both of us wanted to do that for a long time, kind of believing that the programming will be automated and become more of a kind of semantic interface where you talking to a machine and it writes code for you instead of the current state.
00:21:23.505 - 00:21:38.625, Speaker B: And Alex, I'm also curious to hear your background there. What were you working on prior to near? I mean, I kind of define you as a technologist, but I would like to learn like kind of. When did AI also caught your interest?
00:21:39.405 - 00:22:11.271, Speaker D: Yeah, so most of my backgrounds are in systems. I was working for six years At a company called Single Store. It's a pretty fast analytics database, which is I was there, the first engineer. So I joined when it was on the three founders on the Pomeranian. And then I left when it became too big and it was not as fun anymore. But around 2014, so I was still at single store. But around 2014 I started looking a lot into deep learning.
00:22:11.271 - 00:22:52.495, Speaker D: It was back then it was like Tiano or Torch. So you're choosing between every invocation taking five minutes or using Lua. That was before Tether, so pytorch. And in 2016, once I left a Single Store, I immediately started direction. One was building up that company that didn't work out eventually. And another one is I applied to OpenAI and at OpenAI I did eventually spend one month there where I was. It was sort of like a transitioning period where they were trying to see if back then they were doing this thing where they would bring a person for a month and see if the person works with the team.
00:22:52.495 - 00:23:35.845, Speaker D: I don't think they've been doing it recently, but I spent there one month and I never learned if it would work out or not because we got accepted into Y Combinator. So I quit OpenAI and restarted the company. But that was between 2014, 2016. I was doing quite a bit of deep learning. And then with Ilya we were for a year. So starting 2017, mid 2017 until mid 2018, we were doing, we were building a lot of models, testing them with code building research. We have a couple of writeups, published article can see what exactly we're doing.
00:23:35.845 - 00:24:08.055, Speaker D: It was quite cool for the time. And then I guess in 20, in 2022, I managed to get to open AI again. But again, after OpenAI has a very interesting environment internally where you get so energized with the energy of the future that you want to go and build something yourself. So again, after four months of OpenAI, okay, I'm going to go build something cool again. It's a fun place.
00:24:08.675 - 00:24:26.215, Speaker B: Cool. And then would you guys also want to talk a bit about the user owned AI vision and the master plan? I think that was the name of the post I've read. Right. Like the near master plan. Ilya, would you want to talk a bit about that?
00:24:26.595 - 00:25:03.447, Speaker C: Yeah. So maybe just connecting all those pieces. Right. So like kind of we started with Alex near AI. We kind of expected that the growth of and like of AI is going to be kind of what we're experiencing now. Right. Like a exponential curve that we can join in and we Kind of believe that effectively what we call AI developer is going to be a fundamental tool for advancing both AI and kind of broadly how technology works and how we interact with computing.
00:25:03.447 - 00:25:47.201, Speaker C: Now that wasn't happening. We did not really have massive amounts of resources to invest in R and D. Like we couldn't build an OpenAI lab ourselves to do that. We were trying to be smart. We were trying to engage a lot of contributors around the world to create training data for our models in kind of effective way. And so Alex built this crowdsourcing system that, you know, engaged students around the world, you know, from Eastern Europe, from China, from other countries to kind of, you know, label data for us. And we faced this interesting challenge, which was a lot of these people were students, they didn't have bank accounts.
00:25:47.201 - 00:26:18.665, Speaker C: Sending money to China is like, from us, is not really like very effective. You know, some transactions cost like $25. And so we started actually looking at blockchain as a way to solve our own kind of problem. Like, hey, can we use blockchain to pay people? Can we build like this effectively crowdsourcing protocol to pay people? And the answer was no. Right, Like Bitcoin, Ethereum. Right. Transaction fees even back then were already in like, you know, tens of cents to dollars.
00:26:18.665 - 00:26:32.115, Speaker C: And it's also a really weird setup where you first need to buy some crypto to use a crypto app. So as if you get to the job and you need to first buy some stock of that company before you get your first paycheck.
00:26:32.455 - 00:26:35.999, Speaker B: You're asking underage people to purchase stuff.
00:26:36.047 - 00:27:34.393, Speaker C: Yeah, to buy crypto. We really focused on, hey, if we have this problem of we would have used a working know, blockchain, you know, we assume, and we talked in Silicon Valley, like with other people, like there is a demand for a working, you know, easy to use, easy to build on scalable blockchain. And that's kind of how near AI became, near protocol. But now, as we kind of, you know, kept digging, you know, we interviewed a lot of people, we talked to, I don't know, 300 projects over the course of like next year and a half. We interviewed a lot of other protocols as well. And for us it was important to kind of formulate like a long term vision. And this long term vision is kind of in the intersection of, I think all of our backgrounds and kind of views in the world is to kind of give all people control over their asset data and power of choice.
00:27:34.393 - 00:28:29.999, Speaker C: Right. And so for assets like this is familiar to anyone in crypto, but for me Personally, like I'm from Ukraine, my grandparents lost all of their savings when USSR collapsed. I saw hyperinflation where bread went from 10 coupons to 100 coupons to 10,000 to a million within 4 years. Data is something that I think people starting to realize is important. But people obviously have their identity stolen, data leaks constantly happen. And power of choice is something where I think we're in this slowly boiling frog situation where every single platform we interact, which has our attention to capture more of our attention, to monetize it more, they kind of continue pretty much just dumbing down the content, continue dumbing down what we see. And through that like we're kind of losing the choice of what we are, like what we're interacting, what we're consuming.
00:28:29.999 - 00:29:11.177, Speaker C: And through that actually what decisions we got to be making. Right. And I think some of the political situation happening I actually attribute to that. And so that vision, we formulated it in 2019, it's really like we called it open web, but kind of this user owned, like Internet, that's really what it is. It's kind of this fundamental change that we want to go from having centralized parties being kind of extractive middlemen on every transaction, every interaction you have in Internet. And so that's kind of the overarching story of near and kind of what we're building. But if you zoom out into AI.
00:29:11.177 - 00:29:52.145, Speaker C: Zoom in, sorry, into AI specifically, most of these products right now we engage in are actually using AI, not even generative AI, right? Like Google and Instagram, TikTok are using just the recommendation models for the most part. I mean obviously they updated, now it's all transformers across the stack. So it's not that different. But, but fundamentally it's about kind of AI is how you interact with computing. Now it is your interface. But this interface right now is given to you by, you know, these big companies to generate more profit for that company. It's not on your side.
00:29:52.145 - 00:30:31.715, Speaker C: And so kind of user owned AI is this idea that the AI should be on your side. It should be optimized for your success, for your wellbeing, for your kind of economic success as well. And then that AI can modulate your access to information, to actions, to services, to products, to other AIs, to other people. And so that's kind of the core. And from there there's a lot of things that needs to happen from like how do you train models, how do you, you know, how do we come together to do that? How it's executed is edge computing, where data is stored how this is accessed, et cetera, et cetera. And a lot of it is what we're working on in the ecosystem.
00:30:32.465 - 00:31:04.665, Speaker B: Yeah, I think this is a great segue actually into the crypto AI stack. You already started citing some of these. So I've seen you guys in your old presentations that you gave us a breakdown of the crypto AI stack. And at a high level, you break it down into three layers. There's the data layer, there's compute, and then applications. And each of these layers there are different kind of categories, verticals. Right.
00:31:04.665 - 00:32:17.907, Speaker B: Like in data, there's data storage, curation, data labeling, synthetic data generation. In compute, you have distributed training, you have inference on the edge. Right. You have like specialized models, fine tuning. And then on the application you have entertainment, gaming and you know, defi applications, maybe an agent, like managing, you know, money on chain or like transferring money so payments, DevOps. Would you, would you be able to like give us kind of maybe walk through some of these? And then what I'm really curious about is like looking back in the like, let's say one to two years, where do you see the most progress in some of these verticals and which categories kind of have been disappointing in terms of progress and in those, like, what are the kind of main pain points? Like what are things that are like open research engineering problems that you guys think need to be prioritized? We can go in any order with this one.
00:32:18.101 - 00:32:58.779, Speaker D: I can cover a couple of them, which I'm excited about. So generally, I would say within the NEAR ecosystem, we have almost everything you mentioned, potentially absolutely everything you mentioned. And although they are making pretty good progress, there are several areas which I think are slightly more developed at this point. So one that I'm the most excited about always is data annotations because I think generally better data wheels in the long run and data notation, higher quality data. Yes. And blockchain is very natural. So that's how we started near.
00:32:58.779 - 00:33:20.067, Speaker D: Right. We started near because it was very hard to do data annotations. And blockchain is a very natural tool to orchestrate it because you operate with a lot of very small payments. Right. Because everyday connotation is relatively cheap. And the other. And you work with a lot of people who are in geographies or receiving money is pretty hard.
00:33:20.067 - 00:33:31.803, Speaker D: Right. So today if you. But like if you live in. Well, I don't want to mention countries, not to imply that, you know, some countries are doing worse than others, but.
00:33:31.899 - 00:33:46.605, Speaker B: Yeah, there's arbitrage there, Right. Like cost of living is kind of low. But maybe somebody in a first world country would be willing to pay some dollars. That means a lot more in some other countries, right?
00:33:47.185 - 00:34:38.971, Speaker D: Yes, exactly. And if you work on some existing centralized solutions, then first of all they will take first of all a pretty large cut, right? They are for profit businesses like uber is taking 30%. So do those, those companies. And on blockchain you can effectively have it as a smart contract where there's no cut, right? Where maybe that like there's a small org, like a couple of people who take a very small percentage. Ultimately it can be completely decentralized in which case all the money go to the worker. Right? And 30% is a very big, it's a very big cut, right? Like it's, it's more than taxes in many countries and also generally for high quality data there's. If you want to scale it to very large number of annotations, then there is some sort of checks and balances, right.
00:34:38.971 - 00:35:11.371, Speaker D: Where people need to review work of each other. But again you need to design it in a way that reviews don't collapse and people just don't auto accept everything. So all of that is also it makes a lot of sense to orchestrate it on a smart contract so that people who participate, they know the rules and they know that the rules are enforced as opposed to like on Mechanical Turk for example, today the requester has the last say, right. So the worker can do the work with a very high quality requester can, can come and say, you know, like that is not like they can choose not to pay. And there's already obviously this is interesting.
00:35:11.403 - 00:35:21.935, Speaker B: I didn't know about that. So it's basically, they're basically trust, it's a one way trust basically of those who do the work and those who kind of accept the work, right. Like they need to trust the acceptors.
00:35:22.885 - 00:35:54.699, Speaker D: So at scale I'm pretty confident there are some protections against malicious, I guess that people who repeatedly maliciously don't pay people. Right. But in individual cases the thing is that the cost of work is so low that obviously Amazon or like any other company that runs a centralized solution like that, they do not have money, sorry, they do not have resources to review individual complaints. If you weren't paid for your work and your work was worth half a dollar, you know, any support would be meaningless economically.
00:35:54.747 - 00:35:55.003, Speaker C: Right.
00:35:55.059 - 00:36:11.975, Speaker D: So. Right. And so from this perspective, having this like system checks and balance is baked into the smart contract where both the requester and the worker understand how exactly the work will be reviewed by whom and you Know what users they're taking. It makes it way more.
00:36:12.315 - 00:37:10.965, Speaker B: Alice, if I may step in here and explain it in my own words and you can correct me if I'm wrong if I understand this. Correct. So basically there's a data labeling work that needs to be done. You want to decentralize this and allow access to anyone in the world to do this work. But then this raises the question of like, who's going to judge the quality of the work, right? Like, if it's like if I see a response of an LLM and my work is to give a thumbs up and thumbs down, like, who's going to know if I'm just like free riding and like clicking nonsense, you know, thumbs up in order to receive some rewards. So there needs to be some quality control. And then here what you're describing is that you can encode this quality control through a smart contract, some rules of slashing so that if I repeated the act maliciously, maybe I can get away with few kind of malicious or free writing behavior.
00:37:10.965 - 00:37:22.565, Speaker B: But if I continue to do this over and over again, kind of the smart contract logic ensures that I will get eventually like slashed or kicked out of the network. Am I getting this right? Like more or less. Yeah.
00:37:25.985 - 00:37:41.721, Speaker D: Right. The only problem here that you do need to have, like in the ideal case, you would give a task and then every, every task will be done exactly once by one person. So your overhead could be 1x. Right? You will pay exactly the amount of work. So that doesn't quite work because someone does need to review at least occasionally.
00:37:41.753 - 00:37:41.897, Speaker C: Right?
00:37:41.921 - 00:37:59.581, Speaker D: And they like, you know, you can say, hey, I'm okay for half of them not to be reviewed, but you know, like for half of them, I do want someone to take a look. So, so you will need to have some overhead in the system we're running right now. The overhead is relatively high. It's 1.8 give or take in practice. But then we, then we review each work that is submitted. Right.
00:37:59.581 - 00:38:10.185, Speaker D: We don't have any percentage that is immediately accepted. And it's, it's a pretty good, it's a pretty good overhead for the quality we're getting. The quality is pretty high.
00:38:12.045 - 00:38:24.665, Speaker B: All right, very cool. So this was data labeling. Are there any other that's exciting to you? You said there's a few, so.
00:38:25.045 - 00:38:30.225, Speaker D: By the way, talking of data labeling, Ilya, do you remember which company sent us the paper with the. Was it fractional?
00:38:31.645 - 00:38:31.909, Speaker B: What?
00:38:31.917 - 00:38:33.965, Speaker C: Was what was it fractional?
00:38:34.045 - 00:38:40.863, Speaker D: The company that built the nice game for annotating videos. I think it was fractional.
00:38:40.919 - 00:38:41.935, Speaker C: Yeah, yeah, Fractional.
00:38:42.015 - 00:39:09.857, Speaker D: Yeah, yeah. Fractional has published recently a very interesting PDF with a very interesting verification process for the data annotations, which probably has lower overhead than. Yeah, yep. Other than that, I think the most interesting and probably it will be the most impactful direction right now is private inference. I'm. I think people more or less, you know, people who follow it, they kind of understand the state. Right.
00:39:09.857 - 00:39:37.093, Speaker D: So we have multiparty computation, homomorphic encryption or secure enclaves. Secure enclaves probably is what, what's going to be driving it in the short term. Right. Because neither of the remaining two is that far ahead, that far advanced. But, but both of them are making massive advancements. Obviously secure enclaves have a massive disadvantage in the sense that there's a single entity in the world that technically has access to the data. Right.
00:39:37.093 - 00:40:05.521, Speaker D: Intel in case of sgx for example. But it has a massive advantage which is you practically get native speed for all the computations you do. So I think that will be. There's already several companies that are very, very far and to deploying production systems. Right. But the idea here is that I can run maybe locally a small llama, but you know, like llama 405B for almost everybody isn't feasible. Right.
00:40:05.521 - 00:40:24.115, Speaker D: So if you want to be using user owned AI, you do need to be able to interact with the model hosted elsewhere in a way that your data is not visible to anyone. Right. So you absolutely must have private inference if we want to build user on the AI. So that's, that's a prerequisite effectively.
00:40:24.575 - 00:40:43.079, Speaker B: My kind of. I heard this, I heard someone speak about this, I think XO Founder, which, which resonated a lot with me. I think like when it comes to privacy, like historically I don't have evidence that people care. Right. Like we just put everything in Google search. But enterprises really do. Right.
00:40:43.079 - 00:41:00.805, Speaker B: Like you're not going to have a law firm, a medical firm, really like putting all their data on an, on an AI model that's hosted somewhere in the server. So it's definitely a very real problem looking for a solution. Yeah.
00:41:03.945 - 00:41:43.831, Speaker D: I think, yeah. But even if a majority of people doesn't care, it's still the fact that there are people who do care and user on the web. Yeah. Maybe it will not be immediately appealing to everybody. Like people are happily using Instagram every day and TikTok and streaming all their data to Facebook and to CCP. But there's a bunch of people who are less happy with the state of affairs and it makes Sense to go after them and then hopefully like Internet back in the day, hopefully starting from more technical people or people, more privacy, cautious people, it will go and penetrate the call.
00:41:44.023 - 00:43:06.095, Speaker C: But I think worth mentioning, I don't think anybody, any individual right now is willing to share all of their information with any application. Right. I don't think like even if you are searching Google, I don't think you're putting like your SSN number or your credit card number or whatever else into Google search, right? Same as like, you know, in Instagram or whatever. Like, because the reality is we kind of have like a little bit of a model that like, okay, some data we don't want to share and, and like, you know, obviously, you know, different people have a little different understanding where should they put data or not. But the point is like the future where we're going, right, Your AI assistant will be able to help you with, you know, filing your taxes and you know, finding ways to like, you know, file receipts for whatever expenses and stuff. And we'll need to have access to all of this. And so what you don't want is all of this data end up leaking or showing up in somebody else's chat like it happened with some existing AI assistants, right? And so we kind of need that as like, you know, a basic principle if we want to have kind of the next level of user experience where the products are truly, you know, know everything about you and able to interact and kind of transact on your behalf.
00:43:06.095 - 00:44:12.447, Speaker C: So to me this is this kind of like that's a fundamental unlock to get to the next level of trust for people to actually use this as, you know, their shadow versus right now we still need to kind of have a guarded wall against all products because they're run by some other company who is trying to make money off you. Like it's a very natural thing. We need to have a wall actually and like people who, who are not doing that wall is actually end up sadly screwed. So I think that's where fundamentally what we're trying to move to and it's not going to be immediate. Okay, now everybody understands the value of this and as Alex said, it's going to be more technical. People understand like, hey, I can actually trust with more data and it actually is able to do more things now for me and because of that it's a better product and so now I can share it with everybody else. And same was like Firefox was a better browser than Internet Explorer and like technical people started was more secure, it was loading faster and then all the technical people went and installed it for on everybody else's places because like hey, you should use this and not getting viruses from Internet Explorer.
00:44:12.447 - 00:44:16.835, Speaker C: I think like similar adoption curve will happen here.
00:44:18.215 - 00:45:07.639, Speaker B: So. Okay, I think that stays very clearly. I'm like definitely convinces me that the environment is different this time because these tools are just way more powerful, right. And then the more access you give to them, the more useful they will become for you and you really want your data to be private. Alex mentioned tees, right. As near term solution here. There's also other technologies and then another one that I know you guys talk about a lot is basically model like instead of your data going to the model hosted somewhere else, model basically comes to your device.
00:45:07.639 - 00:45:31.345, Speaker B: And so that's really like you run inference on the edge. And here I know that the biggest problem is that basically there's memory requirements on consumer devices and so you can't really run large models on device currently. Any, any thoughts there or what kind of solutions you're seeing on that front?
00:45:32.045 - 00:46:29.355, Speaker C: Yeah, I think, I mean so the core issue is memory, compute and battery, right? If you're trying to run like even 7b on your iPhone right now, it can run it like it's. I think it barely fits and maybe you need to push everything out, out of memory, but also you're going to just burn your battery in like an instant with pretty quickly. And so those are kind of the challenges. Now there are different approaches to move forward from that. One is we've seen the small models are getting smarter, right? Like they stay in the same size but the kind of quality of the model is improving. And that's because you know, between distillation and a few other techniques, we're getting just a better way to train them. There's also research and Alex can add on how can we make models just cheaper to run.
00:46:29.355 - 00:47:25.953, Speaker C: Right. And so this can be lower quantization in different ways. So Alex has been running some experiments around that. But broadly speaking I think it will split into two, right. One is some kind of reasonably smart on edge model that you can interact with that provides you kind of quick response and kind of fills in the gap. And then you have indeed a remote large model which runs inside some privacy preserving technology that you can actually query for, you know, kind of more either like complex research questions or actions on behalf of you on the background. And either way, kind of if you want like for example, hey, send me everyday updates on what are the new candidates that I'm looking for, right? You don't want to run this on your edge device.
00:47:25.953 - 00:47:43.165, Speaker C: Right. Like you want some background process that will like crunch through, you know, 100,000 candidates, you know, analyze them and give you back the results. So you do need some infrastructure like that for background. So I think like this duality. I don't know if Alex, you want to add more on kind of smaller models.
00:47:44.385 - 00:48:09.815, Speaker D: Yeah, I think there is some interesting research happening with smaller models. These like one bit models, one and a half bit models, we're playing a little bit with them. But it's. I think that that potentially is a way even today, I think people already got pretty good at quantizing models so that they can be running on the edge. So there's even more that is potentially feasible in the near future.
00:48:10.635 - 00:48:30.925, Speaker B: Thank you. For sake of time, I'm going to just suggest we kind of move to other categories if you have more to share. Basically we talked about data, we talked about private inference. Yeah. Anywhere you want to go from here.
00:48:31.705 - 00:49:21.535, Speaker C: I mean there's a lot more on the data side. Right. Because there is data labeling, but also data labeling can only get that so much. So how do we get more data? We're running out of data. Is this about users data that is never collected in one place but you can actually train on it or is this about synthetic data that's generated and for specific use cases? Again, what we discussed before, kind of with self play, there is places where you can actually generate high quality data pretty effectively because you can actually simulate that it's correct one way or another. And so there's kind of a whole space of I call it novel data. Like where is the more information going to come from and how as well as up to date data.
00:49:21.535 - 00:49:53.049, Speaker C: Right. How do we get Twitter and what's going on there, how do we get what's going on in news, et cetera. So there's like a whole space there which there's a bunch of projects doing really interesting work both from kind of networks of indexing and scraping novel data to you can kind of pay someone to generate a lot of synthetic data to also having building the simulators effectively like AI based emulators for different environments.
00:49:53.137 - 00:50:08.365, Speaker B: Cool. Are there any other emergent categories that maybe we don't talk about now but you think are going to become important in the next months in the overlap of crypto and AI?
00:50:09.085 - 00:50:57.931, Speaker C: Yeah, I mean there's something that when I mean I invite everyone to join redacted in Bangkok or at least watch the stream, we're going to be talking how to tie together Actually AI chain abstraction, intents and actions, kind of queries and what traditionally Google does into one model that we think is going to be like how we going to interact in this user owned Internet with computing in the future. And so I think there's, there's, there it's, you know, it's about agent communicating, it's about how you, how you transact on Internet, it's about kind of all those pieces coming together. So this is just a little bit of alpha for, for people to, to join the redacted in Bangkok November 9.
00:50:57.995 - 00:51:45.983, Speaker B: To 11 all righty. Definitely a good teaser. So one thing I want to get your opinion on is on the application layer is basically like there's a lot of interest in AI agents participating in DeFi, right? Like whether they maybe some manage some money transfers some money between each other. But effectively there's an AI model in the background and then you take an output of it and then you take an action on it that has some monetary value. Right. And in those type of applications I see two problems. The first problem is the verifiable inference problem.
00:51:45.983 - 00:53:01.325, Speaker B: It's basically like if this is like a DEFI trading strategy bot, how do you make sure that it's actually doing that and not something else? And then the second problem I'm seeing, which is I think a bit more fundamental and I think personally I think a lot of people underestimate is security in LLMs, right? LLM security is very new and so they seem to be very fragile against adversarial attacks, things like jailbreaking, prompt injection, like data poisoning and all sorts of attacks. And I think in my opinion like this determines how much of AI will be for personal use versus how much of it can manage like shared state applications that involves many parties that anybody can call and there's some pot of money that people can call and then move around. Right? Because if you can do like attacks on it, you know, people will just do it and lots of money will be lost. So are you. I'm pretty sure you're given a lot of thought into this, so I would like to basically hear your thoughts.
00:53:02.785 - 00:53:38.241, Speaker C: Yeah, I can start and Alex can fill in on kind of how we're thinking about security overall. So first of all I want to kind of reframe things a little bit because it's really cool. You know, AI agents, it's hype and so you know, let's apply them to DeFi. But effectively this is just a hedge fund. Like you have a set of money and you're, you Know, managing this by buying, selling, you know, doing old kind of tools. And you're trying to use a machine learning model to do that. That's literally how hedge funds work, right? Like, I mean at least like the machine learning based hedge funds.
00:53:38.241 - 00:54:22.757, Speaker C: And so this is not new. Like this has been around for a long time. Like, let's not reinvent the wheel and actually like approach it in the same way you would approach a hedge fund. You have set of instruments and tools to execute trades. You have set of guardrails and conditions. Like I'm sure anybody who've heard of automatic hedge fund trading knows about a hedge fund that lost hundreds of millions of dollars because of a bug in their machine learning code. So there's a set of guardrails that you build around any kind of like automatic trading and you kind of go from there, right? So that's like fundamentally like what you're building is a hedge fund like automated with machine learning.
00:54:22.757 - 00:54:55.685, Speaker C: And so you should approach it in exactly the same way you would do it. And if you haven't like, if you don't know how hedge funds work, right? Like go first figure that out, then build, then build this. That's because I've met some people who are like, yeah, we're doing defi trading. And I'm like, yes, but what fundamentally you do is you're building a hedge fund. So that's the step number one. The interesting thing about near, what we've been building with chain abstraction is kind of an interesting way. In a way it's a prime brokerage for fintech people.
00:54:55.685 - 00:55:46.725, Speaker C: This is a prime brokerage system that allows you to trade across different chains. And well, if you're a hedge fund, what you want is prime brokerage that also will potentially have programmatic enforcement of specific rules. And so one thing you can do on NEAR is actually enforce different conditions. Like you can say, hey, you can only trade that much, you can only do these things. And this applies across your trades on different chains as well. And so NEAR is actually a really good platform to build this kind of AI German hedge funds and trade across all of the assets around kind of web3 ecosystem with all the L2s and layer ones and at the same time have set of rules that you want those things to follow to do this. But I think the fundamental question of security is indeed important.
00:55:46.725 - 00:55:57.617, Speaker C: And so, I mean, Alex has been spending a lot of time thinking kind of the formal verification is our specific point. But I'll let Alex kind of to expand on that.
00:55:57.781 - 00:56:54.407, Speaker D: I never thought of frau verification in this slide. But yeah, we do spend quite a bit of time on for all verification. The thinking here is that there's a good chance that with the AI models improving, formal verification will become more accessible. So languages like Lean have existed for a while and its predecessors like coq. And the only reason why we do not use more of formal verification in the day to day coding is that it takes a lot of time to prove something. It's like a very imprecise estimate, but for something that would take you half an hour to code, it would take you maybe less than half an hour to provide the statements about it. But that will take you like a week to prove them, right? So it's, it's very inefficient.
00:56:54.407 - 00:58:04.625, Speaker D: Right. But if you can have, if you can prove very quickly with the help of, let's say it's going to take you another half an hour or an hour to prove it, suddenly it becomes a reasonable trade off because if you, if you have formally proven something, you're saving on testing, you're saving on, you know, you're saving resources on like maintenance, et cetera. So it's, I don't know, with hedge funds it's a little more interesting, right, Because I presume that the idea there is that the model itself is not doing something in the symbolic space, right? It's like applying intuition to whether it wants to execute a particular trade. So it lends itself a little less to the formal verification there, I think. Yeah, you need some sort of, I guess you always need some level of trust to the person who deployed the model, right? Because, well, you either need to see the prompt, in which case that just works. If you see the prompt, then you're risking if it's a prompt lte. That's a fully working solution, right? You're trusting your money to the prompt, right? If that prompt results in a trick you didn't want to, that's unfortunate, but that's the risk you're taking.
00:58:04.625 - 00:58:34.207, Speaker D: But presumably the authors of the prompt do not want to share it, right? It's the know how, then it's. But you can definitely, I think that problem is solvable. Like, I'm pretty sure that you can have some way of effectively saying like, you know, you audit the prompt, then you have a TE which ensures that this is the prompt that is used. And there's like a set of people who give a stamp of approval saying, hey, that is indeed the prompt that was used. And then if all the money is lost, you have to reveal the Prompt. Oh yeah, you commit to the hash of the prompt. Yeah, exactly.
00:58:34.207 - 00:58:42.495, Speaker D: Okay, you can equal the hash of the prompt. If all the money is lost, you have to reveal the prompt. Yeah, that just works. It's not a particularly challenging problem.
00:58:43.155 - 01:00:25.191, Speaker C: I mean, there's like few sides, right? One is why I think the way we think of formatification is not just zooming out, right? Every single thing that people have built has bugs, and these bugs are getting exploited right on top of it. You can do data injections, you can do all kinds of things. And so formulification is the way to kind of get out of this loop where there's human flaws that are continuously accumulating and then resulting in catastrophic events. And so, as Alex said, the problem being it's really hard to actually make formal proofs as well as even the preconditions. But the benefit is if you have it now, when you're calling some other application, when you're calling some other agent and you have set of guarantees that you want to apply to this, you can actually enforce it and kind of almost like on a transactional level that like, hey, if this is not true about this model or about this piece of software, then this transaction should fail. And so kind of the idea here is that whoever this hedge fund build, they can, they should prove set of guarantees, for example, that they will not, you know, lose more than X amount of money or whatever and they will stop if that happens. And so then like, if you're putting money in, you can actually like, you know, provide set of enforcement that you want, right? That they need to have a right hedge mechanisms to like at any time to provide that they're not going to lose more than this amount of money.
01:00:25.191 - 01:01:14.645, Speaker C: And so now when this agent is trading, it needs to actually execute the hedges in the right way for whatever trades to kind of guarantee this. So you're kind of moving for verification not just for the code of the smart contracts, but also on a transactional level and kind of how you combine these transactions both for the users as well as for when agents are actually executing transactions. So that's kind of the. Again, like, if you think of from a systems perspective, what we want is we want to start guaranteeing some set of conditions to each other when we interact with different agents, be that other AI agents or software that we're interacting with. And kind of formal, formal verification framework is a way to exchange this kind of guarantees in a way between each other and enforce them continuously forward.
01:01:15.185 - 01:01:59.045, Speaker B: All right, switching gears a bit. I also want to focus on distributed trading, just because it's a very large addressable market. A high impact potentially can have a very high impact. So yeah, I want to get your. Basically how much probability do you assign in actually distributed training? Training models at scale that can compete with centralized counterparts? And I know this is a very challenging kind of problem. It's like there's different challenges here. The first one is technical.
01:01:59.045 - 01:03:14.017, Speaker B: The moment you do distributed training, you have to pass a lot of data between nodes. Although there seems to be some solid progress on that front from teams like Private Intellect, News Research, they're really reducing the amount of data that needs to be transferred between different training nodes. The other problem is accessibility problem. Basically there's a scarcity of high quality GPUs just because Web2 giants buy all of them. And then the third one, which I think is also equally important, is a incentive mechanism like challenge. Right, like how? Basically like if you're, if you're training a large model and then if the, if the, if the weights are open at the end of it, like how do miners that contribute to that training basically reap the rewards, right, like, of putting all that effort into the model? So different challenges considering all of this, like how much probability do you assign in viability of this?
01:03:14.201 - 01:03:50.375, Speaker D: I think it's extremely viable. And I think the first two problems are not insurmountable at all. Right. So in terms of data passing, it's not just the amount of data, it's also the speed of transferring the data. Because the way all the existing algorithms are assuming that you can transfer data very fast. Right, so, so one of the problems there is not just to reduce the amount of data, but also to make the training process actually resilient to the latencies. So that there's a couple of papers from DeepMind, you know, like G Loco, that's, I think what Prime Intellect is using.
01:03:50.375 - 01:03:55.015, Speaker D: I'm less, less familiar with what Nose is doing.
01:03:55.925 - 01:03:59.205, Speaker B: News is distro, I think. Yep. But go ahead.
01:03:59.245 - 01:04:26.311, Speaker D: Okay. Right. But even if those papers are not quite getting us there yet, I'm pretty sure that we are, you know, very close to that. I, I don't think there's any fundamental problem with, with the latency. And the only reason why the existing algorithms are not latency friendly is just because, you know, the companies that were advancing the AI recently have had access to Infiniband and their clusters. So they have no incentive to even look into the problem of the latency.
01:04:26.383 - 01:04:33.303, Speaker B: Well, they have negative incentives, not even zero. They have negative incentives to look into it and well, I don't think they.
01:04:33.319 - 01:04:40.595, Speaker D: Were thinking about the problems of others until chatgpt. I don't think there was a big concern with external actors trying to can those large models.
01:04:42.495 - 01:04:43.095, Speaker B: In terms of.
01:04:43.135 - 01:05:29.475, Speaker C: Well, it's worth pointing out Deloco is from DeepMind, which so they clearly have not a negative incentive. So yeah, okay. And for context, like Google pre effectively transformer model and kind of this like what OpenAI kind of pioneered this training. The way Google trained like large models is very much so not on Infiniband. And it was a very much like parameter server kind of worker model which is like DLO is kind of reinventing the same thing that Google was doing before and published a paper that started deep learning with Jeff Dean and Andre kind of back in 13. So it's kind of, you know, we go in spirals.
01:05:30.175 - 01:05:32.895, Speaker D: But they wouldn't call it disbelief if they believed it was good.
01:05:32.935 - 01:05:33.515, Speaker C: Right.
01:05:34.615 - 01:06:03.229, Speaker D: So in terms of GPUs, I think GPUs are sort of available right now. Right. If you go to like between Akash exhibits and others they have. I actually don't know the number on top of my head. Maybe it's not, maybe it's not even remotely enough. But I think if one also the question is how big of a model do we want to train in this decentralized manner? Right. So I mean if we want to train a GPT5, that's a separate question, then we probably need to pull more resources.
01:06:03.229 - 01:06:44.155, Speaker D: But then the question then before we even get to the question of where we get the GPUs, we need to ask the question where do we get money to get those GPUs from? Right. So we need to orchestrate this and then we need to solve the third problem you mentioned. I think there will be solutions. I think you can sort of group them into two categories. One is using existing legal structures, right? So you can just say, hey, this is the model and the license of the model is such that you have to pay. And this is Ilya's point from back in the day. But effectively the companies that are big enough will have to pay because they have legal kins and they're not going to do, you know, weird things.
01:06:44.155 - 01:07:16.339, Speaker D: And the companies who are small enough or the, or the entities, you know, in the jurisdictions that are known for not complying with the laws, they will, they would find a way to, you know, to bypass it anyway. Right. So that might work. Something else that is interesting is you can potentially. I didn't think too much about it, maybe it's not possible, but I'm pretty sure that it's constructible. You can effectively make it such that the model never exists in the open. So the code is open, the data flow is open, but the model itself is never seen by anyone.
01:07:16.339 - 01:07:52.375, Speaker D: It's always in the trusted execution environment. It's fully trained within the key and the only way to bring inference from it is to provide the proof that the payment was made on near like a light client proof, right. So that you know all the inference and like it has to be very small payment. Right. Let's say it's a very cheap model, but you do have to pay to use it, right. So that you can train. If we find a way to train a competitive model, which is problem number four, you didn't mention your model needs to be better than Llama if you train, if you train at 10B and it's worse than llama 7B, you know, the question of payment doesn't, doesn't arise.
01:07:52.375 - 01:08:17.011, Speaker D: So if you manage to train a competitive model, that's like an interesting approach. So I'm pretty sure there are other approaches, but I think all the problems are definitely solvable and hopefully one of the existing entities or one of the new ones, but hopefully someone will actually pull together enough resources to get to the state. And we're also looking into it.
01:08:17.043 - 01:08:17.211, Speaker C: Right.
01:08:17.243 - 01:08:43.635, Speaker D: But well like in the short term we definitely don't want to compete with other players, right? But maybe as we get to the bigger models like GPT5, GPT6, once we have to compete with those we need to see, right. Like if hypothetical prime intellect or moves get to the state where they actually can pull that much resources, I think that will be an outstanding outcome for everybody involved. Right. If that doesn't work out, maybe we will get more involved.
01:08:44.585 - 01:09:44.875, Speaker C: And I think what Alex mentioned, the question is how do you get to a competitive model is actually what we've been looking more at is how do we actually incentivize research? Because the training is the outcome of potentially months and months or men, years of research that need to be powering then into training this large model. And so that's kind of where we've been looking at because the reality is right now you have a few of these closed labs, whereas the research is reasonably effective. Right. I trained something Alex can tomorrow pick up, see what worked, what didn't incorporate into his model and do this again. What happens in open source right now, somebody somewhere did something, they put it on hugging face. But like there's 5,000 things in hugging face every day, nobody can see it. The paper comes out, you know, for a conference in, you know, in six months.
01:09:44.875 - 01:10:13.031, Speaker C: Then, you know, you see that you're like, oh, that's interesting. You go to the source code, it's completely on different, like in different way. It's not completely reusable. You're not sure if the results are correct. Like the data set was different and so you're not able to like reproduce and kind of build on top that experiment that somebody else did. And so that's actually what, you know, I would say like being in what in this research environments where kind of. Right.
01:10:13.031 - 01:11:03.275, Speaker C: You know, we see they're making really rapid progress compared to open source. It's just like such a, you know, kind of stranded environment. So that's kind of where we've been spending more time is like, how do we kind of, how do we coordinate research more effectively in open source in such a way that like, when there is enough novel kind of developments both in model architecture and training and data set refinement in other aspects, you can then go and train like one of those larger models. But first you kind of need to have this innovations and as kind of as research. It's a lot of experimentation, a lot of failure. And so how do we increase the speed of failure pretty much so that we can come toward golden nuggets of innovation from this?
01:11:04.775 - 01:11:30.555, Speaker B: In terms of pulling talent, do you see a problem where just because of the crypto having a bad kind of image and having a pushback from AI community on a personal level, are you seeing that kind of pushback or not really AI people being dismissive of crypto, basically?
01:11:32.415 - 01:12:20.897, Speaker D: I think there is a meme on the Internet which I sort of agree with where they, I think generally in the. I think historically AI was attracting people who are less generally attracted to crypto because the lifestyle in AI was, you know, like you would have to go to the school, you know, like things like tenure. You would either aspire to having a tenure in a university or going to Google or open AI if actually the. It's an antithesis of what a person who would like crypto would want from life. Right. So it was attracting a very different kind of ground. But it's changing now, right? Because today, you know, AI is so deeply, you know, getting embedded into our lives that all sorts of people get attracted to it.
01:12:20.921 - 01:12:21.033, Speaker C: Right.
01:12:21.049 - 01:12:36.861, Speaker D: So there's more and more crypto anarchists who started looking into any last year, year and a half ago, right. Who right now getting to the state where they're very good. Right. And so I think we will have. There will be quite a few people who would not want to work for Google or OpenAI or other big entities because that's against their values.
01:12:36.893 - 01:12:37.037, Speaker C: Right.
01:12:37.061 - 01:12:44.465, Speaker D: But who are very good and will want to contribute. So I think it's going to be. There's not going to be a big problem of finding people who push them.
01:12:44.805 - 01:13:31.543, Speaker B: Yeah. And I think that crowd is quite large. It was surprising to me, but based on what I've been seeing and hearing, I think many people really just leave those companies because of their kind of ideologies, which is great to see. Okay, so I've been putting you guys hostage for over an hour now. So I have one last kind of fun question. This is meant to be like a fun quick one. I'm basically going to list, I'm going to name some tools, platforms, technologies and I'm going to ask you to rate them from 0 to 10 to tell me like how relevant do you think they will be within the next two years.
01:13:31.543 - 01:14:02.885, Speaker B: So this is not like a long term thing. This is basically very solid, concrete. Within the next two years what role will these play in the applications that we will use? Okay, so no, no wrong answers here, but no right answers. But I'm basically. I want to see if you guys are going to differ from each other. Chain abstraction 7, 9 I thought you guys going to say 10 each but both of you. But okay.
01:14:02.885 - 01:14:07.565, Speaker B: Agent to agent micropayments using crypto.
01:14:09.905 - 01:14:14.431, Speaker D: 9 10. All right.
01:14:14.663 - 01:14:30.795, Speaker B: ZK TLS web proofs 4 3. That's disappointing. Okay. Non transformer models minus 7.
01:14:34.055 - 01:14:37.567, Speaker C: I mean there is a Falcon member. Right. So okay.
01:14:37.591 - 01:14:38.555, Speaker D: Minus six.
01:14:42.065 - 01:14:42.489, Speaker B: All right.
01:14:42.537 - 01:14:50.633, Speaker C: Very part of me. I want to see other models. It feels kind of. I know it's good to.
01:14:50.809 - 01:14:52.725, Speaker B: You should write another paper Ilya.
01:14:53.385 - 01:14:59.729, Speaker C: I know if one day I will have time. Yeah.
01:14:59.897 - 01:15:08.020, Speaker B: Verifiable inference and then I want separate answers for each. So ZKML1 yeah like one.
01:15:08.267 - 01:15:09.257, Speaker C: Nice.
01:15:09.505 - 01:15:21.067, Speaker B: Okay, good to know because optimistic ML OPML1 optimistic ML.
01:15:21.251 - 01:15:22.735, Speaker D: Let's see. Zero.
01:15:25.235 - 01:15:37.153, Speaker C: The problem is optimistic ML is that the math you do on the GPU is different from how would you do math on chain or one cpu. And so you're floating operations.
01:15:37.329 - 01:15:37.593, Speaker D: Right.
01:15:37.609 - 01:15:38.569, Speaker B: Non deterministic. Right.
01:15:38.617 - 01:15:42.937, Speaker C: Like a challenge. You cannot replicate the challenge on another hardware.
01:15:43.121 - 01:15:50.685, Speaker B: Does the same problem exist in the consensus based case where you have set of end nodes and you want a threshold of them to agree.
01:15:51.265 - 01:16:10.501, Speaker C: If you need to force them all to run the same hardware then you're kind of okay Exact same hardware. But your challenge is supposed to be run on something that's different. Like presumably on CPUs or something. Right. So unless your challenges are also run on H1 hundreds. But if you do that then yeah. So anyway.
01:16:10.613 - 01:16:14.901, Speaker B: Okay, can we quantify this consensus based verifiable inference?
01:16:14.973 - 01:16:16.185, Speaker D: Let's say one as well.
01:16:17.445 - 01:16:20.385, Speaker C: I mean consensus. Yeah, I would say like three.
01:16:21.365 - 01:16:24.389, Speaker B: That we're not having verifiable inference anytime soon. Okay.
01:16:24.437 - 01:16:28.549, Speaker D: Fh, it's going to be, it's going to be trusted environments.
01:16:28.677 - 01:16:31.941, Speaker B: Right? That was the next one, but okay, yes, correct.
01:16:32.133 - 01:16:42.597, Speaker C: But it's both verifiable and private. Like you get both optimistic and consensus. You don't get privacy. Okay.
01:16:42.741 - 01:16:45.225, Speaker D: No, I think in the next two years it's going to be 10.
01:16:45.965 - 01:16:55.021, Speaker B: Nice, nice. All right. FHE. I'm talking about crypto applications, not like FHE and other stuff.
01:16:55.053 - 01:17:06.285, Speaker C: Yeah, I think six on my side because we'll start seeing rag use cases with fhe. Yeah, it's a close call.
01:17:06.325 - 01:17:09.825, Speaker D: Depends on how far it gets in the next few years. But yeah, let's say six.
01:17:10.845 - 01:17:17.625, Speaker B: That was surprising. I was expecting lower AI website builders like websim. I know this one is your favorite, Alex.
01:17:18.485 - 01:17:19.477, Speaker D: Yeah, that's going to be Fun.
01:17:19.501 - 01:17:28.621, Speaker C: At its 10th formal verification, 11 final.
01:17:28.976 - 01:17:41.395, Speaker B: 1/0 order training train without backpropagation. Basically, as you do inference, you also train the model at the same time. I heard about this from news team.
01:17:46.745 - 01:17:48.793, Speaker C: Need to read the paper.
01:17:48.929 - 01:17:49.965, Speaker B: All right. All right.
01:17:50.825 - 01:17:52.725, Speaker D: The number is right there in the name.
01:17:56.745 - 01:18:10.445, Speaker B: All right. Well gentlemen, this was a pleasure. It was a lot of fun. Thanks for joining. Do you want to share anything in terms of like what you're doing in the upcoming weeks where people can find you? What should they look at?
01:18:12.795 - 01:18:51.975, Speaker C: I mean the biggest thing is in Bangkok we have redacted where we're going to be sharing a bunch of stuff both on AI research side, on kind of AI to AI interactions on chain abstraction and kind of how all of those things come together into one product story and one vision of user on the Internet. So I'm inviting everyone to join. I'm sure we'll have a live stream as well. If you are not in Bangkok. And then from there we'll have, you know, we'll have things for people to engage with. It's going to be open source and everybody will be able to really dig in. So yeah, stay tuned.
