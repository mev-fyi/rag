00:00:00.400 - 00:00:35.877, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors NIA Empowering Decentralized Applications and Blockchain ecosystems. Venice AI, a private and uncensored alternative to popular AI apps. MIRA Unified AI infrastructure secured by crypto and the first on chain multi agent system. To learn more about all of our sponsors, check the description below and dive in.
00:00:35.877 - 00:00:37.145, Speaker A: Enjoy the show.
00:00:50.725 - 00:01:03.045, Speaker B: All right, hey, this is Pondering Durian with Delphi Digital and we have another exciting one for DAI Month. I'm sitting here with a very special special guest, Tory Green, CEO of iNet. Tory, welcome.
00:01:03.385 - 00:01:05.965, Speaker C: Thank you. Happy to be back on the pod.
00:01:06.305 - 00:01:10.205, Speaker B: Yeah. Repeat, repeat guest. You know, I guess that means you made it.
00:01:11.545 - 00:01:13.965, Speaker C: I guess so. Not a better place to be, right?
00:01:14.305 - 00:01:29.035, Speaker B: Yeah. So maybe we just do quick, quick intros. I think a lot of our audience will already be familiar with you and Ionet, but would love to just learn a little bit more about your background and then can dive into my hard hitting question list of some of the updates on Ionette.
00:01:29.405 - 00:02:22.545, Speaker C: Sure. So background started off pretty standard, was did the banking thing, did the PE thing, vc and then took a bunch of operating roles at different portfolio companies like coo, CRO, cmo. And it was a great start for me. Learned a lot about how to scale a Web 2.0 business, but I knew it really wasn't for me. 2016, crypto came along like many of us, went down the rabbit hole, made a lot of money, lost a lot of money, learned a ton and really, really wanted to get into crypto full time, but was hesitant because I felt like most protocols out there were effectively solutions looking for problems and that is, they had no real demand. This all changed in 2019 when I became the CEO of Machine Learning Fintech.
00:02:22.545 - 00:02:57.219, Speaker C: And that's when I really got to understand some of the different challenges facing AI, most notably the shortage of compute. And that's when I had a realization that basically became an obsession that decentralized networks were not only the best way to solve this problem, but perhaps the only way. And eventually that's what led to Ionet. And it has not been an easy journey. A lot of naysayers, a lot of challenges, but we had a lot of allies. We had a great team, great investors, and we just busted our ass for a year. And you know, I think things have turned out pretty well.
00:02:57.219 - 00:03:08.815, Speaker C: We raised 40 million from top tier investors, including you guys. We, our foundation launched a token day one on Binance and we built the world's largest decentralized AI compute network.
00:03:10.035 - 00:03:21.467, Speaker B: Yeah, not, not bad for a couple years here. But so, so I guess my job here is to be a little bit of a naysayer and push back. Even though I'm an investor who clearly. Go for it.
00:03:21.611 - 00:03:24.315, Speaker C: Go for it. We welcome anything.
00:03:24.615 - 00:03:59.285, Speaker B: Yeah. But I guess just going up against some of these massive incumbents is not a short order. I'd say it's a pretty tall task. And so I've heard kind of in the explain it like I'm five narrative, you and others describe Ionet as an Airbnb for GPUs or even a decentralized AWS. Right. And so I am curious to try and understand the costs and the trade offs that you guys will pitch customers relative to some of these large incumbent solutions. Because these are the biggest companies on earth and it is difficult to take them on.
00:03:59.285 - 00:04:07.305, Speaker B: So kind of when you go out and try and onboard customers, kind of what is the pitch relative to some of those other solutions?
00:04:07.645 - 00:04:47.445, Speaker C: Yeah, that's a great question. And to sort of step back for a second. I don't think there are any trade offs in the long run. I think that decentralized compute networks will end up being better, faster and cheaper than anything else out there. And to understand why, you really have to understand the concept of disruption. Disruption is a word that's used a lot, but often incorrectly, but there's actually a very academic, nuanced definition of what it is. And basically disruptive products start out almost always as less performant than the status quo, but they offer one or two things that the status quo can't.
00:04:47.445 - 00:05:10.305, Speaker C: Just to give you an example of this, the sort of original disruptor was the telephone. When the telephone came out, it was much worse than the telegraph. It could only go a couple of miles and so no business could use it. Right. Because they needed to do intercontinental conversations. The only people that really used telephones were small local businesses. But that enabled the telephone to get a foothold.
00:05:10.305 - 00:05:49.949, Speaker C: And then eventually performance got better and better. It moved up market to where it completely replaced the telegraph. I think that's where decentralized compute networks are now as you mentioned, the benefits of decentralization are they're very obvious. It's the ability to attract nearly unlimited compute power at savings of 90% cheaper than Amazon, Google or Microsoft. The one area that I think we do have to work on is the performance. Now for certain use cases I actually think decentralized networks are more performant. We have found that for inference they're even better because they're effectively acting as a CDN for inference.
00:05:49.949 - 00:06:35.505, Speaker C: And so if you have a, someone builds an app in Vietnam, they would much rather get a data center in Singapore that they could get off of a network like Ionet versus work with AWS where they have to ping Texas and they have higher latency. And so I think decentralized networks have largely solved the inference problem. Where there does still seem to be a problem is in training. All things being equal, it's generally easier to train a model in a centralized fashion. But that too is changing. I don't know if you saw the research by Nous, they're doing some very exciting things. And so, you know, right now when we are dealing with customers, we are sort of giving them what they need at up to 90% cheaper than the incumbents.
00:06:35.505 - 00:06:47.645, Speaker C: And so that works for them. And you know, that may be that we're focusing more on inference now, but I think as this market matures, eventually it will become clear that decentralized networks will completely subsume centralized.
00:06:48.205 - 00:07:32.901, Speaker B: Yeah, I mean one of my favorite like business strategy thinkers is Clayton Christensen. You know, innovators dilemma. Like all those examples are just spot on, right. And so I think like, I guess like in my reports I did kind of write a little bit about like the shift towards more modular compute networks over time. And you know, it seems like it might be slightly less performant today, but you know, similar along the, along the same veins. Just like if you come up with a more modular, slightly more commoditized solution that has slightly different attributes, it can basically like sneak up on the incumbents even though it's not like necessarily logical why it would be able to take them on head on based on the current frameworks.
00:07:33.013 - 00:08:02.111, Speaker C: So that's actually the main reason I got into crypto. Read the Internet. Vader's dumb. A long time ago as a vc, thought about it a lot. And when I looked at some of the different emerging technologies, like virtual reality for example, I thought, well, this is obviously going to be owned by Facebook or Microsoft. And really the only technology that popped out in my head that had a true competitive moat was crypto. And once again it is because of the innovators dilemma.
00:08:02.111 - 00:08:12.535, Speaker C: The large companies cannot, they structurally cannot take on crypto companies without completely cannibalizing their own business model. So I think they're stuck and I think their time is limited.
00:08:13.315 - 00:08:31.883, Speaker B: Got it. All right, well time, time will tell. But I like the. I like the attitude. Okay. And then shifting a little bit to Ionet's differentiation within decentralized offerings. So obviously there's a good number of projects, both, I guess, in kind of Web three, as well as certain Web two marketplaces that play in a similar domain.
00:08:31.883 - 00:08:39.927, Speaker B: So what do you think Ionet does differently than some of these other platforms and that you hope to outperform them?
00:08:40.071 - 00:09:02.755, Speaker C: This is a fantastic question. It's one I actually get a lot from investors, most notably because decentralized compute networks have been around for a decade and none of them have really succeeded. So the question is, why? And perhaps more importantly, what makes Ionet different? And the answer is in something called decentralized clustering. Are you familiar with clustering?
00:09:03.545 - 00:09:07.233, Speaker B: I am, yeah. But feel free to give, like, a quick overview for the audience.
00:09:07.249 - 00:09:47.187, Speaker C: Yeah, I'll explain it for the audience. In simple terms, clustering is just the ability to take multiple computers and have them work together as a single powerful supercomputer. Clustering is essential for AI. If OpenAI tried to train GPT3 on a single computer, it would have taken them 10 years. Obviously, with a cluster, they could have done it a lot faster. And so every major company using AI, from Uber to Instacart, is using clusters for inferencing, training, nearly everything. Now, the problem is, in the past, to cluster computers, or GPUs, they had to be in the same physical location.
00:09:47.187 - 00:10:24.547, Speaker C: You had to literally plug them in together. And this presented a problem for DPINs because DPINs are generally scattered all over the globe. And so most D pens could not create clusters unless they had several in a single location. This kind of invalidates the whole Airbnb for GPUs idea right there. What Ionet pioneered was a way to build decentralized clusters. So we can take 50 GPUs from Iceland, we can take 50 from Bangkok, we can take 50 from the US and we can create a global supercomputer. And what this allows us to do is effectively get the best of both worlds.
00:10:24.547 - 00:10:56.347, Speaker C: We can get the benefits of decentralization, so the access to unlimited compute, the lower costs, but we can also serve the enterprise. And quite frankly, this is what makes us unique. This is why we're growing so quickly. We've 10x our daily earnings over the last three months. This is why we've gotten several Web2 customers, like Creo, Endera, Leonardo. This is why most of the other compute depends are suppliers of Ionets. So we've got Aether on board, we've got filecoin on board, we got render on board.
00:10:56.347 - 00:11:01.415, Speaker C: Simply put, what makes Ionet different is we're effectively the deepen for AI.
00:11:03.155 - 00:11:47.055, Speaker B: And I am curious to understand how you're thinking about getting this flywheel going. So I know like you've had a lot of success in onboarding supply. I think it's, you know, 1 million unverified GPUs, over 300,000 verified. So in terms of onboarding supply, like, you guys have done an incredible job and clearly our leaders there. But obviously building a network takes two sides and the demand side is always a little bit trickier, typically in crypto. And so I am curious, kind of in your go to market, it does sound like in recent months you've seen an inflection on the growth side. But curious to hear kind of where that's coming from, both like geographically and kind of the types of customers that you guys are targeting.
00:11:47.355 - 00:12:25.491, Speaker C: Sure. Well, I think sort of stepping back and looking at what our GTM is and I can get into our specific GTM strategy in a bit, but I think the core of it is that simply we're offering a much better product than centralized cloud providers. As I mentioned, we're up to 90% cheaper. And so a lot of it is just really boots on the ground, getting out there, talking to customers and growing that way. When I think about our specific GTM strategy, let's start with the short term first. We're extremely disciplined. We have a very defined beachhead customer which is generative AI startups.
00:12:25.491 - 00:13:00.043, Speaker C: This is potentially, I think McKinsey says that generative AI will add 4 trillion of economic value and these guys have a serious hair on fire problem. I actually just spoke to a founder From a generative AI seed stage startup that is spending 700,000 per month on AI compute. 700,000 per month. That's completely unsustainable. And so these guys have a huge heroin fire problem. They're the perfect customer. We have basically three ways to reach them in the short to medium term.
00:13:00.043 - 00:13:26.421, Speaker C: The first is we already have a pretty good roster of customers. We have over 56 partners. They come from, they're all kind of different generative AI apps from all over the world. A lot of them are already ramped up. A lot of them are coming on in the trial phase. And so I think we will continue to see customers move from trial to full time. And then the ones that are happy with full time, they'll start to do more things like maybe they start with inference and they do training.
00:13:26.421 - 00:14:05.607, Speaker C: And so we're Going to just see a lot of increased growth from our own customer base. Second is we want to build a long term pipeline. We're currently hiring for a ton of sales positions, everything from AES to SDRs to Solutions Architects. And we're actually interviewing, we want to steal from our competitors. We're interviewing people from aws, coreweave, Lambda Labs, get their playbook, build out the funnel like that. And then finally in the short term we are looking to do several channel partnerships and that could be with other customers, it could be with AI tooling companies like Predis awaits and biases. But I think that will get us a lot of short term revenue.
00:14:05.607 - 00:14:34.935, Speaker C: As I've said, we've done a great job so far. We're up to 25,000 in daily earnings and I think we can continue to push this for quite a while in the long term. I think once we conquer this beachhead market, there's still so many opportunities you've got. We can maybe start to service big tech companies. We can maybe service AI divisions of enterprises. So think JP Morgan, think Procter and Gamble. There's academia, there's governments and eventually even the LLMs themselves.
00:14:34.935 - 00:14:37.683, Speaker C: Themselves. Have you found just.
00:14:37.819 - 00:14:59.027, Speaker B: Yeah, I was just going to cut you off like have you found that like enterprises and some of these larger institutions, like do they find crypto to be a sticking point? Like I do imagine there's you know, conservative legal departments that unfortunately make those discussions quite, quite difficult even if you have a superior offering. So is that an area of pushback for now?
00:14:59.171 - 00:15:29.305, Speaker C: That's a great question. And that's why we've, we've done the go to market the way we've done it. The idea is to service the people where price is the biggest problem. You know the ones like I said with the hair on fire problem. Yeah, that's the generative AI startups. Once we get proof of concept there, I think it becomes much easier to make the case for larger tech companies or other types of enterprises. And I think there's sort of a couple answers to this question.
00:15:29.305 - 00:16:07.555, Speaker C: The first is like our clients actually don't ever have to know that they're doing crypto. They can pay in fiat and they'll be done. Most of the crypto stuff happens on the backend, right? We take the fiat, we convert it to crypto and we can pay the suppliers out in that. But for them, for all intents and purposes we are a Web2 company and that is how we present ourselves to the enterprise world. The other thing that enterprises are Very concerned about is obviously security and data protection, and we are doing a lot on that end to build trust with them. First, we're ramping up more SoC2 compliant GPUs on the network. Second, we're running a pretty tight internal process.
00:16:07.555 - 00:16:28.447, Speaker C: We encrypt almost all of our data. Very tight access. Even as CEO, I can't access any data. And we have very, very tight monitoring. So we know who comes in, looks at data, if anyone makes any changes. And then finally, we're building up a very strong security team. We actually just made a killer hire, a guy named Chris Papafenaso.
00:16:28.447 - 00:16:53.855, Speaker C: He was the chief information security Officer at Kareem. Not sure how much time you've spent in the Middle east, but Kareem is like the Uber for the Middle east, so huge hire. He's on there now. He's building a team. And I think, you know, at the end of the day, enterprises aren't that different from everyone else. They want to get the best cost. And so I think once we show proof of concept and once we show that we are secure, I think we will start to see more and more enterprises migrate.
00:16:54.665 - 00:17:02.369, Speaker B: All right, putting. Putting the flag in the ground. Sounds like you guys are hiring on a bunch of different roles across sales and security, so feel free to ping.
00:17:02.417 - 00:17:16.145, Speaker C: Tori, I. I had 12 open positions last quarter, and I was doing 15 interviews a day for almost three months. It was like, I. I was just like, I. I don't know how long I can keep this up.
00:17:16.185 - 00:17:19.435, Speaker B: So, Chief, Chief recruiter. I like it. I like it.
00:17:19.585 - 00:17:24.887, Speaker C: Well, honestly, I think that's the core job of a CEO. You gotta get the right team in there.
00:17:25.031 - 00:17:59.171, Speaker B: You gotta. You gotta. You gotta get the right team and you gotta find the money. Those are the two pillars. Okay. And then shifting gears a little bit, like, I am curious how the supply constraints situation shifting over the last year has impacted Ionet, right? So I think like 20, 23, there was this massive demand supply imbalance, and it seems like it's eased up slightly and at least, like, there's no longer as crazy of a scramble for GPUs. Is that something you've seen or am I reading the tea leaves incorrectly? Like, you definitely have a better vantage point than I do.
00:17:59.363 - 00:18:01.971, Speaker C: I think it's a meme. You think it's a meme?
00:18:02.003 - 00:18:03.291, Speaker B: You think it's a meme? Okay.
00:18:03.483 - 00:18:38.155, Speaker C: I've been reading a lot of the same things, but the fact that I keep coming back to is there's a great quote by Chris Dixon that for Every important compute resource in history, demand has outplaced supply. I don't see this changing with AI. And there's a couple reasons for this. Number one, the rate at which demand is growing is unprecedented. The processing requirements for machine learning are literally doubling every 3.4 months. So that's 10x growth every 18 months and 100x growth every three years.
00:18:38.155 - 00:19:11.409, Speaker C: Even if this slows down, the supply has to keep up with that, and that's going to be extremely difficult. Second, sure, Nvidia is releasing more chips, you're seeing more supply in the market, but that's only a part of a very intricate supply chain. I mean, like, let's start at the beginning of the supply chain, right where they're sort of digging the silicon out of the ground. Anytime you're dealing with natural resources, there's potential shortages. Supply booms there. So that's one problem. But let's say we solve that and then we move on.
00:19:11.409 - 00:19:46.325, Speaker C: Okay, now we make this chips so great. Nvidia's made the chips, now you got to get the chips to where they're going. And once again, this is international supply chain, which is there's all kind of difficulties to shot with. And even in the last couple years we've seen, you know, war in Russia, the pandemic, all these things have disrupted the global supply chain. Who knows, China invades Taiwan. There's all kind of issues that could cause disruption in supply chain. And then finally, even if you get the GPUs to where they need to get, where are you going to put them? Like the data centers are getting full, so you've got to build new data centers.
00:19:46.325 - 00:20:10.925, Speaker C: And that's not easy because that requires permitting, that requires construction, lead time, that requires a lot of energy. You usually have to build these things next to power plants. And so it leaves you with a situation where we know demand is going crazy and the supply chain is fragile. And so sure, there may be ebbs and flows, but if there's one thing I'm not worried about, it is the continued imbalance of supply and demand in the long run.
00:20:11.585 - 00:20:33.891, Speaker B: I think that is a very fair take, just looking at a lot of these curves and the demand. So it'll be interesting to see where we get in about a decade's time in terms of the amount of just raw compute on the planet. All right, so, so now we're kind of hitting our midpoint. I was going to go for a quick rapid fire round of overrated versus underrated. Are you, are you game?
00:20:34.033 - 00:20:35.407, Speaker C: Let's do it. Let's do it.
00:20:35.511 - 00:20:42.487, Speaker B: All right. All right. First off, the scaling laws. Overrated, underrated, neutral.
00:20:42.631 - 00:20:53.275, Speaker C: I think, I know they're sort of breaking, but I don't think I have a strong opinion either way as to where that will continue.
00:20:54.015 - 00:21:11.325, Speaker B: Yeah, I think it's, it's difficult to tell. We will test the limits again in probably the next six to 12 months. Yes. But yeah, like my point is it's, to me, it is difficult to tell until you test it, which is why everyone's making these massive investments. So. Tbd. Tbd.
00:21:11.325 - 00:21:15.819, Speaker B: But it's definitely exciting. I'll be on the edge of my seat for the next six to 12 months.
00:21:15.898 - 00:21:16.218, Speaker A: Yeah.
00:21:16.297 - 00:21:19.345, Speaker B: All right. Number two, Nvidia's current valuation.
00:21:22.485 - 00:21:28.559, Speaker C: I think undervalued in the long run.
00:21:28.607 - 00:21:29.695, Speaker B: Under. Undervalued.
00:21:29.735 - 00:21:54.635, Speaker C: Okay. I mean, it's so tough. There's, as you know, I put my. So it's interesting because I've had a career as a PE guy and as a vc, as a PE guy, I'm going to definitely say overvalued because it's way over any reasonable metric. But knowing the long term path of GPUs, I think that there's, in knowing the growth of demand, I think that it could potentially be under.
00:21:55.545 - 00:22:07.921, Speaker B: Okay. A little bit of a contrarian take these days. Under. I like it. All right, number three, switching gears a little bit, going into a social component. Universal, basic, income neutral.
00:22:07.993 - 00:22:39.175, Speaker C: Well, I, so this is an area I'm very different. I, I don't think we're going to see, like, I'm not saying AGI won't happen. I don't think we're going to see AGI for a long time. My personal belief is that machine learning is actually not on the path to AGI and so we're going to need some other way to do AI to get there. So I think the main idea for UBI is to support that. And so I'm going to go ahead and say that's over. Highly overrated and not needed.
00:22:39.755 - 00:23:04.415, Speaker B: Okay. Okay. Yeah. I also think the most recent studies that Sam Altman supported came back like, relatively ineffective in terms of the, you know, proposed hypothesis. And like, benefits to people, like, proved to be largely very temporary, unfortunately. All right, number, number four, Zuckerberg's commitment to open source.
00:23:05.315 - 00:23:07.291, Speaker C: I love it. I love it.
00:23:07.483 - 00:23:16.877, Speaker B: But, but, but do you think, but do you think he's there for the long haul? Do you think he'll stick around? I guess is kind of the question. So obviously what he's been doing is amazing. But do you think it will last?
00:23:16.981 - 00:23:22.025, Speaker C: I can't tell, but what he's doing now is great. So you know, you have to give him kudos for that.
00:23:22.645 - 00:23:49.505, Speaker B: Okay. And then I'll switch number five. Just based on what you've said already. But I'm curious, based on what you've said on under what timeline would you expect us to reach AGI given you expect it to be a little bit more extended, you know, because there's a lot of people that come on the show and they've like read the Leopold Ashenbrenner thesis and get scaling pilled and think like we might, you know, hit AGI in like two to three years. Curious to get your sense of the timeline.
00:23:50.205 - 00:24:34.049, Speaker C: So it's a very interesting question and probably not a rapid fire answer, but when I. So a bit of my background when I took over as a CEO of Machine Learning Fintech, I really wanted to learn like AI from a code level. And so I took, you know, 400 hour coding boot camp on AI. I'm by no means a developer, but I think I understand it decently from a conceptual level. And like I said, I do not see a path for machine learning to develop into AGI. I mean, if you think about how AI is working right now, it's really, it's, it's. I know it's cliche, but it's pattern matching and it's not even pattern matching on words, it's pattern matching on ones and zero.
00:24:34.049 - 00:24:53.525, Speaker C: And so right now it just has no ability to think effectively. And I see that as being a major hurdle to the long term development of AGI. And so I would hesitate to even offer prediction on that until I saw a new module of AI that I thought maybe could develop into AGI.
00:24:54.225 - 00:25:24.005, Speaker B: Okay, okay, some contrarian takes here in overrated versus underrated from Tori. I like it. All right, we're moving on to decentralized training momentum. And you touched on this earlier. You know, I think there's been some very exciting results from Noose from Prime Intellect in kind of pushing the boundaries of what's possible with, you know, low latency or even asynchronous training. And I am curious if you guys at Ionet have seen any demand on that front. I know.
00:25:24.005 - 00:25:31.001, Speaker B: You know, kind of primarily we're targeting the inference use case, but curious what this might mean for your business as well.
00:25:31.153 - 00:26:06.539, Speaker C: Well, it's just overall positive for the business. You've always been able to train on Ionet. Now you have to remember Ionet is a network and so a lot of what we do is fully decentralized, but we also have data centers on the network where someone can get 50 co located H1 hundreds connected with NVLink, so people can always use that. We've always seen a lot of demand for training. We actually are seeing more demand for decentralized training. It's definitely an emerging area both among our current customers and then new customers are coming in and asking about it. And the great thing is we foresaw this.
00:26:06.539 - 00:26:25.725, Speaker C: And so we've really worked hard on having some frameworks that our customers can use, such as Ray, such as Kubernetes, that make it really easy to train decentralized models. And so I think really any innovation in the space is net positive for Ionet, especially one that allows decentralized training, because that's right into our wheelhouse.
00:26:26.505 - 00:27:10.167, Speaker B: Yeah, I think there hopefully a lot of synergies up and coming over the next couple of years. And then in a similar vein, I think, you know, obviously the Strawberry model, the latest model from OpenAI, kind of shifts a little bit towards a greater amount of inference. Right. As you know, within the foundational model itself. And so I thought that was a very interesting data point that might point to kind of a slightly different trajectory for foundational models in terms of how they scale to the next leg of performance and capabilities. That I also thought might dovetail quite nicely with Ionet, which is, you know, obviously like more of a distributed inference network. But correct me if I'm wrong, you.
00:27:10.191 - 00:27:49.893, Speaker C: Know, I think Strawberry is great. I don't think it directly affects us. The thing about Ionet is we're pretty agnostic to the apps that our customers are building or the models they're using. I mean, what we're really focused on is just providing the highest quality compute at the lowest prices for whatever they want to build. That said, I think obviously once again Strawberry indirectly benefits this because it's a great model. I mean, I don't know if you've seen the sort of performance records of it, but it's blowing GPT3, GPT4 away. And so it only just grows AI, which brings more people onto the network.
00:27:49.893 - 00:27:53.125, Speaker C: We power that. So any new innovation is great for us.
00:27:53.705 - 00:28:20.781, Speaker B: But I also was curious if it was like a net benefit in terms of, I guess, how it was trained, that it might be more aligned with like, you know, at least potentially not as dependent on a massive super cluster if it's primarily greater inference time versus just like the massive scaling law, like a foundational model approach. But I don't know Just thinking out loud.
00:28:20.933 - 00:28:35.345, Speaker C: Yeah, no, I think, yeah, like I said, I think it may have some benefits to us there, but it's still there, it's still pulling in demand for compute. And so for us that's ultimately a benefit.
00:28:35.845 - 00:29:13.755, Speaker B: Got it, got it. Okay. And one of the fun conversations I had last week was with Mo from exo, who's doing some really interesting stuff, you know, pulling in like allowing individuals on. Yeah, yeah. At the edge. And it does seem like, you know, based on his view, you will continue to see a lot of hardware gains on device and you know, the cost of inference continues to go down and so obviously there's going to be room for both. But I am curious if you think, you know, enough use cases might actually get pushed to the edge over time that it could significantly into the inference market.
00:29:14.535 - 00:29:55.533, Speaker C: I think in general I'm pretty bullish on edge computing. I think it is very interesting, especially for any applications that require real time responses and, or require very low latency. That said, there's always going to be room for the cloud. When you, when you're talking about large scale inferencing or when you are talking about high intensity training, you're going to need cloud computer. And I don't have a firm opinion on what that blend will be, but I know that these two will continue to coexist. And quite frankly, that's a great thing for Ionet because if we need to edge devices could join the network. If we need to have enterprise cloud devices, they can join the network.
00:29:55.533 - 00:30:19.801, Speaker C: And so consumers can continue to get what they want. And actually I think that's one of the great things about being a decentralized network. Just imagine for a second that you're a centralized network. Think of the different bets that you have to make. You have to say, okay, well I think that most of the demand will be from cloud computing for training. So I need to buy a bunch of H100. If you're wrong, you can lose a lot of money.
00:30:19.801 - 00:30:58.165, Speaker C: The great thing about Ionet is that, and really decentralized networks as a whole is we can be infinitely flexible. If demand goes more to the edge, we can bring on devices that cater more to that. If it goes more to inferencing devices that cater to that goes to training devices that cater to that. And even if, you know, the H100 falls out of favor when the new B1 hundreds come on, we can bring more and more devices that come to that. And so because we don't really own any resupply, it allows us to be much more flexible. And I think that is one of the inherent benefits of decentralized networks that isn't talked about as much, but honestly could be the killer app that disrupts the market.
00:30:58.825 - 00:31:30.425, Speaker B: Yeah, that's really interesting. I hadn't thought about that extra layer of flexibility, so that makes a lot of sense to me. And then in terms of the Ionet roadmap, so obviously I think you have a lot of room to grow in just your core market. So maybe this is just a distraction. But outside of kind of your core market, are there other areas in terms of horizontal expansion or vertical expansion that you'd be excited to expand into over time or is that just not the focus right now?
00:31:31.565 - 00:31:59.471, Speaker C: Well, it's not the focus, but we were keeping an eye on them. The possibilities are limitless and I'll just throw a couple out. Number one, let's look at GPUs. There are so many use cases for GPUs. Obviously we're focusing on AI, but GPUs can be used for rendering, streaming, gaming. All three of those are going to be huge for the Metaverse. They can be used for zero knowledge.
00:31:59.471 - 00:32:27.229, Speaker C: They can be used for IoT. And we actually already have a lot of clients that are asking about these. And while we don't really spend a lot of time thinking about them, that is one possibility. Possibility number two is some kind of vertical expansion. If you think about what the AI ecosystem is, it's really a three legged stool. There's compute, there's models and there's data. Right now we have the world's largest decentralized AI compute network.
00:32:27.229 - 00:33:08.423, Speaker C: But we've actually been exploring creating a decentralized model marketplace. Think like a decentralized hugging face. Who knows, maybe in the future we create a decentralized data product or partner with someone like a grass. That way we could create this whole ecosystem where people can basically build dapps. On top of that, we've even tested that we have an app called BC8AI, which is a fully decentralized AI app. We could build that ecosystem and have the IO coin function at the center of that. And then who knows? A third potential option is we've got a ton of very high performing GPUs that could basically be used to bootstrap a validator network and become an L1.
00:33:08.423 - 00:33:29.831, Speaker C: I mean, there's just so many things we think about, but at the same time, you know, I'm Looking at this $2.5 trillion cloud computing market where we just have a much better solution and so we're growing so fast, it's really tough to focus on anything else but that other than that right now, yeah.
00:33:29.903 - 00:34:21.615, Speaker B: That makes a lot of sense. It seems like there's a lot of room to penetrate in the core market. But I also do feel like it is one of those things where one of the beauties of decentralized AI is kind of the. It's a beauty and a curse, like the fragmentation, because I would say the ability to coordinate compute is something that makes Web3 Infrastructure very unique. But I'd also say developers do love a bundled offering. And so the amount of fragmentation versus vertical integration as the market evolves, I think might shift towards more vertically integrated over time. And I think we've seen that in defi, just some of the larger apps like basically expand outwards or up the stack.
00:34:21.615 - 00:34:26.771, Speaker B: And I would expect it to end up probably being pretty similar in Web3 as well.
00:34:26.923 - 00:35:04.035, Speaker C: And that's the great thing, dominating, continuing to dominate our current industry, which is decentralized AI compute. It only gives us optionality, right? The larger we are, the more network effects we have, the easier it is to branch into a new product, bring more people on. So I think the path that we're going down now is already setting up the flywheel for expansion. And as we grow, we have more capital, we bring on more people, we can start to do a lot of these things. And so, yeah, I would love for Ionet to become a behemoth. I don't know exactly what that path will be, but I do know as we continue to succeed, it just opens up more and more doors for us.
00:35:04.855 - 00:35:17.115, Speaker B: Okay, well, in that case, what does success look like for Ionet? So, five, 10 years out, if you're looking back, what would you be really excited about as an outcome for what you've built?
00:35:18.335 - 00:35:59.821, Speaker C: I don't know, about five to 10 years. But the long term goal of Ionet right now is to replace AWS as the leading provider in the $2.5 trillion cloud market. I know that may seem audacious, but from my point of view, we are building a better, faster, cheaper product that can quite frankly disrupt the centralized providers. The one thing that we have to make sure we have to do, I don't think it's a question of if, but when. And I don't know if it's going to be us. I obviously want it to be us, but I think the project that wins this is going to win through execution.
00:35:59.821 - 00:36:35.339, Speaker C: And on that note, there are really a couple things that are at the top of my mind. The first is to continue to drive, to aggressively drive demand, basically by undercutting the competition. We offer the same level of performance for a lower cost and we just continue to steal share. Second is continuing to upgrade our product offerings so that our platform is equal or superior to some of the centralized providers. And I think that's going to be along two areas. Number one in performance, but also product offerings. Obviously some of the larger cloud providers have more bundled offerings and you know, we're looking at bringing things on like storage.
00:36:35.339 - 00:37:14.937, Speaker C: And so we will continue to grow in that area so that we can have economies of scope and better serve the customer. Third is bringing our community more into the fold. I think that community is an often underappreciated aspect of Web3 that provides an enormous competitive advantage. I mean, Ionet's community has, is spread over 25 different languages, spread all over the globe. People that are very deep into machine learning. And the more that we involve them and get their insights into our decision making, I think it gives us a better competitive advantage over some of the centralized proposals providers. And then the fourth thing is something I touched on earlier is just continuing to build a world class team.
00:37:14.937 - 00:37:46.715, Speaker C: We have a great team now, but we are actively recruiting from some of the larger Web two players. We want to take their best talent, maybe some of the ones that are sort of disenchanted, what they're doing now, get the playbook from those guys and bring them over. That's ultimately the long term vision. And I think for anyone interested, we have a state of the network address on our website which provides a very, very detailed overview of how far we've come over the last year, what we plan to do in the next year and then really what we plan to do in the long term.
00:37:48.095 - 00:38:01.115, Speaker B: Okay, and then final question from my end post, asi, what does Tory Green plan to do? Once you've been made completely redundant by, by some of your own creations, how are you going to spend your time?
00:38:01.585 - 00:38:23.525, Speaker C: Like I said, I can't see that happening for a long time. I think AI, and maybe this is a bit contrarian, I think AI is a fantastic tool, but the best quote I ever heard about AI is that AI is not going to take your job. Someone using AI will. So I think that I'm going to be at this for quite a long time.
00:38:24.185 - 00:38:34.555, Speaker B: Okay, all right, he's in it for the long haul. Here we go. All right, Tory, well, thank you very much for coming on Delphi's Crypto AI month. It's been a pleasure, really appreciate you coming on.
00:38:34.715 - 00:38:38.619, Speaker C: Thank you. Really, really enjoyed it. Hope to be back soon. Third time's a charm.
00:38:38.787 - 00:38:42.643, Speaker B: Yeah, we have to. Multiple repeats. All right. Cheers.
00:38:42.819 - 00:38:46.605, Speaker C: Cheers.
