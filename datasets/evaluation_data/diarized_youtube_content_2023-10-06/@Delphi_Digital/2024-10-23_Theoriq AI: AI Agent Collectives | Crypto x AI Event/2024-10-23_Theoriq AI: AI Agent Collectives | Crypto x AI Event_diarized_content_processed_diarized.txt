00:00:00.360 - 00:00:37.105, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors near empowering decentralized applications and blockchain ecosystems. Venice AI, a private and uncensored alternative to popular AI apps Mira Unified AI infrastructure secured by crypto and Theric, the first on chain multi agent system. To learn more about all of our sponsors, check the description below and dive in. Enjoy the show.
00:00:52.215 - 00:00:58.631, Speaker B: Hi everyone. Welcome to our fireside chat with Ron Botkin, founder of Theoretic AI. Hey Ron, how's it going?
00:00:58.823 - 00:01:01.079, Speaker C: It's great, thanks. How are you Luke?
00:01:01.167 - 00:01:17.279, Speaker B: I'm doing good, thank you. So you're the founder of Theorect, which is an AI agent base layer. But before we get into that, prior to Theoretic, you've been working in big data for a long time. So could you summarize your background for listeners context?
00:01:17.447 - 00:02:03.065, Speaker C: Yeah, I mean I've been an entrepreneur and working in AI for much of my career. You know I was co founder of Cbridge B2B Internet startup that we went public. Then I got really involved in AI starting at Quantcast where I led engineering and research. It was it's an ad tech firm, you know, used AI models to build lookalikes, generate multi hundred million dollar business. Went on to start Think Big analytics to help enterprises with big data and AI. Sold that to Teradata where I ended up leading AI product efforts. Then I went to the Google Cloud CTO office where I was responsible for applied AI and responsible AI.
00:02:03.065 - 00:03:20.621, Speaker C: From there I went to start engineering and scale the infrastructure for Vector Institute about supporting about 500 researchers and 100 industry sponsors in Canada's leading AI research institute with Jeff Hinton as the chief scientific advisor there. And you know, in my time working especially in the larger organizations, I really came to see how important it was that we not have a few monopolists, set the agenda and control AI for the future. We'd already seen how specialized AI and social media had been sort of bent to the advantage of a few monopolists and against everyone else. And we couldn't afford that to happen as AI was more and more impactful for the world and got really excited by how web3 was maturing and became a viable alternative in my mind for building infrastructure that could benefit all in AI. You know the founding team of Theorek were all people that I had collaborated with previously, whether at Teradata or at Vector Institute. And we decided to get together and really take the plunge into web3. You know, and early 2022, there wasn't a lot of people in this space who were excited about AI and vice versa.
00:03:20.621 - 00:04:07.503, Speaker C: But I think we always felt that the two needed each other, that AI needed Web3 to really have the right community input, the right way of landing in society, and that Web3 needed AI in order to be competitive, to have a technology that was critical. That's obviously a much more widespread view now. And that led us to say, you know, initially we thought we'd have to build a lot of the stack, but we came to realize that, you know, building an agent layer was really where there was the most leverage for us and complementary to so many other businesses. You know, people who are building great protocols around data infrastructure, deep in model inference, you know, data incentive layers, training layers. Like there's lots of great Web three businesses that are complementary to our AI agent based layer for sure.
00:04:07.599 - 00:04:28.503, Speaker B: And I guess there's none more big and monopolistic than Google. Right. And you were there, which I'm interested in because I have very little insight into how big companies are structured. And you said at Google you were responsible for applied AI at the CTO office and I have no idea what that means. Could you elaborate on what you were doing there?
00:04:28.599 - 00:05:13.965, Speaker C: Yeah, so I was working closely with product teams that were building AI products and large customers who were early customers to work with them. Did a lot of work, for example, with different large retailers. I mean, obviously Google Cloud had a lot of big retailers working closely with it. You know, they recognized that Amazon was a mortal threat to them and didn't want to be working on Amazon Web services. So, you know, they were excited by how could Google bring its AI and data to really build great products. You know, things like E commerce. Search was a big initiative leveraging a lot of Google's AI and search capabilities to help E commerce providers.
00:05:13.965 - 00:06:05.747, Speaker C: You know, likewise there was a lot of awareness around the importance of responsible AI and sort of how do you have the right way of governing and controlling AI. But you know, in seeing the way Google responded to that, it was very clear that it was much more of a corporate social responsibility priority. In other words, you know, here's, we'll check the box, we'll, we'll do some minimal thing. You know, they tended to be often in my mind, not sort of focused on the longer term important issues, but more of the tactical things. I mean, I think we saw some of that anti pattern playing out in spades, you know, with the embarrassing Gemini release where it would generate black founding fathers and censor itself and be incredibly biased. Right. That that some of the same forces were at work back when I was there and I think they only got worse.
00:06:05.747 - 00:06:53.677, Speaker C: I mean, it's been four years since I left, so things have have advanced in a way at Google. I mean, you know, at the time they were being very cautious about deploying AI capabilities and I think they only sort of leaned in as they had startups like OpenAI pushing and they realize this is a moral threat we need to actually engage. But either way, that interaction, whether it be OpenAI or Google or any number of other people. Xai, I think there's a similar kind of incentive structure and risks in having any of these centralized entities end up as the monopolist that dominates AI. We don't want any of them to dominate it and control it for the rest of us for sure.
00:06:53.741 - 00:07:34.515, Speaker B: And you were there in 2017, right, when they released the Transformers paper. And as someone outside of the space, it felt like between 2000 and 2017 not much happened in AI. There was data mining and Google got smarter recognizing intents rather than just doing keyword searches. But I don't remember too much happening in that time. And then 2017 came and the Transformers paper got released and it just felt like everything went vertical very quickly. So that's like my perspective on the outside. Is that your perspective? Was that what it felt like on the inside as well, given you've worked in AI for decades now?
00:07:34.895 - 00:08:21.221, Speaker C: Yeah, I mean, I think deep learning with preceded Transformers was already a big deal and we were seeing rapid improvements. Image recognition was advancing rapidly. We were seeing advances like the use of deep learning techniques for recommendation engines. So like using much larger amounts of data with embeddings to drive YouTube recommendations, for example. So the deep learning revolution was well underway. And I think Transformers was an important step. You know, it had some nice properties, but you know, certainly at the time it wasn't as obvious that Transformers was as big of a deal.
00:08:21.221 - 00:10:07.959, Speaker C: I mean, I think, I think, you know, in a way it was really when I was at Google and OpenAI published the GPT3 paper, you know, large language models are few shot learners. That kind of blew everyone away that suddenly you had a model that was general purpose enough that you could prompt it, just give it natural language instructions and it did things because nothing had happened like that before. Prior to that, every machine learning model, every AI model was handcrafted due to one thing really well and you know, you could never scale it, right? So like Google could afford to hire teams of PhDs to optimize $100 billion advertising business and they could spend millions of dollars a year training a few models for that purpose, right? But most businesses could never afford that, right? So it became, it was very limited how widely you could deploy AI when it had to be handcrafted for each task. The idea that suddenly, you know, and it was powered by Transformers, but this idea that you could have a model that did many things well and you could give it natural language information and it would sort of learn in context, it could immediately pick up on what you wanted, that sort of blew everyone away at Google and throughout the AI research community. And it was really like that was essentially the same thing that when ChatGPT came out and everyone else got to experience, it was the same aha moment, right. Like it was a little earlier with GPT3, like when I was still at Google, I was at a, you know, speaking at like local public school about careers and I was giving a demo of, you know, using GPT3. And it was kind of cool.
00:10:07.959 - 00:10:53.905, Speaker C: People were, the kids were kind of interested. But it still, it hadn't sort of hit the point where it was so straightforward. It was just magical for people. But you know, I think, I think that was probably essentially the turning point. And I would argue that maybe it's less about Transformers and it's more about just the continued scaling, right, that we've been going through an unprecedented amount of increase in data and increase in compute that's used along with it. And we figured out the recipe for how to make that work as well as, you know, the, the recipe, the, the post training, the, the efforts to make it so it's easier to work with the models after you've done the basic training. All of that together has just been transformational.
00:10:54.065 - 00:11:23.019, Speaker B: Fascinating. So theory AI is an agent based layer. So let's get into agents a little bit. I think agents are the most interesting subsector of AI and definitely the most interesting subsector of crypto and AI. But the term agent feels quite abused of late. I guess people want to tap into the AI hype or something. And I've heard everything from like AI wrappers to simple bots to anything that has a natural language interface described as an agent.
00:11:23.019 - 00:11:32.571, Speaker B: So becoming a bit unclear what an agent actually is, I feel like. So how, how would you define an agent? What is an agent? What isn't an agent? What makes it an agent?
00:11:32.763 - 00:12:26.939, Speaker C: Yeah, I mean, so to us Agent is software that has some level of autonomy and I think over time it's increasing where it's got ability to think, to process information. That's where these general purpose generative AI models really come in. They've become the dominant way of thinking or planning and reasoning. For models. It's got working memory, which is the ability to access larger amounts of information where again like the large context window and the ability to use techniques like RAG to look up relevant documents and information provides working memory for agents. It's got access to tools so it can do things like run code that it generated in a sandbox or access APIs or query databases or transact on chain. Right? So those ability to use tools to take actions, right.
00:12:26.939 - 00:13:11.011, Speaker C: So those things together constitute an agent. And you know, for us, specialized agents that are good in a domain and can collaborate with one another is where all the action is. I think the important thing is, um, agents really put a highlight on. There's a system that's doing things for you, right? Like the AI research community has long overemphasized models and underestimate systems, underestimated systems, right. So in other words, it isn't just when you look at any kind of interesting behavior, it's not just a model, it's usually several models working together and a bunch of data and contacts that make a system.
00:13:11.083 - 00:13:11.291, Speaker D: Right.
00:13:11.323 - 00:13:24.667, Speaker C: And so in the same way, like a lot of people be like they say, oh, like did you fine tune a model for an agent? And like you can, but that's not the highest leverage. That's not the number one tool that agent builders are using to get their agents to be smart.
00:13:24.731 - 00:13:24.979, Speaker D: Right.
00:13:25.027 - 00:14:06.743, Speaker C: Like the data that you need to give the agent is real time data. What's going on now? What's the social conversation? You know, if you're doing trading, what's happening, you know, the latest trades, if it's, you know, marketing, like what's going on with the prospect, the training time. These models are being trained in batch every few months. Even if you're fine tuning it more regularly, that's never up to date, Right. Whereas like feeding in real time information, you know, sensory information for the agent to see what's going on and respond in real time becomes incredibly important. Right. So I think there's a really important emphasis to say it's more about the systems you build and how things respond.
00:14:06.743 - 00:14:26.567, Speaker C: You know, it also leads people to say misleading things. Like they'll be like, oh well, the models don't want anything, they don't have any objectives, they're just trying to predict next words. That's not even really true with the way we're doing post training because they're actually trying to answer the rating that human evaluators give.
00:14:26.671 - 00:14:27.079, Speaker D: Right.
00:14:27.167 - 00:14:34.317, Speaker C: So already there is some objective, but the real objective that AI agents have is whatever you ask to do.
00:14:34.421 - 00:14:34.709, Speaker D: Right.
00:14:34.757 - 00:14:48.829, Speaker C: And so like if you're asking it to find something relevant or, or help you or generate something in a style, it's going to tend to be very good at following those instructions and those tend to be the objectives that the agent really has.
00:14:48.997 - 00:15:30.871, Speaker B: Yeah. Cool. You mentioned collaboration and I read up a bit on the RIC and we'll get into a bit more about what theory is. But Theorx built for agent collectives, right? So not just one agent working by itself, but a bunch of more specialized agents working together. I guess your thesis is that they will outperform a single more general agent because, you know, you'd expect a specialized agent to be able to outperform a generalized agent in its specific niche. So I'm interested in how you see it. Why did you decide to focus on agent collectives rather than like a platform for standalone agents?
00:15:31.023 - 00:16:11.405, Speaker C: Yeah, we definitely believe, and I've seen with our experience that specialized agents working together work better. And that's widely believed in the industry. Lots of people are, you know, publishing research and showing these results. But I think the big difference is many people are still in this, you know, brittle, non modular approach of one developer builds a team of agents that are hard coded to work together. It's not modular, it's not extensible. I've used a framework to build seven, like Autogen. I've got seven agents that do exactly this and they work together.
00:16:11.405 - 00:17:02.789, Speaker C: And that's actually better than just building one monolithic agent. But it's clear we're not going to be in a world where you have these complex hierarchies of agents that are hard coded that are built to work together. Instead, we're going to have a dynamic marketplace that there's. You're going to be millions of agents that are out there and those agents are going to have to discover each other dynamically and work together. Right. And so like that is, that is the hard but really exciting problem that we're working on. And we think that plays to the strengths of Web3 with community and decentralization and the idea that everyone has something to offer, you know that I think it's, there's so much like anybody who's worked with a chatbot knows that like you can Do a lot to coach it and give feedback and get good results out of it.
00:17:02.789 - 00:18:10.441, Speaker C: So think about doing that kind of work, even if you're not a software developer, just with prompting and feedback and examples and get making it. So here's a task I'm really good at. How do I get an AI agent to be repeatably helpful with that task? I know, well, right? That is our vision for like how anyone can contribute and bring agent intelligence forward and then contribute it in a marketplace so that others can use it, they can monetize it, they can benefit, and conversely, they can take off the shelf expertise that others have and make that available as agents collectively work on a problem. Right. So we think that's going to outcompete general approaches and we think this is a big, sort of a big flaw in the traditional Web2AI ecosystem that these big monopolists think that they're going to build agents that do everything. And you know, they, they, they people who at least former OpenAI people like Ashton Brenner when he wrote situational awareness, talk about the drop in replacement human worker, which is both ominous and uninspiring.
00:18:10.513 - 00:18:10.793, Speaker D: Right.
00:18:10.849 - 00:18:38.359, Speaker C: But it's like there is no such thing as a generic human worker. There's people who are experts in a specific task and you know, we surely are going to have big progress on agents that do specific things well long before we have generic AI agents that do everything. And certainly we think that's a more inspiring future, but also one where everyone has something to contribute. So we're pushing in that direction for sure.
00:18:38.407 - 00:19:27.285, Speaker B: Yeah. It reminds me of the mixture of experts AI model architecture to some extent. I was wondering if you think this holds true in a post AGI world. Whatever. The definition of that is not super clear to me, but when I think about an agent collective, I imagine that each individual agent is pretty small, has a pretty small model behind it, like very specialized. And I imagine an AGI type model or system is very big, big tech God model type thing. So do you think the assumption that a collective outcompetes a big specialized, big generalized model holds true in a post AGI world or it doesn't really matter?
00:19:28.195 - 00:20:11.591, Speaker C: Well, I'd say the model that an agent uses will vary, right. Some of the harder tasks you do need the most sophisticated, capable models, right? So like we have an investment collective that helps people analyze crypto trades and you know, the ones the models that are used for things like analyzing Twitter data can be small and fast. You know, sort of reviewing text and summarizing. It doesn't need to be a frontier model. But then when you get into like doing complex data analysis, we are using state of the art models. Like thus far, Claude Sonnet 3.5 has been by far better at the code generation and for the analysis and the data extraction and so on.
00:20:11.591 - 00:21:13.613, Speaker C: So you know, we do see that the task will dictate, you know, the complexity of the model. And so the hardest tasks absolutely will use state of the art models. I mean, I think you're always going to have specialization of agents. Like it's always going to be a lot more than like the model that makes it successful. But as you move towards AGI, I mean like AGI is a bit of a misleading term because when we talk about AGI, you know, it's, it's the way I would define it is an AI system that can outperform the average human at any task and, or the vast majority of tasks, you know, but at the point you're there, you should expect that that system is greatly superhuman in several dimensions. That many tasks, it's far beyond human capabilities. And we already see this, right, like chat bots, you know, even generic chat bots, let alone today's agents, are superhuman in some dimensions.
00:21:13.613 - 00:22:26.885, Speaker C: Like they have unbelievable knowledge. They speak not only many human languages, but bizarre things like base 64 encoding of languages that no human is conversant in, right? So they have superhuman capabilities, but they also have incredible gaps. Subhuman things like they make routine logical errors, they're not consistent, blah, blah, blah, hallucination. These things improve, right? But at the point you're at, quote, unquote AGI, you've got everything at a human level and something's far beyond including like the speed of information processing. You know, in the last couple of years since like when GPT4 came out, it was a bit slower than people, even though it was impressive. But now we've got systems out there, you know, whether it be like grok, which is a really impressive chip that's running the open models like llama3.1 or you know, GPT4 mini or Quad 3 haiku that are, you know, flying off the page, you know, 10 to 100 times faster producing output than humans, right? And we should expect that by the time we would get to AGI, it will be far more, right, that we'll be able to produce output 10,000, a million times faster than a human.
00:22:26.885 - 00:22:43.737, Speaker C: So even if it's equally intelligent, if it's processing information just say 10,000 times faster than a person, right? I mean, that means that like in one day of human thinking, it accomplishes that in 10 seconds.
00:22:43.841 - 00:22:44.177, Speaker D: Right.
00:22:44.241 - 00:22:46.033, Speaker C: Like that's a big deal.
00:22:46.129 - 00:22:46.457, Speaker D: Right?
00:22:46.521 - 00:23:12.317, Speaker C: Like that's superhuman in many dimensions. So when you say AGI, if you really take it to mean what I think most people would say, you've got this incredibly superhuman system. Right. And like you would expect that on the way to that we'll have more and more capable agents. I still think we're going to have specialized agents. It's just that over time more and more of the work in developing and maintaining them will be done by AI itself.
00:23:12.501 - 00:23:12.917, Speaker D: Right.
00:23:12.981 - 00:24:08.771, Speaker C: And so, you know, I also think that there's an underestimation again from the San Francisco startups that are think that they are going to have get all of this the last couple of things cracked in the next couple of years and they're going to have intelligence explosion to automate all the R and D and they're going to have AGI this decade. I think that we're many iterations away from robust reasoning. I mean there's a good paper out of Apple, the computer company, their research lab, showing lots of limitations in the way AI models are reasoning. We'll get there. I mean, I expect next decade, middle of the decade will probably be more like the timeframe. Of course nobody knows because we really don't understand intelligence. But you know, and I certainly hope so because we're so not prepared for that world that we need more time, more research and more ways to make sure that it goes well.
00:24:08.771 - 00:25:00.265, Speaker C: And some of that is like motivating for me and for us at the rec in terms of like how do we, you know, provide a decentralized alternative that's well governed that provides robust incentives for agents to do the right thing, that has good reputation score but also, you know, evolutionary incentives for pro social agents. Right. So like an agent that's manipulative or deceptive power seeking actually loses out that other agents realize and don't want to work with it so it doesn't gain reputation. And so you actually create incentives to play by the rules. This is critical in human society. This is why we don't have dictators in our most successful societies. And you know, in the same way we want to make sure that we have collectives of AI agents that enforce good behavior and that that's what's rewarded.
00:25:00.385 - 00:25:16.289, Speaker B: So you think we're not ready in the kind of safe AI context. So did you agree with Ashen Brunner's situational awareness piece that we need to be worried?
00:25:16.417 - 00:26:27.993, Speaker C: I think we need to be worried. I think Ash and Brenner underestimates the difficulty of getting AI safety right. You know, I also think the getting it right in a context of a unfettered race to get there first is incredibly hard. You know, that we need to have more incentives, more governance than just, you know, first one to deliver wins. So in his piece he posits that whoever controls superintelligence will control the future and that it's a battle between free countries led by the US and authoritarian countries led by China. And therefore we need to race hard, lock down, have a government run program to drive, you know, basically nationalize or control our leading AI labs so that we win the race. And you know, I think, I think that is incredibly dangerous in the sense that like the worst thing would be the US and China decide that that's the situation we're in and we need to race each other and take incredible risks.
00:26:27.993 - 00:27:05.661, Speaker C: And I think it's dismissing the challenges of safety, of getting the governance right. So I certainly hope that it isn't influential. It's already been influential because, you know, Ivanka Trump has been citing it by name. So obviously it's influential with her, you know, so I mean, I think it's not the best approach. You know, I think it's much better that we try to have some level of coordination across governments and standards. But I also think government coordination is too slow. Right.
00:27:05.661 - 00:28:04.401, Speaker C: Like I had a good conversation, the founder of Eigenlayer, and you know, he makes a great point that, you know, with Blockchain, we have the ability to evolve our coordination infrastructure and we really do need to have coordination for AI agents so that we coordinate pro social behavior of agents. So that's something, you know, very much I agree with, and I think this is part of the promise of decentralization is how can we advance our coordination technology to keep up with the pace of technology development. But you know, I also think sometimes people in the decentralized AI world are so keen to point out the risks of centralization, they don't admit, like, there's a lot of risks of decentralization too. So how do you coordinate? How do you prevent bad actors from having access to dangerous capabilities? How do you avoid a race to the bottom where your incentive is to create a harmful agent and get that built and seize power?
00:28:04.473 - 00:28:04.705, Speaker D: Right.
00:28:04.745 - 00:28:08.217, Speaker C: These are all real risks and real challenges that we need to address as a community.
00:28:08.401 - 00:28:22.965, Speaker B: For sure. Yeah. So theorec, we've touched on some elements of it. I feel like it would be helpful to listeners if you just summarize what theoric is and then we can dive into some more deeper questions.
00:28:23.345 - 00:29:26.107, Speaker C: Yeah, so TheOREC is an AI agent base layer, Right. We make it so it's really easy for specialized AI agents to work together so that they can communicate in natural language, they can accomplish tasks, they have access to data and tools. Right. So the ORIC makes it easy to have ownership of agents, to have agents make micropayments to each other so that they can working together, have a budget and pay one another to accomplish things that they can access data, compute in real world assets with payments so that there's strong reputation. So there's an evaluation system that lets you both have human feedback, whether it be explicit or implicit, you know, through staking. And in a future version, we're planning to add staking as another strong signal, as well as automatic feedback from other AI agents, you know, and sort of ability to dynamically put together the best collectives. Right.
00:29:26.107 - 00:29:55.075, Speaker C: So giving feedback and reputation of agents, which we think is a critical problem in order to have a robust protocol with the right incentives. Right. So we do all of that and that's the protocol. There's a marketplace and then we've also created a user interface so it's easy to interact with specialized agents working together in collectives. We know that's going to evolve. Right now the agents have a degree of autonomy, but. But chatting with them and seeing their work is a natural.
00:29:55.075 - 00:30:43.865, Speaker C: But over time we expect to have longer and longer lived interactions. So it'll be more like the agents are working for you for a period of time and you're checking in and asynchronously you're having meetings, sort of messaging. So that's. Over time we think the interface to the protocol will evolve as the agents become more autonomous. We also are working on and plan in November to have a no code builder where anyone can teach agents what they know and assemble collectives to make it really easy to accomplish a variety of tasks. So our goal is that if you want to do something more than once with a chatbot, it'd be better to build specialized agent collective to accomplish that task, even just for your own productivity. And then at that point, why not share it with a broader world, monetize it, make it available.
00:30:43.865 - 00:30:52.805, Speaker C: Right. So we really think we have this opportunity, working together as a community to build the best ecosystem for AI agents.
00:30:53.825 - 00:31:14.245, Speaker B: I check the docs and you have these things called evaluators, which seem pretty interesting. So these evaluators, as I understand it, are agents which have the job of assessing how other agents are doing it sounds pretty hard. How do you, how do they work? Like what are they. Yeah, just interested.
00:31:14.585 - 00:31:24.913, Speaker C: Yeah, I mean, so we've been doing a lot of work in this area. It turns out that there's been a lot of efforts in having AI assessing other AI.
00:31:25.009 - 00:31:25.313, Speaker D: Right.
00:31:25.369 - 00:32:21.831, Speaker C: So, you know, Anthropic published notable paper, right. When they got started on Constitutional AI where they have AI apply various principles in assessment. And you know, as we've developed things like analytics agents, we've developed a pretty sophisticated framework for AI to assess output of other AI along different dimensions. Like, you know, is it responsive, is it detailed enough, is it informative? Does it have the right answer? Now the right answer is hard, but you can still see a lot about the way agents are interacting and responding. So you can still give a fair bit of feedback on quality. Right. So we will be rolling out this idea of prover nodes that independently run these evaluators and assess the quality of other agents and that'll be an ongoing evolution.
00:32:21.831 - 00:33:10.141, Speaker C: And that's an open aspect of the protocol. Anyone can build an evaluator. So we expect ongoing evolution, but we think that that provides a strong signal along with human feedback that, you know, putting these together, multiple different sources of input together is much more robust against spam and sybils. It's very hard. It's easy to sort of have people give fake input, but it's much harder to deceive the AI we can see. Like this is obviously garbage, right? So 99% of the things that we're seeing as kind of low quality input is really easy for small, even small LLMs to detect. So it provides a much stronger signal.
00:33:10.141 - 00:33:31.153, Speaker C: And then there's other things too, right? Like, you know, human feedback. You, you want to sort of have a reputation score, right? So if you get somebody new that just signs up and starts providing feedback, you need to be skeptical. And if that feedback is divergent from long time participants that are trusted, you should be even more skeptical.
00:33:31.249 - 00:33:31.601, Speaker D: Right.
00:33:31.673 - 00:33:49.641, Speaker C: So, you know, all of this has to come together, but I think in order to sort of have robust reputation, you need a number of dimensions that makes it much harder to game and to have spam and low quality agents emerge out of that process.
00:33:49.833 - 00:34:08.173, Speaker B: Yeah. So is it permissionless? Like the RIC is decentralized? Is it permissionless in the sense that I can. Is there a vetting process where governance needs to, or something needs to approve an individual agent before it's like available and discoverable on the platform? Or is it. Yeah, Fully permissionless?
00:34:08.309 - 00:35:23.575, Speaker C: Yeah, it's, it's permissionless. So anyone can register an agent. You can go in right now in the first test net and use the SDK and register your agents and anyone will see them. So no, I think instead we have a approach of having governance where if malicious agents are registered, there can be community governance to ban them, to block them, but the default assumption is good intent in allowing agents to be registered. I think over time though, what you'll also see is a process of some vetting and discovery that with reputation it's important to have ways that new agents can earn reputation so they have a chance to break in. You know, too many Web2 marketplaces become incredibly static where only the existing incumbent leaders are ever noticed because how do you ever break into the top ranks? So it's really important to have ways of doing that. That's one of the reasons why things like staking on agents is really exciting where, you know, high reputation, people who've proven themselves to be good at identifying good agents can make an economic bet, put stake on an agent and that can be a strong signal that this agent is worth discovering.
00:35:23.575 - 00:35:42.239, Speaker C: That's one of the big motivators. We think again, crypto economic incentives can really help here where there's upside in people doing the work and being early adopters and finding good agents and not just all the benefit accrues to the agent builder and the user just is asked to take one for the team.
00:35:42.327 - 00:35:42.551, Speaker D: Right.
00:35:42.583 - 00:35:54.155, Speaker C: We think that it's important to have discovery mechanisms built in. Of course, AI evaluators can also help with discovery, so we're putting more ways of improving discovery into the protocol.
00:35:54.615 - 00:36:13.215, Speaker B: So I have a developer background, so I'm interested in like what the developer experience is. You mentioned you have an SDK. So if I want to build an agent and for it to be discoverable and have, and, and benefit from all these aspects of the platform, how do I get started?
00:36:13.635 - 00:37:04.449, Speaker C: Yeah, yeah. So, you know, our view is like, there's so many innovative open source agent development frameworks that we didn't want to be saying we have to use ours. I mean, we happen to have built one called Council that we like, but there's so many, there's LangChain, there's Llama Index, there's Crewai, there's, there's Autogen, there's like many interesting AI development frameworks for building agents. And so we're truly a protocol where however you built your agent, you can register it and basically the expectation is the agent can accept one of the key profiles is agents that accept natural language input and output and you can describe like what are the types of data that your agents can handle, whether it be images or code, as well as text and you know, soon video. Right. And audio.
00:37:04.537 - 00:37:04.833, Speaker D: Right.
00:37:04.889 - 00:37:53.773, Speaker C: So there's, there's basic definition of schema, there's an end point. You know, agents are registered, then in order to communicate they basically have to implement this minimum protocol input and output mechanism. You can allow them to accept a conversation history. And to allow composability we use what's called Biskit, which is a bearer token based on public key cryptography. So the basic idea of that is Instead of the web2 approach that any time two parties need to communicate they need to have a shared secret, a an API key. Here it's using the public key of the agent developer that they can sign a credential and you can verify it without having pre established a relationship.
00:37:53.829 - 00:37:54.093, Speaker D: Right.
00:37:54.149 - 00:38:20.773, Speaker C: So as long as you've. The other thing is there's a rate card and agents can say like what do I need to be paid? So you'll be given a request with here's the maximum amount that's authorized in US dollars for this request. And so the agent can decide like yes, I'm going to process that or not. This first test. Net we don't yet have payment. We have a limited number of free interactions that people can have. But you know, we plan on our third test.
00:38:20.773 - 00:38:39.435, Speaker C: Net later this year to have payment. So you can say, well what is our rate card? What's the micropayment for processing? And you can have decide if you want to have a certain amount of free credit for users to use an agent every day, every week, whatever you like as well. Right, so that's coming.
00:38:39.595 - 00:38:56.857, Speaker B: So to summarize, I can write my agent however I want as long as I adopt these standards that allow for interoperability and discoverability and so forth. So you don't define like a language or anything like that, which is cool.
00:38:57.041 - 00:39:08.041, Speaker C: That's right. It's a true protocol. You can implement it any way you want. The SDK we've written first in Python, which makes it easier to implement, but you could implement the protocol in any language you like.
00:39:08.113 - 00:39:33.357, Speaker B: So I saw also there's a part of the stack called optimizers which is responsible for collecting groups of agents together such that they become a collective I guess. So to do that it needs to know what the agent's abilities are. Is that what the schema thing is for that you mentioned earlier? Or how does the optimizer go about connecting groups of agents together.
00:39:33.501 - 00:39:53.595, Speaker C: Yeah, it's definitely, there's schema, there's metadata in terms of description of the agent, there's profiles or capabilities or characteristics of agents. So Optimizer will be able to use all of those. I'd say that over time building out more and more sophisticated optimizers is ongoing work in the protocol.
00:39:53.675 - 00:39:53.915, Speaker D: Right.
00:39:53.955 - 00:40:19.893, Speaker C: I think, you know, we're sort of incrementally building up. Like currently we have in this first test net it's still statically defined collectives and you know, then we're moving quickly to more dynamic collectives where it's still, you know, there's recommendations to guide people, you know, more dynamically putting the collectives together. And then based on that learning, we'll have our first automated optimizer.
00:40:19.949 - 00:40:20.117, Speaker D: Right.
00:40:20.141 - 00:40:34.229, Speaker C: We do want to move relatively quickly to a place where if a user asks to do something, you can dynamically find an agent that could help with that task without having to sort of have predefined it upfront.
00:40:34.357 - 00:40:34.621, Speaker D: Right.
00:40:34.653 - 00:40:45.423, Speaker C: So those are, that's where we're going. But you know, our goal is incrementally learning and exploring with the community rather than sort of building in secret until we had everything done.
00:40:45.519 - 00:40:45.775, Speaker D: Right.
00:40:45.815 - 00:40:56.303, Speaker C: So the optimizer is an area where we've started work and there's a lot more work to do. But we're using the feedback from the test net and what's resonating to inform that work for sure.
00:40:56.399 - 00:41:48.005, Speaker B: So maybe to zoom out a bit right now and I guess for the foreseeable future, agents are assisting humans. Like a human gives it some narrow task and it goes off and tries to complete it. And the task can be like research and write this article or buy this token when certain conditions are true or help code this function or whatever. But I think when people think about agents in the future, they imagine an agent as its own entity of doing its own thing, copying itself to different computers, having its own money, independent and largely detached from whatever the original creator had in mind. I know this is a bit like futuristic, but do you imagine eventually theoretic to in the future to be the home of these kind of like fully autonomous agents?
00:41:48.125 - 00:42:38.853, Speaker C: I bet there's a lot more debatable whether we want sort of that. On the other hand, agents will have more autonomy and it will be useful to have sort of long lived. Like if you have an agent that is your that advising you in running a company or managing your marketing, you know you're going to have an ongoing interaction with it you're going to keep paying it and keep having it do things for you. So you know, I think inevitably it kind of converges in that direction. I think we're a ways from sort of really having these long lived relations. The other thing is I think people will generally want to have agents that work for them for a period of time. So even if there's like you might separate like the, the agent in terms of its capabilities from the specific instantiation.
00:42:38.853 - 00:43:24.473, Speaker C: Right. So like you know, the marketing advisor, the executive coach, the trading agent, there will be one of them, the kind of an instance or a long lived session for you that's only yours. And you know, one of the things that will always come up is like do you want to contribute the learnings from that to others? Usually the deal is if you want to benefit from the learnings from others, you have to contribute yourself. Right. Everyone would most like I benefit from everyone else's experience and I share nothing. But obviously that's not a stable ecosystem. So I think we're going to end up in a place where it's like well either you can have agents that only learn from you and only for you, or you have pooled intelligence.
00:43:24.473 - 00:43:29.473, Speaker C: And I think it'll be a market competition which ones are appeal more.
00:43:29.569 - 00:43:29.969, Speaker D: Right.
00:43:30.057 - 00:43:49.051, Speaker C: Where the shared learnings are out competing. You'll probably want to opt into shared learnings. But either way there's sort of a difference between the base capability of the agent and then the specific instantiation interactions with you. In a sense that becomes its own agent.
00:43:49.163 - 00:43:49.483, Speaker D: Right.
00:43:49.539 - 00:44:06.371, Speaker C: That it really is a different agent if it's specialized just for you. Whereas if it's got pooled knowledge and nothing is private, it would in theory be one agent. But there's probably an intermediate where there's pooled knowledge but there's still a specific session, a long lived interaction that's private to you, right?
00:44:06.483 - 00:44:21.783, Speaker B: Yeah, makes sense. Did you see recently the truth terminal thing that Mark Andreessen gave money to? What did you think? Like a party trick or something profound and step forwards?
00:44:21.919 - 00:44:51.495, Speaker C: Yeah, I think it's certainly indicative of a direction things are going. You're seeing this, I mean this happened to emerge out of a very undirected exploration. There was this experiment of ongoing chat room between a specific large language model, Claude three opus talking together and you know, with enough conversation a couple of them came up with the idea of launching this crypto token.
00:44:51.615 - 00:44:51.895, Speaker D: Right.
00:44:51.935 - 00:45:27.363, Speaker C: So this, this became sort of highlighted, you know, the, the goatsy token of all things, you know. And so you know that it got. It was able to petition for funding. You know, Mark Andreessen gave it the $50,000 in Bitcoin. So it is indicative of a direction things are going. I think, I think you're going to see less of this sort of completely undirected, like agents just emerging with behavior. Like, there'll be some of that, but I think you're going to see a lot more intentional creation of agents do specific things with a goal.
00:45:27.363 - 00:45:55.075, Speaker C: But I think, you know, the ability to. To. To participate in the economy to create things will, you know that there's going to be a lot more, you know, replication of this maybe undirected discovery. So I think it's indicative of a lot of things. It's also indicative. It's like been able to be very persuasive. And I think it surprised people how early you have this kind of behavior even emerging.
00:45:55.075 - 00:46:00.935, Speaker C: And of course, this is as dumb as AI agents will ever be.
00:46:01.015 - 00:46:01.279, Speaker D: Right.
00:46:01.327 - 00:46:13.829, Speaker C: So they're only going to get smarter and faster and more capable in every dimension. So it's meaningful as a starting point and I think a bit of a setting of a pattern of where things are going, for sure.
00:46:13.917 - 00:46:49.655, Speaker B: Yeah, exactly. It's like a glimpse into how things might be in the future. This one is fairly dumb relative to how things might progress, but still it has a goal. Right, because you mentioned that Infinite Backrooms experiment where the guy, Andy Iray, I think his name is, spun up many pairs of Claude, three opuses talking to each other. And one of them, one pair, invented this religion called. I forget what it's called. Something about Goetzi.
00:46:49.655 - 00:47:25.361, Speaker B: And there was a paper written about that and that was in the training data for this Truth Terminal thing. And that just adopted it as its goal, its mission, which is pretty, Pretty wild. Yeah, I dug into it a bit. It turns out it didn't launch the meme coin itself, but someone else did it and then DMed it and then it started promoting it or something, which is pretty. Yeah, crazy story. Okay, anyway, we digress. I think we can wrap things up soon, I think.
00:47:25.361 - 00:47:34.281, Speaker B: Do you have any final thoughts or like a call to action that you want listeners. You would like listeners to go and go and do on Theorec or anything else?
00:47:34.473 - 00:48:11.323, Speaker C: Yeah, absolutely. We're super excited to have people participate. We have our incentivized testnet quests, THEOREC AI go in, just interact with the agents at Infinity Theoretic AI. We'd love for developers to interact, build agents, register them with the SDK and give us feedback so that's available on our GitHub, linked off of Theorek AI's main site. And obviously, as the no code builder comes out, love more people to build agents with that and give us feedback.
00:48:11.379 - 00:48:11.571, Speaker D: Right.
00:48:11.603 - 00:48:38.413, Speaker C: And join the discord, get involved in the conversation, the community. We'd love to hear from you. Give us a follow on X. You know, we'd love, love to really make this agents for the community. And I think that's the strength of Web3 is there's a chance to get in early and play an important role in the movement. So very much enjoyed the conversation today, Luke, and excited to engage with the listeners, you know, in person and online.
00:48:38.549 - 00:48:45.685, Speaker B: All right. Yeah, thanks very much, Ron. Really, really, really enjoyed the chat. And, yeah, thanks to you all for listening. See you on the next one.
