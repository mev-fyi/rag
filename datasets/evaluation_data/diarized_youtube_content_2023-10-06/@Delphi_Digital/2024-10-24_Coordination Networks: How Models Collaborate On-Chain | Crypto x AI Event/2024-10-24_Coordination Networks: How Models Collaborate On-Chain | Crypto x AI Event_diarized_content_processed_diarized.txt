00:00:00.400 - 00:00:31.613, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors. NIA Empowering Decentralized Applications and Blockchain Ecosystems. Venice AI, a private and uncensored alternative to popular AI apps. Mira Unified AI infrastructure secured by crypto and the first on chain multi agent system.
00:00:31.613 - 00:00:37.145, Speaker A: To learn more about all of our sponsors, check the description below and dive in. Enjoy the show.
00:00:48.405 - 00:01:08.499, Speaker B: All right, welcome everybody to Delphi's Decentralized AI Conference. This is the coordination panel conversation. I'm joined by Nick Evans, who's the co founder of alora, and Himanshu, who is the co founder of Sentient. Welcome to you both.
00:01:08.627 - 00:01:09.935, Speaker C: Yeah, thanks for having us.
00:01:10.275 - 00:02:04.947, Speaker B: I wanted to start with a bit of a fun question. We were just talking offline about this, but one of the hottest topics in crypto AI land right now is kind of the rise of these Twitter bots, essentially that are controlled by an offline LLM and some are now trading on pump fun or interacting with people on Twitter. I'm curious. You two come from two of the more prominent crypto AI projects in the space. What do you kind of make of this? Is this like a. Is this just like a passing fad? Is it like a meme? Or is it a sign that we've kind of crossed the chasm where crypto AI is taken more seriously, there's more interest, there's really people kind of paying attention. What do you make of all of this kind of hoopla, Nick? Maybe we could start with you.
00:02:05.131 - 00:02:58.195, Speaker C: Sure, yeah. I don't know if it's a signal that AI people are now paying attention or that they are at mass and it's a sticky thing. I do think it's. It is the kind of like clear direction crypto and AI is going to move in. I think crypto enables such a kind of wider breadth of different financial interactions to take place in a more kind of autonomous or codified environment. And I think on the back of that, agents are this like very useful actor to kind of take the reins from the human actors that have been kind of the de facto users of crypto today. And so I think this, this trend of AI agents being the kind of de facto user of crypto and the kind of primary participant across different financial primitives that emerge within crypto ecosystems is one that is going to pick up steam.
00:02:58.195 - 00:03:28.755, Speaker C: And what I think will be a quite Rapid pace. Agents can just operate significantly more efficiently across a much more diverse and potentially long tail set of domains that make them a really ideal kind of participant in these systems. And I think crypto is ultimately this kind of shared economic substrate by which the kind of coming wave of AI agents will coordinate. And I think we're seeing the kind of early innings of this now, which is pretty exciting.
00:03:29.255 - 00:03:49.479, Speaker B: Himanshu, what about you? Have you, have you been following this? Maybe a broader question here. If this isn't like the tipping point for what excites the public, maybe more traditional AI people in crypto, is there something else that you're kind of waiting for, for that watershed moment, or do you think this might be it?
00:03:49.527 - 00:04:14.885, Speaker A: I don't think this is the watershed moment. For sure. It's exciting. I would say it's exciting. I don't think it's not exciting. Anything like this is exciting in crypto. But personally, I think that if you have just ML algorithms running meme coins, it's not AI, it's the ML part of it, then the whole fun of meme coins will go away, by the way, nothing will be left there.
00:04:14.885 - 00:05:01.221, Speaker A: So the whole point is that the human element of what you want to interact with, if you're just interacting with bots, this is just a bot, then it goes away and it becomes very hard to compete and very hard to have anything going for you. That's one part of it. So I don't think this dominates the meme coin circle, but I completely agree with Nick's point that it illustrates how AI, general, AI agents, the LLM agent, the truth terminal, can spin off these kind of things, can do these actions and interact with other AI agents. It demonstrates that. So it's a cool demonstration of that. I think it brings, it captures imagination. I would like to draw a parallel with how Andrew with his teleport is doing something fun with te.
00:05:01.221 - 00:05:24.373, Speaker A: But of course, TE are not just that. And I don't think there's any watershed movement with what he's doing. He's just trying to make a point so that people can take attention on something like te, which is boring and flat. AI, on the other hand, is not boring and flat. AI, on the other hand. AI agents have a role in every piece of crypto, including, including meme coin, and which has picked up. I'm looking forward to it.
00:05:24.373 - 00:05:38.387, Speaker A: I'm looking forward to how it grows, I'm looking forward to how exciting it becomes. I would imagine that there'll be a world where coins will have a personality and people will associate with that personality. That's a fun direction it can go in.
00:05:38.451 - 00:06:13.831, Speaker B: That's an interesting thought. Coins with personality. I hadn't considered that before. You guys both bring up a good point. The fact that crypto enables these agents to transact permissionlessly, accumulate resources, access to the global financial system. I think one thing that this whole GOAT saga has, kind of one lesson that I've drawn from it is that the kind of coming age of agents will be far more. There will be like a lot of gray area.
00:06:13.831 - 00:06:54.803, Speaker B: Like, you know, I used to think that it'd be like a light switch and all of a sudden there'd just be an agent that could do anything a human could do and it would totally replace humans on chain. And now as I've kind of thought about Goat, it seems like there will be this like slow merger where over time agents will gradually kind of replace the things that humans do and there will be some sort of like centaur type relationship where it's a human and an AI working together to accomplish a goal on chain. How do you guys kind of think about the slow transition to agents? You know, are there certain areas in crypto that seem like low hanging fruit? Where do you kind of see this evolving towards?
00:06:54.899 - 00:08:21.003, Speaker C: Yeah, I guess I can kick that off. I think what crypto sort of moves us towards is this hyper financialized future where kind of by virtue of being able to turn various kind of financial primitives into autonomous pieces of code, we reduce the overhead that it exists inherently in various pieces of financial coordination so much that we can start to expand the universe of things that can kind of exist as financial markets. I think Uniswap is a useful microcosm of this and AMMs more broadly, I guess, in that AMMs aren't more capital efficient than order book based kind of settlement systems. What they enable is a kind of autonomous tool for liquidity provisioning, which allows for a much longer tail set of assets to have supported spot venues and to have supported kind of trading interactions. And I think what agents will be useful for in the immediate term is not only leveraging these kind of autonomous pieces of financial infrastructure, but now in making the actors interacting with those finance that financial infrastructure much more efficient as well. AI is the most deflationary technology to ever exist. It can process massive amounts of information far more intelligently or inefficiently than kind of analog means of compute or kind of previous means of compute, human cognition included.
00:08:21.003 - 00:09:40.091, Speaker C: And so I think it's going to be most immediately applicable in the kind of domain of these long tail hyper specific markets. I think age like one really interesting vertical for agents in the immediate term is I think turning prediction markets into like a much more ubiquitous tool across a wider domain of things. I think prediction markets to now have followed this kind of four year cycle. We wait for this, this kind of singular event or family of events that attracts a kind of enough mind share and enough interest and also exists in a setting where there aren't sort of viable alternative trading venues. And prediction markets are this really powerful tool of creating more specific market settings for various events or topics. The problem is because of them being so specific or long tail kind of, I guess classical actors in those systems often can't make profit from the limited size of those things or it's not viable for them to interact with those types of markets. So I think agents being a more active participant across prediction markets expands the scope significantly in really interesting ways around where prediction markets can be applied and how viable and useful their outputs ultimately are.
00:09:40.091 - 00:09:43.147, Speaker C: So I think prediction markets and agents are kind of this perfect marriage.
00:09:43.211 - 00:10:39.681, Speaker A: So in terms of what AI is doing for crypto, right, There's a whole whole another part of what crypto will do for AI which excites me quite a bit. But in terms of what AI is doing for crypto and what are the exciting areas there, I agree with Nick completely and I go with a slightly different classification of the same thing. So one is about this. All this crypto has two parts, the financial part and the alignment part, the coordination part, which happens on Twitter and which happens on Discord and which happens on all the other places where we assemble, which happen at conferences. So one is this combination of this information which can now happen and which can be coordinated through agents. Just like giving something like Truth terminal, following some feeds and giving it a personality. Similarly, as Nick is saying now you can take some facts, some observations, some news items as they're happening and make it into maybe more creative use cases around prediction market.
00:10:39.681 - 00:11:06.667, Speaker A: So that's one example of the same kind of thing that how do you now assimilate information across these different platforms of crypto into one place? And that's one exciting part. One segment of exciting part. The other part of it is more around how transactions are done. And at least the projects that I see, there are a lot of projects who are trying to change how wallet looks like that. That's a no brainer. People will try it. It's a difficult problem, but people will try it.
00:11:06.667 - 00:11:45.553, Speaker A: Maybe they will try by nailing down intent based Natural language interface, no brainer for scheduling your transactions right for different kind of transactions. That's going to happen for sure. And just like all kind of coding is changing to natural language, smart contract development will change to natural language. That coding copilot is another very natural use case that will come to crypto as it's coming to other areas. So more and more readability, more and more talking in natural language, more and more talking in intent, and many more vulnerabilities. That's what we are headed for.
00:11:45.729 - 00:12:22.283, Speaker B: That's interesting. So it seems like both of you guys believe in a future of many models. Whether every coin has its own model or personality, or there's a lot of models all interacting on chain, there's going to be a lot of these things running wild. So I guess that brings us to the, the name of this panel coordination. So this is the coordination network panel. What is a coordination network and why did you guys choose to build that? Like what problem are you trying to solve? I guess just to start at the highest level.
00:12:22.419 - 00:14:15.285, Speaker C: From my perspective, I think coordination network is in a lot of ways this like umbrella term for ultimately what crypto networks are. I think crypto networks are fundamentally means of turning like different nebulous resources into more kind of tangible, digital, digital commodities. I think it's a, it's a means of creating more specific or tightly scoped market environments to more efficiently provision different resources. And I think within the context of model coordination, that means creating hyper kind of specific market environments by which different instances of intelligence can be coordinated towards the sort of objective of creating a better source of collective intelligence. I don't think the future is going to be one where there is a single model that's dominant across all domains or contexts. I think a lot of the composition and the paradigm of the AI space today has come to be as a function of models existing in these silos and a limited number of large entities accumulating massive amounts of these raw resources that build, that go into building useful AI and other models, not being able to kind of learn off of those, or be merged together with those, or be merged together with their kind of comparably sized counterparts in smaller settings. And so I think from my perspective, it's about creating an environment where models can collaboratively optimize different shared ML objective functions and learn off of one another in the process to create this more performant source of collective intelligence that's sort of borrowing the best pieces of many models across different sort of problem spaces or contexts to produce, I guess, a more Decentralized and performant source of intelligence.
00:14:15.285 - 00:14:22.105, Speaker C: That's how I would kind of summarize it within the context of model coordination or kind of intelligence coordination complementary to this.
00:14:22.145 - 00:15:25.503, Speaker A: Right. So Nick is talking about how when you have different intelligence, how do you coordinate them to become a larger intelligent piece? And actually this is a very exciting part. This is what one of the more exciting part, specifically if it happens with smaller models, that sort of opens up a whole together new different paradigm. And now there's a lot of thought on that small models coordinated well can compete with large models for different type of use cases, agentic use cases. So that's one kind of coordination where you coordinate this many, many different type of intelligence into a more superior intelligence. We are working on somewhat complementary part to it, which is about how do you coordinate the economic incentives between the model builders and these application builders, people who are building this different kind of AI intelligence. So we want to bring in those different models which these AI builders need, because, for example, the model which will be running within the terminal and we want to make sure that the community which helps you build those models is incentivized to do that, is rewarded when those models are used.
00:15:25.503 - 00:15:36.395, Speaker A: That's the coordination we are after, the coordination between the contributors of open source AI and the users of open source AI. That's the complementary part of coordination that sentient is after.
00:15:37.335 - 00:16:18.015, Speaker B: Interesting. One very perhaps dumb question I have is you guys are both going after what seems like an incredibly ambitious use case. It's like very broad there in theory, could be like millions of different models, a ton of different use cases. You could kind of point this sort of solution at anything. So when you're building something this kind of inherently like nebulous, where do you start? What use case do you build first? What customer do you go after? What does that look like from both your ends? Nick, maybe we start with you.
00:16:19.325 - 00:17:15.885, Speaker C: Yeah, I think for us we have a pretty specific focus on the kind of financial applications of what we're doing. I think when you're building these networks, there's obviously multiple sides to these markets. But I think just focusing on the demand side of this network, because I think that's more of what your question pertains to. I think there is a really sort of tangible opportunity in bringing AI closer to DeFi and the sort of DEFI adjacent use cases within crypto. I think speaking of crypto specifically, I see crypto as primarily a financial innovation. It's a way to kind of create different sort of tightly scoped market environments for the more efficient provisioning of different resources, if that's arguable, if that's. If one can argue that crypto is kind of more than just a financial innovation, I think it's more difficult to dispute that most, if not all of crypto's market fit has been born in these financial domains.
00:17:15.885 - 00:18:31.739, Speaker C: DEFI is still the sort of de facto category. These base layer networks are financial kind of domains themselves. And then in addition to those points, I think AI in a sort of coordination context is sort of most viable or production ready within these kind of financial domains. It's much easier to tap into sort of like classical AI and I guess like more mature categories of AI to support use cases in these domains. And so that's a lot of where our focus is in terms of kind of use cases and products integrating or building around the network is today is creating more intelligent kind of lending or debt based systems in crypto, expanding the universe of financial primitives and asset classes into like much further into the long tail. Like we talked about leveraging kind of coordinated models in various financial contexts, improving how liquidity is provisioned across various domains, both in kind of spot settings as well as kind of derivative perpetual, et cetera settings. And so focusing on the financial applications of AI within crypto is where the vast majority of our focus is on kind of the demand slash, use case.
00:18:31.787 - 00:18:56.373, Speaker A: Side right now from our side, Michael, obviously nebulous stuff is not actionable. So what we do every day is we are not coordinating between the builders and users every day. So there are actually two parts to this answer. There are two pieces of technology that we are after. First is that sentient from day one is set up as a model company. We want to build models which are open and we compete with the closed model companies. From day one that has been the pitch.
00:18:56.373 - 00:19:44.645, Speaker A: And so the question becomes what models will we start building first, right? So where is this gap in the model that we are after? And so today the gap for us is very clear. The gap is that you build an AI agent just like what let's say Sonnet released today, an AI agent which can control your computer, right? That's an agent which they build. You build an AI agent and you try to build it on these closed source models, okay? Some of them work very well. In case of Sonnet, I don't think it will work so well on the Sonet, but mostly it works very well. Now we build the same agent on Llama, which is like one of the leading open source models, right? You can try a few More actually it is the leading, but you can try a few more. Quen is pretty good and you can try a few more and you would see that it doesn't perform that well at all. You would see that building even a simple performant application on open model is very hard.
00:19:44.645 - 00:20:28.539, Speaker A: And forget about perplexity like experience. That's why you don't see any application which says we are today on completely open models, a good application, your favorite application you can try. So this gap between these closed models and open models in terms of supporting more performant AI applications is what we are filling first. So we are collecting data with community and we are improving models reasoning capability to support these kind of things. And the first campaign for us is already live. It's we went with obviously the most interesting one in the beginning and which is Werewolf, which is the deductive game. And one may wonder why AI agents playing deductive game helps those AI agents.
00:20:28.539 - 00:21:04.091, Speaker A: It helps because you start learning to think strategically, you start learning to take the right action at the right time. This tool calling is just about taking the right action at the right time based on the input. So those kind of training is what we are doing now. Models which will allow you to open models which will allow you to build performant AI applications just like what you can build on closed models. All the use cases that we discuss are actually performant AI applications. And when I say AI, I mean let's say right now we are focusing on mostly natural language based AI LLM based things. For us, that's the concrete part.
00:21:04.091 - 00:22:12.001, Speaker A: That's where we see a lot of excitement, a lot of demand. If this chatbot that everyone is putting on their thing which doesn't work so well if it's an open model, it's just what we are fixing. The second piece of technology that's the most important focus for us is now when you are building these models and people are contributing to it, how do you get people to actually own it? How can you have models which are open yet monetizable and yet remain loyal to the community which builds them? This is the more sort of rocket science that we are building, which is what we call OML open monetizable loyal models. And we are building a basic set of primitives to enable this OML models. Because once you have these models which are being used by these AI applications and you want to reward the community you build it, you also want to make sure that somehow if someone just takes this model and copies it and deploys it someone else and gains some level of prominence, then you should be able to make sure that the value still goes back to the community who built it. Otherwise the value is just stolen in open. So these are the two parts which models we are building.
00:22:12.001 - 00:22:27.641, Speaker A: We are building models which have reasoning capabilities to support performant applications. And on the blockchain side we are building this technology called OML Open monetizable loyal models to make sure that these models can be still monetized while being inoper.
00:22:27.833 - 00:22:58.575, Speaker B: So Himanshi, just as a follow up on that, I thought I heard you say that you want to focus on the model building side and you're placing a few bets. How does that compare and contrast to what Bittensor is trying to do? Because my understanding is that they're also going after a similar vertical. How do you kind of understand what they're doing? And then how does what you're doing differ from that?
00:23:00.035 - 00:23:27.807, Speaker A: Okay, Bittensor is not focusing on model alone. Bittensor is a very different project. Bittensor is an incentive layer. That's. It's a pure incentive layer. And actually I respect how they reach that conclusion because what Bittensor has enabled is that you want to do any collaborative activity around data or model or any of those things. You can come there, can set up a subnet, define your incentives and it will get done.
00:23:27.807 - 00:24:03.245, Speaker A: That's what Bittensor protocol determines. There's nothing about model or what happens to this model, who owns those models, who uses those models for data sets in bit tensorflow. What Bittensor as a protocol provides you is a way to incentivize people to build AI. In fact, you could have used it for anything else. You could have used it for incentivizing for something else. Any kind of coordinated activity where you can have this builders and checkers, like validators and miners, you could have used Bittensor for that. And the other side of it is that of course we can set up a part of a project of sentient on Bittensor as a subnet.
00:24:03.245 - 00:24:32.285, Speaker A: So what I don't like about Bittensor is the subnet structure. How the whole economy of an asset which is as big as a model. You know, models are several billion dollar assets and that's the aspiration with which one builds model will be subservient to this bigger network and the subnet structure. It sort of, I feel it's. It's a great incentive ecosystem and I like that. But I don't think it has anything going on the AI side that way. It is just that right now AI people are interested in it.
00:24:32.285 - 00:24:52.881, Speaker A: The problems we are Talking about is, suppose you went to Bittensor Subnet and you build this great model and everyone agrees that this is a great model and you have rewarded. Now what do you do with this model? Can you put it in open and allow others to build on it? How do you monetize that? What happens with that model? How do you get it used? Even if you build it on Bittensor Subnet? That's the problem we're trying to solve.
00:24:52.953 - 00:25:05.429, Speaker C: Sorry to jump in. I would love to. I don't know if you're open to it, but just digging into a little bit of the OML stuff because I think it's really interesting. I think that's like the, the really cool stuff you guys are doing. Love to like hear you talk, chat about.
00:25:05.517 - 00:25:10.077, Speaker A: Absolutely, I would love to. I mean, yeah, that's the stuff I would love to talk about.
00:25:10.141 - 00:25:11.101, Speaker B: Let's do it.
00:25:11.293 - 00:25:41.773, Speaker A: Right, right, right. So, so, okay, so the. So what's the problem here? Right? You have a model whose weights are given openly, published in public. Anyone can download and run it and yet you want to make sure that when it's run then people who contributed to the model, let's call them the owners of the model, get rewarded when it gets used. So how can one make that happen? So there are few solution choices. I'll first mention the very hard ones because they will take five years to come up. So first one is you do fhe.
00:25:41.773 - 00:26:25.335, Speaker A: This is actually you can have an FHE based solution where before every query you send a query request to a network who approves that query and only when that signed request comes, you can run this model and get the right output. This is like you can compute this within some fully homomorphic encryption applied to a compute as crazy as AI models. I think let's wait for Xama to solve it. That's the first solution. I mean, actually Xama is one of the collaborators and I hope they solve some big version of this. But you can solve this version just as an example for simpler models, not language models, but for simpler models like regression, because they're already doing something there. It's slow, it takes time, but you can do it.
00:26:25.335 - 00:27:13.199, Speaker A: Now the next version of this solution and you can have an MPC based solution which can be very feasible. But let's ignore that for now. The next version of that solution can be within tee and now this loses the openness property in some sense that the model is never open. The model exists, but it can be monetized and it can be loyal in the sense that when the model gets used, you can track its usage and reward the contributors. And it's loyal in the sense that you can put some rules about how the model can be used, what kind of queries it can answer, and all that can be packaged with the model as front end filters deployed in let's say AWS Nitro. Let's say you trust in aws, it's fine. I'm saying AWS nitro because our internal team is doing prototype on AWS Nitro and there are others who are doing prototypes on.
00:27:13.199 - 00:27:44.269, Speaker A: We are collaborating with others who do prototypes on other form of text. You can build this whole solution today. There is no great end to end solution with GPUs integrated with TE. Everything is work in progress. The difficulty in this is more about what does it mean to be secure within a te, Nothing more than that. So it's a lot of account keeping about that. Are you really sure about what software is running in there? Can you have a reproducible build of that software for me to be able to verify the hash? That's the hard part in teaching.
00:27:44.269 - 00:28:14.435, Speaker A: So when someone comes and tell you we have a TE solution, actually there's something which you cannot check in a call. You have to really spend a lot of time and figure out that are they really clarifying what's exactly running there? Let me try to rebuild this thing and let me see if I get the same hash which they published somewhere. So that's the hard part in te. But it's a feasible solution. You build a network of TEs, the model and the filters always remain within the TE. You get monetization, you get loyalty, but you don't get openness because the page were never open in public. But there's a valid solution.
00:28:14.435 - 00:28:42.725, Speaker A: What we are going for is this third category of solution which we like to call AI native cryptography. And this is what we are building with all of us are building. Pramod, who is leading research, Siwong, who is our AI research lead AI. And the entire team is building this one is AI native cryptography. What we are trying to do is our thought is that it's not like we don't want the model to work at all when it's not supposed to work. We just want to make sure that the performance drops significantly. Some.
00:28:42.725 - 00:29:07.865, Speaker A: Some output is not as good as the original one. So can we not use basic cryptographic primitives embedded in AI for that? So I'll give you an example. First solution. This is the gold standard solution. And then I'll give you the Silver standard solution, which we are taking out now, which we call OML1. The gold standard solution today we can do for smaller models like resnet. Not still for lnm, but the idea is cool.
00:29:07.865 - 00:29:50.051, Speaker A: The idea is that what I want to do is that for every query a hash of queries sent to a protocol, let's now assume it's a centralized entity. But decentralizing this is not difficult. This query is sent there, it signs it and says, okay, I give you permission to take response to this whole query. The sign thing comes out given to the network that the model, the model only responds with the right response if the sign matches, otherwise it doesn't. How can we build such a model? So ideally what I'm saying is there is a cryptographic part here which verifies signature and says 0 or 1. And that 01 bit is what somehow you use it to gatekeep the model. If it's zero, then the model gives junk.
00:29:50.051 - 00:30:08.517, Speaker A: If it's one, the model gives right output. Now these are two different circuits. This model circuit and this circuit. They look very different. And if I give you just like that, you will basically look at the assembly code or whatever, you will look at the assembly code which executes when you're running it and you will separate this from this. Easy to separate. So now we have to mix them somehow.
00:30:08.517 - 00:30:30.545, Speaker A: That's what we do. So we have a technique of mixing them. We approximate this signature scheme with another neural network and then we mix these two neural networks. That's this class of scheme that we are developing. This is the gold standard solution. Today it can be done only for smaller models. So that's why we can't deploy, let's say, a llama with this kind of solution.
00:30:30.545 - 00:30:59.359, Speaker A: Smaller model. What I mean by smaller model is something which was considered reasonable just seven, eight years back. But now people are like this is baby stuff like Resnet and these kind of smaller network, 10 million parameter, 50 million parameter. These kind of parameter sizes, no one likes them anymore. I hope they'll come back. Okay, this is what we are scaling now. The other solution is something very cool which is like the optimistic version of this solution.
00:30:59.359 - 00:31:41.971, Speaker A: If you think of this like zk, this is the optimistic version of the same solution Here what we do is we embed some very specific fingerprints in the model. So there was this lot of research which was done as attck, where you could put some backdoors in the model. And these are like the special queries, like Manchurian Candidate kind of query that when you put that Query, you get a very different response. You get the response that you want. And these backdoor attacks, the special thing about them was that when you fine tune the model or put a prompt wrapper around it, those cannot be removed easily. There was specific way of training the model for these responses, that you can't remove them easily. So they're robust to standard operations.
00:31:41.971 - 00:32:05.051, Speaker A: We use those techniques to put some very simple fingerprints in the model. These fingerprints are the ones which anyone who knows the question can make that question and get the very specific response. No one knows this response. Only the protocol knows this response. So if you can show me that response, then you can prove that you have actually accessed this model and gotten the response. We play around with this. We play around with how these fingerprint queries are designed.
00:32:05.051 - 00:32:35.791, Speaker A: How are they tied to the identity of the model builders? How are they tied to the identity of some other protocol part? The seed of the randomness user can be derived from that and how that response can be kept within the protocol and verified. That's this piece. That's OML 1.0. This is the silver solution. It's optimistic version of it. We put some rails where people can prove that I have accessed this model, but no one came and paid for it. And some penalty can be done for it for now, social penalty, but hopefully later some staking can be brought into it.
00:32:35.791 - 00:32:38.555, Speaker A: These are the two classes of solution that we are building.
00:32:39.255 - 00:32:44.595, Speaker C: Really cool. Yeah, I think it's a really cool approach. So I just wanted to hear you talk about it a bit.
00:32:45.255 - 00:33:18.705, Speaker B: Yeah, that's super cool. Thanks for going so deep on that, Himanshu. I guess just to zoom out for a second, what it all kind of comes down to is trust on some level. How do you trust these outputs? How do you trust these models? And one interesting thing that this whole, like, goat saga has revealed is there's a desire on behalf of crypto users to. They want to trust that it's actually the model tweeting and not. Not a human. Curious what you guys think about that.
00:33:18.705 - 00:33:35.657, Speaker B: Just high level. And then also, like, do you. Do you think that trust alone is kind of like the killer app for decentralized AI because you kind of need a blockchain to deliver that. Yeah. Curious how you guys think about that.
00:33:35.761 - 00:34:29.662, Speaker C: Yeah, I'll answer this question a little tangentially, I guess. I think there's like all of these different ways to create, I guess, like a more verifiable execution of a model or verifiable inference. There's many different cryptographic how much we just walked through most of them, actually. And I think how we're trying to approach this problem is I'd say analogous to, maybe even analogous to the move from transactions to intents in blockchain settings. Today, when we think about interacting with AI, we're thinking about interacting with a specific model. There's this very kind of model centric paradigm of interacting with AI, and in doing so I'm asserting that one that is the best model for whatever use I'm trying to pursue. And I'm also asserting that I'm trusting that that model is being executed as advertised.
00:34:29.662 - 00:36:01.035, Speaker C: This kind of verifiable execution and I think what kind of model coordination networks can start to enable is moving to this kind of objective centric paradigm of interacting with AI, similar to intense relationship to transactions. Instead of me specifying the specific functions my transaction is going to call the specific state, it's going to change, I'm specifying the intent ultimately and allowing this kind of open market of solvers to find the most optimal transaction to achieve that intent. In an objective centric world, or an objective centric paradigm by which we interact with AI, the user, the developer, the protocol, whatever it is, specifies ultimately what they want that AI to do well, what that objective function is, and enables this underlying kind of ever changing set of models to produce the best possible response as assessed by that objective function. And so I think there is potentially a way to subvert this problem of verifiable execution to a degree by moving towards this kind of objective centric paradigm by which what we want the AI to do well is the thing we have to place trust in, if that's even trust at that point, and enable this kind of market setting to solve the most optimal resolution of that objective function, as opposed to this model centric paradigm which does rely on kind of higher levels of trust in how we interact with AI.
00:36:01.735 - 00:36:02.095, Speaker A: Nick.
00:36:02.135 - 00:36:40.691, Speaker B: So I'm curious, like in that flow, in this objective centric world, I would come to Alora or I would just use a front end and type in a objective. You know, maybe I want to place a prediction markets bet or you know, make a defi transaction. What would happen after that? Would would there be like a model that sits in between and then routes my objective to the best model? How is that kind of ranking system done? And then as there's a handoff to like a specific model that then goes and executes my transaction kind of, yeah.
00:36:40.723 - 00:37:20.733, Speaker C: So there's sort of two possible scenarios, one in which this, in kind of, this model routing scenario. And then I think the kind of more default scenario in our existing system is one of model aggregation. But essentially you come to, you place some request to some objective function. I'll use price prediction as a simple example because I think it's easier to reason about. But you want AI to predict the price of ethusd an hour from now. And what you care about is less about this model predicting the price of ethusd. What you care about is that that price is the most accurate prediction as assessed by some loss function or some kind of objective function.
00:37:20.733 - 00:38:39.045, Speaker C: And then that request gets passed to this network of models who are each producing these price predictions. And at that same time this additional set of kind of intelligence actors are working to predict the sort of ultimately realized loss that each of those underlying models are going to realize. So these forecast or these additional kind of intelligence actors are over time trying to learn in which contexts these kind of base models perform well or not as well in to produce, I guess like a more nuanced and performant aggregate prediction as a function of all those models. And then what you get as a function of combining these kind of base inferences or these base price predictions along with these loss forecasts is this kind of forecast, implied aggregate inference that ultimately becomes this inference that you receive. And over time, the sort of ultimate weights of how each model influences the ultimate kind of aggregate inference that is delivered from the network is updated via a set of kind of assessing the ground truth relative to those outputs, etc. To kind of like further in state these incentives and I guess like a more sustainable setting. But that's essentially how it works.
00:38:39.045 - 00:38:56.417, Speaker C: You say I want the most accurate price prediction. This is how I'm assessing accuracy. This loss function, this target variable and this underlying set of models, this kind of bifurcated set of models are working to produce the best possible aggregate output as a, as a function of that objective function. If that makes sense.
00:38:56.521 - 00:39:06.825, Speaker B: Definitely. Yeah, that's helpful. Himanshu, what, what is your, what is the structure look like on sentient side? How does it, how does it compare to what Nick just talked about?
00:39:06.945 - 00:39:24.151, Speaker A: No, no, as, as I said this is completely complementary, right? Like the, the part which Nick is talking about is how these models are coordinating. I curious to know Nick, this sounds like a mixture of experts kind of thing where the weights are trained or like mixture of agents as they call it now, right? Because they may be more than just models, is that correct?
00:39:24.343 - 00:39:27.759, Speaker C: Yeah, I think that's a decent analogy for what's going on as well. Yeah.
00:39:27.847 - 00:40:09.435, Speaker A: So, so he's combining this different model, different AI to serve one output. But what we are working on is about the model themselves, the AI themselves, what models are they built on? And can you, can you provide these models to projects like these? So LLAMA is providing for free a base model, right? But, but if they try to build all this on llama, it doesn't work, right? So can we provide what is needed beyond it? And how do we incentivize people to provide that? That's the part we are looking at. But coming to your earlier point Michael, about trust, I completely agree that this is. Trust in AI will become. I said, right. We were discussing what how AI is helping crypto, but how crypto will help AI. I think trust is a fundamental part of this AI thing.
00:40:09.435 - 00:40:45.759, Speaker A: Imagine if all your information that you're receiving everywhere is being processed by some AI model in some way or the other serve to you, compressed or summarized for you. Your interactions are front face by AI models, right? Even when I'm talking now, I'm sure there should be a video product at some point which is correcting for all different lighting defects and all that. When I'm talking must be there, right? It has to be built. And that's an AI model actually. So then at that point you're not interacting with me, you're interacting with an AI model. I'm just giving some rag support based on what I've said in the past. And if that becomes the case, then this trust is the foundation of this case.
00:40:45.759 - 00:41:16.435, Speaker A: Today there is already this first hint of a first possible crypto AI scam vote. Is Truth Terminal actually doing all those things or is there someone actually making those millions of dollars? Right? No one minds. Everyone is excited. In a science experiment where Truth Terminal is doing all this thing, it's fine. But what if actually a person is taking away all that money and that's really distasteful and in a bad shape. And so you already want to know what model is running there and use cases like for te. This is a ripe use case for te.
00:41:16.435 - 00:41:47.369, Speaker A: People who are building te, they should deploy the trust terminal on te. I should be able to. In fact, we would like to put our fingerprints in something like a trust terminal. And you know that this model is sitting there in open, you can audit it and now you can check it's the same model which is standing there by asking some secret questions, right? And only you can verify it. So these kind of things will come. So this trust and putting this kind of trust in these models is One of the more important things that cryptography and crypto, together with it, with incentives, will solve it. Because how do you penalize.
00:41:47.369 - 00:42:08.785, Speaker A: How do you incentivize people to follow that trust system? Where is that? Where is the. Where is the protocol which records what, the identity of these AI pieces? Right. All that will be done through crypto, I think, and quite excited about that part. That's where crypto will help AI beyond the economic coordination, also the trust coordination way more.
00:42:10.045 - 00:42:51.785, Speaker B: Definitely, yeah. And as we've talked about. So you guys are both kind of building your respective networks. I'm curious. You both rely on or are hoping for a lot of models and applications to come and build on top of you. What is the constraint right now? Do the models just need to get smarter before we see a lot on chain? Did we need something like GOAT to kind of spark people's imaginations? Is there something else that's limiting the development, or is it just time? How do you guys kind of think about how models interacting on chain will develop over the next six to 12 months?
00:42:51.905 - 00:43:34.299, Speaker C: I think there's a number of things. I think one of the. The larger contributing factors is I think while crypto, the crypto industry has been quite excited about AI for a bit of time now, I think the AI industry, many external industries, frankly, are still quite skeptical about crypto. And so I don't think we've seen this sort of flood of kind of interaction from the AI industry into kind of building interesting primitives within the crypto space yet. I think we're starting to see it. I think it will continue to pick up, but just kind of due to the fundamental properties that crypto exhibits and creating these kind of efficient coordination environments. But I would say that's a big piece of it.
00:43:34.299 - 00:44:17.399, Speaker C: I think there's a number of factors. I think there is a lot of AI that has probably been interacting on chain for a while that has just been done via kind of like private trading strategies, or not as publicized or kind of personified as. As some of the newer stuff today. But I do think it's about building a better bridge in the direction from AI to crypto to kind of represent a comparable level of interest that crypto has had in AI for a period of time now. And that's what we'll start to kick off this flywheel of interesting new experiments of AI within crypto, interesting new models kind of being brought to the crypto space, et cetera. But that's kind of my take.
00:44:17.447 - 00:44:26.755, Speaker B: Himanshu, what about you? What do you think you know, crypto, what do you think needs to happen for cryptoeye really to take off and kind of meet people's expectations?
00:44:27.175 - 00:44:50.205, Speaker A: What are people's expectations? I think it's happening. I agree with you in the beginning that this is the step in the right direction. A meme coin, like goat is a step in the right direction. People should visualize that. I think these projects, all the projects, are making good progress. Bittensor is a great ambassador for that, and I see that. I see that at least in crypto, the awareness of AI is very clear.
00:44:50.205 - 00:45:23.757, Speaker A: What part of crypto excites the AI builders that's coming up, that's happening. So, for instance, this campaign that we are running in AGI House, that's a hackathon that we are running, AGI House, has their mailing list, is mostly AI developers, as you will know. AJ House is known for that and it's been a hit, like we have to. It's already. The slots are filled very quickly, and people were very interested in this kind of campaign. So I think. I don't think the resistance from AI builders is of this kind, that they're.
00:45:23.757 - 00:45:37.685, Speaker A: They are. They don't like crypto as such. You don't have to throw crypto in their face for everything. They understand what it means to build this kind of trust network too. So they like this part of it. I don't think there's any friction on that side. So it is happening.
00:45:37.685 - 00:45:55.025, Speaker A: It's a matter of time, Mike, of your options. I choose time. That's all that is. It's bound to happen. I don't think it's a very exciting space. It's a space which is bound to happen. It's not, you know, these kind of themes are coming out of a lot of alignment outside crypto as well.
00:45:55.025 - 00:46:20.239, Speaker A: And everyone is talking about it. Everyone is talking about now how knowledge network, information network has to be controlled, has to be coordinated. This is. This is too much. This is like if you don't do it very carefully or in some particular way, in a decentralized way with appropriate coordination layers, then this will go mad. This will make all of us mad and irrelevant. And that was my initial point about meme coins, which are AI driven too, or tokens, which are AI driven.
00:46:20.239 - 00:46:46.119, Speaker A: True that, you know, once only AI is running tokens, humans lose interest in it. And same for the information. Once only AI is generating information, humans lose interest in it. And that's a big danger. It will happen after a period of madness. And crypto can really help navigate through that by bringing in this trust, by bringing in this coordination through this Rails. I think it's a very important project.
00:46:46.119 - 00:47:08.941, Speaker A: Crypto AI is actually one of the more serious crypto projects at par with Bitcoin and Ethereum in early stages. I think it's that level of project. It's just been a decade. It's not like crypto has been going on forever. So remember 10 years ago at least maybe from where I was seeing from my vantage point, it was a crazy level socio technical project. Right. Bitcoin and follow up on Ethereum.
00:47:08.941 - 00:47:13.053, Speaker A: Crypto AI is the next version of the same project. It's very important. Yeah.
00:47:13.069 - 00:47:26.357, Speaker B: I think to your earlier point, most crypto people struggle with patience, but I take your point. This has been great, guys. Really appreciate you coming on and really enjoyed the conversation. So thank you both.
00:47:26.421 - 00:47:27.725, Speaker C: Yeah, this was fun. Thanks for having me. Awesome.
