00:00:01.200 - 00:00:03.525, Speaker A: You're now plugged into the Delphi podcast.
00:00:06.785 - 00:00:27.479, Speaker B: Hello everyone. Thank you for joining and listening on the Delphi Podcast. Today I'm joined by Dylan Jung from Pond AI and we'll start off with just learning a bit about him and love to understand how you got into crypto and particularly crypto AI and why the intersection really excites you.
00:00:27.657 - 00:01:26.879, Speaker A: Sir, Nice to talk to you Yan today. So I'm Dylan, I'm one of the co founder of Pond. So how did I get into crypto? It's kind of a long story. So I heard about bitcoin back in 2017. So that time there was a Harvard PhD was trading tokens and tried to get me over to trade tokens with him. And at the time I didn't really believe in the story of bitcoin because when I looked it up and all the other news that I saw that I read about scam of bitcoins and it's nothing really positive at the time, but later on, once I learned a bit more about bitcoin, I feel like this is very interesting, but it was two years later, so how did I get into crypto AI? So we started from developing a user search engine because I've been doing business for a really long time and I feel like searching for people is definitely a huge pain from my perspective because I don't know who knows who. I don't know who knows who well.
00:01:26.879 - 00:02:07.017, Speaker A: And it seems like as a third party I cannot do such a thing. You need to process Twitter, LinkedIn and Facebook data, Facebook data at the same time because you have identity misalignment. Because how can I prove yen on Facebook is the same yen on Twitter? And also I cannot process the data from them at the same time because they have different data structure. But I feel like blockchain itself has a unified data structure. As a third party, you can definitely process the data on there. And also it has different applications on it, which means that you have more data diversities. So we started by developing a user search engine.
00:02:07.017 - 00:02:46.447, Speaker A: So apparently different users, they have different on chain exposure. And we develop a preparatory algorithm to process the distance between different wallets. So we will be able to know who knows who and who knows who well. So if today I want to find Vitalik, I can simply, you know, just type in Vitalik's ens. I will be able to know who in my network knows Vitalik and who knows them well, or who knows who that knows Vitalik pretty well. So we started from that way and gradually we have realized that no matter is social data or financial data. Actually they are on the same graph network which is the blockchain engine space.
00:02:46.447 - 00:03:02.119, Speaker A: So we gradually expanded our use case and our focus is from social to Defi to mev. It's the recommendation pretty much. I just fully. We are just fully using blockchain on chain data.
00:03:02.207 - 00:03:16.207, Speaker B: Appreciate it. And before kind of going into the specifics, I think it would be useful to set the stage a bit and just provide a brief elevator pitch of sorts on kind of what the world of Defi looks like with pond.
00:03:16.391 - 00:04:53.685, Speaker A: Yeah, so apparently defi, I think everything with Defi now is kind of fixed, you know, no matter it's going to be the fixed structure or it's going to be, you know, the mechanism itself, it doesn't have any dynamic parts. But what machine learning and deep learning can bring to Defi is definitely a more dynamic structure and a mechanism that allows people, especially developers and project owners, to adjust some of the fixed parts within the defi to be more and more dynamic. And also I heard from one of the founders of a very successful Defi protocol. So what do you feel about AI? And DeFi is more, you know, can be analogy in Web2, in a real world economy it can be such a thing, you know, because the AI is more like the manufacturing business because it generates values, it finds, it retrieves the values from the data. But defi is more like the financial sector, you know, it's kind of like trading or know, secondary market or primary market that help real world economy like manufacturing business. They definitely feel like there's a relationship there. But it definitely needs a systematic effort from AI agents, from AI models and also from the education on the market about, you know, how machine AI, deep learning can help with Defi also in their perspective, in the long run it's definitely, you know, DeFi helps AI and so this should be a combination there because in real world economy it's really, you know, the finance helps the manufacturing business to grow.
00:04:53.685 - 00:05:33.479, Speaker A: So this is something like what we envision. So for now, because Defi has a very matured use case and they have a very mature business model. So it's really AI helps DeFi and later on it will be, you know, DeFi helps AI and AI helps DeFi. Both should coexist and it should be in an equal position. But what the model, what the AI can bring to Defi is really about some dynamic structure. Later on we can touch a little bit on the fee structure because lots of liquidity providers on Defi, they actually don't make money. Although they provide liquidity to liquidity pools.
00:05:33.479 - 00:05:35.911, Speaker A: This is something we can change, which they add.
00:05:35.983 - 00:05:56.913, Speaker B: Thanks, very helpful. Diving a bit in. You guys are building graph neural networks GNNs instead of LLMs, large language models. So I think it'd be useful to kind of describe the difference between the two and with a bit of a focus on why the former works well for blockchain.
00:05:57.009 - 00:06:24.705, Speaker A: Yeah. So AI model. The time when you want to develop an AI model, you need to find the best possible model that's specifically designed for this type of data. So in Web two, the majority of the data actually is language data. So the model or the foundation model has to be a language model. But in web3, the most crypto node native or the most web3 native data is 100% on chain data. So on chain data first of all is not a type of language data.
00:06:24.705 - 00:06:43.941, Speaker A: It's actually a graph data. So what is graph data? Actually it is. As long as there's a data structure, it can be represented by nodes and edges. In the blockchain context is the wallets. Wallets are the nodes. Edges are pretty much just interactions between different wallets. For example buy, sell, transfer.
00:06:43.941 - 00:07:34.723, Speaker A: And also nodes can be contracts. So edges are just interactions between wallets and contracts like bicycle transfer, Mint approve this type of interactions. So since it's a type of graphical data, so the model has to be a graph model. And among other graph models, GN is the most suitable model structure type of model that is designed for processing behavioral data and make predictions. And this is especially useful for processing blockchain data because end of the day it's a type of graphical data and it's very behavioral driven. Just imagine all the what is they are interacting with each other, what is the next contract? They are interacting with each other. So that's why we chose gnn.
00:07:34.723 - 00:07:42.351, Speaker A: And that's what GNN is really about. It's about learning behaviors and predicting behaviors in the future.
00:07:42.463 - 00:07:58.055, Speaker B: And just thinking about kind of the journey that you guys are taking from building a single GNN model to building a decentralized AI model layer. Can you kind of touch on the rationale there and what that's going to look like over time.
00:07:58.135 - 00:09:02.055, Speaker A: Yeah. So initially we started developing a large gn, but we gradually realized actually because the ecosystem is kind of early data comparing with developing a large model, it will make more sense for us to develop different smaller models and smaller use cases driven models. So this is something I will be working on. So just because like I said in the very beginning, all the Use cases, no matter is in social, in DeFi, MEV and all that. They are actually all in the same graph network, which means that we can develop a bunch of models and these models will be assemble together as a model layer to power different applications and also will help developers to develop something with them. So that is why we chose our direction from developing a large, from a large graph model to what we are developing now. It's a layer of models by opening up our infrastructure of models and also the data to the market.
00:09:02.055 - 00:09:59.165, Speaker A: So that's something that we've been working on because we feel like we definitely need a systematic effort to provide models to developers and to applications. And it seems like it's very time consuming for developers to process data and also for them to, you know, develop or come up with different use cases. They prefer to have an existing model and then plug it in use. So that's how we made the transition from developing a large graph model to a model layer. And the model layer itself, it definitely has so many components in it. It has inference, it has, you know, some of the model and data infrastructure we've been using to develop the K. So we have gradually opened it up to the market and also we have built an ecosystem around that from developers, from applications and also we are working with third parties to have their support and also embed some of the models into the SDK, this type of thing.
00:09:59.165 - 00:10:07.215, Speaker A: So a model is the model itself, but a model layer is really about ecosystem. So we are definitely pushing the ecosystem these days.
00:10:07.325 - 00:10:22.259, Speaker B: Got it, got it. Now that's helpful. Thanks. And so I guess kind of touching on what that whole process has been like. I think, you know, you guys had some breakthroughs in terms of the predictive capabilities of these models and so would be great to kind of touch on that.
00:10:22.387 - 00:11:16.927, Speaker A: Yeah. So for now we have shipped different types of modeling security in DEFI and in recommendation. So what we feel about from what we have developed so far is the AI can make a great impact in crypto applications and in crypto use cases. Mostly it's because, you know, the on chain data structure is relatively simple compared with what we have seen in Web two. Because if you want to process language data well, you have to, you know, develop a fairly large language model and you have billions of parameters. And for each time if you train a model there tends to be very time and a capital consuming and heavy. But just because the simplicity of the on chain data pretty much just like interactions between different wallets and contracts.
00:11:16.927 - 00:12:12.155, Speaker A: So the accuracy is pretty high. Mostly it's because nobody's. First of all, not many people, they are doing AI and using AI to processing on chain data, which means that there are a lot of behaviors that actually are very simple to capture. So for example, in our security prediction we had around 92% of the accuracy to detect malicious behaviors and malicious addresses. So for example, if this address is efficient wallet or not, or this token potentially is it going to be efficient token or not? So just because we all have our own inertia as human being when we are, when we are interacting with a token, we tend to have some behavioral partners. We don't change or haven't really realized it. So this is something that we've been doing.
00:12:12.155 - 00:13:11.723, Speaker A: Also on recommendation, for example, we have build a recommendation system with Zora. So we have had like 52% precision compared with Amazon's 6% of the recommendation system. Which means that this kind of recommendation accuracy can help pretty much or the framework itself can help pretty much everything in the market today to build a killer recommendation system. This is something like we didn't really expect by ourselves, but later on when we saw the result, that is pretty amazing to see just because of the simplicity of the on chain behavior. But gradually because there will be more and more machine learning mechanism that has been adapted to this to process on chain data. So we definitely expect that the accuracy of precision will drop, but it wouldn't drop significantly. Which means that we by using AI techniques we'll provide a much cleaner on chain future.
00:13:11.723 - 00:13:50.625, Speaker A: And also we can support a lot of applications that have an AI angle. Just imagine what we are using today in Web2. Pretty much every application that using in Web2, they have a machinery angle. For example, the time when you open your Instagram, the time when you open your Twitter, the time you open your fintech application to all have machine learning angles. In a fintech app that are using, you probably have different algorithms to prevent any money laundering. You probably have some of the risk control on Twitter that you have recommendation system embedded in. But in web3 applications we haven't seen many of that.
00:13:50.625 - 00:13:57.347, Speaker A: But gradually it will be the case and we definitely believe, you know, it won't be long.
00:13:57.531 - 00:14:20.511, Speaker B: Got it. And so how do you kind of go from this, you know, quality foundational model to a platform? And I know you don't want to disclose too much, this is part of the secret sauce of where you're going. So just to kind of, you know, provide some perspective on kind of how you see this evolving into, into a platform I think would be useful context.
00:14:20.603 - 00:15:13.099, Speaker A: Yeah, from our perspective, we feel like model itself is important, but also the ecosystem itself is equally important. Sometimes it's more important than the technology itself. So we definitely feel like first of all the model should be decentralized because a model basically is a business of experiments. As long as you have a very matured machine learning framework and you have developed on the way, you have a killer infrastructure of model and data. And then later on it's really about different types of experiments. We feel like this type of experiment can be achieved by using some decentralized technique. You can leverage the community, you can have some sort of an incentive mechanism to allow people to contribute to this experiment.
00:15:13.099 - 00:16:01.127, Speaker A: And from this experiment you can have as many models as possible. And later on you can find the best possible one from selecting or from evaluating the best someone with some sort of mechanism. So that is something that we feel like the community can help. I can give you one example, which is the Wikipedia. So if today you are Google and Google, they do have a knowledge base, but it's definitely much smaller than Wikipedia itself. But the reason for that is really because the Wikipedia has leveraged the community so everyone can contribute to it, which result in is the largest knowledge base in this entire human world. None of the centralized entity can outperform that because no matter how much capital they have.
00:16:01.127 - 00:16:49.967, Speaker A: But it's really about the community, it's really about experiment. We are adopting the same methodology here to decentralize the model development process. And also we gradually will open up our infrastructure to the community as well. So this is something that we've been doing and so far I think we have that definitely seen some very good traction among our community about what we are doing. And that's pretty much it. And also we are giving developers the opportunity to own their model because the on chain data is transparent, which is not something that Web2 has. So this is saying that who owns the data, who owns the model? So Web2 developers, they have no sense to own whatever they have developed.
00:16:49.967 - 00:17:16.925, Speaker A: They basically just contributed whatever they have developed in the past to big tech because only big tech owns the Data. But in web 3, everyone owns the data, which means that developers can have a chance to own their models or they can own whatever they have developed in the past because they own the data technically. So this is something that we are pushing and this is the angle we are using as well to leverage this, this kind of like decentralization of model development.
00:17:17.865 - 00:17:37.697, Speaker B: So in terms of, you know how this looks in practice. Yeah. How do you see that this existing Is it going to be, you know, existing primitives are outsourcing some of their logic to pond? Is it going to be kind of completely new primitives built from the ground up or do you see some kind of combination of both?
00:17:37.801 - 00:18:45.465, Speaker A: Yeah, so we definitely feel like this is a combination of both because we do have two requirements we gather from different projects. So first of all, just because the industry is still very early and this entire industry is exploring at the moment, so many people, they actually didn't know what to do with machine learning also, although they feel like machine learning makes total sense for them to have such an angle because we have the data, as long as we have the data, we definitely can process those data through machine learning or deep learning. But they don't know what to do, they don't know what the use cases are. So we have to work with them to find what is the best possible way to work together and also what are the use cases they are interested in. So for example, in defi dynamic fee structure is definitely lots of defi protocols. They are interested no matter it's going to be a decentralized liquidity pool structure or it's going to be a lending protocol or staking protocol. They all always want to have a way to maximize the return or lower the risk of their stakeholders.
00:18:45.465 - 00:19:47.141, Speaker A: But also at the same time we have seen projects that do have a sense about what they want to have. For example, some of the defi protocols they are very interested in having strategy recommendation, they have a bun of defi strategy defi strategies and they want to find it out which one works the best for different, you know, users. And apparently they have too many, for example, some of them they have maybe, you know, more than 100 strategies and it's definitely not human understandable, it's definitely not human readable. So they want to provide a recommendation engine for their users to make sure that they can have the best possible strategy that's specifically tailored to their use case or is perfect for them based on their on chain behavioral habits. So this is something I would have seen as well. So it's definitely the combination of both. Yeah, but gradually I feel like with the industry gets more and more mature people will have better understanding about what they want and what they need and what machine learning or what AI can help them to do.
00:19:47.293 - 00:20:15.437, Speaker B: Xyz that's helpful in terms of kind of the implementations, right? You have yield optimization, you have kind of better forms of protection, whether that's managing leverage or more even kind of security risks. Do you see some of these, like I guess what's the lowest hanging fruit? You think that can see an immediate benefit?
00:20:15.581 - 00:21:01.139, Speaker A: Yeah, so we think is that the lowest hanging fruit is definitely trading. Because to process on chain you require a lot of labels. Because the main reason for that is you want to have explanation of things. For example, if I see a contract today, they have interactions with different types of wallets. I want to understand what is the contract and what are those wallets. For example, is this a market maker or is this a defi protocol or such a thing? You require lots of on chain data labels. And this part is extremely immature at the moment in this entire ecosystem because people don't have the incentive to label contracts to label wallets.
00:21:01.139 - 00:21:41.615, Speaker A: And basically how it works is heavily relaying on public goods. So it's a big shout out to all the open source and the public goods developers these days. They are actively contributing to data labels to projects like us. But it's still definitely not enough. If you develop a model requires labels, we need explanation of things, but trading itself, it doesn't really need that many on chain labels like other use cases in defi or in security and all that. So that is the lowest hanging fruit. And trading itself is a big topic.
00:21:41.615 - 00:22:35.225, Speaker A: It can be a trading model, it can be a risk management model, it can be a recommendation model, it can be the combination of the three and it can be much more so treating itself is the lowest hanging fruit for now from data perspective, from a model perspective, we definitely feel like it is something like we can do for example on security and on recommendation. Because for a lot of protocols, a lot of projects, they actually have a very matured ABI and API they can use directly and build a machine learning models around that. So basically model business is really about the data. And I think the difficulty of developing models is really about the data as well. So the lowest hay in food is low or it's not low enough. It's really up to how accessible the data is.
00:22:35.965 - 00:23:21.929, Speaker B: It's tough to kind of talk about crypto and AI right now without really mentioning agents and how much they've kind of taken off. And you see a range from obviously token terminal being kind of the first. And so that's kind of just a creative good origin story and creative agent. And then you kind of gradually go down the spectrum and you see some with a bit more functionality, kind of like the AI 16Zs which is managing a semi AI hedge fund and then like did their own terminal with creating a recommendation engine. Do you see this space being Kind of affected or enhanced by kind of what Pond is building or I guess is there a role that Pond can play in that ecosystem?
00:23:22.057 - 00:24:42.129, Speaker A: Yeah, so AI agent actually is one of the biggest AI model applications these days. So actually there are tremendous amount of synergies between AI models and AI agents because at the end of the day, AI agents are the application of AI models. So with more AI models, more diversified AI models, this definitely will create more use cases for AI agents and thinking about AI system Z or Dither on Terminal. And even we have virtual these days, which is very popular in this market. They all are leveraging content creation models, image generation models and language models. And we definitely feel like this is area that we can tap in and we can support because we'll get more cryptomative models that can literally allow AI agents to complete cryptomative tasks and provide much better services in crypto AI in general. But Yan, I'm just really curious about your thoughts on AI agents because you are very visionary investor and you have seen, you know, lots of, you know, AI powered, you know, applications and AI agents today.
00:24:42.129 - 00:24:45.329, Speaker A: So what are your thoughts on AI agents these days?
00:24:45.417 - 00:25:34.667, Speaker B: Yeah, no, it's a very generous description. It's definitely something that immediately, you know, caught a lot of interest. I think it touches on a lot of areas, right? You have these agents that have varying personalities and have the ability to reach a wide audience, right? So that is compelling on its own. And then you kind of strap on functionality and the ability to even manage money, right? So it's one thing to be kind of a useful tool, it's another to be actually, you know, making investment decisions altogether. And so I think there's a really large surface area there. I think it's also creative because you have these models and these model creators that like you mentioned, otherwise don't really have much they can do in terms of monetization, Right. And so here you have a path.
00:25:34.667 - 00:26:39.335, Speaker B: I think tokens are always going to be one of crypto's biggest unlocks. And so the fact that you can now monetize these agents in creative ways and really improve the amount of access that people have to them, which, you know, allows them to also improve over time, but creating a way for these builders to actually generate an exit. And so what that ends up leading to, I think is, is basically a really virtuous cycle of, of creating this, this two sided marketplace where on one end you have, you know, speculators, which is also, you know, crypto is very, very rich in. And so on the other hand, you have something that can draw in some of the, the best model builders, right. Because now they can actually have a path to liquidity. And so I, I think this, this kind of marketplace is a really unique and really kind of accelerated way to bring the best agents to market and incentivize people to go out and figure out unique use cases. Implementation.
00:26:39.335 - 00:26:52.005, Speaker B: I think it's still very early and what we've seen now is really just scratching the surface. So it kind of going back to what you mentioned earlier, one of the biggest moats is going to be creating this ecosystem.
00:26:52.045 - 00:26:52.221, Speaker A: Right.
00:26:52.253 - 00:27:48.647, Speaker B: Because if you can create a platform that simultaneously connects the speculator speculators in the best way and attracts the, you know, as many of them as possible, it'll then bring in the model builder side as well. And so they have the incentive to build there one because, you know, the speculators are there. But I think all of that brings just a lot of attention. And so building their model there would also generate the most kind of use for their model from outsiders that are kind of looking at these platforms to figure out where they can find value, both from an investing perspective and from a utilization perspective. And so I think it's really trying to figure out this platform idea but also gradually iterating on what works and what doesn't and understanding where the models most compelling. And like it's not just, you know, this one off kind of launch a token and it has to be purely mimetic. Right.
00:27:48.647 - 00:28:22.009, Speaker B: There are very direct ways for these agents to capture value from their use via fees. And so I think like naturally you'll have the quality ones rise to the top based on just the interest in using them. And so yeah, I think creating a really strong platform that you can bring both sides of the market together is really kind of an interesting way for these agents to grow. Because right now you see a lot of them kind of exist on a standalone basis.
00:28:22.057 - 00:28:22.241, Speaker A: Right.
00:28:22.273 - 00:28:47.285, Speaker B: And it's hard to understand what they kind of do. And then it's also hard to judge them against each other because there's no kind of standardized methodology to judge their historic accuracy or you know, kind of intelligence. And there's different scores you can apply. But so I think the platform also helps solve for that and really create discovery for the best ones to really rise to the top.
00:28:47.405 - 00:28:56.781, Speaker A: Right. It's definitely very insightful, I think. I feel like I have learned a lot from your words just now. Thanks for sharing. It was definitely very insightful to me.
00:28:56.853 - 00:29:42.911, Speaker B: Yeah, I appreciate that. Yeah, I'm learning a Bunch as well. Yeah. And so just kind of thinking through continued implementations, I think one of the areas that's always struggled on the OR or I think was initially a massive unlock. And so just we're talking about AMMs, right? They were initially this massive unlock, almost kind of like a perpetual motion machine where we can create liquidity. And then the fees are great. But then we started noticing, you know, there are some deficiencies in the sense that, you know, you have LVR and kind of il and basically the idea that you're, you're always as a liquidity provider trading you're providing stale quotes, so you're selling low and buying high because someone's usually arbitrating the other way.
00:29:42.911 - 00:30:18.581, Speaker B: And so what that meant is that you're. There's probably better ways to deploy that capital. I think that, like, there's some holes in that argument in the sense that, you know, for smaller assets, I think it still makes sense because the goal is to really provide liquidity rather than really maximizing yield. But over, you know, on larger assets, when, when there are other kind of ways you can deploy that capital, it's it, you know, the, the flaws stand out a bit. And so, you know, one of these issues or one of the ways people have addressed it is through a dynamic fee.
00:30:18.613 - 00:30:18.757, Speaker A: Right.
00:30:18.781 - 00:30:35.303, Speaker B: And so this kind of goes back to what you're saying about creating kind of a more dynamic infrastructure or dynamic system for people to deploy these assets into. So, yeah, curious. How are you thinking about the AMM side?
00:30:35.399 - 00:31:08.187, Speaker A: Yeah, so it probably would be shocked after I hear that around like 60% of the liquidity providers for some of the Dexes, they don't make money. Then what's the point for them to provide liquidity? It's simply by providing liquidity, they are not benefiting anything from the market. So the biggest problem for that is first of all they have some toxic flaw. Some of the users probably they would just use some techniques. For example, on one hand they hold certain type of tokens. On the other hand they're providing liquidity. So this is liquidity pool.
00:31:08.187 - 00:32:11.891, Speaker A: And they dump this token, which they don't will cause a significant amount of impermanent loss. And they make another money from this, another source of money from providing liquidity because they caused impermanent loss of another token. And also another thing is, apart from toxic flow, a lot of liquidity providers, actually the Dex itself is not promising a lot of benefits for them, which means they didn't do it deliberately. It's really because they didn't have A very cool tool for them to support such a thing to have a dynamic fee structure for them to adjust based on the market performance, for example. So let's say when the market is not performing well, you can lower the fee, which means that it will further incentive people to contribute more liquidity. So users will have a much lower slippage. When the market is performing really good and the projected P and L of this peril token is going to increase, then you increase the fee.
00:32:11.891 - 00:33:11.325, Speaker A: So you can make sure other liquidity providers, they can benefit from the token price change. And also by from providing such a liquidity to this liquidity pool. It requires machine learning because end of day it needs prediction. You need to predict what's the potential PL of this token pair in this liquidity pool with the market performance in the secondary market of this token pair of this token, of these two types of token, but usually one is stablecoin. So this is something like what the market needs. And Uniswap is about to launch their hook mechanism and hook mechanism is further boost this dynamic fee landscape. So we need to have more and more dynamic parts in defi mostly is because it's a constantly evolving market and it needs some technique to help liquidity providers to safeguard their interests.
00:33:11.325 - 00:33:43.865, Speaker A: And also such a mechanism also applies to other verticals, for example ens. As you can see, if today you are, if you mean to ES today you probably will be charged on a, on a, on a fixed price. This can be 0.1 ether or it can be, you know, 0.3, but it's a fixed price. What cost that is really, you know, we don't have such a tool to support, you know, based on the market condition, you know, how many people they are maintaining. Yes, you can further increase the price of the ENS and all that.
00:33:43.865 - 00:34:48.405, Speaker A: Even lots of fee structure in this industry can be changed. It's not only defi, but also it can apply for lots of applications like ENS and all that. So it's definitely something we feel like we can improve to this landscape. So for EMM the time then we can protect more of the interest of liquidity providers. There will be more liquidity providers that are incentivized to provide liquidity, which will further decrease this leverage for users. Because the liquidity is going to be there, the DAPS is going to be there and we will have more and more efficient decentralized finance landscape and we have more and more efficient dax, which means that we can further boost the involvement and development of decentralized finance compared with the Centralized finance. So all those like change we can in the machine can bring to the defi space.
00:34:48.525 - 00:35:08.255, Speaker B: And would this exist as a vault? So this would be an example of kind of an integration on top of existing defi or. So this you wouldn't necessarily need to rebuild AMMs from the ground up. Right. This would just be kind of a managing vault that would. Otherwise it would tap into the same kind of liquid.
00:35:08.375 - 00:35:39.709, Speaker A: Yeah. I think at this stage you can have incremental improvement for DEFI by working with them to do different features. But gradually with more models, with more and more machining techniques, you can literally just build a defi protocol from ground up. You can literally. I think a lot of people, they are talking about IFI these days. You can literally build an IFI application from ground up. For example, you have a foundation model that's powering your ammunition and you have a recommendation engine based on this foundation model, based on this entire.
00:35:39.709 - 00:36:06.717, Speaker A: On chain behavior, not only the behaviors in your own protocol. There's so many things you can do with some machine learning support. You can literally build something from growing up. But I feel like the engineering works is also very important. But they should be coexist. We do have a lot of applications even in web tool. Their entire base is really about machine learning.
00:36:06.717 - 00:36:41.955, Speaker A: They have graph database in it. So they can do risk management by default from the very beginning. And they can have recommendation. They can always recommend you the best yield strategies and the best portfolio strategy for you. And they will have entire ecosystems support that and they give the recommendations for their ecosystem. So we definitely have seen so many things that has been achieved in Web2 through machine learning and we believe such a thing can be also the case of the future of defi.
00:36:42.035 - 00:37:05.143, Speaker B: No, that's exciting. And then just kind of to think about going back to the structure of these systems and model ownership as part of this platform. How do you kind of think about value capture and both for you guys as the platform and then all the builders you're incentivizing to come on.
00:37:05.259 - 00:38:07.765, Speaker A: Yeah. So I think in what we criticize a lot about Web2 is really about value capturing. The users or developers or contributors, they don't have a share about whatever they have contributed to this company. But from the AI perspective, AI is really about data and it's really about machine learning models and it's really about infrastructure around that and application. So from the data perspective, because you own the data, because data itself is accessible for everyone, which means that everyone owns the data, which means that everyone can own whatever they are using, they are contributing. So it's really not about how much money you make, it's really about how much money you retain from the money you make. So just because the accessible data, which means that other developers, all the model developers, they can own the model they are developing once the model itself has been plugged into real time data.
00:38:07.765 - 00:39:06.795, Speaker A: But in Web two, all the model developers contributed basically because they don't own the data, they pretty much are just donated or they get whatever all the big tech or the tech company who owns the data allow them to get. But that's such a thing that this web3AI can change. So the model ownership is definitely something like developers can own the model, they can own the revenue of the model from people using it because they own the data. So that is the huge difference. And this is the direction we are targeting to allow developers to own their model because they own the data. And we are building this kind of culture mentality for people, for developers, and also for the projects we are working with. So the time when a project uses our model, it's not a model developed by us, it's a model developed by the community.
00:39:06.795 - 00:40:07.481, Speaker A: So we want to give them this kind of mentality that the time when you are given revenue, you are providing revenue to the model providers, you are not providing to us, you are providing to the infrastructure, you are providing to the community. So that is the value capturing part. So we are trying to maximize the revenue of our model developers, of our community. And in the future, even users, they can contribute to this model because we can build a very user friendly interface to allow them to assemble, to combine different models together, to have a model orchestration. So you can have a better AI agent in different use cases and you can have more and more diversified use cases as possible. And also there will be a community that's contributing models and that's going to be very scalable. Because a lot of use cases in today, in this current AI landscape is kind of exploratory, it's kind of experimental.
00:40:07.481 - 00:40:59.451, Speaker A: You definitely need a community to help you to contribute to different use cases and for this ecosystem to develop different use cases and to see what works, what doesn't work. So some of the models that we have developed, for example security and recommendation, they have been proven that they work really, really well. Even compared with web2. They work like at least 3x on 5x. Well 5x better than web2. But there are definitely some types of model we thought that we can ship pretty well, but it turns out because the diversity of the on chain data is not big enough, it's not diversified enough to support different use cases that we thought that makes sense. Which means that some of the use cases is not supported by the current on chain data landscape.
00:40:59.451 - 00:41:18.449, Speaker A: So we need experiments, we need a scalable way to experiment which is to leverage the our community. To leverage our community. The best possible way is really to provide them a model ownership so they can, you know, retain the value they have developed in the past and which will further incentivize them to do so.
00:41:18.537 - 00:41:33.641, Speaker B: Something you you mentioned that I wanted to double click on was kind of, you know, models interacting with each other on, on the platform and that ecosystem. Can you kind of provide some examples of what those interactions would look like? And yeah, yeah.
00:41:33.753 - 00:42:00.597, Speaker A: So I think one easy. What easy use cases can be treating. For example, trading itself is a huge topic. It's not really about a trading model. It can be something like. It's a combination of risk management model, a recommendation model, a token recommendation model, a trading model, on chain behavioral prediction model. It can be so many things.
00:42:00.597 - 00:42:37.265, Speaker A: Just think about all the hedge funds we have heard. Citadel, two Sigma, et cetera. So they have hundreds of models. They function at the same time to make sure that traders or agents, et cetera, they can have better understanding about what's going on. And to make a best possible decision based on that, you definitely need an orchestration of models rather than one single model. And one single model is always not enough. And even the model itself requires constant evolvement.
00:42:37.265 - 00:43:26.105, Speaker A: And this is something about the orchestration of the models. And once you combine these model results altogether, model outputs together, you can make a better decision still taking the trading, for example, let's say you have a risk management to say certain types of token, they probably have another risk profile because of the holders of this token. They tend to be some DJ players or they are fairly new to the market. So 70% of the token holders, they are new to the market. And their word profile proves that they are new to this market. Which means that this token might be some appetite of the risk. In some ways you want to do something around that to manage your risk.
00:43:26.105 - 00:44:23.607, Speaker A: And this token recommendation model, it can be something. For example, back in the days when Uniswap traded fully on chain, when it pumped File axing in 30 days, what happened on chain, what is the behavior two weeks before? And you want to learn that and to apply it to the market to see which token has a similar on chain behavior. And you can have a security model, for example. This is a malicious Address detection model that can help you to detect potential malicious tokens. So to lower your risk so all those models can function at the same time together and so you can have the best outcome for you to make a decision. So the model, one single model is not always, not enough for a lot of use cases. You definitely need a more complicated model orchestration for you to make a decision better.
00:44:23.607 - 00:44:28.203, Speaker A: Yeah, so that's what a modding model looks like.
00:44:28.299 - 00:44:45.323, Speaker B: Got it. No, that's helpful. And just to think about what the next couple of months look like for you guys or what you can share what you're hoping to accomplish and kind of how you'd like to interact with the broader crypto ecosystem as you guys are building out.
00:44:45.419 - 00:45:33.895, Speaker A: Yeah. So in the coming weeks we are going to launch our point system to power developer ecosystem to have some guidelines about what they should work on and what they can work on. And also we will do a lot of research on model development. By providing guidelines to our developers, they will be able to understand a bit more and also catch up the pace quite faster. So they will understand what to do and what are the goals of machine learning tasks we are proposing. And also we will be working with a lot of big players in this market because people gradually understand. People gradually start understanding the importance of AI models because it produces values to the market and we'll be working with them.
00:45:33.895 - 00:46:28.631, Speaker A: And also at the same time we want to further expand our developer community. And also we have launched a series of competitions that allow developers to have some hands on experience and with our guideline, what they can achieve. So we really want to push this entire machine learning boundaries. And also we want to explore use cases with our ecosystem, with our partners fast enough. And also we want to achieve this in a scalable way by opening up our model and data infrastructure to the market so people can use it. People can use the infrastructure we used to develop a cadre model, if we can develop a cater model. And also we have stretched out our model and data infrastructure by ourselves because we've been pushing everything to the extreme by developing our own models, which means that other developers, they will be able to use our infrastructure to develop models by themselves well enough.
00:46:28.631 - 00:46:35.819, Speaker A: So those are the tasks on our roadmap that we are about to ship in the coming months.
00:46:35.967 - 00:47:09.291, Speaker B: Yeah, no, it's very exciting. It seems like the kind of the perfect time to do it as all of the on chain activity heats up with market volatility and that drives a lot of fees. And so I think the surface area for optimization is huge right after that happens and a lot of the stuff tends to look obvious in hindsight. But no, it's great to have that and I think it's timely. So that's exciting. And if people want to kind of learn more about what you're building or. Yeah.
00:47:09.291 - 00:47:12.667, Speaker B: Figure out how to, how to work with you guys, what's the best way for them to find you.
00:47:12.731 - 00:47:38.821, Speaker A: Yeah. So the best way is to check our website, cryptopun.xyz or follow our Twitter. So we're going to launch a series of exciting announcement pretty soon with some big projects and also we're going to showcase what we have and how we can help the developer community to develop models with us with our guidelines and later on we even can work on the guideline together. Yeah. So we're going to really push this decentralization of model development.
00:47:38.973 - 00:47:39.573, Speaker B: Exciting.
00:47:39.669 - 00:47:40.021, Speaker A: Stay tuned.
00:47:40.053 - 00:47:41.773, Speaker B: Thank you so much for joining me today.
00:47:41.949 - 00:47:45.365, Speaker A: Appreciate the time. Thank you so much for having me. Yeah. Thank you so much for your time. Yeah.
