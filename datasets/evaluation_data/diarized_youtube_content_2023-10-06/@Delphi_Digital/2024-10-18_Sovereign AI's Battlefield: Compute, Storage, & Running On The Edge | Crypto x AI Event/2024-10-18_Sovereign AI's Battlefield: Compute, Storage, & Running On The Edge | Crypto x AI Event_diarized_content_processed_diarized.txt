00:00:00.360 - 00:00:35.861, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors near Empowering Decentralized Applications and Blockchain ecosystems. Venice AI, a private and uncensored alternative to popular AI apps. Mira Unified AI Infrastructure secured by Crypto and Theric, the first on chain multi agent system. To learn more about all of our sponsors, check the description below and dive in.
00:00:35.861 - 00:00:37.105, Speaker A: Enjoy the show.
00:00:52.455 - 00:01:17.791, Speaker B: Hello everyone. Welcome to DAI Month at Delphi. Today we are talking all things infrastructure from the frontiers of decentralized compute to storage to pushing the limits of what can be done at the edge. This is Pondering Durian from Delphi and I am joined by a stacked lineup of panelists today. So we have Greg, founder of Akash, we have Mohammed, founder of Exo Labs, and we have Michael, co founder and CEO of Zero G Labs. Gentlemen, welcome to the Delphi Podcast.
00:01:17.983 - 00:01:20.149, Speaker C: Nice to be here, thanks for having us.
00:01:20.237 - 00:01:20.861, Speaker D: Yeah, likewise.
00:01:20.893 - 00:01:33.509, Speaker B: Yeah, so maybe we just get started with kind of quick intros and then for people that might not be familiar, just a 10 22nd download on your project, what you guys are working on and then we'll dive in. So Greg, maybe we start with you.
00:01:33.557 - 00:01:59.391, Speaker D: My name is Greg, I'm the CEO for Oracle Collapse and the co founder for Akash. Akash is the world's first supercloud. A supercloud like the name suggests, is a super cluster that sits on cloud capable compute and offers a cloud like interface while providing a market driven price point and a decentralized mechanism to access hard to reach resources, particularly high density GPUs.
00:01:59.503 - 00:02:00.575, Speaker B: Awesome. Michael.
00:02:00.655 - 00:02:58.205, Speaker C: Well, hey everyone, I'm Michael, co founder and CEO of Zero G. Also zero Gravity in the long form. What we're building is the first decentralized AI operating system and we started at the data layer. So we made sure that we created a decentralized storage network that's fully available, has fast data ingestion and retrieval rates at costs up to 80% off centralized systems specifically made for AI workloads. On top of that we built a data availability layer which can get throughput of up to 50 gigabytes per second per consensus layer so that we can eventually do fully on chain trading for large scale LLMs. And then soon we'll be releasing something called the Serving framework where anybody can register as a serving node and offer things like inference and fine tuning kind of services to the rest of the ecosystem, we really want to be the connective tissue between a lot of Web3AI projects.
00:02:58.285 - 00:03:26.525, Speaker E: All right, Mo, I'm Mohammed, I work at Exolabs. What we work on is essentially allowing you to run any models yourself on your own hardware. So there are many great efforts to democratize AI like meta creating these best models in the world, like for llama 405B. But to use these models, you still need to have like multiple. Like in the case of llama 405, you need like multiple H1 hundreds. What we do with Exo, which is fully open source, you can like any compute. You have like a few MacBooks and run any model fully privately on your own.
00:03:26.525 - 00:03:36.161, Speaker E: And that's it. It's just like your own little private cluster. There's no that work involves anything controlling you, which allow you to run stuff uncensored. You cannot run stuff privately. You can run stuff for free as well.
00:03:36.233 - 00:04:22.395, Speaker B: Okay, awesome. And as the guests can tell, I think we have an interesting group where we have folks that kind of started from the decentralized cloud side and have been expanding outwards. Other folks trying to essentially transform what decentralized storage and retrieval looks like. And then Mo, kind of you coming at it from the opposite angle, pushing things out of the cloud entirely and kind of, you know, what can be done on local devices at the edge. So I think we have some unique perspectives to kind of go at this infrastructure conversation with. But I guess the first topic I really wanted to talk with is kind of more directed at Greg and Michael. And it is the conversation around demand, particularly in competition with the hyperscalers.
00:04:22.395 - 00:04:56.435, Speaker B: Right. And so obviously these are massive markets, but you are going up against pretty dominant incumbents. And so I am curious, you know, how you guys think about your go to market. And Greg, maybe we start with you. It does seem like in Q2 and Q3 you guys have seen, you know, some nice momentum. So kind of curious to learn a little bit about where that demand is coming from and kind of how you guys have started to hone your go to market motion. But yeah, so just curious to learn about kind of the demand side and you know, some of the recent customer wins that you guys have had in Q2 and Q3.
00:04:56.525 - 00:05:32.405, Speaker D: Right. So demand side, we had our. So we measure our success in our progress in leases created and leases, a deployment or an application. Last quarter we saw a 50% gain compared to the previous quarter in terms of leases. So there's clear growth there in terms of revenue compared to last year and daily fees. We 10 xed by full order of magnitude essentially. So there's clear demand in terms of users paying money to deploy, but when stack up to hyperscalers, we are nowhere close in comparison.
00:05:32.405 - 00:06:58.975, Speaker D: And the reason for that is right now the economies of scale are playing a big part, especially with training, but it's also showing cracks. So we saw like what happened last year where yes, you had these hyperscalers which have massive amounts of compute, but they were hyper selective in terms of who gets the compute. So that led to a lot of frustration across the industry and we saw a lot of, you know, researchers, AI companies trying to look for alternatives. I think we caught a bit of that windfall and we were able to, you know, leverage that to gain momentum and establish ourselves as a serious pair. Our big win out of the whole, you know, scarcity of GPUs was Nvidia. So Nvidia being one of the few protocols integrated into Akash. Nvidia's Brev in particular is a uses Akash, Akash's compute because you know, in Akash's model of going to places where compute is available but not available to be used by general public, particularly companies that are doing machine learning or doing crypto mining that have a bunch of GPUs, like high density GPUs, a one hundreds and a two hundreds that they're not using for various reasons.
00:06:58.975 - 00:07:58.925, Speaker D: So Akash can go to places like that and make that compute available in a decentralized manner. So that's in an open source manner too. So that's very attractive to companies like Nvidia because Nvidia obviously want to serve users but they're unable to because of supply chain constraints and they most certainly don't want to lose users, especially researchers and academics who have quite a lot of influence on the future of AI to alternatives beta amd. So they want to keep as many users as possible in the Nvidia ecosystem. And one of the ways to see that come to fruition is by repurposing underutilized supply or underutilized gpu. So I think that allies quite, quite, quite well within media. We're also seeing quite a lot of, you know, a whole cohort of users that have been priced out or unable to get access, like researchers in universities.
00:07:58.925 - 00:08:44.455, Speaker D: So we saw University of Texas using Akash quite a lot. We saw Rochester Institute Technology using Akash quite a lot. Their rumblings about other universities that are leveraging Akash because, you know, in order to get a GPU at least it was extremely hard till about a Few months ago to get these H1 hundreds and A1 hundreds. This story in Semaphore about, you know, how a kid from Cornell was unable to get GPUs from Amazon, but it was able to get on Akash and, you know, create a company that got acquired by Nvidia eventually. So we have some success stories that are in the wild that use Akash. And I'm really glad to see these users come to Fusion. Our next big leap.
00:08:44.455 - 00:09:50.483, Speaker D: And compared to hyperscalers, I think our biggest. There are two sets of innovations and I'm super impressed with what xolabs is doing. Mohammed leading the effort there with decentralized inference. So being able to distribute inference, rather being able to leverage distributed compute clusters to do inference on Massive models 405B is a big win because bringing that onto a decentralized network where the primary advantage is access to resources that are locked away under private networks. And using xo, we can leverage those hard to get resources and do so in a much more scalable manner. Right. And what we're seeing with decentralized training, with efforts like Dialogue or, you know, Distro by News Research, dial co from, you know, Google DeepMind and open dialogue from Intellect, that is proving that we can actually do training across geographically distributed clusters.
00:09:50.483 - 00:10:19.859, Speaker D: So we have training and we have inference, and it's only time that these technologies will come to Fusion. And EXO is doing fairly well in terms of adoption, in terms of recognition by the general public. Right. So it's only a matter of time that the decentralized networks that can leverage distributed compute will be challenging hyperscalers because of these technologies that are getting to main stage.
00:10:19.987 - 00:10:48.445, Speaker B: Got it. Yeah, we'll definitely get to decentralized training, but I want to get Michael in here as well. So kind of also on the storage side. Right. We've kind of seen, you know, maybe less inflection in terms of some of the existing solutions than we might have hoped for. So I am curious, you know, kind of what you intend to do differently at zero G. And obviously performance is one thing, but kind of some of the other keys that you guys are pulling on in order to kind of change the paradigm.
00:10:49.135 - 00:11:40.765, Speaker C: Yeah. Specifically on the storage side, the issue has been in distributed and decentralized systems is that most of the storage that's been offered has been the form of cold storage, which I think is only a small percentage of all storage that's needed. And so if you're just doing kind of log cold storage, it's really hard to retrieve. It's really hard to store, then you're not really meeting the use cases that you need for AI. And so what we've done is we've added multiple levels of abstraction so that all Web2 data types are actually now found in Web3 as well. So you've got the log storage on the bottom, then you've got a key value store, and then you even have transactional storage that you can store onto our network. And with that essentially come fast data retrieval and ingestion rates that you need actually for it to be completely usable.
00:11:40.765 - 00:12:35.415, Speaker C: So we've run a test, for example, doing fine tuning with a 7 billion parameter model, and after a few minutes, basically our storage system caught up and there was no overhead compared to centralized solutions. And so finally we have the kind of technology available that can actually do that. Other issues with some other systems include the tokenomics themselves. So in some systems you have to actually pay upfront for kind of permanent storage. And given how quickly these models expire, why would you want to, for example, store a 7 billion parameter model that's going to be out of date next month forever? So we make it a very kind of flexible use case. So we're targeting about $5 per terabyte per month for our storage solution. So it's really easy to adopt and then really easy to use just like you would a centralized solution.
00:12:35.415 - 00:13:07.447, Speaker C: So that's on the storage side. You asked something about kind of go to market as well. For us, we're Primarily focused on Web3AI as our go to market. So really early adopter market, we've seen a lot of phenomenal traction. We have about 300 projects or so building on top of us. We've recently had spikes of about 5 million transactions per day or so, which has been growing about 100% kind of week over week. So we feel really positive about some of the traction that's coming our way.
00:13:07.447 - 00:13:40.755, Speaker C: And it's going to take a while to catch up with the hyperscalers on whether it's storage or compute side. But I'm very hopeful. For example, I read a study, I think that was maybe two or three years old, where the hash rate of Bitcoin was basically compared to all of Google's data centers. And Google's data centers at that time were only 1% of the entire hash rate of Bitcoin. So we've shown before that we can do it as a decentralized space and I'm very hopeful and excited that we can catch up in two to three years time or so.
00:13:40.835 - 00:14:08.203, Speaker B: All right. Looking forward to it. And mo, I guess switching gears a little bit, kind of just thinking about what can be done on device versus in the cloud and specifically around trade offs in latency in privacy and in cost. So yeah, I am curious to kind of get your thoughts on what makes most sense to be done locally and also in the cloud and specific use cases that you guys are excited about at exo.
00:14:08.339 - 00:14:49.143, Speaker E: Yeah, so in general I try to think about it not so much what's good local, what's good for the cloud, but more like what is the problem you're trying to solve and which type of solution makes sense. And you have all these options available. There's on device, there's maybe locally with multiple devices, there's maybe a hybrid local and cloud and there's maybe fully cloud. So it really depends on the problem you're trying to solve. One of the problems that I'm very interested in that makes sense in the local bucket is if you get privacy, there's all these cool technologies like FHE and so on, but really the simplest and best way to be private is just on premise. So. And privacy for retail is a little bit of a meme.
00:14:49.143 - 00:15:35.227, Speaker E: Like I care about privacy, but most people don't really like use social media web2 all the time. But businesses do. Like for businesses, legal privacy is often a real thing and hedge funds, bio companies, law firms and so on legally have to be private. So from that perspective, what I think about is well, what is the easiest way to have stuff on premise in the context of AI buying GPUs and setting them up and maintaining them is very hard. You literally need to be an Engineer to buy GPUs and set them up privately yourself. And also it's expensive. So what we think about there is just like if you already have loads of hardware in a company or as an individual and so on, could you reuse that to do stuff on premise because you care about privacy? If you think about performance, it's very hard to match the performance of Nvidia GPUs in a data center with anything on the edge.
00:15:35.227 - 00:16:27.641, Speaker E: So if you just care about more tokens per second, it's very unlikely that you'll be able to do that locally versus the cloud. But if you had the constraint of a business of I legally have to be private, then the trade offs are very different there. It's not general cloud versus edge, it's more for this specific context. You have these extra requirements and then something maybe moves onto the other. And in general the kind of Things that could make sense locally are things we care about, privacy, things we care about, low latency. For example, if you play a game, that little bit of latency between your device and the cloud can make the game really annoying versus everything on device. I think cursor AI actually is very interesting, a place where you have hybrids, you have cloud in ChatGPT like the main code generation, but you have a few very small models sitting on device for things like text completion, which just makes cursors so much nicer to use than other things that are fully in the cloud.
00:16:27.641 - 00:16:54.275, Speaker E: But also if you have cursor only locally in your device, you'll not be able to use Claude, for example. And Claude is an incredible model. Claude Sonnet 3 Personally I really like using it. And these things are very hard to do locally. So it depends what you're trying to do. And often cloud makes sense, often the hybrid makes sense, or the edge. And the edge stuff is typically, if you care about cost, latency, privacy and so on that kind of category, there might be a solution that is edge or a solution that's hybrid.
00:16:56.255 - 00:16:58.435, Speaker B: I like that framing. It makes a lot of sense.
00:16:59.015 - 00:17:36.015, Speaker C: One thing maybe I'd add our perspective on device versus fully decentralized is that whenever there's societal level issues involved. So if AI agents or models are going to run full production systems or logistics systems or or administrative systems, they really have to be governed kind of through blockchain economics. So slashing and incentive conditions versus on device. But everything else we think is very appropriate to be on device as well over time as they grow in capacity and so on.
00:17:36.355 - 00:17:54.817, Speaker E: Yeah, on that point as well. Often when you think about these problems, there are two types of AI things you use. There's single player and multiplayer things. Like a chatbot is just like, you know, a single player thing. It's just like me using it and that's it. So that can happen on device. But there are things that just could never happen on device.
00:17:54.817 - 00:18:27.621, Speaker E: Like literally, for example, if I want to run like a social media algorithm or a credit score, like these are natural, like, you know, multiplayer things. So even if you care about privacy or trust or viability, and you could do it on device, it wouldn't work. Like I could run Uniswap on my single local device. Like it's not complicated formula, but like it doesn't work that way because it's the multiplayer setting. So that naturally can never be fully offline. I couldn't turn the Internet off and run use of myself because it's a Multiplayer game and the same for a lot of like things like insurance or social media platforms and so on.
00:18:27.693 - 00:18:28.545, Speaker C: Yeah, super.
00:18:30.005 - 00:18:50.707, Speaker B: All right, switching gears a little bit, but I am curious about each project's superpower. So kind of what is the core essence of your project? What is the one thing that you do better than anybody else? So if you had to point to one thing that succinctly defines, you know, why you guys are creating value uniquely relative to competitors, what would that be?
00:18:50.811 - 00:19:33.051, Speaker D: Well, second startup, I mean the biggest value I think Akash gets is truly open source and truly open community. Right. So today akash has about 500 contributors all contributing in a open setting and overclock labs. The company behind akash employs about 25 people. So majority of the contributions come from outside the company and that's extremely critical for network effects and that's extremely critical for survivability on the long run. Yes you can create companies as a closed source system, but in order to create communities and protocols you need to be open source and that is the foundation of decentralization. Without that foundation, I believe you cannot have decentralization.
00:19:33.051 - 00:20:36.801, Speaker D: So that's why I have, I take up quite a lot on closed source system set, consider themselves as decentralized systems. I think there's no place there. On top of that I think other competitors, I mean Akash has got a phenomenal orchestration advantage in terms of resources. Akash you get like any cloud. You can get access to storage, ephemeral storage or even persistent storage as a matter of fact to IP addresses to you know, a global orchestrator. Like you can have one single file that can configure workloads across the world and you know, connect them, interoperate with them with hundreds of data centers right in a single setting and along with other resources, be it compute, be it GPUs, be it, be it bandwidth and whatnot. So it's a whole holistic cloud instead of, instead of just a simple GPU network.
00:20:36.953 - 00:20:39.005, Speaker B: Got it. Mo, how about you?
00:20:39.425 - 00:20:45.605, Speaker E: I'm not sure what we do really good. I think a lot of bad stuff. Maybe one thing that we do.
00:20:45.985 - 00:20:51.121, Speaker B: Greg was just giving you a lot of love so you're doing something good. So feel free to brag a little.
00:20:51.273 - 00:20:52.697, Speaker D: Yeah, they're open source.
00:20:52.841 - 00:21:20.195, Speaker E: Yeah, I guess we're open source. I guess one thing that we do good is it's what some people refer to as like user owned AI. Like with XO you literally can just be offline, off the grid. You don't need to know anyone and you can have the models yourselves. I made this demo once where I had something like running on my phone and local cluster and I just turned the Internet off. I told her I am lost, help me. Which is to symbolize if you don't have Internet Access or if OpenAI shuts you down, there's nothing you can do.
00:21:20.195 - 00:21:32.405, Speaker E: But in these very local models you could just have your own little self sovereign model. So that's I guess a good thing we do. Yeah, I'll keep it there.
00:21:32.905 - 00:21:33.681, Speaker B: That's a good thing.
00:21:33.753 - 00:22:36.555, Speaker D: Extremely humble Here let me add I think what they do incredible at least the way I see it, they bring sovereignty to AI like no one else can bring sovereignty. Right. There's nothing that beats a private AI, a private model that doesn't leave my network work. So cases like diagnosis, like I had a recent episode, you know, for a health diagnosis that I didn't want to talk to ChatGPT because I know ChatGPT is going to share my data. I had no idea who's looking at my data, right. For something like that I had to spin up a model on Akash which takes about five hours to boot and cost a lot of money, right? RH1 hundreds and whatnot. If I had a model in my home running on my local compute, unused compute clusters with my wife's MacBook, my MacBook, my child's MacBook, I can easily talk to the model and have guarantee privacy guarantees.
00:22:36.555 - 00:22:49.955, Speaker D: I think privacy is extremely underrated in the world of massive AI adoption and what I think exolabs is bringing the true sovereign private models that no one else can offer.
00:22:50.255 - 00:22:54.435, Speaker E: I hope your diagnosis all went well and hope you're healthy and happy.
00:22:54.855 - 00:23:14.133, Speaker D: Oh, I am healthy and happy. It was just the diagnosis. I had a lot of questions I couldn't ask the doctor during the diagnosis so I came back and asked the llama 3.1 was incredibly resourceful. But yeah, in a scenario like that I wouldn't be comfortable asking ChatGPT. I'd rather want a local model. I know that I have privacy guarantees.
00:23:14.269 - 00:23:16.785, Speaker B: Nice. Makes a lot of sense. Michael, how about you?
00:23:17.405 - 00:24:33.099, Speaker C: Yeah, I definitely hope everything's okay with you Greg and all best wishes. If any recovery is needed on our end, I'd say we're very good at pulling out the most hardcore engineering and research talent out of top programs like mit, Stanford, Tsinghua, and as a result from Turing award winner classes and so on. And as a result we have 5 Olympic gold medalists and informatics on the team which then leads us to build fully on chain AI solutions that are actually truly decentralized and then also have the performance requirements necessary for on chain AI to actually really work. And so that's kind of why we had this breakthrough around, for example, the data availability layer. We've been able to showcase that each one of our nodes can do about 30 megabytes per second in terms of throughput, which is already 3x what most entire networks do. And so it's horizontally scalable, so you can get to 50 gigabytes per second, which is similar to what Infiniband does in data centers from a throughput perspective. So I would say that's what we contribute is kind of brutal, high performance throughput.
00:24:33.217 - 00:24:49.239, Speaker B: Well, that is. That is five more Math Olympiad gold medals than we have at Delphi, that's for sure. Um, but. All right, all right. I guess I was going to stop. This is kind of halfway through. I was going to do a quick round of overrated versus underrated.
00:24:49.239 - 00:24:53.871, Speaker B: Are you guys game? Just very quick answers, just right off the cuff. Is that okay?
00:24:53.983 - 00:24:55.515, Speaker E: We can always edit stuff out.
00:24:55.975 - 00:25:07.145, Speaker B: Yeah, yeah. I don't think these are going to be too spicy, but all right, we're going to jump in. We're going to jump in. So Google's current valuation, it's $2 trillion.
00:25:07.605 - 00:25:08.665, Speaker E: Underrated.
00:25:09.365 - 00:25:10.465, Speaker B: Underrated.
00:25:11.325 - 00:25:20.021, Speaker C: I would say in the long term, underrated. If they can get Gemini to consistently be fully integrated and as performant as.
00:25:20.053 - 00:25:33.867, Speaker D: Other solutions, I would say underrated. They have so much going on for them. Their AI is really good. The TPUs are very good. I don't know why they don't get too much love and they have so much cash to buy nuclear reactors.
00:25:33.931 - 00:25:51.015, Speaker B: So my. My pa. My PA certainly hopes you guys are right. All right, number two, the scaling loss as kind of our road to AGI and asi, obviously, Mohammed, you're a fan of the bitter lesson. You guys think this can continue. Overrated or underrated?
00:25:52.165 - 00:26:06.973, Speaker E: I think I would say overrated. I believe in the scaling and that scaling works, but you need to scale the right things. I think just dumb scaling is not what works, but scaling generally is the right direction. So slightly overrated.
00:26:07.149 - 00:26:25.765, Speaker C: I'd say overrated as well, just because I think alternative approaches may get us to AGI the same way. So, for example, what if you put millions of smaller language models together in some type of LLM middleware? Is that going to have a better result? So I think more experimentation needs to be done.
00:26:26.265 - 00:26:32.657, Speaker E: I would say that's scaling, but just in a very different way than dumb scaling. As in more parameters. So that's why I think that's true.
00:26:32.841 - 00:26:33.361, Speaker D: Yeah.
00:26:33.433 - 00:26:34.765, Speaker C: Different types of scaling.
00:26:35.345 - 00:26:58.327, Speaker D: Yeah, I agree. I think it's overrated. I'm a history buff and I've been studying quite a lot of how humans evolved in terms of intelligence. We evolved not because our brains got bigger. We evolved because we started talking to each other. Better mixing and through diversity. It came through our evolution.
00:26:58.327 - 00:27:30.975, Speaker D: We stopped evolving physically about 10,000 years ago. We got smarter over time. So it's not necessarily throwing a bigger model that may result in better intelligence or better smartness. But really, I think, I believe it's going to be. Or not I believe, but there's a strong probability it's going to be a lot of smaller models that may come together that may lead to an AGI. Right. But it's a high chance.
00:27:31.715 - 00:27:58.545, Speaker B: Yeah. The network of networks, Internet of Intelligence model is certainly one we are rooting for. All right, next one. So I know you guys are multifaceted. It's not all technicals and AI. So we're going to shift to economics, the de dollarization narrative. You guys think King dollar is here to stay or there's real legs behind de dollarization and other economies, you know, shifting off the US Dollar.
00:27:58.545 - 00:28:01.225, Speaker B: Michael, I'm going to put you on the spot.
00:28:02.965 - 00:28:04.665, Speaker C: Overrated or underrated?
00:28:05.165 - 00:28:05.685, Speaker B: Yeah, yeah.
00:28:05.725 - 00:28:25.391, Speaker C: What if it's like. What if it's like, right in line? I think there is going to be kind of national challenges, but it's going to take quite, quite a long time before that happens. And it's also going to take quite a bit of time for digital currencies to catch up. So I'd say it's right in line with what most people think. All right.
00:28:25.543 - 00:28:28.167, Speaker B: Properly rated. All right. Greg, what do you think?
00:28:28.311 - 00:28:45.359, Speaker D: Overrated. Overrated for a long time. Yeah. And the reason is the petrodollar, and I think it's going to be semiconductor dollar. It's going to be, you know, energy dollar. Right. Especially semiconductor dollar.
00:28:45.359 - 00:29:30.395, Speaker D: I don't think people are paying too much attention. The power US has over semiconductors. Right now, it looks like TSMC or Taiwan has all the talent. But, you know, the shift really started happening in the 90s, and we are not that far away from bringing back the talent to us. Whoever owns semiconductors will own the economy. And the world is so fragile and so connected to the US Dollar that any disruption to that, you know, stability will bring about a lot of chaos. So we have bigger problems to fry.
00:29:30.395 - 00:30:06.785, Speaker D: We have energy problems, we have semiconductor problems. We have. We are accelerating. And I think, like, the. What I call the decline of Chinese economy really showcases that there is really no other competitor to us in terms of how fundamentally and organically strong the US Economy is. And that has to do with the exuberant privilege that we have as the creators of the dollar. So, yeah, I think it's heavily overrated.
00:30:06.785 - 00:30:18.885, Speaker D: Heavily overrated. It's not going to happen in a long time, but that doesn't mean it won't happen. But it's going to take a very, very long time. Definitely not in the timeline that people think it's going to happen.
00:30:19.005 - 00:30:24.629, Speaker B: Mo. Mo. Very quickly. We've got a lot of. A lot of things to cover, and Greg's got a drop suit, so I.
00:30:24.637 - 00:30:30.125, Speaker E: Haven'T thought about this much before, so I'll pass. But Greg sounded smart, so I just. I'll just agree with that if I have to answer.
00:30:30.665 - 00:30:39.497, Speaker B: All right, all right, we'll take it. And one. One word answer. AGI by 2030. Overrated. Underrated. Greg, let's start with you.
00:30:39.497 - 00:30:40.737, Speaker B: Underrated.
00:30:40.921 - 00:30:41.793, Speaker D: Underrated.
00:30:41.889 - 00:30:43.545, Speaker B: Okay, Muhammad.
00:30:43.665 - 00:30:44.705, Speaker E: Underrated.
00:30:44.865 - 00:30:46.985, Speaker B: Underrated. Okay, Michael.
00:30:47.105 - 00:30:48.445, Speaker C: Same. Underrated.
00:30:48.785 - 00:30:55.217, Speaker B: Underrated. All right, I like it. We've got some bulls here. All right, we're going to shift. Sorry.
00:30:55.321 - 00:30:58.585, Speaker D: Type 1 civilization by next 50 years.
00:30:58.745 - 00:31:01.451, Speaker B: I don't know. I don't know what that means. What's the type 1 civilization?
00:31:01.532 - 00:31:47.683, Speaker D: So Kardashev, scale is this. So type 1 civilization is this civilization that can leverage all sources of natural power on a planet that planet naturally generates and a planet that has, I suppose, planetary defenses to combat external threats. So Kardashian is this like, I guess, Soviet scientist who. Astrophysicists who came up with these scales like type 1, type 2, type 3. So type 1 is supposed to be this, like, really advanced civilization. Type 2 is supposed to be like a civilization that can harness stars using. What do you call the.
00:31:47.683 - 00:31:48.791, Speaker D: The thing around the star.
00:31:48.863 - 00:31:52.545, Speaker B: 50 years. We have 50 years to terraform the stars.
00:31:52.620 - 00:31:53.661, Speaker E: No, not really.
00:31:53.736 - 00:31:54.405, Speaker B: 50 years.
00:31:54.480 - 00:32:08.503, Speaker D: 50 years to be a type 1 civilization. That means a civilization that can harness all the energy produced on Earth. That's really what type one civilization is. And right now we're 0.7. Type 0.7.
00:32:08.639 - 00:32:21.557, Speaker B: Okay, well, I mean, if you guys are this bullish on AGI by 2030, I think a inflection to super intelligence might see us at type one pretty soon. So I'd be. I'd be bullish. I'd take the under.
00:32:21.701 - 00:32:25.221, Speaker D: But underrated means you're bullish on the rated.
00:32:25.253 - 00:33:10.585, Speaker B: Okay, yeah, well, yeah, I Think like we would get there faster, but. All right, all right, I'm switching. I'm going to get us back on track here. So I do want to talk a little bit about decentralized training, which is something that came up earlier and obviously there was the distro announcement from NOS research and I always thought decentralized training was kind of going to be the bottleneck for dai, but some of these results were pretty impressive. So I am curious to get Yalls thoughts on decentralized training versus kind of the current soda and how quickly or if at all these approaches might catch up and the timelines there. So Michael, I know this is kind of near and dear to you guys in terms of data transfer, so I'm curious to get your thoughts.
00:33:11.205 - 00:33:53.731, Speaker C: Yeah, we think parity probably in two to three years. We're doing a bunch of research on it as well. I think there's a few fundamental constraints. Like one is how do you do batch training, how do you do asynchronous training, how do you deal with network latency? So once you figure those three things out, then hopefully you can coordinate a mass amount of GPUs across the network and figure all this out. Then there's also the issue of how do you do verification in such a way that you don't rerun the entire kind of compute? Because that wouldn't make sense. It's already super scarce to get GPUs. If you then have some consensus and you need to verify via consensus, that makes no sense because then you're just rerunning it.
00:33:53.731 - 00:34:33.933, Speaker C: So how do you get the cost of verification super low in that type of environment as well. And so once we figure those few things out, then I think we can get to the holy grail of hey, now we can use all these GPUs that are fully available and let's go train bigger models. But that's only part of the equation, like figuring that out. What about the data side? How do we unlock new forms of data? How do we unlock private data? How do we unlock synthetic data? And so all of that is going to be very important in our journey as well for training. So I would say yes, there's compute issues, but there's also other issues we have to figure out.
00:34:34.029 - 00:35:19.344, Speaker B: Got it. And then I do, I guess switching Mohammed to you. I have heard you talk a little bit about kind of hardware scaling versus software scaling. And in crypto circles there is a decent amount of emphasis in terms of scaling underlying blockchains using software solutions like ZK Co Processors or op co processors. But obviously the hardware is also getting better both in terms of what is used to train the models and in terms of the chips that basically end up on devices. So curious to get your thoughts on whether blockchains actually end up scaling better through software or through hardware or maybe just privacy in general.
00:35:19.804 - 00:36:06.585, Speaker E: I'm not sure fully get the question. I guess in general scaling software versus hardware in cases where scale worked in the past like machine learning breakthroughs like chess games like Transformers and so on. What worked quite well is fairly general methods scaled with really good hardware. So the best methods for image recognition were not network state of image recognition and handcrafted approaches. It was like CNNs to just discover on their own like what the features are and all that stuff and like scale to real our size. I'm not sure in the context of blockchain of like what the trail exactly or. Sorry.
00:36:06.745 - 00:36:30.475, Speaker B: Yeah, so I guess I was thinking about like how do you actually instill blockchains with a decent amount of scaled computation given they are relatively slow. So obviously zero G is taking one approach and then you can also just use like I'd say better chips and tees and do things using hardware.
00:36:30.975 - 00:36:50.513, Speaker E: Yeah, I mean so when I think about a blockchain is like we're using a blockchain because like a central server is bad because we trust. I'm not sure if there's a case where use a blockchain because it's more efficient per se. It's more like well, I don't want to be censored and given to. You want to be censored then what is the best thing you do?
00:36:50.649 - 00:36:58.045, Speaker B: No, yeah, well I guess obviously you would want both. You would want high performance and distributed and trustless environment.
00:36:58.545 - 00:37:21.627, Speaker E: Yeah, I guess I just struggle to see in which setting a blockchain solution is just better performance wise than just a trusted solution. Because if you can't build these things in a setting where you don't trust everyone, then surely the setting where you do trust everyone is just more efficient. It's just like we do the blockchains because we have to. Because like I mean I don't want to trust X, Y and Z. So that's why. But yeah, I hope that makes sense.
00:37:21.651 - 00:37:47.995, Speaker B: Got it. Yeah, makes sense. And I know Greg's running out of time so last question, I am curious like why each of you decided to focus on this specific problem and you know, what does success look like for a cash for zero G and for exo, you know, five years, 10 years out so just mission and then kind of your personal KPIs as leaders of these organizations.
00:37:50.215 - 00:38:27.005, Speaker D: So for Akash, I mean the journey goes back to 2014. So I've been a early contributor to cloud, native technologies, Stack, be it Terraform, be it Kubernetes, Docker and whatnot. And the dream was really to, to really to create an open source cloud. Right. So we were able to, you know, until more recently we are able to create open source, what do you call software. But it was not very clear on how to create open source networks. Right.
00:38:27.005 - 00:39:06.437, Speaker D: I think that's where decentralization came into play. And our vision is we cloud, which is perhaps the most important fabric that holds our modern society together. And a very important public utility should be as free as, you know, the air we breathe and the water we drink and the access to which should not be gated. And that was the whole premise of starting Akash. And we still are on that, on the vision, right. For us, success, success we measure in market share, right. So our first milestone would be 1% market share of the cloud business.
00:39:06.437 - 00:40:13.197, Speaker D: And the reason we think we can get there is because, you know, as compute gets more relevant in especially the AI, there's a case to be made that compute will become a necessity in terms of ownership. So people will have computer own compute. And unlike the current scenario where you know, we have mobile phones and which are significantly less powerful than these centralized servers which are sitting on the cloud, you know, this pattern is not going to be permanent. I think we're going to go back to the older pattern where we have more power at the edge. And like, like what I described about like having private models or you know, I'm my house, I'm building a little data center to, to be able to, you know, run my own AI. You know, I think and we have a lot of devices on the edge are very powerful and we have Tesla vehicles. I mean Elon was famously quote quoted about six months ago talking about how they can leverage GPUs and Teslas.
00:40:13.197 - 00:40:41.565, Speaker D: There's a lot of compute on the edge that's not just, just not being used. So that makes me extremely optimistic in terms of if we can build decentralized technologies, can leverage this compute and the software stack to complement this distributed compute that we can use for useful things, we can effectively get to the market share realistically. So that's how we measure success.
00:40:42.145 - 00:40:46.971, Speaker B: All right, Mohammed, kind of mission and success for exo.
00:40:47.123 - 00:41:15.247, Speaker E: Yeah, Just what I think about is democratizing AI means frontier models available to anyone. So There are two pieces here. There's some ways to create frontier models right now that's like meta. And the second part is once they're created, need to be available, they actually can use them. Because, for example, right now llama 405B is like, hardly anyone can use it. So those are the two things we think about. And most of our effort right now is on the second.
00:41:15.247 - 00:41:46.511, Speaker E: Just like we assume someone has created these open models, we're trusting that Zuck will keep financing this for a while. And given that these models are available, we want to just make it lower the buried entry as much as possible. And there are multiple ways you can think about it. There's business metrics, for example, the value of the company based on how many customers we have, and so on. But I think a lot about just how close we are to the world, because I think about it. I also personally just think a lot about the existential risks of that. I did my PhD was in robotics.
00:41:46.511 - 00:42:29.185, Speaker E: And at the time you always had all these fears about robots, like killing people and so on. These are not the most obvious, these are not the most likely fears. Most likely fears is like right now, when I need information, I used to go to Google, look at different websites and read it. In the future, I used to go to ChatGPT and ask it something and then you get a biased response like, these are the things that are. That's like the more real fears, like truths being obfuscated and so on. And we have internal KPIs for those two goals, like creating frontier models and making them available to everyone. And that's what I think about unwilling to success because also, like, the business metrics look good, but in a world where AGI exists, I'm not sure that even matters.
00:42:29.185 - 00:43:06.439, Speaker E: So they're important to keep track of and obviously for the discipline of making sure you're on the right track. We look a lot of these business metrics, but really I just think about the vision democratizing AI is frontier models available to anyone, which means somebody needs to create them and we have to guarantee they'll be created without having to rely on Zuck or something. And the second one is just like once they're created, you'd be able to run them. And we have two internal metrics for that that I think about, and I'm very. I think we talk about those a lot more. Obviously, the business metrics are also important, but almost secondary. Two metrics for creating frontier models and running them.
00:43:06.567 - 00:43:12.155, Speaker B: Got it. And what are you going to do? Post asi when you're made redundant, how are you going to spend your time, Mohammed?
00:43:14.015 - 00:43:18.343, Speaker E: I'm not big brain enough to think through that scenario. My brain.
00:43:18.519 - 00:43:21.835, Speaker B: You haven't got to the type one societies that we're going to build yet.
00:43:25.215 - 00:43:38.095, Speaker E: My IQ level is fine tuning models or building a small system to think about what I will do in a world in which super knowledge exists. I don't have the IQ points for that, but maybe we'll get there if I augment myself with an AI or something.
00:43:39.755 - 00:43:43.531, Speaker B: It sounds like it's coming soon. All right, Michael, Michael, over to you.
00:43:43.563 - 00:44:20.623, Speaker C: And zero G. Well, our mission is to make AI a public good. And so we want to avoid a world where AI essentially wakes up one day and says like, hey, I can do this much better without humans. And so we basically want to make sure that AI consistently serves humans needs, humanity's needs, is fully aligned, is safe. And for that we take the very strong stance that there's no future in AI without blockchains. And that's the future that we're building towards. We want to get to technical parity with centralized or what we also call closed AI systems hopefully in two to three years.
00:44:20.623 - 00:44:50.311, Speaker C: And after that then really build on the superpowers that blockchains give us. Things like alignment and security and prevention from nefarious actors and fair rewards, distribution, the verification, the trustlessness. So you can actually independently say, hey, is what I'm being served from this inference request actually truly true? And so we want to see a world like that happen.
00:44:50.463 - 00:45:07.915, Speaker B: Okay, I think that is a really good place to end. So thank you guys, really appreciate you taking the time and really look forward to watching Yalls projects evolve because I think you guys are working on really important things. So thanks for coming on the Dai Delphi video conference. Appreciate it.
00:45:08.295 - 00:45:09.875, Speaker E: Yeah, thanks for having us.
00:45:10.295 - 00:45:12.015, Speaker C: Yeah, I really enjoyed it. Thanks for having us.
