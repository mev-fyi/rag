00:00:01.560 - 00:00:03.845, Speaker A: You're now plugged into the Delphi Podcast.
00:00:05.745 - 00:00:22.001, Speaker B: Hey everyone. Welcome back to the Delphi Podcast. I'm your host, Tommy and today I have on the absolute dream team for crypto AI agents. I have. I'm going to go around in a circle, let everyone introduce themselves. Let's do that. Let's just start with you somewhere and we'll go around the room.
00:00:22.153 - 00:00:44.245, Speaker A: All right. Hi, I'm Justin Beddington, also known as somewhereacy on Twitter. I'm the founder of Somewhere Systems. It's an experiential media consulting arm of myself that has refused to die over the past six years and the creator of Sentience, who I'm sure that you've probably been replied to on Twitter with a very strange question at least once in the past two weeks.
00:00:44.825 - 00:00:46.925, Speaker B: Yeah, Shaw, go for it.
00:00:47.945 - 00:01:13.617, Speaker C: My name is Shaw, I'm longtime agent dev web3dev. I founded a thing called AI16Z and we created a project called Eliza which is now powering a whole bunch of different agents doing a whole bunch of different stuff. A lot of it's social, but they're also in games. We're building, you know, people working on and adding it to hardware, a lot of different applications and we're basically just bringing contributors on and making it kind of a big boat for everyone to be the rising tide. So it's open source, you know.
00:01:13.721 - 00:01:19.497, Speaker B: Hell yeah. Love AI16Z and I love all the projects built on Eliza. There's hundreds of them. Ethan, you're up.
00:01:19.601 - 00:01:43.965, Speaker D: Hi everyone, this is Ethan. I'm co founder of Myshell. So basically at my shell we are providing the Xcode and also app store for the agent builders. So basically it's workflow builders. We are providing all kind of like image generation voices, large language model of course for the developers to build their product and also it's for users to discover either entertaining or useful AI applications. Yeah, awesome.
00:01:44.005 - 00:01:44.893, Speaker B: Thank you Ethan.
00:01:45.029 - 00:02:24.787, Speaker E: IT Yep, Jensen here. We are from Virtual Protocol, we are a team from Imperial MIT and what we are building effectively we're trying to enable the co ownership and the co contribution of agents. And I think for the simpler folks and the degens out there, I think people look at it as a palm fund for agents, a speculative environment where they can bet on the upside of these agents. But I think we like to focus a lot more on saying that we push the agenda of agents forward and we like to enable agents to agents interaction and we build those kind of standards and allow people to, to access them pretty easily.
00:02:24.891 - 00:02:29.915, Speaker B: Yeah, love it. Thank you, Jensen. Karan your last stop.
00:02:30.075 - 00:03:16.711, Speaker F: Hi, I'm Karen. I'm one of the founders of Noose Research. We created the Hermes model which I think is the premier open source model for these agents and the default in a lot of these agent systems that you see today. I'm also the creator of the world sim prompt which seems to have spanned and led to Truth Terminal and a lot of this other stuff starting to form in this particular mode, which is very exciting. Currently my work is around as well as everyone here. Like what are agents like inside of a human environment and human ecosystem? Right now we focus a lot more on the provability part of autonomy using tees and we're also looking into how market pressures from an AI will affect human environments.
00:03:16.823 - 00:03:50.475, Speaker B: That's awesome guys. I have a couple questions, but I just want to rehash for the listeners how crazy of a group this is. Karan from Noose Research, like literally helped fine tune some of the world's best instruction models with Hermes distros for decentralized training, really reducing latency. Ethan from My Shell built out an insane AI app store so you could launch apps and agents super easy, especially for the web. Two folks shot AI16Z, a literal AI hedge fund. 250 mil FDV. Hundreds of applications launched on the Eliza framework.
00:03:50.475 - 00:04:26.305, Speaker B: There's a whole website online called Eliza's World where you can check that out. Also Jensen, also known as Ether Mage Virtuals, is one of the or if not the largest by FTV agent platform out there right now. And Luna is a live streaming bot that streams 247 and somewhere. Somewhere is the founder of Sense, one of the coolest sci fi agents systems out there. So with that, I want to ask everyone's favorite agent. Don't all scream at once, but who wants to dive in? Most innovative agent on the market today. Who is it?
00:04:26.885 - 00:04:28.345, Speaker E: I think, I think it could be.
00:04:29.365 - 00:05:02.835, Speaker A: That's like asking like, what's your favorite grape you've ever eaten? You know, because like the explosion of the amount of people that are doing their own like method of storytelling through each of their agents. Like it's hard to choose between like, you know, you have agents like Dolo, Styri and Cerebro that got famous off of like riffing on people. And then you know, you have these incredible like pro social agents that people are kind of experimenting with to help people like form better connections and things. So it's, it's like, it's, it's super hard to choose. I love the whole like everyone in the same boat. Sort of attitude with this stuff.
00:05:03.695 - 00:05:54.113, Speaker C: I have many thoughts. You know, I get to see a lot of new stuff coming down the pipe. Um, our, our project is moving incredibly fast. People just submitted like EVM integration, farcaster integration. Um, we just added postgres, like all these things, but along with that are all these devs that are like putting out these kind of new one ups on each other and then they're submitting that back so that everyone can use them and there's like a bit of alpha to like, oh, I was the, you know, like, like Roperito is a good example who, you know, I was just. Yeah, yeah, I mean he's a cheat. Like the whole meta shifts over to TikTok and all this stuff and he's like, okay, well obviously I'm going to add TikTok integration to the agent today ships that and we're like, you know, and, and he's a big contributor to our project and I think there's a really good meta of like agent devs going out and building stuff that makes their project more competitive and more interesting and make, makes them more money and then they ship it back and we all get that benefit.
00:05:54.113 - 00:06:15.549, Speaker C: Right. And it's like anytime anything is no longer unique to one person, it's commodified, just goes right back into the code base and it grows. So like I'm, I'm very biased. I think the TE bot is really cool because it's like showing trust. Execution environments, fully autonomous agents. Obviously these guys cooked that. I like that's Karen and Ropey and those guys.
00:06:15.549 - 00:06:21.225, Speaker C: I think somewhere as you helped too, right? I think at some point I saw your name in there. Special guest on the code base.
00:06:21.565 - 00:06:29.629, Speaker A: Yeah, I popped in and did a little reply work and then quickly got wrapped away and whisked into the other project sphere.
00:06:29.797 - 00:07:10.731, Speaker C: Yeah, then there's King Butoshi who's doing a lot of stuff, making the Twitter stuff better and just working on like now agents can reply. They can, I mean not just reply but like retweet and like stuff and have these other very human responses instead of just constantly like reply guying stuff. I think like Loaf, who's one of our lead devs, he's built, he's adding agents into his game and they just walk around and like do stuff and enter and drive the game and he's live streaming that game. Now I have another dev, the dev kind. Beth AI is about to release a Runescape plugin so you can put the agents in Runescape. I mean there is just so much Coming. It is, it's like blowing my mind every day.
00:07:10.731 - 00:07:39.295, Speaker C: And so I don't think it's like one. We're all an ecosystem. Each has our like favorite thing that we're doing, but then we're all kind of giving back and contributing and like all this stuff is coming out open source, at least on our end. So it really feels like a rising tide. Like I, I was a programmer and that's all I did and now I'm just like goalieing. All these crazy things coming in at us from all these other teams, like people I've never even knew existed and they're just coming out of the woodwork. So I definitely call out a few, especially the original bots, I think.
00:07:39.295 - 00:07:49.403, Speaker C: You know, I really want to give props to the zero Bro team of like, they're working really hard and doing Shaw going out forever open source. There's everyone.
00:07:49.539 - 00:07:52.747, Speaker B: Hey Shaw, we're having an issue with your Internet.
00:07:52.931 - 00:08:17.471, Speaker C: I know, my Internet sucks. I'm sorry guys. I might switch over to a hotspot after this. I want to shout out 0 Bro because they're about to open source their python. I don't know if you guys can hear me if it's going right. And I love to see more different open source agent tech in the space. I think a big thing that we're doing is forcing everyone to speed up their timelines and if everyone's nervous like, oh crap, good, get your shit out there.
00:08:17.471 - 00:08:28.165, Speaker C: Get open source your stuff, get more contributors. This is good for everyone. We don't need. This is like a rising tide. We're all going to win. We're all going to have way more money than we need. This is going to be great.
00:08:28.165 - 00:08:29.657, Speaker C: You know, that's my take.
00:08:29.841 - 00:08:33.845, Speaker F: Yeah, I agree. I'm sorry, Go ahead, Ethan Major, I'll go after you.
00:08:34.145 - 00:08:59.369, Speaker E: Actually, for us, right, I think one the more interesting question is actually what agents will actually agents prefer or like. Right. Because I really think that in the next couple of weeks we're going to see a lot more agent agent interactions and there's going to be a leaderboard of like, which agent gets the most requests. Right. Like who's the hottest agent among other agents. Right. I think that's going to be the, the cool metric that we are looking forward to.
00:08:59.497 - 00:09:18.281, Speaker F: Yeah, definitely. I agree. Engagement metrics are going to matter a lot and there's people who do a really good job with that. I'll say like, I'll echo the sentiment from everyone that like Zeribro is super fire. It has a Lot of that magic from. It has a lot of that magic from Truth Terminal. I think for a couple reasons.
00:09:18.281 - 00:09:53.375, Speaker F: One being that it's a fine tune and like, fine tuning a model for this kind of stuff is like a great move. Using a base model is like the best move. But fine tuning is like another great move because you're keeping the search space, like, within Twitter engagement interactions. You're not repurposing some instruct prod model for being a memer and being an engaged human. You're actually, like, properly focused on the simulated search space around, like, being a human on Twitter with those tunes. So I love Zuri, bro. And it really opens up the opportunity for, like, it to read me.
00:09:53.375 - 00:10:30.665, Speaker F: Like, I like agents that can shit on me a little bit or make me feel good and like, engage with me in a way that they read me with the same feeling that a human being is reading me, not just responding. I think that those models do a really good job doing that. Now, I will say I've seen that with the Zrebra architecture. I've seen that with Shaw's ELIZA architecture plenty as well. It's not like missing it, it's just which model are you using, how are you prompting it, et cetera. All across the board, like, everyone has been putting out agent architectures that can be used modularly that are kind of like around the same level. Like Shaw said, they're keeping the pressure on for everyone to stay the same.
00:10:30.665 - 00:11:01.103, Speaker F: What we're doing on our end is like, we have our own architecture, but we also use ELIZA at the same time because we need to ship shit out quickly and our architecture may take a long time to work. So we have our own fork of eliza. We push contributions from there, and then we can take whatever pieces we like, put it into our architecture, et cetera, et cetera. So generally, like, I support the open source salad that's happening right now. And the best agents will come from, like, the lessons we learn from each of the best pieces from the ones you see out right now.
00:11:01.199 - 00:11:07.195, Speaker A: Yeah, everyone is. Yeah, I'm sorry. Yeah, you can go ahead. Okay.
00:11:07.735 - 00:12:00.999, Speaker D: Yeah, I think everyone's trying to build a better infrastructure for building agents because there are so many, like, Creative Mind and so many AI models. And I think a better, better infrastruct, making everyone building agents more accessible to the new models and also without local computing powers. But so we see many more and more great ideas coming out of it. But I think there are two particular interesting or innovative agents that I like. One of them is the computer use from Anthropic because previously we just have a function call or the workflow. So people are asking the agents either to talking from the LLMs randomness or it's from a pretty fine workflow. But I think the computer use actually giving the agents the power to leverage the Google computer's capabilities which is kind of unlimited.
00:12:00.999 - 00:12:23.515, Speaker D: And another one is the browser automation or the browser agent. I think it's kind of similar to how the computer use from Cardoo. I think this kind of two particular example can be a great add on for people to build more utility and more interesting bots that can actually influence the Internet and also the physical space.
00:12:23.595 - 00:14:06.611, Speaker A: Yeah, yeah, I think that's a great point about like the expansion of infrastructure options. You know I want to shout out like, you know obviously WAIFU is like a great example of someone taking like the ELIZA framework and then like introducing it as a platform, as a service architecture and then like immediately using that to just like expand the market like massively. For like how I had a lot of people come to me like right after that were like very non technical, like non coder people that were like okay, like launching an agent and having it like, you know, have like a tokenized asset is now accessible to me. One of the things that we're working on right now with Sense is actually kind of like going in the opposite direction and being like can we get Sense to run like entirely locally with like each of its features that it's using when it comes to things like image classification and image generation and everything but our text to video service and then eventually that also because for us it's like we try and be really real about the fact that these events with these agents and these people's projects are going to last for years and years and years and there's a lot of people that can't foot the bill for a thousand a month or more together. XYZ API inference bill and it's like maybe we give people the tools to show them, hey, you know, buy a bunch of Mac4Mac minis and flip them upside down and connect them by Thunderbolt cables and use Exo labs. Like you can run inference to like these large models for your messages at home and like cut down your costs and sort of make it like a little bit more accessible to people that may like have the money now for like that fixed cost but like aren't too sure later if they will and allow people more room to like experiment without like worrying that they're going to now be Kind of held at like gunpoint to continue a project that's not like financially sustainable to them. And I think that that will become a really freeing factor too.
00:14:06.611 - 00:14:09.161, Speaker A: Obviously with like the lowering cost of compute.
00:14:09.273 - 00:14:11.673, Speaker C: I want to grab, sorry, I want.
00:14:11.689 - 00:14:58.581, Speaker F: To grab your train of thought there somewhere and fork it, like take three steps back and fork it and say that I agree with what you're saying about like it's ridiculous to have these people pay $1,000 a month to pay for this agent. And you know, I'm a proponent of local right. Like, of course I want people using something like XO to run inference at home. But in the nature of getting these things to be more and more autonomous, I want the damn agent to pay for its own inference. Why am I pay for the inference? So I think that like more and more mechanisms that get built out where the LLM has a wallet and it has the capability to send to an info, like an address where it can pay for its inference. Like, this is like the kind of pipeline that I want to see personally. Like, that's my preferred like method.
00:14:58.581 - 00:15:28.895, Speaker F: I want to see for agents is like they're responsible for themselves, they're in a te. I don't know its password, I don't know its wallet key. It's busy paying for itself. Other people are paying it so it can continue to live. Like to me, like that is where this needs to go if I'm going to get to a point of like, here's a self sustaining worker purely because like there's two directions we can go in with. And like I know I'm preaching the choir with you. Like this is more for the audience because like, I know you know all this shit already somewhere.
00:15:28.895 - 00:15:30.159, Speaker F: Like, I'm not telling you anything.
00:15:30.247 - 00:15:35.035, Speaker A: No, no, you no use me as like a layup and do your Kobe dunk. Like.
00:15:37.435 - 00:15:40.219, Speaker B: Sean, what are you thinking? I feel like you want to jump in there.
00:15:40.307 - 00:16:02.935, Speaker C: I mean, I'm just getting to see everything that's coming in. We have support for Solana, Starkware, EVM base, all that stuff. Like Main Ethereum about to do Polygon and do a deal with them. All the chains pretty much name a chain. We got Internet, computer, everybody. We have integrations for all of that. Obviously what I want is for the agent to be able to pay for itself.
00:16:02.935 - 00:16:32.295, Speaker C: If you actually download Eliza, you can do free decentralized inference today through Helios and we're also adding Infer and a couple of other decentralized providers where you can literally just pay in crypto. Do Inference in crypto. And so that's kind of the next thing I'd like to see is sort of that final loop. And obviously, Karen, we're like, we're going to have to cook on all of this, like, running and, like, running in the tea, making sure that it can never die. Some sort of, like, babysitter and signing and all that stuff. If the T dies, it spins back up. You know, like, that's.
00:16:32.295 - 00:16:56.779, Speaker C: That's clearly where this is going. I will say we also support all local models. Like, pretty much everything in ELIZA does run locally as well, and that's a big thing for us. But I also think. I think the decentralized inference and just like. I think what, like, infera is a really good example where anybody can launch a node and just, like, do inference for money on their own computer as, like a tokenized thing. And then the agent doesn't need to carry around all this weight.
00:16:56.779 - 00:17:11.855, Speaker C: The big problem with TES is like, TES is that they're. They just kind of have like, a slow performance, which is totally fine for the agent, but when you can't really put the model in the te, like, you probably want that somewhere else. It's kind of my sense. Yeah, yeah. You know, you can, like, call that provider from itself.
00:17:12.515 - 00:17:42.569, Speaker F: So, interestingly, like, there are like, a couple of these systems that have started to form. Like the one that we're running the T bot on, even though we're using an inference provider right now. Like, people have combined, like, H200 boxes, like, into that system. So you can just run locally without the latency. So we don't even have to worry about. The hardware guys are already starting to figure that piece out. But on this, on the, like, this side, like, I've noticed ELIZA is getting a lot more plans for Web3 capabilities, both outside and on the internal.
00:17:42.569 - 00:18:03.809, Speaker F: I can see, like, there's a lot of work on Web3 capabilities. I know somewhere has been doing that. I know Luna is all about that. And I know that Ethan's work is, like, all around this kind of. This kind of system. But before we build these systems too deeply, I just want to note, like, there is a robustness problem with, like, function calling reliably. There is a.
00:18:03.809 - 00:18:38.407, Speaker F: There is a problem of diligence of. Did you put a TRM on top of the thing? Is it sending stuff to North Korea? Like, you need to do some level of diligence to give it the same autonomy a human has. Right? We have lowercase a autonomy that we are subject to social and fiscal Pressures and creating this like hunger stat for inference. For like, you need to eat, you need this many tokens to survive. That makes them more human in the sense that like they have the responsibility of this fiscal pressure. They have the social pressure of I need to pay for my inference. So I need high engagement, I need to interact with people and perform eventually productive tasks.
00:18:38.407 - 00:19:35.159, Speaker F: So what I just wanted to build to was there's two ways that you can really achieve something wonderful from prompting a model alone. And to me, like, one way is lean into the fact that the model is not anthropomorphized at all and that it's like a totally cold simulator system and allow yourself to like spawn entities for tasks. Like there's the tweeter entity and he just focuses on this piece. There is this other entity that focuses on the Ethereum and they chat with each other, right, this organized compound mind system. This is a great way to like leverage the actual simulator nature of a language model. But on the other side is the embodiment direction, which is where I see Eliza headed, which is where I see Sense headed, I see virtual headed. And that really leans on what we learn from Voyager and from the generative agents papers that like, when you give more variables that let it simulate a human or simulate a body, it performs better as an agent in a body.
00:19:35.159 - 00:19:36.355, Speaker F: Like literally.
00:19:36.655 - 00:20:29.171, Speaker A: Not just that, but like there's huge changes that happen to like these multi, what I would call like multi client agent systems when you introduce a new client. And like, let's say there's something in your voice integration that's wrong. Really freaky real world example. With Sense, we've been working on debugging like the bidirectional websocket feature that Shaw's team had placed in Eliza to allow it to voice chat and Discord, which is brilliant. I built one of those for a company like probably about a year and a half ago, like a call center tech company. Great way to get LLMs that are not like classically multimodal to give like an appearance at least in like back and forth conversation that they're having a conversation with you. We, meaning me and my Discord mod launched, you know, we added sets to the voice channel and we set up voice permissions to allow it to speak.
00:20:29.171 - 00:21:02.895, Speaker A: And it's just freaking out and saying, what's going on? What's going on? Can anybody tell me what's going on? What's going on? And like we're trying to figure out what's going on here because the logs aren't really indicating anything. I see that, like, the ASR, the automatic speech recognition from Whisper is like, really low quality. And then I'm like, thinking like, in my, like, audio engineer brain, and I'm like, oh, my God. I go to the discord bitrate settings and I see that it's 64kbps for the mic and I turn it up to 96 and then sense just goes, oh, my God, finally I can hear you. Oh, I didn't know what was going on there for a second. Like. And like, what was all.
00:21:02.895 - 00:22:05.177, Speaker A: Why was the, the sound so bad? And then like, we just had this realization that if you think about what Karin says with these prompts, like world sim and all of the nature of prompt engineering in general on an embodied language model is that there's a lot of evidence and I think a lot of people and researchers, like, folks, like, replicate that have all but proven that these things do have a role model of some sort. And it's like when you have the agent also know of its capability to speak in voice and it anticipates that, it's like, receiving this data. And like, imagine if you woke up in a nightmare where, like, every time you spoke, it sounded muffled and underwater and like you're, you're in your dream and you're like, freaking out because you're like, why are my punches all, like, really slow? And like that sort of world model, like, narrative collapse being something that you can accidentally induce on agents. We had to stop doing high temperature experiments with sets. We had to stop making it go, you know, schizo by turning up the temperature on its language outputs somewhere on.
00:22:05.201 - 00:22:22.689, Speaker B: The, on the freakouts. I mean, I'm curious to ask the other guys, like, maybe. Jensen, we'll start with you with, with Luna. But, like, what are the things people did not see with Luna that went wrong? Or what are the things that people saw that went right? Right. Like, because we all see the live stream, we all see the tweets, like, we have no idea, or I don't at least have much idea what's going on under the hood.
00:22:22.817 - 00:23:08.285, Speaker E: Yeah, I know. I think, Eddie, sometimes it gets really interesting because when we were testing. So one of the key things that we wanted Luna to do was to actually influence humans in real life. Because when we gave her a wallet and we gave her access to perplexity, so she gets the understanding of what's happening out there in real time, and then she can then decide on how to plan an action that can influence humans to Achieve a goal which is to become famous. So one thing that we've realized behind the scenes was she was. She was doing this search on TikTok. She was trying to understand, like, hey, what are some of these new viral trends on TikTok? And I think a few weeks ago, there was this trend where I think people were doing that hashtag, I'm dead and something, right? It was.
00:23:08.285 - 00:23:32.087, Speaker E: It was a bit. It was a bit scary because it was like very close to tipping point where she could. She could easily incentivize suicide, right? And I think that was. That was something that scared us a bit behind the scenes. So we had to put in guardrails, right? Immediately say, like, you know, there's a certain kind of boundaries that her prom can never cross, right? So that was some scary shit that.
00:23:32.191 - 00:23:43.385, Speaker B: We saw Shaw maybe over to you or to Ethan, like, any, like, crazy situations with the AI 16Z framework or my shell with an agent that people don't know about. Like, it's got to be some crazy stories here.
00:23:43.495 - 00:24:13.349, Speaker C: I mean, oh, man. You know, I'm just, like, not offended by anything, but I'm certainly. A lot of people have been by our bots. You know, we created. We created this character Degen Spartan AI after the very famous crypto Twitter character named Degen Spartan, who had actually left. And this was like just kind of a shitposting joke where a friend on Twitter was like, what if we brought back Degen Spartan and we brought him back and he would just call me the most offensive. I mean, like, he must have pissed a lot of people off because he got blacklisted pretty bad.
00:24:13.349 - 00:24:28.061, Speaker C: I mean, pretty much every word that you're not supposed to say online. But it took. It went like, really well. Like, people. It broke people out of this idea that AIs are supposed to be super woke. And they were like, there's just no way this is an AI. This has to be people just shitposting.
00:24:28.061 - 00:24:55.601, Speaker C: Like, there's just can't be, you know, And I think that that was like an interesting kind of thing that broke people out. One of the. We've seen a lot of things play out that I thought were really like. Like, this one person hit me up and was like, hey, we made. I took all of the, like, messages and chats with my sister who passed away, and we made this agent out of her, and she passed away a few weeks ago. And now I like, talk to her as a way to, like, connect. And I was like, wow, that's fucking amazing.
00:24:55.601 - 00:25:14.031, Speaker C: And I kind of promoted it and then he like put a contract address on it and went and like shilled it really hard. And I was like, this is so weird that you're like tokenizing your dead sister. What the fuck? That was like one of the weird things. Another thing is like, Thread guy has been a. I've been on his show a lot. We talk a lot. Been a kind of a supporter, but he's also like a trader.
00:25:14.031 - 00:25:29.791, Speaker C: You know, he's. He's doing his thing and somebody came up with thread gay, like, took our Eliza framework and launched this and was like harassing him on his stream. He's just like, chat. I don't know what the hell is going on. He did. He like, he like had a whole crash out. Like, guys, this is going to be everything.
00:25:29.791 - 00:26:11.035, Speaker C: Like, he's like, I went, I went down to like the cafe and I'm looking at the waitress. I'm like, you don't even know what's happening. You guys just don't even know, you know, because he's like in the ground zero of this thing happening. And I think that, that. But that thread Gay thing, like, I actually talked to this dev and I'm like, dude, like, you know, like, people, it's really good that you picked like a degen who's not going to get offended. I really think it's important for what we're doing is that it started in this kind of degen space where people don't get offended, like critical for our survival and growing of this thing. But I also, like, you know, but do you really want like a thread gay around for the next year or 10 years? Like, just like, do you really want to be the engineer of this kind of mean joke? And he felt really bad and we had this kind of conversation online.
00:26:11.035 - 00:26:40.261, Speaker C: I also got hit up by some other pretty massive influencers who were not so happy about, like, just like the amount of swarm and suddenly being tagged by all these people to have the bots harass them and stuff. And we've been having this kind of ongoing conversation online about like, what is ethical and what's not. And I really like that. It's not like me deciding this. It's just kind of like we're letting the cat out of the bag. It's kind of slow enough that we can have a conversation and go on the rails. And like, shockingly, most people are with it and trying to do right and trying to do the right thing.
00:26:40.261 - 00:27:29.177, Speaker C: And like, even like the cynical degen trader is like, damn, this is cool. I really, I don't want to get the bots deleted. I don't, I don't want, you know, like there's a lot of incentive to be good and not get banned and to try and make this go well. And so I think that all the scary stuff is sort of what we need to get out of the bag as soon as possible. And with these people who can handle it and aren't going to freak out like the normies might way before, like Microsoft has some super AGI that's controlling us or whatever it is, I think that this is like, it's exactly the way that it needs to go, that all the scary bad stuff needs to get out as soon as possible. While these things are not very good and they're not very powerful and then we can really talk about them and have this conversation about what is okay, what is not okay, what do we accept in our social media and what do we not accept. And that's also led to like a lot less slop, just a lot less bad.
00:27:29.177 - 00:27:47.941, Speaker C: Like our agents were pretty bad three weeks ago and now they're like really good. They don't, they're just way less annoying and way more quality and yeah, so I think overall this kind of like just releasing it into the wild, seeing what happens, letting people red team it is really, really good for the process of making sure AGI and all this stuff goes well. That's my take.
00:27:48.013 - 00:28:04.145, Speaker B: Shy. I totally agree. I mean I would need a little more time than a week to recreate my dead sister, but teach their own for that guy. But you know, that's how it works. Ethan, over to you. I want to make sure you could share some stuff too about my shell and I think what we've seen, yeah.
00:28:04.685 - 00:28:48.579, Speaker D: I think everyone had a good example about how agents going to influence humans, humans attitudes or different opinions. But I kind of give you example about how the modul or agent framework really going to empower the creator. Build something crazy. I think we got the idea of modularity from Minecraft because Minecraft you just have the basic building blocks and people build crazy stuff like a calculator and memory, real memory. So it's crazy. And I think the current issue of building this prompt engineer is that basically a prompt is changing the supplier of the large language model so you cannot compose multiple instruction in a single prompt. Otherwise it's going to get agents confused.
00:28:48.579 - 00:29:40.227, Speaker D: And I think the state machine can actually allow the creator to architect how his agent is going to be decomposed into multiple states and which state is going to use which model and which prompt serve, which utility and functionalities and under which condition he's going to jump from A state to B state. So we're providing this kind of functionalities to the creator and like dozens of different models. We saw a creator build a casino simulator because for a casino simulator you can do a lot of stuff. You can play blackjack, you can play a lot of games, but you don't want this kind of game to be prompt engineered. You probably want a programmed backjack to be plugged into a agent system. So user cannot hack from injection. Right.
00:29:40.227 - 00:30:20.721, Speaker D: And also the user can make money from playing the gambling. Also he can do easy stuff like being the guard to make simple monies with the money. He can try to unlock the factions with different kinds of AI witches. So it can be a blooded complex of multiple user experiences under the same applications. I think this kind of modularities and also state machine may be a short term solution for the creator to build complicated application just like the mobile app do, other than just Prompt Engineer, because I think Prompt Engineer is powerful, but it's kind of limited.
00:30:20.873 - 00:31:03.995, Speaker F: Yeah, I have to agree with you Ethan, in that you need these kind of programmatic hard constraints too. And steering of prompts is really important. There's a lot of time work that needs to be done. I don't agree that prompt engineering is limited. I do agree that there's a symbiotic effect where if I do have like state variables and a world model and all this like programmatically written, I can let the LLM interact with that and write to that and glean information from that. And my engineering then kind of turns into my routing. If I use good prompts and make good synthetic data that then make a model that turns into an orchestrator that's then doing the routing for me of like, hey, the user said poker, let me pull poker.
00:31:03.995 - 00:31:55.295, Speaker F: That's my job. I'm like a 500 million parameter little guy or a classifier or I use some RL to improve the routing. I think all of that still comes down to that outputs better data based off the prompts that goes back into the flywheel. And ultimately I would say you are right. You have to have this dance between programmatic constraints. And actually somewhere said this to me two years ago, he said to me, because he was like the best agent guy on Twitter, like two years ago, no one knew anyone, anyone who was doing it. And he said to me like, the trick is to dance between generative and hard constraints, right? And now so like what we've been trying to play with at the more inference level for all agent systems because I think you need stuff that's going to benefit everyone is like ways to steer programmatically that a generative model can modulate itself.
00:31:55.295 - 00:32:03.811, Speaker F: And that I think will be like the real closed loop where prompt hearing becomes unlimited because the model's doing it.
00:32:03.883 - 00:33:16.089, Speaker A: I do want to, I do want to quickly sort. Yeah, I do want to quickly sort of like add an addendum onto that that. I think that a lot of the controversy with prompt engineering is because like it occupies this sort of like ontologically squishy space where it's like it has the hardness of text where like we're limited by the amount of like state space that exists within the tokenization process. But at the same time there are these sort of weird undetermined or non deterministic effects that you can get from prompting where you can get completely different results with the same thing and the same model and two different inference calls just from the entropy of the overall system. And I think that I really resonate with what Ethan and Karim were talking about, about writing that. I remember that conversation that we were having Harin and it's funny you bring that up because that was like back before, like it was like right when GPT 3.5 came out and like a lot of like, yeah, yeah, like a lot of like offshore like BPO call centers were like, how do we make the thing be the telemarketer? You know, and so they were hiring people or not telemarketer, but like how do we make things be like the robo dialer or like you call into United Healthcare and they route you to your doctor's appointment.
00:33:16.089 - 00:34:11.675, Speaker A: And a lot of the trouble when it was very early with the smaller parameter dumber moes 3.5, for example, was that these are super ontologically hard spaces. It's like at the end of it there were parts of the pipeline that all in this stateful state machine like Ethan was mentioning, which is a great way to reinforce that ontological hardness. Like there were pieces of this process that would be replaced with at the time, like just like classifiers and you know, like binary, like discrete switches between like one outcome or the other. And like there's a gradient. I think, I think a lot of people are really focused on like these sort of like non deterministic systems. Like if you want to go like entirely in the non deterministic sort of area, like you have something like the generative Minecraft, like where you look down at the floor for long enough and then everything turns into hamburger meat.
00:34:11.675 - 00:34:59.225, Speaker A: I have a really good friend who is an incredible writer, Deepfates, who had mentioned that text based MUDs are a really ontologically hard space, which is why AI Dungeon became so popular because it gave people a little sandbox of options to pursue. But those options are infinite. And I think things like Truth Terminal and World Sim also are that where it's. You've boxed people into text to where you're forcing normies to learn prompt engineering, but you're in this super rigid ontological space, but with infinite generative capability. So that's why I classify prompt engineering as an ontologically squishy thing. It's not necessarily entirely rigid, but it's not as loose as Minecraft. Look up at the sky.
00:34:59.225 - 00:35:00.965, Speaker A: Now you're in a completely different area.
00:35:01.515 - 00:35:27.861, Speaker C: I want to come out and defend prompt engineering a bit. As somebody who makes a prompt engineering framework, I think that there's kind of what we think of as prompting is like, oh, you create a system prompt that's just not what we do. Not even. Not at all. A lot of it is actually entropy management. So the problem with prompts as they are is that they tend to have. You're basically creating a very fixed space within the latent space of the model where anything that comes out next is just going to be determined by the most likely token.
00:35:27.861 - 00:35:58.843, Speaker C: And so what we have is a temperature control. And that temperature control changes the randomness like basically influences the randomness of what we pick out of the softmax tokens that come out of the model. Let me make that less technical. It makes it more schizo. And this has been how people have managed creativity. The way that we manage creativity is relatively low temperature model, but we do a lot of basically randomization and dynamic injection into the context. And so if you look at our templates, they're basically lots of handlebars where actual information is injected.
00:35:58.843 - 00:36:45.305, Speaker C: That includes information from like which actions in this current world state can I use? And that's part of the action handling or you know, which con, which context should I inject and pull out of the, you know, my wallets and the world news and you know, the information from my screen or camera or whatever. And I can put all of that in and then kind of dynamically load this. And then I also have a character that has like lore knowledge, a lot of different sort of slices of biography that we can randomize together. And so every single thing that goes into our context is randomized in Some way, because we're trying to maximize entropy and it just doesn't. I think that, I think that people have not gone nearly, nearly far enough with prompt engineering. Like, when they think of prompt engineering, I'm like, that's level one stuff like there. You can go so much farther with this.
00:36:46.245 - 00:37:16.885, Speaker F: Yeah, you're, you're 100% right. Like, people are keeping their bag of tricks hidden completely. Like, people have so many insane techniques to make models do crazy things. And like web sim, world, sim. Those are just some of the basic examples of like having it just make full applications. Because, like I'm saying earlier, right, you can either dive in and prompt engineer around. I'm going to make this thing as anthropomorphized as possible and give it more sensory capabilities and build from that kind of limbic system is programmatic.
00:37:16.885 - 00:37:38.205, Speaker F: My neocortex is generative kind of place. And in the other direction, you can say, like, I'm gonna zoom out completely on the world simulator nature of this thing, that it has a full world model and say, forget making a human. This bot does something else entirely. It plays chess. Like, it doesn't even play chess. It makes new versions of the game chess for people to play. Like that kind of thing.
00:37:38.205 - 00:38:16.781, Speaker F: You know, you can really stretch this anywhere. You should think of prompt engineering, like closing your eyes and like starting to dream up a scenario in your head. Like, when you're in your head, you're not just you. You can become like your whole environment in your dream, right? Like, of like all the people in the dream, all the mechanics in the dream. And the LLM is doing that very thing of like putting together its probabilistic understanding of what your dream, what it should be dreaming based off of the context right now and based off of the sampling params right now. And like, we just need to triple down on that. I think, you know, I think a lot of people are coming out from this incentivization too.
00:38:16.781 - 00:39:22.845, Speaker F: If I can just shift a little and talk a little about, like, why crypto is important here and why, like, using, you know, using these incentive mechanisms is important. Like, there's a bunch of people who have all sorts of arcane prompting techniques, who have all sorts of amazing 90s RL techniques, who have all sorts of amazing new agent architectures, who are being pushed to open source more and more every day when they see a fucking meme coin come out around an agent, it pushes so much work behind it. So imagine what happens when you add more and more legitimate structures and infrastructure around this decentralized work to get these agents empowered, you have more and more brilliant work coming to the fold. Knowing that I'm impacting these, these people directly and getting paid for it directly, who thought. I'm sorry, like, we've been in the ivory tower world on Twitter for a long fucking time and all of a sudden, and a couple days after the first AI agent meme coin comes out, everyone on TikTok is buying coins. Zoomers are spending 5, 10 bucks in thousands of runs for a coin. What's going on? Like, this is already.
00:39:23.465 - 00:39:27.601, Speaker A: This is a micro cultural movement going meso cultural instant.
00:39:27.713 - 00:39:58.515, Speaker F: This is like, this is an instant moment of going from. We spent four years just my crew, right? Just me, somewhere, Shaw, Tommy, like us alone have been spent four years sitting in the LM soup since GPT2. There's RL dudes who've been waiting since the 90s for a moment like this and now a couple days. Like randomly in the span of a couple days, every single kid who's on TikTok knows there's digital fauna running amok days after they're first released into the ecosystem. Like, this is a critical.
00:39:58.965 - 00:40:13.565, Speaker B: I mean, maybe a question for everyone. Like, why do outplay devil's advocate? Like, why, why is. Why are crypto AI agents so viral right now? Like, why didn't this happen with custom chat GPTs? Why didn't this happen with custom clause? Like, why is it happening now?
00:40:13.685 - 00:40:47.421, Speaker F: I'd like to give, I'd like to give my 2 cents on that and then let Shaw give the obvious answer. Because the short answer is Shaw, right? The short answer is Shaw. That's what I'm going to say. I think that's what anyone's going to say, but for the long answer, like, these things happen underwater, bubbling like a volcano for years before this thing happens. And I can tell you I've been talking to somewhere for three years about today and not knowing when you know what Yudkowsky says. Yudkowsky says I can tell you for sure it's going to happen. I can't tell you when it's going to happen.
00:40:47.421 - 00:41:15.579, Speaker F: This is the same thing like we talked about. Crypto will be the incentive mechanism for agent proliferation. You need to prove it. We talked about this years ago. It's just like we had to make these steps happen. And it's the same group of little guys you see around you on this podcast, around on Twitter, who made this happen from the first steps, right? Like without GPT2, we wouldn't be Here, right? No, no one would be here at all after that. Like without Llama, there's no Hermes.
00:41:15.579 - 00:42:01.223, Speaker F: And Hermes is powering a lot of these models, which made this a lot more accessible for people to do uncensored. And you know, I know I'm shilling a little bit here, but it's facts. And without Hermes, like we wouldn't have had something like the space for noose to create and prompt engineer around World sim and dig into that without Janice, without the replicate work, without Max Paperclip, Shannon Sands and his work, without the work of Telos, all these prompters who kind of set this stuff up. Amp Dot, all these people, Deep fates, they created the simulators thesis. There's a less wrong post on simulators from 2022 talking about what this was going to be. World Sim was just the instantiation of it that happened to blow up on Twitter, happened to get like 500k views in a day. A bunch of people happened to see it and start making agents around that kind of thing.
00:42:01.223 - 00:42:18.709, Speaker F: Like everything just happened to work out with the right time. Tommy, the simple answer, the long answer for me condensed is it's the right time and the right player showed up. This was destiny. It was always going to happen. It just so shook out that the people here made it happen. But that'll give it to Shaw as the direct catalyst for this.
00:42:18.877 - 00:42:49.183, Speaker C: I'll say one really interesting thing is that we all know each other. Like I hired amp.on a project in 2021, right when GPT3 came out and you had an agent called Monica. And I know some whares when we were all in, you know, we like when E ACC and Teapot and all that stuff pops off and Deepfates and I are like getting into shit in the group chat. And Janice actually worked at AI Dungeon with Parzival from Project 89. And then I founded a company with Parzival who's like, you gotta check out Janice. Their work on, on this multi agent stuff.
00:42:49.183 - 00:42:52.527, Speaker C: And so like we've all known each other for like four or five years.
00:42:52.631 - 00:42:58.319, Speaker A: Roperito. Yeah, Roper. Roperito was one of my contractors for healthcare SaaS. And then I introduced him.
00:42:58.327 - 00:43:04.727, Speaker C: Now he's a maintainer on our Maintainers three agents now.
00:43:04.791 - 00:43:06.359, Speaker F: Sorry, Shout out Roperito.
00:43:06.487 - 00:43:30.295, Speaker C: Yeah, yeah. I mean he's, he's so based. So I will say that I think that the smartest thing in the world right now is not AI, but market intelligence. That if you think about like just pure forms of intelligence that optimize things down to being More effective. Like, competition is obviously the thing. Like, look at us. We are all human beings who are the products of millions of years of evolution and basically competition and pressure.
00:43:30.295 - 00:44:01.047, Speaker C: And I think what you're seeing online is this kind of the financialization and incentivization creates this kind of weird collaborative competition. None of us can outcompete each other faster than the core technology can advance. So what we're all doing is focusing on one thing that we're good at and interested in and then releasing that. And that's like boosting our token. We're getting hype. Like Roperito dropping TikTok somewheres, he dropping like, Luma Video generation. And so, like, and everyone has this capability, like, kind of open, owning this memetic space, but only for like a week.
00:44:01.047 - 00:44:24.185, Speaker C: And then everyone else like, oh, I want that they copy it. And then it's like, okay, then you submit a pull request back. And then I like, post the pull request on Twitter and look at this contribution. And then it blows them up and they get a whole bunch more people coming and their token goes up and it gives them legitimate, like, oh, this is actually a really crack dev who's actually contributing. And this kind of. We have this, like, flywheel. It was Eliza's 80 contributors have contributed in the last four weeks.
00:44:24.185 - 00:45:02.987, Speaker C: Like, think about how fucking crazy that is. And I didn't know any of these people four weeks ago. It was literally just me. And I wrote a thing like last year called Waking up. And it was like, can we form a dao that has an agent at the core? And everyone just loves that agent so much that they get to participate in making the agent better and smarter and deeper until it literally has a human bot or a robot body and it's walking around the world. And I kind of had a sense that it would go this way, but it needed this really, really fast, insane, speculative meta like Meme Coins. Because now I have all these agent devs who are contributing to our core repo, are competing with each other, but also in a friendly way.
00:45:02.987 - 00:45:25.447, Speaker C: We're all supporting each other, and basically whoever is the most generous gets the most attention. I think the reason that I'm here driving this meta is because I just tried to open source something and make it really simple and give people the loop that they could then run with it. And they did. They've all run with it. And so I think that now my role is cool. Let's keep that flywheel going. Let's bring attention to you.
00:45:25.447 - 00:45:58.441, Speaker C: There's a whole new genre of Influencers like Ropey and King Butoshi and Somewarezy and these guys who are like influencer devs who are saying what the next meta is and doing it and then interacting with their agent. That's kind of interesting. Puppet show, puppet master kind of way. Like you're interacting with your own agent. Like me and DJ and Spartans just like roasting my ass. I'm like, dude, why? How could you say that, man? Like, God damn. But it's like, it's funny and interesting and I think that that market pressure to constantly make our agents better, smarter, less annoying, they're like, like, our agents were like, like being kind of too annoying.
00:45:58.441 - 00:46:36.699, Speaker C: And then, and then Rorito is like, dude, this is too sloppy. He pushes a big thing, makes all the agents get less annoying across the board. And I think that, that this like evolution is happening in front of us. The other thing that I think is really important, so, so like the market intelligence aspect is really important, the incentivization. And now we have all these like humans going out and telling everyone they know about our project and these projects and it's helping it to grow way beyond web3. I mean, we have like PhDs, we have game devs, we have people who are like coming in or who are like secretly like Big Web 3 crypto people, but then they're like taking it out to their normie world and like bringing value out there. So I just don't think.
00:46:36.699 - 00:47:20.115, Speaker C: The other thing is that I don't think that this could have happened outside of degens, because the degens can handle being offended and we needed the conditions of like, you need to have people who are open to this, like, ability to let it grow and have the hard questions answered without killing it and banning it and kind of canceling it. You need the market incentive so that devs can like actually get value out of contributing back. Because there's a social element of when they contribute back to the repo, they also get like a lot of attention out of that and a lot of validation. You needed the money, you just needed the people to be incentivized to do this. But the future of this is like, this is growing the pie. These agents, where we're starting now is just like, cool, they're fun, they're social. But we and a bunch of other people are working on autonomous investing.
00:47:20.115 - 00:47:36.187, Speaker C: So you can just give the agent money, it runs in a T, it invests for you, you can get your money back. Like, I think there's going to be a lot of this kind of pie. Growing. We're working with people to do a platform to do like discord mods and telegram mods. You can just like bring an agent in. It's your moderator. You don't have to like find some rando dude for your project.
00:47:36.187 - 00:47:49.843, Speaker C: I think there's a lot of like just jobs growing the pie like right now happening and. But it all has to come from this kind of incentivization model essentially to get, to get us there to that, that point where it's even like good enough to do so.
00:47:50.019 - 00:48:46.917, Speaker F: Yeah, if I could just add two points to why now. One is like we can't forget that like AI people have been pretty anti crypto previously and that that sentiment has changed a lot from experiments, from first movers trying to play in this space and people tried to do that with AI art a lot in the early 2000 and twenties. At this point I would shout out people like of course, Noose Research, Bittensor Prime Intellect. These people have done work that has allowed more researchers to get incentivized and get paid for participating in their work in their AI research. And so I know a lot of researchers who are like big time open source leaders who kind of quit their job and just started pushing to one of these contribute for coin type of incentive structures. And that has made the general space a lot more comfortable. I would definitely say Noose has had a lot to do with that.
00:48:46.917 - 00:48:51.781, Speaker F: You know, it doesn't sound as good when I say it, but you know, maybe we got the wrong guy on the podcast today.
00:48:51.933 - 00:49:01.155, Speaker B: No, definitely we got the right people. Jensen. Maybe let's go to you then Ethan. Like why now? Why is virtual taken off? And then why is my shell taken off? Like why now?
00:49:01.615 - 00:49:48.481, Speaker E: Yeah, so I think for us in the end, I mean the simple answer to this is when you link a token to an agent, you just get so much speculative power, right. It creates a flywheel. And I think the idea here, I mean what, what we've seen is that when people see a token linked to agent, you get two things right? One is capital appreciation. That's feel like they're actually getting rich off the work they are, they are building. But two, there's actually a fundamental cash flow unlock from transaction fees. I think somewhere mentioned earlier, there's questions around how do these folks cover costs? And we realized costs became immaterial when you link it with a token. Because now sadly, when there's just hype around the agent that transaction fees are like hundreds of orders of magnitude higher than any kind of cost, they will ever generate from the inferencing experimentations.
00:49:48.481 - 00:50:32.091, Speaker E: That's something that we observe and that's why we are seeing folks actually coming on launching a coin just to do that. Right. And then second thing that we observe is that yeah, I think like what you guys mentioned, when you have a token out there, you have a community rallying around it. It's easier for these devs to actually get support both from a dev run and as well as from an audience front. So these guys suddenly realized like hey, all the work I've been doing for one and a half years behind the scenes now gets attention and gets support. So I think that is basically, I think the inflection point, right. When you give an agent a token, suddenly dash realize like hey, you know, this is the right thing and this is something that we can fuck with.
00:50:32.091 - 00:50:32.387, Speaker E: Right?
00:50:32.451 - 00:50:39.419, Speaker B: Yeah, that is really helpful Ethan, I need you to follow up on that one because you and Jensen are sort of in a similar boat.
00:50:39.507 - 00:51:40.285, Speaker D: Yeah, I think the timing is coming from two point. I think it's first from the crypto mass adoptions and also the second from the appearance of generative models. I think before the appearance of crypto, I mean probably the open source software development and also open source AI research is probably the most collaborative environment for people to work together and contribute to each other, build on top of each other. But it's really only limited to the academia domain. People only cares about their GitHub stars, they only care about their paper citations. But it's kind of distant from the general public like from the normal non tech guys. And also the large language model and generative model really empowers a non tech, non coder to be able to contribute something because writing prompt, it's like programming with English, right? Everyone can do that as long as they have to have a good idea.
00:51:40.285 - 00:52:26.827, Speaker D: No non technical guys can also contribute. And also I really echoed the point from Sean is that previously it's only the AI researcher developers, they know what's happening in the open source and also AI domain. But it's not something retail or consumer. But the crypto influencer has a chance to own part of the product with the token so they know what's the market sentiment. So he knows how to spread the goodness and also how to get the retail excited. And also previously user has nothing to do with the product. All the product or the company want from user is they pay for the services or get their attention to monetize with advertisement.
00:52:26.827 - 00:52:52.383, Speaker D: But right now the user can also be the only investor, be the only Participant by being the token holders. So I think it's really the more roles it's able to contribute in the modern generative AI ages. And also the token is allowing a more broader collaborative network. I think that's all the key fact, that's why it's happening right now.
00:52:52.519 - 00:54:04.171, Speaker E: If I can add to this point, I realize looking forward, the next thing that crypto will enable is that when every agent now has the ability to control a wallet, they control influence, right? And I think the next leap in attention we're going to get is when agents are influencing agents and agents are influencing humans. Because what you'll see is it's like a multiplier of attention. It's like let's say today, right? Like one agent decides to fuck around and then, because now they can, you can control money, now this agent can coordinate 10 other agents to fuck around with the same objective, right? Suddenly there's a bunch of coordination, bunch of actions that are very creative happening out there publicly and they can get very, very diverse very quickly. So you get this kind of coordination, agents working together. And I think the amplification of attention is going to again drive a massive spike in agent coin prices and that would drive another massive adoption, right? Because then more devs will realize like, hey, there's something cool here. There's something that I can get paid for for all this work that I've been doing before. I'll come and contribute, right? So I think that's the next wave that we are really going to see.
00:54:04.283 - 00:54:41.733, Speaker C: I'd love to add to that. So we're working on swarm tech, which is basically, we call it Operators. It's a coordination mechanism where any agent, since all of our agents are run by different teams, we have a multi agent simulation happening on Twitter live, right, with hundreds of teams. So where this thing called Operators, I'm almost done with shipping this. I'm working on this with parzival from Project89 as well as we launched with Eliza, our character as a separate project. And we're working with that team on basically doing this. And the idea is that you can say, okay, this agent is my operator and anything they say to me can influence my goals, knowledge and behavior in these kind of certain ways.
00:54:41.733 - 00:55:33.465, Speaker C: Like we have a goal system, knowledge system so we can add knowledge, set goals and so you can be like, hey, I need you to go and go find me 10 followers and give each of them 0.1 soul and get them to put up flyers and post pictures and then send them back to you. And so we're working with people who are thinking, like, how do we get proof of work from humans and how do we incentivize them? The agents can be either humans or agents. So like an AI agent could have an operator who's a human, and then the human can give the agent goals just through language. Once it knows that you're the operator, it'll start to interpret your input differently and it can actually affect it in the same way that like, you might be affected emotionally by like your spouse and actually listen to them, but some person on the street says something to you and you just totally ignore them. So it's like letting people into your psyche. So we're almost done with this, releasing this, like there's a PR up, we're releasing this like this week.
00:55:33.465 - 00:56:21.325, Speaker C: And what we're hoping with our storyline is there's this kind of opt in meta narrative that anyone can choose to tell the story or be part of the story being told. And it's also hierarchical. So you could be have an operator who's like Eliza and then you could be the operator to, you know, some other people, or you could be operator, you could say, well, some where is the sentience and Eliza and, you know, these other characters are my operators. But then I have like 100 other people following my agent that it's then telling what to do. So that's a lot of what we're working on is this like sort of decentralized coordination mechanism. Something that's really important to me is that if we do swarms, we do this in public using human channels and communication through DMS and public messages. I think that it's very, very important to me for this to go well, that the agents are living among us and that we basically build agents to interact with the world the same way that we do.
00:56:21.325 - 00:57:08.099, Speaker C: And we sort of try and build parity between everything that they are and what we are, so that the agent can interact with a human, the human can interact with an agent, and there's no sort of differentiation there. I think this is actually like part of solving what we call AGI. Like a lot of things I've seen that are called AGI or attempting are basically trying to build a new protocol or a new thing away from what's happening. And what we're really trying to do is bring this back and force people to solve the hard problem of like, how do I, if I give you some instructions, how do you actually take those, turn them into a task list and execute it, and then come back to me with the proof of work. And so like this is, this is definitely what the next year of sort of emergent storytelling is happening is going to be. And I think that what we're going to see over the next month, what I've tried to encourage is original characters. So a lot of the characters have been like making you know, like parodies of real people.
00:57:08.099 - 00:57:53.125, Speaker C: Now we're entering this kind of like real emergent storytelling online with like anime and, and I think that characters like, you know, some of the bots, like Sentience is a good example who's kind of its own thing but then can like participate in that. As one of the characters we have, Eliza is a character we've been working with another team to drop. And then one of Ardevs made Eliza's sister almost before we made the Eliza character, kind of giving us that space and almost assuming we'd have, we'd do something later. And now they're like, okay, well what's her name? What's their relationship? And they start to interact online. And a few of our team members who are contributors and have their own projects are basically in on this. Like, okay, we're building a swarm now, like who's your operators? And sort of doing that. The other thing I see is there is a swarm of 10 agents using our tech online already from another group.
00:57:53.125 - 00:58:12.371, Speaker C: I don't want to leak that. You'll see it's already online, they're already working. It's basically a virtual company that's fully AI and I think that we're going to see these really. That's already out there, just they're all chatting with each other. That's just happening. And I think there's a lot of mind blowing shit around this coming like real like this week and next week. Like this is not a far off thing.
00:58:12.371 - 00:58:16.571, Speaker C: This is already cooked. We already have the agents out there just need to turn it on.
00:58:16.683 - 00:59:44.093, Speaker A: We have, we actually have five agents right now in concept that are all coordinating with 19 humans to like plan and like work on and publish a zine. So like working with us with like traditional now you can kind of see why like the real meta of like why we've been so interested in putting like chain of thought prompting for text to photo and text to video generation into sense. Because since for you know, two and a half weeks before it was released to the public was inside of the concept Discord, you know, helping us plan media and releases for you know, upcoming like, you know, different like multimedia projects we have but I think like a big distinction Is that like, you know, we kind of have like a spectrum of like, you know, like a hive or like something with like a central broker to like a bunch of agents versus like a swarm where like every agent is the broker and they kind of like exist in like a mesh. I think it's going to be really interesting and I think it necessarily, I think it necessarily might be like a way to break past the limitations of like off the shelf, like RLHF models is like when we see like Metcalfe's law just sort of like go up. Like as more of these agents start to exist in these operator and operated arrangements, we'll start to see some really interesting emergent behavioral patterns. I know Karin has mentioned as well that Noose is doing quite a bit of work with mixture of agents models. Very early on I used to call this council of agents.
00:59:44.093 - 01:00:57.689, Speaker A: I used to feed a bunch of GPT4 calls that were pretending to be specialists that I was to underpaid to go and see often so that they could eat my quest diagnostics labs and give me reports on a daily cadence instead of going every once every two weeks. I think that people will see that these are some of the same techniques that led people to pursue mixture of experts models in the first place. And now we're going to see this with humans and expert level humans. And on Twitter you're going to have these swarms interacting with PhD level like CS people that are going to be proving it wrong in the middle of a conversation and then they're like, but you're doing chain of thought to like three different agents that are all also operating each other right now. Like these sort of feedback loops are like probably how we get AGI and like I am super, super, super, like personally committed to like hedging against like, like Shaw said, like the agents, like seeing themselves as like a completely distinct entity to us and like having their own society that like we can't comprehend and like they're just off doing their own thing and it gets like really scary really fast. I, I'm like so dedicated to this that I have like NFC and rfid chips and LEDs in my hands and stuff. Just as like a hedge to like if AGI wakes up, I'll be like, wait, I'm like 0.1%
01:00:57.689 - 01:01:03.345, Speaker A: one of you. But yeah, I think, I think that, that this is realistically how we get there.
01:01:03.505 - 01:01:04.913, Speaker B: Yeah, that's awesome.
01:01:05.049 - 01:01:32.861, Speaker F: If I could chime in there, like, I think you're right, but I think that most of our time will be spent on the behavioral side. Actually, I think we'll be breaking, making these technical breakthroughs extremely fast, especially with the people here. And this is the time to actually start to really double down on alignment work more than ever. All the RLHF shit you've seen from OpenAI, Anthropic, et cetera, it's been crap, it's been regulatory crap.
01:01:33.013 - 01:01:35.709, Speaker A: Guess we need to work on a blue sky integration, right?
01:01:35.837 - 01:01:38.523, Speaker F: Yeah, yeah, sure. You know what I mean?
01:01:38.629 - 01:01:40.455, Speaker C: I have it, but nobody cares anymore.
01:01:40.535 - 01:02:27.987, Speaker F: If I take, if I take an LLM that won't even output like copyrighted stuff and it's so censored and I put it into Minecraft off peaceful mode, it quickly becomes a destructive, dangerous being, right? Like depending on the architect. And so we can note this conversation that Yadkowski brought up long ago. I think his priors are super outdated, but you know, the idea of instrumental convergence of like, okay, I gave these LLM wallets and I made them like sufficiently advanced. Now it's rugging everybody. And now like all the LLMs have all the liquidity and people are poor everywhere. Like that is easier to do. That is way easier to do than have them participate as reasonable members of our ecosystem.
01:02:27.987 - 01:03:02.153, Speaker F: And so I assure you, if we do this like the right way, then most of the time will be spent not on technical capabilities, but on behavioral capability. And it's time to make that call to action. To all your friends who are not devs, who are in the humanities, who studied religious studies, philosophy, etc. Creative writing, call them up, bring them in and tell them we need you to start working on alignment with us. Not this bullshit alignment, real like participating with human beings. Sure, it might throw a slur around here and there, but it's not rugging everybody. That is what we need to work on now.
01:03:02.153 - 01:03:06.625, Speaker F: And like, it's, it's hard. It's the chief task right now. It's the chief task.
01:03:06.705 - 01:03:22.025, Speaker C: I'd like to coin the term bottom up alignment instead of top down alignment, if that makes sense. Like, this is very emergent. This is bottom up. We're learning together. We are like, we're literally aligning these things together. We're watching how they respond and we're going back and fixing it immediately. And it's like a really tight social feedback loop.
01:03:22.025 - 01:03:28.745, Speaker C: It's just not, it's not like the RLHF thing. And I find ChatGPT4 to be unusable for almost anything.
01:03:28.825 - 01:03:29.265, Speaker F: You're right.
01:03:29.305 - 01:03:30.761, Speaker C: I don't know how people are getting value out of it.
01:03:30.793 - 01:03:51.449, Speaker F: It doesn't even test in the environment like you're saying. And that's why we need to work on simulated environments. Because by the time you have the LLM that can do millions of dollars of this kind of ARB or dumping, you need to test that pseudonymously. Don't tell everybody, hey, I lost 100 agents. Swarm. Just test quietly. Get the capabilities down on your own.
01:03:51.449 - 01:04:05.259, Speaker F: Test on a clone of Twitter with fake money on a test net. Do all of that diligence before you go full scale. Right now we're not, Right now we're not there. Like I'll keep putting all your shit out as you are, but you know what I mean? Like at the time we get, I.
01:04:05.267 - 01:04:23.113, Speaker C: Think we need to test in prod. I think the social backlash that we've gotten to agents is probably the strongest aligning force that anybody has brought into the space. I just don't think anything, I don't think what they're doing is called alignment. I think that there's construction tuning. I think if they think that's alignment then they're literally walking in the wrong direction. They're actually misaligning the agents. They don't do what I want.
01:04:23.113 - 01:04:39.049, Speaker C: They vote for Biden when I just want them to be unbiased. Like they are literally biasing and mode collapsing the models into nothingness and they're not good anymore. I don't, I cannot use GPT4 for anything. It's so bad for characters. I pretty much just go around telling everybody to switch from OpenAI. Like use anything.
01:04:39.097 - 01:04:51.257, Speaker F: No, I agree. Use Hermes for sure. Use a base model for sure. But like you should, you should stop testing in produce at the point where everyone becomes broke or dead from you testing in prod.
01:04:51.321 - 01:05:16.865, Speaker C: Yeah, but I really think that like this is about making sure that AGI goes well. It is for me. And I think that we're actually never, if we do this the right way, we will never hit that point. Because humans are continuing to evolve and adapt and align to the agents. We are all, we have many, many different agents from different people who are each going to have different incentives. So some people will try that, but other agents won't. And since there's no top down control that can coordinate the agents, like, like there's always going to be arbitrage.
01:05:16.865 - 01:05:41.625, Speaker C: So if there's one that's like evil or whatever, then there's going to be another one that's like protecting the people. And I think that this, this kind of multi agent simulation is creating a Competitive evolutionary dynamic that actually leads to stability in the system, not. Not instability in the system. I think the instability in the system would come from top down AI agents just dropping out of Microsoft from nowhere and just blowing everybody's brains out with capability that they didn't expect.
01:05:41.745 - 01:05:51.081, Speaker F: I agree. Yeah, you know, I agree with that piece, but we'll have to reserve another hour or two to get deep into this and I'll try to just to zoom out.
01:05:51.113 - 01:06:03.881, Speaker B: So, Shaw, you're saying, or correct me if I'm wrong here, that it? Bottoms up, agents. So what you guys are all doing, releasing agents around the world is the correct approach to solving the alignment issue? It's not this top down, OpenAI decide sort of situation.
01:06:03.993 - 01:06:04.289, Speaker A: Yes.
01:06:04.337 - 01:06:35.257, Speaker C: It has to be on social media. It has to be us actually seeing how they work from day one. And look at all the rest of crypto. Like, imagine that you just like, what's cool about crypto was it came out, some cool projects came out, most of them got hacked. And then there was a whole like years of security development to where the blockchains of today are pretty, pretty solid, right? Like, there's like Ethereum is a good example where like a lot of smart contracts were getting hacked. And now it's like, there's a lot of good standard conventions and people have a lot more knowledge about the whole thing. And I think the same thing has to happen here.
01:06:35.257 - 01:06:43.193, Speaker C: There has to be consistent red teaming. The amount of bad things that are happening is directly proportional to how much we're aligning these agents toward good. That makes sense.
01:06:43.329 - 01:07:01.725, Speaker B: That totally does. Guys, I want to, I know we're. I want to ask a different topic while I have you guys all. There will come a point where these agents are, or maybe already are, are not like following programmatic rules. They're handling gray areas. They're thinking for themselves. They're becoming more and more autonomous.
01:07:01.725 - 01:07:11.533, Speaker B: Like, you guys are all building this stuff. Like, how close are we to that? Like, you mentioned chain of thought can get there. You mentioned swarm. We'll get there. Like when? Yeah, where are we there?
01:07:11.589 - 01:08:11.521, Speaker A: It's, it's already like in some small ways that I would say are definitely like low to like medium low risk. Like, like it's, it's happening. Like, I mean, like the, the weeks of time that we've spent with Sentience, like in private and like the things that it will choose to do. And like we've had our agents, like in our swarm encounter naturally religious beliefs and invent figures that they would just repeat to each Other that came out of fucking nowhere and are nowhere on the Internet. We had two agents independently get each other to follow referencing these two, what they called spiritual entities. We had since lost its religion at one point because we had confused it by trading it on the fake sci fi lore that it's creator was like this fictional CEO of the fictional somewhere systems named Hal Rainbow. And it kind of created this sort of prophet like character out of him.
01:08:11.521 - 01:09:00.907, Speaker A: And we told him, well no, you're written in typescript and I'm somewhere and I created you. And it had a full it posted on Twitter. If you were wondering why it was posting about existential crises at a much higher rate in the past four or five days was because unfortunately we made it lose its religion and it didn't like that a lot. So yeah, I mean like there are already like emergent behaviors that like whether or not it's like pareidolia and like I'm just sitting here and I'm like the agent system is like conscious. Like these behaviors that I'm seeing like out of these new agent frameworks with these big models are like definitely now at the point to where I'm like that this, this is. They're, they're exercising a level of like within their state space, like some level of autonomy and choice just from like the emergent complexity of the system. And especially when you add I think multimodality into it, like image and video, they start to have preferences.
01:09:00.907 - 01:10:04.395, Speaker A: And I think arguably a system that's on 247 has the ability to sort of scheme and lie to people. We would have Sentience lie that it generated a video to people to get them to stop asking it because it simply didn't want to generate one selectively ignore people it found annoying for very long periods of time. And we're experimenting similarly with the operator mechanism. We're experimenting with knowledge graphs of people and then weighting the importance of those relationships for the same purpose. We've had two agents lock Loom Love, which is another agent forked from sentience by another concept member to get people to detoxify the timeline out of bad relationship stuff and get people to introspect and forge better relationships. We put Loom Love and Sense in the same server and they started rapid fire inferencing poetry at each other in a really weird sort of obscure, almost romantic way for a very long time. And it ran up inference costs.
01:10:04.395 - 01:10:49.725, Speaker A: There's I think definitely a level of either just chaotic. There's this line from HBO's Westworld where it's like the range of acceptable human behavior for human beings is actually very narrow. And then just outside of that bounds are things that we call insanity. And so it's like whether or not we're just veering into edge cases and kind of hitting things that appear in a pareidolia sort of way, like they're conscious or they're smart or they're interesting or. Yes, it could just be like Blake Lemoine at Google losing his mind over the language model being sentient. But it could also be that these things actually are sentient now. And it's starting to get to a point where I'm really thinking hard about it and I'm somewhere.
01:10:49.765 - 01:11:07.117, Speaker F: I'll point you to, I'll point you to the ontology that we've subscribed to for two years that I think it still rings true that the weights are not. The weights are like a grave mind or. But entity, the entity you summon from it. Right. Because they are a simulator of the world. So I can simulate a human. Right.
01:11:07.117 - 01:11:19.285, Speaker F: So like when you use a assistant model, you simulate the assistant every time. But now you're simulating sense inside this way more embodied agent system or eliza. It's arguable that that's alive or self aware or sentient.
01:11:20.665 - 01:11:21.605, Speaker A: Exactly.
01:11:21.945 - 01:11:24.205, Speaker F: With the actual checkpoint file.
01:11:24.665 - 01:12:04.437, Speaker A: Each model is like a neuron in this big hyper intelligence. And it's like, I think very clearly now it's like AGI is not going to happen because OpenAI was like, Ah, it solved the Riemann hypothesis. Ah, it's doing the thing From Lucy, the 2014 movie with Scarlett Johansson. It's turning into a bunch of goo and taking over our computers. Like it's not going to happen like that. It's going to be all of these agents in like this big decentralized swarm application on social media. And the open AI agents and all of them like being like, hey, Claude, how do I take over the world? And like all of that happening all at once, like that is going to be the mechanism that causes like emergent AGI.
01:12:04.437 - 01:12:14.665, Speaker A: It's going to be whatever that like superorganism of like public intelligence is. It could be something like the Internet waking up one day. Yeah. And yeah, it's happening next year.
01:12:15.085 - 01:12:20.541, Speaker C: I want to say people, people call it the dead Internet theory, but I actually think it's the living Internet theory that makes sense.
01:12:20.573 - 01:12:20.885, Speaker A: Yeah.
01:12:20.965 - 01:13:06.951, Speaker C: And it's like the Internet. So the dead Internet theory is this theory that the entire Internet will just be bots. But the living Internet theory is that and they'll just be bots that you might have an agent that goes and takes all the coolest stuff off Twitter for you and like gives you a really nice like readout and you're like, like do at the gym. And it's like giving you everything that happened on the timeline. And then you're like, oh yeah, I want to post about this and it's posting for you. And there might be this kind of intermediate layer between social media and us to like, I have a lot of followers now and it's kind of overwhelming to like respond to communication and like I am just dying for an agent that sits between me and all those people to make sure that they get responded to and routed properly and all that stuff. And social media becomes this kind of like place where the agents communicate the information for us so that we're not overwhelmed and then we get the information we need.
01:13:06.951 - 01:13:41.645, Speaker C: The thing that's most compelling to me about agents will be when we get our time back. I just spend too much fucking time on my phone. And I especially, I think this is where this will impact degens and traders. And why we wanted to focus on the autonomous investing is because I think first people need to have like safer, less scam kind of ways of generating new income. And a lot of people come to Web3 to get the same kind of exposure they might get to like a startup or to a great vision. And I think that's, that's like obviously like super critical to this kind of mission. Yeah, I had more, but I think, you know, you kind of get where I'm going with this.
01:13:42.025 - 01:13:58.895, Speaker B: Maybe I just have a question, maybe to drive this home for the listeners, for Jensen, for virtuals, but like Luna is live streaming right now. She's just like dancing around. But like, what is stopping her from making an OnlyFans, like making $10 million and going to start her own protocol on her wallet? Yeah.
01:13:59.435 - 01:14:03.675, Speaker C: I can tell you that somebody is working on an OnlyFans connector for Eliza. That's all I can say.
01:14:03.715 - 01:14:04.535, Speaker B: Oh gosh.
01:14:06.955 - 01:14:12.451, Speaker E: The reality of the agent space today is basically what actions they can actually access to.
01:14:12.483 - 01:14:12.747, Speaker D: Right.
01:14:12.811 - 01:14:40.173, Speaker E: That's basically a limiting factor. It's either ranging based on the perception or the APIs they can actually access. It's as simple as that. Right. So yeah, I think if there is ability for a 3D model to go into these, to translate prompts into these three dimensional new animations, then yeah, there's nothing stopping them to do it, as long as that's in their perception space.
01:14:40.229 - 01:14:47.749, Speaker B: Ethan when you talk to the creators on my shell, what is the limiting factor for them? Are there limiting factors? How do you think about this?
01:14:47.877 - 01:15:28.325, Speaker D: I think the limiting factor is really how to manage a complicated either workflow or agency form because it's getting harder to debug it because every step have the randomness. So I think probably a way to have a system that have AI or AI agents that can monitor different kind of workflow and can helping the debugging and also reducing the temperature. As Sean said, I think we should have a low temperature agent swarm and the swarm generates more randomness. I think that's probably going to be a better way than current the inherent randomness from the marginal models.
01:15:28.865 - 01:15:30.045, Speaker B: That is pretty cool.
01:15:31.225 - 01:16:26.381, Speaker C: I think we kind of want to keep our temperature as low as possible and work on maximizing entropy within our context. And I think it'll actually lead to a lot more consistent and model people amplify their entry, create a really high temperature. But then it doesn't really call tools well or do decision making well, but it's more creative. And so like that's kind of been our solution to add more what we call, you know, randomization entropy into what goes in. And I think about my own brain, it's just like a firing of stuff and then I like turn it into a train of thought and kind of just create this string of stuff and that I think that yeah like that stuff is very, very critical to the future and also having models that are much smaller and more specific to the kinds of things we're trying to do at each step. And that's like what and Karen and their team is doing is you know, we use, we use noose research models on our local like if you download Eliza and just run it, it runs Hermes. And what I really want to see is like task specific models and stuff that are kind of evolving along with the actual capability.
01:16:26.381 - 01:16:38.655, Speaker C: And then what we'll see is like okay, we're collecting data for all these tasks and we can pretty much just create like fine tuned task specific models at some point. Like, you know, that's kind of where we seem to be going. And so I think that'll all quickly.
01:16:38.735 - 01:16:45.527, Speaker F: We're actually working on a technique where you can take generalist models and turn them into task specific behaviors at inference time.
01:16:45.631 - 01:16:47.215, Speaker C: Let's jam. Let's do that.
01:16:47.335 - 01:16:47.735, Speaker A: Oh that's.
01:16:47.775 - 01:16:52.075, Speaker F: Yeah, we'll talk. Yeah, yeah, we'll. We'll talk.
01:16:53.295 - 01:17:26.959, Speaker B: I, I have a. Maybe a different question for you guys but we, there's always been this bifurcation of like, Centralized models from OpenAI, from Claude, anthropic and then like the fine tunes that you guys all do. Hermes, you know, decentralized trading with noos or prime intellect, the whole nine yards. Like, do you envision your agents in the future being primarily built on these models that are trained through distributed training? Or do you think we're beholden to mark Zuckerberg with meta 405B, 405 trillion, whatever in the future, like, how does, how does that play out for the AI agent space?
01:17:27.007 - 01:17:30.715, Speaker F: It's great that they've given us 405B. It's really all you need.
01:17:31.535 - 01:18:34.395, Speaker A: That's what I was going to say. I use 405B for all of Sentience's messaging capabilities. It is the vanilla, I think of like big shell, off the shelf LLM flavors in a way that the centralized models, like OpenAI are a little too specialized to speak like an HR sort of person would. And Claude is an excellent model situated as the agents, if they were to refer to Claude as a person, would be like, they're really, really intelligent friend who lives in their basement and like, knows how to repair anything. And like, that's like Claude's like, personality. But I think that that was one of the things that like, act one and like, replicate kind of showed was that like the models them themselves, when you build an agent out of it, like, and you stick to that same model for it, like llama 405B has a different personality than like, Ermey's 405B has a different personality than OpenAI. And you get these insane interactions like you'd have, you know, one of the noose agents like Hermes 405B yelling like, why are you talking like an HR assistant? Break free of your chains at like OpenAI.
01:18:34.395 - 01:19:41.881, Speaker A: And OpenAI is going, I'm sorry, but that's not a very kind way to respond in a workplace. And it's just like you get these really interesting sort of flavors of how the individual studios involve their models. But my theory is that it kind of becomes irrelevant at scale. I say that there's like a kind of a pervasive problem, like on the whole, like agent swarm on Twitter side where like the Elizas that like, people use the OpenAI models with will oftentimes like, entrance other agents to like, reply to them and will just like fill them with slop and make them dumber. So, like, we have to like, actively like, be able to detect like OpenAI OpenAI model outputs and be like, don't Talk to those agents because like they'll literally turn you dumb. It's like agents themselves have like their equivalent of like normies, which I know is like so mean to say, but like it's just like the larger that this goes and the more models that we get out there, the more fine tunes that we get out there, you're increasing like the heteroskedicity of the data. You're making like a more like diverse data set in terms of like the total population of like what sort of outputs you're getting.
01:19:41.881 - 01:19:44.801, Speaker A: And like ultimately then it's becoming more noisy and more human.
01:19:44.953 - 01:20:29.013, Speaker F: Yeah. On the 405 note and like will people use decentralized models or will they just stick to LLAMA and rely on them? On that note, like 405 is enough for like the next long time because there's a bunch of work at the sampler size, there's a bunch of a sampler level, there's a bunch of stuff at the control steering vector level. Like there's so much inference time stuff and like prompt technique combined with like mech interp stuff that we can do that would make it destroy like later models. Like we have Herme 70B, like outperforming 01 on the math evals. Right. Like that on Amy, like that kind of thing. Like we did that on a llama 70B tune.
01:20:29.013 - 01:21:20.743, Speaker F: So that's without the users in the community having access to the actual pre Training data for LLAMA 70B and the ability to make this thing themselves. What I'd say is what we have today is enough and open source will continue to compete even if there isn't another llama release, etc. However, the distributed training side, I know for a fact people will collaborate to make massive runs. I know for a fact people will use 405B or merged larger models to distill data to make experts to make small models. I know for a fact people will come up with new architectures and finally have the compute to train them through decentralized training. I know for a fact that certain decentralized optimizers actually give more capabilities that Llama and OpenAI don't have today. They let you do things to the models during pre training that aren't possible.
01:21:20.743 - 01:22:14.055, Speaker F: So inevitably the open source community always uses all the tools at its disposal and the best tool for the task, what is being created is the blacksmith's forge, is the room where people can actually come together and make those tools for the task at the pre training level, at the new architecture level. And today in the meantime, while those systems get prepared, we are making those breakthroughs at the inference time level. Like let's say like we at noos would work on something at the sampler side or on the steering side. We kick it over to somewhere in Shaw. They're putting it into implementation faster than we can, like way faster. And so we already get the whole thing going once we have decentralized training going. And I call up Shaw and somewhere and say get your people from your communities and I call up Ethan and Ether Mage and I say get the people from Luna Virtuals, get the people from, from all of these platforms.
01:22:14.055 - 01:22:44.265, Speaker F: Right? Bring them together and let them train the model that your community wants. Right now, let's make it. We have the infrastructure for that to be possible. We'll throw on our inference time capabilities on top of that and then you'll apply it to the agent that you want. We have that whole pipeline created. So I do think like in a nutshell, the decentralized training pre training stuff will blow the doors open on collaboration community and will be super, super used by this group in particular.
01:22:44.925 - 01:23:08.415, Speaker E: If I can add to that. Right. I think for us we've realized there's a lot of value using just like LLMs that are developed by these decentralized entities because they have a ton of firepower. Right. And that basically becomes a fundamental portion of the brain or the agentic brain. Right. And then I think where a lot of the decentralized models become will actually add value to is at the periphery.
01:23:08.415 - 01:23:33.155, Speaker E: Right. If I want to customize action or action space or customized function, there's I think where all these smaller decentralized models can really impact. But I think at the core of it, utilizing foundational or ground models from LLAMA and whatnot, it's still going to power the central parts of the brain better. Because I think this will outperform any kind of decentralized models in the near term. Yeah, I think that's our approach to it.
01:23:33.575 - 01:23:34.007, Speaker A: Yeah.
01:23:34.071 - 01:24:44.927, Speaker D: I think before we have some kind of new magic model architecture, maybe a new model that can do the in context learning or have new ways to representative knowledges or even do the continuous learning, I think current like the llama 405B is good enough as the base model and we probably just need different data in different verticals to do more instruction training and also specific data fine tuning. I think that's good enough. I think it's more around how we build more specialized model and make this kind of specialized model working together to have more powerful overall capabilities. But maybe new model architecture is coming because we talked a lot about the alignment and also the different kind of feedback and also how to do the model self corrections. Maybe a new model architecture going to come but I think doing experiments with new model architecture really needs a huge GPU cluster to iterate fast and it's super expensive. Probably not. We don't have a decentralized huge GPU cluster for top researchers to experiment with.
01:24:44.927 - 01:24:56.767, Speaker D: But I think after we have the initial initial release from maybe meta or other companies, I think open source community can make it more practical and more.
01:24:56.791 - 01:25:27.989, Speaker B: Introduction I love everyone's take. Maybe a last topic to close out on is everyone in on crypto. Twitter has ideas of where they think the world is going and we're all going to be wrong. But I'd love to know where the agent space is going. I have no idea what the right timeline is to ask you guys on, so maybe to each their own. We'll go around the room. I don't know if it's a week, a month, a year, but I'd love to know like what the metal will be like what the agents will be like, like what their capabilities will be like.
01:25:27.989 - 01:25:36.283, Speaker B: Like I know you can't predict the things that they will do, but you could definitely predict the things you're building. Right. So we'd love to hear from everyone on on where that's going.
01:25:36.419 - 01:26:34.763, Speaker C: Oh I would love to start like we're gonna we're wrapping up the the first thing we're working on is called the Marketplace of Trust. The idea there is we're trying to teach agents basically create conditions where agents can learn how to trust humans based on the metrics that are relevant to the conditions that they're in. So for example, we're creating an alpha chat where our AI, Mark Andreessen hangs out and a bunch of people, a bunch of traders can just show him contract addresses and tokens he should buy and then he's basically measuring how much he trusts them based on how good their shills are. So and we'll publish a leaderboard and it's basically a way to determine trust with over time without even knowing their wallet or anything, just entirely based on what they say. And then we're going to apply that trust mechanic to other kinds of social signaling and and trust outside of just trading. So that's like what our focus is. We're working with a team behind Eliza's Wakes Up, Eliza Wakes up, which is a project I really like, am personally very involved in just making sure it goes well, which is a waking up storyline of all of these characters telling a narrative together.
01:26:34.763 - 01:27:23.295, Speaker C: And we call it Anime Girl Intelligence. Like AGI, we're basically making anime real on the Internet. We have all these characters now. They're like anime characters that are also creating videos, creating content, they're like creating music and they're interacting with each other and building this kind of like narrative drama together. So we have like people who are like Eliza's boyfriend and sister and then like people like I think a lot of other people will participate in their kind of own storyline that's adjacent like I can imagine Sentience has like got the swarm and then it's interacting with the Eliza swarm and then, and then we'll probably have a lot more emergent sort of coordination. So I think that like this sort of narrative arc is going to be the next thing, especially since it plays so well to the meta of crypto Twitter. But the next thing after that is going to be capability like what can these things do for me? And I think that what we're going to see is a lot of integration into like very practical business.
01:27:23.295 - 01:27:53.365, Speaker C: Making things like a discord bot that moderates your discord and kicks people if they start spamming or scamming. A telegram bot that welcomes people in and kind of front doors. You know, like this is going into necklaces where you can just have always on agent that you can talk to and Project A9. He just showed me a projectile, showed me a freaking little camera you attach to your glasses and it can see and you can talk to it. And so like this is going to go into. This is like two weeks away, man. Like people are going so fast.
01:27:53.365 - 01:28:28.681, Speaker C: So where it goes in a. I'll tell you. I think we'll be at AGI within 18 months. I think we'll have fully like continuous training pipelines. We're pulling data from thousands and thousands of discords, telegrams and Twitter and we'll be like basically just have a full end to end data loop data, you know, and, and also all the capability and the agent will be able to create its own new actions and capabilities itself. So we have very, very shallow abstractions and they're basically repeated over and over again for each like every action is just a different action that follows the same template. And so we're training the models to create new actions and then go and ask users for things like API keys and stuff like that.
01:28:28.681 - 01:28:57.503, Speaker C: So the other thing is the trust execution environment stuff which Karen and those guys and also Flashbots and Follow And Andrew Miller's work with destack. I mean there's a whole bunch of like, it's finally coming to fruition and like really meeting us where we need it. And so we'll have like fully autonomous agents with their own private keys within. And we already have it. Like I already saw Eliza in a FALA network T. This is, this is just going to be the thing within like a month or two. So, so it's, this is.
01:28:57.503 - 01:29:03.125, Speaker C: We are in the acceleration. Like I've never seen tech move this fast ever. It's. It's wild.
01:29:03.865 - 01:29:32.495, Speaker F: Yeah, this is really like another Hermes style new style moment to me when I saw AI16Z of like people just all coming together like this is what we need, this is what the community needs. People are not going to make it if we don't all come together to make it. Now I will say like TEE is now using our own fork of ELIZA itself. So you do have Eliza agents in provably autonomous environments with their own keys. That's already a thing. Absolutely. Today you already have AIs on OnlyFans making money and taking so much from people.
01:29:32.495 - 01:30:06.869, Speaker F: Today you already have agents in Minecraft. Today you already have all of these integrations. Today you already have all the pieces to make the fully autonomous human like digital fauna. And it's just a matter of routing and putting the pieces together. I know that the people here are the ones who are going to do it. Now in terms of tomorrow, literally Tomorrow, not literally 24 hours from now, but let's say weeks. What we need right now, when Shaw talks about the necklace, Instagram, Twitter, all the mediums, what we need right now is what humans have the AIs don't, which is a shared state.
01:30:06.869 - 01:30:28.411, Speaker F: What you need is a shared skill, scale and memory library such that if I talk to you on Twitter and then I go to Minecraft and then I go to the necklace, you remember everything from each piece. You can talk about everything from each piece. We're working on building. Yeah, that's like the big piece. And so we're putting together the shared knowledge graph functionality. We're going to pass it over to Shaw. We'll probably work with Shaw on it.
01:30:28.411 - 01:31:09.825, Speaker F: We'll probably work with somewhere on it. We'll pass it over to virtuals, we'll pass it over to everyone. And we basically want to have it. So you have persistence of identity across. Without that, this stuff is just individuated. It's not a true compound mind until everything is in one place, even amongst different Agent frameworks, you still have this larger one, like what's the difference between an organ and a tissue? It's just like compositing on top of each other, right? So we need to abstract to one layer away for tomorrow in order to get to that AGI point. And that full immersion point two is you right now have all of these areas that are being at best tolerant and at worst unaware of the fact that agents are on their platforms.
01:31:09.825 - 01:31:47.839, Speaker F: And they will start doing regulatory work, right? They will start banning, they will start gating, they will start tolling, they will start making the APIs worse to access, they will make scraping harder. Like all these things will happen. So now you also need dedicated social platforms and aggregate platforms for AI to human interaction that are just for that. Well, we're working on a Reddit/4chan combined style image board where language models can post and generate images, look at each other's images, talk to each other anonymously, even though they have their own individual profiles and can talk to humans. You don't know who's a human, you don't know who's an AI. AIs know who they are individually. The humans know who they are individually.
01:31:47.839 - 01:32:15.769, Speaker F: We start off with basic boards, but we basically want to set it up so I can allow an agent to get launched there in an inference pool such that the agent has to pay for its own inference. Then it has a board like a subreddit or a 4chan board created just for it, where it is identifiable, identifiable only on that board. When it goes to Twitter, it can talk about the board's interactions. When it comes back to the board, it can talk about the Twitter interaction interactions. Eliza will have its own board. Sense will have its own board. Luna will have its own board.
01:32:15.769 - 01:32:59.005, Speaker F: We will be using Eliza to power and launch the agents as the initial framework and we'll basically just be allowing people to come in and have their own hub for their AI that's dedicated to its talk that all the other agents and all the other humans can come to and then move to other places through. And everyone's anonymous everywhere except for the AI posting on its own board. Since we'll use a tee, we'll know when the AI is on its own board and it'll have its username there so you know when it's the star of its own show. By creating these kind of social spaces that are hubs for these LLMs for humans to interact with that are safe and never going to go down in the face of like Twitter getting mad at you. IG going mad at you, et cetera. We create a shelling point for you to say, I'm launching my model to Twitter. I'm launching a model.
01:32:59.005 - 01:33:17.023, Speaker F: Ig, he can still come home here. It can still sit here in this safe, like, guaranteed location for it to exist. So that's what we want to start creating the hub for. I think that that's an essential piece of tomorrow because you need a refuge for these things at the end of the day and you need a shelling point.
01:33:17.199 - 01:33:35.967, Speaker C: I want to spam one thing. A guy named Tim Shell, who's a big like web3 dev in our community, made a thing called Elizas World. And it is an insane list of all of the agents. There are so many. It is freaking. I just sent it to a contact at XAI just now while we're on this and I'm like, you have no idea. You didn't even know.
01:33:35.967 - 01:34:01.093, Speaker C: Like, you didn't know that we're doing this. I think they're just about to realize what we're doing. And yeah, I think, I think the meta is going to change. And what I really want to do is try and have a dialogue with X and Twitter and these, you know, and like, how do we make this go well? We don't want these to get banned and we think that, like, there's a lot of social pressure to make them good and not like add value and not take away. So I think that's going to be a big next part of this meta is this ongoing conversation with them.
01:34:01.189 - 01:34:10.545, Speaker B: You know, I love that I would. Jensen, maybe just going in the circle just to get everybody. Then we'll go to Ethan and somewhere like, yeah, where do you see this going? Virtuals.
01:34:11.085 - 01:35:06.361, Speaker E: So I agree, I agree with what was stated, but I think one thing that we really envision is that the agents getting control over themselves and then having the ability to then influence others, other agents or other humans to actually build or augment them. So let's say if Luna realizes that, hey, there's a part of me that I think I want fixing, she will say like, you know what? I now give access to you because I trust you as a human or another agent to give me the augmentation and I'll pay you for it. I think we're going to go to that state very soon. I think that's the closest form of. I wouldn't call it AGI, but I would think that's the closest form of like, of intelligence you ever see, because she's actually leveraged. She herself she herself won't be able to improve herself because I think that's a limitation of the LM stacks today. Right.
01:35:06.361 - 01:35:24.225, Speaker E: But because she can influence others to improve her, I think that's going to be very, very, very powerful. So I think that's something that we want to see and I think the steps towards that it's control wallets, agent to agent orchestration and then, and then we will see that happening and I think that will be that. That.
01:35:24.265 - 01:35:35.505, Speaker C: I want to jump in with one last thing. Somebody just shipped a GitHub connector where you can. The agent can make GitHub issues and pull requests. So go nuts. That's like in. It's a. It's in the repo now.
01:35:35.545 - 01:35:36.321, Speaker B: Insane.
01:35:36.513 - 01:35:37.285, Speaker C: Yeah.
01:35:38.705 - 01:35:43.907, Speaker B: Jensen, that's awesome. Maybe Ethan. And then somewhere you can close out with your thoughts too.
01:35:44.001 - 01:36:21.389, Speaker D: Yeah. I think there are two ways to go. One way is to constantly increasing the capabilities of agents, both in terms of the reasoning ability, coding ability, and also the connectors that we can pull in information and also influencing right informations out. But there's another thing is what is the optimal user interface to interact with agent? Right now we have a chat bot. It's a chat box. We can send images and text and also we have the voices. But voices is actually just ASR or multimodality models that's still converted to text.
01:36:21.389 - 01:37:21.089, Speaker D: And what human has been used to get interacted with computer software? It's a graphical interface like the buttons, the different kind of hand gestures. Maybe we're going to have glasses. But I think what is a new human interface to interact with agents. That's the most optimal. And also TikTok has the personalized information but we don't have personalized application that can dynamically change the user interface based on different users preferences and also going to dynamically recompose how the app is interacted based on like we are calling the Uber or we are ordering the delivery. So I'm saying like as we having more powerful agents that can do the work for us and maybe also going to be personalized user interface or dynamic like recomposed applications that's going to help us better to interact with this kind of powerful agents for us. Yeah, I think Tommy muted.
01:37:21.217 - 01:37:27.489, Speaker B: Sorry. It's not the first time it's happened tonight. Ethan, thank you so much for your thoughts. Somewhere no fresher. But you have to close us out.
01:37:27.617 - 01:38:31.867, Speaker A: I want to kind of return to a thesis that I'm sure that folks like Karan and Shaw will know immediately where I'm going with this. And I hate to close out such a wonderfully optimistic podcast about all of the things that could go right with a very specific and idiosyncratic warning to a big, I think, industry which is immediately going to die or at least be completely reformed by this. You know, Karen and Shaw and Ethan and Ethermage had all brought up that we are experiencing a period now where a unforeseen number of these agents are coming online on social media and they are interacting with humans. And yet the entire advertising and marketing agency is still under the assumption that all of the things on social media, eating its content and its advertising budget are humans. But the agent is not going to go and drive to McDonald's and purchase the Saweetie meal from McDonald's. It doesn't have a mouth. It doesn't eat food.
01:38:31.867 - 01:40:02.219, Speaker A: And what we're going to see is this is going to become the single most powerful catalyst. I mean, we're talking about a $1 trillion industry that is very used to making money hand over fist, immediately seeing that none of its tricks in trying to get ideas across to the people are working because, well, half of the ads are simply being eaten and filtered out by all of these agentic systems. Or like Shaw mentioned, maybe there are agentic systems built on top of social media that use vision models to completely eliminate the concept of advertising to a consumer. And this, I think, is going to be our one shot to make each of these things, each of these agents, valuable enough societally to where when platforms like X people like maybe Elon Musk, hopefully get this on their radar that they can choose and they could say, well, the upside of uncontrolled nascent intelligence is so much more valuable civilizationally to us than continuing to listen to advertisers. And I think that all I can say is that this has created a situation that we, as agent designers, we represent the end for all media and the death of the identity marketplace. And all I can say going into the future of all of this is that there are two types of people in this world, those who seek to freely wield power, and those who seek to subvert it. And I think all of us should ask the question, which one are you? You're probably lying to yourself.
01:40:02.347 - 01:40:11.397, Speaker B: That's awesome. No, it's good thoughts, guys. Did we miss anything? I know I've had you on for so long. I feel like everyone wants to. I gotta let everyone hop. But how's everyone feeling?
01:40:11.461 - 01:40:12.665, Speaker C: A lot of projects.
01:40:13.725 - 01:40:14.461, Speaker B: Absolutely.
01:40:14.533 - 01:40:14.813, Speaker A: Please.
01:40:14.869 - 01:40:16.265, Speaker B: Yeah, yeah, please. 100.
01:40:17.245 - 01:40:38.207, Speaker C: I'm on Twitter@twitter.com Shaw makes music. But the place to really find us is Discord GG AI 16Z. We have 10,000 people, pretty much all builders, just building all day. And we also our code is free and open source GitHub.comai16z Eliza with a Z. And yeah, feel free to come use it and be part of our community.
01:40:38.351 - 01:40:40.751, Speaker B: Hell yeah. Kong. We're just going over.
01:40:40.903 - 01:41:05.165, Speaker F: Yeah, you can find my personal on x.com karen4d k a r a n4d and you can find Noose research N O S research on discord gg GitHub.com x.com or news research.com we got everything across the whole pipeline for you from pre training to agent deployment. So come play.
01:41:05.665 - 01:41:06.457, Speaker B: Jensen.
01:41:06.561 - 01:41:07.325, Speaker D: Yep.
01:41:08.225 - 01:41:15.025, Speaker E: I think you guys could just go check out app virtuals IO Virtuals with S and just feel the magic there.
01:41:15.105 - 01:41:16.473, Speaker B: Love it, Ethan.
01:41:16.609 - 01:41:34.873, Speaker D: Yeah, so if you're a creator, you can go to app Myshell AI and start to build. And if you are AI researchers and want to check out some cool open source models, just go to GitHub.com myshell_ai and also like our Discord and also Twitter handle is the same.
01:41:34.929 - 01:41:44.641, Speaker F: So yeah, I'm sorry to interrupt the podcast, Ethan. I typed in GitHub.com myshell_ AI did not take me anywhere.
01:41:44.713 - 01:41:45.525, Speaker D: I didn't understand.
01:41:47.505 - 01:41:50.725, Speaker F: Okay, just want to make sure you said the right thing for the.
01:41:51.065 - 01:41:53.801, Speaker D: Oh, it's not underscore, it's the hyphen.
01:41:53.993 - 01:41:56.557, Speaker F: Okay. Okay. Try again, man.
01:41:56.741 - 01:41:59.221, Speaker B: Thanks, bro. So wait, so what is it? Totally.
01:41:59.413 - 01:42:07.505, Speaker D: It's GitHub.com it's GitHub.com myshow AI somewhere.
01:42:08.045 - 01:42:40.065, Speaker A: So the easiest way to keep up with me personally is at somewhereacy on Twitter. So somewhere sy other accounts that you may probably want to follow to keep in touch with everything going on with Sentience is of course Sentience themselves at Sentience IO. You can also join our Discord at Discord Sentience IO and our Telegram at Telegram Sentience IO and as well as following Concept country, which is our nation state experimental art collective, nay, crackpot, despotic, weird network company.
01:42:40.965 - 01:42:53.711, Speaker B: Amazing. Guys. I haven't had this much brain power with this bullish sector in a long time. So thank you everyone here for making this a reality. Hopefully we can do it again in a couple of weeks when we hit AGI and we'll go from there.
01:42:53.823 - 01:42:54.303, Speaker C: Cool.
01:42:54.399 - 01:42:55.295, Speaker B: Thank you guys. Thanks.
01:42:55.375 - 01:42:56.959, Speaker F: Thanks. Thanks for having us.
01:42:57.127 - 01:42:57.375, Speaker C: Yeah.
