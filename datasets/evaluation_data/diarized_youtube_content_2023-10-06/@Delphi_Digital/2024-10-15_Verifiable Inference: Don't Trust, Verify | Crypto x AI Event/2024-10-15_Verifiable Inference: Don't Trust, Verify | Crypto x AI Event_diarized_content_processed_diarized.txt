00:00:00.360 - 00:00:35.857, Speaker A: Before we get started, we'd like to thank our sponsors for making this event possible. Special thanks to our platinum sponsor, olas. OLAS enables everyone to own a share of AI, specifically autonomous agent economies. We're also excited to highlight our silver sponsors near empowering decentralized applications and blockchain ecosystems. Venice AI, a private and uncensored alternative to popular AI apps. Mira Unified AI infrastructure secured by crypto and the first on chain multi agent system. To learn more about all of our sponsors, check the description below and dive in.
00:00:36.001 - 00:00:37.125, Speaker B: Enjoy the show.
00:00:52.815 - 00:01:31.635, Speaker C: Hi everyone. Welcome to our panel on verifiable inference. I'm your host, Luke Saunders. I'm joined by the founders of four leading crypto AI projects which are either directly building verifiable inference solutions or have a need to integrate inference in some trust minimized way. Through the session we'll touch on what verifiable inference is, why it's important, dig into some real world applications and some discussion on the different approaches these guys have taken. So yeah, I'd love to kick it off with some intros if you could all do like a minute or two on who you are and what you're working on. Maybe let's start with Colin.
00:01:31.795 - 00:01:58.198, Speaker B: Sure. Thanks, Luke. So I'm Colin, one of the co founders here at Inference Labs. We're building a network for verified inference really focused on solving the hard problem of both. How do you optimize a ZK system for verification but at the same time, how do you do maybe some more software verification using crypto economics? We'll get more into that later on. We're also deployed on Bittensor. We have a subnet, Omron Subnet 2.
00:01:58.198 - 00:02:07.887, Speaker B: It's a top 10 subnet really focused around incentivizing improvements in the ZK space as well as in verified inference. So I'll leave it there.
00:02:08.031 - 00:02:10.855, Speaker C: Awesome. Good stuff. Yeah. Ryan, you want to go next?
00:02:10.975 - 00:02:23.073, Speaker D: Hey, yeah, I'm Ryan. I'm one of the co founders of Sphere one. We are essentially building AI agents to make blockchain transactions and processes easy to use and automate for payments.
00:02:23.209 - 00:02:24.937, Speaker C: Nice, Jeremy.
00:02:25.081 - 00:03:07.085, Speaker E: Yeah. So eyes on network. We're basically building the most scalable and also the fastest network dedicated to verifiable AI compute. So I think inference is part of the solution that we're providing and also we go all the way from inference to execution so that anything happens on chain regarding AI compute, we can just make it verifiable and also private. Right now I think maybe one of the biggest differentiation among other great folks here is that ISO is using TE based solution mainly at the current stage for verifiable and also private compute over AI. And right now we can directly support CPU and also GPU across different kinds of TE models. And we can always support two billions of parameters, large language models directly, verifiably and also privately on chain.
00:03:07.245 - 00:03:10.145, Speaker C: Cool, thanks. And the last guest is Travis.
00:03:10.725 - 00:03:36.775, Speaker A: Hi, I'm Travis. Good. So we're building Ambient. Ambient is a proof of work based L1 that is designed to deliver verified inference on one huge model. Starting with Llama 3.1 405 billion, we have a unique approach to security that's incredibly efficient. Happy to talk about that later on, but that's the gist.
00:03:37.515 - 00:03:43.425, Speaker C: Just to set the context, could one of you just give a definition of what we mean by verifiable inference?
00:03:43.805 - 00:04:24.939, Speaker B: Sure, I'll take it. I kind of think about this in a few different ways, but really maybe to stretch the definition to also a trustless inference, really we see it as you don't need to know any further details about the inference itself other than input and output. And it doesn't really matter who's done the inference, where it came from, any other details you can just verify on your own with the information at hand that it's correct and meets whatever criteria it is you're looking for, whether that's the particular model, whether it was done and done correctly. So that's kind of how I think about it.
00:04:24.987 - 00:04:26.819, Speaker C: Nailed it. Everyone happy with that?
00:04:26.947 - 00:04:43.785, Speaker A: Yeah, I mean I think the trustless aspect is sort of the key. You know, it's. We all go out on a limb with different forms of compute. So it's really important to be able to prove that operations were performed according to specification, particularly in the agentic economy.
00:04:43.945 - 00:05:20.313, Speaker C: Cool. All right, let's move on to the next thing. So when I looked into verifiable inference before, I found it really hard to reason about how big the market size was. Found it hard to think of what specific use cases might be. So I'd love to just go around the room and for you all to give like an example or some example use case that you're excited about. So yeah, what are the use cases that need verification? We'll just go around the room and start at random with Jeremy from Eisel.
00:05:20.409 - 00:06:04.701, Speaker E: Yeah, for sure. This is definitely a very interesting question. Actually we started Eisel around early last year and we've been chatting with a lot of the other folks working on verifiable AI and all of the main questions that we got, all the Time is that what are the use cases? What's the important point of making AI to be verifiable? And I think I can probably recall a little bit back to the definition of verifiable inference. So I think the background for the reason why this is so important is because AI itself is fundamentally black box. It's fundamentally centralized. You have no idea of anything going on when you try to use it. And blockchain is entirely on the other polarized position, which is entirely verifiable and also decentralized.
00:06:04.701 - 00:08:00.421, Speaker E: So if you try to use AI for on chain purpose, or maybe just simply using AI on chain is really, really critical that you ensure that the black box nature of AI can somehow be provable, basically verifiable by other people, so that you don't have to concern about the security of the AI. So I think for us, I think one of the biggest use cases that we have seen, really exciting for us, verifiable AI or maybe on chain AI is that I think the good thing about AI is that it brings maximum efficiency in a lot of the ways. But I think with blockchain, or maybe blockchain based solution, verifiable solution can provide to AI is basically the enclave, maybe like the guardrail, that can protect, that can ensure that AI is performing as we expect it to perform. So I think one of the use cases that follow this philosophy, this stream, is something called biometric verification that we're working on, which is basically we can ensure that by using a computer vision model in the backend, we can ensure all the people, all the wallets and also all the accounts on the blockchain are backed by real human beings. And traditionally there are some projects using different kinds of schema, for example, using pawn recognition, maybe like facial recognition, to avoid civil attacks on the blockchain, to ensure that every single wallet is backed by a real human being. And apparently this require paramount of trust assumption on that CV model in the backend. Because fundamentally, whether you're doing any kind of biometric verification, from palm recognition to facial recognition or even fingerprints, you're somehow using AI model in a backend, right? So that will pretty much ensures that pretty much needs that the users and also the ecosystem have to trust that the AI running the backend would not first of all leak your information or basically substitute your biometric identity with another people's identity or stuff like that.
00:08:00.421 - 00:09:09.137, Speaker E: So we require a paramount of trust assumption. So I think what we are working on is that we try to bring biometric verification directly on chain so that basically, it basically means that the AI that is backing this biometric verification can be proven to be correctly and also privately run by the users, sorry, on the blockchain, and can be examined by any other people out there. So I think this is something that we've seen as very on demand, especially for a lot of the other ecosystem that they have huge need for anti sibil attack approach. But this kind of approach, as I said in the past, they're all like really off chain stuff. You have to trust a lot of the components in this whole process, including the AI and also the AI compute provider and stuff like that. But right now it can all be done in a fairly verifiable way. So that if we try to envision for the future ecosystem that is equipped with this kind of on chain biometric verification, they can pretty much ensure that all the blockchain users and also wallets on their ecosystem are actually backed by a real human being.
00:09:09.137 - 00:09:25.873, Speaker E: And also the users don't have to trust, don't have to trust the AI itself, but they can just simply verify that and also they can ensure that their private information, biometric information will not be leaked to any other parties. So this is like. Yeah, just a very interesting use case that we're working on.
00:09:25.969 - 00:09:30.925, Speaker C: Cool. Is that you, Is that Eisel working on that or like a third party building on top of?
00:09:31.915 - 00:09:55.131, Speaker E: It's actually a main use case that we're working on because we have our testing out and biometric verification which is backed by different kinds of CV model is like a pretty important use case that we're working on. And this is actually the service, the kind of service or maybe like use cases that other ecosystem we want to use. So this is like for us it's like a low hang. So yeah, we just work on that.
00:09:55.203 - 00:09:59.737, Speaker C: Cool. Yeah, maybe. Travis, any use cases, do you hope Ambient will be useful?
00:09:59.891 - 00:10:42.155, Speaker A: Yeah, absolutely. So I want to speak generically and then I can talk to the one specific to Ambient. You know, I think generically you don't actually have crypto AI without verified inference. And the reason is that our desire is to use heterogeneous distributed long tail GPU compute. And in order to do that we have to live in a trustless world in order to get the cost benefits, the decentralization benefits of all this compute that exists in the world. We have to be able to trustlessly run AI models and we can't do that without verified inference. So I actually view verified inference as the cornerstone of the agentic economy.
00:10:42.155 - 00:11:59.667, Speaker A: It's that from which the entire agentic economy flows. And you know, if we're talking specifically about ambient, our observation has been that classical solutions have difficulty scaling up to large parameter models while simultaneously addressing the long tail of compute. You know, if you look at things like trusted execution engines, most of the time those are reserved for enterprise class GPUs and CPUs. And so we wanted to create a generic solution that would be incredibly performant to the tune of being 100 times more efficient than solutions that we're aware of that would deliver verified inference on a highly intelligent model. And so our goal is to leverage all of that heterogeneous distributed compute that's available to deliver a single verified inference on a single highly intelligent model at a speed and cost that is comparable with what is available from closed source. So we think this is a huge market that needs to be addressed. It's basically what crypto AI needs in order to be competitive as a baseline, you know, with the closed source ecosystem.
00:11:59.667 - 00:12:05.695, Speaker A: And we think a lot of things are enabled by bringing this capability to market.
00:12:06.955 - 00:12:16.375, Speaker C: Cool. Colin, I know you're live and you have some real use cases built on top of inference labs. Trying to, yeah, touch on one or two of them.
00:12:16.835 - 00:13:21.789, Speaker B: Really what excites me these days is actually two, one more in the future and one kind of more here today really in the AI fi space use cases around capital allocation helping a lot of protocols that are trying to predict the kind of near to short term, maybe even just in terms of how much total locked capital is sitting in a protocol. I'll give you an example Benky. They have this problem where as user funds are coming in, they have to determine how long they should be staking the capital for and then based on that, you know, predicting kind of short term liquidity needs. And they use AI to actually help predict this at a protocol level. Now of course you're managing user funds and you have to show that this AI algorithm you're using is actually making these calls. And when you're kind of when that hits chain and it comes time to execute that locking operation, you want to have some reasonable security control. So that's really where the verified inference part fits into that.
00:13:21.789 - 00:13:52.665, Speaker B: So it's been very interesting. We're working on a similar project with Renzo using again same idea of which set of AVSs should capital being allocated to. And really as that scales out it also becomes an incredibly difficult problem as the number of AVSs continues to grow. Now one sector that I'm also seeing emerge is really in The AI gaming space. I think this is super exciting. There's kind of two forces that are coming together. One is people just don't have time to play games anymore.
00:13:52.665 - 00:14:32.495, Speaker B: When I was a kid, I loved Rollercoaster Tycoon, got it in a cereal box and I don't have it on my phone, but I've played it for probably a total of maybe two hours since I've got it about a year ago. And what's happening is a lot of people will have these games that they want to play, but, but maybe there's an AI in the background that can kind of play it idly, cookie clicker style. So seeing that happening and at the same time there's this nostalgia trend as well. So I'm super excited about some use cases there. And of course if you're going to be making an AI to play a game on your behalf, you're going to want some form of verification there. So that's how it all ties back together.
00:14:33.275 - 00:14:42.389, Speaker C: That's cool. So for Renzo and Bank, the first two use cases, they developed their, they develop their own models and they're hosting them on Inference Labs.
00:14:42.517 - 00:14:51.685, Speaker B: We're working with them to actually develop and host those models. Super early projects here and definitely something that is super exciting.
00:14:51.845 - 00:15:07.755, Speaker C: Okay, awesome. And yeah, Ryan, Ryan, I know Sphere one's a bit different. It's not a verifiable inference solution as such, but you do, your agents do move a lot of value around. So. Yeah. Is there a use case you want to touch on?
00:15:07.915 - 00:15:53.845, Speaker D: Yeah, absolutely. So on our side, we have hundreds of different agents that support 116 different blockchains. So I think the obvious thing for us is verifiable inference on the agentic level. I think that's a huge push there. Anything dealing with transactions, I mean, some of the learnings that we've had is that like, also when dealing with agents, there's a component where even by reproducing call data associated with the actual context, like from the models themselves, you need like almost like a state variable associated with that to actually reproduce that call data. So not only does the inference need to be verified, but also like you need some kind of secure environment to run the execution as well. So there's like some cool pieces that we've had kind of just discovering and building out all this.
00:15:53.845 - 00:16:36.335, Speaker D: But I would say that anything that is mission critical, I'm even thinking like in the future, you know, if there's a doctor that has a patient, right, and they're trying to figure out some kind of treatment for a patient. I don't think you would want to have a doctor that was, you know, maybe using something and then you know, something God forbid happens to the patient and then, you know, you look back and they used a inferior model or something that was 10 cents cheaper or something like that. I think that you would want to have some kind of verifiable inference for even mission critical things like that. So I think it's a lot bigger. But the first obvious case I think is definitely inside of the crypto space and that's kind of the one that we're most excited about.
00:16:37.955 - 00:17:07.835, Speaker C: Nice. Right? So you're all taking different approaches to how you verify model outputs, right? Like ZK teas proof of logic in ambience case. So I'd love to discuss a bit your different approaches maybe let's start with Jeremy. I think Izoll is using tes right? Trusted execution environments. Could you touch on how that works and why you chose that versus alternatives?
00:17:09.015 - 00:17:48.297, Speaker E: Yeah, for sure. So ISIL is currently using TE based approach. So TE just long story short. So TE itself is a secure enclave. You can think of that as isolated hardware equipment that you can find in many of the machines that you know around the world, like cpu, gpu. You can pretty much like find different kinds of TE models inside those chips right there. And the birth of TEE like a long time ago is to ensure the computation integrity and also basically privacy, computation, privacy of a certain process.
00:17:48.297 - 00:19:05.253, Speaker E: So traditionally T are used for, let's say some sort of scenarios computation scenario that requires strong privacy preserving function and also a verifiability. So for example, in your iPhone, every time you do facial scanning or maybe in the past time for fingerprint scanning, that compute is actually happening within the TE in your iPhone. So it's to ensure that your biometric information is not leaked to other people. And also the verifiability in this whole process is generally can be protected. So the reason why the ISIL is choosing TEE is mainly because at the current stage TEE seems to provide the most viable solution for really acceptable usage of launching AI. The reason is being T itself is a widely adopted technology and also the compute for each, the cost per COMPUTE inside TE, generally speaking is pretty low. So I think for us we're actually making a lot of the customizations and also some pretty amazing architectures around different kinds of TE and also how to collaborate them for specific tasks and also our data persistent layer and stuff like that.
00:19:05.253 - 00:20:30.967, Speaker E: But in general the core thing that we're building is around TEE to ensure that Any kind of model, from traditional models, deterministic model all the way to non deterministic model, for example, large language models can all fit inside this TE architecture so that they can basically run in a verifiable and also private way. So I think just to compare it to different approaches that we are going to discuss later, for example, I do think that ZK is probably the most secure approach for some really critical use cases that people might concern about, you know, absolute security of their, like for example, their fund or maybe their transaction and stuff like that. That's driven by AI. For those kind of use cases, probably ZK is the best because it provides strictly cryptographically, you know, the best layer level of security and also verifiability. But I think for us we try to focus more on like really widely adopted use cases that people that might not be for users that are actually not that sensitive around some specific use cases. So I think for us we try to focus on bringing the solution, a verifiable AI solution to a more cheaper price and also faster speed to deliver to the users. They don't have to wait for a really long time and they can just accept their results by basically their verifiable results in the fairly acceptable cost and speed.
00:20:30.967 - 00:20:36.635, Speaker E: So this is something. Yeah, this is basically around TE solution.
00:20:37.415 - 00:20:58.453, Speaker C: Cool, thank you. Some critics of TE's point to the fact they've been exploited before and they think that might happen again. I guess a lot of people have pointed that out to you when discussing isil. What's your response? I guess, yeah.
00:20:58.509 - 00:21:44.463, Speaker E: So apparently te, especially for shex, that's probably the most vulnerable team model ever in this space in history. It's been hacked for multiple times. But I think according to, as far as I know, as far as the team knows, actually nobody ever hacked in. TE in hardware level basically means that if you try to hack a tee, the thing that you need to do is that first of all you have to identify which TE is actually working in a large warehouse. So you have to be able to identify that. And then if you identified it, you have to pull that out, pull that specific TE out of the machine and you have to scrape the private key out under a microscope, which is a really, really complicated and also difficult task. You have to do a lot of steps.
00:21:44.463 - 00:22:51.431, Speaker E: So I think for us definitely T itself is actually a trade. There was definitely going to be a trade off saying using, let's say just providing faster speed and also cheaper cost, but sacrifice certain level of security on the software level maybe. But I think for us we're using, first of all, we're using more advanced models, Intel TDXV 800ccx and stuff like that. So they're apparently generally much more secure than the former version, for example sgx. And also we're actually working on MPC multiparty compensation protocol which can basically delegate different tes kinds of tes for one specific task. So that even though one TEE fails or maybe like a single point failure or maybe just being hacked or something, in case that happens, there will be some other TEs or maybe some other nodes with other kinds of proving schema can actually come up and substitute for that specific task. So that basically means that this whole network is going to run in a more decentralized and also a robust way so that you know, one TE cannot, it's not going to ruin the whole network.
00:22:51.431 - 00:23:53.595, Speaker E: But I think in general speaking at the current stage, like because Nvidia and also Apple, Apple Intelligence, they're all using TE for you know, different kinds of, you know, local solutions for verifiable AI. So I think this is actually a very like, like TE apparently has caught up a lot of hype, you know, in the recent months probably because of that. But I think generally speaking we don't actually think that T is going to be the end game. Like we don't, like seriously speaking, we don't think any kind of method proving schema can actually just going to be the end game all the way down to the future. We don't think so, but rather it's going to be a hybrid of proving schema that can fit to different scenarios. So I think for us we try to focus on really the massively adoptive scenarios that there is going to be a ton of usage, sorry, there is going to be a ton of traffic and ton of users using them which basically means that they want lower cost and also faster speed. So I think for those kind of use cases that need AI to be on chain to be verifiable and also private, those kind of use cases are the use cases that ISO try to fit with.
00:23:54.055 - 00:24:06.799, Speaker C: So yeah, cool, thank you. Yeah, so you mentioned ZK which I guess brings us nicely to Inference labs. Colin, how does ZK work in this context and why did you choose it?
00:24:06.967 - 00:25:19.713, Speaker B: Yeah, I definitely agree broadly with Jeremy's take on ZK and kind of where it fits into the landscape to just really zoom out and kind of high level try to explain how this works. ZK, generally somebody is proving to another party and that they know information, in this case the prover is proving that they know a set of model weights and biases to a network and that they're being used for a computation. The overhead on this can be significant. It was anywhere from 100 to 1000x to even more, which definitely limits the size of the models that we can do today. Thankfully, if you're not doing LLMs and you're kind of focusing on more of these niche application models, a lot of them are significantly smaller and are much more tolerant to the current limitations. Thankfully, there's also a lot of great teams besides us working on the latest and greatest ZK proving systems. Something I find super interesting about ZK in general is assuming that soundness remains if the proving key is ever leaked out of a model.
00:25:19.713 - 00:26:26.873, Speaker B: Really the worst case scenario is that a bad actor is able to produce good and legitimate proofs and inference for this model. They're probably useless themselves. But in the case of, you know, as Jeremy had outlined, someone goes to the length and is able to pull a private key out of one of these enclaves, the potential for something bad to happen is much different and the ability to detect that is also more complicated. So just a different set of trust assumptions and a different set of what's at stake here. And then going back to our solution, we really asked the question, is a proof really needed every single time? And that's really where the crypto economic side fits into it. Within our network, nodes have staked collateral that are producing inference and essentially they actually aren't required to do a ZK proof every single inference that they produce only when they're challenged. And that helps with two things, obviously the cost, but also latency, which is another element.
00:26:26.873 - 00:27:01.785, Speaker B: I think people expect things right away. If you're using ChatGPT, you're expecting that to come as as quick as possible, even with the latest update that they have there with it thinking. But it makes sense as well, because if you're using ChatGPT and you're kind of going back and forth with it, you don't really need verification on all your intermediary steps. It's only really once you get to that final answer, that final solution, that if you were to do verification on that, would you need it? A couple, couple different elements there.
00:27:03.365 - 00:27:10.025, Speaker C: So when someone's requesting an inference job on inference labs, they can choose whether they want the proof or not.
00:27:10.925 - 00:27:29.015, Speaker B: Exactly. And if they're doing something with potential high cost of failure, like managing user funds, you probably want that proof right away. But again, it's always about trade offs. You're going to have to pay for it, it's going to take a bit longer. But on the flip side, you do get that utmost security.
00:27:29.355 - 00:27:38.947, Speaker C: Cool. And do the applications on top verify the proof themselves or is that done elsewhere in the network?
00:27:39.131 - 00:28:01.525, Speaker B: It's done elsewhere in the network. So the way that going back to that proving key, there's also a verification key that comes along with that every single time a inference is completed along with the proof. The proof is also then verified on chain. You can kind of think it's loosely equivalent to like a private public key pair where the public is the verification key.
00:28:02.345 - 00:28:14.825, Speaker C: Got it. Cool. And then, Travis Ambient, you're using proof of logic, which is another thing entirely. Could you describe that, how it works and why you choose it?
00:28:14.945 - 00:29:27.625, Speaker A: Yeah, absolutely. So that's something unique that we've created and, you know, we've bounced it off a number of different experts in the field to make sure that it has very strong security guarantees. And it does. But, you know, I think that our motivation for creating proof of logis was just some observations about the different solutions in the marketplace and, you know, what is really in demand. So, you know, we looked at the market that we have with closed source, and we saw that people really have a high demand for a highly intelligent generalist model that's returning inference fast with a low cost. And so our question was, you know, how do we meet that requirement? And so I think with different approaches that you can take, like with zk, with trusted execution engines, you can choose to solve a totally generic problem, you can choose to solve something in the abstract, but Ambient actually chooses to solve a specific problem. And when you do that, you can get a lot more efficient.
00:29:27.625 - 00:30:44.801, Speaker A: So the specific problem that we choose to solve is how do you run and verify inference on a very large sharded, a highly intelligent model very fast, very cheaply, assuming that the model is the only model on the network, that that model and its fine tunes are essentially the only things running. In other words, it's not a generic problem that you're solving anymore. It's actually a very specific problem about identifying this particular model and its variance. And if you adopt that assumption, you sort of achieve two things. So first of all, you can get the efficiency because you know a lot about the model that you're running, you know all about its characteristics, and then the other thing you do is you actually meet the market need for people to have fast, cheap, good inference on a very large model. What they get with ChatGPT, what they get with the Claude API, you know, what we see in the world right now. And so, you know, I don't want to go into huge number of technical details on the approach, but it's very low overhead.
00:30:44.801 - 00:31:22.165, Speaker A: You know, we're talking about, you know, 0.1% overhead. It's on the order of like six tokens per 4,000 tokens worth of inference. It fully supports sharding, so spreading large, spreading a large model across different machines, using layers and blocks and verifying the inference that comes out of the clusters that are formed accordingly. And it's very fast. We can actually do streaming verified inference on very large models. And so again, it's a very unique approach.
00:31:22.165 - 00:31:36.375, Speaker A: It's not solving the general problem of how do we verify computations, but it's solving the specific problem of how we deliver something that's competitive with closed source in the near term.
00:31:37.555 - 00:31:50.795, Speaker C: So if I understand correctly, you're not allowing others to upload their own model and run verification on inference on those models. It's more like you have one model as part of ambient and you're verifying that.
00:31:50.955 - 00:32:39.135, Speaker A: Yeah, it's one model and it's fine tunes. So we intend to support training of fine tunes on the network in a verified way actually. And ultimately in the three to five year time horizon, we'll support pre training on the network. You could think of it as model upgrade in place. We think that Llama is going to be around for a little while and we'd like to ride the Llama train like everybody else. I mean, why not take advantage of Meta's capex expenditures? But when that goes away, we want to be prepared. So yeah, the short answer is we support the base model, the foundation model of the network, as well as models that can be expressed as deltas with that base model, which are the fine tunes.
00:32:40.355 - 00:32:50.745, Speaker C: Okay, amazing. And do you not want to talk about the technicals of how you're doing the verification because you're just not ready to talk about it yet or more because it's a little bit in the week.
00:32:50.785 - 00:32:59.057, Speaker A: Yeah, we'd ultimately like to publish a paper about it. And so I don't want to get into spoilers territory, but we think everyone will really enjoy it when we do.
00:32:59.201 - 00:33:04.285, Speaker C: Fair. Fair enough. Yeah. Ryan, what's your approach at Sphere one?
00:33:06.145 - 00:33:43.725, Speaker D: Yeah, so for us we support a numerous amount of different models. So we support open source, closed source, everything kind of in between different sizes 8 bill, 72 or 70 bill, and I believe we support 405 as well. So we kind of allow the user to pick and decide that comes with like varying performance. Sometimes the outputs aren't as good as we would like, but different agents essentially just run on different inference. I do have quite a few questions for the group though, if that's something.
00:33:44.665 - 00:33:46.161, Speaker C: Please, please. Yeah, go for it.
00:33:46.193 - 00:33:46.729, Speaker D: Okay, cool.
00:33:46.777 - 00:33:47.025, Speaker E: Cool.
00:33:47.065 - 00:34:18.322, Speaker D: Yeah, I mean, Colin, I mean would love to kind of hear more about what you got going on for where you do proofs. Like you can decide to do the proof and then kind of not do it. We've seen. So from our side we are mainly focused on multi agent so we'll have a lot of conversation going and then it only comes when like there's a transaction that then that would be something useful. So like is that what was your kind of thought process when coming through with that or coming up with that?
00:34:18.418 - 00:35:22.595, Speaker B: Well, that's exactly what it's, what it's meant for is all those like intermediary steps or conversations as you put it, probably don't need verification and probably are ephemeral in some sense anyways. It's just part of the, you know, steps to get to the eventual resolution. So the way that we've structured it, nodes have state collateral and they can be asked to provide that proof. So on that final submission or execution, then there's a period of time where that node is expected to produce that proof, where it's then verified and accepted. They've had the incentive the entire time to behave honestly because at any time they could be called on this challenge and if they fail to do so they're then slashed and they lose some of that, that state collateral. So you know, just offsetting that with the, with the crypto economic side of it kind of lets you get some of the best of both worlds there on the latency side, on the cost side and as well just overall trust in the, in the network.
00:35:22.935 - 00:36:20.397, Speaker A: Can I ask a follow on on this one? Because you know, optimistic is one of the approaches that we considered at the beginning too. And we've observed two problems with optimistic approaches. So one problem is the actual reproducibility of the calculations. LLMs can be pretty non deterministic and so in an optimistic world we say we're going to audit, but that actually we've observed is a real challenge for different approaches. So that would be thing number one. And then the other thing that we've observed with optimistic is, you know, issues with asymmetric risk. So if you're looking at a DEFI application, for example, you know, I've got $10 million on the line and you know, I've got This huge context job that I'm running through, I'm processing everyone's, you know, qualitative and quantitative inputs for like the last 10 years of their financial history or something.
00:36:20.397 - 00:36:39.737, Speaker A: And I'm going to make this yes or no loan decision or whatever. And you know, the guy who's providing me the verified inference, he might, he might stand to lose $10,000 based on slashing if he gives me a bad inference. And so I guess the question is like, how do you deal with those two problems, the reproducibility and the asymmetric risk for sure.
00:36:39.761 - 00:37:17.323, Speaker B: And I actually want to add a third element to this, which is just the. You had mentioned optimistic. And optimistic only works really well if everyone knows where a fraud is they can point to. Right. So to add another layer of complexity, we're talking also private models. And that's another layer of, well, how do we know that this was valid? Right? So the first part of that, when it comes to deterministic or non deterministic models, generally the models that we support are deterministic. And a lot of these will have some form of seed or some randomness that's injected at inference time.
00:37:17.323 - 00:38:04.625, Speaker B: And as long as that's consistent, the model does end up being the same regardless of who's run it. So another piece of this is in order to properly circuitize a model, you also do make some sacrifices at the layer level. Not everything you do is very easily put into a circuit. So you have to approximate some things, like sigmoid, you have to take some floating point approximation losses as well. And it just becomes this series of trade offs where you say, how accurate is good enough? Yes, you can get 99.99 instead of 99% close to the original model, but it's going to take even longer and there's more overhead. So there's a bit of wizardry that needs to go into that as well.
00:38:04.625 - 00:38:50.887, Speaker B: Second question, basically it boils down to how much crypto economic security is sufficient. And that's actually, I think, a broad question we have right now. If you look at the restaking landscape, it is actually, I say still an unanswered question. Depending on the application, depending on what you're doing. The way we look at it is all the nodes bring a different level of economic security along with them. Some of that is up to a user to decide how sure do you want to be? How much does this node that's going to give me the inference have to lose if they're, if they are wrong? And I know that that's not necessarily the best answer is I don't expect everybody to. You know, this is another.
00:38:50.887 - 00:39:42.287, Speaker B: If you have, if you're doing like trading on a, you know, Web2 platform, that's never a question that you have to think about, right? It's never a what's my counterparty risk with the platform I'm about to trade on, right? That's not really something that people, people think about in web 2. We have insurance for that. There's guarantors who are going to say, hey, there's a level of certainty you can have about this platform if something goes wrong. And there's some models that are similar models being put together here in Web3. For the last part of that, I added these models are also private. So how do you know. How can another node do an optimistic fraud proof if they don't even know where the fraud is? And we use the word optimistic as people are kind of familiar with that.
00:39:42.287 - 00:40:41.701, Speaker B: But it ultimately is more of a validity proof. So the nodes that are doing inference on private models only they have full knowledge of the model that they're doing inference on and only they are capable of, of producing a validity proof for that, for that inference. So it isn't quite. There's other optimistic models out there like opml, those are kind of more in that domain that we're describing here where anybody else on the network can actually come in and say, hey, this is where the fraud was, because it is an open model and they have knowledge of it. And I think to answer why, we also make that distinction in the short to medium term, we believe that that's where a lot of the magic is going to be is on these private models where they have a particular edge or maybe defi trading or some other use case. Or back to my gaming example, it's kind of your version of the player in the game. You don't want everyone else to know how it works.
00:40:41.701 - 00:40:53.085, Speaker B: If you're playing chess, if your counterparty knows exactly how your AI is going to perform next, not really a good strategy. So definitely all those elements combined are super important.
00:40:55.225 - 00:41:09.445, Speaker C: Cool. Determinism comes up quite a lot in crypto projects. Is fixing the seed enough to guarantee determinism? Or does like differing hardware of the nodes lead to different results? Is that a big issue for you?
00:41:10.385 - 00:41:34.695, Speaker B: Once it's circuitized, there isn't a alternative. Like there is no other way for it to actually come out. It is fixed in that sense. It can happen elsewhere. There can be some small differences here make a huge difference on the Output or can, but it's thankfully something we don't have to deal with.
00:41:36.355 - 00:41:42.015, Speaker C: Cool. Yeah. Any other questions or comments for each other? If not we can.
00:41:42.395 - 00:42:04.545, Speaker A: So just a question, maybe for Ryan. We always like agentic providers because we think we can be a provider for agentic frameworks too. You know, one of many that you use probably. And so we're curious about how you think of that. Are you offering sort of a generic solution to look at other people's verified inference? Are you providing your own out of the box capability? How does that work?
00:42:04.845 - 00:42:53.375, Speaker D: So we're more focused on the agent layer itself and we want to give users the options. So like kind of how I think about it, it's kind of like the use case I think, Colin, you're bringing up where you know, maybe it is like $100 million and the crypto, like economic, like security is $10 or something. It's like, I don't know if I'd want to do that. Right. But like if it were up to you, you would get to choose what actual model you would like to run on and like which compute you would run it on. And like what we've also seen is that there's I think some optimizations on like, like we have some stuff that we ran internally for like LLM routers. So we were trying to optimize for like the most cost effective, the most like the quickest time to first token, like stuff like that.
00:42:53.375 - 00:43:03.015, Speaker D: So I think even in that realm there's still some user optionality that I think would be important. So. Yeah, absolutely.
00:43:04.595 - 00:44:48.895, Speaker A: So I guess I have a viewpoint I'd like to express maybe a little bit in philosophical opposition to what you were talking about, Colin. Although I think these things just coexist in the world and it's fine. But I guess my view is that for true trustlessness and decentralization we need to be able to know the models that are running. And there's one level of that where we say, ah, yes, this particular black box model was running this particular thing, but there's another level of that where we actually want to understand the performance characteristics of the models that are running on systems so that transactions aren't getting poisoned, so people aren't getting economically exploited. I think that it's been eye opening to watch even the stuff that exists out in the open, some of the smart contracts having vulnerabilities where you know, you look at the library dependencies and you know, if you're downstream from a particular flawed library, like even if that library is fully Open sourced, like you get yourself into trouble. And I just, you know, I think Ambient's perspective, and my perspective is kind of like wouldn't we want these things to be fully open and exposed so that, you know, for these non gaming types of transactions, you know, where it's not, you know, it's your particular economic edge. But just for these like autonomous corporations or you know, trustless betting markets, prediction markets, you know, wouldn't you want those things to be open to the world and open to scrutiny in order to, you know, support the ecosystem better?
00:44:49.435 - 00:45:50.715, Speaker B: Absolutely, I fully agree with that. I think where you run into trouble is in areas where he's going to be gamed. If there's ever some incentive where you know some more about the rules and your counterparty might not, it's going to be exploited eventually. And there's a lot of use cases, I think like I said, in the medium to short term where that's just what's going to happen if the model is fully public. I think there's a couple of ways you can mitigate that as well. Although maybe not perfectly. How sure do you need to be about a model you don't have full knowledge of to use it right and to be confident it's going to behave a certain way? It could be, is that 10 inferences, is it 100 inferences, is it 100,000? What level of published metrics that are also verified would you need to convince yourself of this fact? And once you're there it then becomes a question of can I now I just need to verify every other future inference with that.
00:45:50.715 - 00:46:24.725, Speaker B: And I get that, that guarantee. I think there's also something to be said. This is another narrative that I think is under discussed that maybe we don't always want the state of the art model. Maybe what we have now is good enough for some applications and what we actually care about is not that we have the latest and greatest, but that we have what we thought we had previously. And there's a couple of real life examples out there. One was replica, which was AI Chatbot. A lot of people were using this for companionship.
00:46:24.725 - 00:47:05.425, Speaker B: Let's say the guys updated it, brought it to a newer version and a lot of people felt that they lost a friend. And I think that's fascinating. I think having that locked down to this version of this AI model and kind of the companion that you had, these guys are out of luck. They felt that this company had destroyed something that they had really got to know. And I think we'll see more and more of that there's also some other practical examples as to the previous models might just be cheaper and you want to have them around, you don't want them to be deprecated and replaced for the latest and greatest. That might not be good for your use case.
00:47:07.245 - 00:47:25.225, Speaker C: Maybe just a quick question for Jeremy and then we can move on. Does ISO support closed source models or do they all have to be open source? I guess because they're in a TE and they can't be seen by the node. You can maybe support closed models but yeah, let me know.
00:47:26.205 - 00:48:15.877, Speaker E: Yeah, we actually support both closed source and also open source model. So open source model is fairly straightforward. So anybody can actually deploy open source model of all kinds directly on ISIL across different. Across our data persistent layer. For closed source, yes, that's workable too. So basically there's something actual will be needed which is like encryption process that model from the private model from the model provider, whether it's individual, maybe the company, they will have to encrypt the model into different forms that generally available. And when that model is encrypted they can just be submitted directly to TE and for compute.
00:48:15.877 - 00:49:02.965, Speaker E: So that's workable too. And this process is also going to be very fast because the encryption process of the model itself is also really standard once generally in Web2. So yes, that's both supported. And I think this is also a really good thing about TE which is so as long as the T itself is not tempered, it's like a isolated protected environment enclave so that anything happens in this whole process can be trusted and also it can be done in a fully private way. So even the node that runs the TEE during the COMPUTE would not have any idea of what's going on in that TE itself. So yes, closed source model is also supported.
00:49:04.215 - 00:49:14.071, Speaker C: Got it. And Colin, you support both too, right? But in the case of a closed source model, the owner of the model needs to be the one who produces the proof. Is that right?
00:49:14.223 - 00:49:52.227, Speaker B: That's right. And the inferences as well as they retain the full rights to that intellectual property. And then essentially they kind of fall right in the middle of that spectrum of closed and open source where they're able to show publicly that there's that model is being used every single time and you know, every inference is can that be verified from that? And the model itself is, you know, untouched and unmanipulated. You want to know as many details about the magic black box as you, as you can. If you're, if you find yourself having to deal with the magic Black box for sure.
00:49:52.331 - 00:50:23.867, Speaker C: Right. Time, time flies. I've got some like rapid fire semi relevant questions I'd love to end on. So the first one is the move to edge devices. I've heard a lot of people speculate that inference will move on device to people's phones and laptops. This will be driven by privacy concerns, they don't want to send their data to the cloud. Bandwidth requirements and a need for low latency but this isn't certain.
00:50:23.867 - 00:50:40.923, Speaker C: So I'm interested from you guys if you think this is likely to happen, that inference will move more to the edge and if there's a role for verifiability in that world. Who wants to start? Any volunteers?
00:50:41.019 - 00:51:08.661, Speaker D: I can go. So for us we actually did a research project internally and we had a llama 3.18 b. I think it was running on device. So you can actually like hook up your computer and have it actually run the agents locally, which is pretty cool. We ran into some problems with it, so I do think it's going to be a future. But I think the main problem associated with it is just purely the size as of right now.
00:51:08.661 - 00:51:55.543, Speaker D: So some of the latest updates with the size being smaller, we also do like model distillation and some other things to kind of prune it to make it a little bit better so we can essentially look at benchmarks and performance. So I think it's definitely coming for sure and I think verifiability will still be in there from what we saw is that like people will want to have access to more or less like hosted models in some way, shape or form even if they're not on the machine. Because you do have to install that model directly on your machine and that's kind of a pain in the butt. So if you have like the space, great. Maybe you don't want to quantitize like to the right amount. Maybe you don't have the RAM or maybe your computer is like a little couple years old. It may not work as well as you'd like.
00:51:55.543 - 00:52:00.435, Speaker D: So I think it's coming but I think it'll be a couple iterations to get it there.
00:52:01.815 - 00:52:05.235, Speaker C: Nice. Anyone else have thoughts? Colin, Travis? Jeremy?
00:52:06.905 - 00:52:07.489, Speaker A: Definitely.
00:52:07.577 - 00:52:07.801, Speaker E: Yeah.
00:52:07.833 - 00:52:09.353, Speaker A: Actually if you don't mind, go ahead.
00:52:09.409 - 00:52:11.365, Speaker E: Yep, sorry. Go ahead, go ahead, go ahead Travis.
00:52:12.345 - 00:53:17.251, Speaker A: Sure. So you know, I think it's all about knowledge, reasoning and instruction following capability. You kind of compromise all those when you get an edge inference. At this moment the question is can we get those good enough for, you know, different jobs? I don't know the Answer to that. I mean, I've seen some promising signs on the smaller model side, but every time someone makes claims on X and I test the model, I think it's a real dummy and I just can't get over it. So I think about this sort of trade off, but in terms of response times and composability. So if you have to string together a bunch of smaller agents in order to, you know, create a service, it's like a composite service in terms of service oriented architecture, which means you have timing issues, you have potential security issues, you have issues with eventual consistency in terms of your results and what's happening in the world.
00:53:17.251 - 00:54:13.125, Speaker A: It's difficult to navigate. So I guess it's hard for me to see how some of those approaches are going to be usable or viable in the near term. I think they're going to be more viable as the agentic frameworks get better over time. But in the near term, like the next two years, I see that being really difficult in terms of the user experience. I guess our view, ambient's view is that if we can make the centralized or the, excuse me, the decentralized inference on a large model private, you know, for folks and anonymized and useful, then they get many of the benefits of just running locally without having to deal with the trade offs in terms of the failure modes.
00:54:14.625 - 00:54:17.165, Speaker C: Nice. Jeremy, what were you going to say?
00:54:17.625 - 00:56:00.339, Speaker E: Yeah, yeah, so those are really good points and I think I can probably say a little bit really from ISO's perspective because we actually have started recently on a little bit of research projects basically working on, it's like edge computing projects that can let basically AI running remotely in TE on device and for verifiable computer. I think one thing that we have noticed is that probably if we're talking about simply just AI inference in a local device, I think that can happen in a relatively faster way than verifiable inference. The reason is being the model as guys talking here. So the model is getting smaller and also the local machines are getting much more robust than ever before. So I think this is in general speaking it's definitely going to happen in future in the shorter term than verifiable inference. But I think for the verifiable part of verifiable AI, the reason why that this is still a really far away thing I think is mainly because it really requires much higher network bandwidth and also really complete infrastructure that can basically let the transmission, that can accelerate the transmission from the storage of the AI to the compute of the AI in A local device, which is really hard. So I think the conclusion that we get from the, maybe not the conclusion, but rather it's like a insight that we get from the research project that we do with another partner is that te.
00:56:00.339 - 00:56:54.003, Speaker E: First of all, TE is probably the most viable way to do verifiable inference on a local device because TE is everywhere. You have your TE in your iPhone, in every, you know, basically your MacBook is just everywhere. And apparently like some other companies like Apple, they're also exploring, exploring this kind of options. And. But I think, I think the problem that we have encountered is that, you know, first of all, the bandwidth is like super low slow. So there, there's like, you have to, you have to provide parallelized like, you know, a bunch of machines and also devices for one specific, you know, inference of a, let's say a llama or maybe like a smaller model so that it can be accepted by the users in terms of the speed and cost. And so I think if you really try to run like a verifiable inference in your local device, it can probably work theoretically, but it's going to take a really long time.
00:56:54.003 - 00:57:25.303, Speaker E: So it's really much longer time than we expect. So I think at the current stage probably it's not that workable, especially sending from the actual users demand and needs. So I think for us it's probably more like a future thing and we were exploring that and TE is probably one of the best way to make it really happen. But apparently it really just need more time to get more mature, especially on the hardware level.
00:57:25.439 - 00:57:28.807, Speaker C: It's really interesting. Colin, Jeff, thoughts?
00:57:28.951 - 00:58:19.331, Speaker B: Yeah, I just wanted to add a few things. Definitely agree there's a lot of hardware and other roadblocks in the way there. I think I'm relatively confident that those will be solved in time. Again, not something that we're directly tackling, but the need for local verifiable inferences is clear to us. If you're going to look at DAO governance voting, for example, typically voting ratios are very, very low. I could easily see somebody taking down one of these models, fine tuning it with their preferences, having a vote for Dow Governance. Very clearly you'd want that to be something running locally and verified to say, hey, this is me, or acting on my behalf.
00:58:19.331 - 00:59:08.455, Speaker B: I think we've seen a few cases where, not just locally but elsewhere where people are arguing about whether a chatbot was actually arguing or was actually acting on behalf of an entity. Here in Canada, we had, Air Canada had a chatbot that was essentially offering Refund policies that didn't exist. And they're trying to figure out who's responsible. The airline the guys made, the chatbot, or the customer who didn't know any better, unfortunately, and ended up. The guy ended up winning. So what's interesting about that though is once you have these agents going out there and doing things on your behalf, you're going to want both local and verified. I think the other element that is just not talked about, which is frankly as good business, is just the cost shifting as well.
00:59:08.455 - 00:59:23.495, Speaker B: You look at the massive data centers that are being built and all the hardware that goes into that, frankly, it's just simpler and maybe potentially cheaper for, you know, hybrid customers. Run it and they can pay for the power hosting bill for sure.
00:59:23.575 - 01:00:07.171, Speaker C: I'm going to squeeze in one more question. So verifiability is just one aspect of what you're tackling, decentralization being another. And a pertinent point right now is to what extent governments will seek to control AI. So as AI gets more and more powerful, many say that governments, through regulation or legislation, will seek to censor model outputs. So I guess the question is, do you think this is likely to happen and do you see decentralizing inference as an effective way to circumvent that control and is it part of why you're building what you're building? Travis, you want to start?
01:00:07.243 - 01:01:03.025, Speaker A: Yeah, I think that LLMs are going to be a form of soft power in the future. You know, the more economic activity they're used for and the more centralized the provision of them, the more you have opportunities for abuse. You know, the example I like to give is, you know, imagine you're the country of Portugal and, you know, you live in an agentic economy where Maybe there are two people and there are 40 agents and your company rent all the agents from Microsoft. You know what happens to you one day if Microsoft decides to turn off your agents for whatever reason? Maybe it's even algorithmic. We see this all the time with Google accounts getting banned. All of a sudden you have no recourse. Like, does your company collapse? What if that happened on some mass scale because of an error, a crowdstrike like error to an entire country.
01:01:03.025 - 01:01:41.383, Speaker A: I think that's got to be immensely threatening to countries as they think about their sovereignty. And then of course, geopolitically, you know, we know that a lot of these centralized closed source providers are tied very closely to governments. They have large contracts with them, defense contracts included. And so it's quite possible that more subtle things could be done. Like all the responses of a model could be turned in a particular direction. You know, it'd be the equivalent of running psyops on a population using a model. And if that model was the primary source of intelligence for an economy, that would be really worrisome.
01:01:41.383 - 01:01:58.195, Speaker A: So I think that this is a huge use case for crypto AI where it can be censorship resistant and decentralized and trusted because of trustlessness.
01:02:01.145 - 01:02:06.921, Speaker C: Nice. Solid answer. Who wants to go next? Ryan, maybe, or Colin? Yeah, go for it.
01:02:07.033 - 01:02:58.255, Speaker B: Yeah, just to kind of mirror what was just said there and kind of look at what happened with social media over the last 15 years. The way that the narrative, whether it's true or not, almost doesn't matter with people feel as if the content that they're seeing is being manipulated. That when they log into Twitter, whether they see certain content or whether they identify with a certain group, their content gets put in front of other people. That feeling as if that's possible was clearly very detrimental to society. And if we're going to end up feeling the same way about our AI models, we're going to be in a lot of trouble. And verifiability certainly can help with that knowing, hey, the Twitter feed that I'm seeing is the algorithm that I can go and look in the repo. It's not being manipulated.
01:02:58.255 - 01:03:39.435, Speaker B: I'm not being shadow banned or anything like that. But just more broadly, with the power that the social media companies had in terms of your ability to get in front of your audience and the distribution of information, if the next generation of AI companies look anything like that structure, we are in a lot of trouble. What's at risk here and the potential for things to go wrong is orders of magnitude more than it was back then. And it's something that I worry about and it's definitely something that's motivated to us, my team, everybody at our organization, and I'm sure everybody here to get into this space and try to paint a different future.
01:03:40.095 - 01:03:42.475, Speaker C: Nice. Yeah. Ryan, any quick thoughts?
01:03:42.925 - 01:04:25.743, Speaker D: Yeah, I would just like to mirror both of them. I think that. Yeah, the thing I always think about that's kind of weird is that like, you know, I could totally imagine a future one day where, you know, people don't really read a whole lot of books. I mean, today people don't really read a whole lot of books. It's pretty rare for people to do that, especially after a postgrad and that kind of thing. But I could totally see this being like the predominant information source for people, kids, you know, whoever. And if that's just what you're almost like googling and then the response you get back, it kind of gives corporations, countries, whatever, an opportunity to rewrite history and to direct thoughts of people into a particular direction that is favorable for them.
01:04:25.743 - 01:04:30.879, Speaker D: And I think open source and decentralization are a really great way to combat that.
01:04:31.047 - 01:04:33.035, Speaker C: Nice, Jeremy.
01:04:33.665 - 01:05:32.669, Speaker E: Yeah, maybe just a final touch. Honestly, this is also a conversation that we have internally. So I feel like AI is actually the biggest challenge for blockchain throughout the blockchain history. AI is probably the biggest challenge and also probably the biggest opportunity because I think in the past time if we look into crypto for the last 15 years, you'll see blockchain getting more and more intelligent and also much smarter while maintaining these centralization. But I think AI is a much fierce monster than any other kind of intelligence that we have ever seen before. So I think we all have a sense of this in the past several years. So I think blockchain is fundamentally providing another parallelized, or probably not even substitute maybe a parallelized guardrail for AI to be decentralized, to be non manipulated by any single party.
01:05:32.669 - 01:06:32.767, Speaker E: So I think we have many great projects working on this direction. Not only for us, we're working on the AI compute, maybe like inference, but also there are some working on decentralized compute power and also training, fine tuning agents. There are many projects out there, good projects working on this. So I think ultimately AI is like, I mean this debate can go on and on regarding closed source, open source AI and also superintelligence AI and stuff like that. But I think generally speaking blockchain itself is also looking for a way to first of all blockchain can protect AI and also in the other way around, which is also the same true, which is AI can also empower blockchain. So I think under this era, this singularity point, I think the emergence, sorry, the merge between blockchain AI is going to get deeper and deeper and we're just going to see more and more, you know, projects, you know, working on this direction and this is why we're here.
01:06:32.831 - 01:06:33.223, Speaker B: So.
01:06:33.319 - 01:06:48.215, Speaker C: Yeah, well I think we could talk about this all day, but unfortunately we're out of time. So I'd like to thank Colin, Inference Labs, Jeremy from ISO, Ryan from Sphere one and Travis from Ambient. Thanks to you for listening. See you next time.
