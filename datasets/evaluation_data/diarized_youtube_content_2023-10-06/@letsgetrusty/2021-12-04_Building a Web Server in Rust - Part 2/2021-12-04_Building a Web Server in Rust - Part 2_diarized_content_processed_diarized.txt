00:00:00.280 - 00:00:37.685, Speaker A: What's up, Rustations? Welcome back to let's Get Rusty, your number one resource for all things Rust. If you haven't already, hit that subscribe button for weekly Rust videos. And most importantly, get your free Rust cheat sheet by heading over to LetsGetRusty.com cheatsheet in the previous video, we started the last chapter of the Rustling book, which has us create an HTTP server from scratch. If you haven't seen that video already, make sure to check it out. Now, in the previous video, we created a single threaded HTTP server. And in this video, we're going to turn our single threaded server into a multithreaded server.
00:00:37.685 - 00:01:18.595, Speaker A: So with that, let's get Rusty. Here's the code for the single threaded web server we implemented in the last video. Now, the problem with a single threaded server is let's say we get two requests around the same time. Our server will process the first request, and once it's done with the first request, it'll move on to the second request. So what would happen if the first request took a long time to process? Well, the second request would just have to wait, and this is not a great user experience. To simulate this, we're going to update the handle connection function to process a new route which will simulate a slow request. The new route is going to be called sleep.
00:01:18.595 - 00:02:22.435, Speaker A: So let's create another variable underneath the get variable. Here we're looking for a request line that specifies a get request to the sleep route. Next, we're going to modify our if statement, which checks for request lines, to handle the sleep request. If we get the sleep route, then we want to return the same thing as the get route, so index HTML. But before we do that, we're going to let our thread sleep for five seconds. We'll also need to bring thread and duration into scope, and we don't need these extra parentheses. Now that we got our new sleep route and we're handling it, let's go ahead and start up our server.
00:02:22.435 - 00:03:06.961, Speaker A: Next, we're going to simulate how a fast request could get stuck waiting for a slow request to finish. To do that, we're going to open up two web browsers. In the first web browser, we're requesting the root path, and in the second web browser we're requesting the sleep path. First we're going to make a request to sleep, and right after we're going to make a request to the root path. Now, what should happen is the sleep request is going to take five seconds to process, and the root request is going to be waiting for the sleep request to process. So let's go ahead and try it out. As you can see, the sleep request took five seconds to process, and the root request was processed instantaneously after the sleep request finished.
00:03:06.961 - 00:03:35.669, Speaker A: Now, there are multiple ways to solve the problem of slow requests backing up the server. But the way we're going to solve it in this video is by using a thread pool. The way this will work is we'll have a fixed number of threads in a thread pool. Let's say 10 threads. Then when requests come into a server, a thread will pick up the request and start processing it. When the thread is finished processing the request, it will return back to the thread pool to take on new requests. This means our server will be able to handle multiple requests concurrently.
00:03:35.669 - 00:04:20.415, Speaker A: Specifically, in this case, if we had 10 threads in a pool, then our server would be able to handle 10 requests concurrently. The reason we have a fixed number of threads is because we don't want somebody to take down our server by issuing a ton of requests, which would spin up a ton of threads and exhaust the resources on our server. With that said, let's go ahead and shut down our web server and start implementing the thread pool. The way we'll implement our thread pool is by first thinking about its public API. So in the main function, we're going to write out how we would want to use our thread pool, and then we can implement the details. This will ensure that our public API makes sense and is ergonomic to callers. First, let's explore what our code would look like if we spawned a new thread for every incoming connection.
00:04:20.415 - 00:05:16.559, Speaker A: Now, as I've mentioned before, spawning a new thread for every single connection is not ideal because it could exhaust our server. So now let's modify this code to see what it would look like if we used a thread pool. First, we'll create a new thread pool underneath the listener variable with four threads. Now, we want the interface of our thread pool to be similar to the interface of spawning a thread. So instead of calling the spawn associated function on thread, we're going to call Pool. Execute Pool. Execute is going to take a closure and give it to a thread in the thread pool to execute.
00:05:16.559 - 00:05:52.815, Speaker A: This API makes it easy for callers to swap out their unlimited thread implementation for a thread pool implementation. Now that we have our desired API, the compiler is going to give us an error saying that thread pool is not a known type. Great. Now we could take the next step and implement threadpool. We're going to implement thread pool in a library crate, so that it's separate from our web server and can be used in other programs. To do that, we'll open up our file explorer, and in our source directory we're going to create a new file called lib.rs. then we're simply going to implement an empty struct.
00:05:52.815 - 00:06:42.815, Speaker A: Next, we want to make the library crate the primary crate. And in order to do that in our source directory, we're going to create a new folder called bin and then move main RS into bin. The last thing to do is import thread pool from our library crate. As you can see, we get a new error saying that no function or associated item name new is found for the struct thread pool. So next let's go ahead and implement the new associated function. We'll go back to lib RS and create an implementation block for thread pool. Then we'll implement the new associated function.
00:06:42.815 - 00:07:30.427, Speaker A: It's going to take a parameter called size, which in this case is going to be of type usize, and return a thread pool type. Notice that we're currently not doing anything with the size argument. We're just implementing enough functionality to resolve the given error. After resolving the given error, we might get a new error and we'll continue down the path of implementing functionality until we don't have any more errors. This is sometimes called compiler driven development and is similar to test driven development. If we go back to main, you can see that our error is resolved. And now we have another error saying that the execute method is not implemented for the struct thread pool.
00:07:30.427 - 00:08:17.355, Speaker A: Now we want our execute method to have a similar signature to the spawn function on thread so that we can swap them out. So let's have a look at the signature of spawn. Spawn takes an argument called f, which is going to be a generic type that has a few trait bounds. The first trait bound is fn once, which is a closure trait bound, specifying that this is a closure which takes ownership of the values in its environment. We also have the send trait bound so that we can transfer the closure from one thread to another. And then we have the static lifetime, which means the receiver can hold on to this type for as long as they need to, and the type will be valid until the receiver drops it. Now let's go ahead and implement the execute method with a similar interface.
00:08:17.355 - 00:09:22.085, Speaker A: The first argument to execute is going to be a reference to self, because execute is a method. And the second argument is going to be a generic that has the following trait bounds, fn once, send, and the static lifetime. Now if we Go back to main rs, you can see that we no longer get any errors. Our public API is now working, but thread pool doesn't actually do anything. So let's go back to lib RS and finish implementing thread pool. The first thing we'll do is update our new function to validate that size is greater than zero, because it doesn't make sense to have a pool of zero threads. To do that, we can simply use the assert macro.
00:09:22.085 - 00:10:20.087, Speaker A: Our function will now panic if size is less than 1. Alternatively, we can return a result type, but in this case we're just going to panic. And because our function panics, it's a good idea to add some documentation. Now that we know we have a valid number of threads, let's modify the thread pool struct to store a vector of threads. Going back to the function signature of spawn, you can see that when we spawn a thread, the return type is join handle. So let's have our thread pool struct store a vector of join handles. In this case, our closures are not going to return anything, so we're specifying a unit type.
00:10:20.087 - 00:10:58.861, Speaker A: Next, we'll need to update the new function. First we'll create a vector with a capacity set to size. Next we'll write a for loop which populates the threads vector. And for now, we're just going to have placeholder code. And lastly, we're going to store the vector of threads in the thread pool struct. Now let's talk about creating threads. The spawn function will take a closure which will execute immediately after a thread is created.
00:10:58.861 - 00:11:34.345, Speaker A: This is not what we want. What we want is to create threads that wait for some code to execute later on to get the behavior we want. Instead of storing threads directly, we're going to store another struct called worker. The worker struct is going to have two fields. The first one is id, so that when we're debugging later, it's easy to identify which worker is doing a particular job. And the second field is going to be called a thread, which is going to contain a thread that listens for jobs to be done. So first let's go ahead and create the worker structure.
00:11:34.345 - 00:13:04.553, Speaker A: Next we'll implement the new function on our worker struct. New is going to take an id, and for now we're going to store a thread that does nothing. Now let's update thread pool to store a vector of workers. And lastly we need to update our new function. Now we have a thread pool of workers, but our execute method is not doing Anything at the moment. So let's tackle that next. The execute method takes a closure as an argument.
00:13:04.553 - 00:13:45.855, Speaker A: What we want to do is send that closure to one of our workers for processing. If you recall from chapter 16, channels are a simple way to send information from one thread to another. This is a perfect use case. When execute is called, we can use a channel to send the closure to one of the workers. To implement this, we'll have the thread pool struct hold on to the sending end of the channel, and each worker struct hold on to the receiving end of the channel. First, let's create a new struct called job, which will hold the closures we want to send down our channel. Next, we're going to add a new field to thread pool called sender.
00:13:45.855 - 00:14:49.005, Speaker A: Here the type is going to be sender from the MPSC module, and we're sending jobs across threads. Next, let's update the new function on thread pool. First we're going to create a new channel. Then we're going to store the sender part inside our thread pool struct. Next, we'll pass the receiver portion to each of our workers. We'll need to update the new function on worker to accept a receiver. Finally, let's use the receiver in our spawn thread.
00:14:49.005 - 00:15:21.901, Speaker A: Now that I've saved the file, we can see that there's an error up above. The error states that we're using a value that has already been moved. This error occurs because in the first iteration of this for loop, we pass in receiver to the new function of worker by value, which moves receiver into the function. So in the second iteration, we can no longer pass in receiver. What we want is for our workers to have shared ownership of the receiver. In addition, listening for jobs requires mutating the receiver. So we want shared ownership and mutability.
00:15:21.901 - 00:16:20.563, Speaker A: We can get this behavior by using smart pointers, but because we're working with threads, we want thread safe smart pointers. For thread safe multiple ownership, we can use the ARC smart pointer. And for thread safe mutability, we can use the mutex smart pointer. So let's create a new variable and put our receiver inside a mutex that's inside an ArcSmart pointer. Next, we'll need to update the new function on the worker structure. And lastly, we'll need to update our for loop up above. Great.
00:16:20.563 - 00:16:52.025, Speaker A: Our code is now compiling. Next, let's focus on implementing the execute method. The first thing we'll do is change our job struct to a typealias. Here we're saying Job is a typealias for a trait object that holds the type of closure execute expects. We use a trait object here so that all types of jobs can be passed to the execute method. Alright, now let's implement the execute method. First we'll wrap the closure we received in a box smart pointer.
00:16:52.025 - 00:17:26.945, Speaker A: Then we'll send our job down the channel. Here we're calling unwrap because send returns a result type. Send will fail if, for example, all our threads stop running. But we know that our threads will continue to run as long as the pool exists. So in this case we can call unwrap. All right. The last thing to do is update our worker structure inside the closure of our spawn thread.
00:17:26.945 - 00:18:13.845, Speaker A: The first thing we want to do is get a job. First we call lock on the receiver to acquire a mutex. Then we call unwrap because acquiring a mutex might fail. Then we call receive to receive a job from the channel. And finally unwrap again because calling receive my fail receive block. So if no job is available, then the thread that acquired the mutex to the receiver will wait for a job to become available. After we got the job, let's print out that this current worker is executing a job and then call the job.
00:18:13.845 - 00:18:57.157, Speaker A: If you notice up above, we get an error which states that the closure may outlive the current function, but it borrows receiver which is owned by the current function. What we want to do is move the receiver into this closure, and to do that we can specify the move keyword before our closure definition. The last thing we want to do is have our closure execute an infinite loop because we want our thread to constantly be looking for jobs to execute. And to do that we simply specify loop before the curly brackets. To understand how this works, let's talk through a scenario. Imagine that our server got four requests at the exact same time. We have four workers in our thread pool.
00:18:57.157 - 00:19:35.089, Speaker A: One of the workers will acquire a lock to the receiver and then pick up a job to execute. As soon as the worker starts executing the job, the receiver will be unlocked. Then another worker will acquire the lock, look for a job, and start executing the second request. This will happen for the third and fourth request as well. When a fifth request comes in, if all the workers are busy executing the other requests, then the fifth request has to wait. The first worker to get done executing their job will acquire a lock to the receiver, pick up the fifth job to execute, and begin executing the job. At this point, our multithreaded server should be complete.
00:19:35.089 - 00:20:17.359, Speaker A: So let's go ahead and run it. Open up two web browsers and we'll try our experiment again. We'll first call the sleep route and then right after we'll call the root route. And as you can see calling the sleep route is no longer blocking other requests. That's it for this video. On building a web server in Rust we learned how to prevent requests from blocking each other by using a thread pool. There are however other ways to improve the throughput of a server, for example the fork join model or the single threaded asynchronous IO model.
00:20:17.359 - 00:20:41.245, Speaker A: If you want to see videos on those models or other web server related videos then leave a comment down below. In the next video we're going to finish up our rustbook series so make sure to subscribe to get notified when that video comes out. And most importantly get your free Rust cheat sheet by heading over to LetsGetRusty.com cheat sheet with all that said, I'll see you in the next one.
