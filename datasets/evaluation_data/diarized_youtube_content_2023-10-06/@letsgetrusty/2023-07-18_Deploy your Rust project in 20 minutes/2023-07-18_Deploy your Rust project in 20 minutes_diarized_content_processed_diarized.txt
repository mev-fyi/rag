00:00:00.560 - 00:00:48.209, Speaker A: In this video, I'll show you how to easily deploy your rust applications using three popular deployment technologies, Docker, GitHub Actions, and DigitalOcean. Before jumping into the code, it's important to understand the high level architecture of our application. We'll be deploying a Rust API which uses postgres as its data store. We'll use Docker to containerize our API, GitHub Actions for continuous integration and deployment, and DigitalOcean as our Cloud provider. Then our API can be accessed by different clients over the Internet, such as desktop, mobile and HTTP clients like Postman. At the end, I'll also show you how to utilize caching to deploy your code changes in near real time. This video is part of a series for the upcoming official launch of my Rust Developer Bootcamp, the all in one Rust learning program you've all been asking for.
00:00:48.209 - 00:01:16.355, Speaker A: The Rust Developer Bootcamp will be launching on August 15th. To join the waitlist, pause this video right now and head over to LetsGetRusty.com Bootcamp and now let's get rusty. Here's our API opened up in VS code. Knowing how this API works at a high level is important for later steps of the deployment, so I'll give you a quick overview. If you want to follow along, there's a link to the GitHub repo in the description. In main, we're using the Axum framework to set up an HTTP server for our API.
00:01:16.355 - 00:01:59.285, Speaker A: Our API has four routes the base route which simply returns hello World, a route to create users, a route to fetch users, and a route to delete individual users. Each route has an associated handler function, and inside the handlers we're using the SQLX library to make database queries. Let's run the API and test the endpoints using Postman. If I call the root route using Postman, we can see that hello world is returned. If I call the getusers route, we get an empty array because currently there are no users. So let's create one. We're going to make a post request to the user route and give it a JSON object containing a name and email address.
00:01:59.285 - 00:02:51.179, Speaker A: As we can see, a user was created with the id1, and if we go back to the get users route, it now returns an array with one user. With that we have a fully working API using the Axum framework. By the way, if you guys want a full tutorial on the Axum framework, let me know in the comments section below. Now that we have a working app, the first step to deploying our application is to make sure, it doesn't run into compatibility issues on host machines, and we can achieve this through containerization, a term that sounds scary and complicated, but can actually be very straightforward as long as you know the fundamentals. Containerization is a technology that allows applications to run in a consistent environment by encapsulating all of the code and dependencies. An application needs to run within a self contained unit called a container. This saves time and effort when deploying applications across different environments and also improves consistency and reliability.
00:02:51.179 - 00:03:25.117, Speaker A: Docker is one of the most popular containerization technologies. It has a large active community and a wealth of documentation and tutorials. It's easily the go to option among many other containerization alternatives. Before we jump into using Docker, we need to understand four key Docker concepts. Docker Engine is the underlying technology that powers Docker. It's responsible for managing containers and providing a runtime environment for Docker applications. A Docker image is a read only template that contains everything needed to run an application, including the application code, dependencies, libraries, and configuration.
00:03:25.117 - 00:03:55.337, Speaker A: Docker images are immutable. Once they are built, we would need to create a new image if any modifications to the existing image are necessary. Any Docker container is a runnable instance of a Docker image. Containers can be started, stopped, restarted, or deleted when they're no longer needed. Comparing this to object oriented programming, an image is like a class or a struct in Rust, and a container is an instance of a class. One cool thing about images is that you can store and share them with others using Docker Hub. Docker Hub is a public registry provided by Docker.
00:03:55.337 - 00:04:30.401, Speaker A: It's a central repository that enables developers to store, manage and distribute Docker images to others in their organization or to the wider community, similar to how you can share your Rust projects on Crates IO. Last but not least, let's talk about Docker CLI and Docker Desktop. As developers, we can interact with Docker using the Command Line tool or the Desktop application. With that background, we can start containerizing our Rust application using Docker. The first thing we'll do is create an account on Docker Hub so we can push up images later on in the video. Once you've signed in, it should look something like this. Next we'll head over to docker.com,
00:04:30.401 - 00:05:10.323, Speaker A: scroll down a little bit and hit this big download button to download Docker Desktop. Once you have Docker Desktop running, make sure you hit this sign in button in the top right hand corner and sign into your Docker Hub account. Now that we're Signed in and Docker Engine is running, indicated by this green icon in the bottom left hand corner. We can create an image for our Rust application back in Vs code, we'll create a new file called Docker File in our project's root directory. A dockerfile is a text file that contains instructions for building a Docker image. Think about it like a recipe that defines the steps needed to build a Docker image. The first thing we're going to do is define our base image, which is the Docker image we're going to build on top of.
00:05:10.323 - 00:05:48.481, Speaker A: In this case, we're going to use the Rust Muscle Cross image, which is designed for compiling static Rust binaries. Then we're going to set the SQL X offline environment variable to true so we can build our code base without a live database connection. Next, we're going to change the working directory to the root of our project. Then we'll copy over all the source code and build our project using Cargo build, targeting a 64 bit architecture running a generic Linux environment with the Muscle C library. This allows us to build a standalone Rust binary. Next we're going to do something interesting. We'll start again with another base image called Scratch, which is a minimalistic base image that contains no files or operating system components.
00:05:48.481 - 00:06:35.379, Speaker A: Then we're going to copy over the binary built in the previous stage, and finally we'll run the binary and expose Port 3000. As you can see, the first stage, which is called Builder, is responsible for building our binary, and the second stage is responsible for running our binary. Because we've created a standalone executable, we're able to run it on the Scratch base image, which will make the Docker image extremely lean. Inside our project's root directory, we'll create another file called Docker Compose. YAML Docker Compose is a tool for defining and running multi container Docker applications. It allows you to describe the different services that make up an application in a single file and then run all of them together with a single command. In our case, we have two services, the API and the postgres database.
00:06:35.379 - 00:07:09.267, Speaker A: First, let's define the database. We've defined a service called DB which is going to pull the postgres image from Docker Hub. This service is configured to always restart if it shuts down for some reason. We're setting the Postgres password environment variable, exposing port 5432 and configuring a volume which will let us persist data between container restarts. Next, let's configure our API service. The first thing we do is specify that once the Docker image is built, it should be uploaded to our Docker Hub account. So here, instead of let's get Rusty, you would specify your Docker Hub username.
00:07:09.267 - 00:07:42.647, Speaker A: Next we set the Database URL environment variable. Specify that this image should be built using the Docker file we've defined. Expose port 3000 and also specify that this service depends on the database service being up. And just like that, we've defined our Dockerized application consisting of two services, the API and the database. The last thing we're going to do is add a Docker ignore file in the root of our directory. This is similar to a gitignore file. It tells Docker which files and directories to ignore.
00:07:42.647 - 00:08:30.811, Speaker A: With that, we're able to run our application using Docker by opening up the terminal and typing in Docker Compose up. This command will handle building images and launching container instances. Once it's done, you'll see log messages from the database container and our API container. If we go back to Docker Desktop and click on images, you'll see there are two new images, ApolloSample and Postgres. And if we click containers, you'll see one container stack running the DB container and API container. Going back to Postman, we can see that our API is able to be called except now our API is being run through Docker. Now that our app is containerized, we can worry about automating the deployment.
00:08:30.811 - 00:09:10.785, Speaker A: This often overwhelms many people, but it actually doesn't have to be complicated at all. For deployment, we'll use two industry standard tools. We'll use GitHub Actions to automate the process of building, testing and deploying our application when new code changes are committed. GitHub Actions is a great option for your CI CD needs. If you're already using GitHub to store your source code and we'll deploy our application to DigitalOcean, then our API can be accessed by different clients over the Internet, such as desktop, mobile and HTTP clients like Postman. Cloud providers like AWS have complicated UIs and pricing structures that can bankrupt you if you're not careful. That's why I prefer DigitalOcean.
00:09:10.785 - 00:09:45.325, Speaker A: It's a cloud provider that focuses more on individuals and small businesses. It has a very straightforward UI and a pay upfront pricing model. To get started, head over to digitalocean.com and create an account. Once you signed up, you'll want to create a project if one is not created for you, and inside that project we'll spin up a droplet, which is the virtual machine where we'll be deploying our application. For this example, I'm going to use the cheapest droplet possible, so I'll pick New York as the region. Use Data Center 1, leave the OS as Ubuntu, choose Regular for the disk type, and then pick the cheapest option.
00:09:45.325 - 00:10:40.375, Speaker A: For the authentication method, we're going to use a password. And lastly, we're going to give our droplet a nice host name. Now that our droplet is running, we're going to configure it by clicking these three dots, then Access Console and then launch Droplet Console. Here we have a terminal opened on our droplet. We want to make sure that our droplet is configured with Docker Engine, and we can do that with three simple commands. First, we'll run sudo apt get update. Then we'll install Docker by running sudo apt get install docker IO and finally we'll install Docker Compose by running sudo apt get install Docker Compose.
00:10:40.375 - 00:11:16.745, Speaker A: Our droplet is now configured. The next step is to set up a GitHub Actions workflow that deploys our application to this droplet. Inside our project's root directory, we'll create a new folder called GitHub. And inside that folder we'll create another folder called Workflows. Finally, inside the Workflows folder, we'll create a file called prod YAML. This is where our GitHub workflow will be defined. The first thing we'll do is give it a name and then specify when this workflow should run.
00:11:16.745 - 00:11:43.905, Speaker A: Here we're specifying that the workflow should run when changes are pushed to the master branch. We'll also make sure that the sqlx offline environment variable is set to true. Next, we'll define our jobs. The first job is called Build, and it will be responsible for building and testing our code. Jobs are performed by following a sequence of steps. The first step here is to check out our code. Then we'll install the Rust toolchain.
00:11:43.905 - 00:12:10.325, Speaker A: Next we'll run Cargo Build and Test. Then we'll set up Docker Build X for building our Docker images. Then we'll log into Docker Hub using our username and password. And finally we'll build and push our images to Docker Hub. Our second job is going to be called Deploy. We'll specify that it depends on the Build job and runs on Ubuntu. The first step is to check out our code.
00:12:10.325 - 00:13:10.861, Speaker A: Then we'll log into Docker Hub using our username and password. Next, we'll install SSH pass and use it to copy the Docker Compose file to our droplet. Finally, we'll deploy our application by logging into our Droplet instance running Docker Compose down to bring down any running containers. Docker Compose Pull to pull the new images from Docker Hub and Docker Compose up to spin up containers from the new images. So in summary, our first job builds our code, tests our code and, assuming the tests pass, uploads updated Docker images to Docker Hub and our second job copies over the Docker Compose file from our source code to our droplet, and then pulls the latest images from Docker Hub and deploys them. Now, you may have noticed that this file has a few placeholders like Secrets, Droplet Password, vars Droplet IP and secrets postgres password. These are values we don't want to directly store in source code, so we create placeholder variables and then configure them on GitHub.
00:13:10.861 - 00:13:42.625, Speaker A: We can set these values by opening our repo on GitHub.com heading over to Settings, clicking Secrets and Variables underneath the Security tab, and then Actions. Secrets are used for sensitive data and will be encrypted, whereas variables are plain text and used for non sensitive data. Let's add a few secrets. Click the new repository secret and the first secret we're going to add is our Docker Hub username. For the name we're going to use Docker underscore username all caps. Then we'll add our Docker Hub password.
00:13:42.625 - 00:14:56.425, Speaker A: Next we'll add our droplet password and then postgres password. The last seeker we'll add is our DigitalOcean token, which will allow GitHub Actions to programmatically interact with DigitalOcean. To generate this token, head over to DigitalOcean and click API and then generate new token. Lastly, we'll add one variable which stores our droplet IP address. To get your droplets IP head over to DigitalOcean, click on your project and then underneath the droplets, you can see the IP address here and simply click copy. With our Secrets and variables configured, we should be able to push up our code changes and have our application automatically deployed. Going back to our code base, let's commit our changes and push them up.
00:14:56.425 - 00:16:00.403, Speaker A: Going back to GitHub if you click on Actions, you should see the GitHub workflow running for your new commit. Once the workflow is finished, you'll either see a green check mark meaning that it succeeded, or a red X meaning that it failed in this case, it failed, so let's click into it to see what went wrong. As we can see here, it failed on the Deploy step. Here we can see that it failed copying over the Docker Compose YAML file, and it looks like it's because it's using the YAML extension instead of the YML extension. So let's go ahead and fix that. Back in VS code, we'll open up Prod YAML, head over to the Copy Docker, compose to Droplet Step and update the file extension from YAML to yml and then push up our changes. All right, the latest commit did pass, so theoretically our app should have been deployed to our Droplet.
00:16:00.403 - 00:16:45.685, Speaker A: Next, we'll log into our Droplet and verify. We'll head back over to DigitalOcean, click the three dots next to our Droplet, click Access Console and launch Droplet Console. Then we'll type in docker.ps to see which containers are running, we can see two containers are running apiexample and postgres, and they were both created recently. Now that we know our Droplet is running our application, let's try to access it from our local machine using Postman. Head back over to DigitalOcean and copy the IP address of your droplet and then in postman, replace localhost with your droplets IP address. And as you can see, we are able to call our API running on our Droplet.
00:16:45.685 - 00:17:30.275, Speaker A: With only these few short steps, we fully automated our deployment pipeline without breaking a sweat. Next, I'll show you how to use caching to drastically reduce deployment times. Before we get to that, if you want to learn more about building real world Rust projects and deploying them to production, I've created the Rust Developer bootcamp, launching on August 15th, which I'll talk about more at the end of the video. To get faster deployments, we'll take three steps. First, we'll update our Dockerfile to use Cargo Chef. Cargo Chef is a Cargo plugin that speeds up Docker builds by caching project dependencies. We'll start by renaming the first stage in our dockerfile from Builder to Chef.
00:17:30.275 - 00:18:09.735, Speaker A: Then, before switching to our working directory, we'll install Cargo Chef. Next, we'll create a new stage called Planner. In this stage, we'll copy over the source code and then run Cargo Chef Prepare, which will generate all the information required to build and cache our dependencies in a file called Recipe JSON. Next, we'll create another stage called Builder. First, we'll copy over Recipe JSON from the previous stage. Then we'll run Cargo Chef Cook to build and cache dependencies and finally build our project. Our Docker build should now be a lot faster if we're making changes to our code without modifying the dependencies.
00:18:09.735 - 00:18:49.695, Speaker A: The next two steps will be done in our GitHub Actions workflow. First, we're going to add a new step to the build job called cache dependencies. This step will speed up subsequent builds by caching dependencies, similar to how Cargo Chef caches dependencies in our Docker image build, but this is specific for GitHub Actions. Next, we'll update the build and push Docker images step by adding some caching options. These options tell the Docker build process to cache Docker images. Subsequent builds can then reuse pre built image layers from the cache. With these three caching strategies implemented, our deployment should now be a lot faster.
00:18:49.695 - 00:19:41.203, Speaker A: Let's commit this code, push it up and verify. If we look at GitHub, you can see that the deployment time for our initial commit actually increased from seven minutes to eight minutes. However, once the caches are populated, deployment times are cut in half. If you've enjoyed this deployment video, you're going to absolutely love the Rust Developer bootcamp launching on August 15 when learning a new language or technology, one of the worst feelings is being stuck in tutorial hell and feeling like you're not making any progress. You're watching videos and mindlessly copying and pasting code. But deep down you know what you should be doing. You should be applying your knowledge, building interesting projects and working through problems.
00:19:41.203 - 00:20:20.345, Speaker A: But it's not that easy. What projects should you work on? What frameworks and technologies should you use? How do you scope the project so that it's not too hard and not too easy? How do you find project partners? And what do you do when you get stuck? The Rust Developer Bootcamp tackles every one of these problems through hands on learning with real world projects and exercises designed to get you out of tutorial hell and shipping Rust code as fast as possible. The Bootcamp is launching on August 15th and you can get early access by heading over to LetsGetRusty.com Bootcamp hope you've enjoyed this video and remember to stay rusty.
