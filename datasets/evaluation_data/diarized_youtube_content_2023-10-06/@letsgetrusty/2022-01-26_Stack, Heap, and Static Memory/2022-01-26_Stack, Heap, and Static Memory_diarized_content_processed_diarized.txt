00:00:00.320 - 00:01:05.659, Speaker A: What's up Rust Stations? Welcome back to let's Get Rusty, your number one resource for all things Rust. Now, I think Rust is a great language, but it's also known to have a steep learning curve, and a lot of that has to do with the way Rust manages memory. Now, as somebody that's coming from a web development background, I can empathize with people learning Memory Management in Rust. Not only did I need to learn ownership and borrowing in Rust, I needed to learn the basics of memory management. Things like what is a heap? What is a stack? What's the difference between manual memory management versus a garbage collector versus smart pointers? And throughout this learning process, I realized that in order to understand the beauty of memory management in Rust, it's really helpful to understand the basics of memory management and what the existing memory management strategies are. With this foundation, you can better understand and appreciate the innovations that Rust brings. So I decided to make a video series talking about memory management more generally resource management.
00:01:05.659 - 00:01:51.389, Speaker A: Starting off with the basics such as what is memory and why do we need to manage it? Then moving on to existing memory management strategies and finally talking about what makes Rust so special. In this first video, I'm going to go over the basic memory regions that you need to know about the stack, the heap, and static memory. Note that this is going to be a high level overview. My goal is not to teach you everything there is to know about memory management. My goal is to give you enough information to be an effective Rust developer. So with that said, let's get Rusty. First, let's discuss what resources a computer provides.
00:01:51.389 - 00:02:28.961, Speaker A: A computer provides two basic resources, computation and memory. Computation is the ability to crunch numbers, the ability to do math, and that's provided by the cpu, AKA the central processing unit. Memory is the ability to store data, and there are two important categories of persistent memory and volatile memory. Let's take a closer look at these two categories. An example of persistent memory is a hard drive or a solid state drive. An example of volatile memory is RAM or random access memory. Note that there are different names for volatile and persistent memory.
00:02:28.961 - 00:03:06.571, Speaker A: Sometimes volatile memory is called main memory or working memory, while persistent memory is called secondary memory. All right, let's compare and contrast these two different types of memory. Reading and writing from persistent memory is slow, while reading and writing from volatile memory is fast, orders of magnitudes faster than persistent memory. On the other hand, persistent memory is abundant while volatile memory is scarce. Think about your computer. You might have a 2 terabyte hard drive, but only 16 gigabytes of RAM this brings us to use case. Persistent memory is used to persist data, for example files.
00:03:06.571 - 00:03:43.269, Speaker A: And by persist I mean when you turn off your computer and turn it back on, your files are still there. Volatile memory is used during program execution. For example, you might have a program that has a bunch of variables which hold data. Now, that data will be stored in volatile memory, and it's called volatile because if the computer turns off in the middle of your program executing, then all that data in volatile memory will be lost. Now, volatile memory and persistent memory work together. For example, let's say you built a web server. While that web server is running, it's using volatile memory.
00:03:43.269 - 00:04:22.907, Speaker A: But your web server may also be connected to a SQL database. And when you add a new record to the SQL database, it's saved in persistent memory. Now, when talking about memory management, we don't need to manage persistent memory. What we need to manage is how our program uses volatile memory while executing. So next, let's look at the different memory regions available in volatile memory. If we look at the diagram to the left, we can see that the gray box represents our volatile memory, labeled here as free memory, or the memory available to us. Within the gray box, we have three distinct memory regions with their own unique characteristics.
00:04:22.907 - 00:05:14.195, Speaker A: Static memory, the heap, and the stack. Let's take a closer look at the characteristics of these memory regions. We'll look at the contents of the region, the size of the region, the lifetime of the values inside that region, and how those values are cleaned up. First, let's look at the static region, which you can see at the bottom. Here, the static region stores your program's binary instructions, static variables, and in rust string literals, it has a fixed size that's known at compile time. The values in the static region have a lifetime that's equal to the lifetime of the program. Now, I know lifetime is a trigger word for rust stations, but in this case, what I mean by lifetime is how long do the values in this region live for? When talking about the static region, values are created when your program starts up and destroyed when your program ends.
00:05:14.195 - 00:05:43.493, Speaker A: So the lifetime of the values is the same as the program's lifetime and cleanup is automatic. As I just pointed out, when the program terminates, all the values in the static region are cleaned up. Next, let's talk about the stack. The stack contains stack frames. When we have a chain of function calls from, for example, the main function calls A, then A calls B, then B calls C. A stack frame will be created for each function call. The stack frame will contain arguments.
00:05:43.493 - 00:06:25.785, Speaker A: The Function was called with as well as variables local to the function. The arguments and variables inside of a stack frame must have a size that's known at compile time, and when a function returns, its stack frame is popped off the stack. The size of the stack is dynamic, meaning that it could grow as your program executes. However, it has a fixed upper limit that's defined when your program starts up. In the diagram to the left, this arrow represents that the stack could grow, and this blue border represents that the stack has a fixed upper bound. And if we surpass that upper bound, we get the infamous stack overflow error. The lifetime of the values on the stack is the same as the lifetime of the function.
00:06:25.785 - 00:07:00.387, Speaker A: Once the function returns, its stack frame is popped off and all the values are destroyed. This also means that cleanup is automatic. When the function returns, the values will be cleaned up. Lastly, let's talk about the heap. The heap stores values that need to live beyond a function's lifetime. If you have a value and you want it to live past a function's lifetime, then you can't store it on the stack, because if you store it on the stack, it will be destroyed when the function returns, so you would have to store it on the heap. Also, values that are accessed by multiple threads must be stored on the heap.
00:07:00.387 - 00:07:37.259, Speaker A: That's because each thread has its own stack, but all threads share the same heap. The heap is also a good place to store large values, because the stack has a fixed upper limit. If you store large amounts of data on the stack, you could exhaust the memory available to you. So instead you can store large amounts of data on the heap, which doesn't have a fixed upper bound. Or lastly, if the size of a value is not known at compile time, then it must be stored on the heap. For example, let's say you have a program that asks users for their name. You don't know how long their name is going to be, so you don't know the size of the string you need to store.
00:07:37.259 - 00:08:07.357, Speaker A: Therefore, you would need to store your string on the heap. Which brings us to size. The size of the heap is dynamic. It could grow as your program executes. The only real limitation is the amount of physical memory you have available. For example, if you have eight gigs of RAM available and you exhaust all eight gigs, well, there's no more memory left, and in that case your computer will slow down and crash. Unlike the stack or static memory, the lifetime of values in the heap is not predefined.
00:08:07.357 - 00:08:40.175, Speaker A: It's determined by the programmer. The programmer decides when values should be allocated on the heap and when values should be deallocated from the heap. This also means that cleanup is manual. The programmer has to clean up memory on the heap themselves. Now, as you've probably noticed, I've highlighted some things in this table. When we're talking about memory management, we're talking about the lifetime and cleanup of memory. And as we can see, for static memory and memory on the stack, the lifetime is predetermined and cleanup is automatic.
00:08:40.175 - 00:09:12.205, Speaker A: So when we're discussing memory management, what we're really discussing is managing memory on the heap. Now in this table, I've said that the programmer is responsible for the lifetime and cleanup of memory on the heap. This is called manual memory management. However, it's not the only way to manage memory on the heap. In the next video, I'm going to go over the different strategies for managing memory on the heap. Alright, that's it for the first video in this series on memory management. If you enjoyed this video, make sure to leave a comment to let me know.
00:09:12.205 - 00:09:28.645, Speaker A: Like the video and if you want to see weekly Rust content, then hit subscribe. Lastly, and most importantly, get your free Rust cheat sheet by heading over to letsgetrusty.com cheatsheet and with that said, I'll see you in the next one.
