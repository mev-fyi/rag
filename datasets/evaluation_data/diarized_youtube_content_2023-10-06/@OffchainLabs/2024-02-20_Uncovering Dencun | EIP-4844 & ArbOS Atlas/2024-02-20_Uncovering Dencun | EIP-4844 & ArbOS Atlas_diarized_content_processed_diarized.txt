00:00:01.210 - 00:00:02.000, Speaker A: We start.
00:00:02.530 - 00:00:03.280, Speaker B: Okay.
00:00:07.330 - 00:00:35.126, Speaker A: All right. Hey, everyone, I'm James, an engineer on the Prism team, and I'll be moderating today's discussion. We'll be talking about the Dankoon hard fork and the upcoming Arbos atlas upgrade. Just some quick intros. Let's kick off with Terrence Ethereum, core dev, prism maintainer researcher, and one of the initial contributors of EIP 4844.
00:00:35.308 - 00:00:36.040, Speaker B: Hello.
00:00:38.190 - 00:00:45.690, Speaker A: Next up we got Preston, prism team lead build tool extraordinaire, and Og cypherpunk.
00:00:46.190 - 00:00:47.500, Speaker C: Hey, what's happening?
00:00:49.470 - 00:00:56.640, Speaker A: Then we have Lee, mastermind behind the nitro stack, and arbitrum tech lead.
00:00:57.810 - 00:00:58.880, Speaker D: Hello, everyone.
00:01:00.450 - 00:01:06.750, Speaker A: And finally we got Daniel Arbitrum, blockchain engineer and Dow governance specialist.
00:01:07.490 - 00:01:08.260, Speaker B: Hello. Hello.
00:01:09.990 - 00:02:00.802, Speaker A: At offchain Labs, we have engineers working on both the layer one and L2, and we'll discuss some preselected questions regarding the major upgrades coming to Ethereum and arbitrum at the end. If there's any time remaining, we'll cover some questions that the community might have. As a note to our audience, some of our speakers will need to drop a few minutes early due to prior commitments, but I'll let you know. Let's get started with the change on the l one side. I heard some big upgrades are coming to Ethereum. Blobs are coming, also known as EIP 4844 or protodank sharding. It was years in the making.
00:02:00.802 - 00:02:11.026, Speaker A: I remember when I first joined, it was already being researched. Terrence, can you tell us a little bit about what's happening on Ethereum?
00:02:11.218 - 00:02:17.842, Speaker E: Sure, I guess. Let's rewind back and just look at what problem we're solving.
00:02:17.986 - 00:02:18.678, Speaker B: Right.
00:02:18.844 - 00:02:34.910, Speaker E: And so how do we get ourselves at this point? Basically, so we have Ethereum. It's a blockchain, it's trustless, and as more people use it, there's more TVL, and then it's harder to move.
00:02:34.980 - 00:02:35.406, Speaker B: Right?
00:02:35.508 - 00:03:17.754, Speaker E: And a few years back, we introduced a concept that is the Rob central roadmap, and most of the community member bought into the idea. So we use rob to innovate on execution, and then we let Ethereum to be what it is. Right, so what is Ethereum's unique strength? So I think Ethereum's unique strength is security and decentralization. Right, but Rob posts data on Ethereum, and there is a high cost of that because of you're paying for the security, you're paying for the decentralization. So posting data on Ethereum is very expensive today. So we want scalable data and how do we get there?
00:03:17.792 - 00:03:18.282, Speaker B: Right?
00:03:18.416 - 00:03:37.794, Speaker E: So that is where then sharding comes in. So we are allowed to have tons of data without trading off on decentralization and security by using technique like erasure coding and data sampling. But then sharding is hard. It will take us many years to get there.
00:03:37.832 - 00:03:38.130, Speaker B: Right?
00:03:38.200 - 00:04:25.060, Speaker E: So what can we do in the meantime? So we can take incremental step, right? So the first step we can take is basically say that now all the node have to store all the data, and then we only store a subset of what then charging will provide. But in the meantime, we get to test some cryptography primitive in the meantime. So we're getting closer to the angle, and then we are experiencing a new gas market, right? So eip four four four prototype. Sharding is basically this idea that we have another gas market for data, and then we kind of separate the concern between institution and data. So, which is designed for Rob. So today, as a row up, we can decide which one to use.
00:04:28.150 - 00:04:33.458, Speaker A: All right, just a quick question. Can smart contracts use some of this data?
00:04:33.544 - 00:05:12.382, Speaker E: Posted this blob data depends on design. So current design is. No, but other data, available data, other DA layer may allow you to do that, but then in the Ethereum design, you cannot do that. And why is that? Because there's always a trade off at play, right? The trade off here is that how long do you store the data and do you get to access the data from the EVM? If you allow the data to be stored for longer, and if you allow EVM access, then subsequently those will have to price at a higher price because of you're allowing those things.
00:05:12.436 - 00:05:12.798, Speaker B: Right.
00:05:12.884 - 00:05:46.730, Speaker E: And then for our purpose, I think we're supposed to design this blob data for L2 to use, such that if we design this for L2 to use, we can kind of price those window of storage to be like, for example, during a fraud period. So, such as more than 14 days. And then we only need to prove that the data is there, but we don't have to be able to retrieve the data, because once you prove the data is there, you can take the proving off chain.
00:05:49.310 - 00:06:00.778, Speaker A: Got it. Thanks, Terrence. Maybe Preston can also chime in a little bit. What's changing for node runners? And why is this so important for the ecosystem?
00:06:00.954 - 00:06:01.438, Speaker B: Yeah.
00:06:01.524 - 00:07:17.718, Speaker C: So just to expand even further on Terrence's answer, in DNeb, we also have an EIP that allows the beacon block route to be accessible in the EVM. This does allow some proving mechanisms within a smart contract. Doesn't allow you to access blob data, but you can prove that it was there. So that's really nice. And as a node operator, there are some new workloads coming, right? We're dealing with blobs, and this is a whole nother layer to the beacon chain in terms of being a node operator. So what you want to think about here is you're probably asking, well, what are my resource requirements here? Do I need to go buy a server or like some hard drives or what do I need to do? Well, the answer is pretty simple, right? There are these blobs that we're keeping around, and the minimum you have to keep them around is 4096 epochs, I believe. And every blob is the same size, and a block can have zero to six.
00:07:17.718 - 00:08:19.850, Speaker C: So if you're doing the minimum spec requirement of that 4000 epochs, you can expect that you're going to have to have storage requirements of an additional, up to an additional 100gb I think is the math on that. It could be less, but it certainly wouldn't be more because that's sort of the maximum. Assuming you're running with that parameters, you can choose to store blobs forever. Keep them in your personal archives if you wish. But if you're just a humble validator hoping to keep the chain going, that's what you'd expect from a hardware constraints. There is a little bit more networking overhead, but I feel that most stakers are not constrained in the networking or bandwidth aspect, and they're most concerned about the disk and compute requirements, which are slightly elevated but not significantly so. It's to the order of 100gb.
00:08:23.890 - 00:08:24.302, Speaker B: Cool.
00:08:24.356 - 00:09:01.850, Speaker A: So the blobs only have a retention period of a limited time. And you can choose if you want to archive it for longer, but that'll require more capacity for storage. And by having this extra space on Ethereum, l two s can take advantage of it and potentially make their transactions cheaper. So let's talk about that with Lee up next. How can arbitrum use some of the new features in Denkun?
00:09:03.870 - 00:10:25.942, Speaker D: Yeah, definitely. So even though smart contracts aren't able to inspect the contents of the blob, they can see that a blob was posted and they can see what its hash is. And a hash is a cryptography term which basically summarizes the entire data into a unique small string. And this is important because the blob data isn't accessible in full, in that it would be very tricky, for instance, to use this in most smart contract operations because for instance, Uniswap, if you're doing a trade, there needs to know which token you're buying or selling, or what price you'd be willing to buy or sell it at, or any other decks. And if you tried to put this in blob data, there's a number of problems, like Blob data is too large, you can't make it smaller for a single trade, and Uniswap wouldn't be able to access that data without proving what was in it, which would then be even more expensive than just posting it in call data. But arbitram is a bit unique in that most of the work happens on L2 itself. And what arbitram is able to do is this batch data that gets posted from L2 to layer one.
00:10:25.942 - 00:11:27.534, Speaker D: It can post in the form of a blob. And it's able to do this because with L2s, L2s are able to do a lot of this work that smart contracts would otherwise be doing at layer one, off chain at L2. And you can sort of think about it, like I mentioned earlier, that proving what that data would be would be just too expensive to make it worthwhile for most smart contracts to use. But because we can reveal the data at L2, we can pay that cost there instead. So it's like how doing a Dex transaction or a swap will be cheaper on L2 than layer one. We're able to take advantage of the data in the same way and only pay the blob posting cost for data. So previously we've had to post data to the Ethereum virtual machine itself, which is both more expensive.
00:11:27.534 - 00:12:20.460, Speaker D: But it's also not as great for node operators, because if you're running a layer one ethereum node, that means that you'd have to store this arbitrum data forever, depending on future eips. But if you wanted to be able to execute the EVM, you need to know what data was posted to arbitrarim. And that doesn't really make sense. If you're just executing an Ethereum node and you don't have an arbitram node running, why are you keeping around all of this arbitrum data? And that's sort of fundamentally what blobs fix is they allow you to not keep around all of this arbitrum data. Arbitrum can use it, but arbitram is sort of isolated in its own little bubble, and that means that if you're running an arbitram node, it'll keep around that data. But we're able to take advantage of this cheaper data because not everyone needs to keep it around.
00:12:24.270 - 00:12:32.080, Speaker A: Got it. And in order for arbordrum to use this, that's where Arbos atlas comes in, right?
00:12:33.250 - 00:13:04.630, Speaker D: That's exactly correct. So utilizing this new data posting format definitely involves some significant changes to the code. And that's where Arpos Atlas comes in. Arpos atlas adds the ability for arbitrum nodes to decode this new format of data and retrieve the data and do fraud proofs over it so that if a malicious validator disputed what's in the contents of the blob, honest validators are able to issue proofs that show what is actually in the blob.
00:13:07.050 - 00:13:22.590, Speaker A: So for Arbitrum, there's actually a lot of different chains. There's arbitram one, arbitram Nova, and the orbit chains. Do all of these chains take advantage of Arbos atlas?
00:13:24.050 - 00:14:18.014, Speaker D: So any L2 arbitrum chain can choose to take advantage of atlas, and even layer threes can take advantage of parts of atlas. So to sort of explain that a little bit. So there's currently an arbitrum dow vote for Arbos Atlas. It's passed the initial snapshot vote, and it's now under an on chain vote with tally. And should that vote pass, that will permissionlessly upgrade the arbitram one and arbitrum Nova chains to this new arbitram atlas. Or so that vote controls whether arbitrum one and arbitrum Nova will be upgraded to support atlas. Other L2 orbit chains can actually upgrade into Arbos Atlas whenever they'd like.
00:14:18.014 - 00:15:28.594, Speaker D: Now, for L2 chains, they'll be able to take advantage of the EIP 48 44 blog posting that we've been talking about only when that goes live on layer one theorem. And that's because right now, layer one theorem doesn't support posting blobs. That's getting added in this upcoming upgrade we've been talking about. But if a L2 orbit chain upgrades to atlas, it will still be able to take advantage of some other features of atlas, like additional EVM opcodes such as memory copying and transient storage. For layer three chains, layer three chains take advantage of EIP 48 44 batch posting indirectly. So layer three chains post their data to a L2 chain, and that L2 chain will likely have cheaper data costs because of the fact that they are then hosting batches with EIP 48 44 layer one chain. So for layer threes, it's not going to matter directly, but rather indirectly through the parent L2 chain.
00:15:28.594 - 00:15:37.110, Speaker D: That said, layer three chains, upgrading to Arbos Atlas will still give them those other EVM features I mentioned, like memory copying and transient storage.
00:15:39.550 - 00:15:54.000, Speaker A: Well, that's a good transition to maybe get some of Daniel's input. If the Arbos Atlas upgrade isn't happening automatically, how does the governance process look?
00:15:55.090 - 00:17:00.740, Speaker B: Sure. Yeah. So, basically for the governance process, and like Lee said, this applies specifically to the two arbitram Dao govern chains, which are arbitram one and Arbitrum Nova governance itself takes place on arbitram one, which is to say that's where governance proposals get submitted and voted on. And then there's kind of this cross chain governance system where basically from there, you can imagine once if a proposal passes, it kind of sends a message out or sends messages out that land, wherever they need to go, which is basically, they might need to execute somewhere on arbitram one itself, maybe on arbitram nova itself, or on the underlying chain Ethereum. And all of that happens, all of that's executed on chain. All that is carried out trustlessly and permissionlessly for this particular proposal, which voting is live, I think it goes for about another week, and it looks like it's on track to pass, but I believe it hasn't hit quorum quite yet. Last I checked, anyway, this particular one is probably.
00:17:00.740 - 00:18:10.170, Speaker B: Maybe I'd have to think about this more, but it's probably the proposal that's maybe the most complex, or at least has the most stuff in it. This particular one involves changes on all three of the chains I mentioned, but that all kind of happens in one proposal in terms of just maybe a little bit about what the change looks like or what actually needs to be changed. And this is all kind of in line with the stuff we just spoke about. But for arbitrage one to be able to accept this new data type, this blob data, there's an update to what we call the inbox contract, where transaction data gets posted just so that it can accept this new data type. There's some modifications to the fraud prover itself because of this new data type that need to be added. And then for those other various changes to the EVM, just in general, there's this kind of arbos upgrade path that needs to happen, which does require an on chain action, which is essentially in a roundabout way, also, so that the fraud prover works and knows what to do. So that'll apply to both arbitrage and one and Nova.
00:18:10.170 - 00:18:22.030, Speaker B: And, yeah, that's where it stands, the proposal submitted. So that's all kind of everything's built and set in stone, and voting is underway.
00:18:23.330 - 00:18:30.190, Speaker A: Does the proposal address it? For all the chains, for just one proposal, or do you need to do it for each chain?
00:18:31.170 - 00:19:15.790, Speaker B: One proposal can cover both chains, which is nice. And it's basically, there's a lot of flexibility in how proposals can be constructed. So you can target any chain and you can essentially execute whatever you want on that chain when you submit a proposal. And essentially the way the contracts are set up is that the Dow, via this governance process, kind of owns all the key affordances for arbitrage one and Nova. So upgradability of all of the contracts the Dow owns and sort of system level parameters that are important, it can set, it could even modify the governance process itself. It owns the governance contracts, too, although that's not happening with this proposal.
00:19:17.490 - 00:19:23.520, Speaker A: Let's talk a little bit about those timelines then for ArB token holders. When can they vote for.
00:19:24.450 - 00:19:57.606, Speaker B: Yeah, so voting goes on for about another week. The place that most people vote, the easiest way to vote is through the tally website. But again, this is all on chain smart contracts. So you can technically vote anyway. You can access smart contracts. Yeah. So about another week that Dow delegates can vote or need to actively do anything after that, if it does pass, then there's about another 13 days or so until it actually activates.
00:19:57.606 - 00:20:31.320, Speaker B: And essentially there's sort of a series of delays put in place deliberately, which, again, is all enforced by these smart contracts. And the idea there is to guarantee that users of arbitram one or arbitram nova, sort of have a window where after an upgrade passes, they can still, if they want to, kind of opt out and withdraw, say, their assets. That's sort of the guarantee, the security guarantee that these were built in mind for. So, yeah, that's why there'll be another bit of a delay before it actually goes live, even once. It's guaranteed to go live.
00:20:33.130 - 00:20:53.930, Speaker A: All right, so in terms of those timelines, coordinating it with the Dankoon upgrade, Preston or Terrence, either of you can take this. Tell us a little bit about the timeline on when Dankoon is happening or what people need to upgrade.
00:20:54.090 - 00:21:26.934, Speaker E: Yeah, sure. So client release are happening this week, and Prism just released our version five point about 30 minutes ago. So keep in mind that besides updating your consensus layer client, you also need to update your execution layer client. And if you are using, for example, relayers and builders, you need to update your mvp boot software. So there are three pieces of software that needs to be updated. Most of them are out. I believe about 99% of them are out.
00:21:26.934 - 00:21:54.450, Speaker E: And the blog post from the Ethereum foundation is coming early next week. And then the hard work date for Danette is March 13. It's about 06:00 a.m. My time, which is Pacific time zone. So it should be about, I think like 01:00 p.m. UTC or something. I could get that wrong, but yeah, that's the timeline.
00:21:55.830 - 00:22:45.278, Speaker A: Awesome. So March is going to be a pretty big month for Ethereum and arbitrum. The Dankun upgrade happens March 13, and then atlas, as soon as that activates, can take advantage of these blobs, which is a good transition into our next section of discussion. Let's talk about the impact of all of these changes. There's a lot of stuff that happens, but what are some predictions on what will happen in the ecosystem because of this? Maybe we can start with the Ethereum l one side, since Terrence just answered the last one. How about Preston, you can answer, what do you think will happen on the ecosystem?
00:22:45.454 - 00:23:46.980, Speaker C: Yeah, so the big feature of Daneb Denkun is blobs, right? EIP 4844. And the impact of that is rather obvious, right. We are taking some of the work that's put onto some of the data that's put onto Ethereum and putting in its own fee space. So the gas fees for l one would not be competing against the l two s that are submitting data, and that this fee market should be substantially cheaper than competing with users that are doing normal l one transactions. So l two should greatly benefit that. But besides that, and there's a lot that could be dove into that. But besides that, there are some other cool features that we have in DNab that I think will spark some pretty interesting changes.
00:23:46.980 - 00:24:39.480, Speaker C: The one I want to highlight is, again, is the beacon block route in the EVM. So there are a couple of actors in the ecosystem right now that have a bit of a trust assumption, namely liquid staking. Providers have to have oracles to really tell what's actually going on in l two. Sorry. In the beacon chain, these liquid staking protocols, they have node operators who are managing the keys, and the l one and the Dow. And however they're doing this needs to understand what are the staking balances? Are operators performing well? Are things slashed, et cetera? Well, with having the beacon block route in the EVM, you don't need to trust oracles anymore. You can just prove that information directly in a smart contract, which is really nice.
00:24:39.480 - 00:25:26.914, Speaker C: Another nice innovation that we have is that, well, maybe it's a design and flaw, but previously in other forks, you have this concept of voluntary exits for validators. Well, they were only valid for the particular fork that you signed them for. And in Denab, they don't expire anymore. They're valid forever. So you can pre sign these voluntary exits and keep those perhaps in your cold storage emergency vault. If you lost control of your hotkeys, well, then you can just publish these voluntary exits. So that's pretty nice for solo stakers in particular, having a backup plan to off I lose my hotkeys and I want to send a cold storage.
00:25:26.914 - 00:26:19.974, Speaker C: How do I do that? It's a pretty good mechanism. Another thing we added was there's an upper bounds to the validator churn limit. So there's this limit on how many. It's a rate limiting mechanism for validators entering and exiting the system. And in the last year or two, we saw a lot of activity, especially post merge of validators joining. And people are saying if we keep going at this rate, all of Ethereum stake will be, all of ether will be staked 100% within like 24 months or something crazy. So we said, you know what, let's set an upper bound on the churn limit so we have time to prepare for that event.
00:26:19.974 - 00:26:35.580, Speaker C: If there were ever an event where 100% of this ether wanted to be staked, it would take a little bit longer. You have more time to prepare in terms of being a node operator. Yeah, a lot of cool stuff.
00:26:36.350 - 00:26:58.100, Speaker A: Awesome. What about on the arbitram side? So the change on eth l one impacts l two s the most. So there must be some pretty spicy takes on what's going to happen in the arbitrum ecosystem specifically. Anyone want to start?
00:26:59.430 - 00:28:09.020, Speaker D: Well, I'm not sure it'll be too spicy, but the main impact will be, presumably call data will become cheaper. It's a bit hard to say how cheaper it will become because this is a market. There's bidding for the cost of blobs and such and the fixed capacity, but it'll increase the overall capacity for call data between and also put it in its own separate market, which will mean that, for instance, previously, when layer one has gotten really congested, if you have some big mint happening or something, or something. Layer two, they don't get as expensive, but they also start getting more expensive because they need to pay layer one posting costs. That'll still happen to some degree. But now the L2s will mostly be paying in a separate market for blob data, which is separate from the market for fees that most of these NFT mints and such will be doing. So that should hopefully not only reduce the cost of call data on L2, but also the variability of call data on L2.
00:28:09.020 - 00:29:37.606, Speaker D: And I'll be very interested to see if that allows for different types of Dapps on arbitrum to flourish, because previously there's been a very interesting trade off that didn't exist on layer one, where call data was much more expensive than storage or execution. And this has sort of changed the types of applications you see. And we've seen some very smart applications like Llamazip and similar, which specifically exist to take advantage of that difference between the cost of call data and the cost storage and execution. And I'm interested to see how this will change the ecosystem to have call data that's cheaper and more comparable, but the cost of storage and execution as it is on layer one. Another good example would be the new EVM opcodes introduced, specifically memory copy and transient storage. These aren't anything new that you couldn't really do before, but once again, they're an example of something that's now cheap to do, economically efficient to do, and we'll be very interested to see what happens with the appsum L2 in respect to these dot codes. For instance, transient storage can allow for much cheaper reintrancy locks, which can help secure smart contracts on L2.
00:29:37.606 - 00:30:17.090, Speaker D: And I'm really excited for that because previously you've had this significant trade off between the increased cost of computation and storage for doing reintrancy blocks, which would pose additional costs to users to provide a safer smart contract. But now there's only a very small cost to providing the safer reintrange seed block smart contract to users. And I'm hopeful that this will sort of help with help not completely extend the flow, but it'll help hopefully cut down on the number of attacks that we see on these smart contracts on arbitrum and other chains.
00:30:18.950 - 00:30:51.920, Speaker B: Yeah, there's a few places in the arbitrum core protocol contracts that could, in theory, make use of the transient storage drop code just to, like we said, do what we're doing now, but do it in a cheaper way. So that could be nice. I would just add the other evm change of kind of nerfing self destruct is also just, I think, one that most solidity developers will be happy about, because self destruct is a very annoying edge case. So yeah, that same benefit and same effect that takes place in layer one development will be on arbitrage. So that is good too.
00:30:54.210 - 00:31:26.310, Speaker A: Awesome. If you're interested in reading more about the blob economics, some of our scientists at offchain labs. Ed and Akaki wrote about it on our medium page. So you can check it out at the off chain labs medium page. Going along with what Daniel and Lee said, do you think there will be new types of applications developed because gas is cheaper?
00:31:28.890 - 00:32:23.180, Speaker B: It's possible, like we're sort of know, lower fees, higher, higher available throughput sort of just benefits everything, which is nice. If it does lead to new types of applications, it'll be the sorts of applications that are sort of very reliant on data because, yeah, there's good reason to believe that data itself, sort of the raw input data as opposed to execution, will get cheaper, certainly relatively cheaper than execution. So what sort of applications would that enable? The sorts of things that aren't necessarily defi focused. Certain types of gaming apps want to use a lot of data, but that becomes inconvenient on blockchains, historically, things like that. Yeah. So that'll be an interesting thing to see. But again, everything needs input data, so everything benefits to some extent, which is good.
00:32:25.390 - 00:32:48.260, Speaker A: All right, so we talked a little bit about the impact on the ecosystems. What about from a developer working on some of these changes? What were some of the things that you learned during this process? I know that the entire process was sort of years in the making. Anyone can take this.
00:32:51.030 - 00:33:19.100, Speaker D: I learned more cryptography in order to do a lot of this fancy blob stuff, and really in order to set up for future work on data availability, sampling and such, Ethereum used KZG commitments, which are this fancy cryptography that's pretty complicated, and it's been a lot of fun working with them and figuring out all sorts of elliptic curve magic and everything. Yeah.
00:33:26.660 - 00:33:33.360, Speaker A: How about you, Terrence? You were one of the initial contributors to 4844.
00:33:33.510 - 00:33:39.012, Speaker E: Yeah. So it's interesting if you look at how 4844 works in the beginning versus now.
00:33:39.066 - 00:33:39.332, Speaker B: Right.
00:33:39.386 - 00:34:00.888, Speaker E: We went through multiple iteration. I think one thing to point out was that before we started that, blob, sidecar and block, they're tightly coupled as one object. So when you're syncing to the node, you are not making separate requests for block and blob separately. You are just getting one big PPP object, and then you import them together.
00:34:00.974 - 00:34:01.368, Speaker B: Right.
00:34:01.454 - 00:34:10.088, Speaker E: So we went through a few iteration on that. In the beginning, they were coupled, then they were decoupled. Then they were coupled again, then they were decoupled.
00:34:10.104 - 00:34:10.236, Speaker B: Right.
00:34:10.258 - 00:34:34.464, Speaker E: So you may ask, why is that? Because there has been always these two contention forces that should they be coupled or not? I think at the end, we all agree that decouple is better because it allows more flexibility on the p two p side of things, that you can do more nicer things, and then you also save on bandwidth and stuff, but then it also opens up a lot of bug risk.
00:34:34.512 - 00:34:34.772, Speaker D: Right.
00:34:34.826 - 00:34:55.912, Speaker E: Because, for example, what happens if you got a block and you never receive a blob? Or what happens if you get a block and there are multiple blobs that's pointing in the same position and now you have this blob duplication, but you cannot slash based on that because that's now a slashing condition.
00:34:55.976 - 00:34:56.540, Speaker B: Right.
00:34:56.690 - 00:35:29.530, Speaker E: So at the end, we ended up adding this block headers object into the blob such that, okay, if today you create two value blob, you must have created two value block before and such that we can slash you. So there are a lot more iterations. So I guess if there's one thing I learned, I learned that networking is hard. If there's something that you think about when you're designing a protocol, you look at networking, you budget one month, but it probably will take. I think it probably will take more than one month.
00:35:34.380 - 00:35:37.850, Speaker A: Preston or Daniel, any other takes on this?
00:35:43.680 - 00:36:05.650, Speaker C: No. This one was a lot of fun. I would say, though I have enjoyed testing and reading about Daneb, I feel like the impact of this upgrade is. It was really exciting to do the merge, and I'm feeling a very similar excitement for Daneb for March overall. Really, I think it was a lot of fun.
00:36:13.060 - 00:36:50.380, Speaker A: Awesome. Yeah. I remember going to ETH Denver and in 2022, and Terrence was talking to Dankrod about this. It's been a pretty cool ride. And our last fork, Capella, was just for Ethereum withdrawals, which is very important for the ecosystem. But it wasn't like a big change like the merge. So Dankun is going to be exciting like the merge.
00:36:52.480 - 00:37:21.510, Speaker C: Yeah. And just to add on to what you said about Eat Denver in 2022, the group had already sort of a working prototype with the old design, of course, but these things take a long time. Right. This took two full years to go from a working prototype to now we're just a few weeks away from production, so it's nice to see it go over the finish line.
00:37:25.900 - 00:37:52.796, Speaker A: All right, so we talked about the impacts. Let's talk a little bit about the future. What's going to happen after Denkoon, and what can Denkoon help enable for the next hard fork? Let's start with the L1 sided. Terence Preston.
00:37:52.988 - 00:37:53.344, Speaker C: Sure.
00:37:53.382 - 00:38:20.948, Speaker E: I can take that. Yes. What's happening in electro, which is code name for the hard work after Daneb. So this stance are purely prison team. It doesn't reflect Ethereum because Ethereum has like twelve other teams and then they also have their researchers. So every team sort of have different opinion. But this is our opinion that we strongly support these following eips.
00:38:21.044 - 00:38:21.304, Speaker B: Right.
00:38:21.342 - 00:39:06.588, Speaker E: The first EIP is EIP 7002 and this one enable El triggerable adzit, which makes nicer for like a node operator for Igel layer rocket pool, such you can send your ad zit for a validator through El. So it allows more trustless staking and restaking as well. And then you have EIP 6110, which you can supply validated deposit on chain. So one trend you've noticed that we have your CL and El, right? So we're constantly making improvement to harness the communication between those two layers. And one is exit and one is deposit. So now you have basically cleaner deposit and you allow more like code health cleanup, even like faster deposit.
00:39:06.624 - 00:39:07.464, Speaker B: So that's nice, right?
00:39:07.502 - 00:39:39.250, Speaker E: And there's another one that you're moving the attestation committee index out of the attestation. So it allows more nicer aggregation form between the attestations and stuff. So I would say electro will be like a smaller hard fork, and this hard fork will be more focused on just more on El and CL interaction to make them cleaner and as well as more like to make the attestation aggregation better.
00:39:43.720 - 00:40:05.850, Speaker A: All right, awesome. And if you're interested on the PrISM team's perspective, we also have a little blog about the eips that we are aligned with on our off chain labs medium page on the arbitram side. Lee Daniel, what can we look forward?
00:40:10.320 - 00:40:52.584, Speaker D: So, yeah, I think it'll be interesting to see. Well, there's a few different, I think, areas of potential research based off of stancun. One of them is we've mentioned before that blobs are a fixed size and they're actually pretty large. One interesting area of research is if this won't matter for chains like arbitrum one and arbitrum nova. But for chains that don't post a lot of data, it might be pretty infrequent that they fill up a whole blob. And there's interesting ideas around. Can you sort of split up that blob data between different chains and do work based off of that? So I think there's interesting ideas like that.
00:40:52.584 - 00:41:13.970, Speaker D: There's other interesting ideas about how you can improve l three bash posting efficiency and maybe support blobs on L2, or maybe you post them via layer one. I think blobs are sort of a new tool in a developer's toolkit, and it'll be very interesting to see what we can build out then.
00:41:15.940 - 00:41:46.744, Speaker B: Yeah, there's also sort of even independent of the 4844 blob stuff, the research team has been thinking about how the sort of price of l one data is tracked on L2 and ways of sort of improving that system and making it more resilient to certain failure modes. So this could be. Yeah. With blobs, you essentially now have these two different components you need to track. I think those discussions will evolve in an interesting way. Yeah. I mean, that's at the lower level protocol side.
00:41:46.744 - 00:41:57.150, Speaker B: Again, on the developer side, hopefully now smart contract developers have these new tools they can use in the execution layer, so it'll be cool to see what's built with them.
00:41:59.360 - 00:42:05.170, Speaker A: How about bold and stylus? Can they take advantage of the changes that are coming?
00:42:08.180 - 00:42:22.550, Speaker B: In short, yes. I wouldn't say they're directly related, but they're certainly compatible. Right. Stylist. I'm kind of trying to think of maybe there's more of a direct connection, but in the case of.
00:42:23.000 - 00:42:32.650, Speaker D: Yeah, I think Stylus. I'm not sure about this, but I think there's plans to support transient storage and stylus just as it's supported in the EVM. So a very similar concept there.
00:42:33.980 - 00:42:44.300, Speaker B: Yeah, that makes sense. Right. So Stylus will be the sort of stylus Vm. You want to talk, maybe give a brief summary of what Stylus is for the people who don't know? Lee? Sure.
00:42:44.370 - 00:43:49.196, Speaker D: Yeah. So, Stylus is the ability to write contracts that you deployed on arbitrum in traditional programming languages, like Rest or C or C Plus plus. And there's basically two advantages here, which are that a lot of developers don't know Ethereum specific programming languages. The amount of developers who know an Ethereum language, like solidity is a drop in the bucket compared to the amount who know, say, c in industry standard. And the other advantage here is that these languages can be executed a lot faster, and faster execution means it can be cheaper execution, too. So computation in stylus smart contracts will be much faster and cheaper than computation in EVM smart contracts, but they'll run alongside EVM smart contracts in the same chain, so they'll be able to interact seamlessly. For instance, you could imagine an ERC 20 written in stylus traded on a normal Dex written in solidity that doesn't know anything about stylus, and that sort of seamless interaction.
00:43:49.196 - 00:43:56.644, Speaker D: I think is very key to the developer experience that Stylus is aiming to achieve. Yeah.
00:43:56.682 - 00:44:48.550, Speaker B: So in terms of the cheaper input data from Blob, stylus just sort of automatically benefits from that because it gets its data from the same place. And then, yeah, like Leo was saying, some of the changes might be brought, some of the execution level changes might be brought into the stylus VM as well, in the case of bold. So bold has to do with this is sort of a new sort of improved protocol for arbitrage validators to use for their sort of dispute fraud proof system. And one of the sort of nice things about, I would say the arbitrage protocol even now, but also with bold, is it doesn't really have to sort of know or care about the details of the execution layer. It can almost treat that as sort of a black box, more or less. That's this thing. It works how it does, and then we can just sort of modify this new challenge system.
00:44:48.550 - 00:45:05.930, Speaker B: The fact that those are kind of separate and disaggregated makes it nice and easy to reason about. But none of the execution level changes directly affect bold itself. So that's all still moving forward.
00:45:08.880 - 00:45:23.024, Speaker A: What about in terms of the dow? Are there any proposals for certain types of features that off chain labs is looking into? Thoughts on that?
00:45:23.222 - 00:46:25.728, Speaker B: Yeah. So nothing comes to mind that's directly related to the Dankoon upgrade, but maybe just more in general, since we're talking about governance, one thing that the folks at l two beat have been sort of updating and putting out information about their criteria for different roll ups or different l two s and sort of stages of decentralization. So they actually initiated this proposal that would. Well, they initiated this proposal that would sort of help arbitram one and nova kind of stay at or reach the next stage of decentralization. And the initial change that looks like will be proposed is a fairly simple change at this stage so that arbitram can sort, you know, they kind of change their criteria, but they suggest that, hey, if we just change this one thing, arbitram one can stay at stage one of decentralization instead of being demoted to stage zero. So, yeah, the change itself is fairly simple. It has to do with the signer threshold on one of the Security Council multi sigs.
00:46:25.728 - 00:47:00.530, Speaker B: If that's raised to a certain level, then we can consider it something so that'll go through. And I think more broadly, that's led to some interesting discussions about this topic moving forward, which aspects of the system, particularly the Security Council, can be sort of whether we want their powers to be limited over time, to be sort of nerfed so that it's more decentralized. I think there's interesting stuff. We have some thoughts there that we want to put out to the Dow, and, yeah, that'll be an interesting and important thing to track.
00:47:04.100 - 00:47:47.700, Speaker A: Awesome. Anyone else have any other takes on the future? What are you excited for? Otherwise, we'll move on to community questions. Okay, we're going to move on to some of the community questions, questions that the community might be interested in. So the first one is, what are some exact numbers on how low transaction fees will be on arbitrum after the Arbos Atlas upgrade and the Dankoon hard fork activation?
00:47:49.880 - 00:48:39.444, Speaker D: So this is a great question. A lot of people have answered this question, but basically, you can answer this question if you make various assumptions about how much blob data will be used. But the problem is that nobody really knows. It's kind of like asking if you have a. Yeah, I don't know. It's kind of like asking if I deploy a new L2 with, you know, if I deploy a new arbitram L2 that only does 1 million gas per second. How higher fees going to be? The answer is, it depends how many people are trying to use the chain at once.
00:48:39.444 - 00:49:45.564, Speaker D: And there's sort of a similar issue here where the arbitram transaction fees will depend on how many people are using Ethereum blob data at once. And various people have had done various estimates of this, but it's hard to know for certain, and it will probably change over time. For instance, not very many chains are going to be supporting EIP 48 44. Initially, a lot of chains are going to be like a week or more out. But in that first week, assuming that the Dow vote passes on time and arbitrum activates atlas, within that first week, there might not be as much bidding for data, and you might see vastly lower fees on arbitrum. But it might be the case that over time, that changes as additional L2 start utilizing this blob data, and the biding war ramps up for this data. So it's hard to see how it'll play out.
00:49:45.564 - 00:50:22.520, Speaker D: There's also other. There could be curveballs, like, for instance, we've seen inscriptions suddenly ramp up data costs on other chains, and it's unknown whether or not we'll see blob inscriptions or not. But if we see something like that, that could very easily drastically change the prices. So I think it remains to be seen. It's really the only fair answer. There's various people's estimates. There's even a prediction market on what EAP 48 44 will reduce blob costs or will reduce data costs by.
00:50:22.520 - 00:50:30.890, Speaker D: But I think the answer is nobody can really give you an exact number and be 100% correct about it.
00:50:35.340 - 00:50:57.704, Speaker A: Got you. Thanks for that, Lee. Let's see the next question. Why is it called Dankoon? I guess it's not really accurate to say like, the hard fork is called Dankun. It's actually two forks, but. Preston. Terrence.
00:50:57.852 - 00:51:40.028, Speaker C: Yeah, I'll start. That's true. Right. So we have the El execution layer and the Cl, this consensus layer. And before these were merged together, we had our own naming convention for upgrades. And we kept that afterwards as well, because there may be situations where we want to upgrade one but not the other. And then, I don't know, people like their naming conventions where the consensus layer is named after certain stars in the universe and the El is named after the upgrades, are named after cool places.
00:51:40.028 - 00:52:08.600, Speaker C: I guess stars are cool places, too, but these are cool places on Earth. And then we have this habit of when they're both upgrading at the same time. We just sort of make up a new word and mush them together. So we have Denkun for this one and then for future ones. They could be pretty silly sometimes, but it is the way it is. We like to have a little funny.
00:52:09.100 - 00:52:22.830, Speaker A: Thanks, Preston. We answered a couple of these, but just as a reminder for our audience, if you're a node runner on ethereum l one, do you need to do anything?
00:52:26.800 - 00:53:06.170, Speaker E: Yeah. So you need to upgrade your nodes, right? So if you're a node runner, you need to upgrade every pieces of software that basically touches your node, allow you to validate if today you run a validator, which means that you need to update your Cl node, El node, mvv boost and validator. And that is if you're outsourcing your block production to a builder, make sure you upgrade everything. Even better. If you have a testnet set up, you can practicing upgrading on a testnet first.
00:53:10.400 - 00:53:11.036, Speaker B: All right.
00:53:11.138 - 00:53:19.840, Speaker A: And for the Arbos Atlas upgrade, where can people vote? If they have arb tokens?
00:53:21.460 - 00:53:43.924, Speaker B: Yeah, if they have arb tokens or if they are delegated votes by someone else with Arb tokens, they can vote. The easiest place to vote is on tally. Tally.com or Tally Xyz, I think. But yeah, so they can vote there. And if on the off chance tally is down, they can vote by directly accessing the smart contracts. But ideally tally.
00:53:43.924 - 00:53:44.570, Speaker B: Yeah.
00:53:46.700 - 00:53:54.350, Speaker A: Sounds good. This is a fun question for the Prism team. What happened to our ascii art.
00:53:58.400 - 00:53:59.064, Speaker E: POTUS.
00:53:59.112 - 00:53:59.884, Speaker C: Supposed to do it?
00:53:59.922 - 00:54:16.284, Speaker E: He hasn't done it. It has been part of his p zero for the whole year. I think in order to enforce that, we probably need like a spec test from the consensus layer spec saying that if today you don't have this ASIC R, the spec test will fail. Your CICD will be read and you cannot hard fork.
00:54:16.332 - 00:54:22.420, Speaker B: So yeah, you should probably have a dow vote as well. Just to be safe.
00:54:24.280 - 00:54:27.910, Speaker C: I think we need an in protocol proof of ascii art.
00:54:31.320 - 00:54:32.070, Speaker B: Awesome.
00:54:33.320 - 00:54:45.390, Speaker A: Well, that's all the community questions that I have. Panelists, is there anything you'd still like to here before we close our call?
00:54:51.680 - 00:54:54.780, Speaker B: Good. I think we covered it. Yes. Awesome.
00:54:54.850 - 00:55:45.820, Speaker A: So, as a reminder to our audience, the Ethereum upgrade Deng Koon is happening March 13. Prism users will need to upgrade to the v five version, and you'll need to upgrade all of those other things that Terrence mentioned. The Arbos Atlas upgrade is ready to be voted on, and we'll take advantage of these new features on Ethereum l one as soon as they activate. So once it activates, as a user, you may see significant decreases in gas costs. I'd like to thank everyone for tuning in and our awesome panel of speakers. All right, ciao.
00:55:46.160 - 00:55:50.280, Speaker B: Thank you. Bye. Thanks all. Bye.
