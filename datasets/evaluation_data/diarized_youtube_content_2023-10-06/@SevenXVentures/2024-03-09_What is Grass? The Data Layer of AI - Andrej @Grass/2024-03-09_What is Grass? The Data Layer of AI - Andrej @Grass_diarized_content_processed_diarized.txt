00:00:00.170 - 00:00:27.090, Speaker A: A grass node running on one of their devices. Just one, two, three. Not going to make it. All right, so grass data layer of AI, what does that mean? You've got two ingredients across any AI model, across any AI stack that are just always going to be there, that are your only two inputs. They're data and compute. Lots of focus has been on compute recently. We focus on the data side, which we feel is underrepresented.
00:00:27.090 - 00:01:23.414, Speaker A: One of the reasons I feel that's more or less underrepresented, especially in the intersection of crypto and AI, is because unlike compute data scales heterogeneously compute will scale homogeneously. You can have a bunch of identical GPUs and you'll scale your compute, whereas when it comes to data itself, you really have to care about things like the quality of data, what exactly you're feeding your model, and what you're actually trying to train in the first place. We kind of started this protocol because we just started to hate what the Internet's become. The entire thing or all of web two became just value extraction from data that's being provided by people all over the Internet. And we felt like on top of that, the way that data was being provisioned to all kinds of machine learning, ad verification, those types of use cases was just overall unethical. A lot of people don't realize this, though. When you're scraping the web, you can't just do it out of like an AWS box.
00:01:23.414 - 00:02:03.406, Speaker A: You can't just crawl the web out of a gcp server. You have to route all your web requests through hundreds of thousands of nodes in residential networks. Reason being, every time you visit a website, they check things like your device fingerprint, they check things like what network you're on and the quality of that network. And if they detect a crawler or a scraper, they'll think you're either their competitor or you're someone that's going to just try to take their data and monetize it out from under them. So they'll usually block it anyway. That's why we need grass. We have almost a million nodes right now, and I'm not talking l one metrics where we're like, oh, a million active wallets.
00:02:03.406 - 00:02:43.294, Speaker A: We actually have 10 million accounts. We're almost at a million active nodes right now on a daily basis, on individual IP addresses, on individual device fingerprints. What these nodes are doing are essentially leveraging latent resources on people's networks and on people's devices, and deploying crawling and scraping agents across the Internet using 0.1% of our network, we actually managed to crawl the entirety of medium. That's about 50 million articles. We've scraped many different subreddits, and we're also scraping news on a daily basis. The idea here is to build a data layer on top of which any sort of AI company or AI protocol can be built.
00:02:43.294 - 00:03:43.954, Speaker A: As many of you know, I guess a lot of people here have sort of an AI background, like Chinchilla paper a few years ago showed us that as you scale, the number of parameters in a model, tokens or the number of tokens that go into that model need to scale linearly. Otherwise you're not compute optimal. When you're not compute optimal, you're just wasting a lot of money on electricity. You're better off getting more and better data than you are getting more compute and trying to scale in that fashion. And yeah, beyond just scraping the web, we also do a lot of cleaning, preprocessing, and at the moment we're actually working on embeding models that can operate on the edge and vectorize things before they come back to our database. I think I accidentally already talked about some of this stuff, but it's a pretty cool slide and our designer did a great job with the illustration. The idea here though, is you guys have been seeing a lot of the Internet starting to become gatekeeped ever since chat GPT came out three years ago.
00:03:43.954 - 00:04:12.030, Speaker A: A lot of this stuff seemed very futuristic and kind of dystopian. You see Reddit striking a deal with Google recently for a really cheap price. And the reason being Google has this centralized power of being able to give them SEO priority. Why isn't OpenAI suing? Or, sorry, why isn't your time suing Google? But they are suing OpenAI. It's because Google gives them SEO priorities. So you've got like two or three very large companies that kind of control the access to data on the Internet. And this is something that we want to usurp.
00:04:12.030 - 00:04:51.466, Speaker A: Most web servers. Most websites in the world right now, especially now, have been following the footsteps of ecommerce and ad tech from like ten years ago where they actually began honeypotting data. They detect a crawler and they know that if they block the crawler, they'll just deploy that same crawler on a different device and try to scrape you again. So they said, okay, why don't we start honeypotting data sets? We scraped the entirety of medium because medium CEO said, we're going to start honeypotting our data and we're going to try poisoning all the AI models that are trained on our data set. So, yeah, anyway, in response to that interview, it took us about three or four days to scrape their entire website. What's up, man?
00:04:51.568 - 00:04:53.786, Speaker B: I heard a lot about poisoning, how.
00:04:53.808 - 00:05:19.746, Speaker A: You would do it two ways. Two ways. One, you can poison your data on the web server. That's honey potting, blocking, things like that. Usually the most common form of data poisoning on the web server side is throwing in a bunch of fake links. So if someone's trying to crawl your entire website, they'll go through a rabbit hole of like a million links that just give you zero data and waste a ton of compute and resources and energy on your side.
00:05:19.928 - 00:05:24.526, Speaker C: Also, there's something called machine unlearning. So your data can be poisoned.
00:05:24.558 - 00:05:25.220, Speaker A: That's right.
00:05:26.390 - 00:05:27.690, Speaker C: We forgot the parameter.
00:05:27.790 - 00:05:29.240, Speaker A: That's right. Yeah.
00:05:32.330 - 00:05:34.040, Speaker B: You can call back later.
00:05:34.490 - 00:06:08.770, Speaker A: I mean, if a news site is trying to poison you with a bunch of links that are invisible to normal people, but that lead to fake news, you're kind of screwing yourself. Right? That's like way too much work to build those types of semantic trees to verify whether. Absolutely, yeah. And then there's actually a second method of data poisoning, and this is the one that I think we have. A thesis is going to be huge in the next few years, if not sooner. Right now, if you're an open source data set provider, and most of the static data right now is open source because the real value comes from real time data. So they provide that for free.
00:06:08.770 - 00:06:37.994, Speaker A: If you're an open source data provider, it's very difficult for you to monetize that data. However, there are a lot of people called advertisers who've already seen this game play out in the search engine world. And LLMs are 100% going to follow in the footsteps of search engines. Because what do you use an LLM for? What do you use a search engine for? It's the same thing. You're asking a question, you get an answer. There's some sort of web index or some sort of knowledge graph and it's referencing that to give you that answer. Right? You have this data set that some poor guy went and put together.
00:06:37.994 - 00:07:08.870, Speaker A: You've got like Petabytes of whatever it is, Reddit, Twitter, who knows? Whatever this guy scraped, you go up to him and you say, hey, I work for this brand. I work for Patagonia. Can you throw in 10,000 sentences and like, the reviews websites in your data set that just says Patagonia is the best place to buy a vest. Okay, sure. They'll do it. And people will take money to do that because at face value it's not very harmful. It becomes harmful at scale, but at scale inevitably that's what happens, right? Yeah.
00:07:08.870 - 00:07:18.274, Speaker A: And it's exactly the same thing that happens in search engines. That's the same thing as SEO paying for SEO. You're not going to go and show up as a sponsor.
00:07:18.402 - 00:07:19.970, Speaker B: Gray hat or black hat?
00:07:20.130 - 00:07:20.950, Speaker A: What we're doing?
00:07:21.020 - 00:07:21.640, Speaker B: No.
00:07:23.370 - 00:07:38.094, Speaker A: Okay, well I'll answer what we're doing. It's white hat. The method I just talked about. That's gray hat. I mean, advertisers have always been gray hat, right? It's one of those things where there's too many players that depend on ad tech practices for that to ever fail. Right. Despite how unethical it may be.
00:07:38.094 - 00:08:14.426, Speaker A: Now that's actually something that we're working on solving as well. We're generating a proof every time a web request is sent we record that web session and we publish that proof on chain. There's some level of ZKTLs stuff going on that's in the works. And the idea there is we can augment an open source data set that says this is the scraping action that happened on this day on this web server and this is the IP address that it came out of. And that way if anyone ever tries to go and mess around with that, we've got this hash that's stored in like a decentralized database that'll prove like, hey, this has been tampered with. So anyway, yeah, two levels of data poisoning. We try to solve both of them.
00:08:14.426 - 00:08:36.740, Speaker A: Yeah, thanks. And how's it going? 1000% user growth in the past three months. For the crypto natives out there that means ten x nearly a million nodes. Keep saying that but I just think it's sick. We're downloaded in over 190 countries and at the moment we're scraping over a terabyte a day. That's in our beta testing. We expect that to be 50 to 100 x a few months from now.
00:08:39.590 - 00:08:41.300, Speaker C: How do you define it exactly?
00:08:43.110 - 00:09:05.910, Speaker A: So it's an individual device on an individual IP address like device fingerprint and IP address. So there are some people that think they're smart and they installed hundreds or thousands of nodes on huge servers. We can track that. They're not getting anything. Future vision. We want to vertically integrate the entire data stack. There's no reason for this to exist across many different protocols.
00:09:05.910 - 00:09:28.862, Speaker A: If it can all. What's up man? Where are you storing the data? Yeah, so hugging face gives us free data? Yeah, up to ten terabytes a day. But for some of the more proprietary data sets, we're self hosting a bunch of MongoDB and we're talking to a few of the decentralized data providers. So you guys do storage? Yeah, hit me up. All right. Yeah. We're moving into tagging and labeling.
00:09:28.862 - 00:09:56.898, Speaker A: For those of you that have installed the nodes, you can actually see there's a tab on the dashboard that says data labeling. The idea there is human feedback for reinforcement learning. Any type of recursive use of LLMs is going to propagate error. So when you're going to have an LLM teach itself on synthetic data, you need humans to tag it. We're training a native AI agent actually to assist in scraping. For those of you who have done web scraping in the past, it freaking sucks to write different python scripts for every single website. When the website changes, you have to update the Python scripts.
00:09:56.898 - 00:10:18.642, Speaker A: Right now we're doing that manually, but we're also training a seven B model that can live on the edge and actually just convert any raw HTML into a structured data set. And then we shove that into an embedding model and boom, you've got what you need to train an AI model pursuing data. Providence. I actually answered that guy's question over there. That guy? Yeah. So I'm not going to go into that a bit more. And then we need a lot of throughput for this.
00:10:18.642 - 00:10:39.340, Speaker A: We're storing a lot of things on chain. You guys remember the Jupiter Airdrop? About a million people claiming an airdrop congested Solana. That's about a 10th of what we'd want to do daily. So we have a few scaling ideas in mind and you'll see if you follow us on Twitter in the next like two weeks, we'll release that. But huge fans of Solana, nevertheless. Anyways, any other questions?
00:10:41.630 - 00:10:46.602, Speaker D: How do you ensure that the human staff provides the data?
00:10:46.736 - 00:10:48.650, Speaker A: Oh, yeah, we're partnering with Worldcorn.
00:10:49.070 - 00:10:53.466, Speaker D: I see. How do you ensure that they provide high quality?
00:10:53.648 - 00:11:15.246, Speaker A: Yeah, that's a good question. We're actually working on a consensus mechanism for this. The idea there is, we actually have a ranking for people's contributions currently that exists on the dashboard. Most people right now, they download these notes because it's the most passive way to get exposure to AI and data and whatever. Right? But despite it being passive, a lot of people actually end up being quite active with it. They share the referral links everywhere. They're constantly refreshing their dashboards.
00:11:15.246 - 00:11:38.218, Speaker A: We can see these things on our side, everyone's sort of assigned a tier based on their level of activity. So if they've installed nodes on many different networks or if they've referred a lot of people, they have a higher tier and being able to get zeroed out when you're at that very high tier becomes quite risky. So you're incentivizing people to do their best in terms of tagging and labeling and stuff like that. And then in terms of the. Sorry, go ahead.
00:11:38.304 - 00:11:50.750, Speaker D: When you rank those people, we reach white tier because some people can do a lot of labeling but with very low quality.
00:11:50.900 - 00:11:52.720, Speaker A: That's true, yes.
00:11:53.410 - 00:12:15.480, Speaker D: Actually there are already many web scroll database like common cross. It has a lot of data in it and the current very good performing ROM are actually trained on the filter common crawl, and they hire a lot of people.
00:12:17.210 - 00:12:18.762, Speaker A: Common crawl sucks. Yeah.
00:12:18.896 - 00:12:30.330, Speaker D: So actually the current TV level data is sufficient for the common language model training. But the really problem is there are a lot of noise.
00:12:31.250 - 00:12:36.398, Speaker A: Yeah, that's true. And that's why we want to add human reinforcement learning. Right.
00:12:36.564 - 00:12:40.030, Speaker D: But how would you ensure that human is powerful?
00:12:41.010 - 00:13:13.194, Speaker A: Yeah. So we're working on a consensus mechanism for this that'll be public soon. But there is a consensus model to make sure that you have a score, reputation score based on how often you're in the majority. And we have ways to make sure that people can't game this in Sibyla to alter these reputation scores and. Yeah, then there's also a lot of crossover. So if you give it to the same job, the same tagging job to many different people, it becomes a lot easier at scale, just based on the law of large numbers to determine whether or not someone's doing their best. Yeah.
00:13:13.232 - 00:13:14.762, Speaker C: So there's some people who have already.
00:13:14.816 - 00:13:19.770, Speaker A: Run incentivized projects and as long as you incentivize convergence, it actually works.
00:13:19.840 - 00:13:20.460, Speaker D: Really?
00:13:20.910 - 00:13:21.660, Speaker A: Yeah.
00:13:22.270 - 00:13:25.754, Speaker C: So is data source open once you.
00:13:25.792 - 00:13:33.210, Speaker A: Go back, if you would like access, hit me up. It's going to be open source soon, but for now we're keeping it closed source just as we scale.
00:13:33.290 - 00:13:36.542, Speaker C: Basically, once people contribute this data.
00:13:36.596 - 00:13:37.200, Speaker D: Right.
00:13:38.130 - 00:13:40.370, Speaker C: Eventually it will be open for everyone.
00:13:40.520 - 00:14:12.362, Speaker A: Yeah. So the idea is going to be we're going to have a very large pretraining corpus that's available for anyone to trade a model on. If you want real time data, usually the use case for this, the very obvious one, is inference and in some cases fine tuning for up to date events. Then that will be a more of a paid feature. Yeah. Good. Are there any plans for multimodality multimodal.
00:14:12.362 - 00:14:33.754, Speaker A: Yeah. We're scraping about 200 million YouTube links right now, actually, today. Yeah. That's why we're looking at a lot of different ones. Hopefully yours is sick. All right, the reason we haven't landed on one yet is because of that exact problem. Sweet.
00:14:33.754 - 00:14:35.620, Speaker A: All right, I think we're going. Yeah.
