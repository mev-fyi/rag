00:00:00.170 - 00:01:19.750, Speaker A: Let me, let me just introduce myself a bit and what avail does and how it all started. So we started avail as like within Polygon in, in 2021 and I joined there as a researcher and became one of the lead researchers there in Polygon and we built this for around two years before spinning it out of polygon into a separate entity. So we now have avail as an independent, credibility neutral kind of DA layer, right? So the main agenda of this talk today is that we want to go through, because it is a new trends summit, we want to go through some of the new trends that we are seeing in blockchain and of course talk about avail and how that enables some of these newer trends that we are seeing. So one of the main trends, and this needs no introduction really, is how we are seeing a recent trends in roll ups. And you must have noticed that Ethereum based roll ups have taken the center stage today. Almost all the activity that happens like around four x of the activity of Ethereum happens on roll ups today. Right? And they have taken the center stage because we have realized that Ethereum cannot scale.
00:01:19.750 - 00:02:40.506, Speaker A: Computational scaling is not possible from within the Ethereum and that's why we are creating roll ups to help scale and take execution off chain while proving transactions on chain. Now, one of the main ways to see roll ups as implemented on Ethereum is that of a validating bridge, which means that the transactions that are submitted to Ethereum are submitted to this bridge. The execution is taken off chain. They are done in a provable manner, either in optimistic manner where it is secured by fraud proofs or using validity proofs in the ZK roll ups. And then finally these assets are then bridged back onto Ethereum. And as this trend moves forward, we have seen that blockchain constructions are inherently becoming modular, right? And we know that the modular word is being thrown around quite a lot in different contexts, sometimes abused the terms as many of these terms in this ecosystem. But what it means to be modular in this context is how the functionality of blockchains are being broken into specialized layers, right? So previously it was a monolithic chain where all the constructions like the settlement, the data availability and the execution were all done in a single layer.
00:02:40.506 - 00:03:38.050, Speaker A: Now what we do is we take these layers and try to separate them out into specific layers thus enabling each one of them to do better in their own terms. The roll ups, they only scale the executional part and we are here to scale the data availability part, right? While Ethereum remains one of the settlement layers of the world. Now, what do we need for these roll ups to thrive? All of these roll ups are hungry for data. If you ask any of the leading DA or the roll up experts, they would tell you that 70% to 90% of the costs that users pay on roll ups are because of data availability charges today. And that's where we come in. We give this bold statement and a new trend which we say that every base layer is going to be a DA layer. That is one of the hypothesis that we have.
00:03:38.050 - 00:04:16.330, Speaker A: And this is where avail comes. So what is avail? Right? So it's a modular base layer. It provides only raw block space to modular chains on top. So it does something called sequencing, which is some of the things that you can use a base DA layer for. For example, in Arbitrum, if you want to force transactions, you can send the transaction directly to Ethereum. So then Ethereum acts as your ordering layer, although you can send a transaction directly to its sequencer, at which point the sequencing happens at Arbitrum sequencer level. So the DA can provide sequencing.
00:04:16.330 - 00:05:00.478, Speaker A: It also packages the data in a manner in which it is provably available and it then publishes this data. So it segregates all the transactional data on a per application basis so that multiple of these applications can send their transactions within the same block of a DA layer. So in Avail we have a robust validator set that sign off these published blocks. But they do not execute transactions, they do not interpret transactions to avail. All the transactions are basically data blobs. And each roll up built on top act as the interpretation layer. They act as the execution layer who take these transactions, interpret them.
00:05:00.478 - 00:06:08.002, Speaker A: And it can be anything we don't want to be for a particular execution engine. It can be Ethereum virtual machine, a Solana virtual machine, a customized ZK VM and so on. And that is where innovations will thrive, right? With new VMs and application environments coming in. Now, one of the main features that we have and main differentiating factor considering other chains that are there, we have a light client which leverages KZG polynomial commitments, erasure coding and data availability sampling to create a verification that the data is available without downloading the entire block. And that is one of the superpowers of DA layers, right? You do not need to download all the data to know that whether it is available or not. And that is one of the classic differences to what Ethereum offers today. Now, this allows a massive and vibrant ecosystem to be built on top because different types of roll ups can come in, they can adapt to their unique needs.
00:06:08.002 - 00:07:01.346, Speaker A: We can create something like validiums, we can create L three S, L four S, whatever. There can be state validating bridges amongst the particular roll ups that are sitting on top of the same DA layer like avail. And they can now share security. So Bridging becomes the strongest type of Bridging, which is not something like a liquidity bridge or light client based bridge, but it's a state verifying bridge, one of the strongest types of bridges that can happen and hence the applications do not need to bootstrap their own validator set. They can come in, use the security of avail and then build applications on top. Now, coming to the again, I won't go through in too much detail, but essentially speaking, the avail block proposal engine works as follows. You take the original data, you do erasure coding on top.
00:07:01.346 - 00:07:43.330, Speaker A: This is to create redundancy in the data so that it's very hard for a block proposer to hide parts of the data. And that is extremely important in a DA sense. And this is one of the core differences from data storage, right? So sometimes people confuse that why don't you just keep the data in something like Filecoin or RV or something like that. The key difference is that you do not trust the block publisher. So all the roll up block publishers can try to hide data and that's why you create redundancy. By creating redundancy you then commit to that redundant amount of information inside the header. This header is validated across the board by all the validators and then the block gets published.
00:07:43.330 - 00:08:23.674, Speaker A: In terms of block production, every application sends uniquely to a particular app ID. The data blobs are chunked and a so called matrix is populated. Because we need a 2D encoding structure, we generate the commitments row wise. One of the key properties of how we are able to do this so efficiently is because there is a homophobic property of KZG polynomial commitments. Sorry. And that allows us to not have to erasure code it code the data all the time, but just erasure code the commitments. I will tell you just what that means and the matrix can be adjusted to demand.
00:08:23.674 - 00:08:56.502, Speaker A: And that is one of the other superpowers of avail. And then the header contains the commitments, the application index and so on and so forth. So this is what I meant by the matrix structure. Each of these cells here are basically scalar fields of the Blstl 381 field. They are taken together to create commitments row wise. These commitments can be erasure coded. The homomorphic property allows you to just compute C one to CN erasure code C one to CN to produce C n plus one to two N.
00:08:56.502 - 00:10:07.374, Speaker A: And then you actually know without having to extend all the column data by themselves in the base layer stack. There are some key trends that I want to highlight just not going through what just avail does, right? One of the key things to understand here in terms of block production is that there is a verifiability finality dilemma in consensus systems. In blockchains, what that means is you cannot have finality guarantees and liveness guarantees at the same time. And that is why we use two engines, much like how Ethereum changed post merge, right? Post merge you have finality delayed and liveness always coming in front. So we have a Babe Grandpa kind of a consensus where Babe is the block production engine which is VRF blaze block leader selection implemented. Inside there is Grandpa, which acts as a finality gadget, which doesn't finalize just individual blocks block by block, but finalizes entire chains. And this is tolerant against network partitions and nodes going down, multiple nodes going down and that comes with the resiliency of the blockchain system.
00:10:07.374 - 00:11:18.150, Speaker A: You must have seen in the recent past things like ethereum the finality got stuck but liveness continued. And that is one of the very good things to happen in blockchains where it doesn't matter if a few nodes are down or the clients do not agree within themselves, but the blockchain must go on. The other thing I want to talk about is how nominated proof of stake kind of is a bit of a different from delegated proof of stake and how delegated proof of stake leads to stake centralization. So we must have seen that most networks where they use a DPOs, there is a large stake and that large stake is with the expectation for a large power which in turn gives them large rewards. And this leads to a cycle which leads to more and more stake centralization. We want to avoid that and that's why we choose nominated proof of stake where we use fragment elections to have something like a proportional justified representation of validators before choosing them into the active validator set. And that is one of the more reasons why we can use something like say 1000 validators in the active set and not like in a waiting list somewhere.
00:11:18.150 - 00:12:33.738, Speaker A: The other thing is about light client networks. So the light clients, what they do is they sample parts of the data. This light client network creates a peer to peer to peer overlay network on top of the base layer and that allows them to sample and keep small chunks of the data which together keeps the replica of the block in a redundant manner. And what that means is even if a validator goes rogue or supermajority of the validators, just think about it, supermajority of a blockchain network wants to suppress parts of the data, you should still be able to be able to know whether the data is available or not. And that is a big guarantee and that is a trend shift that we want to see in this community, right? So what that means is the light clients maintain a DHT of the samples and the cell level proofs are kept. Anyone can run the network, sample it, verify and then assert by themselves whether the data is available or not and not rely on some other network. And the key points to highlight here is that there is no need to trust the validator set for the availability of data.
00:12:33.738 - 00:13:12.278, Speaker A: Even if the supermajority of availed validators want to suppress, you will be able to catch it by using a small amount of samples. Once enough light clients have sampled, the data remains available on the DHT. Think of it like a file, once downloaded, can be uploaded to a torrent network and stays available there even if the server goes down. Think of that as a good mental model. There is no need to rely on centralized RPC providers. That is another trend shift that we want to see in blockchains where we do not rely on RPC providers for verifiable data. We actually download and verify it ourselves.
00:13:12.278 - 00:14:47.062, Speaker A: And then Wallets can verify both execution and data availability, correctness? Without relying on crypto economic guarantees. One of the things which is thrown around a lot is the word security, right? And sometimes hidden under the carpet is kind of what do we mean by security? Because most of the security that we get from existing blockchain systems today is crypto economic, right? So this is one of the things where we want to see a change, where the security is not just crypto economic, but cryptographic and crypto economic. And one of the other things that people talk about a lot is how KZG is inefficient and how it cannot scale and things like that. We have tested this with very high block sizes, something like say 128 MB. And you can see that this is well within our block times and block verification is still extremely fast. This is without any GPU optimizations and so on. Now, what are the things that will be built on top? This is good that there's a base layer with such properties, but how do people use it? So the things that can be built on top are things like sovereign roll ups, validiums optimistic chains, app specific chains, something like general purpose chains, the L three S, L four S, whatever, right? And what does it mean to have a validium? In this sense, a roll up today can delegate its DA not to ethereum, but to an off chain DA layer like avail.
00:14:47.062 - 00:15:54.526, Speaker A: And then the avail attestation Bridge provides an attestation on ethereum that the data is actually available. So then the L2 only needs to give fraud proofs or validity proofs into ethereum and not give the entire data that it does. So in this way, the most costly affair, which is 70% to 90% as I described, can be offloaded to another layer because it takes around $1300 per MB of data to be published on ethereum today. And similarly for healthy constructions. Also, in terms of sovereign roll ups, I think one of the things that we see today, but that is being questioned and that is a good thing that most roll ups are implemented as like a contract on L One, which verifies the execution proofs. But with that comes like either it's hard to upgrade or there is a multisig protecting it, there is inability to fork it out unless the L One forks it and so on and so forth. We want to have sovereign roll ups that do verification instead on the light clients, on the user wallets, on the user level.
00:15:54.526 - 00:16:38.838, Speaker A: And that is why social consensus can be just upgrading your nodes and so on. And this opens up a space for a lot of different things. You can have ope stacks, ZK stacks, you can have base sequencers, sequencers, provers, decentralized, centralized, whatever. In terms of where we are at, I think we are right now running our second long running testnet called the Carte testnet. We have around 100 external validators running as of today. By the end of something like next month we will have around 5000 plus light clients, hopefully. And around one of the upgrades that we are doing right now is that there will be a two X improvement in commitment generation and much more in terms of verification.
00:16:38.838 - 00:17:21.322, Speaker A: We want to launch an incentivized testnet next quarter and hopefully by the end of the year or early next year we will have a main net. There are many, many future optimizations that I don't want to go into in the interest of time. There are KZG multi proofs where we can optimize commitments. There are Snark based light client bridges that we are working on. We want to change how the validators confirm and produce blocks today. We want to do much more in the mempool level and so on and so forth. I think one of the key takeaways from this talk should be that because this is a new trend summit and we should of course talk about some of the new trends that are happening in this space.
00:17:21.322 - 00:17:48.042, Speaker A: The blockchains are going modular. Avail creates a fundamental base layer for this ecosystem. The community is right now building lots of different specialized solutions to tackle problems across the stack. We are trying to do better at the base layer. We want to avoid some of the dilemmas, some of the core problems inside consensus systems. We also want to have better stake distribution and so on. We want to give DA guarantees.
00:17:48.042 - 00:18:17.574, Speaker A: But keep in mind, even if a whale goes down, it should not be a problem. Database sampling is a superpower about how to scale blocks and verify locally. We want to avoid dependence on centralized gateways and KZG is not the bottleneck. There is much better ways to scale. Here is our project details. If you want to take a look at it, feel free to reach out and let's have a chat afterwards. That's it.
00:18:17.574 - 00:18:17.860, Speaker A: Thank you.
