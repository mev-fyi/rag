00:00:00.410 - 00:00:19.070, Speaker A: Okay, cool, guys. Yeah. I'm Carter. I'm the CEO of QED. Yeah. So today we're going to be talking a little bit about, you know, how QED's state model works and how we're able to horizontally scale our blockchain. So just a little bit about me really briefly, like so I come more from the security and reverse engineering community.
00:00:19.070 - 00:00:45.590, Speaker A: I also ran proof of stake infrastructure for a bunch of, like, POS chains. Like, specifically us. We were top 21 block producer for quite a few years. Processed like 150,000,000 transactions, unique transactions in production. So, yeah, we've been building blockchain infrastructure as well. I also built Mod P, which is like a minecraft pocket edition, like, mobile version of minecraft modding engine. We had over 150,000 developers in our community and over 20 million installs across Android and iOS.
00:00:45.590 - 00:01:22.834, Speaker A: So touched a little bit in the Web two ecosystem with that. But yeah, let's talk about serial scaling and horizontal scaling and parallel processing in general as far as it goes with the L2 space. I'm sure a lot of you guys here are familiar with L2 beat, probably. Yeah. So right now if we look at L2 S, I mean, we are able to process more transactions than Ethereum alone could, but it's not some exponential growth curve. Like, we're not seeing like, oh, suddenly all of our blockchain scaling problems are solved. I can build Twitter on ethereum.
00:01:22.834 - 00:02:41.742, Speaker A: Right? This has not happened. And we're even like, low one digits. And you can probably also see a lot of these guys are optimistic roles that do not have fraud proofs, which is a little bit sketchy, but yeah, so with non ZK projects, it makes a lot of sense to have serial execution because every node needs to check the results of every other node. And if one Ethereum node detects that transaction was like, let's say, failed report has a certain state, if another Ethereum node wants to know that this state should be as it is, they have to actually execute those transactions. So if you want to build an l one where you have lots and lots of people running full nodes, then the actual sort of requirements to run the full node needs to be sufficiently low so that you can have lots of people running full nodes. Because as soon as you try to even if you build a parallel state model with an L one, let's say I need a whole data center of computers to run all of the parallel transactions, right? Not many people would run full nodes in that situation because it would be prohibitively costly. Right? So there's this like, with non ZK L ones, there's this big trade off where you don't even want the performance to be that fast because then it makes it harder to run a full node and then you become less decentralized, which is like the whole point why we did this in the first place.
00:02:41.742 - 00:03:17.974, Speaker A: Right? So adding more nodes doesn't increase TPS. It actually kind of lowers it a little bit because we have to make sure that everyone can keep up. But yeah, the reason why blockchains are serial, I'm sure most of the people here are kind of more familiar with this. Like, we have race conditions, right? I have two transactions that try to access the same state slot. Like, you can imagine if I send money to two different people and then someone also sends money to me at the same time. The rights that we're making to my balances in an ERC 20 contract are not atomic. So each of these transactions, if we run them in serial, it will operate correctly.
00:03:17.974 - 00:04:03.286, Speaker A: But if we run them in parallel, each of them have different results about what the end contract state should be. So, yeah, because of this raise condition, you can't really parallelize. Some projects have tried to do workarounds, basically checking to see roughly which state slots each transaction is doing and then later so first I execute them all in parallel, and then I see if there's any conflicts and then retry again. There have been a few attempts in this area. You could also try to use some sort of modern Acid database principles to try to do it, but no one's really figured it out. It hasn't really worked. But then there's, of course, the questions like, can we do better than this? Because right now, the limiting factor on blockchain uses is just that transaction space is scarce.
00:04:03.286 - 00:04:55.654, Speaker A: And that's why transactions are expensive, right? Like biding for gas is the reason why you have to pay. So is there any better that we can do than this? And the answer is yes. We have a new state model that we use on QED. Specifically, for each user, we have a separate contract state tree for the given user. So you can imagine if there's a token contract, every user will have a separate state tree for that token contract. And then we have a tree which contains specifically the hash of Verifier data for a bunch of zero knowledge circuits that are all allowed to authenticate updates to each of the contract trees. And then as a user, when I'm transacting, I'm able to read from the other users contract state treats as they were at the end of the previous block.
00:04:55.654 - 00:05:29.030, Speaker A: So when they're already finalized. But I'm only able to write to my own. So you can imagine that a transfer looks something like, I burn X number of tokens in Tony's name. And then in the next block, Tony can prove that, oh, I've burned these tokens in his name, and he can now mint them for himself. So we also have multi user interactions. So we have a thing called covenants. The way that works is essentially I say, okay, I burn these tokens for you, and if you claim them, you must also do these three transactions right? So we can still have these sort of complex multi user interactions.
00:05:29.030 - 00:06:09.666, Speaker A: We also have some bot users that allow us to do also pretty inherently serial things like build AMMS and order books. But this is just kind of like a little bit of an overview there. But yeah, because we've separated the users, like what users are able to write to. Let me just show you the animation. Yeah. So users prove their transactions locally because we know for a fact that no one else can write to their user's state tree since it's confined to themselves. So the users prove a state transition on specifically this node here.
00:06:09.666 - 00:07:20.406, Speaker A: So you can imagine that each user has one tree of trees, and each of the subtrees is like a contract state tree. And so as a user, what I'm locally proving is that my contract state trees route has transitioned from certain route to a new route, right? And then I generate a series of proofs. Each of these proofs are going to recursively verify a contract function proof that's defined by your original function. The proofs that are allowed to be used to authenticate, like contract state tree roots to be updated are stored in the leaves of this contract function tree. So you can imagine we have over here a circuit which is going to recursively verify that contract function proof which shows that, okay, this contract state tree has mutated from a certain state to a new state. And this circuit here is also going to check to ensure that that proof's verifier data is in the whitelist tree or the hash of the Verifier data is in the whitelist tree for that particular contract. So if we're trying to modify the Nth contract state trees route for a user, we also must have a proof along with it that has Verifier data, which is in the Nth, that is in the Nth contracts contract function tree.
00:07:20.406 - 00:08:10.346, Speaker A: So these leaves index the contracts for their specific whitelists and then these leaves index the contracts for the users. Index one in one tree is the same as index one in another tree. And then we eventually generate a single proof of all of those different transforms that I've done on the user's state route. And we end up with a succinct NCAP proof that basically shows, okay, this user's state route has legally transitioned from state route A to state route B, and then we send the proofs off to the network. So each user has one proof, no matter how many transactions they've done. We send that proof along with all of their state deltas to our decentralized proving network. Decentralized proving network are going to do a series of four to one operations in animation, it's doing two to one.
00:08:10.346 - 00:09:05.350, Speaker A: But you can imagine like, you have one proof which recursively Verifies two proofs along with associated with delta merkel proofs that prove in the case of it being just the end caps from the users from the users like leaf Roots or the Leafs over here. So you can imagine like this is all of the users in the chain, right? We first have a proof that takes these two users proofs along with the Delta merkel proofs that link them to their nearest common ancestor merkel cap. And we verify the proofs and then generate a new proof which proves up to a certain merkel cap. And then we can continue to do this process for all the users in the block until we have one sustained proof for all of the transactions that occurred in the block. And the great thing is we can do this process in parallel. Specifically we get block times of O, of log base, four of N and the four to one proof process takes 400 milliseconds. So you can do like a million users in 6 seconds.
00:09:05.350 - 00:09:26.340, Speaker A: And we've done this on AWS and it works. So we're very happy about that. But yeah, so you can scale it quite easily. Proof size 30 KB. Yeah, proving time is zero point 35 seconds. And this is on my MacBook. We have an iPhone, we have mobile SDKs right now only iOS because we've partnered with a lot of game companies.
00:09:26.340 - 00:10:15.338, Speaker A: We actually just announced our partnership. We're in the Unity Accelerator so hopefully some more official partnerships there. But yeah, on my laptop, which is an old M one, it's like zero point 35 seconds. So yeah, if it takes like let's say 0.1 second to send a 30 kilobyte proof to the next person, then yeah, you can get 223,000 TPS with 1 million people participating in the block. The other thing is, so we've separated our data availability miners from the provers because in order for you to get these fast block times you have to have a lot of provers. Right? So the great thing is we have data availability miners who are keeping the data available by just they have like a random challenge where they have to prove a certain random leaf within our larger state tree.
00:10:15.338 - 00:11:03.626, Speaker A: They have to give a merkel proof which index it is, is determined by a pseudo random number that's the same for everyone and then they hash that along with their user ID. So we get a specifically for each user. But the great thing about that is that in order to prove your transactions locally and to participate in this four to one proving, you don't have to have a copy of the entire state tree. So you can do it on your phone, you can do it. The people that are submitting the transactions can also participate in the proving and there's no trust or anything, there's no real requirement to barrier to entry, to participate. So yeah, we want normal people mining, not just the mining teams, the mining teams can do the data availability. But yeah, the great thing about that is we get one gas fee for unlimited transactions.
00:11:03.626 - 00:11:50.426, Speaker A: Now you do have to pay for your state delta. So you can imagine that if I am making a bunch of transactions which are mutating like the same few state slots, then I can save some money, right? But if I'm just adding more and more state slots you still got to pay if you're storing a bunch of data to chain. But the actual number of transactions that you're running doesn't affect the price you pay because the proof size is the same and you're proven it locally. So it just means that you're going to spend more time executing locally. But these contract function proofs, it takes like generally these take about 80 milliseconds and then these double recursive proofs take like 190 to 200. But we're working on improving our prover, it's 100 bits of fry. So we're trying to make everything very intense.
00:11:50.426 - 00:12:55.918, Speaker A: We could make things a lot faster if we took some shortcuts but we know we have to make sure that especially if we're not doing centralized proofer, that everything is AOK up to spec. But yeah, so vitalik actually recently at a modular summit showed this diagram which was very reminiscent of what we're building. So we're very happy that more people are getting on the tree recursion bandwagon. But the real important thing is that in order to take advantage of this kind of parallel proving you need to have a state model which matches the topology of your proving. So if you're proving in parallel you want to make sure that at each step when I'm doing the recursive verification, I'm also doing an associated delta merkel proof and you need to design your state model around that. And that is the cost of being horizontally scalable is you need to have this very hard state isolation otherwise you would have to implement that sort of back off logic that some of the other parallelizable blockchains are doing in ZK, which is probably not feasible, but you don't really need it. It's pretty easy to build applications.
00:12:55.918 - 00:13:41.598, Speaker A: We also support like I can just I know I've talked a lot but yeah, this just kind of goes through some of the things how I think everyone pretty much gets the idea. I can show you guys how we build contracts so we also support JavaScript TypeScript and Python. I can show you guys how we can type up a real quick contract and compile to Circum or like a circuit diagram if you guys are curious. I don't know, I've talked a lot, I apologize. We have programmable public key cryptography. So public keys on QD are actually it's like the hash of verifier data for some arbitrary circuit. And then when I want to give a signature, I just have to generate a proof which has public inputs that correspond to the message that I'm signing, as well as some contextual information about the state of the chain.
00:13:41.598 - 00:14:03.498, Speaker A: Great thing with that is that if I want to make it QED compatible with Ethereum signatures. I just write an SECP, two hundred and fifty six K, one Verifier in my circuit. Right? Verify. In order to sign transactions on QED, I need to provide an Ethereum signature for this, let's say public key. Right. But you can also make like, bot users lots of fun stuff. This kind of goes into random things.
00:14:03.498 - 00:14:09.158, Speaker A: Yeah. Does anyone have any questions? Do you want to see a JavaScript contract?
00:14:09.334 - 00:14:11.830, Speaker B: Yeah, we can see that and then jump to questions.
00:14:11.920 - 00:14:50.810, Speaker A: Okay, cool. So here we have some voting tally. Maybe we should make something a little bit simpler. So over here we have like an Abi format that we used to define like, the state and all the functions for the contract because we support JavaScript and Python and hopefully more languages in the future. So there needs to be some kind of common calling convention in state layout formatting. Actually, we can see an example over here of what your state definition can look like. So you can imagine that we have some functions and we can address different.
00:14:50.810 - 00:15:51.414, Speaker A: It basically takes some state structure and converts it into the minimum bounding merkel tree so that for any like this is what's used for the programming languages. When I do like, this state something something, it converts it into a merkel lookup, right? Yeah. Anyways, I can make just really quickly. Okay, so here we have a simple voting contract that takes in some different votes and it's going to output a result. So if you see like if I go over here let's remove this constraint for now, I can go age, I don't know, 281292, 212. If I go over here and I click compile contracts, I can then call so we can test out the contract in the browser much the same as something like Remix. However, we also support browser debugging.
00:15:51.414 - 00:16:35.340, Speaker A: So I can go in here and as we can see right now, the results I don't know if you guys can see it clearly enough. I guess you can. I know for a lot of web Two developers this may seem quite normal, but if you've ever tried to debug a solidity contract, you probably know that the solidity debugging experience is subpar. But yeah, so we have debugger. In addition, we can generate a circuit diagram for this particular this is going to be a big circuit diagram, but this just shows like the Arithmetization for our JavaScript code. Maybe I should make a simpler one so that we can see exactly like what's going on. Give me a second.
00:16:35.340 - 00:17:08.070, Speaker A: Compile. All right, see, what are we returning? NUM result. Change this. Let's have one output. Okay. It's just make sure this is working. Should return one if I do that.
00:17:08.070 - 00:17:37.374, Speaker A: Okay. So I can do something like, I don't know, let's say let sum equals zero. I don't know numbers for each x. I don't know like sum plus or equals x, and then I can return sum. And if we then compile and generate a nice circuit diagram, we can see what's going on. So as you can see, it's summing up the numbers right. We can also generate like Circum code.
00:17:37.374 - 00:18:37.358, Speaker A: I can make it more complex, like, let's say I want to do, I don't know, like some function hello, turn, I don't know, x times y plus three. Or let's say like, I don't know if x equals just make it simple for now. Compile, hello, x two, three, compile and click latex circuit diagram, as you can see. Yeah, so it can handle very complex JavaScript programs. It's anything that works in Es six that's not nondeterministic, we currently support. So, like, the ECMA script main unit tests are working. We don't support floating points right now, but all the ECMA script official unit tests that aren't floating point or non deterministic are currently running for JavaScript functions.
00:18:37.358 - 00:19:39.346, Speaker A: Like, you can imagine you have a for loop where the number of iterations is dependent on the inputs to the function. So you can imagine, like, I have some function which takes an input of x and for I equals zero, like k is or I is less than x, we would have an indeterminate number of constraints. So for that, our high level constraint language actually allows us to have loops in constraints. So you can imagine that it has some notion of permuting a function, or iterating a function on itself multiple times. So for those kind of, I guess if those features are used in our high level constraint language, then we compile to our own little VM that's written in Starkey. We also will very soon have midnvm bytecode support. So you can imagine that going through those constraints, you basically just walk from the you do like a depth first walk of one of these constraint trees in my NVM instructions.
00:19:39.346 - 00:20:03.600, Speaker A: Then you get the correct answer out the end. But yeah, if you don't have any of those, then currently you can target R one, CS, plunky two, and Circum, like generalized Circum. So. Yeah, that's QED. I've talked a lot. If anyone has any questions, please let me know. Otherwise yeah, sure.
00:20:07.330 - 00:20:09.280, Speaker B: Open up for a question, I guess.
00:20:12.390 - 00:20:13.620, Speaker A: Okay. Yeah.
00:20:14.150 - 00:20:20.850, Speaker B: You had a slide on Vitalik's ideal architecture for scalability blockchain. Can you maybe give a brief description of that?
00:20:20.920 - 00:20:58.746, Speaker A: Sure. So Vitalik didn't actually talk that much about the scaling end. He was more talking about having one succinct proof for all of the activity that has occurred, which we do implement. So at the end of a QED block, you get a single proof which recursively verifies the previous block proof, and recursively verifies the proof from those user operations that took place. So you get all the transactions of the proof, as well as the contract deployments and external events that we register on QED. But he wasn't necessarily saying that just for full disclosure. He wasn't saying like, oh, you should use a state model that mirrors the hierarchy of your proof tree.
00:20:58.746 - 00:21:02.734, Speaker A: No, that's what we're doing. Yeah.
00:21:02.772 - 00:21:04.030, Speaker B: I got one question.
00:21:04.100 - 00:21:04.622, Speaker A: Sure.
00:21:04.756 - 00:21:17.558, Speaker B: So I was wondering about the DA providers in QD. So basically the way to verify that data is available in their node, I guess, is to we give them the very bottom leaf and ask them to return the result.
00:21:17.644 - 00:21:18.280, Speaker A: Exactly.
00:21:19.050 - 00:21:26.262, Speaker B: But however, does that mean they cannot basically turn away historical data? So you basically have to keep whatever has been there.
00:21:26.316 - 00:21:33.222, Speaker A: So currently the data availability nodes, their challenges are guaranteed to be from the past ten blocks.
00:21:33.366 - 00:21:33.962, Speaker B: Okay.
00:21:34.096 - 00:22:13.650, Speaker A: And the reason why we do that is because technically, as a local prover, I don't have to be synced or proving a local proof chain that's synced ahead. Right. As long as I don't go backwards in time. So in our diagram, I guess it's not there. We store like which is the last block that the user participated, and then there's also a nonce. So I don't have to know about the rest of the world technically at the current block, but we would expect the users to upgrade to the latest Ted after some period of time. Right, but giving that sort of flexibility of ten allows users not to have to quickly submit off their proof chains whenever a new block begins.
00:22:13.810 - 00:22:22.470, Speaker B: Right, okay, so basically we doesn't care about whatever happens before the previous ten blocks.
00:22:23.130 - 00:23:04.086, Speaker A: So we do have a full history solution if you want it, but you don't have to. But for the data availability, the main thing is we have a commit reveal process that generates a pseudorandom number and that's actually done by the people that are participating in the actual decentralized proving network. And so they end up with a pseudoram number via Commit reveal or hash Commit Reveal XOR. And then this value is used. So you hash this value plus your user ID and that generates the random number which directs you to the correct challenge that you have to commit as a data availability miner. And you could not do the data availability mining tasks. You just don't get paid anything if you don't do it.
00:23:04.268 - 00:23:15.286, Speaker B: I was wondering if we're able to implement something like the litecoin for the DA solutions so that we can have a number of users checking on the DA providers.
00:23:15.398 - 00:23:58.902, Speaker A: Probably could. Another thing that we're thinking so this also comes up because let's say I'm local proving, right. I don't have a full copy of the state. Why will the data availability providers provide me this state information right now? It's like, why do ethereum nodes give you why do they respond with the correct answer? Right. There may be some way to incentivize this behavior or to have the portion of the user's final gas fee go to the person that provided them with proofs. Yeah. Once we launch our testnet, we're going to be exploring a lot of those things right now, we just have them generated zero knowledge proof that the merkel okay.
00:23:58.902 - 00:24:14.878, Speaker A: I provided the correct merkel proof with this pseudorandum number, and then that's included as part of the decentralized proving. So we all have one group of people doing this standardized task, which is nice, but yeah, there's definitely a lot of other approaches we could look at.
00:24:14.964 - 00:24:22.382, Speaker B: Yeah, definitely. So it seems like our data providers still be more like segregated, serving different users because different correct. One user only care about their own.
00:24:22.436 - 00:24:22.906, Speaker A: Yes.
00:24:23.028 - 00:24:24.642, Speaker B: They don't care about the entire correct.
00:24:24.776 - 00:25:02.720, Speaker A: Yeah. We're thinking about ways to work around that you could swap every now and then, I guess, or as part of as you're moving up, when you update, if two users next to each other are on different branches, you could probably swap them over. I don't know. It would make things quite complicated. But if we have to, there are sort of ways to go about doing that. But then you have some consistency issues with the way that apps track users, like user ID, so and so, which app ID does it actually map to? So, yeah. Food for thought.
00:25:02.720 - 00:25:07.600, Speaker A: Any other questions? Sorry for talking so long.
00:25:08.850 - 00:25:11.082, Speaker B: I guess we love that. And thank you, Carter.
00:25:11.146 - 00:25:11.580, Speaker A: Cool, thanks.
