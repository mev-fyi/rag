00:00:00.730 - 00:00:11.626, Speaker A: Okay, so hey, everyone, my name is Mark and I'm a contributor to the optimism collective. So I'm here today to talk about interoperability.
00:00:11.818 - 00:00:12.270, Speaker B: Right?
00:00:12.340 - 00:00:18.154, Speaker A: So this is modular interoperability in the op stack.
00:00:18.282 - 00:00:18.766, Speaker B: Right.
00:00:18.868 - 00:00:45.450, Speaker A: I just want to give a shout out first to all the incredibly talented people that helped with this. This is our little Sonny army. I don't know if you've seen the sunnies yet, but they've been starting to pick up on Twitter. They're pretty fun. Okay, so what is interoperability? Right, so blockchain interoperability. It allows for chains to read the state of other chains. Right.
00:00:45.450 - 00:01:33.334, Speaker A: This is kind of like read only interoperability. Technically it's possible to have a write style of interoperability, but that's much more complex. This talk is completely focused on just like reading the state of other chains. Okay, so design philosophy of how to build an interoperability stack, right. It needs to be minimal and flexible, and it needs to just work with existing tooling. Because if you're building a complex thing that doesn't work with the existing tooling, it's going to be really difficult to get devs to adopt it, right? And this architecture is generic. It should be able to work with different roll up frameworks.
00:01:33.334 - 00:02:11.118, Speaker A: But we have been developing this specifically for the op stack. But other people can also adopt it if they want. Okay, so let's start with the user experience. Because you should always design features from the point of view for a user, from a user, right. If you're designing a feature without thinking about the user experience first, you'll probably have a bad time. Like your users might not like it, right? So very simple, right? There's two transactions. You send a transaction to, say op mainnet.
00:02:11.118 - 00:02:52.494, Speaker A: It emits the log. Then you submit a transaction to base. And this transaction that you submit to base, it includes a pointer to the log that was emitted on op Mainnet and the log's data, and an address to call that pushes that log to this address. Right? So some vocabulary, right? We have an initiating message and an executing message. The initiating message, this is the transaction on the source chain. Like in the previous example. That was op mainnet.
00:02:52.622 - 00:02:53.058, Speaker B: Right?
00:02:53.144 - 00:03:56.950, Speaker A: So that's the initiating message and then the executing message. That's the transaction on the destination chain, right? Because like a cross chain message is like you have to send a transaction to the first chain and then send a transaction to the second chain to finalize it, right? And then we have this idea of a dependency set, which this is the set of interoperable chains that can all send cross chain messages to each other, right? So some key properties of this design. There's no force inclusion of cross chain messages. So anytime that you force inclusion of some sort of cross chain message, it adds a lot of complexity. If your cross chain message can trigger execution. Now you need to be aware of the gas market on the remote chain. You also need to be aware of the possibility of all possible chains sending cross chain messages to the same chain at the same time.
00:03:56.950 - 00:05:04.942, Speaker A: You don't want to allow for denial of services, right? So if you have force inclusion of any sort of cross chain message, the best way to design that is to use a queue, even like depositing, to be like a validator in e two, there's like a queue for that, because that is like a force inclusion transaction, right? So we decided to simplify things because we want to be more scalable. If you have a queue, then now you have this kind of synchrony assumption where you have to kind of sync all of the chains, it becomes much more complicated. So no force inclusion of cross chain messages. Much simpler, easier to reason about. We're also leveraging out of consensus policy to scale. So what does this mean? So because there's no force inclusion of these messages, it's really block builder policy. It's one of my favorite things about this design.
00:05:04.942 - 00:06:06.754, Speaker A: I'll get more to it, but basically block builders will help us to scale this. So we have optimistic inclusion with fork choice enforcement. So what does that mean? Right, so these blocks that contain cross chain messages, they are included optimistically, and the fork choice will basically reorg out blocks that are invalid. In this case, the definition of an invalid block that contains cross chain messages, it means that it has an executing message that doesn't have an initiating message. Right? So just to clarify what that means, right, every single cross chain message or cross chain transaction, you need like two sides of it. You need the initiating side, you need the executing side. So if a block contains any executing messages that don't have a valid initiating message, that's considered to be an invalid block gets reorged, right? So our fork choice enforces this.
00:06:06.754 - 00:06:50.720, Speaker A: The fault proof also enforces this. It's kind of just like different views into the same problem. And basically we utilize static analysis of these cross chain transactions to let the block builders really scale, to be able to know if an executing message has a valid initiating message. And I keep saying this, it's probably pretty abstract. I just said all this stuff. This is talking about the four choice rule. So now getting to it, right? This is as simple as it gets, right? This is the smart contract that allows for the entire scheme to basically work.
00:06:50.720 - 00:07:56.286, Speaker A: So static analysis, right? Required message sender equals TX origin, right? So basically you want to send a cross chain message. What you do is you basically take a log on a remote chain. Let's say that there's an oracle on a remote chain and you want this oracle's data in your local chain because that oracle on the remote chain is emitting events, right? So we want to have one oracle and then that Oracle data can be used everywhere so that Oracle is emitting events, right? So what you do is you take the event that is emitted in solidity. It's like the emit syntax. It emits an event, right? So what you do is you have an identifier. This identifier right here uniquely identifies a single event. Solidity calls it an event, but like Geth calls it a know.
00:07:56.286 - 00:10:01.262, Speaker A: So we got log index here, but basically we have this identifier. This identifier uniquely points to a single event across any chain, right? So we have basically like a chain ID, a block number and a log index that helps to uniquely point to a single event, right? And then what we do is we basically see there's this message that's passed in, right? This message is the serialized event where an event can have topics and then it can have data, right? So basically this identifier right here points to an event, right? And this pre deploy, it's built into every chain, right? A user sends a transaction to this pre deploy and basically the user is saying, I claim that this event on this remote chain exists and I want to consume it, right? So the block builder, or like the sequencer does like a quick check before including this transaction into their block. Because if the sequencer includes an event that doesn't match its identifier, then that's an invalid block and it'll get reorged out, right? And the sequencer will lose some transaction fees will be bad. Basically this scheme lets you read in events from any chain as long as the chain is in your dependency set, right? And the chain governor can decide that this is like the bare basic primitive that allows for a chain to read in an event from any other chain. Anyone have any questions at this point? Okay, what's up?
00:10:01.316 - 00:10:06.874, Speaker B: So as of now, those chains will only be able to do the crosschain.
00:10:06.922 - 00:11:01.310, Speaker A: Interoperability if they are using an op stack, right now. Yes, because this is all basically secured by the fault proof. The fault proof will be able to enforce these rules. And it's very complicated to try to build a fault proof that works across, say both arbitram and op stack at the same time. That would just be kind of crazy. So this scheme right here, technically any blockchain framework can adopt this scheme, but we're building into op stack. But if the question is, can an chain use this particular scheme and interrupt with say like arbitram orbit chain in a trustless way, that will not be possible just because it would not be able to be secured by the fault proof.
00:11:01.310 - 00:11:08.900, Speaker A: But I do think that this is like a very minimal scheme that any chain could kind of adopt, if that makes sense.
00:11:11.270 - 00:11:18.280, Speaker B: If the op several are using a shared sequencer like expresso, like other also able to do this.
00:11:18.650 - 00:12:09.030, Speaker A: That's a great question. It depends what is like being shared sequenced between. But I will claim later in the talk that this allows. This is the bare basic primitive for a shared sequencer, at least within the op stack. So is it okay if I get back to this question? Okay, cool. Okay, so some fun things about this. There's no replay protection, right? You can basically reference a remote event as many times as you want and just like pull it in, right? Because of that, we have a higher level abstraction on top of this, which is our messenger, right? And this messenger gives two unique properties.
00:12:09.030 - 00:13:06.130, Speaker A: It gives you replay protection and it gives you domain binding, right? So this allows you to say, I want to send a particular cross chain message to a particular chain, right? So there's a lot of solidity code in there. What I think is fun about this is now we finally have a use case for the anonymous event. The anonymous event has no one's known why that existed for the longest time. It's like the log zero opcode. But what's fun, right, is what you can do is you can basically encode your call to a cross chain contract, and then you just emit it from an event, right? So you encode your call just like how you would in solidity. The exact same syntax works. You emit it from an event and then you can pull that call into another chain and then just pass it right into a smart contract.
00:13:06.130 - 00:14:32.414, Speaker A: So now you've got cross chain calls, right? So we're like layering levels of abstraction. So now we got cross chain calls with replay protection and domain binding, right? So what this means, now that we have cross chain calls, we have like block building, right? So the scalability of this kind of scheme is based on the ability for block builders to coordinate, right? Because you're just basically making these transactions that have identifiers that point to logs, right. If you have two block builders that are basically building on two chains, then they can coordinate to basically point to each other's logs that they emit. At the same time, they could build the block together, right? So this gives you the primitives for shared sequencing. And I also want to claim that it gives you atomic composability. And people like to argue about what is atomic composability. I think that you get atomic composability under the assumption of block builders that coordinate, right? And the idea is that block builders, it's a competitive marketplace.
00:14:32.414 - 00:14:46.662, Speaker A: What's really nice about this scheme, right? This is one of my favorite quotes right here. If you ever have a problem that's too difficult to solve, make it profitable for the MEV people to solve for you, right? Because the MEV people are competing with each other.
00:14:46.796 - 00:14:47.190, Speaker B: Right?
00:14:47.260 - 00:15:35.830, Speaker A: So this is a very difficult problem, right. This is like some crazy knapsack, like align transactions between multiple chains, right? So the MEV people are trying to make money. So make them compete for you to improve your protocol. So that's basically what this scheme is going for. So just to repeat this, you have these cross chain messages. So basically, the block builders, what the block builders would do is they would include an initiating message. And in the other domain, they'd include an executing message that references that initiating message.
00:15:35.830 - 00:16:07.846, Speaker A: But then that executing message would, like, emit an event, right. And then that would count as the initiating message. And then on the other side, you'd be able to include an executing message that references this initiating message. So you'd be able to go back and forth like this, right? And basically the question is, can block builders do this? Right, so this is what I was saying. How difficult will it be for the block builders to coordinate on this?
00:16:08.028 - 00:16:08.662, Speaker C: Right.
00:16:08.796 - 00:17:07.740, Speaker A: So I do think it'll be possible. I don't know how deep they'll be able to go with how many callbacks, but I do think that they'll be able to do at least one. But they'll compete with each other, and through that competition, they'll be able to provide better services for users. So that's what's great about this also. Fun idea, right? Can you build like an RFQ system by basically proposing the executing side while no one has created the initiating side yet? Right. You can basically kind of create this order filling kind of thing where you just make your executing message available, and then somebody that wants to fulfill that executing message can just submit the initiating message. Think that this is like a really interesting design space.
00:17:07.740 - 00:18:05.980, Speaker A: Okay, so some more research questions. So we have this only EOA constraint, right? If we go back to the smart contract, I'll show you. This is probably like smart contract wallet. People probably will hate me, but we have this require message sender equals TX origin. What this means, this enforces that it's the top level call frame, right? So unfortunately, you can't use a smart contract wallet with this. But the reason why is specifically for denial of service protection, right? So the block builder can look at this transaction that's calling this function and statically analyze the call data. It can just deserialize the call data and basically look at this identifier, and then basically, sorry.
00:18:05.980 - 00:18:54.982, Speaker A: It can basically look at this identifier and then basically check to see that this exists. This is the log, the serialized log. So since it's all statically analyzable, we give like a nice easy problem for block builders. It's like just have like an RPC that points like a remote domain. You can do ETH, get logs at this block number and then filter by this address. This is the address that emitted it, right? So that's one RPC request. And then what you do is then you iterate through the logs and you see, does the log index match? So now you have the exact log, you just use the RPC, and then you just serialize the log using this algorithm.
00:18:54.982 - 00:19:37.740, Speaker A: Very simple. And then you just compare that what you fetch from the remote RPC matches what was passed in there, right? So it's a very easy problem for block builders. And sure, it's not super scalable, you're using RPC. Block builders will probably do some crazy optimization where they figure out a way to not go over the network and stuff. But naive block builder was able to just describe the algorithm of how you ensure that this works. So the idea with the only EOA constraint, that removes the ability to do the static analysis. So it makes the problem much more difficult for block builders.
00:19:37.740 - 00:20:18.890, Speaker A: What we would have to do instead is basically, we'd have to emit an event here that includes the message and the identifier. And then basically the block builder, when building the block, they'd have to pull out all of these events and then do the checking then. So it's like checking after the fact or before the fact. When you can statically analyze, you check before the fact. And if you have to look at the event, you already did the simulation. Somebody can dial a service you. Okay, cool.
00:20:18.890 - 00:21:26.076, Speaker A: So another question is what is the topology of all the chains that are interoperable like this? Will it be like a hub and spoke model? Will kind of be like distributed or decentralized? Another thing that's interesting is like this timestamp rule, right? It allows for intra block time travel. So what does that mean? We go back here. See this identifier? This identifier is like a unique pointer to the log in a remote chain, right? So right here you can see that the timestamp. So this is the timestamp that the log was emitted at. It has to be less than or equal to the block timestamp, right? So all the chains are like advancing at the same timestamp. As they produce new blocks, they're all at the same timestamp. So basically, it's impossible to introspect on the total ordering of transactions within a block.
00:21:26.076 - 00:21:58.776, Speaker A: Right to this right here. All the transactions, kind of like the first transaction in the block and the last transaction in the block look the same. So there's like this weird property of time travel within a block under this scheme. I'm not really sure if it's useful or not, but it's just like an interesting observation. Okay, cool. So the past. This is the past, right? Web two.
00:21:58.878 - 00:21:59.432, Speaker B: Right?
00:21:59.566 - 00:23:17.540, Speaker A: Web two architecture operates on a set of horizontally scalable VMs, right? And we have the ability to scale up the number of VMs with demand, right? This is how we're able to service the entire world with our services, right? So the current. So web3 architecture operates on a single VM. Basically what happens is all the applications, they're all competing for this global lock. They're the only one that's able to do anything at a single time. So they get this lock, they're the transaction that's executing, and they're the only one that's allowed to read and write at that time. So the future web3 architecture operates on sets of horizontally scalable VMs, right? The number of VMs can scale with demand. We basically need to look to how web two builds their applications and build our applications in the same way, right? And what this means is we need to fundamentally rethink how we go about designing smart contract systems.
00:23:17.540 - 00:25:03.504, Speaker A: Right now, we design smart contract systems in a way where we assume that the entire system is deployed to a single blockchain and that all of the state is kind of like right there. That's kind of like saying that web two companies should deploy their entire back end to a single computer. It just does not make sense. It's like impossible to scale that way, right? So we need to rethink about how to build things in an interop native way, right? And the claim is that if you can horizontally scale, you can smooth your fee market across many chains. So what this means is you can bring your own gas to the network, right, in kind of like the equilibrium state of being able to bring your own gas to the network. What that means is that in theory, the transaction fees should find an equilibrium around the cost of the hardware, right? So what this means is that basically, if you're able to look at your smart contract system and find the bottlenecks, the hottest parts of your code, that's called, and you're able to basically rewrite the smart contracts in a horizontally scalable way, then you can then get these sorts of benefits where when demand goes up, then you can increase the supply, right? Because right now we live in a world of gas scarcity. Gas is a scarce resource because blockchains produce it and you buy gas from blockchains.
00:25:03.504 - 00:27:12.076, Speaker A: Right, but I think that's kind of dumb. We should be able to bring our own gas to the network by turning on more machines. And that's like the point of this, right? Basically, you design your smart contracts in a way where your smart contracts don't all need to be on the same chain. And basically you can think of it like templates, right? And when there's high demand, you automatically spin up a new chain that has this templated contract in it. And then you start routing user transactions to that chain, right? And not all smart contracts will be able to be kind of like horizontally scalable in this way. But if you can take your really expensive business logic and build it into smart contracts that horizontally scale, then you can get all the benefits of this, right? Okay, so something that I've been trying to think about is what's the right abstraction to make this really easy for developers? Right? So there's this idea of a distributed hash table, right? So like a distributed hash table basically lets you kind of hash table, right? Like a key value pair kind of thing, but with a distributed hash table, all the values are spread out across many nodes, right? So the idea is that what if we had a library that implemented a distributed hash table, but feels like regular EVM storage, right? So if you've written a smart contract, you're probably familiar with writing to storage. Like either use a mapping and you just write a key value pair into the storage, or you do like a low level s store or something like that, right? So the idea is that we need to build abstractions that allow developers to take advantage of this horizontally scalable idea where they don't need to think about it, but they just get the feature for free.
00:27:12.178 - 00:27:12.444, Speaker B: Right?
00:27:12.482 - 00:28:22.468, Speaker A: So can we build a library that implements a distributed hash table, but just feels like regular storage? And the idea is that you just have like a little off chain bot that would rebalance the keys across all of your nodes. And then the idea is that you can add or remove nodes automatically and the whole system would self heal. And the idea is that you'd be able to build parts of your smart contracts on top of this abstraction and it would just give you horizontal scalability in your application. And like I said before, not all smart contracts can be built in this way. So you could imagine some parts of your code base, your solidity code base could be like horizontally scalable. And then what you could do is you can kind of have this pattern where you settle down all the state to another chain and then in that chain that can be like your coordinator, kind of like chain, something like that. Right? So on chain order books are too expensive right now.
00:28:22.468 - 00:29:29.240, Speaker A: So my claim is that you can build an on chain order book using this architecture because you have to insert orders and then you have to fill orders that uses a lot of gas. So if you could horizontally scale and shard kind of your order book deterministically across many different chains, then it should be possible to build an on chain order book. Because if you look at Bitmex or whatever, they definitely run their order book on multiple VMs. It's not just like a single VM that runs all of Bitmex or like Coinbase. So we need to learn from web two with architecture to be able to take web3 to the next level. Phoenix, another ethers Phoenix, more ethers Phoenix and our specs. So if you have any questions, check out our specs.
00:29:29.240 - 00:29:46.610, Speaker A: We have all this specified. It's online, it's free for anyone to use. We're starting to work on it now. Yeah, that's my talk. Who's got questions?
00:29:50.370 - 00:29:56.640, Speaker B: I got one more. Great to involve math players. But would they also be centralized problems?
00:29:57.350 - 00:29:58.722, Speaker A: Will there also be what?
00:29:58.776 - 00:30:00.180, Speaker B: Centralization problem?
00:30:00.950 - 00:30:50.558, Speaker A: That's a great question. Okay, so centralization problem. So every time that you have like an interoperable chain, it adds an execution requirement or a trust requirement for that chain. The sequencer needs to basically when the sequencer is doing this verification of the cross chain messages, it needs to either trust like a remote entity that did the execution, or it needs to do the execution itself. So it makes being a sequencer more expensive. Also the same thing for like a full node. It makes it more expensive to be a full node, because if you're syncing as like a full node, then you need to do the same sort of checks the sequencer does to make sure that the blocks are valid.
00:30:50.558 - 00:31:48.850, Speaker A: Because if you're a full node, you want to reorg out invalid blocks the sequencer produces, because it's like a checks and balances on the sequencer. So the more chains that become interoperable, it does increase the hardware requirements, which is a centralization vector. We have been thinking about how to fix this. I think that when vertical trees are accepted into l one, when l one, it probably won't be the next hard fork, but the hard fork after that. So it'll probably be like two plus years from today they will implement vertical trees. And what vertical trees gives you is stateless execution, right? So the idea of stateless execution, you don't need the full database to be able to verify the block. You can be given a witness that basically includes all of the state that the block touches during its execution.
00:31:48.850 - 00:32:44.710, Speaker A: So without the proof being too large, if we did that now, the proof would just be too large, you wouldn't be able to send it over the PDP network. So one way to decrease the kind of centralization vectors that are introduced with this would be to use like a stateless execution scheme where the blocks are gossiped around the PDP network and then nodes themselves can just statelessly execute the blocks of other chains. And it makes it a lot easier to not need to trust the remote RPC or not need to run the full node of a remote chain. And I think that near has a design. I know that they've moved stateless execution for their sharded execution, so this is kind of similar to that. But I don't know enough about their design to talk about it in depth.
00:32:47.540 - 00:33:34.850, Speaker D: Following that, not sure you see my last two slides. How do you think about async composability compared with atomic composability? I guess it's more like from the application point of view, do you think there are like, how many use cases do you think that requires atomic composability? Or maybe in most cases like async composability is fine, which means we can able to settle execution proofs on multiple chains, at least, you know, as if snapshot time is happening, and then you actually need to some extent avoid the centralization problems.
00:33:35.620 - 00:34:28.080, Speaker A: Yeah, I think that's a great question. So I think for DeFI use cases, atomic composability for getting ARbs, that's definitely a big use case there. I also think that the definitions of atomic and async are very weak definitions, and I bet you that we have different definitions. So there's some notion of force inclusion transactions here. I think some people might believe that atomic composability implies some amount of force inclusion. That might be like one case. Under this case I'm claiming that you can do atomic composability without force inclusion.
00:34:28.080 - 00:35:21.490, Speaker A: I think that from a UX perspective, the speed is just like a magical moment. We want to create these user experiences that are just like the user absolutely falls in love with and they won't use anything else. So that's like a benefit that I see of the atomic scheme, but everything has trade offs. One thing I've been thinking about is how do you use zk proofs in a scheme like this? And I think that basically the proof generation time is just going to be too high to allow for any sort of within block kind of thing.
00:35:22.280 - 00:35:23.380, Speaker D: We can talk about that.
00:35:23.450 - 00:35:23.780, Speaker A: Okay.
00:35:23.850 - 00:36:13.510, Speaker D: I guess the higher level idea here seems to be, I think in the one to two years latency will be great, reduced by both hardware and that far I'm pretty competent. But I think seems that there are some open questions about the UX and specially specific use cases like Defi and a few others. From my point of view, I don't think the speed is an inherent imitation of the ZK side is quite temporary because there are many newer research and newer hard work acceleration coming. But I do think the UX part seems to be quite interesting to see how it's going to play.
00:36:14.840 - 00:36:49.580, Speaker A: Yeah, and I think it really comes down to the block builders, because the block builders under this scheme, the block builders, are the ones that are responsible for building the two blocks in tandem that include the pointers to each other. Right? So basically without the block builders, this scheme is not that good, right? This scheme is completely based on block builders competing with each other to provide more cross chain messages in the same blocks.
00:36:53.040 - 00:37:07.616, Speaker C: Have a little question. But you mentioned that you're very interested in providers kind of solving this issue for you. Have you thought about interplays with a system like Flashpot spa day or just like the normal music pipeline?
00:37:07.808 - 00:37:58.390, Speaker A: Yeah, totally. So I've definitely talked to flashbots about this. Basically it's just like a game of biding on log indices. We spent some time trying to figure out if we could make the API for block builders better, but then decided it was a rabbit hole. Because basically you need to be able to uniquely identify an event. And maybe a different way to do it is do transaction index and then relative offset within that transaction. I don't know, there's different ways to uniquely identify the event, but at the end of the day you need to have these unique identifiers that correspond to each other.
00:37:58.390 - 00:38:54.340, Speaker A: So I think that it could be possible to use suave to do something like this. I've heard a million different definitions of what suave is, but I do know that suave is supposed to allow you to program MeV boost auctions, because MeV Boost gives you this auction where you can basically bid on the proposer including your block. But it's like native code, right? You have to have DevOps people, you have to cut a new release of the code. You have to update the code, cut a new release, have your DevOps people update it. You need to do this every time you want to change the auction. So with suave, it's supposed to let you define your auction in the solidity. Is that your understanding?
00:38:55.560 - 00:39:00.176, Speaker C: Yeah, like you, I don't know what. I do not know what is.
00:39:00.298 - 00:39:29.250, Speaker A: Okay, totally. I think it's supposed to just let you define like an MeV boost auction in solidity. That's like the example app. And then you can define other types of auctions and then it saves you from needing to have DevOps people update the code. Anyone can just deploy a smart contract. It defines an auction, right? So you could build an auction for log indices, but I don't know if that's good ux or not.
00:39:29.780 - 00:39:30.690, Speaker C: Go ahead.
00:39:32.500 - 00:39:33.650, Speaker A: Okay, cool.
00:39:37.060 - 00:39:42.030, Speaker B: Any more questions? Okay, thank you.
