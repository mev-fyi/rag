00:00:10.330 - 00:00:59.866, Speaker A: Thank you. Talking about pro singularity, I think that gets together pretty well because I'm going to talk about pro singularity and I'm going to talk about the battle path to achieving just a little bit about myself. I'm schumo, I'm the founder of Nebron apps and we have an amazing chatting. If you are scared of curves field and like chinese removal theorem. So let's get started. So I think I don't have to come in with this prop, but just this first slide. I do think after ten years we have finally used the decentralized censorship resistance ledger.
00:00:59.866 - 00:01:40.986, Speaker A: And the coolest part is actually short like coding segment, which means it's touring complete. I do think Ethereum will be our civilization segment layer. Right? And I just googled this picture here. Anyone knows what's this picture? It's from Lordic Miss. I don't know how to pronounce it but it's like a YTC Dressel. The word the future of the digital civilization. I think it's kind of like a word and people are talking about what's going next.
00:01:40.986 - 00:02:47.282, Speaker A: Are we going to build like l one, l two l three, l four to l infinity, right? Actually don't know because maybe there are just like lot of l two s and score stack, polygon stack. There are going to be different kind of options. But there is one thing it's pretty clear to me is that a lot of these building layers will be powered by zero, not necessarily zero. So it starts to emerging about all the promising primitives. They're actually all using VTA as the primitives to power one asset or another of their protocols. So for example, I view layer two as a horizontal scaling. By horizontal, I mean we have EvM on layer one and we just replicate what exactly event do in layer two.
00:02:47.282 - 00:03:21.762, Speaker A: Kind of like horizontal scaling. Like we have all these amazing squirrels if you think polygon and Ninia, I know I have a different pronunciation, Ninia, like the last speaker. But I can talk to the team themselves are confused by how we. So I guess this doesn't matter. And we have all this kind of privacy preserving permissives. You can see like somaphore, Macy, they are all using like Je. We have some incoming private defi projects.
00:03:21.762 - 00:04:30.286, Speaker A: I think they are going to launch the midnight protein nocturn. And of course we have ADAC who's using privacy preserving our tools. And the third category is kind of oracles, right? We have axon, like Ronge, the same type of oracle. It's mostly kind of to me like ZK is kind of like glue, right? The basic thing is you're using a verifiable computation, you can glue it. I think Hamlet from score, he talked about, hey, if something just like too much compute, just throw it to a park and then you can settle it, right? It's becoming kind of a universal computing settlement. Primitives you are going to settle on blockchain. I think it's becoming interesting, useful, and that's why people thinking about pro singularity, because the true sort of the protocol design philosophy of pro singularity is that you just need to implementing this single, I wouldn't say simple, but it's just like a single primitive which is verifying zero 90 proof.
00:04:30.286 - 00:05:13.120, Speaker A: And you are kind of pushing off the complexity to the edge. And as layer one, as a base layer, you need some certain kind of like simplicity and you don't want to change the layer one code all the time. Kind of pushing the complexity from my personal point of view is a very good, like a design philosophy. So then the question is we're not there yet. I think one of the biggest problem today in my view is that the proofreprication cost is very very expensive, especially on L one. Here this is number. So it actually depends on which kind of approach system you are using.
00:05:13.120 - 00:06:30.298, Speaker A: The kind of odyssey approach system 416 has the least gas cost, so it actually depend on the size of your pop input. And like 250K is kind of like a good average number. And if you think about that, and there's assumption we have like a 20 g way gas costs, which is actually quite moderate because the previous past half a year's cost, even during the bear market has come like a 26 and 2000 USD east price I do think will be much higher. So it's $1010 right? And the second wide used pro system, like Ko two KBT, we're talking about like 400K get cost which translates roughly to like $16 per proof of vacation. And Stark roughly you need about like 1.5 million guest cost and it's like $60 like $60 for settle single proof. I think Vitalik used to be human said do I want to pay $20 to add privacy for your transaction to buy a coffee? Probably just like once or five.
00:06:30.298 - 00:07:34.874, Speaker A: But that is not going to be sustainable. So that's why we're building a pro signarity with Libra and we do have a very long and progressive period. But what we are focused first is try to reducing the proof verification cost and also scale the proof verification capability some series. The simple question right. How can we make VK cheaper unchanged? How can we realize magic? And our answer is we're using VK, we're using magic to achieve the magic. It's a handwriting question, but it is, right? So this is the first permitted bar beauty called an Evar UPA stand for universal proof aggregator. Let's go into it, right? So this is like a super, super high level idea, what the universal proof aggregator is.
00:07:34.874 - 00:08:03.186, Speaker A: So we have, the proof aggregator will not do the proving for you. You have to give us proof. So I think this is the best interface we can design because we can adapt to any kind of proving infrastructure. No matter you are a privacy preserving protocol, you need a client side proof, which is the general proof is generated by the client devices, or you are like a kind of DK infrastructure, you are using like a GPU powered machine to generate proof.
00:08:03.218 - 00:08:03.414, Speaker B: Right?
00:08:03.452 - 00:08:39.380, Speaker A: We just take proof and we call it universal because we can take proof from different circuits, which means no matter what kind of computation you are encoding, we can have it. So then we take multiple proofs into single bundles in the NIBA and we generate the aggregated proof. So this aggregated proof eventually is going to verify on Ethereum or to deploy to l two. And the cool part here is.
00:08:42.630 - 00:08:42.946, Speaker B: And.
00:08:42.968 - 00:09:35.166, Speaker A: The cool part here is because the proof verification cost is roughly the same. Right now, instead of like verifying six proofs, we're actually verifying single proof. And more specifically, in our first generation of the proof aggregation engine, this is the halo two KVT proof. So you can see the aggregation proof cost actually stays constant compared with no matter how many proofs were aggregated per batches and under some practical settings. Currently we're aggregating like a 32 proof per batch right now. That's like super, super high level picture of what we're doing here. Oh, by the way, questions are welcome, and please interrupt if you.
00:09:35.208 - 00:09:35.800, Speaker B: Yeah.
00:09:39.130 - 00:09:41.506, Speaker C: Is this just one level of aggregation?
00:09:41.698 - 00:09:50.266, Speaker A: I will talk about that five slides ahead of time. How do you know the limit? All right, what's the limit of how.
00:09:50.288 - 00:09:51.370, Speaker C: Much you can aggregate?
00:09:51.870 - 00:09:53.740, Speaker B: The practical answer is no limit.
00:09:56.590 - 00:10:00.950, Speaker A: Yeah, you can aggregate like one man in fruit.
00:10:01.110 - 00:10:02.358, Speaker C: That's totally doable.
00:10:02.454 - 00:10:26.100, Speaker A: But there is a proof of agency trade up, right? And if you want, let's say aggregate 1 million pools, you probably need to do like a multi stage aggregation. And there are some recursion overheads and there's some latency trade off here. I think the real proof singularity is where the last speaker was saying, right, so essentially you can just have one proof per block on any protocol and that would work.
00:10:29.430 - 00:10:32.882, Speaker C: When submitting the aggregated submit to your contract.
00:10:32.946 - 00:10:33.318, Speaker B: Right.
00:10:33.404 - 00:10:39.670, Speaker C: So that means your contract will have the right to update other roll ups.
00:10:40.990 - 00:10:49.718, Speaker A: So we are purely in charge of verify endpoints deploy on various supply chains.
00:10:49.814 - 00:11:06.110, Speaker C: Well now each robot has their contract on their one and when verifying the proof they also update the state of our data. Yeah, that means your contract can update their feature.
00:11:10.390 - 00:11:30.506, Speaker A: First is that if you want to use us. Right, so you need to use our contract interface which means just standard interface, verifying proof you need to change your contract. But I don't think our contract has ability to change their state because first of all immune pool. Secondly, unless we coded something wrong.
00:11:30.608 - 00:11:33.994, Speaker C: So basically they need to integrate with your.
00:11:34.192 - 00:11:37.340, Speaker A: So people use us to integrate us changing their.
00:11:38.990 - 00:11:48.030, Speaker D: This may be the wrong group for this, but outside of Ethereum, are there actual business marketplaces where there's people that actually want to pay for proofs?
00:11:48.530 - 00:11:54.226, Speaker A: Pay for proofs. I have proofs I want to give to you and you're someone who wants.
00:11:54.248 - 00:12:13.078, Speaker D: To pay for them. Outside of Ethereum, I think Nebra is like an app ecosystem for proofs. Who's buying proofs outside of Ethereum? Who's using the other? There's two sides, right? I can proof things to a protocol, there's someone who wants those proofs, who is paying for those, who's on the.
00:12:13.084 - 00:12:14.120, Speaker A: Other side of that.
00:12:15.050 - 00:12:23.062, Speaker D: There's all these use cases like medical identity. Have you seen any use case? Maybe I'm out of sync with your presentation, just curious.
00:12:23.206 - 00:12:31.166, Speaker A: Yeah, that's a perfect question. I think first is that there is a need of verifying for sure. Right?
00:12:31.268 - 00:12:33.566, Speaker C: But if you think about that, you.
00:12:33.588 - 00:12:50.750, Speaker A: Need a consistency layer to set the ground truth. There's an immediate question like hey, you have proof in some scenarios you don't like kind of like ground truth, then whoever should checking the proof without checking whatever you're doing should verify.
00:12:50.830 - 00:12:53.922, Speaker D: I mean this is a new paradigm shift, right? It went from scraping data to now.
00:12:53.976 - 00:12:56.198, Speaker A: Users may be able to just share their data.
00:12:56.284 - 00:12:58.226, Speaker E: People start to say, but who cares.
00:12:58.258 - 00:13:02.358, Speaker D: About which orgs and enterprise are actually going to switch to this model?
00:13:02.444 - 00:13:38.418, Speaker A: Does that make sense? Yeah, preference is going to. I think it's really about who need approve, who need to buy the proof, who need to verify proof. I think the answer is who you need to convince, who you need to settle trust. Right? That's why I mentioned ethereum might be going to be the civilization settlement layer. You sort of have this ground truth layer and every trust kind of grows from the root of the webcast. Right. In some specific use cases maybe you don't need that then that's fine.
00:13:38.418 - 00:14:07.050, Speaker A: You just need to convince you just prove to something that someone you need to convince then there could be some models there and the proof aggregation the protocol were viewed on Ethereum, but we're actually open source. Open source our toolings and you feel free to use on your specific for example enterprise use case scenario. Yeah. Is this similar to the original idea of Mina?
00:14:07.950 - 00:14:08.700, Speaker C: So.
00:14:15.170 - 00:14:21.600, Speaker A: I do think I'm not super familiar with mina mina candle. That's like.
00:14:25.650 - 00:14:27.470, Speaker C: Yeah, the idea was to argue.
00:14:31.750 - 00:14:36.520, Speaker A: I guess what we did a little bit different here is that we're actually.
00:14:38.570 - 00:14:40.920, Speaker B: Express whatever you want to.
00:14:42.170 - 00:15:28.882, Speaker A: What kind of work system is this again? This is like a six currently internally on just the polygon testnet aggregate all kind of work. Sorry. Currently we can aggregate all the work we're building a universal aggregator to aggregate from different proof systems. Yeah, I will cover that in the future. I have you chat offline. The answer is no and for a very good reason. The TLDR is that you need to put the which is twelve degree intention field not just natively but to put that into a circuit.
00:15:28.882 - 00:15:36.680, Speaker A: And that probably give you like 1 trillion cell circuit, something like this in sync thing or not doing that. Yeah.
00:15:38.750 - 00:15:39.500, Speaker B: Great.
00:15:40.510 - 00:15:43.482, Speaker A: Okay, so for different circuits or different.
00:15:43.536 - 00:15:46.394, Speaker D: Orders of this circuit, do we need.
00:15:46.432 - 00:16:17.780, Speaker A: A one time setup of. For example, it's really not a problem for us about your circuit. Right. So for example, if your circuit you are using dorsity circuit, you do need setup, right? And then what you do is that I will talk about a little bit later. Right. You need to upload your verification key to us. And if you are using something like start then you don't then require but the system like aggregation itself just being one trust.
00:16:17.780 - 00:16:58.914, Speaker A: We are not being trusted because we are building using the aggregation system. Currently are building using KLO two. And for the next generation we're thinking about using part and two and neither of them need trust. All right, so here is a Nibar protocol, right? So I just talked about the general idea of the propagation words. And we are a protocol that deploy on theorem and L two s, right? So our protocol has two parts. First is the UPA contract, just a smart contract interface. You can interact with us.
00:16:58.914 - 00:18:12.774, Speaker A: We all have the same address among ethereum layer one and different l two s. And then we have an off chain worker infrastructure and we have a coordinator can listening l two or l one or l two contract. And we have distributed proverbs so these are the three high level design goals of the protocol. First is permissionless, you can see the interface is contract and anyone can submit a proof to us. And second is the censorship resistance. So I don't have a full detail here, but we are going to publish a sensor resistance mechanism design in the very high level will be very similar to you have a box design for example, we cannot sense, even ourselves, cannot censor your connections and there could be some stake imposed. And third, also not least important one is that want to support progressively decentralized proving architecture to approving network.
00:18:12.774 - 00:19:06.970, Speaker A: I don't think we're doing that right now today. But once we actually very careful design the distributed system, it's very modular, so that in the future it's going to be very easy to progressively decentralize. That's like three high level design goals and system infrastructures. There's definitely more effort behind this. So one thing I can think I can talk about a little bit is that we actually build a scalable infrastructure in a sense that if there is more demand we can actually immediately get more workers to join. So our throughput can be bigger and if there's less demand so we can use less workers and to keep the economy. So I think this is the key, right? So for example, maybe during one day, during a single, there's a lot of like ZFconnects on Ethereum, we want our infrastructure kind of scalable with the demand.
00:19:06.970 - 00:20:05.470, Speaker A: So that's protocol. And the next question is how to use us. Like I previously noted, it's actually really simple. So one example is that when we first tried the first burn, the internal testnet, one of our devs, he just like coded the trondle cache example. He just used two days to just like including testing and getting everything actual coding time, probably just like 2 hours. So from some of the high level, right? So on the left hand side is how you verify the zero month proof. Today on zero you just deploy your verification contract, something like Verita console and that one is calling some photographic permittees like pairing, then that's it, right? And now we have an endpoint we call satin contract.
00:20:05.470 - 00:20:27.640, Speaker A: So before you do this, you need to register your key and then you just call later, call setting verify contract with your verification key, your proofs and public, right? And that's it.
00:20:28.810 - 00:20:30.738, Speaker B: Which means it's very easy to integrate.
00:20:30.754 - 00:20:58.586, Speaker A: With your existing like a ZK application, either your application or data. So this is a little preview of the first version of the Testnet product. We're going to release so we have three parts. The first part is the newbar contract. You can see we already get the contract verified. It's basically the endpoint. You can see we have an API called submit proof.
00:20:58.586 - 00:21:28.658, Speaker A: You can submit proof to it. And this is like the last bur of the Testnet internal testnet. We have like a 27K proof already. Next thing is NIBAR SDK. Basically it's a type script SDK to help you to integrate NiBAR with your smart contract. And the next thing is a proof Explorer. And this is building on top of the open source some of our explorers from PSE.
00:21:28.658 - 00:22:03.060, Speaker A: And basically what happens here is that as a dev. So you want to know whether your proof is getting included. We provide both API and proof interface to let you know whether your proof has been included or not. That's the product overview before here. Any questions? Next I'm going to a little bit highlight what happens behind the scene. How us working?
00:22:03.510 - 00:22:13.138, Speaker C: I was just curious, can you go on the previous slide here you just say 32 verified proof and two rejected proofs.
00:22:13.234 - 00:22:22.730, Speaker A: What does the account. So this is a single bundle, right? Single bundle includes the verified proof, but will exclude the rejected proof.
00:22:24.030 - 00:22:25.610, Speaker C: So you're not aggregating?
00:22:25.950 - 00:23:09.174, Speaker A: Yeah, in the final state, the aggregated statement will only come in. So you can think of the aggregated proof as proof of proof. Let's dive into details a little bit in details of the technical parts, right, so currently we're using two layer, two plus layer of the halo two aggregation, which means by default we're two layers. If we have more demand we can expand. It's really due to the current kind of inefficiency of the halo to give this recursion overhead. And the major technique here is that we need to do a lot of non neighbor arithmetics. So I'm talking about the chinese remainder theorem.
00:23:09.174 - 00:24:09.766, Speaker A: This is not a joke actually. So basically we need to code in this way, kind of like a field simulation circuit using the rough idea using chinese remainder theorem based field simulation lookup. And the author is here actually if you want to nerding out the circuit construction details, you can ask Francisco. And on the back end we have a pipeline and distribution proving serving architecture, which means. So basically it's like triggered by the proof summit on chain and we actually chop into the bigger batches, into smaller batches, so that once the smaller batches has been fulfilled, they're already starting the aggregation and we also overlapping different batches as well. So for example, once the aggregate server is working and there is some incoming proofs the next cluster aggregation service will be working as well. It's fully pipeline.
00:24:09.766 - 00:24:49.900, Speaker A: And last but least, we're actually using GPU acceleration to get better latency. So in a sense that from the technical point of view, there are two important metrics. One is throughput. So our throughputs basically can fully be scaled by the demand. Now, the real limit is like how much proof we can put in a single ethereum block, and for the latency, so roughly we get one to two minutes proverbial latency. But your transactions need to have in the man pool a little bit, at least in the ethereum. L one, I think l two s comes the man pool latency is much less.
00:24:49.900 - 00:25:08.938, Speaker A: That's the 1.0 and 2.0. By the way, the 1.0, we will have a test end pretty soon. The Nebra UPA 2.0. We'll try to address a problem that in the 1.0 we can only aggregate graphic team proof.
00:25:08.938 - 00:26:12.498, Speaker A: So instead we're actually building a proof aggregation Vn in a sense that we want to have a generic proof aggregation Vn that supports all kind of proof systems, and also, more importantly, being able to support the future proof systems. Incoming. Because the council ten working in the zero one two proof area is that the cryptographers, they are working too hard and they're building newer and better proof systems over time. So it's kind of moving target. And the idea of proof education VM here is that a single VM instruction, a single VM execution, is a single proof verification. And we can potentially, using the faster recursion techniques, it could be folding, could be like a smaller field, star based continuation to kind of like fold the VM executions, so that we have a much, much powerful premises to aggregate even more heterogeneous proof. And there are some key ideas here we're exploring right now.
00:26:12.498 - 00:27:13.922, Speaker A: The first is that using second curves for even less nonnative arithmetic, it's kind of a little bit similar to like Aztec's goblin plunk idea. And the second idea is that exploring more efficient polynomial amino schemes and designed and optimized for incursion overhead. And lastly, more efficient lookups. That's sort of the technical deep dive we're having under the hoop. Last but not least, I want to talk about some more interesting use cases just beyond just making the proof cheaper. We call universal proof composition, right? Think about, you want to enter in a light bulb at Zulu in 2030, you probably need different kind of proof, right? So first you need a proof of humanity, not necessarily from Lord Coin, but maybe from somewhere else to prove you are human. But maybe we need to have a robot friendly club.
00:27:13.922 - 00:27:44.390, Speaker A: But I'm just picking a particular scenario. You probably want to have a storage proof. You are holding this Zoopas FD and you need to have a proof of exclusion on the forbidden list. For example all the club will ban some like assholes. So you need to prove that you are not. But you need to aggregate that proof, this proof atomically into a single statement. And we can actually just do that from day one because we support universal propagation.
00:27:44.390 - 00:28:49.400, Speaker A: And there could be more interesting use cases to sort of aggregate the composing different vertical vendors and DK startups we can build in more powerful applications. Last but not least, we are hugely inspired by this proof singularity building proposed by Vitalik. And I do think looking even longer term forward, for every youthful protocol being built there could be a debate on to be enshrined a modern shrine and well any like how to protocols input is to integrate proof of irrigation natively in their protocol. Where heck do you talk about that? Yeah, so team effort, like I said, Francisco is here, there are many other folks, and we get a lot of help from great crypto parkers and also New York parks community here. Yeah, I would like to take any further questions.
00:28:59.710 - 00:29:02.570, Speaker C: This notion of proof aggregation VM.
00:29:04.430 - 00:29:04.698, Speaker A: Can.
00:29:04.704 - 00:29:09.130, Speaker C: You elaborate a little bit more? What are the basic operations of this Vm?
00:29:09.470 - 00:30:04.282, Speaker A: Yes, I guess first why you need a VM, right? Because thinking if you view the future, there are going to be many, many. We don't want to just write a circuit for each group system that was too much for us to handle. But if you're just using a single VM, for example like risk payroll, you'll be very efficient to handle cryptographic specific primitives. So our VM is more like a chiplet. So we're going to code cryptographic primitives, for example, curve operations, hashings. When we use as kind of like a circuit, we're going to use a memory argument to sort of hook this kind of mini chip together. And then on top of that beauty and dsl to specify.
00:30:04.282 - 00:30:41.930, Speaker A: And more specifically, if you think about different processes, people are using kind of a different number for process, for reaction error. We kind of need to be more flexible. It doesn't make sense to build a single circuit for each of them. And then after we have just have a very minimal DSL to express the proof verification, and then you can using the fast recursion technique, either continuation or building to kind of aggregate single practice.
00:30:49.710 - 00:30:59.018, Speaker E: I think this message won't be an issue for a while, but I'm curious if you thought of proverb collusion attacks.
00:30:59.034 - 00:31:02.240, Speaker A: At all can be more specific.
00:31:02.610 - 00:31:10.420, Speaker E: Yeah, like a scenario where I don't know how the tasks are delegated to the perverse. That maybe is part of my question.
00:31:11.270 - 00:31:47.950, Speaker A: I guess maybe I can do a little bit of clarification first. Right. So what prover can do and cannot do. So what prover can do is the proverb cannot include your proofs in a single batch. That's the only batch on the proverb can do, basically. So, basically, the proverb cannot submit a malicious proof because we are immutable systems. And because unless you're breaking the cryptographic premise, which actually gives the proverb less like, attacking premises, the real tricky thing actually happens is because everything is MeV.
00:31:47.950 - 00:32:35.040, Speaker A: If the proof, the effect of the proof will end up into some DeFi transactions. For example, we're talking about talking to long term, like, private DeFi protocols. Be worried about some of the MCDV scenarios. Basically, if the prover has some powers of not including your transactions or see your transactions getting some metadata from the proof, by construction, it's impossible because ZK. Right. But maybe there's some public data, like public input. You get some metadata, then theoretically you may have some front revenue opportunities, but when it becomes, like a long tail risk, we probably won't deal with that right now.
00:32:35.040 - 00:32:45.278, Speaker A: Yeah. Thanks for interactive talk.
00:32:45.364 - 00:32:50.734, Speaker B: Thank you. Yeah.
00:32:50.772 - 00:32:52.330, Speaker A: And we have two more speakers.
