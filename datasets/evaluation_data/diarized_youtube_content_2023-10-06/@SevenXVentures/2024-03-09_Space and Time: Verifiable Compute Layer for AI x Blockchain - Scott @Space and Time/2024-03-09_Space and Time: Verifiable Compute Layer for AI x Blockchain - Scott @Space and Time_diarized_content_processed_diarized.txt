00:00:00.570 - 00:01:12.366, Speaker A: All right, well, welcome to a journey through space and time. Space and time is basically a ZK proven database for Ethereum, for EVM chains, and for non EVM chains. We're building the verifiable compute layer to kind of tie in AI and blockchain together. And I'm glad that the folks from story went on before us because they talked a lot about the legal ramifications of IP rights. And in a way, we're actually going to talk about some technology that can also kind of be ancillary to that or tie into that, or be supplementary to kind of the legal ramifications of IP rights. But actually understanding how do we train LLMs in a verifiable way? How do we prove that an LLM was operated responsibly from a data set that was curated responsibly and didn't contain copyrighted IP? Sort of the other side of the coin in a way, using zero knowledge proofs, we were fortunate enough to kind of spend the last year working with Microsoft on a POV around different ways to build responsible LLMs. I'm Scott Dijkstra, co founder and CTO of space and time, and my background was cloud data warehousing boring web two kind of centralizing the world's data into cloud analytic black boxes.
00:01:12.366 - 00:02:06.582, Speaker A: And it's really exciting to be building something that's more decentralized, taking the Internet's data and putting it in ZK proven, community owned, community operated databases that can be proven to smart contracts or proven to an LLM. Before we get into what it means to build a verifiable LLM, or how a ZK proven database somehow enables that, I just want to talk about really briefly kind of what we're doing at a high level. Space and time has invented and patented a novel zk proof for databases, proof of SQL. It's a really, really fast zk circuit that runs on Nvidia GPUs to prove multi million row tables to smart contracts. Now, those multi million row tables generally contain indexed blockchain data. We index data every event from every transaction, from every block, from major chains, EVM and non EVM. And then we load that into a ZK proven system that smart contracts or LLMs can access later.
00:02:06.582 - 00:03:29.550, Speaker A: Space and time is essentially an indexer, plus a ZK prover that proves SQL queries against that indexed data. We're main netting this year and it's going to be a really fun journey as we build towards the truly decentralized space and time network this year. But this talk specifically is about LLMs and about what we can do with zero knowledge with MPC and threshold signatures, and with training data sets that are proven and collected responsibly to create sort of an end to end system for LLM training and usage, where we can prove to our clients as an LLM provider or as a decentralized network that's operating your own LLMs, that what you're doing is accurate and correct, and you've created your LLM network responsibly. It's kind of funny, because as we go through this POV with partners like Microsoft and others, we kind of realize there's no way to do this other than blockchain. There is no technology that's going to enable sort of the antithesis of deepfakes, the antithesis of watermarked content other than the chain itself. And I think obviously, crypto has found plenty of PMF being a casino. It has found plenty of PMF being a better tool for global finance.
00:03:29.550 - 00:05:23.186, Speaker A: But we're about to see some incredible PMF over the next two years using the chain for Providence. And I'm glad that the story protocol team actually teed this up because we're going to sort of carry on that story from where they left off. This is the intersection of AI and blockchain as we've spent the last year with Microsoft trying to figure out what the heck is a responsibly built and responsibly delivered LLM. And sort of this think tank of leaders on the cryptography side and the AI side that were coming together, we started to map things out left to right. On the left side, you have sort of proof of source as you're collecting your data, as you're training your data, and as you're kind of gathering a data set that's going to be used to train an LLM? Does that data set contain copyrighted information, protected IP? Does that data set contain sensitive PII, private, identifiable information from customers? Have we sanitized out the sensitive customer data and have we sanitized out the IP? Well, space and time is not in the business of doing that, but interesting protocols like story protocol could. In addition, how do we know that all the content that we're training on is either synthetic or human created? Are we training on data that was generated by an LLM for the training of another LLM? So sanitizing out synthetic data will be really important, and I'll talk more about that in just a second. Moving on from the source data, how do we prove that once we've collected this source data set that it hasn't been tampered from the source? How do we sign or watermark each individual sort of table or batch of data that we collect from the web, from enterprise data warehouses, from multimedia, across NFTs, or the web itself? As we collect this data, how do we prove that it hasn't been tampered from the source? Why does that matter? We're already seeing 100 million dollar lawsuits of the New York Times suing OpenAI for the content that they trade on, saying that OpenAI trained on New York Times articles when they shouldn't have.
00:05:23.186 - 00:06:51.758, Speaker A: And whether OpenAI did train on New York Times articles or not is almost irrelevant. What's more relevant is what kind of answers come out of the LLM when asked to repeat those articles verbatim in a lawsuit? How does a litigator come back six months later or a year later and look at an LLM provider and say, hey, I know you trained on my protected IP. I know you trained on my articles or my media or my movie or my photo? The problem is that LLM provider can just simply scrape that protected copyrighted data out of their training data set immediately after training. Right? They can just erase the majority of what they trained on, erased all the New York Times content, all their Disney content, all know Google publications that they've scraped all the Twitter data they've scraped from Twitter before Elon locked down the API. And as they do that, how do litigators a year from now, or two years from now, or even just six months from now have any kind of confidence that when they look back at a data set that an LLM provider claims they use to train an LLM was actually the data set they really use to train the LLM? You'll need some kind of tamper proof system that can cryptographically prove that a data set has not been changed since training? Well, Scott, what about the next piece of it? Let's say you could actually prove that a data set hasn't been tampered. How do you prove that the untampered data set was the same data set actually loaded into the LLM during training? And that's been the focus of our research. That little connection right there.
00:06:51.758 - 00:07:37.434, Speaker A: It's not about proving inference itself. We're not trying to build ZKML. There are plenty of incredible teams already working on that and accelerating in that vertical. Rather, it's about how do we prove that the training data set that an LLM provider said they trained on actually made it into the LLM and is the same data set that litigators can refer to later during arguably a lawsuit. And I'll get into how we do that and what our thought process is behind that piece of the cryptography in the middle. We're talking about ZKML or some kind of optimistic ML or some kind of cryptographic confidence. Whether it's OPML or zkml or even consensus driven, that inference was done correctly.
00:07:37.434 - 00:08:21.406, Speaker A: This is the majority of what we see of series a funding rounds right now of high quality teams that are building really impressive cryptography to try to prove the LLM inference. The problem, of course, is proof time overhead. Right now we're seeing anywhere from 180 to 1000 x overhead, both in cost and proof time, to build zk proofs against LLM inference, and I worry that that's going to be problematic. What might be more practical for on chain use in the short term is a consensus driven or optimistic approach, and there are plenty of startups working on that. Really. That middle piece is the one area space and time itself is not focused on. We bring in partners and Microsoft brings in partners for that.
00:08:21.406 - 00:09:51.260, Speaker A: We are not building ZKML. My concern is this. You collect your data set responsibly, you do everything right. You sanitize sensitive customer information out of the data set, you sanitize protected IP as best you can. How do you prove that when you're providing this LLM to a third party who's paying you for it, that you're actually running llama two and not llama one? Or you're actually running maestrel seven instead of six? Or you're actually running Falcon whatever instead of Falcon three? Or how does a decentralized AI network give confidence to their consumer that they're actually running the correct model that the consumer is paying for? With the right weights, we potentially see an approach coming to the table soon where the LLM can instead of building a ZK proof of the actual LLM inference, we kind of create a hash of all the work done during LLM inference, and then a statistical model that can statistically prove within a reasonable statistical likelihood that a certain model with certain weights were ran based on the hash that was outputted from a certain input along with the LLM inference. It's not quite as zk proof, and it's not quite as cryptographically or mathematically sound as zk proof, but it could be a two x or three x overhead rather than 180 x overhead. Let's say that middle piece gets solved by some brilliant people, either in this room or people you know, that are already working on solving this problem at scale in a practical way.
00:09:51.260 - 00:11:07.406, Speaker A: Moving on to step four, once we can prove that the LLM inference was done correctly. What about rag? I mean, the big issue is everyone's just dumping all their enterprise data into a vector search database with no sock, two compliance, no care for privacy, IP rights, security. Everyone's just taking all their enterprise data out of Snowflake and postgres and just dumping it into a vector search database along with all their multimedia, all the vector embeddings of all their pictures and videos. And when they do that, and they retrieve from that vector search for retrieval augmented generation, where I'm retrieving content from a vector search database, adding it into a prompt, and then sending that prompt over to OpenAI or some decentralized LLM network that's providing me inference, how do we know that vector search database doesn't contain New York Times articles? We're back to the same thing. Whether the LLM was actually trained on copyrighted content, or whether copyrighted content was pulled from a vector search database and injected into a prompt during inference really doesn't make a difference from a legal aspect like the courts will see that the same way. So we have to prove, or at least give some cryptographic confidence from the consumer side in that they're using a vector search the right way. This is optional, right? This is sort of ancillary to the whole, like, LLM usage.
00:11:07.406 - 00:12:18.246, Speaker A: This is about correct usage of vector search. And then finally, probably the most interesting area of research that we're going down is watermarking content that has been synthetic, that has been generated, synthetic content that was AI generated, either from an LLM or a generative photo model or generative video model. How do we watermark that content so that browsers or content delivery systems can sort of alert the user that this set of content is synthetic AI generated. This to me is even more problematic than trying to zk prove LLM inference. The reason it's so problematic is even if you come to some kind, even if the industry starts to adopt standards of how we cryptographically watermark generated content, there's really no incentive for browsers to adopt those standards as well, or like any kind of content delivery system, to adopt those standards and alert consumers that content is generated. Because generated content is still only a tiny, tiny percentage of the content on the web today. And even as that grows, even as the percentage of generative content on the web grows over the next five years, I imagine it'll still be a relatively small percentage of what's out there.
00:12:18.246 - 00:13:31.270, Speaker A: Just given the sheer magnitude of content already on the web, even when it grows to 30 40% of the web, how do we get browsers to adopt the standards that we as like cryptographers implement on watermarking content? So I'm teeing this up because this is what we've been thinking about for the last month while building a zk proof of SQL. At our core, we're just doing zk research and we're just building a blazing fast proof of SQL in a SQL database so that either litigators or smart contracts or LLMs can request that data and we can prove that we haven't tampered with it. So an LLM provider can load their training data set into space and time and then load it out of space and time into an LLM for training and have cryptographic confidence that they can provide to a litigator a year later that no, I have not tampered with my training data set. Here it is, and it does not contain any copyrighted information. So we think about it this way, kind of going end to end from left to right. Think about indexed blockchain data that we're bringing into space and time along with terabytes of crawled data from a web crawler that crawls the web. Think of like a grass protocol.
00:13:31.270 - 00:14:43.570, Speaker A: Love what they're doing. And I'm sure there will be others like them that try to build very high quality browser plugins that crawl the web on your behalf and pay you to do so. Creating structured and unstructured data like structured data like blockchain information, stock market information, unstructured data like Text corpus from news articles or scientific publications or photos or video, and then signed off chain data provided by source data providers. Think like a tradfi market. Think of Bloomberg signing all of their data, both research reports as well as raw market data, and sending it into this training data set so you could train an LLM on text corpus or train another model that's more sophisticated on maybe like stock market research from Bloomberg. That data, when it enters space and time as a network, space and time threshold signs every piece of data and creates cryptographic commitments on that data. Essentially a watermark that we can use later when zk proving so that when an LLM or a smart contract on the EVM or a judge requests some data, we can return a query result as well as a cryptographic proof.
00:14:43.570 - 00:15:20.910, Speaker A: We just got awarded yesterday a second patent from the US patent Office on a proof of vector search. So we now have a proof of SQL and a proof of vector search kind of in our back pocket. And we'll be building out that zk proof for vector search as well. Basically an approximate nearest neighbor that we do via ZK with like a reasonable practical overhead. So all that means is space and time just becomes like persistent ZK proven persistent memory for the LLM, like SQL storage and vector embedding storage. That's ZK proven for the LLM. And for litigators that are curious about what's actually happening under the hood with that LLM.
00:15:20.910 - 00:16:50.682, Speaker A: As this data gets trained and as kind of this continued drift occurs with models like Falcon, llama, two, Maestro, where continued fine tuning or continued training of additional data through these models drifts, the models and their outputs. Those outputs can kind of be sent all the way back to the beginning as input data to a new training data set within space and time, where we can kind of track that drift and prove to the consumers using a model that sort of the model operators are doing everything correctly, they're running the right model with the right weights and with the right training data set. Something that we're really, really excited about right now is just the insane proof times that we've gotten down to for SQL. What a lot of folks in the ZK space are just entering the zero knowledge space. Don't realize is that if you hyper focus on a very, very narrow problem set, you can really, really optimize a ZK proof with some novel techniques, some novel circuits that get you much, much better performance, arguably 100 x performance over like a ZK VM, for example, that has to support arbitrary computations. So ZK coprocessors that are trying to just support everything like hey, give us your JavaScript script or your rust script and we'll actually run it as a circuit for you. Because of that unconstrained problem space, there's usually much more expensive and much more costly proof times.
00:16:50.682 - 00:17:38.426, Speaker A: Like we're seeing proof times anywhere from five minutes to 35 minutes on arbitrary scripts against arbitrary sets of data. However, if you can hyper focus on just one specific problem set, in our case it was SQL. Just hyper focus on the types of computations that you see in SQL. Aggregating data filtering data sorting, data projections joins you can hyper optimize your ZK circuit and get insane proof times. So we built this GPU accelerator from the ground up called Blitzar over the last three years to make this run super fast. Because the problem is, if we're talking about terabytes of training data for an LLM, no one's going to want to wait two weeks for a ZK proof to complete, much less two months or something, right? People aren't going to want to even wait two minutes, really. They're going to want zk proofs as close to real time as possible.
00:17:38.426 - 00:18:20.680, Speaker A: Where we are right now, our most recent benchmarks are down to about a million and a half row tables. We can prove within a million within Ethereum block time. So like, if you're running a zk proof of a medium complexity query on a single Nvidia GPU against a one and a half million row table, we can prove that in under 10 seconds. We're down to subsecond for 100,000 rows. So if you want to just run a proof against a table of 100,000 rows, we're down to like 0.8 seconds, which is really exciting because I don't think anybody really imagined that subscond ZK proofs would be possible, especially delivered and verified on chain. And with that in mind, I'll kind of get into a quick little demo of space and time and we'll wrap there.
00:18:20.680 - 00:19:02.374, Speaker A: The purpose of this was just to introduce kind of our technology and where it plays into this verifiable LLM space and kind of where we're going. But enough talking, let's see it in action. What you're seeing is just a fun user experience on top of space and time. Of course, our core business is zk proofs and we just thought, hey, if we have all this indexed blockchain data and we have this very fast proving system, then let's at least build a fun ux on top of it. Powered by AI, that makes it really easy for customers to find the data they need and query it. Oops. Cat test.
00:19:02.374 - 00:19:48.646, Speaker A: One cat test. It really does not want me to type this. Oh, nice. Okay, sorry about that. Okay, so the first thing you see when you open up space and time are different data sets from popular chains that we've already indexed. Of course, that can also include enterprise data sets at terabyte scale uploaded by our own customers, as well from different data sets that customers provide off chain Bloomberg, the fidelities of the world that are loading their own research reports and tradfi data. If we open up Ethereum, for example, we'll see a familiar kind of dune style data model.
00:19:48.646 - 00:20:28.840, Speaker A: You can join the transactions table with your blocks table with your smart contracts table, et cetera. Our hyper focus is zk proofs on blockchain data, but of course, in this effort to secure LLMs, we've expanded well beyond just index blockchain data. Like here's for example, like a contracts table you can join with a transactions table. So then when you go to write queries, you can say select all from Ethereum E. Maybe I'll be able to type. Sorry about that. Okay, thank you.
00:20:28.840 - 00:21:11.698, Speaker A: From Ethereum blocks and as I start typing in a query, it kind of auto updates to show me the tables that I've referenced in my query and any other tables that I can join in order to make it easier for me to write my query. But the real goal here is this. Just give litigators, auditors, smart contract developers, anyone who needs this data a much better user experience. With AI, nobody wants to write SQl. It's a very powerful language for aggregating data, but it's also very annoying to write. So you should just be able to type in a prompt. Show me all Ethereum wallets with a balance greater than 100.
00:21:11.698 - 00:22:02.100, Speaker A: And when I submit this, in a few seconds OpenAI is going to retrieve a bunch of context from our vector search database about these tables and a long year long prompt engineering. A year's worth of prompt engineering takes that set of schema information that we vector, retrieve and writes accurate space and time dialect SQL that can actually be executed against the prover. And then the prover returns back a proof in 4 seconds against this data set. And we can now make this query more complicated. So let's say show me all Ethereum wallets with a balance greater than 100 and at least two transactions on chain. And I don't know, give me another criteria here. Maybe one transaction over $1,000 this week.
00:22:02.100 - 00:22:59.004, Speaker A: So this is a relatively complex query. I'm guessing it's going to be about a 26 line query that's going to get generated here, which would take me like 30 minutes to write because I'm not that good at SQL. It's kind of an annoying language. And what gets generated for us is essentially like, let's see, 31 lines of SQL with a subselect and like a sub query and a CTE within it. And then executing this against the prover might take a minute because we're talking about a billion row table and a relatively complex query, but the prover can then return this result back to a judge or back to a smart contract, or even back to an LLM that needs a subset for training or fine tuning. And finally, once you're happy with the queries you've executed, you can open up. I'll call right now.
00:22:59.004 - 00:23:08.210, Speaker A: You can just create a new dashboard by just dragging in any visualizations you've saved and you have a new dashboard. I know we're tight on time, so I'll wrap here. Appreciate you guys.
00:23:12.340 - 00:23:31.140, Speaker B: Thank you so much. Any questions? Right, so have you ever benchmarked the language model training in Norway? In any widely acknowledged benchmark like human or math?
00:23:33.260 - 00:23:46.152, Speaker A: All we're doing is securing the training data set. And the MMLU is a benchmark of the quality of inference against subjective and objective benchmarks of the outputs of the LLM, whereas we're securing the inputs to the LLM.
00:23:46.296 - 00:23:51.170, Speaker B: Right. But if you filter out all the data that have copyright issues.
00:23:52.900 - 00:24:04.196, Speaker A: You'Re wondering how much that brings down the quality of inference on MMLU when you sanitize out all of the 30% of the training data that was copyrighted, right? Yeah.
00:24:04.298 - 00:24:14.672, Speaker B: Right. So, in fact, that if you filter out all the copyright issues data, then ROM will be very dumb.
00:24:14.816 - 00:24:25.050, Speaker A: And you're right. So we tested this and it brought like MLU score down from like 81% on Maestro down to like 60, something like 64%.
00:24:25.580 - 00:24:27.370, Speaker B: It should be lower than this.
00:24:28.220 - 00:24:30.344, Speaker A: Well, they probably didn't sanitize everything.
00:24:30.542 - 00:24:51.010, Speaker B: So the current way that they will solve the problem is that besides for mutual, they just raise money to buy data to buy the copyright system so they can train a model that surpasses the GPD 3.5 and close the GPD four. So that's a real world way to go.
00:24:52.660 - 00:25:30.636, Speaker A: And to your point, as corporations like Reddit sold their data set for 60 million or something too. As more and more corporations that have data sets like Twitter and Reddit start selling them to Maestro and Google Gemini for eleven figure, ten figure deals, it'll be interesting to know how those data sets get secured. Like how does Reddit provide Google Gemini their own private protected IP? It should really be like a ZK proven database that they use to provide the embeddings and encode the embeddings rather than giving Gemini the raw data. You know what mean?
00:25:30.738 - 00:26:27.550, Speaker B: Yeah, right. For the language model, the company that has the capability to train the foundation model usually has a lot of money, like Mitro, like Google, like Microsoft. So they do have the capital to buy those copyright data, and then they release the trained model to the public and so the open source community can use it. So that's how it works. Currently in the open source field, I think the provable RN or the training process, its customer might be some big company instead of the decentralized lab. So they don't have the compute to train. Have you talked to any large companies that are willing to use this kind of.
00:26:28.100 - 00:26:48.900, Speaker A: Yeah, much more. On the enterprise side, we haven't really gotten started on the web three side. We're talking to a lot of the Capgeminis and KPMGs of the world that want this from a corporate consulting standpoint. But where I think it's most interesting is in decentralized LLM network that will need better data that they can't necessarily afford to purchase.
