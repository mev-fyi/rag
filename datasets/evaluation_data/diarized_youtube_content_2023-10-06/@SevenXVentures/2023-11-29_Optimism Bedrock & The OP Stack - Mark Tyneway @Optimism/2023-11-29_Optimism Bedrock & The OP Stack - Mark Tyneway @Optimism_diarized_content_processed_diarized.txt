00:00:01.370 - 00:00:49.994, Speaker A: So my name is Mark and I'm a contributor to the optimism collective. And I'm here today to help share some good mental models for roll ups because I think that a lot of people have like high level kind of knowledge of roll ups but you know, getting some mental models because I do a lot of fun things with roll ups. So this is Ethers Phoenix. Ethers Phoenix is very important. You should know Ethers Phoenix because Ethers Phoenix will reward you in the future if you play pop of some games today. So just always remember that. So Bedrock Bedrock was like our big release of Rolo software, right? And it's a new architecture for Rolos because we've rewritten the system many times.
00:00:49.994 - 00:01:41.040, Speaker A: I think Bedrock is like the third or fourth, basically the design philosophy, right? Minimal gift from Ethereum. This is really, really important if you're working on a roll up and your gift from Ethereum. The bigger your gift from Ethereum is, the more difficult it will be to keep up, the more difficult it'll be to have people come and contribute. So yeah, minimal dip. Most important thing also we want to design the proof to fit the system instead of designing the system to fit the proof early on, early iterations of optimism, we designed proof first and then built the system. And that resulted in a large dip from Ethereum. So we've learned a lot.
00:01:41.040 - 00:02:07.486, Speaker A: Cool. So we have a modular design in the way that bedrock is architected. Right. One of the key ideas is that we have this split between the consensus layer client and the execution layer client, right. And shout out to proto. It would not be possible without proto. And the rest we have this op node.
00:02:07.486 - 00:02:56.130, Speaker A: The op node is what replaces the consensus layer client. And then our execution client has like about 1000 lines of code as the diff. So you could run a full bedrock node running both this execution client with a patch plus the op node. Right. And this allows for a multi client role architecture. Okay, so first important concept, block derivation. Right? So what block derivation is it's basically taking available data and it does this like filtering and mapping from the available data into the l two blocks.
00:02:56.130 - 00:03:41.460, Speaker A: Right. So kind of the problem statement is like out of all the possible data that's available, how do we filter this into the data that we care about and then turn that data that we care about into the inputs to blockchain. So for ethereum, the inputs that is like the execution payloads, like the input to the execution API. Right. So yeah, in practice this is turning Ethereum call data over block data into alti blocks. So I think Archim calls is like the filtration function. Different projects have different names, but derivation, it's very important.
00:03:41.460 - 00:05:23.458, Speaker A: All right, so we're very early in the age of roll ups, so there's some really cool things that I think we can do with the derivation function if you modify it, right? So what if we add to the derivation function the ability of one roll up to modify validity conditions in another chain derivation function, what does that look like? I don't think anyone, I'm not aware of any projects building anything like that design. Can you build threshold photography schemes directly into the narration function itself? So you're kind of adding these MEP prevention tools to the consensus. Exactly. Directly in relating. And the other question is, can you have a base style leader election directly in the derivation function itself? So this is kind of like an idea that the Ethereum foundation is really into, like allowing layer one ethereum to basically do the ordering instead of say like the Lc sequencer. So these are all design spaces that are very underexplored and could be explored in the future. Okay, so this is another fun one, right? So this derivation function, right? What happens if you have two different l ones as the loop? Your derivation function, right? So normally roll ups, they operate, the derivation function operates over one l one.
00:05:23.458 - 00:06:55.954, Speaker A: It maps the data in l two blocks. So what happens if you have two l ones as inputs? The derivation function, right. And then this kind of turns your roll up into a chain or a bridge, right? And what if there's a world where anybody can fully validate the bridge by running these roll up nodes? And what if you allow for smart contracts to this bridge node? What does that mean for the bridging ecosystem as a whole? So downside, right? Seven day withdrawal window to withdraw to the other side. So there's two solutions for that. One is you can kind of build these like fast liquidity bridges where basically somebody else has money on the other side and they can do an off chain validation that you withdrew to them and they can just give you the money, you just give them a little fee or validity proofs. I suspect that if we had a bunch of bridges built like this, it would probably kind of look like the lightning network. You have hubs, spokes, and then you could potentially have a payment channel kind of thing going through them.
00:06:55.954 - 00:07:42.430, Speaker A: So this is something I'm really interested in. So ether spoenix. Okay, so we have this concept of deposit transactions, and this is one of the big dips that optimism execution has compared to l one execution. And basically the idea is that as derivation is running, it will build these deposit transactions. So deposit transactions are not signed by users necessarily. They are just included by derivation deterministically. And they can also be kind of thought as like system transactions.
00:07:42.430 - 00:08:36.850, Speaker A: So basically the idea is that the first transaction of every single l two block contains contextual information about l one that gets pulled into l two. For example, like the l one block number, the l one block hash, the l one base fee, and then there's some other fee parameters. But this kind of allows us to pull information from l one into l two. That is useful for the l two system to say like charge fees or APIs for developers. So deposit transactions, they can also originate from users. Right. This idea is that an l one event maps directly to a deposit transaction.
00:08:36.850 - 00:09:56.186, Speaker A: So there's a one to one mapping here and deposit transactions. This is how you kind of, it's a censorship resistance property. Assuming that someone on l one is not going to censor you, you can deposit and you're guaranteed that it's pulled into l two with a relatively low latency because of the way the derivation pipeline works. Okay, so research question, what can we build with generalized system transactions? Right, and an example here is because like this idea, right? One event on l one maps directly to a deposit transaction on l two. So can we build an l two that basically automatically ingests all chain link oracle updates into l two automatically as part of the l two command release? Technically, chain link doesn't emit events because they don't want this kind of thing to happen. But there's no reason why we can't take, say, the set of execution traces as input to the derivation function. That would add some more complexity and cost.
00:09:56.186 - 00:11:21.970, Speaker A: But if we had the set of execution traces, we could really do anything with this, right? So I think this is a very exciting, okay, so question, can you virtualize liquidity from an l one amm into l two? Now, basically the idea here, right, is the assets would all wave on l one still, right? So is there like an interesting sort of amm that you can build, or some sort of like d five protocol where the set of events that are emitted by this protocol, l one, are kind of like virtualized into l two, where users can kind of interact at a much lower latency, and then there's some sort of process for kind of batch settling the state back into l one, like ownership wise. So yeah, I think that this is really interesting for people building d five. Cool. So this is another question that I have. So basically one major problem is how do you bootstrap a dFi ecosystem on an L two? Especially with proliferation of l two s? There's a lot of l two s that don't have a lot of liquidity. Right. So I'm wondering if it's possible to say, okay, so the problem here is lending.
00:11:21.970 - 00:12:02.498, Speaker A: Lending is important in an economy. If you want a good, healthy on chain economy, you need to be able to borrow. You need some lending. But lenders don't want to lend a lot of money unless they're sure that bad debt can be liquidated. And the only way they can liquidate bad debt is if there's liquidity to do so. But if you're on a chain that doesn't have a lot of liquidity, then your debt ceiling is only going to be so high. You only take out so much debt because you need to have enough liquidity, and then the project needs to always be aware of how much liquidity is on the chain, so they can basically modify the debt ceiling over time.
00:12:02.498 - 00:13:01.350, Speaker A: It's kind of annoying. So my question is, can we build some sort of trustless liquidation mechanism into l two, where basically through these events, the derivation pipeline is able to pull information about liquidity on l one into l two? And then if we have trustless rallies to do the liquidation, can we have higher debt ceilings on l two s that don't have a lot of liquidity? This is like my idea. I have no idea if it'll work, but I would love for a d five project to test this out. And basically the idea here is that this is infrastructure that helps to bootstrap liquidity on new l ships. All right, this is a fun one. I really like this one. So this is more of a thought experiment.
00:13:01.350 - 00:13:40.180, Speaker A: But the idea is, using the same sort of scheme, you could create a roll up where basically all of the ether that is burnt on l one, you reincarnate that ether into a roll up where, like, a person that burned it on l one gets to own it on l two. Right. It's like a very interesting idea. Would that ether be valuable? Right. And this is totally something you could build with the op stack. So I hope that someone builds this, because it would just be like. It'd be really fun.
00:13:40.180 - 00:14:07.798, Speaker A: It could be like the new, like. Yeah. Cool. So, all right, back to multiple clients. Right? So what I was saying was executing plan requires about a thousand line gifts to work on alchemism. So we have op GEF. That's the client that we maintain.
00:14:07.798 - 00:15:15.474, Speaker A: We have op Aragon, and we also have Opref, and they can all sync networks already. So basically the idea here is that client diversity is important for preventing bugs from becoming consensus. We've seen this on l one a bunch of times where there's a network split, and because there's multiple clients, social consensus is easily able to determine which chain to follow, right? Yeah. This is why the minimal diff is very important. We are now getting all the work the paradigm team is doing through reps for free. We're just like scaling the contributor from spy, keeping this philosophy of a minimal diff. Okay, so proofs, proofs are very important, right? So proofs, that's how the bridge works, right? You either have an optimistic proof or you have a validity proof, right? So basically the idea is that the way this system was built, it's modular, so it's not coupled to any particular proving system.
00:15:15.474 - 00:16:42.826, Speaker A: And the idea is that we want client diversity of proofs, because for an l two, client diversity of proofs gives you the same security model as like an l one that has actual client diversity of full note implementations, right? Because in the proof system, if there's a bug, then money can be stolen. So the idea is that if we have multiple proofs, then one way to design it would be if someone is able to show that two proofs resolved differently, then the bridge can be paused, right? And then when the bridge is paused, then social consensus can come in and determine which proof system is like canonical. And then we can triage fix the bug and then get the proofs together. So this would give the exact same sort of security that l one has where multiple full node implementations they're running, they're running, there's a bug chain split. Social consensus comes in, figures out which chain to listen to, and then picks the right one, fixes the bug. So l two s can never be as secure or they can never truly inherit the security of an l one unless they have proof diversity proof diversity is very important. The thing about the obstac is that we can adopt validity proofs when they're ready and there's two different teams.
00:16:42.826 - 00:17:31.370, Speaker A: People are here today talking about how they're working on liquidity proofs. So it's only a matter of time until liquidity proofs are ready and they can be slotted into the multi proof system. Okay, so I think that deploying a roll up is the new deploying a smart contract, right? We have basically back in 2016, 2017, it was like pretty crazy, like deploying a smart contract, it was like the wild, wild west, right? Nobody really knew what they were doing, like crazy things were happening. There was a lot of innovation. Deploying a smart contract these days is kind of boring, right? We've done a ton of stuff. The tooling is very mature. We know different design patterns.
00:17:31.370 - 00:18:13.930, Speaker A: We've explored them a ton. So the future is now. You deploy your entire roll off, right? And your smart contract system is in that roll off. And basically this solves one of the major incentive miscompatibilities in the ecosystem, which is dapp developers don't earn any money from usage. You could build a fee into your protocol, for example. Uniswap did that. They haven't turned it on, so who knows? So transaction fees are a source of revenue per apps, right? More users, revenue goes up, right? And that's how the web two world works.
00:18:13.930 - 00:18:56.230, Speaker A: So we need this in crypto and web three for web three to work there. And basically what we need for this to really be true is we need low latency, cross two lane messaging. Because once we have low latency, cross clear messaging, just liquidity can move between all the different l two s. And there's a bunch of different projects working on this. And I think that this is what will really be, like the big unlock. These are scenics. All right, so I've got a slide about roll misconceptions.
00:18:56.230 - 00:20:04.350, Speaker A: So basically, I think there's like, one really healthy way to think about roll ups is really decoupling the bridge and l two in l two's finale, right? The outputs. So the outputs of a blockchain or like a roll up would be like, say the state route, right? And the inputs would be like the block or the transactions. So the bridge only cares about the outputs. And you should always think about the bridge. The bridge just needs a view into the output, right? So when you have an output and you give it to the bridge, like state route, people say you're posting the state routes to l one. It's really like, you should think about it as like you're proposing possible outputs to the bridge, right? And it's up to the bridge to defend itself from malicious proposals. So in the case of the Zk roll up, you need a validity proof.
00:20:04.350 - 00:21:07.890, Speaker A: Validity proof will prove that this is the correct output that deserves to be on the bridge. And with an optimistic roll up, in an ideal optimistic roll up, anybody should be able to make a proposal about the l two outcomes. So, like, this is the state of the l two at this block I propose. And then it's up to the fault proof to basically defend from malicious outputs. So thinking about the bridge as like a separate thing from the blockchain is a really good mental model, because there are two different things. Like, one thing that people get confused about is if, say, the fault proof removes an output from the bridge contract, does that cause a reorg on all two? And in the world where they're coupled, that makes a lot of sense. But in the world where they're decoupled, it doesn't make any sense because they're two different things.
00:21:07.890 - 00:21:44.700, Speaker A: The l two is kind of responsible for its own ordering. Right. And all of the outputs that are proposed at the bridge, they're proposals. They're not necessarily like, they're not actually saying, like, this is what the state of the l two is. It's like, I claim that this is what the state of the l two is. So, yeah, the outputs are only relevant to the bridge, and the outputs are not relevant to the l two ordering or the finality of the l two. The outputs are just to give a view of the l two.
00:21:44.700 - 00:22:18.474, Speaker A: Right. Should be considered as claims rather than the truth. That's pretty much my presentation. I kind of went really fast. All right, who's got some questions or comments? Why is the bridge withdrawal period of seven days? Good question. Well, there once was like a high priest that came in and said seven days, and then everyone believed it. I think that seven days.
00:22:18.474 - 00:23:09.320, Speaker A: I would love to figure out ways to reduce seven days, but the idea is that seven days is long enough to, say, play the fault proof game right and get it included. Arbitram came up with an interesting way to try to reduce that, where they used some probability to kind of look at. I think it was like they had something. It was like the number of blocks that the number of missed slots or something, or the number of. They tried to quantify it through reorg out blocks where the next slot built on the same block instead of progressing the execution layer. That's the envelope map. Right.
00:23:09.320 - 00:24:03.850, Speaker A: The cost to censor how many blocks is going to run the entire censorship fee. And so you call it like, what is the average fee cost of a block on l one and how many blocks or whatever? Right now back on, I think last I ran the map in that ballpark of like, pa. Yeah, I mean, there's a lot of assumptions there. Like, you're assuming that the infrastructure is always live, right? So I think that with time in running these systems, in production, we can probably shorten it. But right now, there's like, no. Okay. So one idea that I was thinking about was, can we say, have the fall proof window or the finalization window for the bridge be dynamic based on some sort of, like, over collateralization of the bond.
00:24:03.850 - 00:24:45.686, Speaker A: Because the whole idea is that the security comes from the bond because you put down this bond, right? And if you're lying, then the counterparty can behave your bond, right? So one problem here is that if the bond becomes under collateralized, right. So you want more time to allow the base fee to come back down so the bond can become collateralized. So it's like, if I have two weeks on l one, I get, like, one. Eat on, like, l two or something. I got to go faster, maybe. I have no idea if this will work, but this is like a design space that I've explored. It might not work at all.
00:24:45.686 - 00:25:37.240, Speaker A: But I do agree, like, seven days is like a terrible thing because it's also. You have to do, like, back and forth, right? It's like the multi roundedness of the interactive game makes it, like, a much harder design. Because the other thing we had, the rainbow bridge right on near, was, like, literally the first transaction that was possible, right? And then SMG has done their little playful experiment of like, okay, it's like, however many eth to buy a block and send through it, which you can use as an approximation of like, that's the cost to guarantee I bought the whole block. So in the multi, how do you consolidate the. For example, for our. Yeah, that's a great question. So we don't have a concrete design for it yet, but we've explored a few different things.
00:25:37.240 - 00:26:25.000, Speaker A: There's, like a world where we kind of have multiple bridges. And I think there would always be some amount of time with a validity proof that's, like, added on top. So one thing about validity proofs is I really don't understand why everyone talks about validity proofs as if there's not bugs in them, right? They're like brand new crypto. So this is the thing. Validity proofs bridges. To operate on validity proofs, they basically couple execution with proof validation, right? And I think that's a big no no. And the reason why is because someone that is able to find a bug in that proof, they can submit the proof to the bridge and instantly the execution happens.
00:26:25.000 - 00:27:13.158, Speaker A: So the idea with an optimistic bridge is the validity of the proof is decoupled from the execution. So this is good because it gives some time for layer zero, like the community to notice that someone found a bug in the bridge. So I think in the short term, probably there will be, like, if there's a combination of valid proof and optimistic proof. The validity proof will probably shorten the amount of time for the withdrawal, but not make it complete. But this is just like, we don't have necessarily concrete plans, but this is kind of just like one design piece. And potentially, let's say you have two different kind of code base. One from phenomena, one from zero.
00:27:13.158 - 00:27:47.614, Speaker A: And these two agree, even make it faster. Totally. So then we have like this trade off between time and cost is like generating many different proofs. Hopefully they become a lot cheaper, so it's easier to do things like that. But yeah, completely. Two questions. One is this bond value, how much is it right now? And I just do not have a good mental model and I didn't find it on the docs.
00:27:47.614 - 00:28:35.300, Speaker A: And the second one is, how do you think PBS impact the censorship resistance of layer twos? Totally great question. So there's two ways you can design bonds. The better way to design bonds is basically every step in the game. You put a bond down that is large enough for the next step. So this keeps the bonds relatively small. So this means bond size is relative to the amount of gas that's used for the next step. So ideally, we can have really optimized smart contracts to keep the bond size.
00:28:35.300 - 00:29:11.450, Speaker A: How low is that? Two e 200 y. It'll probably be lower than that. I don't know, just ballpark. Got it. But early on, we'll probably add a nice padding to it, so just in case the infrastructure is down or something. Right. But over time, as we run it more, I'm sure that we can make the bond more efficient, both by being sure that it works in practice and optimizing the smart contracts.
00:29:11.450 - 00:30:04.238, Speaker A: Also, data analysis of the base fee price over time. Right. But one thing is that past performance doesn't indicate the way the future is going to play out. Every bull market is different in its own insane way. So if we just strictly rely on historical data, we're like, all right, we want five nines of assurance that the bond will be large enough during this amount of time. I have no idea if there's going to be another board capes sort of thing that pushes the gas price to an absurd amount. So, yeah, I think TLDR it won't be like many east, it'll probably be larger than it needs to be early on.
00:30:04.238 - 00:30:45.900, Speaker A: And yeah, it'll get smaller. And then your other question was about flashbots. Made another. Mine actually made a proof concept of me boost with the op stack. So it does work. Sorry, I was asking from a censorship resistant perspective because now I can bribe builders to always censor your thing, become the cheaper, the more expensive. Yeah, I mean, it's totally a thing.
00:30:45.900 - 00:31:24.520, Speaker A: I would say it's very similar to l one, I think with lower, a little bit different. Right. Because I have no idea. This is my intuition. I don't know if the data backs me up, so help me out here, but my intuition is that with faster block times, you have less of an advantage of having the best algo. Because if you have the best algo and you have a long block time, your really good algo can crank for a really long time building a block. And then you also have.
00:31:24.520 - 00:32:04.846, Speaker A: With short block times, you have to build a block really quickly. So having a really good algo might not be the best thing. We might have more simple heuristics, more people can figure out, creating blocks and l two s. So I have no idea if the same block builders will always win PBS and l two with world block time in the center. I have no idea if there will be many block builders and there'll be more difficult to build a monopoly. So, yeah, I really have no idea. I don't know.
00:32:04.846 - 00:33:10.802, Speaker A: Nobody was running pbs in production on an l two with, like, two second lockdowns, too. I don't know if this question makes sense, but people talk a lot about forced inclusion. Is there such thing as forced execution? Does that sort of end up. So forced inclusion is forced execution, at least with the op stack, basically, and it's low latency as well. Basically, the idea is that you send a transaction on l one to this particular contract. It emits an event, and we run with a confirmation depth of like, three, I believe, on main net. So, you know, within, you know, three blocks, you know, your deposit will be pulled in, and you could, in theory, run with no confirmation death, and just instantly be pulling in l one blocks, and instantly including deposits.
00:33:10.802 - 00:33:57.074, Speaker A: And as soon as the deposit is included, it's executed. So if there's like, l one block, and my transaction is in there to be forced to cool, then there's an l two block. I'm guaranteed it would be executed before the l two block. Yeah. So every l one block corresponds to one or more l two blocks. On op mainnet base, there's six l two blocks that correspond to every l one block, because the block counts 2 seconds. So when you send a deposit to this contract in l one, basically it'll be pulled into the derivation pipeline and instantly be turned into a deposit, and then instantly executed.
00:33:57.074 - 00:34:43.766, Speaker A: And all the deposits are placed at the front of the l two block. So the sequencer doesn't get to choose when it includes deposits. It's like built directly into the protocol under the derivation pipeline. So you're guaranteed to get this like low weight. So you have question about what you just said previously about validity. You said that if I have no way to fit, they're going to be a window. Because I do not understand why you're saying that, and I do not agree.
00:34:43.766 - 00:35:38.310, Speaker A: Okay. Mostly want to have advice. Totally. So, I mean, definitely feel free to disagree, but I 100% believe that someone will steal a ton of money out of a bridge that is secured by, you know, even Zcash had a bug in its implementation. I think Zk sync, I heard one in Aztec. I think that all these proof systems have bugs in them right now, not the cryptographic protocols. I'm not an expert, so I can't claim about the actual, like the math, I can't make any claims about that, but the implementations definitely have done.
00:35:38.310 - 00:36:25.570, Speaker A: I do agree with that. Other protocols would be also in danger then. I mean, what, I mean, that's a big. I understand the assumption, but isn't that careful? Everyone can choose their own risk parameters. And also note that these designs aren't necessarily finalized, but it's basically when you're rolling out new cryptography. Boring cryptography is the best cryptography. I love cryptography that's been around for years and years and years and is trusted.
00:36:25.570 - 00:36:50.830, Speaker A: Nice and boring has been in production for like 20 years. That's the best kind of cryptography. And definitely feel free to disagree. That's perfectly cool. But all of this new cryptography is amazing. I love it, and thank you so much for implementing it. But it's very new, so we have no idea what sorts of tax are in it, what sorts of bugs are in it.
00:36:50.830 - 00:37:46.430, Speaker A: Right? And if we just start yoloing it into bridges, there, 100% will be hacked and people will lose a lot of money. Without a doubt. So I think that as a community, we need to be responsible with the way that we secure our bridges. And this is just my personal philosophy. It's perfectly cool if, like you, some of the protocols already have a delay. So some of the CKE protocols actually have kind of a mandatory delay when you drive just to cover the scenario where just in case there's a bug, just in case. So even like the teams that are building that have the delay, CKC has how long? 1 minute.
00:37:46.430 - 00:38:16.890, Speaker A: MD has a constant. Okay, so with obstac, you could build many roll ups, right? So imagine the sequester goes away. Like, I'm not running it anymore and I have like a bunch of cache on it. Can I totally. Okay. In the current version of the op stack, yes. So the bridge's liveliness depends on its permission actor in the current system.
00:38:16.890 - 00:39:12.346, Speaker A: But a future iteration will remove that restriction and anyone will be able to basically propose the state of l two, the output to the bridge. And then basically in the world where the sequencer goes away, anybody will be able to just propose, say that this is the state of l two of this block, and then prove their withdrawal. But how would that state be validated? Because I have to propose a valid state transition. Right. So how would that validation work? Great question. So liveliness of op stack chains is guaranteed as long as l one is live. So if there's no sequencer, then l one blocks will still be like, you can just turn on a node and it will start pulling in l one blocks, and it will make blocks that only contain deposits.
00:39:12.346 - 00:39:59.834, Speaker A: Okay. Right. So the idea here is that if the sequencer does not make the data available soon enough, then the chain will just start continuing on making blocks. Only full of deposits leave the sequencer behind. So this is what kind of gives you the ability to withdraw no matter what. Because what you can do is basically you send a deposit to the l two, and then the l two then basically triggers your withdrawal. And then your full node is able to, like, your full node is still live because l one is live, right.
00:39:59.834 - 00:40:40.346, Speaker A: So you're able to reconstruct the deposit only block, and then you know what the state route is. So then you can take that state route and propose it to the bridge. And then this is after we upgrade, we have anyone can make proposed bridge. And then the way that would work is if you're lying, then the fault proof would dispute it and then remove it. But if you're telling the truth, then your proposal would sit there for one week, and then you'd be able to do your withdrawal. The fault proof system decides if my late, say, transition is correct or not. Yes.
00:40:40.346 - 00:41:29.434, Speaker A: The fault proof system determines if your claim about is valid. Sorry, last impression. What do you think of insurance based bridges? You said insurance based bridges. Can you elaborate a little more? So insurance is, I think it's not that in this bridge validator, they can say that, hey, this money is transferred. And if this statement rewards. So the validator itself is an underwriter. I see.
00:41:29.434 - 00:42:07.830, Speaker A: Okay. I think this is kind of similar to what maker Dow is building. Maybe like these, like fast, where there's basically like the idea is you want to withdraw, so withdraw to me on l one and I'll instantly give you funds on l one. Just give me an extra little percentage. Is it the same thing? Right. So minor difference is you can remove any during. Okay.
00:42:07.830 - 00:42:27.334, Speaker A: Yeah. I mean it sounds pretty cool. I'd love to learn more. I'm very cool that I saw some foundations to reduce. Back off to check. Totally. Yeah.
00:42:27.334 - 00:43:04.150, Speaker A: So there, there's two main things that will do that. One of them is a new batch serialization made by proto. Great work. It's going to reduce the fees. How's the fact that they basically implemented this new batch serialization suite that is able to greatly reduce the size of the batches. And this is before compression. Basically it's very effective for chains that have low throughput.
00:43:04.150 - 00:43:54.918, Speaker A: So this should greatly reduce the size of the cost, the amount of data that's posted. And then once four four four goes live, we're working on integrating it with proof of concept implementation that is getting productionized. And yeah, we should be able to ship it and then utilize for it before and greatly. Small question. Follow up with that white color is ultimately totally benefit. Yes, great question. So the idea is that the old system is very happy.
00:43:54.918 - 00:44:26.362, Speaker A: I'm so happy that we deleted all that code. We learned a lot while writing it. With a two second block time. It was like more sustainable, right? Like with the old system, rare blocks were easily mined. Timestamps were really good. Hard management. Intermediate statements from receipts in favor of the status codes.
00:44:26.362 - 00:44:40.980, Speaker A: Computational overboard was too high to improve state transaction. Doing this, we repeated the same mistake on products. Then he said okay, not books next time. And then I buy a.
