00:00:00.250 - 00:00:30.934, Speaker A: Hey everyone, my name is Yi Long and I'm the CEO of Megaeath today. Thank you. Today I would like to share with you some of the performance related works we've been doing at Megaeath. And in particular, I would like to give you a quick overview of today's performance at the Ethereum execution layer and share some thoughts on where we think this is going in the future. Okay, cool. So. So at Megaeast, we're working on techniques to accelerate Ethereum at layer two.
00:00:30.934 - 00:01:10.018, Speaker A: So most of our works focus on the improving the execution client. So that would be gas, Aragon or red. And unfortunately, it would be impossible to improve to design truly high performance systems without first understanding the state of the art. Unfortunately, we couldn't find any good performance data. They're up to date. I think this also explains why the limitations of existing systems are usually poorly understood outside maybe the core dev team. So we decided to take a rigorous approach from the very beginning, named measure, then build.
00:01:10.018 - 00:02:11.378, Speaker A: So following this approach, there are basically two steps. We first need to carry out a very deep performance measurement of the existing systems to learn the real problems, and then we can design new tech's needs to actually address these problems. So this talk will be structured in two parts. In part one, I'm going to share some interesting lessons we learned from profiling the rest. Execution client. I'm going to try to answer questions like what are the current bottlenecks of execution client, and how did they change under different conditions? And in part two, I'm going to share some thoughts on how we can actually improve the execution performance at layer two and how much performance we can squeeze out of it eventually. Okay, so before we dive into numbers, let's review some background knowledge.
00:02:11.378 - 00:02:58.018, Speaker A: So an execution client basically has two major texts, execution and mercurialization. Execution is basically executing the transactions and applying the state change and updating the blockchain. State and mercurialization is computing or updating the state route after these state changes. In rest, there are two sync modes that differ in how often they update the state route. So there's historical sync, which is used to sync from the Genesis block. And in this mode, rest will only update the state route after processing a large batch of blocks. And there is also live sync, which is used to keep up with the tip of the chain.
00:02:58.018 - 00:03:41.966, Speaker A: And in this mode, the state route is only updated. The state route is going to be updated after each block. So our task server is pretty powerful. It has 512GB memory and it has a pretty fast memesd. The CPU core count doesn't really matter here because most of the workloads are going to be single threaded. And in order to simulate machines with different memory sizes, we're going to use the Linux cgroo command to control the amount of memory available to the rest process. And we run a lot of experiments, both historical sync and live sync, and we run them under different memory limits.
00:03:41.966 - 00:04:23.630, Speaker A: So this allows us to study the bottlenecks of execution clients under different modes and how these factors actually change under different conditions. Okay, so let's talk about execution first. This is our first experiment. So we run the historical sync with all 500GB memory, and we only persist. Our state changes every 500,000 blocks. This graph shows the execution throughput measured in TPS. X axis is a block height and y axis is TPS.
00:04:23.630 - 00:05:33.762, Speaker A: If we focus, and it took us like 24 hours to sync from the genesis block to the tip, there will be about 18 million blocks. And if we focus on just the last 1 million blocks, we can see that red can actually process transaction at 14,000 tps, which is pretty good. And know that this is all single thread performance, because rest doesn't have parallel evn yet. And if we measure the performance in gas per second, this is what you have, just same graph, same access, different y axis. And if we also focus on the last 1 million blocks, rest can achieve 1000 to 2000 million gas per second. So modern EVN implementations are actually quite, actually quite efficient, at least when the state fits memory. So how much memory do you actually need to run transactions to hold the state? This screenshot is a summary of the database tables in a fully synced ref archive node.
00:05:33.762 - 00:06:12.606, Speaker A: So you can see that the total footprint, this is the archive size is 2.3 terabytes. But fortunately, in order to run transactions you only need about in memory you only need about 100GB data. These are called a plain state in rath. And by the way, the MPT will take another 126GB. So this figure shows the time breakdown of historical sync. If we look at the red component, this is executing transactions.
00:06:12.606 - 00:07:24.998, Speaker A: We can confirm that executing transactions is indeed the bottleneck of historical sync, responsible for almost two thirds of the total sync time. And I'm pretty sure that the other components, there are some low hanging fruits that we can optimize them further, which will bring the percentage of executing transactions even higher. To understand why executing transactions are so expensive, we instrument the ReVN interpreter, which is the EVN interpreter implementation used by Res to collect very detailed performance metrics of each is this and we generate the following opcode cost table so this table has a lot of detailed information, interesting information, but it's not meant to be read on stage. So I'm just going to quickly walk through the important information here, high level information here. So first of all, each row is an opcode and each column is a different metric. So in this table. So for each opcode we have its total count during the entire historical sync.
00:07:24.998 - 00:08:35.140, Speaker A: We have the total time spent on each upcode measured in seconds and also as a percentage of the total time. And by the way, we order this upcode in descending order of the time consumption. And based on that we can compute accumulated percentage of time spent on the top k upcodes and we can also compute the average cost per invocation for each upcode. And finally we also show the category of this opcode. So based on that table we can graph the top 30 most time consuming opcodes and each blue bar here. So the x axis is the opcode and each blue bar shows the percentage of time spent on an opcode and its value can be read from the y axis on the left. Sorry.
00:08:35.140 - 00:09:29.758, Speaker A: And then the red curve is the cumulative time percentage and the value can be read on the y axis on the right. So we can see from this graph that 1st 90% of the time is spent actually spent on just the top 30 opcodes. And second, the top five opcodes are static call, keycheck 256, slow call and push one. And we can see that when the state fits in memory slow only takes about 8.8% of the total time, which says state access is actually not a bottleneck here, it's purely CPU bound. In this case we can also group opcodes by their category, provide an alternate view. This pie chart shows the time breakdown by category.
00:09:29.758 - 00:10:12.426, Speaker A: There are a few interesting things here. First of all, if you look at the left, the host plus system upcodes will take more than 50% of the time. Let me explain a little bit. So the host upcodes are basically used by EVN to interact with the surrounding environment. So like reading and writing the storage, outputting the logs, et cetera. And the system upcode is basically keepcheck 256 plus a few other functions. And these system and host upcodes are relatively complex functions that implement it directly in the native language of the execution client.
00:10:12.426 - 00:11:09.262, Speaker A: So in this case we're profiling rest, so it's rust. And second, the stack operations is taking another 29% of the time, and the rest 20% time are spread across arithmetic, bitwise operations, et cetera. So an interesting conclusion we can make here. Is that an EVM JIT compiler can actually only achieve at most two x speed up on Ethereum's historical workload because it can only accelerate upcode. On the right hand side of this pie chart, the host and system upcodes are already directly implemented in the native language in rust. Okay, so now we have a pretty good understanding of the execution performance in a CPU bound setting. Let's switch gear a little bit and talk about the impact of memory.
00:11:09.262 - 00:11:54.750, Speaker A: So it's no secret that rest can run pretty fast when the entire state fits in memory. But what if there's not enough memory to hold a state? Folklore says the performance will suffer. But is it true? Let's find out. To understand this impact of memory limit, we conduct this experiment. We basically rerun the historical sync experiments and vary the amount of memory available to the rest processes. We'll go from 816 30GB to 512GB. And we also, in addition, we have to reduce the disk flush period from 500,000 to just 1000 blocks, so that the experience can actually complete without running out of memory.
00:11:54.750 - 00:13:05.090, Speaker A: And this value of 1000 is chosen empirically, and the result is actually pleasant surprise. So this figure shows the total historical sync time of just the last 1 million blocks as a function of memory limit. So if we compare the data points at 8GB and 512GB memory, we see that the overall slowdown is actually less than two X, meaning that even with just 8GB memory, rats can actually comfortably process 7000 to 8000 transactions per second in historical sync. Why is that? The other three curves actually provide more information to explain this behavior. So, from top to bottom, we have three other curves. The orange curve is the amount of time spent on executing transactions. The green curve is the amount of time spent on loading data from the underlying database into Ras'cache layer.
00:13:05.090 - 00:13:24.314, Speaker A: And finally. Sorry. And the green curve. Yes, that's a green curve. I forgot what I said, by the way, the green curve. This is also where disk I os happen. And at the bottom, the flat red curve is the amount of time spent on writing state changes back to the underlying database.
00:13:24.314 - 00:15:04.490, Speaker A: So based on the shapes of the three curves, we can confirm that when you increase memory limit from 8GB to 512GB, the total historical sync time drops because the total execution time drops, which is the orange curve, and the total execution time drops purely because the total cache missed penalty drops, which is the green curve. But why do random disk I o only incur an overall slowdown of less than two X? It turns out that that's because today's Ethereum workload actually has very strong temporal locality, meaning that data they access recently are very likely to be xx again in the future. So this is a two level cache architecture used by rath. So what happens is that in rest, when EVN wants to do a state access, it first looks into its own cache layer, and if it couldn't find the data there, rest will consult the underlying database, in this case MDBX, and try to load the data into its own cache layer. And this database also has its own cache implementation. And in this case, MDBX simply relies on the operating system's page cache. So based on our data, we realized that the cache miss ratio at these two levels are only 21% and 31%, respectively, meaning that only like 6.5%
00:15:04.490 - 00:15:44.938, Speaker A: of requests actually hit the disk. And we also measure the average missed penalty. It's around 18 microseconds. So basically the disk latency, disk I O latency. We've been studying execution performance so far, and let's also include mercurialization. Now, this is a TPS graph I showed you earlier of the last 1 million block in historical sync and rest can do 14,000 Tps. And this is what happens when you have to update your state through and persist state changes.
00:15:44.938 - 00:16:24.082, Speaker A: After each block, the throughput just drops to just dropped to 1000 tps. So live sync is 14 times lower than historical sync, even in a purely in memory setting. So we measure it, and we measure one level deeper. And there are actually two factors that can explain this behavior. The first one is the addition of mercuryization. Mercuryization alone incurs a 9.3. X slowdown compared to on top of historical sync.
00:16:24.082 - 00:17:14.534, Speaker A: And then because live sync needs to write data back to the database after each block, so it incurs an additional 1.5. X slowdown. But why is mercuryization so expensive? Well, the reason is that in order to the state trial, you have a tree structure. In order to update a state route, there's going to be a lot of tree traversal. And this tree traversal will translate a lot of database read operations. And even though we're running on 500GB memory, even though all the database tables related to the MPT fit in memory, the software overhead associated with this database, in particular MDBX read operation is pretty high. We measure it to be about 1.2
00:17:14.534 - 00:18:03.590, Speaker A: microseconds per operation. Okay, so things get even worse when MPT doesn't fit in memory. So we also rerun this lifesync experiment using under various memory limits. This table summarizes the result so it's easy to see that the mercuryization cost far exceeds the other costs, like execution cost and data persistence cost. In addition, when the memory limit goes from 500GB to 8GB, the demercialization has a slowdown of 6.9. X, which results in an overall 5.3. X slowdown of the TPS.
00:18:03.590 - 00:19:15.226, Speaker A: Okay, let's do a quick recap before going into part two. So we start with nice and juicy 14,000 tps at the beginning with historical sync, and then we add mercuryization in live sync and it drops to 1000 tps. And then if we further limit the memory limit to 8GB, we get only 20 tps. And finally, if theta phono operators are not comfortable with using running entire CPU at 100% utilization, if they're only given 10% CPU utilization, then we're looking at maybe 20 tps. So you see the trend here, right? Fortunately, we'll be able to do much better. Okay, so at megaease we believe that l two is actually the best place for performance innovation. The reason is that l two enables the so called heterogeneous scaling, meaning that different types of node can vary significantly in their numbers and in their hardware requirements.
00:19:15.226 - 00:20:25.060, Speaker A: So for example, at layer two you can have just a small number of sequencers, and because of that you can afford to run them on very beefy machines. And if you're doing ZK roll up, you have this provers that's going to run on specialized hardware in the future anyway, to reduce the proof generation cost. Or if you're doing op roll up, then if you design your challengers to be stateless, then their hardware requirement could be extremely low. And finally, there's the roll up full notes they don't need to re execute transactions, so they can actually run on very cheap commodity machines, but there will be tens or even hundreds of thousands of them depending on how popular your roll up is. Note that this idea of heterogeneous scaling is actually introduced long time ago. So for example, Vitalik's endgame post back in 2021 explained. He argues that if you want to do high performance blockchain, it seems inevitable that you will end up with a relatively centralized sequencer design.
00:20:25.060 - 00:21:41.550, Speaker A: However, it's acceptable if you can also achieve trustless and decentralized block validation. In addition, you still have some antisensorship protection mechanism in place. So let's just run l two sequencers on Salana level servers, which means like 1000 CPU core, at least 256 memory gigabytes, memory fast NVMe SSDs and over 100 gigabits per second network bandwidth, and we run that on beefy machines, and we can apply all the software tricks we know to improve software efficiency. So to improve execution performance, we can do parallel EVN to exploit parallelism both at the CPU and SSD level, and we can do JIt compilation or stylus EVN plus like techniques to boost the single thread EVN or VM performance. As for mercuryization, we could paralyze merkel updates. Rest doesn't do it yet. That's why all the workloads we were testing are still single threaded, and we could also optimize the DB just for mortilization.
00:21:41.550 - 00:22:38.830, Speaker A: And we could also design more efficient authenticated data structures to replace MPT or vertical tree. But sequencer are not really our problem, right? So the real question is whether your full node can actually keep up with the sequencer. So this is your full node, maybe it has four core CPU, 8GB RAN, and 100 megabits per second network connection. How can it keep up? So let's walk through the workflow of a full node. So first of all, these roll up full nodes, actually technically they don't need to re execute transactions because they can just rely on external proofs to check the correctness of state transition. So instead they can just receive state diffs over the network they're advertised by the sequencers. And the nice thing about it is that these state divs can be highly compressed, which means you can save a lot of precious network bandwidth at the full nodes.
00:22:38.830 - 00:23:31.710, Speaker A: After receiving the state divs, the full node needs to update apply the state changes to its local database. But fortunately, these are going to be batch rides, and batch rides can be executed relatively efficiently. Well, as I mentioned earlier, data locality also help because hopefully most of the updates can be absorbed at the cache layer of the DB, so reducing the disk I o and finally, the full node needs to update its state try. It needs to maintain a state try if it wants to serve flight clients. And know that with 8GB memory, there's no way to fit your MPT in memory or a state try in memory. So this is going to be the most challenging part, but we are actually quite optimistic about it. We think it's actually doable by combining a number of smart optimizations.
00:23:31.710 - 00:24:18.890, Speaker A: So our vision here's our vision for future high performance l two s, and I call them mega roll up. But I've been a little bit sloppy in the use of terms here, so feel free to replace roll up with l two validian, optimum, or any other term as you see fit. So we envision that each mega roll up, ZK, or op in the future will be able to provide 100,000 to 200,000 tps. The complex, smart contract interactions, not just peer to peer payments. And each mega roll up can support very large state like terabytes of state without performance degradation. And there could be hundreds or even thousands of them. In addition, sequencers of mega rollups will be somehow decentralized.
00:24:18.890 - 00:24:58.486, Speaker A: However, we don't want to reintroduce the network or consensus bottleneck. And mega roll ups will be built on top of, must be built on top of high bandwidth DA layers such as Eigen Da. And optionally, they might also need a specialized and very highly optimized settlement layer on top of Ethereum, if Ethereum will become a bottleneck. Okay, so that's all I have, and I'm happy to take any questions. Yes, please.
00:24:58.668 - 00:25:17.220, Speaker B: You said the sequences will be like the fast machines and the rest will be behind it. So basically there'll be something like Solana is doing because they have the validator, which is like creating four blocks and then moving to another validator, and the rest are just basically taking the changes, am I right?
00:25:19.190 - 00:25:58.254, Speaker A: I don't think so. So in this case, roll up full nodes. They do need to keep up with the sequencer. It's just that they're performing a little bit less work. For example, they do not need to re execute transactions, and they can receive smaller amount data in the form of state Divs. So basically, the overall network performance, the end to end performance, is not going to be determined by your sequencers, because it's very cheap to run to rent very high end servers in the cloud. And there are so many software optimizations you can do at the sequencer level.
00:25:58.254 - 00:26:16.946, Speaker A: But the real question is, given 8GB memory, four core CPU, and 100 megabits network connection, can you optimize your full node under this constraint so that they can keep up with, say, 100 to 200,000 TPs? Okay. Yes, please. Would it be fair to call them.
00:26:16.968 - 00:26:20.710, Speaker C: Full node then, or would be a light client? Or do these state tips carry proof?
00:26:22.170 - 00:26:45.418, Speaker A: So I call them full? No, because they still hold the entire state of blockchain. Yes, but I know it's a little bit deviation from the usual term that people are usually more comfortable with. So yes, these will be the full nodes that keep the entire state, but do not re execute transactions.
00:26:45.514 - 00:26:57.346, Speaker C: Okay, got it. But I guess, how would they know that they get assigned transaction payloads later? Like, how would they know that the sequence is not forging transaction data?
00:26:57.528 - 00:27:17.844, Speaker A: So the thing is that they're going to rely on external proofs to validate this state transition. So in the case of ZK roll up, they'll be checking the proofs. And in the case of optimistic roll up, they just have to wait for the challenge period to expire. Yes please.
00:27:18.042 - 00:27:34.220, Speaker D: Have you thought about the architecture design choices impact on, say, make zk EVM, make a proof of execution on the way, be harder to make approval mega.
00:27:37.760 - 00:28:14.772, Speaker A: The thing is that the techniques we're developing are compatible with both optimistic and zk roll up or L two. But right now we are more focusing on the optimistic style l two because, not because of performance. Both op and zk can be made to run at 100 or 200,000 transaction per second. I think the difference here is the transaction fee, how expensive it's going to be, and ZK is probably going to be more expensive due to the proof. Sorry, is that your question? Maybe I misunderstood.
00:28:14.916 - 00:28:30.990, Speaker D: So for example, if people want to build a proofable parallel execution EVM, right? Okay, so suppose people are taking mega east spec of this parallel execution EVM model to do the proof of.
00:28:33.520 - 00:28:34.076, Speaker A: The.
00:28:34.178 - 00:28:42.160, Speaker D: I just wonder, some of the architecture decisions you made for the parallelization would make the proof over and harder or easier.
00:28:43.380 - 00:29:15.288, Speaker A: I think the short answer is that doesn't matter, because this parallel Evn, well, if you're doing, say you're using block STM algorithm from the Aptos, you report it to EvN, then. The nice thing about is that the parallel execution part is fully encapsulated inside evN. So from the outsider, it's no different than a single thread EVN just runs faster. So to prove it's doing the right job, it's the same as a single thread evn.
00:29:15.384 - 00:29:15.930, Speaker C: Got you. Thanks.
