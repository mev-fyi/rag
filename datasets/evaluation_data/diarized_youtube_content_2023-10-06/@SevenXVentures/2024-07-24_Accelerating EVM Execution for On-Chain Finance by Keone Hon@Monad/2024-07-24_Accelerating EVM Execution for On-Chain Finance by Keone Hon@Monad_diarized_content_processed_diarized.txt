00:00:00.240 - 00:00:34.536, Speaker A: Hi everyone, Keone Han here from Monad. We're building really performant EVM execution as well as a new consensus mechanism, ultimately building a full client from scratch to make Ethereum really performant. One particular use case is for option finance. Okay, I'm gonna, I'm just gonna take like 2 seconds to try to think.
00:00:34.568 - 00:00:35.940, Speaker B: Of how to do this better.
00:00:37.400 - 00:01:22.736, Speaker A: Stack something. Is there something we can stack? I can put that be a little iron. Okay. It's just cause I have a lot of like button pushes. All right, so just to talk about three different things within Monad, I'm just going to actually go really briefly through them and then just give an opportunity for folks to ask questions. A couple of things that I'm going to talk about today. Number one, asynchronous execution, which is, I think the topic I want to share more about because I feel like in the long term all blockchains will implement asynchronous.
00:01:22.736 - 00:02:06.620, Speaker A: They're all layer one blockchains. We'll implement asynchronous execution. Then we'll talk about what optimistic parallel execution looks like in Monad, and then lastly talk a little bit about the databases that we built. So just for background, most blockchains, most layer one blockchains, that is, blockchains that have a consensus mechanism, have interleaved consensus and execution. So you know what that looks like here, both execution and consensus. And the thing to keep in mind is that consensus is really expensive because. Thank you.
00:02:06.620 - 00:03:05.030, Speaker A: Nice. Consensus is really expensive because it involves cross the globe communication, and therefore that actually ends up taking up most of the block. So in an environment where nodes have to execute prior to coming to consensus, or rather the leader has to execute prior to sending out a block proposal. And then although their nodes have to execute prior to voting, it means necessarily that execution can only be a small fraction of a block time. So if you look at Ethereum, twelve second block times, but about a rough budget. And of course this is approximate because the actual budget is expressed in gas terms, but the equivalent of it is about 100 milliseconds, which is less than 1% of that block time. So that's pretty crazy because it means that there's a shrinking factor on time and you actually only have a very small amount of that block time to use for execution.
00:03:05.030 - 00:03:52.680, Speaker A: So maybe just to draw this schematically, we have execution as the dark blue component and consensus is the lighter blue. And I just showed a couple of blocks here. So schematically from a time and blocks perspective, this is what interleaved execution ends up looking like. So in asynchronous execution, we just do the really dumb obvious thing of only coming to consensus without executing prior. So the nodes are just agreeing. They're talking to each other about the official ordering within the block. There's no requirement that the nodes have executed in order to do that.
00:03:52.680 - 00:05:07.350, Speaker A: So schematically, you know, what this looks like is that block number one has consensus happen, and then once that completes that, two things can happen in two separate swim lanes, which is consensus on block two and execution on block number one, which we just consensus to over. So now it looks kind of like this, and then schematically looks like this. So far, it doesn't look that different. But the thing to realize is that you can actually raise the budget for execution substantially now, because now that the execution's in a separate swim lane, it can take up the whole lot of time and expectation, as opposed to only a small fraction. So, just to summarize, this one simple trick, and of course there's some complexity to it, which we can talk about in the questions if you're interested, but this one simple trick can raise the budget for execution substantially, even without making actual improvements to the execution itself. And all we need to do is move from an interleaved model to an asynchronous model. So this is why I think that in the long term, all blockchains will end up adopting this asynchronous execution model.
00:05:07.350 - 00:06:07.820, Speaker A: We've already seen Tolly from Solana talking about this, and I think actually ethereum researchers as well, so, you know, expect more of this in the future. Okay, so again, I'll just try to move through this kind of quickly and then just leave time for questions. So optimistic parallel execution is the idea of running many transactions. Sorry, just to restart. In Monad, similar to Ethereum and most other blockchains, transaction blocks are linear, so there's no dag here. And then the transactions within one block are also linear. So right now in Ethereum, transactions are serially ordered, and the goal is to try to still have this property where officially transactions are serially ordered, but then they're, sorry, linearly ordered, but not serially executed.
00:06:07.820 - 00:07:14.590, Speaker A: We want to do this by working smarter, not harder. So there's some fancier names for this, but it's actually really just sort of doing the common sense thing, which I'll kind of try to describe to you very quickly. So the idea is run many transactions in parallel, assuming that they're all starting from the same starting point, aka optimistic execution. And for each transaction just do some bookkeeping to keep track of all of the inputs to that transaction, aka the storage slots that were read, and then also keep track of the outputs. A key the storage slots that were mutated in the course of running this transaction. So we run many transactions in parallel, generate many pending results in parallel, extract of an input and an output, and then we, so that's phase one. And then phase two is to step through these pending results serially and commit them and reschedule work if any input has been utilated.
00:07:14.590 - 00:08:06.300, Speaker A: So I'll just say that one more time, which is again like two phases. First stage is running many transactions in parallel for each transaction, making a pending result, which has the bookkeeping of the inputs and outputs for that transaction. And then step two is step through the pending results serially and ensure that all the inputs have not changed since that point in time. And if any input has changed, then reschedule work. And then the last thing to keep in mind is that in the event of rescheduling, so actually the most expensive thing in execution is not usually that cpu time to do that transaction. It's actually the storage lookup. Because storage lookup means going to disk and disk is relatively slow compared to cpu time.
00:08:06.300 - 00:09:02.136, Speaker A: So on rescheduling, generally that input is actually in state or starting is in memory. So therefore that reschedule is actually quite cheap because it's just a lookup against cache. Okay, so I'm going to go kind of faster here because I don't have, I feel like I already kind of explained it. So we have five transactions, and then obviously like the serial thing is just the dump thing of running like that. But if we run in parallel, then we could, you know, say we have three cores run through transactions in parallel, and then, you know, start. Each one generates a pending result and then once that floor frees up, then start writing another transaction. Okay, so just to give one simple example of what.
00:09:02.136 - 00:10:14.800, Speaker A: Cause I think the question that people always ask is like what happens if there's two transactions that are dependent upon each other? So maybe just to give an example, like say that I start with 1000 USDC and Alice starts with zero and Bob starts with 300, and then there's a couple of transactions. Maybe the first transaction is me sending 100 USDC to Alice, and then the second transaction is something unrelated, and then the third transaction, me sending 100 USDC to Bob. So I'll say that again, like the starting state, I have 1000, Alice has zero and Bob has 300. And then there's like two transactions that are pretty close to each other, where I sent out 100 to both of them. So if we do this intuitive algorithm, this is what I want to emphasize, like we're doing things optimistically. So we run these transactions at the same time. The first one has inputs of my USDC balance is 1000, analysis is zero, and then it has outputs of my USDC balances 900 and Alice's becomes 100.
00:10:14.800 - 00:10:58.680, Speaker A: That would be what transaction one does when we run it optimistically. And then transaction number three, again, the transaction sheet was something unrelated like Charlie mentioned. NFT. Then in transaction three, because we're doing this optimistically, I again, in this run of this transaction, start with 1000, Bob starts with 300, and then the outputs are, I end up with 900 and Bob 400. So when we try to commit these pending results, we can commit that first transaction fine. The second one, because it's not contending also is fine. And then the third one actually is going to have a problem because the input at the time that we executed was 1000.
00:10:58.680 - 00:11:46.920, Speaker A: But now we can already see that the input should have been 900. So we just need to reschedule the time. So we just go redo it. Okay, yeah, so I kind of gave you this a couple of times, so I'll just skip. But one other thing to point out is that I think people often think that parallel execution requires access lists, because they're thinking of how Solana has quote unquote access lists like pre specification of all the dependencies. But this does not. And I actually think that this is quite important that requiring access list is really bad UX because if you get the access list wrong, then that transaction will fail.
00:11:46.920 - 00:12:29.306, Speaker A: But it's quite conceivable that you could get the access list wrong, because the only way to get the access list is by simulating the transaction. And at the time of the actual execution, some little bit of time has passed. You might actually not have exactly those set of storage slots being the ones that are the dependencies. So the optimistic algorithm is actually better in my opinion, because it doesn't have the correctness dependency on correct specification of the access list. And then also it's actually means the transactions are smaller. So like in the end state, I think the actual bottleneck is going to be network bandwidth, nothing execution. Therefore we should try to make the transactions as small as possible.
00:12:29.306 - 00:13:09.076, Speaker A: We don't want it to get blown up by having to specify all those access lists. Okay, just maybe really fast on this third one. Okay, so SSD's are really a great piece of technology. They've gotten better and better every single year. Right now you got a pretty cheap SSD with over a million IOP's I o operations per second. So you can think of an SSD as a bottle that has a pretty wide bottleneck. A lot of data can go in and out at the same time, but what we need is software that can actually take advantage of this wide bottleneck.
00:13:09.076 - 00:13:59.160, Speaker A: And unfortunately with the existing database models, that's really not the case. So the job of parallel execution really depends on efficient parallel state access. We need to be able to pull many dependencies like all those inputs from SSD in parallel. And we also don't want one transaction that's pulling data from a database to block another one that's also doing that. We should do these rules in parallel. So this problem needs asynchronous I O and it needs really efficient usage of that entire million ions per second of bandwidth. And right now, ethereum lives.
00:13:59.160 - 00:15:16.510, Speaker A: Ethereum data lives in Merkel Patricia tree. And then that Merkel Patricia tree is stored inside of another database like GlobaldB or public in the case of death, RoxDB in the case of Solana. And because this data structure, the mercenary tree, is actually being embedded in another data structure, it means that navigating to one leaf in the Merkel Patricia tree, aka going all the way down, you know, some tree is actually going to require, like many other tree lookups under the hood for each of those nodes being traversed. This is really inefficient and is not going to saturate the SSD's I O capabilities because there's lots of sequential lookups. So MonaDB is just doing the intuitive, obvious but painful from a development perspective thing of building a custom database to natively store the Merkle tree data on disk and then also utilizing IO, urine and other optimizations like bypassing the file system. And so the result is that we have this parallel state access or, sorry, this parallel execution that's running many transactions. Parallel can also pull a lot of data that is the dependencies for all those transactions in parallel.
00:15:16.510 - 00:15:27.390, Speaker A: Okay, so like I said, I just want to go through those quickly and then open up for questions. So wanted to see if anyone had any questions.
00:15:28.690 - 00:15:39.780, Speaker C: Yeah, I would go first because Monet is more performance. Like are there any new type of like our topic sunshine financial? I was just curious, any new projects are you building on top?
00:15:40.760 - 00:16:58.400, Speaker A: Yeah, I think a couple of clear examples are fully on chain limit order books where all the submits, all the cancels, like all the state is on chain which is important for full composability as well as self custody. Composability requires all the data be on chain because then that means that you can route like some other smart contract through an order book and swap atomically as a subroutine. I think also frequency of oracle updates, like if you think about the frequency at which chain link updates on Ethereum main net right now, I think it's like once per hour or any time for ETh USDC anytime the price changes by 50 basis points. So that's a very inaccurate oracle. And it has all these downstream effects because then lighting protocols perpetuals, everything that's pricing off of that has a huge fudge factor. So then there's a lot of inefficiency. So having the ability for oracles to push data as is public good on chain, very frequently very accurate, that ultimately has downstream effects on the competitiveness of the DeFi landscape against centralized finance.
00:16:58.400 - 00:17:35.100, Speaker A: Yeah. So Monad has 1 second block times. I didn't really talk about that part because this is like an execution discussion, but 1 second block times with a budget of a billion gas per second or a billion gas per block. So in effect they look like, you know, 1 second auctions. And you know, the, how it gets sorted out in terms of whether submit wins or cancel wins is just subject to that auction.
00:17:35.960 - 00:17:38.620, Speaker C: Got it? Yeah. Any questions from the audience?
00:17:44.050 - 00:18:11.580, Speaker B: I'm just wondering like how do you compare with the loneliness parallel execution method, maybe with three parallel fusion method, because they leverage to use the state access to read the different states to make a parallel. And I understand that Monad or an upholst using some optimistic parallel execution. I just want to say, see your opinions on the differences or the comparison between these two kinds of methods?
00:18:13.480 - 00:18:37.512, Speaker A: Yeah, I think it's. So your question was comparing Monad's optimistic parallel execution against certain other forms of execution that have parallelism embedded perhaps through stronger object models, ultimately allow pre specification of some of those dependencies, is that right?
00:18:37.656 - 00:18:46.704, Speaker B: Exactly, exactly. I understand there are some trade offs here. So I just want to hear like what is the bonus advantages on what lucent provides?
00:18:46.872 - 00:20:06.428, Speaker A: Yeah, well, Monad is fully EVM compatible. So we just think that most developers want to build for the EVM. They want to build solidity, they want to use all existing libraries and tooling and all the apps that already exist that they can compose on top of in order to support EVM. That means that, well, just comparing to SwE in particular, where SwE has swe move, which is, you know, has this much stronger object model where, you know, you can know for a given app interaction like exactly what pieces of, of state are going to be required. We don't have that at the time of like scheduling that work or at the time of doing the work. So I think of optimistic parallel execution as more like almost a waterfall approach, where this analogy is probably sense, but it's like there's many streams and there's just water flowing through each stream that is like state access coming back from the disk as fast as possible. We're just trying to saturate the bandwidth of the, of the SSD, but we're doing that with a constraint that we don't know anything about the transaction ahead of time.
00:20:06.428 - 00:20:16.560, Speaker A: And we're just like executing bytecode when we rather do an s load than go and like fetch that piece of data from disk. So there's just like weaker, weaker assumptions, basically.
00:20:17.050 - 00:20:17.666, Speaker D: Correct.
00:20:17.778 - 00:20:20.270, Speaker B: Makes sense. That's not my question.
00:20:21.890 - 00:20:30.122, Speaker A: Hi, thank you for talking. So I've been wondering if you have been using GPU to improve the execution.
00:20:30.146 - 00:20:31.970, Speaker D: Of Eva, because that has been something.
00:20:32.010 - 00:21:15.620, Speaker A: That has been going around, probably not as huge, but this would have to say, yeah, I think GPU is pretty out of scope for what, you know, what we're trying to optimize for, because most smart contracts actually have very little computation within them. They're really more bottlenecked on state access than they are on cpu time. I think that if there were smart contracts that were trying to like invert big matrices or something, that it's possible that like EVM on GPU, with all the EVM opcodes supported on GPU, could be interesting. But I, I think it's just a very different kind of smart contract than what exists right now. Thanks so much.
00:21:17.920 - 00:21:19.500, Speaker C: Thanks. Any more questions?
00:21:22.360 - 00:21:23.860, Speaker B: Thank you for the code talk.
00:21:24.400 - 00:22:13.050, Speaker D: I didn't fully understand, like the address handling mechanism in case of conflict stay, because like two transactions of conflict start from the same community state, then one of the two changes the furthest than the other one. And so you said that the other one is rescaled, but how does it work? Like, what are the execution risks embedded with this kind of scanner? Because I mean, like, I expect Armona to find a very huge defi system because of the performance. And so this thing could be a thing in terms of searchers and everything like that, right?
00:22:13.390 - 00:23:08.732, Speaker A: So just to mention a couple of things that maybe you like, start to address your questions. Your question is like, please re explain, like how contention between different transactions, like how that ends up getting resolved. So the first thing to point out is that the transactions have a total ordering. Like they're numbered from one to whatever, like 10,000 or 15,000 or something within that block. And the end result, the true result is the result that you would get from running them just one after the other. So if we have two transactions that are both, that are in contention and transaction, you know, it's like transaction ten and eleven that are in contention, then transaction ten takes priority. Like if they're both run in parallel and we produce pending results for both of them, we're going to try to commit ten first.
00:23:08.732 - 00:24:00.668, Speaker A: And if they get committed successfully and it ends up mutating some input, that now transaction eleven is reading, transaction eleven has like lower priority. It has to be re executed. So I think something that people worry about when they first hear about parallel execution is like, oh, how do we know that? Like we're not gonna get an infinite loop of like, you know, constantly. Like we re execute one thing and then that mutates some other thing which then causes something got executed already to like have to get re executed. But that's, that's not the case. Because if you think about the algorithm that I described is literally every transaction gets executed at most twice. It might get executed once in the first pass, and then we generate a pending result.
00:24:00.668 - 00:24:51.396, Speaker A: And then at the time that we try to commit that result, we might have to re execute it. But once it's re executed, then it's good. It can't be dislodged at all. So I think, yeah, just in general, the total ordering that exists in all blockchains is actually super helpful for resolving this like connection problem because there's always a priority that's well defined. The earlier transaction always has priority and also re execution. Just to point out one other thing, it only happens if there is contention between an earlier write and a later read. So like I said, if transaction ten writes to a particular storage slot, that transaction eleven that is about to read, and transaction ten doing that, right, ends up mutating it.
00:24:51.396 - 00:24:56.532, Speaker A: That's when transaction Eleven's pending result gets invalidated, has to be re executed.
00:24:56.716 - 00:25:11.298, Speaker D: And one last question. When you say a transactional right first and the other, you mean like a naive FIFO system or like a PGA system, which one transactions prioritize the type of higher gas that is packing.
00:25:11.484 - 00:25:59.140, Speaker A: Oh yeah. So all of this is downstream of the question of how the order gets chosen. So I think you can just think of it as a black box where like, you know, the orders are you chosen, like everyone's job is to, like, execute this list of transactions that the leader originally chose. So the question of how, like, you know, a block builder would, like, find the optimal, like, ordering among transactions, that's like, you know, out of the scope of the protocol in this discussion. But yeah, this is all, like, assuming that we've chosen ordering already. But then in terms of, like, how the leader will end up choosing the ordering and the, you know, if it's running the default Monad client, it's a priority gas function.
00:25:59.800 - 00:26:00.240, Speaker D: Thank you.
