00:00:00.170 - 00:00:30.946, Speaker A: Decentralized AI. And the first question is, what do you want to decentralize? And also, what does sort of AI flow look like? So we can talk about decentralized. First is, you know, there are two types of AI flows. There is the closed one, which is open AI, go figure. And then Gemini, and you've seen it, they give you an API access paid portal. There's no composability. You just have your own flow.
00:00:30.946 - 00:01:04.862, Speaker A: There's the open one, or actually, I should say actual OpenAI, which is just given out. The model weights are just released freely, given away. You can do whatever you want to do with it. Full flexibility for any further use. And this question is on decentralizing AI. So the first question is, which parts of this flow do you want to really decentralize? Do you want to decentralize in the first place? And if so, what parts do you want to decentralize? I was trying to think about what the AI flow looks like. Sunil mentioned this also just earlier.
00:01:04.862 - 00:01:34.394, Speaker A: You collect the data, you train the model. Someone hosts the model, you have an inference. There's a prompt, and there's an inference. Then you worry about alignment afterwards, and then there's a whole economics aspect around it. So this is the AI flow, and you ask, do you really want to decentralize? And if so, which parts? Well, data collection, you probably want to get decentralized because that gives you multiple varieties of opinions. Feel for the data, which parts are toxic, non toxic. It's a societal thing.
00:01:34.394 - 00:02:13.302, Speaker A: So you want to decentralize, and if you want to measure any forms of decentralization, well, you probably want to say, who gave what. Was it actually useful data? It actually helped something, or just spam prevention, some ways to measure data, then you want to train the model. We want to make sure that the model was trained in a right way. That means all the training parameters were used appropriately. What the data you collected is represented in the model, that's, you could say, proof of training. I'm using these proofs as very lightly here. We'll get a little bit into it.
00:02:13.302 - 00:02:54.094, Speaker A: So what about hosting the model? Do you want to train the model in a decentralized way? Well, there are projects trying that, but it's a tough thing, even in a centralized way. And perhaps if you can get away without just being at a centralized training, but prove somewhere else, you kind of decouple the security from the efficiency. A large part of what I talk will have this flavor I want to decouple. AI is already an onerous affair. It's taking all bunch of the energy, the capital, attention, and the last thing you want is to make it more inefficient. So it's crucial to keep the efficiency going. At the same time, take some decouple trust and efficiency.
00:02:54.094 - 00:03:32.286, Speaker A: This is largely the theme of this talk, so I don't want to decentralize training, and I think to a large extent you can get away with it. It's tough enough on a centralized setting. Next model, hosting, it's bad enough and you need lot of compute and it scares to host large and large models. And if it's split around lots of computers, then it's like you can imagine what's going to happen on top of that. Probably shouldn't be doing that, but then you need to make sure that it was all done right. If it's in a central place, what goes in, what goes out, how do you know it's right? So that's the so called proof of inference, or proof of, and same with inference. Hosting and inference kind of go together.
00:03:32.286 - 00:04:00.702, Speaker A: If you host centrally, then inference just becomes easier. It's just one machine optimized for that runs it. How about alignment? Alignment is something sort of separate from effectiveness. It's in fact complementary. And one of the grander challenges of AI has been to build good AI effectively effective AI at the same time, be robust. Robust is somewhat vaguely defined, but alignment is one way of defining robustness. You want to be non toxic.
00:04:00.702 - 00:04:31.506, Speaker A: And the effectiveness, most people can agree the data says what the effectiveness is, the data is the proof. But alignment is not entirely in the data. It's a societal, cultural, civilizational thing. And you want society and culture and civilization to have a vote on that, which is separate from model hosting and inference. And in a centralized setting, the open AI, which is the closed AI, they're coupled together and the whole fiasco. Someone mentioned Gemini. The reason is that they coupled the two things, and you pay the price for that.
00:04:31.506 - 00:05:01.580, Speaker A: You really want to separate them. That part decentralized them all for it. And finally, you want the economics to be truly shared by everyone. If society is contributing to data collection and data toxicity assignment, you want the economics to be appropriately shared, the intellectual property to be shared. Bedrock of this country and large part of the modern civilization is property rights. And you want AI property rights. So sure, this is a given.
00:05:01.580 - 00:05:39.546, Speaker A: And having property rights comes down to proofs of the economics of AI. So I'll give you a flu for what do I'm using these words, proofs on the side. And what do these proofs look like, and here are a couple of examples that I personally work on and we're working very much at Sentient, which is if you have a. And I'll give an example of this is already there in software. You can have a software that's privately held with an API access to it, username, password held behind a firewall. Nobody sees the software. GPD four is already kind of a good example, except it's not a software.
00:05:39.546 - 00:06:14.746, Speaker A: It's more like just a model. And you can go through an access and then you get it. The other option is you give away the software, but try to get some economics out of it. If you give away the software fully, then that's the open aspect. And sometimes it has a benefit. I mean, eventually in the long arc of software history, I mean, 20 years you've seen that Unix has won. And right now you have these two extreme models where I just wanted to continue this, Microsoft would release its.
00:06:14.746 - 00:06:31.722, Speaker A: Microsoft Word gives it to everybody. It's impractical to have host it on Microsoft.com for every word processing task. Doesn't make sense. So they give you for your laptop, but then license it so they will get some value out of it at the price of some privacy. The way they give it to you is not the source code. They give you the binary.
00:06:31.722 - 00:07:01.194, Speaker A: I don't even know what the equivalent of a binary version of AI model is. It's one of the challenges we work on. So you want to share the model, you want to give away the weights, but it shouldn't be useful unless the prompt that's fed is assigned version. Think about it. And then we talked about model inference. I think many talks, probably, I attended a few, but probably every talk talks about it. AI itself doesn't care about exactly proving what it wants to do.
00:07:01.194 - 00:07:22.042, Speaker A: Sunil was talking about hallucination. It's just part of the AI itself. On top of it. The things are continuous. If the model itself is not so serious about what it's producing, the last thing you want to do is whole bunch of cryptography to verify what it was doing. You don't want to take something more seriously than that itself takes. So what does that even mean? How do you deal with this? We want to build new tools.
00:07:22.042 - 00:07:41.240, Speaker A: This is sort of, I'm also on my side. I'm a professor and I think about the science and the underlying principles. So I call this AI native cryptography. Cryptography is sort of built in a discrete digital setting where every bit is critical. There's no semantics to it. You want to get it exactly right. And that's what cryptography is built.
00:07:41.240 - 00:08:03.850, Speaker A: AI has other aspects to it, in particular. Three I want to highlight. First, it's continuous. You might have discrete things for data in AI, like words and stuff, but end of the day, embeddings have won. Calculus has been a powerful force, and it shows its power centuries wide, across centuries. And it's a powerful way of thinking about even discrete objects. It's continuous AI representations, continuous.
00:08:03.850 - 00:08:20.398, Speaker A: Second is that it's not just continuous. They also have some other geometry properties. There's topology, there's local global topology, differential. It's a manifold low rank. There's whole structures around it you want to represent. And if you want to represent a model, you want to obey those geometries. Cryptography is not built for discrete.
00:08:20.398 - 00:08:38.194, Speaker A: There's no any of this geometry. And finally, models are not deterministic, they're random. And you want to get it only approximately right for a given input. Heck, you don't even want to get everything right for every input. You want to want it, on average enough inputs. You want the output to be approximately right. So you want to obey these native AI.
00:08:38.194 - 00:09:09.726, Speaker A: Call these AI native cryptographic primitives. Once you have them, these two would be some examples, core examples. Then you have the building blocks of decentralizing AI. Okay, so that's the one part I wanted to talk about. But if you saw the first slide very, very quickly, such a platform would be a decentralized AI platform. But there was a G there, and I wanted to take two minutes talking about actually defining AgI, if nothing else, I'll define it. So there's a whole polymer.
00:09:09.726 - 00:09:35.994, Speaker A: What is Agi? A lot of people, reasonable people, also disagree, and they also can, of course, a whole debate about whether it's in a year or like ten years, 40, or never. I mean, you hear this all the time, but it's good to first define it. And if you cannot define, then it's kind of tricky. Here's a definition. So gi, so I. We understand I is doing very well on a specific task. Very well means superhuman, or at the very best, human.
00:09:35.994 - 00:09:51.502, Speaker A: And superhuman is I, is intelligence. Very narrow, specific task. GI refers to more than union of I. I want to say. I want to say something like two to the power of I. It's at a different level of infinity, and it's obtained by interactions of I. I really liked sort of where David was going.
00:09:51.502 - 00:10:36.618, Speaker A: And the strategies that come when intelligence interacts with other intelligence in an incentive driven way is this is also how evolution discovered intelligence. I mean, we put hydrogen, carbon, oxygen, the building blocks of chemistry with evolutionary pressures, long term reproduction, ability to survive, and also short term pressures. That is, you get to eat, you get a good meal, and both these short term, long term pressures, you put them under this cooker, and then evolution, intelligence arrives, and general intelligence arrives, and objects like us get out of this evolution. You can take the same thing. I mean, crypto, we want to think about. Not crypto, sorry, I haven't talked about crypto anywhere. So that's my definition of AGI.
00:10:36.618 - 00:11:25.594, Speaker A: AGI are, if you want to build AGI, then if you want to build AGI, then you need to have any number of intelligent agents, sovereign interaction, permissionless, composable, but most of all, incentive driven way. And what platforms allowed such interactions? Sovereign, composable, incentive driven. In fact, the recent Sora already had simulations and interactions of different versions of the agents critiquing each other. We also saw this even in GPT four with GPD, three with 3.5 with human feedback team. There's already some interaction aspect of it, but you have to take it to the fullest extreme. And the closed platforms like open Air, would also already try them, run tournaments internally.
00:11:25.594 - 00:12:21.570, Speaker A: But there is actually already a platform around which allows any number of agents openly participate, just freely on the Internet. And your incentives, it's with a wallet, just one number of tokens you have, it's called a blockchain. You can try it out on other AI platforms to get AGI, but that's just blockchain by another name. And so that's the subtext if you want to get AGI, blockchains are it, you can try doing it some other name, but it's just the same. Of course, there is a flip side, which is always there, which is you want to decentralize AI because it could be dangerous, especially agi, something that is self replicating and manipulating. We haven't understood how modern economic structures, capitalism especially, deals with self replicating powerful intelligence forms. And it's not clear what good outcomes are.
00:12:21.570 - 00:12:49.878, Speaker A: Wars. Decentralization is just a good thing. I've heard even governors and state representatives talk of decentralization. You know, that things are. There's very low gravity is flowing perfectly when the most representatives of the most central form of governance talk about decentralization. And decentralization is Crypto's middle name, actually, it's also his first name and last name. And this is the killer app for Crypto.
00:12:49.878 - 00:13:10.970, Speaker A: So maybe I should just say that to build Aji, you need crypto platforms. It's just necessary. I said it by definition and crypto, we can all agree, needs a killer app. And that's what project sentient is. It's about building platforms that allow. I said it out. You can take a look.
00:13:10.970 - 00:13:42.182, Speaker A: Sovereign, incentive driven AI agents to interact about. I think David talked about several applications. What would be the first set of agents that interact? What would be the first feel for where these strategy space emerges? One can look at it. And I teach a class at Princeton. It's on web3, princeton.edu. It's taught on Sepolia. You're welcome to take a look.
00:13:42.182 - 00:13:56.960, Speaker A: And many of the projects are all about trying such experiments on agents on Sapolia. Okay, that's me.
00:14:02.290 - 00:14:04.034, Speaker B: Do we have any questions?
00:14:04.232 - 00:14:06.242, Speaker A: Oh, there are questions, yes.
00:14:06.296 - 00:14:07.266, Speaker C: Do you see agents.
00:14:07.368 - 00:14:08.162, Speaker A: Can I get your name?
00:14:08.216 - 00:14:08.734, Speaker C: Vijay.
00:14:08.782 - 00:14:09.214, Speaker A: Vijay.
00:14:09.262 - 00:14:22.760, Speaker C: Do you see agents getting into wars? Because obviously agents that you use may generate more economic value for you. And I have agents that might lose out to your agents. And how is that going to play out?
00:14:23.610 - 00:15:05.794, Speaker A: Yeah, I mean, AI touches all aspects of society. I don't know, but I would like it to be community owned. I think if I would like community to have a say, the model is. I wanted to highlight one thing, is that whatever that strategy comes out of it, again should have the aspect that everybody has, the model, but it doesn't work at all unless the prompt is fed with something that is signed by the network, that in this case sentient. The platform signs it. It could be a threshold signature, it could be based on stake, it could be based on reputation. That part is that's where the governance models comes in.
00:15:05.794 - 00:15:39.006, Speaker A: And what you may call the modern day property rights are set as in the era of AI. But the key is that the model is not held behind a paywall, but it's open in the spirit of Microsoft giving away word, not for philanthropy, but they want it to actually be used in the same spirit. How it's governed is sort of a meta layer. That is, I'm not discussing here, but that's for society to talk about, actually. That's the kind of thing that Google gets into trouble because now they have to do both sides. Yes.
00:15:39.108 - 00:15:50.706, Speaker D: When agents are interfacing, how do you view the case where they would be in Nash equilibrium and not parato efficient? And can they kind of self correct?
00:15:50.808 - 00:16:13.958, Speaker A: Yeah. Spectacular. Can I get your name? Arian. Aryan. So Ariane is asking, what's the equilibria associated with such agent interactions? It's hard enough. Does cooperation emerge? Classical studies show that even in simple cooperative cooperation has emerged despite clear competition. And we as a species are very good examples at cooperating.
00:16:13.958 - 00:16:32.338, Speaker A: We are built to trust and cooperate. And the evolutionary structure, that's one of the grand challenges. I didn't list. I said one grand challenge in AI, to do effective AI and robust. There are two other grand challenges. One of them is exactly what you just said. It's an open question.
00:16:32.338 - 00:16:40.450, Speaker A: This is game theory at scale and in a sort of a turing complete space, not in a simple sort of a payoff matrix.
00:16:40.790 - 00:16:43.698, Speaker E: Yes, you mentioned that training in a.
00:16:43.704 - 00:16:47.518, Speaker D: Decentralized fashion doesn't make sense, which makes.
00:16:47.544 - 00:16:59.522, Speaker E: Sense for companies like OpenAI who don't want to deal with communication overhead. But do you think there's an emerging market of people who are more cost sensitive than they are waste sensitive, who are willing to use the solution?
00:16:59.666 - 00:17:00.474, Speaker A: Can I get your name?
00:17:00.512 - 00:17:00.950, Speaker E: Oliver.
00:17:01.030 - 00:17:21.994, Speaker A: Oliver. So Oliver is want. Oliver is asking about decentralized training. I didn't say it doesn't make sense as much as I said it would be so much better if it were centralized. It's hard enough to train good models with centralized resources. We are resource constrained, as you can see by Nvidia's market cap. And I just said that it's better if everything were centralized.
00:17:21.994 - 00:17:37.782, Speaker A: It's hard enough there. Nvidia chips are hard to come by, so. That's what I meant. I didn't say that. There is no role for decentralized training, especially if data is decentralized. You might wonder training should also be. Perhaps there's room for it.
00:17:37.782 - 00:17:55.850, Speaker A: I didn't mean to dispute that. But it is so much simpler if you could just do it in some centralized way and not have to worry, just resources wise. In a perfect world, resources will go away and we'll get there. Does that make sense, Oliver? No, I didn't want to say that. I didn't want to rule that out. Yes, Sunil.
00:17:58.370 - 00:18:41.046, Speaker F: So maybe to go off Arian's question with sentient, do you guys think about enforcing certain game constraints for agent interaction? Because I think there has been some research, actually into how agents interact when incentives is the main reward function. You go back to the days of reorgs and bitcoins. They let open reinforcement learning agents who've made reward functions, how much money they made go and sort of act as miners on the casito chain. And what ended up happening is that the chain just ended up stagnating and reorganizing so much because every time one would rework, another agent would try and find economic center to reorg. Ended up being very destructive, not at all efficient. So I imagine that when you guys think about designing sentient and what this means, you do have to add an economic constraints.
00:18:41.078 - 00:18:42.620, Speaker D: Do have you guys given any thought to.
00:18:43.070 - 00:19:24.466, Speaker A: So the question is, I talked about AI versus AI versus AI, but what's the rules associated with this? Who sets them? And that's the question. And I think about it all the time. It's not something we solve right now, but it's one of the things. I mean, it's related to audience question, as you pointed out, because when AI versus AI interact, what's the end game? What's the equilibria that come out of that? Are they societally good? Who evaluates whether something is societally good? And perhaps societal good should be just in sort of plain text. And many religious texts are like that. They are not written in some code. It's like the Bible.
00:19:24.466 - 00:19:55.506, Speaker A: And religion is a fantastic trust platform. It's scaled over millennia, and they also have some codified good behavior, social behavior, and you can go back to the text a millennia ago. So natural language seems like we are built for good ways to agree upon and to convert that into equilibria on which rules of the game, such that the outcome will be these texts. Can I build off of that, please?
00:19:55.608 - 00:20:26.780, Speaker D: So, with why I think it's not going to be a long term issue is because the structure of governance allows for two parties that can benefit from the efficiency to work together to essentially break away from that nash equilibrium, because otherwise, there's no form of aligning yourselves with each other's optimal output. And so if that entails just kind of knocking up some arbitrary stake that can be slashed to facilitate that movement, greater efficiency, so be it.
00:20:28.190 - 00:20:29.530, Speaker A: Thank you, Aryan.
00:20:29.870 - 00:20:35.610, Speaker B: Yes, my name is Gemma. I don't have a very intelligent question. I have two follow up clarification.
00:20:35.690 - 00:20:36.462, Speaker A: Can I get your name?
00:20:36.516 - 00:20:36.954, Speaker B: Gemma.
00:20:37.002 - 00:20:38.160, Speaker A: Gemma, yeah.
00:20:38.930 - 00:20:55.140, Speaker B: First part is, you mentioned that there were two challenges. You mentioned Aryan triggered the third. What's the fourth? And the second question is, what call to action would you give people who want to learn more? Are there multiple, depending on who you are, how can I go and play with what you've done at.
00:20:55.770 - 00:21:30.522, Speaker A: Yeah, we have some prototypes already available. We wrote a paper last summer called Sakshi. Some of you may have seen it. I'm a plumber of the information world. I like to build networks that scale globally. I built other networks in the past, and for me, this is another era of building platforms, information platforms, this time with economics and trust and games built natively and you have some aspects of them already. Sentient was only released on Monday.
00:21:30.522 - 00:22:06.842, Speaker A: We came out as a. So I hope you can give us a little bit of slack. Today is only Wednesday, but weekend will get somewhere that you can play. But what I wanted to say was that sentient is AI native. That is, you come in with natural language, you come in with Pytorch, you come in with Tensorflow, you come in with your Jupyter notebooks. It has crypto ethos, but it's an AI first project. The number of mind space developer mind space, just the humanity's mind space in AI is like 100 x that of crypto, maybe even more.
00:22:06.842 - 00:22:34.050, Speaker A: Despite what BTC's value today is, it's really 100 x. Moving just 1% of AI to crypto is will more than double AI Crypto's base, user base. And sentience has a modest goal just to do that. And I do my mild service at the university, teaching about 10% of Princeton undergrads. I call them Gen C. They're all Gen Z age wise, but this is generation Gen C for Crypto.
00:22:38.230 - 00:22:38.980, Speaker E: Yeah.
00:22:39.430 - 00:22:42.180, Speaker A: Sentient was only released on Monday as a project.
00:22:43.690 - 00:22:49.242, Speaker B: Going back to the first part of the question you mentioned, there's three challenges, and there was a fourth that you didn't touch on.
00:22:49.376 - 00:23:31.126, Speaker A: Yeah, so I didn't quite get there, but I think Agi is really a grand challenge. How to get towards Agi, what's the right path? And these are mathematical questions we haven't understood evolution to. The current mathematical models of evolution don't quite explain how intelligence came so quickly relative just by using gradient descent and just mutation. In other words, it doesn't quite explain how quickly evolution has. And these are projects, problems that mathematicians and physicists. I just attended a talk at IAs the other day on this, so it's been on my mind. So we don't quite know even for evolution, what, beyond mutation it has worked.
00:23:31.126 - 00:23:49.978, Speaker A: And similarly, how agi comes up with AI interactions and what's the right sort of gradients to give are fascinating scientific questions. There are other partners here who can talk about the crypto aspects. I mean, I'm a crypto guy, but in this talk I was put wearing on my professorial hat.
00:23:50.154 - 00:23:50.686, Speaker B: Thank you.
00:23:50.708 - 00:23:50.940, Speaker A: It was.
