00:00:00.480 - 00:00:31.550, Speaker A: Yeah, happy to be here to talk about a special technique in mega ETH, which we call node specialization. So a bit about me. My name is Lei and I'm a co founder and CEO of Mega Eth. Yeah, so first, some background on mega Ev. So we see a very special opportunity to bring the value of performance to the crypto world. And we call this vision a real time blockchain. In particular, we define it with three criteria.
00:00:31.550 - 00:01:31.420, Speaker A: First is that the blockchain should be able to process transactions in real time as they arrive at the chain. And we specify that by a one millisecond or lower block time. Second, the blockchain should be able to execute and commit the states, and also propagate the states within a one millisecond latency. So in other words, from a transaction getting to a sequencer, to the state being computed and being ready for those fetch, the latency should be less than one millisecond. And third, the blockchain should be able to sustain this level of real time responsiveness, even under heavy load, and that is a throughput of more than 100k transactions per second. So a bit about our funding team. The funding team consists of computer science PhDs from Stanford and mitzvah, and also business professionals from consensys.
00:01:31.420 - 00:02:54.974, Speaker A: All right, so in the next ten minutes, my goal is to help you understand what is node specialization as a technique, why we think it's inevitable to implement it if you want to build a real time blockchain, and also how meta is pushing this idea to the extreme, to build the very first real time block. All right, so first, what is known specialization? So the idea of node specialization is actually quite simple. It's about taking the tasks in a blockchain and assigning them to different cohorts of nodes. And then by doing so, we're able to optimize each cohort of the nodes corresponding to the particular task they are assigned to. And this is a comparison to a traditional blockchain model, where every node will be able to, where every node will be responsible for every task in the blockchain. So to make it more clear, we can look at kind of an abstract architecture of a blockchain, and we can see why no specialization is so important here. The key reason is that blockchains are inherently heterogeneous systems.
00:02:54.974 - 00:03:52.010, Speaker A: So what I mean here is that if you look at the flow of transactions in a blockchain, first you have transactions coming to the blockchain, and the chain has to sequence those blocks and execute those transactions to get to blocks and the final states after the transactions has happened. And ultimately you want to serve some users or some applications, so you have to disseminate the updated the latest state to the users. And we call this process dissemination. And of course the users want to get the proof, gets the kind of the confidence that the state they are accessing is correct. So you have to use validator to validate the latest states and generate proofs for the end users. And of course you also want to make sure that transactions, once confirmed, will not revert. And in other words, you need a process of finalization for these transactions.
00:03:52.010 - 00:05:23.402, Speaker A: And the key takeaway out of gap two is that these tasks have very different requirements in terms of the performance of the nodes that are assigned to these tasks and also the decentralization level of these nodes. So here I'm showing a trade off between the node level performance and the decentralization. So on the horizontal access I'm displaying the node hardware requirement. You can think of it as how powerful a node is, the beefiness, and on the wide access it's how much decentralization such kind of nodes can get to. So the green line is an inherent tradeoff between these two parameters, because if you want really powerful nodes, you cannot really hope for any users to be able to maintain and set up these nodes because nobody wants to have a server with 1 ramdhennae humming along in their living rooms or in their data centers. And on the other hand, if you want really good decentralization, you have to really build very light windows like Raspberry PI's, which people are happily kind of plugging into their browsers and their homes, right? So this is an inherent trade off. You can get very powerful, but not too much decentralization or vice versa.
00:05:23.402 - 00:06:31.018, Speaker A: So if we look at these tasks, right, for sequencing and execution, I think as Kione mentioned, it's super IO intensive, it's super cpu intensive sometimes. So you do need really powerful nodes and for validation, because it's kind of the bedrock of blockchain security, right? It's because so many pairs of eyes, so many validators are kind of monitoring what the sequencer is doing, so that we guarantee that sequencer is correct. So you do need a large high level of decentralization. And in a traditional architecture, because when there's no, no specialization, every node will have to work on every task, and in particular there's only one type of node, one configuration in the network. So you are forcing all the tasks to converge onto one point on this tradeoff spectrum. So for example here I can use a very lightweight node to run all of these tasks. And here it's like the trade off that Ethereum staking.
00:06:31.018 - 00:07:04.520, Speaker A: Ethereum says that I want home staking, I want people to be able to just set up second hand desktops to run Ethereum nodes. So it's good level of decentralization, but there's not enough power in those nodes. And that's why Ethereum nodes cannot really execute transactions so fast. And that's why you have a low throughput. Yes. So yeah, you can see that validation and finalization, they are super happy because they have, they gather decentralization. But yeah, sequencing is not being served really well.
00:07:04.520 - 00:07:53.296, Speaker A: On the other hand, you can also try to use very true servers to serve as our nodes. So in this case, sequencing will be really happy because it gets this level of power, but the chain will not have too much decentralization. And this is kind of the other end of this trade off spectrum for traditional blockchains. And so you will see the key takeaway here is that no one is served, served well simultaneously. Not everyone served well simultaneously. So what we can do with node specialization is to set up different, to define and set up different cohorts of nodes for each task. So we will have, there are two be nodes dedicated to one task, which is sequencing and execution.
00:07:53.296 - 00:08:58.374, Speaker A: And on the other hand we have super likely nodes to do validation and we have other types of nodes in the middle to handle these tasks. And here the key takeaway is that you will see everyone's super happy because there's one tailor made type of nodes serving their task. So let me quickly summarize. It's because that blockchains are inherently heterogeneous. There are different tasks with different requirements on the performance versus decentralizing. With decentralization trade off that you want different kinds of nodes, and if you do not specialize the nodes, then you are forcing every task to converge on the same kind of hardware specification and decentralization level. So everyone has to take a compromise, while if you specialize in the nodes, then everyone will be served much better, even optimally, because you can just tailor made the node for each particular task.
00:08:58.374 - 00:10:23.570, Speaker A: And if we put it back to mega e, we will have a few sequencers, actually one at any given point in time, with 100 plus cpu cores, one to four terabytes of memory, the latest generation SSD's, and really good hardware to run sequencing. And on the other hand, the provers will be really lightweight. Once if you core half a gigabyte of memory and literally no storage needed to run annotation. And in the middle we use Ethereum and eigenvae for finalization because we think it's the best combination of security and performance. And we will have rapidly lightweight full nodes on par with current ethereum nodes to kind of replicate the state of the current blockchain and then disseminate the results to our users. So yeah, to quickly summarize, we have massively powerful sequencers to execute the transactions, maximally lightweight provers to validate the transactions and get to a really decent level of decentralization, very accessible, home operable, full notes for you to kind of get up to date with the blockchain state. And we were inherent security from Ethereum and Inda.
00:10:23.570 - 00:11:32.200, Speaker A: So before I end, there is a quick twist of the plot. I've been talking about no specialization all the time, but I actually, it's just the first step towards a real time blockchain. So in other words, I want to say that no specialization is not sufficient for us to arrive at a really high performance real time blockchain. It's a good step towards that goal, but it's not sufficient. On the other hand, it's a really necessary step because again, it allows us to tailor make a particular type of hardware and software combo for each task in the blockchain so that we can be really optimal in terms of the hardware and software execution. And in particular, many of our optimizations on the node level are not really, you cannot really replicate these optimizations if you do not do no specialization. So I want to make probably one example, which is the parallelization algorithm that I mentioned, that kione mentioned.
00:11:32.200 - 00:12:58.080, Speaker A: So I guess different from many other parallel EVM, we do not use optimistic concurrency control, which I think air monad and many other players in the field are working on. And the key reason here is a failure called cascading upward in the sense that if it happens to be that in your block, the transactions have so much interdependency to the level that it will even be slower for you to attempt to paralyze them. Because the thing is that if you have so much interdependency in your rock, you can literally not paralyze them physically. It's from a mathematical standpoint. However, if you attempt to do that, this attempt will actually cost you more time and resource consumption than just kind of executing them sequentially from the very beginning. And by not using what we call a deterministic parallelization algorithm, we can actually avoid that problem and use a class of algorithms called non deterministic parallelization algorithms, which will be provably free from this kind of cascading abort problems. And for example, this is a key difference between us and other blockchains working on parallelization, and it's only enabled by no specialization.
00:12:58.080 - 00:13:12.600, Speaker A: All right, so I'll conclude here, take questions, and I would invite you to look at our research content on our website and also attend other events at EBC. Thank you.
00:13:15.700 - 00:13:27.244, Speaker B: Thank you. Quick question, similar to Monet, like, what kind of new projects are really on top mega Eve right now, except for fully unchartable decks he only just mentioned?
00:13:27.372 - 00:13:52.360, Speaker A: Yeah, so good question. We have very interesting consumer apps, which I cannot tell about too much, but we have a really good cohort of builders, which we call Megamafia. Some of you might have been to our hacker house. It's just kind of three blocks down the road. And I would really invite you to go and talk to the builders.
00:13:53.900 - 00:14:00.480, Speaker B: Cool. And also because we have different types of requirements, are we using different methods to curate them?
00:14:02.420 - 00:14:06.132, Speaker A: Different methods to curate, like bootstrap, the.
00:14:06.236 - 00:14:10.124, Speaker B: For example, validators, sequencers, because the requirements.
00:14:10.172 - 00:14:57.590, Speaker A: Are different, of course. So, yes, good point. For sequencers, we envision them to be run by really professional providers. We will eventually decentralize the sequencers, but not by putting a consensus algorithm in a critical path like layer ones do, but instead rotate the active sequencers. But yeah, we envision only kind of professional hosts to kind of operate them. And on the other hand, provers are really, like, we really put a lot of thought thoughts into kind of making provers as lightweight as possible. So we, as I mentioned, the hardware requirement be one cpu core, Ava gigabyte Rami, and it's even more likely than a $30 raspberry PI.
00:14:57.590 - 00:15:08.310, Speaker A: So we kind of think that even current Ethereum home stakers will be able to do that. Actually, the hardware requirement is actually lower than Ethereum nodes.
00:15:09.690 - 00:15:18.710, Speaker B: Got it. And yeah, and also because we are real time blockchain, the latency is super low. So I guess the sequencing method we use is.
00:15:21.890 - 00:15:29.070, Speaker A: Yes, so exactly. We do. Currently, at least at launch, we do not plan to put any kind of auctioning in crypto at.
00:15:30.460 - 00:15:32.520, Speaker B: Cool. Yeah. Any questions from the audience?
00:15:39.820 - 00:15:51.480, Speaker C: Just in my understanding that compared to the monad can be understand as layer one, so it is cannot ask some high end hardware to run it and.
00:15:52.020 - 00:15:53.612, Speaker A: Magnetic issues a layer to you.
00:15:53.636 - 00:16:03.614, Speaker C: So it kind of can improve the hardware requirements and that just that everyone on live client can do the validation. It kind of like increase the TBS. So don't understand?
00:16:03.662 - 00:17:02.522, Speaker A: Correct? Um, yes, yes, correct. So the key here, but as I mentioned, right, so, okay, so I think, I think the trick you mentioned is basically no specialization, right? Because the key insight here is that you don't need everyone to be fast in L2, you only need the sequencers to be fast in a L2 or any of the blockchains, right? And yeah, that's one contribution contributing fact. But as I mentioned here, it's so mega eth is definitely not just kind of turning up your hardware to eleven, right? No special edit really just gives you the freedom to kind of stack your hardware as much as you want and also run those optimizations however you like. So for example, all these optimizations, they are only kind of practical when you have specialized nodes. Makes sense.
00:17:02.586 - 00:17:25.760, Speaker C: Another quick question is, I read the magnitude before and claims to be the 1 million tps, some kind of like a very high number of tps. But my question is, is it really necessary to achieve so high TPs? Because sometimes you just need to budget or provide some proof of the series of transactions. So what's idea about it?
00:17:27.780 - 00:17:32.460, Speaker A: Well, you mentioned to batch transactions. Yes, exactly.
00:17:32.500 - 00:17:41.242, Speaker C: Maybe just give me something, let's just maybe batch several transactions to the state to verify on layer one. Some kind of like this.
00:17:41.346 - 00:18:17.206, Speaker A: Yeah, correct. But someone has to execute these batch of transactions, right? So layer one will have the job of kind of really securely verifying the proof of this batch. But the job of L2, or the job of a blockchain that an end user actually interacts with will be to actually run through every individual transaction in this batch. So you cannot kind of escape that problem from that problem. So the high DP's is really needed. Yes. Ethereum may not need 100 kb, but some will have to provide because otherwise who is going to run the special connections and to kind of continue on the question.
00:18:17.206 - 00:19:10.314, Speaker A: We really envision kind of an abundance of computation on chain and I, we don't think this is there now, because when you write smart contract you still worry about gas golfing, you still want to minimize the gas cost, but you do not do that when you do JavaScript programming, right? A chrome tab is 1gb plus my memory now on my MacBook. It's crazy, but we want that craziness to happen on blockchain so that it kind of releases the productivity from the developers, right? It's like assembly language programming back in the eighties versus JavaScript Python programming. To the eyes of an assembly programmer, JavaScript is insanely inefficient, but it's insanely powerful and insanely productive to programming that language. And we want that habit, we want that to happen on logic also.
00:19:10.362 - 00:19:12.630, Speaker C: Boston totally makes sense. That's all my question.
00:19:14.170 - 00:19:35.908, Speaker D: Thank you so much for invitation. Can you go back a few slides about foundation? Just a very curious sense. I mean, we understand Ethereum blocks takes 12 seconds. Do you think that the speed of ImDA is something that is suitable or are you probably looking for a faster alternative?
00:19:36.004 - 00:20:11.640, Speaker A: Yes. So the key here is to realize that for Dau, it's not sitting on a critical path, right? Yes. So what matters here is throughput in terms of how many bytes, either DA or any DA can kind of provide data that they began before per second versus how long it takes for it, for eigenda to respond to a particular request. So in that sense, idenda is more interest. I mean, idea is kind of the leading product in both routes, both throughput and latency, but yeah, and the throughput is totally suitable for us. Got it. Thank you so much.
00:20:13.020 - 00:20:41.100, Speaker E: So I have a question around the real time, time improving part. So how is that achieved? Because right now we know we have ZK proof and op proof. I'm not so sure about the op proof, but in terms of like ZK proofing, it actually requires a lot of communication power and it's not as efficient as right now.
00:20:41.610 - 00:21:07.948, Speaker A: Yeah, very good question. So that's why we are not doing ZK proof and launch. So probably I should mention it. So here I'm for maximum lightweight, I do mean op provers or op validators or just thought proved generators. And for op proof, it's not on a critical path. The way it works is like we assume the sequencer is correct until such a thought proof is raised postdoc. Right.
00:21:07.948 - 00:22:05.240, Speaker A: So yeah, that's how it works. Thank you. How are you thinking about sequencer rotation? Sequencer rotation? Yeah, sequencer rotation as in the mechanism to implement it or the comparison to say a consensus protocol that sits in the critical path. The method to explain there are many, I think one feasible way is to do an auction, for example, for sequencing writes. And that would be kind of the incentive level and we are still kind of working on it. On the kind of the technical side, how to actually implement sequencing rotations from kind of a code level. It will be, you will need a consensus algorithm, it can be a layer one or we can run our own consensus algorithm, which I don't think is really needed.
00:22:05.240 - 00:22:31.270, Speaker A: So you need kind of the option having transparently on a chain with consensus algorithm, with kind of good trust in your trust, and then kind of the auction result will be deterministic based on the bits that happen on this chain. And everyone will just look at the chain and see that, hey, next hour, I should go to that sequencer and I'll just go there. Thanks.
