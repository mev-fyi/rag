00:00:03.320 - 00:00:33.598, Speaker A: All right, thank you so much for inviting me. I'm very happy to be here. And today I'll be talking about sailfish, a latency efficient, DAG based protocol. So, this is joint work with Nabesh and Aniket. So we've been talking about multiple proposals, and DAG based protocols are, you know, a natural way to have multiple proposals. So the talk will be split into two parts. The first part of the talk, I'll try to compare and contrast, you know, DAG based protocols with leader based protocols and try to see, you know, what the differences between the two are.
00:00:33.598 - 00:01:12.250, Speaker A: And this would be the smaller portion of my talk. And the second part of my talk, I will talk about, you know, this protocol itself and analysis on this protocol. So the problem we're talking about is state machine replication, where there are a group of parties who agree on a sequence of transactions submitted by external clients, and they do this even if some of these parties are faulty. Okay, so you have transactions coming in one at a time. They agree, and they output these blocks. There are two key properties that we are looking for, safety and liveness. Safety says that two non faulty parties do not disagree, and liveness says that a transaction proposed will be eventually executed.
00:01:12.250 - 00:01:54.506, Speaker A: So we want to achieve both of these properties, safety and liveness, even when some of these parties may be byzantine. So let's say some t out of n up to t out of n parties are byzantine, and you want this to hold under different network conditions. So the network condition of interest over here for us is going to be partial synchrony, where the network can be asynchronous for a while, but eventually it is synchronous. And it is customary to solve this problem for t less than in O three, under partial synchrony. And if you look at this historically, there are two key metrics that people have tried to optimize. One is latency, and one is communication complexity. So, latency says that latency is the time to commit a client transaction.
00:01:54.506 - 00:02:42.450, Speaker A: And communication complexity is the amount of communication and bits that happens in between parties. And if you go back to the literature and look at all of the protocols, you'll find that most, at least many of these protocols, they have an approach which follows, roughly speaking, the phase King paradigm, where there is a designated party called as the leader, and the leader is responsible for proposing blocks of transactions and getting all parties to agree on them. Okay. If the leader is malicious, of course you cannot expect progress. But if the leader is honest and if the network is good, then you expect progress. As well as safety. So many protocols such as PBFT, tendermint, hot stuff, pala and so on, you know, follow this, follow this chapter.
00:02:42.450 - 00:03:46.418, Speaker A: And in fact, if you look at the latency and communication complexity metrics PBFT already gives you from 2000, gives you ideal latency, hot stuff, hot stuff to give you ideal communication complexity. So then the natural question is, aren't we done? Do we need anything more? Well, what about throughput? So intuitively or theoretically, we always think of, we relate communication complexity and throughput, right? So if you have low communication complexity, it implies you can get a high throughput. But in leader based protocols, it turns out that your leader can be a bottleneck and this can happen in two different ways. So first, concern over here is unequal scheduling of work where you can have a leader. Let's say you have a thousand party system, and let's say in a vanilla leader based protocol, it is sending a 1 mb block. So if it is sending to 1000 different parties, then it is sending effectively 1gb worth of information. But if you look at this from the perspective of individual parties, they're only accepting 1 mb worth of information.
00:03:46.418 - 00:04:19.226, Speaker A: So there is unequal resource usage in that sense. And second, if the leader fails, of course, will change the leader and eventually the protocol will progress. But for that time being, there is no progress that happens. So because of these reasons, despite having good communication complexity, we may not have good throughput. And these are known problems with leader based protocols. And there have been approaches to improving throughput. So there are notions called as extension protocols where you extend one bit consensus to l bit consensus.
00:04:19.226 - 00:04:50.610, Speaker A: And the key idea over here relies on erasure coding techniques to improve them. So there is this other line of work called data availability oracles. Dank sharding tiramisu are examples over here, even over there, they rely on erasure coding techniques. In fact, the first two lines of work, they are similar. It's just different names by different groups of people. There is a different technique called narwhal, which is a systems level optimization that separates transaction dissemination from consensus. So you rely on different worker nodes to actually transmit data.
00:04:50.610 - 00:05:41.080, Speaker A: So all of these approaches really do help in improving throughput. So this new line of work on DAG based protocols actually address both of these concerns in a more natural manner. And they do this by ensuring that every party performs a similar amount of work at all points of time. So because they're doing similar work at all points of time, it fundamentally solves this unequal scheduling problem that we were discussing earlier. And second, because your DAC construction itself does not rely on the leader. So you have a situation where even if a leader fails, the construction proceeds and eventually you will see progress. So the disadvantage though, is since everyone is working at all points of time, you do have high communication complexity.
00:05:41.080 - 00:06:35.380, Speaker A: So before we move on to talking about our protocol itself, I want to make one comment on the terminology itself. So both of these terms, leader based or DAG based, they're actually misleading terms if you think about it. Many leader based protocols construct chains, which are dags. And DAG based protocols typically rely on leaders to achieve consensus. So in my view, the key properties that are achieved by Dag based protocols are the two properties that I described earlier. And while leader based protocols can give you these properties in some ways or some subset of these properties, DAG based protocols give them to you in a very natural manner. With that, if you look at existing DAG based protocols, they give you improved throughput at the expense of communication complexity for small to moderate sized settings.
00:06:35.380 - 00:07:13.810, Speaker A: But they do not achieve ideal latency. And that was the focus of our work on salefish. So we asked, can we obtain DAG based protocols with ideal latency? So in the following, I'm going to describe the core structure of a protocol, and I'll tell you at a high level what the core safety argument is. And I'll show you a couple of graphs on performance. Okay, so it's a DAG based protocol. So everyone is maintaining a DAg. So what you see to the left over here is the frontier of the DAG for a given party at the end of round r minus one.
00:07:13.810 - 00:07:52.084, Speaker A: Okay? So you move from round r minus one to round r. So party, let's say the first party over here proposes a block. It points to two t one vertices from the previous layer, and similarly, other parties will propose similar blocks. Okay, so over here, when I say, you know, you're pointing to two t plus one vertices delivered from the previous round. I mean it in a very specific sense where this delivery happens by a reliable broadcast protocol. So you propose it, you pass it through a reliable broadcast, and once the reliable broadcast protocol ends, it delivers. That is when we call it as delivered.
00:07:52.084 - 00:09:00.454, Speaker A: And this, it turns out it gives you two nice properties. One is non equivocation and one is totality. So, non equivocation says that if I have delivered a block from a given party in a given round, no other party will deliver a different block by the same party in the same route. And totality says that if once I have delivered a block everyone will deliver this block eventually. So together, what this ensures, if you apply this for every vertex, what this ensures is that eventually every party is going to build the same dag. Okay, so if everyone is going to build the same dag, aren't we done? Don't we have agreement? Why do we need any consensus? Well, if you look at the view of any party at any point in time, that may be different, eventually they're going to be the same for a given round, but for now they may be different. So as an example, the view of party PI over here at round r may be that it has received the first three vertices but not the last one.
00:09:00.454 - 00:09:35.430, Speaker A: And for PK it may be the case that it has received one, two, four and not vertex three. Eventually they'll receive it. But if I have to order right now, that may not be the case. And this is why we need agreement on whether a vertex is delivered. So how do we obtain consensus and order these vertices? In our protocol we have these designated parties called leaders. So the vertex that has a symbol, the leader symbol on it is the leader for that vertex. And let's assume that everyone knows who the leaders are for each of these rounds.
00:09:35.430 - 00:10:04.720, Speaker A: So our goal is to commit a leader vertex in any round. So if I'm committing a leader vertex in round r, what we do is all of the parties in round r one, they prioritize pointing to the leader vertex in round r. So they will wait for the leader vertex until some time out and make sure that they point to it. If it doesn't arrive by then, maybe they won't, but until then they will try to prioritize pointing to the leader vertex.
00:10:05.060 - 00:10:05.788, Speaker B: Okay?
00:10:05.924 - 00:10:40.784, Speaker A: And if it so happens that you know this leader vertex in round r, let's call it vertex v. If it receives more than two t plus one votes, or there are more than two t plus one vertices from round r plus one pointing to it, then we can commit this leader vertex. And this is when I'm going to order some vertices in the past. Okay, so you do this in two steps. The first step is you try to find path to a leader vertex in previous rounds. So the largest previous round. And you try to recursively commit vertices in that round.
00:10:40.784 - 00:11:26.030, Speaker A: So as an example over here, this round r leader vertex is pointing to this around r minus one leader vertex. So you try to recursively come at this round r minus one leader vertex if you haven't done so, and then you order all of the vertices that remain that are still pointed to by this round r leader vertex. So you can apply any deterministic ordering that you prefer. So that's the protocol over here. There's just one step that I haven't mentioned. So you prioritize pointing to the leader vertex. What if the round r leader is byzantine? We know that because we are using reliable broadcast, it cannot equivocate.
00:11:26.030 - 00:12:17.818, Speaker A: But what if what it can do is, you know, it can feign a crash, it can ensure that you do not receive this delivered block at all. No, perhaps no one receives it. So if a roundout leader vertex is not delivered in a given amount of time, let's say until some timeout, all of these parties send a no vote message to the next leader. And this next round leader, round r plus one leader would use a certificate of two t plus one party stating that, you know, we haven't voted or we haven't delivered this round r leader. So it uses this to justify the fact that this round r leader does not exist. So if from round r plus one leader's perspective, either I point to the previous leader or I show the certificate saying that this leader does not exist. Yeah, so that's the protocol.
00:12:17.818 - 00:13:16.930, Speaker A: And the safety argument over here is. Is actually quite neat. So let's look at if we want to ensure everyone has the same ordering, what do we actually need? So the core guarantee we want is the following. Let's say there is some vertex, some party that has committed a round r leader, and let's say there is some other party who has not committed this round r leader by satisfying the rule that we described earlier. Because maybe some vertex was not received in time and it could not see the two t plus one votes, but it commits some other vertex in a higher round r prime. Okay, what we want to ensure is that both of these parties order all of the transactions that they have received in the same order. And because when we are committing this higher round leader vertex, we are trying to find a path to a previous leader.
00:13:16.930 - 00:14:15.736, Speaker A: What we want is to ensure is that this path really exists. Okay? So if some party commits around our leader vertex, then there is a path from round r prime leader vertex to this vertex, and this should hold for any higher r prime. Let us see how we can, you know, satisfy this claim and we'll do this in two parts. We'll first talk about when r prime is the immediate next round, and then we'll talk about any higher r prime. Okay, so when r prime is the immediate next round, observe that there are two t plus one votes from around r plus one on around r leader vertex. And out of these two t plus one votes, t plus one of them are coming from honest parties. And since if they vote, they will not send a no vote message to the next leader, a no vote certificate cannot exist that is sent to the next leader.
00:14:15.736 - 00:14:51.142, Speaker A: And if the next leader has to exist, it has to point to this round r leader vertex, because it only has two options. Either show a novord certificate or point to the previous leader vertex. So for this reason it has to point to the previous leader vertex. So this is part one for an argument for higher rounds is simple. And I'll show you an argument for r plus two and for, you know, r plus three and so on. The same or a similar argument actually hurts over here. We are interested in two different quorums.
00:14:51.142 - 00:15:21.450, Speaker A: The first quorum over here is q one. So from in round r. So in round r plus one, observe that two t plus one parties are pointing to this round r leader vertex. And let's say this quorum is quorum q one. It is denoted in orange color. If I look at the leader vertex in round r plus two. Now it also has to point to two t plus one vertices in round r plus one.
00:15:21.450 - 00:16:07.906, Speaker A: So this is denoted using the green arrows. And let's call this quorum q two. So the two quorums q one and q two has to intersect in t plus one vertices. And out of these t plus one vertices, you know, up to t may be byzantine, and there has to be at least one honest vertex because of which there will exist a path from the leader vertex in round r plus two to the leader vertex in roundup. So that's the core safety argument for the protocol in terms of latency. You know what a leader vertex of round r gets committed in round r plus one. So essentially it's two RBC's.
00:16:07.906 - 00:16:31.640, Speaker A: But it turns out that you don't need to wait until two RBC's are done to actually commit this leader vertex. In fact, because of the way our quorums are aligned for the second, in round r plus one, you can actually commit as soon as the first message in the RBC is received, because of which the latency to commit is one RBC plus one delta.
00:16:31.940 - 00:16:32.636, Speaker B: Okay?
00:16:32.748 - 00:17:16.040, Speaker A: And for non leader vertices, you know, when you commit round r leader vertex, you commit non leader vertices from round r minus one as well. And they were proposed one round earlier. So that's why it requires one additional RBC to commit them. So the key advantages of sailfish are, you know, it's the latency you achieve one RBC plus one delta and two RBC plus one delta respectively. And this is better compared to all prior work. Second, we have the protocol as a leader in every RBC round. So this is compared to state of the art protocols like show, you know, this improves on the latency of non leader vertices.
00:17:16.040 - 00:18:07.840, Speaker A: And finally, the architecture over here, by relying on RBC's, it simplifies things quite a bit compared to, you know, works like cordial minus and mysticity, which open all of these black boxes. So the final thing that I want to mention is, you know, the performance results. You know, we spoke about things happening in theory. You know, can we observe these latency advantages that we see in theory, in practice? Okay, so we compared our results with Bulshark and Shoal, and we set up, we actually modified bull sharks code to accommodate sailfish. And we set up our experiments in such a way that all of them were geo distributed in five GCP regions. And here's a quick graph that we have. On the x axis you have throughput, and on the y axis you see latency.
00:18:07.840 - 00:18:46.520, Speaker A: There are three different curves over here, one for bull shark, one for shoal, and one for sailfish. So the way these experiments are performed is you keep adding more and more transactions to the system until the system. So initially the system can handle it, and that's why your latency stays flat even when your throughput is increasing. And at some point the system cannot handle it. And at that point, all of the transactions are buffering and eventually the latency shoots up. So you always see these hockey stick curves. And what we are interested in is the inflection point, okay? So, because this is a throughput versus latency curve, higher throughput and low latency is good.
00:18:46.520 - 00:19:24.350, Speaker A: So to the right and lower is good. And as you can see with sailfish, with the blue curve, you see, you know, it is to the right and lower than the other two. And in terms of, you know, how does it, how does it scale with respect to the number of parties over here? We conducted these experiments for different network sizes, 1020 and 50. And as you can see, everything that we said in terms of latency holds true for as we scale in as well. So on the y axis, you have latency, and the blue line is always lower than the orange and the red line.
00:19:26.850 - 00:19:27.378, Speaker B: Okay?
00:19:27.434 - 00:20:05.190, Speaker A: So with that, let me conclude my talk. So we first spoke about DaG based protocols. We observed that the key things that they give you are equal scheduling of work in progress at all times. Then we spoke about sailfish which runs in RBC based rounds, and we saw that it achieves a latency of one RBC plus one delta and two RBC plus one delta for leader and non leader vertices, respectively. And there's a third part which we actually did not discuss in the talk that is supporting multiple leaders in a given round. Thank you, and I'd be happy to take questions.
00:20:14.770 - 00:20:39.618, Speaker B: Hi, so there is a theoretical minimum latency in which consensus can be run, at least with this failure tolerance. And if I recall, it's about the same as the theoretical minimum latency for RBC. So is there any hope for getting the latency for something like a DAG based protocol like sailfish down to the theoretical minimum for consensus? Or is there a fundamental difference between the latency? These kinds of things take so that.
00:20:39.634 - 00:20:58.020, Speaker A: The theorem minimum latency for consensus or byzantine broadcast in partial synchrony is three delta and over here one RBC. If you assume a latency efficient RBC, it's two delta time. So two delta plus one delta is three delta. So over here for leader vertex, we already achieved theoretical minimum latency.
