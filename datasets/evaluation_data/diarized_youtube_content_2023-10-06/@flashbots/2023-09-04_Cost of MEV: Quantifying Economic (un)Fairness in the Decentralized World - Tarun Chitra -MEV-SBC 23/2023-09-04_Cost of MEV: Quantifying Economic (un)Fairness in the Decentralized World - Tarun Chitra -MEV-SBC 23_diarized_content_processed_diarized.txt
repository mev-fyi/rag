00:00:02.890 - 00:00:35.222, Speaker A: Cool. So you know, I think a lot of the talks have been sort of about qualitative phenomenology and in some cases a little bit of quantitative phenomenology with regards to mev. But this talk is going to be in the other direction. I gave a talk sort of about things related to this in Paris, but it wasn't in Beamer. So that's your warning. But this is obviously joint work with Guillermo Theo and shithage. Cool.
00:00:35.222 - 00:01:43.680, Speaker A: So maybe let's start with the first question, which is can you really define mev correctly? I mean, the real problem is that there's a lot of different things that people have proposed which all have very different formalisms, sort of different reasons for why they exist that have different equilibria. And now there's things that maybe I'd say the majority of talks today would argue that a lot of the FCFS fair ordering stuff doesn't have the positives people expected. But then how do we also look forward at things like suave, time boost, et cetera and how do we compare them to one another in a way that's sort of rigorous? But I feel like no talk is full without a meme. But I think all of these claims sort of effectively kind of say here is this part of the world that I think is mev and that's it. I don't care about anyone else's. All I care about is the thing that my model was built to solve. And unfortunately that means that when you have ten different models it's impossible to compare them and you kind of run into a bunch of sort of epistemological issues.
00:01:43.680 - 00:02:58.866, Speaker A: Okay, so maybe let's try to without giving a little more formalism. So a lot of these mechanisms, you could argue, are sort of dynamic ordering mechanisms. A lot of the new things like time boost, suave, anoma and what you're really doing is you have some means where you're indirectly specifying some constraints that are restricting sets of orderings or sets of transactions. And one question is if we have a payment f given to validators given from that depends on this choice of orderings, how much more fair is one set than the other? And the word fair is a very politically charged word, I think, in many contexts, not just in this industry. But I think we're going to go with the definition from this classical paper by Halvarian, who's head economist at Google, also sort of professor here a while ago, which is the worst case payoff is not too different from the average case payoff. That's like a very nice qualia definition. Okay, so let's now go to the text definition of mev.
00:02:58.866 - 00:03:53.800, Speaker A: Mev is any excess value that a validator can extract by adding, removing or reordering transactions. There's four things we have to define here excess, adding, removing and reordering. So what do we need to define it? Well, first we need a universe, a set of transactions that represents all the state transitions that are allowed, could be very large, could be very small if we restrict it to a particular application. The second is we need a notion of a payoff function, and a payoff function gives you a value for a set of transactions and an ordering. So, sets of transactions, like I said, you start with this universe, we have some subset of it given a set of transactions of N transactions. There's also an ordering represented by an element of the symmetric group pi. And the intuition is that the payoff to a validator for including T with this ordering is represented by F.
00:03:53.800 - 00:04:25.650, Speaker A: But now we also still need a little bit more than that. To understand censorship, to understand reordering, we need to look at how F varies over its domain. Unfortunately, this domain is quite large, as you can see. It's a set of subsets, it's a lot of permutations. There's a lot of exclamation points that will be in this talk, unfortunately. And so this is needed to define excess. This sense in which this maximum, this kind of optimization problem is boundable, is this notion of excess.
00:04:25.650 - 00:05:24.100, Speaker A: Okay, first question, isn't T way too large? Shouldn't this be like sharp p complete? Shouldn't t be some very huge thing? For sure, pick your favorite combinatorial, worst case function, and of course you can find a way to make T that big. On the other hand, you can do things, and we've seen a lot of talks do this today where you restrict T for a particular application. So maybe you're a constant function market maker or an AMM, and you have a kind of small space of transactions like it is uncountable, but there are equivalence classes of them that are finite in terms of trades, changing liquidity. Similarly, for lending, you have sort of three classes of actions, supply, borrow, liquidate, and you can analyze these for these subsets. So the next question you might say is, hey, I am an Mev searcher. I am making a new type of auction. I am doing some preference expression thing.
00:05:24.100 - 00:06:02.270, Speaker A: How in the world do I describe this function F? This function F that represents a payoff to a validator, is kind of hard to reason about. But if you think about it, every time someone is simulating a sequence of transactions before they submit a bundle, they're evaluating F. So you're getting point wise evaluations of F at all these points. Okay? So now the next question is, why is it hard to quantify mev? And so I think I want to try to give you some combinatorial reasons for how to think about this. So we have the set T. We have a set of transactions that we want to look at. We have a permutation on S elements.
00:06:02.270 - 00:06:55.070, Speaker A: Okay? This means that the domain of the function is the union of all K element subsets plus the set of permutations. That is a big set. How big is it? Well, it actually will be something like bounded above by some constant times the size of the number of transactions factorial, but it's bounded below by t factorial because the worst case is you include everything. So the space of all payoffs that are represented by an mev auction is actually quite large. Not super surprising, but sometimes good to have the symbols there. So now again, we get back to where we started. What's the definition we want to use of fair? Fair is, again, intuitively, worst case payoff is not so different from a random payoff.
00:06:55.070 - 00:07:50.770, Speaker A: And the reason for that is that in some idealized world, the random payoff sort of represents the ability of the validator to not be able to censor, to not be able to add, not be able to reorder. Now, how we compute that average is the devil in the details, but let's take a look at this. First we have the worst case payoff. Take any set of transactions, take a permutation, try to find the maximum over that. Next, we have the average case payoff, in this case simple average. And we define sort of relatively simply the cost of fairness as the difference between the worst case payoff and the average case payoff. Now, there's a notion in which a payoff is fair if C of F is small and the rest of this presentation will be to make small a little bit more rigorous.
00:07:50.770 - 00:08:43.460, Speaker A: So we'll actually only really focus on reordering and they'll have some notes on censorship at the end. But there's sort of the same techniques can be used for quantifying censorship, but for the rest of the presentation, assume the set of transactions is fixed and we're really only looking at reordering. You can think of this as like encoding the subset as taking the maximum. Another kind of technical detail is that we just sort of assume there's no nontrivial invariant subsets. That means that there's not sets of permutations that leave the value function invariant. If you know a little bit of algebra, there's ways of removing this. But just for simplicity of this talk, you don't have to think about those, but the paper does have those.
00:08:43.460 - 00:09:37.670, Speaker A: Okay, let's do some high school bounds. What's the expectation of this function? Expectation of this function is just sum all the values, divide by total number of values and guess what? The sum is greater than the max. This implies the kind of simple upper bound which is that this sort of cost of unfairness is always bounded above by the worst case value multiplied by one minus one around factorial. And a very key important thing to note is that this bound is achieved on the indicator function of single permutations. So what that means is my payoff is if you give me a particular ordering, I give you one, maybe I give you one ETH. And if you don't execute the block with that ordering, you get payoff of zero. What also turns out to be true is that the converse is true.
00:09:37.670 - 00:10:38.402, Speaker A: So any function that is saturating the maximum must be an indicator function. And you can sort of intuitively think of these indicator functions as liquidations in DFI. So imagine I have in this figure, I have sort of three trades that are minus one, so they're going down in price by one, four trades that are going up plus one. And I'm looking at different permutations of those trades. And suppose that I only get liquidated if I touch minus three on this and you can see that different permutations avoid touching that point and other permutations do make the trades touch that point. So there's a sense in which, again, the sort of basis of the set of functions of these payoffs that you have in mev are really generated by things that look like DeFi liquidations. Okay, so what are some properties of CF? Well, it's positive homogeneous translation variant sort of comes directly from the definition of max and min expectation.
00:10:38.402 - 00:11:26.066, Speaker A: But one important reason you might care about this is that now you can only consider payoffs that are sort of between zero and one because you can always rescale and translate. So this means you really are looking at this multiplicative factor between zero and one minus one ran factorial. So now we get to the question of what does it mean to be small? So we have this notion of fairness. We have a notion of saying I have the worst case, I have the average case, I'm fair if it's small. Normally in computer science you say, okay, say I have a graph theory type problem. I might say there's some threshold as a function of the number of nodes. And if some property of this system is less than that threshold, then you don't achieve an average.
00:11:26.066 - 00:11:53.970, Speaker A: If it's greater than threshold. You do an example being the airdrone phase transition and edge density being log n over N. Unfortunately for this problem, it's not quite as simple. There's not a simple threshold phenomena like that. It really depends on sort of some notion of the smoothness like calculus style stuff. But of course the domain of this function is discrete. So we need to take some intuition from calculus and apply it to these discrete objects.
00:11:53.970 - 00:12:42.650, Speaker A: There's sort of two ways of quantifying smoothness, how smooth a function is. So a perfectly fair payoff function is one that no matter what subset or reordering, you generate the same payoff. Sort of flat that is sort of the smoothest thing. A perfectly unfair one is the thing we just saw where there's only one particular ordering that generates the maximum payoff. And there's different ways of measuring this form of smoothness and we consider kind of two of them. So one kind of other note I want to make. So some of the earlier talks today, I think like Phil's talk, some of the kind of prior work on AMMS tries to bound these types of fairness quantities in terms of ratios.
00:12:42.650 - 00:13:31.230, Speaker A: So the ratio might be take the max, take the min, divide them, or take the max, take the average, divide them. These measures aren't really smooth though. So for this liquidation example, the max min one is unbounded and the competitive ratio, which is the average one, is very large. It's the size of the domain, so it doesn't really tell you that much. So there's a sense in which the additive version of this is a lot smoother when you have to deal with things that are very not smooth payoffs like liquidations. Okay, so as I said, let's take some inspiration from calculus. When you take real analysis, first type of smoothness you learn about is Lipschitz smoothness.
00:13:31.230 - 00:14:30.114, Speaker A: Lipschitz smoothness is when I have a metric space that has a metric D and I say that, hey, the change in value of the function is upper bounded globally by some constant times the metric. Here we make a sort of similar version of that, except we consider the set of permutation invariant metrics and it turns out these things are sort of nice to work with and they also are what you expect, like LP norms, for instance. And one fun fact, not sure if this is a great fun fact for party, but is that if F is smooth, the cost of fairness is two L smooth. So it sort of says if your function is smooth, this difference between the max and average is also smooth. So I'm only going to kind of briefly go over these examples. I'd read the paper if you really want to understand these fully. But in front running you have trades from users and you have trades from the validator, where trades from the validator means whoever's doing the front running.
00:14:30.114 - 00:15:29.094, Speaker A: That's a little delta and you have a notion of a payoff that is the AMM curve's change in price, including the front run and not including the front run. And a really nice thing you get is that the gradient of the trading function, which is capital G, this is the thing that measures how much slippage you get for different AMM, right? So Uniswap has this sort of one over square root g and Curve has a sort of like one over x to the C dot dot dot. There's a way to write as a cubic g. But the key thing is that the gradient of this function controls the deviation between the max and average. Sort of what you expect intuitively, right? An AMM is sort of smoothish. Sandwich attacks, on the other hand, you can show have actually better bounds than the worst case front running bound, which is sort of a somewhat unintuitive thing to see. But yeah, I'll leave that for the paper.
00:15:29.094 - 00:16:04.430, Speaker A: Since we have five minutes, I want to go to the more interesting things, which is spectral graph theory. So one problem with that initial definition of smoothness and smoothness saying something about how fair something is is that it's global. I have to look over all possible pairs and I need an upper bound over all of them. But maybe I only care about things locally. Maybe I only care about transposing two sandwich attacks. Maybe I only care about a tiny change to a liquidation. I don't care about globally reordering the whole block.
00:16:04.430 - 00:16:46.090, Speaker A: This is in a lot of proposals where people have these kind of like lanes within a block. This kind of matters a lot more because you don't really care about the interaction between two separate sort of chains. So one question is can we localize this notion of smoothness? Well, one way is to try to find a graph associated to the domain of the payoff function and talk about how things change as I move locally on that graph. So here's an example. So this is a graph of permutation. So this is sort of a set of possible orderings. Like maybe you wrote a mevm contract in Suave, maybe you wrote an intent in anoma.
00:16:46.090 - 00:17:48.370, Speaker A: And these are the set of allowable orderings. And there are some sets of transitions between those orderings that are allowed by the way you wrote your code. So now you say I want to bound the deviation around a single point because this is the initial ordering I have. And in the set of other orderings validators can provide, how much worse can they make it subject to the constraints of, say, a system like swap? And the idea here is you're really thinking about the local properties of this system. So what you can do is you can define a sort of local cost as sort of the square difference of the costs of things on this graph. Having bounds on this allows you to really make these kind of local smoothness definitions. And this function CG, which is just doing some linear algebra, very basic linear algebra happens to be translation invariant and homogeneous.
00:17:48.370 - 00:18:32.382, Speaker A: What does it mean to bound CG? Bounding CG is bounding the eigenvalues of the operator l. If l has eigenvalue decomposition u transpose sigma u, then the graph Fourier transform of F is UF. So how do we interpret this Fourier transform? The Fourier transform, the frequencies sort of measure smoothness. In some sense, they measure as I as I go through, I take a permutation. I look at the behavior of the payoff function as I go along a cycle. How much does that change? And a very nice fact is that this measure of fairness is zero. If all the high order Fourier coefficients are zero, that's effectively saying it's a constant function.
00:18:32.382 - 00:19:23.460, Speaker A: It gives you the same payoff. It's a batch auction. Another nice fact, but of course we're not going to talk about the proof is that you can bound this unfairness cost via the spectral mean. So if I could compute the eigenvalues of this matrix l, I can now tell you something about your payoff function. So you can kind of imagine a world where you write a contract like something like something in Suave, something in Noma, you are able to convert it to a payoff function and then you're able to get some bounds on what the max and average deviation can be. And if you know anything about Markov chains, this is sort of analog of cheeger's inequality. So again, this gets back to how do you use this? You have these systems that give you constraints on sets of orderings.
00:19:23.460 - 00:20:02.704, Speaker A: When you have those constraints are defining a graph like this. And if you can find the spectrum associated to LaPlace of this graph, you could say something interesting. There's sort of two problems. One is, hey, this graph is huge and if that matrix is dense, then worst case O of N factorial cubed, which don't try computing any of those for N greater than ten. But if it sort of partitions nicely or the graph is very sparse, this could actually be extremely easy to compute. And the other thing is the bounds are kind of loose. So with the last 30 seconds I'll just give you a high level view.
00:20:02.704 - 00:21:08.680, Speaker A: There are ways of sort of trying to decompose the operator l bound the spectra on the decomposition of these invariant direct sum components. And there is this kind of Wygerson uncertainty principle that lets you get a bound on the multiplicative piece that you can use. All right, we're basically done. So formalized mev in terms of combinatorics, which is nicer than programs defined fairness as this worst case minus average case and then showed that spectral bounds which bound the smoothness of these payoff functions helps you bound the fairness, this notion of economic fairness. And then we should sort of there's some linear algebra you could do to try to give certificates of fairness to a user if their sort of protocol is going through a particular ordering rules such as suave or anoma and Fourier analysis or the symmetric group is quite important here for this and with that I don't know how much time there is. But none.
00:21:22.980 - 00:21:54.044, Speaker B: So really good talk. One thing I was curious about is that you mentioned like liquidations and how by changing the ordering you could affect you could sort of change the fairness, but just more empirically. What does it look like in DFI? Because D five is mostly reliant on chain link. Right. And I guess maybe the edge case is like okay, now the 30 minutes T wop is now moved into a range where some people can get liquidated, but really most people have moved away from B three T wops for instance. Right. Are you referring to something more general?
00:21:54.242 - 00:22:42.344, Speaker A: Yeah, I think the idea is just more that any payout function that is a function of the set of included transactions and the reorderings can be decomposed into things that look like these TWAP based liquidations. So the idea is that you can sort of represent your payoff, which might be your NFT auction, it might be AMM, it might be friendtech bonding curve, whatever. Those things all can be sort of change of basis, like from linear algebra. You can just change the basis to a liquidation basis. And the liquidation basis is easy to work with. That's like kind of the beauty of this, right? The Fourier transform over finite groups, whether a billion or non Abelion sort of constructs this basis. And the idea here is that your notion of fairness is sort of bounded.
00:22:42.344 - 00:23:54.070, Speaker A: It's easy to bound it in this basis. So you change basis and then you kind of try to get some effective bounds. That's more what this is about, just in terms of practical application of this work. Is it correct to think about things like PBS or PEPC as constraints on the set of permutations? That can be done and that in turn sharpens the bounds and allows you to read more carefully about the effects of these different mechanisms on the Amoeba landscape? Or is that just totally off base? I'm not sure if PBS itself I'm not sure if PBS itself restricts maybe not PBS, but, you know, the whole like I think like, Suave is a great example, right? Like, if you look at a mevum contract, a mevum contract has some conditions that you're writing as a piece of code, but that piece of code is restricting that. It is restricting the set of permutations and restricting the set of included transactions. Anoma is doing the same thing, right? So Anoma and Suave are I think the things you should be thinking about is like, hey, I'm giving you this particular constraint on your ordering for your application. How much more fair or less fair does that make it? And these bounds give you a way of doing that.
00:23:54.520 - 00:23:55.430, Speaker B: Makes sense.
00:23:59.630 - 00:24:21.570, Speaker C: Another quick, somewhat related question on applications. Would a potential application of this be using this to set an upper bound on a Dutch auction for a full block order, rather than doing, like, the PBS bundle based auction system? And would that be a potential improvement just due to the nature of Dutch auctions?
00:24:22.550 - 00:24:55.530, Speaker A: Yeah, I could see something like that working. I have to actually try to write that out. But yeah, certainly for certain auction types, you can represent them as things that look like the inverse of liquidations and then you can kind of bound these things more easily. But yeah, the idea really just boils down to what things can we take that let you say something about how smooth these payoff functions are, and in Dutch auctions, oftentimes they're not smooth at certain thresholds, but maybe you can show that in your case that's not true. For instance. Cool.
00:24:55.600 - 00:24:55.880, Speaker C: Thank you.
