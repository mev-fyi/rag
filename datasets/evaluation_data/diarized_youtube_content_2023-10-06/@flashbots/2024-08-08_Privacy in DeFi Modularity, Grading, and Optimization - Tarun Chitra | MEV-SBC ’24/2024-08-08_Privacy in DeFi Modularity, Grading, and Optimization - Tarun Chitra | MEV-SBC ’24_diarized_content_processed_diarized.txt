00:00:03.760 - 00:01:02.966, Speaker A: Hey, everyone. So I think the previous talk did a lot about the, I would say some of the more concrete implementation questions around privacy in Defi. This talk is going to be more about some of the theoretical type of problems and sort of what things that might be useful for solving them look like. So modular privacy is a made up term made up by me. But I feel like there's this natural thing when you think about writing confidential defi applications where there's three very broad strokes types of state that you have. You have fully public state, you have statistically private state. So that's something like differential privacy or something with some pir type of retrieval mechanism, and then you have fully algebraically private.
00:01:02.966 - 00:02:07.280, Speaker A: So when I say algebraically private, I mean t fhe potentially mpc. And one really important thing about writing these applications is you have to understand when you cross a state boundary. When I convert a private state into a public state, clearly I've crossed a boundary, but maybe I have some private state that generated some output, and from the output, you have some information about the private state from the public output. And understanding when you cross those boundaries is quite important. Another important thing about privacy is different levels of privacy lead to very varied composability guarantees. And one question is, how do you blend these guarantees in a unified manner for users and developers? I think we'll talk about how to think of this in a more algebraic way. Finally, the last thing is confidential DeFi can only really succeed if it has any ability to be competitive pricing wise with public Defi.
00:02:07.280 - 00:02:53.260, Speaker A: Public defi is just so much cheaper, it's never going to work. We're going to talk a little about that. The only show I will have is in the lesson I have who's in the audience have a paper on something related to resource pricing today. All right, so modular privacy. So the first axiom you should always remember is you really can't have fully private finance. Fundamentally, finance relies on selective disclosure for supply and demand to match. So you need some public state, right? You need a price, you need an interest rate, you need a fee, you need something that inherently has some notion of aggregating many people's information and constructing a public output.
00:02:53.260 - 00:04:05.842, Speaker A: In the long run, users just won't utilize protocols that don't provide any public information. Now, I added this disclaimer that meme coins don't count, because I needed to add the disclaimer given the post Solana address, send money meme coins, which for some reason have no public disclosure, unsurprisingly. But if you look at the general sort of literature, within the economic side of the world, a lot of things are based on selective disclosure. So all of auction theory is based on some notion of how private my valuation for an item is. Do only I have a value for the item? Does my value depend on your value? Do our values interact with one another? But on the other hand, finance does partition private state. So there's a sense in which all transactions, and I don't just mean blockchain transactions, I mean outside real world touch grass transactions can always be partitioned into sort of three rough states. One is fully public state.
00:04:05.842 - 00:04:35.200, Speaker A: This is a government telling you a tax rate or a reserve rate or a public price. One is private state, individual balances of banks and users and entities. And the final is things that are public aggregations of private states. So this is things like Libor. These are indices, these are CPI indices. These are things where the inputs themselves are not fully public, but the output is public. Some aggregation, some function that takes in those inputs.
00:04:35.200 - 00:05:13.270, Speaker A: One other thing about this talk is, it's not going to be particularly formal, but it will talk a little bit about a research direction that I have been going on. As a disclaimer, let's take the classic blockchain example. You know, the easiest thing to analyze, constant function. Market makers, they're sort of. Because they're the easiest, it's sort of. They're a nice testing ground. But of course, for generic applications, you will need more strength.
00:05:13.270 - 00:06:11.640, Speaker A: But for CFMMs, a natural thing to note is they have a fully public pricing mechanism. And there's a very simple algorithm for figuring out how to invert the total balances from just knowing the trades or the prices. On the other hand, they're easy to make statistically private. So, a, they're hard to make algebraically private because they disclose a price. And you can always invert what people's trades were, but you can make them statistically private. So the first version of this, you know, is a paper of mine from a few years ago, which shows how to get differential privacy for these. The second version is actually a question of, well, if I wanted to be differentially private, how do I have to change the amm? And this is a paper from last year from Goyal and Rampser, which sort of shows there's a minimum amount of noise you have to add to the fees that users pay.
00:06:11.640 - 00:07:30.762, Speaker A: So in order to get privacy, there has to be a randomization of either the fee or the price. It turns out the minimum distortion is the fee. So again, this example shows you this notion of I have public state, I have a price, I have statistically private state, which might be balances or might be some aggregation, and then maybe I have algebraically private, which is a single individual state. So, you know, especially if you look at how people are trying to write private DeFi applications in general, you do see people sort of crawling in the dark in their code bases, manually segregating things into these kind of three types, where there's this clearly public thing that everyone can call and read, there's this clearly statistically private thing that is an aggregation of a bunch of private state, or indirectly leaks a bunch of private state. And then there's the algebraically private piece, which is what a t or fhe gives you. In a world where this were easy to write software for, you could imagine a library that handles boundary crossing. So like, hey, when I up convert some private state to public state, I make sure that this only is encapsulated to some particular area, or vice versa.
00:07:30.762 - 00:08:27.804, Speaker A: If I have statistically private state that I want to make private, I have to add more noise before I put in enclave. So this gets us to the natural question of what does composability look like for these kind of private actions? And the interesting thing is, when I have fully public or fully private data, so fully public could be finance, fully private can't be financed in the sense of there's no public data for people to make an interaction against, those are self composable. Once I stay in the enclave, and if I never leave, I'm fine. And if I'm always public, it's always fine. But the problem is, again, when we cross a boundary, we lose some composability. So the first sort of inklings of this are when you look at differential privacy, your differential privacy worsens when you compose functions. Now you can bound how much it worsens, but it definitely gets worse.
00:08:27.804 - 00:09:40.050, Speaker A: And it gets worse than potentially really bad way, depending on your calculation. But the important thing is, statistical privacy has partial composability. Like yes, if I compose two statistically private datasets, I might get a worse notion of privacy in the sense that someone only needs fewer data points to recover the input. But nevertheless, at least there's some partial composability. And the one thing I want to point out is there are many different results that are inequivalent for how this partial composability works. And so this in and of itself tells you that you can imagine that a developer who has to think about this would get extremely confused as to which one they're actually really using. So I think a lot of these composition theorems sort of suggest you should think about privacy in this algebraic term, in these algebraic terms, which is you have fully private state, you can lift it, you can forget that you're fully private by adding some noise and get statistically private.
00:09:40.050 - 00:10:51.762, Speaker A: Or you could reveal it fully and you get public, and you could kind of think of this idea of like, when I'm in one of these states. So when I'm fully private, I can do compositions that are fully private. When I'm fully statistically private, differentially private, I can do compositions and with some loss. When I'm fully public, I can do compositions like Ethereum today. But r1 question is, what happens if I go to the right and then I go down, so I take some private state, I lift it to a differentially private state, and then I compose that state? How much worse is that guarantee than my initial state? This is a natural question to ask, because this is a question of, well, if I knew the answer to that, I could make a compiler that takes in your code written as purely public state, and figures out how to allocate to each of these groups. One nice thing, if you think of it in this manner, is you get a minimal lossiness guarantee. This kind of view preserves your sort of information theoretic privacy within each category.
00:10:51.762 - 00:11:51.008, Speaker A: But the other thing is, it makes software development a lot easier. And I think that in and of itself, is probably one of the real reasons people don't write private software. Private defi to quintus question earlier okay, the final thing that I want to kind of talk about a little bit is, okay, well, great. So now we know we should be thinking about different types of privacy. We should be thinking about how the different types of privacy compose. And potentially, if we knew how to do that, we could make it much easier to do private Defi. So in the long run, what properties do we really want from an economic perspective, out of these protocols, you need some notion in which there's a amount of revelation that a user has to specify how much state they publicly reveal to get an outcome that they want.
00:11:51.008 - 00:13:15.910, Speaker A: So how much do I have to reveal about how much liquidity I'm providing in a lp pool do I have to reveal to get my fees optimized? On the other hand, you want to reduce efficiency due to increased privacy as minimally as possible. You have this natural minimax like problem, and your goal is to find a compromise between these two extremes. It's possible that there might not exist such a thing. But for CFMMs we know that does exist, which is some suggestion that there are a set of DeFi protocols that would give you this property. The second thing is we want the welfare for all the users of the private version of the protocol to always be sort of some constant fraction of the public version in some sense that sort of says, hey, look, even though I'm going into the private mechanism, I'm only losing at most 10% efficiency, I'm only getting a 10% worth price. And the final thing, which I think is something that's often missing in papers about private applications, is the user should be able to individually choose their desired level of privacy versus efficiency. I think in a world where the developer chooses for the user, it becomes a lot harder for the user to even reason about whether they value the privacy or not.
00:13:15.910 - 00:14:10.590, Speaker A: You should have some way of tuning it. Something I've been working on, and this is an extension of the differential privacy for CFMMs is. Well, actually, maybe you can formulate this as an optimization problem where I want to meet these three constraints, where I minimize how much state is revealed for a given level of efficiency. I also try to make sure the welfare satisfies this competitive ratio bound, and the sovereignty piece is some input the user can give to the optimization problem. This thing, let's go through what this says step by step. So we have a function which measures our notion of efficiency. And this takes in the state that is revealed, as well as the sum hyperparameters theta, which you could think of as the user input.
00:14:10.590 - 00:15:00.322, Speaker A: You could also think of that as constraints on the output that the protocol gives. These constraints can also be resource prices. So these can be things that are how much am I willing to pay per unit of state that's revealed? Or in the te context, how much am I willing to pay if it costs me k times more to execute in tes? Maybe this data might be a budget. I'm only willing to spend a total amount of resource in this environment. X is a valid state of the protocol. So this could be your set of public balances, could be your set of reserves. Now here's where we get to ensuring that this welfare constraint holds, which is consider a notion of a projected state.
00:15:00.322 - 00:15:50.844, Speaker A: So this is your state that's fully private and your statistically private state. So you can imagine that there's some model in which you have a disclosure mechanism that projects some of your state to be fully private, some of it statistically private, and now measure the efficiency under those disclosure rules. So this is where a developer chooses p or sp. And finally, we have this function, c. So if you remember here, I said that the welfare of the private protocol needs to be at least c times the public protocol. And the goal of solving this optimization program is the user could say, hey, I want c to be 10%. I'm only willing, I will take as much privacy as you can give me, measured in this way.
00:15:50.844 - 00:17:01.134, Speaker A: But I don't want to be 10%. I don't want to be more than 10% worse than the public mechanism. The idea here is that trying to formulate this optimization problem allows you to take in user input while also allowing you to think through sort of some notion of this privacy versus efficiency trade off. And you can formalize this, but this talk won't talk about that. So a natural thing you might say as well, hey, look, how do we price these resources? This is sort of a thing, I think, in pretty much every blockchain that has real users that people are facing every day and thinking about, which is how much should blob space be in Solana? It's how much should state contention cost. And so there's sort of a natural thing of like, well, how do we even price these public first private resources? So a natural thing you could do is again, formulate this optimization problem subject to some constraints. So these constraints in this case might say, hey, look, I'm only willing to spend 20% more than the purely public gas cost.
00:17:01.134 - 00:17:44.298, Speaker A: That's what the first constraint says. The second constraint says, I'm only willing to lose up to 20% efficiency. And then the third constraint is sort of like a security budget. Like, I want at least k bits of privacy. And how exactly you measure that depends on how you define P and SP, but just as an illustration. So I think if you have a formulation like this, you might say, hey, well, who's solving the optimization problem? If the user has to solve it, we're dead on arrival. But if you look at the market right now, especially when you look at the off chain actors and intent solving type of things, you could imagine that the solvers are actually solving this optimization problem on behalf of the users.
00:17:44.298 - 00:18:14.420, Speaker A: So the users submit c. The users you could think of as submitting constraints, they submit the 0.8 and 1.2 and k here. Then the solvers try to return an optimal matching. And this would at least allow a way in which users don't really have to think about this optimization problem, but they're able to get some guarantees on the privacy versus utility trade off they make. That's it.
00:18:14.420 - 00:18:16.280, Speaker A: And I'm shilling my paper again.
00:18:23.150 - 00:18:24.450, Speaker B: Any questions?
00:18:29.750 - 00:18:56.620, Speaker C: So you talked about these three general categories of privacy, levels of data, and statistical privacy is a sort of variable category. And wanting to build software that uses these and presumably moves data between these explicitly, do you think that techniques from programming language based security, specifically information flow based techniques, can be useful here? Or is this not amenable to that?
00:18:56.780 - 00:19:37.456, Speaker A: Yeah, for sure. So if you think about this, like grading type of thing, of like, hey, on the vertical lines we're composable. On the horizontal lines we're like mossy. You can basically try to take information flow type arguments and add in some lossiness factor. When I go from private state to differentially private and then I compose, do I have an information flow guarantee that bounds, that's what this is trying to get at, is that information flow is focused on purely private, purely public, and that boundary, well, usually you can have.
00:19:37.488 - 00:19:51.152, Speaker C: Multiple levels of different security and things can move to strictly more restrictive levels. Public data is allowed to influence private data and you can have various label structures as opposed to just two.
00:19:51.256 - 00:20:57.680, Speaker A: Yeah, that's true. I guess my main point is if you look at, if I think about these kind of what composability guarantees you have in differential privacy, you would need some way of picking which layers, how tolerable each layer is as you move between them. I think that's the type of thing that people have done, especially in mobile oses, where there is actually some actual real usage of differential privacy. In the Google Android keyboard, they aggregate a lot of information and that's how the recommendations are done. That's an example of differentially private, um, use case that has like a few hundred million users. And, uh, that, you know, in, in that case, they just spent many years tuning this, and there's like a million papers you'll find of like, how do you choose your epsilons? And I think that's the devil is definitely in the details of that. But I think the end user should dream of just a compiler where they, they don't have to think about that, right? Like, they can, they don't have to think about those transitions.
00:21:02.900 - 00:21:24.160, Speaker D: I just have a short comment. I guess it's related to Isaac. Yeah, I'm happy that I'm not the only person who got reminded of information flow. So I actually forwarded you a paper called viaduct, and I thought you might be interested. It's like exactly what Isaac is talking about. It has an information flow. You can like declassify certain information.
00:21:24.160 - 00:21:50.072, Speaker D: So it allows some kind of release, but there's a type system that makes sure your privacy and integrity labels are internally consistent. We actually have a compiler. The vision is you can just write the ideal functionality and then the compiler will synthesize the cryptographic implementation for you. So it's a little bit like what you were saying towards the end.
00:21:50.256 - 00:21:51.640, Speaker A: Well then, yeah, great.
00:21:51.720 - 00:21:53.452, Speaker D: I thought you might be interested. Thank you.
00:21:53.496 - 00:21:54.120, Speaker A: Thanks.
00:21:55.020 - 00:21:56.160, Speaker D: I saw one more.
00:22:04.340 - 00:22:33.240, Speaker B: Since. Thank you for the talk, by the way, but since you're like, the state is 100% composable at different layers of privacy. So like, fully private is fully composable, fully public, fully composable. Does overall system welfare actually increase if you remove the user's ability to specify what level of privacy they want? For example, if you're running an auction in a trusted execution environment, the auction is more efficient if everything is in the enclave versus some being inside and some being outside.
00:22:34.260 - 00:23:35.136, Speaker A: Yeah, that's a good question. So a lot of that is in this kind of framing of this optimization problem. A lot of that is hidden in the definition of EFF and c where it's like, well, I have some notion of efficiency per unit of state that could be hidden in this. Is it just purely the read and write of the cell type of efficiency? Probably more like what Elaine was talking about earlier? Or is it the execution efficiency? In the case of a CFMM, you have this huge blow up in complexity for evaluating how to execute the trade. If you add per usent noise, that's where the devil's in the details. But I think the nice thing about Amms is they're very simple, so you can write out this optimization problem for them and get a closed form solution. I think for other programs, like for lending protocols and stuff, you have to be much more careful about that delineation because, yeah, the security properties of those systems will change quite a bit.
00:23:35.136 - 00:23:53.364, Speaker A: Whereas amms, it's like, okay, I got a worse price, but I didn't lose, I don't immediately like lose all my money in the same way as a liquidation. So there's kind of something you have to be a little bit careful of. But yeah, definitely there is. You know, this is a stylized view for each kind of program. You need to think about an optimization problem like this.
00:23:53.432 - 00:23:54.760, Speaker B: Got it, got it. Thank you.
00:23:57.340 - 00:24:27.250, Speaker E: Hi, thank you for the talk. In hardware security, there's this concept called micro architectural leakage descriptors. So you take a hardware performance optimization, and then you can essentially create a model of what information is leaked given a given optimization. So it's like a formal way of categorizing information leakage through side channels. I'm wondering, is there an equivalent formal way of describing the efficiency versus privacy trade off in terms of a given way of doing a market maker?
00:24:27.590 - 00:25:23.220, Speaker A: Yeah. So for the amms, there's definitely between these two papers, I think they explicitly write out this sort of information loss in terms of number of bits of the price that you can recover per number of bits of knowledge, or, sorry, number of bits of the trade that you can recover given a certain number of bits of the prices you see. So you can think of as a transfer function of like, I have this much entropy input, I have this much entropy output. So it looks like these bottleneck type things, except I think you're not really explicitly trying to minimize those because that'll be sort of NP hard and you kind of minimize some surrogate. And because all these cfms are convex, the surrogates, the convex surrogate has the same answer as the NPDEH, which is like why we kind of can get these results for more generic functions. It's harder. You will run into your natural.
00:25:23.220 - 00:25:46.840, Speaker A: Can I solve an approximation algorithm? Because there's no way to solve the exact optimization minimum. But yeah, there's probably a generic way to write it in terms of bottleneck type things of mutual information before and after of the transfer function. But yeah, minimizing it is usually too hard. So people end up finding some surrogate proxy, they minimize.
