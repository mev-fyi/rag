00:00:00.970 - 00:01:05.060, Speaker A: Great. All right, so I'll give a short introduction of you, Justin, before we get started. So, Justin is a researcher at the Ethereum Foundation. We had interacted for a while already before, but we met quite recently in Palo Alto in the end of January. And Justin then gave a great presentation on mev and cryptography together and outlined a lot of amazing topics. And so I thought it would be really cool to, one, record it because it was not recorded, and two, to have that presentation with all of you at the Theorem Foundation. Justin has been there for the last three or four years, if I'm not mistaken, Justin, and has worked on a wide variety of topics, including applied cryptography, which is why it's awesome to get his insight, because he's also been working in the last year, arguably since he's joined the ATM Foundation on mev as well.
00:01:05.060 - 00:01:18.020, Speaker A: And so having those two things together is super interesting and I think to us is very relevant. Justin, is there anything you'd like to add before you get started?
00:01:18.630 - 00:01:21.300, Speaker B: No, this is great. All right.
00:01:23.530 - 00:01:31.980, Speaker A: Awesome. People will trigger in a little bit more, but I think we should get started because we only have an hour and yeah.
00:01:35.150 - 00:01:58.490, Speaker B: Okay. Fantastic. Thank you, Alex. So I bought this whiteboard today. I'm not sure how well it's going to work, but let's see. So basically the talk that I gave a few weeks ago was split into two parts. One is looking at the intersection of mev and cryptography at the consensus layer.
00:01:58.490 - 00:02:56.818, Speaker B: And this is kind of the layer that I focus the most in the context of Ethereum. And then kind of the second part is kind of this thesis that I have that at the application layer, we're going to see encryption as playing a very important role. Like in consensus, we don't really have encryption. We have hashes, we have signatures, we have zero knowledge proofs, all sorts of stuff, but no encryption. And so it's kind of interesting because in a way, cryptography was started with encryption. And so we're kind of going back to its roots now. One of the reasons why I want to talk about consensus is because one of the things that's happened to me is that I've tried to rephrase everything in the language of mev.
00:02:56.818 - 00:03:51.320, Speaker B: Because I think that's this new kind of hidden force, which is now no longer so hidden, which is really affecting the way we think within the ethereum foundation. And so here I've listed kind of five pieces of cryptography that interact with mev at the consensus layer. So we have Ssel. So that stands for single secret Leader Election. And the idea here is that when we sample a proposer in the future, we want only that proposer to know that they're a proposer. We don't want the whole world to know. So that's where the secret comes in.
00:03:51.320 - 00:04:56.442, Speaker B: And this is important because if you're a home validator and you don't have a really sophisticated networking setup, then it's actually fairly easy, if not extremely easy, to dead automize your IP address. Meaning that we can map validator identities which are validator pub keys to your IP address. We know which Validators will be publishing blocks in the future and then we can just go DDoS them. And the reason why this interacts with mev is because if you have two slots, so you have slot one, which is 12 seconds in the Beacon chain, and then you have slot two, which is also 12 seconds. And they're next to each other. And the attacker controls this slot. Slot two, then.
00:04:56.442 - 00:05:52.990, Speaker B: There's actually a very easy way for the attacker to effectively double their Mev. How did they do it? They can DDoS the proposer in stop one. So this is going to be an empty slot. And so at the end of the day, they're going to end up with a slot which is twice the length, which is 24 seconds. And EIP 1559 kind of gives you this dynamic block size, which means that mev in these two blocks, in these two slots should be able to fit in this slot here. And actually you can extend this attack to three slots. So if there's a slot zero here, which is on the left of slot one, then you can just DDoS that and now you have three slots worth of mev.
00:05:52.990 - 00:07:17.640, Speaker B: And so previously the way that we used to think about the security of blockchains is in the context of purely honest or purely malicious actors. And we would try and reason and say, okay, well, the beacon chain has sufficient liveness and sufficient security if let's say less than half of the Validators are malicious. But here we have this more subtle thing going on where we need to take into account rational Validators. And even though they're not kind of outright breaking the safety and liveness of the beacon chain, they're doing bad things here because they're disrupting the beacon chain and also kind of griefing other Validators. Now one interesting thing to note is that the single, the very first S is also important in the context of mev. And the reason is that let's imagine that there's so can I erase this? Was it clear? Can you guys see?
00:07:18.730 - 00:07:22.010, Speaker A: Yes, can see and it's clear. I have a question for you though.
00:07:22.160 - 00:07:22.778, Speaker B: Go ahead.
00:07:22.864 - 00:07:27.606, Speaker A: Would you prefer questions at the end of the presentation or throughout?
00:07:27.798 - 00:07:29.340, Speaker B: No, throughout is perfect.
00:07:29.710 - 00:07:36.110, Speaker A: Okay, great. And are you able to see when we raise hands on Google meet or would you rather we just shout?
00:07:38.290 - 00:07:41.850, Speaker B: You can shout or raise hand, whatever you prefer.
00:07:42.010 - 00:07:49.570, Speaker A: Okay, cool. Please also feel free to not answer a question to make sure you get to the end of your presentation.
00:07:53.670 - 00:08:41.330, Speaker B: Okay, so I'll go ahead and erase that. So the single being relevant in the context of mev is something we discovered only very recently, but basically you have your slot here. And let's say that there could be some sort of fallback mechanism like some blockchains like DEFINITY for example, or Algorand. You can have multiple proposers. And the idea is that if kind of proposer kind of number one doesn't show up, then there's someone else who's invited and there's a precedence list. So one validator might have precedence over another validator. Now let's say that an attacker controls two proposals.
00:08:41.330 - 00:09:36.066, Speaker B: What they can do is they can submit two proposals but they're not going to submit them at the same time. They're going to submit the first one kind of let's say halfway through the slot when they're supposed to do and then they're going to submit the second one which will take precedence over the first one a little bit later. Now there's kind of two scenarios. Scenario number one is that this block was published so late that most attestors don't get to see it. And what that means is that the block kind of gets discarded. So there's no change here. But there is a small probability, a non zero probability that this block will actually be seen in time by sufficiently many validators attesters and that this proposal will not make it on chain.
00:09:36.066 - 00:10:37.510, Speaker B: And so what we have basically is in that scenario the attacker is able to get a little bit more of mev because they've conducted what we call a time buying attack. They've managed to buy let's say 2 seconds worth of time. And so during these 2 seconds they've been able to receive more transactions and therefore create a block with more mev. And so it's kind of lucky that we chose the single leader per slot for the beacon chain and in hindsight other blockchains kind of got that wrong. Okay, great. So the other place where cryptography comes in and it intersects with Meg is VDFS. So just a real quick recap.
00:10:37.510 - 00:11:20.146, Speaker B: We have basically a commit reveal scheme for randomness. If you all know this please tell me and then I can skip. But basically you have 32 slots. These slots form an epoch and during each epoch the proposer is invited to reveal a secret that they've committed to. And then you take all these secrets that are revealed, s one up to S 32. Some of them might be missing because they didn't reveal, they didn't create a block. And then you just hash all of these secrets and that's going to generate randomness for future epochs.
00:11:20.146 - 00:12:51.060, Speaker B: They're going to generate randomness R for future epochs. And the problem here is that if you're the very last proposer here, then you actually have the option do I reveal S 32 or do I not reveal S 32? And that's going to influence the future. And so one of the things that we do here basically is that we introduce VDFS which gives us a time delay so that the value R is only known way after the decision to reveal on reveal S 32 is based. So if you wanted to bias the randomness, you can't meaningfully do that because any action that you take here, you will only know its consequences well after your slot where you can take the action. And so basically that removes the bias. But right now the Beacon Chain doesn't have VDFS and it doesn't have Ssle. And so what the attacker can do is bias the randomness in such a way that they have more slots in the future.
00:12:51.060 - 00:13:47.480, Speaker B: So it's possible, for example, that if they were to reveal, they get zero slots in the next epoch, but if they don't reveal, they get two. And so that's to their advantage. They may also want to try and bias the randomness so that they get more profitable slots. So for example, they might know that at some point in the future there's kind of an NFT auction starting at a very specific slot. And so they might want to bias the randomness in such a way that they will be the proposer for that slot. Or as you can see here, the last few slots in an epoch are more valuable than the first few slots. And so they might bias the randomness in order to get more final slots, in order to bias the randomness even more.
00:13:47.480 - 00:14:54.566, Speaker B: It's kind of interesting where there's kind of an interplay here between VDFS, between these two attacks, where an attacker could DDoS the last few proposes if he's happy with the randomness so far. Okay, great. So basically what we're trying to do here is harden the protocol in such a way that we have these well defined and segregated modules where all the incentives are kind of very well understood. And so here we have this randomness module. It's super hard and there's no weird dependency with other parts of the protocol once we have the VDFS. So we've addressed Dos based mev, randomness bias based mev. Another form of mev that we have on the Beacon Chain is reorg based mev.
00:14:54.566 - 00:15:57.120, Speaker B: And so what we've done here is basically use BLS signatures which allow us to have many, many votes in parallel. So if you look at Bitcoin, for example, it's an extremely slow kind of voting process. You have one vote every ten minutes. And in order to get a reasonable notion of finality, people say that you have to wait the six blocks, basically six confirmations. And with BLS, the idea here is that let's try and get as many validators to vote in parallel as opposed to voting sequentially. But the problem here is that if you get many, many validators voting in parallel, that's many, many signatures that you want to try and verify. And just to give you an order of magnitude, I believe that ethereum right now is verifying something like 800 signatures every second.
00:15:57.120 - 00:16:50.714, Speaker B: What BLS gives us is this idea that we can take all these signatures, aggregate them and only verify a tiny one. And so what that means is that we have almost no reorgs. And the reason is that we have this extremely fast confirmation mechanism and we actually want to push that to the next level. Something that we call single slot finality. The idea of single slot finality is that, as the name suggests, for every single slot, you get finality. And so there's no possible reorgs even of depth one. And so, again, this kind of simplifies the problem of mev, right? Because now you've removed, for example, multi block mev.
00:16:50.714 - 00:17:18.140, Speaker B: And so we're in a position where we can analyze things on a slot by slot basis. The challenge here is that we need to potentially be aggregating hundreds of thousands of signatures in a single slot. And so there's kind of engineering challenges to just doing all of that work off chain in a very small amount of.
00:17:21.070 - 00:17:31.950, Speaker C: Question. Justin, can you elaborate on I don't know if this gets too technical, but why is it that signature aggregation is possible with VLS signature as opposed to the current scheme?
00:17:33.730 - 00:18:36.610, Speaker B: Right, so the reason is basically that we're working with elliptic curves with pairings, and the pairing is an additional algebraic structure, which means that we can construct signatures which have more algebraic structure to them. And the way that it works, actually the aggregation is super simple. It's just point addition. So if you have two signatures, the way that you aggregate them is by adding them. And if you have two pub keys, the way that you get the aggregated pub key is by adding them. If you take, for example, two Schnar signatures or whatever, you don't have this nice additive property because you're doing all sorts of other things. You're taking hashes, you're getting randomness, it's an interactive protocol, blah, blah, blah.
00:18:36.610 - 00:18:53.560, Speaker B: Whereas with BLS signatures, you're just taking these points and doing almost nothing to them. And so you can have this additive property to them.
00:18:54.090 - 00:18:55.480, Speaker C: Amazing. Thanks.
00:18:59.870 - 00:19:44.680, Speaker B: Okay, great. So we've addressed Dos based mev that should completely go away. With SLE, randomness bias should completely go away. Real based mev should completely go away. And then there's this other thing, cross domain mev, that might be a little bit of a stretch, but I'm basically arguing here that all the scalability effort that we're doing on Ethereum could be seen as a way to reduce cross domain mev. And the reason is that we want to try and build a domain which is large enough for the whole world. Back of the envelope calculation is we will get to a point where we're doing 10 million transactions a second, and that's enough for the whole world.
00:19:44.680 - 00:20:40.950, Speaker B: And part of the reason why we'll get there is right now, let's say we're doing ten transactions a second. Roll Ups will give us 100 x, and then Sharding will give us another 100 x. And then if you look at basically the computational bottlenecks in scaling, there's several of them. There's things like Compute storage disk, I O bandwidth. And it turns out that the only fundamental limit to scaling blockchains is bandwidth. Everything else, we can have very fancy cryptographic magic to solve it. For example, Compute, we can have a snark where the validators, they only need to verify the snark.
00:20:40.950 - 00:21:31.980, Speaker B: And so Compute doesn't really become a bottleneck for the chain. And then for state and for disk I O you can do things like Statelessness, where again, the validators they don't have to store anything and they don't have to do any disk I O because whenever they read and write to state they're given the merkel path on the wire from the Internet, and so they don't need to go fetch that from disk. And it's actually cheaper to go ahead and do that. And so here I've added data availability, sampling and proof of custody, which involved semi fancy cryptography. But wait.
00:21:34.670 - 00:21:55.410, Speaker C: Why are you saying that bandwidth how you can with cryptography, sort of reduce your requirements on State and Compute, but why are you saying that you cannot do the same for bandwidth eventually with some new cryptography or whatever? Why is it fundamental?
00:22:00.310 - 00:22:59.398, Speaker B: Good question. So, I mean, I don't have like an impossibility proof, but kind of in, like, intuitively, like, you know, we need to come to consensus on data. And the data can be just like random noise, very high entropy. There's no kind of cryptographic compression that you can do. And if you try and do data availability off chain, well, suddenly you lose the shared security with the rest of the system. And so in order to have shared security data availability, you really need the validators to go ahead and actually download the data. There's just no way around it.
00:22:59.398 - 00:23:25.920, Speaker B: Like, someone, some subset of validators needs to go download the data. And one trick that you can do is you can say, oh, well, we don't need everyone to go download all the data. Instead, we can just have committees download the data. But that's shouting. And so that's kind of the limit of the state of the art is like, we ask people, a small subset of validators to go actually download the data.
00:23:33.430 - 00:23:35.234, Speaker A: Do you have a follow up question? Can I ask?
00:23:35.272 - 00:23:37.140, Speaker C: No, that makes sense. Yeah, go ahead.
00:23:38.230 - 00:23:56.806, Speaker A: Why does this solve cross domain MEV? I didn't follow. Is it just because the domain is big enough that there's no cross domain? Okay. All right, thank you.
00:23:56.828 - 00:24:25.594, Speaker B: I mean, there will be other forms of domains within ethereum potentially exactly like roll apps and whatnot, but you've mitigated the problem in the sense that you removed a little bit of friction of having to deal cross chain, for example, and having to deal with different security domains, having to deal with different block times and all sorts of complications. Got it.
00:24:25.652 - 00:24:26.420, Speaker A: Thank you.
00:24:29.270 - 00:25:28.434, Speaker B: And then kind of a final piece of crypto is the commit reveal, which we're using in the context of PBS. And I guess you guys are all very familiar with this and it's interesting because it kind of ties in a little bit with what we're trying to do here, right? So the idea of using encryption here is that you basically prevent front running and mev stealing. And this is what PBS is all about. It's about preventing med stealing. But the problem is that here we want to try and work at the very low granularity, we want to be working at the transaction granularity or the bundle granularity. And it turns out that commit reveal doesn't really work very well at low granularity. It has to be high granularity.
00:25:28.434 - 00:26:54.530, Speaker B: And the reason is that you're doing a couple of things. One is that you need to have some sort of incentive mechanism, some sort of collateral to punish whenever someone doesn't reveal. And this becomes especially problematic when you have these kind of free options that could be worth much more than the collateral or options that are worth more than the collateral because some people reveal and then your decision to reveal is based on what other people do. But then the other big problem with PBS is that you need some sort of data availability oracle, you need a way for the chain to know whether or not the information has been revealed. And this goes back to the BLS thing where the only way that we know how to do data Availability Oracles is to ask a committee to go ahead, try and download the data and vote on the fact as to whether or not this data is available. And the BLS aggregation only works if everyone is voting for the same thing. If it's a different signing of a different message, then suddenly the BLS aggregation is broken.
00:26:54.530 - 00:28:34.530, Speaker B: And so basically the data availability Oracle is very low bandwidth and you can't go vote for high granularity things like transactions. So if you want to prevent kind of front running and mev stealing, then you need some form of encryption at the application layer. And as I mentioned, kind of my thesis is that we're going to move to a world with more and more encryption and these encrypted mempools. And kind of one of the reason is that from a user standpoint, you want to know that your transaction has been published, has been gossiped, so that you have high guarantees that it will be included in the next block if it pays sufficient fees. If you go ahead and give your transaction to a trusted third party who says, okay, I'm going to try and include your transaction in one of your blocks while you don't have this kind of this nice UX guarantee of fast on chain inclusion. Another problem of having to trust a third party is that they may front run you. There's a trust assumption here and so that's not ideal.
00:28:34.530 - 00:29:25.490, Speaker B: And then kind of a third problem is that if you're sending your transactions to a trusted third party, well, you're potentially creating centralization. And the reason is that if you have one party who has this privileged access to lots and lots of all the flow, such as Flashbots potentially, then they could potentially win every single PBS auction. And so you're in a position where you have a very centralized builder because they have this privileged order flow, and they win every single auction. So really, just for the health of the ecosystem, we also want to push for encryption.
00:29:27.270 - 00:30:03.470, Speaker C: Sorry, one question there. Like, perhaps stepping back a little. If I understood correctly, you're saying you cannot do encryption at the transaction level, but you would have enough bandwidth to do encryption at the block level, so you could presumably do some commit reveal at the block level. Right. So question is, like, why is that not enough? Is that like, a UX thing that you don't want to if you say you encrypted everything and it will be like one block reveal, is that a UX thing that you want to have people wait for one block until they know what happened, or is it something that I'm missing?
00:30:08.050 - 00:30:41.770, Speaker B: So the problem that we're trying to solve here is just the whole process of building a block or sequencing the transaction. And my claim is that we can't use commit reveal for individual transaction. And the reason is because otherwise you need to have kind of a data availability oracle. You need to know very high granularity which transactions that were committed have actually been revealed.
00:30:43.470 - 00:30:49.660, Speaker C: Sorry, I get that. My question is, why would commit reveal at a block level not be enough?
00:30:50.370 - 00:30:58.880, Speaker B: Oh, it is enough. It is commit reveal at the block level. That's all PBS is doing. There's no encryption here.
00:31:02.870 - 00:31:11.346, Speaker C: So why cannot you encrypt then? I'm not getting okay, so basically the.
00:31:11.368 - 00:32:09.906, Speaker B: Problem is, before you have the block at the mempool, from the user standpoint, I have my ledger wallet or my metawas wallet. I create a transaction. What do I do with it? What we're doing today is we just broadcast it publicly, and you get sandwiched. And so what you want to do is you don't want to get sandwiched. And so option one is to give your transaction to Flashblot Protect, but then Flashblot has they can rug you because they can't front run you potentially, because you have to trust them. There's a problem of centralization because now Flashbot Protect might be in a position where they win every single auction. And then the third problem is that it's possible that the next block will be created by the Eden Network, and it won't include Flashbot transactions.
00:32:09.906 - 00:32:15.906, Speaker B: And so if you had published your transaction publicly, then you have better guarantees about on chain encryption.
00:32:16.018 - 00:32:17.960, Speaker C: Yeah, no, I get it now. Thanks.
00:32:18.350 - 00:32:49.860, Speaker A: Justin, I have a question on your concern around order flow. To me, it feels like if you have encryption of transactions, the order flow problem is still very much there. It would just be at the source. So say I'm a wallet. Say there's like encryption in the whole network, but I'm a wallet, and for every transaction that users submit, I'm going to add my little back running transaction after it and submit that as an encrypted payload to the network. So there's still order flow there, right?
00:32:52.070 - 00:33:40.530, Speaker B: Right. So basically what I'm assuming is that the process of creating a transaction will be done locally on your computer without necessarily needing a service provider. So if you take, for example, I don't uniswap, you sign uniswap transaction. Uniswap has no knowledge of what transaction you're creating because you've downloaded all the information required to locally make a decision as to what your transaction will be. But you're right. If you go through an exchange, for example, the exchange can gossip an encrypted transaction on your behalf, but they also know what the plain text is because they actually crafted the transaction on your behalf.
00:33:42.630 - 00:33:57.946, Speaker A: What if more than that, there's, like, an incentive where uniswap tells users, hey, if you can submit your transaction without anything, or you can click an opt into a system, or you'll pay a lower fee because we'll background. You if you allow us to do.
00:33:57.968 - 00:33:58.540, Speaker B: That.
00:34:00.350 - 00:34:07.050, Speaker A: Themselves are such that as economically rational actor you might want to do something with your order flow.
00:34:08.910 - 00:35:10.750, Speaker B: Right? So I guess a couple of thoughts here. The first one is if you use a service that kind of refunds you, the mev that are actually not too bad, right, because you haven't leaked so much value. And then the other thought is that I think we're going to see tools that empower individuals, that make them sovereign individuals, and where the tool will basically try and create transactions that are close to optimal. And so one possible example is you could have a browser based aggregator like one Inch or Matcha. It takes the top three DEXes, uniswap, SushiSwap, and whatever curve. And then based on the liquidity there, it just does a small calculation in your browser and then creates a transaction. So you don't really need to use a service provider.
00:35:10.750 - 00:35:13.150, Speaker B: Um.
00:35:16.150 - 00:35:17.060, Speaker A: Thank you.
00:35:21.030 - 00:36:12.900, Speaker B: Okay, great. So let's assume that this thesis is correct and we want encryption. Well, one of the things we want, which Clement Reveal doesn't provide, is this idea of guaranteed decryption or guaranteed output delivery. And there's basically three flavors that I know of of encryption with guaranteed decryption. One is threshold decryption, where basically a committee can force decryption of a ciphertext. Delay decryption. I mean, it is actually called delay encryption, but it might make more sense to call it delay decryption because the act of decryption can only happen once delay has happened.
00:36:12.900 - 00:37:49.140, Speaker B: And then there's something so this is by delay, I mean sequential computation, very similar to VDF. And then there's witness encryption, or maybe better called witness decryption, where in order to decrypt a message, you need to provide a witness basically proving that some statement is true. And this could be any statement you can think of it as basically it will decrypt a message if you give it some snark basically a short proof that some statement is valid. And so one of the things you can do for example with witness decryption is that you can build a hybrid of threshold decryption and delay encryption. So one of the problems with threshold decryption is that you kind of need to trust the committee to not decrypt too early. And so what if instead we kind of had an N out of N committee where the only way where you could have this early decryption is where every single committee member is dishonest. If you have even one single honest committee member then you can't have this early decryption and this mev front running.
00:37:49.140 - 00:38:58.966, Speaker B: But the problem with this kind of N out of N committee is that you have very bad liveness right you just need one single committee member to be offline and you get no decryption whatsoever. And so the statement that you could use in the witness decryption could be kind of an all statement. Either the committee kind of all came together and agreed on the decryption or kind of as a fallback mechanism. If at least one of them is offline, then you have the delay. So the problem with the delay is that it introduces latency, it might introduce a few seconds of latency. And so basically you kind of have this whole peacetime wartime mechanism where in the default scenario you kind of have this optimal user experience where you kind of get instant confirmations of things and you have a very weak trust assumption. You have what's called an honest minority assumption.
00:38:58.966 - 00:39:30.750, Speaker B: You just need to trust if one entity in your committee is honest and then if for some reason one of them is offline then you have a fallback mechanism and you have to wait a few seconds. This is a little bit reminiscent to the Arbitrum design that was released, I think, yesterday. For roll ups. They have this validium or they fall back to a roll up when the committee loses liveness.
00:39:32.130 - 00:39:49.498, Speaker C: But can you call like arbitrary functions in the witness? Could you say like n out of n but less like -1 something so that after n seconds it's automatically with one validatory suffices Something like that.
00:39:49.664 - 00:40:23.490, Speaker B: Yeah. You can have arbitrary statements. And one of the use cases, for example, of witness encryption is you could have two blockchains. So in the context of cross domain mev you might want a message to be decrypted. If two blockchains both have both finalized and then you do some sort of atomic swap or whatever and the statement could be blockchain A has finalized and blockchain B has finalized.
00:40:25.130 - 00:40:55.038, Speaker C: I'm curious how the delay conditions enter the witness. So my understanding was witness is like you have to solve an. MP complete problem. Right. So you encode it. You encrypt to one instance of the problem. So how does the time play a part? I can see, like, the threshold, how it could relate to that because you could just distribute the statement in bits and pieces in all the committee members.
00:40:55.038 - 00:41:01.566, Speaker C: But how does the delay get encoded in a witness? Does that make sense?
00:41:01.748 - 00:41:50.698, Speaker B: Richard right. So what's going to happen in practice for witness encryption is that it's not going to be verifying an arbitrary statement. It's actually going to be verifying a snock for that statement. You're going to compress the statements as much as possible. And so in the context of threshold encryption, what you could do, for example, the statement could be, I know N out of N or K out of N signatures, and I'm going to make a Snark out of that, and that's going to be able to decrypt my thing. Now, for delay encryption, what you can do is you can use a VDF. So you do the sequential computation.
00:41:50.698 - 00:42:09.362, Speaker B: That's a very expensive thing. And then at the end of your computation, you have a short proof. And then you take that short proof and you wrap it into a snark and you make it even shorter. And then you feed it to your ciphertext and it will automatically decrypt.
00:42:09.506 - 00:42:11.720, Speaker C: I see. Thanks.
00:42:13.610 - 00:43:19.398, Speaker B: And then if you want your statement to be like and alls and complicated things, then you do the same thing. You just have a Snark, which encapsulates this more complicated statement. And someone wrote an E Three research post recently where they used witness encryption to build a trustless two way bridge between bitcoin and ethereum. We used to think that you need IO obfuscation, which is a very fancy primitive. Witness encryption is still very fancy, but, like, nowhere near as fancy as obfuscation. And there is actually an implementation of witness encryption with potentially very large performance penalties and also potentially shaky assumptions, like cryptographic assumptions that may be broken. But my best guess is that in 1020 years, we will all be using witness encryption.
00:43:19.398 - 00:43:31.520, Speaker B: And you can kind of think of it as the pinnacle of guaranteed decryption. Like any form of guaranteed decryption can be implemented with this encryption. These are special cases of this.
00:43:33.670 - 00:43:34.660, Speaker C: Makes sense.
00:43:37.590 - 00:44:25.170, Speaker B: Okay, great. So we have this idea where we want to somehow encrypt the transactions as a user. These encrypted transactions, they just get packed in a block and you don't have front running because you don't know what the content of the transactions is. And then you have some sort of process of automatically decrypting the content and then executing it, the transactions on the blockchain. But there's like a bunch of complications that kind of arise from this high level idea. And one problem is, what about all the metadata? The metadata could leak information. That's one problem.
00:44:25.170 - 00:45:26.850, Speaker B: And another problem is that the metadata could be useful for things like antispam where you kind of need the metadata in order to protect yourself against spam. So let's look at four pieces of metadata that I've highlighted here. Like the first one is the IP address of the sender. And so here I'm kind of assuming that people are using services like Tor that will shield their IP address. Another really important piece of metadata is the sender and the nonce. And so the reason why we need this is because at the peer to peer network level, we need to know whether or not the transaction is valid. And in particular, we need to know whether the sender has enough balance to cover the gas costs.
00:45:26.850 - 00:46:44.142, Speaker B: And so the naive solution is saying, oh, well, we just reveal the sender and then don't send the clear. But hold on, the sender is like a really critical piece of information that you don't want to reveal. And so it turns out there's a very easy trick which is that you use ZK SNOX, you use zero knowledge. And basically what you do is you prove in zero knowledge that the sender, which is encrypted and the non which is encrypted, has enough balance. And the way that you do that is you refer to a state route in the past and you provide a merkel path from the sender's balance to the route and you prove that the balance is at least some minimum in order to even start interpreting the transaction. Let's say 21, enough to pay for 21,000 gas. But that doesn't quite solve the problem.
00:46:44.142 - 00:48:12.650, Speaker B: And the reason is that what if you create many transactions for the exact same sender and nonce? So what you need to do is you need to provide in the clear as a public input, kind of the hash of the sender, the nonce and the private key. Now, the reason why you introduce the private key is because it's something that only the sender knows. And the reason why you include the sender and the nonce sender address and the nonce is so that if you were to create two transactions with the exact same nonce and the exact same sender, then that piece of public information, some sort of hash, would collide. And so you'd know that you're under a spam attack and you can just drop the connection to whoever's spamming you. Okay, great. Another kind of interesting piece of metadata is the gas limit. So, kind of naively, you'd think that the gas limit needs to be in the clear, but there is actually a small change to the EVM, which means that we can just simply encrypt the gas limit.
00:48:12.650 - 00:49:29.410, Speaker B: And the reason is that one thing that you need to guarantee is that if a transaction gets included on chain, the sender has to pay something, right? It has to pay the minimum, let's say, to just deserialize the transaction, check that there's enough balance and just check the signature and do like the routine basics so that will cost 21,000 gas. And so you want to have a rule which says the Sender always has to pay this 21,000 gas. And then there's kind of the normal execution of the transaction where you look at the gas limit and then you say, okay, does the Sender have enough balance for the full gas limit? If yes, then run the transaction and then issue a refund at the end. If no, then immediately I bought. And that's a failed transaction. But it's not exactly a failed transaction in the sense that the Sender has lost the 21,000 gas. Okay, great.
00:49:29.410 - 00:50:10.880, Speaker B: And then there's this final piece of metadata, which we need to take care of, which was the size of the transaction. And one very easy thing that you can do here is just pad with zeros and then encrypt so that every transaction has the same size. Or maybe you could pad to the closest power of two so that you have very reasonable shielding here. One piece of research that I'd like to do, and I haven't done yet, is to look the distribution of the size of the transactions just to try and understand what kind of privacy do you get if you run the closest power of two?
00:50:12.470 - 00:50:17.250, Speaker C: And how about bandwidth there? Would you incur bandwidth cost by padding?
00:50:18.230 - 00:51:05.794, Speaker B: Okay, yeah, that's a great question. So the downside of padding is that you have to pay for more data availability, and data availability is expensive. Right? And so if you're rounding up to the closest power of two, then potentially your transaction is twice as expensive than you need it to be. Which brings me to three kind of more complicated problems that arise. Like these metadata stuff. They can be solved with just very basic cryptography and paying a little bit more. But there's kind of these more serious problems in a way that appear where we need heavier cryptography in order to solve them.
00:51:05.794 - 00:51:56.638, Speaker B: And this is where kind of the frontier lies from a research standpoint. So one of the things that you've identified is just the added cost of paying for front running protection. Ideally, we don't want to pay this cost. And so one of the things we want to do is what I call transaction clipping, where you just clip away the padding. And so there is a way to remove the padding and it basically involves fully homomorphic encryption. So you can have fully homomorphic encryption with all three flavors. So you can have threshold fully homomorphic encryption, delay fully homomorphic encryption, witness fully homomorphic encryption.
00:51:56.638 - 00:53:15.260, Speaker B: But the important thing is that you can operate on the ciphertext and you can do computation on these ciphertext. This is the functionality that could be simulated with trusted hardware like SGX. But ideally, you want to do it with pure math and with SGX. The idea here is that SGX receives encrypted transactions, it decrypts them kind of within its trusted enclave or whatever. Now that it has them in the clear, it can clip the padding and then pack as densely as it can, kind of a block with no padding and encrypt that and then have some sort of mechanism threshold delay or witness to force the decryption afterwards. So the way that you use fully homomorphic encryption is just to have basically a circuit which takes as input the padded and encrypted transactions and as output it removes the padding. That's basically all it does.
00:53:15.260 - 00:54:11.310, Speaker B: One of the things that's important is that the output is always fixed size. And the reason is that if it's not fixed size, then you can basically try multiple different inputs and depending on the size of the output, kind of try and guess the size of the transaction kind of reverse engineer. And so it needs to be fixed size. And the way that you achieve that is if all your transactions don't fit within your fixed size, then you just discard some of your transactions and if the fixed size is larger than all the transactions that you have, you just pad with zeros at the end. Okay, great. Any questions so far? Okay, great. So an even more extreme problem than transaction clipping is state diffing.
00:54:11.310 - 00:55:26.040, Speaker B: And this is something I'm very excited about in the context of ZK rollups. So one of the, in my opinion, huge advantages of ZK rollups over optimistic roll ups is that the data availability cost will be significantly lower on ZK rollups versus optimistic roll ups. And the reason is that on an optimistic roll up you need to include as input the transactions, all the transaction data needs to go on chain because that's required for the dispute process to happen with the fraud proofs. But with ZK rollups you have this trick up your sleeve where it suffices to put on chain the state diff. So let me give you an example. Let's say that we have 1000 people that are all donating funds to the Ukrainian donation address, right? And so they're all kind of incrementing the same address over and over again. And so option one is to have 10,000 increments of, let's say one if each.
00:55:26.040 - 00:56:36.122, Speaker B: Or you can have one single state diff which says for this whole block the balance has increased by 1000 e without having to detail all the intermediate kind of incremental steps. And so in this case, you kind of get like an extreme example, but you get kind of 1000 x compression of the data. And so it's hard to tell exactly how much cheaper it will be, but it might be 2345 x, maybe even ten x in some cases cheaper to use a ZK roll up versus an optimistic roller. And so the question then becomes how do we get this trustless front running protection with encryption and the state diffing? And the only answer that I know is basically to use fully homomorphic encryption. But here it's much, much worse than the transaction clipping because the transaction clipping is a very simple circuit. It just looks at zeros and then packs things. There's almost nothing happening.
00:56:36.122 - 00:57:37.710, Speaker B: It's a very shallow circuit. And with fully homomorphic encryption, you basically pay for the depth of the circuit, the circuit depth, basically the number of gates in the longest path. And here, in order to do the state diffing, you kind of need to run the EVM within in the context of polyhomomorphic encryption. Now, we've only just started doing ZKE EVM. Zkevm might have 1000 x overhead or 10,000 x overhead over the EVM. And fully homomorphic encryption kind of is even more expensive than ZK by, I don't know, maybe a couple of orders of magnitude. And so it will take maybe ten years in order for us to have fully homomorphic EVM.
00:57:37.710 - 00:58:21.790, Speaker B: But I think that is the end game. But in the meantime, maybe in a couple of years, two, three years, we might be able to do the transaction clipping, no problem. That's a much easier problem to solve. Okay, great. So fully homophobic encryption can do these two things. And another thing that it can do is basically the problem of bundle selection. So the way that I envision the end game of block building is that there's kind of two parts to every single block.
00:58:21.790 - 00:58:58.762, Speaker B: There's the priority transactions at the top and those are kind of the mev rich transactions like Arbitrage and whatnot. And those will consume a small portion of the gas, let's say 5%. And then there's going to be all the other transactions that don't really care. They're not competitive transactions, they're not competing with other transactions. Instead they just want to be on chain. And that will represent, let's say, 95% of the transactions. Now this 95% is the easy part.
00:58:58.762 - 01:00:06.990, Speaker B: And the reason is because you have this flow of encryption transactions, you can't distinguish them. It just looks like random noise coming to you. And so there's not much you can do as someone who wants to try and extract mev. And maybe the arbitrary model is going to be good enough in the sense that you just take first come, first serve and call it a day and you do the decryption. But unfortunately, first come, first serve doesn't work for the priority transactions. And the reason is that now you're introducing a latency game because there's this competitive aspect and so you're reintroducing centralization for this top 5% of gas usage. And so what I'm hoping we can do is come up with a way to do decentralized block building for these bundles.
01:00:06.990 - 01:00:56.362, Speaker B: And the brute force way, which I think we will be able to do in ten years or so, is to run fully homomorphic encryption for the EVM. But that's very expensive. But it turns out there's a shortcut which I think gives us most of the value. And the shortcut involves looking at access lists. So you can have a bundle which says, I'm willing to pay X amount of ETH for accessing this part of the state. Now, the access list is encrypted so no one can see them and no one can use that information to front run. And it's like perfectly granular.
01:00:56.362 - 01:01:46.254, Speaker B: It tells you exactly what it's reading and exactly what it's writing. And the job of the circuit that is going to select the bundles and pack them into a block is just to basically take the optimal bundle selection such that any two bundles have this joint access list. So you're never in a position where you have two bundles of writing to the exact piece of state. Now this is a little bit limiting in your ability to create optimal blocks. And the reason is that it's possible you could have two bundles that both write the same thing and they're both valid. But my intuition is that you can build blocks that are near optimal purely by looking at access lists. And actually that's a question I have for you guys.
01:01:46.254 - 01:02:00.914, Speaker B: Maybe an interesting research question is just looking back at all the historical data and seeing whether or not you can come up with a strategy which only looks at access lists and give us blocks that are optimal within 1% or something.
01:02:01.112 - 01:02:39.120, Speaker C: Yeah, I actually did some data analysis on that. So not coming up with a strategy, but yes, looking at clashes in a relay of different bundles and clashing like I look at clashing at different levels. Are they clashing at using the same target transaction included in the bundle or are they clashing more deeply in? I looked at the traces and saw where they were accessing and that was like an old analysis. It was a while back, but yeah, definitely. I agree with you. I would expect what you're saying that not much would be lost if we could do that. I was surprised that we were expecting more clashes intuitively than what we had.
01:02:40.130 - 01:02:51.490, Speaker B: Okay, that's really good news because the circuit for doing kind of set comparison and making sure that these are disjoint sets sounds very doable.
01:02:55.930 - 01:03:24.190, Speaker A: Sorry. Why is fully Mormorphic encryption so far away? What needs to happen during those ten years in your eyes? And can any of it be accelerated? I mean, we have a strong incentive for this to happen sooner. What needs to happen? I don't know if it's too technical or if we get into very detailed stuff.
01:03:24.260 - 01:04:28.718, Speaker B: No, there's like a high level answer, which is that we need a very similar kind of development to zero knowledge proofs. So 30 years ago, zero knowledge proofs were possible in theory, but if you wanted to create one, I don't know exactly if that's a true representation, but you need to spend on the order of all the energy in the universe in order to go create one because that's how inefficient it was. And then people just came up with these huge optimizations, like several orders of magnitude every single year. And now we're at the point where we've squeezed out a ton, and things have become practical, and we're starting to do hardware acceleration. That's the kind of optimizations we're looking at. And I think that fully homophobic encryption is the same as zero knowledge proofs, but just shifted by ten years. So we're just ten years later than zero knowledge proofs.
01:04:28.718 - 01:05:28.040, Speaker B: And there's this kind of Moore's Law for fully homophobic encryption where, roughly speaking, fully homophobic encryption is being sped up by ten X every single year, which is kind of insane. This has slowed down in the last year or two, but this trend has held through for a long time. One of the things that DAPA is doing is it spent something like I forget exactly, but something like $15 million to actually go build an ASIC to accelerate fully homomorphic encryption so that it can be practical. So I think the hardware, which will give us two or three orders of magnitude acceleration plus a few more kind of theoretical kind of mathematical tricks that have been happening for the last decade or so, and then we'll be in a good position to use it.
01:05:29.450 - 01:05:33.800, Speaker C: Why does Tarpa want to accelerate this? To backdoor it?
01:05:35.770 - 01:06:05.346, Speaker B: I think they want to be able to. The way that you usually sell for your homophobic encryption is like the cloud, like Google cloud or whatever, DigitalOcean or AWS, but you don't have to trust AWS. And so if you're running mission critical, like, military stuff, then you want to do secure computation where you offload the actual computation, and then you get this result without having to trust anyone.
01:06:05.528 - 01:06:21.350, Speaker C: Yeah, but it seems like it will be them yielding power. Right. It seemed contrary to most of their interests because it empowers individual. Ultimately, if this is, like a backdoor free technology, this is ultimately powerful.
01:06:21.930 - 01:06:38.650, Speaker B: It's like short term thinking, where, like, Tor, for example, tor was invented by the US. Military, as I understand the US. Government, and it gives them more power in the short term because as the military, they don't have to go translate the whole US. And whatnot. But you're right. In the long term, it empowers individuals.
01:06:49.430 - 01:07:00.694, Speaker A: We are over time. So if anyone has to leave, please do. I mean, Phil already has left. He thanked you. Justin, could you stay a few more minutes, Justin? Or do you have to go?
01:07:00.812 - 01:07:03.094, Speaker B: No, I can stay. I can stay. No worries.
01:07:03.292 - 01:07:11.080, Speaker A: Okay, does anyone have questions? Because I definitely have more questions, but Ele and I have been monopolizing the.
01:07:11.470 - 01:07:20.590, Speaker D: Well, I do have a question I didn't get. Why exactly would you need to hide the metadata?
01:07:22.610 - 01:08:19.470, Speaker B: I see one piece of metadata that I think is important to hide, for example, is the sender. Let's say that the sender is just an NFT enthusiast. Let's say it's DCinvestor ETH, and every time he makes a transaction, he's probably buying an NFT. Now, if you combine the metadata know you're seeing a transaction from DC Investor ETH and it just happens to match with a new NFT kind of know that's public information. When the NFT auction happens, then you can make an informed guess that DC Investor is going to be buying NFTs in that auction. And so what you can go and do is you can go front run him by just buying up NFTs ahead of him and then making him pay a higher price for his purchase.
01:08:20.130 - 01:08:20.638, Speaker C: Okay?
01:08:20.724 - 01:08:24.980, Speaker D: So it's just that okay, I thought there's maybe more reasoning behind it. Okay?
01:08:27.750 - 01:08:57.740, Speaker B: But the complication is that if you do encrypt it, the naive solution is just encrypt everything. But then you have a problem where you can't guarantee the anti spam property that Ethereum today provides, which is you just look at the sender address and the signature and that's enough to determine whether or not they have enough balance to pay for the transaction. And if they don't, then that's kind of a discarded transaction without even having to execute it.
01:08:59.090 - 01:09:15.620, Speaker D: There's another unrelated basic thing that I didn't quite understand with regards to commit and reveal schemes. Why wouldn't a simple hash based commit and reveal scheme for every transaction? Why wouldn't that work?
01:09:16.630 - 01:09:17.380, Speaker B: Right?
01:09:20.230 - 01:09:24.260, Speaker C: That was the bandwidth thing you told me. Right, you explained that.
01:09:27.210 - 01:10:19.320, Speaker B: So I think one of the bigger problems is in order for it to be incentive compatible, you need to have some sort of penalty if you don't reveal. But the problem is that knowing whether or not someone revealed is a very difficult problem. It's like a subjective thing because for some people they might see a piece of data as being revealed, some other people might see it as not revealed. Someone could publish their data right at the boundary, which is considered kind of the deadline for publishing. And then half will see it, half will not see it. And so that's basically consensus over datability, which is very expensive. And the way that we know how to do consensus over data availability is basically you have a committee that vote as to whether this one hash is available yes or no.
01:10:19.320 - 01:11:05.910, Speaker B: And because everyone's voting on the same thing, the way that you vote is just by signing a message with one bit you can do the aggregation. So what happens is that we aggregate all the signatures that have a bit one and we aggregate all the signatures that have a bit zero, meaning that they think that the data is not available. And so basically what you end up doing is that the blockchain verifies two signatures. But if you're voting over N messages, n individual transactions, then you have an exponential blow up. And the reason is that you have two to the N possible combinations of votes that you could be making. And so the blockchain needs to go verify two to the N signatures, which is extremely expensive.
01:11:08.170 - 01:11:27.382, Speaker D: I'm not sure a very simple example. First, before I can commit transaction, I just commit to the hash of the transaction and that hash gets into the blockchain and at some later point in time, I just reveal the actual transaction.
01:11:27.526 - 01:11:28.220, Speaker B: Yes.
01:11:28.610 - 01:11:36.110, Speaker D: Why is there more than I mean, there are just two signatures that need to be checked, right, two times the transaction signature.
01:11:38.050 - 01:12:47.320, Speaker B: Okay. One of the problems is that you can just put lots and lots of commitments. You can say you can commit, for example, to say, I'm going to buy I'm going to buy one E, I'm going to buy ten E, I'm going to buy 100 E, I'm going to buy 1000 E. And so basically, you have the option to buy some combination of these transactions, and you also have the option to not buy anything. And so what you can do is you can look at what other people reveal and if what they revealed, for example, is a juicy unisoft transaction that can be sandwiched, then you can just reveal transactions around that juicy transaction to make the sandwich. So basically you're in a position where you can grieve people. And so what you need really is you need to have some form of collateral which is fairly high because it needs to compensate for the attack that you're going to do.
01:12:47.320 - 01:13:36.980, Speaker B: And you need a way for the protocol to know when to actually, you know, you know, liquidate the, the collateral. Now, one way to do it without the data available to Oracles is just as you said to just put everything on chain and if it doesn't go on chain within a certain period of time, then you destroy the collateral. But no, you're right, you can do that. But I guess you need the collateral to be very high to compensate for the potential loss that you're inflicting to others.
01:13:37.830 - 01:13:49.350, Speaker D: In my naive view, it would just be like a commit and a reveal transaction like a pair would only be valid if I paid all the gas is upfront in the commit transaction.
01:13:49.510 - 01:14:30.360, Speaker B: Yeah, but the problem is that it's not just the gas. And the reason is that the option value could be much, much higher than the gas. The option value could be 100 e and the gas could be just a tiny fraction. I mean, that's maybe like the simplest way to explain it. You need a way to quantify the option value. And even if you do quantify the option value perfectly, you also need a way to kind of compensate those that you've griefed and now you need to measure the griefing. It just becomes very ugly very quickly.
01:14:30.360 - 01:15:29.642, Speaker B: But if you do it on a kind of one block at a time and you have single start finality, then there's only two cases and you can't just reveal based on other external information because all the information has already been incorporated in the block that you, that you've built as one big monolithic thing. And you've already perfectly priced how valuable this block is to you. And so here you can just charge upfront as you said, which is exactly what PBS is doing. It just charges upfront for the tip and the base fee for all the transactions in the block. But here you can't really grief anyone other than yourself by not revealing. Okay, cool.
01:15:29.696 - 01:15:30.540, Speaker D: Thank you.
01:15:35.820 - 01:15:43.230, Speaker A: Maybe one question. What do you think we should be working on at Flashbots related to this?
01:15:45.360 - 01:16:41.144, Speaker B: Right? So Alex, you asked for a spicy take. I guess my spicy take here is that if you don't have a crypto team and you're in mev, you're not going to make it. And so maybe just strategically speaking, it's good to start investing because it might not be the short term game. I mean, we are starting to see it. For example, for Threshold Decryption, we have three different projects. We have Shutter, we have Osmosis, and we have Drand that all have kind of this Threshold Decryption. But I think incrementally over the years, we're going to slowly kind of go from this flavor to this flavor to this flavor, and more and more transactions will be encrypted.
01:16:41.144 - 01:17:51.636, Speaker B: And the way that I think it will happen is that first it will be on a per roll up basis where you have this encryption. And the reason is that roll ups can define their own consensus rules. For example, the perfect example might be Arbitrum. Arbitrum is planning to have encryption with this fair ordering and fair sequencing service from chainlink. And then what I foresee happening is that there's going to be a bunch of experimentation and competition and then standardization, and then eventually maybe even the layer one ethereum itself will implement some sort of encryption as well. Why don't you do that today? Um, partly because, you know, the there's other things that we need to work on. You know, we need to work on scalability, for example.
01:17:51.636 - 01:18:50.520, Speaker B: That's a huge thing. And there's also various security upgrades that we need to do of all this stuff. But I guess another reason is that we're in this luxurious position where we can delegate, right? Like the whole roll up centric roadmap maybe means that we might never need to go ahead and have this encryption because all the transaction flow will be at the roll up level anyway. But one of the things that I'm predicting is that there will be a notion of enshrined roll up where basically the community will say, in ten years time, we clearly know what the best roll up is. It's a ZK VM. We basically want to provide the same functionality but at layer one. And so there's a bunch of advantages that you get from that.
01:18:50.520 - 01:19:38.064, Speaker B: One of them is that you no longer need any form of consensus. And so you don't need another token, because the reason why roll ups need consensus at a minimum is because they need to upgrade the. Rules of their own roll up. So, for example, a lot of roll ups, they want EVM compatibility, but the EVM changes over time. We're adding new opcodes and stuff like that. And so in order to maintain EVM equivalents, you need to have governance, and that's an attack vector. And so if we were to have an enshrined roll up where the rules of the roll up upgrade at the same time as the layer one, then you have a better roll up.
01:19:38.064 - 01:19:51.630, Speaker B: And there's like a handful of other ways in which enshrined roll ups are just better than kind of application level roll ups. But that's kind of thinking ten years into the future.
01:19:52.480 - 01:19:56.110, Speaker A: Got it. Do we have any other questions?
01:20:02.740 - 01:20:12.370, Speaker D: All right, I can have two minor remarks, I think, with regards to the metadata, I think one point that is missing is time.
01:20:13.540 - 01:20:14.290, Speaker B: Yes.
01:20:15.300 - 01:20:19.460, Speaker D: This is nothing you can encrypt or something, but something that needs to be considered.
01:20:19.960 - 01:20:22.470, Speaker B: Yes, absolutely.
01:20:22.920 - 01:20:37.668, Speaker D: And the other thing is, I'm not sure I can agree with your statement on bandwidth is expensive. I think if you think about the last decades, let's say that the bandwidth is just getting cheaper and cheaper.
01:20:37.844 - 01:21:12.470, Speaker B: Sorry, when I said bandwidth, I mean metaphorically, I don't mean the actual bandwidth in, like, bytes per second. If you think of the data availability oracle as being, you give it a hash and it tells you true false available. Is the data under this hash available? Yes. No, this oracle has low bandwidth, as in you can give it a hash every slot, but you can't give it 10,000 hashes every second. If you had 10,000 transactions a second, it just wouldn't work.
01:21:18.470 - 01:21:30.374, Speaker C: Amazing. Justin, I learned so much today. I think we should have you every week, man. Thank you so much for this presentation. It was great.
01:21:30.492 - 01:21:42.890, Speaker A: This was awesome. I'm so glad this was recorded as well for the members of Flashbots that weren't there. I think we're going to carry that knowledge over there's, like, a lot of things that you said that are very important, in my opinion.
01:21:44.830 - 01:21:45.482, Speaker B: Cool.
01:21:45.616 - 01:21:55.230, Speaker A: Before we leave, Justin, do you have anything you want to share with Flashbots? Anything else? Sorry, you want to share with Flashbots or any questions you wanted to ask? I don't.
01:22:01.570 - 01:22:28.330, Speaker B: Mean I still want to see that presentation from know, like, the diving into the numbers and quantifying all sorts of things. You know how I asked, you know, what what percentage of bundles, you know, are conflicting? And like, I have I have maybe ten or so very similar questions about kind of the nature of mev that you're seeing, but maybe that's for another discussion.
01:22:28.670 - 01:22:53.006, Speaker C: Yeah, I think Alex posted there. We can resend that over Telegram. I have a summary of that so you can check it out. It's an old analysis, so take that with Tweezers. But yeah, I think it's something I've been pushing for us to jump into again. I think it's an important question for us. It's directly relevant because it touches upon the optimal algorithm for merging.
01:22:53.006 - 01:22:59.730, Speaker C: Right. That was the initial motivation to do it, but definitely I would share that and happy to continue a conversation.
01:23:01.930 - 01:23:21.174, Speaker A: Amazing. All right, well, Justin, thank you for staying 24 minutes over and for an amazing presentation as well, with logistical with this physical board. Feels great. Physical board. Yeah. Thank you so much. Have a great rest of your day.
01:23:21.174 - 01:23:34.480, Speaker A: Thank you for sharing all of this with us. I think we might have, like, additional questions, et cetera. So we have communication channels. We'll be sending all of those. Yeah. Thank you.
01:23:34.850 - 01:23:36.174, Speaker B: Thank you so much, guys.
01:23:36.292 - 01:23:37.520, Speaker C: Thank you so much.
01:23:38.050 - 01:23:39.450, Speaker A: You too. Bye.
