00:00:02.360 - 00:01:09.684, Speaker A: Okay, great. So hello everyone. GM and I guess G afternoon for the past nine minutes. So about a couple of years ago, I wrote this post called endgame, where I basically pointed out that. I basically pointed out that a bunch, a lot of the different blockchain scaling paths that people were thinking about, including layer one scaling, including different forms of L2 scaling, if you push all of those paths to their limit, and if you add the kinds of scaffolding that you need to actually make them sane from a decentralization and censorship resistance perspective, they actually end up in a surprisingly similar place. Basically a voice where you get centralized what I called block production, though I think it's also actually possible to think of it as being something that's much more restricted than production. Then you have decentralized validation and you have strong anti censorship protection.
00:01:09.684 - 00:01:56.522, Speaker A: So this is a post that came out about over two years ago. And since then, I think we've gone a lot from theory to practice. And one of the ways that I think about how a lot of these things look like in practice is basically that how ethereum scales is an industrial organization problem as much as it is a technical one. Even if the same pieces get built, different choices and who builds them can lead to big differences in outcome. So, analogy, the efficient way to serve food at scale across the country is to have many locations. In each location, food gets made in a central kitchen and people can come and order it. How many people here have been to one of those in the past year? Okay, not many hands raised.
00:01:56.522 - 00:02:48.446, Speaker A: Interesting. But, but, you know, but it's each location, a branch or a franchise. Right? Branch has like commands and control, like arranged from the top down. Franchise is like, you know, anyone can come in and say, you know, yo, dog, I have a location, I'll pay a fee. And like, I want to use your logo and serve your stuff. So there's, this is a kind of sub discipline in economics, and, you know, it can really. Yeah, like, try to explicitly think about the trade offs of, like, even if the same kinds of pieces get made, like, what are the actual consequences of different pieces that are happening? So going through a few examples of this, right? So first example is the state transition function, right? So state transition function is basically just if Ethereum scales, then you need to have lots and lots of transactions.
00:02:48.446 - 00:03:45.196, Speaker A: And these transactions are being processed by a set of rules. And these rules talk about how accounts work, how smart contracts work and so on and so forth. And these rules have to be enforced somewhere. Now, Ethereum, for the last few years has had this roll up scaling, a roll up centric roadmap where basically Ethereum as a blockchain provides shared security, and it provides data scalability through things like EIP 4844, which we're going to have in I guess about 25 hours. Then you have roll ups, which are these L2 protocols that actually handle execution and all of that infrastructure on top. Now, theoretically, you can achieve the exact same outcome by basically just having a big system that has monolithic shards inside the protocol. And there are other platforms that do this.
00:03:45.196 - 00:04:20.894, Speaker A: In the case of Ethereum realistic possible futures, I think there's basically three that I can think of. And basically one is you have different roll ups, and each of those roll ups makes their own highly customized stage transition function. And this is a major basis for people to compete with each other. You have fuel, which has been stage two for a while and deserves praise for that. You have arbitrum and arbitrum stylus. Then you have the whole starknet ecosystem and Cairo and their proof based vms. You might have even more in the future.
00:04:20.894 - 00:05:32.244, Speaker A: Another option is basically roll ups generally reuse the EVM. And a third option is roll ups use a hypothetical Ethereum lawyer, one EVM precompile. So this is a post that I wrote about a few months ago that basically talks about this idea that maybe Ethereum should include a ZKe EVM precompile as part of its kind of layer one functionality. And if we do that, then basically it comes much closer to a world where we have what was called sharding ten years ago, including execution sharding. But as a roll up, you're basically responsible for spinning up your own shard. And if you do a good job of attracting applications to your shard, then, you know, you get to collect the priority fees in your shard and you get to make money that way. But these are like very from a technological perspective, there's a lot of similarities, but from a perspective like who builds it, you have a bunch of different consequences, right? So like for example, if rollups make their own STF, you have more options for users, faster iteration in VM technology.
00:05:32.244 - 00:06:22.374, Speaker A: Cons are greater software bug risk, more confusing for users? If you want to make it easier to understand for users, then like fine, everyone uses the EVM, but you have somewhat less bug risk. But you still have some risk of bugs, but also less room for creativity. If you don't want bug, if you want to really minimize the risk of bugs losing your money, then Ethereum l one EVM pre compiled can do that. Basically, the more you enshrine, the less application code attack service there even is. There's also no need for roll ups to have governance because they just kind of automatically upgrade when Ethereum upgrades. But there are trade offs in this case there's actually efficiency trade offs. Enshrined zkvms do involve some actual technical sacrifices, and IAM also talked about them in the post.
00:06:22.374 - 00:07:26.572, Speaker A: So you have these different options and they actually have some interestingly different consequences. But a lot of these consequences, like they don't appear on usual axes like scalability versus decentralization. They appear on these weird new axes that we think less about, like speed of innovation of different things, like risk of software bugs and the extent to which different pieces of the system need to have governance. Right? So like for example, if you're the sort of person who loves the idea of Ethereum ossifying and you're totally afraid of Ethereum governance doing crazy things in 2029, then you know, going up to the next two columns and especially the top column is good for you. But if you're the sort of person who actually believes off chain governance is best, and you're afraid of any of these weird gadgets that have inside of L2 s, then like the bottom thing is actually best sequencing. Right? So procedure for determining which transactions go in the next raw block. So independent sequencing every roll up does it for itself.
00:07:26.572 - 00:08:22.486, Speaker A: Shared sequencing as a piece of L2 infrastructure. And then you have base sequencing and all of these Drake and ideas that try to bring layer one and L2 together. Yeah, and if you look at just kind of what the trade offs of some of these are, I think between independent and shared. The big difference is basically if you do independent, then you get a risk that shared sequencing happens anyway through side channels. And then if you do shared sequencing, then like, there's greater risk that there's some kind of ecosystem wide monopolization. If shared sequencing is a L2 infrastructure, there's a risk of shared sequencing protocols becoming rent extractors. If you do it at layer one, then you have a different risk, which is that if something weird happens at that layer, then you get more risk spilling over onto layer one.
00:08:22.486 - 00:09:41.814, Speaker A: Right? So actually, yeah, this was something I also wrote briefly about in my post kind of almost two years ago on April 1, praising bitcoin maximalism. And I basically talked about how like, the thing about bitcoin simplicity that makes sense is not like the technical simplicity of the VM, because like, you can make the VM way more performance with only a tiny bit more technical complexity. It's like having applications is bad, right? And the reason why having applications is bad is because applications create weird stuff like maV. And the effect and the centralizing effects of MAV spill over to the base layer. And like if we basically, if we want ethereum itself to be maximally protected from all that, then like you have sequencing be very far away. So that whatever centralization, whatever weirdness happens that gets collected by outside actors and whatever gets pushed onto l, one becomes relatively time independent, right? So that's one example of an economic incentive. And then of course, the flip side is like, which of these protocols actually gets to collect the mev? And this is one of those trade offs between basically sequencing being based and sequencing happening at some higher level, right? So from a technological perspective, you get similar things happening, but a lot of the consequences, again, are economic.
00:09:41.814 - 00:10:15.422, Speaker A: They're about like code dependency, whether you're basically leaning on l one or doing things separately from l one. And these kinds of issues, proof aggregation. So roll ups need to publish proofs to chain. A proof is about 500,000 gas or something like 5 million gas. If you're a stark, this is a lot. It would be really amazing if every roll up could just publish an update every block. But at this point, it feels like everybody is just launching a roll up.
00:10:15.422 - 00:10:46.558, Speaker A: And so we probably have like more than 60, 30 roll ups at this point. And so we have like too many for the gas limit if they want to publish every block. But the status quo is that every roll up figures out for themselves. Simplest code, minimum trust dependency is high gas cost. But what if we have like aggregation protocols, right? Basically, instead of 20 roll ups publishing 20 snarks to chain, you like publish, somehow publish one snark to chain. And we have aggregation protocols. And there's different ways in which this could happen.
00:10:46.558 - 00:11:28.746, Speaker A: So one way in which this could happen is you have ethereum ecosystem wide proof aggregation. So basically you have a protocol where rollups can opt into this, where basically they submit their proofs into a mempool. And then it's the role of builders in the mempool to just take these proofs and then make a proof of the proofs and then publish of a proof of the proofs along with the state routes. And then a contract just like makes one call for each roll up. And you only need one proof, right? So very good gas savings, some level of opinionated choices, some amount of shared interested code. Another way in which a very similar thing is happening already is like we're seeing aggregation within ecosystems right. So like Starknet has their own aggregation.
00:11:28.746 - 00:12:17.774, Speaker A: You have like layer threes, and then the layer threes commit into L2, the L2s commit into layer one. And so ultimately there's like one proof. And so you get the same effect of like proofs inside proofs. But you know, this, like, if this is a thing that individual roll up ecosystems do separately, then like this is good for large ecosystems, but it's less good for small ecosystems. And like you have to deal with more trusted code account abstraction and key stores is another one, right? Basically, yeah. So we have wallets and one very basic security property that's considered totally standard in regular cybersecurity is he wants to be able to expire keys. You want keys that control an account before, to no longer control an account later.
00:12:17.774 - 00:13:11.924, Speaker A: This is something that eoas do not do. And it's one of the five fundamental reasons why in the long term eoas need to die. But if you have some account abstraction based setup, then basically you have to store the information somewhere of things like which keys currently have the right to process transactions from an account. And the question is, where does this data get stored? The status quo is basically if you have a smart contract wallet, you have a copy of that smart contract wallet on every single l two. And this is like simplest, fairly simple code, fairly minimal trust dependencies. But then there are also obvious costs. And the cost is like, well, if you change the keys in one place, then are the keys still going to change in all the other places? And you have high gas cost of changing all the keys around in different places.
00:13:11.924 - 00:14:17.170, Speaker A: And there's also a risk that you'll just accidentally forget somewhere. It's like you have a dose of safe and you change your keys in three different places, but you just totally forgot that you have a copy of the same safe somewhere else, and then you forget to change the keys. And then two years later, one at a time, the old key holders get hacked, or someone else goes after them, or they think that everyone else forgot about them and they just go and steal the money. So there's cons of the current approach. A very natural approach is, well, instead of having that data live in every place will have the data live in one place, right? So like either use l one, but l one is a bit too expensive. Like use some l two to store this information of like which keys currently have the right to access some account. One idea that I talk about is like this minimum keystore roll up proposal, right? Basically you create like this very minimalistic roll up whose only function it is, is to like store this account related logic and to store the rules for updating it.
00:14:17.170 - 00:15:09.384, Speaker A: So basically trying to keep the thing pretty simple, trying to keep trust dependencies pretty low, requiring some standardization. And then if you want to update your account, then you'd only have to update it in one place. And then basically every time you sign your transaction, like that transaction would include a snark that proves the current status of your account. And this is something that is going to become cheap in the future because we have proof aggregation, but there's still a question of who builds it. Another option is, of course, we just kind of let users pick, base their accounts on whatever chain they want, and you just put your key store there. You store the logic of who is allowed to access your account, and just whatever chain you want, and different users do it differently. There's tech blow up risk here.
00:15:09.384 - 00:16:01.542, Speaker A: So you have an n squared tech below it because you might have n different types of proofs for n different roll ups, you have more trust dependencies. But on the other hand, it requires this less shared infrastructure. And it's the sort of thing that's maybe more likely to happen anyway by default. So you have approaches that are very similar, but then you have differences in practice depending on a crew does it. So I guess the conclusion of all these different things is basically that, you know, once you start getting into the weeds, there's like a lot of these choices about, like, which actors are actually responsible for which pieces, right. And if you go back to the endgame post and, you know, you think about how from a technological perspective, right. All of the different things, like, feel like they're lining.
00:16:01.542 - 00:17:04.348, Speaker A: They're lining up toward the. Yes, same conclusion. But what's the big difference between, let's say what I call an ideal l two scaling future versus what I would call ideal Solana, where you actually have consensus nodes being able to run on laptops and have sensitive inclusion channels and have starks for everything. Well, the answer is basically you have this boundary of which actors are responsible for building and maintaining which parts of the ecosystem and what their incentives are, and across all of these different areas, like, the difference between those things matters a lot, right? So basically you have two separate questions, right? There's always the tech arrangement, and then who's responsible for building and mutating what. And probably the biggest actors that we have are like, one is independent layer, two teams. Another is basically standards groups. So you have the roll up improvement proposal group.
00:17:04.348 - 00:18:21.434, Speaker A: We have eips, you have ERC is potentially more standardization in the future. And then you have l one core protocol teams, depending on which of these pieces is responsible for building things, you start to have some different consequences. I think the main axis on which the consequences are different on all of these is basically, you know, like, one, you know, who has the incentive to build the thing? Two, is there some incentive, like, is there an incentive for people to try to get some monopoly and like, start extracting rent from the thing? Three, kind of where does the code bug risk lie? And like, you know, and how much risk of code bugs is there? And probably about a couple of other questions, right. And so I think in a lot of these questions that we have in the future, it's going to be a similar thing. It's like even if the final outcome is the same, the path for getting there also defines these boundaries between what kind of actor is responsible for what. And this will decide a lot of the details.
