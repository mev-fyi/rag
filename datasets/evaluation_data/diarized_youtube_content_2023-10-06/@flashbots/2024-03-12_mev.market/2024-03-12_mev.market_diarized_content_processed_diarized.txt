00:20:44.770 - 00:20:48.200, Speaker A: Can everyone hear me okay.
00:20:50.250 - 00:20:50.878, Speaker B: I.
00:20:51.004 - 00:21:33.174, Speaker A: All right, I guess I'll get started out of respect for the time of all the other speakers. So I'm giving a speech today on everything is MeV. Why is MeV the most important design consideration in blockchain? Why is everything MeV? So today I'm going to elucidate my thesis why MeV is the most significant property we should consider when reasoning about decentralized infrastructure. I'll begin with a brief introduction to my mental model of MEV as a property. Then I'll talk about how first principles thinking around MeV ties into all of the major blockchain verticals. Finally, I'll give some approaches to MEV awareness in protocol design and business strategy. So MeV is a property of consensus.
00:21:33.174 - 00:22:17.160, Speaker A: To have a cogent discussion about MeV, we first have to answer what MEV is at the most fundamental level. There are many definitions for MEV which make sense in various contexts. Sometimes you want a definition that lets you quantify things, or a definition that you can argue in a rigorous mathematical sense. However, my mental model for MeV is quite a simple and broad one, which I believe presents the best foundation for thinking about protocol design. MEV is any financial reward which incentivizes the consensus of a blockchain to continue progressing. In other words, if a blockchain is walking forwards, MeV is the reason why. So this includes the block reward rewards from block building strategies, gas fees, internalized protocol rewards such as chainlink tokens, and a whole host of other things.
00:22:17.160 - 00:23:00.902, Speaker A: Why is this definition superior to other definitions? Well, it lets us think about the totality of what's going on in the interactions of many blockchain protocols. From a game theoretic perspective, MEV surrounds us. So anyone who knows me on a personal level knows I like soft topics like talking about my feelings, relationships, and codependency. And few relationships are more codependent than the relationship between a protocol designer and MeV. Whether you like it or not, MEV is something you contend with, and it's up to you if the relationship is a healthy one rather than a toxic one, because you can't ever break up. I'll cover some examples of common verticals where I believe that MeV awareness is crucial. So blockchains and l one, this is kind of the obvious example.
00:23:00.902 - 00:23:35.018, Speaker A: MeV is the reward for progressing consensus, typically extracted by miners or validators in the form of block rewards and gas fees. Additional value from protocols can also be extracted by whoever is ultimately building the block. Misaligned incentives leads to centralization because there can be increasing returns for scale trading. It's like another obvious example. Sophisticated teams compete using different edges to capture value which protocols are leaving on the table each block. This is usually atomic or nonatomic arbitrage and sandwiching opportunities. But of course there's an insane amount of other potential strategies.
00:23:35.018 - 00:24:41.234, Speaker A: Misaligned incentives here usually leads to the entrenchment of one to two trading teams, which is kind of what we see on Ethereum today. Builders and relays Building is essentially just another trading strategy, although it's kind of unique in the business and product development that's required to source general order flow and achieve MEV coverage for the strategies, which are not executed in house. So relays are a marketplace of blockspace, which takes block space from validators and coordinates market makers to settle it. I see the relay market as a key shelling point for MEV activity over the next two to five years, as relays gradually become interchange. Misaligned incentives here have some of the worst potential of any vertical to create massive centralization. So Dexes this vertical is a key source of extracted MEV because price discovery tends to occur off chain and there's a block time lag which leads to arbitrage profits. One of the most important open questions in the market is the best way to internalize this MEV to the liquidity providers or to the users? And whether batch auctions, dynamic fees, better oracle pricing, or active liquidity is the best mechanism to reduce LVR loss versus rebalancing.
00:24:41.234 - 00:25:40.258, Speaker A: There's also a lot of open questions about what's a fair split for the MEV between the users and between the liquidity providers bridges and interoperability protocols. So anytime consensus is being settled across multiple domains, you have a host of metaverse iterations. Who has the control over finality? What crosschain opportunities can be executed atomically? For example, having a validator execute on two chains simultaneously means that a crosschain opportunity could become atomic. Misaligned incentives here leads to centralization of stake because there's network effects wallets and RPC providers. So wallets control the users, who are generally quite sticky, and they're reluctant to move elsewhere because there's a high degree of trust required for a wallet, and user transactions are responsible for the vast majority of MEV creation, and thus wallets are one of the key power brokers in the transaction supply chain. And misaligned incentives here will lead to users getting suboptimal execution order flow auctions. So you could argue these are part of a wallet's functionality, or that they're directly underneath the wallet on the vertical.
00:25:40.258 - 00:26:45.422, Speaker A: There's a lot of MEV considerations in how we solve intents and distribute value for order flow. I personally think it's a very complex picture because some of the value in order flow only exists in batches. For example, one approval transaction means nothing to me, but 1000 approval transactions is probably a strong buy or a sell signal. There are also questions about how we price the value of order flow between the users and liquidity providers in an open market, similar to the considerations in dex design intent protocols. So intents have a lot of interesting considerations in respect to solving them effectively. I hold a contrarian belief that a lot of the design being done right now with intents is in the wrong direction because we're focused too much on expanding the complexity in which we can express the intents, rather than focusing on the best ways to solve them. I believe an important MEV consideration is how we can create some kind of Lego building block for intents that appropriately constrains the expression so that we can appropriately constrain the solving oracles significant price updates from oracles creates a lot of value from liquidations and other types of MEV who should get this value.
00:26:45.422 - 00:27:52.050, Speaker A: Can we design the oracle update Mechanism so the MEV is internalized? Is this even a good idea, or should the protocol try to absorb it somehow? I think this is a very interesting design space because you actually need Oracle MEV to incentivize liquidators to protect your protocol from bad debt, staking and restaking. This is again interesting because it blends MEV considerations with tokenomics, as the MEV rewards can become part of the staking rewards, which can accrue to tokens, or you can even split off the MEV rewards into a second token altogether. There's also a lot of MEV considerations with restaking, when validators can run arbitrary code and can get slashed like on eigen layer. Finally, stablecoins. This is a really interesting case because MeV is responsible for the stability and the instability of stablecoins. Typical stabilization mechanisms rely on people arbitraging away dislocations, and the typical collapse of a stablecoin in a death spiral involves huge amounts of MEV extraction. Like we saw in the case with lunar, the entire design space of stablecoins is about harnessing positive MEV activity for the benefit of the protocol and reducing the effects of toxic activity.
00:27:52.050 - 00:28:28.730, Speaker A: So designing infrastructure that's neverware. How do we develop a healthy relationship with MeV? There's a famous saying in economics, show me the incentives and I'll show you the outcomes. And this applies aptly to protocol design. Designing protocols which are neverware, means creating incentives which foster the outcomes that you desire. I'll run through some of my mental models for thinking about designing protocols in different verticals. So, first I would start with macro analysis. What are the mental models? What is the supply chain for any kind of decentralized commodity? Where does it start and where does it end? For example, blockspace is a digital commodity.
00:28:28.730 - 00:29:17.982, Speaker A: The market starts with intents or preferences over state transitions, as well as physical block space. These commodities flow down through wallets, RPCs, towards the marketplace for block space, which we currently call a relay. Validators on multiple chains sell their block space to the relays, who offer it to solvers, which may be builders or searchers. By visualizing the flows through the supply chain, you can start to build a picture of the vertical. What are the barriers to entry? What do people need to enter the market? Who can stop you from entering? And why does it cost money? Whose permission do you need? What are the edges? So the typical edge in MEV is capital. For example, to be a CFI DeFI trader on Ethereum right now, you need significant capital in order to bid away entire binance dislocations across multiple pools. This leads to only a small group of well capitalized searches dominating the market.
00:29:17.982 - 00:29:47.142, Speaker A: Another edge is engineering, which of course relates closely to capital. This could be the quality of the engineering, for example, building low latency systems. Or it could just be the general moat of having done a lot of engineering. Alpha edge is the edge of having some kind of insight into the market, which other people don't have. While this sounds pretty cool, it's actually the rarest edge to have, as the market is quite transparent. However, some people do have an alpha edge. Jane street, for example, was running manta builder for a while, and you could see they were hedging a lot earlier than other people.
00:29:47.142 - 00:30:20.206, Speaker A: Why were they doing this? Obviously, they had some kind of insight into likely future prices, which other people didn't have. Meaning that if the price went up, they could hedge in advance, knowing that it was likely to be a good trade and they would be more competitive for it. Business development is the final edge, and unfortunately it's quite common. Teams strike deals with validator operators for access to stake for order flow, unique access to protocols, giving them an advantage over competition. So understanding these edges is the key to understanding how the market is likely to progress. Who has the power to commoditize. There's a principle in business that you should always commoditize your complementaries.
00:30:20.206 - 00:31:15.278, Speaker A: Essentially, this means that if you sit next to someone on the vertical, you should figure out how to increase their competition and drive their profits to zero. A classic example would be how the original flashbots auction commoditized atomic searching atomic searches are complementary to the flashbots auction because they're submitting bundles to the auction. If atomic searches gained too much power, then they could launch a competing product of flashbots or just bypass flashbots altogether. However, the flashbots auction had the effect of commoditizing atomic searching by encouraging competition, driving profits to near zero, and leaving them unable to control the vertical. Right now, we see that order flow and capital are still distinct edges in the building and searching market, which are yet to be commoditized, leading of course to the centralization that we see today on Ethereum. In my opinion, the most interesting use case of swab is its potential to commoditize the capital and order flow edges. Lastly, what are the dependencies? Distribution is always about dependencies.
00:31:15.278 - 00:32:17.798, Speaker A: When you're in the middle layer of the vertical, you can't pay for Google advertisement, so whose permission do you need to grow? Are they motivated to help you? Are they likely to try and commoditize you or launch a competing product themselves? Are you actually solving any problems for your dependencies? This is a key issue in blockchain more broadly, where a lot of people are just building with no consideration to whether people would actually use their product. They just care about cool game theory, microanalysis, the game theory, and the mechanism design. So for this type of analysis, we look at the protocol in the context of the game theory and the mechanism design, because at the end of the day, all blockchain protocols are just different types of repeated games. In other words, there are players rules, incentives, strategies, and resulting equilibriums. Who are the players? Typical players in blockchain games include searchers, solvers, builders, market makers, users, protocols, relays, validators. Anyone who participates in any way is a player in the game. What are the rules of the game? It's common for these rules to be defined in code, such as smart contract code or client code.
00:32:17.798 - 00:33:10.770, Speaker A: However, these may also be defined socially, and you might be surprised in MeV how loosely some of these rules can be defined. For example, some time ago it turned out on phantom network that reorganizing the entire blockchain was just a social rule, and no one would actually slash you if you did that. Many of the rules around propagating Mem pools are also just socially enforced. It's very important analyzing the protocol to make a distinction between the rules you have to follow and the rules that you are supposed to follow. What are the incentives? Rules are enforced using various financial incentives, and you get rewarded by doing one thing or punished for doing another thing. It's important to quantify what the incentives are and how they compare against the opportunity cost. For example, if the incentive of a MEB opportunity is higher than the cost of getting slashed, then does it make sense to just get slashed? Strategies and equilibriums whenever you have incentives and rules, you have resulting strategies and equilibriums.
00:33:10.770 - 00:33:45.250, Speaker A: Often the strategies may be different to what you think. It's generally most helpful to look first for the dominant strategy, which is the strategy that results in the highest expected value, regardless of what anyone else does. In the classic prisoner's dilemma game, for example, the dominant strategy is to be a snitch on polygon. It might be to submit as much spam as humanely possible. The two common types of equilibriums that we find in markets are dominant strategy equilibriums and nash equilibriums. A dominant strategy equilibrium is the outcome we get when everyone plays their dominant strategy. And a Nash equilibrium is a situation where it doesn't make sense for anyone to change their strategy.
00:33:45.250 - 00:34:15.310, Speaker A: Creating good games so how do we actually design good Mev games from the ground up? I'll offer some of the first principles thinking that I believe is worth following. One, reward the behavior you need to happen. If you want people to do something, create an incentive. If you don't want people to do something, punish them. Don't rely on social consensus, because lots of people aren't going to care and will do it anyway, especially when there is money at stake. Two, play to people's strengths. Give players in games jobs which they are actually good at.
00:34:15.310 - 00:34:41.990, Speaker A: For example, solvers are professionals at taking and pricing risk. So they are the people who should be taking the risk. In melware designs, a great example of this is the across bridge, which uses solvers to outsource the risk of finality. Two, there will always be edges emotes, so choose those edges wisely. You cannot create a game with no edges, so use the edges that serve your purpose. Think about what kind of barriers to entry these edges will create. Three, don't assume everyone is nice.
00:34:41.990 - 00:35:08.206, Speaker A: Don't be stupid about people's incentives. Basically, people will do B to b, work with each other. Hackers will come along, searchers will try and exploit you. Think about timing. This is actually a common misalignment between founders and VCs. VCs often have a different perspective of where the timing in the market is, and founders often exploit this in order to raise capital. So you need to think when you're building about what the market will be like in six months time, or when you launch your product, not what it's like right now.
00:35:08.206 - 00:35:34.050, Speaker A: Think about where the MeV, who creates the MEV and where it should go. So MEV has to go somewhere. So think carefully about where it should go. Maybe you want to run a batch auction and internalize it to users. Maybe you want to give it to liquidators to protect your protocol from bad debt. Maybe you want to give it to validators, the protocol itself, or some other party altogether. This is often a philosophical choice, not just an economic one, as there's many competing definitions of fairness.
00:35:34.050 - 00:36:20.098, Speaker A: Lastly, what are the optics to the market, and also to regulators? Lastly, think about the optics. Is your protocol going to protect sandwiching and piss off regulators? Is your token going to make you look like a ponzi to the people on e three research forum? Bad optics can quickly destroy your project, even if your execution and everything else is basically perfect. Okay, so to wrap things up, I hope I've managed to present a different perspective here on Mev and convince you that we need to abandon pretenses like solving or eliminating MeV. MeV is a fundamental property of blockchain, and it's up to us to harness it for good or evil. It can be democratized, it can be distributed, it can be internalized, but it cannot be destroyed. MEV is not good or evil. It's a neutral incentive.
00:36:20.098 - 00:36:46.670, Speaker A: It's the most important incentive in designing decentralized protocols, because blockchains are just games and Mev is what keeps the score. MeV is everywhere and everything is Mev. Thank you. I guess. Yeah, I can take questions if there's any questions. Or we can move on to the next speaker.
00:36:51.070 - 00:36:51.820, Speaker B: Okay.
00:37:25.630 - 00:37:28.890, Speaker C: Does anybody know if there's a particular way to access the slides?
00:37:36.160 - 00:37:37.110, Speaker B: You it?
00:38:06.860 - 00:38:36.112, Speaker C: All right. Good morning everybody. It's an honor and a privilege to be here. First, I'd just like to thank the ethereum research community, the broader MeV community, flashbots and othermind, for organizing the events, and in particular TNSN. The title of my talk today is going to be called path dependent topology of MeV discourse. An overview. In part one, we're going to talk about path dependence, kind of how we got here.
00:38:36.112 - 00:39:06.312, Speaker C: And in part two, we're going to introduce a new concept, architecture, topology separation. So what is path dependence path dependence is a concept in the social sciences, referring to the process where past events or decisions constrain later events or decisions, I. E. History matters, past events or decisions affect future events or decisions in disproportionate ways. In some MEV past dependents, such as priority gas auctions studying MeV. I. E.
00:39:06.312 - 00:40:08.316, Speaker C: Clockwork Finance, and the famous article Ethereum's Dark Forest, which described generalized frontrunners what do Mev Geth, MeV boost and proposer builder separation, and then some control systems, order flow auctions, inclusion lists, and encrypted mem pools. So MEV is real. The flashpoints 20 paper in 2019 conducted an experiment that empirically proved the existence of MEV, and in particular, priority gas auctions were one key finding. Bots were competitively bidding up transactions, bidding up transaction fees in order to obtain priority ordering for Dex arbitrage transactions. And here's a visualization, and you can see on the y axis gas bids in GUI and on the x a axis time since start. But there are some negative externalities of the PGAs, unnecessary peer to peer networking load, inefficient minor, and searcher coordination. Failed bids were reverting on chain, which led to a very poor user experience.
00:40:08.316 - 00:40:53.580, Speaker C: I'm sure many of you in this room were familiar with that. And no granular preference expression for searchers. So what did he do about it? Studied mev. The clockwork finance paper was written by some excellent researchers at Cornell, and they attempted to define MeV and a couple of definitions you see here. I know you're probably familiar, so I won't go through them in detail. And there were a couple formalizations of MEV as well. And then there was a great article written by the folks at paradigm called Ethereum is a dark forest where they talked about a generalized front runner in the mempool, and there was a rescue attempt of user funds who had sent some funds mistakenly to a uniswap contract.
00:40:53.580 - 00:41:46.920, Speaker C: And so they used an obfuscation technique of sending a transaction first that was conditional and required to be executed before a second one. And the hope was there would be enough time to get that second transaction in before one of these generalized frontrunners saw it. But as it turns out, the generalized frontrunner saw the transaction and was able to claim the funds that had been lost. And this became even more evident during D five summer in 2020, where you had high gas fees, both congestion and contention, and again, failed transactions on chain. So you walk in, see this what do an example of the Mempool in Budapest Chungri. So the researchers at flashbots formed a collective, and they launched a product called Mev Geth. And the goal was to move the PGA game out of the public memp pool with the flashbots auction.
00:41:46.920 - 00:42:21.220, Speaker C: And the flashbots auction introduced a new transaction type expressivity bundle merging and mega bundles. One of the challenges was it was a single client with about 90% minor adoption, and it wasn't merge ready. Also, the social contract between searchers and miners was loosely unenforceable. Now, there were complaints that were thrown in the discord at the time, and there were some folks tracking it internally, but it was very difficult to enforce these things. And here's an example of the topology. It's an imperfect picture. Enter the MEV supply chain.
00:42:21.220 - 00:43:09.820, Speaker C: The discourse advanced and the MEV supply chain provided a new mental model on how to think about the interactions. Before we transition to proof of work, I want to hone in really clear here. The MEV supply chain describes the chain of activity, which helps users transform intentions into finalized state transitions in the presence of MEV. And I'm sure this is the meme that a lot of you guys know very well, but I think it's actually been very helpful to the broader community in understanding the network topology. It came with a warning, however, centralized dystopia, where each role in the flow would be verticalized. So, searcher builder, proposer relay, all the same entity. And unfortunately, we are starting to see some of this verticalization today.
00:43:09.820 - 00:44:27.370, Speaker C: However, really what we want is decentralized block building or distributed building, allowing for competition at each layer in the stack. And this is one reason I think, that everybody should be very bullish on swab. This transitioned into merge ready MeV boost. Some of the motivations were to wall off validators from MEV complexity support, client diversity, and it introduced new actors, builders who had previously been taken the role by mining pools and relays. Some of the challenges are transaction censorship, relay attacks, missed slots and information leakage, and in particular an unstable equilibrium such as generated by OFAC sanctions, relay dependency builder centralization, and the more recent timing games. So there's been much discourse about proposer builder separation, and much research within the community. There are many flavors of PBS single slot, two slot EPBs with proposer timeliness, committees with relays, multiplicity gadgets, and also generalizations such as Pepsi, Pepsi Boosts, Pepsi DVT, and Diet Pepsi.
00:44:27.370 - 00:45:58.310, Speaker C: Around the same time, more and more mev management solutions or ideas came into the fold in the discourse, such as order flow auctions, which Nathan just talked about encrypted Mem pools, which are great for front running protection and censorship resistance. And there are many different types of encrypted mempools. This is taken from Justin Drake's presentation on encrypted mempools. Check out the slides after for more details and inclusion lists again, an active area of research which allows proposers to force builders to utilize the available block space and if they can't do so, they must use the unused space for proposer selected transactions. And again, the research community is very strong and the discourse has centered from starting with CR lists to metboost plus plus forward inclusion lists, no free lunch, cumulative non expiring inclusion lists, EIP 7547 concurrent block proposals and recently released last week committee enforced inclusion sets. So now we're here, we're starting to see the emergence of intents solvers and off chain marketplaces for counterparty discovery. So what does this look like and what is a useful mental model for this? So now I'm going to talk about how we think about intense adenoma and this concept of architecture topology separation sponsored by the Rainbow stakers of Ethereum.
00:45:58.310 - 00:46:42.460, Speaker C: Shout out to Barnabay. So unbundle everything. So what is architecture? According to Wikipedia, network architecture is the framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used. The topology is the arrangement of elements, links, nodes of a communication network. So what's a simpler example? Imagine that you are designing a city. The architecture is the overall plan for the city, including where everything is supposed to be and how it all works together. And the topology is how you decide to connect to everything, roads, bridges, parks, determining the best way for people to move around the city.
00:46:42.460 - 00:47:47.022, Speaker C: Ethereum's architecture the architecture is like the protocol stack. It defines the core components like the EVM accounts, transactions, blocks, consensus. It specifies high level system design principles and interactions, and it includes features like smart contract languages, EVM upgrades, and EIPs. Ethereum's topology is how the network works in practice and describes organization, interconnections of different node types, outlines, roles like full nodes, light nodes, miners, validators illustrates flow of data and transaction between nodes, and it's designed to support core functions like transaction propagation, consensus and execution. So briefly, what are intents? A colloquial definition of intents are credible commitments to a preference function over a shared state space, or more mathematically, atomic flow constraints. And here's a definition that we describe in our recent research. If you check out art anoma net.
00:47:47.022 - 00:48:18.566, Speaker C: You can find this in the intent machines report. We formulate intents as a pair consisting of transition function and a partially weighted predicate over state transitions. The guiding intuition for this formulation is to separate control from desire. Warning, intents are not magic. There's been a lot of hype around intents in the discourse, and I just want to pose this warning that skepticism is good. Read people's specs, read the code base. Be skeptical, challenge us.
00:48:18.566 - 00:49:10.150, Speaker C: Challenge folks that say they can settle your intents. So I'm going to make a claim in a sufficiently generalized intent centric system, even ethereum the game theory incentive analysis is mostly a function of the topology, not really of the architecture. If you fix the variable of the architecture, incentive structures are a function of the topology, intents, decouple architecture and topology. Intent centric architectures like Inoma provide a specific way of organizing subcomponents into a structure designed to serve the purpose of the overall system. A couple examples are peer to peer routing and transaction execution. So what do we mean in peer to peer routing? Traditional architectures have peer to peer routing hard coded into the mempool protocol. Tendermint mempool is like a great example of this, and you can see this today with some of the spam issues that are ongoing in test nuts like barachain.
00:49:10.150 - 00:50:09.542, Speaker C: Intent centric architectures separate p to p routing into a subcomponent. It allows different implementations, prioritizing speed, programmable disclosure, and other things. We also have a paper on this, so see me after if you're interested. Transaction execution it's traditionally tightly coupled with consensus and data storage, decouples, counterparty discovery and settlement, which enables different execution environments like the EVM, which we all know and love, the Solana virtual machine move, or the Enuma resource machine. Intent centric topologies roles and responsibilities explicitly define based on intents, allows programmable configurations with dedicated nodes for routing, solving, execution, and consensus, and promotes flexibility for different users communities to experiment with suitable topologies. So let's do a thought experiment. So we'll do me view market conjectures and refutations.
00:50:09.542 - 00:51:01.100, Speaker C: I had to get the word market in here, so we'll go over order, flow, trust, and opacity. These were three excellent concerns put forth by Quintus and Giorgios in a recent piece I believe came out around May or June this year. So this is a meme from sheen from flashbots. You can see incense and solvers, MeV dystopia and centralization coming in in the Trojan horse, right? So these things are cool but they come with risks. So here's an order flow conjecture. If intent execution is permissioned and permission set is not chosen with care, the migration out of the public mempool threatens decentralized block production on ethereum, this would be terrible outcome. Here's a potential refutation users monitor intent execution and switch away from bad operators by evaluating performance incredibly threatened to leave.
00:51:01.100 - 00:51:44.254, Speaker C: This would be what we call the slow game or hold accountable. You can think DyDx slashing committee due to network effects, users must coordinate to collectively switch operators as needed. Operators will try to lock in users and extract MeV trust, as many solutions require trust and intermediaries. Development of new intent based architectures is hampered by high barriers to entry, implying lower rates of innovation and competition to ensure execution quality. We already see some of this today. Trust refutation topology depends on what trust relationships the protocol can model. Anoma aims to standardize the protocol to model diverse trust relations explicitly.
00:51:44.254 - 00:52:53.306, Speaker C: Users to send intents initially to friends or local community pools, even before sending to the Coinbase mega solver, though they can still do that if they would like to. Configurable nodes for different roles should lower switching costs and making the long term incentives transparent. Opacity, as many intent architectures entail, the user surrendering some control of the onchain assets and permission memp pools imply a degree of impenetrability from the outside. We risk building an opaque system in which it is unclear how or whether users expectations are met and threats to the ecosystem remain undetected. Opacity reputation intents define their own settlement conditions, controlling what routing disclosure happens within the network back to the user intents can require signatures from gossip nodes or encryption of data and information flow control. Precise control over information flow is probably table stakes for intense systems going forward. It's not optional, so some conclusions from this talk the MeV discourse and decision making is path dependent intensity coupled protocol architecture from topology.
00:52:53.306 - 00:53:21.330, Speaker C: The architecture topology separation removes monolithic constraints if you fix the variable of the architecture. Incentive structures are a function of the topology. User choice monitoring and incentivization ultimately determines system behavior. Wait, it's all architecture topology separation, always has been. And one last bonus. See, I know what you're thinking. Actually, I've been thinking this ever since I got here.
00:53:21.330 - 00:53:27.180, Speaker C: Ethereum is an excellent settlement option for inoma from an architectural perspective. Thank you.
00:53:55.080 - 00:54:33.350, Speaker D: Hello friends. I am Tina, in case you haven't met me, there's no alternative. That said, I just want to point your attention to what I'm most excited about, because if you have any questions? What we would like to do is we will ask each of the speakers to answer. They have to answer. So ask your questions. Make it fun. Okay.
00:54:33.350 - 00:54:57.020, Speaker D: All right. It's eastglobal TV. Yes. So just go to eastglobal TV. That's where you access the side chat. And again, for all the speakers, they will answer your question in writing. There we go.
00:54:57.020 - 00:54:58.140, Speaker D: Smt.
00:55:12.760 - 00:55:21.850, Speaker B: Trying to make it full screen. Anybody know some magical f button, top left.
00:55:29.740 - 00:55:30.490, Speaker E: View?
00:55:36.470 - 00:56:15.566, Speaker B: Nice. We solved it together. I expressed the intent. Okay, so we talked about the past and the present of Mev. We said that everything is Mev, and we understand how we got there, but I'm thinking about what was even earlier. So a lot of us very idealistically came to blockchain space, thinking that what we're creating is some kind of markets for the future, for very, very distant future. The markets that will be very accessible, permissionless, efficient.
00:56:15.566 - 00:57:14.130, Speaker B: So anyone in the world, and maybe in the future outside of the world, will be able to participate in the market without any centralized players, right? And technology advances, and we're learning more and more how those future markets can look. And that's what we'll talk about. We'll talk about a bit of like a near future, and also we'll experiment a bit with thinking about the very distant future. So my name is Thomas Tanchak. I work at Flashbots and mind. So the question for everyone is, how would the market be in 3024? And it's a lot of thinking here, and I spend a lot of hours talking to Chad GPT about it. And I'll mention to you later what we found together, but as a bonus, because we went so far that maybe this would be too hardcore for the stock.
00:57:14.130 - 00:58:10.950, Speaker B: So I'll go back in time, not so far. So when we realized electronic trading practically started in 1971, not that long time ago, if I think that from 1971 to 2000, this is when the electronic trading practically flourished and became ubiquitous and used for almost majority of the markets that had significant volumes, even, just like maybe last year or so, some of the last outcry, trading peaks were closed in Chicago. So people are still going to trade floors and shout their prices, maybe even nowadays, but it's disappearing. So we're trading electronically. This is normal for us. And the transition happened over 30 years, and the blockchain started in 2009. So we can think of maybe the equally substantial transition over 20 or 30 years in the blockchain space.
00:58:10.950 - 00:58:52.274, Speaker B: So expectation is that blockchain will become the future for the markets. Well, we asked those questions as well. And a flashboys was the book published in 2014. And then we also mentioned Flashboys 2.0 2019 as bringing some concepts from traditional markets till blockchain markets. We realized that we have the same problems on a different scale, maybe in slightly different technological environments, but similar problems. And if we start thinking about traditional markets, blockchain markets, and you look at the trading, we start to use more and more often the same language, solve the same problems.
00:58:52.274 - 01:00:09.146, Speaker B: And there would be a suggestion, maybe there is some convergence, that those markets will become one. And the onchain trading, even as we say that blockchain started in 2009, we can think of history of Ethereum and through stablecoins and DFI all the way to nowadays, talking about MEV a lot as this recent history of the on chain markets, what will happen next? We'll talk probably a lot about PBS, as we mentioned already, trusted execution environment, markets like suave. You've heard suave a few times in the previous presentations. And I think a lot of thinking about the future of markets is thinking about how do we build the markets for AI? Will we build the markets for AI or AI build the markets for themselves? And that bonus thing will be interplanetary markets and GPT conversations. So when I say the same problems that we face in traditional financial markets and the blockchain market, so how many of them did we solve on blockchain? Because when we come to blockchain, we say blockchain is revolutionary. It provides this immutable ledger. It provides permissionless access.
01:00:09.146 - 01:00:56.010, Speaker B: It provides like never stopping virtual machines that keep pushing things and nobody can stop them, right? They're decentralized, so we provide decentralization. We talk about cryptography and blockchain, giving us the ability to encrypt things and decrypt them conditionally. But the problems of trading is like liquidity. Liquidity problem remains the fragmentation of liquidity across different venues, different chains. Now we're talking about the bridging problems, the limited access problem of the markets. Like if the markets were some of the most developed markets would be not available for any nondeveloped countries. And I'll argue that much of those problems would just move to blockchain because they are not technical problems.
01:00:56.010 - 01:01:30.150, Speaker B: They are very often the problems that exist outside of technology in the real world. The front running and latency problems. Well, blockchain started with 15 seconds blocks on Ethereum, now 12 seconds. Obviously, front running and latency will both matter. Scalability will be probably the problem of any system that tries to keep growing and giving more capacity for trading market manipulation problems. Well, this is not related to technology, cybersecurity risks. They stay probably to some extent.
01:01:30.150 - 01:02:14.510, Speaker B: Well, we have constant hacks of the smart contracts. The systemic risks I think very often we haven't seen yet, like the proper systemic risks on blockchain. And we may see them with some of the major stablecoins collapsing or the staking restaking, leading to some major collapses around Ethereum and major protocols. So we've seen things like FTX happen. I think they were just very, very small comparing to what is awaiting us and what we'll have to deal with. Let it be like the chain, major chain like Ethereum stopping because of the major issue of the clients, insider trading. Well, this exists outside of chain.
01:02:14.510 - 01:03:01.934, Speaker B: Can you hard code on chain in smart contracts, the protection from insider training, operational risks. Well, again, like deploying of smart contracts, problems with executing the tech. So many problems stay and we'll have to solve them together. And we'll solve them either through some technology improvements or through regulation. And probably AI systems will be helping us with that, both in the traditional markets and blockchain. So once again, blockchain solutions brought permissionlessness, notarization, decentralized encryption. I say decentralized encryption because when you think about things like trusted execution environment, SGX, any encryption models, well, you can use them in traditional markets.
01:03:01.934 - 01:04:49.026, Speaker B: When we think about SWAF, could you deploy SGX system that would allow you to deploy algorithms attestation to the algorithms used and allow people to trade within that system on a centralized market? And you would have also a promise that nobody modified the program because you have exactly the same promise of attestation. Well, you do that on blockchain in decentralized ways, there's some improvement, but it's worth always to come back and think like which problems we really solve. Composability on blockchain for sure, this is a great improvement in a sense that we bring the layer that is common for everyone through Ethereum models, but at the same time we bring multiple different chain models and bridging, and solutions like Solana and Ethereum will be very much incompatible. So we're losing composability very often through the competition. So is the problem with technology or just the competition by itself causes composability trouble? Could we provide all of those without blockchains at the same time? Is the blockchain enough to ensure the problem solving? Coming back to a few things from the past and how they change nowadays. So I was reading how the open outcry model worked in a sense that when I started looking at those videos of people shelting prices and the signals that would show to signal the trades they wanted to execute, right, they were showing prices and showing the quantity. And I thought like, well, so was front running there, like, was it a public mempool, practically, as you could just look at all of this and all transactions were in the open.
01:04:49.026 - 01:06:06.462, Speaker B: And this seems intuitive, natural. I asked the questions in my conversations with chat GPD, how would you avoid it? How would you protect? And there were regulators and there was monitoring and people were observing, and also the fact that everyone was trading in public was assuming that everyone was putting their reputation at stake. So you didn't hide your identity, right? So everyone was looking who is behaving not the way that they would like to, and they would block them from the market or invoke the regulatory action against it. And in a way, as blockchain community, like the MEV community, we started looking at a public mempo and started thinking of, well, who can we identify and call them out as their reputation is at stake? And very often when we look at the market makers, we think that. And Nathan mentioned that just be nice in a sense that they don't expect people to be nice, but that reputation systems work as the social consensus model to much extent. So once again, we didn't solve that by technology, but by the sheer nature of human interactions. So once again, a question.
01:06:06.462 - 01:07:33.610, Speaker B: What will happen with the AI markets? Will you have there the risk of reputation? Can we introduce the blockchains, AI, the concepts of reputation, that will be strong enough so we can trade within these systems and regulate them efficiently. Now, the dark pools, this is probably as well in Budapest, just less people. So dark pools were introduced, they're operational in the traditional financial markets, right? So this was the answer of how to ensure the protection, the privacy of trades for the large orders to be executed. They got bad reputation and then invoked regulation because, well, once again, don't expect everyone to play nice. If you create a dark pool and everyone believes that you'll be playing fair, because you provide, as an alternative trading system, you provide this opportunity to execute in private and algorithmically execute trades in chunks and so on and so on. Well, at some point, maybe because you're a centralized entity, maybe not you, but the manager that comes after you, the next CEO or the next operator of the dark pool, maybe they'll start doing something bad just for the profit, maybe they start front running you or reordering and whatever. Now this is, once again, can we solve it without blockchain.
01:07:33.610 - 01:08:39.060, Speaker B: The same entity can run SGX and can run the model that will allow you to trade. And now you have attestation of the models and you have encryption. You don't need blockchain for encryption, but you lose the ability to decentralize it. And via blockchain, we have to ask a question. Is decentralization absolutely beneficial to those systems? Like, is it enough to make them so much better than just moving SGX systems and those models to the traditional markets? So do they converge and say, like, how much of the centralization I need and how much of the latency of traditional markets and the experience of executing trades at volume, low latency processing we need? And then I ask about the censorship resistance of blockchain. Right? So we look at blockchain as a whole, and we think that it should be censorship resistant. We should prevent blocks from excluding some of the transactions.
01:08:39.060 - 01:09:50.520, Speaker B: If we think about the look at the market in general, then the questions that we ask that lead to censoring are like, who can really trade? So is that person, is that institution allowed to trade or they banned by some regulator? What can be traded? Because whoever may be banned from trading some specific goods or services, we may want to obviously want to avoid human trafficking and so on, how can it be traded? So even if you trade something that is perfectly legal and you yourself as an entity, you're not coming from the region or institution that is banned by the regulator. The way you trade might be banned. Like you can be front running and so on. So all of this we want to regulate. And a permissionlessness doesn't really help that much with it. It makes it much harder, which means that the big volumes and big markets don't want to play that game because it's risky for them. So is blockchain solving that problem for them and giving them best liquidity, best place for their liquidity.
01:09:50.520 - 01:10:27.012, Speaker B: And there is a big chance that what will happen on blockchain will be stratification of the market. That nowadays we look like on the left, right, we're just like a gray area. You join blockchain, you know that you're trading somewhere around the shady things. Everything is mixed together. And that's why very often blockchain is targeted by the regulators. They say like, blockchain is bad place because you cannot avoid trading the dark market there or like being even commingling assets. And what we see from institutions now is that they want that second thing.
01:10:27.012 - 01:11:16.500, Speaker B: Like they want you just put it in layers and say that for anything that I trade on this blockchain, I want to make sure that my assets will never touch that black market area. And in a way, this is good, because you say that blockchain is technology in this format. It's a platform that universal for both black and white markets. And there is somehow acceptance, probably from the institutions that we operate on the same market, as long as we separate it. Because if you accept this as technology, it's unavoidable to use that any separation, that any other separation would look like this, you would have to build chains for the good things, chains for the bad things, and this wouldn't change much. It would be just a bit harder to operate. It would be even more bridging and more pretending.
01:11:16.500 - 01:12:01.840, Speaker B: And how to efficiently build this? Well, I think this will happen. Will it be efficient? Probably will be very tiring. But I don't think we can avoid KYC on blockchain. This does not necessarily have to introduce centralized regulators, but it will create the layers on chain of, just like locks of how you can transition between the layers. You will always be able to move your assets from the white market to black market. Right? You can do that for some kind of burning bridges, but you will not necessarily be able to come back because some players will say, I don't want to trade with you. I want to exist in that space that is regulated by self regulated.
01:12:01.840 - 01:12:59.720, Speaker B: Even so, you don't need to have centralized regulators to still arrive at the market. That will be KYC and censoring. Because censorship, in the end, is a natural function of our social interaction. Some things we accept, some things we don't. And with all of this and introduction of AI agents, I will ask, will we have a AI regulator, some AI systems that will deploy within those dark pools, that will act on behalf of deciding what is right and what is wrong. So definitely with the new technologies, we can say that not only we designed this AI regulator, but we want to make sure that it always behaves in a predictable way, that it's not corrupted by someone not having bias. So for sure, the technology around ZK, ZKML, probably verification of model execution, will be integrated into the dark pool.
01:12:59.720 - 01:14:20.610, Speaker B: So I'm saying, yes, everything probably will happen on blockchain, but not for the permissionlessness, not for the accessibility without KYC, not because of encryption, not even because of decentralization, but probably only and only because of the function of the general ledger and authorization. And sometimes we think that all of these other things are just intermittent. The fact that everyone can access it is just temporary, because in the end we simply separate always the world into layers and we decide who will be allowed to which layer and temporary blockchain is just this mix that allows everyone, but it will not be probably forever. And even if you can access blockchain, but you cannot access 99% of the liquidity on the blockchain, can you really access anything? Not necessarily. So I think the blockchain will be there in 1000 years, probably looking very different. It will be there mixed with some crazy encryption models, with some AI, maybe without us anymore. It feels like blockchain is very natural solution for AI to just talk to each other, to trade, to verify each other's actions and to look at the history.
01:14:20.610 - 01:15:03.330, Speaker B: So does it mean that there'll be convergence? Yes, because it feels unimaginable that there'll be no need for decentralized trusted ledger. And all the other functions can be built on top. And all the other functions already feel natural for the financial markets. So the blockchain itself will be lowering latency. We'll be introducing more and more scalability and capacity. We'll be stratifying the market into multiple layers of how good the market is. And traditional finance will be tokenizing everything so it can be digitally traded because no other trading models will be efficient enough.
01:15:03.330 - 01:15:34.250, Speaker B: And at bonus that I mentioned. So what Chad GPT told me, well, for 1000 years forward, you have to start thinking about time relativity. So if you're running a hedge fund, maybe you want to run it on a very fast moving ship. And how do you really agree on time if your nodes are in different time systems? So it's interesting read. So try to have this conversation as well. Think about 1000 years ahead. Thank you so mUch.
01:15:34.250 - 01:27:27.250, Speaker B: The types of deals that you're willing to engage with, particularly around order flow. Question to you, Joe, because you said shadowy. But also to anyone else on the panel.
02:25:35.590 - 02:26:05.770, Speaker D: Please find a Seat. Okay. And also I would like to ask for help to close the door as we are starting and the live stream is now back on. Also just a couple PSA. Our lunch will be served at one starting one. FeeL free to go at your own leisure. It's on the fourth floor cafeteria.
02:26:05.770 - 02:26:11.100, Speaker D: And so I think we have another hour of content.
02:26:13.010 - 02:26:13.806, Speaker F: Yeah.
02:26:13.988 - 02:26:31.140, Speaker D: Please prepare your questions ahead and log on to eastglobal Tv site chat for questions. And we would like to ask all of the speakers to respond. Async in the side chat. Thank you.
02:26:36.390 - 02:27:43.910, Speaker F: Okay, great. So hello everyone. Gm and I guess g afternoon for the past nine minutes. So about a couple of years ago, I wrote this post called endgame, where I basically pointed out that a lot of the different blockchain scaling paths that people were thinking about, including layer one scaling, including different forms of layer two scaling, if you push all of those paths to their limit, and if you add the kinds of scaffolding that you need to actually make them sane, from a decentralization and censorship resistance perspective, they actually end up in a surprisingly similar place. Right. Basically a place where you get centralized, what I called block production, though I think it's also actually possible to think of it as being something that's much more restricted than production. Then you have decentralized validation and you have strong anti censorship protection.
02:27:43.910 - 02:28:40.442, Speaker F: Right? So this is a post that came out about over two years ago, and since then, I think we've gone a lot from theory to practice. And one of the ways that I think about how a lot of these things look like in practice is basically that how ethereum scales is an industrial organization problem as much as it is a technical one, right? So even if the same pieces get built, different choices in who builds them can lead to big differences in outcome. So, analogy, the efficient way to serve food at scale across a country is to have many locations. In each location, food gets made in a central kitchen and people can come and order it. How many people here have been to one of those in the past year? Okay, not many hands raised. Interesting. But is each location a branch or a franchise? Right? Branch is like commands and control, like arranged from the top down.
02:28:40.442 - 02:30:07.062, Speaker F: Franchise is like, anyone can come in and say, yo, dog, I have a location, I'll pay a fee, and I want to use your logo and serve your stuff. This is a kind of subdiscipline in economics, and it can really try to explicitly think about the trade offs of even if the same kinds of pieces get made, what are the actual consequences of different pieces that are happening? So, going through a few examples of this, right, so first example is the state transition function, right? So state transition function is basically just if Ethereum scales, then you need to have lots and lots of transactions. And with a lot, these transactions are being processed by a set of rules. And these rules talk about how accounts work, how smart contracts work, and so on and so forth. And these rules have to be enforced somewhere. Right? Now, Ethereum, for the last few years has had this roll up centric roadmap where basically Ethereum as a blockchain provides shared security, and it provides data scalability through things like EIP 4844, which we're going to have in I guess about 25 hours. Then you have roll ups, which are these layer two protocols that actually handle execution and all of that infrastructure on top.
02:30:07.062 - 02:30:53.442, Speaker F: Right. Now, theoretically, you can achieve the exact same outcome by basically just like having a big system that has kind of monolithic shards inside the protocol, right? And there are other platforms that do this. In the case of Ethereum, realistic possible futures, I think there's basically three that I can think of. And basically one is you have different roll ups, and each of those roll ups makes their own highly customized state transition function. And this is a major basis for people to compete with each other, right? So you have fuel, which has been stage two for a while and deserves praise for that. You have arbitrum and arbitram stylus. Then you have the whole starknet ecosystem and Cairo and their proof based VMs.
02:30:53.442 - 02:32:23.090, Speaker F: You might have even more in the future. Another option is basically roll ups generally reuse the EVM. And a third option is that roll ups use a hypothetical Ethereum layer one EVM pre compile, right? So this is a post that I wrote about a few months ago that basically talks about this idea that maybe Ethereum should include a ZKe EVM pre compile as part of its kind of layer one functionality, right? And if we do that, then basically it comes much closer to a world where we have what was called sharding, like ten years ago, and including execution sharding. But as a roll up, you're basically responsible for spinning up your own shard. And if you do a good job of attracting applications to your shard, then you get to collect the priority fees in your shard, and you get to make money that way. But these are very, from a technological perspective, there's a lot of similarities, but from a perspective of who builds it, you have a bunch of different consequences, right? So, for example, if roll ups make their own STF, you have more options for users, faster iteration in VM technology cons are greater software bug risk more confusing for users? If you want to make it easier to understand for users, then fine, everyone uses the EVM. You have somewhat less bug risk, but you still have some risk of bugs, but also less room for creativity.
02:32:23.090 - 02:33:41.658, Speaker F: If you want to really minimize the risk of bugs losing your money, then Ethereum l one EVM pre compile can do that. Basically, the more you enshrine, the less application code attack service there even is. There's also no need for roll ups to have governance, because they just kind of automatically upgrade when Ethereum upgrades. But there are trade offs, like in this case there's actually efficiency trade offs enshrined ZKVMs do involve some actual technical sacrifices, right? And IAM also talks about them in the post. Right? So you have these different options, and they actually have some interestingly different consequences. But a lot of these consequences, they don't appear on usual axes like scalability versus decentralization. They appear on these weird new axes that we think less about, like speed of innovation of different things, risk of software bugs, and the extent to which different pieces of the system need to have governance, right? So, like, for example, if you're the sort of person who loves the idea of Ethereum ossifying, and you're totally afraid of Ethereum governance doing crazy things in 2029, then going up to the next two columns, and especially the top column, is good for you.
02:33:41.658 - 02:34:47.114, Speaker F: But if you're the sort of person who actually believes off chain governance is best, and you're afraid of any of these weird gadgets that have inside of layer twos, then the bottom thing is actually best sequencing, right? So procedure for determining which transactions go in the next roll up block. So independent sequencing, every roll up does it for itself. Shared sequencing as a piece of layer two infrastructure, and then you have based sequencing and all of these drachian ideas that try to bring layer one and layer two together. Yeah. And if you look at just kind of what the trade offs of some of these are between independent and shared, the big difference is basically, if you do independent, then you get a risk that shared sequencing happens anyway through side channels. And then if you do shared sequencing, then there's greater risk that there's some kind of ecosystem wide binocularization. If shared sequencing is a layer two infrastructure, there's a risk of shared sequencing protocols becoming rent extractors.
02:34:47.114 - 02:36:16.410, Speaker F: If you do it at layer one, then you have a different risk, which is that if something weird happens at that layer, then you get more risk spilling over onto layer one. Right? Actually, this was something I also wrote briefly about in my post kind of almost two years ago on April 1, praising bitcoin maximalism. And I basically talked about how the thing about bitcoin simplicity that makes sense is not like the technical simplicity of the VM, because you can make the VM way more performant with only a tiny bit more technical complexity. It's like having applications is bad, right? And the reason why having applications is bad is because applications create weird stuff like MEV, and the centralizing effects of MeV spill over to the base layer. And basically, if we want ethereum itself to be maximally protected from all that. Then you have sequencing be very far away, so that whatever centralization, whatever weirdness happens that gets collected by outside actors and whatever gets pushed on tail, one becomes relatively time independent, right? So that's one example of an economic incentive. And then of course, the flip side is which of these protocols actually gets to collect the MeV? And this is one of those trade offs between basically sequencing being based and sequencing happening at some higher level, right? So from a technological perspective, you get similar things happening, but a lot of the consequences, again, are economic.
02:36:16.410 - 02:36:53.846, Speaker F: They're about codependency, whether you're basically leaning on l one or doing things separately from L one. And these kinds of issues. Proof aggregation, right? So roll ups need to publish proofs to chain a proof is about 500,000 gas or something like 5 million gas. If you are stark, this is a lot. It would be really amazing if every roll up could just publish an update every block. But at this point it feels like everybody is just launching a roll up. And so we probably have more than 60, like 30 roll ups at this point.
02:36:53.846 - 02:37:42.710, Speaker F: And so we have too many for the gas limit if they want to publish every block. But the status quo is that every roll up figures it out for themselves. Simplest code, minimum trust dependency is high gas cost. But what if we have aggregation protocols, right? Basically, instead of 20 roll ups publishing 20 snarks to chain, you somehow publish one snark to chain, right? And we have aggregation protocols. And there's different ways in which this could happen, right? So one way in which this could happen is you have ethereum ecosystem wide proof aggregation. So basically you have a protocol where roll ups can opt into this, where basically they submit their proofs into a mempool. And then it's the role of builders in the mempool to just take these proofs and then make a proof of the proofs and then publish a proof of the proofs along with the state routes.
02:37:42.710 - 02:38:52.206, Speaker F: And then a contract just makes one call for each roll up and you only need one proof, right? So very good gas savings, some level of opinionated choices, some amount of shared entrusted code. Another way in which a very similar thing is happening already is like we're seeing aggregation within ecosystems, right? So like Starknet has their own aggregation. You have layer threes, and then the layer threes commit into layer two, the layer twos commit into layer one. And so ultimately there's one proof. And so you get the same effect of proofs inside proofs. But if this is a thing that individual roll up ecosystems, do separately, then this is good for large ecosystems, but it's less good for small ecosystems. And you have to deal with more trusted code account abstraction and key stores is another one, right? Basically we have wallets and one very basic security property that's considered totally standard in regular cybersecurity is like you want to be able to expire keys, right? Like you want keys that control an account before, to no longer control an account later.
02:38:52.206 - 02:40:36.480, Speaker F: This is something that EOAs do not do. And it's like one of the five fundamental reasons why in the long term, EOAs need to die, right? But if you have some account abstraction based setup, then basically you have to store the information somewhere of things like which keys currently have the right to process transactions from an account, right? And the question is like, well, where does this data get stored? So the status quo is basically if you have a smart contract wallet, you have a copy of that smart contract wallet on every single L two, right? And this is like fairly simple code, fairly minimal trust dependencies. But then there are also obvious costs, right? And the cost is like, well, if you change the keys in one place, then are the keys still going to change in all the other places, right? And you have high gas cost of changing all the keys around in different places. And there's also a risk that you'll just accidentally forget somewhere, right? It's like you have a gnosis safe and you change your keys in three different places, but then you just totally forgot that you have a copy of the same safe somewhere else. And then you forget to change the keys. And then two years later, one at a time, the old key holders get hacked or someone else kind of goes after them, or they think that everyone else forgot about them and they just go and steal the money. There's like cons of the current approach, right? A very natural approach is, well, instead of having that data live in every place, we'll have the data live in one place, right? So either use l one, but l one is a bit too expensive, use some l two to store this information, of which keys currently have the right to access some account.
02:40:36.480 - 02:41:41.486, Speaker F: One idea that I talk about is like this minimum key store roll up proposal, right? Basically you create like this very minimalistic roll up whose only function it is is to store this account related logic and to store the rules for updating it. So basically trying to keep the thing pretty simple, trying to keep trust dependencies pretty low, requiring some standardization. And then if you want to update your account, then you'd only have to update it in one place. And then basically every time you sign your transaction, that transaction would include a synarc that proves the current status of your account. And this is something that is going to become cheap in the future because we have proof aggregation, but there's still a question of who builds it, right? Another option is, of course, we just kind of let users pick, base their accounts on whatever chain they want, and you just put your key store there. You store the logic of who is allowed to access your account. And just like whatever chain you want and different users do it differently.
02:41:41.486 - 02:43:59.358, Speaker F: There's like tech blow up risk here, right? So you have like an N squared tech blow up because you might have n different types of proofs for n different roll ups, you have more trust dependencies, but on the other hand, it requires this less shared infrastructure. And it's the sort of thing that's maybe more likely to happen anyway by default, right? So you have approaches that are very similar, but then you have differences in practice depending on who does it. Right. I guess the conclusion of all these different things is basically that once you start getting into the weeds, there is a lot of these choices about which actors are actually responsible for which pieces, right? And if you go back to the endgame post and you think about how, from a technological perspective, all of the different things feel like they're lining up toward the same conclusion, but what's the big difference between, let's say what I call an ideal l two scaling future versus what I would call ideal Salana, where you actually have consensus nodes being able to run on laptops and have sensors, inclusion channels, and have starks for everything? Well, the answer is basically you have this boundary of which actors are responsible for building and maintaining which parts of the ecosystem and what their incentives are, and across all of these different areas, the difference between those things matters a lot. Right? So basically you have two separate questions, right? There's always the tech arrangements, and then who's responsible for building and mutating what? And probably the biggest actors that we have are like, one is independent layer, two teams, another is basically standards groups. So you have the roll up improvement proposal group, you have EIPs, you have ercs, potentially more standardization in the future. And then you have l one core protocol teams, right? Depending on which of these pieces is responsible for building things, you start to have some different consequences.
02:43:59.358 - 02:44:56.290, Speaker F: I think the main axes on which the consequences are different on all of these is basically one, who has the incentive to build the thing? Two, is there an incentive for people to try to get a monopoly and start extracting rent from the thing. Three, kind of where does the code bug risk lie? And how much risk of code bugs is there? And probably about a couple of other questions. Right. And so I think in a lot of these questions that we have in the future, it's going to be a similar thing. Right. It's like, even if the final outcome is the same, the path for getting there sort of also defines these boundaries between what kind of actor is responsible for what. And this will decide a lot of the details.
02:45:00.070 - 02:45:01.620, Speaker B: Yeah. So.
02:45:08.730 - 02:45:30.510, Speaker D: In the, in the interest of time, we would like to ask Vitalik to answer questions from in the site chat of eastglobal TV. And if you don't know how to spell it, it's on the top right there. So thank you. Next, we have a panel, and you are also one of the panelists.
02:45:30.590 - 02:45:30.974, Speaker F: Indeed.
02:45:31.022 - 02:45:32.450, Speaker D: Maybe you'll be standing.
02:45:32.870 - 02:45:36.100, Speaker F: Okay, how many chairs do we need.
02:45:36.470 - 02:45:38.040, Speaker D: If you choose to stand?
02:45:38.650 - 02:45:39.206, Speaker F: Oh, I see.
02:45:39.228 - 02:47:14.920, Speaker D: It's a standing committee, but if needed, I can, I think there's one more chair over there. Yes. Wait, which side?
02:47:18.860 - 02:47:19.930, Speaker G: The next one.
02:47:23.360 - 02:47:23.820, Speaker D: Okay.
02:47:23.890 - 02:47:25.084, Speaker B: I think it's fine.
02:47:25.282 - 02:47:26.030, Speaker D: Okay.
02:47:26.640 - 02:47:28.030, Speaker B: We can't move this.
02:47:32.760 - 02:47:59.070, Speaker D: Okay. So I'll give them, let me just get it started. Hello, friends. If you want to eat lunch at some point, please, let's get started. And again, ask your questions live on eastglobal TV, please. And we can interact async. Thank you.
02:48:01.200 - 02:48:24.896, Speaker B: Hello. Is this working? No. How about now? That's good. Okay, cool. Welcome, everyone. We have a bit of a collection, a variety of people on the stage today. I would classify them into two groups.
02:48:24.896 - 02:49:19.328, Speaker B: We have one group of people that are sort of thinking about protocol designs, putting together many different kinds of nodes, like big nodes, small nodes, as we were just discussing in the previous talk. And then we also have Lev, who's operating one of the big nodes. And so it gives a bit of a different perspective there. And the idea today is to sort of flesh out some of Vitalik's questions and generally try to understand how people are thinking about this heterogeneous endgame where we have different nodes playing different roles, what are the desired properties of the markets between them and how they interact, and what are the kind of actors who fulfill these roles. So I'll maybe ask everyone to just give a quick intro to your name, your affiliation, and then sort of what kind of nodes you'd say you get to do with. Are you operating a node? Or if you're not, if you're running a protocol at the very high level, what are the kind of nodes involved? Maybe starting from Lev.
02:49:19.504 - 02:50:05.460, Speaker F: Hi, I'm Lev, I'm from Beaver build. As you guys probably have heard, block building involves not only running normal nodes, but also being a node in the sense of being responsible for what transactions get executed and included. Hello, I'm Mustafa at Celestia in terms of nodes because it's just a DA layer, so there isn't much Mev right now unless people build base roll ups. So there's no proposed builder separation, there's just no normal validator nodes. But there's also a big emphasis on data availability sampling light nodes, so that even though tenement only supports 100 validators, the idea is that end users can run data availability sampling nodes to kind of get assurances about the state of the chain.
02:50:07.080 - 02:50:29.820, Speaker E: Hi, I'm Josh at Astria. We're running a shared sequencer. So generally the nodes we're worried about are like sequencing nodes. But my perspective is that in a lazy shared sequencer they kind of look like relays in like a PBS system. And we kind of know obviously it's like early, but we assume the ecosystem will develop into a PBS system. And therefore the important part of our system is to run large nodes with like relatively high bandwidth.
02:50:30.560 - 02:50:56.260, Speaker F: Hi, my name is Yem, I'm from scroll. We are building on Ziki EvM layer two. So we have not super controversial, we have two type of nodes. One is sequencer, one is prover. Our sequencer is not as lazy as because we need to actually order transaction, execute transaction, and post data to EVM layer one. And then later approver will kind of generate proof for each block and then submit a proof and then layer one, verify the proof. So it's like non controversial layer two construction.
02:50:56.260 - 02:51:45.968, Speaker F: Hey everyone, my name is Joe from Aztec. We're building a privacy focused layer two on Ethereum, also trying to make it decentralized. So we have a few different nodes in that peer to peer nodes transaction pool, similar to scroll sequencing nodes. We care a lot about those being permissionless. So I guess they're small nodes. And then on the proving side of things, we'll have some slightly larger nodes for proving marketplaces filling in to actually build the proofs. Yeah, I guess in Ethereum, if I had to categorize it into three types of nodes, I would say like specialized nodes that do various kinds of aggregating or proof generation and witness generation, then you would have nodes that are participating in consensus as validators.
02:51:45.968 - 02:52:00.490, Speaker F: And then just like nodes that any user runs to verify the chain, which I expect in the long run, we'll just be verifying one proof and doing one round of data availability sampling, though it's still a while until we get there.
02:52:01.100 - 02:52:29.040, Speaker B: I'm miced up. Thanks. Okay, so Joe, I think I heard you mention both permissionless and markets in your response. And so when we look at these different sets of nodes, there's usually a couple of selection processes involved. One selection process is deciding which nodes are even part of your system. And then even within that, there's like a delegation of roles. So for example, in Ethereum, you select a proposer, it's like a consensus leader that produces blocks.
02:52:29.040 - 02:52:55.550, Speaker B: And then on top of that, we've built another market where there's now some selection of a blockbuilder. And at least it made a big impression on me. Last year, John Shaw made this argument for proof of governance. He was saying, if you look at Lido, look at these staking pools, eventually just ends, in an abstract sense, selecting nodes. So I'm curious, how do people on this panel feel about the role of governance and the trade offs between governance versus something that's more permissionless or looks more like a market?
02:52:57.040 - 02:53:30.544, Speaker F: Yeah, I can start, and I think depending on the properties you need from nodes, like if you're going for censorship resistance, you need to have permissionless kind of entry into that. And I think everything downstream of that is a market design question to try and keep things in an equilibrium that stay that way. And we saw that on the previous panel where if we get that wrong, maybe you get builder centralization. But I think aligning kind of censorship resistance and permissionless entry into a market is a key kind of criteria for node selection.
02:53:30.592 - 02:54:34.140, Speaker E: For me, I think our view would be kind of like we're trying to go from centralized sequencing to decentralized sequencing. And so one of the fundamental trade offs is just going to be, you're running a consensus network, you're going to be slower, you're going to have higher latency. So we want to optimize our operator selection, our node operator selection for quality, right? Like the ability for these entities to run a high performance, high throughput network, so that the trade off cost of going from a centralized network to a decentralized network is minimized. And so in some way it's kind of this not making perfect the enemy of good, where we would prefer to not have a permissionless system that leads to just like stake weighted kind of like selection, where maybe they have a lot of stake but they're not good. At operating nodes, and therefore the network is like lower performance. And we'd prefer to do something like proof of governance where you're saying, well, we're actually manually picking nodes, we're getting the decentralization from like a geographic perspective, we're making that permissionless trade off, but we're also having a higher throughput network, and we think that allows us to kind of go into this kind of decentralized versus centralized trade off with a lower cost to an end user.
02:54:35.680 - 02:55:21.956, Speaker F: I can also add more like even if you are not decentralized yet. So on our roadmap, we are committed to decentralized. But the way we think about how we select different parties like sequencer and prover, is that sequencer and proverb have totally different reason why they want to be decentralized. Because for sequencer it's more for like you want permissionless, you want, no one can censor your transaction without going through the layer one to kind of paying that costly cost. And prover is more for liveness, because what if one day we kind of start to go away? It's very hard to kind of assemble a lot of really strong computation power prover to kind of generate proof and keep your network running. So it's totally two separate roles and so they have totally different requirements. And the way we think about how we select role rule is like how much want to do in protocol? Because there are two ways to design a protocol.
02:55:21.956 - 02:56:02.300, Speaker F: One is that you can imagine there is just one node running the row of a proposer, where you have only one type of proposer, and then you select this proposer will generate block and also produce block produce proof. But even in this case, you can still delegate the block building to a separate building market builder market. And then you can outsource this kind of proof generation to a separate proof market. So those two roles are still separated, but they are kind of in the protocol. You don't need to worry about who is running a builder, who is running approver. You just select one party who you trust and will submit the data and approve. And the other choice is the protocol will select both a sequencer and proverb.
02:56:02.300 - 02:56:33.280, Speaker F: And that might require a very complicated design because you need some slashing for sequencer who produce maybe bad blocks. You might also want to punish proverb if they are not submitting proving time. And that makes the incentivized economic become more complicated and makes the kind of flow become a little bit more complicated. But that's basically how we are thinking about how we are selecting different roles and how we decentralize our protocol. And for our roadmap, we will decentralize proof first to be safer, and then we still control the block generation, and then later we'll decentralize both parties.
02:56:34.260 - 02:57:08.220, Speaker B: Yeah, maybe throwing this to Lev, you're in a position where it's a very market based selection mechanism. Right. And we had like a brief discussion this morning around what, and the previous panel also highlighted this. Having this kind of market ends up changing the role that a node might play. Originally, blockbuilders just had a very specific function in Mevboost and the PBS roadmap. And then on top of that, additional features came around. People started implementing private RPCs.
02:57:08.220 - 02:57:18.930, Speaker B: And so I was wondering if you could maybe give some insight into that. And then I'd be curious if that changes people's perspective on whether you want a permissionless market or anything like that.
02:57:19.620 - 02:58:27.380, Speaker F: Yeah, so I think as far as we're concerned, and I would kind of speak to both the specialized node part and the sort of normal user node part here, not about consensus nodes, which I think are very governance. I describe them as like governance constrained. Like governance explicitly reasons about what they're meant to do and tries to. And also in terms of the code that's shipped by core devs is more or less what consensus nodes end up running, while everything else is this kind of wild west, which is, I would say, like, not really constrained by governance, and it's really governed more by market forces at this point on Ethereum l one specifically, and I think that this can have a lot of unforeseen implications that often are not discussed in governance at all. So, like, which APIs do users use to submit transactions? Which APIs do users use to interact with state, and which parties are able to provide those APIs. So how do block explorers work? How does a user simulate a transaction in order to figure out state to decide what transaction they actually want to do? These are things that very rarely come up in the governance context. And have, all of these have market.
02:58:27.380 - 02:58:51.550, Speaker F: It's not just a block building that has market forces behind it, but also providers of RPCs. That's a business model. Right. And they have specific node requirements that are very different to builders, for example, all of these things, there's market forces behind it. And I think people used to talk more about historical state, for example, stuff and state rent than they do in this conference, but that's a big part of it as well.
02:58:54.080 - 02:59:10.400, Speaker B: Yeah, just to add to that, maybe Kevin, your colleague earlier was talking about the sort of trade offs of including blobs or not including blobs and how market forces actually push you to maybe not include blobs. That's maybe something that's different in a proof of governance setting.
02:59:10.560 - 02:59:17.540, Speaker F: Yeah, I think that gets very interesting probably with things like blobs and also proof aggregation that was discussed in the previous.
02:59:18.120 - 03:00:05.670, Speaker B: Yeah, yeah. So Ezekiel proverm markets came up earlier and I don't know much about them. So maybe you can educate me. But one thing that I wonder about sometimes thinking from PBS is the impact of closed source optimizations. So if you have a market, obviously like markets can have many kind of selection mechanisms, but generally there's some selection for a more efficient prover. If someone has a hyper optimized, they've got some breakthrough with FPGAs and whatnot, hyper optimized prover, they might end up dominating your market. So I'm curious, maybe just generally in these kind of node markets, but in particular in the prover setting, what do you guys think are the sort of the properties that you want from the market?
03:00:07.080 - 03:00:50.352, Speaker F: Yeah, I can talk first from my perspective. So again, go back to why we want to decentral prover is because we want liveness, because even if we are not operating scroll network proofer can still generate proof. So that's why we want this kind of poo market. And there is always a backup for someone to generate proof. And it's a very interesting question because different from mining proof market is quite different because if you get ASIC, you literally can beat any prover in the market. You can literally become the best prover and take all the kind of the power to produce block and which I think right now at this stage is actually a good thing because we are kind of incentivizing the proverb technology to improve faster and faster. Because right now the proof generation takes around like five minutes or something.
03:00:50.352 - 03:01:54.328, Speaker F: And we want that to reduce to 10 seconds to achieve real time proving and then have some interability and all those kind of nice properties we want to have. So I think that's why I think starting from incentivizing fast approver, but eventually becoming like we cite some time window as far as you kind of submit proof within that reasonable time window, then you can get rewarded, will be a more fair way to kind of always, because if your system is always on by the fast approver, they can always leave and then your system gets more vulnerable. And then in term of this we want to trying to kind of firstly incentive a faster prover to improve overall the ZK ecosystem because you need an order of magnitude improvement on the zk performance. But secondly, then eventually move to a more fair and then more people get opportunity to generate proof instead of just one party dominate the market. Yeah, I guess I have some thoughts. I think Mina maybe pioneered this idea of a maximum proof delay and they have a proving marketplace on their chain. But the rough idea is that you have to be careful in market design of what you optimize for.
03:01:54.328 - 03:03:20.180, Speaker F: If you optimize for fastest proof wins, then you'll create huge centralizing forces around that. But if you actually optimize for cheapest proof proof in 1 minute wins, then you may be able to enable everyone in the room who's got a lower cost of hardware or electricity to participate in that marketplace. So the rough idea behind approving marketplace is that you can have maybe a more centralized coordinator that can work with a lot of different kind of provers to make a proof for cheaper, but on the same long time horizon. So I think over the long term, blockchains are very good at coordinating, and so we should see that for proof construction as well, rather than centralized markets, right? Yeah. Well, the nice thing about proof markets, if you do it based on an auction scheme, is even if someone dominates, you can show that they're not going to dominate exactly 100% of the time. And the reason basically is that if they are dominating 100% of the time, then it's in their interest to keep on bidding less and less to make more profit until there is at least an epsilon of a window for someone else, which is something that it does mean that a lot of profit goes to one actor, but it at least creates this incentive to be the backup guy and at least be around for that. But that's something that you can kind of tune up or down.
03:03:20.180 - 03:04:04.160, Speaker F: And if you can create a market design that's sort of in principle more friendly to multi actor participation, then you might even be able to kind of encourage better quality in whoever those secondary actors are. One example would even be if you just explicitly break up the claim into different parts and ask different provers to provide what those different parts are. Another way to do the same thing is like backup blocks. You just kind of built into the protocol market. You always incentivize a separate backup block to do the same thing. It will have less reward, but you'll always then at least have two proving marketplaces compete and you'll have liveness, as.
03:04:04.230 - 03:04:05.520, Speaker B: You were saying, at scroll.
03:04:06.500 - 03:04:37.020, Speaker F: Just one thing to add, because Justin actually had another interesting idea, which we can pay money to buy Ziki ASIC because that will be really fast and then airdrop to a bunch of people as backup. So that even if your pool goes down, there will be hundreds of people who hold your ASIC and then can kind of start running this ad backup. But I don't know whether it's realistic, but it's just an interesting idea. Yeah, I looked at one of Justin's VDF AsiCs a few hours ago. So I mean, all the guy can ship.
03:04:39.200 - 03:05:03.328, Speaker B: Josh, if I understand correctly, your design also relies on there must be some external block producing entity that executes transactions because for horizontal scaling, your nodes don't have chain VMs. Right. How do you think about the markets and sort of the way that you select these external parties?
03:05:03.504 - 03:05:40.256, Speaker E: I think the way we think about it is kind of to the point on governance is like, it would probably be accurate to call our initial set going to be like a proof of governance chain, proof of authority, whatever you want to call it. We're going to specifically select entities to be the sequencers, the validators, the relayers. I think those are all equivalent in our system. And then to Lev's point, we're going to have the builders be this kind of market driven entity. Right. That's kind of the desire is we're saying the governance will choose this kind of core entity and then you have market driving factors for actually selecting the other entities upstream. And I think it's desirable to have kind of market driven effects to kind of optimize a lot of that.
03:05:40.256 - 03:06:12.132, Speaker E: Right. Because I think the big trade off is we want the in protocol actors to be relatively lower resourced such that we can then have market incentives that will pay for the higher resourced actors that need to do simulation across multiple chains or whatever. And we've already seen builders can make money. Right. They will, as we discussed with the blob thing, right. They will do the work if it is profitable for them and they will not do it if it is not profitable. So it lets you kind of back out of having to do a lot of upfront kind of governance design of saying like, hey, this fee mechanism, or like this, my work is profitable.
03:06:12.132 - 03:06:15.740, Speaker E: The market will just define what is like a profitable behavior. And we think that's desirable.
03:06:16.640 - 03:06:57.928, Speaker F: Yeah, well, I think the challenge with all these incentives is like, there's things that we want that you can incentivize and there's things that we want that we clearly can't incentivize. Right. I think the big thing that we value, that's not really incentivizable in a robust way is this abstract idea of being an independent signal. Right. We have a little bit of incentive toward that with things like these quadratic penalties and the fact that going offline only really penalizes you if you go offline at the same time as more than a third of all the other nodes and stuff. But in general. But those kinds of rules by themselves are not enough.
03:06:57.928 - 03:07:38.790, Speaker F: Right. And there's the question of to what extent you even can make in protocol incentives for that versus to what extent this becomes a task for the higher level ecosystem and you basically just get more solar staker airdrops, which is might actually fix things. Yeah, I guess one thing we've struggled with a bit is when trying to design these markets, if you try and solve one market, you probably will create an inadvertent market somewhere else, and you don't really know which one's worse, the one that you kind of were trying to solve for or this one that you didn't think about over here. So you've got to be quite careful of that.
03:07:39.160 - 03:08:03.980, Speaker B: What are you worried about happening? I guess the question probably is what are the kind of actors you want to fill the roles in your system? Do you have any preference? Or like for example, Lev was talking about how they end up doing a lot of user facing things and supporting different OPCs and getting hit up in discord because people want to know why the transactions didn't land. And that obviously the response to that depends on the kind of actors.
03:08:05.760 - 03:08:33.348, Speaker F: Yeah, I think it's just actors that are aligned with your network's philosophy. So we're a privacy network, and so we would want actors that kind of don't censor and put users transactions on chain regardless of the contents. And I think that's a key tenant for us. So if we not careful and we create just a pure capitalistic market, then some of those properties may get eroded. That's our number one worry.
03:08:33.524 - 03:09:39.324, Speaker B: Okay. Taking a step back, I think there's general agreement that it makes sense to have smaller nodes, many more of them that perform some key validation tasks, and then big nodes that do some of the heavier resource things constrained by the small nodes. But one thing that it's not clear to me where it falls is consensus and the confirmations that it provides to give. I guess this touches on the base roll up discussion as well. Some roll ups want to have their own sequencer set running consensus, providing confirmations, and then eventually post that to Ethereum's largest set and others just want to go straight to Ethereum in celestia, there's an interesting parallel where the celestia validators are this rotating set and they're running the consensus. And unlike in the Ethereum roadmap, or like one version of the Ethereum roadmap, the small nodes are only sampling data, but not running consensus. And so consensus is a much more specialized thing.
03:09:39.324 - 03:09:48.670, Speaker B: It's constrained to a smaller number of nodes. And I'm curious, I guess anyone can weigh in, but Mustafa and Vitalik probably have the obvious candidates here.
03:09:49.120 - 03:10:40.816, Speaker F: Yeah, I mean, the approach that the philosophy is that it's okay to like, let's not assume we have decentralized block production or even block proposing, and let's try to rely as much as possible on things like light nodes and data availability sampling to make sure that the chain is correct. But obviously, the most important part of having decentralized block production and proposing is to have censorship resistance. But my general thought there is. Well, no chain has had a Nakamoto efficient more than 40. So is there diminishing return between having 100 validators and 1000 validators given that no chain has had that commercial efficient much bigger than that? Obviously. Maybe. Obviously with decentralization, it's like you only realize you need it when you actually need it.
03:10:40.816 - 03:11:00.010, Speaker F: So we don't know for sure yet. It could be the case that you do need more validators. But the trade off there is like with tenement, with 100 validators in tenement, you get instant finality for each block. But obviously there's a little commercial coefficient to actually kind of like control two thirds of the set.
03:11:01.740 - 03:11:06.440, Speaker B: Just to give more context there. How are the validating nodes selected?
03:11:07.500 - 03:11:11.972, Speaker F: Yeah, it's just delegators delegates their stake to those validated nodes.
03:11:12.116 - 03:11:14.056, Speaker B: Does governance play any role?
03:11:14.248 - 03:11:52.360, Speaker F: No, there's no proof of governance or anything like that. It's just like any person that owns tokens can choose who they delegate to. But there is still also a liquid staking, kind of like monopoly or geoloppy issue in splash. There is some early liquid staking protocols and there's a threat that they could have a monopoly as well. And for celestial, it's even worse if there's a monopoly because there's no smart contract environment on celestia. So the liquid staking protocols are like on other cosmos chains that have IBC connection to celestia. So if they have a monopoly, it's even worse because the tokens will end up in other chains.
03:11:52.360 - 03:12:21.920, Speaker F: So we're also looking at things like proof of governance. But one of the kind of proposals in the celestial community is this idea where, I don't know if I can fully explain it here, but idea of like instead of having proof of governance for choosing to prevent monopolies for liquid staking, you would just have this idea where anyone can kind of create their own liquid staking token with their own set of validators and make it very easy to swap them. And that in theory might prevent monopolies from occurring.
03:12:24.500 - 03:12:26.592, Speaker B: I don't know if you want to weigh in or not.
03:12:26.726 - 03:13:53.860, Speaker F: Yeah, I mean, I think on the bigger question, one of the interesting things about the execution tickets proposal in particular that makes it really clear, right, is that it basically takes PBS basically to its full conclusion and you basically have builder, a tester separation pretty much. Right. And the idea is that attesters are the thing that actually becomes sort of the uninsentivized incentivizer. And the thing where you're not collecting MeV, there's like a nice long four second window within to publish stuff and so on and so forth. And that's a set of actors that in principle, through various, even non protocol incentives you can encourage to be widely distributed throughout the world. And then once you have that exist as an incentivization layer, it's like relatively less bad for whatever else to happen in the builder layer or other parts of the Egypt pipeline, right? And the nice things about a tester is basically that if you have a block, then the block already comes with a proof, and all you need to do is just be the same thing as a light node. You verify the proof and you do a data availability sample query and you do the same very minimal checks that any node would need to do in order to verify the chain.
03:13:53.860 - 03:14:09.460, Speaker F: And that's a thing that in the long run can be much easier to run. But everything earlier in the pipeline does definitely feel like there's definitely pressure for it to inherently keep getting more sophisticated.
03:14:10.360 - 03:14:50.210, Speaker B: Yeah. So with the exception of Celestia, all of the protocols we've discussed today started with small nodes and then the big nodes have sort of been added on as this external market and we leave it up to the market to fill in this interface. I'm curious if because of that there aren't some oversights in the protocol, could be designed to be more friendly to these big node operators. And Lev, I guess you could also weigh in here if there was some requests you could put in as a big node operator to protocol designers to make life easier for you, or things that are obviously inefficient from your perspective, if any.
03:14:53.220 - 03:15:02.100, Speaker F: Firstly, I'm not sure if it's exactly that should be. The objective of the question is what is? I think big note is like a very broad.
03:15:02.520 - 03:15:04.836, Speaker B: Yeah, blockbuilder in your case, I would.
03:15:04.858 - 03:16:27.344, Speaker F: Say that I usually view it in terms of more of like molding the role of the big note operator versus try to make life easy for them, quote unquote, because I think as long as they're incentivized, they'll always figure it out. So the question is, what are their efforts working towards? And what you can do is you can fail to incentivize them to devote their efforts to the thing that actually helps the ultimate end user somehow. And that's when you've failed, I think, is if they all have budgets of millions of dollars devoted to optimizing something which is clearly not the kind of social good of the group, but on the contrary, if you do manage to have it so that they're devoting their efforts to doing something that's useful, then you've succeeded. So I would put it that way, rather than how to make their life easier, just because historically it just hasn't really been how we've seen things. But I think that especially with things like execution tickets, I think it opens a very interesting area for figuring out what kind of standardization will be required for these different parties to interact. I think right now there's been some degree of kind of spontaneous standardization on things like builder APIs and non standard RPCs, both pre and post PBs. But there's also some growing divergence there.
03:16:27.344 - 03:17:00.410, Speaker F: And I think that as these things get more complex, it's very unlikely that we will spontaneously converge on the same APIs. And I think it's anybody's guess how, if execution tickets are implemented, how exactly it plays out in terms of who the different participants are. But I could imagine something that forces a greater degree of standardization there in order for it to be basically possible for the builder for a particular slot to actually aggregate order flow and talk to other people required to make the block that they need to.
03:17:03.260 - 03:17:18.110, Speaker B: I don't know if any of the protocol designers have other perspective on that. Where have you thought about how to enforce standard behavior across the nodes participating in your markets? Or is that sort of something that's a bit further down the line?
03:17:18.480 - 03:18:31.740, Speaker E: This relates to the earlier panel from the builders, and maybe it's not trying to enforce standardized behavior, but it's kind of thinking about bootstrapping and saying why? I asked the question of do the builders spend any time thinking about other ecosystems and you're thinking, okay, if I'm bootstrapping an ecosystem and I need builders to participate, what can I do to lower the barrier to entry? To have these kind of more sophisticated participants in an ecosystem that is maybe more nascent and therefore kind of like building APIs, kind of to your point of making it easier for the big nodes because we view them as kind of necessary or maybe inevitable would be the better word of the market structure will lead to these entities being there. How do you accelerate that path to getting them there? Ideally you can have more decentralization in that, but it's really like bootstrapping. It's know, one of the questions I have is what do builders not like about the builder APIs or the system? They've know Lev's point, they'll figure it out. There's a bag of money and it's just like if you go figure out how to use those APIs, however jank they are, there's money there, so they'll just go do that. But presumably there is something better. And so we do spend a bit of time thinking about that and saying, look, if these are kind of one of our customers in a multi sided market, we would like the integration pain to be lower for them because that's just going to make our sales cycle much easier.
03:18:31.920 - 03:18:58.800, Speaker B: Yeah. Maybe just to motivate why I framed this in terms of making the life easier for big nodes is that as we sort of, one of the biggest motivations for this kind of separation is scaling. And so you want to have the node resources utilized maximally efficiently. And it's not clear to me that that design goal has been there necessarily in the way PBS has been designed. It definitely hasn't. Right. It was more about the MEV perspective on things.
03:18:58.800 - 03:19:12.580, Speaker B: And so for example, in PBS today, state route calculations, quite an expensive task. And that's done dozens if not more times in a slot, which is wasteful because you only really ever need one computed.
03:19:13.720 - 03:19:19.510, Speaker F: Well, at least well, in the status quo, everyone has to compute the state route to verify a block. Right?
03:19:20.940 - 03:19:33.290, Speaker B: Right. In the block building world, if you can update your block bids more quickly, that should allow you to have in some sense a more efficient market.
03:19:34.460 - 03:19:45.324, Speaker F: In principle, there's like software that you can write to compute a Delta and update a state route based on a delta. Right. There's stuff that probably hasn't been written, but could be.
03:19:45.522 - 03:19:47.036, Speaker B: Yeah, Liv, looks like, yeah.
03:19:47.058 - 03:21:07.268, Speaker F: Just an interesting example of that is that given that in practice today on Mainnet, the majority of blocks are produced by block builders, for example, and they accept transactions through an API that promises not to include reverting transactions. Effectively. The whole point of having paying for reverting transactions and a lot of how the gas model was envisioned is now sort of been kind of deprecated by builders saying that, oh, we actually have tons of computing resources, given how much transactions we're getting now, and we use a bunch of random heuristics to decide which transactions are spam or not. And then you often get, firstly, you get this strange dislocation between how core devs think about resource usage in the protocol where block builders are actually expected to recompute things many, many times and resimulate everything in every single order, which is totally different to how the kind of vanilla mempool was designed and where the kind of gas metering considerations, I think, came from. And I think also, secondly, this creates, I think it's going to create difficulties for, not just for blockbuilding. There's this huge kind of question that looms over the space, which is that users need access to RPCs, often for historical state. This has been kind of the case for much longer than PBS.
03:21:07.268 - 03:21:34.490, Speaker F: We have probably questionable incentives in markets around providing RPCs, especially with other chains that might have much, much bigger state growth that I think are kind of still to me, as far as I've seen, are kind of unanswered. Was that getting into the point of your question at all?
03:21:35.420 - 03:21:38.890, Speaker B: That touched on my question. I was trying to think of where to take it next.
03:21:40.220 - 03:21:41.924, Speaker F: I have a question for the panel.
03:21:41.972 - 03:21:44.956, Speaker B: That's okay. Yeah, shoot, yeah.
03:21:44.978 - 03:22:41.576, Speaker F: I mean, following up from this talk, so we're assuming that there's kind of like a multi roll up ecosystem. First of all, how do we know that's going to be the case? Do we actually think users will use, let's say there's ten general purpose EVMs. Do we think users will actually use all those EVMs? Or do you think there's like a natural kind of like marketplace for users eventually only use one of those chains? Maybe arbitram or optimism will get all the market share. And if there is a multi roll up existing, what incentives will there be to be used? Different roll ups maybe? Could there be like application specific roll ups or something like that, or. That's kind of my question. I think there's always going to be multiple based on either features, or you can maybe call it a feature, but.
03:22:41.598 - 03:22:43.016, Speaker B: The security assumptions of the roll up.
03:22:43.038 - 03:23:56.732, Speaker F: So even if you're just implementing EVM everywhere, you're going to make different trade offs in how you do that, unless you have the pre compile option on the. So I think even if your features are the security of your DA or your proof system, optimistic versus ZK, there's always going to be different places on that trade off space you'll sit. So I think this is a question I keep asking myself, and we have been have multiple internal discussions. I think it's just like even from in five to ten years where we are, like, I think people here all believe that blockchain really, in next five to ten years, really the core financial infrastructure, if the core financial infrastructure and all those kind of Internet activity all happen on just one, two server, I think it's just like literally impossible. Even if you look at right now, like Amazon, Google, Twitter, they all need even more servers than what we need right now. So I think it's just literally like one rob just can't scale to enough throughput we really need, because every rob talk about 1 billion people bring to Ethereum ecosystem. But if you really do some calculation, which we have, is that if 1 billion people, everyone send one transaction per day, that's already like over ten K throughput required.
03:23:56.732 - 03:24:29.080, Speaker F: So, which is literally, even if you use all parallel evm, every kind of fancy scale you have, it's just impossible to kind of run on just one server without even decentralization. So there will be many, but I still don't believe that there will be tens of thousand, because the interpretation is still very hard to achieve, even with shared sequencers. So I think there will be several general purpose rob with different value propositions, some security, some other DA. But then users still need to be aware of, like we chant, they need to use to kind of be aware of secure assumption and all those properties.
03:24:30.380 - 03:25:10.504, Speaker B: I want to ask one question and then put it to the audience. I also don't know how much time we have or if there's anyone watching at all. We can just go forever. I'll ask the question then. And then the police can tell me what are the blockers to central? There's been a lot of talk about big nodes, big sets of nodes and interaction between different nodes. But most of what we're looking at right now is just a couple of AWS servers. So why aren't we? Obviously things are challenging, but more specifically, what are the obstacles to getting to these more decentralized stacks that we're looking for?
03:25:10.702 - 03:25:42.924, Speaker F: Yeah, I can talk about. So firstly, I think eventually we definitely commit to being decentralized. But the question is how you take this decentralization go to market and how you progressively do that. Because right now the biggest reason is that because arbitrary optimism providing millisecond pre confirmation experience. If you kind of are sufficiently decentralized, it's just like your system won't be used by people, by users. That's a lesson I learned after we launched our mannet is like you talk about security, doesn't really matter. People have been living in a system with no proof for two years, so they don't care about decentralization proof.
03:25:42.924 - 03:26:18.172, Speaker F: So it's really a matter of how you kind of take this kind of designation progressively to market. So that's why you design your approver at least then sacrifice the proof confirmation side. And then you think about how you gradually do that step by step. And the more practical challenge is that right now the EVM has been even fully audited open source. But still, it might have some Sunday spark. So if you take a very, very aggressive step to decentralize your stack, you give all the control to anyone can join a proof and produce a block. It's very dangerous actually, because someone can just catch Sunday's error, completeness error for the KVM circuit and then just do something bad without.
03:26:18.172 - 03:26:47.816, Speaker F: If you are not instrumenting likm into layer one and use layer one to govern this liquidm, it's just very dangerous to do that. So it's mainly coming from user need and how you aggressively do that. And also some practical challenges on the key side. And also I think maybe ad tech has more compliance issues, so they need to decentralize layer. But for us general purpose one is less concerned. Yeah, maybe I can elaborate on the kind of decentralizing the sequence of risk. So on Ethereum, if there was some critical consensus bug, you always have the option to hard fork away.
03:26:47.816 - 03:27:35.930, Speaker F: And the assets maybe, except USDC exist on both chains. In an l two world that's not the case. So if there's a fully decentralized sequencer, then if someone can exploit an issue in the VM or the roll up circuits, then you can't hard fork away because most of the funds are kind of l one based and they've come from Ethereum. So an attacker can come in, take those funds out and they'll live on Ethereum. And regardless of if you fork the roll up, the roll up won't have any assets in it anymore. So I guess the number one issue in trying to decentralize here is balancing security with kind of, this kind of decentralized sequence of property, because that's how you can propose a block that contains an invalid transaction. And training wheels help.
03:27:35.930 - 03:27:45.492, Speaker F: Multiple proof systems can help, but that's kind of the thing that takes the most time to develop is around training wheels to get to a fully decentralized.
03:27:45.556 - 03:28:16.292, Speaker E: L. Two, yeah, I think there's a couple of points here. One that's very key is just like, where are you spending your resources? What is your differentiation? So we have tools like ZK roll up, right? Where a lot of their time and effort is spent on their specific proving mechanism. And so maybe focusing on decentralization as a consensus algorithm and optimizing that and developing a set of entities, they can run a fast network that is just not their kind of core differentiation against all the other roll ups. So say, yeah, we'll do that later. But everyone has progressive decentralization on their roadmap. It's been like three years, whatever.
03:28:16.292 - 03:28:58.828, Speaker E: No one's kind of done it enough that there is a forcing function for someone to say like, oh, optimism, or arbitram made the first move, they decentralize the thing. Now there is like a market pressure to say like, oh, everyone else has to do decentralization. Everyone just kind of comfy in it. There's like the cynical take of just like is a competitive market and people are making a lot of money. We see like in the MeV space, there's also the optics case, right? If you run a centralized sequencer that is equivalent to call it like a private mempo or whatever, where you can say, okay, no one's getting arbed and we're getting collect all the revenue here. That's just very good from a market perspective in that you get to say, we're doing good by our users and we get more money that allows to be more competitive. So I think there's a lot of things at Astria, the kind of position we're taking is saying we're going to go do the decentralization thing first.
03:28:58.828 - 03:29:15.648, Speaker E: And ideally there as like a market demand to kind of buy that versus build such that you can say each roll up does not need to go invest their own effort if it's not a priority for them. But if that is a functionality they want, they can go kind of inherit that or buy that from someone else. That's kind of the theory here.
03:29:15.674 - 03:29:25.160, Speaker B: I think that makes a lot of sense. We have apparently three minutes, so if there's anyone with a burning question in the audience, now is your time to shine.
03:29:25.500 - 03:29:46.930, Speaker C: Patrick Vitalik talked about this at the part of the issue that in the market strategies many teams are chasing total value locked as somewhat of a vanity metric. Is there a better KPI that we could use to evaluate success of a roll up project?
03:29:48.420 - 03:30:10.550, Speaker F: Yeah, I think it should be here in 510 years. And so you have to design it to do that. That's kind of our take. And yeah, you have to design decentralized, I think, to do that. Ethereum talks about being World War three proof. I think it's maybe an extreme take, but I think you need to start thinking about that because if you're just going for TVL, then I agree, you probably won't survive some of the bumps in the road.
03:30:11.400 - 03:30:46.816, Speaker E: I am by no means like a DeFi expert or like a finance expert, especially relative to this room. But one of the things kind of my team was looking at was like, if you look at Jane street as an example of a very centralized market, right, the numbers are going to be off. But it was something like they had like $15 billion of, call it like TVL or capital assets, and they did like $1.5 trillion of volume. So there's something interesting in the ratio of how much volume of trading you can do in a year relative to how much assets you have. I don't know what the Ethereum ratio is going to be, but my assumption is bad, certainly not like forwarders of magnitude. And even if we look at Salana, which made different trade offs, right.
03:30:46.816 - 03:31:12.670, Speaker E: Salana has as a community, they would argue against TVL being used as a metric. They claim it's the vanity metric and they do have higher total volume of trading happening relative to a smaller amount of TVL. So I think there's thoughts like that. But fundamentally, I guess there's also going to be just like subjective user experience concerns, right? Like where do the users actually go? Where do they think they're getting the lowest slippage, the fastest kind of execution, the highest quality execution, whatever.
03:31:14.320 - 03:32:11.360, Speaker F: I think we learn a lot after we launch our mannet. Firstly, I don't care about TVL before we launch because it's a vanity metric. And I think TVL always flow to the chain has highest yield and the chain has highest yield is like the chain that hadn't launched a token, because you can always use your token to kind of pump your yield and then liquidity flow from one place to the other. But the reality that people still care about this matrix a lot, and then, because once you get high enough PVL and then you get market attention and then developer will come to your chain because they want to access to this liquidity. I think what's really funny about layer two is that people, at least I went to Ethereum community because I think research, vibe, decentralization, all those stuff, layer two in higher security, but actually another narrative kind of become like Ethereum win because Ethereum had larger liquidity and layer two narrative is only a way to access liquidity easily. And then imagine like Salana has a similar level of liquidity. Does Salana really win at least? I don't think so.
03:32:11.360 - 03:32:54.364, Speaker F: But the market people easily get formal and then easily to get attracted and then want to build at least in this kind of either to formal market. So I still think it's an important matrix. But I think a more housing matrix might be like if we really bring like 1 million user into ethereum ecosystem and everyone deposit like 1000 USD in your ecosystem, that's a more healthy TVL instead of one big, well deposit 1 billion for really high sticking reward or something like that. So I think a more healthy asset into your LP and how your TVL is composed is very important. But short term, the best thing you can get is like a high TVL number as a marketing signal and then long term build and then stay relevant for the next five to ten years.
03:32:54.562 - 03:32:58.476, Speaker B: Okay, cool. I think the question was directed to you, Vitalik, so do you want the last word? And then we can.
03:32:58.578 - 03:33:27.770, Speaker F: What's the question? Sorry, that was the same. Yeah, ideally we'd like. Well, I think in order to get any other metric working in a way that's not trivial, to break, we need working proof of personhood protocols, and we need them to not just be working, but actually widely adopted. So that's one of those things that I've definitely been continually excited about.
03:33:28.780 - 03:33:31.930, Speaker B: Okay, awesome. I think that's it. Thanks everyone.
03:33:35.840 - 03:33:40.350, Speaker D: Thanks everyone. Lunch will be on the fourth floor. Thanks.
04:28:00.990 - 04:28:03.660, Speaker B: Okay, am I on.
04:28:09.390 - 04:28:09.958, Speaker D: It?
04:28:10.064 - 04:28:40.390, Speaker B: Testing, testing. Yes, I'm on. Perfect. Hi everyone. So today I'm going to talk about this idea I call the united chains of Ethereum. This vision whereby the different l two s and roll ups can share a sequencer and be much closer together than they are today relative to the silos that we currently have with Ethereum rollups. And so in some sense, this is all about fixing ethereum fragmentation.
04:28:40.390 - 04:29:40.262, Speaker B: So the talk will be in three parts, one synchronous composability and all the advantages that come with that. Two decentralized pre confirmations. We're going to have decentralized sequences. How do we do pre confirmations in that context? And then three, this slightly more opinionated idea that maybe the l one proposers themselves can be doing the sequencing, and that's called base sequencing. So one of the key ingredients to get synchronous composability is this idea of shared sequencing. So different execution environment, different so called domains can share a sequencer, and then that one entity has monopoly power, at least within their slot, to simultaneously sequence multiple roll ups, or L two s. And just so that we're clear on definitions, there's different types of synchronicity.
04:29:40.262 - 04:30:46.400, Speaker B: So one thing that you could ask, for example, this shared sequencer is synchronous inclusion. To make sure that two transactions, one on roll up A and one on roll up B, both get included on chain. At the same time. You could ask for something stronger, which is synchronous execution, where basically you have guarantees on state routes, not just on inclusion. And then you have the strongest form of synchronicity, which I call synchronous composability, where you also have assets that can clear immediately in and out. And so basically these are like guarantees that the shared sequencer can give on different data structures, either on the transaction route, which mercury is the transactions, the state route, or the withdrawals route, which captures assets that go out of roll ups, and l two s. Now let's imagine that roll ups A, B and C have opted into a shared sequencer and imagine that the passage of time is from the top to the bottom.
04:30:46.400 - 04:31:59.142, Speaker B: Now, the simplest thing you could do is you could have each roll up, in some sense be its own silo and be executed perfectly in parallel and have maximum efficiency. But another thing you can do with shared sequencing is you can have the shared sequencer do some magic across the roll up. So for example, at the top of the block here, the shared sequencer could choose to do some sort of fancy arbitrage, for example, which synchronously happens. But you can also, throughout the block, have opportunities for synchronicity. And in some sense, what shared sequencing is all about is exposing to the market this additional value that synchronicity can provide, synchronous composability, and then ultimately providing more value to users and to roll ups. So if we zoom in to this little portion here between the yellow and green roll ups, what is really happening? So you have this synchronous, so called supertransaction, which has multiple constituent subtransactions. So here we'll have three subtransactions, a yellow one, a green one, and then another yellow one.
04:31:59.142 - 04:33:09.246, Speaker B: And really one of the important things here is that there's asset transfers that are happening in real time. And so you have this idea of real time settlement where the roll ups are immediately clearing their assets and are capable of sending them to other roll ups. And you can have these subtransactions kind of depend on what's happening in the other subtransactions. So, for example, you could have this final yellow call depend on the result of the green call, and that might introduce some amount of locking. So this is why, basically, the way I depicted it is that only one subtransaction is executing at any given point in time. And so that kind of has a cost, right? This virtual locking that the shared sequencer can provide means that the user is going to have to pay for gas simultaneously on roll up a and roll up B, even though at any given point in time, maybe only one single roll up is active and executing. Now, one of the things that's kind of important here is this idea of proving latency, because ultimately, settlement latency depends on how fast you can prove things.
04:33:09.246 - 04:33:59.230, Speaker B: And so imagine that we have synchronous composition at the top of the block here. Well, that's actually pretty good because we have a whole 11 seconds, if the slot time is 12 seconds, to do the proving and to include that proof towards the end of the block. But things can become really tricky if you want to provide synchronous composability at the very end of the slot, you might only have a few milliseconds. And this is why the really fast real time proving is a requirement to unlock full synchronous composability across the full block. Now, I'm very optimistic about low latency proving. Like, there's all these really cool ideas around folding, which are basically very fancy, very efficient recursive proofs. There's the idea called nova that pioneered a whole amount of literature.
04:33:59.230 - 04:34:34.998, Speaker B: And we're also starting to see SnOC proving AsiCs. So we have the very first proving ASIC from axial. I'm going to be talking about it at the ZK summit. It's live, it's real. They're building test boards, and it provides a huge amount of cost reduction, of opportunity for parallelism and power reduction. We're talking orders of magnitude improvements with these chips. Now, I guess an alternative to real time proving, because real time proving will still take a few months, is liquidity providing.
04:34:34.998 - 04:35:19.990, Speaker B: So you can kind of simulate this idea of synchronous composability with liquidity providers. And so here in this example, even though these yellow dashed arrows and the green dashed arrows might take time, it might take, let's say, seven days for settlement and asset clearing to happen. You can have these liquidity providers basically provide immediate liquidity on both sides. But that comes with downside, it comes with fees. There's like capital inefficiencies. It comes with intermediation and complexity. And it also comes with liquidity constraints, because if you want to do NFTs, for example, if it's a one of one NFT, then that's very difficult to provide liquidity for.
04:35:19.990 - 04:36:27.730, Speaker B: And if you want to operate in size, for example, if you want $100 million of liquidity, then the liquidity provider might not have that kind of liquidity to provide in the first place. So that was part one. Any burning questions so far? I mean, the idea here is that, again, with this very basic and simple idea, which is shared sequencing, we can have these currently siloed execution environments kind of go back to what we have at layer one. Like at layer one, you can have two contracts which can synchronously compose of each other. You can have, for example, aave synchronously compose with uniswap and execute a liquidation, which is something that we don't have at the role of player today. So we don't have the equivalent of one inch or matcha. Like you can't go to a website and say, give me the best liquidity across all the roll ups, just because you're dealing with all these separate sequences.
04:36:27.730 - 04:37:05.546, Speaker B: And it becomes extremely massive. Okay, part two, which in some sense is orthogonal to shared sequencing, is the very simple question of, okay, we have pre confirmations with centralized sequences. That's super easy. They work really well. Like, how do we do pre confirmations in a decentralized setting? And it turns out the answer is also surprisingly easy. So basically what you can do is you can have the user determine who is going to be the sequencer in the next slot. So here there's this notion of a look ahead.
04:37:05.546 - 04:37:49.270, Speaker B: You can see who the next proposers will be, who the next sequencers will be, and the user very simply makes a request to the next preconfirmer and then gets a promise from that preconfirmer. And again, this is a promise on execution, not just inclusion. It could be even a promise on asset clearing, like the strongest kind of promise. And really the only thing that changes is that at every slot there's a different sequencer. So what is a decentralized sequencer is just a fancy centralized sequencer, where the centralized sequencer changes at every single slot. And one, I guess, difference is that it's how we do slashing. So with a centralized sequencer, you're leveraging reputational collateral.
04:37:49.270 - 04:38:30.466, Speaker B: There's arbitrams or optimism's reputation, which is worth probably billions of dollars, and they're not going to go ahead and screw users and reneg on their pre confirmations. But in a decentralized setting, where anyone could be the next sequencer, you need a different type of collateral. And here you just use financial collateral. And there's two types of faults that are slashable. There's safety faults and liveness faults. Safety faults is when I gave you a pre confirmation that such and such transaction will execute in such and such way, and it turns out to not execute in that way. So that's a very clear safety fault, which should never happen if the sequencer is honest.
04:38:30.466 - 04:39:32.194, Speaker B: And then you also have liveness faults, which are a bit of an edge case, a bit more subtle, where the sequencer is really trying to be honest. But they were just offline at that point in time. And that's something you also need to penalize for, because they were not able to honor their pre confirmation, just because they went offline and they were not able to produce a block. Okay, now, I guess one question is, how do we do incentivized block building, and how do we modify bevboost to take into account these pre confirmations? So right now, the pipeline for MevBoost is like, unidirectional. You have the builders that push blocks to relays, that push block to proposers. And really the main thing that you need to do is basically provide information in the other direction. So you have the users, which are now this new entity in the block production pipeline, that have pre confirmations with the proposer, or pre confirmer, or sequencer, however you want to call it.
04:39:32.194 - 04:40:31.306, Speaker B: And then these pre confirmations can flow upstream to the relays and ultimately to the builders. And now the builders are tasked to build blocks that respect the pre confirmations. So really, what is this? It's like a fancy inclusion list in some sense, or pre confirmation list that constrains the block builders in what they can do. And one important detail here, I guess, is that the relays could enforce that these constraints are respected. But nowadays, a lot of the relays, at least the competitive ones, are optimistic relays, where they don't check in real time, these constraints. And so what you'd need is the builder to be collateralized to the same extent that the proposer is, in case the builder doesn't honor the pre confirmations that the proposer did, so that the proposer can be compensated by the builder if a bad block was built. Okay, so just as a quick summary slide here.
04:40:31.306 - 04:40:58.440, Speaker B: In some sense, centralized pre confirmations and decentralized pre confirmations are the same thing, but there's just a few things that change. One is that the sequencer, instead of being fixed, is now rotating. So at every single slot you have a different sequencer. That's easy. The slashing is also a little different. You're now using financial collateral as opposed to reputational collateral. Fine.
04:40:58.440 - 04:41:29.198, Speaker B: And then another final thing which is interesting is how MeV is done. So today, with decentralized sequences, you can kind of think as decentralized sequencer, providing a makeshift, super cheap encrypted mempool. As a user, I send my transaction end to end, encrypted to the sequencer. No one can see it. I can't get sandwiched, for example. And so I get this Meb protection. And oftentimes there's a policy which is that we're not going to screw our users, we're not going to sandwich them.
04:41:29.198 - 04:42:35.458, Speaker B: Another policy might be like some sort of first come, first serve. But these policy based sequencing, I think, is going to be much harder in a decentralized setting. And so you have to move to something which is a bit more market based, something like PBS, for example. Okay, so we've gone through these two parts. Part one is like, there's potentially a lot of power to Ethereum reuniting and fixing its fragmentation with shared sequencing. And if you're given a decentralized sequencer, actually providing pre confirmations is fairly easy. And now what if we take an existing shared sequencer, which we kind of all in our subconscious know exists, can we kind of compile it in some sense to modify it, to give pre confirmations for that and kind of make it a really great sequencer? And this is what base sequencing is all about in some sense.
04:42:35.458 - 04:43:20.450, Speaker B: You can think of Ethereum as being like this service that provides multiple modules. So there's the settlement module, which all the L two s will use. There's the data availability module, which all the roll ups will use. And then there's this third module, which in some sense is new because people have only realized recently that they can go and leverage it, but it's been there all along, is the sequencing module. So you can use the L one proposers as sequencers for your L two. And just like call data was this somewhat hidden thing that people only realized could be reused as data availability for l two s. L one sequencing was also this hidden thing that existed from Genesis.
04:43:20.450 - 04:44:44.730, Speaker B: Now why would you want to use Ethereum sequencing? Well, it's for all the same reasons as to why you'd want to use Ethereum settlement or Ethereum data availability. You inherit the security of l one, you also inherit a lot of the credible neutrality that comes with it, and you also inherit composability. Now how does it work exactly? Like who are the sequencers in the context of base sequencing? So in the specific design that I'm proposing here, we basically have a subset of the l one proposers who opt into becoming sequencers. And the reason why it has to be a subset is because they have to come forward with collateral, right? They don't have reputation, so they come forward with this financial collateral. So the green ones have put forward collateral and they're a subset of the proposers in the look ahead. And we're going to call those sequencers and everyone else who's not a sequencer, who didn't put forward any collateral, we're going to call them includers. And basically the idea here is in order to provide pre confirmations as a service by the L one proposers, we're just going to do exactly what we talked about in the previous section.
04:44:44.730 - 04:45:37.214, Speaker B: We're going to have the user communicate with the next sequencer, which happens to be this green one, which is collateralized. A request will be asked from the user, a promise will be given by the preconfirmer. And one of the things that we can also do is basically have the includers be given the right to include and settle pre confirmed transactions. So the includers don't have the right to order transactions. But if they have an ordering which is given to them from the next sequencer, well, they can go ahead and include that and settle it. And so basically you have best in class user experience. On the one hand, you have the 100 millisecond latency of sending a request, getting a pre confirmation really fast.
04:45:37.214 - 04:46:42.670, Speaker B: And you also have the next slot settlement because this includer can immediately settle in their slot whatever was sequenced by this pre confirmer. And again, there's a very simple diagram here where we modify meth boost so that the blocks take into account the pre confirmations that were given by the proposer. Now one of the questions you may ask is, if I want to provide pre confirmations, shouldn't I be very sophisticated? And the answer is yes, you need to have at least enough bandwidth to support user requests from all the roll ups. So that's potentially tens of thousands, hundreds of thousands of transactions per second. You need to be running full nodes for all these roll ups. It could be hundreds of full nodes, and you need to do all sorts of fancy things like pricing the pre confirmation tips. You need to have high uptime, low latency.
04:46:42.670 - 04:47:47.874, Speaker B: And so really it's fair to ask, do we really want l one proposers that are running on a raspberry PI to be pre confirmed? And the answer is no. And the good news is that we have this idea called execution tickets, where as a validator that's running on a raspberry PI, I don't have to do block proposing. Instead, my main task is attesting. I also have to build inclusion lists for sensory resistance, but I don't have to do the sophisticated sequencing. Now, unfortunately, execution tickets are a hard fork in the future, potentially, and they would take several years to come out. And so what can we do in the short term? Well, in the short term what we can do is basically introduce a trusted entity here, semitrusted entity like the relays, which we could call the gateway. So this would be a pre confirmation gateway, and the proposer would delegate their sequencing rights to the gateway.
04:47:47.874 - 04:48:56.762, Speaker B: So now the gateway can provide these pre confirmations on behalf of the proposer, and they would be collateralized in case they cause a safety fault. Now this whole idea of base sequencing is relatively recent, and a lot of the roll ups had their own plans to do different things, but in the last few weeks and months there's been more and more interest from the ecosystem. So we've had Tyco that's been a base roll up for a very, very long time. A few days ago I learnt about this new roll up, which is also going to be based. And then also a few days ago, one of the founders of the very top roll ups on production today said that they're going to be pushing for them to become a base roll up. And then there's all sorts of infrastructure providers that are also starting to build infrastructure dedicated to base roll ups. There's preso Sorella chainbound, and there's other entities, for example lime chain and nevermind, that are doing like research and consultancy.
04:48:56.762 - 04:49:48.080, Speaker B: So really the ecosystem is very nascent. But there's more and more momentum behind this idea of using the L one proposers for maximum security, maximum credible neutrality, but also being able to leverage the TVL of the L one. Because if you zoom out and you look, okay, where's the TVL? Sure, there's like arbitram has 10 billion or 15 billion, but then you have the L one, which has half a trillion or 600 billion. So it's orders of magnitude multivl at layer one. And when you have this base sequencing, basically the roll apps can synchronously compose with the L one. And so you get access to all these pools of liquidity. You get best in class user experience, because now you can have complex applications that live in multiple places, including some on DL One.
04:49:48.080 - 04:50:30.410, Speaker B: And yeah, that's basically the pitch for base sequencing. Okay, last slide. So right now we're in this place where we have centralized sequencers, and I think we all agree that we want to move to decentralized sequencing. And there's going to be quite a big lift, in my opinion, to get from decentralized to centralized. Like we need to think of all sorts of things like MEP protection, PBS, intense OFAs data compression and whatnot. So I think that's where 80% of the problems need to be solved. And then within decentralized sequencing, there's a subset of problems that are specific to shared sequencing.
04:50:30.410 - 04:51:33.280, Speaker B: And that's where maybe, let's say 15% of the problems need to be solved. So we need to solve, for example, MeV sharing as a roll up, I opt into a shared sequencer. In doing so, by default, I'm giving my MEV to that shared sequencer. Like, can we basically identify where the MEV came from and kick it back, rebate it to the roll ups that generated that MeV? There's also other problems that we can think about, like deposit sharing and execution sharing. But in generally speaking, there's fewer problems to be solved than just decentralized sequencing alone. And then there's this kind of final subset of even more specialized sequences, which are the base sequences, where in some sense there's very little to solve. It's just being opinionated about saying, okay, the pre confirmers are going to be the l one proposers, but everything else, all the other technical problems around pre confirmations and shared sequencing that can be solved using off the shelf technology.
04:51:33.280 - 04:51:36.880, Speaker B: And that's it. Thank you.
04:51:44.310 - 04:53:01.814, Speaker D: Questions in the side chat, please. I know there are so many questions you have, so please check on East Global TV. And we just have all of the slides for the afternoon. Most of them are updated on the agenda page Meb market. So can access it and review it. Can I do the proposed review by any chance, or the present review? I don't know. If that'll work.
04:53:01.814 - 04:53:20.368, Speaker D: I'm not sure. Oh no it does. No it doesn't. I'm sorry. All right, that's fine. I'll do the other one instead of. Okay.
04:53:20.368 - 04:54:06.908, Speaker D: Hi everybody. I'm going to speak today on execution tickets and Mev distribution in shared and based sequencing. So this is what Justin was referencing at the end of his talk is how the big question of this talk will know. Shared sequencing is great, but roll ups aren't going to use it if it means that they lose all of their MeV revenue. So I'm the lead researcher and I'm also an engineer at espresso. Ben Fish was originally supposed to give this talk, but he had some issues with his plane flight, so I'm filling it. So the general structure of this talk is we're going to go over what's isolated sequencing, what's shared sequencing, and then how can we build a mechanism that redistributes MeV in shared sequencers? Okay, so let's start with what sequencing is.
04:54:06.908 - 04:54:43.236, Speaker D: So we have roll ups, they horizontally scale ethereum like we know, and they all settle to the layer one either using fraud proofs or zk proofs. There's a lot of advantages to this. One of them is that they can shard computation across a lot of different roll ups. This includes having different VMs that are maybe niche and are built for specific use cases. And they're also really good because they also give heterogeneity. So we can leverage powerful nodes that can produce zka proofs that can be verified by very weak nodes. So this is all really wonderful, but roll ups today run isolated sequencers.
04:54:43.236 - 04:55:25.732, Speaker D: And so isolated sequencers are basically each roll up has a server, usually like a centralized server, but it could be decentralized. And each of these servers, a user submits their transactions to them. Each of these servers does not know about the sequence of transactions submitted to other roll ups. And so the drawback of this is obviously interoperability. So if you have these isolated sequencers, interoperability is difficult because if you want to interop in a safe way, you need to settle to the L one and then have another roll up read what you settled to the L one and then have that first roll up read again what the second roll up settle to the L one. This is all quite slow. The other drawback is that you can't actually have atomic execution.
04:55:25.732 - 04:55:59.490, Speaker D: So atomic execution being that a set of transactions will either all execute or all fail. You can't do this unless you have a shared sequencer so what's the job of the sequencer to begin with? So we have users, they submit their transactions to this server. The server puts them all in a total order, puts them into a block, and it posts this to the L one. But another really powerful thing that the sequencers can do is provide pre confirmations. So pre confirmations are very powerful for a lot of ways. There's. Pre confirmations could be an entire talk just by themselves.
04:55:59.490 - 04:56:41.632, Speaker D: But pre confirmations, the biggest thing that they provide is this kind of fast finality. If you trust that the sequencer is honest, then you can trust that your transaction is final when it's pre confirmed and you don't have to wait for l one settlement. And then finally, the third thing that sequencers can do is they can optimize for data compression. So instead of having to post every single roll up block to the L one, which can be costly, they can post a chunk of blocks every few minutes, for example, like arbitrum posts every few minutes. Okay, so you say, well, a natural extension of a sequencer is that they can run an auction. And this is something that we already see in Ethereum today. This is very similar to what PBS is in Ethereum today.
04:56:41.632 - 04:57:13.432, Speaker D: It's an auction to build the most profitable block. Sequencers can do this too. We don't see sequencers do this right now, but it's more than possible that they could. Okay, so that was an isolated sequencer. Now let's talk about what a shared sequencer is, aka what a based roll up is. Okay, so in this case, the users are actually sending their transactions to the L one. The L one is a shared sequencer, and then the roll ups can execute their roll up from the data on the L one.
04:57:13.432 - 04:57:39.224, Speaker D: But note that the users are not sending their transactions to specific roll ups, they're sending them to the L one. And this is really great, as Justin mentioned in the last talk, because we have the security and the decentralization and the credible neutrality of the L one. This is all really good. And you also get improved interoperability. Now there's two parts to the interoperability that you get. So one is you get really great interoperability between all of your l two s. You can now do these atomic transactions.
04:57:39.224 - 04:58:15.052, Speaker D: But the even better, well, maybe not even better, but also the really important part is that you get atomic transactions between the L one as well. What was on the previous slide is kind of like vanilla based sequencing. And on this slide I'm going to offer maybe another version of base sequencing. That's a little bit different, but is also a shared sequencer. So in this case we have this layer 1.5 that sits between the L one and the L two s. So in this case, the users submit their transactions to this layer 1.5
04:58:15.052 - 04:58:51.636, Speaker D: sequencer. The sequencer does ordering and it also provides data availability on all of the transactions. The roll ups read the data from the layer 1.5, they execute, and then they post their state transitions and proofs to ethereum just like normal. And so you may be asking, well, what's the point of adding this extra layer 1.5 in there? And I'll discuss that in a few slides. Okay, so similar though to what I was saying earlier, how a natural development of isolated sequencing or anything is that you naturally are going to run an auction to see who can propose the most valuable block.
04:58:51.636 - 04:59:25.460, Speaker D: And so that's going to happen in shared sequencing as well. So a few key points about this auction. So auctions will basically auction who has the right to update all of the roll ups or a subset of roll ups at once. We'll kind of get into the details of this a little bit later. And that another good thing is that the proposers of this auction can offer atomicity over transactions, so they can offer atomic execution. They can also fulfill user intents via pre confirmations. The point here is that these auctions and the proposers of these auctions can be very flexible.
04:59:25.460 - 05:00:07.410, Speaker D: So as a result of this auction, you will end up having like a big super block that contains all of the individual l two blocks. They also will contain any cross roll up transactions, and this can be settled to the L one. So again, this is very similar to the PBS paradigm in Ethereum today. Okay, so let's look at shared versus isolated auctions. Well, you might ask, okay, shared auctions seem pretty good, but there's a few open questions about them. One is how do you share the revenue? In this case, where you have the shared sequencer, they're deciding the ordering of the transactions, and that means that they get to extract all of the MEV. So maybe that's not so good.
05:00:07.410 - 05:01:19.480, Speaker D: The other thing is, well, if you are going to share the MeV allocation among roll ups, how do you do it? How do you decide which roll up gets which percentage of the revenue? And then finally, is there a stable mechanism through which roll ups can dynamically participate? So perhaps a roll up wants to participate in shared sequencing only if it meets a certain threshold of value for them, but they don't want to otherwise. Is there some mechanism, we can develop that roll ups have full sovereignty and customization over whether or not they participate in this. All right, so there's many analogies for kind of this problem of like MEV or in general profit reallocation. So a few analogies are bands playing at a music festival. The festival sells tickets to the entire festival, but obviously some bands are going to bring in more attendees than others. So how do you decide which bands get which portion of the revenue? Another analogy would be a travel agency selling tickets. If the travel agency sells tickets from two different airlines, how do you decide which airline gets which part of the revenue? So this is just an open problem in many spaces.
05:01:19.480 - 05:02:05.096, Speaker D: Another example might be to kind of make this point is a shoe auction. So if you have a company that manufactures left shoes and then a company that manufactures right shoes, to us, it seems intuitive that you would sell the shoes together. Like, why would you not do that? But if we're looking at this from just a pure kind of theoretical perspective, you need to prove to both company and a that they're actually better off selling their shoes together than they are separately. So how can you do that? And the goal is that they should always be better off. Otherwise they don't want to participate in this shared auction. Okay, so that's kind of how we can frame this problem of how do we distribute Mev. All right, so now, actually, let's try to solve this.
05:02:05.096 - 05:02:59.640, Speaker D: I'm going to offer a straw man solution first. Okay? So we have each individual roll up, ABCD and E, and then we have a bundle that contains all of these roll ups. So a first attempt might be that bidders who build the blocks, they can bid on the bundle, or they can bid on the individual roll ups, or they could bid on both. Each roll up will have some bid associated with it. And basically, you either decide that you're going to bundle all of the roll ups together, or you're going to sequence them separately. And so how do you decide this? You say, okay, well, if the sum of all of the individual bids for the roll ups is less than the sum or less than the bid for the entire bundle, then you say, okay, I'm going to sequence the bundle. If the individual roll ups all add up to a higher amount than the bid for the bundle, you say, okay, we're going to sequence them all separately.
05:02:59.640 - 05:03:41.732, Speaker D: And then when you give your revenue back, you say, okay, well, each roll up receives, like, their proportion of the bid. So if you had the bundle and you had a bunch of bids, say the bundle bid was 20. And then each roll up had these individual bids, you would reallocate the revenue proportional to the amount that was bid for them. This way, hopefully each roll up is getting back revenue proportional to the amount of value that they're adding to the shared sequencing ecosystem. But you might think this has a little bit of a problem. And the problem is shill bidding. So the example is, well, let's say roll up A and B artificially bid for a really high price.
05:03:41.732 - 05:04:12.944, Speaker D: Say like in this example, they bid for like $10, even though they're only worth like three. Well, what can happen is two cases. So one, this roll up artificially bids higher. And then if the bundle wins, they get more of the MEV redistribution than they really should because their value was actually three. But now they're getting like ten back. So this is not good. You can also have the case where let's say you have roll ups A and B, who are both shill bidding and are both bidding higher than they're actually worth.
05:04:12.944 - 05:04:47.796, Speaker D: Well, then the bundle might not actually win the auction. And then you get all of the roll up blocks are sequenced individually, and you lose your shared sequencing. And basically you roll up A and b can ruin it for everybody else. So this isn't a very good solution. And the issue is that shill bidding can prevent the efficient allocation of the MEV redistribution. Okay, well, let's try to fix this. So if A and B are going to shill bid and overbid their values, well, maybe we just allow proposers to bid on only subsets of roll ups.
05:04:47.796 - 05:05:25.000, Speaker D: So instead of a proposer having to bid on a bundle of all of the roll ups, they can say, hey, I'm just going to bid on the bundle of C, D and E because I believe that that bundle matches the value. But they think, let's say they think roll up A and B, their bids are too high, they can exclude them from the bundle. So now proposers can bid on arbitrary bundles. This can be any combination of roll ups. Shill bidders, if they shill bid too high risk, being excluded from the winning bundle. So that's not good for them. And then optionally, you could burn part of the bid to disincentivize shill bidding.
05:05:25.000 - 05:06:18.730, Speaker D: So this would be an example. Instead of sequencing all of the transactions, you would just sequence together, say roll ups B, C and D. All right, so we say, well, okay, shill bidding is a problem, but I think shill bidding actually represents something that roll ups want, which is a reserve price. So why might a roll up, want to shill bid one? Yes, it could be just to increase their revenue, but it could also be that they have a minimum price that they want to get before allowing someone else to sequence for them. So the idea is that roll ups can set a reserve price. So basically this is that each roll up has this reserve price, and if no one bids higher than that reserve price, then the roll up sequences for themselves individually. But if someone does bid higher than the reserve price, then the person who bid higher will win and they get to sequence for that roll up.
05:06:18.730 - 05:06:56.368, Speaker D: And a very interesting thing about this is that the reserve price can be dynamically adjusted. So you can think of a few extremes. You could think of a roll up who wants to participate in a shared sequencer, but maybe is in a transitionary period. They could set their reserve price very, very high at first to maybe caution or protect themselves from something, and then lower it as time goes on. Reserve prices could be adjusted based on some oracle of price, et cetera. So these have a lot of flexibility and they're pretty powerful. And the point here at the bottom is what I call ad hoc shared sequencing.
05:06:56.368 - 05:07:54.980, Speaker D: So ad hoc shared sequencing is basically a marketplace for roll ups to sell the block proposal rights by the slot. So this means that a roll up can decide to participate in this shared sequencing market. For each individual slot, they can decide to sequence themselves for slot one and then do a shared sequencer for slot two, et cetera. But you might say, okay, there's a couple of caveats. You say, well, how do you actually implement this? So if you'll notice, a couple of slides ago I said that proposers can bid on any combination of bundles. Well that might work for when you have like five roll ups, but when you start to get to 100 or 1000, you have this combinatorial problem where you have so many combinations, and how does this work? How are you going to possibly find the most optimal allocation for all of the roll ups? So the solution is that you can just find a good allocation, something that's just good enough, E. G, one bundle plus a few individual roll ups.
05:07:54.980 - 05:08:45.380, Speaker D: But you run this auction well enough in advance that you give users time to find a better one. So for example, if whoever is running the auction finds a certain solution, and then a few minutes later someone else is actually, here's a more optimal solution that produces even higher revenue for all of the roll up. You can then switch to that other solution. But an important requirement of that is that you need to run the auction well enough in advance to give people time to do this and then the bids also need to be public. But if these two requirements are met, then it's very easy to check if a certain allocation is more profitable than a previous allocation. So, okay, that's problem one, and there's a solution to it. But you might say, okay, well, if the auction has to be run so far ahead of time, well, bidders don't know the value of the blocks.
05:08:45.380 - 05:09:25.028, Speaker D: So this is where it's very different than like proposer builder separation today, where blocks are produced just in time, only milliseconds before they need to be sent. You say, well, bidders don't know the value of the block, so they have to bid on the expected value of the block. But the expected value of the block probably isn't going to change. So MEV itself can be very spiky, but in the expected long run, you don't know which slots the MEV is going to spike or not. So you have to bid on this kind of flat expected value. Well, if the expected value is all the same, then the bids are always going to be the same and the same people are always going to win the auction. So have we really done much here? Because we're just going to get the same builders over and over again.
05:09:25.028 - 05:09:54.940, Speaker D: The solution, we can run a lottery instead of an auction. So let's dive into what that looks like. Okay, so we will sell lottery tickets. We'll start with some starting price of the lottery tickets. So we'll sell these lottery tickets for some price. There'll be a cap on the number of tickets for each sale and there'll be initial ticket prices. If all of the tickets were sold for some lottery, then you raise the price in the next round.
05:09:54.940 - 05:10:53.100, Speaker D: If there were significantly fewer, then you lower the price. So this way the price of lottery tickets is dynamically adjusted over time. And then you take all of the bundles that were bid on and you find some sort of revenue maximizing combination of the bundles. If there are ticket purchasers who have bid on bundles that don't make it into the final bundle, like let's say I bid on to build for roll up A and B, but actually a bundle for A, B and C one, well, then I'll be refunded since I bid on roll up or the bundle A and B. And then finally you randomly pick tickets from each of the winning bundles. So, for example, if you have a bundle of roll ups, A, B and C, and the bids for that bundle were like eight, nine and $10, you would randomly pick from the bidders of eight, nine and ten to see who actually gets the rights. And this can also be weighted on the bid.
05:10:53.100 - 05:11:29.870, Speaker D: So, for example, if someone bid really low, only one dollars, they would have a very low chance of winning this randomized lottery. All right, so finally, we're just wrapping up the final portion of this is we say, okay, so we have this mechanism where we can do Mev sharing. We run this auction or this actual lottery. It's based on these combinatorial bundles. And in this way, these bundles are capturing the actual value that roll ups have. And we're returning this value back to the roll ups. But how does this actually integrate into consensus? So, a few things.
05:11:29.870 - 05:12:22.280, Speaker D: One is proposer attest or separation. So, execution tickets, this works very well with execution tickets because the proposers are not randomly elected, but they're people who have to buy these lottery tickets. And so parties bid for the right to become a proposer, just like the parties are bidding for all of these different bundles. So this is how we implement this lottery into consensus, like proposer selection. What about censorship resistance? So a really good thing about allowing proposers to bid on multiple bundles and not have to bid on the entire bundle of all roll ups is that you get some better censorship resistance. So, for example, some roll ups, maybe privacy roll ups or other roll ups that might have, say, OFAC sanctioned transactions on them. A lot of proposers might not want to sequence for them because they don't want to deal with that risk.
05:12:22.280 - 05:13:22.160, Speaker D: So if you allow multiple proposers to propose at once, you can have proposers that say, hey, I don't want to touch this privacy roll up, and they won't bid for it. But you can have other proposers that will bid for it who don't mind these sanctions. So this is a really important thing for censorship resistance, is that you don't have just one party sequencing for everybody, because that one party might not align with the incentives for all of those roll ups. Another example might be you have a roll up that wants to do first come, first serve ordering. Well, maybe some proposers, they don't want to deal with that because that's not revenue maximizing for them, but a proposer that values first come, first serve ordering will agree to sequence for them. And then finally, an open question is, well, how do you actually support all of these simultaneous proposers inside consensus? So, this is personally something that I am very actively working on right now. The idea is that, okay, you can naively basically just run.
05:13:22.160 - 05:14:20.304, Speaker D: So if you have n proposers, you could just run n versions of consensus all at once. That would be a naive way to do it, but there's probably a lot more efficient ways, for example, aggregating things, or using some sort of a zero knowledge proof to prove that a proposer's block is valid, et cetera. But this is a pretty open question. And then finally, just some final thoughts are that how should we actually redistribute the revenue proportions to different roll ups? This is an open question. You can imagine a future where maybe you have tight knit communities like superchains or hyperchains, and they all participate as a group. That's very possible within this framework. You could say, okay, well, is this compatible with based sequencing without making changes to the Ethereum L one? So this goes back to several slides ago when I said, you have this layer 1.5,
05:14:20.304 - 05:14:53.820, Speaker D: and I'm like, well, why would you have this layer 1.5? This is one of the big reasons, is that you can actually do based sequencing with that layer 1.5 without having to make core changes to Ethereum. And to be fair, that is what espresso is doing. And then finally, how often should you run the lottery? Obviously, the lottery can be very complex. You're trying to optimize for this combinatorial auction, so the more you run it, the more complexity it has, but also perhaps the better reflection of price that you get. So anyway, that is all I have.
05:14:53.820 - 05:15:35.150, Speaker D: I guess a summary, a takeaway of this is that you can redistribute revenue among roll ups using a mechanism where you allow multiple proposers at once. So instead of having one single proposer for all of the roll ups, you allow different people to bid on either individual roll ups or bundles of those roll ups. You choose the most optimal allocation based on revenue, and then you have multiple proposers. And this has a lot of benefits, like censorship, resistance, better, more proportional allocation of revenue, et cetera. Anyway, thank you very much. Thank you.
05:16:57.280 - 05:17:15.852, Speaker B: Oh, it's all right. Maybe we're at the microphone. Okay. It just changed the slides with the mouse pad. Is it okay? Sure, we'll try it. Okay, cool. So my name is Connor, and I'm going to give a very high level talk on l two proposal and builder election mechanisms.
05:17:15.852 - 05:17:46.540, Speaker B: I thought this was going to be less of a technical audience, so I think I was promised, tried five people, I think. I haven't seen anybody yet, but there's probably some people out there. Anyway, hopefully it's enjoyable for everyone. Okay, sorry. So the talk is proposer and builder election mechanisms. So what is an election mechanism? Well, the basic election mechanism on layer one are the basic mechanisms we've seen. The first one we've seen was proof of work where you solve a puzzle, and if you solve the puzle, you get the right to build a block.
05:17:46.540 - 05:18:32.408, Speaker B: Since then, we've seen other things like proof of stake. So the proposer is typically elected proportional to the amount of stake that you have. Okay, so again, on this proof of stake model, where first there's some election based on the amount of stake you have, we've actually seen a second election mechanism happen where the proposers are actually offloading the right to build a block to somebody else. So this has been sort of become famous through MeV Boost. And in MeV Boost, you have this concept of builders communicating bids to relays. And the relays are trusted to ensure that the blocks are valid. Then the validators will look at, query all the relays, and then choose the highest value bid that they see, and the relay will send them the block.
05:18:32.408 - 05:19:04.148, Speaker B: So this process works because both the builder and the validator trust the relay. So when we're looking at this sort of transition to l two s, we're going to have to maybe address the trustedness. And adapting something like MV boost also involves some trust. So we might not be able to use it directly, but what can block builder elections unlock? So the first thing that we need from a block builder election or a proposal election is civil resistance mechanism. Right. So that was what proof of work did for us. And proof of stake does a similar job.
05:19:04.148 - 05:19:48.388, Speaker B: The next thing we have is validator builder separation. So with a builder market, we can actually allow the validators to become these small nodes, as was discussed earlier on, and offload this builder work that's sort of computationally intensive. The next step is to actually incentivize builder innovation. So, again, we saw today that there is some specialized teams emerging as a result of this stuff. Whether for good or for bad, these guys are implementing high tech solutions to this problem. And then the final step, hopefully, I'm not sure if we're there yet, is where with all this innovation happening, we get to a point where the users are actually maximizing their welfare. And is that possible with the current paradigm? Possibly not, but there's a potential that it might allow us to do this.
05:19:48.388 - 05:20:35.008, Speaker B: Okay, so the teams of this talk. So after this high level introduction of election mechanisms and proposer builder mechanisms, I want to give you a high level. So the focus is L two s. So I want to impress upon you that the decentralized nature of l two elections today, I'm going to define some proposer builder mechanisms that we might be able to, election mechanisms that we might be able to use. I am going to sort of hypothesize about the decentralized l two elections up tomorrow and then talk about some solutions that will get us there. Okay, so the state of play currently today. So we have basically all of these top l two s, and they're all using trusted, trusted sequencers, sorry, the majority of them, we obviously have Tyco that's based polygon, proof of stake that's permissionless, but has other issues.
05:20:35.008 - 05:20:58.360, Speaker B: Is it a roll up? Is it an L two? Not necessarily sure. And we have aztec fairnet. So Aztec is obviously under construction. This Fernet proposal potentially is permissionless, but again, it's a proposal. It hasn't been implemented yet. So we're basically in this trusted environment. And the point of the talk today is to sort of maybe talk through some possibilities to decentralize these trusted players.
05:20:58.360 - 05:21:39.848, Speaker B: Okay, so there's lots of different ways that we can define l two election mechanisms. I spent the last six months looking at this, and I came up with something like maybe 200,000 combinations of properties that can sort of define what we want from an L two election mechanism. Three of the main ones, the most important ones, are who are the candidates. All right, so what sort of people can we choose from? Trusted, permissionless, and based. These are sort of three high level, high level categorizations. When the election happens, is it early or is it just in time? So MMV boost is a just in time auction. A proof of stake mechanism is typically early, and there's different combinations of these things that we can use, such as execution tickets, which I'll talk about in a little bit, and where the election happens.
05:21:39.848 - 05:22:22.708, Speaker B: So MV boost is happening out of the protocol, while in protocol mechanisms like proof of stake or execution tickets, they're happening in protocol. And what are the benefits of each of these? I'm going to go through sort of a combination of some of these nontrusted solutions and see which ones make sense and which ones don't. Okay, so just to give, again, we talked about this today a few times. Who are the election candidates? So, trusted candidates, typically elected by protocol governance and can't be removed without some sort of governance procedure. Permissionless people or permissionless proposers, anybody can seek election. So examples include proof of stake, proof of payment, proof of work, and then based. So, in the base model, we've seen a couple of people talk about base already.
05:22:22.708 - 05:23:21.540, Speaker B: The LT proposers are decided by the L one proposer mechanism, but the base roll ups decide that this is absolutely fine and this is sort of a different one that's not necessarily trusted or permissionless. Okay, so I mentioned I was going to go through three different combinations. So we had the type of candidates in the election. Now we have when the election takes place. So in this MeV boost picture we had from earlier on, we had two elections taking place, the validator, which was an early election mechanism, and then we had the builder, which was just in time, just before the block is propagated to the network, the auction is settled between the validator, the relay, and the builder, and the block is propagated. The builder only finds out that they're going to build a block in the instant before the validator or the instant after the validator sends a signature to the relay. Okay, so some of the thoughts that have been generated around these early versus just in time debates.
05:23:21.540 - 05:24:02.550, Speaker B: So I found this really cool article online. Kintas wrote this paper, wrote this article. So basically, just to go through this statement, so, early auctions, this is maybe an opinionated piece as opposed to something that was derived with straw mathematics, but it's based on a lot of truth. So early auctions are likely to be won by a very small set of parties who find themselves in a position of a monopolist capable of extracting rents. So once you know that you win, you can then possibly leverage that position to extract rents. Two of the main benefits of early auctions appear to be pre confirmations and reward smoothing. So we've seen a lot of people talking about pre confirmations today, a lot of hype there.
05:24:02.550 - 05:24:47.184, Speaker B: But again, this is sort of one of the two main benefits that we see. It is not clear how much more valuable these benefits are in early auctions compared to just in time where those benefits may also be possible. I'm not going to necessarily go into why those may also be possible in early auctions. Sorry, just in time auctions, but we have things like see your lists, inclusion lists, and potentially even forcing execution lists that may force or that may be able to allow the benefits of early auctions to also be applicable to just in time. But again, see this article for more information. And then we had this third categorization, which was in protocol versus outer protocol assumed. So the protocol itself assumes that X is going to be building the block, but x actually decides to delegate to y in protocol enforced by the protocol.
05:24:47.184 - 05:25:41.120, Speaker B: So the protocol itself actually knows which private key is going to be producing the block, and through this election mechanism, the protocol knows who's going to be producing the block. Okay, so just going back again, maybe I've sort of, maybe derailed slightly from where I was planning to go, but just to sort of bring it back to, these are the type of election mechanisms that exist, but we've seen from the previous table that I had that all of these election mechanisms are based on this trusted assumption. So the fact that we have early versus just in time, we're in protocol versus out of protocol isn't necessarily important because we have a trusted entity at the center of everything. So we're seeing a lot of L two s talking about decentralizing. So if on this decentralization path, they're going to have to transition into some combination of these other things, and I'm going to talk about those now. So decentralizing the proposal role. So this section is going to go a little bit deeper.
05:25:41.120 - 05:26:29.460, Speaker B: Again, I wasn't expecting to go too deep, but still we're going a little bit deeper here. So we're going to go from the initial layer of builder markets to layer two. Well, that's what I'm calling it. So some decentralization questions just to warm up before we go into each combination of election that we can run. Right. So should an elected l two proposer outsource block building? So we're seeing in l one that the elected proposer is outsourcing to the MeV boost market or the PBS market. Should an l two proposer do the same thing? And another question, can we remove the need for outer protocol election mechanisms without trusted third parties? So I'm going to sort of maybe propose some solutions to these things or some pros and cons, and then get sort of maybe into the categorization that I built up earlier.
05:26:29.460 - 05:27:16.900, Speaker B: So when I was thinking about whether or not we should run these, to get trusted proposers to run these builder markets, Kintas basically sent me this question, and I was like very resounding. So why LT proposers would outsource to a block builder market must be discussed. So a reason for this or some reasons for this. So the proposer enforced rules can provide equivalent guarantees as the proposer themselves. So we're seeing things like CR lists, allowing proposers to actually, or proposers to potentially exert some influence over the blocks that get built. So in the worst case scenario, a trusted proposer or a proposal that gets elected can basically just dictate to the builder. I have all of these conditions for the blocks.
05:27:16.900 - 05:27:56.688, Speaker B: If you don't satisfy these conditions, the block is invalid. So the proposer should see this as a way to get some value, additional value than the blocks that they would produce themselves. Potentially, they're getting at least the same revenue that they would if they're building the blocks themselves. Almost definitely greater because they're just accessing a decentralized market of builders. Builder markets typically adapt to meet demands. So we've seen today where the builder builders that we were talking earlier on, they're going to adapt when they see what demands actually happen, as opposed to trying to predict what's going to happen, as opposed to protocol designers or dows, which they have to predict what the community will want beforehand. So it's a different way of approaching problems.
05:27:56.688 - 05:28:39.912, Speaker B: And typically, again, there's pros and cons to each. But this is sort of true generally for these builder type entities. And the meme of why we want to outsource block building, based on the discussions earlier on, we want to keep the validators small nodes that the low barrier to entry and outsource this hard computation to other people or with specialized entities reasons against this. So adding an extra layer of protecting the proposal and the block building market introduces complexity. This added complexity introduces costs. So they're upfront cost, but cost all the same. And the meme against this, that's sort of resounding true to l two dows is outsourced, builders equals mev.
05:28:39.912 - 05:29:27.096, Speaker B: Now, again, is this a sort of a relevant comment, or is this just sort of like maybe a misunderstanding of what outsourcing building does? It's not necessarily clear, but this is the view that they have. And again, whether it's true or not, we'll see. So the complexity paradox of blockbuilding. So this was sort of a nice way of putting it from dmarts, where when considering whether or not you should outsource block building, you're sort of faced with this dilemma as a proposer. Increased complexity, but greater welfare, or less complexity, but less costs. So which one's actually better for the user? It's not necessarily clear, but again, that's up to you. This talk is not necessarily opinionated, but just to give people to sort of generate their own ideas.
05:29:27.096 - 05:30:24.930, Speaker B: And again, probably the welfare is better, but again, the cost may be significant, and again, that's up to the actual roll up themselves. Okay, so I posed two questions earlier on, maybe you don't remember, but the second one was, can we remove these out of protocol trusted third party elections? So the first talks today, people were here. One of the things that people kept mentioning was the vertical integration or the verticalization or something to do with the word vertical. And that basically meant that all of the entities through the MEV supply chain were becoming the same entity, or that was the path that we were on. So trusted outer protocol elections, there might be the cause or there might be the effect of this process. And basically that's probably a bad thing in the tradfi market, bringing it back to the tradfi people that hopefully are out there, that's equivalent to one of one colocation with an exchange. And that's just basically something that we just don't want to be happening.
05:30:24.930 - 05:31:24.404, Speaker B: Another reason. Okay, so that's sort of motivation for why we want to do this. So what protocols can we use in protocol solutions? Probably need early auctions. So there is actually proposals for just in time auctions, removing the trusted third party. But the solution that we have for this is based around this enshrined PBS concept. And then I just pose this whenever because I've seen even some of the EF guys questioning the possibility of EPBs, or is EPBs the way forward? Is it going to be the end solution? It's not necessarily clear to me, or possibly just. That's sort of the easy way to do this, but it's possible this last point where trusted execution environments suave might offer us a workaround that may not actually involve any trusted individuals, but the trusted execution environment themselves is a separate issue.
05:31:24.404 - 05:31:41.268, Speaker B: Okay. Right. These were the warm up questions. Hopefully you're still with me. So let's go back to the categorizations that I proposed earlier on. So we had the entities, right? So we had trusted based permissionless. We're getting rid of the trusted.
05:31:41.268 - 05:32:05.244, Speaker B: So now we're left with base permissionless. And when it comes to based, we basically don't have too many options. We either have to choose. So we don't have options in terms of in protocol or outer protocol. It probably has to just be in protocol. Again, there is an outer protocol option there where the proposers are running MV boost, but let's just go with this one. So when it happens, we have early, and in this early mode, we have two possible solutions.
05:32:05.244 - 05:32:30.184, Speaker B: We have execution tickets, which again, espresso. We're talking about Ellie. And we have this value capturing base roll up idea, which I wrote an article on this a while ago. Or if we do adjust in time, we have this alternative solution, which is called alternate PBS. Another article that I wrote, I was really into base roll ups for a while. Still am. But again, maybe for a different reason to everybody else, but still, I think they're pretty cool.
05:32:30.184 - 05:33:01.504, Speaker B: If we go the permissionless route, we have these other options where we. So this is the tree going down the tree. We can either run earlier, just in time. If we go early, we have imported call solutions. We also have out of protocol solutions, but they're not so interesting. The imported call solutions that we have execution tickets or proof of stake. So proof of stake is sort of the base case, and it probably isn't the best way to outsource block building if we're trying to sort of, maybe try to outsource the block building to specialized entities if we're bringing those entities back into the system.
05:33:01.504 - 05:33:33.772, Speaker B: So execution tickets seem like a way that we can split up the validators and the builders. So that seems to be, again, execution tickets coming up multiple times here. If we go just in time, we can go in protocol, which is enshrined PBS, or we can go out of protocol and this outer protocol. So again, everything in this tree is removing trusted entities. Right? So out of protocol, we can possibly use this Te or suave. I didn't want to name drop Swav, but again, it's a very veiled intention. Okay, so the solutions maybe just giving you a high level understanding of the solutions, in case you haven't read my.
05:33:33.772 - 05:34:10.056, Speaker B: If you haven't been following my blogs. Okay, so we have execution tickets, which, again, wasn't my proposal to propose the whole block. Okay, so this is maybe just two distinctions of execution tickets that I didn't get into. So I mentioned that there was originally 200,000 possible combinations of auctions. One of those other combinations that I didn't get into was whether we auction off the full block or we auction off some of the block. So again, there's a lot of people or specific people in the industry that are talking about possibly running two or n possible slots for impossible sub blocks for any given slot. But we can go a little bit further than that.
05:34:10.056 - 05:34:38.450, Speaker B: We can get a bit more crazy, especially in this early stage. We can talk about auctioning off access to specific smart contracts, specific roll ups, specific accounts, specific parts of the block. This parts of the blockaccounts framework is something related to the state lock auctions in Solana. It's possibly a way forward that you're not buying execution tickets for the whole slot. You're buying execution tickets that will touch only your own account. And any other account may be invalidated. But you have that unique right.
05:34:38.450 - 05:34:57.396, Speaker B: Okay, these are ideas. The entry PBS. So an honest majority committee runs the permissionless auction. So again, if. Whenever. I'm not 100% sure, and then there's some other solutions. Again, these solutions I came up with are in conjunction with the flashbots and Nethermind team.
05:34:57.396 - 05:35:44.352, Speaker B: So alternative PBS for base roll ups, where we have this trusted in protocol committee with a permissionless override. So the trustedness is still there, but there's permissionless override, which should keep these people honest. And then we have value capturing based rollups where we use something related to a Harvard attacks, where you're basically stating the value for producing a block, and that can be bought off you, again, specifically for base roll ups, where you can actually lose your right to someone behind you. And this value should technically outsource, outsource building. But is it removing the big nodes from the system? Again, that's a separate issue. And then don't forget your tea. I actually removed the tea slide because I felt like I was drinking midnight tequila last night, and I decided that it might be a bit too heavy.
05:35:44.352 - 05:37:07.170, Speaker B: But the slides are available online, so people can have a look at them in their own time. That's everything. When do we actually continue?
05:37:17.810 - 05:37:26.938, Speaker D: Give it like four minutes before. Get the crowd back in, because it's.
05:37:26.954 - 05:37:32.830, Speaker B: Been like, no breaker, no, just with us.
05:37:32.980 - 05:37:34.798, Speaker D: Supposed to be one. Don't know where it went.
05:37:34.884 - 05:39:52.080, Speaker B: All right. Yeah, exactly.
05:39:53.010 - 05:39:54.654, Speaker D: I think we can just get.
05:39:54.852 - 05:39:57.714, Speaker B: I think we should start, really, we should start now.
05:39:57.752 - 05:40:00.482, Speaker D: Are you waiting for something where you got what you need?
05:40:00.616 - 05:40:45.194, Speaker B: I have everything. I'm waiting for Tina to give me a signal, but. All right, cool. So I'm Christophe, and I present you something on shared sequencing economics. So, this is loosely based on some recent FC paper, financial cryptography paper I had with Akaki from offchain Labs. We want to understand the economics of shared sequencing. So, context.
05:40:45.194 - 05:41:25.370, Speaker B: So we have currently still. People come in. All right, we currently have around about 50 roll ups on Ethereum, as we're speaking today, are checked on l two beats, either live in production or currently sort of to be deployed. And that seems to satisfy sort of the original goal of the roll up centric roadmap. Right. We wanted to scale Ethereum, so, which means we wanted to have many roll ups in order to scale the throughput of the base layer. But we have then this additional problem of now, fragmentation of usage.
05:41:25.370 - 05:42:07.930, Speaker B: And in particular, if you're interested in financial applications, you have fragmentation of liquidity across several domains. Right. And that's sort of not good. And of course, Vitalek has already written about this in 2021, right in his end game post. So there was one particular scenario where you have this different roll ups, sort of being many roll ups. And then he had this idea that probably there should be cross domain MAV extraction that would link together all of these different roll ups. And you can think of shared sequencing as a technology that might facilitate that MaV extraction or that might facilitate sort of moving liquidity around between different domains.
05:42:07.930 - 05:43:14.970, Speaker B: So that was the starting point of this project. And then we were wondering, okay, those questions might be obvious to you, but we wanted to understand them theoretically. So suppose we have shared sequencer and we have this cool functionality that we can have atomic executions of bundles of transactions on different roll up domains, right? So it would be super cool if I can place one lakh of my trade as an arbitrage arbitrum and one on optimism. And I'm guaranteed that they both executed or neither of the two are executed. And then you could be wondering, okay, what does it do to MeV extraction? What does it do to cross domain trading? What does it do to overall market efficiency in the system? And what does it do to revenue for the sequencers or the sequencer, the shared sequencer and so on. What does it do to revenue on each sequencing domain? And you can now answer this sort of as an armchair theoretician because they are not live yet anyway. So you can go full theory berserk and do game theory.
05:43:14.970 - 05:44:03.918, Speaker B: And that's what we did. So here's a mental model I like about trading on these different domains. So you can think of in domain or cross domain trading as a contest. Trading as a contest. And sort of, if you think about designing transaction ordering policies, what you actually also do, if you do shared sequencing, you sort of orchestrate a contest or the rules of a contest, a trading contest, right? So if you think about what roll ups are nowadays doing first come, first serve, you basically design a contest around faster inclusion. If you do priority gas auction like an ancient days on Ethereum, you incentivize some kind of bidding competition. And now here's the shared sequencing sort of idea.
05:44:03.918 - 05:45:03.090, Speaker B: So if you do shared sequencing, basically what you do is you reduce noise in the competition, right? You make it easier to coordinate between different domains and that sort of removes some kind of noise in the competition. That means first of all, maybe you can realize more MeV gains that you previously couldn't realize and maybe that will be beneficial to those competitors in this context. That's the entire idea. Now let me give you a minimal model of that economic situation. So think about two searchers competing to make a profitable cross domain up trade, MeV trade, whatnot. So it has some value to be executed. And then if you want to win this competition for the trades, then you would like to place two transactions, one on two domains, right? One on arbitrage, one on optimism before the competitor.
05:45:03.090 - 05:46:54.962, Speaker B: If you're successful, if you manage to put in both before your competitor, then you're good, right? So you can generate that value for yourself. Now, if you think about this competition, you can generically think about a competition by investment in some kind of signal, right? So if you're a searcher in this world, then what you can do is you can sort of invest to be better in this competition. And I give you some kind of interpretation of that. So if you're in a first come, first serve world, then you basically invest into reducing your timestamp, right? You invest in reducing your latency. You invest in colocation, for example, right? So that's your investment, your signal, right? In a PBS world, you have a very sort of complicated ecosystem, but in the end, you invest in some way or another in a bit, right? Or maybe in some kind of infra that helps you placing your trades on top of the block or in the position that you like them, right? And if you think about other kind of solutions, like arbitrum has proposed and I've worked a bit on, you might do a combination of both, right? So you might do some latency investment plus bidding and so on. So depending on how you set up those rules, you can get a different kind of way of organizing this competition. But in the end, what you have is some kind of signaling strategy and some kind of investment by those searchers in this world, okay? Now I wanted to talk a bit about this randomness in this contest, right? So I told you, okay? My mental model of a shared sequencer, like on a bird's eye high level view, is just a way of reducing noise in the competition.
05:46:54.962 - 05:48:33.350, Speaker B: So you can think of a noisy competition, right? So you make your investment as a searcher, as an arbitra, but you're not sure that you win, right? So there's always some noise component to it. There's still some random factors in this world. And so in the end, what you have is some kind of model where you would combine some signals that you produce from your investment plus some random factors that are out of your control. So that's the kind of model we have in mind, okay? And then you win, basically, when your signal plus your noise is better than the signal plus noise of your competitor, all right? And now you have a bit of a grip on what does it do to competition if you compare shared sequencing regime to separate sequencing regime, and it's simply for the fact that you now have compare a situation where you need to win sort of one global competition versus a situation where you need to win several sort of local races, right? Think about the situation where you have to place one trade on one domain, one trade on the other domain. What you have to do is you have, in the shared sequencing case, you just need to produce one signal, and that signal plus noise should be better than that of your competitor. If you do separate sequencing, you produce two signals, right? And both of them plus a noise, the added noise should be better than those of your competitor. So in mathematics, you compare basically a cumulative, and then you compare the squared of the cumulative.
05:48:33.350 - 05:49:51.178, Speaker B: If you think about independent noise, right? Maybe there's some correlation, right? So if I'm good in placing trades on arbitrum, I'm also good in placing trades on optimism. So it might not be independent, but certainly it's not perfectly correlated, right? So you have more variance in this shared sequencing regime. Okay, so what does it tell us in the end? What does it tell us about my initial questions about, okay, do we have more efficiency, more investment, more revenue, et cetera, pp. So you can solve this model, assume some stuff, find a symmetric nash equilibrium of this game, and then it can tell us certain kind of things about the effects of domain integration, right? What does it do in the end? The model tells you there's an equilibrium, there's a threshold on value, right? So if the value of the opportunity of the ArP is too low, people will just not extract it, right? It becomes too expensive. You wouldn't do that. If it's high enough, there is some threshold after which it's high enough and worthwhile to exploit it. Then both bidders enter in this sort of competition, they try to outcompete each other, and then the investment obviously is increasing in value and depends on the variance in this competition.
05:49:51.178 - 05:50:59.694, Speaker B: Here's a punchline. If you compare shared sequencing to separate sequencing, then just this threshold is lower, right? Which seems to be obvious, but it's good to prove mathematically, right? So intuitively there are more opportunities that you can extract in this game, because now you do shared sequencing, you can extract more MeV, you can have more trades, and that should have a positive effect on all kinds of global parameters, right? Liquidity provision and so on. So that's sort of maybe not the most surprising part, sort of meat spacing intuitions. But then you can also think about actually conditional on entering this competition. What does it do to investment? So you can think about, well, do those searchers actually compete sort of harder? Do they invest more in the world of a shared sequencer versus the world of separate sequencing? And so now you have two effects, right. So you basically charged them twice in some sense. If you do separate sequencing, they're still competing in this cross MeV extraction game, one on each domain.
05:50:59.694 - 05:51:40.386, Speaker B: But then you also have sort of a more reliable signal, right. So you have more returns on investment if you do shared sequencing, right. So you have a better control of your outcome. So your investment controls here gives you a better idea of given what you invest, you have a better idea whether you will actually gain something. So those are the two effects, and then you can trace them off, right. So what it does in the end is gives you kind of more like nuanced answer of equilibrium investment. So you have a dependency on the noise in this contest, and you have a dependency on market structure on the sequencing rule, right.
05:51:40.386 - 05:52:22.814, Speaker B: So it depends, actually that's what we find. It depends very much on how you organize a competition in the different sequences in the shared sequencer. Give you a bunch of examples. So we looked into constant cost elasticity, which is maybe an okay approximation to what we would expect in a first come, first serve competition. And there you have basically a result of the kind. Okay. In a shared sequencing regime, not only will people will enter the competition for lower values of the arbitrage, but also conditional on entering, they will invest more.
05:52:22.814 - 05:52:42.806, Speaker B: Right. So they will compete harder in this regime. Okay. So that's sort of the bottom line of this proposition. Then we looked into another kind of sequencing rule, time boost. I don't know whether you know it. That was a proposal arbitram about half a year ago or even longer ago.
05:52:42.806 - 05:53:35.800, Speaker B: So where they basically came up with a sequencing rule where they have the order transactions based on a score, and the score combines a timestamp component and a bidding component. So people send in a bid together with a transaction. We take some function of the bid and the timestamp, call that the score, and then order transaction based on that score. Okay. And in that case we figure out, okay, there it depends very much on the value of the art and on the noise distribution. So for this kind of sequencing policy, it can be sometimes the case that if you do shared sequencing with it, that you generate lower revenue than you would if you do separate sequencing. So it really depends on these different parameters of this game.
05:53:35.800 - 05:54:38.678, Speaker B: Finally, if you do some kind of PBS type of market structure, I mean, it's very hard to come up with a sort of reasonable idea of what is the cost structure of the competitors it is in this market, right? So there are all kinds of things that collectively create a signal for them. So they might pay some expenditure on fees, on tips and so on, on infra, on some kind of payments, intermediaries in the MEV supply chains. It's not entirely clear, you tell me. Maybe it's some kind of almost constant marginal, I would say, right, because there are substantial economies of scale in these kind of markets. But regardless, what you can find in the end is you might have some kind of, depending on the cost structure, you have some kind of situations where shared sequencing has ambiguous effect on sequencer revenue. It's not always that you make more money if you do shared sequencing, which might be surprising. The first part was less surprising, right? So overall more value is generated, overall more MEV is extracted.
05:54:38.678 - 05:55:38.974, Speaker B: But from the point of view of the sequencing entity, if it wants to generate some revenue, it might be that it makes less if they integrate the domain rather than if they stay separate. So TLDR of all of that is mechanism matters, right? So for this comparative statics, oops. So that sort of looks at the global pie that you can extract, right? Of course you can also look sort of what we've seen in previous talks. We can look at revenue across distributional concerns of the kind of, so there's one domain in another domain, one domain generates less values in another domain, right. And so they might have conflicting or different motivations to integrate. Right. So they might have more or less of a gain from integration that has bearing on whether they actually want to do that.
05:55:38.974 - 05:56:06.290, Speaker B: Right. We also heard about we have to solve this problem of revenue sharing. So even if we make overall more revenue, we have to think about how we do that. And that also has bearing then on who will participate in these games. And of course all of this is sort of a theoretical exercise. I have absolutely no idea what is the size of unrealized cross domain MoV that would get unlocked with shared sequencing. Right? So you can only have gas.
05:56:06.290 - 05:57:17.838, Speaker B: Gas is around that. So those are some caveats, okay, to wrap that up a bit. So if you do this kind of comparison, horse racing comparison of shared and separate sequencing, and look at it from a very much bird size view on just the structure of competition that it induces, then you find that the mechanism that you use, the transaction portering policy that you use or the market design that you use determines very much who gains and loses from domain integration and actually how much we gain or how much we lose from domain integration. So mechanism design matters a lot. And sort of second sort of conclusion I would give is some people or some entities, some players in this game might benefit a lot, right? Might benefit a lot from integration, and some might do less. Or there are even circumstances where you make less revenue from integration, domain integration. So if you really want to bootstrap this entire thing, right? If you really want to, because there are economies of scale, right.
05:57:17.838 - 05:57:34.340, Speaker B: Shared sequencing only works if many sort of participate. Then I think some serious persuasion is necessary to. You need to talk to people that. To convince them that they actually make more from chat sequencing. All right, think that's it.
06:01:32.250 - 06:02:02.454, Speaker D: Hello. Hello, friends. All right, someone help me make the mic louder. Hello. I hope that you will be awake for this part of the debate. So we're going to have an Oxford style debate at one of the oldest markets in the nexus of the world. So let's stay tuned for it.
06:02:02.454 - 06:02:51.510, Speaker D: But before that, what I would like you to do is go on MeV market. The agenda is being live updated as we're procrastinating and close to running out of time. So please open up the page on your phone and scroll down to the great sequencing debate there. We have inserted a poll. Apologies for the Google form. I wish we had more time to prepare with open source technology. But here there are two statements, one for each poll, that I would like to ask everyone in the next five to ten minutes to make your choice.
06:02:51.510 - 06:03:54.340, Speaker D: Yes or no to the question, we'll take a snapshot. At some point during the debates. And after the debates, we will ask you to decide whether you want to alter your opinion on the particular statements. And we will take another snapshot. And that's how we determine the winner for the full flow of the debate is projected on the sidewall over here and as a modified version of the Oxford style debates. But before we start, since it's a very complex and nuanced subject, we're asking Ellie to give us a framing of the nuances and the technical trade offs. Actually does my mic.
06:03:55.270 - 06:03:55.874, Speaker B: This one.
06:03:55.912 - 06:04:29.146, Speaker D: Okay. All right, so we're going to have two debates. The first one will be shared sequencing or not shared sequencing. The actual question is a little bit more nuanced than that. And then the second one is going to be based sequencing or non based sequencing. Okay, so I'm just going to give a frame of the first debate so shared versus non shared sequencing. Okay, so a lot of this is kind of similar to the things I mentioned earlier in my talk and that were mentioned earlier in other talks.
06:04:29.146 - 06:04:55.474, Speaker D: So we have non shared sequencing. Each roll up has its own isolated protocol that determines the sequence of all of the roll up transactions. This is the state of l two sequencing today. And then we have shared sequencing, which consists of two main parts. The first part is a proposer assignment mechanism. So this is what determines the set of proposers for the next roll up blocks. Usually this is just one proposer.
06:04:55.474 - 06:05:28.210, Speaker D: So you think in the vanilla based sequencing, this proposer is the L one proposer. It's just one proposer that proposes for all roll ups. But this could be some set of multiple proposers. But the point is that all the roll ups agree and they use the same proposer mechanism. The second property is some finality gadget. So this finality gadget would finalize all of the proposed roll up blocks, and it provides a confirmation to users that this is in fact finalized. So these two properties are completely separate.
06:05:28.210 - 06:06:13.710, Speaker D: They're very separate, but there is a lot of benefit in sharing them. So, for example, if roll ups use the same proposer assignment, but they have different finality gadgets, that's kind of hard then to reason about your interoperability, because one is final under some set of conditions and another roll up is final under another set of conditions. So the finality gadget is the base. Finality gadget is always ethereum for all of the L one roll ups. But you can imagine maybe another case where roll ups share kind of this layer 1.5, which is a fast finality gadget that can come to finality a lot faster than Ethereum comes to finality. But then again, everything is eventually settled to Ethereum.
06:06:13.710 - 06:07:18.370, Speaker D: So here's a few examples of what a shared proposer assignment is. So, as I said, I'll start with the OG based sequencing, which is where your shared proposer is, the L one proposer. This is probably what's most familiar to most people. You can also have a fixed shared proposer set, which is basically some set that you go through, round robin, et cetera. And then you can have a dynamic auction, which is what I mentioned earlier, where you dynamically choose and elect the proposers. So the question for this debate, which is the more nuanced version, is, is it desirable to have a future where you have global shared sequencing, where Ethereum, l one, and virtually all the roll up blocks are sequenced by the same entity within one slot? So the question is this a desirable future? And note that this question is separate from the question of is there value in sharing a finality gadget? So that's out of scope for this. So this question was modified at the very last second.
06:07:18.370 - 06:07:55.246, Speaker D: So here's some common arguments. They might not match up quite with the question, but I think that they're still useful for us to talk about. So, common arguments for sharing a proposer assignment. So one is that you optimize for cross domain user intents. If you have a shared proposer assignment, you can agree to have shared proposers and do these cross chain arbitrages, et cetera. Another argument is that if you combine it with real time proving, this enables trustless synchronous composability, which is very important. You can actually get this without real time proving using something like Polygon's AG layer.
06:07:55.246 - 06:08:43.742, Speaker D: But if you have this shared proposer assignment, you can enable this trustless synchronous composability. And then finally you can say that, well, if you don't have shared sequencing, parties are going to try to be shared sequencers anyway, because there's value in sequencing for more than one wallet roll up at a time. So you might argue that shared sequencing is kind of the inevitable end goal. So some common arguments against shared proposer assignment is that it's not optimized for front running protection. So if the shared proposer isn't trusted, or they aren't randomly elected, or you don't have some sort of mechanism to prevent this, the shared proposer is going to have maximum rent seeking behavior. And maybe that's not what roll ups want. The second thing is that it might create bottlenecks that reduce performance on the L two.
06:08:43.742 - 06:09:36.180, Speaker D: So yes, cross chain transactions are great, but you might say the amount of cross chain transactions are very small compared to just the individual roll up transactions. And maybe having this shared proposer is going to create bottlenecks because you're trying to have all of these l two state combined, when a lot of it actually does not need to be combined because it's completely logically separated. You could also argue that some roll ups simply don't benefit from it. You could argue maybe some roll ups, maybe you have some sort of a gaming chain that really, it doesn't have a lot of use for cross chain transactions, and if it does have use for them, they really don't need to be atomically executed. That's also an argument. And then finally you could argue that roll ups want to keep their MeV. So if they have this shared proposer assignment, they might lose their MeV, although this was addressed in my earlier talk.
06:09:36.180 - 06:10:02.110, Speaker D: Yeah. So that is, I'll go back to the question. The question is, is it a desirable future to have global shared sequencing where you have one entity that is sequencing for both Ethereum and almost all of the roll up blocks within one slot? All right, I guess now it's time for the debate. It.
06:10:23.920 - 06:10:26.830, Speaker B: Yeah, why not? It.
06:10:38.680 - 06:10:50.600, Speaker D: Okay, so we're going to have Justin go first with his opening remarks, I guess, in favor of this question. So I'll hand it over to Justin.
06:10:51.920 - 06:11:24.176, Speaker B: Okay, great. So I guess one point here is around understanding the benefits and the cost. In terms of the benefits, I see shared liquidity as being something that users will want. I also see shared pre confirmations as something users will want. Imagine that you have a complex application, for example, a marketplace, which involves identity. It involves an escrow agent, an insurance mechanism. It involves reputation.
06:11:24.176 - 06:12:04.220, Speaker B: And all of these things live in different execution environments. Now, if I want to make a complex transaction, I have to interface with all these different counterparties. And basically your Ux falls to the weakest link, like whoever is the slowest to provide you a pre confirmation, that's going to be your pre confirmation latency. If one of them kind of rugs you, then now your whole transaction kind of has to reverse because you want everything to go through or none of it to go through. So I think there's going to be market demand for this synchronous composability. And in some sense we can't avoid it. It's going to happen, whatever, in all futures.
06:12:04.220 - 06:13:14.708, Speaker B: Now, one of the things you can try, can do is fight it, where you basically give sequencing and pre confirmation rights to independent parties. But what I expect will happen there is actually even more complexity, and basically the shared sequencing layer will be built on top of it. So you can imagine off chain markets and very fancy maybe Suaf based markets that aggregate and allow for each individual proposer to delegate their sequencing rights to this mega sequencer. And so in some sense, what shared sequencing is all about is minimizing the cost of shared sequencing, which is a complexity and sophistication, all while still giving to users what they want, which is the synchronous composability. Now, talking to Dancratle just a few minutes ago, my understanding is that he wants to try and minimize the cost of builder centralization. And one of the things that he'd like to see is basically homogeneous virtual machines. So if I want to build a shared sequencer.
06:13:14.708 - 06:13:54.420, Speaker B: I basically need to specialize in one virtual machine as opposed to 100 virtual machines. And I think there is merit to this. And I think we are going to see a parallel distribution of virtual machines, but I still think that we are going to see application specific virtual machines that specialize in gaming or auctions or dexes or liquidations or whatever it is. And in some sense, if we want to see this long tail thrive, we don't want them to be isolated as islands, because if they're isolated, then now they're not connected to the rest of the economy, and it's much harder for them to survive. And so in some sense, shared sequencing allows for a thriving and diverse long tail.
06:13:56.120 - 06:13:59.528, Speaker D: All right, the rebuttal, right.
06:13:59.614 - 06:15:09.020, Speaker B: So I think the potential ux advantages of shared sequencing do exist. I think there's a debate to be had about how big they are going to be, because right now a lot of crypto users are like DGEn power users, defi traders and so on, and they benefit a lot from atomic composability. I think with the roll up system we will see an ecosystem emerge that will have a lot more applications. Applications will probably congregate around certain roll ups, and they will have most of what they need on one roll up, rather than being distributed across several different ones that have to be composed atomically. I think that's not a practical future, even in a highly shared sequencing world, because there will be large costs to it. And I believe that that is actually like what a rollup will ultimately aim for. Like each roll up will get its own, will develop its own ecosystem that has some independence of the general ethereum ecosystem.
06:15:09.020 - 06:15:49.316, Speaker B: But there are undeniable UX advantages. The problem with shared sequencing is I see it as potentially the biggest centralization risks that exists in Ethereum, especially if we actively try to work towards such a future. Centralization can come from many different layers. We know that by now. It can come from the staking layer, it can come from the builder layer, which we're seeing to some extent, relay layer, I don't know. It can even come from the application layer. Like if one application takes over most of activity on Ethereum, it's also a centralization risk.
06:15:49.316 - 06:16:43.176, Speaker B: And I think the problem that I see with these shared sequences is we are through the back door building a component into Ethereum that comes with a very high concentration risk. And the concentration risk is because the cost of market entry is going to be very high. My problem is that someone has to build a piece of infrastructure that potentially interacts with hundreds of different roll ups, each with their own, not just virtual machine, but systems, different smart contracts and so on. This is going to be a highly sophisticated, not just piece of hardware. I think that part is almost trivial, but even that's going to cost millions, but it's going to be proprietary software, because once you build that, there's no interest in putting that into open source. And we don't control it, we cannot design for it, because it happens organically. This just happens.
06:16:43.176 - 06:16:51.420, Speaker B: And so we're going to have a big problem that there will be very few of these, and they will essentially be in control of the ethereum roll up ecosystem.
06:16:52.960 - 06:16:55.820, Speaker D: All right, Justin, your rebuttal?
06:16:57.280 - 06:17:47.180, Speaker B: Great. So I guess one rebuttal is that if there's going to be a very long tail of virtual machines, we're going to see many builders specialize for each individual virtual machine, and that's basically going to be searchers. So what is a layer one searcher? Behind the scenes will be a layer two builder. So basically, builders today, you can think of them as being centralized, like Beaver is very scary. They build half the blocks. But actually behind the scenes, there's dozens of entities which are called searchers that do provide some amount of decentralization and robustness there. And if you do want to compete as a smaller actor, well, just start as a searcher and specialize in that specific virtual machine.
06:17:47.180 - 06:18:31.950, Speaker B: The other thing I'll say is that centralization in some sense is inevitable. And the best you can do is segregate the validators from the sophistication. And we have proposed a builder separation, which is what it's all about. And we want to make sure that the builders, even if there's two or three builders that they can't censor, they can't cause liveness failures, they can't cause safety failures, they can't do front running, they can't do all of the bad things. And in some sense, once we've really cleanly separated things out, then the last thing that remains is mimetic downsides. It's just a bad look. If Citadel were to build 90% of the blocks, but fundamentally isn't maybe not so bad.
06:18:31.950 - 06:19:05.652, Speaker B: And so what I would push for is a few things. One is really making PBS more robust. So that means making the decoupling between the validators and the sequencing even stronger. And the best design that we have here is called execution tickets, where as a validator, I'm no longer given sequencing rights. I only have two rights, attesta rights and inclusion rights. In the context of inclusion lists. So I still contribute to consensus and finality.
06:19:05.652 - 06:19:43.220, Speaker B: I still contribute to censorship resistance with inclusion lists, but I don't have these sequencing rights and transaction ordering rights. Instead, that's given to a sophisticated entity. And then the other thing we should do is take the L one sequencer or infrastructure and make it as robust as we can. So we have inclusion lists, we have encrypted mempools, and all of these things kind of make it so that even in the worst case, in some sense, we need to think adversarially. Like, let's just assume the worst case. Let's assume there's one mega builder. Even in that case, we want ethereum to be world War three resistant.
06:19:46.330 - 06:19:49.560, Speaker D: You have about two and a half minutes left. Do you have anything you want to add?
06:19:50.090 - 06:20:42.786, Speaker B: Okay, so one of the things that we were talking about with dankrad just a few minutes ago is, okay, it's less about a binary thing. Like is shared sequencing good or bad? It's more about the subtleties and nuances of what is the barrier to entry, to become a builder. And one of the ideas that was put forward is homogeneity of the virtual machine. And I think this is something that we're going to move to eventually. So there's this idea of a native roll up, which used to be called an enshrined roll up. What is a native roll up? It's one that natively reuses the EVM. So today, if you're an EVM roll up, you're not native in the sense that you're trying your best to be EVM equivalent.
06:20:42.786 - 06:21:24.514, Speaker B: But more likely than not, you're not. You're going to have bugs, you're going to make trade offs for performance, you're going to have all sorts of things. And so one of the things that we're working on is basically exposing this Zke EVM precompile. That means that rollaps can natively use the EVM. And if that precompile catches on, then we can expect homogeneity. And that's one way to think about it, is execution sharding? But execution sharding, as designed a few years ago, was very opinionated. We had 64 shards, or 1024 shards, and they were each exactly the same, and they were each like maximally enshrined and native.
06:21:24.514 - 06:22:28.890, Speaker B: But with this pre compile, we actually bring back the programmability of Ethereum in the sense that it's just a pre compile. And if you want to have a wrapper around it. If you want to have tokenomics, if you want to have public goods funding, if you want to have governance around it, you can do that. You can have your own brand, your own community, your own ecosystem, and still benefit from the perfect equivalent to the EVM. Because literally you're using the EVM natively, you don't have bugs, because bugs then are the responsibility of the L one and the L zero. And you also don't need governance for your virtual machine because you're in a position where every time the EVM changes rules, which is to say every time we have a hard fork at layer one, the opcode or the precompile kind of magically upgrades. And so the future, I think will be very clean and more homogeneous and we're in this temporary awkward phase, puberty phase, I guess, but it's something that we need to go through in order to reach adulthood.
06:22:28.890 - 06:22:29.978, Speaker B: Awesome.
06:22:30.064 - 06:22:33.530, Speaker D: Thank you. Thank Ron. What about your rebuttal?
06:22:34.290 - 06:23:57.974, Speaker B: Yes, so I think I'm going to reply to three different points that were made. So I think the first one was the question of is it okay, are these builders actually that complex because they can outsource some of the process to searches? So I think to me, the core argument that I feel Justin makes about shared sequencing is that it provides this global composability. I think if you want that, if that is what you believe it will provide, it effectively means there has to be at least someone out there who simulates this all together. So even if you have individual searches somewhere, their software effectively will need to run in that big piece that simulates everything and figures out how it all fits together. Otherwise you're not going to get composability. So I don't think that this effectively totally, it might happen if there's some very specialized thing, but as soon as that becomes interesting enough for the global composability, it will have to be run in the global sequencer with a global prover and so on. So I think this concentration risk is not remedied by that in terms of how okay the central party is.
06:23:57.974 - 06:24:35.490, Speaker B: I think the argument that Justin is making here is that we can contain everything. Like all the downside that this brings by adding more consensus gadgets like say for example, inclusion lists and so on. And in the end it will be perfectly acceptable to have this. I do not believe that this is true. And the reason is this, like, okay, let's even assume that, let's say there's only one party, there's only a citadel doing it. They will still as a minimum have the power to stop their sequencer. This is one thing that you cannot stop.
06:24:35.490 - 06:25:25.218, Speaker B: They can always turn it off. So this will give them some power. In the case where say, they don't do this too often, they only do this like once every few months or so, but exactly when it gives them a super beneficial trade. So I think there's still a big problem here that creates more centralization and that basically allows someone to exploit these advantages. But I think they get even more power than that. They get the power to use this power that they have to switch it off to hold the rest of Ethereum hostage. They can now start making deals with validators about, well, I mean, if you do X, say if you put this transaction in the inclusion list, then it's going to be switched off.
06:25:25.218 - 06:26:20.942, Speaker B: So they're going to be like secondary effects. If you have too much concentration, like at some point you're going to get these effects where someone can hold the whole of Ethereum hostage for their benefits. And the only way to not have this, in my opinion, is to have, yes, I'm relatively okay with having some relatively large parties doing things, but there needs to be easy replaceability. It needs to be possible on a relatively short time scale with a limited investment to replace all the centralized party that the protocol depends on. Because yes, the natural equilibrium for many of these is going to be to have something like three to five or something. I think that's an acceptable thing to have like three to five parties who are most of the time running sequencing, as long as once they start colluding, we can have someone else who's stepping in and replaces them. And there's not going to be like a huge downside.
06:26:20.942 - 06:27:05.230, Speaker B: And the damage they can do is very limited. When you lose that ability, and that is my concern. With shared sequencing, then you're running into big problems where people have too much power. And I had a third point, but right now I can't unlock my phone. Okay, last point is about native roll ups. So I can see that you can potentially get to a sort of execution like sharding roadmap by adding construction. That basically adds an easy way to prove the EVM.
06:27:05.230 - 06:28:07.626, Speaker B: I want to propose an alternative, which is this. I think at the point where we are able to build sequences and provers large enough to run say 100 or 1000 times the current EVM scale, I think what we should actually naturally do is instead put these resources to scale our current L one. I think this is something we should definitely eventually do. I think we will within the next five years arrive there. And I think it makes sense to start scaling the gas limit. Put an ZkevM prover on L one, put it on the data availability layer and say like, yes, at some point we do need sequences, but if we do it this way, first we get totally like for free global composability because it is actually the Ethereum L one, we get a lot of control. We will know exactly how much it costs to sequence this, to run these transactions, and we get to build the infrastructure.
06:28:07.626 - 06:28:40.694, Speaker B: We get to know that everyone can run a basic set of open source infrastructure. So I think this is a much better world. And I think we will run into a world where within five years doing, say 100 XeVM will be able at a sequencer cost of say less than $100,000. I'm pretty convinced of that. And I think if that's what we want, this is what we should be aiming for. And I totally think that should be Ethereum's long term strategy. And I also think this is not actually a threat to roll ups because ultimately roll ups will build on their own ecosystems.
06:28:40.694 - 06:28:54.300, Speaker B: They will have their own value provision and they will be their own brand that provides users with a different experience that is good for certain applications that will continue to run on them.
06:28:54.830 - 06:29:08.290, Speaker D: Okay, so we were going to go into questions now, but I'm wondering if maybe we can go a little bit off book because I feel like Justin has a lot of responses. He would like to say to that. So maybe I can give Justin like a minute or two to respond to that. I could see him squirming in his chair.
06:29:08.710 - 06:29:43.194, Speaker B: Okay, I guess one cool thing is that once we have this ZkvM pre compile, in some sense we can remove the gas limit. Because what's the reason why we have a gas limit is because we want to prevent denial of service. But if verifying an EVM block is constant time, it takes one millisecond. You just have to verify the Snoc. Then you no longer have this analog service. So you can stuff a trillion additions and multiplications and bit operations and it will all be kind of compressed into a single SNOC. And so in some sense I agree with Dankrad that we can basically just remove the gas limit.
06:29:43.194 - 06:30:52.450, Speaker B: Maybe we do need a gas limit specifically for state bloat, right? We don't want to write to state too much, but there is actually maybe an argument to be said that the limits on data availability also limit the state growth. Another thing that I agree with Dankrat is around the need for a fallback. Just because we have sophisticated infrastructure doesn't mean that we shouldn't have unsophisticated infrastructure and that act as a fallback. And so my mental model for many things, actually, is that we have this optimistic default path, which is hyper optimized and provide best outcome for users. But then in World War III situations where all citadel has been blown up, then we do need to have these unsophisticated builders and searchers kick in. And one of the things that makes me very, very optimistic about this future being relatively straightforward is actually encryption. And the reason is that once all your mempools are just like encrypted white noise, then really the main thing that you can do is just concatenate encrypted transactions.
06:30:52.450 - 06:31:22.800, Speaker B: Once you can't see, inspect inside, then for the vast majority of transactions, you can just almost first come, first serve, concatenate them. And even. There's no sophistication there, right? You don't need capital, you don't need an army of PhDs, you don't need fancy algorithms and all of that stuff. So I am optimistic that, yes, we can have more homogeneity on Ethereum, we can have no more gas limits, and we can also have these fairly simple fallbacks that start kicking in.
06:31:23.330 - 06:31:26.142, Speaker D: All right, Donkra, do you have any quick response before we go?
06:31:26.196 - 06:32:31.762, Speaker B: I mean, I feel like everything he said was agreeing with my points. I don't know if there's any response to. I guess my point is that all of these things of fallback increasing the number, these are not mutually exclusive. With shared sequencing, we can have both. Why not both? I mean, it, I think will be interesting for you to sketch out design where you can actually have both in terms of the actual shared sequencing future, where you have things like synchronous composability. Right. I think that is not convincing to me that there is a default fallback path on all of these, right? I mean, one very graceful way to fall back is just to disable the synchronous composability.
06:32:31.762 - 06:33:39.310, Speaker B: It just fall back to the pure parallel, just concatenate encrypted transactions and don't worry about anything else. I mean, there's also this intermediate kind of level of sophistication, which is maybe Suav. Right, which is simultaneously, you can think of it as an encrypted mempool, and it has the ability to kind of have complex interactions within them. As a minimum, the shared sequencer would still get the ability to arbitrarily switch off the synchronous composability, which means they can use this at a time when it's convenient for them. For example, if they can exploit something on a roll up that someone else cannot because they don't have enough capital and would require exploiting the synchronous composability. Right. One thing that can be leveraged is actually, you know, providing cross roll up account abstraction and cross roll up bundles.
06:33:39.310 - 06:34:48.174, Speaker B: And there's like, for example, polygons aggregation layer ideas in that direction. And here, once you have that infrastructure, it's actually safe to use the inclusion list. So your slow path, your maximally simple and robust path, is actually still to an extent compatible with the synchronous composability. I think the main thing that you potentially lose is the pre confirmations. So you lose this real time ux, you fall back to 12 seconds. But one of the things that Vitalik noticed recently is that the Internet always, very often has these glitches, right? Like how many times a day do you have to wait a few seconds for things to reboot for your wifi to get back to normal? And I think it's fine if sometimes we gracefully degrade and lose this instance ux. So this seems wrong to me, that you're claiming that you can get instantaneous composability just from inclusion lists.
06:34:48.174 - 06:35:35.970, Speaker B: It wouldn't be instantaneous. You lose the pre confirmation, but you keep the composability. That seems untrue to me. Okay, so the way it would work is that you express your super transaction, your cross roller bundle, as something that you can include in the inclusion list, and then whatever builder ends up building a block, they have to respect this intent, this design. But how can they respect it without being able to sequence on all the things that interact with each other? Like someone has to have the shared state of all the roll ups that interact with each other. Yeah. So that would be the minimal fallback in case.
06:35:35.970 - 06:36:22.686, Speaker B: Okay, but that is a global shared sequencer. Yes, but there's a different notion of global shared. There's one that's maximally simple where if you zoom in, you could have these searchers that just do one thing and just concatenate transactions. Yeah, technically as an entity, logically it's one global sequencer. But that doesn't mean that there's one entity in the world that has a farm of servers that's doing all the sequencing. I mean, at least you need all the state to know what exactly the transaction does, right? Otherwise you cannot have composability. Well, let's say you have 1000 roll ups and you want to compose between two roll ups.
06:36:22.686 - 06:36:45.834, Speaker B: Well, you only need to have run a full node for these two roll ups. And so for 99.9 of the full nodes, you don't have to run them. And so you can have these pairwise searches, I guess, that are more robust and much more simple than having a global view over everything. I think a searcher is not enough. You need an actual builder who does both at the same time.
06:36:45.872 - 06:36:46.074, Speaker D: Yeah.
06:36:46.112 - 06:36:52.270, Speaker B: So the layer one searcher is an L two builder. These are like different layers of abstraction.
06:36:53.650 - 06:37:09.490, Speaker D: All right, should we, maybe we go to questions for a couple of minutes. Okay, so I'll first give it to Donkrod. What are your opinions on things like hyperchains and superchains, where there are communities that share a sequencer, but they don't share a global shared.
06:37:11.930 - 06:37:44.190, Speaker B: I mean, like, I think to me a super chain is in the end just one roll up. Like I would say, like, it's just like a different way of creating what I would call logically one roll up with like just different partitions that are individually administered. But I think that doesn't make them separate roll ups. Basically, you could just have these different partitions just be their own smart contracts. So I think the global thing is logically one roll up.
06:37:44.260 - 06:37:57.010, Speaker D: Okay, yeah, makes sense. And then to Justin. So in your previous talk, you talked about execution tickets probably being several years away. What do you see as like, while we don't have execution tickets, how we should enable this shared sequencing?
06:37:57.670 - 06:38:41.774, Speaker B: Right. So the basic idea is delegation. So the sequencing rights would be given to an L one proposer. And that l one proposer, if they choose to not be a pre confirmer, they can delegate their sequencing and pre confirmation rights to some other entity. And now you can ask yourself, okay, so I call it the gateway, so logically call it the gate. Now we can ask ourselves, what makes sense to who should be running the gateway? Well, the gateway needs to be mutually trusted by the proposer, of course, and they also need to be trusted by the users. And one of the reasons is actually around the pricing of pre confirmation tips.
06:38:41.774 - 06:39:28.554, Speaker B: And this is a really delicate balance. So every time a user makes a request for pre confirmation, they have to pay a corresponding tip for this service. And there's basically two outcomes that are bad. Outcome number one is that the tip that the users have to pay is actually too high, in which case the users won't be happy and they'll get ripped off and outcome number two is the tip is too low, and the proposer kind of is not happy and feels that they've been ripped off. And so really, you want this neutral party, which is trusted to provide the pricing on both sides. That is fair on both sides. And if I were to look at the existing ecosystem and try and map this gateway role to one of them, I think it would be the relay.
06:39:28.554 - 06:40:08.890, Speaker B: And the reason is that the relay is already this kind of trusted brand, which is meant to be credibly neutral, that is already trusted by the builders and the proposers, and now they need to be trusted by a third type of entity, which is the user. And actually being trusted by the builders and having all the builder connections is useful. And the reason is that once we have pre confirmations, we modify PBs in such a way that the builders still have to build these MEV maximizing blocks, but with the constraint of having to satisfy the pre confirmation. So really, it's like this four way game where we have users, proposers, builders, and then relays in the middle that are just neutral coordinators.
06:40:09.390 - 06:40:18.980, Speaker D: Okay, as a follow up question to that, how would you respond to people who say that we should not be adding more responsibilities to the relays, we should be removing relays from the system.
06:40:19.590 - 06:40:55.690, Speaker B: Right. So I've always been of the opinion that relays should be this temporary thing, and then they should just die. And the previous kind of roadmap was enshrined PBS, and the promise there was, okay, we're going to remove the relays now. There's kind of good news and bad news. The bad news is that EPBs looks way less attractive today than it looked a year or two ago. And so that as a roadmap, looks less feasible, but also it might take more time. But the good news is that we actually have an even better design idea, which is these execution tickets.
06:40:55.690 - 06:41:35.030, Speaker B: And so, sure, we might have to wait a little bit longer, and relays might have to live longer than expected, but ultimately, we'll be in a better place. And I think this speaks to ethereum's, I guess, philosophy, which is do things slow, but do things properly. We don't want to take too many shortcuts, and we want to incorporate a lot of the research. And it just so happens that, in some sense, with execution tickets, we made this quantum step in terms of our understanding of MeV. And I think that is a much better design than enshrine PBs in the short term.
06:41:35.850 - 06:41:55.040, Speaker D: Yeah. Thank so. And then a question to Donkrad. What do you feel about the argument that these builders, for particular roll ups, will end up centralizing anyway, because there's profit to be made by building for multiple roll ups, and that they'll just centralize themselves outside of the protocol and we'll just end up with a global shared sequencer anyway.
06:41:55.890 - 06:43:07.314, Speaker B: I mean, I think that is a good question for me, that's different from the normative question of, like, do I think it's a desirable future? Well, right now I see it as a danger. I am currently not convinced that it is actually the natural future, because I do believe that roll up ecosystems are developing independently and it does seem like they're doing their own thing. And I actually don't know if there is such massive value that the market concentration will be worth it for someone. It may be, but I'm not completely convinced that that is true right now. If it is true, then I would actually stand by my argument that if this is where we're heading, we're still better off saying, hey, let us put everything on scaling our base layer so that that is going to be the big shared sequencing layer that we can control and where we can control centralization. Yes. Roll ups either become their own independent ecosystems or maybe if in the worst case, we were actually to see this is inevitable, this is guaranteed to happen, maybe we would have to abandon the roll up centric roadmap.
06:43:07.314 - 06:43:08.920, Speaker B: I don't believe this is the case.
06:43:09.370 - 06:43:16.834, Speaker D: All right, sounds good. Okay, so now we're going to go into the closing statements. I'll give Justin two minutes for his closing statements.
06:43:16.882 - 06:44:00.310, Speaker B: First, so, I've known Dankrad for twelve years, and actually he's the one who got me into bitcoin in 2013. And we were working on this FPGA project, and even back then we were arguing all the time. And he leans on the pessimist side of things, I lean on the optimist side of things. And usually truth is somewhere in between. And I think we actually have so much more common ground than this debate would suggest. And so, yeah, I think I actually very much agree with Dankrat on many things. I agree that we need to have this really robust infrastructure, so even if there's one mega builder, it can't do evil.
06:44:00.310 - 06:44:52.790, Speaker B: I agree that we need to have these graceful fallbacks. But at the same time, I still think that. By the way, I also agree that it would be nice to have these Zke EVM enshrine pre compiled that don't have any gas limits. So we can really do as much as we can at layer one. But at the same time, I think that the short and medium term market forces, the network effects of synchronous composability, will lead to some amount of centralization. And that's something that we shouldn't necessarily be fearful of. It's something that we should be mindful of and try and design a future which is long term satisfactory.
06:44:52.790 - 06:45:15.680, Speaker B: As a researcher, we have this bias over the long term. And I also think ideologically, what are we trying to build here? We're trying to build infrastructure that will last decades and centuries. And sure, if the next five years are a little awkward, then so be it. I personally don't care. And so I think, yes, we need to be mindful of the next five years, but I remain optimistic about the long term future.
06:45:16.290 - 06:45:18.590, Speaker D: All right, wonderful. Don grab.
06:45:20.050 - 06:46:39.622, Speaker B: Yes, I mean, I think that's a great sort of closing statement in terms of generally, I think, as before, my point on this is not about what is going to naturally happen. I think that's the second point. Like what a good question to ask, where do the forces lead us? But I think my primary point is I want to know what is a good future? I think there are two different things here. There's the advantage from the UX point of view, there are the disadvantages from the centralization point of view. And I think the natural thing is to ask the question, where do we want to be in five to ten years? And I think it's very good to, even for roll ups to ask themselves the question, should I, for example, share the sequencer? Is that the right thing to do right now? And I think there are good arguments to think about. Well, maybe this is not what you should aim for right now. It's a lot better for the roll up ecosystem to have a lot of diversity, and I think it's also ultimately better for roll ups in terms of their economics, because roll ups are building their own ecosystems.
06:46:39.622 - 06:47:01.140, Speaker B: They do want to sort of build out like something that is independent and will generate them some amount of revenue, at least. And so I think it's a good question to ask. And I think the normative answer for me is still like, no, I'm not willing to say, I want to build a global shared sequencing layer right now.
06:47:01.830 - 06:47:46.960, Speaker D: All right, thank you both. This was a really great discussion. So I would like to ask everyone to go on MEB market again and take 1 minute while we're waiting for Josh to grab a sip and to think whether you want to revote or edit your vote. So this was the split before the debate, we'll give you 1 minute to think about. Some choices in life are important.
06:47:48.770 - 06:47:51.090, Speaker B: Justin is a carefully.
06:47:53.510 - 06:47:55.060, Speaker D: I'm so glad I'm not.
06:47:56.230 - 06:47:57.650, Speaker B: Where are the other quotes?
06:47:59.510 - 06:48:57.400, Speaker D: There's a set of nuances you get to see later. And so yeah, if you would like to revote or edit your vote, just click on the poll under round one. And so in about two minutes after this, after the next round two starts, we'll take another snapshot and we'll reveal it at the end of round two debate together with all right, great job. Yeah, I have another intro for that going to go into the next one. I'm just going to give a brief intro like I did for the previous one about what the question is. So this debate is going to be based versus non based sequencing with Josh and Justin really quickly. We'll just go over based sequencing.
06:48:57.400 - 06:49:49.112, Speaker D: So based sequencing you have l one nodes or the L one. Proposers get to sequence l two transactions, and then based sequencing also inherits the liveness and the credible neutrality of the L one. There are some other properties that some definitions of based sequencing include, but for this discussion we're going to call that our definition of based sequencing. So there's advantage of this. You get good decentralization, liveness, censorship, resistance, you get improved interoperability with the L one and also between multiple roll ups. I already discussed this part, so I'll skip through here. In vanilla based sequencing, without pre confers, you would have to transactions would only be sequenced every 12 seconds on Ethereum, and you don't really get the UX that you get from centralized sequencers today.
06:49:49.112 - 06:50:17.104, Speaker D: But obviously with pre confirmations you can get like less than 100 milliseconds latency, as Justin described in his talk. So we have base sequencing 2.0, which is basically, we can use an external finality gadget. This is kind of that layer 1.5 I talked about previously, but I'll kind of skip over that because we already talked about that. So here is maybe a rundown of the differences. So base roll ups you get transactions are dependent on some unfinalized ethereum state.
06:50:17.104 - 06:50:45.528, Speaker D: So I want to explain this one a little bit further. In current roll ups today, a lot of roll ups will choose to base their roll up blocks on finalized Ethereum state, or at least Ethereum state. That's like sufficiently deep in the chain so it's not going to reorg in vanilla based sequencing. The transactions are reliant on the L one block that they're included in. And maybe this L one block has a lot higher chance of reorgang. Maybe that's not good. Base roll ups give you synchronous interactions with the L one.
06:50:45.528 - 06:51:32.152, Speaker D: Since it's the same party that's proposing for both the L One and the L two non base roll ups, you only get asynchronous interactions. And then finally with base roll ups you can have some additional fast finality layer. This layer 1.5. But if you don't have that, then you get the finality of Ethereum, which maybe is kind of slow. With non based roll ups, it's very easy to get this fast finality gadget and you kind of get it for free. I'm actually going to skip these and we'll just kind of go into the debate. So I don't actually have a question slide here, but basically our question is going to be, should roll ups transition to base sequencing or not? With the definition of base sequencing, that one, it's the L one proposer that gets to decide the transaction order.
06:51:32.152 - 06:52:04.170, Speaker D: And two, base sequencing inherits the liveness, incredible neutrality of the L one. So with that we can go ahead and jump in. Okay, let me just set up my timer here. I'm not sure where Tina went, but we also have a poll for this one as well that we wanted. Poll now and at the end. So go ahead and vote in your poll for poll number two. Okay, let's start with Justin, your opening arguments for three minutes.
06:52:05.820 - 06:53:01.556, Speaker B: Okay, so I think Josh and I both agree on the previous question, which is whether or not we want to have a shared sequencer. And now the question is which shared sequencer? And then basically we need to look at various properties of the shared sequencer. And in my opinion, there's like three that are very, very important. One is credible neutrality. And the reason why it's important is because we want to build this neutral playground for every roll up and their competitors to feel comfortable opting in to this shared sequencer. So the classic example that I give is that imagine that if arbitrum comes up with its own shared sequencer and optimism comes up with its own shared sequencer, are they going to mutually use each other's shared sequencer? And the answer is going to be probably no, because they have these competitive dynamics. And in some sense, if you look within the Ethereum ecosystem, the Ethereum shared sequencer is the most credibly neutral.
06:53:01.556 - 06:53:42.548, Speaker B: There's nothing more credible neutral. You've already opted in, you've already bought into Ethereum as a credible brand and credible system. So there's no loss of credible neutrality to opting into Ethereum layer one as a shared sequencer. The other aspect of shared sequencing is security, and this is a very similar argument. So when you build a roll up or layer two, there's like several things that you need to care about. You need to care about settlements, data availability, execution and sequencing. And in some sense, from a security standpoint, at first approximation is going to be whatever your weakest link is.
06:53:42.548 - 06:54:46.368, Speaker B: So if your weakest link is, let's say, some sort of off chain consensus that uses your token, and this token only has, let's say, a billion dollars of economic security, well, that's going to be your weakest link from a liveness standpoint. Like I can 51% attack your external sequencer and then do bad things like extract toxic MEV and maybe force a mass exit or disable pre confirmations or whatever it is. But if you use the Ethereum L one as a sequencer, then now there's no additional security assumption that you're introducing for all of these things, settlement data execution and sequencing is the same security assumption, and Ethereum is secure, and there's no weakest link because they're all the exact same strength. And so from a security standpoint, that is the optimal thing you can do. And then there's this final question, which is around network effects. Ethereum has a ton of network effects. It has half a trillion dollars of TVL.
06:54:46.368 - 06:56:05.540, Speaker B: It has applications that possibly will never migrate, like cryptopunks will never migrate to arbitrum. Like cryptopunks are native to the L one, ENS is native to the L one, and then various whales are native to the L one and might never want to move for various reasons. And I think by tapping into these network effects, that is the best thing you can do, instead of having a cold start problem where you start your shared sequencer from scratch, and now you need to ramp up your TVL, well, now you can just incrementally plug into what already exists, which is half a trillion soon several trillion dollars of TVL. And this kind of brings us to this notion of Meltcaff's law, right, where the value of the network effect grows quite drastically in the size of the network. And so if you have, for example, Ethereum layer one, which is, let's say, ten x larger than arbitrum, well, that's going to be 100 x delta potentially in network effects. And so really it's really difficult for an isolated sequencer to build the same kind of network effects. And so I think there's going to be these natural market forces for the L one to win out because of credible neutrality, security and network effects.
06:56:05.960 - 06:56:07.488, Speaker D: All right, Josh.
06:56:07.664 - 06:57:15.880, Speaker E: Okay, so my view is going to come from kind of a perspective of, I would say, like incumbent versus startup, and that is that Ethereum, as Justin Drake pointed out, is like the dominant kind of existing platform. But fundamentally, it was a chain that was not created for the purpose of doing sequencing or shared sequencing, if we want to specify to that it was a chain made to be an L one, and then throughout its lifecycle, right, it was adapted to support L two s and the roll up centric roadmap and various things on top of that. But fundamentally, it was never, from day one, built to support shared sequencing. And thus there's a lot of kind of like, pitfalls. I think if you look at the L one and a half designs or l two or whatever, I want to call this, where you have a separate consensus mechanism for a shared sequencer, you have a strong benefit of kind of finality from that. And you can use consensus mechanism that gives you this kind of fast finality. And you don't have this slower finality dependent on the base layer, which is fundamentally why people used sequencers in the first place, right? The original designs of roll ups were base roll ups.
06:57:15.880 - 06:58:12.648, Speaker E: They assume you just submit the transactions to the L one and then they are derived into the roll up state machine. But people moved to sequencers and centralized sequencers because those were the low hanging fruit, and they provided users with a better experience. And generally, my view is that the market cannot be ignored. And what I mean by that is there's a sense of like a domestic versus an international thing. And what I mean by that, within the sphere of Ethereum's domestic domain, it can say these are the changes we're going to do, but it cannot prevent an international or like an exogenous kind of competitor, something like a salana. Or in some case, you can view the L two s with centralized sequencer like this from creating a product that has a different trade off space and a different design consideration that may provide a more desirable user experience. And fundamentally, it's not within kind of like Ethereum's purview to prevent those external actors from creating a service that may be more desirable.
06:58:12.648 - 06:59:07.660, Speaker E: And so if we want to kind of confront, say, centralizing vectors from some of those things, we need to create a product from scratch that is kind of built to satisfy the kind of expectations of users that they're used to on kind of centralized sequencers today. So if we want a decentralized sequencer, if we want to have the shared sequencers, I think we need a native chain that is just built to support that specific use case that doesn't come with some of the baggage is not the ideal term. But this kind of legacy kind of sense of ethereum doing many things. Fundamentally, the modular ecosystem I view as being something about separation of concerns and building more kind of limited pieces, somewhat of like a Unix philosophy where it's a small tool that does one thing. You can build a chain that is existing purely for the sense of doing shared sequencing. It is not also doing settlement, it's not also doing execution of its own, purely exists to do shared sequencing.
06:59:08.000 - 06:59:08.844, Speaker B: Is that.
06:59:08.962 - 06:59:14.556, Speaker D: Yeah, that was perfect. Three minutes. All right, Justin, you have five minutes for a rebuttal.
06:59:14.748 - 06:59:52.300, Speaker B: Okay, so I guess there were two points that stood out to me. One is that the market kind of initially started with the most naive solution, which is base sequencing, and then for some reason moved away from base sequencing. And this move from the market might suggest that base sequencing is inadequate in some way. Now, the way that I see it is that the move from base to centralized sequences is for three key reasons. Key reason number one is that base sequencing, a few years ago we didn't realize that it could provide pre confirmation. And today we do know how to provide pre confirmations with base sequencing. And so I think that is one of the major unlocks.
06:59:52.300 - 07:00:21.060, Speaker B: The second thing is around MeV protection. Like right now on Ethereum layer one, you don't have MEV protection. You broadcast your uniswap transaction on the public mem pool. You're going to get sandwiched. And roll ups were like, oh, well, we can provide MEV protection for free, just trust us. And they are trustworthy because they're entities with lots of reputation and they provide this great MEV protection. You won't get sandwiched on arbitrum or uniswap.
07:00:21.060 - 07:01:09.812, Speaker B: And again here, it's almost a technological problem where the L one needs to catch up in terms of services that are being provided. And the way that you catch up is by building encrypted mempools using something other than trust. So you can use a threshold encryption, you can use delay encryption, you can use SGX, which is enclave encryption. But there needs to be some sort of a solution for users. And then the third reason why roll ups preferred centralized sequencing over base sequencing is like this subtle thing around security training wheels. My guess is that every single roll up has critical vulnerabilities every one of them. And so this is really scary because now an attacker who's a black hat can go and exploit all these roll ups one by one.
07:01:09.812 - 07:01:52.684, Speaker B: But the roll ups, they're very clever. They're like, we're going to accept the fact that we have vulnerabilities, but we're going to have layers of defense, we're going to have security in depth. And one of the layers of security in depth is the centralized sequencer, which will only include on chain transactions that have been vetted. So if an attacker tries to exploit some sort of weird edge case and craft some sort of fancy transaction, the shared sequencer is going to say, hey, that doesn't pass the smell test. I'm not even going to include it on chain. And so you can't even exploit the bug even though it's there on chain. And so what I think we need to do is basically have defense in depth, which isn't just based on centralized sequencing.
07:01:52.684 - 07:02:48.900, Speaker B: So we need to have multiproving, we need to have formal verification. We might need to have this native EVM opcode idea. But all this to say that the reason why the roll ups are not using base sequencing is not for fundamental reasons that can be fixed. It's just because there was like this quick hack to get MEP protection to get the training wheels and to get the pre confirmations, all of which we can get with base sequencing. Now the other point that was brought up by Josh is that in order to do pre confirmations really well, best in class pre confirmations, we need to build something from scratch because otherwise you're going to be like missing features and whatnot. And actually, I totally disagree here in the sense that we just got so, so lucky. We got lucky that from Genesis there was this amazing pre confirmation layer that was hiding in plain sight.
07:02:48.900 - 07:04:04.332, Speaker B: And in some sense, it's a little bit similar to call data, right? So call data was just there from Genesis at day one. And roll ups have been able to consume call data. And sure, we have been able to improve it with the blobs, we have more data. But putting aside scalability, which is not an issue with pre confirmations, in some sense, call data is good enough. Now, okay, I said scalability is not an issue for pre confirmations. Just to make sure this is all very clear in people's mind, basically we can have proposer builder separation, right? So you can have the proposer run on a raspberry PI and you can have the builder just run 1000 full nodes and have a super cluster so scalability is not an issue there. Now, the one small detail that does come into base sequencing, but it's really a minor detail, is around the size of the look back, look ahead.
07:04:04.332 - 07:04:40.840, Speaker B: Thank you. So basically on the beacon chain today, you can know who the next 32 proposers will be, actually between 32 and 64. And it would be actually slightly nicer if this was 128 or always 64. But this is like such a minor detail, and it's a very easy, hard fork to just increase the look ahead that, sure, maybe there's a few things that can be polished, but fundamentally, we just got extremely lucky. And without having to do anything, we can have a best in class shared sequencer.
07:04:41.500 - 07:04:43.828, Speaker D: All right, Josh, your rebuttal?
07:04:43.924 - 07:04:51.340, Speaker E: So I guess the assumption we're working on is that base sequencing is going to provide the fast pre confirmations via.
07:04:51.760 - 07:04:53.916, Speaker A: The kind of, I don't know what.
07:04:53.938 - 07:05:19.430, Speaker E: We want to call the design, but the request and the response to the user. And then potentially with the gateway. Right, wherein we have a user sending a transaction potentially to a gateway, because the gateway may be, call it like a reverse proxy that knows where all the other pre confirmation preconfers have registered themselves. And the capacity to find these preconfers is dependent on the ability to see them in the look ahead, right?
07:05:19.880 - 07:05:20.708, Speaker B: Yes, exactly.
07:05:20.794 - 07:05:21.430, Speaker C: Okay.
07:05:21.960 - 07:05:53.100, Speaker E: So I guess my view on that is that fundamentally there's a question of kind of how you're going to get the kind of in these pre confirmations. What we've done with this gateway is we've introduced yet another, I'm going to call it like a proxy, but we've introduced one more trusted third party layer in the mix of this entity that a user is interacting with that is responsible for kind of like redirecting user transactions. Redirecting is not the right word, but forwarding user transactions to another set of entities.
07:05:53.180 - 07:05:57.052, Speaker B: I mean, that's pre execution tickets. With execution tickets, we don't have this extra party.
07:05:57.116 - 07:06:32.764, Speaker E: Right. But I suppose that leads back to my original point on some of the difficulties of using kind of this. Again, I don't want to call legacy or whatever, but it is the idea that you said fundamentally, ethereum is a conservative blockchain. Very intentionally so. Right. It has half a trillion dollars on it is not something that you want to kind of mess with in a very liberal manner. And so the development of some of these kind of pre confirmations in what I would call a best in class kind of capability, relies on features being added to the L one.
07:06:32.764 - 07:07:00.784, Speaker E: And I think one of the big kind of points to generalize this, right. We have the specificity of execution tickets, and to some degree I think various inclusionless designs would also provide benefits to this pre confirmation design. Right. And know EPbs, though, I guess we're more transitioning like execution tickets. That's like a similar solution here. Right. For what was Connor terminating your ahead of time versus your just in time kind of like proposer auction.
07:07:00.784 - 07:07:14.644, Speaker E: Right. But fundamentally, these are things that have kind of conflicting priorities, from an improvement to like the Ethereum L one. Right. Kind of harkening back to the debate with Dankred.
07:07:14.692 - 07:07:15.012, Speaker B: Right.
07:07:15.086 - 07:08:14.936, Speaker E: Fundamentally, there are split opinions on what is the optimal thing to spend effort on in the development of core development time of research time for improving like Ethereum, the L one. And there are questions of whether that effort should be focused on providing execution tickets, which may be positioned as something that dramatically improves the ability for ethereum l one to be used as a shared based sequencer. But maybe that conflicts with development resources that would be spent on whether it's vertical trees or some other kind of. There's a long list of improvements being researched and worked on. So there is this kind of divide. And I think this is one of the things we see in l two s more generally, wherein again to one of dancard's points, they've developed their own kind of ecosystem in their kind of community. And one of the things that gives them is this capacity to have their own priorities and not necessarily be kind of beholden to the l one's fundamental ecosystem.
07:08:14.936 - 07:09:36.004, Speaker E: So when we think about kind of credible neutrality as a benefit here to some degree, it is also a, it's probably too harsh to say like it reduces to the lowest common denominator, but directionally in that way it is saying if you want to have an improved functionality that requires the base layer to improve, you will be dependent on the velocity of that base layer. And again, that's why I as an individual, have not kind of historically spent my time in the Ethereum research community. I went to more of the celestia and that modular ecosystem, primarily not because I dislike Ethereum or the kind of Ethereum research ecosystem, but because I thought this is a place that is doing very similar kind of directional kind of solutions of it's still a modular design, but is doing it from a new starting point and is able to have higher velocity because again, we pay the trade off of not having half a trillion dollars of security. You have to bootstrap your security. It's very difficult to bootstrap these networks, but it gives you the advantages of being able to move dramatically faster. And my view is that that lets us get a product to market sooner. And a lot of this might be biased by the current market timing and the kind of mental kind of space you're in where the moment you start seeing the coins go up, there is a sense of kind of mania, kind of falling upon the broader industry.
07:09:36.004 - 07:10:12.050, Speaker E: And so there is this desire to say, we need to get something out now. And when you rely on an intentionally kind of conservative base layer, you're not necessarily able to kind of keep up. And so I think you'll see kind of divergences from, like l two saying, well, yes, sure, maybe I want a shared sequencer, and maybe it would be nice to use a base sequencer, but I'm not comfortable waiting five years for that, or three years for that, or however long it is they're going to work with what in some ways I think could be argued to be like a less optimal solution, but something that is available now rather than something that's kind of theoretically in the future.
07:10:13.060 - 07:10:38.700, Speaker D: Yeah. So I have a couple of questions I want to ask, but we can also continue this debate. So I have a question that I'm going to ask both of you. We can start with Justin. So this was something that Josh brought up before this panel, was that in classic base sequencing, all of the l one validators are participating and they can all be proposers. But in the design for base sequencing with pre confirmations, it's only a small subset. And the question is, is that really based sequencing?
07:10:40.240 - 07:11:25.044, Speaker B: Right. I mean, I have relaxed my definition to base sequencing, where some subset of the L one proposers are positive facing as opposed to all of them. Now, one of the interesting things is that I actually expect if base sequencing to kick up to be successful, that 100% minus epsilon, meaning almost all layer one proposers will start playing that game. And the reason is that it's similar to mefboost, like on Mefboost today. I think it's something like 95% of validators use meth boost, which is the vast majority. And the reason is that, show me the incentives, I'll show you the outcomes. And the incentives are actually pretty stark.
07:11:25.044 - 07:12:05.370, Speaker B: And the reason is that let's assume that only 50% of the L one proposers have opted in to being sequencers and preconfirmers. What that means is that your average block time as a base sequencer is actually two slots. So it's 24 seconds. And so you're going to be able to get 24 seconds worth of flow and Mev and whatnot. And this is even more stock at the very beginning, let's say there's only 1%. Well, now you have like 100 times more mev because your slot times are 100 times longer. So I expect if we can get through this cold star problem of like 1015, 20%, we'll quickly get to 95%.
07:12:05.370 - 07:12:26.910, Speaker B: And part of the good news here is that there are large entities and infrastructure builders that are looking to do just that, solve the cold star problem. So I don't want to leak too much alpha, but there's some big operators out there that want to play this pre confirmation game, and they want to solve the cold star problem.
07:12:27.840 - 07:12:30.030, Speaker D: All right, Josh, I'll ask you the same question.
07:12:30.560 - 07:12:56.104, Speaker E: Can you repeat maybe the specific question? Because I can talk about the general kind of principle of my view on is this based sequencing? And I think my view, or the kind of question I kind of have about that, is if you are requiring a subset, and I agree. Right. It's like, what, 20%, I think with like a 32 look ahead, gives you like a 99.5 or 99.95 or something, and 30% gets you like four nines. And that is sufficient. Right.
07:12:56.104 - 07:13:48.612, Speaker E: And that is what you get Lido and Coinbase and Corus one and P to p or whatever, and you're good. Right. And so I don't think that's difficult. I think the question is, what is the kind of trust assumption, may I be right word, but how are we defining what is something that is based, when we say a subset of the entities, which may be a superset of the entities, are now required to run an additional piece of software that is like a sidecar that presumably has higher resource requirements? I think in Mevboost, right on the margin, right. There are like higher resource requirements, but functionally that is outsourced to the relays and the builders and the searchers. But in this case, for the preconfers, right. We are going to run into a similar situation where again, we're going to say, okay, the preconfers, if they want to do it themselves, but as evidenced, validators seem to prefer being lazy entities.
07:13:48.612 - 07:14:25.364, Speaker E: And now we have, once again, kind of, and I don't think. I think proposal is entered good. But there is this kind of sense in which, and maybe this is like a broader kind of theoretical question of what is in protocol. What is out of protocol, right. Is the MeV supply chain like part of the Ethereum protocol? If like 93% of the blocks for what, a year and a half now have run through that? That's kind of part of the protocol. If it went away, things would be problematic. But there is this question of is it still based if you're adding significant software, that we assume that a large majority, like a supermajority of the nodes in the network are going to run, and then we're going to lead to kind of inevitably them outsourcing that once again.
07:14:25.364 - 07:14:57.836, Speaker E: So I don't think it's necessarily, I'm arguing against that. I just have a question of when we say something is based and thus it is kind of inheriting just the l one, we are actually assuming we are adding a new software mechanism on top of that, and there's just some consideration that should be given to the fact that we are adding software and we are adding trust assumptions. And presumably there are a relatively small n number of actual preconfers that are going to be like pre conference and whatever the entity we call that the preconfers would outsource to, to do whatever. The builders, the L two builders, searchers, whatever.
07:14:57.938 - 07:15:19.792, Speaker B: Yeah. No, yeah. We want the validators to not be sophisticated. And if they do run sidecar, it should be like minimal on the same order of magnitude as mefboost. And the gateway idea is basically all about that, right? You just delegate to a gateway. If you're not happy with the gateway, just choose another gateway. But all the heavy lifting is done by the gateway.
07:15:19.792 - 07:15:28.360, Speaker B: And then even cleaner than that, where you don't run any sidecar software whatsoever, you don't even run webboost. Is execution tickets.
07:15:30.780 - 07:15:32.212, Speaker D: Anything to add to that, Josh?
07:15:32.276 - 07:15:41.980, Speaker E: No, I guess I disagree lately, but it's one of these things like, in a year, is Yuki going to write a blog post that's going to be like, no one cares about the gateways. Who is thinking about incentivizing the gateways?
07:15:43.600 - 07:15:44.824, Speaker B: The ultrasound gateway.
07:15:44.872 - 07:15:45.470, Speaker E: Yes.
07:15:47.680 - 07:16:13.008, Speaker D: All right, so this question will be for Josh. So you talked a lot about how Ethereum has a lot of legacy issues, and maybe another project that can really make a specialized piece of software can come in now and offer this solution. So what is your view in like five years when maybe Ethereum actually can, they can implement execution tickets? Maybe we have zero or real time proving. Do you still see a space for a separate sequencer entity?
07:16:13.104 - 07:16:13.316, Speaker B: Yeah.
07:16:13.338 - 07:17:15.384, Speaker E: So I guess I don't think it's necessarily the, like, there's like two sides. There is like, Ethereum comes with extra things you don't necessarily need or want for the specific subset of tasks you're trying to do, right? It is like a batteries included thing, right? It has an execution client. You're like, do I actually want a full EVM to do my sequencing or whatever? Right? And then there is a question of to the five year timeline of does it take five years to add in a core functionality that provides an enshrined or like an in protocol kind of solution to that? So I think trying to predict five years, like, props to you, Justin, for working on those timelines, but I have been in crypto full time for a little over three years now. Trying to predict five years out sounds like a completely absurd kind of like guessing game to try to figure out where the state of the industry will be, because it's been very different in the three years that I have been here. But I do think, if anything, we will probably see more things because again, execution tickets were discussed what, eight months ago is or so when they came out.
07:17:15.422 - 07:17:15.576, Speaker F: Right.
07:17:15.598 - 07:17:23.870, Speaker E: As like an idea. EPBs was 24 months ago or something like that. Was it 2021?
07:17:24.320 - 07:17:27.848, Speaker B: Epbs is quite old. But yeah, execution tickets is from October, December.
07:17:27.944 - 07:17:48.976, Speaker E: Yeah. As an idea, it is like a relatively new thing. And so to think, well, in five years we will implement the idea that we have thought about for eight months. And you're like, well, presumably in the next five years we will think of some new things. And I think there's an inevitability of. I think this applies to software generally. I don't think there's a fault of Ethereum.
07:17:48.976 - 07:18:26.448, Speaker E: I think this is like how software works. I don't see the velocity of Ethereum kind of implementation, kind of accelerating. You hear people talking about the ossification. So whether it is like execution tickets are resolved and you say, okay, great, we can get rid of the gateways and we can get rid of the relays and whatever, and we've gotten rid of a bunch of sidecar software. But I presume in five years, the blockchain space, if you go like Tamacha's talk, right? Where are we going to be with the AI chains, the AI agents trying to trade on chain? And maybe there is some thing where we say, well, okay, now we have some sidecar that's trying to wrangle the AI agents, and maybe we'd like to enshrine whatever AI agent like Wrangler thing. We're just wildly speculating. Five years, a very long time.
07:18:26.448 - 07:19:02.892, Speaker E: And I think there's inevitability of, like, there is value to creating new things that satisfy a problem. I think one of the questions is how you avoid constant bootstrapping issues for this, to some degree, it's just like inevitable, but you can kind of find a balance. And it's like why we don't want only large corporations. It's beneficial that larger entities, corporations, chains, foundations, whatever, slow down over time. It leaves room for startups to kind of come in and create something newer that is with just less. Right. It's like a more minimal thing, but it solves the task kind of more neatly because it's not trying to do many things.
07:19:02.892 - 07:19:07.470, Speaker E: So I think that's just like an inevitable, it's not a crypto specific thing. It's like a market structure thing.
07:19:08.000 - 07:19:10.640, Speaker D: Makes sense. Do you have any response to that, Justin?
07:19:11.380 - 07:19:34.870, Speaker B: Yeah, I mean, I do agree that you want to move fast and maybe we'll reach a singularity within five years. And so Ethereum's roadmap is kind of useless. I mean, I guess there are trade offs, right? Because from an economic security standpoint, right, you're starting from scratch. And so Ethereum has $130,000,000,000 of economic security. How are you going to compete with that? It also has credible neutrality. It also has all these network effects. So I think there is a trade off.
07:19:34.870 - 07:20:10.496, Speaker B: Sure. I think it's important to be the last mover, not the first mover. I mean, one of the things that Josh mentioned a while back was, okay, in terms of how do we prioritize execution tickets? That's something that we need in order to get, like, best in class pre confirmations. And I agree with Josh. It turns out that just very luckily, execution tickets have a whole list of advantages. One is that they remove proposer timing games. So right now, proposers are incentivized to play timing games, where they delay the get header call so that they can get more MEV.
07:20:10.496 - 07:20:34.228, Speaker B: And we're starting to see sophisticated operators do exactly that. That goes away with execution tickets. Another one is MEV smoothing. So right now, as a validator, I'm incentivized to join a pool because MeV is really spiky and it's basically lottery. It's gambling. If you love gambling, fine. But most people just want to run a business or have predictable yield.
07:20:34.228 - 07:21:13.190, Speaker B: Another really cool thing that execution tickets give us is Mev burn, where the Mev no longer leaks to the proposer, but it leaks to the ETH holders. And there's all sorts of reasons why this is a better outcome, like monetary policy reasons and just sustainability reasons and whatnot, and predictability reasons. And then they also remove the relays. Right. We want to get rid of the relays and execution tickets help with that. And so in some sense, pre confirmations is like an afterthought benefit of execution tickets. But I think they will be heavily prioritized in the months to come.
07:21:13.560 - 07:21:24.424, Speaker D: All right, so I have one more question, and it's for Josh. A common criticism of outside sequencers is that they can never attain the economic security of ethereum. How would you respond to that?
07:21:24.542 - 07:21:54.724, Speaker E: Yeah, I think that's like, that's not true in an absolute sense over like an infinite timeline. But, yeah, I'm not assuming we're just going to ramp to $500 billion on any new chain. Right. There is fundamentally, it is at a level where you can't just inject that much money. There's just like, even at a nation state level, there's not just that much money sitting around to inject. The DGens can bid up what they want, but usually that's going to fall out at like 10 billion or something like that. So I think that's true.
07:21:54.724 - 07:22:50.836, Speaker E: I think one of the questions is kind of like, what is the actual attack user story that occurs that is susceptible to a lower economic security? So I think we all generally assume people would notice if someone was trying to acquire adversarially, like whatever, like $170,000,000,000 of east. That's just not going to go unnoticed. But I guess my view would be that number is actually probably quite low. It is fundamentally one of these things in the kind of like near the pros and cons of centralized points in the system where if someone's like, say a chain goes off, it's at $2 billion or whatever, right, which is like a high number. But given the market, that's not like an unreasonable number for a new network to kind of achieve over a six to twelve months time frame, they're inevitable to new chains of like, well, most of that's probably going to be locked for twelve months. So you just can't even do, it's like 17% or whatever or like 20% or 30%, like floating. So there is that.
07:22:50.836 - 07:24:10.972, Speaker E: That is obviously problematic in many ways, but there is like, okay, you can't attack it because literally the coins are locked, but as it comes unlocked, you have centralizing vectors, right, of like, okay, the early investors are probably going to own whatever, like 3% tranches. Are they just going to market? Sell that. If they say, hey, you have five large investors. And one person said, hey, one guy messaged all of us and wanted to buy all of our things. You're like, someone is probably going to raise an alarm similar to even, where are you going to get kind of liquidity on these things, right? You're going to be like, okay, Coinbase, Binance, OKX, right? You're like someone is also going to be like, hey, someone is buying all of the coins of one thing. So in a proof of stake system it's just like how actually low is the bar? I think east classic had some forks immediately after, right? Somewhat before my time, but I don't think we've ever had an actual economic kind of attack of that form. So my view is it is like a good thing to consider, but it is somewhat of like theoretical, is too weak from like, oh, it's not a real concern, but it is less of a concern than I think people worry about in that in these newer chains that are using the kind of standard VC distribution mechanism, your risks are probably going to be more like regulatory from like, oh, you have too many insiders saying less.
07:24:10.972 - 07:24:43.528, Speaker E: So like someone else is going to be able to adversarially take over the chain. So I think it's a different design space. Again, there's problems with that. It is not credibly neutral, it is not permissionless, it is not truly decentralized. I would argue that you probably see like ten chains that we would probably call decentralized if that. I'm not overly stressed about that attack factor. And again, it's one of these things that the way the price action on these things work, the value will kind of rise corresponding with the usage of it.
07:24:43.528 - 07:25:01.710, Speaker E: I don't think we've seen evidence of a chain that had whatever the proof of stake value of a chain's validator set was like a billion dollars and there was like $10 billion of TbL on the chain. I don't think we've seen evidence of that ratio happening. So not an immediate kind of stressing point, but I think there's the truth, right? There's weaker security.
07:25:02.500 - 07:25:04.284, Speaker D: You want to give a quick rebuttal?
07:25:04.412 - 07:25:52.352, Speaker B: Yeah, I mean, from a fundamental standpoint, the way I think about it is the security ratio, like how much value are you securing and how much is being secured and how much is securing? And I think a security ratio of, let's say 100 is reasonably good. If you want to try and extract toxic MEV, maybe you can extract 1% of the TVL through market manipulation, let's say. And if we want to be the settlement layer for the Internet of value, and we want to have a trillion dollars secured on Ethereum, then. Yeah, and you want a security ratio of 100, then you need a trillion dollars of economic security. And so you can't just think about it in absolute terms. You also need to think about it in relative terms. The other thing, which is kind of less fundamentals and more memetic is this idea of a shutting point.
07:25:52.352 - 07:26:14.328, Speaker B: Like what is the blockchain that is the most secure in the world? Because that is a magnet for devs. It's a magnet for users and whatnot. Because that is the one that is most credibly neutral. Going to live for decades and centuries is the one that has the most robustness. And so again, it's not just about the absolute number, it's also about the relative ranking of the economic security.
07:26:14.494 - 07:26:22.148, Speaker D: Okay, so we're running a little short on time, so maybe each of you can give your closing remarks in like 60 seconds. Go ahead first, Justin.
07:26:22.324 - 07:27:02.832, Speaker B: Sure. I guess I'm just still extremely bullish on base sequencing. I think I do agree with Josh that we do need to move fast on the execution tickets for all sorts of reasons, not just pre confirmations, but in the grand scheme of things, that is a minor detail. And I am confident that the Ethereum community has gained this flexibility, this hard fork muscle. We were able to pull off the merge. We are able tomorrow. Let's fingers crossed, pull off proto dank sharding.
07:27:02.832 - 07:27:09.556, Speaker B: And I think if we can get protodank sharding, then we can get execution. That's so much simpler than proto dank sharding or the merge.
07:27:09.668 - 07:27:11.428, Speaker D: All right, Josh, 60 seconds.
07:27:11.604 - 07:28:06.030, Speaker E: I'd say I'm all in favor of Ethereum. I strongly supportive of them getting execution tickets, and I think it's been very impressive to get the merge done to see 4844 finally kind of like get out the door. I think that'll be very exciting. I do fundamentally think there's room in the market for competitors that can act as a positive kind of forcing function. Right? Like what you don't want is there to only be incumbents in the market such that they don't have exogenous kind of forcing functions to say, hey, you have to move quickly because otherwise someone is going to try to take your market share. So I think my view is not necessarily adversarial to Ethereum in a sense of like, I am competing with Ethereum, but it is in the interest of we want to create competitive pressure so that the overall space can kind of improve as rapidly as possible and everyone kind of has sufficient incentives and motivation to kind of do their best work.
07:28:06.480 - 07:28:08.670, Speaker D: All right, thank you both. This was really great.
07:29:29.280 - 07:29:30.108, Speaker B: We're good.
07:29:30.194 - 07:29:31.884, Speaker F: All right. Hey, everyone.
07:29:32.002 - 07:29:57.200, Speaker B: How's everyone doing? Long day? Yeah. Anyone still tracking prices, or are we, like, fully tapped into the infrastructure now? Bad prices. Everyone's below the seat checking their portfolio. Okay, I'm going to talk about cake. So I spent a lot of time looking at the MEV stuff. I was previously a founder at Flashbots, and then I left. And then I'm actually more interested now at the application layer.
07:29:57.200 - 07:30:35.010, Speaker B: So I want to talk a little bit about that. I want to talk about how MEV is actually critical for the application layer. I think we heard in many of the previous presentation, oh, MeV will be solved at the application layer. The application is going to take away the MEV, so we don't need to worry about it. So I guess my role now is to say, how does that actually happen? And sort of provided a framework for people to think about it. So ankit and I, ankit over here, we work at Frontier Research, and we put together this thing called the cake framework. So what is cake? If anyone was here in 2019, it was amazing.
07:30:35.010 - 07:31:07.396, Speaker B: The community was, like, tiny. There was like, maybe, I don't know, 100 developers in the space. And we all knew each other. We all hang out together, and then we built these really cool applications that no one used. And it was kind of fun, but also sad, because as soon as a user tried to use a web three app, they had to pay like, $200 to be able to post a message. And so it was very clear that there was a fundamental problem with the way that development on Ethereum happened at that time. We had the usability.
07:31:07.396 - 07:31:57.980, Speaker B: We had applications that had account abstraction, that had gas fee abstraction, that was fully based on stable coins, and so the user could go directly from credit card to onboarding. But we didn't have the scalability, so it kind of didn't matter. The users didn't come fast forward a little bit of time. And we find ourselves in this situation where we have a really damn complicated network of middleware that all try to solve some specific niche, edge case solution, and need for application developers that are higher up the stack. And this complete mess also has resulted in all the l two landscape that we have, all the different roll ups that we have. I wonder if anyone has tried to use cross chain application. Has anyone put a hand up if you've interacted on multiple chains.
07:31:57.980 - 07:32:06.460, Speaker B: Okay, put your hand up. If you would classify that experience as easy and user friendly.
07:32:10.500 - 07:32:12.208, Speaker F: All right, so this is kind of.
07:32:12.214 - 07:32:25.536, Speaker B: The scenario that we're at, right, of. Your wallet has multiple different chains. You have to pick your RPC, you have your favorite brand of roll up that you want to use, either your red, blue, green, who knows? And then you also have to bridge your assets.
07:32:25.568 - 07:32:27.216, Speaker F: You have to build your bridge.
07:32:27.328 - 07:33:07.388, Speaker B: You don't know if the multi sig at behind the bridge is going to rug you or not. It's a bit of a mess. My take is if we are to make the most of all the middleware that we've built, all the stuff that we've built, and actually build applications that are going to deliver on the dream of web three, we need to do better from a UX perspective than the status quo that is today. And so that's what cake is all about. It's about providing a framework for thinking about all these Legos, all these chain abstraction Legos to build better applications. And hopefully in 2024 we can have applications that are actually usable. That's a dream.
07:33:07.388 - 07:33:42.076, Speaker B: We have a new set of users that are coming into the space. We have block space scalability. Now, I think this is probably the most exciting time to be building web three applications. And I just hope that we make the most out of it because I've been through three cycles already and I'm ready for actual applications and users. And we're close, actually, I want to do a show of hands. Who has used telegram bot? Okay, so we have a decent chunk of people. So telegram bots are actually probably like one of the coolest applications in my mind that exists on web three today.
07:33:42.076 - 07:34:18.180, Speaker B: It serves one of the core use cases, which some people look down on, which is speculation. But that's where a lot of the demand for blockchains is today. And it does so by abstracting away a lot of the complexity of interacting with blockchains. So I would propose that telegram bots are actually a great example of the cake. They take care of the application layer and offer that in a way that's most optimized for user experience. And they hide away all the complexity of what sits beneath it. So we present a framework that has multiple different layers and trade offs in the design of the components beneath those layers.
07:34:18.180 - 07:34:57.092, Speaker B: And we think that it would be good for application developers to have a simple, unified interface to be able to tap into all of this instead of having to navigate a bunch of different wording and, I don't know, branding and terminology across the different things. Does that sound like a good idea to do? I think so. Right yeah. Okay. I want to have a quick show of hands because I want to read the room a little bit. We've segmented the cake into the application layer, the permission layer, the solver layer, and the settlement layer. By show of hand, who here is working on the application layer? Wow.
07:34:57.092 - 07:34:59.300, Speaker B: We have three, four application layer people.
07:34:59.370 - 07:34:59.940, Speaker F: Okay.
07:35:00.090 - 07:35:38.000, Speaker B: We're an infrastructure heavy team here. Who is on the permissioning layer? So, like wallets, signatures, permissioning intents? Maybe a couple people. Who's on the solver layer? Who's building solvers? Okay, what about mempool routing, auctions, all this kind of stuff? Yeah. Okay. The silver crew is over here, and then the settlement layer bridges, layer ones, anything related to execution oracles. What are all you guys building? There's no one else putting your hand up. Okay.
07:35:38.000 - 07:36:07.660, Speaker B: All right, so we have a mix across the cake here. So I'm super excited about this. Like I said, I think it's the most exciting time so far to build web three applications because we have the block space scalability. But please, please, let's build it in a way that actually makes user experience great. I'll pass the mic over to. Oh, yeah, this is my meme. I'm basically saying a lot of this is possible because of MeV.
07:36:07.660 - 07:36:54.440, Speaker B: I want to clarify this. So to me, Mevboost represented one particular shift in mindset for the development of the ethereum ecosystem, which was breaking away from the Ethereum transaction format. So when you shift over from a pre PBS to a post PBS world, you introduce, yes, this specialized role that is a block builder that we spent the entire day today talking about. But what this block builder can do is actually introduce new transaction types, and those are often referred to as intents. And we're talking about pre confirmations and all kinds of cool stuff. I think these are all critical components that lead towards what is cake and actually enables better user experience for users. So this means everything is cake, and cake is inevitable.
07:36:54.440 - 07:37:18.690, Speaker B: So, yeah, we'll see more about this later. We have a working group, actually, we're doing a session on Friday with working group members. So if you're building something that's relevant and you'd want to participate, we'd welcome your participation. It'll be in the same space, I think so, yeah. All right, I'll pass the mic over to Ankit, and it'll talk about why chain abstraction and different trade offs in.
07:37:19.560 - 07:37:20.020, Speaker G: Hello.
07:37:20.090 - 07:37:20.710, Speaker B: Hello.
07:37:27.720 - 07:38:11.288, Speaker G: So thanks a lot, Stefan, for introducing the cake. Let's talk about why chain abstraction. So I'll talk about three things. Why chain abstraction? What are the trade offs with chain abstraction? What do we even mean by chain abstraction? And what are the six designs or six pieces of cake that emerge from if you want to build something like a chain abstraction experience for the end users? So let's go back 20 years. Let's go back 20 years. Suppose SMTP, TCIP, HTTP, all these standards had not emerged and you have this PDF of your wedding and you want to invite people and you want to send this PDF to all your friends. What do you do? You basically Google an email aggregator.
07:38:11.288 - 07:38:54.770, Speaker G: You basically type your to and from address. You paste a PDF and you get this list of three options on how are you actually able to send this PDF to your friends? So there's a fast option where you pay $20, but it's very quick. There's a medium option and then there's a cheap option where you have to wait like 20 minutes, but it's very cheap. And then you go through this and then you somehow send this PDF to your friends and you get a wrapped version of the PDF. After spending tens of dollars and after spending tens of minutes, you get this wrapped version of PDF. It does not even have the date. And so you have this experience of you spend so much money, you spend so much time and you do not even get the actual PDF that you are trying to send.
07:38:54.770 - 07:39:45.660, Speaker G: Thankfully that did not happen. Thankfully we had HTTP, TCIP, SMTP, all these standards that emerged where instead of going through this loop or hoops of spending a lot of money, a lot of time, you have this experience, seamless experience of drag, drop and send. And these delightful applications could have been built just because you have these all of set of standards where the users do not care how they are interacting with these underlying layers of the cake of the web. Two cake. You have just this delightful experience that is available to be built for the application developers. Unfortunately, this is what we have today in crypto. So you have these different roll ups, you need to have money to spend for fees on all these different roll ups.
07:39:45.660 - 07:40:15.848, Speaker G: Every time you switch a roll up, you need to somehow do this, which is very irritating for me personally. And then when you are trying to move assets from one chain to another, you have these list of options and it takes a lot of time, it takes a lot of money and it just causes a lot of friction. It just causes a lot of user drop off. And this is not what application developers want. We still have a lot of activity on all these roll ups. It just shows you how powerful economic incentives are. But if you want to build delightful experiences.
07:40:15.848 - 07:41:12.510, Speaker G: This is not what you should give to the users. So what is chain abstraction? So block space is abundant right now, you can literally one click deploy a new roll up. What is the bottleneck? Is the Ux? How do you move these assets from one chain to another? And so the chain abstraction is an ideology where the users should not know what network they're on. The user just connects their wallet to the app, clicks on what they want to do, and then in the back end, in the infrastructure layer, everything that happens needs to happen. What network needs to be deployed is a DAP decision. The users should maybe not care that much about it. And in our opinion, chain abstraction will lay the groundwork to build the next set of applications, and will lay the groundwork to bring in the next magnitude of the users, where most of the users might not care about some of these constraints, and they just want delightful application and delightful user experience.
07:41:12.510 - 07:42:08.204, Speaker G: So that's chain abstraction. Let's look at what are the key trade offs that if you are building a chain abstraction framework, if you are trying to build something like this for application developers, what are the key trade offs that you would want to take into account to somehow answer some of the questions that emerge during different layers of the cake. So the first question that you want to answer is what type of message or what type of thing that you are trying to transfer between two different chains. Are you trying to transfer a message or any type of information? Or are you trying to transfer value? So if you're trying to transfer message, then messages are all type of information, any type of information, but it needs to be lossless. So if you are trying to participate in a governance vote, you do not want your RIA's vote to be converted into a maybe. And because it has to be a lossless, it has to use the most secure pathways. So it covers all types of transactions.
07:42:08.204 - 07:42:46.872, Speaker G: And some examples are governance votes, approvals, stuff like that. The second type is value transfer. This is what blockchains are made for. You need to conserve this like scarcity and not atomicity properties when you are trying to transfer value. And so you need to do this in a lockstep fashion where you deduct funds from the sender and you credit funds into the receiver. And it can be lossy because it can be lost. Like if you are transferring $100, you are happy maybe by just receiving $99 and you pay one dollars to whoever is a third party or a counterparty that is trying to facilitate that transaction.
07:42:46.872 - 07:43:27.460, Speaker G: And so you can leverage third parties to basically facilitate that transaction. 95% to 99% of the transactions today in blockchains are value transfer transactions. This is measured by the gas paid. And some types of value transfers are transfers, swaps, gas, stuff like this. The next set of trade offs come based on how you are trying to leverage the solvers into different layers of the cake. And so if you decide to actually leverage solvers, how you bring in solvers into these different layers of Craig bring in their own set of trade offs. And so if you are on the permissioning layer, there's this trade off between UX and agency.
07:43:27.460 - 07:44:12.896, Speaker G: And so different users lie on a different set of spectrum on how much control they want over the transactions versus how much they are willing to let go to different sets of applications. And so, like EOA accounts are accounts where users have almost all the control. And then account abstraction. Wallets are where users are willing to find a counterparty to pay for gas. And then policy based wallets, like telegram wallets or near wallets, are where the user just signs one thing and then maybe there are 30 different things that happen in between, all in the back end. The user just does not care about it, it just happens for them. So there is a spectrum between UX and agency, which these different types of wallets or permissioning systems basically give to the users.
07:44:12.896 - 07:45:00.976, Speaker G: The second thing that you need to figure out is how do you find the optimal path. So suppose you want to do something cross chain between chain A and chain B. There might be 30 different things that the solver needs to do, or the user needs to do. You as a chain abstraction layer or chain abstraction framework, what you would want to do is you would want to bring in multiple set of solvers who can look at what the user wants to do and find the optimal path. And they have their own competition in between, and they find the optimal path and give it to the users. But because there are multiple solvers who are looking at what the user wants to do, the state of these chains might deviate and you might lose on some execution guarantees. And so this is the trade off that you want to resolve on the solver layer and on the settlement layer.
07:45:00.976 - 07:45:49.136, Speaker G: What you would want to do is solve this trade off between low fees and execution speed. And so if you want to transfer information, if you want to use the most secure pathways, you would want to wait for finality guarantees. And so if you want to exchange information between two chains, you would wait for finality. You would wait for economic finality, or any type of finality that you want for, and that introduces some time delta. Whereas if you take on some solvers and they're willing to take some risk, they would charge for their services, but they're willing to take some timing or finality risk and give you that service in exchange. But all these different trade offs bring us to the six different designs that we think emerge from these different sets of trade offs. To just recap like there are five trade offs.
07:45:49.136 - 07:46:27.840, Speaker G: One is between information and value transfer. The second is between the user agency. The third is between fees, execution guarantee and speed. So based on these three trade offs and how you can bring in solvers, there is this trilemma that emerges between execution guarantee fees and execution speed. And if you want to take only execution guarantee and low fees, what you would want to do is just use the in protocol paths. Just use paths that are determined by the protocol. So this might look something like a base sequencing, or this might look something like an IBC on cosmos.
07:46:27.840 - 07:47:40.180, Speaker G: If you want to improve your speed, what you would want to do is bring in solvers into the system, bring in solvers into into your chain abstraction framework. And so if low fees and execution speed is to be optimized, then you would want a solver competition, where multiple solvers look at your order flow and compete with each other to find the optimal route. And if execution guarantee and execution speed is preferred, then you would want the solvers not on price to compete, not on price, but on speed. So based on the trade offs that emerge, these are the six pieces of cake that emerge. All of these different designs of how to transfer value or how to transfer information between different chains are in opinion their own set of product market fit. They're also a little bit composable with each other. And so if you want to transfer value and you want to use the in protocol parts, then a good example is USDCCP, where the token team comes in and says, on all of these chains, this is our canonical token address, and this is the bridge that has the right to burn and mint the tokens on all the chains.
07:47:40.180 - 07:48:39.640, Speaker G: If you want to do message transfer, but still use this in protocol path, still let go of speed, but have good high execution guarantees and low fees, you'd want to go with an ecosystem aligned bridge route. So this looks very close to like a shelling point of an ecosystem, where an ecosystem says, okay, this is what we are providing for you, and then all the application developers can gravitate towards that. And so things like IBC, Agiler, polygon are examples of ecosystem aligned bridges. There is one issue with the roll up roadmap, which is in IBC. Because Cosmos has very fast finality, you can very quickly basically pass messages in protocol path. But because roll ups or optimistic roll ups are seven day finality, that is a very slow path for all the users. And so what ends up happening is applications tend to start bringing in solvers to basically give that speed to the users.
07:48:39.640 - 07:49:13.700, Speaker G: And so if you want to transfer value and have a solver competition, then you would want to bring in multiple solvers. And whoever has the best price, you will give that order flow to them. So examples are jumper bungie, which are competitions of these bridge protocols. Uniswap X is a competition of solvers. And then if you want to do multiple types of messages, then you would go the wallet route where things like near avocado, Alfred, are example, these look like super apps. Aggregators or aggregators. So this looks like an aggregator, but this looked like an aggregator of an aggregator.
07:49:13.700 - 07:50:13.364, Speaker G: And so if you want to go for speed, then what you would do is you would bring in all the solvers, but say, hey, whoever is the first will get the MeV or we'll get the execution rights. And examples of this are across. And then finally, if you want high execution guarantee and high speed, then basically you would go the exclusive batch option route, which is very similar to execution tickets. And that's what Robinhood uses as. And so these are, in our opinion, the six pieces of cake, six different products that emerge if you are trying to give that cross chain or chain abstracted experience to application developers. And so to conclude, we think that a lot of the users that might come in the next cycle do not care about what network they are on. They just want delightful application, delightful experiences.
07:50:13.364 - 07:50:39.750, Speaker G: And that's what chain abstraction is all about. We introduced the cake framework, which is a way to think about how these different layers interact with each other. What are the trade offs in each layer? And I think if we want to give this delightful experience, then we need to define or we need to adopt a common standard to basically move these messages across. So that's all on my side. Thanks a lot.
07:51:28.330 - 07:51:37.210, Speaker F: Can you hear it? Good. Okay, sweet. Let me find my slides.
07:51:43.450 - 07:51:46.680, Speaker D: Sorry, my bad. It's literally here.
07:51:48.330 - 07:52:05.360, Speaker F: Let's go. Okay. Hello. All right, we don't have much time, so I'm going to get started. Please stop talking. Okay, cool. Last talk of the day.
07:52:05.360 - 07:52:24.980, Speaker F: Cool. Thank you. Yeah, cool. All right, last talk of the day. You should have a notepad under your seat. We're going to be doing some differential equations no, just kidding. There will just be a couple of jokes, some shilling, and then maybe a panel, I'm not sure.
07:52:24.980 - 07:52:52.858, Speaker F: Okay, cool. Yes. So suave. A big node framework. So if you haven't been around all day, often we've been talking about this framework of small and big nodes. The TLDR is, it's a much simpler sort of mental model for thinking through protocol actors. Instead of having like 15 different actors all across multiple different domains, you have two, you have big nodes.
07:52:52.858 - 07:53:23.320, Speaker F: These are heavy resourced machines. These are typically block builders, sequencers. They potentially run multiple VMs and benefit from the network effects of operating multiple VMs. They also do things like data availability, sampling, and generating, like, ZK snarks, all for the small nodes who are validators of chains. And they sort of take this information, they will just validate the blocks that come out of these nodes. Cool. So let's get into it.
07:53:23.320 - 07:54:13.030, Speaker F: So what happens when small nodes and a big node love each other? Well, you get a scalable blockchain. What happens when small nodes and big nodes love each other? That's plural. Big nodes, you get a scalable, decentralized blockchain. And this is basically the end game. And so multiple paths sort of lead to this world where we have a couple of big nodes producing blocks and just tons of small validators validating these blocks. And some of the key points are you need one honest builder to serve the network for liveness and censorship resistance. We only need two for an efficient market, and we just need an honest majority on the validator set to remain decentralized.
07:54:13.030 - 07:54:57.300, Speaker F: So why do big nodes need small nodes? Well, they're searching for validation joke from quintus. So do big nodes and small nodes always form a happy union? There's like two ways to look at this. One is l one, PBS. The other is l two. So, sequencers generally, we haven't seen too many issues with sequencers and l one validators becoming too annoyed with each other. And also generally on L one, we've also seen that this union is pretty happy. Block builders make validators much more money.
07:54:57.300 - 07:55:56.550, Speaker F: Validators now don't need to do this heavy computation that would probably take them ages to be able to spin up, or they would most likely outsource. Anyways, there's one small exception of the unbundling attack, which happened. I won't go into it, but someone spun up a validator and then abused some trust for the block builder because of a bug in the webboost protocol. But, yeah, so we talk a lot about this big node and small node union, and particularly in the context of PBS. But one thing we don't talk a lot about is how users fit into this picture. So we're going to dive a bit into some of the ways in which users are perhaps disadvantaged in this union with a case study on the big bad, big nodes on Ethereum. So if you've ever read the Daily Mev, it's a very hot newspaper here in London.
07:55:56.550 - 07:56:44.150, Speaker F: So one of the concerns is builder centralization. And so, as we mentioned, we just need two for an efficient market. We've already reached points where there have been builders that have gotten over 50% of the blocks, one in like a seven day period, I believe. And so while we only need two, and that still leaves room for two, that's great. The jump from, I would say, like from ten to 50%, or even one to 10% is much larger than the jump from 50% to 100%. And we've already anecdotally heard that builders that are. One of the builders that has gotten over 50% is like self limiting the amount of blocks they won, which is sort of a story we also hear with liquid staking pools.
07:56:44.150 - 07:57:22.542, Speaker F: Another one is builders leaking private transactions. This has happened on multiple occasions. A user trusts a block builder with their transactions. There's an agreement, they won't leak it, and for some reason it comes out. Another one is builders leaking block data. There's been a few scenarios where at the end of the block, there's still some arbitrage left on the table. So block builders are trying to capture this by giving information or the ability for searchers to backrun the entire block.
07:57:22.542 - 07:58:02.240, Speaker F: And while that's great, when the user consents to this, you plug in an RPC, you know where you're sending it to. They publicly advertise that this is a functionality they do. Some block builders are not advertising this functionality and they still do it behind the scene. And additionally, we have no observability into this type of behavior, so it's potentially risky. Additionally, you can imagine really bad scenarios where if there's a white hat rescue transaction involved in the block, this now gets leaked and it's concerning builders back. Running user transactions is another one. There's multiple order flow auctions on the Ethereum network today.
07:58:02.240 - 07:58:35.750, Speaker F: This is a way for me as a user. I submit a transaction, I let a network of searchers submit backruns. This service then picks the best one and sends it to block builders. We've seen some scenarios where block builders are back running these transactions as well, when there's still MeV left on the table and they're not attributing it back to the user as these protocols were designed to be. Builder verticalization. This is also a story as old as PBS. So there's verticalization between searchers and builders, builders and relays.
07:58:35.750 - 07:59:34.654, Speaker F: Now we're also hearing stories of builders also directly petitioning validators to vertically integrate. Yeah, and then builder incompetence. It's hard running a big node, so it's hard to estimate the amount of value that's been lost by hiccups in builder infrastructure. Okay, so taking a step back, what properties do we want from the market? We mentioned liveness, so we want it to be expensive to cause missed slots. We've already seen relays which operate in optimistic mode, failing to demote builders who have submitted invalid blocks, causing to, I think upwards of seven to ten blocks have been sent to the network that were invalid and the chain doesn't halt. But there are no transactions during this period, which is bad censorship resistance. So there's multiple mechanisms there that are being worked on.
07:59:34.654 - 08:00:16.730, Speaker F: So CR lists are a popular one. Also, multiple concurrent block builders or proposers is another interesting space, user protection. So encrypted mem pools and then credible compute. As a user, I would like to know that what the builder advertises is what's actually being run on my order flow. And then there's a couple of other ones, like revenue maximization, geographical decentralization to avoid regulatory capture, and permissionlessness, which we won't go into. So there's two relevant trends here. We expect big nodes to continue to be a thing and to continue to become larger and larger.
08:00:16.730 - 08:01:05.514, Speaker F: Searchers are becoming more sophisticated, solvers are becoming more sophisticated. We even have zk provers and zk prover markets that are now coming online, which are additionally requiring more and more resources to do their job properly. And then lastly, we also expect it to be useful to be able to hook into this logic in a much more customized manner. And so that is basically you want your application to hook into this big node block production process. So one example of this is the cowswap protocol. Users send in intents and then solvers find the best routes through these intents to match users. So they have a piece of off chain infrastructure which scores these solutions that solvers send in.
08:01:05.514 - 08:02:05.902, Speaker F: And it's the case that often, or not often, but sometimes, because they score these solutions on top of block simulations, where it actually gets simulated, where it actually gets inserted in the block once it lands on chain is actually subpar compared to a solution they threw out. So this is just one example of how being able to more intimately hook into the block construction logic is ideal. Again, though, to be able to use this functionality requires more resources and to be a bigger node. So we believe obviously that suave is the answer to both of these trends. We want to satisfy these ideals and then also provide a generalized platform for anyone to be able to build on top of this and insert your own logic. So what is suave in sort of the joke theme in the beginning, it's a prenup between the users, the big nodes, and the small nodes. Users want to be able to enforce certain guarantees between these parties before they are to join in.
08:02:05.902 - 08:02:34.550, Speaker F: Holy matrimony. So yes, you can think of suave as a programmable mempool that you can put your big node on top of. We provide a couple things. One is advanced privacy primitives. So from our experience in running MeV share and some of our other off chain applications, not one set of privacy rules fits all applications. In fact, some applications don't want any privacy. Some applications, like dark pools want complete privacy.
08:02:34.550 - 08:03:25.078, Speaker F: So we give you the ability to program your privacy primitives. Next is credible compute capabilities. So this is as mentioned earlier, you want to be able to ensure that when you use a builder, they're doing what they said they did, and then low latency message passing is obviously another feature to basically allow communication between a lot of these big nodes. So yeah, applications on Swab can take on a variety of forms, mostly in this talk we're discussing block building. So you can imagine a block building protocol as being one of these big nodes that lives on top of this programmable mempool layer. But additionally you can build trading protocols oFAs searchers and even crazy stuff like AI and web. Two applications, we do have an LLM running in a TE.
08:03:25.078 - 08:04:24.678, Speaker F: Talk to me later about that. And then the other cool thing is, typically you get no composability in the off chain environment. But because we're using solidity and trying to put as many of these nodes on sort of the same cluster of hardware as possible, you actually can achieve off chain interoperability between these protocols. So what does it mean to build your application on suave? Yeah, so it's a place where you can define this off chain logic, whether it's block building, whether it's an auction, whether it's intent platform. It's also a way to define your data storage requirements. And then if you want to make your application decentralized you can also define consensus and your peer to peer protocol as well on top of it. And then as well, in order to keep with the current paradigm where when you deploy a smart contract on L one, you don't have to operate it anymore.
08:04:24.678 - 08:05:08.230, Speaker F: You can also pay a network of Te coprocessors to run this application for you all autonomously. You could even do crazy things where perhaps the protocol pays for the fees itself. Yeah, and then it's also a place to define multichain applications. So in that previous example, let's say you had protocol logic with your Ethereum node set up. You could set up a cluster of these nodes running with an optimism chain node and allow these two to operate together and settle like cross chain intents. And you can keep kind of stacking these protocols on top of each other, sort of like Legos.
08:05:09.550 - 08:05:09.866, Speaker B: Yeah.
08:05:09.888 - 08:06:10.570, Speaker F: So the TLDR is basically, if you're looking to provide your users guarantees that you're not abusing them, the swab platform provides integrity on your application logic if you're looking to provide your users privacy on their data. We also offer trusted execution environments which are available today, as opposed to sort of like a lot of other solutions like MPC and FHE, which we also hope to support, but are just not latency compatible with the current requirements. And then additionally, we provide you a way to decentralize your application. Yeah, so that was basically it on the sort of explainer of how you can run your big node on suave. And so this last section will just talk a little bit about what people are building, because they be building. So if you have ideas, come to the flashbots forum, we will help you validate and build them. There's already been a ton of examples on that.
08:06:10.570 - 08:07:01.610, Speaker F: The first one is we've had a PoC of integrating suave as the block builder on Astria, which is super cool. I recommend checking that out. We've had a composable block building example by Jinsuk at Nethermind, who. It's essentially a protocol for a block builder to work on half of a block and then share that with another block builder, but still be able to come out with one coherent block and distribute the revenue between them. Miha from Eden Network also has built a binance, Oracle, which is super cool, a super cool primitive if you're trying to build like some type of LVR minimizing off chain auction. We've also had an example of a frequent batch auction that was built. We had a hackathon project to build a blob merger, which is exciting since blobs go live tomorrow.
08:07:01.610 - 08:07:40.330, Speaker F: We think this will be very interesting for roll ups, especially smaller roll ups who don't need the entire blob space in the next couple of months. There was also a multichain intent application that was also built and prototyped. And then lastly, this isn't a prototype, but we've recently been getting a lot of interest for supporting Rip 75 60, which is a standard for native account abstraction. And so this would allow you to set up a wallet on suave, which allows you to control remote accounts on separate chains. All from that chain.
08:07:41.870 - 08:07:42.330, Speaker B: Yeah.
08:07:42.400 - 08:07:53.420, Speaker F: So that's it. Yeah. Come and hack on the alpha rigid testnet. That's our suave testnet. APIs are in flux, though, so it is for true pirates only. Thanks.
08:10:06.610 - 08:10:15.482, Speaker B: Hey, man. How are you, Vegas? You've been talking a lot today. I listened on my ride over my airport over. It's like listening to these debates.
08:10:15.546 - 08:10:16.818, Speaker D: It was pretty cool.
08:10:16.904 - 08:10:29.090, Speaker B: Oh, yeah, the thing that he was. Yeah. So this is the end. Yeah.
08:10:29.540 - 08:10:32.716, Speaker F: You caught the last closing in, Dustin.
08:10:32.748 - 08:10:43.720, Speaker B: You're good. We're sending everyone to drinks. Kind of. Cool. Okay, let's get started. Good job. Thanks, quintess.
08:10:43.720 - 08:11:57.180, Speaker B: So we have, I think, a closing panel on application specific sequencing. And basically one way to categorize sequences in my mind is that you have the general purpose sequencers that will cater for general purpose roll ups as well as some more specialized virtual machines. And then you have a completely different type of Beast, which is the application specific sequencer, which for some reason or another, they've decided to not opt into the largest or the de facto shared sequencer. They have very, very good reasons for doing something else. And I guess one type of example here could be gaming, right? Like, I might want super low latency or super high uptime, or I want to very precisely control the fees that I have to pay for sequencing. I don't want to be subject to activity from other applications and suffer MeV spikes and things like that. I think another potential use case is Dexes.
08:11:57.180 - 08:13:05.060, Speaker B: So right now we have this problem where Dexs, like uniswap leak MEV, specifically the sex Dex arbitrage opportunity to the proposer. But there's these ideas like, like where when you introduce an application specific sequencer, and then now you have the opportunity to auction off the MEV opportunity and rebate it back to the LPs so that it no longer leaks to the sequencer. And that's actually net beneficial to the users in some sense. It's returning Mev to the users. And we have a very similar idea with oral calls and with liquidations. So I guess my first question is, do you agree with this dichotomy of shared sequencer versus application specific sequencer? And what other applications do you think could make sense to be application, to basically use an application specific sequencer? I got ideas. Okay, so this term, application specific sequencer is kind of new to me.
08:13:05.060 - 08:13:17.270, Speaker B: I kind of like it. It was coined yesterday. Yes, coined yesterday. Okay, it's interesting. We're done with this stuff. Come on. I was the only one that didn't know what it was.
08:13:17.270 - 08:13:32.252, Speaker B: Okay. Going to be on a panel. Going to be on a panel on applications. But here'd be my argument. Okay, because, Justin, we've talked about this before. You're a futurist. You're thinking five years out, right? And generalized things seem great.
08:13:32.252 - 08:14:20.220, Speaker B: Five years out today. The killer app, or the broadest use case of a lot of these blockchains, and where most of them have comes from, is from trading. So in a multi chain world, it's going to be swapping assets from chain A to chain B, or bridging the same asset from chain A to chain B. And I have two points here. So, first of all, it seems logical that you may want to today design a specific solution to capture MeV and make this killer use case the most efficient and offer users the best execution. Today, that seems logical, given that it's a big part of the market. And part two with trading in particular, there's another feature that allows you, it actually opens up the design space, which is the fact that these assets are fungible.
08:14:20.220 - 08:15:11.390, Speaker B: So if you're going to fill a user on a different chain, you can literally just fill. It doesn't have to be the asset on the origin chain. Whatever the same asset is, you can fill them. And so the bridge which can do swapping that I work on called across, takes advantage of this in this quote, unquote, intent based architecture to use other buzwords, where we're using the fungibility aspect to fill users on the destination chain in a sort of application specific sequencing, if you will, where the market makers are purely taking on the risk of best execution or competing for best execution for the user. And we are laser focused on trying to make that be as efficient as possible, even if five years from now there might be more generalized ways to do it.
08:15:11.840 - 08:15:12.316, Speaker G: Got you.
08:15:12.338 - 08:16:05.470, Speaker B: I think I would push back against the dichotomy that you drew, too. When I saw application specific sequencing, what I thought we were talking about today, actually. So that was new to me, was the notion that applications could program generalized sequencers to have sequencing rules that are specific to them. So you have your generic EVM chain today, but you're an application on that, that wants some sequencing rules for your application. So I think on Ethereum l one, you can see this with solvers as an example, where you're trying to give a user the best trade possible, and the optimal way to do that may be to place it in a certain place in the block back running another trade as an example. So you want to program the sequencer to have some special logic of how to handle your transaction. So I think there's a third way actually, which is what I thought we were talking about.
08:16:05.470 - 08:17:00.564, Speaker B: In some sense, you want to try and marry the two, because there is a need to have separate rules for specific applications. And sometimes that means creating a totally new sequencer, and that comes with composability trade offs. So for example, Sorella or Calswap is not as composable as an L one contract. But if you could take a shared sequencer and kind of customize it to your application and retain the composability, that would be the best of both worlds. Yeah, maybe. And again, I want other people to talk, but there's like kind of two paths here. Like do you start with the specific use case of optimizing, let's call it swapping, and then figure out how to generalize that through chain abstraction? Through maybe what you're swapping actually is all you're doing to move from chain A to chain B is you're moving enough gas to execute the user op from account abstraction to do the thing on the other chain.
08:17:00.564 - 08:18:01.436, Speaker B: I can paint a path for chain abstraction flowing from an intent based bridge, or kind of. So you start specific and generalize, or do you start generalized, do you build the generalized thing over here too? And I think we're just going to see the different approaches play out. I'm just confused. Who's the sequencer in the application specific sequencing world? Is it still the market builder supernode, or is it someone else? It's someone else. So in the case of Sorella, they have their own external consensus, who's like custom designed for maintaining an order book and matching orders, and they use that in order to auction off the LVR. Okay, so then they produce like a set of transactions, like a block, and then they post it somewhere. What do they do with that ordering? Yeah, so they produce a block which has this finality certificate.
08:18:01.436 - 08:18:33.548, Speaker B: So basically two thirds of the validators have signed off on it. For their own roll up. Yeah, for their own roll up. And then that gets settled on l one. I think Calswap, they have basically a centralized sequencer, which is like application specific, and you can think of finance as being its own application specific sequencer. I guess I do agree with Robert here. Why isn't the sequencing just a program that runs within the block builder at whatever merge time that they're doing? Well, if you could do that, it would be amazing.
08:18:33.548 - 08:19:03.236, Speaker B: But neither Cal swap nor Sorella did that. Well, hold on. It's okay. So my mental model for Sorella is you've got a block and you want to do a bunch of trades on a dex and you want to capture LVR or whatever. So you're going to sequence all the trades in that block. There's like a unit here, and that's what you're sequencing. And then for cowswap, you have a batch auction over some period of time with a group of trades, and you're going to solve that and sequence that.
08:19:03.236 - 08:19:26.120, Speaker B: Right. But then there's the other way, which is just, you have a trade, aka a user intent and. What's up, Cal? Swap a rule up now? No, but this is where I'm trying to use the application specific sequencer idea here. This is the new idea. Okay, but let's kill it. So let's delete that and just talk about best execution for a user on a swap. I have a trade.
08:19:26.120 - 08:19:55.268, Speaker B: I have a swap. I want to auction it off. There is no sequencing. Exactly, because it's just one single trade, but because I'm not specifying how that trade gets executed, in a sense, the market maker is abstracting away how they're going to end up doing it. Right? Yeah. And so I don't know if this fits in your framework, but I look at that as like a single action. I'm capturing all the MeV, I'm not leaking any information about how I'm going to execute that trade.
08:19:55.268 - 08:20:56.696, Speaker B: I'm principaling the risk as a market maker, and there is no mev leakage, and it works pretty well. And you can do it today, but you don't have the composability aspect. You don't have the composability that Justin really wants for the ethereum ecosystem. Well, so then you layer on top of that, right? So it's like, okay, what is the composability I want? So if it's just swap asset A to asset B from chain one to chain two, and then deposit in a lending protocol? Well, I think it's more like everyone has their liquidity on l one, and you don't need a Dex on any l two s because you can just borrow that liquidity at runtime. When you do the sequencing within the suave node that's running on the block builders, or you have the market maker do that with their own sophistication. And so they're the ones providing liquidity, but then they require inventory on whatever layers, right? Sure. That's less efficient than just having a single pool of aggregated liquidity on one place, maybe, but we do it pretty well.
08:20:56.696 - 08:21:34.920, Speaker B: The across bridge has. Yeah, but really we're talking about the future here, talking about today. One of the things that is challenging about this is that everyone has their own application. That's sequencing. Everyone has their own set of information, everyone has their own liquidity. And you want composability between these things because of the idea that we have in crypto that the sum of parts is greater than the individual parts in and of itself. So the cowswap example is a great example for that, or a solver that's sending a transaction, you want to be able to backrun someone else's transaction, but you may not trust the market maker to have access to your private information.
08:21:34.920 - 08:22:28.804, Speaker B: And so that's why we're bullish about the notion of having privacy for these things. Trusted execution environments where you can get this composability and collaboration between people, because we do think it gives users better execution in the limit, because it enables collaboration where you wouldn't have it otherwise. I have a question about suave and sequencing. So if we have a program which is like an application specific sequencer that we deploy onto suave that is run by a block builder, how does that interface with programs that are not deployed to suave? That's an interesting question. I mean, presumably your program has some set of APIs, and you'd be able to feed it inputs at some point in the block building process. I think we'd have to sit down and really look at the details. But anything that is external to swab won't have access to its internal state, at least between blocks that it has produced.
08:22:28.804 - 08:23:09.910, Speaker B: So maybe it can produce like a blob for the suave stuff, and then a different blob for the non suave stuff. Yeah. And I think it's really up to your application to define those rules of what you are sharing, when and how the external world interacts with it. Right? So imagine a batch auction that's a program on suave that you want to settle at blockbuilding time, right? Of course it's going to take a bunch of inputs throughout the blockbuilding process, and it's up to that application to define what that looks like. So batch auction wants to have people trading until the very, very last minute. And probably it defines some sort of API through we use solidity for how people interact with it. So it is application specific.
08:23:11.480 - 08:23:56.064, Speaker C: The way that I took application specific was more so an application that's built on some type of execution layer that's off chain, that posts some types of commitments on chain, which the chain that you're posting the commitments to is somehow aware of. So I think it depends on the perspective of the operator or the user of that particular protocol. So what we're building with anoma is a generalized architecture for intents. So from the perspective of a user who wants to settle natively on anoma, it would not be application specific. It could be a generalized application. But from a user that wants to settle to Ethereum, it could be an application specific sequencer. Just back to your first question, Justin.
08:23:56.064 - 08:24:45.920, Speaker C: In terms of the types of applications, I think there's actually a lot of low hanging fruit. For instance, if you think about proof of personhood or a digital identity, there have been teams like circles, Ubi, who have done various things to try to bootstrap their network. They've had issues building out the infrastructure, and they need something a little bit more generalized. And in particular, they need something that provides some sort of information flow control, or as others might say, privacy. So this would be the type of application that you could build on inoma and still reason about it on Ethereum. In addition to that, there's things like, I wrote a blog post about a potential application intent centric Kickstarter with dominant insurance contracts. This would allow you to aggregate demand side preferences.
08:24:45.920 - 08:25:24.320, Speaker C: So for instance, if you wanted to do something like contact a hardware manufacturer and get them to agree to a spec to build a phone for you, we all in this room could express our intents. We could do multiple rounds of consensus, and then we could settle potentially on the spec with the hardware manufacturer. And this is something that you can't do today on Kickstarter. Another example of an application could be like a multi chat application. So you think about like encrypted slack or discord. These are the type of applications that you could build with an application specific sequencer, but still have some awareness on the base layer in some type of construction, perhaps like a plasma.
08:25:26.740 - 08:25:57.870, Speaker B: Gotcha well, I was just going to. So, Patrick, one of the things you said, right, is back to the beginning, what you're saying, this idea of a protocol that you define rules, which we could call the sequencing rules on chain, and then you can have off chain actors that do things that then get settled on chain and verify that those rules were followed. And so I'm being pretty abstract here, but this is like sequencing rules, right? Defined on chain. Off chain actors do things.
08:26:00.480 - 08:26:02.030, Speaker F: Yeah, I think I follow.
08:26:02.640 - 08:26:25.632, Speaker B: Yeah. Okay. Too abstract. But my point here, the thing that I just kind of want to shake and be like, guys, we're missing it. It's like, if 95% of what users want to do here is swap asset A to B, or swap asset A to B and then execute code on destination chain, if that's like 95% of what users want to do, that set of rules we can define on chain pretty clearly.
08:26:25.696 - 08:26:31.664, Speaker C: But what is on chain, though, right? Like on chain ethereum or on chain celestia or on chain the cosmos hub.
08:26:31.712 - 08:26:53.100, Speaker B: Right. Okay, let's go back to what I've actually been talking to Stefan about a little bit here. We want a standard for a cross chain intent. Okay? So this is an order ticket of what a user wants to do. And they sign it, and it says input asset on chain a, output asset on chain B. That's what I want to do. Okay, so there's my order ticket.
08:26:53.100 - 08:27:27.560, Speaker B: You call it a pre confirmation messaging exit standard. Yeah, precomp messaging standard. A base precomp messaging standard. But it is a cross chain limit, order based precomp message standard. It's a cross chain limit order that maybe also has some extra stuff on the destination, do this other thing, execute this arbitrary code, and the user signs this. Okay, so what do we want to do now? And we're talking this multichain world. We want user assets to get escrowed somewhere.
08:27:27.560 - 08:27:54.284, Speaker B: We want searchers to compete to fill this order. We want the best searcher, solver, market maker, whatever we want to call them, to fill the order with their own capital. And the advantage we have here is fungibility. So we don't need to do anything on chain. They can just fund it. And then after we verify that that order is fulfilled, we release the user assets back to the solver. And then it gets complicated.
08:27:54.284 - 08:28:20.808, Speaker B: Like, okay, are the user assets on the origin chain, they get paid back on the origin chain. Does the solver have crosschain inventory? I got a lot to talk about all that stuff, but as a basic concept, this works really well. And it isn't composability, but it approximates composability from the UX perspective. Right. And you can do it today, like across is doing it today. Uniswapx is doing it in a single chain context today. One inch fusion is doing it today.
08:28:20.808 - 08:28:45.810, Speaker B: It works today. Solves a pain point. It's cheap, good execution. My pitch, I mean, I guess what you're saying is that you have this one application swapping which doesn't require composability. And so you might as well have a custom design solution which is very efficient, specifically because of the fungibility of assets. And if you don't have that, this falls apart. And that's where I go back to where you are.
08:28:45.810 - 08:29:22.572, Speaker B: Piece of cake. One of the downsides here is that let's say optimism, or let's say Ave wanted to liquidate the position and access the liquidity. This doesn't work. Doesn't work. And so this is where the application specific sequencing comes in and you break the network effects. And so I guess one way to phrase the question is, what are the applications that are silos that don't need composability? And I think one of them could be gaming and another one could be swapping. So I think this also goes to the market structure of those roll ups here, too.
08:29:22.572 - 08:29:53.892, Speaker B: So I'm not a big gamer. I don't know a lot. But to me, like an onchain game is going to run on its own l two, and it doesn't really need to talk to other l two s. Very often, at most, you just need to bring assets to pay gas on chain or something like that, too. Another interesting example is all like these perp dexes, they're kind of like casinos, right? Your money goes in and then they don't really want to come out. They're not really composable. Most of these perp dexes, it's true, right? And their whole business model is like, get deposit.
08:29:53.892 - 08:30:37.088, Speaker B: Well, it's in the name. Here you go. Get perpetual deposits. The applications that actually require composability today, I think, are relatively few and far between. The ave crosschain lending, I think, is the best one. Okay, here's a question for you guys. Is it that we have these application specific small applications because we haven't reached enough maturity to build complex applications, and once we have multiple building blocks and we can compose, like, money Legos, then finally we'll have this rich and compelling application? Yes, now you're speaking my language.
08:30:37.088 - 08:31:32.950, Speaker B: I mean, that's a vision that we've all been working towards, right? Like the fact that everyone's just speculating on top of crypto today, and that's like 95% of the usage is not the dream world I think everyone in this room sort of dreamed about when they joined into the space. At least it wasn't what I joined in for. Right? When I started in 2017, it was like, wow, everyone's thinking about dows and putting Uber on the blockchain and whatever else. IcO days were fun, but now the dream of web three is kind of like what's happening. People aren't actually developing these applications and having them reach meaningful scale, I think I have a question for the panelists, which is actually, I wonder, how many application specific chains do you think there will be? My thesis is that there will be as many as there are smart contracts on Ethereum today. And we'll see people build all kinds of new applications using this magic infinite block space that l two s have created. But I wonder if you guys disagree with that.
08:31:32.950 - 08:32:16.004, Speaker B: I disagree. I agree with you that there's going to be lots of chains. But where I disagree is that a lot of them will share sequencer, and so in some sense they will act like one chain. Will that mean that they are constrained in block space by that one sequencer that they share? Or how does sharing constrain them? Sharing gives them composability. It doesn't constrain them in terms of scalability because you have PBS. So I think there needs to be a very good reason for an application to opt out of a shared sequencer. And even if there is some opting out at the game level, for example, or at the swapping level, I think there will be shared sequencing there.
08:32:16.004 - 08:33:13.124, Speaker B: So it's possible that there will be a gaming sequencer for all the games and then a defi sequencer for all the swapping or whatnot. But Justin, do you think there is some cost to shared sequencing? Right? What is mean today? There is some cost. We don't have the tech to do it. And I'm saying maybe that cost gets less and less and less, but there is some marginal cost to shared sequencing, right? Okay, so one of them is that you can't do MeV protection easily because centralized sequencing does it. Another one is pre confirmations are trivial with shared sequencing, and then another one is the whole security training wheels. But once we've solved these problems, which are all solvable, then I don't think there's any downside to opting into a shared sequence. So I guess this is the question though, because there's a path dependency here, where if there's a cost today that is not insignificant, and I believe you, that that cost is going to go down and probably become insignificant at a future state.
08:33:13.124 - 08:34:31.544, Speaker B: But if there's a cost today and there isn't a lot of benefit, like again, if you have different gaming l two s that don't really need to talk to each other, they want to offer the cheapest user experience today, so they're going to develop their own things, and then other band aid solutions emerge. And then you have a migration problem in the future if and when shared sequencing becomes cheap enough. One observation I have is that I feel like in the beginning of the last cycle, everyone was really excited about composability, and we had a lot of really interesting and novel applications. I remember people used to wrap dye with a bunch of stuff and we thought we would pay with wrapped dye instead of die itself would be the token. And it seems like a lot, the trend has actually been in the opposite direction, where there used to be more applications that were built on the notion of composability, and now there are fewer. Actually, every one of the big D five protocols is increasingly trying to own more and more pieces of the stack themselves and internalize it themselves, as opposed to this vision of a bunch of different composable protocols. And so I think if you take the last four years empirically, we've seen the composability thesis play out in the opposite direction, where we're having more silos as opposed to more composable protocols.
08:34:31.544 - 08:35:19.970, Speaker B: I think this would lead us to think that it's not as big of a deal with sequencing as we think. Maybe you can pitch me the opposite side of the argument, Justin. I mean, empirically, just looking at market caps, there are network effects, just bitcoin, for example, all composable within itself, and it has half the market. What does it mean for bitcoin to be composable within itself? Well, it's kind of abstractly speaking, like composability at different layers, not just at the sequencing layer. You share the L one, you share the L zero, you share the assets, you share the culture. And I think my personal thesis that we're going to see these strong network effects that are going to lead to winner take most in various aspects. And I think we are seeing that with bitcoin as a great example.
08:35:20.660 - 08:36:10.364, Speaker C: I want to go back to Steph's point about application lock in. I mean, it's hard to explain in this context, but in our architecture, applications are unbundled from particular chains. So the native units of state are called resources, and the resources are controlled by resource logics, which you can think about as predicates or smart contract code. Those resources internally can live on different instances of anoma. And so, as a user, you can specify you want to use a particular application, and it doesn't necessarily matter which partition of state the application lives on. So this flips the concept on its head, where typically you think about security domain first and application second. But in our architecture, you think about application first and security domain second, which.
08:36:10.402 - 08:36:22.880, Speaker B: Actually ends up being more decentralized. I think we're almost up on time. Tina, let me know how we're doing.
08:36:23.810 - 08:36:24.246, Speaker D: Yes.
08:36:24.308 - 08:36:25.560, Speaker B: These two got to run.
08:36:26.410 - 08:36:27.160, Speaker E: Yeah.
08:36:28.090 - 08:36:33.460, Speaker B: Okay, perfect. Shall we wrap it up here? Yeah, sure. Okay, perfect. That's a hard cut.
