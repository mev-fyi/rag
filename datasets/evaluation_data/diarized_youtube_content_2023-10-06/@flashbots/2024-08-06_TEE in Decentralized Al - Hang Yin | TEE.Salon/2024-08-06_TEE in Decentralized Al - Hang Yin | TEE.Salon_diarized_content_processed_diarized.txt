00:00:03.440 - 00:00:57.730, Speaker A: All right, so yeah, pretty intense step. So we have been talking about all the technical details, the infrastructure and the series. So I probably can just wrap up everything by a very like use case focusing lightning topic. So yeah, all those are multiple decentralized ait, but I will try to keep it as simple as possible. And I'm not going to focus too much about the vision or too much about the technical, but just a little bit to share my idea and I will also want to hear the feedback from everyone here. So yeah, by the way, I'm ham from Fala Network and I'm the technical guide CTO. And today we're going to talk about decentralized AI.
00:00:57.730 - 00:01:44.630, Speaker A: So maybe first thing first, why we would even need a decentralized API. That didn't mean we're going to produce the proof for every model output and verify that by the end user, at least for me as an end user, of changing test about the output, it works, it works pretty well. So I will not verify any signature from over there, but it actually works. It actually makes sense. And apparently like those are the things we have been talking about for a long time. So I'm not going to repeat it. AI is centralized now, so I'm just going to skip this.
00:01:44.630 - 00:02:54.046, Speaker A: And how is AI being centralized? So basically there is like a self impressive, first of all, most of the big large language models are created by some single companies because they own the data, right? And they build the private AI models. And since the private AI models are using the best dataset they have trained, so they have the best performance, people are going to use the best model, right? So that's why they have made a huge amount of money. And to make it worse, when people are using their model, they also generate the data for the centralized company. So this is like, if we have no way to break it, then yeah, we can foresee. No, like, no, no, like AI will be decentralized. But is it possible to break this? I do think there are some insurances. If we can have, say, better decentralized AI, then probably we can get users striving to use that.
00:02:54.046 - 00:04:06.420, Speaker A: And this actually makes a little bit sense because apparently I think the quality of the model is, and I think like, although I'm not a part of opening, but I believe they have already exhausted all their training data because they have scrapped all the data they can from all over the world. That's why they have that GPT four, GPT 40, but never GPT five, right? So if say we can somehow use like a better model, then probably like if like we can build a better model, the majority of them are going to use, this model is going to generate revenue. And once we get revenue from the desktop model, then we actually incentivize people to provide their data for training. Say like open air has all the public data, but even more data, your data. My data is not a part of OpenAI's data. If we combine all those things together, theoretically we can have an even larger training dataset. So that's why if we come out all user own data, it will be way more than any company.
00:04:06.420 - 00:05:14.164, Speaker A: Now we have the better decentralized AI and we have the user switch to the decentralized AI instead of the centralized AI. So this is the basic assumptions of the strategy. So in this case we can have like individuals, companies contribute their data, maybe one person, 2%, just random number, and we mix them together as a big training dataset. And then we can train the AI models and we can use the use model by the buyers and they get the revenue and we share the revenue, all the banks from all the contributors. But in this process, the interesting thing is what are we using? Probably we already have some kind of like a data protocol. A bunch of them are working on getting data from individual users, but it's not useful right now because we do need two things to make everything work. One is verifiability and another is confidentiality of all the computation here.
00:05:14.164 - 00:06:16.844, Speaker A: So basically this diagram of the AI model and during this collection, everything must be very. Because why? Because we want to make sure as long as you contribute, you're always going to get the right amount of the reward from the final model already. So it should be verified. Both building the training set and training the model and serving on the other side of the problem is even like harder than something ZKML can solve. Not talking about the performance overhead, but ZKML can also. Confidentiality problem here we are going to mix the data set from different individuals together and then we're going to train the model with their data. So if we're going to do some kind of just a verification, I'm not going to share my user data and you're not going to share it with me.
00:06:16.844 - 00:06:55.002, Speaker A: How can we find somewhere we both trust to train the layout? This is where, this is why this has become even more complex. We need confidentiality. So yeah, combine those like a requirement together. This is basically some kind of computation possible is very. Yeah, so yeah, so far it's all like the basic idea. And of course this is super, super hard. I'm not saying we can actually make this happen.
00:06:55.002 - 00:07:36.250, Speaker A: Maybe within a few like months or even this year. There's no like, there's no existing technology between a super, super large model with database and confidentiality and you know, there's no single. It was both like data collection, the training, serving all those cards. So yeah, I think this will be like a long term effort. But we do see the hope. The hope is now we have GPU T. So yeah, I believe so far the only gpu we see in H 100.
00:07:36.250 - 00:08:32.950, Speaker A: It was just a launch last year and maybe next year. But how does it work is basically as a user you can send arbitrary input to H 100 TE and within the TEV is basically some environment where you can run any program you can deploy with normal H one. You don't need to check any codes for example. And you run model in the output with some kind of proof, the localization or some other kind of attestation. Then you can verify that by the end user or the. And to break it down a little bit, actually the h 100 t, it doesn't work just standard. It must be work with tv.
00:08:32.950 - 00:09:16.430, Speaker A: So how does it work with CBAC is on the one side we have, on the other side we have usually it's Intel Gex or AMDs we deploy. So basically it's not like intel Sgx. It is computational virtual machine. And in the computer virtual machine you have a whole operating system. You have a linear driver. So the driver is even running t. The driver will go to bootstrap the team member and there should be some kind of verification between the drive and CTE to make sure there's no like anything wrong in between.
00:09:16.430 - 00:10:18.160, Speaker A: Then we load the program into the CPU tv, just like a simple Linux program. It will interact with radio driver and interact with TPU GPU. And the driver itself actually creates a kind of two way end to end in channel between your program and the GPU. So that's how everything combines together. How does the attestation work? So on the GPU side Nvidia has the GPU attestation and it's going to measure like the GPU firmware, the hardware and also the programming. On the CTO side you are already, already familiar with that. We can have the TDX or ST attestation and then intel endorse this one, intel endorse this one.
00:10:18.160 - 00:11:05.822, Speaker A: Nvidia endorses this one. And the CPU attestation has the reference to the GPU attestation. So they have the checkout trust from intel Nvidia to the programming on the inside now. So yeah, this is basically how that works. But of course there are lots of different features. Like now there's only like it only supports one single server that you cannot have thousands of GPU's connected together. To trim some really interesting stuff, I think it's already possible to train something like fine tune lemma three in a single GPU or cluster of eight gpu's together on this machine.
00:11:05.822 - 00:12:15.514, Speaker A: And of course, and how about the overhead? So unlike the DKML or any other photography based solution. So this is a simple diagram that I got from some other weight. Left side is, right side is so if three to 5%. So the left one is for inference, the right one is for trace. The bottomnet of enabled Te GPU is actually the bandage between the CPU. So when doing the training then there are a lot of data exchange between the CPU and GPU. That's why if you enable overhead of around 5%, but in heavily depends on the real model and how the structure is.
00:12:15.514 - 00:13:12.438, Speaker A: The model is. So yeah, that's basically the background and there are some challenges, but I think before getting into the challenges I want to show you a simple background. So here. Yeah, actually this is not the background, it's more like. So we make a very. So here we actually create some kind of Nvidia remote execution from the hybrid and we can copy it and then verify it with the median strand. So they have the hands the we just paste it here and get verification for now.
00:13:12.438 - 00:14:31.278, Speaker A: So this is how it, and then we run lava street. So I can just ask some simple questions. And here we actually make it very easy to be verified by having the PDF. So we find the output very simple. And then we also have these. So now we already have how it will look. So basically we provide semi decentralized, because we don't have to do a decentralized network, still just a few, but we can make sure it's already protected by the tee.
00:14:31.278 - 00:15:34.040, Speaker A: So it's kind of, kind of okay to trust it to someone. So here what we offer is step of API. So you can request the extension report from the server and it will give you the signing. But in this TDX, and yeah you can verify that with media. And we have the chat API pretty similar to those like APIs call that to chat. And eventually we're going to have some signature response. So HTTP response response so that you can use the response, the signature response to verify the contents in the response of the HTML.
00:15:34.040 - 00:16:11.144, Speaker A: In the demo you showed you only verify the signature of the output, how do you bind the output to the actual. So in that, that demo is actually very, very simple. Just show this is something you can do. But of course here what we're going to do is in the signature it will commit to all the input and output. And in the input it will look like. Let me check. Okay.
00:16:11.144 - 00:17:10.020, Speaker A: It will look like something like this. You can see there will be the model here. And I think eventually we're going to allow people to load, stack and actually model available on hackingbase. So you have the hacking Face ID here and this is the input. Then the outputs, all of them will be combined. Is there anything you're getting good application for? Is this fairly easy to work with? Well I think eventually it should be pretty because once we have everything set up, already set up, then what we need to do is just to create like an image. And in the image we can include our software to load the model and pretty similar switch with the development outside of the tv.
00:17:10.020 - 00:17:46.990, Speaker A: But the problem is there are two things. One is how can we get the brand expensive? Well it's super expensive, but to make it worse, like in my understanding, but I haven't tried, in my understanding, if you buy something it has the capability of the TE, but you need to enable that with the approval. So like you're going to make a full computer. Is it a subscription or is it like. Well it's a free subscription. So basically they are going to ask why are you going to use it? And you should be. Right answer.
00:17:46.990 - 00:17:51.410, Speaker A: They're going to.
