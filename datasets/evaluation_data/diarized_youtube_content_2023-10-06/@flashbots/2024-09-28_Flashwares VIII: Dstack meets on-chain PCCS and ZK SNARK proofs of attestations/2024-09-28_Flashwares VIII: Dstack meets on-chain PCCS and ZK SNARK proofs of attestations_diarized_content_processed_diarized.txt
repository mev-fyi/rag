00:00:07.160 - 00:00:07.820, Speaker A: Okay.
00:00:10.305 - 00:00:31.135, Speaker B: Amazing. So hello everyone to the 8th edition of Flashworks. This time we have Andrew Miller presenting his version of the destack. And we have. We are joined by people from Automata by Daigong and Cheng Long Chapter and yeah, this is going to be super fun. Andrew, take it away.
00:00:33.075 - 00:01:16.395, Speaker A: I'll show you my Destack demo. I had a lot of fun with it, as you will see. I think it comes off as hard to explain, so I will appreciate people jumping in for questions. I'll go smoother reacting to derailing questions than I'm trying to pick the order. So this is meant to be a provocative prototype on how to make blockchain native confidential virtual machines. So it's meant to be a kind of parallel track to show some ideas, especially of how to keep things aggressively simple in terms of interface. With the idea that I left off for this prototype, all of the hard parts, including actually running it on tdx, we use mocked dummy attestations.
00:01:16.395 - 00:02:18.688, Speaker A: Maybe you'll see when we get to that. But the plan is for this to, you know, be suggestions that can be integrated with all the good work of reproducible builds like with yocto and for full systems too, I'd say. One of the main things that I've been inspired with is I think just how simple the Grammy SGX interfaces are. Especially really the only thing you need to ask the host for from writing an enclave is give me access to a ceiling key or in our network it'll be a replicated key and give me a subdivided attestation by a tag just for this portion of my app. And Grammy provided a really cool interface just with dev attestations. So for CVMs I wanted to have something where your apps are containers and your containers can just ask for things like this from the host, which is still part of the TD Trust domain, part of the confidential virtual machine. The idea is a nice environment for apps that run as containers inside a confidential virtual machine and then as simple as possible.
00:02:18.688 - 00:03:00.491, Speaker A: So this is a speed run style prototype. So the goal is to be fully self contained and explainable. So the code here is in total just a bunch of bash scripts and a few python scripts. I think all together the lines of code. The site's comments is under 1000, so it's meant to be able to read through and follow and decide what you think about each bit of it and then the cool features that are the components to go over. So this handles full automation of joining a new node to the network and getting a copy of the shared key so this is something that Keaton did. Anything based on that does and the Sura demo did as well.
00:03:00.491 - 00:03:27.295, Speaker A: But here the automation works pretty smoothly and as I'm more lightweight. So that's to show off. It has this cool feature where you deploy containers for your applications. So hopefully this demo works. I push a container hash onto Sepolia Smart Contract and that's how I redeploy an app. And the apps get to serve their own TLS server connection. So it's like a peer to peer network.
00:03:27.295 - 00:04:15.219, Speaker A: Any node with enclave can join and when the node joins it also has the same copy of the private key. So it's able to serve the same HTTPs websites. And that really is most of the features actually, especially for automated Yes, I want to point out the what we do with this off chain decap because it's an alternative approach to the ZK compression trick of making on chain coordination of replicated CDMs cheap. Basically we only do attestation checking in two places. One is at audit time so it doesn't have to be done on chain and the other is from an existing node before it helps a new node onboard. Both of these don't have to be done on chain, there could still be benefits to doing them on chain. And I completely skipped all of the things having to do with PCCs and that definitely needs to be on chain.
00:04:15.219 - 00:05:00.855, Speaker A: So talking about how on chain PCCS fits in is probably the most fun thing at the end of this. And everything that I'll show you is I'm just going to run it on my laptop because it's a virtual machine plan that is built to run on TDX and it uses real TDX attestations just they come from a dummy service. But for the sake of this network and development environment, I can just run the demo from my desktop. So I did mention all of the components, but they're out here like this. Is there anything I wanted to call out here before more interesting part? No, probably not. Probably the most interesting thing now is just to do the demo of watching it start up. Actually I can even do the demo of building the image.
00:05:00.855 - 00:05:42.355, Speaker A: Normally building a whole image, especially with yocto doing the complete thing, maybe doing it the right way takes kind of a long time for building. But for this little prototype kept it pretty small. The install script is small in a bit because I broke it up into two bits to cache. So the starting point is just the Ubuntu cloud minimal image. That's like a 250 megabyte download expanded about 1 gigabyte, install all of the dependencies in one step so I can cache it and all of my stuff I've defined afterwards. So that's why it goes faster to just edit the stuff I've defined afterwards. Like startup scripts and replicating script, all of that.
00:05:42.355 - 00:06:08.755, Speaker A: Oh yeah. One other call out here is I don't quite have a real light client in here. Obviously putting like Helios and Beacon Note or something in there would be appropriate to do next. But I'm just using Cast as a cheating light client. But that's where that fits in. Okay, so building didn't take too long. This is kicking off trying to join the network to a node that's already here.
00:06:08.755 - 00:06:41.989, Speaker A: I have these quad panels up. I mean, this is clearly the virtual machine that is being loaded. That's supposed to be like the tv. The top left is untrusted host. So I have all of the sending transactions done from outside the enclave. I mean, in general, that's I guess, a thing I try to encourage in different ways, but moving complexity out of the enclave into the untrusted world where it's easy to change. So here I'm actually sending coins from totally just broadcast private key.
00:06:41.989 - 00:07:11.915, Speaker A: So be too quick to see it on Sepolia. All right, so a register transaction. So all of the nodes to join the network you have to post on Sepolia. So Sepolia is the coordination chain. So a register message and an onboard message. This is just tailing the log file from the virtual machine that's running. So because I didn't, I can answer some questions just by logging into this.
00:07:11.915 - 00:07:53.789, Speaker A: Obviously in a production, when you don't allow a root shell into your confidential virtual machine, but in this debug environment, that's what's there. I can go into this protocol more. I think a couple of things that I will point out, because they are sort of cool, are how I mount an encrypted disk. So I mean, mainly all that's defined here is a startup script and then those four little modules. It's like the startup script. First it mounts the untrusted host volume. So this is how you can just pass, you know, untrusted data from there.
00:07:53.789 - 00:08:55.687, Speaker A: So this is where I put things like API keys and certificate signing requests. Certificate non private stuff goes in there and then in here I also have an encrypted container and then that encrypted container later on when I get a shared key, we will actually mount the encrypted container from that image using this shared key. So the first interesting script that did everything that we just saw here is the replicator script who like you run it the first time it bootstraps a key and puts it onto the smart contract. There's like a bootstrap. I have to go down pretty far. But somewhere in here is a bootstrap command, right? So that's obviously setting the public key that's going to be associated with this contract and serve joins and gets a replica of that private key. The protocol then is pretty similar to what Sura did, because you register on chain and you don't, I guess.
00:08:55.687 - 00:09:39.265, Speaker A: So one thing that a capture allows me to move this off chain is I have just some. I think this should be replaced with IPFs or some gossip of some kind. But I don't actually pass the entire quote on chain. I just say that the untrusted host is responsible for gossiping the quotes as needed, but they won't proceed with onboarding you until. So you still have to do an on chain component, but the quotes themselves are passed around off chain. And maybe one interesting thing to see from this is we don't put the quote on chain, but in the onboard phase, the onboard phase consists of a ciphertext. So that's the encryption of the shared private key to the new node.
00:09:39.265 - 00:10:49.665, Speaker A: There's the mrtd. In this case, that's the MRTD of the dummy enclave service and the fmspc. So the CPU version of the new node that's just registered and the signature is basically the EVM friendly attestation, because this is the signature using the shared private key on the onboarding message that helps the new node get a copy of that key. So you know, you get this instead at which the chain is easy to check, instead of just an ECDSA signature, instead of a quote, which would be more expensive for the on chain environment to check. And this makes sense just if you take the invariant that only the valid enclave nodes ever have the copy of the shared private key and then you know that invariant's maintained because they only shared a private key with other nodes that have passed for remote attestation checking anyway. So you get the private key, there's an encrypted data volume as well. And you can then use this as it's a little bit like a sealed file because you could pass that encrypted container around and any node that gets a copy of the shared key, so that's all of them can open up and load that encrypted container.
00:10:49.665 - 00:11:43.697, Speaker A: So it's not even exactly a Replacement for sealed files because it's actually, well, they're portable encrypted volumes because they aren't locked to a particular cpu. Where else do I describe next? Let me just control the container, I think that goes next. Oh, and I will be able to show the TLS trick thing too, I think. So I have the greeting app being served from some website on a server node that I have. This is just an example app, but it's a container. Actually this is more complicated because I tried to use this reproducible Docker build thing that couldn't get it to work. At least it didn't.
00:11:43.697 - 00:12:17.045, Speaker A: I added a bunch of stuff, but it didn't make my Docker builds any more reproducible than they were without it. So the app is really just this like a little static flask page. And the way that the app is launched, I'm calling Kubernetes. It makes Matush grin when I say that. And it's because of my ETH contract. And it has a container tag and the container tag has a whole container name. Where to get it from Docker Hub and more importantly the app.
00:12:17.045 - 00:12:57.765, Speaker A: So you know that you're getting the right thing. And so this actually tries to read from from a tarball containing Docker images. So even if this is offline, it will just try to read whatever you pass in from the host and if it matches the container hash that it's expecting on chain, it will go ahead and deploy it and if not, it will try its best to fetch it. So you don't have to be online. But it can also just pull from Docker Hub kind of automatically. What else is going to be cool there to see why this TLS works. So this has a TLS certificate.
00:12:57.765 - 00:13:57.969, Speaker A: It's fun to see the public key like 5e a5. So that's clearly just ordinary certificate authority sign like let's encrypt signed TLS key. But the way that that actually works is that the look at the Kubernetes script. Wait, wrong script. I think wanted to look at unstoppable TLS script. So what this does is it gets a derived key from that shared secret xpriv and so the certificate private key is just derived from the shared secret. So all of the nodes that run this will get if they've joined first they joined the network and get a copy of the shared secret.
00:13:57.969 - 00:14:52.987, Speaker A: Then they derive the exact same private key for TLS certificates. They all generate an output to the untrusted host volume, a certificate signing request that's only really useful. So the person who owns the IP address and can pass the let's encrypt check. So everyone derives the same private key, outputs the certificate signing request just in case and then as soon as a certificate is available on the host volume, it goes ahead and writes it in the RAM disk for later. And then I get a reverse proxy out of it just by using nginx. I guess the NGINX configuration, you know, shows that it's reading the private key from the encrypted volume, the certificate from the untrusted volume. And so the TLS connection is handled by the NGINX using the shared private key that's only shared by the enclave nodes.
00:14:52.987 - 00:15:29.473, Speaker A: And so the guest app can be really simple, I guess. No, I never showed the guest app example Python. I said it was simple flask. But yeah, so it just thinks that it's taking raw HTTP because the TLS is handled at a separate layer. This is showing that it can access the guest service just because the, you know, this is using Podman as the container driver. So Podman is mapping this host name to particular thing on the host. So this gets you an app specific derived key and you can get the app specific EVM friendly attestation.
00:15:29.473 - 00:16:11.239, Speaker A: So this is just signing a fixed Message welcome to DStack. But that's what's being displayed here. And so I should be able to use this with the read contract to verify. So if that was the digest of the message I'm signing and that's the signature that should check out just how this is what I want to get to. Those are like Grammy level simplicity of interface features and that should be all that an app needs to ask from the, from the TDX specific stuff. Okay. And then I think the last thing to show off for this is that I can, you know, I just joined my node to the network so it can, I can communicate that as well.
00:16:11.239 - 00:17:07.215, Speaker A: And because it really is using the same TLS certificate, if I just overwrite my hosts file or you know, did this at some other level at the OS or my router could even do this, then I can, yeah, I can actually serve the same thing. And even though the domain name is the same, I'm actually loading this just from my local host. So this would even work if it was air gapped? No, no actual network communication is needed, but it still passes the TLS check. And then I think the only other thing that could be fun to show off, but it won't take the full attention. So maybe that's a good Time to pause for questions. But I can go through the flow of pushing an app to. Yeah, pushing the app to Sepolia and then watching it redeploy automatically.
00:17:11.355 - 00:17:12.819, Speaker B: All right, Amazing.
00:17:12.947 - 00:17:16.255, Speaker A: Yeah, take questions while I just fuss with the template.
00:17:17.915 - 00:17:23.095, Speaker B: I have some questions, but Daddy and Shengland, do you have any questions?
00:17:23.955 - 00:17:50.325, Speaker C: Yes. Yeah, I think it's a great demo actually. Especially the part that interacts with some of the Kubernetes component. I mean, because that's a way to really bring all these enclaves into production. Because right now I guess most of the modern like web applications are running like in Kubernetes clusters. I think one other thing that's actually quite interesting is how the. How the web traffic is being served.
00:17:50.325 - 00:18:39.129, Speaker C: I mean, well, nginx, because this is also something we faced before, like we run some production workload we hope that we could have. I mean, in Kubernetes the model is you have some gateway like nginx or other kind of gateway to forward to proc the traffic and basically distribute or route it to the corresponding pod. I saw that here. Andrew used actually a shared TLS secret which is quite. Yeah, which is quite good because I think I remember when we used. I mean we have to use kind of a vanilla ngx, then really just allow it to pass through the TS connection to the eventual pod. Then we're actually terminating the TLS connections directly from each of the pods with the users because we don't know.
00:18:39.129 - 00:19:01.265, Speaker C: We haven't really figured out a better way to do it. Clearly it's not that secure in terms of the like DDoS defense because we're kind of exposing each of the backend service to public. But we don't really have good solutions. But the one that you just showed is. Could be one. I mean if there's a shared key that can be just used by the nginx service.
00:19:04.405 - 00:19:53.483, Speaker A: Yeah, it seems like there's a bunch of different. I mean one thing that I started thinking about a lot is that there's a very big difference in separation of roles between everything that's handling like IP flow routing by IP address. And I think most of the DDoS kind of things probably fit in there. And then the step that's actually ending the TLS connection, which I think just should be done at kind of a separate part. So I mean, one of the things that in this case it's just the NGINX that is running inside the td and I'm not providing anything clever outside the td. Like here I am just connecting directly to. I Mean I guess really the right thing to say that that encounters first is in the run script.
00:19:53.483 - 00:20:51.243, Speaker A: So I'm just using whatever I'm connected to 4002 because I'm just relying on my QMU's post forward thing there. So I guess I would say it still makes sense to do some level of routing and perhaps ddos prevention at the untrusted host outside of the enclave. And that doesn't necessarily need to constrain or guide how you do the TLS management. One of the questions that I've been uncertain about and tried it different ways, this is now the different way of trying it. But I like how it's turned out this way is that the like it would also make sense to have each app get, even if it's going to be derived from a shared secret, get its own derived private key to use for TLS however it wishes. I guess because it can get a key you could, you could still do that anyway. But it doesn't strike me that that it adds much.
00:20:51.243 - 00:21:50.745, Speaker A: I'm still trying to figure out what I think of this because like the pods running in the container driver they can treat their the routing that is imposed on them by the container driver as trusted. So I mean this is really what's meant to be shown off in that illustration of this where like this looks weird because I am getting a secure thing or even this one is I'm getting an attestation so other people will need to, you know, check its validity. But I'm getting it just by asking on HTTP and I'm asking just by a host name. So who knows who has remapped my host name. But the only thing that is processing network things for this container is the container driver at the top. So it has authority over how host names and routing are interpreted. So I think it's reasonable to say as long as that's implemented securely then you can treat this kind of local host routing as secure from the viewpoint of this app running in a container.
00:21:55.365 - 00:22:49.405, Speaker C: Right. I actually also have a follow up question on this because when we also like when we do the production services we're really thinking about how we can make use of Cloudflare because they have really great like DDoS defense and also like load balancing across the globe. But then they actually ask you for so you either hand over your TLS key or they're going to run a kind of a proxy that terminates the TLS between user and your services. Clearly if we choose this kind of middleman it will break this kind of guaranteeing But I'm thinking maybe for CDN services like this kind of Internet skill, maybe they could also run a T or similar like nginx and into the accident. So that could really help with the relaying the connection, you know, trust matter.
00:22:51.505 - 00:23:52.747, Speaker A: I mean Cloudflare, that is interesting. I mean Cloudflare is. To me it seems that if you allow the man in the middle to end your TLS connections the way that Cloudflare does, then I mean it's not enough to say Cloudflare would run in a te, because that's saying that then it just passes traffic to your service without them continuing that chain of authentication. I guess if, if Cloudflare were running in a TE and you were already providing a TE based application, maybe there's still a way that that seems unfair. Maybe it comes down to whether Cloudflare then offered to do essentially something like this with like a full chain of custody of verification of the certificates that they use to, to understand TLS connection themselves. So I mean it's, I would think that nothing less than, you know, full T buy in for that would make it work because you still want the whole chain of, you know, explaining where the certificates come from by T. Right, Right.
00:23:52.811 - 00:23:53.495, Speaker D: Yeah.
00:23:58.075 - 00:24:30.345, Speaker B: All right. I believe that Cloudfront does offer some, some non terminating proxies. So we can try to do some of this without giving up the TLS keypad. Yeah, the limited application, of course. Right. I have a question. So Andrew, you're saying early in the presentation that you are still moving out the complexity, right? You're moving the complexity out of the enclave and into the untrusted host, which I agree with.
00:24:30.345 - 00:24:51.185, Speaker B: But is it still as important in tdx? And why, why is it so important still compared to sgx? Because in SGX that was pretty obvious. Because it was very difficult to do those things in sgx. Yeah. How. Why would you still want to move the complexity outside of the enclave for tdx?
00:24:53.085 - 00:25:24.221, Speaker A: I mean, I have a vibe preference for this, but I think it's hard to articulate all that well. So I mean, I'll take another stab at it now. I don't think I've made a very convincing full explanation of why I think that's the case. You always have the choice. I mean there's even two heuristics. Like one viewpoint is that one good justification is put everything inside the enclave, whatever code you are going to run that's part of your application, it doesn't get less secure by putting it in the enclave. So why not just do that rather than try to subdivide up just the minimal component.
00:25:24.221 - 00:26:07.689, Speaker A: I think the strongest area of reason, maybe I can. It gets across, you know, better. We can even get further into how to assess it. But it's like audit cost. So I think that in a mature world where we are doing this, deploying enclave updates looks very similar to deploying smart contract updates where it's fairly accepted that, you know, every update to a smart contract could be rugging all the users of all of their defi. So you have to treat software updates really securely. So the top tier best practices involve mandatory notice periods like you upload your smart contract code and it doesn't kick over until a month elapses during which auditors are supposed to review it and review it for full coverage of everything that changes and so on.
00:26:07.689 - 00:26:41.375, Speaker A: So I think in a world where there are, there's a liquid market for security auditing of TE applications, then you will essentially get billed based on the lines of code in your enclave somehow. And somehow all the stuff that's outside the enclave doesn't have to be audited when you upgrade it because no one's watching what you upgrade outside the enclave anyway. Whereas any changes need to go through hopefully a rigorous audit process that includes full coverage of everything that changed. So yeah, that's best explanation I have.
00:26:44.595 - 00:26:47.779, Speaker B: Yeah, I think that's right. There's better, better boundaries.
00:26:47.827 - 00:26:48.035, Speaker C: Right.
00:26:48.075 - 00:27:08.905, Speaker B: Between the fully trusted code that manages your secrets and then everything else. Right. Amazing. I hope to talk more about the PCCs and attestations after the next presentation. Yeah, that's gonna be. That's gonna be interesting. All right.
00:27:11.365 - 00:27:11.701, Speaker D: Cool.
00:27:11.733 - 00:27:14.265, Speaker B: Andrew, is there anything else that you'd want to present?
00:27:15.485 - 00:27:17.225, Speaker A: No, not that comes to mind yet.
00:27:19.405 - 00:27:27.305, Speaker B: All right, so we should switch over to on chain PCCs and ZK snarks of attestations.
00:27:30.245 - 00:27:33.825, Speaker C: Okay, I'm sharing my screen now.
00:27:37.805 - 00:27:38.245, Speaker B: Okay.
00:27:38.285 - 00:27:40.625, Speaker C: Are we good? Can you guys see the screen?
00:27:42.325 - 00:27:43.185, Speaker B: Yep.
00:27:44.245 - 00:28:03.511, Speaker C: Okay. Right. So to start off I would just do a quick intro of myself. I'm Dele, the co founder of Automata Network. I think together on the call is general is also here we'll be presenting the our recent work also it's. It's fully open source so later on we can. We will also go through some of the reports.
00:28:03.511 - 00:29:01.645, Speaker C: So this is the kind of the latest solidity implementations of the DCAP libraries. It's kind of an equivalent of what intel has been doing in their V4 version for Intel SGX and TDX. But our contribution here is really like we're trying to bring this attestation on chain. Therefore, for enclaves that running independently somewhere, they don't have to rely on a lot of intermediary components, e.g. intel PCCs or even sometimes the similar service from cloud providers. Our assumption is if we can move a lot of components on chain, actually this is possible because we have cheaper and cheaper on chain infrastructure like rollups. So of course the same thing can be done even on layer one if the gas cost is acceptable, but normally super expensive.
00:29:01.645 - 00:30:08.115, Speaker C: So I would start with something that we've been thinking about along this line because we realized that not only tes a lot of things, especially hardware is now they have attestations and normally the attestation protocol is conducted off chain and so for people to verify it, normally you either directly verify the attestation or you have to trust some third parties. So in this case we're thinking maybe this third party should be just the word computer, which is for example Ethereum chain. So actually that's possible. So we started this kind of work I think almost one year ago and the initial result was kind of funny because it's super expensive. But we also learned a lot along the way. We tried multiple optimizations, we take some of the other people's optimization as well. So yeah, I think for those people who don't really know much about attestations, I'd like to also just briefly introduce it a little bit.
00:30:08.115 - 00:31:06.945, Speaker C: But I guess everyone here is aware of all the terms and the details. So intel actually had a couple of specs. Initially they did Intel EPID attestations, but later on, I mean currently they're actually deprecating it. So I think next year sometime it's going to become available so the entire service will be done. The reason the deprecated is I think they realize that a lot of the enclaves are run by data centers and data centers normally have very strict rules on security so they don't want to have the kind of the dependency on external resources. For example, attestation in apid really has a strong dependency on Intel API and even users have to get a key to access this kind of API and to verify the attestation. So it's becoming less ideal.
00:31:06.945 - 00:32:39.861, Speaker C: So that's why they're being pushed towards this kind of third party attestation primitives which essentially I think the reason they put the name data center here is it's also being cut off heavily, being adopted by cloud providers like aws, Azure or other service providers. So along this line we had v3 initially which just completed the attestation for Intel SGX but later on I think when we roll out TDX they also had a new version before which is different from V3, but it's good that they still support Intel SGX, it's just that the code format is slightly changed. Also in the latest documentation I think they mentioned about V5 but I don't see or at least we haven't really been using this latest spec yet. For Now I think V4 is probably most up to date specs. So I think also I'd like to also just a little bit touchdown on this decapit particles. If you look at the original design that intel proposed, it's really complex and a lot of components it does look scary. And that's also the reason why we're trying to reduce the complexity and just minimize interaction so that we just need to trust maybe the code that runs in the enclave and certain solicit code or some data that's being posted down chain because if everything's on chain it become publicly auditable.
00:32:39.861 - 00:33:41.385, Speaker C: Also it's full of transparency so we don't have to get into some kind of like argument about whether this is secure or not. So initially they have a couple components. The intel has a PCs service that actually shares or tells whether a certain certificate or certain quotes is including the legitimate collaterals. Also PCCS is actually considered a local cache that's running inside the data center. That's also why they bring this up because they want cloud providers to have their own internal service that they are happy with. So when their own instances coming up there's no really Internet access because everything is considered local within the platform. And of course the enclave component, which it could be Intel SGX or TDX like the trust domain, I mean it's kind of similar concept but from there I mean that's actually the target to be verified.
00:33:41.385 - 00:34:43.105, Speaker C: So the enclave will generate code and sign over by the attestation key. Then the report, the entire code and the attachment key will be verified ideally by through PCCs, through eventually through Intel PCs as well. But later we will see how we are able to do this without relying on these third parties. Also this is how big is the entire hierarchy right now that intel is relying on the SGX stuff is relying on. So they literally bring up a whole tree of certificates so that this new PKI can really serve the attestation better. Because in the past everything had to go through Intel's API. But in PCCs if you have enough certificate chain, if you have certain certificate pinned in your trust domain Then you can basically verify one step and just be sure that maybe this code is legitimate.
00:34:43.105 - 00:35:31.043, Speaker C: So here we comes to the on chain attestation part. So I think our initial implementation was really simple. It's a V4 replica in solidity. Apparently there might be a lot of design flaws, but I think it does one thing, it basically verify the quote. Although I think at that time we really have to also provide the collaterals as well. So everything has to be every raw data that you obtain from the enclave or the PCCS service has to be supplied to this contract so that they can be verified. But I think it's also open enough so that people can just take it and also just implement some of the custom logics.
00:35:31.043 - 00:36:41.329, Speaker C: So that because protocol developers knows better about their own protocol, so they know what kind of data is expected, what kind of format is expected as well. That also means each of the attestation results will be kind of obsolete because they are not, they are not under the same format. Very often that each of the quotes might have different format or even the result is not compatible at API level because it might have certain different encodings or really just presenting in a different format. So that's why we are thinking about how we can kind of unify everything together. And along the line we also figured, okay, maybe it's just not only doing the quote fully on chain because on chain is expensive. So we're also thinking about how to do this off chain, but with zero knowledge proofs. Because I think ZK has been used across entire ecosystem for scaling purpose, for scaling the roll ups.
00:36:41.329 - 00:37:19.509, Speaker C: But I think similarly we can just use ZK to scale up the transition as well. So the actual verification logic of the quote could be down in a ZK circuit. Then we just need to take the output of the proof of the circuit and put it on chain. So the verifier could just go ahead and verify a very lightweight proof. In a sense that this is also scaling because less work is being done on chain and the end result is verified very cheap manner. So this is actual PCCs in a very simplified format. You see that the remote party.
00:37:19.509 - 00:37:44.495, Speaker C: So look at the bottom. We have the machines, we have the remote party that want to verify the machine. So initially the remote party issue a challenge. SGX machine takes a challenge. Start generating code. While generating a code, it actually has to fetch some collaterals, collaterals from the PCs service. After it gets everything ready, it will generate the code and make sure the signature is done also.
00:37:44.495 - 00:38:54.217, Speaker C: Then the entire quote along with the signature is passed back to the remote party verifications. So while the remote party doing the verifications it again needs to take the collaterals from PCCs because in both cases PCCS is considered a shared shared resources. And it's also true because most of the data there is static. So once certain trust certificate is established it normally would stay there for a while unless it's being revoked. It's just like any other PKI services. There will be issuance of the certificate, there will also be revocation of the certificates. So if we combine, if we really want to move the PCCS on chain a simple format will be just okay, let's have some collaterals really stored on chain and then verify could be just kind of linked to that PCCS storage so that the now the interface will be for all the off chain quote will be really simple because they don't need to provide letters anymore because this is already included and updated to kind of latest date.
00:38:54.217 - 00:39:47.729, Speaker C: So of course there will be some like other maintenance work to be done to make sure that this is up to date. But for the user of the verifier or the remote party that needs the quote now it's really simplified because they can ask the SGX machine to just upload the code on chain to verify or it could also take some other enclave code and direct it by on chain. So this made possible because some of the on chain PCCs. So now this is kind of the new workflow I think I just briefly mentioned it's actually a lot simpler. I mean the workflow is streamlined. Also over here we just put more details so that you can really see how everything is going. So yeah, so also I like to mention that we actually just open sourced all the attestation related libraries.
00:39:47.729 - 00:40:33.029, Speaker C: So we have a couple reports. I will probably go over that also later. I think what's really interesting is we're offering two different routes for attestations. The first one also the most important one is full implementations of solidity based verifications because it doesn't rely on any other libraries. It's just like pure solidity library. But the downside is on layer one it'd be very expensive on some L2 if there's less optimizations there, for example some pre compile is not there then the entire attestation could be still very costly. The second route we're trying to present is actually the ZK route.
00:40:33.029 - 00:41:16.191, Speaker C: So that means you will rely on some like third party zkvm. So in our case it's actually risk zero. We work with them. We developed Our own guest program which is rust based for the verification logic. So again it's similar to what's being done on Chain but it's just moved into the circuit. So once the proof is generated normally like after two to five minutes depending on the like the whether the bonsai service from your serial is available or scalable enough. So once you have the result from the like the approver it will give you a receipt and other like related information to verify.
00:41:16.191 - 00:42:08.551, Speaker C: So you could just present that information on chain to verify the like the entire quote. So these are the couple repos I will just quickly go through it but later on let's move on to the GitHub page and take a closer look. So this is the Decapo. It's the latest version of the like. It's a Successor of the v3 repo so we already archived the old one in this new one it's able to do v3 and v4 both because all the code is moved here and supports both Intel SGX and Intel TDS as well. Next thing is this decaprs this is really a small tiny libraries that includes all the verification logics like for the quote. This is also the same code that being loaded into the as a guest program into the wiki.
00:42:08.551 - 00:43:04.275, Speaker C: VM also over here is quite interesting I think it's kind of the when you really start doing development you will feel the pain of Intel SGX we also shared the same consent pane so we developed this. We actually open sourced the same library that we've been using for SGX gave I think for TDX it doesn't really need such a huge SDK because the model there is simpler. You could just load your code and with certain shape layer that's enough. But for SGX it needs some well thought to deny and you really have to do it manually to make sure certain things are trusted, certain things are not trusted. This is all the ripples I like to move to the GitHub pages but before we do that maybe we could also have some discussion on the. On like whatever we shared just now.
00:43:06.255 - 00:44:00.045, Speaker D: Oh okay. So just. Just to kind of answer some of the questions that Andrew has. So I think Daly mentioned a lot about collaterals so the people might be wondering like what are the collaterals that we are talking about? Right. Thalee actually has a slide that contains the certificate kind of like information slide number five. So these are basically the collaterals. The collaterals include things like the root certificate that intel uses to issue all the different intermediate CAs and finally sign off on the actual, what's it called, the provisioning keys that are being embedded in the, in the, what's it called in the cpu.
00:44:00.045 - 00:45:44.395, Speaker D: One interesting thing is that part of the collaterals, sorry, part of the collaterals also include things like what is the current TCB info for like my coating and clay for my provisioning and clay. So you might be asking again, like, what is this TCB info? You can think of it as like in order for us to make sure that the software or the hardware that we are running, the bugs that we know are patched, we would kind of like maintain this versioning of the firmware that we are running, the enclave that we are running, stuff like that. And in the case where let's say researchers like Andrew find a bug in like Intel SGX or Intel tdx, then we would have to, intel would like patch it and then would kind of like bump up the version of the tcb. So TCB stands for the Trusted Computing Base. It's like the set of software or the environment that we trust. So part of this, what is the current tcb? If your TCB is of a certain version, what kind of status are you considered? Are you considered like up to date or are you considered like out of date? Is it revoked? Do you need like hardware hardening or software hardening? Stuff like that? So all this information is encoded as part of this TCB info and all this information can actually be updated.
00:45:55.125 - 00:45:57.745, Speaker C: There's a follow up question from Andrew.
00:46:03.085 - 00:46:45.749, Speaker A: Yeah, sorry, I guess I'm just kind of noodling on the terminology of these. But it always strikes me that there's like different, they serve different purposes like the TCP info. Maybe it's more obvious that it's for the verifier to look at because there's potential app customizing logic about what to do with the fields that are in the TCP info. Whereas the thing that really seems hardest to do anywhere except close to where the quotes are generated, so it tends to go near the prover, near the quote generator, is the intermediate certificates. But there's never anything for the client or for the verifier to do with information in the, in the intermediate certificates.
00:46:45.797 - 00:47:29.427, Speaker D: Yeah, so this is a little bit tricky because. So for example, the verifier will actually need the intermediate certificate and the root certificate. It depends on to what level the verifier wants to verify the certificate. So for example, the verifier can just say that I trust the Intel HX root CA certificate and then that way we don't have to verify that the intermediate certificate is legit. So we don't have guarantees that the intermediate certificate is legitimate. We just know that there is a chain that links the final, the leave certificate to the root certificate. But the path might actually be wrong.
00:47:29.427 - 00:48:59.165, Speaker D: That means it might, it might end up being an invalid path. So, so the hacker, an attacker, a malicious actor can actually say, let's say they are, they are able to kind of like trick, trick the intel root CSR to sign off an additional certificate that the user don't trust. So this way if we just rely on the intel root CS cert to actually verify the search in, this search in will actually be legitimate because the intermediate cert is signed off by the root CA cert. I guess I'm rambling a little bit, but I think the kind of like the idea is that we can actually do cert pinning to make sure that the entire search chain is actually correct at each step of the verification. This part is actually the interesting part. So in kind of like intel terminology, all of this is considered collaterals. But like what Andrew mentioned, for example the verifier and the verify the enclave and the verifier, they actually use different parts of this certificate.
00:48:59.165 - 00:50:02.521, Speaker D: For example, the enclave itself would actually require the lift certificate to be obtained from the PCCs. Now what is interesting is in the case where the enclave is not able to actually get the lift certificate, then the kind of like the signing procedure would actually differ. So the kind of like the, what's it called, the not provisioning key, the attestation key that is generated by the provisioning enclave would actually be generated based on the latest TCB that is recorded in, on the, on the chip itself. That means it, it. Okay, I, I think it's clearer the other way. If I explain it the other way. If there is a pcck, then the key will be generated based on the pcck.
00:50:02.521 - 00:51:51.347, Speaker D: If there's no pcck, then the enclave will actually generate a key based on the raw, kind of like raw information that is present on the platform itself. So this is the, this is the reason why currently at the, at the moment the non type 5 code verification you actually, you are, you're unable to actually verify it. There's no way anyone is actually able to verify this kind of codes other than intel themselves because of the way how the keys, the attestation keys are being generated. I think I went a little bit too deep in there. But because of this I guess we can have a discussion later. There's actually an interesting line of Work that we are pursuing where we would actually not rely on the intel root CA certificate to perform our remote attestation. So we would either use this intel root CA cert to perform a bootstrap or we can make use of the old web of trust model where I meet somebody and then we will have a CPU enrollment party or something like that where we'll enroll each other's CPU onto like this on chain database and then we can make use of a third party CA to actually issue, sorry issue PCK certificates on behalf of Intel.
00:51:51.347 - 00:52:00.141, Speaker D: So yeah, I guess we can discuss this later. Cool.
00:52:00.173 - 00:52:08.741, Speaker A: I want to interrupt the PCCs also because I think that you might. Were you going to talk more about the on chain PCCs later as an extension?
00:52:08.813 - 00:52:16.305, Speaker D: Yeah, DALIT is going to share some information about the TCB recovery and the on chain PCs.
00:52:18.405 - 00:52:18.789, Speaker A: Right.
00:52:18.837 - 00:53:11.365, Speaker C: I think if you can still see my screen I'll move on to other tabs. Just let me know if it's not showing up. So this is actually the first repo I'd like to share. It's where all the smart contract exists. So you can see that this is like almost 100% solidity code. So over here we have a couple of components but I guess the most important one is the decapitation part where it parses the raw code and just go through the entire data structure and trying to basically walk through the specific chain and verify each of the level. So we also have a PCC router over here so that it can when needed it can reach collaterals like those intermediate certificates from the actual storage which is on chain PCCs over here.
00:53:11.365 - 00:54:22.625, Speaker C: So also the code verifier, I think we actually included two versions, V3 and V4, so it will be just correspondingly also in this contract in this repo we have included both modes. So the direct attestation on chain. So some comparison here shows that it's instant but it cost almost like 4 million gas. There might be not much difference across different chains unless these EVMs can have pre compile that bring the cost down. But in that case we really have to also support that pre compiling our contract. Also if you take the snark proof route with V0 so the generation which happens offline takes 2 to 5 minutes but verification is also instant because the footprint on chain is much smaller. Eventually it only costs you like 300k gas but then you're kind of involving or introducing a dependency like the receiver services like proof generation as well as your code.
00:54:22.625 - 00:54:59.441, Speaker C: Of course it also involves the trust under Your entire infrastructure. They're designing like RISC VM as well. To make sure this is part of your trust boundary, we also included a couple really simple examples. I think the interface is actually unified, so you just need to pass like whatever you're getting from your local code. So if you have a quote you want to verify directly, you put it in this interface. If you have a zikified version, you put the journal and the zeal into the interface. So that's about this ripple.
00:54:59.441 - 00:55:41.427, Speaker C: Of course it's maintained using Foundry, the latest toolings, so I think that will help with some development as well. Of course we have the corresponding Qing version deployed on our own SNAT and also Ethereum policy testnet. I think we also received requests to deploy into maybe Sepholia and other chains, so we'll be doing that later on. Of course there will be a mainnet version which you can find on these addresses. So the benefit of this is it's production ready. Also we might cover the actual transition cost in the future. So just maybe in a couple of weeks we will have a ground program to just talk about this.
00:55:41.427 - 00:56:28.599, Speaker C: So people who really want to do attestations, they don't have to really worry about the cost. They could just go to our main net and do the verifications. So the next ripple we have over here is the onchain PCCs. So again this is mostly like solidity code but you could consider it's just a storage of the collaterals, basically the raw data that whatever we fetched from intel services. So that also means some maintenance work is needed in this ripple. We're thinking okay, maybe a DOS like structure could be there. So everyone who's a member of this could just go ahead and update it.
00:56:28.599 - 00:57:08.525, Speaker C: But for now, for ease of operation, where is the one that's updating it so it has multiple parts? I probably won't be going into the details. So actually it's just more like archive storage. If you have corresponding thing like enclave identities or some of the FMS PC or even the PC key stuff, it will be just stored correspondingly into each of the structure. Yeah, so this is just the storage part. It has a small tool, it's probably not. Yeah, I've talked about the next one. So we also have this QPL library.
00:57:08.525 - 00:57:51.541, Speaker C: This is more like a helper library or the wrapper of whatever we have on chain and also includes some logic that help us to get the certificates and also Update the onchain PCCs. So that's why it's written in Rust. It's a command line program. So we can use this to kind of maintain the PCCs. Also similar Rust code can be used by your enclave code to actually fetch the PCCs for code generations. And over here we also noted that. So by default it will go to the on chain PCCs to read the collector.
00:57:51.541 - 00:58:36.155, Speaker C: But if there's a miss in cache for example certain certificates are newly provisioned by intel is not like really updated on chain. It will try to fall back to like the cloud providers PCCs. But if that's another miss, eventually we will just talk to intel to get the kind of the final like correct answer. But yeah, they really have to have that. And also that means you really have to like get the get the Intel API key in order to talk to the API service. So this is a QPI library. Over here is an example that kind of a hacky example that works with the official like over here it's actually the repo that we just created.
00:58:36.155 - 00:59:21.361, Speaker C: We have a scaffold ripple that's really also just in grass. But the previous version we open source also includes a example for like the official Intel C example like code generation verification example. So you could literally take their C code, compile it together with this like dynamic library. Because like the end result of this compilation is actually a dynamic library. So you could use that example with this library and it still works like to complete the transitions. So the QPL tour here is actually very useful for updating the collaterals. I think just a couple of days ago intel actually released.
00:59:21.361 - 01:00:04.251, Speaker C: Sorry, intel really had triggered this TCP recovery and the updates were kind of issued in the pccs. So we really have to go there and also update it. So we open source ripple actually before this date and later on we received this kind of notifications so we just decided to go ahead and update the information. So this is actually the updating examples here. So you can see that over here the decoded input. I think you roughly get the idea. So this is the actual payload that you'll be getting the TCP update information from Intel.
01:00:04.251 - 01:00:30.701, Speaker C: So. So this is another one just for the enclave identity. Yeah, so right. I think that's basically the like several key contract offers. We have other open source ripples, but maybe it's just smaller dependencies. I wouldn't be mentioning it here today. So that's.
01:00:30.701 - 01:00:38.725, Speaker C: Yeah, that would be everything. I think we can maybe just discuss a little bit on the how this thing can be working together.
01:00:38.805 - 01:01:11.879, Speaker A: I think. Can I just ask a little more just on that last thing because we also Saw the TCB info monitor and are thinking boy, it would be great if everyone's smart. Contract on every chain lit up at once when intel just quietly publishes one of these. So yeah, it looks like you already saw all of those there. I'm trying to think of what you know, I'd love to see a solidity event that's super clear. Like this PCB info on this SVN number for this FMS PC is now out of date. Something conspicuous or dashboard.
01:01:11.879 - 01:01:32.575, Speaker A: Like if you have the contract in there, that's pretty cool. I assume anyone can post to that contract. I mean, I guess I have detailed questions about like, you know, overwriting expiry times for example, versus just allowing anyone to upload a new one and if it, you know, downgrades an existing one but you can't replace stale ones, stuff like that.
01:01:33.395 - 01:02:04.085, Speaker D: So right now the current implementation is actually permissioned. There are checks involved when we are accepting an upset. So for example, the dates would have to be valid. So it has to be a valid date range. It has to be after the the previous. The date of the new update has to be later than the update that you are trying to replace. So it has to be a valid date.
01:02:04.085 - 01:03:10.771, Speaker D: It has to pass the signature test. That means your PCB info or your certs have to be has to be verified to the appropriate either intel root, intel root cert or the appropriate intermediate certs. Currently this is the case. I think what is even more interesting is that it allow us to actually blacklist certain platforms before intel actually performs the TCP recovery. We can actually do it on our end if everyone is using this on chain PCCs to obtain the collateral information and to make things easier. That's why we kind of wrote the QPL library. So what this QPL library does is that it would first and foremost use the on chain PCCs.
01:03:10.771 - 01:03:42.305, Speaker D: And in the case where it doesn't, it will actually fall back to the common PCCS servers that the cloud providers use. And then if it fails all of this, it will actually fall back to the Intel PCs. So to me it's actually a superset of the QPL that you will get either from intel or from the cloud providers. So I would say like, why not just use this qpl? Sorry.
01:03:45.125 - 01:03:52.581, Speaker B: Okay, so a couple of questions. Do you also have TDX TCBs already in those on chain TCS contracts?
01:03:52.773 - 01:04:35.595, Speaker D: Yes, there is. So right now for the on chain contract you can actually just go to the on chain contract and then you can copy and paste your code. Your TDX code Directly into the contract and then you can click verify and you would try to verify. You can go to the DECAP attestation and then the contract. Read contract is fine. I think so for read contract it doesn't get posted on chain. So you can the verify and attest on chain.
01:04:35.595 - 01:05:05.709, Speaker D: Yep. And then you can just directly paste your code there and then you can click query and then it will tell you there will be a structure for it. We have a sample code in the Automata DCAP repository you can go under. I think it's JS and then there's a small little. Yeah, and then. Sorry, there's some. Yes.
01:05:05.709 - 01:05:31.521, Speaker D: Sample hacks and then you can add a 0x correct. Yeah. So the output bytes is actually the code structure that is extracted from the. From the. Sorry. The output bytes is the report structure that's actually extracted from the code. Because the code has been verified already.
01:05:31.521 - 01:05:51.911, Speaker D: So we can use any. If it returns. Oh, it's false probably because this is an old code. So if it returns true, then you can just use the output byte as if it's verified. If it's false, then you wouldn't want to use the output.
01:05:52.033 - 01:05:58.935, Speaker C: Yeah, I think it might get affected by that TCP update. So the example code is invalid. Now.
01:06:00.635 - 01:06:47.181, Speaker B: I have a funny story to tell here and I think this is what's going to push us into this on chain PCCS immediately almost because we were using this bare metal server and we had our decapitation station set up there and this 19th of September upgrade to TCBS simply removed our server from the TCB info list. Because they didn't actually. They didn't add and didn't make it out of data or they just changed it. So now our service just doesn't exist anymore. It's not a byte DDS configuration. So if you try to use the collateral provided by the intel services, it's just not working anymore. So we had the.
01:06:47.373 - 01:07:00.077, Speaker A: They didn't upload. They didn't upload a new TCB info that says it's deprecated. They just did not include that in the one that their API returns. So if you just refresh. Wow, okay, I get it. I misunderstood that before. That's so interesting.
01:07:00.077 - 01:07:05.785, Speaker A: So you would just fall back to using the already loaded TCP info if you know nothing else has gotten on top of it.
01:07:07.005 - 01:07:31.005, Speaker B: So yeah, this functioning PCs I think is, you know, we need governance on top over this. Like it cannot be that we have no idea and suddenly all of our calls stop working and all of our apps we can't migrate out of our apps. Right. Even if there is some kind of vulnerability, we have to migrate somehow. And this simply your calls just don't work anymore.
01:07:31.945 - 01:07:32.233, Speaker A: So.
01:07:32.249 - 01:07:37.377, Speaker B: Yeah, on chain PCs. Yes, please. Absolutely.
01:07:37.441 - 01:07:45.665, Speaker C: Yeah, we'd love to see if you guys can do some testing because we need to collect more feedbacks whether this works or not works.
01:07:46.685 - 01:08:21.245, Speaker B: Yeah. I think Andrew, the simplest thing for us right now would be to use the QPL for verification that uses the on chain PCCs. I think that's quite simple to do. And yeah, I think just, Even just the PCs is already like a big improvement for us. And then. Yeah. Whether we want to do the on chain attestation verification is I think a separate question probably because especially with the zk, we are using so few of those attestations that we can probably afford to have them verified on chain.
01:08:21.245 - 01:08:49.595, Speaker B: And this just improves the. Yeah, just strictly improves the transparency. Right. Of the of events that are happening. So I think that it's still worth doing this on chain. All right, another question. Okay, so you are maintaining those contracts on three different chains now, right.
01:08:49.595 - 01:09:13.255, Speaker B: How do you think of going cross chain with this? Do you think that if there's a chain where we would want to have PCCs on, we would just deploy the contract there and upkeep? Or are you thinking of some kind of like a breach or some kind of proof coming from the other chain?
01:09:13.915 - 01:09:34.305, Speaker C: Yeah. So issue with the bridge is that you're introducing another trust assumption. Right. You have to trust their bridge network is secure. So that's why we now choose to just directly upload it on chain because that's actually removing more. You don't have to trust more trust parties. Right.
01:09:34.305 - 01:09:53.280, Speaker C: But if it's cheap enough or if it's reliable enough, I think it's also possible we could just maybe on new chain it could just choose to fetch certain certificate from our centralized chain. Maybe a PCCS chain that stores all the attestations.
01:09:53.472 - 01:09:53.968, Speaker D: Yep.
01:09:54.016 - 01:09:56.696, Speaker C: So like store all the certificates. Yeah, yeah.
01:09:56.720 - 01:10:02.792, Speaker D: So. So one of the things that we are pushing for really, that's why Daly mentioned about like the attestation layer.
01:10:02.848 - 01:10:03.040, Speaker A: Right.
01:10:03.072 - 01:10:35.499, Speaker D: The attestation chip is that that's what the our automata mainnet would be like used for. We hope it will be a place where we can kind of like collect all the different attestations. Collaterals. I'll call it collaterals. It can be certificates, it can be TCP info for the TE platforms that we are interested in. So currently it is actually for the intel family. Intel sgx, intel tdx.
01:10:35.499 - 01:11:25.875, Speaker D: We are looking to actually do something similar actually for the AMD SEV platform too. So because for AMD SV you actually need to contact the KDS. So AMD has a similar thing like Intel's PCCs, but AMD is actually running it themselves. So we want to kind of replicate this on our chain. So the idea is that you have one place where we can collaboratively, through a dao, manage it. It has all the latest information. The community can decide what kind of platforms should be blacklisted, even if like intel or AMD doesn't agree with it.
01:11:25.875 - 01:12:12.395, Speaker D: And like I kind of like shared a little bit earlier, there's one thing that we are actually exploring where we would actually kind of like totally deviate from Intel. So we will just use Intel's root of trust to bootstrap our own kind of like attestation framework. And then from that point on we can actually issue certificates or collaterals that is say EVM friendly, that uses for example the Koblitz curve instead of the prime curve. And that would actually make verification of the attestation report really cheap on chain. So those are like some ideas.
01:12:14.415 - 01:12:21.115, Speaker B: I see. So what exactly are you proving with the snark? Like what's the inputs?
01:12:23.655 - 01:13:32.631, Speaker D: So for the. So basically we make use of the RISC 0 VM, the ZKVM to kind of like write a GAS program that performs a verification. So the heavy part actually in terms of like generating the proof is actually on the cryptographic routines. Rezero did an optimization for the Kublitz curve, but because the DCAD actually uses the r curve, the P256 curve, it's really expensive. It takes about five to 10 minutes to actually get proof generated for it. So we actually kind of like we took the effort, so we wrote the actual Rust program to make sure that it works. And then we actually modified the P256 crate to actually make use of a mount mod gadget that res0 has.
01:13:32.631 - 01:14:35.971, Speaker D: So this would actually. How is it? This makes the routine actually more optimized for res 0 because instead of emulating the entire like Malmok operation, it actually uses a custom circuit that performs it. This actually reduces the cost. It improves the efficiency by about 10 times. So I think from 150 million cycles it reduced it to about 12 million cycles, which actually kept the time of of producing the Z the proof to be about like less than a minute currently on Bonsai. So that's that. And what you give the guest is basically the code plus the associated collaterals.
01:14:35.971 - 01:15:22.385, Speaker D: That you need to verify the code with the. Then what's that called? The guest program would actually use all the provided information, verify it, and produce a receipt. You can think of it as like output witness. So in the output witness you commit values like the certificate, what's it called? The root certificate that I'm using, the intermediate certificates that I'm using, hashes of those certificates, the hash of the TCB info that I'm using. So with all these like commitments, you can actually verify on chain that everything is correct and the results, you can actually trust them.
01:15:25.285 - 01:15:55.273, Speaker B: All right. You know, the thing that comes to mind, to my mind here is that you could use, you know, the automatas mainnet as the golden source of the collaterals and you could actually push to all the other chains just the ZK proof along with just the necessary, which is very little. Right. It's like that you need like 6 jsons or something like 5 certificates and 3 JSON files.
01:15:55.329 - 01:15:55.925, Speaker C: Right?
01:15:58.945 - 01:16:24.881, Speaker D: I can't remember exactly. I have to refer to the structure. But yes, it's basically right now what we are doing is we are including the entire raw code. But of course we can see how we can optimize it. We can either include a hash of the code if we are fine with the code being available off chain or stuff like that. So yes, it can be just hashes. The commitment can be just hashes.
01:16:25.073 - 01:16:27.737, Speaker B: Oh, interesting. But then.
01:16:27.801 - 01:17:13.735, Speaker D: Oh yeah, interesting because one of the design choices that we want to have is that you can have on chain, you can compose on chain contracts that would perform certain actions depending on the contents of the report that is included in the code. So for example, I can include some user data that has some instructions that I want people to operate in. Or yeah, a public key. Where the public key would be like one would be an enrollment of a key that is allowed to say submit certain transactions to a pool on chain.
01:17:17.035 - 01:17:36.219, Speaker B: So I think that. So okay, so this would be already super useful if even if you have another chain that doesn't have the full collaterals as long as you put the intel CA there and the hashes. Yeah, or the hashes. But then it's very difficult to verify. Right?
01:17:36.267 - 01:17:36.763, Speaker C: Like.
01:17:36.899 - 01:17:37.939, Speaker D: Yeah, exactly.
01:17:37.987 - 01:18:18.793, Speaker B: Because you don't have a source for those hashes. You could do this if you had a storage proof from the automata mainnet. Right. Like if you proved that you know that this certificate is coming from this contract from the storage and then that would be trusted. So you deploy what you deploy on this on those other chains is just a Simple contract that verifies that says that it's going to mirror this specific contract address on automata maintenance and then you can prove the storage and the storage would be just the collaterals. But that's just. But that becomes very complex very quickly.
01:18:18.793 - 01:18:52.495, Speaker B: But I think it would be very cheap to push like quotes and verify that stations on any chain basically at that point. Interesting, right? I have some, I have some questions less related to PCCs so I know. Andrew, do you have any questions on this?
01:18:53.595 - 01:18:53.907, Speaker D: No.
01:18:53.931 - 01:18:54.735, Speaker A: Go for it.
01:18:55.155 - 01:19:12.385, Speaker B: All right, so what do you think of Azure's other stations? So they use, they don't use dcap, right? They have their own maa like Microsoft Station, whatever. Do you think we can ever support something like this on chain?
01:19:14.205 - 01:20:18.445, Speaker D: So the tricky thing about Azure's environment is that this is purely my guess because I don't have anyone from Microsoft actually confirming this with me. But what they have is they actually have a paradiser for all their VMs. This actually allows them to do this whole like secure safe boot workflow because the paravisor is actually in charge of loading all your VM firmware information. So what it means is that if you. So for example, right now on Azure, if you actually generate a snapshot of your. Of your Azure instance and you replicate this snapshot, it would actually be exact no matter which user or which instance you run in. Like the UEFI will be exactly that same uefi.
01:20:18.445 - 01:21:28.685, Speaker D: The customization that you make it will be exactly the same. There'll be one single difference and that is tied to the instance information. So that part would differ during the boot process. Because of that they have this Steam EFI program within their whole boot process and I'm guessing like you guys are aware of it, the way how it works now is that they would include, they will include kind of like a opaque handle in their vtpm. You read this opaque handle and then you will ask their MAA to actually provide you with the actual physical code generated from the tee. This is the current way Microsoft is doing. Can they actually provide a direct way for us to call? I think so, because the thing is the paravisor can actually provide a pass through mechanism for us to actually query for the TE directly.
01:21:28.685 - 01:21:50.675, Speaker D: Whether Azure would do it or not is another question. I don't think they would do it in the near future. So if we are going to use Azure then we will have to trust this huge black box in the middle. Yeah.
01:21:52.535 - 01:22:23.445, Speaker B: Yeah, I think that's right. All right, so I have a question on your SGX framework, I guess. SDK you called it. How do you see, how do you see like the future with tdx? How do you see, what do you think we should be providing to TDX developers that we are not providing right now that you are providing to the SGX developers.
01:22:24.105 - 01:23:04.437, Speaker D: So we are actually working on both ends. The reason why we are open sourcing the SGX SDK is that there is a lot of people that is asking us like what is the framework that you guys that we are using to develop our. Our enclave, sorry our SJX services. So usually we will tell them that we are using some internal build system, but it's based on tclave so we can go take a look at tclave. And then we kept repeating that. So we were like, you know what, we'll just package it nicely and then release it. So anyone who wants to use it can just use it.
01:23:04.437 - 01:23:49.855, Speaker D: So you can point directly to it. What we are doing on the TDX and actually it's actually for both TDX and SEV is to come up with an agent that users can actually include in their image. The agent will be responsible for generating attestation reports and perform like, what's it called, boot measurements during the boot up of the image. So that is what we have in mind for like CVM type PE users. So. Yeah, I see.
01:23:50.795 - 01:23:56.385, Speaker B: Yeah, that makes sense. All right.
01:23:58.445 - 01:24:19.741, Speaker C: Do we have a couple minutes? I'd like to just show that small demo again because we just. I think our engineering is also watching this live stream. I think they just uploaded a new quote. Example quote. Like to just correct that one. Yeah, over here. So this is.
01:24:19.741 - 01:24:43.425, Speaker C: I think we have a new sample code. Yeah, I think just now we'll make sure it's a correct one. Then we put it into the explorer here. Use this interface. 0x0x okay. Yeah. So now it's true.
01:24:43.425 - 01:24:46.525, Speaker C: And you see the output bytes has a lot of information.
01:24:48.475 - 01:24:49.255, Speaker A: Yeah.
01:24:49.635 - 01:25:35.775, Speaker D: So if you're interested in seeing what the output is, there is. What's it called? This output is exactly the same as the. It has the same structure as the receipt that is generated by the RISC 0 guest program. There is a small little tool in the automata DECAP repo under JS there's that index JS or something like that. Yeah, this. So you can just use node JS to run this and then you can include. What's that called? You can include a hex value in it and it will return you either true or false and then information about the code body basically.
01:25:35.775 - 01:25:59.035, Speaker D: So it's just a small little tool to kind of show how you can interact with the contract. And like if you want to just try out. What's that called? Try verifying an attestation. Like you're doing development work, you can just make use of this to quickly check it out.
01:26:01.815 - 01:26:22.871, Speaker B: Right. Amazing. So what would you suggest as for Andrew's destack, like what configuration of attestations should we be using? I think on chain PCs is a must. What else should we be using? ZK Snarks Sorry.
01:26:22.943 - 01:27:06.223, Speaker D: I think Andrew actually mentioned something very interesting which is in destacks case actually the verification can be done off chain because. Well, for the on chain portion there isn't anything interesting that you would want to do. You just want to establish certain facts and then the secret is actually kept, sorry, totally off chain. In that case then the verification can be done off chain. Right. As long as each of the participants is confident that the other entity that I'm interacting with is actually trustworthy. So for that case I think off chain verification is okay.
01:27:06.223 - 01:27:30.579, Speaker D: But in certain cases where for example, I want to allow users to connect to say a service provider that is running within a tee, then in that case, unless each of the user would perform this remote attestation verification themselves, it would actually be a lot easier if the chain can perform this verification and then because we trust the chain.
01:27:30.627 - 01:27:30.811, Speaker C: Right.
01:27:30.843 - 01:28:21.695, Speaker D: Then we can just directly use the result on chain. That would be a lot more effective in certain cases where we want to do say key rotation or allow permissioned access to some state on chain. I'm guessing it's easier if you can just directly verify the. What's that called? The authenticity of this public key. Yes, of the, of the, of the public key that is going to be interacting with the, with the on chain certificate. Because if you are doing this on chain then, sorry, off chain then you need to actually form some sort of consensus or some way to actually move this result on chain such that the contract can actually make use of it.
01:28:24.955 - 01:28:55.055, Speaker B: Yeah, I think so. But also the just the on chain PCCs I think is very useful because if you think about it, even if we verify data station off chain, there is still the question of what are we using for the collaterals. And as you're suggesting, we may want to restrict what's allowed. Right. We don't necessarily want to go with what Idler is suggesting. We want to maybe be a bit more restrictive and also have governance over the TCP so that they don't just disappear.
01:28:55.355 - 01:28:56.495, Speaker D: Yes, totally.
01:28:58.235 - 01:29:18.135, Speaker B: All right, yeah. I think that's the next step for us to go. Include the QPL for verifying using on chain PCCs. All right. Amazing. I don't have any more questions. And yeah, Andrew, any other topics?
01:29:18.675 - 01:29:19.935, Speaker A: That was fantastic.
01:29:23.835 - 01:29:31.045, Speaker B: Right. Amazing. So thanks a lot for the presentation and the Q and A.
