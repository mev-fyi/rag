00:00:02.720 - 00:00:03.104, Speaker A: That's good.
00:00:03.118 - 00:00:32.764, Speaker B: Okay, cool. Welcome everyone. We have a bit of a collection, a variety of people on the stage today. I would classify them into two groups. We have one group of people that are sort of thinking about protocol designs, putting together many different kinds of nodes, like big nodes, small nodes, as we were just discussing the previous talk. And then we also have Lev, who's operating one of the big nodes. And so it gives a bit of a different perspective there.
00:00:32.764 - 00:01:10.930, Speaker B: And the idea today is to sort of flesh out some of Vitalik's questions and generally try to understand how people are thinking about this heterogeneous endgame where we have different nodes playing different roles, what are the desired properties of the markets between them and how they interact, and what are the kind of actors who fulfill these roles. So I'll maybe ask everyone to just give a quick intro, say your name, your affiliation, and then sort of what kind of nodes you'd say you get to do with. Are you operating a node or if you're not, if you're running a protocol, what are the, at the very high level, what are the kind of nodes involved? Maybe starting from Lev.
00:01:11.122 - 00:01:26.114, Speaker C: Hi, I'm Lev, I'm from Beaver build. And as you guys probably have heard, block building involves not only running normal nodes, but also being a node in the sense of being responsible for what transactions get executed and included.
00:01:27.974 - 00:01:56.934, Speaker A: Hello Mustafa at Celestia in terms of nodes, because it's just a DA layer. So there isn't much Mev right now unless people build base roll ups. So there's no proposal builder separation, there's just no normal validated nodes. But there's also a big emphasis on data availability, sampling light nodes, so that even though tenement only supports 100 validators, the idea is that end users can run, they've got the sampling nodes to get assurances about the state of the chain.
00:01:58.674 - 00:02:21.344, Speaker D: Hi, I'm Josh at Astria. We're running a shared sequencer. So generally the nodes we're worried about are sequencing nodes. But my perspective is that in a lazy shared sequencer they look like relays in a PBS system. And we assume, obviously it's early, but we assume the ecosystem will develop into a PBS system. And therefore the important part of our system is to run large nodes with relatively high bandwidth.
00:02:22.204 - 00:02:47.734, Speaker E: Hi, my name is ye, I'm from scroll. We are building a ZkVM L2. So we have like, not super controversial, we have two types of nodes. One is sequencer, one is prover. Our sequencer is not as lazy as like, because we need to actually order transaction, execute transaction, and post, post data to ECM layer one, and then later prover will kind of generate proof for each block and then submit a proof and then like layer one, verify the proof. So it's like non controversial L2 construction.
00:02:48.874 - 00:03:15.854, Speaker F: Hey everyone, my name is Joe from Aztec. We're building a privacy focused L2 on Ethereum, also trying to make it decentralized. So we have a few different nodes in that peer to peer nodes transaction pool, similar to scroll sequencing nodes. We care a lot about those being permissionless. So I guess they're small nodes. And then on the proving side of things, we'll have some slightly larger nodes for proving marketplaces filling in to actually build the proofs.
00:03:17.134 - 00:03:51.954, Speaker G: Yeah, and I mean, I guess in Ethereum, like if I had to categorize it into three types of nodes, I would say like specialized nodes that do various kinds of like aggregating or proof generation, awareness generation, then you would have nodes that are participating in consensus as validators. And then just like nodes that any user runs to verify the chain, which I expect in the long run will just be verifying one proof and doing one round of data availability sampling, though it's still a while until we get there.
00:03:52.694 - 00:03:53.518, Speaker B: I'm mic'd up.
00:03:53.566 - 00:03:54.154, Speaker G: Thanks.
00:03:55.294 - 00:04:29.801, Speaker B: Ok, so Joe, I think I heard you mention both permissionless and markets in your response. And so when we look at these different sets of nodes, there's usually a couple of selection processes involved. One selection process is deciding which nodes are even part of your system. And then even within that there's a delegation of roles. So for example, in Ethereum, you select a proposal consensus leader that produces blocks. And then on top of that we've built another market where there's now some selection of a blockbuilder. And at least it made a big impression on me.
00:04:29.801 - 00:04:47.104, Speaker B: Last year, John Shah made this argument for proof of governance. He was saying, if you look at lido, look at these staking pools, eventually just end, in an abstract sense, selecting nodes. So I'm curious, how do people on this panel feel about the role of governance and the trade offs between governance versus something that's more permissionless or looks more like a market?
00:04:48.684 - 00:05:22.002, Speaker F: Yeah, I can start. And I think depending on the properties you need from nodes, like if you're going for censorship resistance, you need to have permissionless kind of entry into that. And I think everything downstream of that is a market design question to try and keep things in an equilibrium that stay that way. And we saw that on the previous panel where if we get that wrong, maybe you get builder centralization, but I think aligning censorship resistance and permissionless entry into a market is a key criteria for node selection.
00:05:22.058 - 00:06:25.624, Speaker D: For me, I think our view would be we're trying to go from centralized sequencing to decentralized sequencing. One of the fundamental trade offs is just going to be you're running a consensus network, you're going to be slower, you're going to have higher latency. We want to optimize our operator selection, our node operator selection for quality, the ability for these entities to run a high performance, high throughput network so that the trade off cost of going from a centralized network to a decentralized network is minimized in some way. It's this not making perfect the enemy of good, where we would prefer to not have a permissionless system that leads to just stake weighted selection, where maybe they have a lot of stake but they're not good at operating nodes and therefore the network is lower performance. Prefer to do something like proof of governance where you're saying, well, we're actually manually picking nodes, we're getting the decentralization from a geographic perspective, we're making that permissionless trade off, but we're also having a higher throughput network, and we think that allows us to go into this decentralized versus centralized trade off with a lower cost to an end user.
00:06:27.324 - 00:07:11.834, Speaker E: I can also add more, even if we are not decentralized yet, our roadmap we are committed to decentralized. But the way we, we think about how we select different party like sequencer and proverbs is that sequencer and prover have totally different reason why they want to be decentralized. Because for sequencer it's more for you want permissionless. No one can censor your transaction without going through the layer one to pay in that costly cost. And prover is more for liveness because what if one day we start to go away? It's very hard to assemble a lot of really strong computation power prover to running generally prove and keep your network running two separate roles. And so they have totally different requirement. And the way we think about how we select role really like how much we want to do in protocol.
00:07:11.834 - 00:07:49.726, Speaker E: Because there are two ways to design a protocol. One is that you can imagine there is just one node running the role of a proposal where you have only one type of proposal and then you select this proposal will generate block and also produce block produce proof. But even in this case, you can still delegate the block building to a separate beauty market builder market. And then you can outsource this kind of proof generation to a separate prover market. So those two roles are still separated, but they are kind of in the protocol. You don't need to worry about who is running a builder, who is running a prover. You just select one party who you trust and will submit the data and proof.
00:07:49.726 - 00:08:24.824, Speaker E: And the other choice is the protocol will select both a sequencer and prover. And that might require a very complicated design because you need some slashing for sequencer produce, maybe bad block. You might also want to punish proverbs. They are not submitting proving time. And that makes the incentive and economic become more complicated and make the flow become a little bit more complicated. But that's basically how we are thinking about how we are selecting different roles and how we decentralize our protocol. And for our roadmap, we will decentralize first to be safer, and then we still control the block generation, and then later we'll decentralize both parties.
00:08:25.254 - 00:09:10.494, Speaker B: See, yeah, maybe throwing this to Lev, you're in a position where it's a very market based selection mechanism, right? And we had like a brief discussion this morning around what, and the previous panel also highlighted this. Having this kind of market ends up changing the role that a node might play. You know, like originally blockbuilders were, you know, just had a very specific function in Mevboost and PBS roadmap. And then on top of that, additional features came around. People started implementing private rpcs. And so I was wondering if you could maybe give some insight into that. And then I'd be curious if that changes people's perspective on whether you want a permissionless market or anything like that.
00:09:11.274 - 00:10:25.244, Speaker C: Yeah, so I think as far as we're concerned, and I would kind of speak to both the specialized node part and the sort of normal user node part here, not about consensus nodes, which I think are very governance. I would describe them as governance constrained, like governance explicitly reasons about what they're meant to do and tries to. And also in terms of the code that's shipped by core devs, is more or less what consensus nodes end up running, while everything else is this kind of wild west, which is, I would say, not really constrained by, by governance, and it's really governed more by market forces at this point on ethereum l one specifically, and I think that this could have a lot of unforeseen implications that often are not discussed in governance at all. So, like, which APIs do users use to submit transactions? Which APIs do users use to interact with state and which parties are able to provide those APIs. So how do block explorers work? How does a user simulate a transaction in order to figure out state to decide what transaction they actually want to do? These are things that very rarely come up in the governance context. And have all of these have market, it's not just a block building that has market forces behind it, but also providers of rpcs. That's a business model.
00:10:25.284 - 00:10:25.484, Speaker G: Right.
00:10:25.524 - 00:10:43.024, Speaker C: And they have specific node requirements that are different to builders, for example, all of these things, there's market forces behind it. And I think people used to talk more about historical state, for example, stuff and state rent than they do in this conference, but that's a big part of it as well.
00:10:45.724 - 00:11:01.996, Speaker B: Yeah, just to add to that, maybe Kevin, your colleague earlier was talking about the sort of trade offs of including blobs or not including blobs, and how market forces actually push you to maybe not include blobs. That's maybe something else. Different proof of governance setting.
00:11:02.180 - 00:11:09.792, Speaker C: Yeah, I think that gets very interesting with probably with things like blobs and also proof aggregation that was discussed in the previous presentation.
00:11:09.908 - 00:11:57.224, Speaker B: Yeah, yeah. So Ezekiel, proven markets came up earlier and I was, I don't know much about them, so maybe you can educate me. But you know, one thing that I wonder about sometimes thinking from PBS is the impact of closed source optimizations. So if you have a market, obviously like markets can have many kind of selection mechanisms, but generally there's some selection for a more efficient, a more efficient proverbial. If someone has a hyper optimized, they've got some breakthrough with FPGA's and whatnot, hyper optimized prover, they might end up dominating your market. So I'm curious, maybe just generally in these kind of node markets, but in particular in the prover setting, what do you guys think are the sort of the properties that you want from the market?
00:11:58.724 - 00:12:41.912, Speaker E: Yeah, I can talk first from my perspective. So again, go back to why we want to decentral prover is because we want liveness, because even if we are not operating score, network proof can still improve. So that's why we want this kind of poor market. And there is always a backup for someone who generate proof. And it's very interesting question because different from mining proof market is quite different because if you get ASIC, you literally can beat any prover in the market. You can literally become the best prover and take all the power to produce blocks. I think right now at this stage is actually a good thing because we are kind of incentivizing the poorer technology to improve faster and faster, because right now the progeneration takes around like five minutes or something.
00:12:41.912 - 00:13:28.674, Speaker E: And we want that to reduce to some 10 seconds to achieve real time proving and then have some interability and all those kind of nice properties we want to have. So I think that's why I think starting from incentivizing fast approver, but eventually becoming resize some time window as far as you kind of submit proof within that reasonable time window, then like you can get rewarded, will be a more fair way to kind of always, because if you are, your system is always dominant by the fast improver, they can always leave and then your system gets more vulnerable. And then in terms of this, like we want to try to kind of firstly incentive fast approver to improve like overall, like the ZK ecosystem, because you need an order of magnitude improvement on the performance. But secondly, then eventually move to like a more fair and then like, you know, more people get the opportunity to generate proof instead of just one party dominates the market.
00:13:30.014 - 00:14:21.898, Speaker F: Yeah, I guess I have some thoughts. I think Mina may be pioneered, this idea of a maximum proof delay, and they have a proving marketplace on their chain. But the rough idea is that you have to be careful in market design of what you optimize for. If you optimize for fastest proof wins, then you'll create huge centralizing forces around that. But if you actually optimize for cheapest proof in 1 minute wins, then you may be able to enable everyone in the room who's got a lower cost of hardware or electricity to participate in that marketplace. So the rough idea behind approving marketplaces that you can have a maybe a more centralized coordinator that can work with a lot of different kind of provers to make approved for cheaper, but on the same long time horizon. And so I think over the long term, blockchains are very good at coordinating.
00:14:21.898 - 00:14:27.786, Speaker F: And so we should see that for proof construction as well, rather than centralized markets, right?
00:14:27.890 - 00:15:37.584, Speaker G: Yeah. Well, the nice thing about proof markets, if you do it based on an auction scheme, is like even if someone dominates, you can show that they're not going to dominate exactly 100% of the time. And the reason basically is that if they are dominating 100% of the time, then it's in their interest to keep on bidding less and less to make more profit until they're is like at least an epsilon of a window for someone else, which is something that does. It does mean that a lot of profit goes to one actor, but at least creates this incentive to be the backup guy and at least be around for that. But that's something that you can tune up or down. And if you can create a market design instead of in principle more friendly to multi actor participation than you might even be able to encourage better quality in whoever those secondary actors are. One example would even be if you just explicitly break up the claim into different parts and ask different provers to provide what those different parts are.
00:15:39.204 - 00:15:57.174, Speaker F: Another way to do the same thing is backup blocks. You just kind of built into the protocol market. You always incentivize a separate backup block to do the same thing. It will have less reward, but you will always then at least have two proving marketplaces compete and you have liveness, as you were saying at scroll.
00:15:58.074 - 00:16:19.024, Speaker E: Just one thing to add, because Justin, Justin actually has another interesting idea, which is we can pay money to buy ZK ASIC because that will be really fast and then airdrop to a bunch of people at the backup, so that even if you go down, there will be hundreds of people who hold your ASIC and then can kind of start running this as backup. But I don't know whether it's realistic, but it's just an interesting idea.
00:16:20.804 - 00:16:28.504, Speaker G: Yeah, I looked at one of Justin's VDF asics a few hours ago, so I was like, I can ship.
00:16:30.764 - 00:16:54.876, Speaker B: Josh, if I understand correctly, your design also relies on there must be some external block producing entity that executes transactions because for horizontal scaling, your nodes don't have chain vms. How do you think about the markets and the way that you select these external, external parties?
00:16:55.060 - 00:17:46.574, Speaker D: I think the way we think about it is to the point on governance. It would probably be accurate to call our initial set going to be a proof of governance chain, proof of authority, whatever you want to call it. We're going to specifically select entities to be the sequencers, the validators, the relayers. I think those are all equivalent in our system. And then to Lev's point, we're going to have the builders be this market driven entity. That's the desire, as we're saying, the governance will choose this core entity and then you have market driving factors for actually selecting the other entities upstream. And I think it's desirable to have market driven effects to optimize a lot of that, because I think the big trade off is we want the in protocol actors to be relatively lower resourced such that we can then have market incentives that will pay for the higher resource actors that need to do simulation across multiple chains or whatever.
00:17:46.574 - 00:18:07.184, Speaker D: And we've already seen builders can make money, as we discussed with the blob thing, they will do the work if it is profitable for them and they will not do it if it is not profitable. It lets you back out of having to do a lot of upfront governance design of saying, hey, this fee mechanism or this work is profitable. The market will just define what is like a profitable behavior. And we think that's desirable.
00:18:08.284 - 00:18:49.664, Speaker G: Yeah, well, I think the challenge with all these incentives is like, there's things that we want that you can incentivize and there's things that we want that we clearly can't incentivize. Right. Like I think the big thing that we value that's not really incentivizable in a robust way is like this abstract idea of being an independent signal. Right. And you know, we have a little bit of incentives toward that with things like these quadratic penalties and like the fact that going offline only really penalizes you if you go offline at the same time as more than a third of all the other nodes and stuff. But that's in general. But those kinds of rules by themselves are not enough.
00:18:49.664 - 00:19:10.224, Speaker G: And there's the question of to what extent you even can make in protocol incentives for that versus to what extent this becomes a task for the higher level ecosystem and you basically just get more solar stake or air drops and which is like, might actually fix things.
00:19:12.204 - 00:19:30.344, Speaker F: Yeah, I guess one thing we've struggled with a bit is like when trying to design these markets, if you try and solve one market, you probably will create an inadvertent market somewhere else and you don't really know which one's worse, the one that you kind of were trying to solve for or this one that you didn't think about over here. So you've got to be quite careful of that.
00:19:30.764 - 00:19:55.434, Speaker B: What are you worried about happening? The question probably is what are the kind of actors you want to fill the roles in your system? Do you have any preference? Or like for example, Lev was talking about how they end up doing a lot of user facing things and supporting different rpcs and getting hit up in discord because people want to know why the transactions didn't land. And that obviously like the response. That depends on the kind of actors.
00:19:57.374 - 00:20:24.794, Speaker F: Yeah, I think it's just actors that are aligned with your network's philosophy. So we're a privacy network and so we would want actors that kind of don't censor and put users transactions on chain regardless of the contents. And I think that's a key tenant for us. So if we not careful and we create just a pure capitalistic market, then some of those properties may get eroded. That's our number one worry.
00:20:25.154 - 00:21:26.742, Speaker B: Okay, taking a step back, I think there's general agreement that it makes sense to have smaller nodes, many more of them that perform some key validation tasks, and big nodes that do some of the heavier resource things constrained by the small nodes. But one thing that it's not clear to me where it falls is consensus and the confirmations that it provides to give. I guess this touches on the base, the base roll up discussion as well. Some roll ups want to have their own sequencer set running consensus, providing confirmations, and then eventually post that to Ethereum's largest set. And others just want to go straight to Ethereum. In celestia, there's an interesting parallel where the celestia validators are just rotating set and they're running the consensus. And unlike in the Ethereum roadmap, or like one version of the Ethereum roadmap, the small nodes are only sampling data, but not running consensus.
00:21:26.742 - 00:21:40.114, Speaker B: And so consensus is a much more specialized thing, constrained to a smaller number of nodes. And I'm curious, I guess anyone can weigh in, but Mustafa and Vitalik probably have the obvious candidates here.
00:21:40.774 - 00:22:27.438, Speaker A: Yeah, I mean, the approach that we've kind of the philosophy is that it's okay to have, you know, semi like. Let's not assume we have decentralized block production or even block proposing, and let's try to rely as much as possible on things like light nodes and data availability sampling to make sure that the chain is correct. And also, obviously, the most important part of having these decentralized block production I'm proposing is to have censorship resistance. But my general thought there is. Well, like no chain has had a Nakamoto efficient more than 40. So is there a diminishing return between having 100 validators and 1000 validators, given that no chain has had nakomatic fusion much bigger than that? Obviously. Maybe.
00:22:27.438 - 00:22:51.514, Speaker A: Obviously with decentralization you only realize you need it when you actually need it. So we don't know for sure yet. It could be the case that you do need more validators. But the trade off there is like with tenements, with 100 validators in tenement, you get instant finality for each block. Obviously there's a little computer coefficient to actually control two thirds of the set.
00:22:53.374 - 00:22:57.834, Speaker B: Just to give more context there. How are the validating nodes selected?
00:22:59.094 - 00:23:03.434, Speaker A: Yeah, it's just delegators, delegates to stake to this validate nodes.
00:23:03.734 - 00:23:05.434, Speaker B: Does governance play any role?
00:23:05.814 - 00:23:47.360, Speaker A: No, there's no proof of governance or anything like that. It's just like any person that owns tokens can choose who they delegate to, but it does. But there is still also liquid staking, kind of like monopoly or geolopy issue in Celestia, there is some early liquid staking protocols and there's a threat that they could have a monopoly as well. And for celestial, it's even worse if there's a monopoly because there's no smart contract environment on celestia. So the liquid staking protocols are like on other cosmos chains that have IBC connection Celestia. So that monopoly gets even worse because the tokens will end up in other chains, but one. So we're also looking at things like proof of governance.
00:23:47.360 - 00:24:13.364, Speaker A: But one of the kind of like proposals in the celestial community is this idea where I don't know if I can fully explain it here, but idea like, instead of having proof of governance for choosing to prevent monopolies for liquid staking, you would just have like this idea where anyone can kind of create their own liquid staking token with their own set of validators and make it very easy to swap them. And that in theory might prevent monopolies from occurring.
00:24:15.704 - 00:24:18.200, Speaker B: Vitalik I don't know if you want to weigh in or not.
00:24:18.352 - 00:25:28.510, Speaker G: Yeah, I mean, I think on the bigger question, like one of the interesting things about the execution tickets proposal in particular that makes it really clear is that it basically takes PBS basically to its full conclusion. And you would basically have builder attester separation pretty much. And the idea is that attesters are the thing that actually becomes the unincentivized incentivizer. And the thing where you're not collecting me, there's a nice long four second window within to publish stuff and so on and so forth. Like that's a set of actors that in principle through various non protocol incentives that you can encourage to be widely distributed throughout the world. But then once you have that exist as an incentivization layer, it's like relatively less bad for whatever else to happen in the builder layer or other parts of the pipeline. The nice things about a tester is basically that if you have a block, then you can, the block already comes with a proof.
00:25:28.510 - 00:26:01.024, Speaker G: And as all you need to do is just be the same thing as a light node, you verify the proof and you do a data availability sample query and you do the same very minimal checks that any node would need to do it in order to verify the chain. And that's a thing that in the long run it can be much easier to run everything earlier in the pipeline does definitely feel like it just does. There's definitely pressure for it to inherently keep getting more sophisticated.
00:26:02.004 - 00:26:02.316, Speaker C: Yeah.
00:26:02.340 - 00:26:41.734, Speaker B: So with the exception of celestia, all of the protocols we've discussed today started with small nodes and then the big nodes have sort of been added on as this external market and we leave it up to the market to fill in this interface. Yeah. I'm curious if because of that there aren't some oversights in the protocol could be designed to be more friendly to these big node operators. And Lev, I guess you could also weigh in here like if there was some requests you could put in as a big node operator to protocol designers to make life easier for you, or that things that are obviously inefficient from your perspective, if any.
00:26:44.174 - 00:27:46.304, Speaker C: I mean, firstly, I'm not sure if it's exactly should be. The objective of the question is what is. I think big node is like a very broader, in your case, I would say that I usually view it in terms of more of like molding the role of the big node operator versus trying to make life easy for them. Because I mean, they will, I think as long as they're incentivized, they'll always figure it out. So the question is what are the efforts working towards? And what you can do is you can fail to incentivize them to devote their efforts to the thing that actually helps the ultimate end user somehow. And that's when you failed, I think, is if they're, you know, if they all have, you know, budgets of millions of dollars devoted to optimizing something, which is clearly not the kind of social good of the, of the group, but on the contrary, if you, if you do manage to have it so that they're devoting their, their efforts to, to doing something that's useful, then you've succeeded. So I would put it that way, rather than how to make their life easier, just because historically it just hasn't really been how we've seen things.
00:27:46.304 - 00:28:51.934, Speaker C: But I think that especially with things like execution tickets, I think it opens a very interesting area for figuring out what kind of standardization will be required for these different parties to interact. I think right now there's been some degree of spontaneous standardization on things like builder APIs and non standard rpcs, both pre and post pbs. But there's also some growing divergence there. And I think that as these things get more complex, it's very unlikely that we will spontaneously converge on the same APIs. I think it's anybody's guess how. If execution tickets are implemented, how exactly it plays out in terms of who the different participants are. But I could imagine something that forces a greater degree of standardization there in order for it to be basically possible for the builder for a particular slot to actually aggregate order flow and talk to other people required to make the block that they need to.
00:28:54.844 - 00:29:09.624, Speaker B: I don't know if any of the protocol designers have other perspective on that. Where have you thought about how to enforce standard behavior across the nodes participating in your markets? Or is that sort of something that's a bit further down the line?
00:29:10.084 - 00:30:23.174, Speaker D: This relates to the earlier panel from the builders, and maybe it's not trying to enforce standardized behavior, but it's kind of thinking about bootstrapping and saying why? I asked the question of do the builders spend any time thinking about other ecosystems? And you're saying, okay, if I'm bootstrapping an ecosystem and I need builders to participate, what can I do to lower the barrier to entry, to have these kind of more sophisticated participants in an ecosystem that is maybe more nascent and therefore building APIs, to your point of making it easier for the big nodes because we view them as necessary or maybe inevitable would be the better word of the market structure will lead to these entities being there. How do you accelerate that path to getting them there? Ideally, you can have more decentralization in that, but it's really bootstrapping. It's like one of the questions I have is what do builders not like about the builder APIs or the system? They've gotten Lev's point, they'll figure it out. There's a bag of money. And it's just if you go figure out how to use those APIs, however jank they are, there's money there, so they'll just go do that. But presumably there is something better. And so we do spend a bit of time thinking about that and saying, look, if these are kind of one of our customers in a multi sided market, we would like the integration pane to be lower for them because that's just going to make our sales cycle much easier.
00:30:23.514 - 00:30:52.940, Speaker B: Yeah, maybe just to motivate why I framed this in terms of making the life easier for big nodes is that as we sort of, one of the biggest motivations for this kind of separation is scaling. And so you want to have the node resources utilized maximally efficiently. And it's not clear to me that design goal has been there necessarily, like in the way PBS has been designed. It definitely hasn't. Right. It is more about the MEV perspective on things. And so, for example, in PBS today.
00:30:52.940 - 00:31:04.104, Speaker B: Stateroot calculation is quite an expensive task and that's done dozens if not more times in a slot, which is wasteful because you only really need one computed.
00:31:05.364 - 00:31:10.410, Speaker G: Well, at least well in the status quo, like everyone has to compute the stateword to verify a block.
00:31:10.482 - 00:31:24.814, Speaker B: Right, right. But even in the blockbuilding world, if you can update your block bids more quickly, that should allow you to have in some sense a more efficient market.
00:31:26.074 - 00:31:36.704, Speaker G: In principle, there's software that you can write to compute a delta and update a stateroom based on a delta. There's stuff that probably hasn't been written, but could be.
00:31:37.564 - 00:31:38.516, Speaker B: Lev looks like he wants to.
00:31:38.540 - 00:32:54.474, Speaker C: Yeah, there's an interesting example of that is that given that in practice today on Mainnet, the majority of blocks are produced by block builders, for example, and they accept transactions through an API that promises not to include reverting transactions. Effectively, the whole point of having, paying for reverting transactions and a lot of how the gas model was envisioned is now sort of been like kind of deprecated by builders saying that, oh, we actually have tons of computing resources given where the, you know, how much transactions we're getting now. And we use a bunch of, you know, random heuristics to decide which transactions are spam or not. And then you often get, you know, firstly, you get this strange dislocation between how core devs think about use resource usage in the protocol where, you know, block builders are actually expected to recompute things many, many times and resimulate everything in every single order, which is totally different to how the kind of vanilla mempool was designed and where the kind of gas metering considerations I think came from. And I think also, secondly, this creates, I think it's going to create difficulties for, not just for block building. There's this huge question that's looms over the space, which is that users need access to rpcs often for historical state. We don't, we have a.
00:32:54.474 - 00:33:25.468, Speaker C: And you know, this has been kind of the case for much longer than PBS. We have probably questionable incentives in markets around providing rpcs, especially with other chains that might have much, much bigger stake growth that I think are kind of still to me, as far as I've seen, are kind of unanswered. Was that point of your question at.
00:33:25.476 - 00:33:30.424, Speaker B: All that touched on my question? I was trying to think of where to take it next.
00:33:31.844 - 00:33:32.944, Speaker A: I have a question.
00:33:34.684 - 00:33:35.824, Speaker B: Yeah, sure.
00:33:36.284 - 00:34:28.932, Speaker A: Yeah, I mean, following up from this talk, so we're assuming that there's a multi role of ecosystem. First of all. How do you know that's going to be the case? Do we actually think users will use, let's say there's ten general purpose evms. Do we think users will actually use all those evms? Or do you think there's a natural marketplace for users eventually only use one of those chains? Maybe arbitrary optimism will get all the market share. And if there is a multi role up system, how do we like, what incentives will there be to be used? Different roll ups? Like maybe could there be like application specific roll ups or something like that? Or like, you know, that's kind of my question. I think there's always going to be.
00:34:28.948 - 00:34:54.684, Speaker F: Multiple based on either features, or you can maybe call it a feature, but the security assumptions of the roll up. So even if you're just implementing EVM everywhere, you're going to make different trade offs in how you do that, unless you have the precompile option on the slides. So I think even if your features are the security of your DA or your proof system, optimistic versus zk, there's always going to be different places on that tradeoff space you'll sit.
00:34:56.224 - 00:35:48.372, Speaker E: So I think this is a question I keep asking myself, and we have been have multiple internal discussions. I think it's just like even from five to ten years where we are like, I think people here all believe that blockchain really, in next five to ten years, really the core financial infrastructure, if the core financial infrastructure and all those kind of Internet activity all happen on just one, two server, I think it's just literally impossible. Even if you look at right now, Amazon, Google, Twitter, they all need even more servers than what we need right now. I think it's just literally one rob just can't scale to enough. Like, you know, throughput we really need. Because every rob talk about like, you know, 1 billion people bring to ethereum ecosystem. But if you really do some calculation, which we have, is that if 1 billion people, everyone send one transaction per day, that's already like over ten k throughput required.
00:35:48.372 - 00:36:20.644, Speaker E: So, which is literally, even if you use like all parallel evM, every kind of fancy scale you have, it's just impossible to kind of run on just one server without even decentization. So there will be many, but I still don't believe that there will be like, you know, like tens of thousands because, you know, like the interpreter is still very hard to achieve even with shared sequencers. So there will be like, I think there will be several general purpose rob with different value propositions, some security, some other DA, but then users still need to be aware of like we chain, they need to use to kind of be aware of secure assumption and all those properties.
00:36:22.064 - 00:37:02.110, Speaker B: I want to ask one question and put it to the audience. I also don't know how much time, time we have or if there's anyone watching at all. We can just go forever. Okay, I'll ask the question then. And then the police can tell me what are the blockers to central like. There's been a lot of talk about big nodes, big sets of nodes and interaction between different nodes, but most of what we're looking at right now is just a couple AWS servers. So why aren't we? Obviously things are challenging, but more specifically, what are the obstacles to getting to this more decentralized, these more decentralized stacks that we're looking for?
00:37:02.302 - 00:37:39.944, Speaker E: Yeah, I can talk about. So firstly, I think eventually we definitely commit to being decentralized. But it's like the question is how you take this decentralization go to market and how you progressively do that. Because right now the biggest reason that because arbitram optimism, providing millisecond pre conformation experience, if you kind of are sufficiently decentralized, it's just like your system won't be used by people, by users. That's a lesson I learned after we launched our magnet, is like you talk about security, doesn't really matter. People have been living in a system with no proof for two years, so they don't care about decentralization proof. So it's really a matter of how you kind of take this kind of decentralization progressively to market.
00:37:39.944 - 00:38:13.344, Speaker E: So that's why you decentralized approver at least then sacrifice the proof confirmation side. And then you think about how you gradually do that step by step. And the more practical challenge is that right now the key VM has been even fully audited open source. But still it might have some Sunday spark. So if you take a very, very aggressive step to decentralize your stack, you give all the control to anyone, can generate proof and produce a block. It's very dangerous actually, because someone can just catch Sundays error, completeness error for the KDM circuit and then just do something bad without. If you are not enshrining the KVM into layer one, use layer one to govern this KVM.
00:38:13.344 - 00:38:29.924, Speaker E: It's just very dangerous to do that. So it's mainly coming from user need and how you progressively do that, and also some practical challenges on the key side. And also I think maybe adhesive has more compliance issues so they need to decentralize. But for us general purpose one is less concern.
00:38:30.664 - 00:39:10.526, Speaker F: Yeah, maybe I can elaborate on the kind of decentralizing the sequence of risk. So on Ethereum, if there was a some critical consensus bug, you always have the option to hard fork away. And the assets maybe, except USDC exist on both chains. In an l two world that's not the case. So if there's a fully decentralized sequencer, then if someone can exploit an issue in the VM or the roll up circuits, then you can't hard fork away because most of the funds are kind of l one based and they've come from Ethereum. So an attacker can come in, take those funds out and they'll live on Ethereum. Regardless of if you fork the roll up, the roll up won't have any assets in it anymore.
00:39:10.526 - 00:39:37.824, Speaker F: So I guess the number one issue in trying to decentralize here is balancing security with this kind of decentralized sequence of property. Because that's how you can propose a block that contains an invalid transaction. And training wheels help. Multiple proof systems can help, but that's kind of the, the thing that takes the most time to develop is around training wheels to get to a fully decentralized l two.
00:39:38.364 - 00:40:16.280, Speaker D: Yeah, I think there's a couple points here. One that's very key is just where are you spending your resources? What is your differentiation? So we have two Zk rollups where a lot of their time and effort is spent on their specific proving mechanism. Maybe focusing on decentralization as a consensus algorithm and optimizing that and developing a set of entities. They can run a fast network that is just not their kind of core differentiation against all the other roll ups. So say, yeah, we'll do that later. But everyone has progressive decentralization on their roadmap. It's been like three years, whatever, no one's done it enough that there is a forcing function for someone to say, oh, optimism, or arbitrage, the first move, they decentralize the thing.
00:40:16.280 - 00:40:50.466, Speaker D: Now there is a market pressure to say everyone else has to do decentralization, everyone just comfy in it. There's a cynical take of just is a competitive market and people are making a lot of money. We see in the MEV space. There's also the optics case. If you run a centralized sequencer that is equivalent to call it a private mempool or whatever, where you can say ok, no one's getting arbed and we're going to collect all the revenue here. That's just very good from a market perspective in that you get to say, we're doing good by our users and we get more money that allows us to be more competitive. So there's a lot of things at Astria, the position we're taking is saying we're going to go do the decentralization thing first.
00:40:50.466 - 00:41:07.202, Speaker D: And ideally there is like a market demand to kind of like buy that versus build such that you can say like, you don't, you know, each roll up does not need to go invest their own effort if it's not a priority for them. But if that is a functionality they want, they can go kind of inherit that or buy that from someone else. That's kind of the theory here.
00:41:07.218 - 00:41:16.694, Speaker B: I think that makes a lot of sense. We have apparently three minutes, so if there's anyone with a burning question in the audience, now is your time to shine.
00:41:17.114 - 00:41:38.454, Speaker D: Patrick Vitalik talked about this at the DM for annel a couple weeks ago in Denver. It's part of the issue that in the bear market strategies, many teams are chasing total value lock to somewhat of a vanity metric. Is there a better KPI that we could use to evaluate success of a roll off project?
00:41:40.034 - 00:42:02.084, Speaker F: Yeah, I think it should be here in 510 years. And so you have to design it to do that. That's kind of our take. And yeah, you have to design decentralized, I think, to do that. Ethereum talks about being world war three proof. I think it's maybe an extreme take, but I think you need to start thinking about that, because if you're just going for TVL, then I agree, you probably won't survive some of the bumps in the road.
00:42:03.104 - 00:42:40.990, Speaker D: I am by no means like a DeFi expert or a finance expert, especially relative to this room. But one of the things my team was looking at was like, if you look at Jane street as an example of a very centralized market, the numbers are going to be off. But it was something like they had $15 billion of, call it TVL, or capital assets, and they did $1.5 trillion of volume. There's something interesting in the ratio of how much volume of trading you can do in a year relative to how much assets you have. I don't know what the Ethereum ratio is going to be, but my assumption is bad, certainly not four orders of magnitude. Even if we look at Solana, which made different trade offs Solana has as a community.
00:42:40.990 - 00:43:04.234, Speaker D: They argue against TVL being used as a metric. They claim it's a vanity metric, and they do have higher total volume of trading happening relative to a smaller amount of TVL. So I think there's thoughts like that. But fundamentally I guess there's also going to be just subjective user experience concerns. Where do the users actually go? Where do they think they're getting the lowest slippage or the fastest execution, the highest quality execution, whatever.
00:43:05.304 - 00:44:02.908, Speaker E: Like I think we learned a lot after we launch our magnet. Firstly, I don't care about TVR before we launch because it's a vanity metric and I think tv always flow to the chain has highest yield and the chain has highest yield is like the chain that tends to launch a token because you can always use your token to kind of pump your yield and then like liquidity flow from one place to the other. And then like. But the reality that, you know, like people still care about this, this matrix a lot and then because once you get high enough TVL and then you get market attention and then developer will come to your chain because they want to accept this liquidity. I think what's really funny about L2 is that people, at least I went to Ethereum community because I think research, vibe, decentralization, all those stuff, L2 in higher security, but actually another narrative kind of become like Ethereum win because Ethereum had larger liquidity and L2 narrative is only a way to access liquidity easily. And then imagine like Solana has a similar level of liquidity. Does Solana really win at least? I don't think so.
00:44:02.908 - 00:44:45.844, Speaker E: But the market like, you know, people are easily get formal and then like easily to get attracted and then like want to build at least in this kind of easy to formal market. So I still think it's an important matrix. But I think a more housing matrix might be like if we really bring like 1 million user into Ethereum ecosystem and everyone deposit like one sound USD in your ecosystem system, that's a more healthy tv instead of like, you know, one big, well like deposit 1 billion for like really high, you know, sticking reward or something like that. So I think a more healthy net into like you know, your lp and like your like, you know, how your TVL is composed is very important. But short term, like, you know, the best thing you can get is like a high tv number as a marketing signal and then like long term view, then stay relevant for the next five to ten years.
00:44:46.264 - 00:44:46.776, Speaker A: Okay, cool.
00:44:46.800 - 00:44:50.060, Speaker B: I think the question was directed at you, Vitalik, so you want the last word and then we can, can.
00:44:50.172 - 00:44:51.044, Speaker G: What's the question?
00:44:51.164 - 00:44:53.544, Speaker A: Oh, sorry, that's the same.
00:44:55.084 - 00:45:19.284, Speaker G: Yeah, I mean, you know, ideally we'd like, you know, it's like, well I think in order to get like any other metric working in a way that's not trivial. To, like, break, we need working proof of personhood protocols so well, and we need them to not just be working, but, like, actually widely adopted. So that's one of those things that I've definitely been continually excited about.
00:45:20.304 - 00:45:21.520, Speaker F: Okay, awesome.
00:45:21.552 - 00:45:22.944, Speaker B: I think that that's it. Thanks, everyone.
