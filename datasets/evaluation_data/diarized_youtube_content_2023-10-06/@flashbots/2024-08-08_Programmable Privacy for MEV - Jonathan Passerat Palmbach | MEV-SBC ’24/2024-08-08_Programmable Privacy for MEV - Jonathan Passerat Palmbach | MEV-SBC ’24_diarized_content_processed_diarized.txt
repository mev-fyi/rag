00:00:02.800 - 00:00:50.120, Speaker A: All right, everyone, good morning. Okay, so you might be familiar with this terminology of programmable privacy is getting traction. And recently at EFCC, I realized that it was being mentioned, but like a few other similar terms were mentioned as well. So I was like, let's try and clarify these terms, if we can. First of all, let's start with what do we mean by privacy, and in particular privacy at flashbots. But first, a little detour in privacy in web3. I think when we mean privacy in web3, we've been very much driven by the idea of ZK proofs protecting transactions.
00:00:50.120 - 00:02:11.540, Speaker A: This idea of shielded transaction, that's pretty much what people have meant for quite some time when they were talking about privacy. More recently, we've been discussing private smart contracts in private mempools as well. So that's kind of like the pids that are associated with privacy. And this emergence as well, of core processors, either powered by SGX, as we might see a bit later today, some as well, powered by SAt. So all of these terms basically are a bit of a mixed bag when we talk about privacy, and it's not really clear what we mean. I think what's important though, is that these primitives all are meant to unlock new use cases, and that's what we mean by privacy at flashbots. It's not about providing privacy for the sake of privacy, it's more about what use cases can we unlock by introducing these privacy primitives? And in particular, how can we achieve better features for our protocols? How we can remove trust in flashpots as an entity within the ecosystem, for example, and increase the resilience of protocols in the short and medium term.
00:02:11.540 - 00:03:21.916, Speaker A: This is very much anchored around tes and in particular Intel's SGX and NTX. But that doesn't mean that this is the only thing we're exploring. There are multiple paths being explored, and in truth, there has been at least two prominent proposals for what we mean by privacy and programmable privacy in flashbots. One was actually meshare now being the engine behind Flashbust protect. The idea here is that you give the user some form of control over what part of the transactions to reveal. So we're not talking about any cryptography or anything, we're talking about some kind of selective disclosure of information. Here you can go from the most private version, which only discloses the hash of the transaction, which is basically pretty much useless for a searcher, all the way down to exposing all the fields of the transaction, which help the searcher make more elaborate strategies on the transaction.
00:03:21.916 - 00:04:53.140, Speaker A: But also release a lot more information about your trade. So it's about whether you're comfortable with that or not. It's a trade off. Another approach, very different, is this drug that we call a t builder and here vids to start in a way abstracting the services that we used to provide and wrap them into some more formal guarantees provided by these tes. In particular, one aspect that is interesting beyond providing pretrained privacy because that's something we were offering in a way by social contract before, is this idea of verifiability. We start introducing new features in this game and in particular users can verify that the refund rule in the builder has been applied correctly because it is run within t and thus they can rely on the traditional remote attestation based guarantees of the ESGX TDX stack to obtain more formal guarantees on what's happening on their transactions. So just to summarize a bit what we've been talking about here, we've used the word privacy in a few different contexts and I would say that the last one for example would qualify more as confidential computing.
00:04:53.140 - 00:06:19.680, Speaker A: That's what we're doing when we're trying to embed SGX and keys in the game. Also, all of these secure and confidential doesn't equate, so we'll talk a bit more about that later. But that's something to keep in mind. We also label meshe as like a programmable privacy approach because we're providing hints to the search and we give some level of control to the user, but it's not really quantified, it's not really clear what kind of privacy guarantees we're giving here. If you expand to these cloud awards related to previously that I was mentioning before, you could realize that there is a lot of terms that are being used these days, some in the more web two academia lens got previous enhancing technologies, which is the one I think that has got the most traction overall. But we start to see confidential computing is very attached to intel and hes secure computing and cryptid computing for the most software cryptography declination and in web3 we have programmable privacy. But we also started to talk about confidential defi and decentralized confidential computing and my take is that were all doing the same thing here.
00:06:19.680 - 00:07:29.306, Speaker A: Theres no magic there and it's nice to have these memes that can gain some traction. But what's more interesting is to better understand the properties that these protocols that these primitives are providing. And that's why I like to think about improved scene in sync technologies that like three different aspects, technologies that are trying to protect what I call the input privacy. So really the privacy of the data that you inject in the computation, that's usually what we mean by confidentiality. That's where we are here. And you'll see that's one of the aspects that we are focusing a lot of time on, for good reasons, but that's clear the state of things today. There's like these other side of privacy, which is, from what I've seen, less of a concern, less of a focus at least, which is output privacy.
00:07:29.306 - 00:08:32.950, Speaker A: And it's really about how much information does the results of a computation leak on the data you put into it in the first place. And usually that's something that we tackle with things like differential privacy, for example, where we can give some very formal guarantees about the level of information that is being disclosed. And the glue between these things tends to be around smart contracts and blockchains in general. You can even think about very fast bulletin chains that tend to be useful sometimes to make the bridge between the two. So that's what we're going to qualify as, enforcing policies. The more the governance part of the privacy that allows users to have maybe a bit more control on the information they disclose. And the sad thing is that these things tend to be closed here on this diagram, but we tend to study them a lot in isolation, whereas pets should really be combined to shine.
00:08:32.950 - 00:09:34.600, Speaker A: And we'll see a few examples of what that means here. Most of these technologies are in isolation, amazing, but they don't really provide an end to end solution. Instead of talking about previous enhancing technologies, there is another term that I think has been coined by Nigel Smart, which is, let's switch this acronym to partnership enabling technology. Let's bring them together to make the most of them, but let's also use them to bring users together in joint computations. Let's just double click on each of these topics to see what we mean by that. Fhe plus snarks fhe, for example, is very interesting in the sense that you'll see with a concrete example later. FTE by itself cannot really be deployed as a protocol.
00:09:34.600 - 00:10:24.840, Speaker A: You need some training wheels in a way to make sure that the thing works. And in particular, we're very interested in this idea of not necessarily verifying the whole computation first, because that's very challenging, but in particular, validating the ciphertext that we inject in the computation. The reason is, if you don't do that, if I can inject any invalid ciphertext in a decryption process. For example, in decryption Oracle there are some very simple attacks that allow you to gradually leak more and more entropy and eventually disclose the private key. You really want to make sure that the ciphertext that you're getting into a decryption oracle for example, are correct. They are well formed ciphertextproofs for that. There's two instances of that.
00:10:24.840 - 00:11:48.282, Speaker A: Xama is building one for tfhe in their rust library, and PSE has recently put together a paper called Greco. They do that for another scheme for BFE. I think when pairing together FHe and tes also makes sense because fhe by itself again doesn't, doesn't provide these verifiability. So maybe we can get that by relying on trusted execution environments and the remote station property. The idea here is that it's going to be way cheaper and way faster to obtain the integrity property from tes, but it's also pretty fine in terms of assumptions in the sense that even if you don't trust fully the TE, or even if the TE is breached for one reason or another, you don't lose everything in the sense that you've got these, you still are protected from a confidentiality point of view by the encryption. You lose the integrity, but you can still be confident that your data is not being leaked. That's why it's important as well to have this distinction between input privacy, privacy and all of these things, instead of just having programmable privacy as a whole.
00:11:48.282 - 00:12:51.328, Speaker A: I think ZKP tes, something that we've seen over the past year or so I think is the emergence of multiprovers. So it's way cheaper, it's way faster to produce roofs, as in remote attestation based proofs that can be an alternative to snark for rollups, for example. There's a few projects that already offer these kind of services. That's pretty interesting, but more importantly, that also can act as a training wheel for snarks. Snarks are very complex protocols. It's not completely ruled out that we can still find bugs in the implementations of some of them. So having the TE act as a fallback can be highly beneficial in this context.
00:12:51.328 - 00:14:41.518, Speaker A: But also the other way round can be interesting to study in the sense that if you just have a z key proof, you don't really have any guarantees on the you're assumed to be the prover, to be the one controlling the data. So if I want to delegate proving, if I want to generate proofs for data that is sensitive, I either have to do that by myself on my laptop, or the only option I have is to trust the prover doing that on my behalf. Running approval within a TE, for example, this time allows you to have to add the confidentiality guarantees on the data you're providing to the prover and still obtain some verifiability guarantees from the proof. And it's kind of like going in circles and helping one another in the case of tes and snarks, because once you've generated a proof that is based on tes, for example, it is not the kind of proofs that we use to verify on chain to unlock a next action in a smart contract. But the good thing is that you can also snark the attestation verification process. The automated team has done that, for example, for both SGX and TDX, you can turn back the TE proof in a way into a snark if that's something that you find more appealing for your application. So another good highlight of the combinations here, we've also seen ZKP and NPC's being brought together for quite some time.
00:14:41.518 - 00:16:14.510, Speaker A: I thought it was just like an academic endeavor, this paper on collaborative snarks, but recently Renegade has used it to power the dark pool. And the idea here is that you you don't generate a proof by yourself anymore. You run an MPC protocol to inject shares of a witness within the computation and spit out a single proof out of it. That's pretty interesting too, giving you more a concrete instance of what do we mean by that. I've recently been working on a prototype run an arbitrage, a background arbitrage protocol using fat. The idea was that we want to provide more formal guarantees in protect, for example, we want to go beyond offering guarantees as an entity to more formal guarantees that are by cryptography, for example, going towards more privacy by design, but retaining some level of programmability. To that we just take a very simple setting where we focus on a single uniswap V two transaction swap, and we use sat to represent the strategy that a searcher would run to back run this transaction, for example.
00:16:14.510 - 00:17:15.290, Speaker A: So I'll just skip the details on what is fhe. I think you might have an idea, and if you don't, I'm happy to talk about that later if you want. Just to give you an idea of the setting we're using, a single user encrypts their transaction and sends that to the searcher infrastructure. Here we assume that the searcher has a large enough machine to run to run the computation and the searcher injects some preferences, some parameters into this construction, and the output of this computation is an encrypted bundle, or at least an encrypted background transaction. If the background was successful, the searcher has no ability to decrypt this transaction. It's a one off computation, but it doesn't have any feedback on what's going on here. Everything goes to an assumed trusted builder that will decrypt and insert that into a block.
00:17:15.290 - 00:18:36.120, Speaker A: It's a highly unrealistic setting, of course, but just with this very simple setting, there is a bunch of challenges that stand out. Of course, the computational overhead is still fairly large, and we might need some more bespoke optimizations to to make this algorithm faster. But mostly, as you've seen, by using FHE as a standalone technique, we can't really provide any real world guarantees here, right? We lack the integrity of the computation. We have other challenges that are pretty hard to perform in FHE, one being performing ECD, say signatures for example, to actually finalize the background transaction. And we would need to also have some kind of public key that is controlled by a set of validators. So that would mean threshold fh key, but in itself it's no silver bullet in the sense that we need guarantees on what the validators are doing with our shares. So luckily t's are pretty good at this, and I wanted to just give you an idea of what it would mean to turn this prototype into what could become an fhe private mempool in a sense.
00:18:36.120 - 00:19:41.060, Speaker A: So in order to do that, we need to lift these barriers that we've encountered. The signature barrier, for example, would be pretty hard to solve in pure fhe. CDSA is not very fhe friendly as a, as a signature algorithm. So some ideas could be changed, some NPC in there and engage in a protocol with the builder. Either sign the ciphertext and then do something with a builder to verify the signature before it's being decrypted, or maybe use a con abstraction to replace ECDSA altogether by a more efficiency friendly signature scheme. Here we start to combine to confidentiality primitives to get to a new feature, the threshold decryption. It's something that in a way is more advanced because we start to see networks not in production yet, but at least being announced to provide fhe keys and fhe threshold encryption.
00:19:41.060 - 00:20:58.364, Speaker A: And it turns out that they already backed by enclaves. So in Xama's construction, for example, they rely on AWS Nitro, but there's nothing preventing them from running that into SGX and to keep building in the defense in depth design, it might actually be desirable to rely on multiple different tes in case one is being breached. You can still rely on the others to complete the protocol. For example, interestingly, we have not talked much about this output privacy thing that I was mentioning in the first place, and I think that's critical to making these more interesting protocols become a reality. You might recall that such as do not get any feedback from these designs for now. And there's a clear reason to that we don't want to open what we call a covert channel where the searcher could start to encode information by gradually requesting the service and trying to infer some information by doing that. So it's pretty hard to make sure that you close all of them, but the trade off is that you reduce the programmability for the searcher.
00:20:58.364 - 00:21:57.750, Speaker A: For now, we guarded by the fact that the competition runs in a circuit, so we have some guarantees on what is being run. But in the ideal world, you would like more formal guarantees on when is being released and more programmability for the searcher. So that's something that would be more interesting in my opinion. And just to wrap up this presentation, I think like to just a few calls for actions, call for collaborations. I think what stands out from the state of things today is that I not all the elements of the programmable privacy stack are described thoroughly. There's a recent effort in a paper by Andrew and colleagues on programmable privacy, Sok, which is a great effort in this direction, but that's clearly the thing that we need to do more often. And I think one way to do that is to basically focus on opening the black box of pets.
00:21:57.750 - 00:23:38.750, Speaker A: And yes, it's great that we have means like programmable privacy, but we need to be more precise on what we mean by privacy. And we need to not be afraid to open these black boxes and really study these methods and protocols, not in isolation anymore, but in combination for end to end solutions. In particular, we can learn, I think, a lot of by building community tools and baselines for the common use case that we've identified, and then to better understand the impact of adding programmable privacy to these use cases like dark pools and batch options, for example. The questions that remain is that, as I said, we focused a lot on input privacy, right? We've been working a lot on fhe, we've been working on MPC, on ZK. We've been leaving out a little bit too much, in my opinion, the ad privacy, which would be tremendously helpful in, for example, quantifying the amount of information leakage in cover channels, for example. It's not really clear what an attacker could do with this data being leaked, so it's something we really need to investigate further. And on the more te specific thing, I think by also opening the box of the te itself and going more towards open tes, we can not only learn more about the guarantees we're offering, but also get to a better state of security overall.
00:23:38.750 - 00:24:29.440, Speaker A: It's not crazy to think that we can see the same acceleration and development that we've seen in decay over the past ten years if we all together focus on this, on this aspect. And the last point I would like to mention is that, yes, all of that is not really about privacy. For privacy, it's really about information control. And for that we need to put the user at the center of the design and really think, always think, about what is being input in the computation and what is being output by the computation and what leaks out of that. So thank you very much for listening. Just a few references to the things I've been mentioning, and if you have any questions, I'm happy to take them now or later at the break. Any questions?
00:24:32.460 - 00:24:50.340, Speaker B: Thanks. Thanks. The talk so why do you think we don't see as much emphasis on output privacy in, in the industry? Is it that it's a harder problem or is it less marketable? Or, like you're saying, we see less of it?
00:24:53.360 - 00:25:18.960, Speaker A: I don't know if this one works. Okay, let's take the other one. I'll get this one for now. I think that's a good point. Yes, I definitely think it's less marketable in the sense that it's mostly about providing stronger guarantees and not really unlocking use cases. In this case, new use case really comes from the input privacy. That's what allows you to enter applications you would not in the first place.
00:25:18.960 - 00:26:01.240, Speaker A: And I think the other reason why this is maybe less studied, it's like it's less generic in the sense that you need to study it for every single application. It's harder than, for example, working on accelerating ZK or FHE, which is going to have an impact for the whole community. We have tools like differential privacy, but if you really want to apply them, it's not just about building the tools, it's really about making the analysis of your application. So it's more of an auditing process, like a security audit proof process, rather than building tools in this direction, I think.
00:26:01.400 - 00:26:01.920, Speaker B: Okay, thanks.
