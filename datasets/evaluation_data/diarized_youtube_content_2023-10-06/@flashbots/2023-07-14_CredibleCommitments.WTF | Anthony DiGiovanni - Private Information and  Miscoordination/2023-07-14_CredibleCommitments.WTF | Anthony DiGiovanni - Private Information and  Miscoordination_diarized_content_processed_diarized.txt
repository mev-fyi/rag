00:00:00.730 - 00:01:00.800, Speaker A: Cool. Yeah. Thank you, Jim. And Liz is not here, but he also invited me for yeah, and thanks, Jasper, for sort of setting things up here with the SPI intro because some of this work is going to supplement that discussion. Yeah, maybe I should just say a bit about generally what central long term risk does for context. So yeah, I do not know. Crypto, largely what I'm hoping can be like, mutual benefit from this is clr does some largely work that's in this cooperative AI reign, we're concerned about scenarios where there's lots of delegations to powerful AI systems and they face these theoretic traps that human societies face for a while now.
00:01:00.800 - 00:02:11.990, Speaker A: Right. So what could be the mutual benefit here? We can get a more grounded understanding how the practicalities of cryptography interact with conditional equipments that AI systems might make. And yeah, in turn we can share with you guys some updates on the theoretical aspects. That's what I'll focus on largely here. Basically, what is our ideal for rational agents? Cooperating what conditions would be sufficient for cooperation to be rational? It has this interplay with conditional permits. So, yeah, as Zin said, there's going to be both about the private information problem and conditional solution to that and the miscoordination problem as well. And yeah, this is joint work with my supervisor, Jesse Clifton, also at Clark, and I'm indebted to Casper for some discussions.
00:02:11.990 - 00:03:33.090, Speaker A: Yeah, so I want to say a bit about the scope of all this. My work is largely motivated by thinking about scenarios that we currently don't base with current AIS. As oppressive as they are, as we're seeing in every week of news about GBTS, they're still not like fully autonomous agents that humans have delegated ISTEx decisions to. But when we think about scenarios where we do have really advanced powerful autonomous agents making these decisions where cooperation failure is large, that's the kind of scope that this work is intended to apply by two. And consequently, that's why this is going to be at a pretty high theoretical level. My sort of research philosophy is like, I don't want it to be too anchored to particularities about what current models look like because progress in AI can often be quite surprising and made in directions you wouldn't expect. But we'd like to get at these kind of fundamental aspects of what are the incentives of rational agents.
00:03:33.090 - 00:04:49.480, Speaker A: But at the same time, we want to think about somewhat AI specific capabilities that might be important, like greater commitment ability, conditional commitment ability than humans may have through prospects of mutual transparency between assists, whereas humans, at least unaided by certain external commitment technologies, can't read each other's minds. Yeah. So to get as concrete as one can with this sort of thing, you may want to imagine something like auto GPT is like people attempting to leverage these state of the art large language models into agents. But imagine that for like an n greater than four, large enough that these are advanced. Oh, by the way, at any time I'm saying something that's pretty amusing to you, feel free to drop. I want this to be pretty chill and hopefully the talking thing didn't be too long. Any time for discussion.
00:04:49.480 - 00:06:03.710, Speaker A: Yeah, sorry. Let me move the new thing. Nice. So, yeah, we want to consider like, an ideal in the sense of what are conditions, where if these are all satisfied, you make it so that any rational agent is incentive to cooperate in some sense. And if you negate that question, this tells you we want to say, like, what are conditions that are necessary for there to be cooperative failure? This is kind of working paper establishes its results, but it's largely kind of a corollary of the conditional disclosure paper. I'll get into a second, combining that with talking about equilibrium selection, you get this breakdown of sort of three major reasons that rational agents might fail to cooperate are, one, that they have inability to commit to cooperative outcomes. I'll talk to the right here.
00:06:03.710 - 00:06:41.554, Speaker A: Second is like, private information problems. That's the first thing I'll go over. And lastly, miscoordination, where even if we have cooperative equilibrium, existing agents make conflicting demands for different equilibrium with the sorts of examples that Casper talked about. And this is emitting some relatively minor exceptions. You'd also need like, randomization ability. But that seems like relatively straightforward. Yeah.
00:06:41.554 - 00:08:05.448, Speaker A: And I'm largely going to bracket the commitment ability problem, at least as far as these are the kind of commitments necessary for cooperative agreements, largely because this is like the kind of thing that crypto offers promise for making credible commitments. But the tricky parts are, even when agents are able to make credible commitments, what problems do they base? That post office or cooperation. So I'll focus on these other two. I'm going to not spend too much time on private info, largely because I was pleased to hear there's so much familiarity with the idea of conditional disclosure already. That end, it seems like overall a slightly less neglected problem by ongoing research and coordination. So we'll spend a little bit of time on this and then move on to this coordination. So the problem is kind of a combination of incentives to bluff as to not accurately represent their private information and also presence of exploitability of information that you might want to otherwise want to disclose.
00:08:05.448 - 00:08:50.360, Speaker A: So I'll go over both of those pieces. In this example, let's imagine we have two AIS, GBD Alice and GBD Bob. I'll just call them Alice and Bob, the rest of all this, who are tasked with negotiating over some good. And because we'll assume that by default, the valuations that Alice and Bob have on this good are not common, but hence this is private info. Alice, of course, knows her own valuation of the good, and Bob knows his. And what I'm going to say here applies to both sides. But let's just consider one side for simplicity.
00:08:50.360 - 00:09:49.470, Speaker A: Alice has this on its face incentive to pretend to have a lower valuation of G than she actually does. I see there's discussion of this market for lemons problem earlier. It's a very similar idea. If Alice could convince Bob that she doesn't value the good that much, she can get like a much better price for it, is the idea. And a straightforward solution for this, if you had AI system have ability to share something like their ward function they're trained on would be the following. So I'll just slide that yeah, to extent that you're familiar with alignment problems, of course, like reward functions themselves. This is like the incentives you give when training, say, a reinforcement learning agent.
00:09:49.470 - 00:10:55.170, Speaker A: Those don't directly translate to what the agent is actually optimizing for what its preferences are. But let's say you get alignment going pretty well in the single agent case, such that if Bob didn't see this order function published, bob would know you could be able to check what Alice's valuation of the good actually is. So verifying the private information could in theory get rid of the potential inefficiency here. But if it's not possible for Alice just to publish just like the kind of subset of her summary of her reward function, that proves here's my valuation was good and nothing else. Maybe yeah, they just don't have that much fine variant control over what they disclose to each other. And instead this needs to be this pretty course training. Like I publish my whole reward function.
00:10:55.170 - 00:11:52.550, Speaker A: We can debate how realistic this particular example is, but in general you might have inability to not selectively disclose exactly the kind of info you want. You might want to have to disclose a bigger set of info containing that info. So the worries that, yes, publishing this word function tells Bob, okay, this is what Alison's documentation is, she's not bluffing. But other word function info that Bob learns might make Alice exploit. You could tell stories that just lets Bob figure out. Like here's some weird adversarial example that Alice has that would make her behave in pretty stupid exploitable way on some inputs. For example, in general, you can imagine, yeah, it's just like risky to share too much info about yourself.
00:11:52.550 - 00:12:31.640, Speaker A: So because Alice faces this risk of being manipulated by disclosing too much, she refuses to disclose. And unfortunately this results in inefficiency in the sense that the trade fails to happen. This is a pretty low stakes. You can imagine that eventually they can keep toggling and eventually trade. So some time is wasted. But this is just like a toy example of inefficiency get from private so far. Yeah, nice.
00:12:31.640 - 00:13:22.090, Speaker A: Yeah. And this is all again, thankfully this is already discussed. This basic insight was indented to the Myerson Salar 3D theorem. So what's like a nice capability that could resolve this problem? Conditional disclosure. Because even if you have to disclose more info than you'd like. What Alice can do is say, I'm going to commit only to disclose this reward function that proved my valuation and some other exploitable things if and only if Bob is committed to exploit me. And yeah, so similarly, Bob might want to do this symmetrically.
00:13:22.090 - 00:14:32.990, Speaker A: If this kind of conditional disclosure is available, then Alice is incentive to refuse the disclosure and the private problem goes away in the sense that an efficient Bayesian Nash equilibrium now exists just like that. It's not equilibrium when private boats involved. Basically, yeah, that's all theory of the sort of ideal solution to private info problems if you had pretty sophisticated conditional commitment ability. Yeah. In the discussion we could talk about some of the practicality, but for now I'll move on to miscoordination. So yeah, I want to start by grounding something in a case where this awesome miscoordination can show up when you have commitments being implemented with blockchain to the extent that I understand the practicalities. So if I say something that's wrong here, please feel free to correct me.
00:14:32.990 - 00:16:14.190, Speaker A: So one takeaway we got from Zinn's Talk yesterday is that you have some blockchain mechanisms or protocols where we don't have just like arbitrarily fine grained continuous time to publish and broadcast. If that were the case, then you could just say, like, whoever commits first their commitments instantaneously broadcast to everyone else, and everyone else knows, okay, I just have to follow this commitment, I kind of lose the race. But if there is this kind of minimal time between when we can have commitments that are broadcast between the agents, then there's a potential for multiple commitments to be submitted onto this protocol within that time frame. And so the second mover had some learned that the first mover has already committed before they committed themselves. So this leads to practically simultaneous commitments in the strategic sense. So again we have Alice and Bob who are instead of haggling over some discrete good, now let's say they just want to decide some allocation of this grid and it's divisible. Again, something like the demand game that Casper's talked about could also be an.
00:16:14.190 - 00:17:25.900, Speaker A: Yeah. Between T and T, plus this minimal block time, both Alice and Bob have submitted some commitments. And trouble is that if their demands of each other are incompatible, for example, Alice demands 70% of G and Bob demands 50%. So when they're incompatible, the trade doesn't happen again. We have some inefficiency. So yeah, I want to make the connection to SPI here and talk about basically some breaking sort of the two nice features of SPI apart and talking about under what conditions do we need one of these parts to make sure the first part works, that it's a high level story here. So what this SPI proposal gives us is a pair of conditional commitments that could resolve miscoordination here.
00:17:25.900 - 00:19:17.012, Speaker A: So putting things in the language of how this is described in the SPI paper. We'll say that it's like Pi of G is the way that Alison and Bob would play this original game. So deciding how to allocate this good or which demands they make in the allocation of the good and this is like the way they play this game is kind of induced by this pair of so you can think of the Pi here as this function mapping from the game to the way they play. And the function here is the pair of delegates that Casper's SPI talk was talking about implicitly, even if there aren't literal delegates that Alice and Bob have when they're submitting commitments to this protocol, the sort of virtual players that end up being created by the pair of commitments they submit is the pair of delegates involved. And yeah, so the natural way you can resolve miscoordination with the ski gear is this conditional commitment to both apply that same pair of delegates they would have applied to the original game. They're going to apply that to the SPI G of S. Just to recap, it's like transforming the payoffs that the delegates are playing according to instead of the original game payoffs.
00:19:17.012 - 00:20:28.420, Speaker A: And you can also restrict the action space. The reason I'm going through this is just to highlight that we can have an implementation of the idea I'm going to talk about through SPI, even if there isn't directly a transformation of chaos being done by these commitments. The agents submit SPI is like one way you can get the commitments that the agents submit transformed into something that creates what's? SPI oh, sorry, let's say creative improvements. And what is the definition of coordination? Like wealth and maximizing? No. So coordination in this context would be like the agents end up both aiming for the same efficient equilibrium or efficient outcome. So they make compatible demands and they end up in something efficient. And this coordination is not bad.
00:20:28.420 - 00:21:46.990, Speaker A: They predict. Yeah, so maybe I just also want to highlight, just to be clear about that, usually we think of coordination in terms of agents kind of with the same goal, but who are making decentralized decisions. And so they end up in inefficiencies because they weren't able to literally talk to each other. The kind of miss coordination I'm talking about here in these bargaining problems, broadly construed, like the demand game example or that threat game example, you don't need the items to share. So in fact, their goals can be quite different and the barriering problem comes from them having the and yeah, so last piece of this is like as Kafka said, this needs to be a conditional commitment. You play the FPI if and only if the other is also going to do that and if and only if the other is going to apply that same pair of delegates to the SPI. So I'll talk more about the significance of general.
00:21:46.990 - 00:23:59.530, Speaker A: What's the thing that we care about whether it's mediated through SPI or something else. A failsafe is like the term on where by default we have some pair or more than two agents still having this problem set of commitments that make incompatible demands or are incompatible in the sense that when they're put together with each other the commitments are run on each other. Then the outcome is what the failsafe does is maps those to different commitments that improve on original thing. And yeah, the goal of this is to reduce the downsides of this coordination. So for example, an implementation of this failsafe thing could be committing to let your delegates. So the question I want to talk about is when exactly are agents incentivized to use safes like SPI's for example, given least they have evaluation that's probabilistic in the sense of I think you're 80% likely to make this really high demand against me and 20% to make like a fair good, et cetera. And I'm emphasizing this point because so the motivating question as framed in Casper's talk was even if agents don't want to say what's the expected utility of this strategy but rather want to have some with probability one guarantee that we pray to improve, is it possible to do that? So we saw that that's true.
00:23:59.530 - 00:24:18.130, Speaker A: But in cases where just in general, we want to be able to talk about when agents realistically do have some beliefs about each other, what needs to be true about their beliefs such that they'll either want to use failsafes or may have some incentives not.
00:24:21.780 - 00:24:31.136, Speaker B: And is this adding the belief part is what makes spell safes different from the previous outcome?
00:24:31.328 - 00:24:31.828, Speaker A: Yeah.
00:24:31.914 - 00:24:32.550, Speaker B: Okay.
00:24:41.340 - 00:27:23.310, Speaker A: Right. So similar to the I guess like generalizing the problem that was also discussed in the Safe parado improvements talk about how you need to commit to use to not modify your delegates to trick to give yourself a better deal. Say in general the prima facie problem that's gesturing at or that's illustrating is that having this risk of miscoordinating, of having some inefficiency when our demands are incompatible translates to bargaining power or the other player. Meaning Bob might think, okay, if we do use a failsafe, alice is likely to demand more because she no longer has anything to lose when she protected from the downside of this coordination. And yeah, so the example in the story of DOLE's case there was if the human doesn't care about some page yeah, a page of binary digits being written out, I'm probably like botching or mixing up examples from the talk. But if humans we don't care about that, and we make an AI care about that thing such that whatever happens when it has bargaining failure with another agent is this thing we don't care about, we would have this incentive on its face to train this AI to be more aggressive in its I really want to break this down into two questions. When exactly and why is cheating a problem? This cheating thing being that Bob has this worry that Alice is going to in more because the risk and coordination are gone, or at least heavily mitigated and to when it is a problem, what are the qualitative kinds of capabilities or conditional commitments? Which of those do we need to avoid getting problems? So similarly to how, with the private info problem, we said, in general, you need conditional disclosure to be able to solve that.
00:27:23.310 - 00:29:22.320, Speaker A: What kind of conditional commitment, in that sense is in general necessary and in some cases sufficient to avoid this sheeting problem? The motivation for both of these questions yeah, cool. So what I want to highlight is that if Bob's worry is that Alice is going to demand more when failsafes are available, if Bob is rational and not just kind of stubbornly refusing to use this thing, that gives us brighter improvement. Why would Bob want to refuse to use the failsafe or refuse to commit to the SPI, for example? For that to be the case, my claim is that Bob needs to have some chance before this interaction happens, to incentivize Alice not to cheat. In particular, Bob needs to have some power to credibly commit, to ignore failsafes that are potentially cheating. Why? Because if this weren't the case, if they're totally acting simultaneously, just like stepping into this human game, then Bob's choice to commit to ignore a fail state that cheats is not going to influence Alice's decision whatsoever because it's done simultaneously. Alice hasn't seen this commitment to ignore. So in some sense, we need the game to have this extra sequential step for cheating to be concerned.
00:29:22.320 - 00:30:05.890, Speaker A: It does seem realistically that this would often be the case, though. The AIS are just, like, opt into existence and hooked up to a blockchain, and they submit some commitments beforehand. They may be thinking about like, okay, I'm going to get into potential bargaining interactions with other AIS. We may have this misplaced thing. Oh, here's this FPI thing. It sure would be unfortunate if I lost bargaining power from someone cheating at this. And so, yeah, realistically, we will have this sequential structure happening.
00:30:05.890 - 00:30:59.088, Speaker A: However, you both need the sequential structure that I'm depicting here, where there's a potential to commit to ignore the pension cheating failsafes. But you also need that Bob's commitment to ignore them can be made credible, or at least efficiently credible. If Bob just has some cheap talk of I'll ignore this failsafe, that makes me worse off. But Alice doesn't buy this. Yeah, she thinks Bob's bluffing. Alice may still insist, okay, let's use the stalesafe that cheats. Once Bob has gotten to that decision node and hasn't credibly committed to ignore, well, sorry, I should step back.
00:30:59.088 - 00:32:16.150, Speaker A: If Bob actually hasn't committed to ignore, then he's gotten to the situation of, yes, I've been cheated, but I might as well go along with this because at least it makes us better off when we miss work. And so if he anticipates that being a problem, he's only going to want to commit to ignore his failsafe. Make both sides better off if he expects this to actually influence Alice decision. Now, the last thing I want to add to this is that in these games with conditional commitments, in principle, you don't need a sequential structure for there to be influence on the other agent's decision with your commitment. It's possible. Yeah, that me. Bob is committed to ignore potentially shitting failsafes because he expects Alice to be committed to not use failsafes conditional on seeing that Bob has this commitment to ignore.
00:32:16.150 - 00:33:29.322, Speaker A: Could that in itself could happen even when there's no sequential structure to the interaction. That dynamic, I think, is important to keep in mind to get a sense of even when there's no opportunity for this preemptive credible commitment to ignore, there is still an incentive to ignore cheating failsafes. However, what Alice can do despite that is commit to have a commitment that reacts exactly the same in the sense of making the same demands against commitments that either go along with the failsake or ignore it. If that's the case, then again there's 10th, like, no incentive for Bob not to go along with tails. Maybe I should use the whiteboard just to really make the sphere.
00:33:29.466 - 00:33:33.790, Speaker B: I already take the picture, so I already took the picture.
00:34:21.740 - 00:34:46.256, Speaker A: Let's say that the climb and Bob makes kind of by default is C.
00:34:46.358 - 00:34:47.010, Speaker B: Is.
00:34:48.900 - 00:36:50.010, Speaker A: C, but follow up the failsafe so the original C, like, ignore everything else then, as long as Alice's commitment has the exact same okay, this is a bit bubly. You can actually see the improved coordination via failed safe leap conditioning programs for the technical details about this. But as long as I'll use this equality to mean like the demands in some sense are equal between Ellis's program D which does want to use the Pailsafe if DF on C makes the same demands as DF on c of F then sort of by construction alice has made it so that there's no incentive for Bob to use this C. That ignores when we use the conditional null commitment by Bob might have been his route for preempting cheating trickery.
00:36:52.750 - 00:37:02.960, Speaker B: Why would the person sending the commitment D do that? Make the same demand no matter if the other person does it or not?
00:37:05.730 - 00:37:48.826, Speaker A: Because she's anticipating that people like Bob might be worried about losing bargaining power from the shoot. Yeah. So in particular, Bob does need to kind of think before he's committing this commitment. Okay, someone like Alex is going to be pretty smart and realize that I would want to commit not to go along. Cheating failsafes all else equal, but I'm not going to refuse to go on with them. It's this property. So if Alice is smart, she'll make her commitment at this property.
00:37:48.826 - 00:39:17.216, Speaker A: And the key thing is that this is totally consistent with cheating meaning that this is a statement about the equality of the responses of Alice's committees to different kinds of commitments by bond in this program equilibrium kind of setting. But it doesn't tell you anything about how aggressive are Alice's demands relative to what she would have done if the failsafes weren't. Those are two different things. And so we can have Alice being cautious in this sense of she's not going to demand any more against the failsafe following one than the failsafe ignoring. So she can't fully cheat in this sense, but she is still free to make this demand, not the one she would have made without failsafe seal. Is this distinction clear or it's rewatch discusses? Yeah. I should reiterate that in practice we may have the sequential structure in real world.
00:39:17.216 - 00:41:05.900, Speaker A: So as far as we expect possibility of committing to ignore cheating failsafes to be present, I think that's sufficiently likely that agents who do want to avoid miscoordination would not want to. In general, you don't want to take the risk of cheating. So a reasonably cautious yet non exploitable thing to do is to use a non cheating failsafe, like how SBIs are constructed when you're like. We think it's reasonably plausible that this incentive this is mainly to highlight when does this intended exist and why is it the case? Because this will help us say what capabilities we need, like when initial payments to avoid the cheating. So what are those capabilities that you need again, as a generalization of what an SPI is doing? What we need is this conditional commitment to only use the failsafe if the other side applies it to their default not cheating. In this diagram, just looking at the bottom two thirds with a rectangle and circle, this is the classic program game story. I have a program that makes me output in action conditional on your program vice versa.
00:41:05.900 - 00:42:09.040, Speaker A: But what we've added is also programs that can condition on each other's beliefs. And the reason that's useful is that I claim in general necessary. There's like an example, a proof of this kind of result for Sonic classic games in her paper. Yeah. The necessity of belief conditioning. And what helped makes it also sufficient is that pL2's beliefs give P one all the information P one needs to say. Like, is this the program that P two would have wanted to submit if I did commit to ignoring because the beliefs tell you, okay, what would Alice have expected me to play? How do we not use failsafes? Therefore, what would her best response with this program game structure? What would her subjectively optimal thing to do have been under those beliefs? So belief conditioning helps us do this whole conditioning on you not cheating.
00:42:09.040 - 00:43:28.846, Speaker A: Sorry, this is a bit taking a bit longer than expected, but I really just wanted to make this clear. Yeah. What this is doing in the SPI framing is that this part where Alice and Bob commit to use the same delegates as if SBIs weren't available in general, when they're allowed to pick more Hawkish delegates anticipating this or give extra instructions to the delegates, that kind of introduces cheating as Casper described before. The term I'll use for committing to not cheat in this sense make the same depends as you would on Vita Paul. We'll call that default and yeah, belief and conditioning lets you verify rather than just taking it on the agent's warrant. Yeah. Okay, last punchline here is if you can credibly commit to ignore bailsafe.
00:43:28.846 - 00:44:46.858, Speaker A: So we do have this sequential game structure I described a few slides ago. You can prove this itself is not in the Failsafe's paper currently it's ongoing research formalizing this, but yeah, you can prove that this default preserving property plus some conditions on the player's beliefs about each other. Basically the kind of beliefs that I was talking about here. I expect Alice to be reasonable in the sense that he's not going to make unequal demands here as a function of my fail state using and fail state ignoring commitment. If they're police satisfy that property and we have the default preserving conditional commitment, this makes the failsafes individually rational. Yeah, I'm happy to explain formally what that means in the discussion. So some ongoing research is like getting this general sense of if we have different ways that layers beliefs might be updated by learning something about each other's commitments.
00:44:46.858 - 00:45:11.240, Speaker A: For example, are they committed to use salesafes when are failsafes still rational? Cool. That's all for the main substance of the talk. I'm going to put up some possible discussion prompts, but if people have questions on the stuff I talked about directly, I'm happy to. Thanks.
00:45:19.500 - 00:45:26.668, Speaker B: Like in the paper, did you mention anything about verifying the beliefs? How realistic do you expect that to be?
00:45:26.834 - 00:47:05.390, Speaker A: Yeah, I think this is probably a pretty hard problem. So largely the motivation with the paper was just to emphasize what is the property that you need to show this is an open problem. It's not clear that we can do this with current tech. Yeah, because as far as I understand, we don't have human bits that can read human designs. Fortunately, if we have AI systems learning this kind of thing and we have some kind of soft commitment power used by humans to delegate to those AIS without influencing how they bargain too strongly then the hope is that if these AIS are sufficiently autonomous and can't be gained by the humans who define them, then the AIS who are implementing failsafes could condition on each other's beliefs by looking at parameters of neural networks. I do expect it to be challenging to verify this. I don't know how portable verification works, but in principle it's like the high level story is like AIS can have more transparent to each other minds than the humans have, at least.
00:47:05.390 - 00:48:11.490, Speaker A: Yeah, yesterday we were talking about simulation as. A means to verify. So if we're treating we're considering the beliefs of the network then we can pretty much simulate network. Right? So we would simulate if we define a headlight then what would this network which is kind of a way to build the model or believe software. Yeah, that sounds right to me. I think I think maybe the challenging part is you can't just when you're simulating you need to be able to say what do I intervene on in simulating these beliefs? So what input do I give to this network so that this corresponds to believing that failsafes will be ignored. So you can check that's the case.
00:48:11.490 - 00:48:28.020, Speaker A: Yeah. Maybe this is more chopper than I expect. Which would be great. I'm informal about that. Practicality. Yeah. I have a question maybe some of you answered.
00:48:28.020 - 00:50:15.204, Speaker A: So in case of beliefs so the way our understanding is some probability over some space, right? The beliefs the way I understanding is as passing probability distribution space. But then this is somehow like a continuous object, right? You assume that this is being given as an input. So how important is in all this framework that I know this with perfect accuracy? Yeah, for the formal result you need the condition on the exact beliefs. But I think in practice there's some slack here because for one thing because the failsafe provides a pareto improvement unless it's like a really trivial one the agents would think there's some margin of error here. As long as I still expect to gain a lot from sweating a stormmate then I can tolerate break some chance there being low fidelity, lower than perfect fidelity in the leaf conditioning. But yeah, more to the point yeah, I suppose you need enough resolution to be able to make it so that you have some guarantee that whichever beliefs you kind of feed into this mapping that tells you what the default program would have been that this is close enough to the real world. And so maybe there's some interesting work to be done in getting clarity on how continuous function is.
00:50:15.204 - 00:50:23.220, Speaker A: I guess if you break things slightly do the default programs wildly change? Yeah, I have a bounce.
00:50:27.800 - 00:51:00.130, Speaker B: We know that the resolution of program equilibrium, some of the methods that is for example the Goddelian numbering stuff makes the resolution really brittle. As in the brittleness is kind of leading to a catastrophic outcome and such things. Can we combine them with, say, failsaids and SPI to say oh, if in our program resolution greed hulks, we fail, can we renegotiate, whatever.
00:51:01.400 - 00:51:57.990, Speaker A: Yeah, I think that's a cool idea. I haven't thought of that, but I think it feels like less central than agents having incompatible demands and that's why it is incompatible. Why there's a miscoordination. But yeah, if they're both aiming for the same equilibrium but just the implementation is too brittle. Yeah, maybe failsafes could help with that too. You might still have this regress of are the failsafes also? Yeah so in particular how failsafes would have to work or like how SPI would work. And as a big example of this is you need to be able to reading this code to say are you committed to play this SBI as well and if the way that's written is also really brittle then it's also not going to work.
00:51:58.440 - 00:52:42.980, Speaker B: But I mean in reality I can kind of see this as kind of levels like social consensus, right? Maybe everybody kind of agrees deep down thing all happens say like we both want to solve this program in Burbria and brief box and we can kind of all agree that at least we need to solve to something that does not lead to us all be fat team. We are the same negotiation protocol and that's like the same negotiation that is easier to dabbish than the specific programming theoretical commitments that we are trying to play in that specific game like DC.
00:52:43.720 - 00:53:34.090, Speaker A: Yeah tentatively that sounds promising. Yeah I think that's a nice aspect of this discussion is like I'm boxed in my theory land of here's the central motivating problem but maybe missing other practical cases where this is important. Speaking of getting more concrete, I'm curious to dig into some of the stuff on here. I guess we talked about this issue of you get missed coordination from sort of a lag time between when it's a ROTIC apps. But if anyone is burning to share some other example you've seen in the crypto space where this would be relevant and need to hear that.
00:53:38.380 - 00:54:09.760, Speaker B: I kind of want to the programming resolution. Can we use SPS to mitigate that problem? Yeah over there. If realistically, we want to implement some kind of programming cryography. If systems, let's say, actually on blockchain two, users commit to some transaction, either a smart contract or some transactions.
00:54:11.460 - 00:54:11.872, Speaker A: That.
00:54:11.926 - 00:55:15.144, Speaker B: Depend on other users committed transactions. They simultaneously do the commitment. As their commitment are like interdependent as they want to play some specific game. Let's say I want to settle some trade against you like I want to trade asset A to SIB. If you commit to trade and then we set hardware trades then we really don't want the brick on this to happen because maybe the brick on it will close some trade but at least we want to trade just maybe we want to negotiate whatever the price returns. Yeah so in that example I would imagine maybe we have some already established a little consensus on that. If this first level programming resolution fails, then we all use this real time negotiation on Blockchains to further negotiate the price.
00:55:15.144 - 00:56:25.340, Speaker B: But then to compute or to simulate those negotiation process, it costs a lot of computational resources. Computational resources, as we know, is not that it's kind of scarce. Blockchain do you see any way that we can further layer this process? Maybe we compute the first level programming grew resolution, some broad blockchain and the negotiation process is taken care of by a smaller set of nodes. There's some design throughout there. Welcome. And for the blockchain people here, I mentioned this because in a lot of the smart transactions, especially Vlad initially mentioned, one of smart transactions usage is real time user negotiation. And real time user negotiation sounds a lot like Failsis and SPS where the commitment of the protocol is a negotiation protocol between users.
00:56:25.340 - 00:56:37.280, Speaker B: I don't think Lab has a specification or has thought about this know I trust there are some interesting ideas to swallow.
00:56:39.220 - 00:56:40.192, Speaker A: There you go.
00:56:40.326 - 00:57:01.976, Speaker B: You mentioned one slide, what Shin was talking about yesterday, where you have two in combat in the commitment. But I don't really understand how that ties into what they matter. And I feel like this has to do with this real time negotiation problem. Incompatible simultaneous commitments. Can you maybe just draw that link again?
00:57:02.078 - 00:58:54.210, Speaker A: Oh yeah, I guess I could literally draw it, but I don't want to get rid of this one. And the one comes down there where we go here's. Yeah so the I mean the argument is that if like at time T and this time T plus looks like a T as well little T is our block time. So in this region there's no broadcasting that can happen between those two times as I understand it. So because there's no broadcasting, you might have like Bob submits a commitment here Alice does here Bob's might be to demand 70% alice's kind of unconditionally. So basically because there's no broadcasting, alice doesn't see that Bob has committed fear and so makes any whereas like if there had been a continuous time for broadcasting and Bob's commitment were broadcast, alice could just say I'll go because the demand has been made. It's incredible.
00:58:58.390 - 00:59:31.150, Speaker B: So then the argument was that if I broadcast my commitment along with a pay safe in case the commitments don't resolve or the brittleness makes it that we don't find the efficient outcome. So if I make the demand but also I have this pay safe that's embedded in my commitment, I'm able to edge against situations where I might get into outcomes where the commitments don't result and then with the paysafe I can achieve the SPI outcome.
00:59:36.370 - 01:01:09.446, Speaker A: How would the look back in the scenario? Yeah so there is this linear question of which field saves use? Exactly. Number one voice cracker. So this is something that Casper's Kicker gets into that you kind of have a SPI selection problem in the sense of how do the agents agree on the failsafe? We'll suppose that there's yeah suffice to say in some scenarios there's kind of like either like a unilateral SBI you can make or it is bilateral but sort of neither side can game it and so they're all incentivized to be skull on with that failsafe. Even if they don't get all the leaf grandfather frontier because they still have to solve SPI selection. The sales paper gives an example of this. But yeah, I think in general, this second order coordination problem crops up, but there are ways around it. So it's like say that that's been solved, then you could have liked the yeah.
01:01:09.446 - 01:02:02.000, Speaker A: How would this work? It yeah, in principle, like if this is like here, 100% goes to Alice, you're 100% ghosted Bob. This is fine. I see. And the freight of rockier is there. Oh, them. If what we simulate happens when they don't trade is just like go down here, then the failsip could just kind of map this outcome to anything whatsoever. They kind of easy focal point to coordinate on would be like something that's fair in some sense.
01:02:02.000 - 01:02:43.844, Speaker A: It could also oh, yeah. Maybe I should just illustrate the thing I was talking about, the example given in the paper, because this might just make things more concrete. Sorry. Just in the same position. Like what? Something like I get the fret, you get the bird and the fred is burned and why they burn it. Exactly. We all dialtic.
01:02:43.844 - 01:03:25.770, Speaker A: Right. 50% pestate would be equivalent of just saying we never do anything that person. So we kind of want to have lost everything on the part of Frontier doing business. At first, it's just a crypto way of saying it on the Frontier. This is some pot. Sure. I think what I want to say is that burning might make sense if the reason you end up birding is yeah, they can't agree getting all the way to Frontier, but why do they? Staff bar is my question.
01:03:25.770 - 01:04:24.262, Speaker A: And so this thing I'm going to illustrate if I can remember this properly and give you a principle way of doing that. So let's say one of the optimated one here, .7 bob in three Alice that's up here. Else's demand would like, slip unfairly. So it's up here. And what they do is go to the intersection like these, these lines that are like perpendicular to these. Um why is this nice? The reason is that, like, you can't game this by demanding more for yourself if Bob instead of having 0.7
01:04:24.262 - 01:05:09.960, Speaker A: went to 0.9. And so this is where this other demand made by Bob but Alice's stays the same, then the point goes down here and Bot hasn't profit at all. So the nice property here is that each player's allocation depends on the other player's demand, not on their own. But it's still a cradle agreement on this one. I didn't come up with this, by the way. It's this blog post called cooperating with agents with Different Notions of Fairness Exploitability. Something like that.
01:05:09.960 - 01:05:16.630, Speaker A: Yeah. I really would have liked to cite an academic source on this in the paper. But listening.
01:05:19.790 - 01:05:24.010, Speaker B: When you have there's not like one attempt.
01:05:25.790 - 01:05:26.540, Speaker A: Yeah.
01:05:29.950 - 01:05:34.640, Speaker B: We won't have the speaker. Oh, wait, is that the yeah. Okay.
01:05:35.410 - 01:05:56.794, Speaker A: We do have this to this number as well. Should we? Yeah. Nothing but crypto.
01:05:56.842 - 01:05:58.550, Speaker B: You cipher's threats.
01:06:01.610 - 01:06:12.360, Speaker A: Like in regular, like this thought, but in an academic paper. Yeah, I guess we're probably at time.
01:06:15.700 - 01:06:23.020, Speaker B: Yeah. Okay. It's.
