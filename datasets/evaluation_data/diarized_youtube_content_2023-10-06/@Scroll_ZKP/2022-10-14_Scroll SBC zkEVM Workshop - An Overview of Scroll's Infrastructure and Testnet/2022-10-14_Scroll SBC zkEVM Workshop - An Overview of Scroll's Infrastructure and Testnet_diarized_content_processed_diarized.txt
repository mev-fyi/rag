00:00:07.240 - 00:00:25.064, Speaker A: Yes. Hello everyone. I'm Hy Chenshen. So before I go into our talk, I just introduce a little bit on our program schedules. You already have the schedule program in your hand. So Sandy just gave like an introduction to the scroll. Next, I'm just going to talk about our infrastructure designs and a little bit our testnet.
00:00:25.064 - 00:01:07.110, Speaker A: And I was just trying to do a live demo. And after that, Yington from the dcache, they're going to talk about the programming in the hello two and then some latest progress. What happens in the hello two? And then Mason is going to talk about our ZKe EVM architecture and how we build a construct SDKE EVM. And then after the break, I'll just then come back and talk about some performance optimization we have been done for the ZKE event. Okay, so let me get started. And also, we don't just want to take this very formal presentation. So you can feel free to ask me, break me, like interrupt me with any questions.
00:01:07.110 - 00:01:48.866, Speaker A: I will just. Very welcome. Okay, so I'm going to talk about our score's architectures. Yeah, just one sentence to summarize what score is building? Score is building an EVM equivalent of Zkrop solution L2 chain to scale the Ethereum and then to bring a lot of more transactions and lower transaction gas cost for the Ethereum so we can bootstrap next billions of users. And then before I dive into some technical details and then our architecture designs, I just want to first talk about our design principle that drive the decision of our design and why we choose this approach. We want to build. So there's like four principles.
00:01:48.866 - 00:02:23.860, Speaker A: And the first and foremost, which is the most important thing, is what we think. We believe that ensuring the user security should always comes first. And then for scroll, this means that the L2 transactions should have and share the same security level as the transactions are happening in the Ethereum today. And then without trusting even the L2 operators. And then during the design, we should never trade off any user security for any efficiency or any other reasons for that. Just need to guarantee the user security. User can always maintain the access to their phone on the L2 or layer one.
00:02:23.860 - 00:03:16.954, Speaker A: And then second, we think an effective EVM equivalent solution, sorry. An effective ethereum scaling solution should allow users and developers like a smooth path, like a seamless migration path from the existing dapps and then existing developing tools. And I would believe that maintaining the EVM equivalents is the best way to achieve this goal. And then third, the decentralization is one of the key aspects of the blockchain. Usually it's like the overlooked or improperly traded for some efficiency reasons. But we think like decentralizing is one of the key property and that we shouldn't maintain even at the L2 context. And also the decentralization can guarantee a protocol to be very resilient to any censorship or coordinate attacks in the scroll.
00:03:16.954 - 00:04:12.094, Speaker A: We consider that decentralization across many different aspects and different layers, including the sequencers, the provers, and then the community of users and developers. And then fourth, for a user to enjoy a great user experience on the L2. So believe like there's two importances we need to achieve. One is that the transaction fee on the L2 should be way cheaper, like the orders magnitude cheaper than the base layers. And then the second is that user should experience some instant pre confirmation on the L2s and a reasonably fast finality on the layer ones. So this both requires a good efficiency you can achieve in the L2, so that to be able to user to have a great user experience. Now next, let's take a look at see how we use these design principles to guide our decisions and our approaches.
00:04:12.094 - 00:05:13.138, Speaker A: So first, ensuring the user security and maintaining the EVM equivalents lead us to a ZKe EVM based Zkrop solution. So first, the Zkrop is guaranteed like a very great security by relying on the mass and the Xeon large protocols on the behind. And then the ZKE EVM which achieves the EVM equivalency is the holy grail inside the Zkrop solutions. And then you can maintain the EVM equivalence, meaning that all of the user experience and the developing tools will be just directly compatible with the L2. When you have the ZkevM and then ZkevM, because it's a Zkrop solution, so you still have to share the security guarantee from the Zkrobs. And then second, the decentralization leads us to design a decentralized approval network. So when we started designing the ZKE EVM solutions, we quickly realized that putting the Ethereum virtual machine into the ZK proof will lead to a resulting a great overhead for generating the proofs.
00:05:13.138 - 00:06:47.810, Speaker A: So in order to reduce the time to finality on the L2s, we decided, so we should build a decentralized proven network that are going to help us to generate the leaky proofs. So there are two main benefits for having a decentralized proven network. Because we have this seamless, permissionless decentralized proven network, any community member can come and become run a proven node and then the community will be incentivized to build a better, substantially better solutions, including both hardware solution or software solution to boost up, to help reduce the proving time so that we can have better solution to generate a ZK proof. And then second, when we designing our proving infrastructure, we make sure it's highly parallelizable, meaning that if you want to scale up, we can very easily scale up by just adding, simply adding more proven nodes into our network so that I can achieve very great throughput for generating the ZK proofs. And then third, so for the efficiency, so to improve the efficiency of the L2s. So our approach is to leverage all of the innovative research driven solutions across all of the community. So our Zkem designs takes advantage of many recent breakthroughs, including the zero notch proof systems, and then the proof aggregation, and then the very innovative hardware acceleration solution.
00:06:47.810 - 00:07:35.694, Speaker A: And then second, so we also focus on to develop in the open source manner, so that this allow us to collaborate with a lot of open source communities, including the PSE team from the Ethernet foundation and other open source collaborators and contributors. And then we believe that all of the community contributed to this open source approach. So can lead to a best and then most effective and efficient solutions for the Zke, EVM and the Zkrop. Okay, next I was going to talk about the scroll architectures. So first I was just going to talk about the basic modules and components that happens inside the scrolls architecture. So first you have like layer one chain, which is the Ethereum, and then the L2 is the scroll chain. So in the Ethereum there are mainly two parts.
00:07:35.694 - 00:08:30.258, Speaker A: There's two contracts deployed on the layer one. One is the bridge contract mainly for bridging any assets or relaying any messages between the layer one and L2. And then actually this bridge contract will be also there's one part of bridge contract deployed on the layer one, and one part of will be deployed on the L2. And then the second contract deployed on layer one is the Rob contract, which we are going to accept the data availability transactions that are going to include some block data and the transaction data inside the Rob contract. And also include a solidity verifier to verify the ZK proof that going to be sent to the Ethereum for the finalized any block. And on the L2, the main functionality is coming from the scroll node, which is responsible for generating L2 blocks, accepting RPC requests and also relaying the messages and also generating the proofs. That will be all happens in responsibility for the scroll node.
00:08:30.258 - 00:09:28.814, Speaker A: So inside the scroll node there are three modules. Mainly the sequencer is basically just processing the L2 transactions and generate L2 blocks, also processing any RPC requests. So our sequencer implementation takes a fork of the Go Ethereum implementation with some minor tweaks for the L2 purpose. And then because the Go Ethereum is the most popular execution client for Ethereum node, so by directly forking based on the Go Ethereum, so that gives us the best compatibility with all of the existing tools. And then all the RPC interfaces will be the same as when you interact with the Ethereum. And then second, inside the score node, there's a relayer, which is going to monitor all of the smart contracts we deployed on layer one. For example, if find any deposit event happens on the bridge contract, it's trying to going to relay this deposited message to the L2s.
00:09:28.814 - 00:10:17.794, Speaker A: And also the relayer will be responsible for submitting any row up data to the row up contract. And then third, there's a coordinator here inside of gonode. The coordinator is going to watch for any new blocks and getting the execution traits from the Goyasm. And then once the coordinator receives a new execution trace for a new block, they're going to dispatch this execution trace to one of the rollers inside the roller network. And then just ask the roller to generate the ZK proofs. And then the roller, then if it finishes the ZK proof generations, it will just send it back to the coordinator. And the coordinator just will relay that to the relayer for the roll up to row up the validity proof and then inside the roller.
00:10:17.794 - 00:10:57.940, Speaker A: So can talk about roller as well, because roller is many contact with the coordinator. So the roller is basically running a ZKE EVM inside. And then we also expect the roller to be run on a hardware accelerator in order to reduce the time to generate a ZK proof. Because the ZK EVM is quite heavy ZK circuit and very complicated. So if you use the accelerators, you can be reduced the total time to generate a proof. And then right now we are using a GPU prover to generate a proof. But we expect like the community and new hardware companies can come up with new accelerators, can further reduce this proof generation time.
00:10:57.940 - 00:11:49.028, Speaker A: And then all of rollers basically is a prover node. And then we have this decentralized approver network where is run by different community members in the future. Any questions so far? Okay, so we can also take a deep closer look into what happens inside the roller when you're generating the proof, because it's also just not very simple, just running the ZKE EVM. So first the roller will receive this execution trace from the coordinator and what's inside this executing trace. It includes the following things. First yes, the execution step. When you execute transactions inside the Ethereum machines, for example, you will execute all of different opcodes like add, multiply, or like push and pop to the stack and then read the right to the memories.
00:11:49.028 - 00:12:39.692, Speaker A: And including also like you're getting some any block information from the block headers. And the second thing, you will include the block header and the transaction data going to be the input to the execution trace. That's when you need to get any information from the block header and also what transaction data, like core data you have. And then third, we also need to load all of the contract bytecodes to make sure that we are actually executing the bytecodes actually from the deployed contracts, not like from arbitrary opcodes. And then last is there's a Merkle proofs. They're going to prove, like help you to prove the Merkle tree transition from the old state route to the new state route. And then after the roller received this execution trace, it will just hand this execution trace to the module called circuit input builder.
00:12:39.692 - 00:13:42.104, Speaker A: This circuit input builder is going to transform all of the traits output from the gosm and then assign into the certain circuits which will be a 2d giant matrix. Probably I'll get more idea like after Yington and then Mason talk about more about halo two. So you're just going to have a few giant two dimensional matrix and then you're going to expand all of the execution traits and then also generate some auxiliary data that helps to generate the proof and then fit into all of the witness in those circuits. And then next when we talk about Zkevam, actually it's not a single circuit, it includes a set of circuits that works together to generate the proofs. So because if you have everything inside a single circuit, then it will lead to even greater overhead. So that's why we separate them into different parts of circuits and then connect them through some lookup tables, which I think Mason will talk more about that in his talk. So this gives you some flavor of the circuits inside the ZKE EVM.
00:13:42.104 - 00:14:45.780, Speaker A: There's like an EVM circuit, it's the main core circuit that proves the transition of this virtual machine state from the previous step into the next state by applying some of the opcodes. And then this ram circuit was going to constrain any read write operations happens inside the Ethereum virtual machine, including the stack, push and pop. The memory read and write and then the storage modification and then the storage circuit is actually another auxiliary circuit that's going to prove the state transition using the Merko proofs we talk about in the executing chase. Apply those merkle like the proofs for every change, including read and write on top of inside the story circuit that can prove the state root exactly transit from the old one to the new one. And there are also a few other auxiliary circuits I won't just talk about here in this talk. And after you run all of the circuits, so each circuit will actually execute one proof. So there will be like a bunch of the proofs you will collect from executing the ZKEVM.
00:14:45.780 - 00:15:37.000, Speaker A: But if you're uploading all of the circuits into the layer one and then validate them one by one, then it definitely will be resulting a very large gas cost in the layer one. So in order to reduce the gas cost inside the layer one verification. So we need to further using an aggregation circuit that aggregates all of the proofs from the ZKE EVM and into a single aggregation proof. And then here we just call it a block proof, which corresponds to a single block. So the aggregating circuit, what's inside aggregating circuits, it just replicates the verification logic and then just represented it inside a ZK circuit so that it can be redo everything like you need to do inside a verifier. And then you can do that inside the circuit. And then you'll generate a new proof that once you verify this block proof, you just basically verify all individual proofs inside the ZkevM circuit.
00:15:37.000 - 00:16:37.450, Speaker A: Okay, so this is what happens inside a single roller. But usually it takes longer time to generate a single ZkevM proof for then generating a block. That's why we'll have multiple rollers that's going to run in parallel to generating the block proof for different blocks. And then if you want to further reduce the overhead of the verification on the layer one, so you can reuse this aggregating circuit and then to aggregate those block proofs again into a single proof, so you can only verify a single aggregation proof that can verify all of the blocks inside in this aggregation proof. Okay, so previously I just described all of the modules inside our architecture. Next I'm just going to show up how that works in actions when to do inside the ZK drop. So in the Zkrop, like the overview is like you have two chains, one is the same chain, the one is the L2 chain.
00:16:37.450 - 00:17:26.190, Speaker A: Here is a scope. The L2 chains can generate more blocks than the layer one. So like the one layer one blocks maybe within that time period, like 15 seconds or 12 seconds after the merge, and then you can generate multiple L2 blocks. And then what you're going to do is you're first going to aggregate all of the blocks within that time of period and then can just send up all of the data like the block data or transaction data, and then rob sent to the next block inside layer ones. And then after a while, because the proof generation takes a longer time, then you can have an aggregation validity proof that aggregates the previous batch of the blocks. And then you can send that proof into even later blocks inside the sim. And then after that the block basically can be finalized.
00:17:26.190 - 00:18:24.386, Speaker A: So with that you can actually have the different status of the L2 blocks. So first, initially the block, once a block is proposed by a sequence and included in the L2 chains, we call it a pre committed status. So when a block is pre committed, this block is not canonical part of the L2 chains, because nothing has been settled like sent to the layer ones. But you can still do some actions optimistically on top of the pre committed blocks based on L2 and the second. So once the block is generated on the L2, so you can collect all of the data you need and then just send that posted to the row of contracts on the deploy on the ethereum. And after that transaction is confirmed, then this block can be translated from the pre committed status to the committed status. And with that you have more confidence because the data of your transaction means like it already be written into the layer one, so no one can modify that again.
00:18:24.386 - 00:19:07.678, Speaker A: So it is only seeing like it's not sure if I committed status. You are not sure that your transaction has been executed correctly on the L2. So in order to finalize a block, you need to then use the ZKe EVM to generate a validity proof of all of the L2 blocks and all of transaction like steps will happen. And then you send that validity proof to layer one and then be verified by a verified contract. Then that means this block has been finalized, and then that block has been finalized as a canonical part of the L2. So to see more concrete example in the context of a scroll. So first you have the block generated by the sequencers, and then you will roll up the data from this block and then send up to the rob contract.
00:19:07.678 - 00:20:00.634, Speaker A: And then after this block has been confirmed, transaction been confirmed, then the block status should be transit from a pre committed block to committed block. And then meanwhile you also will extract this execution trace from this block and then send that to the coordinator and then being folded to one of the roller for the proof generation. And then you can do that for every like the new blocks you get. And then there are multiple rollers, it's going to generate the proof for each block. And then after a while the coordinator will collect the block proof from those different rollers together. And then you can further create a special task with all of the proof block proof and then send to another roller for this aggregation proof generations. And then after that the proof generation, the aggregation aggregate proof generation has seen down.
00:20:00.634 - 00:21:04.546, Speaker A: You receive this aggregation proof and then you can send that proof into the layer one roll up contracts, and then that verification contract will take all our previous data. Like you roll up to the contract as the public input to this verifier contract so that you can then finalize all of the blocks that be included inside this aggregation proof. Okay, so I'll probably quickly go through how the bridge works. I think running a little bit late, but I think how does bridge work is quite standard for the layer one to L2 transactions. So the user just interacts with the bridge contract inside the layer one deploying ethereum. So you do call like deposit token function inside the bridge contract, and then that bridge contract will just emit one event inside the bridge contract, say like someone just deposited some tokens and then the layer one bridge contract will just freeze the token whatever token user transferred to the layer one contract. And I'll just freeze inside that contract.
00:21:04.546 - 00:22:11.498, Speaker A: And then the relayer will just monitor all of the events generated from the bridge contract and wait for a few confirmations on the layer one. And after that it will just then send a transaction to the, send a new transaction in the L2 bridge contract, and that bridge contract will then either mint a new token for the ERC or just transfer some ethereum ether to the user's L2 address. And then second, on the other hand, if you want to withdraw any tokens from the L2 to the layer one, you interact with the L2 bridge contract call to withdraw the tokens. And then the L2 contract bridge contract basically will also emit an event that relayer we're going to monitor but at this time. So relayer does not no longer wait for L2 block confirmation, it will wait for this block that, including this withdrawal transaction to be finalized in the layer one at that time. Then it will send out this transaction to the layer one bridge contact, which then finally will transfer that token to the user's layer one address. Okay so for our pre alpha testnet, so we have like score layer one.
00:22:11.498 - 00:23:11.280, Speaker A: So we deploy as a fork of the ECM with the proof of authority based consensus and score L2 which is a zero knowledge rock deployed on top of the school layer one. And right now we are running five provers like a small cluster on the back. As I talked before, we only have the block proof. So we just, meaning like we just rock every blocks, generate a proof for every block and then just rock that block proof to the layer one. We have the down finished aggregating multiple block proof into like a single proof and then we have a few pre deployed applications on top of the pre alpha testnet. So we have faucet application on top of the score layer one and then second like we have a bridge contract on both layer one and L2 for any relaying for transfer tokens between layer one, L2 and then a swap application that is a fork of the uniswap v two. That's going to give users ability to swap tokens and also provide liquidity on the L2.
00:23:11.280 - 00:24:06.238, Speaker A: And then we also have developer, a small like row ups explorer that's going to show the block status whether it's being pre committed, committed or finalized on the L2. So you can see all of the block numbers and then corresponding block hash and then whether it's finalized or not. And they're also going to show this transaction hash that's going to like for example the commit transaction hash will corresponding to the transactions that are going to roll up the data and then finalized transaction is going to correspond to the roll up, the validity proof to the data one. Okay so before I go into the demo, so there's like also you have sign up to become an early test and contributor. So there's like a sign up at school IO so can still sign up. And then we are going to allow every day allow a few hundred people to come and test this testnet. And also we are hiring this like a page for hiring page.
00:24:06.238 - 00:24:10.700, Speaker A: You can find all of the open positions on our website and.
