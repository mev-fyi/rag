00:00:00.170 - 00:00:22.000, Speaker A: That's how you see who's like. Does anyone else have any questions? There's actually quite a few of you out here out of the light. I got a microphone. You can talk into it. We won't bite. Hey, Bart. Got one right here.
00:00:22.000 - 00:01:19.922, Speaker A: Can we take this off with unity and hear some of your takes, hopefully spicy, on the current state of superchain? I mean, you've been working for optimism for a while, right? On fraud proofs and whatnot. And I was kind of intrigued by your comment on the current opistack architecture, and I think it relates very much to interoperability and whatnot in the future. So how do we go from where things are today to the vision that you presented? So I'm not sure how much I'm allowed to tell, but I can tell you that optimism is now. Well, op Labs is now working on the. Oh, okay, there we go. I think they have a solid technical roadmap for it. It's mostly things that you could imagine.
00:01:19.922 - 00:01:53.374, Speaker A: So they're not going with something like wild and unproven. So they're staying really safe. So there's space also for new research to figure out new things. I think what's missing from what I was presenting is more like the standardization decommoditization layer. So they will provide a way to do the inter op, basically, but it will still be up to us to make the standards and to have people agree to the standards. I want to say something else. Yeah.
00:01:53.374 - 00:02:36.910, Speaker A: Also, I think at the moment, they've been very happy to let the roll up as a service, take the mantle of letting people run the chain, and I would encourage them to make them easy for people to actually run the chain. And I mean a certain way. Optimism is a money headed beast, right? But governance at least is aware of the problem. That's why I got this grant for the Europe tool. But it would be great if upstream, like oplabs makes it easier for us to keep up with what they're doing and make deployment super easy for anybody, and not just, like, conduit Caldera and all these guys. Yeah. Thanks, Barte.
00:02:36.910 - 00:03:02.754, Speaker A: Anyone else? Let me check my time because the battery on my watch is dead. We got five minutes. Give it up for. Dude, I forgot your name. Norswap. Sorry, it's too early, man. Wow, you're jacked.
00:03:02.754 - 00:03:36.450, Speaker A: You're like a brick wall. Thank you, everybody. Have a nice day. Almost broke my hand. Thanks for everything, man. Who wants a turkish chocolate? Is anyone, like, hungry? Want some glucose? It's caramel. It's locally sourced artisanal chocolate.
00:03:36.450 - 00:04:02.380, Speaker A: Oh, dude, this is going to be challenging. All right, are you ready? See if my USA college pong days are. Here you go, dude. Short. I screwed up, so I was in a frat, but I never won the pong tournament, as you can tell, because now you have to go look for it. But, oh, he's like, really looking for it now. I feel bad.
00:04:02.380 - 00:04:23.460, Speaker A: We're kind of in like, a minefield if you. Yeah, give it up for my guy, the chocolate man. Look at that, dude. You got a clap. A round of claps for that. Yeah. Let me see if our next guy is ready person.
00:04:23.460 - 00:05:12.454, Speaker A: But how are we feeling? Did any of you go out last night? Raise your hand, let me know. My guy right here with the glasses. Anyone have, like, a nice dinner yesterday? You did. You did, too. Investor dinner? Or like, what was it? You guys just went out? Went out on the town, had some kebab. Ragu, ragu here. What about you? What? There was an even an after party.
00:05:12.454 - 00:05:45.466, Speaker A: So you stayed up late. You did, dude, you're here. You made it. I went to bed at like, midnight, and I still didn't come here on time, but I made it before. But I asked yesterday because you guys seem like new faces. But is it anyone's first time here in Istanbul? First time in Turkey. Okay, cool.
00:05:45.466 - 00:06:08.418, Speaker A: I'm going to ask again later because it's really interesting. What about, like, are you guys. Anyone from Europe? America? America? Where at? New York. What's up? I was born in Jersey. What about Asia? Anyone here from. Okay, okay. Got a few.
00:06:08.418 - 00:06:23.240, Speaker A: This is crazy. It's like, all over the world. We got a dude with a cool PFP coming up. Odysseus. I hope I pronounced your name right if you're here. Are you here? Hey, guy. Come on up.
00:06:23.240 - 00:06:44.510, Speaker A: GM GM. Thanks for being here on this fine turkish morning. By the way, if you guys have a moment to go outside and look at the. It's really, really beautiful. I don't know what company you're from. Odysseus. Where are you from? Greece.
00:06:44.510 - 00:06:59.154, Speaker A: But your project name, Pilax. Come on up, man. Don't be scared. I won't bite. Give it up for Odysseus, guys. For Pilax. L two.
00:06:59.154 - 00:07:09.320, Speaker A: Protocol monitoring is an MEV problem. Always was. Wow, that's a nice beard, dude. What's up? Shake my hand. Holy shit. Welcome. Here you go.
00:07:09.320 - 00:07:20.380, Speaker A: No, just this. I can give you mine, but I'll have to walk next to you the whole time. Here's a clicker. Nice boots. Too. Dude, you're styling today. Welcome.
00:07:20.380 - 00:07:34.106, Speaker A: Yeah, it's a big one here. Just. Does it work? Yeah, cool. Yeah, cool. Hello, everyone. Thank you for having me here. Today we'll talk about why blockchain monitoring is meV.
00:07:34.106 - 00:08:12.330, Speaker A: It's one of these talks where everything is mev in the end. So first we will need to define what is blockchain monitoring and incident response, then how it ties to MeV. Right. We'll circle back to the event here and say why? Roll ups is basically a very good fit for what we're discussing here. And finally, we'll try to create a sketch for an implementation. It's important to talk about this. We can't just continue losing so many billions of dollars every year in hacks.
00:08:12.330 - 00:08:36.306, Speaker A: We need to become better than this. Billions. So it's important to talk about monitoring. It's important about talk incident response. And the usual incident response is this timeline. Basically, all companies should follow the same timeline, more or less. This is very well defined in web two.
00:08:36.306 - 00:08:58.054, Speaker A: First of all, you have the symptoms. Something is happening in your systems. The problem is surfacing. And hopefully you have defined alerts. At some point the alert will trigger, right. The issue is becoming so apparent that there is alert going on. And then you have the detection.
00:08:58.054 - 00:09:29.170, Speaker A: So you go from surfacing of the problem to the detection of the problem. And hopefully an engineer will acknowledge the problem, right. And will start mitigating. So mitigating is you stop the problem from existing. Maybe you don't solve the root problem, but at least you make it that it's not visible to users. And after you mitigate, then you can start working on the long term fix, communicating with users, et cetera. And this is the usual incident response timeline.
00:09:29.170 - 00:10:17.810, Speaker A: And good monitoring is proactive, really, because when we talk about monitoring, we mean that you should know what your system does. You should have set up alerts to be able to detect when something is off. And the system should be proactive. It should be activated before the problem becomes apparent to users. Right. You should speed up more servers before the users start having issues from loading your website. In the blockchain context, when we talk about blockchain monitoring, we don't mean about the monitoring of the nodes per se, the blockchain validators, because blockchains are very special.
00:10:17.810 - 00:11:03.986, Speaker A: There is a huge abstraction between the application and the backend, essentially. So protocol engineers, when they create a protocol they don't need to care about. How does it work on ethereum? They just deploy it and it just works. It's a hyperstructure as Jacob from Zora likes to say, and I really like this metaphor, you deploy it once and it runs forever. Right? So when we talk about monitoring blockchain applications, we mean the protocol itself, not the operation of the nodes. And the blockchain has two states, essentially it has an unfinalized state and the finalized state. And we need to talk about this because we are trying to be proactive here.
00:11:03.986 - 00:11:43.350, Speaker A: Right? We said good monitoring is proactive. And to be proactive we need to do something about it before it becomes a permanent issue. And in the blockchain context, a permanent issue becomes when the transaction becomes finalized. This is when the hack, for example, will become part of the blockchain's history. So a transaction will start as finalized. When you transmit it, it goes to the mempool, it's unfinalized transaction, it will be added to a block, it's still not finalized. And then depending on the Consensus protocol, it will start becoming having a larger finalization certainty.
00:11:43.350 - 00:12:33.840, Speaker A: And at some point it will be finalized. And when it's finalized, there's nothing we can do. We start doing damage control, but we can no longer be proactive. So in the blockchain context, mitigation is mev, because the group of activities we like to call MeV is the group of activities that we do to affect transactions that are not yet finalized, sandwich attacks, front running or whatever, we want to interfere with the transaction before it gets finalized. So in the blockchain context, mitigation is MVP. Let's create a small glossary so we are on the same page. The attacker is an entity, an agent that wants to put a protocol into a hug state.
00:12:33.840 - 00:13:25.470, Speaker A: We have the guardian, which is another entity that wants to stop, that doesn't want the protocol to enter into this hug state. And finally, we have the hug state, which is defined by the protocol essentially. And it's any state which might invalidate some assertion, right? It's defined by the state. For every protocol, the hacked state is different. So let's say a simple example. For example, the attacker transmits transaction in the mempool that will result in hug. And then the guardians, they see the transaction in the mempool and they want to front run it, and to front run it, they emit another transaction with higher gas.
00:13:25.470 - 00:14:12.910, Speaker A: Let's simplify it, let's not talk about PBS. This is the simplified and maybe outdated image of what's happening. So it emits this second transaction, the mitigative transaction with higher gas. So when the node takes both of them and adds them to the block, it will first execute the mitigative transaction, and then it will execute the malicious one. And because the mitigative transaction posts the protocol, for example, the malicious transaction will revert, right? So the problem is mitigated, the protocol is secure. And this is a simple diagram of what can happen and actually is happening right now. We have seen that in the wild.
00:14:12.910 - 00:14:53.990, Speaker A: It's worth noting that not all hugs are one transaction. They could be multitransaction hugs, but it's a segmental model. You could think that multitransaction hug is like the hug having a slower finalization as a procedure. So is mitigation is maybe. I think it is. We've seen an example of how they're super relevant, and let's see what kind of mitigations we can do. The simplest mitigation is front running, right? We described before, it has some challenges.
00:14:53.990 - 00:15:22.062, Speaker A: First of all, it's coordination, because you have to know the transaction. You want to front run. So it has to go to the public mempool, for example, if it goes to private mempool, then you don't have view to that. So it's coordination and economical, right? Because you go into this bribing war with attacker, where each is bribing for the transaction to be first. So that's a challenge. Then it's censoring the transaction at the node level. So you send them a lease transaction to alchemy.
00:15:22.062 - 00:16:12.450, Speaker A: And alchemy never transmits it, but instead maybe transmits the medicative transaction. Again, there is economical, expensive. It's coordination, because one node might censor you, but the other won't. So there is like this prisoner's dilemma between the nodes and social, that's not only always acceptable. And finally, we have the censored and reorg block mitigative action. So basically here, this private mempool will emit, this block will propose this block with the malicious transaction, but the other nodes will not vote for it. So they will wait for the next slot, and they will vote for the next block that has the medicative transaction.
00:16:12.450 - 00:16:56.450, Speaker A: And then we don't care if the other block eventually finds its way. We don't care about that, because now the protocol is secure. Essentially, what we are doing, we are doing a small consensus attack, right? We are doing a small rear at this point, maybe one block deep, but it's enough to protect the protocol. So let's try to formalize it. I had a lot of math notation in it, but I removed it. So basically, this node, this thing that the guardian runs to monitor. Basically it takes every transaction, maybe it's in the mempool, maybe it's a transaction that comes to the RPC endpoint.
00:16:56.450 - 00:17:47.776, Speaker A: It takes the current state and applies this new transaction to the current state, then checks if the new state has any invalid state in it, it has any of the hacked states. We have defined the protocol have defined. And if there is inside this new state, any of the hacked states, then it does mitigative action, as simple as that. And then it runs again. And I think the economics of this makes sense because we have seen the economics working right now. And what I mean is the following. First of all, every protocol has, let's say, some value exposed.
00:17:47.776 - 00:18:48.690, Speaker A: We don't know about that value, but it's less than the value of total locked in the protocol. And also there is some bounty, right? Which is also less than the TVL of the protocol. But what we do know is that the guardian bounty and the ethical value and the social value is more than the value exposed. And what I mean about this ethical value and social value is this value that the garden will get by doing this mitigation. And it's not financial, maybe it's reputation, maybe if they collaborate and they return the funds, they don't get prosecuted, right? And we have seen that working. We have seen generalized front runners, front running attacks. And then instead of keeping the bounty, which may, you know, instead of keeping the 20 million funds that were hacked, right, instead of keeping that, they have returned it, which is not financially rational, right.
00:18:48.690 - 00:19:18.040, Speaker A: They have returned it because there is some other value which doesn't have a monetary nature. And this value is more combined with a bounty than the value they would get from the hacked funds. So there are some challenges. We have an incomplete mempool view. We don't see all the transactions. Maybe there is a private mempool that will never tell us the transactions. I will never be able to see that.
00:19:18.040 - 00:19:48.480, Speaker A: And we'll have to do this complex reorg attack and the coordination issues. You have 15,000 validators in Ethereum. You have to coordinate with them to do the mitigations we talked about. So it's a very complex mechanism and incentive scheme you have to create. So what do you do? You go to roll ups, right? Roll ups are essentially centralized. Intrinsically more centralized. At least the mempool is one currently.
00:19:48.480 - 00:20:39.168, Speaker A: And I think for the next years, and at worse, when we have this decentralized sequencer and shared sequencers, it will be a federated system of a few tense nodes, right? It's intrinsically much easier to develop a system for this small group of nodes or incentives. So how do we define a roll up? I like this definition from James. It's an opt in subset of another consensus. It keeps a superset of the state. The roll up state has the baseline state of Ethereum and some extra state of the roll up. And it has a custom state transition function. It could be fuel, or it could be EVM like optimism under roll up.
00:20:39.168 - 00:21:33.830, Speaker A: Currently we have two big families of roll ups. We have the optimistic roll ups where you have the sequencer. It receives transactions, it sends the transactions to it sequence the transactions. So users send transactions, they get some order, then they get sent to the executor, which executes the state transition function, creates their roll up blocks which are then sent to the proposer to be committed to the layer one. Right. And you have the challenger that will play the optimistic game. Then you have the ZK roll ups where the sequencer receives transactions and then it sends to the prover, and the prover will batch them into blocks and create proofs and will submit the proofs to the Ethereum air one.
00:21:33.830 - 00:22:23.172, Speaker A: So these are the two families of roll ups we have right now. Let's see how we could implement the scheme we talked about with a centralized sequencer. We could, for example, put this module before the sequencer and we have this ethical censorship. So any transaction that's sent it gets simulated with a monitoring loop. We discussed if it produced a hacked state, it discarded and the rest of the system is the same. Or we could put it with an executor. And the reason we would want to put it in the middle of the system is because the sequencer eventually will not execute and the sequencer of the executor will be different.
00:22:23.172 - 00:23:04.668, Speaker A: So since we're already executing the transaction, we don't need to simulate again, we just check the resulting state. We don't need to run the state transition function twice. So this could be more optimized. As you saw, I didn't mention ZK roll ups. I didn't mention what we do with decentralized or shared sequencing because there are some challenges for sure. First is the forced inclusion, because currently users can go directly to the layer one and force a transaction. This is a censorship protection system that most optimistic roll ups have.
00:23:04.668 - 00:23:59.820, Speaker A: But the good thing is that there is a time delay to that. So it's like 12 hours or something. So the Guardian has ample time to check the transaction that is directly transmitted to the layer one and see if it's malicious, post the protocol, et cetera. So that's not really a problem. The second challenge is shared decentralized sequencers. What do we do when we have many sequencers and they have some way to decide who will propose the next roll up block? Let's say I feel very comfortable saying that there is a solution because the system is much smaller, you have tens of nodes. So I'm pretty comfortable saying that there is a scheme where, for example, if a malicious transaction find its way in the roll up, the sequencer will get slashed.
00:23:59.820 - 00:24:44.704, Speaker A: I think the mechanism design of that will be pretty simple. Then you have the ZQ roll ups. That's more interesting, because in theory I could run the prover myself, right? Apply the transaction to the state of the roll up and then just submit the proof. Or I could be a known prover and I'm honest, and then I do this, which is still an honest behavior from a protocol perspective. So the problem is I can execute locally and create the proof with ZQ rollups. With small protocol changes, we could see a future where all blocks, all proofs that are submit to layer one. There is a second proof for every proof.
00:24:44.704 - 00:25:44.744, Speaker A: The second proof is that the transactions that are included in this proof did not invalidate, did not produce the hacked state, right? This is totally possible right now. So with small changes in the ZqlAPs protocol, we could have the same protection. So you prove the transaction followed the protocol rules, and you also prove that the transaction was not malicious, did not hack some protocol. And of course, how you define what's the hacked state, right? How you do that. And again, here there is a solution. I think we could see something like a registry where every protocol signs up like a smart contract, and they put their invariants as a smart contract. So there is a public registry of what protocol rules are, and then the monitoring agents can fetch that and use that as a point of reference.
00:25:44.744 - 00:26:30.700, Speaker A: And if malicious transaction lands, there is a way to prove it unchained, right? You can prove unchained that this new variant, it passed the sequencer, didn't do anything about it. I want to slash them. So I feel very comfortable saying that all of these challenges are solvable right now. Censorship is not that bad. That's a very hot take. But I think generally as an industry, we have agreed that the social layer is more important than blockchain. Know Avi, he said on Twitter, what are you going to be? Are you going to arrest me? That's exactly what they did.
00:26:30.700 - 00:27:15.750, Speaker A: We have agreed that code is not law. Even if a smart contract allows you to do something, it doesn't mean that you should do it. Right. I think as an industry we are going towards that area. You can see with sovereign roll ups, for example, which are roll ups where they don't have economical state, they depend on the social area agreeing which sovereign roll up is the right one. Right. So I think generally what we're describing here, which is a form of censorship, is much more acceptable that the censorship we're seeing with, for example, OFAC sanctioned Transactions.
00:27:15.750 - 00:27:39.650, Speaker A: Right. That was not an acceptable censorship. Eric Wool talked about it in about that. But I think the censorship we're discussing here is more acceptable. And a quick plug to tie everything together. I'm working in all of these things I described above. It's some early ideas.
00:27:39.650 - 00:28:24.910, Speaker A: Sure. And early designs. I used to work at Nomad, so this whole thing really strikes home with me. Currently, Philax, which is the project's name, has a private alpha. It doesn't do the MeV stuff we talked about currently. What it does, it's allow you to monitor your protocol with solidity. And basically you can turn the huge bespoke alerting script you have, which is horrible, to a very small solidity contract that you can define basically, like foundry did with testing.
00:28:24.910 - 00:28:57.980, Speaker A: But all of what I described above are feasible. Like some early tests have shown that the design makes sense. Thank you very much for your time. You can find me on X on Telegram. You can find X as well. Maybe we have some time for Q A. Yeah, well, let's give it up first.
00:28:57.980 - 00:29:26.100, Speaker A: If anyone has questions for Odysseus, I got a microphone here, run over to you. Got a couple minutes for Q A. It's a bit early. Oh, you got one? Oh, you have one too. Nice. What's up, guys? Coming in strong today. That's awesome.
00:29:26.100 - 00:29:51.200, Speaker A: Yeah, don't all get up at once. I know you just woke up and stuff, but questions are encouraged and we have the time, so feel free to jump up. I think we're good. Probably. All right, man. You're so good that they don't. Yeah.
00:29:51.200 - 00:30:05.422, Speaker A: One more time. Give it up for Odysseus. Thanks for coming, man. Yeah, see you around. Oh, wait, let me get this. He's running off with it. I like it because it has a laser.
00:30:05.422 - 00:30:49.810, Speaker A: See? Yeah. I'm not going to let you run away with this. Okay, just out of my curiosity, are there any side events that you guys have been to? Did you guys go to Devconnect or is anyone going to devconnect? Let me just start there. Yeah, you two. So three. Okay. Are there side events that you guys are doing outside of here? Are you just mostly coming here? Anyone? You? What else are you doing? Yala shout.
00:30:49.810 - 00:31:29.146, Speaker A: Avalanche house. Oh, that's sweet. Does that mean you stay there? You like sleep there? Okay. Oh, it's just like a day event, right? Okay, cool. Anyone else doing any side events? So I'm trying to figure out how this all pieces in with dev connect. I find it super interesting if there were like, let's say if l two beat or l two days, excuse me. Was hosting an event at night, let's say like an opening and a closed party.
00:31:29.146 - 00:31:59.370, Speaker A: Would you guys do that? Like, would you guys go to something like that? Let me see hands. It's like your typical go to a bar, free drinks sort of scenario. Cool. What if it was like a dance part? Like a dance event with like a dj? Okay. You're like, yeah, cool. Just curious. Just curious.
00:31:59.370 - 00:32:18.574, Speaker A: It's good feedback for us. Is Justin Drake here? What's up, man? Yeah, come on up. Let's invite Justin from Eth foundation. Give it up for Justin. Pleasure. Here's a microphone for you. Let me check if here's his s one.
00:32:18.574 - 00:32:39.558, Speaker A: And I have a clicker for you. Take your time. We're a little early, right? Yeah, a little early. Should I wait the five minutes? Nah, nah, it's okay. Okay. In which case we'll have extra time for questions. Perfect.
00:32:39.558 - 00:33:30.866, Speaker A: Exactly. Okay, so the topic of my talk today is base roll ups, which is basically a very special class of roll ups where the sequencing of the roll up blocks is done by the layer one itself. So you call it a layer one sequenced roll up. And more specifically, the roll up sequencing rights are given to the layer one proposers. And it turns out that base roll ups have these really nice and kind of magical properties almost. And the layer one sequencing is kind of this module that ethereum offers that's hidden in plain sight, and one that I expect rollaps will be using more and more in the future. So the talk is going to be split into three parts.
00:33:30.866 - 00:34:12.520, Speaker A: First, I'm going to talk about the definition of a base roll up. I'm also going to introduce some other terms and try and just unpack the construction a little bit. Then I'm going to talk about advantages of base roll ups, and then finally some of the downsides and mitigations to those downsides. Okay, so I'm going to be using a lot of emojis in this talk. So the sunglasses means based and scroll means roll up. And when you combine the two, you get a base roll up. If you change the glasses to not be sunglasses, then you get a non base roll up.
00:34:12.520 - 00:35:04.578, Speaker A: Okay, so now let's talk about some of the key definitions in the space. So if you're a layer one, if you're a L2 chain, or even a layer one chain, there's three fundamental modules that you need to build your chain. You're going to need some form of sequencing, some sort of execution or virtual machine. And you're also going to need a data module to post all your transaction data or your state diffs. And it turns out that the layer one provides or will provide all three of those modules. So data, we're familiar with call data, but also blob data with sharding. Now, in terms of chains that consume l one data, we call them roll ups.
00:35:04.578 - 00:35:54.318, Speaker A: That's the definition of roll up. If you don't consume l one data, then you're a valid. Now, in terms of the sequencing, if you use the l one sequencing module, basically you're delegating, you're giving sequencing rights to the l one proposers. Then you're going to be called based and otherwise you're going to be not based. And then eventually there's going to be this upgrade to Ethereum, most likely, which is going to add an opcode or a pre compile to verify EVM execution within the EVM. And so this upcode will keep track of the rules of the EVM as they change. And you'll be natively using the EVM execution, at which point you'll be called a native rollup.
00:35:54.318 - 00:36:41.498, Speaker A: Okay. It used to be called enshrined rollup, but people didn't like the term enshrined. Okay. And if you combine all three, then you kind of get this native base roll ups. Okay, so one of the key things that enables base roll ups is proposer builder separation. So really what a base roll up is saying is anyone can go extend the next block, and whoever's the next proposer has monopoly power to go build that next block. Now, before builder proposal separation, the way that a proposer would build a block is that they'd listen to the mem pool and you could have multiple transactions that are proposing extensions to these base roll ups.
00:36:41.498 - 00:37:36.530, Speaker A: And these different suggestions would all conflict with each other. And so there would only be one block that would make it on chain, and then all the other roll up blocks would basically be failed transactions that just waste space. Whereas with builder proposer separation, you're in a situation, you have one builder that takes care of building the whole block, and they can make sure it's a really optimal block with no wasted space, and they can build the best roll up blocks that they can without having conflicting blocks. So proposal build separation, which we have now, is a key building block for base roll ups. Now I kind of want to give you the full pipeline. So at layer one, we have layer one searchers that give bundles to layer one builders. You could call this searcher builder separation.
00:37:36.530 - 00:38:21.422, Speaker A: And a similar thing is going to happen within the rollups, right. We're going to have roll up searchers and roll up builders. And in the case of base roll ups, it turns out that the L2 builder is the layer one searcher. So we basically take the existing pipeline searcher builder proposer and we kind of extend it with roll up searches at the front. Okay, so that's basically the construction of a base roll up. You're just allowing anyone to extend your roll up with roll up blocks. And the natural entity that is capable of inserting these blocks is going to be the layer one proposer.
00:38:21.422 - 00:39:08.322, Speaker A: But these sequencing rights are in practice delegated to block builders that will build very good sequence, very well sequenced roll up blocks. Okay, what are some of the advantages? So what I'm going to do, to talk about the advantages of base roll ups is compare them to non base roll ups that are decentralized. So think of a decentralized sequencer which is external, and we're going to compare it with one which is internal, basically reusing the ethereum layer one sequencing. So one of the key advantages is simplicity. And we don't need an external consensus if we use an l one sequencer. And we don't need an escape hatch. Right.
00:39:08.322 - 00:40:13.122, Speaker A: The escape hatch is basically there in case the external consensus fails. And actually you can think of the escape hatch as being a delayed based sequencer, right. It's a sequencer which comes in after a certain timeout, for example, timeout of 1 hour. And it's just much simpler to just embrace the l one as your full sequencer and not have to bother with this kind of two phase sequencing, one with external and then with the fallback with the escape hatch. And one of the amazing things of layer one sequencing is that it's literally zero lines of code. So you get a sequencer which is decentralized and even a shared sequencer, and there's zero lines of code that are dedicated to sequencing because you just allow anyone to add blocks and it's even simpler than a centralized sequencer, because with a centralized sequencer, the centralized sequencer would sign the blocks and you'd have to verify the signature on chain, which you don't even have to do with base sequencing. So that's simplicity.
00:40:13.122 - 00:41:07.094, Speaker A: The first advantage, the next great advantage of base sequencing is around liveness. So if you take an external sequencer, there's a risk that the escape hatch has to be used because the external consensus has failed. For example, it's been 51% attacked. And the good news here is that users can always use the escape hatch, so they always have the ability to exit. But it's really, really bad news for the roll up itself if a mass exit happens. And the reason is that all the network effects that you've spent years building now suddenly collapse because all your users have exited. And so basically there's a risk that your network effects reset, and then there's something even worse, which is that there's an incentive for an attacker to go attack your roll up if you have an external consensus.
00:41:07.094 - 00:42:14.800, Speaker A: And the reason is that if you can take over the external sequencer, you can extract toxic mev. So one example of this is if you censor Oracle updates, so if you censor Chainlink, for example, you can cause liquidations, you can manipulate markets, et cetera, et cetera. So if the cost of attacking the external sequencer is, let's say a billion dollars, but you can extract $2 billion from toxic meV, then it's just a matter of time before your roll up is going to get attacked. And the amount of toxic mev basically depends on this timeout, if it's one day or 1 hour or whatever the timeout is before you fall back to becoming a based sequencer. Now, on the other hand, with the l one sequencing, you're going to have a roll up which is going to live as long as Ethereum itself. So if you want to build a roll up which is going to last decades and centuries, really your only choice is to go with a base roll up. Now another lens on liveness is just security.
00:42:14.800 - 00:43:01.840, Speaker A: If you have an external sequencer, you're basically adding a new assumption, right? So you've already bought into the security of Ethereum, but now you're buying into the security of some other system with another token. And almost certainly you're going to have a small amount of economic security. Right. On Ethereum today we have about $55 billion of economic security. But if you have an external sequencer with a new token that might only have a few billion dollars of economic security, and it's very hard to acquire economic security. Even bitcoin, which is the second most secure blockchain, has less than $10 billion of economic security. So it's extremely hard to compete with Ethereum on economic security.
00:43:01.840 - 00:44:25.260, Speaker A: And I think the final and third really big advantage of base rollups is their neutrality. So if you have an external sequencer now, you're introducing a new system, a new brand, a new token, there's just more moving parts, as opposed to just reusing what the rollup has already bought into, which is consuming Ethereum data. It might as well consume another module from Ethereum, which is the sequencing module. And so I kind of see base sequencing as being this most credibly neutral shared sequencer for the rollup ecosystem. And you can have competing rollups, for example, arbitram and optimism kind of agree to use this middle ground, neutral ground of ethereum for sequencing, especially that shared sequencing, and this is the topic of the next panel, is extremely valuable for things like synchronous composability. Basically, when you have multiple roll ups that have a shared sequencer, they act like one big roll up, and that's extremely valuable to fight the fragmentation of liquidity, for example. Okay, so to summarize the advantages, we have three big ones.
00:44:25.260 - 00:45:09.106, Speaker A: Base roll ups are maximally secure and live. They're maximally simple and they're maximally neutral. And I think these three advantages will make base roll apps the natural shelling point for roll apps to adopt in the future. Now, you might ask, why don't we have base roll ups today if they're so amazing? Well, it turns out there's downsides. There's three big downsides, and each of these three downsides have to be addressed before we can have base roll ups. The first downside is actually around security. It turns out that centralized sequencing is more secure today than decentralized sequencing.
00:45:09.106 - 00:46:24.826, Speaker A: And the reason is that pretty much every single roll up virtual machine has bugs, has vulnerabilities, right? You have projects that are trying to be EVM equivalent. They're going to write these extremely complicated fraud provers, or extremely complicated snark circuits, which are going to have bugs. And the centralized sequencer basically is an additional layer of safety that will never introduce a bad block on chain, or at least you're trusting it to not do so. Whereas if you have a decentralized sequencer, as soon as a black hat has identified a vulnerability in your vm, they can immediately exploit it because it's permissionless, decentralized sequencing and so, in order to get a base roll ups, we need to fix the vulnerabilities in virtual machines. That's kind of a prerequisite, actually, it's a prerequisite for any decentralized sequencer, not just base sequences. And there's all sorts of mitigations that can be had for the security. We can have delayed settlement, for example, a 24 hours delay, whereby if a bad block is sequenced, it's not immediately executed.
00:46:24.826 - 00:47:24.654, Speaker A: And so there's time, for example, for a security committee to press the big red emergency button to pause the roll up. But there's also other techniques, like multiprooving, where you have multiple implementations of the state transition verifier on chain. You can also wait for time to pass and have the Lindy effect play its role, or you can use very fancy and heavy machinery, like formal verification, to basically prove that there are no bugs in your virtual machine. But this is going to take time. And actually, once we have the last three, we'll be in a position to have best in class like bug resistance with this idea of the native EVM verification with this new opcode within the EVM, that will benefit from the diversity of execution clients. But that's for another talk. Okay, so the first big downside is security, which will eventually be addressed.
00:47:24.654 - 00:48:13.138, Speaker A: The next big downside is around MeV. So if you take any chain, there's basically two sources of income for that chain. Source of income number one is congestion fees, and source of income number two is contention fees, also known as MEV. Now, MeV accrues to whoever the sequencer is. And so if you have a roll up that's doing its own sequencing, it will accrue both the congestion fees and the MEV. Whereas if the sequencing is done by the layer one, well, the roll up no longer has sovereignty over this income stream, so it's giving up the MEV, also known as the contention fees. Now, my answer to this is that, yes, it's true, but it's not a big deal.
00:48:13.138 - 00:48:54.506, Speaker A: And I talk about the MeV gambit. Basically, you're making a sacrifice, and it's a tiny sacrifice. It's a little bit like sacrificing a pawn in a game of chess, but then eventually winning the game. Now, to understand why MeV is kind of this tiny pawn, we can look at the data today, the fee data on the layer one. So roughly 80% of the income of the fees come from congestion fees, and 20% comes from MEV. So, concretely, I just looked it up this morning. There's on average 800 e per day of MeV and 4500 e per day of congestion fees.
00:48:54.506 - 00:50:04.770, Speaker A: So it's actually 80 515 is the breakdown. But what I expect will happen is that MEV will go down, will tend towards zero for two reasons. Reason number one is that we're going to have encrypted mem pools that are going to remove sandwiches, and sandwiches make up a very decent chunk of this 800 e per day of MeV. And then reason number two is that the rest of the MEV is mostly what's called sex deck's arbitrage, arbitrage between centralized exchanges and decentralized exchanges. And there's like these new generation decentralized exchanges that are being worked on that are MEV internalizing or MEV aware or MeV smart in such a way that they don't leak MEV to the proposer or the sequencer, and instead it reinternalizes the MEV and gives it back to the liquidity providers. Long story short is that MEV might be a tiny portion, it might be only 1% relative to the congestion fees in the future. And so you're giving up a tiny amount of MEV, tiny amount of income for neutrality and security and simplicity.
00:50:04.770 - 00:50:36.080, Speaker A: And so it's a very good trade to be making. Okay, now we've gone through security as one of the downsides. We've gone through the economic downside of sacrificing MeV. Now we have the third downside, which is a UX downside. The Ux of centralized sequencing is amazing, right? You make a transaction within 100 milliseconds. You get a pre confirmation from the centralized sequencer. Your wallet shows you a green checkmark, everyone's happy.
00:50:36.080 - 00:51:36.494, Speaker A: On the other hand, when you have a based roll up, you inherit the twelve second block time of the layer one. And there's no pre confirmations at layer one, at least not natively. And it turns out that with restaking you can do base pre confirmations. So you can basically add pre confirmations to base roll ups by having some of the l one proposers opt into becoming pre confirmers. So they opt into these new slashing conditions whereby they now have the right to issue pre confirmations to users on the order of 100 milliseconds. And if they don't honor these pre confirmation promises, they get slashed. And if you want to read all about these base pre confirmations, there's an e free search post and there's also a 1 hour discussion on YouTube.
00:51:36.494 - 00:52:25.902, Speaker A: So I actually believe that once we have these two building blocks, restaking and inclusion lists. We'll be able to have base pre confirmations and that addresses the final downside of base roll ups. So just to zoom out a little bit, where are we today? Today we're on the left here with centralized sequencing. We want to be on the right for multiple reasons, for regulatory reasons, for security reasons, for neutrality reasons. How do we go from centralized sequencing to decentralized sequencing? Well, whatever the flavor of decentralized sequencing, we need to solve several issues. We need to solve security, for example with multi proving. We need to solve mevn front running with encrypted mempools.
00:52:25.902 - 00:53:11.854, Speaker A: And we also need to solve the pre confirmation problem so that we can match the user experience of the centralized sequences. And once we have these three things solved, we have a choice. Do we want to adopt a based decentralized sequencer or a non based decentralized sequencer? And I think the choice will be pretty obvious at that point because base sequencers, again, they're the most secure form of sequencing that you have and they're also the most neutral form of sequencing that you have. And that's pretty much it. Thank you. Sick. I think I have time for questions, right? Yeah, you do.
00:53:11.854 - 00:53:25.314, Speaker A: Lots of time for questions. You have so much time, I don't know what to do. We're going to kill it though. I got some stuff lined up for you. Nice. We got questions. Yeah.
00:53:25.314 - 00:54:02.610, Speaker A: One question. Can we keep the slides just in case the questions refer them? First of all, thanks for the talk. It was really cool. Can you guys put the slides up for him? Yeah. Thank you. They'll come. So, yeah, I think one question that I'm trying to wrap my head around is it seems as though based sequencing, l one sequencing relies a bit upon the idea that there will be value capture for sequencing these l two blocks, that there is some mev and thus there's some incentive for the builder to add the block.
00:54:02.610 - 00:54:41.914, Speaker A: Of course, what I wonder about is sort of the bootstrapping phase in a new based l two. And how do you get them to have this incentive if there is not a lot of activity at the beginning? Okay, fantastic question. So the incentives are the same as the l one for block production, there's two of them. One is issuance, so every single block can have some issuance. And then two is the transaction fees. Now what you're saying is that initially there's going to be no transactions and so no transaction fees. How do you have blocks that are being created? And the easy answer here is issuance.
00:54:41.914 - 00:55:48.850, Speaker A: So for example, if you roll up like scroll or starknet or whatever it is, DK sync, you can have one scroll token per block just to incentivize the block production. Cool, thanks. There's a question over there. Hey Justin, can you talk a little bit more about the pre confirmations? And what are going to be the conditions that the sequencers needs to opt in into, or the validators? I guess they are one validators. Okay, great question. So the way that the pre confirmations work is that some subset, let's say 10% or 20% of the layer one proposers, opt into these new stashing conditions. And on Ethereum we have what's called a look ahead, so we can see the future, we can see which proposers will be proposing in the future.
00:55:48.850 - 00:56:38.046, Speaker A: At the minimum, we can see the next 32 proposals. Now what we're going to do as a user is contact the first preconfirmer and kind of ask them to pre confirm my transaction. Now there's two things, and basically the pre confirmmer will return what I call a pre confirmation promise, basically saying I promise to include your transaction at this position, and this is how your transaction will execute. So if you're doing a uniswap trade, for example, you know exactly what price you're getting and you know exactly what the post date would be. Now, there's two failure modes. One is a safety failure mode, and the other one is a liveness failure mode. The safety failure mode is when the pre confirmmer made a promise, but then that just doesn't respect the promise.
00:56:38.046 - 00:57:11.626, Speaker A: They just include other transactions, not the ones that they promised you. In this case, it's very easy. You can just flash all the 32 e because there's cryptographic proof that the promise was not respected. The second failure is a liveness failure. It happens sometimes, it's relatively rare, but it happens sometimes that the proposer just doesn't show up and there's a missed block. In that case, my suggestion is to have a smaller penalty. It could be 0.1
00:57:11.626 - 00:57:39.990, Speaker A: e, it could be one e or something else. And the way that you want to price the pre confirmation tips. So one of the things I didn't mention is that users should pay for the privilege of having pre confirmations. They don't need to pay a lot. It's probably going to be less than a cent. But there needs to be some form of incentive for the pre confirmers to be preconfirmers. And depending on how confident the preconfirmer is, that they won't be offline.
00:57:39.990 - 00:58:19.140, Speaker A: They can agree on how much will be slashed. So, for example, I'm a user. I really, really want to make sure that I'm going to get pre confirmed. I'm happy to pay a whole cent, but I want you to be slashed a whole e if you don't show up. Now, one of the kind of optimizations to improve liveness is actually to contact multiple preconfirmers in parallel. So in your look ahead, which has 32 proposers, there might be, I don't know, five of them that are preconfirmers. Contact the first two or three and get pre confirmation promises from all three.
00:58:19.140 - 00:59:26.898, Speaker A: And if the first one, just for some reason, accidentally goes offline, you can rely on the promise of the second one, which is conditional on the first one being offline, so you can still get the green check mark and have the confidence that if the first one goes offline, you just fall back to the next one. Are the pre confirmations binding? Yes, they're binding. And if you don't respect them, either through a safety fault or liveness fault, you get slashed. And can the non preconfirmers actually change the state and influence the state so that the pre confirmers actually cannot be, the promise cannot be fulfilled? Yeah, that's a great question. So the non pre confirmers can include transactions on chain, but those, if they are not pre confirmed, will have to wait until the preconfirmer has spoken in order to execute. So there's this out of order, kind of delayed execution of non preconfirmed transactions. But what I expect will happen in practice is that the vast majority of transactions will be pre confirmed.
00:59:26.898 - 01:00:09.750, Speaker A: And what can happen is that the non preconfirmers can include pre confirmed transactions even though they were not pre confirmed by them. So what I expect will happen is that preconfirmers will pre confirm transactions and kind of gossip the pre confirmation promises publicly so that anyone, including the non preconfirmers, can include those transactions on chain immediately. There's a question there and a question there. This is good. You got good questions. Good questions. So you mentioned that searchers and builders, right, like an l two builder is like an l one searcher.
01:00:09.750 - 01:00:51.414, Speaker A: In practice, l one searcher builders have vertically integrated pretty heavily by now. I don't know what the numbers are off the top of my head, but pretty aggressively. Is there a concern that we just see that continued integration and we end up with, for a given l two, there is one person that is all of the order flow on this? And then what are the implications of kind of just how that order flow is going to push down to the l one and the value capture. Yeah, great question. So the separation that I showed in the slide is more of a logical separation in practice. What I expect will happen is there will be some amount of centralization, especially if we're talking about evms. Right.
01:00:51.414 - 01:01:45.266, Speaker A: Because if your roll ups in EVM, then the current layer one builders are specialized in EVM, and so they're going to be also roll up builders. But if it's some sort of really weird virtual machine, something really exotic, then it might be like a specialized builder for that. Now, in terms of the concerns around centralization, the good news is that from a fundamentals perspective, there's no concern. And what I mean by fundamentals, I mean things like liveness, things like safety, things like censorship, things like front running. We can solve all these problems. We know how to solve them, at least from a research standpoint. So we have tools like inclusion lists, we have encrypted mempools, we have enshrined PBS, we have MeV burn.
01:01:45.266 - 01:02:33.314, Speaker A: All of these things actually help solve these problems. There is one final problem which I'm worried about, which is the memetic problem. It would be a really bad look if Citadel or some other stratfy entity were to build 80% of ethereum blocks. Not that they could do anything bad, but it's just not a good look. And there's multiple ways to have a better looking builder that's still kind of the dominating builder. Option one that I see right now is something like Suav, which is kind of this decentralized building using either SGX or fancy cryptography. And then option two could be a centralized builder, maybe like Titan, that's trying to be like this credibly neutral builder.
01:02:33.314 - 01:03:28.334, Speaker A: But I would have an expectation that these builders that are trying to be credibly neutral, like open source as much as possible, be very transparent about their flows, try and basically not integrate with searchers, just provide the incredibly neutral service. And we're not there yet, but I'm hopeful that we can get there. Questions at the back. Can you elaborate more on how risk taking can help the pre confirmation? Yeah. So when you're making a pre confirmation, you're making a promise off chain to do some sort of action in the future. Now there is a risk that this action doesn't happen. Basically there's a risk that you break your promise.
01:03:28.334 - 01:03:56.490, Speaker A: Now, in order to achieve incentive alignment, there needs to be some sort of penalty for not honoring your promise. And the way that you enforce this penalty is with additional slashing conditions. So, I'm a pre confirmer. I'm a layer one proposer. I've committed my 32 e. If I don't respect a pre confirmation, I stand to lose my 32 e. And now I'm incentive aligned to honor the promises that I've given to users.
01:03:56.490 - 01:05:14.774, Speaker A: So it means the pre confirmation software has to be run as part of the validator, side by side. Can you repeat that? Does that mean, like, there will be kind of a pre confirmation software that running side by side with the proposer? Okay, great question. So you could have the layer one proposer just run a new kind of pre confirmation client, just like they run an execution client or consensus client. If it turns out that the bandwidth load or the computational load of providing the pre confirmation for potentially hundreds of roll ups is too high, then you can delegate your pre confirmation rights to some sort of new entity, which would be a pre confirmer. And I think if I were to look at the existing ecosystem, the relays are actually a great candidate to be pre confirmers because they can be mutually trusted by users and as well as mutually trusted by the proposers. So, yeah, what I expect will happen is that there's a new role that's kind of formed pre confirmers, and it could be filled by the existing relays. Got you.
01:05:14.774 - 01:05:46.446, Speaker A: So, theoretically, it can be a totally independent new network of. Yeah, it could be like a separate gossip channel, where basically users gossip their pre confirmation request, and then the relay responds with pre confirmation promises. Okay, thank you. There's one here and then one here and one here. Yeah. And then let's cap it at that. We'll pivot into your next section.
01:05:46.446 - 01:06:30.170, Speaker A: Okay, can you talk a bit about the mempool in this design? In centralized sequences, you have one place to send them, but now where do you actually send your transactions? As a L2 client. Yeah. So today we have a centralized, encrypted mempool, and we want to move to a decentralized, encrypted mempool in the future. Because we're kind of relying right now on the centralized sequencer to not do front running. We're trusting them to not do that, not do sandwiches and things like that. But in a decentralized context, the way to solve front running is to keep the encryption, but it needs to be decentralized. Now, there's various ways to do decentralized encrypted mempools.
01:06:30.170 - 01:07:22.670, Speaker A: One of them, which arbitram is going with, is threshold encryption. So basically a user gossips their encrypted transaction, and then all the transactions in the mempool are decrypted in one go with a shared decryption key that's tied to some sort of committee. You can use delay encryption, which is like fancy cryptography, whereby all the users submit their encrypted mempools, and then somehow, by cryptographic magic, all the transactions decrypt. After some amount of time, it could be a few seconds. And then there's another flavor of encrypted mempool, which is decentralized, which is using secure enclaves like SGX. And this is what suave is trying to build. And I think all three approaches will be tried in production.
01:07:22.670 - 01:07:43.510, Speaker A: Do we have time for one more question? Yeah. Okay. There's a couple of people on the front, I think. Well, I want to offer you the opportunity. Do you need to step out to reset or go? No, I'm good. Yeah. Okay, cool.
01:07:43.510 - 01:08:11.390, Speaker A: So we'll do that and then we'll announce the panel. Yeah, I'll make it really fast, basically. Just wondering here. Your talk was entirely neutral in terms of optimistic versus zk roll ups. Just wondering if there are any meaningful changes to the design space. When you think about whether you want an optimistic based roll up or a ZK based roll up, whether it uses Fraud proofs or validity proofs. Right.
01:08:11.390 - 01:08:50.246, Speaker A: So the construction is basically the same. You can think of sequencing as being a module which is orthogonal to execution. But there are some advantages of ZK roll ups around the value of shared sequencing. So when you have a shared sequencer. I said that all these roll ups act like one big roll up. That's less the case for optimistic rollups than it is for ZK rollups. So with optimistic rollups, the way that you get the synchronous composability or simulating it is with liquidity providers, but it doesn't work for NFT, and it's kind of clunky and expensive.
01:08:50.246 - 01:09:22.280, Speaker A: When you have ZK rollups, you can have this kind of interleaved execution where truly you have synchronous composability. And truly all these mini rollups act like one big rollup. So I think there's going to be strong network effects around shared sequencing because of this synchronous composability. And in order to really tap into those network effects, you're going to have to be a ZK roll up. Sweet. Give it up for Justin. Great job, man.
01:09:22.280 - 01:09:41.180, Speaker A: Yeah, feel free to grab a seat. So Justin's going to stay on stage. He's our moderator for our next panel about shared sequencing. Where'd you. Oh, I guess. Justin, whenever you're ready, come back up here. Take a moment.
01:09:41.180 - 01:10:15.222, Speaker A: Charles, if you're here, Charles. Luke, come on up, bud. So, Charles from espresso, we have Tomek Tom from Nethermind, mark the kaleidoscope from op labs, and we have a panda coming on stage. I'm so excited to see Josh the panda. Where is got? It's a human. Hey, man, I was kind of hoping they were bringing a panda on stage. Okay, come on, on.
01:10:15.222 - 01:10:24.762, Speaker A: Come on. Here. Get over here, guys, don't bite. Don't be scared. Take a seat. We have turkish artisanal water waiting for you. Give them up.
01:10:24.762 - 01:10:52.740, Speaker A: Clap it up for these gentlemen. Are you guys meeting for the first time? You all know each other? Kind of, sort of Internet friends. So you guys should all have a microphone for each one of you. Just give it a little tap on the top to make sure it's on. Sweet. Sweet. And Justin, take it.
01:10:52.740 - 01:11:43.620, Speaker A: Oh, before you start, actually, sorry, I want everyone in here to know I'm going to say it now, and I'll also say it during lunch break, but this is going to take us into lunch break. So yesterday, everyone was looking for lunch. Everyone went downstairs. And you may or may not have been involved in the not chaos, but there was just a lot of overcrowding downstairs, and no one knew that there was lunch upstairs as well. So I encourage you to tell your friends. It sounds like everyone is just like the flow of traffic going downstairs, but it seems like if you go upstairs, especially everyone that's in here, because the rest of the world isn't hearing this, but if you go upstairs, there's lunch upstairs as well. So just a little note for you guys.
01:11:43.620 - 01:12:18.974, Speaker A: And, yeah, I'll say it again at lunch break, but thank you, and have a good panel, guys. Clap it up for them. Thank you. Okay, so this is a panel on shared sequencing. And I guess there's two aspects that are represented on this panel around shared sequencing. One is the producers of shared sequencing. And I think we have two projects here conveniently sitting next to each other, and we have the consumers of shared sequencing, which I guess is on the other side, I guess.
01:12:18.974 - 01:12:47.554, Speaker A: Shall we do a very quick round of introductions and kind of confirm that you are indeed a producer or consumer? And if you're a producer, kind of give us the one sentence pitch on why you're producing something interesting. And on the consumer side, give us the one sentence pitch on what you're looking for. Cool. Yeah, I'll start. Great to be here. I'm Charles from Espresso Systems. We're building the espresso sequencer.
01:12:47.554 - 01:13:32.322, Speaker A: It is a shared sequencing network. And first I will say, justin, amazing presentation, super concise, agree with all the points, except maybe the conclusion. I do think that espresso, which we're building, does hit a very sweet spot on the design space of sequencing. That hits a lot of the aspects that you would want, and I'll go into that a little bit more. But what we are focused on with espresso is, first of all, being as credibly neutral as possible. And part of that is being able to scale to, ideally the same validator set, or at least the same size validator set as ethereum. So over 10,000 physical nodes, while at the same time providing very, very fast finality.
01:13:32.322 - 01:14:19.630, Speaker A: So the same user experience of having almost instant pre confirmations that users get with roll ups today with centralized sequencers. So, yeah, excited to talk about that more, Josh. I'm the founder at Astria. We're building a shared sequencer. I think I'm somewhat of the OD man out in that we are primarily focused on the celestia ecosystem. I previously worked at Celestia Astria kind of like spun out of that. And so the inspiration for the shared sequencer was looking at with this kind of new, abundant data availability, what does the roll up landscape look like in that new environment? Are we going to have more roll ups? Are we going to have higher throughput roll ups? What is the tooling that is desirable to build this? And one of the things we kind of focused on was wanting to be decentralized by default.
01:14:19.630 - 01:15:15.894, Speaker A: And so when we get a new crop, ideally of roll ups being developed, they're not going through the kind of progressive decentralization where everyone launches and then needs, like, a long roadmap to say, okay, we're going to get to decentralizing. And so that was kind of our inspiration for building a shared sequencer, I guess the kind of really quick. Our design is relatively simplistic, and that is a tendermint chain. It provides fast block times on tendermint in the range of one to 2 seconds with like a tendermint size validator set of up to 150 or so. Okay, great. So I guess just to summarize a little bit, I guess what you're saying here is that the value that you're providing, is that your shed sequencer, beyond the ethereum ecosystem, you also include the celestial system and maybe others. Is that the case for espresso as well? So for us, we're agnostic to various parts of the design space.
01:15:15.894 - 01:15:46.266, Speaker A: So first of all, agnostic to vm, agnostic to the type of roll up. So it could be an EVM roll up, it could be an SVM roll up, it could be an app specific roll up. Secondly, we're agnostic to the approach to mev. So it could be full on priority fees, mev sandwiching and all that. The role could take a threshold encryption approach. So something like mev mitigation strategy, any of that could be compatible with espresso. We're working with off chain labs right now, the developers of Aratrim, to build a time boost based builder.
01:15:46.266 - 01:16:47.780, Speaker A: So we're compatible with those types of schemes. As and obviously to answer your question, Justin, while we are managing the stake table, and we plan to be using techniques like restaking to scale to the Ethereum validator set and empower Ethereum validators to participate in running this sequencer, which really can be viewed as a fast finality layer on top of Ethereum that could be replicated to other chains as well. Gotcha. Hey, so my name is Mark and I'm a contributor to the optimism collective. We maintain the op stack that powers L2s like Op mainnet and base. So we are a consumer of shared sequencers. But if we think about the incentives, really, I think that eventually all consumers of shared sequencing will probably want to build their own shared sequencers because of vertical integration can just provide a much better user experience.
01:16:47.780 - 01:17:54.434, Speaker A: Think about Apple. Apple is now building their own chips, and they go all the way from the hardware to the software to provide this really cohesive, great user experience. But what I would be looking for in a shared sequencer, one thing is alignment, right? And alignment is like this big nebulous concept, but what I mean specifically here is alignment on values like censorship, resistance. I'd also be looking for cost, right? We want security at the cheapest possible per penny amount of security. If there's like two different solutions and they both offer the same amount of security, but one is cheaper, obviously we'd want to go with the cheaper one holding everything else constant. Hi everyone, I'm Thomas. So I think that I'm a consumer for shared sequencing from multiple angles.
01:17:54.434 - 01:19:19.054, Speaker A: So one would be working with flashpods. We see espresso as a potential promise of solutions for cross domain mev coordination. So block building on multiple chains at the same time in a coordinated manner. So if we think about suave delivery on flashpods, then espresso and Ishirtiv Austria espresso radius is there as well, can be providers of this coordination for deploying blocks on multiple chains. Then from netamide perspective, when building infrastructure, we think of the shared sequencers as yet another building block within this modularity set of solutions. So when constructing integrations, when advising l two builders, chain builders, on how to deliver security, how to design consensus, which of the modularity aspects to include in their designs. That's when we think that we consume those proposals, those research materials, and probably from the third perspective, as starknet, we look at this, is it a viable proposal for sequencing? Can we use it in a different VM model? Not EVM.
01:19:19.054 - 01:20:45.086, Speaker A: Can we use it in a Zk roll up? Can we use it with our ideas that are very often quite distinct from what is proposed? Maybe like closer to the standard ethereum solutions? Okay, fantastic. Thank you. So what I want to do now is ask some yes no questions and see if people agree on various topics. So one of them is, do you agree that the killer reason why we want shared sequencing is synchronous composability? Do you agree? If you agree, raise your hand. Okay, medium? Why not? Why else would you want a shared sequencer if it's not for synchronous composability? I think it's a promise. Others still, I see it as a difficult task to deliver in the models that are currently offered the composability with l two s buying in that shared sequencing and composing that with their models. I think the differences in timings of broad production on different chains, different finality models, the requirements for good pre confirmations designs, it still feels very far ahead.
01:20:45.086 - 01:21:29.278, Speaker A: So in this way, I say okay, as a promise. Maybe this is a killer feature, but I have a lot of skepticism, but with lots of hope as well that we'll see all the perfect solutions very quickly. I'd love to have it, because I wasn't raising my hand. I actually don't think that synchronous composability specifically is like the core benefit. And I guess maybe we might get in debate over what do we mean by synchronous. In my mind, the view is that if you want to have state, let's say assets, right? If I want to have tokens on roll up a and tokens on roll up b, and those both use the same shared sequencer. If I want to make a transaction on roll up a, using the funds on roll up b, fund mentally, that requires a bridging transaction, and thus I don't believe that is true.
01:21:29.278 - 01:22:23.342, Speaker A: Synchronous composability. Like you're not going to get a flash loan if you want to do something where you say, okay, I have assets on a and assets on b, and I want to do a buy on a and a sell on b using just those assets. Sure, you can have a bundle with. Like we can get into varying levels of security guarantees and unbundling and what kind of commitments, but fundamentally you are going to need that. And then we can think about, okay, what about if we have very fast, very cheap validity proof for roll up a and roll up b? But fundamentally there's still some, I think, research and thinking to be done on, okay, how do I actually know that a given transaction executed on roll up a? My kind of mental model as it stands now, is we always need at least one block delay to do this. Generally that doesn't really stress me out that much. I think that's probably fine, but I do think that shared sequencers and kind of how we came about here was not to say, hey, I have rollup a and I have rollup b, and I would really like these to be able to interoperate as though they were run roll up.
01:22:23.342 - 01:23:27.390, Speaker A: It was far more of like a tooling thing of how do we build rollups in a better way with less overhead for rollup developers. Okay, so basically Thomas is saying that, yes, composability is a killer feature, but it's very difficult. It'll take time, and there's other things that we can get in the short term, such as pre confirmations and maybe finality. And what you're saying, josh, is of you quibble with the synchronous part, you agree with the composability, but synchronous composability is very difficult. It's difficult to do flash loans, and maybe you're limited because the proof kind of has a one block delay. Now, my take here is that eventually we're going to have real time proving, and we might even have like hybrid approach where you only have real time proving for the transactions that need to compose, and that might be like 1% of the transactions and the rest can be delayed. But for those things that we can do real time proving, we can actually get flash loans.
01:23:27.390 - 01:24:56.350, Speaker A: So it is possible to do flash loans between roll ups. So I think more or less there's kind of agreement that composability on the right timescale is kind of the killer use case. I guess my next yes, no question is, is composability with your competitors valuable? Right? Are we trying to build a shared sequencer for competitors to find neutral playing ground? Or is it just the fully integrated Apple and Google silos that kind of don't collaborate? And what do we mean by competitors? Are we talking, say, optimism? Yeah, optimism and arbitram. Is the vision that optimism and arbitrum use the same shared sequencer, or is the vision that they each have their own sequencer? So if you think that they should be sharing with competitors, raise your mean. I guess that begs the question, because you mentioned that optimism is looking to be like, fully integrated Apple style. Isn't that kind of contradictory with the idea that competitors would come in? Yeah, totally. I think whatever sequencer can offer the best execution for users is going to end up winning.
01:24:56.350 - 01:25:59.506, Speaker A: And I think one benefit of building a shared sequencer for homogeneous block space, like all the chains are using the op stack, right, is that the assumptions are all the same. So it's much easier to kind of, as a shared sequencer, understand the cases in which there will be a reorg validity conditions. When you're building a shared sequencer that operates across different technology stacks, it becomes much more complex. And I wonder if it's kind of like the weakest link kind of security model. In that case, you always kind of have to account for the weakest link. And my claim is that a shared sequencer that is building on homogeneous block space will be able to outcompete a shared sequencer that is building on non homogeneous block space, because it'll be simpler to design less lines of code, likely, and it can really specialize for that ecosystem. Okay, got you.
01:25:59.506 - 01:26:58.018, Speaker A: So if I were to try and summarize, what you're saying is that in the short term, when you have a very homogeneous project, you can deliver shared sequencer very fast, but ultimately the market will play out and it might be like winner take most, and market dynamics will force that you kind of compose with the rest of the ecosystem. Okay, very interesting. Now I guess we can split the discussion between short term and long term. It looks like these are different discussions. I guess the value of neutrality in the short term. How big is that? Do you think it's okay to just compromise on neutrality in the short term? Raise your hand if you think it's okay to compromise on neutrality of the shared sequencer in the short term in order to be pragmatic. Okay, no one raises.
01:26:58.018 - 01:27:25.514, Speaker A: Okay. Everyone thinks you should be just fully neutral from day one. Basically, neutrality is always relative to something. So relative to what? Here? Yeah, I guess if we think neutrality, right? I do not consider myself like ethereum aligned in the meme. I'm like celestial aligned, I guess. But therefore, from some context perspective, whatever, right? I am not neutral. I am biased towards Celestia versus ethereum.
01:27:25.514 - 01:28:17.374, Speaker A: I would view a large amount of Ethereum projects being biased towards celestia versus celestia on the margin, right? And so that is a bias. I am not credibly neutral across all possible swaths. If we think about shared sequencers and when we think short term, right. You have to think about, I think fundamentally, like the bootstrapping question, to some degrees is like a customer acquisition question, right, of like a shared sequencer is entering the market fundamentally. Shared sequencers are middleware, essentially, right? And if you think about b two b middleware, historically, you have to generally get like an anchor customer. And what you're going to do is say, what is the pitch that is going to get me anchor customer and whoever is my anchor customer, who is my revenue generation? Presumably some large double digit percentage of all of your revenue. Even if you have long tail customers, that is going to have at least like a forcing function to try to bias you towards the first mover in your system.
01:28:17.374 - 01:29:29.810, Speaker A: You can try to maintain credible neutrality, kind of like over that, but it depends on the power negotiation thing. So it's like, do you have to be less credibly neutral to land an anchor customer if you're going and trying to get existing players? And I think that's just going to be kind of like a market condition. Similarly to Mark's point, right, from just software architecture, engineering design, know, my understanding of espresso's design, and certainly Astria's design is attempting to be very generic towards the execution environment. But what that means is to Mark's point on the weakest link, the coupling between potentially how op node and a lot of op nodes code is checking for reorgs and understanding the state of the l one, how much of that code is getting baked into the shared sequencer? Is your logic more complicated and that you now have, okay, there's a consensus over the validity of the fork choice rule of a block on the roll up. And now is that integrated natively into the l one contracts that optimism's kind of core bridge has done. You get complexity into that, you could probably enshrine a shared sequencer to say, hey, this is optimism's super chain shared sequencer. And thus there is somewhat of a blessed kind of op collective core code integration.
01:29:29.810 - 01:30:16.690, Speaker A: If you're saying, well, I want a shared sequencer that can work with arbitram and Cksync and optimism you're going to have a harder time getting such kind of a tight code level integration with any one of those ecosystems. So when we think about this bias or neutrality, right, it's like at all layers of stack from business decisions to engineering decisions. Thomas, I very much agreed with that. It's a major challenge to navigate from the business perspective of managing collaboration between the competing l two s. So engineers will very often seek those solutions that are introducing composability. So it's not so hard to convince engineers and researchers to start working on something together. And then you bring business and there will be a location.
01:30:16.690 - 01:31:16.950, Speaker A: And it's very risky for chi sequencers to get this idioms from particular chain ecosystem, lto ecosystem. You do that and you start being seen as the native sequencer of there. And the competitors then who capture it, it's like you're losing that promise of composability. I think it's the trickiest thing for shared sequencers. One of the things that I got here was that, okay, maybe there's some perceived loss of neutrality because you're going with one anchor customer first, and it sounds like there's also layers to this, right? Basically the shared sequencing depends on the context of what you want to sequence. If you want to sequence just the op stack or you want to sequence ethereum or you want to do sequencing cross layer ones. I guess my previous model was that sequencing is going to be winner take most, but maybe not.
01:31:16.950 - 01:31:40.538, Speaker A: Maybe what's going to happen is going to be winner take most within their context. But then as soon as you zoom out there's layers of sequencing. Like I guess we could ask a yes no question. Do you agree that there's going to be layers of sequencing and it's not winner take most? Yeah, I guess it depends on what you say. Layers. And I also want to call back to your previous thing, right. You asked the question of neutrality in the short term versus the medium term, the long term.
01:31:40.538 - 01:32:10.742, Speaker A: So I specifically wanted to answer the question like the short term. Right. And that's initial customer acquisition. Do you have to be biased to get the initial one? On the question of layering, we can also think about over what timescales, right. Do more layers accrue if we look at kind of like someone from yesterday at the restaking summit or something had a good thing on number of integrations that a given web two application has versus a web3 application and it's like 15. And it's like, I worked in the middleware space in web two, it's kind of annoying. It is like a valid business model.
01:32:10.742 - 01:32:49.474, Speaker A: But generally, when I came into crypto and I saw, it was like, okay, we have like a app that talks to Ethereum L1. And I was like, well, that's not how web two works by a large margin. Are we going to have layers? And over time, I've seen the. Now we have these searchers, and we have the builders, and we have our PBS, and maybe we enshrine that. Now we have L2s, and we're going to put the searchers and the builders top of the L2, and maybe we can compress some of this. Generally, I do assume there are going to be more layers, whether those layers fall under sequencing. What do we mean when we say sequencing? Because also, I am not assuming that the validators or the sequencers in my network are actively being the sophisticated entities which are actually determined in the block ordering.
01:32:49.474 - 01:33:23.138, Speaker A: Naively, I'm immediately assuming that I'm like, yeah, there's going to be a PBS market, and you're going to have searchers and builders that are doing that. So it may be more accurate to call the shared sequencer network, the shared proposer network, because fundamentally, you're like the back end of the sequencing stack, and that you're not literally creating the sequence. You are accepting or blessing the sequence. So I think there will be more layers. I think suave is very clearly. Is suave a shared sequencer? Right. We don't really know what a shared sequencer is in a concrete point of view.
01:33:23.138 - 01:34:08.980, Speaker A: But directionally, you say, like, SWab has a decentralized network of order flow auctions and searching and building in that that produces a sequenced block. Does that sequence block go into a shared sequencer directly? Is there another layer there? I think we're too early to know the question, but long term, it wouldn't surprise me if there's another three and a half, four layers here. Right. That makes sense, Thomas? Well, just to answer that, Swav doesn't. We definitely don't feel like it's a shared sequencer. We feel that shared sequencer is a receiver of the blocks built by Swab. So SWaf is more like the centralized block builder, plus privacy mechanisms, set mechanisms for that to compose the order flows and block building.
01:34:08.980 - 01:35:10.114, Speaker A: Yeah, right. So I guess just what you were saying is that, I guess part of it is it's very hard to predict how the ecosystem will evolve. And there's a lot of complexity, especially if you're dealing with cross layer one because you don't have this uniform playground to play with. I guess I want to try and simplify the discussion a little bit and take just zoom in to a single layer one. I guess my question is, do you think that is going to be winnertic most within that layer one? And I guess the second part of the question, if yes, do you think that is going to be basically the layer one itself, the base sequencer that's going to win because that's the most credibly neutral and the most secure. Probably winner take most. But generally the model I follow is that in any given kind of competitive segment, you'll have a one and a two.
01:35:10.114 - 01:35:49.646, Speaker A: You may have a third. The third is usually a middling third. So you might see 60 30, ten. Maybe you see 80 1010 or whatever. Maybe it's just 80 20 and there's no third. I do think we've seen this in coke, PEPC, optimism, arbitram, right? These similar colors even. We can question whether, again, is optimism going to use espresso? Is arbitram going to use espresso? Are one of them going to say, what if they both integrate and then one of them says, hey, we're underperforming and we think we could differentiate ourselves by now leaving the shared sequencer and doing our own thing and how that kind of fleshes out, I think bringing the L1 as a base sequencer or whatever we want to call that.
01:35:49.646 - 01:36:43.442, Speaker A: Right. That is also like another thing. And that gets into the credibly neutral from what perspective? Right? If you are just another layer and you are viewed as like a middleman relative to the l one, and the L1 is trying to make its own product and you're just trying to maintain your turf profitability just existence right on that layer, you're no longer perfectly neutral to the L1 ecosystem's development roadmap, noting that we're in a decentralized system and these roadmaps are very contentious on literally everything. So it's not like a great answer, but I do think it would probably be win or take most. But how that ratio splits, I would assume there's at least a second player making some kind of argument in the ecosystem. And then again, it's also dependent on how many roll ups we have and what the market power of any given roll up is. Is that a winner take most market? Okay, so you're saying the network effects are strong, but they're not strong enough that it's totally dominating for one.
01:36:43.442 - 01:37:38.260, Speaker A: Yeah, again, right, like monopoly pricing, power is like a pain. And the goal was that we have enough open source software development that the cost of bootstrapping another one of these systems is sufficiently low that we don't end up in an environment where there is a winner take most and they have incentives to be like a monopolistic actor. I would say that the network effects around shared sequencing are incredibly strong. So to that end I would also say that it is winner to take most. But I don't mean that to the sense that the value would all accrue to a single shared sequencing layer. I think any shared sequencing layer needs to be very fair and needs to be very reasonable in terms of what types of value is it extracting, how much value is it extracting. I don't think it can be really rent seeking at all, because ultimately the roll ups themselves are the ones, the roll ups and their stakeholders are the ones that decide and have the power over what sequencer is used.
01:37:38.260 - 01:38:58.270, Speaker A: And to your second question, Justin, about whether I think roll ups will coalesce around base sequencing, I do think that is yet to be seen. You did make a lot of good points on what needs to happen for base sequencing to succeed in your presentation. And I would say that what we are building at espresso is trying to get as close to base sequencing as possible without being base sequencing. And while preserving a lot of the extremely great benefits, such as fast pre confirmations that we do provide, that we are able to provide with hotshot, which is the consensus that underlines espresso. Also, it's interesting to think about what does it mean that you are the winner? Probably very often we think that you attract the most liquidity or usage, and question is, what happens to liquidity when you cannot move freely in a coordinated way, in an atomic way? And what happens to liquidity when you actually have full freedom of movement and you move anywhere without any risk and without any time delay? So at the beginning you are fragmented, right? Because you're simply limited. You deploy liquidity through onrams and you feel like it's too much risk. I don't want to wait to move it to another chain, so I'll let the opportunity go and maybe I'll be maintaining liquidity on all the different separate chains.
01:38:58.270 - 01:40:38.046, Speaker A: If I can move around without any friction, then actually it's an opportunity for a winner takes it all situation, because liquidity will be staying together like it'll be keeping cozy in one place and doing just this super fast jump for opportunities and then come back. So that almost says that some of the chains may feel reluctant for massively improved coordination mechanisms, whether it's bridging or shared sequencing. And then it will be more likely that it will be within the ecosystem of like op stack will say, okay, so we agree on a shared sequencer because we start from this position and we are feeling like a family of change, but then it will be harder to jump to the next layer, as you're suggesting. So I do think that a lot of the block building will be almost winner takes all, but I'm hopeful that we can figure out a way to change the game so that it's less harmful, less negative externalities from this. Like, for example, if we could figure out a way to do more collaborative block building that might help to improve things like censorship, resistance. And also on your point about credible neutrality, I just wonder sometimes if, I think credible neutrality is incredibly important and it's a great property, but there is no free lunch. And sometimes I wonder about what are the properties of a system that is explicitly not credibly neutral, and will those systems be able to outcompete systems that are credibly neutral in some ways.
01:40:38.046 - 01:41:36.538, Speaker A: And I think that because it's a free market, systems will eventually exist that are explicitly not credibly neutral and attempt to outcompete the credibly neutral systems in the ways that they have advantages. So that's just something that I try to be mindful of when thinking about these systems. If I were to summarize your view, Mark, with Thomas's view, you know, these network effects, which are extremely strong, will take time to materialize because composability is just so difficult to achieve. But that's kind of a good news, because in the short term we're going to have all this experimentation and we might have compromises on neutrality, and that's totally fine. It's great. We need to experiment because the space is so immature. But then as it matures, there's this economic forcing function around shared liquidity, which is going to reinforce the network effects.
01:41:36.538 - 01:42:07.946, Speaker A: And in the end, we might have a really great outcome where there's maybe one or two or three shared sequences, and it's not like a total mess for users. And we have the benefit of trying out things so that what eventually wins out is not just because it was a first mover, but because it's actually truly better technology. Okay. And on that point, I think we're out of time. Thank you so much. Is that right? Or do we have more time? No, you have 30 minutes. Oh, we have 30 minutes.
01:42:07.946 - 01:43:06.982, Speaker A: Okay. I guess we can open it up to the audience, or was there any specific topic? I guess I can kick something off that I guess is related to Namash's point, related to the liquidity thing. I do think when we try to look at, like, I spend a lot of time thinking about classic market structures and what are analogies? I spent a lot of time reading into telecommunications from the distinction between infrastructure and content providers. But there's this missing gap of we have liquidity, and where does liquidity play into that role? Who is the liquidity provider? What are their preferences in the space? And right now, I think liquidity is like, for lack of better word, captured at like a state machine level right now, right? It lives in a state machine. I think what kind of came out to me on the uniswap x paper very specifically, right, is you see this offer of the solver in the uniswap off chain, whatever can provide liquidity from off chain. And so you have a question of, like, that's kind of freeing liquidity, where you say, hey, look, I want to participate in these arbitrages or whatever. I want to be a market maker, essentially.
01:43:06.982 - 01:44:03.066, Speaker A: In this market, I can have my liquidity mostly living on binance or whatever, and I can bring it on chain to settle like an arbitrage. And when we think about that, you question what is kind of the network they want to kind of participate in? Does this liquidity, what is liquidity's kind of desire? Right. Like liquidity wants to get returns on itself? Right. Now, the best way to do that is presumably provide liquidity on the chain with the strongest network effects? Do we actually see state machines like roll ups actually being disintermediated entirely to an app layer? And then is that dependent on the app layer and the order flow system and the settlement, which would be, in this case, like a shared sequencer for a given roll up? That is kind of where my head has been thinking about all of these market structures and where that happens. And now we say, well, maybe the rollups kind of get disinteremated to a degree. It's like the applications that have the front end of the flow. And now it's, well, we're not thinking about does optimism or arbitram care about what shared sequencer.
01:44:03.066 - 01:45:17.510, Speaker A: We're thinking about, well, where is uniswap deciding to have their front end user order flow? Because fundamentally, you go look at the charts, and it's like 40% of all, the flow is like uniswap. So everyone else is like a relatively middling player compared to them. Is that a winner take most market? Is that actually where we see the network effects consolidating, and does everyone end up falling to being a service provider of the front end? That is, I think the larger question for all of this, and we just haven't hopped up to that one more stack of, like, we'll think about it from the applications perspective, which controls the flows. Yeah, I think that's a great point, because we have this sex Dex problem. I used to think that arbitrage was not toxic, but actually it's toxic to the lps because they're being hit by this toxic order flow. And the only way that I know right now to have very good trading experience, have composability, and kind of not have the lps be exposed is, as you said, to have application layer sequencing. And then the question becomes, is that kind of horizontal form of sequencing that kind of cuts through multiple shared sequences, and who has precedence and how do these games play out? Is going to be fascinating.
01:45:17.510 - 01:46:04.742, Speaker A: And you were talking about innovation. And I think that we definitely crushed walls between the chains, between the layers recently. And we started innovating very rapidly, thanks to modularity, thanks to shared sequencing talking, thanks to the decentralized block building approaches, what swab, what MeV is talking about. And surprisingly, we started crashing also the barriers between the off chain and on chain. If we say this uniswap X starts to decide how liquidity jumps on chain very quickly in a way, and we start using chain as a settlement. And it's not only the on ramps, off ramps that decide how we arbitrage between Sachs and Dex. So lots of innovation, how it will end.
01:46:04.742 - 01:46:53.030, Speaker A: I think there will be a bit of a play of the slow transition towards on chain of all liquidity. Now it's still in tradfi, but when tradfi starts to feel that it's so easy to move on chain, and maybe this is that floodgate of saying, okay, so maybe that's where majority of activity will be happening. And we'll be having this improvement in transfers of liquidity within all the chain ecosystems. So everyone will start moving there by default. And it will be that just jumps to the tradfi, to the old world, that will be less and less common. And we could have expected that for a long time, that at some point this will be this tipping point where the liquidity on chain will be dictating prices around the world on all the assets. And this is almost like the ultimate win of all the vision of defi.
01:46:53.030 - 01:48:47.882, Speaker A: Maybe it's a way to reconcile the kind of base sequencing with the non base. The non base could be specialized for applications because that's maybe a use case that base sequencers just can't cater for. You mentioned kind of credible neutrality, right? You mentioned in your base roll up talk, right? We have these mev aware dexes, right? And so we say, okay, well, what if the Dex is like an application that is like mev aware? Is that defined as a roll up? Is that defined as like a network of things? What is the distinction? One of the things I think about a lot is if we look at, okay, we have a swab as an order flow action with uniswap going into that, and then a settlement. And then that settles under a shared sequencer, which may have multiple state machines. What is the important distinction between having one state machine that has multiple pools of liquidity on it and each of those pools of liquidity, whatever, like your USDC Eth pair, whatever being distinct state machines themselves? It's like at some point these kind of barriers and walls and abstractions kind of start blurring together and you question that, okay, who is actually driving this? What is the actual desired architecture? And one of the reasons I think about this from a devtooling perspective is more the like, well, okay, when we think about our shared sequencer and our go to markets, are we encouraging people to deploy roll ups to a shared sequencer? Are we encouraging people to deploy applications to the shared sequencer where the way in which you have created an application is by creating a roll up for that. But that is fundamentally an implementation detail under the hood. And maybe going even further with the architectures, we can think of some kind of multi based roll ups, the ones that will be based on multiple layer ones, just to atomically move the native assets for those layer ones? So if I keep saying that cosmos, Salana and Ethereum will merge, maybe they will merge on this L2.
01:48:47.882 - 01:50:02.130, Speaker A: So L2 will be just facilitating the asset transfers between the L1s. But the asset transfers between l two s will be facilitated by the base shared sequencers from different L1s. Fascinating. I have one more burning question and then I guess we can open it up to the audience. My question is around the timing of shared sequences. One of the things that I highlighted is that when you have a centralized sequencer, the centralized sequencer can act as a lay of safety because they can make sure that invalid blocks that kind of exploit a vm vulnerability can't go on chain. Do you agree that this is a big problem that needs to be solved before we can see sequences? And if so, do you agree that it might take years before we fix all these vm vulnerabilities? And therefore shared decentralized sequencing is just something that has to wait? Is this just kind of correct? My understanding is this saying that there are bugs in a vm and such that like a bridge contract that doesn't have a delay on it or something from an initial asset issuance on an l one, you have one bug and all the money's gone, and there's no reconciliation from that.
01:50:02.130 - 01:50:53.762, Speaker A: And pushing it to a shared sequencing layer or something is going to remove the kind of centralized safety guard kind of in that mechanism. The way I guess we view it is like our shared sequencer is lazy in that it is giving a commitment over essentially like a four choice rule over the ordering of the goon block. But there is still a distinct roll up node that is fundamentally responsible for deciding whether or not it's going to update its state based on a soft commitment from a shared sequencer, a firm commitment from presumably like a data layer. And then the actual kind of issuance of assets on that given roll up is still going to be defined by the rules encoded in that state machine, such that it could still say, yes, I'll take a soft commitment here, but I won't unlock funds or whatever for a delay on a bridge or whatever like that. Right. You still have some of that. I guess there's a question of also hard fork ability, essentially, of exploit things.
01:50:53.762 - 01:51:33.882, Speaker A: Right? And so you push a. Well, we had a reliance on one entity to a reliance on some shared sequencer network. That's reorg. I think we look at this a lot from, if we think about like an l one and like an increased decentralization, right? Like how much drama is there about of like, what if there's some attack on Ethereum? Would it hard fork. Again, the Dow is still like a very controversial thing, but it starts to look a little bit like that when you say, well, I'm a roll up and I use a shared sequencer, and if I have a vulnerability, if my resolution to that is to hard fork the shared sequencers kind of state versus just the state that is maybe purely within that. Again, we get into a credibly neutral thing. Right? If you say I'm a roll up, that is 80% of the flow in a shared sequencer.
01:51:33.882 - 01:52:49.618, Speaker A: Yeah. If I have an exploit, I'm probably going to be able to get through to reorg that shared sequencing layer. If you're the more long tail kind of user of that shared sequencer, and you have an exploit, maybe you're going to be out of luck and your assets are just going to be gone. If you have an exploit. It's not a good answer, right, in that we don't have a concrete thing there. But I do think it's something to kind of keep in mind from, to use the celestial terminal like a sovereign roll up, right? How much sovereignty do you have over the validity of your state machine? And what are you giving up by offering that to someone else? So, something about worrying about bugs is when the block space is all homogeneous, you don't need to necessarily worry about bugs in remote chains as much, because the same bug, if there is a bug, the bug is everywhere, and we can kind of encapsulate it into the same ecosystem. So one kind of mental model to think about with shared sequencers, when it comes to say, like building with the op stack, is how can we go about building mechanisms that make it easier for sequencers to just trust each other? And on top of that, a shared sequencer can just arise.
01:52:49.618 - 01:53:47.034, Speaker A: It's much easier to build a shared sequencer when these mechanisms exist. And kind of like I mentioned before, this idea of the weakest link security model, when you're going across many different heterogeneous chains, as a shared sequencer, you need to think about what is the weakest chain, what is the weakest security provided by any of the chains that I'm sequencing over. And that's what I need to keep into account. So, big benefit of homogeneous block space. I think a good prior art for that problem of weakest link is if you're in the cosmos ecosystem and you do IBC bridging, there is an inherent past dependency to the existence of a token on any given chain. If I make three hops through whatever, atom to neutron to osmosis or whatever, that is not the same as going the other way and landing on the same chain. And the unwinding of these assets is frankly like an annoying known problem.
01:53:47.034 - 01:54:42.220, Speaker A: But it is like the technically correct mechanism, because there is a fundamental distinction in what an actual asset on a given destination chain represents based on the path and the trust assumptions it made in its journey to land on that chain. And there's a question of if we see a similar thing where, yeah, okay, these are a lot of heterogeneous things. You can bridge between them, but they're all wrapped a little bit differently. And it is a problem that seems kind of manageable at a handful, like five whatever chains. If you're at 100 and you have this graph of, you can route combinatorially, whatever that many ways, it is kind of like a difficult to resolve problem, and I think you will see some kind of centralized thing in there. So there's a big question, that weakest link thing, does it lead to more consolidation and saying, actually this fragmentation is undesirable. We want some strong security assumption on some more centralized thing, and then we have kind of like hub and spoke model or something.
01:54:42.220 - 01:56:09.560, Speaker A: Before getting into the implications with interoperability and bridging and all that, I do have some questions about the way we're looking at this, Justin, because these security issues that you identified, they seem a little bit orthogonal to the approach that espresso and at least I believe Astri are taking, and the lazy sequencing approach that Josh mentioned, which is really just taking data, arbitrary data, and really just outputting an ordering on it. I think the claim here is that centralized sequencing as it is right now, somehow avoids these concerns, because the centralized sequencer can really manually intervene if there is some sort of bug in the vm. And it seems setting aside implications from something bad happening on one roll up affecting something else, if there's bridging between the two, it does seem like even with a decentralized sequencer, at least in our models, the roll up and the execution part of the SAC could still intervene if something terrible were to happen. So I want to hear your thoughts on this. Seems orthogonal to me. Okay, so what you're saying basically is that in this lazy module, there's kind of a two phase sequencing. There's like the default, which could be Astrio espresso, which will propose a sequencing, and then it kind of gets stamped by some sort of centralized, one of one committee almost.
01:56:09.560 - 01:56:46.574, Speaker A: And then if it passes these two steps, then it goes on chain. Well, I'm not saying that it's stamped. Right. Ideally, users are trusting espresso for pre confirmations and for finality, but if something were to drastically happen, something were to go wrong in a centralized model, then the roll up would have to take special action to fix it. Right. And it could be the same case. Even with a decentralized sequencer at a technical level, you have a guest node that is actually just reading a stream of ordered transactions and executing that and updating it to state db and getting a state route.
01:56:46.574 - 01:57:10.822, Speaker A: Right. And if you have a bug in the process, of how that happened. Right? Same thing. Right? Like hard fork, new software version. If block height greater than this carve out this, you are still relying on the social consensus of people accepting the defined thing. And the kind of question is, where does that block stream come in from? In a shared sequencer, it comes in presumably from this other shared sequencer network. In a centralized sequencer.
01:57:10.822 - 01:57:59.990, Speaker A: It is coming from a centralized sequencer. But even in look at Arbitrum's diagram models, which I'm slightly more familiar with writing, they clearly distinguish a sequencer from a validator in their model where the sequencer just proposes this ordered list of transactions, and the validator is the one that generates a state route, makes a claim on the actual end state of any given block height, you're kind of forking that validator role. And so maybe it looks kind of the same, but you do have a more complicated kind of like social consensus problem. And that roams back to the credible neutrality stuff. Right? So you're saying that the social layer can intervene. And I can see that in two cases, one is where you have a sovereign roll up and you can just reinterpret the stream of transactions. And then option two is you have delayed settlement between the inclusion and the execution.
01:57:59.990 - 01:58:20.960, Speaker A: And then here you can have emergency action from on chain governance. But in the case where you have a non sovereign roll up which doesn't have delayed execution, then you do have the problem. Okay, I buy that. Yes. Okay, great. And I think that is the end game. Right.
01:58:20.960 - 01:58:54.890, Speaker A: One of the killer use cases of ZK roll ups is this instant settlement. And many ZK roll ups are non sovereign. I mean, all the Ethereum ZK roll ups are non sovereign. Yeah, I guess I maybe don't see the distinction, the difficulty of the validity proof. Are we talking if there's an error in the validity proof machine? Right. Then I think it's solved. Still the same way as what you mentioned in your previous talk of multiproving or various fallback reclamation kind of mechanisms of how do we avoid what we might be up to, like a couple of hundred people? Multiproving.
01:58:54.890 - 01:59:12.846, Speaker A: Exactly. Yeah. It's the question. I think there's a fundamentally always going to be a trade off of like if you do instant settlement, you have paid for your coffee and the person has left the store and there is a bug, then you're like, well, the person is already out the store. You're not getting that back. And that's always going to be just not even in ZK systems. It's fundamentally like settlement systems.
01:59:12.846 - 01:59:50.814, Speaker A: Right? There's many tradfi settlement systems like Swift and whatever, right. That you say, well, you took the claim and you settled your thing, and it does not work to go backwards, no matter how much you want it. I just don't think shared sequencers structurally change. How fast are you willing to have your settlement? And what risk of bugs are you accepting in that mechanism? If you say no, we settle and it's like one block and we're done and we're moving on, I think you still expose yourself to that kind of design space and risk space. Okay, we have ten more minutes. Let's open it up to questions from the audience. We have questions here, a couple of questions.
01:59:50.814 - 02:01:00.626, Speaker A: Where's my guy? Hey, yo, what's up, guys? This was fun. So I want to bring this back to the talk about liquidity as what we're all kind of, I guess, competing for. I think it's sort of, like, reductionist to say that we're fighting for the same set of liquidity. Obviously, I'm biased here, but thinking about Uniswap X and kind of extrapolating that it's only around, like, the Uniswap front end flow is only, like, 15 ish percent of all uniswap flow, right? So it's not a ton. But then we think about just, like, all flow through Uniswap, which is like ten ish billion a week. So it's a decent amount. But I guess I think it's tough to sort of conflate everything and fight for the same pie.
02:01:00.626 - 02:01:56.760, Speaker A: And kind of like, stepping back to what Tamas said about we want new liquidity, we want to grow the pie, and actually have more different types of liquidity coming into the space. And so, I guess, stepping back, I'm curious to hear all of your thoughts on what actually is the ideal state for sequencing for dexes. We can talk about the uni chains, the application, the app chains for most of these dexes. I think we sort of fight to come to the same conclusion where it's like, again, we're fighting for the same liquidity. So I guess, I think in the ideal world, we head to a state where we have unified liquidity layer or something. Maybe Uniswap becomes that, but I guess, yeah. In your minds, what is the ideal state or end state? Anyone want to take that? Sorry, I thought it was question in that direction.
02:01:56.760 - 02:02:59.528, Speaker A: The ideal end state for all of this. I think that's what I mentioned. I think that everything will act for us as a single system in the end, we will see all those chains that for us nowadays are so different and so maybe belonging to different tribes as a one mechanism with lots of different engineering details. And as for liquidity, in the end it will always just be all on chain, and probably very much concentrated in some area and maybe deployed in some niche areas for very specific use cases. Or it has to convert. I'm not sure if it answers perfectly your question, because you were talking a lot about maybe the more close near future with insect uniswap x coming, and with other solutions, with solvers, with talking about the order flow. So we were always talking about building the new financial ecosystem.
02:02:59.528 - 02:03:51.828, Speaker A: We're not really talking about building ten financial ecosystems. And I always talk about maybe breaking the borders, because that's what we had in the world. We had the capital controls and borders and bringing them again to digital asset space. I feel this will be very quickly crashed into like a single field for liquidity flowing, moving very fast. Does it answer the question, or I missed some nuance in it, or I missed it entirely? No, it's all good. I meant more so on the technical side. But I think I agree with that vision, though, on the technical side of implementations of those methods of solving the liquidity, like the swaps.
02:03:51.828 - 02:04:47.720, Speaker A: Yeah, this is evolving fast. The, with the RFQ mechanism nowadays, passing that to the external players. I think with ZK systems, we move all the computation off chain, right? So the simulations and solving of establishing the prices, the auction systems, it will move off chain as well. And the setting will be happening entirely on chain, and all the assets will be on chain. So I think this is the way as we go, that there'll be much more. We'll see the entire set of maybe tens or hundreds of players that are capable of solving all the, all the swaps and movements as. As one ecosystem.
02:04:47.720 - 02:06:18.548, Speaker A: So maybe even this temporary competition between the different RFQ systems, we'll see it as one market, the same as we see it in traditional finance, I guess giving like a quick shared sequencer perspective here or whatever, right? Fundamentally, I see the benefit of a shared sequencer from when we see a lot of this kind of auction mechanisms moving off chain. The benefit is like this hotel train problem thing, right, where you can say, I can have things on two places, and even if you have to have assets on different chains to make a specific execution on a specific pool or whatever, right. Fundamentally, you're able to get some level of guarantee with varying levels of kind of commitments and trust assumptions in it that you will say, I will buy here and I will sell here for multiple chains. So it kind of is one mechanism of kind of getting more homogeneous liquidity from a desired perspective of maybe liquidity providers or maybe takers more. So we want the liquidity to be used more efficiently, which is presumably gathering in one venue where as I'm some market maker or whatever, I want to integrate with one API, not like 15 APIs, that's more efficient. From a technical perspective, I do think there's questions about what pushback you see from an end user perspective over centralization. But right now, generally I would agree in the short term, the demand is make things better because the user experience could be better, and we will presumably get more capital flows into the system if the user experience is better.
02:06:18.548 - 02:07:15.772, Speaker A: And from a competitive perspective, it's more desirable to be an environment where capital is flowing into the system such that we're less competitive with each other in a direct sense of feeling like there is one bag of liquidity and we must all vampire attack each other to get any sufficient liquidity. So those are kind of the things I see as like a desirable trajectory of the space. I think it's really easy to hypothesize about what the technical architecture of these systems will look like, whether they're built with intents or whatever other buzwords there are. But I do think it's really important that the end goal is that actual value is being created for end users because it's really easy to kind of get caught in the fun of the research and the technical design. But at the end of the day, we need to build things that users want to use and provide value for them. So I think that needs to be the end state. Okay.
02:07:15.772 - 02:07:49.572, Speaker A: And on that note, we're out of time. Thank you so much for your answers and thanks to the audience. Yeah, you guys don't have to worry about turning it off. They'll do it on their end. Thanks for showing up. We're breaking for lunch now, so I made this announcement earlier, but we're doing lunch upstairs and downstairs. Don't feel the need to crowd all downstairs.
02:07:49.572 - 02:17:48.182, Speaker A: Like, if you guys want to potentially get shorter lines and quicker access to food, definitely go upstairs. I guess these folks are hanging around if you guys want to catch them while you have the chance. Otherwise, we'll see you back here in 1 hour. You. It's it. Don't don't don't don't don't don't don't don't don't don't don't dip dip. Dip.
02:17:48.182 - 02:18:46.822, Speaker A: Dip. Dip. Drip. Dip. Dip. Drip. Dip.
02:18:46.822 - 02:18:54.810, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Drip.
02:18:54.810 - 02:33:34.074, Speaker A: I see. Read it. It's you. Don't. Don't. Don't. Rip.
02:33:34.074 - 02:33:38.970, Speaker A: Dip. Dip. Drip. Dip. Dip. Dip. Dip.
02:33:38.970 - 02:33:43.230, Speaker A: Drip. Dip. Dip. Dip. Drip. Dip. Dip.
02:33:43.230 - 02:33:48.618, Speaker A: Dip. Drip. Dip. Dip. Drip. Dip. Dip.
02:33:48.618 - 02:33:53.162, Speaker A: Dip. Dip. Drip. Dip. Dip. Drip. Dip.
02:33:53.162 - 02:33:57.418, Speaker A: Dip. Drip. Dip. Drip. Dip. Dip. Drip.
02:33:57.418 - 02:34:03.842, Speaker A: Drip. Dip. Dip. Dip. Dip. Rip. Rip.
02:34:03.842 - 02:34:11.714, Speaker A: Rip. Rip. Rip. Rip. Rip. Rip. Rip.
02:34:11.714 - 02:34:22.410, Speaker A: Rip. Rip. Dip. Dip. Dip. Dip. Rip.
02:34:22.410 - 02:34:44.370, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Dip.
02:34:44.370 - 02:34:49.910, Speaker A: Dip. Dip. Dip. Dip. Drip. Dip. Dip.
02:34:49.910 - 02:38:16.490, Speaker A: Drip. Drip. Drip. Drip. Drip. That's. Here's it's you.
02:38:16.490 - 02:49:30.214, Speaker A: It's it's sa it. Don't. Don't. Don't. Dip. Dip. Dip.
02:49:30.214 - 02:49:33.734, Speaker A: Dip. Drip. Dip. Dip. Dip. Dip. Drip.
02:49:33.734 - 02:49:49.290, Speaker A: Dip. Dip. Drip. Dip. Dip. Dip. Dip.
02:49:49.290 - 02:49:53.658, Speaker A: Drip. Dip. Dip. Dip. Drip. Dip. Dip.
02:49:53.658 - 02:49:57.706, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Drip.
02:49:57.706 - 02:50:14.142, Speaker A: Dip. Dip. Dip. Dip. Drip. Dip. Dip.
02:50:14.142 - 02:50:18.630, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Dip.
02:50:18.630 - 02:50:22.054, Speaker A: Dip. Dip. Dip. Dip. Dip. Rip. Rip.
02:50:22.054 - 02:50:46.958, Speaker A: Rip. Rip. And dip. Whip. Dip. Dip. Dip.
02:50:46.958 - 02:50:52.290, Speaker A: Dip. Dip. Drip. Dip. Dip. Dip. Dip.
02:50:52.290 - 03:05:30.706, Speaker A: Drip. We it's f Don'T. Dip. Dip. Dip. Dip. Drip.
03:05:30.706 - 03:05:34.550, Speaker A: Dip. Dip. Dip. Dip. Drip. Dip. Dip.
03:05:34.550 - 03:05:44.454, Speaker A: Drip. Dip. Dip. Dip. Dip. Drip. Dip.
03:05:44.454 - 03:05:48.950, Speaker A: Dip. Dip. Drip. Dip. Dip. Dip. Dip.
03:05:48.950 - 03:05:58.590, Speaker A: Drip. Dip. Dip. Dip. Drip. Dip. Dip.
03:05:58.590 - 03:06:03.870, Speaker A: Dip. Drip. Dip. Dip. Rip. Rip. Rip.
03:06:03.870 - 03:06:11.710, Speaker A: Rip. Rip. Rip. Rip. Dip. Rip. Rip.
03:06:11.710 - 03:06:17.794, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Dip.
03:06:17.794 - 03:06:21.586, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Rip.
03:06:21.586 - 03:06:40.390, Speaker A: Rip. And dip. Dip. Ding. Dip. Dip. Dip.
03:06:40.390 - 03:10:18.370, Speaker A: Dip. Dip. Dip. It it's don't check. Check. Sweet. We're going to invite our next speaker up on stage.
03:10:18.370 - 03:10:38.470, Speaker A: Hope you guys had a good lunch. Clearly, I ate too much sugar and lost track of time. Come on up, Ryan. We have Ryan Knitz from base Coinbase. Let me get you this clicker, if you don't mind grabbing that. Actually, yeah. That remote.
03:10:38.470 - 03:10:57.710, Speaker A: And this is for you, sir. Good luck. I'm here if you need anything, all right? Yeah. So if you press next on the remote, it'll just cycle through your should be good. Yeah. This isn't you. This is the wrong deck.
03:10:57.710 - 03:11:42.070, Speaker A: Do you guys mind changing his? So go back one previous while we're waiting for that. How's everyone doing today in Istanbul? Such a beautiful day. While we're waiting for them to bring this up, I walked out of my hotel. Is it still on? And I was so happy. It was so sunny, and I'm just walking along, and then I climbed this mountain, and then I wasn't so happy for a second because I just had a big turkish breakfast, and I'm like, up at the top of the mountain. And then I started feeling better again. So happy to be here.
03:11:42.070 - 03:12:01.132, Speaker A: I ate too much baklava during lunch and I have like a semi stomachache from all the sugar, so I need a walk. But enough of me talking. I think this should be better. Yeah, that looks better. We just need it up on the main. Okay. Yeah.
03:12:01.132 - 03:12:20.156, Speaker A: So you have maximize your time. All right, well, welcome everyone. I'm Ryan Knitz. I'm here to talk to you about base and the path to decentralization. So this is the agenda. We're going to talk about why base. We're going to talk about why decentralization.
03:12:20.156 - 03:12:55.080, Speaker A: We're going to talk about how decentralization. And then we're going to touch on the ecosystem for a second and then I'll share some news, some things that are relevant to base, what's going on? And then I'll close out a bit. So why base? Can I get a raise, a show of hands, who's familiar with base? Okay, that's awesome. Who's deployed a contract on base or adapt on base? Okay, that's not as awesome, but we'll get there. So for those that don't know, base is an l two network. It's built on the op stack. It's a public permissionless blockchain.
03:12:55.080 - 03:13:25.748, Speaker A: It's being incubated by Coinbase. So I forgot to touch up on who I am. I'm Ryan Nitz. I'm the head of Base Devrel. I have been with Coinbase for a couple of years before coming into more of a field based role. I spent 20 years as a software engineer, mostly focused on Internet infrastructure. So one of the reasons that we're building base is because we believe that on chain is the next online.
03:13:25.748 - 03:13:58.880, Speaker A: We believe that in order to increase economic freedom, we need to move people more on chain. And so let me just talk you through a little bit of the history about how we got here. And so pre Internet era, you were basically offline. If you wanted information, you were consuming it from a centralized organization. You get it from your newspaper, you get it from a bank. If you wanted to build an application, you really had no kind of way of distributing this globally. You were kind of siloed based on where you were and where you were living.
03:13:58.880 - 03:14:41.672, Speaker A: The Internet came around and that's the technology that's obviously enabling us to stream this live. That's the technology that built, that provided the infrastructure for Google, Instagram, Facebook, WordPress, et cetera. And so information started to flow. We actually started churning into more of a global economy. And for the first time, in history, anyone anywhere could build an application that could be consumed by anyone anywhere else in the world. And this really kind of revolutionized what was going on. But what was missing, and what is missing in that online revolution is the ability to do so for anyone anywhere, to build an application with value that can be consumed by anyone anywhere.
03:14:41.672 - 03:15:10.640, Speaker A: And so this is where we're at now. And we believe that this is going to be the most important fundamental developer platform going forward in human history. So a lot of you probably have seen this. This is the number of Ethereum addresses, unique Ethereum addresses. And this is continuing to grow up and to the right. And so the number of users that are on chain continues to grow. This is the number of developers.
03:15:10.640 - 03:15:35.244, Speaker A: This is a little bit less. The trend is still up and to the right, but this is a little bull bearish cycle. But given the number of people that are in Istanbul this week for Devconnect, I'm pretty excited. I'm pretty confident that we're going to continue growing up and to the right. And we're seeing all these use cases move on chain at a staggering pace. We're seeing social media and friend marketplaces come on, chain. We're seeing restaurants.
03:15:35.244 - 03:15:42.556, Speaker A: Come on, chain. We're seeing games. Come on, chain. We're seeing collaborative art. Come on, chain. And we're seeing advocacy. Come on, chain.
03:15:42.556 - 03:15:59.568, Speaker A: With stand with crypto. And these keep growing and growing and growing. A lot of you have seen this chart before. This is the number of l one transactions. That's the blue line. That's the number of Ethereum transactions. And the number in red is the number of L two transactions.
03:15:59.568 - 03:16:27.008, Speaker A: And you can see just a couple of years ago, the number of L two transactions was less than 100th of what the l one transactions were. You can also see that the Ethereum transactions are basically flat. And that's because Ethereum for more or less is running at capacity and there's not a lot of room for those number of transactions to grow. So where we're at now, just a quick shill for l two beat. Obviously, it's one of my favorite websites. Look at it daily. That's where this chart came from.
03:16:27.008 - 03:16:55.268, Speaker A: So where we're at now, we're about 4.74.8 x number of transactions that are occurring on l two s versus the l one. And this number will also continue to go up and to the right over time. So we'll be at five x, we'll be at ten x, 100 x. In the not so distant future of transactions on l two s but we're far from everyone being on chain. There are ten, 8 billion people in the world. There are roughly 10 million people that are on chain.
03:16:55.268 - 03:17:17.856, Speaker A: So there's plenty of opportunity over the next decade for these people all to come on chain. The key blockers that we're seeing that we are identifying as what's prohibiting that kind of growth on chain. Moving the rest of the users is cheap. Block space. Block space is still too expensive. Brian Armstrong is not shy about sub $0.01 transaction 1 second block times.
03:17:17.856 - 03:17:52.830, Speaker A: So this is what at base we're moving towards, and I'll discuss 4844 EIp in a bit. The wallet technology is another kind of blocker that we're dealing with. Wallets were designed by engineers, for engineers. I personally like them, but the UX is not good enough for the masses. And so getting the next generation of wallet technology out there so that the experience on chain is as seamless as it is online right now. And lastly, identity. Identity is something that people are starting to work on and it's starting to occur, but there's still really a blocker there.
03:17:52.830 - 03:18:53.312, Speaker A: The blocker is having verifiable identity and having identity that can establish credit. Right now, if you want to take out a loan, you can take out an overclowerized loan, but that's not what's going to grow an open global economic system. It's going to be identity that leads to credit, and that's going to help grow and bring more people on chain. So one of the reasons why is Coinbase building a decentralized network out of a centralized organization is really the question that I'm sure a lot of people are asking. And the answer to that is that we believe that decentralization increases economic freedom. We believe that decentralization increases property rights, so that you can own what you own, stable currencies, so that your assets are not inflated away overnight. It increases free trade, so that no matter where you are in the world, where you were born, where you're currently living, you'll have access to the same services if you have a connection to the Internet that everyone else has access to.
03:18:53.312 - 03:19:28.440, Speaker A: And why is economic freedom so base believes that economic freedom, decentralization is what will enable the open global on chain economy. Economic freedom is really good. So there's a number of studies out there that showcase that economic freedom increases life expectancy, literacy, company formation. All the things that you want to see go often to the right. And the things that you don't want to see, all the bad things decrease with economic freedom. And so it's really, really good. So to summarize, decentralization enables the open global economy.
03:19:28.440 - 03:20:06.640, Speaker A: Open global onchain economy improves economic freedom, and improved economic freedom makes the world a better place. And so this is the why decentralized. Now getting into how to decentralize is more of a challenge. So we've been building base for a couple of years now, and really these are the three major challenges that we found. There's a technology issue, there's a maturation issue that we're dealing with. It's being launched out of a centralized coinbase with a lot of eyes on it, and the regulatory environment is definitely uncertain. I'm going to go back to another l two chart.
03:20:06.640 - 03:20:41.440, Speaker A: So one of the things on the technology maturation side is the fully decentralization of l two networks. And so l two beat does a great job in terms of defining the different stages. So stage zero, full training wheels. If something goes wrong, you can step in and you can fix that problem. Stage one is less training wheels, but still not fully decentralized. And stage two is the same security and the same decentralization as what you see on Ethereum. And so you can see that there's a couple stage ones, actually, there's one stage one in this chart.
03:20:41.440 - 03:21:15.176, Speaker A: There's a bunch of stage zeros, there's one stage one, and there's no stage twos right now. So it's not just base that's dealing with this and the op stack, it is everyone and nearly everyone in the l two space that is trying to get into the maturity of the network. The other thing is, this has been done before. It's being launched out of Coinbase. So the op labs launched the op Mainnet. This came out of op labs. The off chain labs launched Arbitrum.
03:21:15.176 - 03:21:56.036, Speaker A: And these were all from centralized organizations before they were fully decentralized into the Arbitrum Dao and into the optimism foundation. But this took some time, and they did not have the same level of scrutiny as being a publicly traded company. They didn't have the scrutiny of the eyes of the world on them. So this is one of the challenges that we're dealing with, and nobody's actually done this from a large organization before. The regulatory environment is something that is also that you probably feel more than you see. You feel it in terms of every day when you're building, trying to deal with the different issues that you're seeing. You feel it in the legal bills, the tens of the hundreds of thousands of dollars that you're spending.
03:21:56.036 - 03:22:34.452, Speaker A: And that's all taking away from the engineers that you could be paying. And all this slows down innovation. And the sooner that we can get people to actually put a stake in the ground in terms of regulatory clarity, the sooner that we'll all be able to just focus on innovating instead of dealing with this kind of headache. So now I'm going to talk a little bit about how to decentralize. We covered the kind of concerns and the issues that we're facing in decentralized. So now we'll go into the pad the plan for decentralization. So these are the three commitments that we're making over the course of the next number of years in order to get to a fully decentralized base.
03:22:34.452 - 03:23:08.224, Speaker A: And so first is an investment in technology maturing the technology stack, the op stack, the public goods stack. The second is participation in an open neutrality framework, and the third is through funding of public goods. And I'll dive into each of these as well. So the technical investment. So we are the first external organization to be contributing to the op stack. We have been working with the optimism foundation on op Geth, op node. We're also working with paradigm on the opreth that should be ga later this year.
03:23:08.224 - 03:23:57.872, Speaker A: We're working with the Aragon team as well. We're also working on the next generation of the Security Council for the superchain in terms of defining the technology so that the upgrade process is not limited and it's defined through a decentralized organization. Lastly, we've been contributing open source. Everything that we're building for base is being added to the base or base GitHub ecosystem or organization. So we released pessimism, which is a variant set of tools which anybody that's building in the op stack can take advantage of. You define all your use cases and it'll quickly alert you if there's some sort of issue that's observed on the chain. The other thing that we're building is OPVM and WAgMe, and these will be released this quarter, this part of the year.
03:23:57.872 - 03:24:40.510, Speaker A: So these are just enabling more of a modernization of the tool set for building on the op stack. And so these are not base specific. Anybody who's running either in the superchain or isolated can take advantage of this tooling as well. Additionally, one of the challenges that we have is how do we get to ZK proofs. And so the optimism foundation put out a mission, an RFP and risk zero and one labs actually put out proposals. And actually I saw demos yesterday which is pretty amazing of bringing ZK proofs onto the op stack. And so not just for base, but anyone that's running on the op stack will be able to take advantage of these.
03:24:40.510 - 03:25:38.636, Speaker A: So this is kind of a roadmap 2024 we want to get to stage one early 2025, we want to get to stage two. So that way we'll be fully decentralized and have the same security and assurances that you do on the main net, the L1 for Ethereum. And this is again, not just something that base can take advantage of, but this is anyone, Zora, PGN and any other op stack roll ups can take advantage of this technical investment participation in open neutrality framework. So as I mentioned, we want to get to the same permissionless level as Ethereum op mainnet. And so to enable this, the law of chains was defined. And the law of chains is really defining the requirements around state transition in terms of security and liveliness of the service. And if you don't meet those requirements, there'll be consequences as well as universal governance for approvals and upgrades.
03:25:38.636 - 03:26:37.776, Speaker A: And so all this will enable base to operate in a more decentralized manner. In a decentralized manner. So then we'll have the same level of freedom, or we'll all have the same level of freedom. This law of change is really great because if somebody comes in and tells Coinbase that you have to censor these transactions, the optimism foundation and collective actually has the ability to actually reroute the sequencer to someone else so that they can remove that kind of issue that would happen. So that's something that is incredibly exciting for everyone. Funding of public goods we announced not too long ago that a portion, roughly 15% of all sequencer revenues, will end up going to the optimism foundation. You can view this on chain, you can look at ether scan, you can look at base scan, you can see these assets moving into the funding of public goods, which also is great because base projects are now eligible for optimism funding.
03:26:37.776 - 03:27:28.048, Speaker A: So retro, PGF, anyone who applies to that, if you're building on base, you also are eligible for those level of funding. So just to summarize, we have an investment decentralization, we have a participation in the neutrality framework, the law of chains, and then funding of public goods is our path to decentralization. I'm going to touch really quickly on the ecosystem. This is kind of the logo page that we launched with. There's more than 100 DApps protocols, infrastructure providers who have enabled base. We are looking for the next generation of builders and definitely would love for you all to apply to be a part of the base ecosystem, and that's the earl that you can go to in order to participate. And what we're really looking for is we're looking for innovative use cases.
03:27:28.048 - 03:28:10.972, Speaker A: We're looking for people who are bringing new projects on chain. So I think this is actually a golden opportunity that we saw when the Internet came around, where people started deploying apps or businesses that were offline online. And there was a huge revolution of productivity, there's a huge revolution of wealth creation. And what we're seeing is we're seeing that happen right now with the move to on chain. And so it's really a golden opportunity for developers and builders out there. I want to touch on what's new real quick. So we co authored the EIP 4844 with the Optimism foundation and the Ethereum foundation.
03:28:10.972 - 03:28:49.368, Speaker A: And for those that are not familiar, it's a mechanism to make l two s more efficient in terms of the promotion of information to the l one. It is coming out in the Cancun release. We're hoping that that will be in early 2024. We're expecting networking fees, network fees to drop roughly 50%, if not higher. And it fixes kind of the problem that Ethereum wasn't really originally built for l two roll ups. And so instead of monkey patching it, it makes it part of the core infrastructure. So we are busy with the canyon fork, which is integrating 4844 into the op stack.
03:28:49.368 - 03:29:29.690, Speaker A: It's also adding functionality to ensure compatibility with the Shanghai release. So there's push zero create to deployer. Things like that will be coming out in the canyon fork hopefully in December for all the op chains. We recently launched sepolia, which is great, getting ready to migrate there. And then lastly, we recently announced a paymaster on our testnets. And what this does is any smart contract wallet is able to get access to one ETh per week of testnet funds. And so you can close out those faucet tabs and you can actually start using the ETH there for free.
03:29:29.690 - 03:30:20.356, Speaker A: We are also announcing that we're moving the paymaster into Mainnet. And with Mainnet, what we're looking to do is we're looking and still trying to figure a lot of this out is can we sponsor contract deployments, a number of smart contract deployments to Mainnet, giving more people who may not have a bunch of eth laying around the ability to push their apps into production, as well as potentially sponsoring networking fees for smart contract wallets. So there is a bit of a hurdle there. You do need to integrate smart contract wallets into your protocol, into your DAP. But we believe that this is the future in terms of how we're going to solve some of the issues with usability with wallet technology. So I would ask you if you're interested in discussing the Paymaster to reach out. That is my farcaster.
03:30:20.356 - 03:31:13.630, Speaker A: And then there's the telegram and everything like that. We're looking for a number of early adopters to get in there and start taking advantage of the Paymaster for the hackathon this weekend. If you're participating in the hackathon, definitely reach out, contact me, and send me your smart contract wallet address, and I'll get you allowed listed for the hackathon for the production for the main net Paymaster. What this is going to do is this will enable $50, roughly worth of ETH that you can use to provide gasless transactions you can use to deploy smart contracts, et cetera. We're going to send around for people that reach out. We're going to send around an example about how to deploy smart contracts using a smart contract wallet so that you can take advantage of the Paymaster. We have a simple dap that we're going to send around that enables you to post your smart contract into a form, and then it'll deploy via that smart contract address.
03:31:13.630 - 03:31:48.264, Speaker A: And so, just to summarize, we covered why base, why decentralization? Path to decentralization. Touched on the ecosystem, touched on what's new. And I just want to leave you. This is Jesse Pollock, the creator of bases chart. And so we touched on how, in a very short period of time, we went from one 100th level of transactions on l two s to being nearly five x today. And this is where we think we're going very quickly. So thank you so much for joining us today and look forward to hearing from you all.
03:31:48.264 - 03:32:07.324, Speaker A: Thanks. Wow. Do you want to take questions you actually like? Oh, absolutely. Your american energy is like, yeah, I got to get this together for the afternoon. You got five, six minutes if you're okay to take. Oh, awesome. Yeah, I'd love to take questions if people have them.
03:32:07.324 - 03:32:50.744, Speaker A: Yeah. Hands up, because I have some. How did you guys choose the op stack? The question is, how did we choose the op stack? So we did a tour of the Ryzen. We looked at all the different technology out there, and one of the things that we really liked about the op stack is it's all released under MIT license. We like their vision of the superchain. So for those that are not familiar, the superchain is going to be a series of l two s that are all built on the same technology that will eventually have seamless interoperability of transferring value between the different networks. We also like the optimism foundation.
03:32:50.744 - 03:33:18.404, Speaker A: We like the team members that we met, and we like their approach to moving everyone on chain. So it was culturally aligned. We liked the licensing and the technology stack and the super chain vision. Anyone else? Because I have a good follow up question. Go ahead. Yeah, good talk. So I have a quick question on.
03:33:18.404 - 03:33:54.748, Speaker A: Do you have some sort of appliance to reduce the gas cost on base, for example? Yeah, the question is about the paymaster and reducing gas costs on base. Sorry. So reducing gas costs is twofold. So 4844, we believe, will take and reduce the gas costs by about 50%. And we'll continue to work with the Ethereum foundation and optimism foundation in order to further drive those costs down. We're not going to be satisfied going from ten cents to five cents. We don't think that's going to open up globally, the global economy on chain.
03:33:54.748 - 03:34:39.292, Speaker A: The other part is what I discussed, which is the Paymaster. The Paymaster is a program. It's a utility that connects to enable smart contract wallets to provide gasless transactions effectively subsidized by base. And so it is a bit of a hurdle where you have to shoehorn and you have to integrate ERC four three seven into your protocol, or DAP. But we believe that this is where the industry is moving as a whole, regardless. So this will enable select protocols to have subsidized gas. So this is a program that we're rolling out.
03:34:39.292 - 03:35:15.610, Speaker A: We're still trying to figure out the details. Please reach out. We want to listen, and we want to learn from everyone, and we want to understand what people really need. So looking forward to hearing more. Cool. That's a really great response. Anyone else on this side? Because I'm over here, I want to say it's like you guys are in this transition phase, that you're figuring it out, because when you look at the numbers, like arbitram is number one by x amount, and I'm confused as to why that was going to be my question.
03:35:15.610 - 03:35:37.344, Speaker A: But you guys are still in this transition phase and you're about to implement these new. Because for me, the writing is on the wall. But I know arbitrum is very focused on their product. It's almost, like, confusing. I don't know, but it sounds like you guys are on this very long winded way. And I understand from. I'm a marketing guy.
03:35:37.344 - 03:36:12.296, Speaker A: So from the marketing perspective, they are just hyped right now. Everyone's focused on them. I know that was like a long winded statement, but I don't know if you have any thoughts on that. Yeah. The way that we view base is we view base as a bridge on island. We built base not only for the greater ecosystem as a public permissionless blockchain, but we also built base as a way to bring Coinbase on. Coinbase has been building centralized applications for a long time and we believe the future is on chain.
03:36:12.296 - 03:37:03.396, Speaker A: And so we are working not only with the ecosystem of our partners, but we're also working with Coinbase to retrofit their apps to bring them on chain. So over the next five to ten years, and a lot of them even sooner, will start to move more and more on chain. And so we eventually see that the 110,000,000 Coinbase retail clients, we eventually see them moving on chain, and then we think that from there they'll start to explore other ecosystems. The cross chain messaging has gotten much better. The integration between networks is getting better and better and better. So we think that once they hit base, they'll eventually explore other ecosystems and there'll be different flavors. Like Zora is doing an amazing job with really fine art and with art based nfts.
03:37:03.396 - 03:37:39.216, Speaker A: And so we think that people come onto base and then eventually look over at Zora for that or look at other networks for that. So base has gone from zero to nearly 600 million in TVL in roughly two months. And we expect that to continue roughly in the trajectory that we've seen. So the goal is not to be number one. The goal is to build an amazing experience, an amazing ecosystem for developers that draw consumer people and finance on chain. So that's our ultimate goal. I love that response.
03:37:39.216 - 03:38:09.744, Speaker A: Thank you so much. And we have our last question here. And then we're cutting it. Yeah. So since we're running out of time here, quick question regarding the table that you've shown earlier where you have the stage zero, stage one, stage two, and is everyone else on the. I mean, I'm not sure about where it came from, but is everyone else on the same page here with the proposed progression? And do you think you guys will arrive there first at stage two? Yeah, I hope so. I'm not sure we'll be the first.
03:38:09.744 - 03:38:44.012, Speaker A: Arbitrum one is currently a stage one and we're a stage zero. The source of the l two stages comes from l two beats in terms of like, is everybody on the same page. Hard to say. I think it generally represents where the industry is as a whole. I'm sure it'll get much more scientific as time. Yeah, I'm expecting, given arbitrum already has a bit of a head lead to getting to stage two. So they'll learn.
03:38:44.012 - 03:39:04.940, Speaker A: Likely hit it first, but we expect a lot of progress. There's a number of those networks that you saw that are built on the op stack as well. And they should all be able to take advantage of the investment technology by base, by optimism. And everyone else building on that chain of those chains. Amazing. Guys. Give it up for Ryan.
03:39:04.940 - 03:39:19.160, Speaker A: Ryan, thanks so much. Here, I'll take that. Ryan, you did great, man. I appreciate your time. Thanks for coming. Ciao. Up next, we have Cooper from Aztec.
03:39:19.160 - 03:39:40.270, Speaker A: Cool. They got Cooper's thing up. Come on up, man. Give it up for Cooper, product manager at Aztec. Here, just speak into this and they'll turn it on for you. Oh, here, take this one us on. And here's your remote, sir.
03:39:40.270 - 03:40:06.340, Speaker A: Hi, everyone. Can you hear me? Okay, is this good volume? I'll take that as a yes. So today I'm going to be talking about comparing censorship resistance in L2 networks. My name is Cooper. I'm a product manager and researcher at Aztec Labs. Aztec, for those not familiar, is a privacy first L2 on Ethereum. We recently released our sandbox.
03:40:06.340 - 03:40:41.808, Speaker A: But not going to talk too much about aztec today because we're not in production. And we're going to talk a lot about things that are actively deployed. And quick disclaimer, Aztec Labs is a big company. It's a big project, big community. These are my own opinions, my own research should not reflect those I work with or the project itself. So if I say anything you disagree with, please just come at me and me alone. And so the agenda today, we're going to talk why care about censorship resistance? We'll talk about censorship resistance properties and l two s today.
03:40:41.808 - 03:42:14.640, Speaker A: And we'll talk about what we can do about it moving forward and a little bit about what the future looks like. And so, yeah, part one, why care about censorship resistance? Fundamentally, it gets back to the question of why are we all here? Why are we working in cryptocurrency? Why does cryptocurrency exist? And I hope that most of you would say that we're here because we believe in the freedom to do what you want with your own money. We live in a world where certain parts of the world are going through hyperinflation where governments are taking people's money without due process where countries are just straight up trying to ban the industry, arguably because it's good at the first two reasons. And so if you're here today and you're wondering why work in crypto or why are these things important, I always would probably encourage you to come back to the fundamental assumption that we're building censorship resistant software because we all believe in the freedom to do what you want with your money. And I would also say relative to global history, we've historically operated during times of kind of global peace, right? The industry is not very old. We have not seen major conflicts, but we might. And I guess I would ask the question of, are we ready for those things? Are we ready to have truly censorship resistant, scalable, and in aztec's case, arguably private versions of money that can actually give people options to opt out of the existing systems.
03:42:14.640 - 03:43:08.844, Speaker A: And these are things that you really proactively need to be in place. You need censorship resistance and privacy in your systems, and you need money in those systems much earlier than you actually need to use them. At the point that you're trying to get funds into censorship resistant systems, it's probably too late. And, yeah, so if we aren't building censorship resistant software, what are we really doing here? You're building arguably slower databases and probably shouldn't be contributing to core protocols or encouraging people to put millions or billions of dollars into them. And so that's enough about why care about censorship resistance? I think it's kind of fundamentally the philosophy of the industry. And we're going to spend most of today talking about censorship resistance in L2s today. And yeah, what does censorship look like in L2s? I have a bunch of diagrams.
03:43:08.844 - 03:43:46.840, Speaker A: You guys are all very technical. You're at Devconnect. They're very simple, just supposed to get the point across. The industry is predicated on mountains and mountains of research and nuance. So I apologize for the lossy diagrams, but the basic scenario when people think about censorship resistance in L2s is you mostly have a centralized sequencer, and your average user is going to just be able to send transactions or intents to that sequencer, and the sequencer can just straight up ignore them. Most of them have rpCs. Those RPCs can have straightforward filters for IP restrictions or otherwise.
03:43:46.840 - 03:44:43.420, Speaker A: And it is totally up to these centralized sequencers to include your transactions in a block. And so we're going to talk today about the mechanisms that L2s can actually build and have in place today that prevent this type of censorship from happening. This is the most naive case that you can possibly think of and usually applies to optimistic roll ups. But you can think of another scenario, right? If you have a centralized sequencer and a centralized prover. Usually these are operated by the same companies, but you can imagine a world where, for different reasons, they're operated by service providers or third parties. And so you could have a scenario where you get included in a block, but whoever's doing your zero knowledge proving and actually generating your roll up can say, hey, I'm just not going to prove this block because I want to censor you. And so we're going to talk today about the mechanisms that people have built and thought about and have into production today to circumvent these scenarios.
03:44:43.420 - 03:45:34.380, Speaker A: And so the first mechanism that most people are familiar with is forced inclusion. And this is the ability for users to bypass that sequencer that you saw in the first diagram and submit transactions directly for inclusion in the l two S history. Easily understood as adding your transaction to the sequence or bypassing the sequencer. I would say these aren't universally accepted definitions yet, but I think we can all do better on formalizing them. And generally everyone understands what bypassing the sequencer means. You go directly to l one, you submit your transaction, you get added to the transaction history, but you also have what people traditionally call forced exits. And so this is the ability to bypass the sequencer, but also ensure that your actual state changes are executed to remove assets from the l two.
03:45:34.380 - 03:46:27.392, Speaker A: And so these are actually different things. A lot of people talk about censorship resistance in l two s, and they're like, oh, they have forced exits, we're good. I think my goal today is to convince you that it's a little more nuanced than that and actually has a bit more trust assumptions implicated in project censorship resistance guarantees. And so you might be thinking, aren't these kind of the same things, but in a variety of production systems, some of them let you actually add your own transactions to history, but do not guarantee that that transaction will ever be published to l one and your state changes will be reflected. And so you actually need both of these. You need forced inclusions and forced exits to support actual, what I would call practical forced exits, the genuine ability to remove your assets in the event that anyone operating the infrastructure is choosing to censor you. And yeah, I kind of made this up.
03:46:27.392 - 03:47:08.396, Speaker A: Feel free to disagree if you want, but at the end of the day I think it illustrates the point. You can have the ability to include your transaction, but that does not guarantee that the state changes of your transactions will be executed as you intended. And I present a challenge for anyone brave enough can you, like you guys are smart people, you all work on protocols. Can you actually force exit a production l two in the next 48 hours? I'm willing to give you aztec swag, maybe even the hoodie I'm wearing right now or otherwise. Have you managed to do so? And I would be very impressed with you if you do. Truthfully, I've tried. I haven't been able to figure it out.
03:47:08.396 - 03:47:39.030, Speaker A: I'm reasonably technical. And I can also say a variety of our smart contract engineers have tried as well and not been able to figure it out. And so we talk about censorship, resistance and l two s being predicated on the ability to be forced exit. But I think it's a lot harder than people think. So I put this challenge out there for you and anyone watching this as well to try this. We're building an entire industry predicated on being able to force exit these l two s right now while they're centralized. And I don't know if you actually could.
03:47:39.030 - 03:48:33.236, Speaker A: So we'll talk a little bit more about this as well. Let's talk about another scenario. Even if you have forced exits, let's say you can self sequence and self propose, you're not guaranteed to be able to actually forced exit. The reality is, if there's only a whitelisted set of entities that can submit transactions to the layer one, unless you have a valid fraud proof scheme and you can prove the difference between the transaction sequence and the actual state executions, you cannot guarantee that you won't be censored. And so I think that this is a really critical thing. Your ability to actually not be censored is probably interactive and does probably in some cases require supporting fraud proofs or zero knowledge proofs or whatever your security scheme is. And so just being able to say you can force inclusion does not guarantee your right to exit.
03:48:33.236 - 03:49:13.540, Speaker A: There's other trust assumptions, which is what we're kind of getting at here. And I think that this is the most obvious scenario. You have a user who's trying to force inclusion. The centralized sequencer doesn't chooses not to execute it, so they initiate a fraud proof scheme or some way to prove that this transaction should have been executed. And they have a quick upgradable Security Council who's very heavily incentivized, let's say from outside actors or different governments that might not like them. And they see your transaction in the forced exit queue and they see it might take 24 hours to execute. They can just remove the entire delayed message inbox.
03:49:13.540 - 03:49:54.050, Speaker A: These are real things that could happen. And so I guess the point being is that upgradability has direct impacts on your ability to achieve censorship resistance. This is why a lot of people talk about the gold standard being 30 day minimum upgrade windows. And yeah, so if a network has these things and they support them, even if they're well documented, there's an easy website you can go to that doesn't mean that these things are guaranteed to be there tomorrow, let alone when you might actually need them. And. Cool. So we've outlined some of the options, some of the challenges, some of the mechanisms that you can put in place.
03:49:54.050 - 03:50:36.080, Speaker A: You need some types of forced inclusion, you need some types of forced exits, you need ways that users can actually guarantee that those things are operating the way that they were intended to be, through fraud proof schemes or otherwise. And we'll talk about production now, and it looks a little bit like this. Everyone's building a Zke EVM and they want to differentiate their ZKe EVM in some practical way. And they're prioritizing these feature differentiations over censorship resistance in a lot of cases. And I'm not going to name all of these. This is taken straight from l two beats risk analysis. Shout out to our sponsors of this event and taking these things very seriously.
03:50:36.080 - 03:51:03.930, Speaker A: This should not be the source of truth for these things. These things constantly change. They're updated. Projects are shipping new stuff literally every day. But you can go to l two beats website and you can click into projects and you can go and see which one supports sequencer failures, which is your ability to put your transaction into the transaction history. And you can go l two beat makes it really easy. And you can see what censorship implications there might be for L2 networks that you're using.
03:51:03.930 - 03:51:38.390, Speaker A: You can also go to l two b's website and see who has proposer failures. A proposer failure and a sequencer failure, if they don't support both, might mean that you're not able to practically force exit from the network. Theoretically, you might be able to. All these networks are different. They have quirks and different things that you might do. They might have different tweaks and changes. But in general, unless you go to l two beat, which I'm trusting as the source of truth, because we're all at their conference today, you might not be able to force exit, which could be quite bad.
03:51:38.390 - 03:52:17.520, Speaker A: And again, this is just the list from l two beats website. I actually don't know the current status of all of these chains. So for the latest information, I would refer to you to there. And yeah, these are related. As I said, your ability to force exits predicated on your ability to be forced included in the history, without getting your transaction in the history, it's going to be really hard for you to exit. But there's a whole other problem. Like even if they theoretically support these things, if the smart contracts support these things, do these projects have sufficient documentation and tooling actually required to execute these steps? I don't know if I've seen it anywhere.
03:52:17.520 - 03:52:57.516, Speaker A: You go to docs and it's like, hey, we support these things. Here's how you would do it. But then what? You go dig through their code, you try to abstract away how they're doing proof generation, and you read the entirety of the protocol spec to figure out where to put your transaction to exit. That is not viable for actual users or people who might be getting censored in production. And so that could be a whole other talk about what tooling and what the ecosystem is needed to support practical force exits for average users. And this is kind of the elephant in the room is which l two s support what I would call censorship resistant upgrades. There's a big balance here.
03:52:57.516 - 03:53:33.080, Speaker A: As you can see, a lot of these have asterisks which say our quick upgradability is purely a security feature, it will only use it in the event a bug occurs or we need to pause the network. And that might be great, that might be a worthy trust assumption. Trade off. It might be worth it for a certain chain to be updated whenever they want, if they're using the latest cryptography schemes or otherwise. So I don't mean this to say that this is a binary spectrum. It's not like you have safe upgrades or no upgrades. I'm just saying it's nuanced and it does impact your censorship resistant guarantees.
03:53:33.080 - 03:54:07.548, Speaker A: These networks can support forced exits fully, they can have great documentation, but if their multi Sig or their security council is compelled to remove them, they probably will. Or they might, and you might not be able to use it. And so a real world example, I'm not preaching from the choir here. Aztec connect. A previous product from Aztec had a forced exit mechanism. It had an hour every day that someone could self sequence and self propose. But we actually found a bug in the self sequence and self propose feature in the escape hatch.
03:54:07.548 - 03:54:48.050, Speaker A: And so the safest thing to do was to upgrade the duration of the escape hatch to the maximum value possible. You can see it says 136 years. Our quick upgradable council updated it to 136 years, so no one could actually use the escape hash maliciously and steal everyone's funds. And so this is a good example of why projects are choosing to do these things right. It's not a binary spectrum, and you can improve user safety in practical, real world examples if you support, I would say, non censorship resistant upgrades. And again, shout out to l two beat for holding us accountable for this and holding the industry accountable. I think that these are really good things that average users need to be aware of.
03:54:48.050 - 03:55:46.050, Speaker A: And so this is just a summary of production l two censorship resistance mechanisms, mostly taken from l two beat. Again, and I'm saying practical forced exits in the sense that I personally believe that you could actually exit from these networks if you wanted to, with the documentation tooling that's publicly available, and no preferential support from the gigabrain cryptography teams that work on some of these projects. It's also a list of who's supporting fraud or validity proof schemes, which, as we talked about earlier, plays into the picture, and which ones support what I would say is safer censorship resistant upgrade mechanisms. Again, I'm not saying that these things are good or bad. I'm just saying that this is where in the trade off space these projects have decided to be. They're prioritizing other things, other features, other types of support, other types of user safety, potentially over censorship resistance. And so I'm not just saying these things.
03:55:46.050 - 03:56:31.856, Speaker A: What can we do about it? I think everyone's talking about this. The base folks are talking about this, there's panels about it, there's all the shared sequencer projects, there's decentralized proving marketplaces. But at the end of the day, we really fundamentally need to decentralize L2 infrastructure. We need to make sure it's permissionless. We need to make sure that it can be operated in any jurisdiction that users might want to. And that is really the only way that we can ensure that L2s can retain as much credible neutrality as possible, and also ensure that users in jurisdictions where there might be heavy, heavy incentives to censor them can actually use these things, participate in the financial world, and participate in what we're all trying to build. And so we can.
03:56:31.856 - 03:57:00.460, Speaker A: Very easy. This is low hanging fruit. Like we know how to do these things. We can just rotate sequencers across anyone who wants to do it. You can have new leader elections through auction mechanisms, through random selection functions, through VRF. This is relatively low hanging fruit. Eventually you might find a sequencer willing to put your transaction in a block similar to the way l one works today, you might find a proposer or a builder that's interested in your transaction.
03:57:00.460 - 03:57:48.932, Speaker A: This is not that difficult. You're using techniques from other l ones, other projects, and we can do the same thing for ZK. Eventually you might find approver willing to actually approve your blocks for you. This is not the end game. This is not the ideal solution, but it's actually as easy as this to improve the censorship resistance properties in L2 networks. If we had a very diverse set of stakers who are operating sequencers improvers, you generally get much better censorship resistance guarantees. And I think the major takeaway today is that us, as the group of people building these things, as those who are in the small subset of the world who actually understands how these works and can contribute to building them, we all need to agree to make censorship resistance non negotiable.
03:57:48.932 - 03:58:39.944, Speaker A: Right now, there's over $10 billion within L2 protocols that I personally would say don't have great censorship resistance guarantees, and to me, that's a problem. This signals to the market and to protocol developers that it's worth taking the shortcuts. It's worth compromising censorship resistance, potentially, in order to gain early market share and adoption, which is bad. And so if there's one takeaway, it's that we need to hold each other accountable through social pressure. We talk about social consensus all the time. The way you achieve social consensus is by encouraging products and projects to build the future that you actually believe in. And we need to hold each other accountable for actually building these things in a credibly neutral, censorship resistant way, even if it costs us potentially being first to market, which is hard to say.
03:58:39.944 - 03:59:16.820, Speaker A: But I think it's important to remind each other, back to the beginning, why we're all here. And we're here because we believe in the ability to program censorship resistant money, and no one to take that away from you. And so, yeah, thank you for listening. We can build more censorship resistant software together. I think it's up to us to hold each other accountable and actually implement the world that we talk about. If you're interested in Aztec, we're doing a lot of research on decentralizing our sequencing and decentralizing our proving for the upcoming L2 roll up. It has full programmability, public and private state composability.
03:59:16.820 - 04:00:08.728, Speaker A: So there's about nine or ten ideas for ways that L2s can decentralize their sequencer, and another nine or ten ideas for how L2s can decentralize their proving infrastructure. Yeah, we also have a sandbox. If you want to write your first private smart contract on Ethereum, go to Sandbox Aztec network and follow me on Twitter at Cooper Coons, where if you have any questions about this, I'll be somewhere around this event for the rest of the day and happy to talk about it. And if you do manage to exit from a production l two, at some point during Devconnect, please take a video and tweet it at me. I would love to see it. And we need to start building knowledge libraries and content repositories for how you can actually use these censorship resistance mechanisms. Yeah, that is all for me.
04:00:08.728 - 04:00:35.072, Speaker A: Thank you for listening, Cooper. We have about five minutes if you want to take some questions. Is there anyone that has questions in the audience? I'm in the middle of the room. If anyone wants to raise their hand, I'll come to you. We got a guy up here. You could also just yell at me if you want. We got for the recording here.
04:00:35.072 - 04:01:03.624, Speaker A: My lovely volunteer is coming in. Thank you, Abel. Hey, really appreciate your talk. Thanks so much. I'm really curious about your challenge about forced exits. I don't think I'd be able to do it in the next 48 hours. But just for curiosity, how would you validate that a forced exit has actually occurred? Because if you just do, like, the forced inclusion part, for example, almost certainly the correct state transition will happen.
04:01:03.624 - 04:01:31.456, Speaker A: So how can you validate that the exit was actually forced? Yeah, it's a good question. You would basically put a transaction in the delayed message inbox. Some of them have 24 hours. They have different ways that you actually get included. If that works, I would say you've successfully forced exited. If they choose to censor you, then you'd have to go through a fraud proof or other dispute mechanism. If you're trying to exit from a ZK roll up.
04:01:31.456 - 04:01:58.970, Speaker A: Hopefully, the ZK roll up does not provide an avenue in which they can bypass the transaction sequence. But it's basically, you put a transaction in the delayed message inbox, which is trying to use the escape hatch. If it actually gets included in the chain, you've used it successfully. If it doesn't, then you have other problems and you've been censored out of the force message or the transaction history. Okay. It doesn't relatively low a bar, then. Yeah.
04:01:58.970 - 04:02:11.164, Speaker A: It should be so easy that you should be able to do it in 48 hours, which is the point. It should be simple. Oh, maybe I can then. All right. Yeah, you should take the challenge. Okay. And yeah, tweet at me.
04:02:11.164 - 04:02:26.876, Speaker A: I would love to give you some swag or something like that, and then we can. That would be a huge win. If you're like, I did the shit in ten minutes, that would be great. We'll see. I did it in ten minutes. Here's the hoodie. Yeah, classic.
04:02:26.876 - 04:02:42.744, Speaker A: I'd give you way more than a hoodie if you can do it in ten minutes. Love it. Good question, man. Anyone else? I'm over on this side. I got Mike runner Abel over there. I think we have, like. That was a pretty lengthy question.
04:02:42.744 - 04:03:00.640, Speaker A: Let me check our time here. Cool. Well, I will be in the lobby if anyone wants to talk about it or talk about Aztec. Thanks, Cooper. Give it up for Cooper. I'll grab this. Yeah, throw that over there.
04:03:00.640 - 04:04:01.720, Speaker A: All right, we got a few minutes between calls here. We're going to have Jordy from Polygon up next, which I'm pretty stoked about. I haven't heard a polygon speaker ever before. Do you guys know coinfessions? Do you want to hear some? Let's see. One. There was a few that I read this morning that really leaves a good message. I was part of the team in an NFT project, I stored 40k USD on FTX from mint proceeds that founders had me custody.
04:04:01.720 - 04:04:38.944, Speaker A: FTX started to collapse, and I warned them about my intention to move those funds elsewhere for safety. They replied not to worry, that nothing was going to happen. The next day, FTX froze withdrawals. Luckily, I didn't listen to them and withdrew the funds. They never asked me if I had got the funds out and rugged the project soon after. So if you smell smoke, there's usually a fire. I got one in the audience.
04:04:38.944 - 04:05:10.956, Speaker A: That's all that I need right now. Let's keep going. I had a few ones here that I liked. I ran a Facebook group on crypto about bitcoin back in 2014, 2015. At the time, I had 140 BTC, around 40k, but I sold it around four to five k to go traveling. My friends at the time thought I was a millionaire because BTC went up to 20k. But little do they know I deleted my facebook due to me having no life, not because I wanted to hide my wealth.
04:05:10.956 - 04:05:38.212, Speaker A: I'm living at my parents'house now and waging in my free time. That means, like, a hourly job. Life sucks. And I've wasted two years of my life in crypto, doing well, doing nothing. He says here. That one's all right. So just so you know, there's some really depressing stuff on here that I had to sift through.
04:05:38.212 - 04:06:08.722, Speaker A: I tried to find some good ones. Here's another one, guys. Be careful out there. This is crazy. So this guy took the affiliation links from 200 YouTube videos explaining how various coins work with the how to buy XYZ token in the title. Said he made 150 grand off of it. Convinced my mom to buy chain link at 15 usd.
04:06:08.722 - 04:06:47.060, Speaker A: Last bull run. At the time, I was so convinced that link was going to 100, I told her not to sell. She kept convincing me to sell at $40 because she told me that's already worth 20k from seven k. Good thing I listened to her and sold them all at $40. In hindsight, my mom is a better trader than me. The emotions of trading in this game, I don't know if there are. Who holds tokens in here? Where are you guys at? Does anyone here hold like, USDC? Let's start there.
04:06:47.060 - 04:07:28.400, Speaker A: USDT? What about bitcoin? All right, that's cool. Ethereum. That's so crazy. It's like a 50 50 split between ETH and not holding ETH in the crowd, which is like wild to me because we're at an ETH conference. There was one more Solana question mark. Cardano. What about nfTs? I'm a self proclaimed NFT djen, so no big deal.
04:07:28.400 - 04:07:45.610, Speaker A: All right, it's 230. It's time to call the dentist. That's a dad joke. If anyone caught that Jordy from Polygon. Let's bring you up here. Where is this guy? Hey, nice pink shirt. Hello, Maddox.
04:07:45.610 - 04:07:57.210, Speaker A: Friend. Let me get you one of these little guys. Feel free to put that there. Make yourself at home, my friend. Here's the microphone for you. That works. And here is the remote.
04:07:57.210 - 04:08:16.654, Speaker A: Is your thing up? Yeah, your slideshow is right there. Perfect. Yeah, man. Thank you very much. Yeah. Well, hello, everybody. I'm Jordan Valina.
04:08:16.654 - 04:09:15.062, Speaker A: I'm the technical lead at Polygon ZKBM. And in this presentation, mainly, I want to explain what we are doing at these days. As all of you know, we launched the Polygon ZKBM mainnet Veta in March, last March, and during this month has been very intensive in onboarding, a lot of applications, understanding how the network works. And, well, here my presentation is going to split all the work that we are doing currently and the work that we are going to go, that we plan to go in the next month. So one of the things that we are working very much right now is in just trying to get better scalability cost here. EIP 4844 is in the next door. Probably will come in the coming months.
04:09:15.062 - 04:09:54.638, Speaker A: So we need to be prepared for that. For the ZK roll ups, this is a little bit more challenging for the optimistic rollups dealing with that, because you need to prove that the data, you need to prove that the data availability is there inside the circuit. So this is a little bit more complex. We're also working a lot in data compression. Carlos the other day explained how we are doing that. But here we see that the data availability cost is the, I would say it's the bottleneck is the biggest cost for the people using roll ups in general. And one way to solve that is with data compression.
04:09:54.638 - 04:10:36.702, Speaker A: Data compression we can do a six x seven x improvement in the quantity of data that you would need to store the transactions. And if we put that together, I mean if we put the data compression together with VIP 4844, thinking about a 50 x improvement in the gas costs of an l, two transactions is perfectly suitable. On that. Another topic, and very important is node improvements. When we started, we created a node that more or less works. But if you want to scale, this node needs to perform. And here I want to.
04:10:36.702 - 04:11:24.030, Speaker A: For example, one of the important things is that we are right now integrating very much with eregon. We believe in this diversity of clients and currently Erigon is very close to. So they are already synchronizing the full network and we will have a different client for the ZKBM that we believe that will improve a lot the performance of the network itself. Thank you very much to Igor and all the gateway people that has been working a lot in the last month. Security, I mean this is probably the most important part. This is a very hidden part because it happens very much under the scene. But we have been running the bug bounty and some other small audits for different pieces.
04:11:24.030 - 04:12:48.898, Speaker A: Since I'm in the space, I'm always get acceptable for bug bounties. But at the end I get impressed on how important and how good are the audits here. I want to thank you to a lot of the bug bounties there is from companies, from security companies that they are just checking the code and they do some reports and important security companies in the space and even outside this space, and some, even some hackey that we don't even know the name, but they are reporting amazing bugs or amazing comments on the code. So I want to thank you all of them, because I think this is really important. And actually we are seeing how the code matures, especially in the proverb. There is one thing that's probably the most difficult one is having a poorly soundness prover is hard and it's very easy to make bugs on there and it's very subtle in a lot of times. And we are seeing for the degree of sophistication and the things that we are seeing that the security of the proverb is getting improved.
04:12:48.898 - 04:13:34.626, Speaker A: Just to be clear, because we are running in these training wheels, these security issues don't affect at all the current network. But this is something that at the moment that we decentralize approver, this will become much more important. So it's important to be ready and to check and to be sure that all this technology gets absolutely safe. Like I mentioned that in the bug bounties we paid like more than half a million dollars in the last month. Edrock. Edrock is the next hard fork that we plan to do it. Let's see if we can go for Christmas with this launched in Mainnet.
04:13:34.626 - 04:14:18.946, Speaker A: But Ed Rock mainly is converting the type, so we will become a type two roll up. So instead of being a type three, we will be mainly a type two roll up this hard fork. I mean this is improvement here we will include all the paddings, so it will be possible to validate paddings in the Zkevm Shadow 56 that was not available until now. And now it's going to be available. Xmot I think that chain link is using this very much. And especially if you verify things like RSA and things like that, this is an important piece to come. And there is also right now, for example, we have like one transaction, one l, two blocks.
04:14:18.946 - 04:14:49.478, Speaker A: We are going to do now many transactions, one l, two block. This was a design that we saw that was not good. And we are just switching and changing now in the next hard fork and then some small optimizations, improvements that also will come in there. There's a lot of work, but this is also, we're working hard on this front. What else? Forced transactions. I mean the last previous presentation was talking a lot about these four transactions. So I don't need to explain what are the forced transactions.
04:14:49.478 - 04:15:55.560, Speaker A: This for us is a priority. We believe in these four transactions. Censorship resistance is really important. We need to say that forced transactions are available actually in testnet they are enabled and they have been always there since the first day, except that are disabled. And why they are disabled? Because the problem of the force transactions is that the vector space, I mean anybody can send any kind of transaction and this opens the vector space for security a lot. And we are still not convinced that some of these force transactions could stop the network for some days before enabling that we want to run a bug bounty. So we want to run a bug bounty so that we are more safe that enabling this is not going to break the network at some point.
04:15:55.560 - 04:16:44.146, Speaker A: We are also thinking in maybe limiting the force transactions means that you can send any transaction, you can force any transaction. L, two, we probably are going to limit which transactions you can force. Of course, with raw transactions, for sure we will enable, but we will limit which transactions you will force also to reduce the vector space of this. But clearly we are going in that direction. And I fully agree with my previous person that was talking before me from Aztec, that this is really an important part that we need to enable as soon as possible. But with the responsibility that when we enable, we are quite sure that the network will continue working. What else we are working? I mean, we are also working in the improvement of the prover.
04:16:44.146 - 04:17:59.274, Speaker A: Actually need to say that the prover is not a limitation, especially in the cost. If you see the cumulated cost of the prover for generating the proof. So when you are sending a transaction, the main cost that you are paying is mainly data availability, it's even higher, the fact that the gas cost of sending the proverb on chain. But the cost of generating the proof right now is very low and it's not the critical path, but even that, of course, improving the performance is also good. And there are other important things that we want to solve. One of the things is, well, we call it vatcos, but one of the problems that these ZK rollups have is that right now, what the proverb, the proof that we have is a monolithic proof. What means that? That means that you can prove a certain, in each proof you can prove certain amount of resources.
04:17:59.274 - 04:19:13.346, Speaker A: For example, you can prove, I don't know, 2000 ketchups, or you can prove 10,000 multiplications, or 5000 additions or XOR, depending. You have different resources, a fixed number of resources in the proof, and you can just proof that. So if you are trying, for example, do a transaction that's doing a loop of ketchups, or you are extra using one of those resources, of course this could be a very crazy transaction, but it's perfectly possible, then you will not be able to include that transaction in the proverb. So you would not be able to prove actually the trick that we are doing now, because we need to prove anyway, and especially in the forced transactions, anybody can send any transaction and maybe somebody sends a forced transaction, that's a loop of ketchups. And how we prove something that's not provable. So here, the way we are doing here is we keep kind of a contours we call Zika counters. So the idea is that if we prove that these transactions needs more catch acts, that's the ones that we have.
04:19:13.346 - 04:19:53.090, Speaker A: Actually, we trade this transaction as a no operation transaction. So we are able to prove that this transaction is not provable. And then we trade this transaction not as it should be, but as a no operation transactions. And this works well. For example, for the force transactions schema, that means that we need to keep kind of an accounting system in the full ZK proof for each of the resources that we are using. So before, for example, before doing a ketchup operation, we check that we have enough ketchups and then we do it. Or we just take this transaction as an operation.
04:19:53.090 - 04:20:45.154, Speaker A: But this is slightly different of what ethereum does. I mean, how we can, can we do it better? Well, one way is that we are doing is we are building a different kind of proof. Instead of building a monolithic proof, we are extensively using the idea of recursion proof. So the idea is having, instead of having a monolithic proof, we have like each state machine can have a subproof, it's like another proof. And then with aggregation, we aggregate to a single proof many of the proofs. And in a system like that, the idea is that, for example, if you need a lot of binary state machines, one thing that you can do is you can build maybe three proofs of binaries, or three, four, or whatever number of binaries that you would need. You execute whatever you would need.
04:20:45.154 - 04:21:38.530, Speaker A: So you can grow as much as you can as you want. The proof, of course, the proof will take longer to be generated, but you will be able to prove anything. This idea of infinite recursion, I mean, you can aggregate as many proofs as you want. You can also, for example, in the main processor, you can also aggregate, you can execute many chunks of code, so you can do the proof as big as you want. So with this, we will solve the problem of these ZK counters. We will keep the ZK counters, but not for deciding if we include or not, or if we know operation or not transaction. But we will keep that for economic reasons, because the prover is going to be bigger or slower, then the prover needs to receive more value.
04:21:38.530 - 04:22:35.494, Speaker A: I mean, if the proverb needs to prove something that's bigger, they will have to receive a bigger amount than if it's a smaller amount. So for economical reasons, we even keep track of how many counters we are using, but this is not. It will not affect the user, it will affect the infrastructure system, so that the provers that have to build bigger proofs will be compensated in the right way. This is more, a little bit more technical, but for the technical people here, we are using a lot of tricks for communicating between different proofs. So transferring. I mean, we have a kind of a common bus where you can transfer information from one state machine to another state machine. You can understand these state machines as coprocessors, and here there is a nice technology that happens to this.
04:22:35.494 - 04:23:14.426, Speaker A: So these systems will benefit a lot, will decrease, and the memory that's required has a lot of advantages. The proverb is going to be much cheaper because it's going to be much useful. So we are working very much in these vatcops. And the other part is the pill two. So, as you know, all their immediation is done in pill. It's pill one at the beginning, we are extending the language. We are rewriting a little bit the language mainly for including the vatcops, including this variable degree.
04:23:14.426 - 04:24:22.742, Speaker A: So this idea of proof, so we can embed it in the language itself. But there are a lot of other things that, according to the experience that we got in the original pill, there is a lot of improvements that I think that will help better to improve the performance, the auditability, the readability of the system, the design itself. I think it's a huge advance. And we are actively working in this pill, too. At the end, the idea is that with this, we can have this pill two we are using for ourself, but this can become a generic language, and it can be seen as a processor builder. I mean, you can have a processor can be maybe a specific processor or human can be, for example, a risk five processor or valida processor, or any other processor in there. And the day is to have different models where you can plug in.
04:24:22.742 - 04:25:10.914, Speaker A: Maybe you have 64 arithmetics, 256 misarimetics, and so on. And the idea is that you can create your own system with the needs you want, and then you can build and you can program on top of this system. This looks like a little bit like the people that design hardware, like embedded systems, where you can put different pieces, and then you can build on top of that your application. And finally, the other thing that we are working a lot is, especially in the Polygon 20 infrastructure. I've been talking very much in a single chain. It's a ZKBM. But the idea of the Polygon 20 is built, this constellation of these constellations of chains.
04:25:10.914 - 04:26:30.026, Speaker A: Here we have a polygon ZKVM but here the idea is to bring to this Polygon 20 constellation polygon Pos but here is going to be other validiums, other roll ups, private roll ups here the idea is that anybody can build the roll up, that is the idea of the CDK so that you can be your network and you can plug in here and Polygon 20. What we'll give here is mainly we are talking about the alexa y breach. This idea is that all the networks in the bridge will share the same liquidity. So that's easy to transfer value between the different chains, no matter if they are public, private, semi private. Some of them may be validium, some of them may be not validium, some of them may have their own token, some of them may have centralized sequencers some of them they may have decentralized sequencers for the centralized sequencers. The idea is we're building this staking pool, that's the idea of the poll token, to build this staking pool so validators will be able to pick which network they want to validate and they get coordinated on that. And then we have the aggregation layer.
04:26:30.026 - 04:27:54.090, Speaker A: That's probably the one that I want to talk about because this is a very interesting one. The idea is that in Polygon 20 the idea is instead of each network, or besides each network having their own proverb, their own proof, the idea is to aggregate all the proofs of all the networks to a single proof so that the change will not have to pay, they will not have to pay the gas cost of sending the proof or validating the proof on layer one. So this will open, especially for small networks, that paying and running these costs can be high and at the same time the quantity of. So because it's a single proof and the proof is aggregated, the quantity of proof, that can happen. I mean we can generate maybe a proof every minute or every 30 seconds and this proof can become very much like a clock and this clock for data transfer because we have this proof and the proof also warranties that what's there, it actually happened. The idea is that this act as a clock and then you can do that transfer between one net and the other. And so we are interested in doing this clock or doing this proof as fast as we can.
04:27:54.090 - 04:29:02.962, Speaker A: So the idea here is to aggregate all the proof to a single proof to reduce the cost and make things. And here it's going to be an aggregation layer mainly it's going to be probably in the long run it's going to be a consensus layer where mainly it's going to be responsible of two main things. One of the things is coordinate the generation of this proof. I mean just collecting, maybe the provers will ask for generate a proof, they will be committed and then parts of the proof are generated. So it's going to be a consensus layer for the coordination of the proof generation and also for sequencers. I mean when you are sequencing, maybe the sequencers will commit to this network so that the other chains can use this information according to this commitment. So it's going to be this chain that needs to go fast, that will be used to coordinate these things and of course this proof then later on is when it's sended to Ethereum where its Lexile ybridge resides and where all the coordination at the end, all the security get embedded with.
04:29:02.962 - 04:30:02.694, Speaker A: So yeah, right now, of course this is the long run. Well this is the recursion of the proof. And right now we have a constellation with one single star, that's the ZKVM. So here the main goal right now is to put the next star, just to have the next chain in the system so that we can start having these transfers. And this electrolyte bridge, the Lexal wide bridge is already developed and we are in the audit phase and we hope that very soon we are able to have one network and another and we can start growing on that and yeah, next year. Thinking importing the POS, these are a little bit, the plannings and the timings that we have on our head for the polygon ZKVM. So yeah, that's it.
04:30:02.694 - 04:30:52.770, Speaker A: Mainly that's my presentation. Thank you, Jordy. We have eight minutes. Does anyone have any questions? I'll go over on this side. Thank you very much for the updates and the presentation. I'm kind of curious with respect to forced transactions. I do understand rationale, however, what if you also allowed a simple transfers on l two? That would essentially kind of enable censorship resistance for l two addresses, not just withdrawals.
04:30:52.770 - 04:31:33.346, Speaker A: They seem to me like very simple transactions. No smart contract execution, nothing. I mean, I still wouldn't be able to maybe take my funds out of some complicated smart contract and whatnot, right. But at least I would be able to move out of an address that might be potentially censored. Yeah, the idea is, we call four transactions. The idea is that you should be able to put any transaction and you know that this transaction is going to be executed. And this can be generally going to be a withdrawal transaction, but it can be maybe a vote transaction or transfer to another breach transaction or whatever.
04:31:33.346 - 04:32:17.022, Speaker A: So this can be any other two transactions. This is the goal. This is the main picture. What we are thinking, but this is a temporary solution to enable these transactions as fast as we can, is we are thinking in restricting the kind of transactions that we admit to just withdraw transactions, which we expect that are the most natural. Start with these transactions. And this because you reduce the vector attack of the system. This give us more confident that give us more confidence to enable these things.
04:32:17.022 - 04:33:19.662, Speaker A: But just you need to see that a little bit. An intermediary part for enabling that. You need to think that convincing security people about that. It's hard. I mean, you need to be very confident. But my question was, why just a withdrawal and not for example, also simple transfers, I mean we can extend to that, but there is asking for transfer, simple transfers, probably they're not dangerous, right? If the network is censoring you, why you want to be in that network? So I would say that with withdrawals, I would say most of the people should be more than enough for most of the people. But I agree that we can find some edge cases and some places where other transactions would be required.
04:33:19.662 - 04:33:57.460, Speaker A: And that's why the system by design should accept any transaction, not only withdrawal transaction. So I think that I agree with you. Anyone on this side, otherwise, Abel, there's the guy over there. Thanks for the talk. My question is about big proofs. So you said that for a user, it's completely transparent. If they submit very expensive transactions to prove.
04:33:57.460 - 04:34:29.520, Speaker A: But you also say that provers, that proof, big proofs, produce big proofs, they are compensated more. How does it work? Yeah, the idea is that this is very much about up to the sequencer. So the sequencer is free to include or not include the transaction. So the idea is that if a sequencer selects one of these transactions, that, for example, does a lot of ketchup. That's expensive. That's an expensive transaction. Who ends up paying? The prover is a sequencer.
04:34:29.520 - 04:35:08.252, Speaker A: So the sequencer can decide, okay, including this transaction to the system. It's not worth it for me because I will have to pay more to sequence these transactions. Because when you are sequencing the transactions, you are paying the cost of the proverb. When you are sequencing, when you are sequencing is when you decide, okay, so at the end, yes. Okay. The user ends up being aware because the sequencer will let or will not let them include that transaction. Okay.
04:35:08.252 - 04:35:40.170, Speaker A: Does that make sense? Yeah, makes sense. Regarding that, it's a little bit more complex because when you are sequencing transactions. You don't know. You are just sequencing. You don't really know yet how much is going to cost the prover. Okay, so the way we are solving that is so the sequencer will have to pay like a maximum, and then we will get back the remainings. Just.
04:35:40.170 - 04:36:04.130, Speaker A: You want to keep thinking on that. That's where we are moving. Anyone else? Anyone else? Going once, going twice. Let's give it up. Thank you, Jordy. Thank you very much. Appreciate your time.
04:36:04.130 - 04:37:05.380, Speaker A: Do you know where that remote is? The clicker? Did I give it to you? Maybe I put it in my. I'm sorry, man. Good thing I got it from you. Ciao, man. Have a good day. One moment, because we're trying to change the presentation to the proper one. Yeah, soon.
04:37:05.380 - 04:37:41.130, Speaker A: They're just changing the presentation. Oh, look, hold on. Let me help you out here. Open that again. Yeah, he's not here. Hey, Luca, are you here? Did you submit your presentation? You did? What's her name? Agneschka. Agneska, right.
04:37:41.130 - 04:38:24.162, Speaker A: Aggie, do you guys have another folder? Oh, sweet. Natalie's here. So, Lucas is called, tracking time finality to ELT. Say again? Oh, he's number seven. Yeah. That down. Yeah, he's number seven.
04:38:24.162 - 04:38:40.200, Speaker A: Yeah, exactly. Amazing. We did it. Thank you. All right, Luca, thanks for being patient, man. Let's give it up for Luca from l two beat. Here you go, sir.
04:38:40.200 - 04:39:04.670, Speaker A: Let me give you this clicker as well. You need the remote? Yeah. Here you go, sir. Hi, everyone. I'm Luca, researcher at l two beat. And I'm here to talk about finality, which is a topic that we have been researching for quite a while now at l two bit. And in particular, I want to discuss how to track time to finality of l two transactions.
04:39:04.670 - 04:39:30.994, Speaker A: So let's first talk about what is finality. What is the goal of finality? And the goal of finality is to prevent this scenario. So you have hif that wants to buy Bob's car, right? So Bob says it's five e. Okay. Sending Eve sends five e to Bob. Bob checks the wallet. His balance has increased, received, and sends the car to Eve.
04:39:30.994 - 04:39:58.906, Speaker A: Eve receives the car and goes away with the car. But then something happens. The chain reworks. And then what happens is that Bob money is gone and this car, too. So, with finality, we want to prevent this scenario. But first we need to understand what is a reorg? So, what is a reorg? You have the blockchain. You add a block to the blockchain containing the transaction and sending money from Eve to Bob.
04:39:58.906 - 04:40:33.722, Speaker A: And Bob is looking at this block. He considers this block as the tip of the chain. But then a fork happens. A fork happens, and Bob is still looking at the other block, because the fork choice rule tells him that the orange block is still the tip of the chain, right? But then the other fork gets built upon, and Bob's view of the chain changes. And this is a reorg. A reorg is like when a node changes his view of the chain, of the tip of the chain. That's when a reorg happens.
04:40:33.722 - 04:41:29.670, Speaker A: And in this case, since now Bob is following the second fork is as if this block has never happened, but actually not quite right, because this transaction is still valid. So it can be retransmitted in the fork. But if there is in the fork a transaction that invalidates the other one, like, for example, eve sending five e to Carl and the balance is not enough, then you get a double spending attack, right? Another failure scenario that can happen in blockchains is when two different people have different views of the blockchain. So like for example, heave is seeing the orange block at the tip of the chain. So from Eve's perspective, the money is sent. But from bomb's perspective, since he's on the other fork, he doesn't see that his balance has increased. And the consensus protocol's goal is that all correct nodes in the network must eventually agree on a single consistent view of the ledger of the transaction history.
04:41:29.670 - 04:42:25.194, Speaker A: And this is why reorgs happen, because eventually they need to agree, right? So one of the views has to reorg to follow the same chain, the same ledger, probably. As all of you know, the capturem presents like a trade off. When designing consensus protocols, you can either favor consistency or availability in case of partitions or dynamic participation. And so when a network partition happens, you can either halt a network and be like safety favoring, or you can continue to produce blocks on either partition. And in this way, the network is still available, but you lose consistency because they're going to confirm potentially different blocks, because there is a partition. And if the partition is known, it's easier to prioritize safety. Otherwise it's easier to prioritize liveness, like in Nakamoto consensus.
04:42:25.194 - 04:43:18.970, Speaker A: These are two amazing papers, of course, to read about these trade offs. But still, the thing is, there is this trade off, right, between safety and liveness in consensus protocols. And ethereum prioritize liveness with LMDgoSt, but also has this other consensus algorithm, which is casper Fft. Like when the network conditions are good, you can also finalize blocks, which means that these blocks are never going to be reor. You're never going to fork out these blocks if they are finalized, right? But you need to have good network conditions. If those conditions are not met, you can still like, the network is still going to be available using LmDgOST. And these protocols that are using one that is like safety favoring and another that is liveness favoring are formalized in what is called ubunt flow protocols.
04:43:18.970 - 04:44:10.430, Speaker A: This is a very good paper. I suggest reading that. Okay, so like on Ethereum, we can say that a transaction is finalized when it reaches Casper FFG level of confirmation, right? But what confirmation are you using? Like, are you using Lmdgost? Are you using Casper fft? Or are you using, maybe the notification that you get in the front end of your applications when you send the transactions? Or maybe you're checking getterscan and you see that your transaction has this success batch with one block confirmation and 14 included 14 seconds ago. Or maybe you see this on another cater scan like Explorer. You see success on your transaction and confirmed by sequencer. Or maybe you're checking the block that contains your transactions and you see unfinalized on the block. Or maybe you see unfinalized safe.
04:44:10.430 - 04:45:03.770, Speaker A: Or maybe you see finalized. Or maybe you're on Zikasynkira and you see Zikasynkira processed ethereum sending. What does it mean? Or maybe you see Zikasync era processed and ethereum executing, or Zikasync era processed ethereum validating, like what is going on here? Or maybe you're on Arbiscan and you see that the block containing your transaction has 183 l one block confirmation. So is your transaction finalized, like maybe like 183 l one confirmation or a lot, right? But then you go on Twitter and you see this tweet. Arbitrarium transaction confirmation execution is instant, but it takes seven days to achieve main net finality. Do I need to wait for the challenge period to pass? And then you see this tweet by Barzak. But wait, I've heard that optimistic roll ups need seven days for finality.
04:45:03.770 - 04:45:44.846, Speaker A: You need to wait seven days for l two state finality on l one. That's a different thing. So there is a lot of confusion and the UX is not helping here at all. So at Altobeat, we want to try to make more transparent all this stuff that is going on, right? So let's talk about roll ups. There are two types of rollups based on the kind of data that they're publishing. On l one you have either transaction based roll ups publishing inputs on l one, so transaction data, or you have stated based roll ups publishing outputs like Zikasin, Cura, DyDX, Darknet and so on. And the finality guarantees for these roll ups are different.
04:45:44.846 - 04:46:35.578, Speaker A: And why is that? Unlike l one, like on l one, when you receive a block, you have the list of transactions, and you also have a state root in the header, and you need to execute the transactions to check whether the state that you have computed corresponds to the state in a block header. And if they do not correspond, you discard the block with all the transactions in it. But in transactions based roll ups, this is different. Those transactions are going to include it in the ledger regardless of the validity of the state route. Like even if the state route is going to be proven invalid, those transactions are not discarded. So in transactions based roll ups, you don't need to wait for the state to be validated either in an optimistic or Zika way to be sure that your transaction is final. Like you don't know about the state, but you know that it is included.
04:46:35.578 - 04:47:15.526, Speaker A: Whereas in state diff space roll ups, this is not possible. You cannot accept a state diff without approve, because a state diff is an output of an execution, and you need to prove that that execution is correct. But estate diffs roll up, you also have this trade off between proof cost amortization and time to finality, because you want to reduce the cost of proof generation and proof verification. So you want to wait for more transactions before publishing a proof. So you can amortize the cost better. But this means that you're going to wait more to publish your proof. And this means that also your time to finality is going to be worse.
04:47:15.526 - 04:48:23.250, Speaker A: So in general, someone would expect that time to finality for transaction based roll ups to be much shorter than state diffs proofs roll ups, but also like in stated's roll up, there is a way around on this. You can aggregate proofs across many Ziki roll ups like sharp is doing for Starkx system and Starknet. And in this way, even if you roll up the finalities, but in the derivational spec, you can check this on their documentation on GitHub, they have what is called a sequencing window. And the sequencing window on optimism is 12 hours. And this means that if the sequencer behaves in a certain way, your transactions, even if it's published on l one, it might not be final within 12 hours. Because optimism is not a based roll up, it has some rules regarding sequencing. And in the worst case you will have this 12 hours delay even if you see your transactions like in seconds when you send them to the network.
04:48:23.250 - 04:49:04.522, Speaker A: Also scroll, scroll is a transaction based roll up. And so also in this case you might consider your transactions final when they hit l one. And so you don't need to wait for the state to be validated. Right. But this is not the case because in their smart contracts they have this function that can revert patches if the state route regarding this block, this batch has not been finalized. So this goes back to the case in which you need to wait for the state route because they can reverse patches and it's a permission function. So as I said, it's not enough to just look on chain.
04:49:04.522 - 04:49:36.566, Speaker A: You need to check the derivation function of the roll up. You need to check whether there are permission function that can delete patches. But also there is this very interesting case regarding state diffs roll up. So this is a tweet that I posted a while back and goes as follows. Let's say I want to sell my car for five e via a state diffs roll up. After some time I notice a state diff showing that my balance has increased by five eth or more. Who should I sell my car to? Like I don't have transaction data.
04:49:36.566 - 04:50:12.434, Speaker A: I cannot see who has sent me this five eth. Right? But some of the answers were interesting. So ask a sequencer for explanations. Well, maybe, but the sequencer might be down or malicious and doesn't want to answer me. So I have no way to get transaction data from anywhere, right? Or my favorite one, you give a little piece of your card to every address that has minus five eth in their account in the buck. Well, maybe. Or the new NFT owner, and this is an interesting one like this could solve this solution, because you could see in the state diff that the NFT has been transferred from your account to another address.
04:50:12.434 - 04:51:22.262, Speaker A: But this means that the fact that you're using state diffs leaks to the application logic. So if you're building a zkey VM ethereum equivalent roll up, the narrative is that you can deploy your application without any modification to the roll up. But if the fact that you're using state diffs leaks to the application logic, then you also need to change your application, right? So you're not ethereum equivalent, I'd say so on adult bit, we started tracking liveness of these roll ups because liveness can be thought as a lower bound indicator for time to finality. Because if your roll up for example is posting data once per month. You cannot expect the time to finality of your l two transaction to be like in the order of magnitudes of seconds or minutes. Right? So we started tracking liveness for this is Zikasync era, for example, and Zikasync era state updates. And you can see that there are some like in the hit map, there are some hours in which Zikasync era hasn't posted any updates.
04:51:22.262 - 04:52:16.620, Speaker A: So in some cases ten to finality is in the order of hours for Zikasync era. And also we're running some anomaly detection algorithm to detect whether there are times when the sequencer or the proposer is down. This is Zikasync era and this is optimism. You can see that optimism very much different because it's posting data much more frequently and it doesn't have much downtime usually. And this really affects time to finality. Like you would expect time to finality to be much shorter for optimism rather than Zikasync era. But as I said, optimism has this 12 hours period sequencing window in which in the end, time to finality is worse for optimism, even if you see that it is more live on chain, so it can be thought of a lower bound, but still not a very good one.
04:52:16.620 - 04:52:36.226, Speaker A: And I'm also very excited to announce that we just released this feature, the liveness feature on Alphabet. This is currently live. You can go check. Yeah, I want to thank the team. They're amazing. They have worked really hard to push this feature. So the feature is live.
04:52:36.226 - 04:53:10.970, Speaker A: You can go check it out. Helltobit.com scaling liveness we are tracking live metrics for liveness, for all the chains, for all the roll ups. There are still some edges that needs to be sharpened, like the anomalies detection algorithm. But this type of data is amazing. So we can also show in real time this certain sequencer or proposer are down. But still there are a lot of limitation to this approach, like in the approach of using liveness as an indicator for time to finality, and this is an example.
04:53:10.970 - 04:54:27.922, Speaker A: So let's imagine that you have an l two that has a block time of 4 seconds, while l one has a block time of 12 seconds, right? So there are four l two blocks for each l one block, and the l two is posting the data for only two l two blocks on l one at a given point in time. And this means that since the l two is producing more blocks than they are posting on l one, you're going to get increasingly behind l one. So you can see in this example, in the first block there is a delay of 0 second, but then it becomes 8 seconds and 4 seconds, 12 seconds and 8 seconds, 16 seconds and 12 seconds and so on. So even if you see that in each l one block, some data from the l two has been posted, you cannot trivially see whether the roll up is behind the l one, in the sense that you cannot see whether the delay between l two software confirmations and finality on l one. So this delay might be very big. And for example, one example that we discovered is Tarknet. Starknet is posting a lot of data on l one with an average of 12 seconds.
04:54:27.922 - 04:55:29.258, Speaker A: Like they're posting data in each annual block. So you would expect time to finality to be very short, but actually they are behind l one 6 hours. So when you post a transaction on Starknet, you need to wait 6 hours before seeing it on l one, even if it seems very live just by looking at l one. But what about soft confirmation? So we've talked about Casper, FFG, LMD, Ghost, and you can imagine something like soft confirmation being something at the beginning, like, this is the wrong mental model, don't use this. But still, can you get any guarantees of soft confirmations? L two soft confirmations. And this is also a problem that we're trying to study. And there are many ways, like you could track the sequencer repetition, for example, and see whether the subs confirmation corresponds to what is posted on l one.
04:55:29.258 - 04:56:05.166, Speaker A: That would alone, in my opinion, being an interesting metric. And also for accountability of sequencers on l two. But also we can track economic guarantees of slashing system. You can imagine ll two having some consensus, giving some confirmations, and then a slashing system model one, so that if the sequencer lies, you can slash them. And this is something that we want to study in collaboration with Nethermind. We are currently working on this. We also want to have some metrics regarding the perspective of different nodes.
04:56:05.166 - 04:56:42.234, Speaker A: Like if you're running a line node, a full node, or if you're the sequencer, you're going to have different economic guarantees based on your perspective. And also we want to track at any given point in time, what is the amount of value that is at risk of being reorked out, because not final. And that's it. Thank you. Sweet. We have about ten minutes. Congrats on the new rollout.
04:56:42.234 - 04:57:18.886, Speaker A: That's awesome that you guys are doing an alpha leak. It's not really a leak, but giving some alpha during our little sessions here. Does anyone have any questions for Luca? If not, I have ten minutes of coin sessions for you guys. Any questions for Luca? Going once, going twice. Got one. Yeah, my guy Abel is right in front of you, dude. Shaking his hand like it's broken.
04:57:18.886 - 04:58:02.794, Speaker A: Hello. Sorry, you said that. Is there a metric for measuring the l two soft confirmations? How would you compare soft confirmations across l two networks which have different designs and different mechanisms? How do we measure, what metric, what unit do we measure that? Yeah, that's a different topic. Of course, it depends whether the soft confirmation is given by a consensus running on al two, if the sequencer is centralized or not. But also, it depends whether you have any economic guarantees. Right. So if you have a slashing system, we can compare different soft confirmation based on the amount of stake that you can slash on l one.
04:58:02.794 - 04:58:29.080, Speaker A: Right. And in this way, you could measure and compare different soft confirmations based on that. You're good. He's good. You coming around here to give or are you just walking? Do you have a question? This guy? No, I saw that. There's a guy over here. There he is.
04:58:29.080 - 04:59:14.850, Speaker A: I have a question. Do you see any pros or cons between the state of based and the full transaction history based roll ups on this falling behind in the state? What you just mentioned, for example, that what we saw with optimism, I think that there are two times in the one es lock time. No. So I don't think there is much difference regarding falling behind. The main difference between state diffs based roll ups and transaction based is that in one case, you need to wait for the state route to be verified. But regarding falling behind, it all depends on how fast you can post data. Right.
04:59:14.850 - 04:59:54.308, Speaker A: And with state diff space roll up, it could be worse because you need also the time to generate the proof. Right? So there could be cases in which you're falling behind because of that, because of the proof generation that you need to post on l one. And yeah, I would expect this problem to be not to be present in transaction based roll ups. But still, you could still have the problem if you're not fast enough in publishing data. You good? He's good. Anyone else? Anything come up for them in the meantime? Hey, you up here? I got him, Abel. All right.
04:59:54.308 - 05:00:20.424, Speaker A: You got him? My dude's getting his steps in today. Hi, I got a question about the new product. First of all, congratulations on launching. Thank you. Product. But, you know, like, there are tons of different l two algorithms, and then how do you kind of come up with a unified metrics for liveness of those different type of. Regarding liveness or in general liveness.
05:00:20.424 - 05:01:03.020, Speaker A: Regarding liveness, yeah. For example, we are tracking both batch data submissions and state updates. And for most of them, those two aspects are the basics of every roll up. In every roll up you would see state updates and some kind of data submissions. Right. It is more tricky with state diffs roll up because they're going to be published within the same transactions. But also there are some roll ups that usually there are different processes in publishing data and publishing state routes in most cases because the sequencer, the proposers are different roles.
05:01:03.020 - 05:01:40.824, Speaker A: But this is not true for all the rollups. Like for example, linear is posting data with state routes most of the time. It's easy in this case for liveness to create a framework that encompasses all the technologies. But also we have the anomaly detection algorithm and there are certain roll ups that are very regular in posting data. So when you detect anomaly, it is actually anomaly. But for some other roll ups, they have different schedules in posting data. Sometimes they post it very often and sometimes they do not for hours.
05:01:40.824 - 05:02:16.310, Speaker A: And this is for example starknet. And in that case we probably need a custom algorithm. So we try to find a framework that encompasses all the roll ups, but in some cases there are always exceptions and we need to adjust for that. Yeah, I'm trying to ask maybe you guys going to come up with different type of liveness within different ecosystems, like sub ecosystems. I mean, we are tracking ethereum roll ups, so we just need to track. Yeah. Thank you.
05:02:16.310 - 05:02:35.716, Speaker A: Cool. Anyone else have questions? Otherwise we can clap it up. Going once, going twice. They turned the lights out on us. All right, let's give it up for Luca one more time. And congrats to l two beat for the launch. Successful launch.
05:02:35.716 - 05:13:00.130, Speaker A: Yeah, you can put it on the table, guys, we're taking a break now for five. Well, we're five minutes to the half hour mark. It's coffee time. Feel free to get up, stretch your legs, do some squats, some kicks. I recommend you come back here in like sooner, 20 minutes or something because I don't want to call him savior. Vitalik's coming back, baby. Dah, it don't SAP.
05:13:00.130 - 05:14:17.450, Speaker A: Dip, dip, dip, dip, dip, drip. Dip, dip, drip, dip, dip, drip. Dip, dip, dip, drip. Dip, dip, dip, dip, drip. Dip, dip, dip, dip, dip, dip, dip, drip. Dip, dip, dip, drip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip, dip. Rip.
05:14:17.450 - 05:16:35.310, Speaker A: Rip. Rip. Dip, drip, drip. Dip, drip, drip, drip, drip. Only night. That's it. You won't.
05:16:35.310 - 05:28:00.680, Speaker A: I won't I you. It's won't. I won't. I won't. I it it's. It sa. Don't.
05:28:00.680 - 05:29:23.404, Speaker A: Dip, dip, dip, dip, drip. Dip. Dip. Dip, drip. Dip. Dip. Drip.
05:29:23.404 - 05:29:29.948, Speaker A: Dip. Dip. Drip. Dip. Dip, drip. Dip. Dip.
05:29:29.948 - 05:29:35.884, Speaker A: Dip, dip, drip. Dip. Dip. Rip. Dip. Dip. Dip.
05:29:35.884 - 05:29:40.628, Speaker A: Dip. Rip. Dip. Dip. Dip. Rip. Dip.
05:29:40.628 - 05:30:14.800, Speaker A: Dip. Dip. Dip. Dip. Dip. Dip. Dip.
05:30:14.800 - 05:30:24.300, Speaker A: Dip. Dip. Dip. Dip. Dip, dip, dip, drip. Dip. Dip, dip.
05:30:24.300 - 05:36:34.206, Speaker A: Drip. See? Yeah, that's you. I'm. It's. Don't. Check. Check.
05:36:34.206 - 05:36:46.802, Speaker A: Come on up. Come on up. Yeah, why not, dude? Come hang. Come kick it. Let's get to know one another. Stay a while. It's going to wait a little bit.
05:36:46.802 - 05:37:02.710, Speaker A: Let the room fill up some. All right, so here's the deal. We are one short. They only have six microphones here. Vitalik, you definitely. They're both mine. So, yeah, you two can share.
05:37:02.710 - 05:37:32.142, Speaker A: Vitalik has one, and I guess the three of you, you guys buddy up and I guess you'll have your own over there. Yeah. We have locally sourced, artisanal turkish water for you. I think it's good high ph. Made sure that you guys are nice and hydrated up here. Vitalik, when there's, like, ten minutes left, I'll come and give you one of these, and then we can maybe do questions if you guys have time. Okay.
05:37:32.142 - 05:37:45.038, Speaker A: Total, we have. You're saying 60 minutes or 60? Yeah. So at 50 minutes, I'll come and do one of these. Okay, sweet. Great. So are we ready to go? Yeah, you guys are. Guys, welcome our panel.
05:37:45.038 - 05:38:04.220, Speaker A: This is so hyped that these folks are here, the powerhouses of l two. I'm so pumped to be here, guys. Take it away. Okay, great. So welcome to the L2 panel, where we're going to talk about the road to decentralization. Oh, yeah. One last thing.
05:38:04.220 - 05:38:55.690, Speaker A: So, as a surprise, we're going to start this off in the format of a Scott Alexander presidential debate, which means that, ben, your forbidden letter is you. So, in answering the first question, you're not allowed to use the letter you. So please introduce yourself and your project and your favorite animal. Okay, well, try again. Okay. I'm Ben from optimism, and my answer was going to be an eight legged animal that lives in the ocean. But I'll say the octopus, which was our original optimism, mascot of that word, which I cannot use the letter.
05:38:55.690 - 05:39:16.730, Speaker A: Use up. This guy's got us beat. So what's interesting in optimism? Oh, man. Well, we just introduced a plasma design, which is different than a roll asterisk P, which is super, super cool. In collaboration with the lattice folks. Oh my God. This is impossible.
05:39:16.730 - 05:39:26.820, Speaker A: I can't do this. I'm second guessing everything that I say, more so than normally when I'm on stage with a bunch of people. This is brutal. V. This is horrible. V. Amazing.
05:39:26.820 - 05:40:06.850, Speaker A: So your forbidden letter is v. So, can you introduce your project and yourself and your favorite animal? Yeah, my name is Joe. I'm one of the co founders of Aztec, which is. Okay, we are a cryptid L2 on ethereum, but we use a different word usually, which I can't say beginning with P. My favorite animal is the narwhal. It's our new mascot for noir, which is a general purpose, succinct programming language. Ck programming language.
05:40:06.850 - 05:40:35.290, Speaker A: Okay, so your forbidden letter is w. So introduce yourself and your project and your favorite animal. Sure, my name is Henry. I work at the stark work. Sorry, work contains a w. Yeah, I collaborate with the Starknet foundation with. I'm a collaborator at the Darknet foundation.
05:40:35.290 - 05:40:59.602, Speaker A: And my favorite animal are ostriches. Amazing. Okay, so Alex, your forbidden Letter is X. So introduce yourself, your project, and your favorite animal. Sure. I'm Alex with Ks. I'm from ZK sync building ZKVM roll up.
05:40:59.602 - 05:41:29.740, Speaker A: And if you follow Zk sync, then you will know that my favorite animal is a cat, which is of this. Cool. So, Tagril, your forbidden letter is a. So introduce yourself, your project, including your favorite leg thing. Hello, my name is Toggirl. I do research. Research contains a day.
05:41:29.740 - 05:42:00.050, Speaker A: Turn my mic off. I'm going to be cracking with scroll. And we're building the ZKVM. It's not the. But I can't use the other word. So I'm kind of forced to roll up on Ethereum. And my favorite animal favorite is the quokka.
05:42:00.050 - 05:42:21.058, Speaker A: Amazing. Quoka also contains an a. Okay, your forbidden letter is b. So introduce yourself, your project, and your favorite animal. I'm Jordan Belina. I'm the technical lead. Yeah, the technical lead of polygon CKBM.
05:42:21.058 - 05:43:02.278, Speaker A: And my favorite animal probably is ant. Or if you want the ants, because for me, ants, yeah, it's one of the animals that have more interesting social communities and social organizations by themselves. So I've seen a very super interesting animal and is one of my favorites. I mean, a lot of interesting things happens in the ants world. Okay, amazing. Okay, well, we shall go back to the normal format for the rest of the hour. But I just like to say, though, I was expecting to come up and defend your one of n honesty assumption, but we did just demonstrate perfectly.
05:43:02.278 - 05:43:55.110, Speaker A: How fraud proofs work in l two networks and we have our one of n challenger right here among us. Okay, the first question after this is basically let's talk about decentralized validation and the road from being a fake roll up, which is actually a multisig, to being a real roll up and where you are on that, and basically where you see the rest of your transition to becoming a fully fledged and decentralized roll up in terms of validation as being start here. Yes. Cool. Yeah, so I mean, this is obviously super critical. We've got to reach stage two, as enshrined by the dude to my right here. So Ultimately, for us, decentralized validation boils down to multiple redundant implementations.
05:43:55.110 - 05:44:29.010, Speaker A: Like our analysis is that the systems that we're building have extremely high level complexity. And rather than work in what feels like a sort of sisyphian effort towards proving that this one implementation has absolutely no bugs, what we want to do is have multiple proofs so that the risk that a bug occurs in the same place at the same time is the only way that the bug propagates into a safety failure. So that's what we're doing in terms of progress there. There's kind of two aspects to the progress. One is like the initial implementation of reference, implementation of the fall proofs. We just launched our fall proof alpha last month. Very exciting.
05:44:29.010 - 05:45:07.134, Speaker A: Woo. Yeah, there we go. What does alpha mean? Alpha means that there are not redundant implementations, so you can play dispute game and prove outcomes, but ultimately, other than that, it's running on Testnet and it's just a reference. The other aspect to that is that you need redundant implementations to be able to do this. So that's why optimism has emphasized a lot a multi client sort of client diversity ecosystem. So we now have two consensus layer clients and four or five execution layer clients at this point. So that's not immediately manifested, for example, in any way in the alpha, but it is a significant step forward that we're super excited about.
05:45:07.134 - 05:45:30.930, Speaker A: Right, so this is clients, not proof systems, right? That's right. But the way that we build proof systems is we take the clients and we compile them into a proofable context. So it actually is super relevant because this is the implementation that ends up in a proof. Got it. Okay. Yeah. So what is mastec's take on this? Yes, so I think we have a kind of slightly different take, I guess based on past experiences.
05:45:30.930 - 05:46:25.398, Speaker A: So we're trying to design the next version of Aztec to be launched as a stage two roll up at a minimum, and that's based on kind of experiences with aztec connect and not being at that threshold and learning the hard way that kind of iterating these systems in production with hundreds of thousands of users is a lot harder than it actually looks. So designing it right from day one is the approach we're taking. We have a public forum, forum aztec network, where we do all of our kind of rfps for decentralizing the sequencer and proving there. And the one at the moment which kind of links back to the question is around training wheels. So we have a process of kind of deciding which training wheels we're going to apply to actually get to stage two. My favorite one so far is one called BLS training wheels. It's basically shoehorning in BLS aggregate signature next to the ZK proof system and making sure the two agree before a block is valid.
05:46:25.398 - 05:47:01.030, Speaker A: So that's the approach we'll take likely at launch to get confidence in the ZK proof system. Henry. So currently sequencing on Starknet is centralized. We recently converged on a consensus algorithm to distribute sequencing in the midterm, I would say. And in the meanwhile we are also working on everything that goes with it, meaning having various implementations of sequencers. We have free compatible sequencers so far. We're also working on prover diversity and node diversity.
05:47:01.030 - 05:47:30.206, Speaker A: We're currently in between stage zero and stage one. I think at this point what misses is we're fixing a few things. Most specifically we want to have. I mean, we're thinking about having a mechanism for forced transactions or escape hatches, and then we're just looking if things work well and eventually when we feel more confident, we'll be able to take off the training wheels. Right. But force inclusion is on the sequencing side. Right.
05:47:30.206 - 05:48:12.298, Speaker A: It's not on the validation side. Fair enough. Right. One question is, what do you think is the biggest thing that's stopping you from going to stage one in terms of validation? Like tomorrow. So what's stopping us from moving to stage one? Yeah, I think delays in upgrades right now I'm not sure that answers exactly your question, but I think right now the system is not secured by many parties, so it can be upgraded very fast, and that's an issue to move to stage one, is that correct? Right, yeah, well, I mean. Right, so you're basically using upgrades to fix emergency bugs. Exactly.
05:48:12.298 - 05:48:22.046, Speaker A: Right. And you want to be able to do that very quickly, even without going through a big multi sig. Yes, understood. Okay, that makes sense. Yeah. Okay. Yeah.
05:48:22.046 - 05:49:03.098, Speaker A: Alex, what is your take on decentralizing validation in Zksync. And when are we going to see the Zksync world? Go to some, I guess, greater decentralization. What are the thoughts and the plans? Sure. So I would separate decentralization from stage one, stage zero, stage stage two, because the stages apply to trustlessness. Like how much do you need to trust the team behind the core team, the core contributor? So we are working on decentralization of the sequencer that is coming soon. We have plans for decentralizing the prover and the prover. You actually can run the ZK stack now completely on a single laptop.
05:49:03.098 - 05:49:54.400, Speaker A: Like with the prover. You have a GPU prover with under 16gb of ram, just a powerful gpu normal gaming machine, and you can run your own complete blockchain, full ZK roll up on one single laptop. I think that this is really cool. Can I ask how long it takes to prove the equivalent of like a 30 million gas ethereum block with a laptop? I can't answer that out of top of my head, but we can calculate the cost is going to be still the same as when you prove it. Ten minutes, 1 hour, one day. I honestly don't have this out of the top of my head, but I think one block is provable in something like 15 seconds or. I don't remember, it's seconds for a piece of, of like, for one big circuit, something like.
05:49:54.400 - 05:50:51.326, Speaker A: So the stages are about trustlessness. Going from stage zero, which we are today with Ziki sync era, to stage one is relatively easy because we just need to enable level up the security council to where we are with Ziki sync Lite. And we're going to do that right after the audits for pujama is completely live. We just run a 1.1 million crowdsourced Security Council security contest on code arena, which is, I think, the biggest in its history. Going to stage two is meaning like, making your roll up completely immutable. I don't think anyone is here will be comfortable with this for a long time, because it's not about any bottle testedness of the proofs or running a multiproof system, which I'm a big fan of and we definitely want to see in the future.
05:50:51.326 - 05:51:44.114, Speaker A: It's about the long tail risks of having a black swan in any part of your code for anything like whatever. So you still need some way to recover from those very unlikely but very deadly mistakes that could still be there. And we know that bugs are still found on bitcoin or ethereum corridor systems years after they launched. So we need to solve this problem somehow, and I think that once the solution is found, it will equally apply to everyone. I think for us, the main steps that we need to take to be a stage two roll up is there are a few things that we need to do on the censorship resistance side, but when it comes to validation. So what we care about is to minimizing the possibility of user fund something happening to user funds. So there are three things that we're working on.
05:51:44.114 - 05:52:57.122, Speaker A: One is a multiproover and it's already implemented. It's an SGX based prover that we're going to run in parallel with our halo two prover. We've already run it on the testnet and it proved all the historic blogs, so we would soon deploy it on the testnet full time, and then after some testing it'll go live on the main net. Then I've proposed a year ago of a concept of multiverifiers, which is a notion of using two different implementations of the validating bridge to ensure that basically implementational bugs don't result in funds being drained, but instead result in aliveness failure. Because if two of the contracts disagree on a state mutation, you just pause the contract and let the governance decide. And then three, with those two, you essentially eliminate the possibility of either the proof being broken either through a bug or having an error in the specification of the proof itself. Basically it can no longer result in your funds being stolen because you have two proofs running simultaneously.
05:52:57.122 - 05:53:46.678, Speaker A: And you're also protecting yourself against smart contract having bugs because you need to have two contracts have the same bug, essentially. And then the third step that we need to take is formal verification of the validating bridge. And I feel like those three steps will allow us to have delayed upgradability, which is enough to be a stage two roll up. Yeah, in our case here, I would distinguish the remaining things. So one thing is things that are pending to implement, and other things are decisions that are more security decisions if we feel confident enough to enable some actions or not. In our case, for example, the four transactions, we have all the system designed with the four transactions. Actually in testnet, four transactions are enabled and we are to a single transaction to enable that.
05:53:46.678 - 05:54:34.006, Speaker A: And here it's more a matter of convincing security people and understanding. I mean the problem is that when you enable, when you decentralize in general, but in this case, when you enable force transactions, you open the back door probably to stopping the network or somebody can send the transactions there. And here we are a little bit afraid that the system is not mature enough and here is, well that's why we want to run a bug bounty before enabling that. One thing, interesting thing that we will do is we will limit the forced transactions. So just allowing only not any forced transactions, but just how we throw transactions on things like that, just limiting the vectors attacks. And this is one of the pieces that are remaining for being at the stage two. The other piece of course is delaying that.
05:54:34.006 - 05:55:01.646, Speaker A: Again, it's another transaction. I mean, delaying is just setting a time, 30 days, 40 days, whatever you want to set for the upgradability if we enable those two. So if we just do these two transactions, we would be in step in stage two here. The problem is it's just a matter of responsibility in the security. We are ready or not from the security perspective. But it should be very soon. I mean, I'm pushing to be that as soon as possible.
05:55:01.646 - 05:56:23.690, Speaker A: Okay. I want to call out one very specific area of this partial decentralization space. So while you're at stage one, or even when you're at stage two, and you might have proof systems that disagree, you need to have some kind of security council that is able to either override it or decide what's happening. And I know that there's a lot of challenging questions around what should the composition of the Security Council be, what kind of people can be part of it? How comfortable are people making the identities public? And then if the identities are not public, then what's the point of having one instead of just being completely centralized? And so for anyone who is interested in answering, how do you think about those questions of how to choose the Security council and how to communicate it and manage all of those things? If Security Council prerequisites for stage two, I think stage two is defined as like no security council. Stage two is defined as either no security council or there is a security council that is only allowed to have power when the system disagrees with itself. No, I see them with multipose here. One of the discussions that we had in the Security council is for choosing these people is, for example, if these people should be public or not.
05:56:23.690 - 05:56:57.778, Speaker A: In one side, you want these people to be public because this is what gives confidence. I mean, this is a trust, reputation, people that's confident in the other side. You don't want to put these people in risk. Or even, for example, we were talking, for example, maybe a lot of the people in the Security Council may be in this room because they come all to devconnect or things like that. And then it's okay, maybe not all of them should be public. But how you prove that some of this here, maybe some zero knowledge applications can be used for proving that somebody's trusted, but you don't know which. But there are some thoughts on that site.
05:56:57.778 - 05:57:41.374, Speaker A: But for me, the goal, maybe it's going to take a long, but for me the goal is not having systems without security console. I mean, for me, governance less systems. This is what for me, the holy grail. I think that we are very far away from there at this point, but it would be good not to forget where we are going. I think it's a matter of just making certain trade offs. So as Jordy described, if you have the governance Security Council public, then you're putting them at a certain risk. But at the same time, if you have them private, I would argue that there is no point in having a security council because you essentially have to trust the team that built that security council.
05:57:41.374 - 05:59:04.750, Speaker A: That security council actually exists because there's no one I can go to and verify and check whether they actually are there, or it's just a team pretending that they have twelve different people running the multi sig. I think you can have a public security council, but while simultaneously minimizing their risk. So for example, in a stage two, the only thing that they realistically can do is pick between two or more proof systems if one of them fails, which means that the only way they can basically be targeted. If you both found a vulnerability that can steal funds, and then you can target them to basically force them to pick the one that was faulty in all the other scenarios, there's no risk for them because they cannot do anything about the protocol. And so I think it's just about minimizing what they can do in the protocol, and that way you can just protect them against people potentially attacking them in hopes that they can steal money from the bridge. Agree to this. Not sure that we can minimize the role to just picking between different implementations because you still have some part of the contract that is shared and there might be still bugs in this part.
05:59:04.750 - 06:00:01.102, Speaker A: Could you make the part that's shared as simple as a safe wallet? That would be the ultimate goal, but you still have some mechanisms like withdrawals and processing of the blocks. Initially, of course, you can write completely separate implementations and actually have them run in parallel, which might make sense that will increase the overhead of the blocks. Maybe this pays off. The other thing is the Security Council could just have a vetor right on the governance decisions of the governance of the protocol. So if you think of a protocol with, I know with some token with a lot of stake. And then in an emergency event you have to get a really high quorum of the stake to propose an emergency upgrade. Then Security Council could have the right to instantly freeze the contract for a short period of time and then only to veto the decision that the governance is taking.
06:00:01.102 - 06:00:49.920, Speaker A: And then they can do it on a private voting manner so that they cannot be pressured by some authorities to vote one way or another. So some ideas here, I think for me, just having some key basics of fully open source code and knowledge sharing within kind of the ecosystem. A lot of the people who could be on a security council are working at a project already. So there's not many people who could actually technically adjudicate on the actions of that council. So making sure that all code is public, that needs to be so they can adjudicate on kind of what they're deciding on. And then maybe working between the people on this stage to make a security council together could be an interesting way forward because we're all trying to achieve the same goal here. Amazing.
06:00:49.920 - 06:01:48.530, Speaker A: More than I don't have a strong opinion and we don't have a proper roadmap as to what this Security council should be. And I very much agree with Jordy that this is a matter of governance to me. And I think that those designs, I mean those mechanisms are often better when they're emergent than design. And that this starts with engaging the community and starting with, I mean, if you need talented people on your governance, on your security council that know what they're doing, you can't just hand pick a few, you have to have a reservoir of people so that you can switch, you can switch them and you can have an efficient governance. And this starts with like, this takes time. Educating people around those matters takes time in my opinion. Okay, so I think just want to move on from the verification question and let's actually talk about decentralized sequencing since I know a lot of us have been racing to talk about this already.
06:01:48.530 - 06:02:20.966, Speaker A: So yeah, I guess starting with Ben, when is optimism and the greater optiverse going to move to decentralized sequencing and how. Yeah, so great question. The work has begun. We have an incredible RFP done by the folks at espresso to implement an mvp of decentralized sequencing in the op stack. Very notably, I would say that that is not, it's still very related to the verification problem. Right. Because there's a matter of coming to consensus on a sequence with something that's more than a one of one outcome.
06:02:20.966 - 06:03:24.190, Speaker A: And then there's the way of validating that during the validation of the proof system. So, in general, the way that we think about this is, sequencing is very unlikely to be one size fits all. There may be some that are actually very happy with a single centralized sequencer because it makes sense for a particular use, case and application. So the way that we think about extending this and also not also keeping the design space open, is it will probably be a pluggable system where you effectively introduce a notion of predicates over sequences. So you submit a sequence that you say the decentralized sequencer agreed on, and then whatever sequencing scheme you want has a pluggable predicate that will return true or false, based on whether it believes that that decentralized or centralized sequencer indeed made that sequence. Obviously, the other important piece of this is to have the ability to force include transactions. Right? Like, once you have that, I think I'm much more comfortable with sequencer experimentation, because even decentralized sequencing comes with its own risks and consensus failures and that sort of thing.
06:03:24.190 - 06:03:56.822, Speaker A: So I think no matter what, you have to have the ability to force in transactions with some latency. So we'll introduce that concept with a standard way to verify predicates around custom sequencing rules on top of that, and kind of go from there. Okay, yeah, for Aztec. So we intend to launch with decentralized sequencer. And I think when you're deciding things like this, you have to think about what you're prioritizing when you're decentralizing the sequencer. And for us, it's censorship resistance. So we've picked a sequencing algorithm called finet.
06:03:56.822 - 06:04:33.830, Speaker A: It's based on a random beacon of VRF over a staking public key. And so we intend to select between 103 hundred sequences a day using that. It's not kind of the fastest way to get transactions confirmed. It's not going to get the highest throughput. But in our trade off space, censorship resistance was the kind of thing we were aiming for. So yeah, we picked a centralized sequencing algorithm that prioritized that on our end. Right now it's centralized, and we settled on, we're going to use a variant of tendermint to sequence on Starknet.
06:04:33.830 - 06:04:56.590, Speaker A: Essentially, that's this. Okay, can I start? If you. In our case, we are not seeing like a single roll up. I mean, we're thinking about the constellation of roll ups and each roll up, it could make sense. So at the end we see the sequencer very much as a consensus piece. Centralized sequencer is a consensus where, I mean, it's a dictatorship. Consensus.
06:04:56.590 - 06:06:13.570, Speaker A: Consensus somehow. But different networks can have different needs. At the end, if we go to the trilemma, each network can decide where they want to be, and depending on the network, they want maybe high finality, or they want a high decentralization, high security, or different places, and each roll up. So I think here there is a space for different roll ups, and each roll up may have different consensus at the sequencer level, with different trade offs on that. And the idea is to use different consensus mechanism in this idea to build different sequencers for different networks. I think for us, at some point, while working on decentralization, we realized that the best way to decentralize the sequencer is just by removing it completely and getting rid of it and giving the sequencing rights to the prover, because we were struggling a lot with how to balance the incentives between the sequencer and the prover, because the sequencer has all the control over the ordering, and therefore they control how much mev gets attracted. So we were experimenting with PBS, and at some point we realized if you decentralize the sequencer and decentralize the prover, it's likely that the sequencers are going to outsource block building to some specialized builders anyways.
06:06:13.570 - 06:07:02.066, Speaker A: So essentially, in this case, your sequencer becomes a middleman between the builders and the provers. So why not just remove it? Right? So then the next question is, what's your roadmap for decentralizing the prover? So, currently there are a few things that we're working on. So, one is how to implement PBS within this framework. Two is our current working model. The leading model is adopting hot stuff, too, so that the two round modification to hot stuff. And also we have to ensure that the proving is asynchronous to block production. Because what can happen is, for example, the selected prover proposes the block, but then never publishes the proof.
06:07:02.066 - 06:07:49.474, Speaker A: So we need to ensure that the proof is still published somehow. So the current model is we assign a certain amount of time, so let's say 100 blocks for that proof to be published. And if that proof is not published, then another proof, a fallback prover, is selected for that particular block, and it gets to submit the proof and then steal the reward from the elected leader. I think what Targul just said can be also reversed, defined as the sequencer gets to become the prover. Because this is the same thing both ways. Yeah, you kind of still have to decentralize the sequencer. Then if the proverb and sequence is the same thing at Zkstack, we started to work on decentralizing the sequencer way before we even learned about espresso Labs efforts.
06:07:49.474 - 06:08:46.882, Speaker A: So the work is now coming to maturity and we'll also release the first version. But Zkstack is modular and you will be able to replace the sequencer just the same way. Some people will need centralized sequencer, some people will use whatever labs builds, some people will use espresso, some people will build their own versions or maybe use cosmos or something else. But we will for sure provide the default or fully open source version of sequencer based on hot stuff as well, because we need the blocks to be subsequent or like 1 second confirmations. That's like the magic number, which gives you this feeling of instant confirmations and magic Ux. Is it original hot stuff or hot stuff two? Yeah, I think it's hotsuff two. Okay, so do any of you have opinions on one particular decentralized sequencing strategy, which is Justin Drake's favorite, which is becoming a based roll up? Yeah, we have strong opinions.
06:08:46.882 - 06:09:19.298, Speaker A: I think. I love based roll ups. I think our whole algorithm is not pure anarchy, where kind of you're just letting it be a first price gas auction. On l one we have a smart contract that runs the finet sequencing algorithm on l one. It has a few rules it enforces to try and get slightly better pre confirmations or slightly better ordering. But if that doesn't work, it falls back to a based roll up. And so it's kind of like a six minute window where if a proof and a block is not generated in that time, we become a based roll up.
06:09:19.298 - 06:10:05.806, Speaker A: I think it's the only way to really enforce kind of censorship resistance without growing a staking set to the size of Ethereum. We have a similar approach, actually. We have the happy path where we run our own consensus, and then if that doesn't work, we fall back to forced batches, which is essentially base roll ups. Because in our opinion, the important properties that we're looking for is fast pre confirmations, which we don't get with base roll ups. And also, as I mentioned before, one of the important properties that we're looking for is how to easily distribute the incentives between the provers and the sequencers. And then base roll ups by default is just not trivial to see how that will work. I think Justin could disagree that you don't have fast pre confirmations with know.
06:10:05.806 - 06:10:40.402, Speaker A: I know, I had a chat with him about it, everyone had the chat. But the problem is essentially the same. Then if you have to get pre confirmations, you're essentially reaching consensus on the sequencing in PBS with proposals, with builders. So it essentially comes down probably to some version of centralized sequencer. But the party is kind of like asynchronously coming to consensus by the Ethereum validators. But whatever, we're agnostic. Like as I said, the case tech is modular.
06:10:40.402 - 06:11:34.842, Speaker A: You can do centralized, decentralized with our version with someone else or a best based sequencer, and think the market will decide eventually. We don't have to make predictions. Okay, so next question. So it feels like either all, or at least most of you are offering both a roll up offering and some kind of validium offering. So how do you think that relationship will continue to last in the longer term? Do you see one of the two as being a transition stage? Do you see everything converging? Do you expect to be offering both for the foreseeable future? And how do you see that divide in the value of one or both sides of it? Let's go backwards. Start with you. We're bullish on validiums and volutions, and especially volusions, because the Ethereums look, we're building for decades.
06:11:34.842 - 06:12:41.358, Speaker A: We are building something that will absorb all of the world's financial transactions. All of the touches, like whatever touches value, those are like millions of transactions per second that happen everywhere. So it will take us time to get there, but we will eventually get there. I have very high confidence on this. So to process all of that, it's invisible that we will do this on a single monolithic system, no matter how big and scalable that is. Even if we go for full sharding with data availability sampling, there will still be more transactions that we need to process. And the question will become like, how do we prioritize those transactions, which of them are super valuable, so that we have to get full 100% security of decentralized Ethereum network behind them, and which ones are purely of computational nature, or some low value transactions that you don't really need to bother to consume the super valuable resources, like some micro payments, some social networks, some games where you care about validity, the enforcement of the rules, but not necessarily of the state? Think of oracles.
06:12:41.358 - 06:13:11.520, Speaker A: Oracle updates are like, it's important to have to verify all the checks, all the signatures, all the rules, maybe do the averages, whatever. But the state is totally discardable. Why would you do it on the roll up? Right? Like you can do it on the volution. It's very simple. You just drop it on the validium part of the volution and you have a read only access from the roll up instantly at a very low cost. Right? So that's our take. And we think volutions are the future.
06:13:11.520 - 06:13:56.170, Speaker A: It's more personal opinion here. The thing is that when you have a roll up, mainly you have the data availability and computational where you store the proof. And the security that you get is the worst out of the two. I mean, if you choose one that you get the data availability in somewhere and you get putting the proofs in Ethereum, the worst of the two is the one and then it's okay, that's the thing. So for me, validiums are temporary solution the best thing if you are using the same layer. If you are using layer one, then you get the warranty that you have the same security. It's good or bad, but you have the theorem security.
06:13:56.170 - 06:14:46.278, Speaker A: That's it out of the game. When you have to choose between two different systems, then which is the worst? For me, what would be easily would be to choose always the layer one. The question is, is Ethereum ready? And here is more a question maybe for Vitalika when full dank sharding will be available. But because I think this is the end game. And the other question is how data availability could scale in Ethereum. Would Ethereum support data availability for all the transactions that Alex was thinking about? Or we need to think about other chains or other solutions for storing data. Just to clarify, in volition, validium and roll up accounts have completely separate security assumptions.
06:14:46.278 - 06:15:31.722, Speaker A: Roll up accounts inherit security from Ethereum, period. Validium accounts can like they have security assumptions. It's not the worst of the two. No, I think Jordy was specifically talking about validiums there. So I think in a world where you have abundant data availability, there's not really a lot of need for validiums and volitions because they're derivatives of validiums. There are certain niche use cases where validiums still can be useful. So for example, if you're building a decks that you want to hide the data for specific trades, then validium makes sense.
06:15:31.722 - 06:16:15.880, Speaker A: But in general, I think once the data becomes data availability, there becomes sufficiently high throughput, then roll ups should be even for the cheapest microtransactions, the cost of publishing data should be negligible, so it won't really matter. And luckily Ethereum is slowly taking steps toward it, with 4844 coming soon. And then after that prototype sharding. And hopefully from there we're just going to keep improving the base layer and the throughput capacity of the base layer in terms of data availability. Roll up. Plasma's back, baby. No.
06:16:15.880 - 06:16:51.586, Speaker A: Yes, it is. Yeah, so I mean, literally today, I think within the past few hours, Latice just announced their op stack mod that we think eventually will land in the superchain. That takes a plasma approach. And what that means is that you can use data availability. That is not l one, but the goal is to minimize the impact that the failure of that data availability layer has. So in a plasma system, you don't have the case that safety fails if the data, the alternative data availability layer fails. You have the case that liveness fails and the chain turns into an indeterminate state.
06:16:51.586 - 06:17:53.174, Speaker A: While there's a dispute resolution game being played on Ethereum, there's a lot of complexities that come with designing applications at massive scale to really take advantage of that. Things like congestion oracles for the l one become more important. But in general, we pretty strongly feel that Alex is right. There's going to be a wide array of data availability solutions, or at least some sort of trade off spectrum for application developers and users to be able to use. I think it remains an open question, given application use cases, what is the minimal impact and the closest to l one security that we can retain? Yeah, for Aztec, we've got kind of this problem on hard mode. Like encrypted data is larger than everyone else's data here, so we have to think about kind of different properties of the data and where that should go. So there's kind of the kind of traditional data that everyone has kind of commitments to our Merkel tree nullifiers, and for the time being, those will stay on l one, either through blobs or call data, depending on which comes first.
06:17:53.174 - 06:18:31.906, Speaker A: Then there's kind of user data, which is like encrypted logs, which lets me find my data. And there the trade off space is a bit different. You can kind of make the claim as a user, if you don't receive that log, you haven't been paid. So that data may not need to go on l one. It could go on a different DA solution. But over time, we'll probably also likely follow Alex and add kind of different DA trees in to kind of have different trees inside Aztec that have worse security assumptions at the trade off of cost. So on our end, Starknet is currently a validity roll up.
06:18:31.906 - 06:19:34.806, Speaker A: Next year we're gonna ship volition, and we're gonna leave it up to smart contract developers to decide which variable they want to store on chain or off chain. The initial data availability layer we're going to use is going to be nodes running starknet nodes. So in a way it's going to be as secure as say l two consensus. We do want to leave it up to developers to use extra data availability layers down the line, but it won't ship immediately in the beginning. I think leaving the opportunity to smart contracts developer to choose where they want to store the data makes it possible to not make that choice at the network level. And then you can just see the market play out in a way kind of like you have in DeFi where people can decide to collateralize loans, for example with native assets or off chain assets. And then you just get to see what people value there will also see the market play out and see what people want to use.
06:19:34.806 - 06:20:47.470, Speaker A: Are they ready to pay more for on chain data or off chain data? As far as validium is concerned, I think it's really not publishing data on chain for validium. It's a really interesting use case for enterprise blockchains or private chains. Okay, so getting to another big question area. So application layer decentralization, right? So as we get from l one to l two, we enter a much more hyper intermediated world in that there's many different kinds of actors that you need to manage relationships between l two s, relationships between users and l two s, applications and all kinds of other areas. So what are some areas there where you're concerned about the centralization or areas where you see a clear path to trying to getting them to be more decentralized that you care about? And what do you think we as an ecosystem can do to improve the question, let me repeat the question, make sure I'm hearing it right. The question is, at the application level, where are our centralization bottlenecks right now? Yeah. Application middleware.
06:20:47.470 - 06:21:29.980, Speaker A: Yeah. I mean, infuria alchemy, name your node provider here is an obvious one. I think we live in a world right now where a lot of folks are not running their own nodes, and that's understandable because it's tough and there are trade offs that come with running your own nodes. So I think running light clients on the client is going to be, on the user side is going to be a really big deal. I think we need to see a lot more of that. I am personally a little surprised that we have not seen more aggressive adoption and development of even things as simple as software where you make an ETH RPC call and the response comes back with a proof that you verify that allows you to verify the outcome of that RPC call. Assuming that you have the hash of the state at the very least.
06:21:29.980 - 06:22:07.378, Speaker A: So I don't know, that's kind of the obvious answer. Like everyone's been mad at the state of RPC provision for a while, but it's definitely a big one. Okay. Yeah, I think RPCs and kind of this notional Aztec of how you find your encrypted data, we're looking at kind of different information retrieval systems. The simplest version of that is give a server your private key or run it in SGX. And that's not a good path we want to go down. So I think researching Pir oblivious message retrieval and trying to work out how users can find their private data is one area I think applications need to kind of look into to make sure things stay private.
06:22:07.378 - 06:23:02.026, Speaker A: Actually, since I hear the project representing privacy on this panel, I wanted to ask another type of data retrieval problem, which is in a privacy system, for a user to be able to spend their coins, the user also needs to hold secrets. So how do you think about custody of those secrets and storing that in a decentralized way that still doesn't weak the information? Yeah, it's a good question. I think it starts with a key system or a key derivation system. That kind of means that you can, from one key, try and redireve all those secrets. So that's one thing we're kind of thinking about right now, looking at kind of prior art of zcash and making sure there's one kind of master key that you can redireve everything from. And then if all the data is on chain, theoretically with that key, it's possible to kind of redrive all your secrets and get back to that point. Do like Normie friendly backups of that master key.
06:23:02.026 - 06:23:21.342, Speaker A: It's a great question. You could maybe shard it. Yeah, snapshot somewhere. But it's an unsolved problem at the moment. Yeah. Regarding decentralization at the application layer, I agree with what was said on rpcs. I think light nodes are definitely something we light clients is something we need.
06:23:21.342 - 06:24:12.218, Speaker A: I think especially in a zig rollup, there's a really interesting design space where you can have strong guarantees on the state at any given time, but you don't really need a lot of data. And I'm really curious to see innovation on what kind of data you retrieve on the state and how you can get a trustless vision of specific criteria of the state, which is not there yet. And then I think another really important thing where we need diversity is bridges, particularly for token ones. It's interesting, I mean liquidity fragmentation is never good. But there's a balance here between liquidity fragmentation and then security. If your only bridge fails then that's bad. I think we should start where the need or the propressing burning need is the highest.
06:24:12.218 - 06:24:57.866, Speaker A: With regard to centralization versus decentralization, I think the bottleneck we have today is in security and specifically security of the DAPps that are hosted on centralized websites. You can hold all of you, all you want, trust the smart contracts of safe that have been verified formally and so on and super battle tested. But then you go to the website safe.com or whatever that is, and your DNS is compromised because someone called the support and said that they represent safe and those guys just believe them and everything is gone, right. So we need to start there. And luckily we have mechanisms to do this. We have ENS, we have decentralized key management for websites.
06:24:57.866 - 06:25:38.380, Speaker A: We have ipfs where you can securely store websites and browsers can verify the hashes of the actual code. We can go one step further. It's a little hard to manage those upgrades of this website. Maybe we can create something like a decentralized registry, like some smart contracts that holds the versions of the website and then we manage them. Similar to how we manage the smart contracts. You announce a new version, then people have time to review it, find bugs, and then only then it goes live. And then we manage somewhere like a contract list of smart contracts that you know, that you're interacting with uniswap and not with some fake stuff.
06:25:38.380 - 06:26:34.890, Speaker A: I think this is the much more burning need than going beyond, because I agree with Ben. It's easier to provide proofs for requests even if they are provided. If they're coming back from centralized providers, you can be still sure that they correspond to correct state of your chain at a given block. So you're saying we need some sort of file system that works even across the blockchain operating system? No, the multi secure decentralized operating system, MS DOS here. That there is one interesting thing in the roll ups. It's a thing that's a little bit different from the layer ones and is that we have already validated the state on chain. So that's a little bit different because for example, when you are trying to synchronize a client, you don't need to start from the beginning.
06:26:34.890 - 06:27:23.118, Speaker A: You can synchronize the state or even the state that you need. You don't even have to have all the state validate with this resonant state on chain. And if you want from there synchronize for the new sequencing or what the new, the new thing that happened there. And I think this is one of the things, for example, that will help a lot. Running your own nodes, specialized for specific applications, or at least being sure, don't having to trust some central party at some point. Maybe you need a central party very much like a light client. I mean, you need a central party, but at least you get the warranty that if they respond, that what they respond is correct.
06:27:23.118 - 06:28:20.030, Speaker A: And I think this can be very helpful for a lot of applications, but we need to see how this evolves. I think I agree with you. Essentially, the validating bridge over roll up is a glorified light client. So we already have that basically baked in, into the design of the roll up. And I think what Alex mentioned is something that is very important, because you can solve smart contract upgradability through governance. You could do certain things, but this is a very big problem, because sometimes when I go to use certain websites, like certain apps, I still feel a bit uneasy whether I'm actually using the app or like somebody called up the DNS, as you mentioned, that basically just is running a scam website that is going to steal all my money. And it's important for us to figure out how to ensure that users don't have to worry about things like that.
06:28:20.030 - 06:29:10.640, Speaker A: Because if I'm sometimes feeling uneasy about using apps, how will an average user who hasn't been using crypto for years is going to feel about that? So I think we need to improve that to then think about other things. And it goes hand in hand with the problem that you don't know what you're signing when it's a complex transaction, because it has thousand parameters and you have to manually parse them and you don't know the function hash codes and so on. So the wallet signing experience is the other second biggest problem to it. Yeah. If you solve the uX, you solve everything, basically. Yeah. By the way, I think for builders in the audience, creating either wallets or interfaces or other kinds of gadgets that give users more information about what the heck they're agreeing to when they click sign as a startup opportunity.
06:29:10.640 - 06:29:34.342, Speaker A: Yeah, it's a thing you can do. I think it's time to maybe go through a couple of questions from the audience. So anyone in the crowd wants to give us a question. Thank you. Yeah, I'm going to steal one of your. Or Abel wants to steal. Abel's going to run the mics around.
06:29:34.342 - 06:30:08.174, Speaker A: Can I maybe steal this one? Thank you. Got some half confident hand raises here you. Hi Vitalik. My question is, is plasma coming back? Aztec is coming back. Yeah. Okay, we'll have a test. I think the really nice thing about having such a diverse team also is that different parts of the ecosystem can focus on all of the different directions in parallel.
06:30:08.174 - 06:31:22.474, Speaker A: Right? So whenever the trolls complain that we're changing narratives, it's like, no, we're not changing narratives, we're adding them. So plasma is coming back and data and dank data availability is going forward even stronger. Yeah, one more question, maybe I can. Okay, here I have a question, since we have talked a lot about the decentralized sequencer and also the L2, but as a developer especially we want to develop something for the network policy. We want to make it sure everybody can join that and have a very low cost to join the activities there in the policy. So which one will have the lowest cost for us to deploy the applications there and for the normal people to use in the applications? Any of you want to take that one? The question is. Yeah, can you repeat the question in 1 second? Sorry.
06:31:22.474 - 06:32:10.682, Speaker A: Okay, I think if I understood the question right, you're asking how can we lower cost for everyone? Because right now it's too damn expensive. Yes, too expensive. And as we know, Vitaligo always referenced a story that in Mexico you bought a coffee there, but you have to pay $5 for the cost. I mean the transaction cost right now the biggest driver for transaction cost currently is the data availability. Yeah. So the big thing we need to do for lowering the cost is lowering that availability or if you want compressing the data, I mean just needing more data, that's the other way to do the things. I think here at least in the roll ups, this is the main fight that we are right now in the roll up site.
06:32:10.682 - 06:32:48.950, Speaker A: Probably it's compressing data in the layer ones is more expanding the capacity of the data supported. And the good news is that things are happening, compression is coming, data availability prototype sharding is coming and talking about maybe a reduction cost of ten x 20 x 50 x I think is something that will happen very soon. I mean we're working for that certainly. Okay, maybe one more question from the audience. Sure, yeah, I think you've had your hand up for a long time. That guy's got a big hand. Yeah, get up there.
06:32:48.950 - 06:33:17.918, Speaker A: Thank you. Yes, thanks. So my question is regarding the. So most of you guys are around stage zero at this point by the way, give a shout out to arbitrarium. They are unfortunately not here, but I saw their ad right before the panel, so they are here in spirit. But they've also done a good job getting to stage one, according to l two beat. So making progress, I think, also across the board.
06:33:17.918 - 06:33:36.478, Speaker A: Keep going. Oh, yeah. Thanks for the remark. Congratulations. So what was the question? You're good. Oh, I thought you cut me off saying that we're out of time. Yeah.
06:33:36.478 - 06:34:25.186, Speaker A: So the question is, what's your, I guess, vision for the next couple of years? Who is going to arrive there first? Well, I know Vitali just made the remark about arbitrum, but between you guys, let's do this just like only five word answer aloud. What's your own expected time frame for getting to stage one and stage two? I didn't hear the question. Five what? Five word answer. What's your expected time frame for getting to stage one or stage two? Um, okay, we're going when the community is. I can answer sure, in one word soon. Okay. As soon as possible.
06:34:25.186 - 06:34:45.206, Speaker A: I mean, just a matter more than five words. Okay. Few months next year, respectively. Okay. I appreciate the definiteness. Let's give this guy a round of applause. And remember that anyone else wants to subject themselves to the accountability of time when community super involved.
06:34:45.206 - 06:35:26.040, Speaker A: Okay. I think when it's ready, I think there's no point trying to be first here. We got to get it right on these systems, so we're going to design it correctly so it can survive the test of time gap from one, two, two, shorter than zero to one. Okay? Yeah. And so maybe just as a last question for everyone, what are some areas that you think all of us here could cooperate on more to try to achieve some of these goals both more quickly and more safely for everyone? Standards. What did you say? Standards. You have four more words.
06:35:26.040 - 06:35:52.990, Speaker A: Yeah. I say specifically, most of these systems have l one to l two. Messaging bridges. They all look different. If we want to be able to kind of bridge across different roll ups, they should at least have the same interface on l one to make that a little bit easier. And maybe the same for account abstraction. If you're building it in the protocol, standardizing how that looks for wallets will make everyone's lives a lot easier.
06:35:52.990 - 06:36:23.290, Speaker A: I'd say outreach and awareness. We're still a very small bubble, and I feel the more we work together to explain to people outside our bubble what it is we work on, the better we do. I will agree with Alex here. I'm agreeing with you too much. Today, standards are the most important, and we picked an unfortunate name for l two specific standards. Is it rip. Yeah, it's a roll up improvement proposal.
06:36:23.290 - 06:37:03.014, Speaker A: I don't think we're doing it well here. We can do better, but, yeah, I think at least the progress here started somewhat. And we've been chatting about a few things, about standardizing a few things, and hopefully in the near future, we're going to adopt at least certain standards for certain things. Because at the moment, if you're building for multiple roll ups, I really respect you. I can't even imagine what it must be like, especially interacting with the bridges. Okay, let me cut you guys off. This has been great.
06:37:03.014 - 06:37:26.400, Speaker A: Let's pause because while I have you, I just want to grab a photo with you guys. If you just put your mics down, come up behind the l two days sign. We just want to get all these legends in a photo here. Put it up on the twitter. Yeah, give it up for them. This is an epic panel. Behind is better.
06:37:26.400 - 06:37:45.214, Speaker A: Get all close and cuddly and snugly. Vitalik, you got to be in the center. I guess that's it. Yeah. This is sick. Oh, yo, come on up here, my guy. All right, everyone on three, stay Vitalik.
06:37:45.214 - 06:38:03.420, Speaker A: One, two, three. Vitalik. Thanks, guys. I have a one of one on opensea for two eth of these guys. If anyone wants in, just hit me up but one more time. Big round of applause for them guys. Thank you.
06:38:03.420 - 06:38:20.762, Speaker A: That was funny. Yeah, don't forget you got your bags. You guys did great. Great panel. Two thumbs up. Matt K. Approved cryptomatk eth.
06:38:20.762 - 06:38:35.766, Speaker A: Send me two eth. I'll give you one of. Yeah, just two eth. Yeah. Bidding starts at one e. Cryptomac eth. Hit me up.
06:38:35.766 - 06:39:07.856, Speaker A: I'll send it over. The mass exodus. I feel so bad for Denison. Denison, where are you at from Tally? My guy's right here. Oh, were you offering me the ethereum? No, please don't go. I'm sorry, man. It's tough.
06:39:07.856 - 06:39:24.170, Speaker A: It's tough. Dude, the same thing happened yesterday. It took, like, 20 minutes for everyone to leave. I haven't had. Your presentation is right there. It's already loaded. Yeah, grab that one.
06:39:24.170 - 06:39:55.744, Speaker A: Let me get you a microphone. You guys are missing out. We got tally right now. Like him. I think I was. This is for main stage. Are you at maybe the side stage? So go back to the front and go upstairs.
06:39:55.744 - 06:40:14.330, Speaker A: It's like, in that corner. Like, upstairs. You'll find it, man. Just ask someone in an l two beach shirt to help you out. You got this. Did you like that open sea joke? That was pretty good. Right.
06:40:14.330 - 06:40:30.560, Speaker A: All right, get out of here, everybody. Yeah, let's kick it off. Denison, the folks that are here, thank you for staying. Give it up for Denison from. Hello. Hello. Thank you for staying.
06:40:30.560 - 06:40:44.790, Speaker A: Following up Vitalik and all that gigabrain is hard. Look at this guy. This guy just walking out. Walking out. That's right. I see you. I remember that face.
06:40:44.790 - 06:41:16.360, Speaker A: Hey, folks, for all the tens of you who've stayed to listen to me chitchat about cross game governance, it's coming for the l two s. Thank you for so much. Love to have you here. You are now, all my true friends. I'm going to airdrop you something special, maybe. Anyway, my name is Denison Bertram. I am the CEO and co founder of a dow tool called Tally.
06:41:16.360 - 06:41:45.988, Speaker A: You might have heard of it. Your dreams comes to haunt you. So my talk today is going to be about cross chain governance. We've talked a lot about l two s and what the future is going to look like, and there are going to be a lot of l two s. So at tally, what we are mostly focused on is thinking about how do we make dows work, how do we make community consensus work. And this is a very challenging task, right, especially at scale. It's hard to do on ethereum layer one.
06:41:45.988 - 06:42:11.088, Speaker A: It's hard to do on L2 s. And now we've got the problem of many, many l two s coming. All right, so first I'll tell you a little bit about Tally. We are the operating layer for dow. So you can kind of think of it like there's this social layer that exists. These are the people who participate in organizations. They are the ones for which we build software and we hope that they use.
06:42:11.088 - 06:43:22.676, Speaker A: Then we have the operating layer, which is where tally sits, the organization layer, which is all these Daos that are doing things, mostly protocols, and then the jurisdiction layer. Right? And so we're going to really talk about, as you see here, sort of like we talk about blockchain, blockchain, private blockchain, geographical blockchain. What we're going to talk about is how do we actually bridge information for dows between these, right? And if you're not sure what geographical blockchain here means, it means l two s where, for whatever reason, the sequencers and validators are geographically located, but they still get the security guarantees of the underlying leg, too. So many of you might be thinking dows are dead along with your precious jpegs, but they're not. Tally helps dows really unleash their potential and their treasuries and what we've seen is that in the past ten months, they have really kicked into action. They are spending a ton of money, they are getting a ton of things done. So in the past ten months, they've spent over $200 million and they're on track on tally to spend about 300 million by the end of the year.
06:43:22.676 - 06:44:15.872, Speaker A: So this is the depths of the market. Maybe there's something to do with the fact that all these folks aren't djens on OpenSea anymore and they're getting something productive done. Daos run every critical function on tally. These are organizations that are doing a lot of things, and it's important to think about all these things that they're doing because they do it in many different places, which is sort of like the topic of this talk, right? So they are doing their formation, they're doing protocol operations, they're doing treasury management, and now more and more they are doing it across all of the L2s. So the challenges are becoming very real for them. This is especially important because the next wave of protocols are all dows and they're all going to be natively crosschain. Right? So what we're looking at is this real explosion of new infrastructure types, right? We have new roll ups.
06:44:15.872 - 06:44:44.304, Speaker A: There are a lot of roll ups coming. We had the leaders up here, but you can imagine there are many more l two s and roll ups. We have staking services, DFI ZK stuff, data availability stuff, and the application stuff. There are a lot of dows coming. Right. All these protocols need to be decentralized. So what we did is we actually did a research report for the optimism collective on what does cross chain governance look like.
06:44:44.304 - 06:45:36.316, Speaker A: Right. Optimism is doing a lot of work on constructing the superchain, but there's a lot of l two s that are building out their own hyperchain orbit chain experiences of many different roll ups built on top of their technology. How do these communicate with one another? So I'm not going to read all of these things, but we basically went through to evaluate three different technologies, message passing, storage proofs and ZK snarks to figure out how are we going to actually communicate between all these roll ups. Right. And I think maybe it's important for me to set the stage a little bit of what the challenge is here on day one. We have one roll up, we have one blockchain, and Alice is delegating her voting power to Bob on this one blockchain. Right? So let's say Alice has 100 tokens, she delegates those hundred tokens to Bob.
06:45:36.316 - 06:46:09.396, Speaker A: Bob now has 100 tokens worth of voting power on this blockchain. Great, simple. Now Alice bridges her tokens to a new blockchain, to blockchain b. Is she still delegating to Bob? Well, the answer is no. Right. The state of that blockchain is confined to that blockchain. So when she bridges her tokens, she's actually simultaneously changing the state of her nature of participation in governance.
06:46:09.396 - 06:46:23.276, Speaker A: So here's a little bit of methodology. These slides will be available. I'm not sure where. I'm just going to say that you can find me, I can get them to. You already talked about message passage for ZK starks. Yeah. So it's a little bit detailed.
06:46:23.276 - 06:46:50.536, Speaker A: So I'm just going to walk through the problem. So Alice has transferred her tokens to a new layer. Now she's no longer delegating these tokens anywhere. She's still delegating technically on the initial blockchain, but she's now delegating zero tokens because the tokens have actually left. Right. So let's say she delegates to Bob again on blockchain B. So now Bob now has 100 tokens in voting power again.
06:46:50.536 - 06:47:28.416, Speaker A: Well, that's great, but what happens if she delegates to someone else? Let's say she delegates to Chad on blockchain B. Right? So now Chad has 100 voting power on blockchain B. Every time Alice moves her tokens between blockchain A and blockchain B, she's also changing who she's delegating her voting power. Right. And this creates a really complicated UX problem. How is Alice going to take care of tracking her delegation across these networks? Right, because we're talking about just two networks. But now let's say we have a third network or a fourth network, right? On the stage.
06:47:28.416 - 06:48:07.856, Speaker A: We had maybe five networks. We know that there's already an entire list of networks. So when we start to think about what governance looks like, we realize we need to start passing state around about the participation in dows between these blockchains. So the first technology we investigated was message passing. This is pretty easy to understand. This is using native bridges, right? So we can use bridges such as wormhole, for example, to pass messages between layers, right? Because these messages can be things like voting. These messages can be things like delegations, but these messages can also be things like the outcome of proposals, right.
06:48:07.856 - 06:48:42.648, Speaker A: People may be voting anywhere, from anywhere, with tokens delegated from anywhere. Message passing is interesting solution. It's pretty reasonable to implement today because we don't have such complexity, but it actually gets very tricky. This problem that I illustrated a little bit earlier was that every time Alice moves her tokens, whether she's putting it into some sort of other program on another blockchain or just moving it herself, she's also changing the state of her delegation, of her voting power in her dao. And this is very messy. Right. This gets very complicated.
06:48:42.648 - 06:49:22.772, Speaker A: And when we start using message passing, what we have to do is we have to go, okay, Alice has to delegate to bob on chain b, but if she's voting on chain a, then she has to have bob vote on chain b and send that message down to the underlying layer, ethereum, pass it on over to blockchain a and send it back up. Right. So when you have maybe one or two blockchains, this is quite doable, but it gets complicated very fast. So the pros and cons of message passing is that the system is simple and practical to implement today. It's also very importantly, easy to understand. We understand how bridges work. They're very comfortable, they're very secure, reasonably.
06:49:22.772 - 06:49:46.880, Speaker A: And so it's easy for us to use these today. There's a lot of ways you can optimize this. A lot of different games that you can play, constructing where people vote and how they do delegation. And the user can always interact in a self sovereign way. The cons, however, are that the system is slow. One of the reasons why it's slow is that the l two s have this finality period that is very different. Right.
06:49:46.880 - 06:50:26.584, Speaker A: Optimism, arbitrary. You have to wait a certain number of days before Alice passes this delegation down from one network to another before it's considered finalized. And this shared finality of state actually starts to become a real issue as we go forward. Because if you think about it for a moment, every l two is actually somewhere different in its lifecycle of finality. When you are on l one ethereum, all smart contracts and programs are interacting with one another in the same place in finality. But once you move to l two s, they are always somewhere different. Someone's a little bit closer to being finalized.
06:50:26.584 - 06:51:11.304, Speaker A: Someone's maybe very far from being finalized, and that makes it very difficult to understand state. So in this case, of course, we just have to wait. And then the other cons are, it becomes impractical as the number of chains go up. So the other thing we investigated was storage proofs. And again, there's a whole paper on this in the optimism foundation forum, or the optimism collective forum, which you can read, but storage proofs was a second solution, storage proofs you might have heard of. It's basically a way of proving state using storage proofs. This is very convenient, because now we don't have to send a message for each action that we take through the layers from l two to l one, over back up to a different l two, right? So it becomes more convenient.
06:51:11.304 - 06:51:50.372, Speaker A: But this has its own drawbacks as well. What we can do is we can aggregate voting power across multiple chains, which is interesting, right? Like in the case of message passing or other sort of like ways of doing it. Basically, you have to go chain Alice or Bob. Whoever has the voter, whoever's the voter is, they have to somehow interact on every chain that they have voting power. They have to vote on every chain that they have voting power and send that message down through a bridge and then back over to the destination and where they're voting. Right. When we get to a thousand chains, obviously nobody's going to click through metamask a thousand times just to make sure they're voting.
06:51:50.372 - 06:52:14.000, Speaker A: Right. Storage proofs actually allow you to aggregate this all off chain. You can go to every chain. You can use a third party service such as tally, perhaps, to aggregate all these storage proofs, and then you can collectively put them all on chain. Right. They're easy to generate, so users aren't locked in third party vendors such as ourselves. And you can add custom voting logic and contracts.
06:52:14.000 - 06:52:46.152, Speaker A: But we have another problem here. What we're doing now is trying to prove state in a lot of different places and sometimes a lot of state. So if we think about Alice again, we might say, okay, let's make life simple. And we'll only do delegation on one network, right? So now we say, okay, Alice has delegated to Bob on network a. We'll assume that she's delegating to Bob on every network. Right. Now what we have to do is we have to have a storage proof that Alice is delegating to Bob on network a.
06:52:46.152 - 06:53:40.508, Speaker A: And then she has to have a storage proof of her token balance on every network that she might be delegating to Bob, right? So the amount of state here starts to get very large as well, right? And today, this is actually fairly expensive at any sort of scale. So it's a really robust method, but it actually doesn't solve our problem when it comes to making good ux and being reasonable in terms of cost. A great example here is Linda, one of the top delegates in the optimism ecosystem. She has 50,000 delegations, right? So you just imagine trying to prove 50,000 storage proofs per chain. Not practical. So the final technology that we looked into, and is the one that we actually recommend for the future, is ZK snarks. Right.
06:53:40.508 - 06:55:42.876, Speaker A: This is really compelling, at least in paper. It's very compelling because of their efficiency, right? So ZK snarks basically roll up all these starks that we do where we prove all this storage proofs, but we can do it all off chain, and we can create one simple, succinct proof that we can put on chain of everyone's voting balances, right? So this is really interesting, because now what we can do is we can have an off chain service, even a decentralized network of ZK provers or proof generators, and they can go through, and they can create all these proofs on all these networks, and then they can aggregate them recursively and give us one little snark at the end that is very cheap to verify on chain. So we can take all the work of storage proofs, we can go through all those networks that Alice is delegating to Bob, we can roll it all up into one nice little snark, right? So the pros of this are, we have this flexibility, accommodating arbitrary computation. This is something that's really interesting with the tech solutions that we have today. Teams like axiom or risk zero, you can actually build arbitrary compute inside these ZK proofs that you generate. So you can even do really amazing things in the future where you can take in data that's off chain and create custom strategies on voting, right? You can do really fantastic things off chain that can be proven on chain at a constant cost, but are basically simpler and easier, right? So not only do we get the advantage of being able to prove your voting power across all these networks, being able to vote on any network with your voting power from every network, it's cost effective, right? And there's a lot of opportunity here for improvements, explorations. Now, the cons here are time variability and proof generation.
06:55:42.876 - 06:56:10.108, Speaker A: There's always a trade off, and today the generation of these proofs is getting faster. But for some delegates, it's not going to be fast enough. If you are tribing 50,000 delegations across 1000 l two s, you're going to have a bad time. That's just how it is. But it's getting better. And there's ways that you can sort of come together with little solutions that can make this a little bit easier. So we'll see that coming.
06:56:10.108 - 06:56:44.288, Speaker A: It's also largely untested, right? What we see when we talk to teams about this technology is that it's new. People don't fully understand it. When we talked about the original solution message passing, it's very easy to really wrap your head around how it works. If you're a dev building a crosschain governance solution, it's pretty easy to read through the docs of a bridge and figure out, okay, these are how the smart contracts work. This is how the message gets passed. That's great physique technology. You end up having to trust it a bit, unless you're really deep in the weeds and how this stuff functions.
06:56:44.288 - 06:57:38.404, Speaker A: But there's actually a con that I'm not listing here that goes across the storage proofs in the ZK snarks. And I mentioned it a little bit with message passion, and that is the shared finality. This is always a problem today, and we're looking forward towards solutions that might make faster finality a more realistic possibility here. Because again, when you do ZK snarks or you do storage proofs, you're actually required to have some sort of state route from these other networks from which you can prove inclusion of this state inside of. And the fact of the matter is, most l two s are not in the same place. Right? So you can't say right now what is the voting power of Alice definitively because her state hasn't yet finalized. So in all these systems, you run into a little bit of trouble where we don't have this shared sense of state finality.
06:57:38.404 - 06:58:09.196, Speaker A: So this is going to be very important in the coming tech upgrades that we see to hopefully get faster state finality. Because no one wants to wait seven days for their vote to get processed. Right. No one wants to wait seven days to just figure out whether or not they're actually participating in a vote with the voting power they expect. There's a little diagram of the trade offs in each of the architectures. So thank you. That's my presentation.
06:58:09.196 - 06:58:40.380, Speaker A: I'm glad you folks stayed. I'm happy to talk about any of this. We are actively researching how we can build better cross chain governance. If you are working on a project that needs crosschain governance or you're building some technology that makes cross chain governance easier, we would love to talk to you. This is something that's very dear to our hearts, we think is very important. We see this growing use case going forward and we're excited to make it happen. Folks, thank you so much for coming to my TED talk.
06:58:40.380 - 06:58:58.124, Speaker A: This has been fantastic for all of you. Stayed dear, dear to my heart. You're funny. That was really good. We have time, actually. Would you like questions? Right here? We have, like, five minutes here. Talk into the mic so we can get on the recording.
06:58:58.124 - 06:59:24.628, Speaker A: Please let me get out of the shop. Can you guys pull up his slides again? Is that what you were pointing at? Yeah. Can we go back to the slides? Yeah, they're coming. They're coming. Look at that one before. You got to scroll through storage, Bruce. Or ZK snark? No, the diagram, the summarization.
06:59:24.628 - 06:59:37.832, Speaker A: Oh, yeah, sure. I just want to take a picture. All right. Are there any other questions? What a bummer. All right, we got a question over there. I can also send it to you. Lots of pdfs, bros.
06:59:37.832 - 07:00:33.906, Speaker A: Lots of pdfs. Thanks, father, for the talk. Amazing energy. Woke me up. The question is, in all this setup that you described, where is the governance contract actually live in the l two? Do you imagine a world where the governance contract live in other l two s or even the l one so that people can actually vote directly? And how does that play out with something like snapshot where the whole voting actually happens off chain? So the question is, where do we see the governor's contracts, these Dow contracts living? The answer is, they could live anywhere, right? I think some people would be very attracted to the idea that you could vote from anywhere, anywhere. And then the outcome of that would be able to be executed and used anywhere. That makes it more complicated.
07:00:33.906 - 07:01:40.506, Speaker A: Right? But it's possible. The problem that you have is the same as what I've just described. You somehow have to share state among all these chains. Now, you could make it simple and say, hey, Alice, if you move your tokens to chain b, that's its own world, and chain b votes as its own sovereign voting block, right? And for some dows, that might be interesting, right? Because you can imagine a world where there's some sort of like l two infrastructure that's shared on l one, and each chain has a vote themselves, and the outcome of their local election gets ported down to l one to manage it. So you could do something like that. For example, in general, though, you still have the state problem. Like, how are you going to share this important piece of state across all of these different networks, which are actually each individually, somewhere very different in their state finality process, right? And how do you make sure that these actions that someone takes on chain a are properly reflected in good time on chain b? Because you could imagine there might be a governance decision that happens on chain a that affects chain b.
07:01:40.506 - 07:02:33.254, Speaker A: But third, parties who are observing the chain know that the outcome of this decision won't make it over there until x time, and they have some sort of advantage to do something in that time. You can imagine, for example, some sort of like zero day hot fix on an exploit being approved on chain a. Well, how do you get it to chain b fast enough, right? So in that kind of situation, if you did have kind of a better or more robust sense of like shared finality, you could use the ZK proof, right? You could put the proof on every l two that you wanted to get this hut fix out at the same time and get this executed at the same time. Right? So you could do interesting things like that, but with the message passing solution, it'd be a little bit harder. Oh, you got another one. Nice. Yes, just as many as Vitalik.
07:02:33.254 - 07:03:24.726, Speaker A: Yes, that's great. Yeah, similarly, I really loved your energy and it was a really great talk. Probably a fairly basic question, but could you give any examples of situations whereby cross chain delegation is actually needed rather than a project kind of just choosing an individual chain and kind of for whatever reason, be it cost, for example, and just saying that all kind of Dow related votes need to be on that particular chain. Yeah, there's a really good reason why you would want this. Let's say that we have a dow on, let's say chain a. Well, let's just say we have a dow on chain a, right? It's a super cool Dow, everyone loves it, it's really awesome. And there's an attacker, an attacker wants to pass a proposal.
07:03:24.726 - 07:04:27.726, Speaker A: They don't even have to be an attacker. They could just be like game theoretic masters, right? So there's something that they want to have happen. They want a proposal to be voted on and for it to pass, right? But they know that 51% of token holders are going to vote no. So what is the easiest way to win this proposal? Move 2% of token holders off that network. Right? So you do the board dolphin drop on chain b and say, hey, you get two board dolphins if you hold this token from chain a. And maybe you can convince 2% of those token holders to move their token over so that they can get these rewards. Right? Now, it's a silly example, but you see this all the time with yield farming, right? People take other projects tokens and lock them up and actually reduce the active available supply in the project.
07:04:27.726 - 07:05:29.678, Speaker A: So you could actually come up with a pretty clever attack where you figure out what percentage of tokens do you need to move to another network because essentially what you're doing, right, in practice, the tokens don't actually leave, right. The tokens don't just get suitcases go off the chain and head over somewhere else. They get locked into bridge smart contracts, right? So you effectively take them out of circulation. So that would be the reason why you'd want this, because otherwise you can have people reduce your token supply. And now this doesn't have to be malicious. This could simply be because for some reason, over time, people start moving their tokens to another network, right? And the problem here that could arise is maybe you don't have enough tokens anymore on the network to reach quorum, right? So now you have a dow failure mode, not because of anything malicious, just because the people who participate in it for some reason had something more interesting to do over there. And let's say now there's some sort of like hack or some sort of problem, and the l two halts.
07:05:29.678 - 07:05:51.740, Speaker A: And to upgrade the l two, you need to run a proposal. But the proposals run on the l two and there aren't enough tokens there anymore anyway to vote. Now you're kind of stuck in a really bad situation. So that's kind of a good example. Give me one more question and I'll have more questions. Look at that. Amazing.
07:05:51.740 - 07:06:28.262, Speaker A: I have more questions than metallic. That's winning. Winning. Yeah, I wanted you to win. And I guess one of the issues with the L2 governance crosschain stuff is that usually when you breach those tokens, they are not useful anymore. You have to bridge them back to Mainet. So unless you do something like locking and having a custom bridge that changed the token supply like you mentioned or something like that, that token on L2 is really hard.
07:06:28.262 - 07:07:25.174, Speaker A: I've been working on some kind of research for that. But yeah, I haven't seen anything usable right now, and I really like your talk because it opens my mind in so many things. Thanks for that. Yeah, I think what you're getting at is when you bridge a token, the token you get out on the other side doesn't necessarily implement all the functionality of the original token. Folks don't really think about this very much, but the ERC 20 token that you may have on one network may not actually be functionally equivalent to the ERC 20 token that you get on the other network. This happens a lot with governance tokens, right? They implement a function specifically called the votes, like ERC 20 votes or ERC 721 votes, and then when they get bridged, they come out as just vanilla ERC 20s. So you couldn't even necessarily use them in governance if you wanted to because they've lost their governance functionality.
07:07:25.174 - 07:07:45.546, Speaker A: So that is also a problem that's also a problem that can be solved with storage proofs or ZK snarks. Sick. Give it up for Denison, guys. Thank you. Yo, you did great. You did great. Are you from Colorado, by chance? No, I'm from New York.
07:07:45.546 - 07:07:56.574, Speaker A: You're a New Yorker. I feel like I was watching south park. You got to hook up to Tally. Okay. Got to hook up to Tally. Tally is a great company. Thanks, folks.
07:07:56.574 - 07:08:05.080, Speaker A: Thanks so much for coming. You did great. Very engaging. Up next, we have Guillaume. Yes, I said it right. Guillaume from ETh foundation. Everyone give it up.
07:08:05.080 - 07:08:22.170, Speaker A: There's a microphone and a little clicker. All right. Oh, laser. Yeah, laser presentation is right there, and you'll be able to click through. Perfect. Thank you. Yeah, someone made a mistake of giving me a laser.
07:08:22.170 - 07:09:07.706, Speaker A: Thanks for coming. Yeah. So there's this thing happening on layer one that's called statelessness or vertical trees. You might have heard of that. And the goal of this presentation is to raise some questions and maybe some warnings that, yeah, maybe if you're an l two developer, more than maybe, actually, if you're an l two developer, if you're deploying your apps on l two, you should start paying attention. So I will start with a very basic description of vertical trees. The concept is fairly complex, but I'll try to keep it short.
07:09:07.706 - 07:09:58.630, Speaker A: Why does l one want to switch to vertical trees? Well, there are basically two reasons. The first one is the state is growing, and we just want a data structure that is going to take less space on disk. And it also allows for smaller proofs, so that the proofs are so small that we can package them in a block or alongside it via a different network. Doesn't really matter how you get them, but the idea is that once you get them along with the block, you can reconstruct a view of the tree. That is, let's say. Yeah, it's actually just enough for you to execute that block. So when you have that, you have, effectively, a self contained execution unit.
07:09:58.630 - 07:10:24.930, Speaker A: You do not need anything. You do not need to store anything on your hard drive. You just get everything from the network. You verify it, you move on. And while we were at it, we tried to make it a bit more ZK friendly than the current interfaces. So those were the design goals. To achieve that, we had to change the structure of the tree.
07:10:24.930 - 07:11:29.762, Speaker A: So there used to be like. Currently there is actually a two level tree, sorry, starting over. Currently in the ethereum state, the data is stored between two trees. There's the account tree, and then you look for your account going through the first, through that tree, you get the root of the account storage state, and then you open the storage state tree and you go and read it, read the slot you're looking for. What we do here is that we just mix in, in order to find the item of an account, we mix in the address of that account and the storage slot number. We mix them together. That gives you a key in the tree, and this is where you will find your account item.
07:11:29.762 - 07:12:02.482, Speaker A: So I said storage slot, but actually we're using something different. We call it an offset. So there's more than the storage slots in there. There's also account items like the nons, the balance. They also have their own offset. So this is where you can find every data pertaining to an account. Another thing that is important to know is that the tree groups leaves by groups of 256.
07:12:02.482 - 07:12:48.938, Speaker A: So there's some locality, and that is going to be important. And another important thing is that we are no longer using regular hashes. We're using something called vector commitments. It's a bit smarter. It lets you prove with some efficiency that one value inside a vector is exactly that value, exactly at this location. So you do not have the problem that you need to worry about the siblings. The proof size only depends on the depth of your node, and as a result you can make the width of the tree much larger.
07:12:48.938 - 07:13:33.386, Speaker A: And so the proof gets that much smaller as a consequence. So the structure of the tree, the tree is divided in three sections. There's the branch nodes. The first section is just branch nodes. So it's pretty much the same thing as what we find in the current miracle Patricia tree, except that there are no potential extension node in the middle. It's only branch nodes. And then the next session, the one in green, is the equivalent of an extension node, except that the extension will systematically be 31 bytes.
07:13:33.386 - 07:14:24.640, Speaker A: So we get the entire key except the last byte. That's called the stem, but it's effectively just an extension. And why do we put the complete stem instead of just the range of values that are equal or that are in common? That's because we want to be able to insert siblings to that subtree without recomputing the commitment. And at the last level you have the values. So like I said, there are 256 of them at most. So they're represented in yellow. And yes, they're split in two for some mathematical reason that I won't get into, but as far as you're concerned for this presentation, think of them as grouped by 256.
07:14:24.640 - 07:14:50.582, Speaker A: Right. So we are changing the hash function. We are changing the way everything is stored. That means we have a lot of data we need to somehow extract from the merco Patricia tree, rehash the keys, reinsert it. That's a lot of data. We estimate that we'll have to hash about a billion keys in a year. It's a very risky change.
07:14:50.582 - 07:15:17.994, Speaker A: It's very resource intensive. So to solve this problem, we came up with the overlay method. And the overlay method is fairly simple. At the core you have two trees. One of them is the old merkel patricia tree that is frozen. And there's a new tree, a fresh tree that starts empty, that is called the overlay tree, and it's a verco tree. And so the merkel Patricia tree is frozen.
07:15:17.994 - 07:15:55.974, Speaker A: So like I said, you can read from it, but you cannot write to it. And so the overlay tree, the vertical tree, is not frozen, it's read and write. It's readable and writable. And the data from the vertical tree clobber the data from the Merkel tree. So if you read the vertical tree first, if you don't find your data there, you go to the merkel tree. And if it's not in that tree either, then it's gone, it's not present. But we do more than just using it as some kind of cache.
07:15:55.974 - 07:16:37.850, Speaker A: We also copy at every block n values from the Merko Patricia tree into the veracle tree, so that when all the values have been copied in the end, the whole data that used to be in the MPT has been either overwritten or copied into the vertical tree. And this way the conversion will be complete. We can delete the remnants of the Merkel Patricia tree. So just as a quick summary, imagine you have two trees, so you have the base tree. Okay. Laser doesn't work. Yes, safety.
07:16:37.850 - 07:17:08.834, Speaker A: They were not completely crazy in giving me that. So you have the base tree on the left and you have the overlay tree on the right. Imagine you have four accounts in that tree and only three of them have been copied. So if you want to read account two that has been copied, you go first to the vertical tree. You find it, everything is fine. If you want to read account four that has not yet been copied in this representation, you go first to the vertical tree. You don't find it.
07:17:08.834 - 07:17:40.062, Speaker A: You go to the miracle tree and you find it there. By the way, I didn't mention. But the order of the accounts has changed because we are using a different hashing function. So yeah, the order will be different and if you write something new it will go directly into the Veracle tree. And yeah, basically that's the principle. There are also changes, and that's probably what's most interesting to this talk. There are EVM changes.
07:17:40.062 - 07:18:27.206, Speaker A: Most of it remains the same. We don't expect to change the instruction set really, but we might add some pre compiles to help developers. And more importantly, the gas model changes because the tree structure changes. So the gas model has to adapt. And I was talking about those groups, I was talking about, sorry, not groups, groups of leaves at the end, at the bottom. Sorry. You have to pay gas every time you go through the green node and also every time you access one single leaf.
07:18:27.206 - 07:19:12.246, Speaker A: But you only pay the first time you access each one of those flanks. Then if you access them over and over again, the cost is very cheap. And the reason for that is because we no longer optimize for the size on disk. We are trying to build something stateless, so this space doesn't really matter as much. But every time something happens, something is red or something is added, the proof of the block, like the proof that is associated with the block, will get larger. And so it's this proof we're trying to. The growth of this proof has to be disincentivized, but once you access it, once the evil has been done, the proof has grown.
07:19:12.246 - 07:19:58.122, Speaker A: Any subsequent read doesn't matter to the size. So that's why the gas model is changing. And that's where the question happens. Arises that currently l two s, at least at the EVM level, are trying to do their best to match the semantics, the behavior of the l one evm. But this change is happening and so the question arises, should this continue? And honestly I don't have a clear answer about this. I'm more interested in starting a conversation here. Is stateless useful to l two s? I don't know.
07:19:58.122 - 07:20:44.614, Speaker A: Currently a lot of sequencers are centralized. Do you need statelessness for that? Not currently, maybe in the future, but yeah, I don't see the urgency at least. There's also a lot of design like experiments happening in l two, especially on the zk side of things. Maybe vertical trees are not the best tool for that. Maybe they are. It might be better in the case of optimistic roll ups because vertical means less like smaller proofs. Smaller proofs are good for optimistic roll ups that do a lot of fault proofs and availability proofs but yeah.
07:20:44.614 - 07:21:13.602, Speaker A: So the decision is still open. What I want to point out is that if you want to, like, you are l two developer like a core dev, and you want to implement vertical trees on your platform, you have to pay attention to a few things. State conversion is a big one. Pre image distribution, l one, no client that I can think of keeps pre images. Actually, that's not true. Arrogant and rest do. But yeah, it's a bit special.
07:21:13.602 - 07:22:08.338, Speaker A: But everyone else ignores pre images. So the question is, how do you distribute those pre images? And of course, the gas model needs to be. And, yeah, like, what I want to say here is that we might be at a point where a split in the design, or at least in the behavior, might be acceptable. It's time to accept that what's good for l one is not necessarily good for l two. Maybe it is, but yeah, we need to discuss that. So, for example, one example of this would be, even if optimistic roll ups want to go for Veracle trees, what they could do is use a different method. We chose the overlay tree method that I explained before, but we looked, before we chose this one, we looked at others.
07:22:08.338 - 07:22:51.246, Speaker A: There was one that was called the bulk conversion. And that might actually be the best choice for optimistic roll ups. And the way it works is that basically you have three clients running side by side. One of them, the one on the left, follows the chain up until a pre agreed conversion height. And at this point, they stop following the chain, and they will start converting their state to vertical. So, do an offline conversion. And the middle client, the middle chain, will keep on following the chain like continue normal operation until the fork block.
07:22:51.246 - 07:23:48.034, Speaker A: And the hope, or at least the goal, is that by the time they reach that fork height, the vertical conversion will be done so that a third client, the one on the right, can replay all those blocks from the middle, but in vertical mode. And do that. Do so before it reaches the four block, at which time the central one stops operating. And the one on the right becomes the canonical chain. It's a method that is a bit risky. So that's why we didn't pick it on mainnet, because it's complicated for sync, it's complicated for reorgs. But if your sequencer is centralized or centralized enough, that might be the better option.
07:23:48.034 - 07:24:22.750, Speaker A: It's the less complicated, for sure. When it comes to ZK roll ups or any snark type roll up, we do have some friendly features. We use Pederson hashes. We use a curve bander wagon that plays nice. We don't use RLP, we don't use SSZ, we use IPA for which you might already have code or circuits for. And we only use a tree. So you don't need to add a lot of columns to your tables.
07:24:22.750 - 07:25:34.662, Speaker A: But we don't really have any circuitry for building the components and honestly I see no way to do the conversion efficiently. So maybe actually there was a third model that we looked at which was to just start from a fresh tree, insert in the tree, and still use the MPT as a backend. So never really complete the conversion. But yeah, from what I see, the conversion is already a long process as it is. If on top of that you want to implement it in circuits, yeah, I don't see how you would do that, but by all means prove me wrong. The other question that has been asked is should we maybe forget about rocket trees and move on to get some l two designs on l one? So the answer is like TLDR. No, the reason for this is because we on the l one cannot afford to change the design all the time.
07:25:34.662 - 07:26:10.814, Speaker A: At some point we need to deliver. We need to deliver something that is secure, so we can't use all those fancy, untested primitives. We can't really change the design. It takes a long time to change the design on l one. So once we set on the design, we need to focus on it, deliver, and maybe move on to a later design in a couple of years. And that's the good thing about this approach with vertical trees, is we learned to do it to change the state format the first time. We learned a lot from that.
07:26:10.814 - 07:26:58.720, Speaker A: We know what mistakes should not be made. So we're going to be prepared in case something in a couple of years, or maybe more like five years, makes sense, makes more sense and we would be ready to make a new transition less painful. Some tweaks are acceptable, but they really need to be justified. But yeah, for the most part that ship I sailed or whatever, the train is gone. And more importantly, we have a mascot now, so there's no way we can change that. That's it. And if you're interested, I have stickers of that mascot, so I'm happy to share them.
07:26:58.720 - 07:27:59.570, Speaker A: One last tool is we're offering because there's a lot of work to do to implement vertical trees. It's a lot of complex mass, there's very little support in solidity. In fact, as far as I know there is none. It would take a long time for developers to have to re implement all the libraries so that bridges can verify virtual proofs and all these kind of things. So what we're working on is a state proof verification pre compile with clearly a focus on vertical proofs. But in fact I already received a proposal to actually already support MPT as well. We intend to make the pre compiled version so that we can support future proofs if they make more sense in the future.
07:27:59.570 - 07:28:32.890, Speaker A: So yeah, we already have something. So that would help. We could share the code that is available in Geth, in Nethermind, in all the clients, so that solidity developers can rely on that to make the switch to vertical more smooth. And I'm saying solidity developer, but that's also l two developers, of course. So yeah, that's pretty much it. One last thing I want to say is that we have built a testnet. It's running in vertical mode.
07:28:32.890 - 07:29:11.062, Speaker A: The Veracle happened at Genesis and it's based on Chappella. So Shanghai plus capella, it puts the proofs in the consensus layer blocks and they can be extracted. So we have this block explorer called Dora that can already display information that is extracted from the blocks. And we're working on a second testnet that will do the conversion not at Genesis, but later. So it's a pre preparation step for the shadow fork. So yeah, that's it. Come and deploy your code on Testnet.
07:29:11.062 - 07:29:35.634, Speaker A: Make sure that you are compatible. Find out what problems are going to affect you, because they will. And yeah, if you have questions, you can always ask me on Twitter or after this talk. Yes, that's it. Thank you. Before you run off, my friend. All right.
07:29:35.634 - 07:29:58.860, Speaker A: Yeah. Question you what? Sorry. Yeah, just one. Yeah, just go. Thank you. Yeah, quick question. You mentioned that the fork from miracle trees to virtual trees is going to be very risky for the main net.
07:29:58.860 - 07:30:44.594, Speaker A: Is the burden going to be more specifically on the clients and on the actual stakers? So the machine of the stakers will have to be able to handle that switch. Yes, no more. Oh, yes, sorry. Yes, we need to transfer the state. We believe that eventually all those stakers will benefit from it because the size of the state will be smaller. They will have more options not to store the state actually, but this state exists and ultimately all the validator, all the nodes on the network are responsible for it. So yeah, it's a shared effort and everybody will need to convert.
07:30:44.594 - 07:31:22.318, Speaker A: Yes. Have you already run some models for the ramp, for example, that will be needed for that? Because there will be a lot of calculation for the state changes. Yes. So we have designed a method to precisely keep it under reasonable boundaries. So I'm trying to remember exactly if it worked with 16 gigs, but with 32 gigs, it definitely works. So, yeah, we tested that on machines from five years ago. We tested that on arm, like raspberry, not raspberry PI.
07:31:22.318 - 07:31:40.522, Speaker A: I keep forgetting the name of that board. Rock five B. These machines are. I mean, the rock five B is really struggling, but it can still do it. Yeah, it's doable. We're making sure that it will not take the network down as it happens. Okay, cool.
07:31:40.522 - 07:32:21.062, Speaker A: Thank you. Anyone else? One more. Thanks. So I understand the whole point of the quiltry is for performance improvements in various ways. Have you quantified any of these? Like, what sort of performance implications are expected from the full conversion to vertical trees after we get rid of the original Merkel tree? Right. I'm sorry if I give the impression that that was going to give a performance improvement. It's going to improve the size of the state, which is the biggest problem.
07:32:21.062 - 07:32:57.026, Speaker A: But execution wise, the crypto primitives are a bit slower. We are currently on par with what execution like main net execution does, but it's not going to improve the performance majorly. No. The interest of Veracle trees is really about offering stateless execution, so not storing the state and being able to sync immediately. So, yeah, okay. In a way, that's an improvement in performance, but in pure cpu performance, that will actually be slightly worse. Interesting.
07:32:57.026 - 07:33:19.324, Speaker A: Thanks for the clarification. Sweet. We'll do one last call. Oh, there's two over here. Hello. Thank you, Gio. It was a great talk.
07:33:19.324 - 07:34:10.628, Speaker A: So, I wanted to ask, during the conversion process, what are things that altos should pay attention to? Or in general, how can the Altus, or the entire ecosystem prepare for the conversion process? Thank you. How can the ecosystem prepare for the convention process? That was your question, right? Yeah. Well, join the testnet, figure out what the problems are going to be. And apart from that, I guess there's no other way. You need to figure it out. There will not be any surprise, or at least there should not be any surprise, as long as you are knowing beforehand that your contracts will be probably broken, or at least they will be more expensive. Upgrade your contracts.
07:34:10.628 - 07:34:41.430, Speaker A: Like figure out how your contracts need to be upgraded, how your bridges need to be upgraded. But that's precisely while we're doing this, while we're building this testnet, we're just saying this is where. Come and check where your infrastructure is going to break so that we can prepare. And I think there was one more over here. Is it? Yeah, back there. Thanks, Abel. Thank you.
07:34:41.430 - 07:35:38.612, Speaker A: I just wanted a little clarification on the slide on the migration. When you show the overlay tree, we are going to read from the vertical tree first and then go to the fallback to the merkel tree. Yes. It's not clear if the value that it's read will get inserted into the vertical tree after that. Right. No, I mean, we could do that, but I don't really see a reason to, because the issue is that every block you need to rehash and the hashing is quite expensive. So if you would insert, that would add, if you just read, a read would turn into a new write.
07:35:38.612 - 07:36:28.548, Speaker A: So there's a bit of a gas model tweak here. And I think, yeah, we would like to keep the gas model as simple as possible. In fact, we would like to keep it identical. So the reason why we don't write back is really because of that, I try to keep the gas model during the transition as similar as what it would be after, so that you don't have to handle two guest models, one during the transition and one after. Okay, that makes sense. Thank you. But then there's another question you mentioned in a previous talk, I think in NCC, the past one, that we will be going to increase the gas price of reading because of that during some time during the migration.
07:36:28.548 - 07:36:45.800, Speaker A: Right. I lied. Okay. No, that was the conception at the time. That was the conception at the time. But we found a way with a database optimization trick to not have that. Wonderful.
07:36:45.800 - 07:37:10.150, Speaker A: Thank you. Anyone else, last call. It's 06:00 p.m. One more. Does he come to the front? Okay, this is officially the last question of the conference. No pressure. Perfect.
07:37:10.150 - 07:37:57.990, Speaker A: Could you explain what that database trick is to enable that gas savings? Yes. Okay, so that's really involved in core development. But sure, I can. I mean, it's not very complex. We have what in guests we call the snapshot, but it's called flat database in others. And this is a simple key value store keyed by the hash of the hash of the address and then the hash of the slot. If you keep this model just for that flat database, because in geth, for example, and also in Besu, although I'm not quite familiar with the internals of BeSU, but from discussions I've had with the Besu core team, for example.
07:37:57.990 - 07:38:27.704, Speaker A: Yeah, you have this flat database and it's separate from the tree itself. So when you read, actually, if you just put the MPT value right next to the vertical tree value, you can read that in one swoop. Which is just a simple database read. So, actually, whether you're reading one tree or two tree from one tree or two trees at the same time, makes no difference, really. I mean, negligible difference. Awesome. Thanks.
07:38:27.704 - 07:38:44.020, Speaker A: Welcome. Thank you. Let's give it up for a successful conference. And thank you, Guillaume. Thank you for closing us out. Strong man. Appreciate your time, dude.
07:38:44.020 - 07:39:11.470, Speaker A: Yeah, I'll take the microphones. They told me that there's an after party. They said that the link is underneath the agenda on the website. I heard Steve Aoki is playing. Don't take my word for it. Just a little birdie. Official after party is at 630 until 1030.
07:39:11.470 - 07:39:34.444, Speaker A: And I just need to make sure that I know who it was sponsored by. It's sponsored by. I know. L two beat Metamask and Linea. So, guys, let's give another round of applause. This has been an epic conference. I am so glad to be your mc for the day.
07:39:34.444 - 07:39:50.760, Speaker A: The few days here. It's been a pleasure. I hope my jokes were fun and keeping you engaged. Get home safe, have fun in Istanbul and get out there. Live your life. Thanks for coming, guys. Yeah.
07:39:50.760 - 07:39:54.090, Speaker A: Cheers, man. Appreciate you coming. That was a really good one.
