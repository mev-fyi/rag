00:00:07.370 - 00:01:12.578, Speaker A: In the last talk, we are going to talk about some performance optimization for the ZKE EVM. So I think, first things, I'm going to describe what happened inside the halo two provers or inside the funkish provers with the Kzag commitment. And I'll then talk about what kind of optimization we did and then what's the performance, performance number we have so far achieved. So, first of all, I think previously also, Yington showed this kind of diagram of what happens inside this halo two or plunkish circuit to do so. First of all, at the top level, you have the circuit arithmetization, where you write the circuit, you define all of your custom gates, the lockup arguments, and then the permutation you're going to do to constrain circuits. And then second, we will convert those circuit arithmetic and then define, it becomes into the, convert them into the polynomials, and it will becomes the relationship between the polynomials or the constraints between the polynomials, quite like the polynomial identities. So you can see in this step, in the second step.
00:01:12.578 - 00:01:53.450, Speaker A: So I just have a lot of equations of all of those polynomials. And those are a bit of constraints that the prover you're trying to constrain, and then generate the proof around that. And then, last thing, what you're going to do is using some of the polynomial commitment scheme to do that. And then in our particular case, you're probably going to commit to your polynomials and then open at a random points that are going to check in the verifier. And what we are using for the polynomial commitment scheme is KZG, the commitment scheme. There could be some other commitment scheme, for example, the IPA used in the original halo two. And then there will be fry as another commitment scheme.
00:01:53.450 - 00:02:50.174, Speaker A: But in our proverb we are using for the Zkevm, the scroll, or like psevm, is that we are using the KDG commitment scheme. Okay, so actually Yunto has already covered a lot about the Plunkett arithmetization. So you have different types of columns, you have advice columns that's going to put any arbitrary wittiness data into those advice columns. And you have fixed columns for some fixed data you can put into that that could be like already preprocessed before in the verification and approving key. And then there are selectors that control all of the custom gates and also the lookups you're going to do. And then there's some instance column inside your circuit which is going to hold any public input data inside that. And there are three major primitives, basic primitives inside the plunker arithmetic.
00:02:50.174 - 00:03:32.346, Speaker A: So one is the custom gate. So you can define any constraints or relationship between all of different cells with rotations, and you can do the lookup argument with a very powerful thing. So you can prove like a column which had all of the value inside of one column. It belonged to some of the table columns that you predefined or you can generate through the device. And then the last thing will be the permutation, which you can copy data from previous rows to the next rows. Those are the primitives you have in the Plunketch semanticization. Okay, so next I'm going to talk about, by the way, when we're building the ZKe event circuits, we mainly use the first two, the custom gates, and then the lookup argument.
00:03:32.346 - 00:04:23.706, Speaker A: So the permutation, we didn't use a lot inside that, because once the EVM circuit has a lot of flexibility inside that, so the permutation need to be fixed per circuit. That's why the permutation is not heavily used. So, I'm just going to talk about more about the custom gates and lockup argument. So, first, the cost of adding a custom gates. So let's take example. You define this t shape custom gate, involve like the three elements in the first row, and then the one more element in the next row, and you can define the constraint is that VA multiply VB, multiply VC minus VB equals to zero. And then that will be then translate into this polynomial identity, which you call like a one, which is defined as like the column.
00:04:23.706 - 00:05:16.510, Speaker A: Like when you commit the column a one, it will become a one x as a polynomial for this column. And then multiply a two, and then multiply a three minus a two omega x equal to zero. And here the omega x basically defines like this is the next row with regard to the previous three rows. And then, because this custom gate may not apply to every rows in these three columns, so that's why you have a selector column. And then when you can set a selector to one at that rows. So that means this custom gate only check the row we have here mark as zero in the red regions, and then that will be also corresponding. Change the polynomial identity here, so that you need to multiply one additional polynomial into the previous polynomial we have.
00:05:16.510 - 00:06:00.846, Speaker A: So that means like this polynomial will only check whenever the selector is set to one at that row. Okay, so then that means in total, in your circuit, if you have dg custom gate arguments there, then you will end up into dg number of those polynomial identities. In your final proof you need to work on in approver. Okay, next thing, talk about the cost of adding a lookup tables a lookup argument. Sorry. So for example, let's say we want to prove all of the value inside the a one column, but belong into the values into the t one column, which is some table column there. For example, see here.
00:06:00.846 - 00:06:47.694, Speaker A: For the a one you only have value 1212, or one two three, like there has arbitrary values and you can have duplicate values. And for this table you only have one two three are all the value values you can have for the a one. So what are you going to do? There are some different versions of this lookup argument, so I'm talking about the one. It's specifically used in the hello two. What are you going to like in the hello two is that you will generate two new columns from the a one and then t one into this a one prime and a t one prime. So you're going to do is like for the a one column, you're going to sort every value inside a one column and then create this new columns. And then for the t one, you're kind of like sorting this column into the new things.
00:06:47.694 - 00:07:48.330, Speaker A: But what you do is you will map this value one into the first appearance value one inside the a one column, and then rest of them will just paddle with zero. And then you will put the value two at the same row as the first value appear in the value two inside the a one prime column, and then so on and so forth. That's how you generate this a one prime and t one prime column. And then second step is that you need to constrain this a one prime column is actually using all of the value inside the original a one column to construct this new a one prime column and same for the t one prime columns. So you need to add this additional multi set check to constrain the a one prime and the t one prime from the original a one and the t one columns. That means like there's two polynomial identities, you need to add in your final proof. And then third things that you see constrain this is finally the constraint, the value that all of the a one value will belong into the value inside the t one prime.
00:07:48.330 - 00:08:40.078, Speaker A: So what you're going to do is you could check all the value inside a one is either equal to your previous row value, or you're equal to the same value in the t one prime column. And then you will have some initial condition check which is like the first value need to be same that's like how you're adding two more extra polynomial identities inside the proof. So to summarize, for the cause of lookup argument. So first I forgot to do that. When you do this multi set check, you are going to construct another column like the Zt column, which is like z column here, I forgot to mention here. So this is going to help you to this kind of helper. So I kind of had way beyond that because there's a lot of math behind that.
00:08:40.078 - 00:09:37.454, Speaker A: But you create a new column to check this running sum check that's going to constrain this permutation check. So that's why in total, if you have Dt lookup arguments inside your circuit, you need to add additional three Dt polynomials or columns you need to construct. So those kind of are virtual columns which doesn't exist in your original circuit. And then you will have additional four DT polynomial identities, those like four constraints you have in your final proof. Okay, so now let's take a look at what happens when you generate the proof like in the halo two plus the KZG provers. So first of all, in the first phase we define there are four phases in total. So the first phase is that you are going to assign, like given the input data you have, you need to assign all of the values into your giant two dimensional matrix.
00:09:37.454 - 00:10:28.126, Speaker A: I would call the witness assignment. And then next things you are going to do is for every column inside your circuit, including all of the advice column, fixed column like instance columns, et cetera, and permutations, you are going to commit them. You first need to convert them into polynomials and then you will do some commitment using whatever commitment scheme you are using inside your approvers. So like we call the commit like fx, a lot of things, those things. And then the second phase is going to do some extra processing for the lookup arguments, which you need to generate. Construct all of the a prime x, t prime x and the ZTx that's going to use to prove constraint, this lookup arguments. And then for all of the additional polynomials you generated, you also need to generate some commitment.
00:10:28.126 - 00:11:40.822, Speaker A: You need auto commit all of these extra additional columns there. And in the third phase what you're going to do is you need to compute the giant quotient polynomials, which you include all of the custom gaze constraints and all of the lookup arguments, constraints and fermentation arguments there. And then create a giant quotient polynomials. And then with some random linear combination of all of the polynomials you have identities you have. And then, so that's like the h prime x, which is the hx divided by the x n minus one, which means that this original X-H-X should be true for all of the value from the one to n you have. And then because this h prime x, the degree of the h prime x is pretty high depending on the highest degree you have for all of your custom gates and the lockup arguments. So you are going to slice this h prime x into certain number of the slices and each slice of the polynomial will have the degree up to n like for each polynomials.
00:11:40.822 - 00:12:45.390, Speaker A: And here, how many slices? Here is the two to the extended k minus k, where this extended k is kind of the, this difference between the extended k and the k is determined by the highest degree for all of the polynomial identities. So in terms like what's the degree example here? This l zero times zx minus one, the degree of this polynomial identity is two, because you have almost like l zero times z. So there's two polynomial multiplied together. So that's kind of defined the degree called the degree of all of the polynomial identities. So here are the differences here is determined by the highest degree of all of the polynomial identities you have. And then you also need to commit all of the sliced hi prime x polynomials here. And then the last things what you do is going to do the openings of all of the polynomial you have at the random points and all of the rotations you have.
00:12:45.390 - 00:13:52.254, Speaker A: The rotating here is meaning you create like at the current row or the next row or the next second rows, those kind of other rotations. And then you will use this multi open protocol at the final to do the pairing check, those kind of things, running some multi open protocols. So these are the four phases you happened in behind of the proverb when you do the proof generation. Any questions so far? No? Okay, so next I'm going to talk about the type of different types of computation you happen inside proof generations. So the first is the wisdom generation, which just only like in the finite field, you do a lot of computations and then filling them into this two dimensional matrixes to do like assignment. And then second, what you're going to do is you need to convert all of the point values between the point values into the polynomial coefficients. That's how you get a column of values and then convert them into a polynomial, which you apply.
00:13:52.254 - 00:14:35.834, Speaker A: The operation you apply here is doing the FFT to do that. And then you do the Ifft to, sorry, you use ifft to convert from point values into the polynomial coefficients and use ffT. Or maybe like I wrong about that so anyway, so you use these two operations to convert between these two things there. And then the next thing is you need to do some polynomial multiplications and divisions. So those are the vector operations, which means if you have convert your polynomial into those point values, then what you need to do is you multiply those point values together. So those are like all the vector operations. And then the fourth is do the polynomial commitment here, because we are using the KDG.
00:14:35.834 - 00:15:20.550, Speaker A: So we are just using the MSM operations on the elliptical curves you have, you choose for the KDG. And the last things I could do is like open polynomial and the random points. And then one more thing is you probably run some multi open protocols there. So those like the open polynomial at the random points is that you're given a scalable value, and then you evaluate your polynomial with all of the coefficients you have inside the polynomial, and you can calculate what value the polynomial evaluates at a certain x you choose, at some random x you choose. So let's see what happens. What kind of computation is involved in each phases. So, in the phase one, which we do like the witness assignment and some pre processed columns.
00:15:20.550 - 00:15:57.458, Speaker A: So that kind of competition you do is witness generations and MSn, FfT and iffT. So MSN, which is you need to use to commit a polynomial and FFT. And IFFT is which you need to generate some of the, you need to convert the original columns into different values and then different coefficients. And the second, like in the lookup arguments, all the combination you do. The first thing is you need to construct these permitted columns, which I described before. And then you also need to do the MSM and IFFT. And the third things for the computer quotient polynomial.
00:15:57.458 - 00:16:43.574, Speaker A: What you need to do a lot of things is to do the polynomial multiplications and divisions, so that you can construct this final quotient, HX polynomials. And they also do some fft here inside phase three, and then the phase four, which evaluate the polynomial at the random points, and then run the multi urban protocol. What you need to do is to do the polynomial evaluations, the multi open protocols, and then some MSN you need to do inside of phase four. Okay, so that's all about all of the proverb, what prover does in behind. So next, I'm going to talk about some optimization for the ZKeVM we did so far. So this is like the previous graph we did. So actually there are two different types of the circuits here.
00:16:43.574 - 00:17:18.082, Speaker A: So one is all of the circuits inside the ZkevM. So those circuits they just generally have more custom gates, more lookup arguments, but fewer rows. And then another type of things is the aggregation circuit which have more rows and then fewer custom gates and then lookups. So take example to compare the EVM circuit we have so far. And then versus the aggregation circuit we wrote. So the EVM circuit we use two to the 18 rows inside our circuit that can handle around about a million gas. You can have to handle that.
00:17:18.082 - 00:17:52.166, Speaker A: The gas is inside ESM gas and then you have 116 columns inside that. But a total like you have almost like 2500 different custom gates and then 50 lookup arguments. And then the highest custom gate degree we have currently have is nine. But there's also adjustable so you can have some config. You can change it to smaller value like five. But currently we just use the config as high custom gate degree is nine. And if you want to handle more gas inside the event circuit, then you need to have more rows.
00:17:52.166 - 00:18:27.560, Speaker A: For example, if you have two to 19 rows then you can probably handle 2 million gas. And in comparison, the aggregating circuit we have is two to 25 rows because we need to aggregate all of different circuits inside one aggregating circuit. That requires do a lot of ECC operations inside the aggregating circuit. Right now we need to have the two two to 25 rows to aggregate all our proofs we have inside the Vke EVM circuit. And then we have 23 columns and one custom gate. Seven lookup arguments and then the highest custom gate degree is five.
00:18:28.650 - 00:18:30.118, Speaker B: Hi, can we have a question?
00:18:30.204 - 00:18:30.550, Speaker A: Yeah.
00:18:30.620 - 00:18:44.960, Speaker B: So you have Brian ask if you have so many custom gates, is there a selector per custom gate? Because it seems like you don't have 2498 columns. Right.
00:18:45.730 - 00:19:09.714, Speaker A: So you have share a lot of selectors across different custom gates. For example, we have one gadget, almost a one gadget for each opcode. But inside each opcode you have different, like you have multiple custom gates corresponding to that gadget. And then one gadget you should usually constrain by one or two selectors to constrain that custom gate gadget.
00:19:09.762 - 00:19:13.778, Speaker B: Yeah, it's like you implemented the opcode switching.
00:19:13.874 - 00:19:14.566, Speaker A: Yes.
00:19:14.748 - 00:19:19.226, Speaker B: Without having to use like 2500 selectors basically.
00:19:19.408 - 00:20:11.414, Speaker A: And then does that columns count also include for lookup arguments? No, this is the original columns you have. So if you want to see for this, you end up like around 300 columns after you counting down all of the additional columns you have and the points you have. Okay, so now let's look at some performance numbers. So if we run this for the event circuit case study look at example of the event circuit. So if you do the proof generation on the cpu using the hello two. So right now we are benchmark on the AWS G five Tarvac large instance, which have 48 cpu cores and then 192gb of cpu ram. And then there's four amid a ten gpus.
00:20:11.414 - 00:21:05.022, Speaker A: So if you're running like everything inside the cpus, that's the amount of the time you need to spend in each phase. And then the total amount of time you have for the proof generations, about like four and a half minutes to generate for the EVM circuit. And then, so the first optimization we did on top of the CPU is trying to move the MSM FFT IFT into the GPU kernels, which involve like for example phase one, you have these things like the. So actually every phases will be involved, some of the things that will be happening inside the GPU instead of inside the cpu. And then now let's take a look at what happens after you apply this GPU kernels, where you can see like the phase one, the most of time is spent is doing the MSM or FFT stuff. And then, so you can reduce that from 70 seconds to down to under 5 seconds. And then, so it counts like all of the things up then.
00:21:05.022 - 00:21:33.638, Speaker A: Now the proving time is about under 2 minutes. So you can achieve like a 2.52.4. X speed up. And then this is using the four gpus. So you can evenly distribute the different ffts into different gpus and then different MSM to different gpus. So that's like using, utilizing all four gpus on this AWS instance. And then, so next things we realize is now the bottleneck, the most time spent is in the phase three, which is the highest time.
00:21:33.638 - 00:22:17.342, Speaker A: What phase three does is not only the FFT, so it also does those polynomial multiplications and divisions. So it also can be very easily to be paralyzed using the gpus. So the second things we optimize we did is to compute those quotient polynomials inside gpus. And then, so now you can see that you can half the time you can spend in the phase three from 33 seconds to 28 seconds, 28.7 seconds. And then the third thing like that. So right now, I think the third thing we did is to, in a phase two, there are certain part of things like construct the permutate columns for all the local arguments that happen, like they're still down in the cpus.
00:22:17.342 - 00:23:01.278, Speaker A: But if you can pile out pipeline the cpu part of the things with the gpu. So when you construct the current when you're doing the MSN or FFT for the new columns you generated for this lookup argument, then at the cpu you can also to construct permitted the new columns for the next lockup argument. So I'm doing like this pipeline stuff to optimize the phase two. And then what you get is like in the optimizing three, you can decrease the phase two from 23 seconds to 5.9 seconds. And so the total time, like now, if we want to generate the proof for the EVM circuit, is like around 58 seconds. So it's below 1 minute.
00:23:01.278 - 00:23:46.944, Speaker A: And if you compare that all the optimizing we apply with the original cpu, once they achieve almost like five x speed up comparing, they're using the cpus. Any questions so far? Okay, so next sector, just take a look at it, for example. Sorry, there's one question. Yeah, sorry. And is this for proving validation one block of transactions? Yes. Okay, so next, do you want to look down? Like they'll look into details, how much time you spend inside MSM and FFT. FFT.
00:23:46.944 - 00:24:49.400, Speaker A: Like if you're in the cpus, you can synchronize, you can take a look. So that's almost like 64% of the total time we'll be spending doing like the MSM FFT iffts, which you can say like, this is some rough number there, but if you want to look at the gpus, right? So the gpus takes the latest optimization you have here. You can find like actually after you moving all of FFT and IFFT MSM into the GPU kernels, now the total time you spend in the GPU for those operations is only like around 16% of the total proof generations. And then the rest of them actually now becomes all of the 58% of time. Although here in the phase three, this part of 25 seconds is doing the polynomial multiplication divisions that also mostly happened inside the gpus. But the rest of them you can see like the. So currently we haven't moved this like a multi open and then polynomial evaluation into the gpus.
00:24:49.400 - 00:25:37.848, Speaker A: So those part of things could be like the next target to further optimize, to reduce the prove regeneration time. Okay. But on the other case for the aggregation circuit, now we also did similar optimization to the aggregation circuit. And then now you can find actually, because the aggregation circuit, the differences between aggregating circuit versus the EVM circuits is that aggregating circuit has more rows and then fewer custom gates. Now you can find that the phase one takes now the most of the time in all of the proof generations, so it takes almost like 2 minutes in the phase one. So what happens, like in the MSM and FFT, it doesn't takes a lot of time. That's like almost like 2 minutes.
00:25:37.848 - 00:26:26.106, Speaker A: What it's doing is doing the witness assignment, because you have a very light circuit, and if you feel them like one by one, row by row. So it takes a lot of time inside the cpus. Actually, that's something that we're continually optimizing and working on to improve that, to see if we find any way to paralyze this proof generation witness assignment inside the cpus to utilize multicores. And then you can see like in the phase three, which does a lot of polynomial multiplication and activations, that's actually less time than the event circuit, because it has less custom gates. So that you have fewer polynomial modifications. You need to do like operating the phase three. Okay, so there's some quick summary.
00:26:26.106 - 00:26:54.886, Speaker A: We have some takeaway we have from all of the optimization and then the comparison things. So the FFT and the MSM is still like dominating time if you're using the cpus. Although currently we're not sure. Definitely, probably we haven't heavily optimized the FFT and MSM in the cpu. So that could be like some hand wavy claim we have, because we haven't really optimized our bills. We spend most of the time just optimizing the GPU kernels. But it could be still the case.
00:26:54.886 - 00:27:41.458, Speaker A: Like FFT and MSM could be dominating if you don't use the GPU kernels. But that won't be the case actually for if you have the gpus, because it only adds up to 16% of total time. And then, so for the large circuits, the wittiness generation is really kind of a bottleneck for the proof generations you have. And then for different type of circuits, you have different characteristics for different circuits. So you need to do apply some different optimization to different circuits. So in the aggregating circuit, so more rows lead to higher cost of large ffps. So you need to have better large FFP kernels in the GPU and EVM circuit actually has lots of custom gates, so that you need to do better polynomial multiplications and probably polynomial evaluations better to do that.
00:27:41.458 - 00:28:23.058, Speaker A: And then, so that you need to tune the performance based on different circuits layout. Okay, so previous x are all about the proof of optimization we did so far, and then we'll continue working on that. So next, I'm just going to talk about how we want to build our decentralized, proven network. Yeah, so this is the kind of the architecture I showed before. So we have this decentralized proven network so that everyone can join and build that. But how we are going to reach to that goal, like the final goal there. So I think right now what we are doing in the stage one is we build a GPU solution for generic proof or the ZK event circuits.
00:28:23.058 - 00:29:13.220, Speaker A: And then we're going to build like. So right now we're using some AWS instances for the proof generations, but we'll also build some private GPU clusters to provide a more stable and then cheaper solutions for building for the proof generation power on our testnet in the stage one. But the next stage we are going to do is we're trying to collaborate with some hardware partners and companies. And then to see how we can share our results so far and then share our proof systems. And our organization did some profile we did. And then to see how we can work with them together to build a better customized hardware accelerators. And I would believe that with better custom provers it can still can improve shorten the proof generation time.
00:29:13.220 - 00:30:14.310, Speaker A: You can see how we can do better paralyzed on the rest of part we currently spend in the cpus and then how to make the MSM and FFT even faster than the gpus. And then at a stage three, our plans to going to definitely open source our GPU provers we have so far. And then probably we'll have some add up, a little bit more optimization with some permissionless license so that everyone can use as some baseline so you can run the GPU server yourself to become our prover, a roller to join our decentralized network. At that time we see if there are some solutions from the hardware companies so that people can also buy those custom hardware from those hardware companies and then to use that as their approvers in parallel. That's pretty much what I want to say about our decentralized proven networks.
