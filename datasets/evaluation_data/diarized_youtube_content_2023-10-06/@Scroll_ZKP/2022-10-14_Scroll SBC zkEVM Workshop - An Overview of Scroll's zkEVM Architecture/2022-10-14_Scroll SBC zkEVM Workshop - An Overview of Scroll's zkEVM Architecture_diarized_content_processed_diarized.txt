00:00:07.580 - 00:00:42.196, Speaker A: Yeah. Hello, everyone. I'm Mason, and I'm going to talk about scroll and the Zkavm that we're building. So, yeah, just so that we're clear on what we're building, there are a couple of different types of Zkavm types of zkavms. And so the top one is like, language level. So this means that you take a higher level language, like solidity or Yule, and you transpile that to a snark friendly vm. And then that's sort of like what you produce, is your knowledge.
00:00:42.196 - 00:01:22.836, Speaker A: Proof of. And so this is what matterlabs and Starkware are doing. And then there's bytecode level zkvms, which is what us, scroll, Hermes and consensus are doing. And so in these, you interpret the EVM bytecode directly. And then you sort of prove the bytecodes are executed correctly, one by one. I guess in our case, we have one gadget that proves each, or usually like, one gadget that proves each EVM opcode. And then in Hermes's case, they, I believe, have, like, micro opcodes.
00:01:22.836 - 00:01:56.984, Speaker A: And a bunch of these micro opcodes will correspond to. Will map to each EVM opcode. But there are still certain differences between this and what happens at a consensus level Zkavm, one of which is like, we can have different. We use different state hash function. So the state routes are different. Then potentially, we discussed earlier the gas pricing might be different. And then the final one is a consensus level zkvms.
00:01:56.984 - 00:02:37.996, Speaker A: And so this one would basically just be a ZK proof that, in fact, that sort of like the state route transition is correct according to the Ethereum l one consensus. And so this is what, on the roadmap for Ethereum was mapped by the Zk snark everything node. We're building a bytecode level evm. Zk evm. All right, so let's talk about what we're actually doing. And so Highchand had a diagram that was sort of like. That showed our workflow.
00:02:37.996 - 00:03:17.500, Speaker A: And just to go over it again, so people have transactions. We run them on our forked Geth node, and this produces an execution trace. So there's the execution logs, block headers, the transactions themselves, the bytecode for all the contracts that got created, used. And then Merkel proofs for all the storage slots that got touched. And then this gets fed into our ZKVM circuit. All the circuits that comprise our ZKVM, those circuits produce a bunch of proofs. All these proofs are then fed to the aggregation circuit.
00:03:17.500 - 00:04:06.424, Speaker A: The aggregation circuit produces a proof that all these proofs are in fact valid. And this aggregation proof gets sent to the l one contract, which does all the stuff that high chin talked about earlier. And then at the same time, the geth node provides some data to the l one contract so that everyone else can figure out what it is. The validity proof is a proof of. Okay, sorry, in my talk, I'm just going to be talking about the stuff that goes into the ZKVM circuits and then the stuff that comes out of it. So the execution traces and then the proofs. Okay, so at the very top of the RZKVM is the EVM circuit.
00:04:06.424 - 00:05:13.456, Speaker A: And so this constrains the operation of the EVM state machine. So what happens here is there's things like the amount of gas that you have left in your execution, the program counter, various things like various call context block context, constraint variables. Let's see what else is there. So, sort of just like a lot of the bookkeeping that you do as the EVM sort of goes like bytecode by level, bytecode by opcode by opcode, like executing the transaction and calls. But then to keep the engineers, I'm one of the engineers at scroll. And so to keep us sane, we break this down into a bunch of other components. And so the first one is, there's a bunch of Zk unfriendly opcodes, like ketchak is one, and then all the logical operations and or exclusive or not, what have you.
00:05:13.456 - 00:06:28.484, Speaker A: And so these are done by lookups? Well, actually, well, I mean, everything else is done by lookups too, but these are also done by lookups. And so in the case of the logical and ors or whatever, these are done by the lookups that Yingtong just described into fixed tables. So rather than write the constraints that show that, hey, a and b is equal to C, we just look it up directly. There's a table that has every single input to, and then the corresponding output. And then, because these are fixed, everyone can just see that, in fact, we have filled out this fixed table. Yeah, I'll talk about how that's actually possible later, because Ketchak, it has the table Yingtong just described earlier, right, where there's a table that has all the pre images in it, along with the catch values. In Yingtong's talk, she called it the state circuit, which is actually what it's called in the code, but I think it's more clear to call it the ram table.
00:06:28.484 - 00:07:30.750, Speaker A: And so this verifies that the reads and writes aren't actually that the reads and writes have been done according to how you'd think they would be done, which is so that if I write a value to some slot and then I read a value from it later, that the value hasn't actually changed. And then, let's see. So then there's a bunch of other lookups. Like there's the bytecode lookup, right? Which basically proves that in fact the opcodes that the EVM claimed it's executing are in fact the opcodes in the contract itself. There's a transaction lookups which verify that the call data in the transaction is the call data in the transactions that we're claiming. And then the block context, which is sort of like has things like the block number, block hash, timestamp, stuff like that. So all these are done by lookups, right? And so this sort of just is like how our circuits sort of talk to each other.
00:07:30.750 - 00:08:26.736, Speaker A: There's more circuits on top of all of these that can make sure that the values in these tables are actually correct for the and. Or like the logical operation table. These are fixed. So we don't need a circuit to prove that these are correct. For the ram table, we have a circuit that shows that in fact the values aren't changing in between reads and writes. And then there's also the MPT circuit, which is showing that for initial reads that the values that were initially reading out of, say, storage slots can actually be found in the state tree. There's various things like the bytecode circuit, and there's like interdependencies between all of these.
00:08:26.736 - 00:09:13.228, Speaker A: Right? The bytecode circuit actually has to look up ketchack values. The catchack circuit proves that every row in the ketchack table actually does correspond to the shaw three. The TX circuit has look up into the ketchack circuit table because you have to verify that, in fact, these transaction hashes are correct. And then additionally, you also have to verify that we have signatures for all these transactions because you can't just sort of stuff it full of transactions that we're making up. And then the transaction signatures are themselves verified by the ECDSA circuit. So this is sort of how everything depends on each other in the ZKVM circuit. So.
00:09:13.228 - 00:09:21.392, Speaker A: Yeah, if you guys have any questions, feel free to ask. All right. Yeah.
00:09:21.526 - 00:09:29.840, Speaker B: So I might not have full context, but the verifier will have access to all the rod.
00:09:32.040 - 00:10:06.576, Speaker A: No. So there's the aggregation circuit at the very end. Right. Actually, the verifier has actually access to basically none of these tables other than this one, because this one is fixed. The other ones are assigned. Well actually, sorry, that's not true. I think some of them are instance columns, like what Ying Chong was saying.
00:10:06.576 - 00:10:33.400, Speaker A: So these are public to the proverb and verifier. And then, so this table is fixed. But then for example, the RaM circuit is an advice. These are all advice columns. So the verifier doesn't actually see them. The verifier only gets a proof that the proverb provides that all the constraints in these different circuits are satisfied.
00:10:34.540 - 00:10:41.770, Speaker B: Would there be some merch to make sure that the byte code that you're hanging lives here actually is.
00:10:42.960 - 00:11:00.560, Speaker A: Yes, I believe there should be some more dotted lines here, I think. Yeah, right. For example, you need an MPT lookup to confirm that these bytecodes that we're claiming are actually in the state tree.
00:11:01.380 - 00:11:04.130, Speaker B: The MPT circuit, I guess, would talk to the.
00:11:04.740 - 00:12:04.464, Speaker A: Yeah, well there should be an MPT table here that consists of MPT proofs for all sorts of facts. And then the bytecode circuit would look up into it every time it's saying like hey, this is the bytecode at this address. Okay, so I guess, yes, that's sort of like the high level picture. And then now I'm just going to talk about sort of like how some of these circuits work. So the EVM circuit, the one at the very top here, basically looks like we have one region for every execution step, I guess. And usually each execution step corresponds to an opcode. And so in this example you can imagine that the opcodes that we are interested in proving, the first one is push.
00:12:04.464 - 00:13:08.536, Speaker A: And then I add whatever is on the stack and then I multiply and so on and so on. To talk about slot I in particular a one through aw are sort of all advice columns that the prover assigns. And then q op basically is a selector column that just says basically it's one however many block, I think. I mean in this case every six rows is one, which just means that basically every six rows I prove the execution of one opcode. And so there's some context variables. So like pc stands for program counter, sp is the stack pointer, gas is how much gas is left, and then there's more. And then there's also these sort of selectors for each of the opcodes themselves.
00:13:08.536 - 00:14:08.200, Speaker A: So there's one for addition, one for multiplication, one for shift, right, and so on and so on. And there's also error states of the, there's also error states of the EVM rights. Like I'm out of gas, I've overflowed the stack, whatever. And then there's these v zero through vn are sort of operating values. And so these have basically context dependent values, I guess context dependent names, depending on which opcode I'm currently proving, they have different interpretations. And so let's see. Okay, but first, one thing that we should talk about is how do you handle 256 bit words? So the EVM has 256 bit words, which is unfortunate because the field prime that we use is slightly smaller than 256 bits.
00:14:08.200 - 00:15:38.468, Speaker A: And so the way we deal with this is, let's say we have like a word a, and then we break a down into its constituent bytes, so there's 32 of them. A is equal, actually equal to this value here, right? So like the first least significant byte plus 256 times the second least significant byte, and so on and so on. And then what we do is actually when we want to assert the equality of two evm words, rather than check that if we have a and b, rather than check that a is equal to b, we can't do that directly. So instead we compute this random linear combination of them. And so this is sort of like the random linear combinations that Yington mentioned earlier between custom gates, where so you generate this data according to some public randomness, and then we compute this sum like a sub RLC, which is instead of using a one times 256, you just have a one times theta, and so on and so on. And then this thing, mod Fp will actually be small enough to fit in the finite field. And so this just means that when we compute our lookups, when we compute our lookups, we just compute the RLC of these values instead of just using the bare values.
00:15:38.468 - 00:16:44.412, Speaker A: And so the technique of breaking up the words into bytes also comes up when we're computing things like logical operations on the words, right? So if you want to compute the bitwise end of a and b, then you can just do this byte byte and look up the bitwise end of a zero and b zero and a one and b one and so on and so on. Let's say we are back to the add opcode. So we also have these opcode selectors. And so the constraints for these selectors are basically like what they're supposed to be is zero and binary selectors, and exactly one of them is one. And the one that's one is the opcode that we are trying to prove. And so the constraints that we have in this case are right. So it's basically the selector times one minus selector is zero.
00:16:44.412 - 00:18:04.730, Speaker A: Which means that each selector is either zero or one, and then the sum of them is one, which means that exactly one of them has to be one. So this sum here can't overflow because there's not enough terms in this sum to overflow. And then, so in the case of addition, we prove it byte by byte in the way that you do addition in base ten on a whiteboard. So this first constraint says that the first byte plus the second byte of the first byte of a plus the first byte of b is equal to the first byte of c plus potentially the carry bit times 256. And then this thing, so this thing gets repeated for all 32 bytes. And then we are missing constraints here that say that in fact the carry bits are zero or one. But then if all these constraints hold, then it will hold that the value represented by the bytes of c is actually equal to a plus b.
00:18:04.730 - 00:19:07.230, Speaker A: So then there's sort of like, sort of like EVM bookkeeping to do. The program counter here has advanced by one because now we've successfully proven the addition opcode. The stack pointer has gone down by one because the addition opcode pops two values off the stack and then pushes their sum onto it. And then the gas has gone down by three because that's the gas cost of the addition opcode. And then there's also more like we have to check that the stack pointer is still less than, still less than 1024 and things like that. Any questions so far? Yeah, question about the random layer foundation. Yeah, is there some way to, where's the randomness coming? Do you need to do an extra step of the trusted setup to get like.
00:19:07.230 - 00:19:42.330, Speaker A: So I think the randomness would be like basically some hash of the public inputs that you pick. But yeah, you have to be careful about it because if like transcripts so far or something. Yeah. Right. But yeah, you can't just insert any value for theta in there. And then there's a lot of carefulness you have to do, right. You have to actually make sure that none of the values that you're randomly combining are themselves random linear combinations because then issues come up there too.
00:19:42.330 - 00:20:43.256, Speaker A: Sorry, you also had a question. Yeah, maybe a naive question, but what's preventing you from picking a bigger prime and avoiding, how do you do this randomly? Accommodation. Sorry, the reason you have to do this randomly. Right? So if the prime are larger than two to the 256, then this is actually like a hardware problem for us, right? Because you have to multiply these larger primes now they don't fit into 256 bits. So now like, yeah, okay, let's see. Okay, so now we, like, in our EVM circuit we have c, which is equal to a plus B. But then all this sort of needs to be communicated with, communicated with the ram table.
00:20:43.256 - 00:21:29.640, Speaker A: And basically what happens here is that when we pop the a from the stack, what actually happens is that the EVM circuit says this row exists in the ram table. And then, so I think this value corresponds to vb, right? So I'm saying that at position 1023, I got VA. At position 1022 I got Vb. And then this push goes back to position 1022. And then. So the EVM circuit, all it's really doing in this case, right, is constraining that VC is equal to VA plus Vb. So which brings me to the ram table.
00:21:29.640 - 00:22:37.484, Speaker A: Or actually, no, it doesn't. So far I've all just been talking about the constraints themselves. So the other part of generating the proof is you have to fill in the witness. You have to actually fill in the witness from the geth traces that we have. There's just a lot of sort of data processing to do here where the geth traces come to us in a JSON form and then we have to process them so that now there are these rust structs and we fill in the context, right? So guess just tells us, here's a program counter, here's a stack pointer, here's the gas, right? But then these things don't have a corresponding gas value, so we just have to compute them ourselves. But this is just like, okay, put one for the opcode that we're executing right now, and then zero for everything else. So then we also have to fill in the intermediate values that were used to prove that, in fact, VA plus vb equals Vc.
00:22:37.484 - 00:23:50.848, Speaker A: So, like, here are all the bytes of VA, here are all the bytes of Vc, here are all the carry bits. Here are all the bytes of Vc. Okay, I think halo two, the main branch right now, only lets you look up from fixed tables, but we've modified it so that you can now look up the lookup argument actually works for into advice columns as well. And so this is very important for us because a lot of our lookups are not fixed, right? Like the RAM table changes every time for every new transaction. So it wouldn't work if they had to be fixed. The, the Ram circuit is the thing that constrains the Ram table. Earlier when we're executing the VM or the EVM, these are random access, right? So you might read from the stack, and then you read from the memory, then you write to the stack, and then you read from the storage, and then so on and so on.
00:23:50.848 - 00:25:07.790, Speaker A: Right. But what we need to prove is that if I write to the stack and then I read from the stack, then that value doesn't change in between. But the order of the reads and writes in the EVM circuit aren't conducive to proving this. The Ram circuit basically has the same contents as the Ram circuit proves this. And the way it proves it is it sorts the reads and writes in a different order. So instead of them being in execution order, in this case it sorts them so that all the stack read writes come first, then all the memories, and then all the storage reason writes, and then within the stack reason writes, we sort them additionally by the address and things like the call id, so that we can verify that within a call id within, we have read write consistency for every particular stack address. Let's see in this case.
00:25:07.790 - 00:26:05.280, Speaker A: So this read write index sort of tells you how the EVM, I guess. Yeah, like how the EVM, in what order the EVM accessed these elements. But then in the Ram circuit, the Ram circuit constraints that these things appear in this sorted order. And then now we can basically write these simple constraints that say if it's a read, then the values don't change. And if it's a write, then the value can be whatever you want it to be. And then additionally you also have to add constraints saying that initial read to a stack should be zero, an initial read to a memory should also be zero, and an initial read to a storage should be like whatever the MPT circuit says it is. And so that's also a constraint that has to show up in the Ram circuit.
00:26:05.280 - 00:26:54.304, Speaker A: Yeah. What's the reason for that? Well, because you want all the reason rights to a particular address to be like one after another. Because it's like what Yinchang was saying earlier. You don't want to have these long distance constraints, right, where you actually can't even express it. In Halo two, you can't say I read it here and then ten rows later, or maybe 100 rows later, depending on what's happening in the EVM, I read from it and then I want, these two values have to be the same. So if they're sorted, then it's actually just like the next one, sorted by.
00:26:54.342 - 00:26:56.930, Speaker B: Address, but also sorted temporarily for each.
00:26:57.460 - 00:28:02.084, Speaker A: Yeah, so the sort order is like the tag, the address, and then the read write index. This is actually a simplified version of it. There's like even more things because there's other things that are effectively like random access, like different call context variables are in this transaction log logs, even things like here are the accounts that I've accessed in this transaction. And so all of these are in the ramp circuit, but they're not in the slide for simplicity. Anything else? Okay, these are just some of the constraints in the Ram circuit, right. So that if it's a read, then the value has to equal the previous value, I e, it can't change. And then we also have to check that these values are valid.
00:28:02.084 - 00:28:40.150, Speaker A: Right, so RW is zero, one. The tag is one of stack memory or storage. So these correspond to just one, two, three. And that in the case of the stack, the address has to be within 1024. And then this is what I was talking about earlier, this constraining the sort order of the keys. So basically first they're sorted by tag, then they're sorted by address, and then they're sorted by read write index. And there's actually a final constraint that you can't repeat a read write index either.
00:28:40.150 - 00:28:54.970, Speaker A: All right, so yeah, just want to thank all the community members who have helped us write this code and are continuing to help us write this code. Are there any questions? Any more questions?
00:28:55.500 - 00:29:05.020, Speaker B: Yeah, could you explain a little bit what the advantages are of going for bike code equivalents rather than the transpilent version?
00:29:07.200 - 00:29:28.916, Speaker A: Well, okay, so I guess one is just like, there are some implementation things that are easier, I guess. What kind of advantages are you interested in? Because there are trade offs, of course. Right.
00:29:29.018 - 00:29:47.050, Speaker B: Yeah, I was just thinking that since presumably as far as portability, people generally have their source code in solidity or whatever, are there implementations that are easier? Are there like nuance that the other version doesn't support?
00:29:47.580 - 00:29:55.740, Speaker A: Um, yeah, I think I'll differ to hyen.
00:29:56.160 - 00:30:33.400, Speaker C: So I think one important is like the remote bomb developers with another set of the toolkit, so they reuse a lot of things like that being reused, a lot of extra plugins for another compiler, and also extra compiler infrastructure could be like adding more security concerns like if there's anything like compiler. So any compiler infrastructure probably need some extra modification. So by keeping the pyco compatibility.
00:30:41.500 - 00:30:42.024, Speaker A: Oh yeah.
00:30:42.062 - 00:30:47.640, Speaker B: Ranka, do you also need the bytecode compatibility?
00:30:53.940 - 00:31:14.290, Speaker A: It, well, yeah, I guess in the case of create, right, you would need to, I think you could work around that, right. But yeah, with bytecode compatible you don't have to worry about it.
