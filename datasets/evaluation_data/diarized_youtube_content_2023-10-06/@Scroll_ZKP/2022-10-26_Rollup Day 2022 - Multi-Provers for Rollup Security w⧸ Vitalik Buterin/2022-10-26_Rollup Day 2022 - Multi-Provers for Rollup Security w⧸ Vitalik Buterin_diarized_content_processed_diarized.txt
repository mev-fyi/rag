00:00:16.730 - 00:01:20.454, Speaker A: Great. Okay, so thank you so much. And today I will be talking about multipruit multi proofs. So start off introducing the problem, right? So today almost all roll ups are still on what I call training wheels, right? There is still some kind of mechanism that can basically override the proof, and we cause whatever outcome it wants inside of the roll up if it decides that the code has a bug in it, right? Like there is some kind of multi sig override governance thing, whatever. And in pretty much every roll up that exists today, with very few examples. So fuel v one is one of them, and I think some of Starkware's products might be another example. But with very few exceptions, everything is on some kind of training wheel, where even though there is some kind of proof system that is theoretically there, the proof system isn't really in charge, right? So there's this page on ltubebeat.com
00:01:20.454 - 00:02:31.594, Speaker A: where if you go to the risk analysis tab, it shows you kind of the status of some of these, right? And for all of these, some of the proof techniques are either in development or they're overridable, right? So if something is upgradable without a delay, that actually means that it's overridable. So basically, every roll up that we have today is not really actually controlled by code. It's still ultimately controlled by some kind of group of humans where you have n of them and m of them can ultimately push through whatever they want, right? So this is a status quo. And the question that I want to ask is, how can we actually move beyond the status quo? What would it really take to move us to a world where roll ups are actually trustless, or trust minimized, and where the fraud proofs and the ZK snarks that extremely smart people have been spending thousands of hours working on actually mean something. So why is almost every roll up using training wheels today? The answer is basically code risk.
00:02:31.642 - 00:02:31.998, Speaker B: Right?
00:02:32.084 - 00:02:54.562, Speaker A: So this is just one random sample from the GitHub repo of the privacy and scaling explorations team's ZKevm. And if you just git clone it, and then you go to the circuits repo, and then you do like find pipe, Zarg, WC minus L and fancy Linux stuff to figure out the total number of lines of code in the whole thing, you get this number, 34469.
00:02:54.616 - 00:02:54.882, Speaker B: Right?
00:02:54.936 - 00:04:48.886, Speaker A: So 34,469 lines of circuit code that is needed, that is in the circuits of the ZKVM. And this doesn't even get into the complexity of the circuit compiler itself. So basically there's like a lot of black box demons hiding inside this entire ZK circuit to polynomial verification, blah blah blah, pipeline, right? And for simple programs, it might be possible to make some kind of proof that is bug free, right? If all you want to do is just prove that something is a polynomial, then, well, okay, fine, you can prove it. You can make kcgs and they're simple enough and it's probably fine, right? So EIP 4844, we can do maybe go a step further and start thinking about the shuffle proof in a single secret leader election. Okay, that looks like the circuits are getting a bit more complex and it's like ten equations instead of one. And it's like maybe on the edge get to proving an entire evm. And it's just crazy, right? So I think 34, 469 lines of code are just not going to be bug free for quite a long time, right? And so the question is, well, if we can't make 34,469 lines of code bug free, then what are we going to do about it, right? Is there a practical near term and a medium term alternative that actually can still get us some degree of trustlessness or a trust minimization? So one simple option, and I think this is an option that a lot of people are gravitating to already, is this idea of a high threshold governance override, right? So basically you have some number of guardians and you have some high threshold on it.
00:04:48.886 - 00:06:08.014, Speaker A: So here you have a six of eight, you can make it be a twelve of 15, you can make it be 42 of 43, you can make it be 70% of token holders, like whatever, right? You have some kind of high threshold override where if it's very clear that some kind of bug has happened, then the guardians can override it and they can say, okay, fine, this thing that got accepted by the proving system is actually an invalid state route, and the governance is going to replace it with some different state route that it decides is valid. But because the threshold is very high, it's very unlikely that it would be able to actually push through something incorrect, right? Because if you want to push through something incorrect, you would have to actually corrupt like 75% of this group of people, right? So this is one approach, right? You basically combine a proving system with a high threshold override, and then you get some level of trust minimization, right? So in order for a state route that is correct, according to the code to pass through, you only need two of three of the eight guardians, to be honest. But then in order for a state route that's incorrect, to pass through, according to the fraud proof, to pass through, you would need six of eight guardians to be dishonest.
00:06:08.062 - 00:06:08.514, Speaker B: Right?
00:06:08.632 - 00:07:18.982, Speaker A: So you have some degree of trust minimization, and you get to the point where the code doesn't have absolute power, but at least the code means something, right? And so the question is like, well, how long is it going to take until the roll ups that are in this room, the roll ups that are in this ecosystem, will get to the point where they're comfortable at least doing this, right? At least getting to the point where they're not like purely run by training wheels, but where they're run by some linear combination of training wheels and actual code. That's an attempt to prove the EVM. So this is option one. But option one, I think, has a couple of weaknesses, right? So one of those weaknesses is that it still does have some vulnerability to governance, right? So the vulnerability to governance is not that high, but you can totally imagine scenarios where you actually do mess up and enough of the governors actually do get corrupted at the same time, and the system does end up actually freezing or doing something really bad.
00:07:19.036 - 00:07:19.350, Speaker B: Right?
00:07:19.420 - 00:07:48.974, Speaker A: So that's one issue. Another issue is that I think in general, these kind of like community governance actors, there's a lot of kind of complexity is involved in choosing them. There's the social question of which ones different groups of people will trust, the legal question of who is going to be willing to be a governor and what even their risk and responsibilities are. Just like a whole bunch of issues that actually come up when you try to actually create a set of guardians.
00:07:49.022 - 00:07:49.378, Speaker B: Right?
00:07:49.464 - 00:09:00.406, Speaker A: So I think option one, if you have to do it, then I think it's a good idea to do it. And I think it's definitely an improvement over the status quo, which is basically where you have a governance committee that generally can just override the prover and make it lead to whatever result it wants. But ideally, it would be nice if we could have something other than this, right? So option two, multi prover, right? So the idea here basically is that instead of having a multi sig of people, you have a multi sig of different proving systems. So the philosophy behind this should be pretty simple. The Ethereum network to some extent does something similar, right? Because we have multiple implementations of the Ethereum protocol. And so right now, I think both prism and Lighthouse have somewhere around a third of all the validators. And so if either prism or lighthouse have a bug but the other clients don't, then the worst case is that the chain stops finalizing for a few hours, and then it comes back to normal.
00:09:00.406 - 00:09:04.794, Speaker A: And the average case might even be that the chain just actually keeps going and ignores them.
00:09:04.832 - 00:09:05.322, Speaker B: Right?
00:09:05.456 - 00:10:19.698, Speaker A: So multiple implementations basically allow for a much more resilient network, because if one implementation has a bug in one place, then chances are another implementation will not have a bug in the exact same place, especially if that other implementation is created by a different team that has a different philosophy and even just a fundamentally different architecture strategy. So, in this diagram here, the judge Hammer and the scales of justice are meant to represent fraud proofs and arbitration, as some of you maybe might have already guessed. And the spooky looking chip there, that stable diffusion generated for me two days ago, is supposed to represent a zero knowledge proving system. So doing a multi between a fraud proof and a ZK roll up is actually a really powerful idea, because fraudproof based systems and ZKVMs are just designed so fundamentally differently, they rely on such fundamentally different assumptions that basically the level of correlation between them is going to be very low.
00:10:19.784 - 00:10:20.420, Speaker B: Right.
00:10:21.110 - 00:11:54.906, Speaker A: Basically, the only way in which you might have some kind of correlation is if either there's some kind of bug or ambiguity in the yellow paper in a particular place, or the same people are involved, or there's some really clever attack against both of them. But the bar to get there is very high, right? You're not just going to get the exact same kind of bug in a fraud prover and a Zkevm by accident. And then, even within fraud provers, there's, like, a bunch of different approaches, right? So one approach, for example, is that you make a fresh new, like, basically a new implementation of the EVM that's designed around proving one specific computational step. Another approach is you compile the guess source code into some minimal virtual machine, like mips, for example, and you then make a fraud prover for mips, and you just push the entire guest code through that, and you just create a fraud prover of mips and have everything go that way, right? But instead of guess, maybe you could stick Aragon in there, or maybe you could stick another mind. Or maybe instead of mips, you could use some different machine, right? So there's a lot of different ways to make a fraud prover. There's also different ways to make a Zkavm, right? So the PSe Zkavm is kind of a direct compilation. The Polygon Hermes team, I believe, is doing this clever thing where they first compile the EVM to an intermediate language, and then proving that intermediate language only takes, like, I think about 7000 lines of code instead of 37,000 or maybe.
00:11:54.906 - 00:11:59.602, Speaker A: Sorry, it's like the representation of the EVM in their assembly is what takes 7000 lines of code.
00:11:59.656 - 00:11:59.874, Speaker B: Right.
00:11:59.912 - 00:13:02.386, Speaker A: So there's different approaches and then there's the ZK sync strategy of just going straight solidity. So there are different ways to architect these systems. And if you have three different proving systems that are architected very differently, then you might have a lot of redundancy. So this is another approach. Option two, b more complex variants of the multiprover strategy, right? So one idea is kind of self canceling, right? So if someone submits two conflicting state routes to one particular prover and both state routes pass, then that prover gets turned off, right? So the idea here is basically that if some prover is able to accept multiple state routes, then clearly something is wrong, right, because it's saying yes to two conflicting outputs. And so you shut it off and either you reduce the size of the multisig or you replace it with governance that has to choose a different prover. So that's one approach.
00:13:02.386 - 00:14:59.050, Speaker A: Another approach is if no successful message gets passed through a particular prover for seven days, that prover is turned off, right? So if the prover is deadwalked, if the prover ends being unable to accept even things that are valid, then you can shut it off too. So one of the interesting things about these two ideas is that they're actually kind of inspired by smart contract wallet designs, right? So the concept of self canceling, that's a very close parallel to this concept of vaults that I think Emian, guncier and a couple of other people were really promoting a few years ago, right? Where basically you have a smart contract wallet where you can initiate a withdrawal, but then that withdrawal takes 24 hours to finish. And before those 24 hours come, that exact same key can cancel the withdrawal, right? So the idea basically is that if you get hacked but you still have your key, then you can constantly prevent the hacker from actually taking the money. And then there would be some third override key where if one key clearly keeps canceling itself, then that third key with maybe a one week delay can actually take funds out, right? So basically this approach is like that exact same idea, except for a personal, instead of being applied to a personal wallet using private keys, it's applied to a roll up using multiple provers. And then the second idea, basically it's like social recovery for provers, right? If approver is clearly not able to do something, then some other mechanism can switch it, right? So you can actually do some surprisingly interesting and clever stuff here. So this gets us to a third technique, right? So this is a two prover plus governance tiebreak. So we're going to make two provers and we're going to make them very different.
00:14:59.050 - 00:15:46.858, Speaker A: So one of them is going to be ZK and one of them is going to be optimistic. And we have a two of three mechanism, right? Basically. So we have a two of three between the ZK, the optimistic and governance. So there's actually a bunch of different ways to architect this, right? So one of them is the thing that I mentioned on Twitter about a month or two ago. Basically the idea being that when you submit a block, there is a 24 hours time window. And after that 24 hours time window, the block gets accepted if there is a snark. And within that 24 hours time window, if someone opens up a fraud proof, then the fraud proofing game and the snarking game, they both run.
00:15:46.858 - 00:16:38.858, Speaker A: And then if they agree, then whatever they say is accepted as the result. And then only if they disagree, the governance has to come in and provide the correct answer, right? So if you want to take an approach that minimizes the governance's role and makes the governance be a more emergency thing, then you'd probably want to do that, right? You'd want to create a system where by default you want to have a kind of two of two between the optimistic and the fraud. Between the ZK and the fraud. And the way you do this is by having a time window. And because you have a two of three, you can be more aggressive. Instead of seven days, you can make it 24 hours. And you can say for a block to be accepted, it has to both have a snark and have a 24 hours window pass to make sure fraud proofs didn't come in, right? And then if they disagree, then you do some governance thing.
00:16:38.858 - 00:16:58.462, Speaker A: But if you are okay with governance being kind of more regularly active, then there is another approach, which is for every state route that gets submitted, you just let all three of these games run. And then as soon as two of these games accept a block, then that block gets accepted.
00:16:58.526 - 00:16:58.946, Speaker B: Right?
00:16:59.048 - 00:17:59.246, Speaker A: And so in the happy case, blocks would actually succeed basically immediately, right? Because when you have a block, then a ZK proof passes for that block and the governance accepts a block and you have a proof, and then the block is finalized within less than an hour. Basically your only limit is going to be how quickly you can make ZK snarks. Right now it looks like ZK proving EVM blocks is like somewhere in the hours. But in the future I have a lot of faith in you guys. The technology will improve and we're going to get ZK snarks in 12 seconds, right? Right. Okay, so there's different options, right, is basically what I'm saying. There's like this large design space of different options that you can choose, different trade offs that you can take different depending on how much you value speed versus how much you value minimizing the role of the governance versus a bunch of other considerations.
00:17:59.246 - 00:18:43.742, Speaker A: There's a lot of options that you can make and it's probably worth thinking really deeply through these different options and figuring out which one of these actually makes sense, right, so advantages of this approach, right. Is that it actually combines all of the advantages together, right. So for this approach you don't have to trust the governance because even if the governance is completely evil, even if seven of seven get corrupted, it can't contradict the provers. If the provers agree and you're protected from a bug in either one of the two provers, and ideally if the provers have a very different construction, then the chance of the two having simultaneous bugs is going to be very tiny.
00:18:43.806 - 00:18:44.130, Speaker B: Right.
00:18:44.200 - 00:19:08.458, Speaker A: So that's the advantage of this kind of design. One other interesting thing that's worth talking about here is what does the code look like for the multi aggregator? Right, because the goal of this is to try to minimize the number of lines of code that you have to certify. Yes, this is definitely bug free and if it's not bug free, people are going to lose eleven and a half billion dollars.
00:19:08.544 - 00:19:09.034, Speaker B: Right.
00:19:09.152 - 00:19:26.338, Speaker A: So you don't want that to be 34,416 lines of code, but maybe it has to be 100 lines of code, maybe it has to be 200. You want to minimize that as much as possible, formally prove that as much as possible coordinate on using the same code as much as possible.
00:19:26.424 - 00:19:26.722, Speaker B: Right.
00:19:26.776 - 00:19:31.906, Speaker A: So the other question is how do we actually minimize the multi aggregators themselves?
00:19:32.008 - 00:19:32.754, Speaker B: Right.
00:19:32.952 - 00:20:04.110, Speaker A: There's a lot of different possibilities. One interesting one is that you just literally use agnosis safe wallet, right? Like you just literally throw coins into a gnosis safe wallet where you have three different keys that are owners. It's just a plain old two of three agnosis safe and the three wallets just are. One of them is itself agnosis safe of four or seven guardians. Another is an account that pushes through a message if a snark tells it to push through a message and a third one pushes through a message if it tells a fraud groover to push through a message.
00:20:04.180 - 00:20:04.606, Speaker B: Right.
00:20:04.708 - 00:20:19.922, Speaker A: So if you do that, then you can even reuse existing code for the thing that actually does the aggregating, which I think is really cool and I think really does reduce the surface area of code that you have to trust unconditionally by quite a bit.
00:20:19.976 - 00:20:20.386, Speaker B: Right.
00:20:20.488 - 00:20:24.450, Speaker A: But these are all also things that are worth thinking about.
00:20:24.520 - 00:20:25.138, Speaker B: Right?
00:20:25.304 - 00:20:34.422, Speaker A: So conclusions here. So I think the big one is that Zke evms are not going to be bug free for a long time.
00:20:34.476 - 00:20:34.934, Speaker B: Right.
00:20:35.052 - 00:20:40.034, Speaker A: And this is something that's probably worth internalizing and accepting.
00:20:40.082 - 00:20:40.342, Speaker B: Right.
00:20:40.396 - 00:20:53.242, Speaker A: Basically, what's amazing about the Zka space is that I think it's the one part of the crypto space that actually has exceeded expectations in how fast things are going to come.
00:20:53.296 - 00:20:53.754, Speaker B: Right.
00:20:53.872 - 00:21:07.762, Speaker A: You got like the merge and it's like, oh, it's going to come in 2015, oh, 2017, and, oops, it came in 2022, but then we got zkevms and it's like, oh, they're going to come in 2030, maybe 2027, and then, oops, we have prototypes in 2022.
00:21:07.816 - 00:21:08.226, Speaker B: Right.
00:21:08.328 - 00:21:47.246, Speaker A: So that's like the good news about ZKe evms, but the bad news about zkevms is that I think we're going to have this long period of time during which they exist, but they're like fairly untested. We don't know if there's bugs in them. There might be bugs in some scary proof systems, there might be bugs in circuit compilers, there might be bugs in the ZKVM code itself. And so for some number of period of time, which I think will easily last quite a few years, we are going to have these kind of circuits that we trust to a high degree, but we don't trust completely.
00:21:47.348 - 00:21:47.902, Speaker B: Right.
00:21:48.036 - 00:21:52.494, Speaker A: And the fact that we trust them to a high degree means that we should use them.
00:21:52.532 - 00:21:53.070, Speaker B: Right.
00:21:53.220 - 00:22:25.542, Speaker A: And it means that we should actually get the benefit from them and not just create systems where we're giving lots of power to a multisig. But the fact that they're not perfect means that we also have to compensate for the possibility that something about them actually breaks. So with multiple implementations, with governance backups, with multiple implementations and governance backups, we can minimize the chance that bugs are going to actually lead to catastrophic outcomes. There is a trade off space between security versus bugs and security versus bad governance.
00:22:25.606 - 00:22:25.930, Speaker B: Right.
00:22:26.000 - 00:23:17.222, Speaker A: And I think this year everyone's optimizing for security against bugs, which is probably correct. Ten years from now, I think everyone should be optimizing for security against bad governance, which is probably going to be correct then and between now and ten years from now, we should kind of slowly move that slider from kind of trusting governance more to trusting the code more as the code becomes more trustworthy. So I think keeping governance involved is a good idea, but it's also a good idea to keep it only involved in emergencies. Now, this is intended to be about kind of a talk about L2s, but I think one other interesting thing that's worth mentioning here is that there is also a layer one angle to this concept of ZK multi proving.
00:23:17.286 - 00:23:17.658, Speaker B: Right?
00:23:17.744 - 00:23:21.982, Speaker A: And the issue here is we want to use ZK vms on layer one.
00:23:22.036 - 00:23:22.640, Speaker B: Right.
00:23:23.730 - 00:23:48.962, Speaker A: My vision for the future of running an Ethereum node is basically that you should not need to have a piece of fancy hardware. You got your phone. You should be able to have a full Ethereum node staking a million dollars of eth if you want to, running on a phone. And how do you validate an Ethereum block? You get an Ethereum block. That Ethereum block might contain three and a half megabytes of data. Okay, fine. And the snark?
00:23:49.026 - 00:23:49.398, Speaker B: Right.
00:23:49.484 - 00:24:12.874, Speaker A: Download the three and a half megabytes of data, hash it, stick the hash into a public parameter, verify the snark. Done. Block is correct. It would be lovely if we could get to the point where verifying Ethereum blocks and running an Ethereum full node is as kind of simple and low resource and decentralization friendly as that.
00:24:12.912 - 00:24:13.354, Speaker B: Right?
00:24:13.472 - 00:24:42.690, Speaker A: But in order to get to that future, we need to have a snark that can verify everything. Right? We need to have a Zk EVM and a Zk Ethereum consensus layer, and probably recursive ZkZK and like ZK everything. And we're basically going to have to trust everything to a bunch of polynomial maths some weirders in universities invented. Okay, not everyone's in university. Dropouts contributed a lot to the ZK space. Three cheers for dropouts are contributed to polynomials.
00:24:42.770 - 00:24:43.510, Speaker B: Yay.
00:24:45.370 - 00:25:51.126, Speaker A: But if we want to actually get there on layer one, then we're also going to go through this period of time where we can't trust one implementation to be infallible. And so then the question is, well, what would a multiple implementations vision for ZK snarks at layer one actually look like? And I think here there's some interesting answers, right? So one possibility for this is that, okay, we have different clients. We have prism and guests and lighthouse. We have five execution clients. We have five consensus clients. And maybe we're going to also have five ZkeVM engines, right? And so instead of there being 25 client combinations, we're going to go up to 125 client combinations, which makes Ethereum five times more diverse and secure. So then the question is like, okay, somebody creates a block, and then the peer to peer network is just going to generate a proof for a block of each type, right? And like, okay, maybe you have a node running ZkeVM engine a that created a block.
00:25:51.126 - 00:27:08.680, Speaker A: But then because the data is all in the clear, once it gets out there, someone else can come along and they can make a Zksnark that is compatible with ZkavM engine B. And then people running engine b on their clients are going to be able to verify it. And so you are going to be able to basically get the same benefits of client diversity. But in this ZkVM world, right? Then the question is like, well, how do we actually get there? What is the first part of the Ethereum consensus that we actually are willing to zK? Is it actually going to be possible to ZK proof things quickly enough? There's a lot of these different issues, but I think realistically some kind of multi proofing future like that, and possibly even a hybrid future, right? Possibly even, we might even see a future where, for example, all the institutional stakers still run regular phone odes, but some home stakers run a couple of experimental ZK provers. We might end up going through a couple of those phases. But I do expect that some kind of multi proving and hybrid proving system is going to be the future on layer one as well, and not just L2. So thank you.
00:27:08.680 - 00:27:23.940, Speaker A: Oh no. Did I miss a slide? No, there's like a. There's a happy giraffe there. Okay. Yeah, there's a happy.
