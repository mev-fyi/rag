00:02:27.880 - 00:46:37.732, Speaker A: Me we've been together so long the story moved on but we denied the love we felt that risky side sentiment oh, what a man oh baby, come back to me please dummy, can't you see that I'm addicted to you that I'm addicted to you oh baby, come back to me please darling can't you see that I'm addicted to you that I'm addicted to you damn. Together so long the story moved but we denied the lovely thing that risky side we're sentiment baby, come back to me please darling can't you see that I'm addicted to you? That I'm addicted to you oh baby, come back to me please darling can't you see that I'm addicted to you that I'm addicted to you come back to oh baby, come back to me please dummy, come that I'm addicted to you that I'm addicted to you oh baby, come back to me please darling can't you see that I'm addicted to you that I'm addicted to you I can't remember when it's time to go when I look in the mirror tracing lines with a pencil I remember what came before I wanted to think there was endless love until I saw the light dim in your eyes in the dead night I found out sometimes there's love that won't alive New York City such a beautiful disease New York City such a beautiful such a beautiful get better when you just say goodbye I lay awake one more night got it vision I want to deny New York City such a beautiful disease beautiful New York City such a beautiful such a beautiful day it talked about no regret as it slipped from my hand to the scuffed tile floor I rode a train for hours on end I watched the people pass me by it could be that it I can stand it no more no I can stand it no more, no because I can stand it no more no I can stand it no more, no test, test, check. Cool. Testecular test, check. Good. I can't stand it no more no I got. Hello, everyone.
00:46:37.732 - 00:47:15.452, Speaker A: Yeah. So, ladies, gentlemen, cats, esteemed guests from all around the globe, welcome to l two days, the premier event dedicated to ethereum layer two scaling solutions. I'm thank you. Thank you. I'm Piotr from l two beat, and I'm thrilled to be here alongside my co host, tog rule from scroll. Together, we represent the joint forces behind this conference. Thank you, Piotr.
00:47:15.452 - 00:47:44.840, Speaker A: It's absolutely thrilling to be here. I think, as all of you, I'm excited to be at DevConnect. And let's start with a bit of a history lesson. I think this was a bit of an amalgamation of the ideas that both l two beat and scroll has worked hard on for the last couple of years. So it all started with l two Amsterdam last year at DevConnect. At the first devconnect, I'm not sure if you guys been there. It was fantastic.
00:47:44.840 - 00:48:20.580, Speaker A: And then we also organized roll up day at DeFCON in Bogota. And at some point we realized that we both wanted to do a similar event here. So we were like, let's just join forces and do it together. So that's how l two days was born. Yeah. And over the next two days, we have prepared for you a rich agenda filled with insightful talks, spicy panels and interactive workshops. We'll explore a range of diverse topics, and it's all centered around layer twos.
00:48:20.580 - 00:48:40.852, Speaker A: At the conference, we'll have three stages. So this stage is the main stage. It's for everyone, basically. It doesn't matter what level of knowledge you have. A lot of the talks are going to be introductory level. We're going to have a lot of panels that are discussing more general things. And then there's a side stage on another floor.
00:48:40.852 - 00:49:02.048, Speaker A: You can follow the sign edge around the building that is going to be more technical. And we're going to have a lot of technical panels and talks there. Yeah. And alongside our two stages, we also have a third stage. It's downstairs. And we're going to host some workshops for governance topics there. It's actually an all day thing.
00:49:02.048 - 00:49:35.832, Speaker A: You can join and go as you please. And after, sorry. Alongside all of the programming, we are also going to have two breaks, one in the middle of the day for lunch and one later in the day for coffee. And of course, this event would have not been possible without our generous sponsors. Here we have API, three arbitram, linear and polychain. So let me invite Nina from Arbitrum on stage. She's going to give a short keynote on Arbitrum.
00:49:35.832 - 00:50:08.710, Speaker A: Thank you. Hello, everyone. Hi. Thank you for coming. And Nina here from Arbitron foundation. And again, I'd like to welcome everybody here to join us today. And it's such a pleasure to see so many researchers, developers, engineers, builders joining us today or on its way of joining us today.
00:50:08.710 - 00:51:24.536, Speaker A: Arbitrom is currently the largest layer two ecosystem, and it's so excited that we have seen such a wide adoption over the past two years, since we launched in 2022. And what's more importantly, in 2023 we see so many different options of l two s, l three s, data availability layers. We have the dks, the Ops, and we're living in a time where all the most brilliant engineers and researchers are joining hands to solve one of the most pressing and critical issues of the Ethereum community and the blockchain technology Arbitrum project started as an academic project back in 2014 and we launched officially in 2020. It took us a long time to build the technology and we have been with the scaling issues for nine years. We have made friends along the way and we are look forward to extend our research, extend our technology for the next ten years of scalability and the blockchain and scaling solution. And again, I'd like to welcome everybody and hope you enjoy the event. Let's talk scalability.
00:51:24.536 - 00:51:37.940, Speaker A: Let's talk layer twos. Thank you very much. Thank you very much. Yeah. Thank you. And now to our next sponsor, API three. Welcome on stage.
00:51:37.940 - 00:52:02.620, Speaker A: It's the wrong video. Hey, guys. Ugur from API three. Even though it's the wrong video, I'm just going to say we're happy to be here. Back in the day, when it came to oracles only serving, there was a time where oracles were only serving main net. That's all everybody cared about. But that has obviously changed.
00:52:02.620 - 00:52:35.540, Speaker A: We essentially want to bring oracle services everywhere in a verifiable and decentralized way, straight from the source. We're very happy to be here with all of the layer twos building and all of the great, amazing people that we have here that we're working together with to scale Ethereum. Thank you very much. Thank you to API three. And next on our list is Shuyao from Linea. Please welcome her. Cool hat.
00:52:35.540 - 00:53:00.664, Speaker A: Hey, guys, I'm Shuyao. Great to see everyone. Linear is a zkevm. We're part of consensus, and we also build your favorite wallet, which is metamask. Yeah, super happy to be here. I think a lot of kudos to l two beat and also scroll for organizing this. The only thing I would say is if you're building on layer two or you're building on top of layer two, that you are definitely aligning with Ethereum.
00:53:00.664 - 00:53:09.760, Speaker A: So we're all here aligning. Thanks. Perfect. Thank you. Thank you. Thank you very much. And last but not least, we have Luke from Polychain Capital.
00:53:09.760 - 00:53:30.152, Speaker A: Please welcome Luke on stage. Hello. Good morning. I certainly don't have any eye catching slides or even much of a bow speech. I just wanted to say thank you to everybody. Definitely the organizers. There was so much work going into this.
00:53:30.152 - 00:53:53.490, Speaker A: My email box really, really felt it. I read some of them. And to the speakers who will be, and definitely Togrell and Peter, and to all of the attendees, as I'm sure they'll fill in and flow to this fantastic room. Looking forward to a super exciting couple of days. Feel free to grab me if you have any questions about what I do. Research partner in cryptography at Polychain. Thank you very much, everybody.
00:53:53.490 - 00:54:32.584, Speaker A: Thanks a lot, Luke. Thank you. And finally, it's my pleasure to introduce to you our master of ceremonies today, Matt Kochevsky. Hey, Matt will be your guide in the journey through l two days. And outside of this stage, we're also going to invite you to our second stage upstairs on the second floor and the workshops room downstairs. So thank you once again. Thank you and giving up to you, Matt.
00:54:32.584 - 00:54:51.810, Speaker A: Thanks. Hey, guys, what's up? Yeah, let's get it. Let's rip it. So huge. Shout out to piot and togurul. Those guys are dope. We're starting.
00:54:51.810 - 00:55:29.756, Speaker A: Our first speaker is Torgan Matskinga. Sorry I'm butchering your name, guy, but yeah, I'll give a little intro to myself as we catch up. But I'm excited to hold space for you guys in this room and make sure things are flowing properly and efficiently and smoothly. So sit back, relax, enjoy your time here. This venue is sick. Super awesome. I highly recommend you do a few laps over the next few days to get accommodated to the space.
00:55:29.756 - 00:55:48.522, Speaker A: It's really great here. Come on up. You should have a microphone here. Give it up for Torgan. Hear you say check, because check. Check. Sweet.
00:55:48.522 - 00:56:05.300, Speaker A: Very well. Do we have a clicker? Yeah, I got a clicker for you right here. It even has a laser. Perfect. Enjoy. Hello, everyone. Thank you for waking up so early.
00:56:05.300 - 00:56:44.090, Speaker A: I hope I can make it worth your time. So, my name is Torgan. Today, the presentation I want to give you is know before you build, right? So you might have heard a lot of l two s want to be ethereum equivalent these days. This means different things to different people, kind of. So today I want to run you through kind of the things that you need to be aware of how the l two s might not behave exactly the way that you expect. So, about me, just real quick, I work at chain security. We do smart contract audits.
00:56:44.090 - 00:57:21.594, Speaker A: We've had the pleasure to work with a lot of the great teams in the space. And yeah, our job is to make sure that you guys can feel safe with your contracts. So there's like 100 different l two s nowadays. There's a new one launching every week. Of course, I can't cover each one of them in detail, so I've just picked four of the biggest ones for you. We have optimism, optimistic roll up. Of course we have the polygon zke evm, we have arbitrum, and then we have Zksync era, which is another zkevm or.
00:57:21.594 - 00:57:56.658, Speaker A: Yeah, let's say that. Also we have a short honorable mention of scroll. So, first of all, I just stole this image right here from Vitalik's blog post on different types of zke evms. So this is kind of a sliding scale of the design space of zkevms. I mean, I'm just throwing the optimistic roll ups on there as well. I think it makes sense. So all the way top left, we have the type one zke evms.
00:57:56.658 - 00:58:38.398, Speaker A: These are also known as like Ethereum equivalent. So the idea there is that you have an l two where your vm is exactly the same as on ethereum. And this is good because then you can reuse everything, right? It's exactly the same as l one. You don't have to think about anything at all. So if every l two was a type one, I wouldn't have to be giving you this talk today and everything would just work. But of course, if you can't change anything, it means you also can't improve anything, which you might want to do to get more performance, for example. And so that's why you might want to go down this sliding scale.
00:58:38.398 - 00:59:42.754, Speaker A: So on the other extreme, we have the type four, which is basically not EVM equivalent, where you've made a lot of changes in order to get better performance and to also make it easier to ZK approve your vm, perhaps the downside is that you won't be able to reuse all of the Ethereum infrastructure. Right? You're changing a lot of things. So a lot of tooling will break, a lot of contracts will have to be rewritten. And also you need the expertise of how this specifically works. Right? So if you hire me to do an Ethereum audit, I've been doing this for a long time, I know exactly what's happening. If you hire me to do an audit on another thing, I need to figure out all of the intricacies there, and I don't already have that in my brain cache. Right, so these are kind of the trade offs that you have here between the different design decisions, how you could be EVM compatible or not, right? So to just give a rough idea of where these projects are.
00:59:42.754 - 01:00:19.860, Speaker A: So polygon ZkeVm or Hermes as it was called before, is trying to be a type two CkeVm. So that means they want to be fully ethereum equivalent. They want to be EVM equivalent, but not ethereum equivalent. Right. So they're changing like the back end stuff, but from the application layer it should look exactly the same as Ethereum at the moment. They're still missing some of the pre compiles, but those are coming soon tm at some point. And then their goal is to be type two, so that you don't actually have to care on the application layer for the most part.
01:00:19.860 - 01:00:59.302, Speaker A: Then as a contrast, we have Zksync, which is like all the way down on the scale. They've made a lot of changes, the reason being that they want to get more performance, right, so they have improved the design, but of course that also means they've changed the design. So a lot of the tooling might not work, some of the things might not be as expected. They have a custom compiler. You can't just deploy your bytecode from Ethereum to Cksync, but instead you'll have to use their compiler, which of course has been in development for less time than the normal solidity compiler. So there's more bugs and so on. And then we have the optimistic rollups.
01:00:59.302 - 01:01:57.106, Speaker A: Optimism is trying to pretty much be EVM equivalent. And then we have arbitrum, which is also up there, but they are leaving themselves a little bit more wiggle room to change things. So this talk is just about the smart contract layer. There's of course a bunch of other things you need to consider when choosing which l two you want to use, if you want to use them, and so on. Right? There's all of the questions that I'm sure all of these will be talked about at length today. I think some of them are especially important, such as the security assumptions that you're making. If your l two works really well in theory, but the base contracts are upgradable by some team, multisig or a dow or something, you can always have these governance type attacks where your contracts just get upgraded and then it doesn't matter if the system works in theory, if the admins just rug you.
01:01:57.106 - 01:02:28.350, Speaker A: So I think these are things you need to pay a lot of attention to. There's also the data availability problem that people have been talking about a lot. But this specific l two s I chose here are all roll ups, right? So you use Ethereum for data availability. You can also make other trade offs there. But so let's get to the smart contracts and especially the opcodes that don't work the same way as on Ethereum. And so one of the most important ones to be aware of is the origin and caller. So caller is the message sender.
01:02:28.350 - 01:03:41.618, Speaker A: Here you have a little bit of ambiguity because usually on l one, there can only be one contract on one address. Right? But on L two, that stops being true, because you could have a contract on L two at a certain address and a different contract at the same address on l one. And this is what is also known as address aliasing. And so this is a problem because the trust that you have on the l two contract might get extended to the l one contract. So if you imagine that on L two you have like a uniswap clone, you know exactly what that code does, but there might be a different contract deployed on l one. And so if you have some kind of force inclusion mechanism where you have messages you can send from l one to l two, it could happen that maybe you give an approval to this safe contract on L two. But then there's a force inclusion of a message that goes from l one to l two, and then the message sender in that transaction will be the same address.
01:03:41.618 - 01:04:44.938, Speaker A: Right. And so then if you have, like, a contract on L one that can steal your funds, they can use the approval that you gave on L two by using this force inclusion mechanism. And so the way that this is usually solved in most of the systems is that you don't allow contracts to make to be the same message sender on L one and l two, but instead, for the l one contract, if it's making transactions on L two, you add some deterministic change to it so that it happens from a different address on l two than on l one. And so this means there are addresses on l two that can be a message sender even though nobody controls that address. Right. There's no contract and there's no eoa that controls this address, but the transactions just happen from there. And this is something that you need to be a little bit careful about, because that also means that the only way to make a transaction from that address is from the original l one address.
01:04:44.938 - 01:05:52.294, Speaker A: So if you assume that you're making transactions on l two and you assume that you control that address, you might have an issue if you don't have the control of the original alias l one address anymore, or even if you have maybe a contract on l one that isn't fully generic, where you can't make all types of calls, you might end up in a situation where you can't actually make the call on l two that you want to. Another special thing about this is that you can actually have origin equal caller for a call that came from a smart contract. Right. So on l one, this is kind of like a hacky way to figure out if the message is coming from an eoa, because the origin can never be a contract. But if you have one of these l one to l two messages, then the origin and caller will actually both be like the aliased address of this contract address. And so, yeah, if this is an assumption that you're making, you should definitely be aware. Next one is everyone's favorite, self destruct on optimism.
01:05:52.294 - 01:06:08.562, Speaker A: Arbitrum. They have it. It's called deprecated. You can still use it on Zk roll ups. So on Zk sync, they just give you a compile time error in their compiler. They don't want you to use it on polygon. They implement the send.
01:06:08.562 - 01:06:46.654, Speaker A: All right, so they don't destroy the contract, but they do force send all of the eth. I guess this is kind of what might also be the l one semantics soon, then we have the difficulty or prev randow, this one is a little bit weird, because on l one it comes from the beacon chain. Of course, on l two, you can't access that, so all of the roll ups, they just do something different. Right? You get these constant values. On optimism, you get a random value chosen by the sequencer. You probably don't want to use it for randomness. So in general, just don't use this unless you're on l one.
01:06:46.654 - 01:07:18.250, Speaker A: Then prev random is kind of okay. Then we have the EVM block number and block timestamp. So on l one, it's obvious, right? There's only one block number. On l two, it's not so clear, right? You have l two blocks. You have l one blocks. And it used to be actually that most of the l two s returned the l one block number and block timestamp. But what that means is that you can have multiple blocks with the same block number, right? And your contract is probably not expecting this.
01:07:18.250 - 01:07:52.326, Speaker A: And so recently, everyone has kind of been making the shift to actually have the l two block number in there. So, for example, zk sync, they just switched recently. Now it returns the l two block number and timestamp. And arbitrum, they love l one. They still have the l one block timestamp. So again, just be aware of how the specific l two that you are deploying to handles this. Then we have push zero, the problem child.
01:07:52.326 - 01:08:20.862, Speaker A: Right. Everyone loves it. Nobody has it in general, it's not implemented. Most people are trying to implement it at the moment. It's not live on most chains, so you have to be careful because the solidity compiler will by default give you push zero. And the problem is that in your local testing it might work, but then when you're deploying to the chain, it doesn't work. Right.
01:08:20.862 - 01:08:49.506, Speaker A: And so you need to set up your local testing environment to also not support push zero to make sure that it's the same. And it's pretty easy. You can just, in your config file, set the EVM version to Paris. I think foundry actually now also has that as a default. But yeah, just make sure that your test environment matches the chain that you're trying to deploy to. We do have very recently we have some push zero, right. We have Polygon Zkevm.
01:08:49.506 - 01:09:09.386, Speaker A: They implemented it end of September. We have scroll. They also launched with push zero. They actually launched with a lot of opcodes. They have too many opcodes for me to tell you about, so kudos to them. Then we have the block hash, right. On ethereum it's just the block hash.
01:09:09.386 - 01:09:34.598, Speaker A: On arbitrum, they're a bit lazy. They just give you the hash of the block number. It's not very intuitive that that's what it would return. But you can get that for the last 256 blocks. With this opcode on optimism, you can get the l two block hash, which is what you would expect, but only for the last 256 blocks on polygon. You can get it for all blocks ever here on Zk sync. They also just recently changed this.
01:09:34.598 - 01:09:55.322, Speaker A: So it used to return the l one block hash. Now it returns the l two block hash. All of these things, everything is in flux. You probably don't want to use it. Yeah, that's kind of the main takeaway of most of these. Then the next one we have is Coinbase. Right.
01:09:55.322 - 01:10:16.322, Speaker A: Coinbase is the address that the validator payment goes to on l one. This is basically either undefined or it just gives you some random value. On Zke. Evm polygon zkevm. It gives you the sequencer's address. That's probably like the closest thing to a validator that you can get. But yeah, probably don't want to use it.
01:10:16.322 - 01:10:54.586, Speaker A: Unless you really want to bribe the sequencer or something. I don't know what you're into. Then we have just a quick note on Zk sync. So as I put it into the chart before, right, Zk sync is like on the bottom where they made a lot of changes. So they are not targeting EVM equivalents or I guess compatibility in a way, but it's a very long shot. So they have their custom compiler, you will need to use that instead of the normal one. They have a long list of opcodes that work differently or are not supported.
01:10:54.586 - 01:11:44.480, Speaker A: So if you are going to deploy to zksync you might want to look into that. Or you just pray that the compiler solves everything for you. One interesting thing also is that create works differently, so it will deploy contracts to different addresses than everywhere else. So if you're used to having your contract on the same address, you can't have that on cksync as far as I know. Or you need to at least do some magic to make that happen. Then the other thing we have is gas, right? So for arbitrum optimism and polygon zkevm they did not change the gas costs of the opcodes at all. So you don't need to worry about that.
01:11:44.480 - 01:12:35.406, Speaker A: There is of course a cost because on these roll ups you have the l two cost, right? And then you have the l one data cost because you have to push all of the call data down to l one. And this is usually factored into the gas price, but not into the gas cost of. Not the amount of gas, but just the price of the gas. So that means that the contracts don't really need to care about it. It can just fluctuate without the apps knowing anything. On Zksync era they changed all of the gas costs to make it more reasonable. The reason we have gas is so that you can't do complicated things that cause denial of service, right? And so they repriced everything to take into account the cost of proving the things.
01:12:35.406 - 01:13:29.646, Speaker A: Makes a lot of sense, but it also breaks things. So there was one case, for example where there was a project that deployed to ZK sync. They raised a lot of money and then they wanted to use the solidity east transfer to get the money out. But that by default only gives you 2100 gas, which is supposed to protect you from reentrancy, right? That you can only make a transfer and do nothing else. But because on ZK sync the cost of sending ETH increased the 2100 gas was not enough, right? So then there was all of this ETH stuck in the contract, you couldn't get it out. Since then ZK sync has said that they will change the gas cost of this to prevent this exact issue. They did know about it before there was even a compiler warning project ignored the compiler warning, didn't deploy to testnet to see if it works.
01:13:29.646 - 01:14:12.266, Speaker A: They just went straight to mainnet. You probably don't want to do that either, but yeah, so Zksync has said they will change the gas costs. I wasn't actually able to figure out if that is live yet, but if it's not, it's coming soon. So soon you will not have to worry about ETH transfer on ZK sync. But you probably don't want to be hard coding 2100 gas anywhere, because all of this stuff is changing very quickly. Right? Like you can see a couple of months ago a couple of things were different, and I'm guessing that we're not quite at the end of the road there. Then on Polygon Zke EVM, they didn't change the gas costs, but they have exactly the same problems as SDKsync.
01:14:12.266 - 01:14:46.518, Speaker A: Right? Like some things are harder to prove and some things are easier. For example, hashing is very hard to do in SDK proof. It takes a long time to prove. And this would mean that actually you want the gas cost of hashes to be high. But on zke evm they just don't care about that and they solve it on a different layer of the stack. So they have spam protection at the sequencer level. So if you just submit like 100 transactions that all are doing a bunch of hashes, then the sequencer will just not include you, right.
01:14:46.518 - 01:15:47.950, Speaker A: It'll put in a couple of hashes in each batch of blocks and then eventually everything will be included. But they will be kind of rate limiting you at the sequencer level instead of the gas level so that they don't break the contracts. So if you are planning on deploying to polygon Zke EvM with a lot of hashing or other complicated things, then you might want to test against a realistic testnet or mainnet to make sure that the sequencer isn't seeing you as spam and isn't blocking your transactions from going through quickly. Yeah, so this was all of the security stuff, right? Now we get to the really fun stuff. This is the gas golfing part of the talk. You should take everything I'm about to say with a grain of salt. You probably don't want to do most of these things, but if you're the type of guy who writes the inline assembly to save that little tiny bit of gas.
01:15:47.950 - 01:16:30.250, Speaker A: Yeah, you can do some of crazy things on l two s to save some gas, right? So as I said before, the gas cost on l two, you have the execution gas on l two, and then you have the l one data cost of pushing all of the roll up data down to ethereum. And the data you're pushing is the call data. So the inputs to your transactions, but the transactions are actually happening on l two, right. And at least at the moment, the l two cost is generally negligible. So I think it's something like 99% of the cost is data in most of the l two s. This will change with dank sharding in the future, hopefully. Right? We'll see.
01:16:30.250 - 01:17:14.754, Speaker A: But for the moment, you really want to optimize for l one and not for l two, because your l two is like performant or not used as much or whatever. And the l one is super expensive on l one. For example, you never want to be doing s store, right? It's super expensive. It's the most expensive thing you can do on l two. Most expensive thing is call data, right? So you don't want to optimize for s store anymore, you want to optimize for call data. So how do you do this? First of all, you do calcs on chain, right? You build the calculator on l one. An optimization would be like to give some extra call data with maybe some pre computed values to make it easier for your contract.
01:17:14.754 - 01:17:37.840, Speaker A: So it has to do less calculations on l two. It's probably not worth it, right? You probably want to do the opposite, actually. You want to give less call data and have more on chain computations. Kind of in the same realm is using storage, right? L one storage, super expensive. You don't want to use it if you can help it. L two storage, it's your best friend, right? It remembers things for you. It stores things crazy.
01:17:37.840 - 01:18:25.306, Speaker A: So what you could do is save on call data by storing more things. So for example, you could imagine having like two functions that do the same thing, but one of them, you can give a function argument, and then you just store that function argument. And if the user wants to call the same function with the same argument again, they can just call the other version that doesn't have any arguments. And then you load from storage what they were doing last time. And you can kind of use that as a cache to save on call data. And then your overall cost will be lower because you're pushing less call data down to l one. Now, if you want to go completely crazy, this is very dangerous, but you can define a custom Abi right? So the default API that everyone uses is actually pretty wasteful.
01:18:25.306 - 01:19:11.370, Speaker A: So the way that it works is first you have four bytes, which are the function selector, and then you have your call data afterwards with your function arguments. But everything is padded. So if you have a Boolean value that you're passing to a function, it actually gets padded to 32 bytes in the API. So the first 31 bytes are all going to be zero, and then the last byte is going to actually have your boolean value in it, right? So this is wasteful. Why would you do this? Of course, for compatibility, right? But if you really care about gas, you can just say screw that, throw the default API out of the window and you build your own ABI. And so the way you would do this is you just use the fallback function. You have no other functions.
01:19:11.370 - 01:19:55.926, Speaker A: And if every time you make a transaction you will get into the fallback function, right? Because you're not fitting to any of the other function selectors. And then in your fallback function you can actually just decode your custom API yourself. So you can say, for example, I don't want four bytes of function selectors, I'm fine with 256 functions, I don't need more than that, right? Most contracts don't need that. So you just interpret the first byte as your function selector instead of the first four, and then you continue. Maybe you say the next byte is a boolean, right? And so then you just take one byte instead of 32 bytes. So you're saving yourself a bunch of zeros and so on. You can just define a custom API for all of your functions, right, with your function selector being your first byte.
01:19:55.926 - 01:20:22.702, Speaker A: And in that way you will be saving a lot of l one gas. Now of course there's a reason why people like standards, right? This is kind of the same idea as with the chains. Like here. You're taking your contract and you're pushing it to the bottom right of my chart, right? You're not being very compatible and you're getting more performance out of it. You're saving some stuff. You do need to be careful. You cannot disable the default ABI.
01:20:22.702 - 01:21:05.220, Speaker A: So if one of your function selectors collides with something in the default AbI, so let's say you have some view functions, right? You maybe want to have some getters for values in your contracts. It's possible that your custom AbI and the default ABI collide, and then there will be some values for which you cannot call your functions. You will be able to call your function with everything except exactly those values where the first four bytes collide with the function selector of your view function. So be very careful if you actually do want to do this, and you're crazy enough for that, there is a blog post on ethereum.org by one of the optimism developers. It goes into a lot of detail. You can check it out for fun or to actually do it.
01:21:05.220 - 01:21:29.722, Speaker A: I'll leave you off with a warning. If you do all of these things that I just told you about, you will build the pain robot, right? You don't want to build him. He's not maintainable. So you really need to think about, do you really care about the gas that much that you're willing to make these trade offs? You have usability. Like, users have no idea what the fuck is going on. If you have their custom API, you have maintainability. People are not used to it.
01:21:29.722 - 01:22:00.500, Speaker A: Somebody else will not be able to maintain your code or maybe even yourself. If you forget what you did back then, you have security. You're adding complexity. Complexity is always a security risk. And then you have the portability, of course. Right? Like, if you're over optimizing for your l two deployment, it won't be optimized for l one, for example. Or if in the future the data costs change on the l two, you might have over optimized for a thing that passes with time.
01:22:00.500 - 01:22:35.040, Speaker A: So that's it for me. If you guys do deploy to an l two, I recommend reading the specific documentation for those chains that you're deploying to. They usually have a dedicated page that tells you all of these things that I also told you about today and more. There's also a cool page called rollup codes, which is kind of a summary for a bunch of different chains if you just want to take a quick look to see what you need to pay attention to. Yeah. And that's it for me. I hope you learned something.
01:22:35.040 - 01:22:51.234, Speaker A: You can follow me on Twitter if you want. You can contact me on telegram, or I'll be here today if you want to chat. I'd be happy to. Thank you. Check, check. Give it up for Torgan, guys. Thanks for being our opening speaker, man.
01:22:51.234 - 01:23:00.838, Speaker A: You did great. Thank you for this. Cheers. Cheers. Are you around anywhere in the building today? Do you guys have a booth? No, you're around. Cool. So if you guys.
01:23:00.838 - 01:23:25.594, Speaker A: Yeah, look out for Torgan if you guys have questions. Up next we have Tony from Ethereum Foundation, I think, talking about PBS. Mev Mev. It's a funny one. Hey, guy. Sorry. Mevburn the last Mevburn okay, mev burn, impossible.
01:23:25.594 - 01:23:47.686, Speaker A: Directions you go, sir. And I have a nice little clicker for you. There's even a laser on it just in case you want to use that. All right, enjoy everyone. Tony, clap it up. Perfect. Yeah, thanks everyone for showing up.
01:23:47.686 - 01:24:09.758, Speaker A: My name is Tony, I'm with Etherim foundation. And today's talk. So it was introduced by math, math stuff and so on. But I will focus on meth burn today. So just a quick temperature check. How many of you have already heard about Mathburn? Okay, this is very good. So many people have already heard about Mathburn.
01:24:09.758 - 01:24:58.518, Speaker A: So I hope I can still give you some additional details that we just recently uncovered. Everything about the why we need Mafburn and also covering how we want to do it. Just to give you a quick rough outline what I'm going to talk about, I want to start with the problems. So first, defining some problems, why we actually need Mafburn, and then continue what we know about the current landscape that helps us to define some parameters at Mafburn. And finally, we will look into the current proposed design of Mafburn and go into the very depths of that. So let's start with the problems. The first problem that you can see here on the chart, it's overcompensating validators.
01:24:58.518 - 01:26:00.990, Speaker A: What that means is I argue that currently validators get paid too much for the security they provide. We all know that validators receive execution layer rewards and consensus layer rewards. And the consensus layer rewards consist of attestations, which are source votes, target votes, head votes, and in addition to the attestation rewards, validators also receive rewards for being part of a sync committee or for proposing a block. So far so good. Then we got the execution layer rewards, which consist of the priority fee, then Coinbase payments, which are payments that are directly paid to the validator from the builder directly to the validator. And there is the math boost payment. So currently around 93% of all validators are using math boost, which means that the execution layer rewards are only the math boost payment, which then of course contains already the priority fee, the Coinbase payments and every math that the builder wants to pass on to the validator.
01:26:00.990 - 01:27:03.346, Speaker A: And what we can see here is in the chart, the execution layer rewards take up around account for around 20% of the total rewards, which means the beacon chain normally was already designed to secure itself by issuance. Right? So the execution layer rewards are more or less here a bonus that the validators receive for becoming a validator. And this overcompensation then leads to many, many validators joining the validator set, which has some negative externality, because one might think, okay, the more validators we have, the better it is. But actually, a lot of validators have also some negative externalities. Like we have to do a lot of BLS aggregation, a lot of signatures have to be aggregated. And of course, when we already think about single slot finality, it's beneficial to not have that many validators for single slot finality, right. There is also a discussion about increasing the max effective balance, but this is a different one.
01:27:03.346 - 01:27:56.142, Speaker A: So we want to get the execution layer rewards down a bit to make it not that attractive for validators to become validators and to kind of balance out their incentive rewards, the economic incentives. Again, the next problem is the spikiness of MEV. What you can see here on the chart is the aggregated daily gas revenue and MEV profits validators make. It's aggregated on a daily basis, so the gas is more or less the priority fee. And MEV is everything above the priority fee that still goes to the validator, from the builder to the relay, and eventually to the validator. And what we can see here is we see three very extreme spikes. The first spike originates from the FTX debacle.
01:27:56.142 - 01:28:43.650, Speaker A: So when FTX blew up, a lot of Mev was, there was a lot of mev in the market. And if your validator proposed a block in those days, you were very lucky, right? The second very big spike was caused by the USDC DPAC. So when USDC depacked, we saw huge Meb. And again, if your validator was part of that, of a block where all this USDC arbitrage happened, then you were again very lucky because you earned a lot of money. And the last spike was caused by paper ming, the meme coin. So a lot of people were trading meme coins back then, which also led to a lot of meV. And we can see the spikiness in MeV already on a daily basis.
01:28:43.650 - 01:29:48.950, Speaker A: But it's not only on a daily basis where math is very spiky, it's also on a per block basis. So, for example, ten days ago, we saw that one validator received 319 eve for proposing one block, right? Which is not bad, definitely. And if that was your validator, then congrats, because it's not a bad profit for proposing a single block. So within 12 seconds. And exactly, this spikiness of MeV causes a lot of problems, because imagine you're the proposer of that block, right? You're proposing a block with 319 e in it that you get, of course, the proposer after you might see that and might think, okay, it would be actually cool if I could reorg this guy out and capture that MeB for myself. So we have a very big incentive for the validator that follows such a big math opportunity to trying to reorg out the previous validator. And this creates some very unhealthy instability in the consensus.
01:29:48.950 - 01:31:04.810, Speaker A: Also, thinking of Rocketpool, thinking of Lido, thinking of oval, distributed validator technology and stuff, it is not very healthy if we have such extreme math spikes, because imagine the following. A validator could collude with a builder, could tell the builder, hey, if you have a math boost payment that is extremely large, then we could do the following. Instead of sending it to my fee recipient address, you could just send it to another address that is private. I set it up, you send it there, and in return, I will give you some of this math back. What you would then do is you would forego this moving within your own pool. This means one validator in a distributed validator set could steal all the MEV from the other validators that are actually in his team, in his squad. So, very huge spikes cause a lot of variance, especially in absolute terms, which then introduce a very unhealthy consensus situation where we might see a lot of reorgs that are just motivated by the profits that originate from MEV.
01:31:04.810 - 01:32:29.798, Speaker A: So, what do we know about the current landscape when it comes to transaction fees, MEV and stuff, and how we can best implement a mechanism that smooths the MEV slightly across all the validators? So, what we know is that tob, the top of the block, is kind of the fillet of the block, right? So, to get into the top of the block, you have to pay a lot, and you pay through priority fees and through the Coinbase payments. And what we can see here in this chart is the burn. The EAP 1559 burn is very constant. While for the top of the block, we see that builders added searchers express their demand in both priority fees and Coinbase payments. So we can see that as of transaction index ten to 20, we see a very constant, first, a quite linear rise, and then it turns into a very exponential increase of the fees that you earn as a validator if you include those transactions. Second, we also know that with increasing transaction indices, the share of the burn decreases, which makes a lot of sense. So we can see here that let's focus on transaction index 80, for example, where the total burn makes up 99% of the transaction's gas.
01:32:29.798 - 01:33:28.502, Speaker A: While if you look at the first transaction indices, we can see that the burn is only half of the total gas that the transaction consumes. So to summarize that, we know that there is a very high demand for getting included into the top of the block. And this demand is expressed by increasing the priority fee and also the coinbase payments. And searchers, of course they want to get into the top of the block because this guarantees them to be able to do a successful arbitrage to sandwich some user before other searchers could sandwich that user, and so on. Second, we know that the share of the burn increases with transaction indices. So this means that if you're a proposer that only builds blocks with 1020, maybe 30 transactions in it, you still capture most of the profit. So kind of the long tail doesn't bring you much profit.
01:33:28.502 - 01:34:50.182, Speaker A: It's more really the searches transaction that pay you a lot, that bribe you in order to include them. So then you might ask, why don't we just simply burn the top of the block, right? So if we know the top of the block, they pay a lot of gas, why not just simply increasing the base fee when approaching the top of the block? And this could look like this. So what you can see here is that transaction indices, and I have a line with the EAP 1559 burn, which is normally at the current situation, very constant throughout the whole block. And now imagine what happens if we exponentially increase the base fee when we approach the top of the block. So going from high transaction indices to low, what happens if we just exponentially increase the base fee as of transaction index k, we just say we increase the base fee exponentially for every transaction that really wants to be in the top of the block. This might sound very cool and might sound that it would work, but of course we would already have implemented that if it wouldn't have some shortcomings. And these shortcomings are essentially a very inefficient math market.
01:34:50.182 - 01:35:46.390, Speaker A: So imagine the following. If the block value, if the actual block value is higher than the total burn, then everything is cool, because we would still not burn as much as we would like to, but we would not create some inefficiencies within the math market. The actual problem here lies in the actual block value being lower than the burn. Imagine you would have to burn more than you would be able to extract at a certain math opportunity. Then the searcher would not extract that mev in the current block, but leave the math opportunity on the table and wait for it to become profitable. And of course you might already see that why this is not very healthy for the whole market. If we have these strange behaviors where there is a math opportunity, but it is not captured because we have risen the payload base fee very dramatically.
01:35:46.390 - 01:36:20.414, Speaker A: So the top of the block naive approach of meth burn is not working. Instead, Justin proposed this meth burn mechanism that is a little bit more sophisticated. So you can find it on efresearch, on the title, a simple design. And in the following. I just want to quickly describe how this meth burn mechanism works and what are the advantages of it. What you can see here is two slots. So the first slot is the blue arrow.
01:36:20.414 - 01:37:03.570, Speaker A: And then we have another slot, a second slot, which is the green arrow. Of course, at the beginning of the slot at t zero, what a builder would do is a builder would want to find out what is the head of the chain. So a builder would run the fork choice rule and then the builder knows, okay, that is the head of the chain. And this is where I build my block on top of it. So as soon as you know what is the head, you can start building a block that builds on top of the head of the chain. And builders continue to do so. So they continuously build blocks and improve their blocks until the validator asks the relay or builder to deliver a block.
01:37:03.570 - 01:37:41.854, Speaker A: What is now new compared to the currently working math boost design is this time d between t one and t two. In this time. So what happens there? The builder produces blocks until t two. But at t one, the proposer will already look at what is the highest bid at t one. So the proposer will look at the public bid pool and checks what is the highest bid. I have seen at t one. The attestors of the next slot, they do the same.
01:37:41.854 - 01:38:33.082, Speaker A: They also observe the bid pool and also remember what was the highest bid that I saw at t one. And t one is d seconds before t two, which is the end of the slot. So now the attesters and the builders and the proposer have remembered what was the highest bid that I saw at t one. Then, as usual, when a new slot approaches, the proposer of the second slot will ask the relay for a new block. And then the proposer has to select a block that burns at least what the attesters agreed upon in t one. So remember, the attestors have set their local view of the payload base fee. So the payload base fee is what is burned.
01:38:33.082 - 01:39:35.442, Speaker A: And then the proposer must select a block that burns at least what the attestors will agree upon. And this works like the following. The proposer selects a block. The block contains some math burn and then the attestors will only attest to that block if it burns at least what their perception of the minimum burn was. So kind of we have this asynchronous assumption that we weaken by introducing this delta time d where the proposer would see okay, let's say at t one the highest bid was ten e and then at t two the block value, the mev boost payment is 15 e. Then the proposer would have to select a block that burns at least ten e because all the other attesters of the upcoming slot will also have the same perception of the payload base fee floor and only attest to the block if it burns at least the ten e. In a different picture it would look like this.
01:39:35.442 - 01:40:11.358, Speaker A: So these are the different entities involved. You have builders, builders submit their bids to a public bid pool. And this bid pool is then observed by both the attestors and the proposer. So the proposer looks into the bid pool and checks what is the highest bid at d seconds before the end of the slot. And the attestors do the same. So they both try to find out what is the highest bid that I saw 2 seconds or d seconds before the end of the slot. Then the proposer selects a block or a bit, it's the same.
01:40:11.358 - 01:41:50.986, Speaker A: So the proposer selects a bit from that bit pool and already has to think about what was the highest floor, the payload base fee floor that I saw. And then the attesters will only attest to that proposer's block. If the proposer selected a block that burned enough, and this then kind of the burn mechanism is enforced by the attestors because if the proposer, if the payload base fee floor d seconds before the end of this lot was ten e, but the proposer selects a block that only burns five e, then the attestors will say, okay, this is not enough and I will not attest to that block, which means the block will get reorked and the proposer and the builder lose all the money they would have gotten through that block. So for the proposer it's very bad to select a block that doesn't burn enough, which means the proposer, if he wants to be on the safe side, the proposer will just maximize the burn. In order to make sure that all the validators in the slot will agree that I burned enough as the proposer, let's look into some simulations how this could affect the current landscape. Of course maybe just to say this is based on some assumptions that the builder behavior doesn't change with introducing mev boost, with introducing meth burn, which is a very strong assumption, but in this case very helpful to allow us to already simulate some stuff. What we can see here is the total area between.
01:41:50.986 - 01:42:43.806, Speaker A: Below the blue line is what the builders are, what the proposers currently get, right. So everything below the blue line, also the orange area. This is what the proposers currently get in math boost payments for proposing a block now with math burn, we want to burn large parts of this math payment, not only to smooth it out, but also to decrease the absolute variance in the math boost payments. And you can see this is definitely a very optimistic estimation. So roughly around 60% to 80% should be burned. With introducing Mapburn, what does that mean for the validator? So validators will of course make less money. Let's maybe focus on the left hand chart first.
01:42:43.806 - 01:43:25.306, Speaker A: We can see that we shift the line, the distribution from the right a little bit to the left. We might increase the relative variance, but definitely decrease the absolute variance. And the absolute variance is what we focus on, because it's the absolute variance that creates these incentives to reorg out some other validator or dos him. On the right hand side, you can see that currently the median math boost payment is around 0.5 Eve. And by introducing math burn, this will be reduced quite dramatically, to 0.2 e.
01:43:25.306 - 01:43:56.114, Speaker A: Right. The burned value is of course not lost. It goes to all the eth holders. So instead of going to the validator, it is burned, it decreases the circulating supply, and thus is good for all the eth holders. If we look at how the bids behave within a slot, so builders start bidding. Some builders start bidding very early. As soon as you know the head of the chain, you can start producing blocks as a builder.
01:43:56.114 - 01:45:21.742, Speaker A: And then at some point, builders submit their blocks currently to a relay with mapburn to a bit pool. And what we can see here is a very slowly increase bit value from minus ten to minus six or minus five. And then we see that the bid values increase quite dramatically. This means for you as a validator, if you want your block very early, let's say 6 seconds before your actual slot starts, then you lose out a lot of money, because the builder's bids are not very high yet. But if you wait long enough, for example, until, if you want your block 2 seconds before the slot starts, then you will receive around 80% of the value, compared to what the medium bit is exactly at the start of the slot. So this is very important for us in order to determine what should the variable d be like, right? Because d creates this asynchronous assumption that you want validators and attesters to agree on the same burn floor. And what this chart shows us is if we set d to minus two, which is 2 seconds before the slot, then around the bit should already have reached around 80% of its final value, which means that by then 80% should be burned.
01:45:21.742 - 01:46:07.246, Speaker A: Of course, again, I want to stress that it assumes that the builder's behavior doesn't change with introducing Mafburn. Right then let's also focus on the delta time d. But let's only focus on the blue line. I hope you can read it good enough. On the left hand side it's the absolute values, and the blue line is the percentage of the total. And on top, the top chart shows the impact of the delta time d on the tip, which is the mev burn tip that the validator still receives. And on the bottom chart shows the impact of d on the amount of math that is burned.
01:46:07.246 - 01:47:10.614, Speaker A: And what you can see here is on the upper chart that if we set d to 0.5 seconds, which is 0.5 seconds before the slot starts, then of course the value between then of course the burn will be much higher than setting d, for example, to 2 seconds. If we focus on the upper hand chart, it's like at 0.5 we can see that around zero to 5% of the math value would still go to the validator, while if we look on the other hand, on the other side, at second two, we see around ten to 15% would still go to the validator, which means that we would reduce the validator payment by around 85%. On the lower chart you can see the impact of the delta time t on the amount of mev that is burned. And again we can see that if we set d to a very slow, very low value like 0.5
01:47:10.614 - 01:47:54.082, Speaker A: seconds, around 90% to 95% of all the math would be burned. On the other side, if we set it to 2 seconds, the amount that is burned is much lower, around 75% to 80%. And of course d cannot be chosen arbitrary, because we need d to allow the proposer to determine the real, the true payment base fee floor. So let's say we set d to 0.5, right? So there is a bit that comes in 0.5 seconds before the end of the slot. Then the proposer would have.
01:47:54.082 - 01:48:35.150, Speaker A: Then we would have to make sure that the proposer sees that bit. Because imagine you're a validator in Australia and maybe you're not perfectly connected to all other validators. Right? There is large latency involved and stuff. So maybe you have not yet seen that bit that came in 0.5 seconds before the end of the slot, but maybe the majority of the attesters saw that bit. What would happen to that builder is the builder would think he would burn enough, but actually he doesn't. So his block would then be reorged out, or to be more precise, it would not be attested to by the attestors.
01:48:35.150 - 01:49:13.120, Speaker A: So setting it to 2 seconds means that let's say you're a very bad connected validator, you're a solo staker and you have not a lot of bandwidth and you're very bad connected and also your peers are bad connected. So you would have 2 seconds to make sure that you saw that bit in time. And 2 seconds is definitely enough to make sure that you saw the payload base fee floor. You saw the same payload base fee floor then, yeah, test us. And this allows the proposers to have two more seconds in order to receive the highest bid. Right. This brings me already to the end.
01:49:13.120 - 01:49:52.250, Speaker A: You can see free codes. We got free writings on the Mapburn stuff so far. The first one is Justin's original proposal. Then we did some simulations on Mathburn and very recently Mike Newter published how I learned to stop worrying and love Mafburn. And yeah, I think I have a few more minutes. I'm happy to answer questions and thank you very much. So we have microphones and our lovely volunteers are microphone runners.
01:49:52.250 - 01:50:35.750, Speaker A: Anyone has questions, feel free. Oh, yeah, here, you can use this one, Tony. Thank you. Questions over there. Hello. Couldn't this team be prevented with block coinbase transfers and private mempools like flashbots? Test it. So the question is like, for example, if the bit pool would be private, if that would prevent the burn in conjunction with block coin best transfers.
01:50:35.750 - 01:51:30.726, Speaker A: Yeah, that's a very good question. So test, test. Yeah, that's a very good question. So the question is like, what happens if the builders don't submit their bids to the public bid pool, but instead they submit it directly to the validator? Right, of course, yeah. This is a very valid concern that maybe builders or some validators will open up their own endpoints and say to the builder, please don't contribute to the payload baseb floor because I don't want to burn anything and I want to keep anything for myself. This is a very valid concern. And there are some arguments why builders would not do that, for example, one argument is like builders would use the public bid pool because then they can make sure that their validator truly sees.
01:51:30.726 - 01:52:54.498, Speaker A: So every validator sees their bid, right? And also, it's like the prisoners dilemma, right? Let's say one builder says, okay, I will not submit my bids to the public bid pool, but then there will still be other builders that do so. And submitting to the public bid pool also means that if a validator would also mean that you will have the security, the safety, that every validator will see your bid in time. So let's say you're a builder that doesn't contribute to the burn because you don't submit to the public bid pool. Then it would mean that other builders would contribute to it. So they defect and they will set the payload base fee floor. But this is, of course, if you go to the eve research post, especially to Justin's one and the last link, you will see a large discussion exactly on that topic. How can we incentivize builders to really use the public bid pool and not collude and do something else? But to be honest, it's already a thing today, right? Builders could already collude today if all the big builders, let's say Rsync builder beaver build, titan flashbots, if they would collude and say, why don't we just pay nothing to the proposers and keep all the mev for ourselves? We all have around 20 20% market share.
01:52:54.498 - 01:53:15.370, Speaker A: Why not just split it up? Every fourth block is mine and I will pay nothing to the proposers. So this could already happen today, but it doesn't happen because these builders don't collude. Sweet. Thanks for the question and the answer. Let's give it up for Tony. It's all the time we have for that. Thanks, man.
01:53:15.370 - 01:53:28.186, Speaker A: Thank you. I love your Twitter handle, by the way. It's my favorite dj. Yeah, Nero. I don't know if you know Nero. Do you know Nero, the roman emperor? No. Oh, it's a Rome emperor.
01:53:28.186 - 01:53:52.390, Speaker A: Okay. That's funny. I am an electronic music guy, so I just know them as the dj. So, yeah, there's a roman emperor out there, if you guys didn't know, named Nero. Learned something new today. I believe we have Georgios from gnosis. If you're here, come on up, buddy.
01:53:52.390 - 01:54:28.802, Speaker A: My guy, Georgios. And by the way, you want to hold onto that microphone or do you want to. Yeah, I'll keep it up here so you're not like, lugging it around. Thanks for doing that, by the way, Georgios. From gnosis. Are you here? Is that you? No. Are you Georgios? Hey, look who made it.
01:54:28.802 - 01:54:46.342, Speaker A: He's here. Woo. Give it up. I was just doing a little song, know, make sure that everyone's engaged. Oh, here's your a. There's a laser on there too, in case you want to. Yeah, it's pretty cool.
01:54:46.342 - 01:55:09.862, Speaker A: You can't see it, but yeah, it's a laser. All right. Okay, enjoy. Thank you. Hello everyone. So yeah, I'm going to talk to you about how we use Hashi to build community governed cross chain security layer and why this matters. So a couple of numbers.
01:55:09.862 - 01:55:56.470, Speaker A: So it depends on how you look at it. You have more than 70 bridges around, lots of TVL, lots of value secured by those bridges, more than 11 billion. And the traffic is quite high, so there's a lot of usage, more than 5 billion the last month. So there are some fundamental problems in the cross chain space. And there are two main issues here. Two main categories. One, probably everybody knows it is the security issues, like more than 25 billion has been lost in bridge exploits recently and the biggest hacks in the space have been bridge related.
01:55:56.470 - 01:56:47.800, Speaker A: But there is also a second issue that not much talked about, which is the vendor lock in and loss of trust. So many D five protocols, they want to go cross chain and also l ones, they need a bridge, but they don't trust one specific implementation. And they want to hedge against these security issues, against these exploits. And of course they want to avoid also vendor locking. They don't want to be locked in in a specific implementation. So number one, regarding the security issues, since last year we've heard this quote that the ZK bridges are trust minimized, trustless designs, and the l two s will solve this. But let's see if they really are going to solve this.
01:56:47.800 - 01:57:29.518, Speaker A: On the right side, you can see this table shows the majority of the bridge hacks, if not all. And I've highlighted there just two cases which are due to compromised keys. So 20% of the hacks are due to compromised keys. And this is actually, yes, that is solved by the l two s because you don't have any keys controlling the bridge directly. But everything else you can see are because of smart contract bugs. And that's here the key point. Even l two s and ZK bridges, they still have a kind of a single point of failure.
01:57:29.518 - 01:58:36.780, Speaker A: And so far we didn't have any big issues, but we should definitely expect such bugs to be discovered at some point also in this implementation. So there will be, I mean, hopefully not. And hopefully not big issues, but they will be at some point on this list as well. Here there are some learnings from Ethereum protocol development, and I'm talking about the client diversity, right? Everyone on the consensus side or the execution on Ethereum, they talk about we need client diversity, we need more than one implementation of the same client. So you can see here we have different implementations, gith, nethermind, Aragon and so forth, and we advocate. Our point here is that we need a similar system for bridge implementations, especially for ZK bridges, for example, even l two s. So here is a rhetorical question.
01:58:36.780 - 01:59:26.710, Speaker A: What is the higher probability of a bug? Is it like in a go compiler or in a Zika circuit? And apparently the answer is, at least from my side, is a Zika circuit, because super complex math that very difficult to audit and understand. So here are some key conclusions so far. We're not advocating here a specific bridge designs. All designs have trade offs. What our conclusion was is that trust in just one security mechanism, one implementation, is not enough. And also even dozens of audits or 2030 audits that some protocols might do are still not enough. So long story short, 100% secure bridge will never exist.
01:59:26.710 - 02:00:18.214, Speaker A: Does not exist. Let's go now to the second point about the vendor logging and loss of trust. So here what I mentioned before, this is some examples how some bridges severely impacted l two s, sorry, l ones and d five protocols. The recent multichain hack, for example, severely impacted the phantom ecosystem and also curve. So we saw that they had a vendor locking, they had a specific implementation, and then they were really impacted. The whole ecosystem was destroyed and they had no easy choice to switch to a different implementation right away. Similar also to the nomad hack, how it impacted some ecosystems.
02:00:18.214 - 02:01:37.982, Speaker A: And we're seeing a trend lately that most of the biggest DeFi protocols, such as Aven maker, they're building their own infrastructure, so they don't trust any of these specific implementations, and they don't want to be vendor locked in, of course. So yeah, how do we fix this? The first step is we have to go from a monolithic view that bridges offer right now to a modular bridge stack. And I will explain what I mean by now. So most of the bridges currently, and you can take any example, I don't want to name specific ones, but you can imagine here can be a wormhole axelr or any other bridge, or even the canonical ones from gnosis chain, for example. They are kind of monolithic, right? So they offer all these three layers, as in here. So the security and transport layer the execution and messaging, and also the application layer, which can be assets, right? So the security can be any kind of implementation, can be a multi sig, can be a committee mates, can be a Ziki lite client or some optimistic approach. Then there is a messaging protocol on top that will transfer, that will basically transfer the message and parse it, and then something will happen in the application layer.
02:01:37.982 - 02:02:27.730, Speaker A: Most of the times it's a cross chain asset, but it can be also. So cross chain governance or anything else. And one important thing to emphasize here is the broken incentive on this stack. So you can see here on the left side that the value capture and the market share is only on the top layer. And what I mean by that is when you transfer cross chain assets and tokens, this is where the fees are captured. This is where the user will pay the fees and whoever is on this layer, they will capture the value and the fee. They can earn some money, right? But the lower part of the stack, which is a security layer, has all the risks and the biggest costs.
02:02:27.730 - 02:03:06.734, Speaker A: And. Exactly here we see there's some fundamental problems. That's the reason that bridges capture the whole stack, right? So they want to be able to capture the value on the upper layer. And of course they need some security implementation because somehow they need to transfer the messages and they kind of subsidize the cost and the maintenance of the security layer through these fees, however. And then we have many different implementations of the security layer, which is good. The thing is that it's not good is that they are siloed, right? So we don't have any kind of collective security, it's just siloed implementations. Right.
02:03:06.734 - 02:03:53.600, Speaker A: You can get again any kind of bridge, the axlar network, the warm home network, they are very secure by themselves, but they are siloed. And then of course good security is also very expensive and long term, unsustainable for more bridges. So they will sometimes cut corners. And here on the green part is just some initial thoughts, why we not use a shared security layer? So you can have a shared security budget with higher security and lower the risks and the costs. So all the bridges can focus on the upper layers where the value is actually captured. And more conclusions here. So everyone needs actually the security, but no one really wants to maintain it and pay for it.
02:03:53.600 - 02:04:54.850, Speaker A: How do we fix this incentive alignment and suggesting a new bridge stuck? There are three high level ideas here that we want to initiate. So first of all is shared security as a service to all the bridges. So exactly what we mentioned before, you share the security layer, then you need to align the incentives. So the value captured on the asset layer, where the users and the protocols are actually paying the fees, should be proportionally distributed across the other layers, and especially the security layer and the providers of the security layer. And of course, some values here. Everything in this new stack needs to be based on open standards, no vendor lock ins, and of course, healthy competition, which we don't have right now in the bridge space. Here is a reference to the Uniswap foundation bridge assessment.
02:04:54.850 - 02:05:50.690, Speaker A: If someone here has been in the bridge space or looking at bridges in general, most probably have come across it is Uniswap foundation was trying to find the best bridge for their governance, and there was a huge discussion by every bridge in the space why one design is better than the other. In the end, Uniswap decided to assemble a team of independent experts and actually find what's the bridge design. And here it's an amazing analysis, really great document. But I'm just summarizing with one sentence, which is the actual conclusion is you have to use multiple bridges to create redundancy. Similar conclusion to ours. Why? Because, yeah, let's now talk about what Hashi is. So, Hashi was introduced before this assessment, and like in March of this year, and we actually arrived in the same conclusions.
02:05:50.690 - 02:06:25.840, Speaker A: So we need to aggregate security and redundancy. And this came up actually from the analysis we're doing, how to make the Gnosis chain breeze more secure. But of course, we realize that this is something that can benefit the greater ecosystem and not a specific use case only. So Hasi has two main principles. One is obvious. There is no reliance on one single security mechanism. And there is the concept of distributed trust.
02:06:25.840 - 02:07:21.102, Speaker A: And another important part is that we focus only on one specific and very important and vulnerable part of the stack. So the stack I showed before, Hashi focuses only on the security layer. It doesn't try to capture the whole vertical, for obvious reasons, because we want to do one thing right and concentrate on that, some benefits of this approach. So, first of all, that makes Hashi very thin, stateless, non upgradable layer. Of course, because of its nature, it allows to have no vendor logins, incentivizes the security providers accordingly. Our intention is to make it governed by protocols and bridges and of course, hedging against any kind of security issues. And of course you say, okay, what's the catch? It cannot be the perfect solution.
02:07:21.102 - 02:07:54.940, Speaker A: So here the catch is a very clear trade off security over latency cost. So this thing is many times more secure. But of course, a lot more expensive and slower. And this is something from our experience and analysis that most of protocols and users are willing to take. They're willing to pay a bit more to wait a bit more for more security. And waiting a bit more actually can be solved via optimist debrising and things like that. But it's out of scope of this talk.
02:07:54.940 - 02:08:45.082, Speaker A: And now we arrive to the new stack, which is very similar to what I showed before. So the application layer is the same execution layer. Again, you have different kind of cross chain mechanisms, implementations, but here I added one thing. So there is the healthy competition and explain exactly how this happens. And then we have of course, the transport layer, and the security layer is offered by Hashi, which is like aggregates all these different approaches. So you can imagine here it can accept any kind of bridge security implementation like wormhole Axelr, or any kind of other ZK bridge. We have actually right now, three zklite client implementations added there.
02:08:45.082 - 02:09:34.646, Speaker A: And of course we have adapters for all the bridges, wormhole cellar Axelr. So you can add any kind of security layer you need there. And of course there is also healthy competition on this level as well. And actually in the next slide, I'm going to summarize what is the key changes of this model and what are the biggest benefits. So, hash aggregates the security with multiple providers and incentivizes them to provide secure and consistent service to the layer. Also this security, this aggregated shared security is provided to the messaging layer. So different kind of protocols and for messaging protocols, just to make it more concrete.
02:09:34.646 - 02:10:31.214, Speaker A: So you guys understand, typical messaging protocols are connect, hop across and. Yeah, actually many others also socat, I believe. But the messaging layer does not care about security, but it can inherit security from the Hashi layer and they can optimize, they can kind of compete. That's what we mean with healthy competition. They can compete without compromising security and users in the end. And of course, the application layer where there you can imagine a token bridge and a governance bridge or anything else, they can choose whichever messaging layer implementation is better for them or they think is better for them. And now, yeah, the elephant in the room, something that most people would think about, and many do not talk that much.
02:10:31.214 - 02:11:26.826, Speaker A: What about governance and upgradability? Because in all major systems, including l, two bridges, including many Zika bridges, there is the upgradability factor, or there is the governance factor, how we govern this thing. So there is always kind of an external party, it can be kind of multi Sig by many different parties, but essentially, these will control the system. It can break it down completely. So it's kind of, again, single point of failure. So we want to talk and solve this as well. So the vast majority of the bridges and l two s at the moment are kind of upgradable via governance. And there are some measures, including delay on upgrades, for example, and things like that, which are still okay, but we want to go one step further and actually minimize the governance powers.
02:11:26.826 - 02:12:08.800, Speaker A: And here in this diagram, you can see a very simple state machine which describes exactly that. There always be at least to one level of the system, depends on which level you talk about. You will definitely need some governance multisig to define initial parameters or set initial security providers and so forth. So you'll definitely need that. But what you can do here is you can allow this multisig to of course initialize everything on the init state up left. And then you have two states. You have a green state, which is normal operation, and red state, which is kind of quarantined state.
02:12:08.800 - 02:13:12.790, Speaker A: So when the system is in the green state, so kind of normal state, the governance doesn't have any power. It cannot just go in there and change anything or whatever to destroy the system or rag the funds and stuff. But if something happens, and something happens is defined in its state, something happens, meaning one of the security providers does not respond for a specific amount of time, or one of the security providers provides a different response on the others, which means it's compromised. Then the system goes to a current instate where the governance can again jump in and say, okay, now we have a problem, how we solve it. We have to replace this specific bridge with the other bridge, or update a light client, for example. And this is, we think is a very important part of the system. And finally, what we suggest here is a paradigm shift.
02:13:12.790 - 02:14:10.582, Speaker A: So there is the feeling that the crosschain industry in general does not work collaboratively. It's kind of a race to the bottom. Everyone is trying to build the whole stack, capture the biggest market share vendor, lock in the majority of the protocols against the others. In the end of the day, there is always to the expense of the end user with all these hacks and issues that we saw and will continue to see. So our goal here is to build a security first stack that will actually grow the pie, right? So that will make the users and protocols feel more secure, so more of them will go cross chain. More people will start using cross chain systems and bridges. And this is where I think it's a way to improve the system yeah.
02:14:10.582 - 02:14:52.142, Speaker A: So I think the summary is that collectively with all these bridge players and bridge providers, we can define this new stack where everyone equally competes instead of owning the whole stack. And that will lead to better security, bigger volumes, more users and. Yeah, that's about it. Thank you very much. I think maybe because I left. Okay, cool. We have connection.
02:14:52.142 - 02:15:10.540, Speaker A: Do you want to take some questions? We have some time. Yeah, I didn't see any timer there. Yeah, I was about to come run up to you and give you the five minutes, but you have six minutes, technically. All right. Of course, guys. Yeah, there's questions here from gnosis. If you guys have questions, this is definitely the time to ask.
02:15:10.540 - 02:15:35.058, Speaker A: Got our buddy over here running the microphone. We have one so far. Once it's done, you can raise your. Georgia, thank you very much for your presentation. Really good. One fundamental question. In the classic security theory, it says that there are non unhackable solution as long as the rewards of the break in is higher than the cost of the break in.
02:15:35.058 - 02:16:27.054, Speaker A: The reason why we have so many bridges absolutely smashed is because, well, it's massive rewards. If you hack into the bridge in your slide where you say, what is the potential solution? Here you say shared security, which will ultimately lead to one particular solution that will be like a default go to for majority of the bridges. Don't you think that fundamentally it creates ultimate reward for hack of this layer or this piece of infrastructure that will lead to ultimate breakdown of pretty much every bridge that will use it? Yeah, that's a very good question. Thanks for that. Yeah, that's true. And that's exactly the reason that we make this security layer. So this was not kind of a technical talk, but you will see that this layer is so thin, it's on purpose made super thin.
02:16:27.054 - 02:16:57.120, Speaker A: So everything is kind of, first of all, it's immutable. It will be, of course, audited. But if you see the technical part of it, it will be very difficult. It will be stateless, essentially. Right. So, yeah, this gets a little bit more technical, but it's a valid concern. But we think that this design is just better than what we have right now.
02:16:57.120 - 02:17:34.872, Speaker A: Cool. Anyone else? Any other questions in the crowd? Don't all get up at once. I know, but this is, Georgios is a great guy from gnosis. Anyone? You're good. You got taken care of? Cool. You got the microphone back? All right, if there's no one else going once, twice. Cool.
02:17:34.872 - 02:18:03.312, Speaker A: Give it up for Georgios, everyone. Thanks for coming, man. Great job. We have some time, probably a few minutes so lunch is in after this next talk. Oh, you guys are all here. Are you the moderation people? Sweet. Cool.
02:18:03.312 - 02:18:31.492, Speaker A: So I'll invite you guys up, assuming that there will be a bunch of questions at the end of it. Usually during panels there are, but we have panel l two s from the perspective of application builders. Let me just do a vibe check. How's everyone doing? Are you guys alive? You ready for lunch? Thumbs up. Thumbs down. Thumbs up. We're alive.
02:18:31.492 - 02:18:53.330, Speaker A: Sick. I forgot. So I shouldn't be saying the word sick. I learned that that's a bad word in Turkey, so I'm going to do my best to remove that out of my vocabulary for the next few days. Also, heads up to everyone in here. Don't say sick, but, yeah, I'll introduce these guys. We're pretty much on time.
02:18:53.330 - 02:19:08.784, Speaker A: We have our moderator, Luke Pearson. Come on up. Ugor Eskender. Wow, these are really difficult names. Leftetis Andre. I know that one. Alex Cutler.
02:19:08.784 - 02:19:24.312, Speaker A: Alex Cutler. Nice. PFP. You're Luke. Yo. Your picture is way different, dude. You, like, grew up or something? All right, give it up to our people here, guys.
02:19:24.312 - 02:19:39.964, Speaker A: This is artisanal, locally sourced turkish water. Make sure you share one bottle for every two people. It's very. Oh, yeah. Are there enough chairs? You guys can have one each if you want. Okay, cool. Here you go.
02:19:39.964 - 02:19:51.328, Speaker A: This is for you, sir. Thank you very much. Yeah, and then you guys should have your own mics. All right. Sick. Oh, I just said that word again. I'm going to go over here.
02:19:51.328 - 02:20:17.660, Speaker A: Good luck. Give it up. It. Good morning, everybody. Thank you very much for the warm welcome. Thank you very much. It's not on the boardslake, but today we'll be doing what I think will be a very exciting panel with diverse opinions from a lot of different perspectives on applications in and around the industry.
02:20:17.660 - 02:21:44.392, Speaker A: Titled l two S from the perspective of the application builder, we'll be talking a lot about the developer and the user experience, some of the intrinsics and specifics around existing l two s, a lot of where we've come from, where we are, and definitely, hopefully, where we're going. And for that, I have five wonderful panelists from around the industry, and I'll give them their namesake, but allow them to introduce themselves one by one. And afterwards, we'll start to go into some questions. We have really quite a plethora of designers and builders for their particular networks or the particular companies in that some build on specific and some are tailored to individual l two s for different reasons will go in. Some are agnostic. And I think their insights just from being builders alone and people who are building in the app space, not necessarily on the protocol or network space, will give some, I think, strong and interesting answers to the questions so rapidly but enthusiastically prepared. So you have Eskander from Ens, Alex from Belladrome, Alex from Veledrome, Ugo from API three.
02:21:44.392 - 02:23:08.428, Speaker A: I'm sorry the names were hard. I completely concur on that. Andre from Lifi and Lefteris from Roti. Sorry, that's Rotkey, not the food item, which I mistakenly named it. So when it comes to, I guess starting with this broad picture for the broad question, as we're trying to refine into something we'll speak widely about when it comes to building all of these different l two s, we have quite a few considerations, the scalability considerations, the interoperability, the security aspects, a kind of full stack that we're moving towards like a well ossified, highly functioned secure web3 stack, which I think we're on the precipice of, and events like this, I guess representative and displaying of such, but for just each of you. So we can start with a very high level each of the decisions in coming to these applications for the security, for the scalability, the interoperability is broad. What would you say is the most particular and important aspects to your respective case studies, your respective projects when deciding where and how to build on which l two s? Hey, so I'm left teres, I'm the founder of Rotkey.
02:23:08.428 - 02:23:56.828, Speaker A: We don't deploy specifically on any l two s, but we integrate with them. So for us the factor would be basically TVL and number of active users. So we would really care about if there is enough users to make sense for us to spend any development time integrating with a particular l two. Because well, if there aren't any users then there is not much point of us spending time to integrate. Hey everyone, I'm Andre, I'm the head of defi. For Lifi, we are a defi transaction router. So what that means is the first layer is any to any swaps.
02:23:56.828 - 02:24:45.120, Speaker A: The next layer it can be any swap to any kind of LP token, defi, token, crosschain, contract pool. And the last layer is about multi message aggregation. For us, when it comes to deploying on new l two s, it's actually aggregating new l two s. And we take a bunch of things into considerations when doing this, like first user adoption. The next one is how the governance is structured there, and also very importantly, what kind of roll up there, is there? Hey, guys. Ugur from API three. We are an oracle project that mainly focuses on building decentralized and also verifiable data feeds.
02:24:45.120 - 02:25:23.630, Speaker A: For us, the choice, I mean, obviously we want to be everywhere because oracles are key infrastructure that you basically need everywhere you build or defi products more specifically. And where we deploy is we want to be everywhere and work together with chains on that. There's obviously costs involved and overhead. You have specific l two s with certain kinks. The op stack calculates call data differently. So there's like certain things we have to consider, but generally we don't have a preference, so to speak. We want to support anyone that in any way needs data.
02:25:23.630 - 02:26:20.196, Speaker A: Hello. Alex from Velodrome and Aerodrome. If you're not familiar with us, we're the two largest exchanges on op maintenance and on base. We're fairly partisan in the sense of we are extremely bullish on the op stack. And I think that comes down to two things. One is, of course, just that we're very bullish on the underlying technology. I think when you look at the landscape and the players who are basically embracing the op stack, committing to contributing to the, you know, folks like Farcaster, folks like Coinbase, as well as the op team, it makes us very bullish.
02:26:20.196 - 02:27:20.764, Speaker A: Or, if you want to get punny, optimistic about the future of the op stack. But it's not just the technology, right? It's the broader vision I think you're seeing from this team. I think they've got a compelling vision in the op superchain for how this technology will scale and connect together these op stack chains and abstract away a lot of the complexity of movement between them. But beyond just the superchain, it's also this vision of retroactive public goods funding. I think if you look at the last big bull market we went through, right, and this explosion of alternate layer one chains, so much of the fuel behind those initially was, of course, to some degree, user experience. Once you use a fast chain, a cheap chain, you never want to go to mainnet ever again. So Opsec has that in spades.
02:27:20.764 - 02:28:39.656, Speaker A: But it was also this degree of incentives. So these chains were able to dole out large amounts of funding. Unfortunately, we saw what happened once the funding ran out, which was everybody just moved on to the next big area of funding. But with the op stack, the op collective, they have a vision which was basically envisioned by Vitalik for retroactive public goods funding, whereby the revenue that these op stack chains create through fees on op mainnet, on base go essentially to a fund. And this fund, in perpetuity, funds public good builders across essentially the superchain on op stack chains. And I think this is really compelling because, of course, if we're going to kind of reach the next frontier, there's a ton of building that needs to be done, whether they are like user facing applications or whether it's underlying infrastructure. And the thesis behind retroactive public goods funding is it's much easier to sort of look at everything, see what made the impact, and then allocate essentially support to those builders retroactively than it is to do that on the upfront.
02:28:39.656 - 02:29:30.844, Speaker A: And so I think in the op stack chain you've got the best tech supported by a really powerful, ongoing, perpetual funding mechanism that's going to really, I think, accelerate building. So that's my point of view. Hi GMGm. My name is Eskander and I'm the head of product and strategy at EnS. I think the primary component of what we are looking to maintain in our multichain l two strategy is the trustlessness and security of users domains and names. And so the way we look at it is like, even if we're not moving the protocol off of Ethereum, because we want to maintain the level of security and trustlessness there, there are other components of the protocol and the application that can leverage l two s. So for example, on your name you can set your Twitter or your telegram as part of your name.
02:29:30.844 - 02:30:38.188, Speaker A: And then when that component is moved to the l two, we can reduce the cost at that level. So I think it's finding other ways to use the l two s, even if you're not moving your entire protocol over. Thank you. Some excellent answers and I must apologize for what I now realize was a moderation faux par of not letting you hang for a beat to introduce yourselves. But fortunately, you all intertwined some introductions of yourselves and projects well into the answers. I think, thematically, the leftarists Andre and Escanda, being sort of l two agnostic, spoke about this multichain, I guess, direction or approach to a multitude of different l two s and their requirements, or I guess, their interests in deploying on. In this case, all or many l two s differs from it's ugur U-G-U-R uga and Alex.
02:30:38.188 - 02:31:42.836, Speaker A: But I do want to take just for one more moment, a bit of a deeper dive into the technical side. If we could talk just about some of the compilations, some of the optimizations, maybe even the language. Language has been very strong. You see these general purpose vms down to the risk zero stuff the scroll, the halo to the API writing rust API so people can write in these static languages which of those on the compilation, the execution environment and the language is most important for where you're going to deploy and why. I could start from our perspective at Lefi is not that much about the language and the compilation as we're working with bridges and we're working one with native bridges, which we need to aggregate them. However, on the other hand, we're also working with liquidity bridges. So for us it's always tricky when we're looking to deploy on optimistic roll ups because of the dispute period of bridging back.
02:31:42.836 - 02:32:48.292, Speaker A: So we always need other liquidity sources when we're targeting and when we're looking at what other app chains or roll ups to aggregate. So that's the main one for us. Other than that, obviously user adoption, but we're talking about the technical point now, so that would be for us. Yeah, I think if you're able to better emulate the Ethereum developer experience, it makes it easier for when you're looking into what l two s to migrate over to. In terms of optimistic versus ck roll ups, the optimistic roll ups seem to have a bit of a developer experience advantage at the moment, but I would expect the ZK chains to catch up quickly. So I don't think there's necessarily one thing that really is kind of like a linchpin, but it's a compilation of all those things that kind of impact your decision on where to deploy. Yeah, for us specifically, I would say it's that you kind of have the same experience everywhere.
02:32:48.292 - 02:34:12.196, Speaker A: So it is pretty harsh with all these layer twos coming up, if they have certain kinks. So if we need to adjust contracts and stuff just for an l two, it creates like a little bit of an issue and might specifically because it's like infrastructure, might require further testing than if we can basically copy paste EVM code and are sure it's just going to run similarly to how it did on any other chain. For us, it's basically as close to EVM equivalents as possible so that we can easily integrate. From the technical perspective, it's pretty simple because we just do portfolio tracking, management and accounting. So EVM equivalence for us is paramount. Other than that, as we said before, like user adoption, I would just say this is one of the primary reasons why we're focusing exclusively on op stack chains, right? Because for any of the op stack chains that are going to participate in the broader super chain, what you have is a guarantee that the underlying technology stack will always be the same. And so when a new upgrade gets deployed to optimism mainnet, that will likewise be deployed to base mainnet at the same time.
02:34:12.196 - 02:36:08.680, Speaker A: And what we expect to be, of course, a whole thriving ecosystem of op stack chains. And so for us, it makes it very easy if we're building interoperability between these chains, if we have protocols existing on each chain, we're really building just on one core technology stack, and that stack will be upgraded in tandem. And so it dramatically simplifies the sort of experience of building, I think, broadly speaking, perhaps because the examples of scroll arbitrum, many of the large and pertinent l two s, we have relatively low for our panelists overhead in picking and choosing particular ones. You have great systems for interoperability, and I think that's because from really where we were sort of 2016, 2017, along like a spectrum of creating on the far right, really your own custom cryptography for every single operation, even from just the first ever sort of ZK, the digital signature, right up to these general purpose proving everything on the side of some level of compilation, perhaps through like a compiler, which they're doing a lot of the AI operations now, I think nil released a couple of days ago, their l two, that goes down to sort of compiler s, but into LLVM, and then right to the other end, these risk five and EVM proving sets inside of ZK. So long as you stick on the non writing your own custom circuits, for every single cryptography you stay in this side, it seems to be highly interoperable. But perhaps that leads to a new generation of potential ideas if you're able to change the barriers to entry. Some of this market, where you could argue up at this end, if it were to be, you would have something sort of an app specific l three esque.
02:36:08.680 - 02:37:11.644, Speaker A: And down here, stick with your general purpose l two s for your individual experiences and individual applications. Do you see great benefit from highly customizable, perhaps just on doing very fast digital signatures and circuits, and staying away from the general purpose side and canvassing that with what application specific l three s could look like? Do you see that being a future? Do you see that being useful? Do you see some potential use cases around? I mean, the big one right now for some of these pieces is sort of the ML and AI operations. And most low constraint circuits can be proved in the general purpose roll ups that we have. But for the future looking forwards, or perhaps even some needs you have now, do you see a world where the application specific l three s. These customizable algorithms and cryptography specifics are useful to you and your experience. For us, that would be a nightmare. So interoperability is like paramount.
02:37:11.644 - 02:38:15.510, Speaker A: And it's very beautiful that we have managed to achieve this with the current l two s that we have, in whatever sense we have managed to do that. So if you tell me as a developer of an application that integrates with all of these, that then I would have to integrate for a part of the users with a very specific customized l two and then another one. This is terrible developer overhead and maintenance for years to come, which is something that I would absolutely want to avoid. This sounds like a nightmare to me. Would there never be any blazing fast features you can provide, let's say, in the management and tracking of crypto assets? If people are linking this to maybe the next generation of HFTs and really want to request all of that data specifically, especially as the DeFi ecosystem becomes more complex? Perhaps not, but just the first thought, it's already quite very complex. If we take complexity further. Oh man, I don't know.
02:38:15.510 - 02:39:13.220, Speaker A: Yeah, I mean, it is possible to integrate new stuff and new revolutionary stuff, but there should be some kind of standardization around them. Not like one chain goes and does of their own thing, and then another does and goes and does of their own thing. Eventually, if users request it, and if there is many, many users that want this new thing, then it would happen. But it would be nice if we try to achieve some kind of standardization and interoperability, even in this new stuff. So it feels like a bet that you always need to know which scaling solution is going to be the winning one. And I think that in the long run that hurts the ecosystem. Of course it makes it more competitive, and only the best ones will be alive in the next five years or so when the industry hopefully will be a bit more mature.
02:39:13.220 - 02:40:17.560, Speaker A: But even for us at Lefi, it's about liquidity, fragmentation, and having more l three s and another l two and another tech stack on optimistic roll ups. It doesn't make it easier. One for the developer experience, and two also for the user, because we're pun intended, the bridge between developers and the users, and for the developer side of things. As you said, there are a lot of text, text, but also for the user. And I think that's the most important thing that we should focus on, that the user doesn't know which one to choose, doesn't know where to go, doesn't know how to bridge there. Especially hard to bridge out of that ecosystem and also from the power user, the research perspective, that's even harder because it's so much more to comprehend. So coming back to your initial question, in my opinion, there are way too many right now.
02:40:17.560 - 02:41:25.680, Speaker A: And I think that for the sake of the ecosystem, we should focus on what we have right now and scaling ethereum, because I guess that's what we're all about here. With what we've got right now, we're on a very good track, but I wouldn't want to over exaggerate where we are right now just for the sake of raising more money for vcs for a new tech stack that is very similar to others that we have right now. Speaking from an infrastructure perspective, just I think if you want to do custom things with an l three and stuff, it might be beneficial, but you will have to consider that the infrastructure needs you might have, but also like development needs, you're overcomplicating things. So attracting it's already hard enough for current l two s that are very general purpose to attract, let's say all of the infrastructure players like you have to pay oracles, maybe third party bridges and so on. And all of this gets even more complicated if you then do some custom stuff that they need to adjust for. So attracting all of that, let's call it like infrastructure capital that you need to really take off. And we see that across the board.
02:41:25.680 - 02:42:07.720, Speaker A: You need a certain basis for people just to get started and overcomplicating. That might be a bit of a stretch, but it also depends essentially what you want to achieve. Like do you need certain players, certain tools available on your l three that you want? Otherwise, I would say I also support that opinion. I think we're currently diverging into. I actually don't know how many l two s are currently launching. I lost complete track, but it's getting a bit from an infrastructure provider perspective. We're currently focusing on being able to support 200 chains or something like that's where we see this going, unfortunately.
02:42:07.720 - 02:43:11.970, Speaker A: So maybe it is time that essentially the industry, like you mentioned, decides to focus on something, because otherwise we will end up having to provide, for instance, price feeds on like 200 chains. Yeah, I think this is where the vision around the op stack and the super chain becomes very compelling. If you look at just two examples of protocols projects that have decided to focus on the super chain, so farcaster and worldcoin. Right now they are deploying on op Mainnet, but they know to scale their products, right. They are going to need dedicated chains to be able to provide the kind of speed and thorough put that will actually support their products at scale. So the first step for them is deploying to kind of one of these hubs of the superchain. So op mainnet base mainnet could be another.
02:43:11.970 - 02:44:37.704, Speaker A: But as the underlying technology that will underpin interoperability between these begins to cement, and I expect we're probably hearing a little bit about this at the superchain summit nearby here, they will evolve into their own chains, but the goal is to abstract away the feeling that these things are not connected to one another. And so for us, as a decentralized exchange, supporting liquidity needs across these, that's going to be super important, because we don't expect that a very application specific l two built on the op stack integrated into the super chain will need all of the core infrastructure on that chain. What they will need to do is be able to access that infrastructure elsewhere and do it in a way that is hopefully abstracting away a lot of the complexity. And that could be just liquidity on a token. That could be any number of different factors. But yeah, if we're going to build it all on Ethereum, and you're going to have a social media app on Ethereum, basically you're going to need that sort of dedicated l two playing with some of those assumptions, but with all the technology and security guarantees all the way down. Yeah, I think there's definitely enough block space today for application builders to kind of prove why they would need an app specific chain.
02:44:37.704 - 02:45:48.212, Speaker A: So I would say like focusing on the generalized l two s and then growing your application to the point where an app chain could make economical sense, especially if you're not intending to be generalized like as a roll up would be the next step. So don't really see it today, but I do think in the future, as cost comes down, it could make a lot of sense depending on the use case. I think hopefully correctly, that all of the answers embed themselves around the same sort of pointer or opportunity cost, with more factors beyond just what are we building and how many do we have and how many users do we have in the web3 space. It's be almost better to see how these paradigms be shifted and how people would think differently if we did have a much larger growth or a much larger interest in the number of users in web3 as a whole. I think for the l two s that exist, there is a massive need to solve things like liquidity. Fragmentation was mentioned, user fragmentation was mentioned. And I think we should definitely come to that next and see what your considerations are there.
02:45:48.212 - 02:46:52.312, Speaker A: But I kind of personally think that some of the sort of the killer app things that made some of the l two s what they are will come with the top end or the far right. As I was waving my hand, real custom algorithms, perhaps, for gaming l three sharding games. If like a ubisoft or a rockstar want to do it, they would kind of do it independently, perhaps move away from ethereum l two, or hedge funds when they're sort of getting into the game of moving capital around validator networks to take as much mev. I think that will be a lot of in house specific. But now, especially given so much of this industry as community first and open source, and for everybody sticking down that end and solving the interoperability of the fragmentation of the existing platforms you have is a good focus and also changes, as what was said before, users, so on. User fragmentation. I know for the sort of chain specific panelists we have, the answer would be a little different.
02:46:52.312 - 02:48:19.508, Speaker A: And for those like Rotke and Li fi, the answer would just be more about user fragmentation, but solving this cold start problem, understanding what user fragmentation looks like in different settings, what are the key features that you think about from or were, and why to deploy on specific l two s in the mind, beyond the builder, beyond the developer of the user, and how it affects them in different cases? I could start. So user fragmentation is probably up there with liquidity fragmentation because it's basically a derivation of liquidity fragmentation. And we're treating that as thinking that multi hopping will solve this partially. So by multi hopping I mean you can start from any l two and end up on any l two, or you can settle on mainet and you can also go to non evm chains, because those are also some sort of scaling solutions in themselves. Not scaling for Ethereum, but scaling for the problems that ethereum sort of say are out there. So multi hopping solves for this. Using an aggregator solves for this, because the user, and user fragmentation comes from the fact that the user doesn't know which bridge to choose, which decks to choose, which is the best price, what route to choose.
02:48:19.508 - 02:49:26.312, Speaker A: So that's why for us as an aggregator, that's our whole value proposition that we want to solve for user fragmentation first and then for liquidity fragmentation. So it's a very tough business when you have more l two s popping up and everyone wants to have those users that we have and to offer them basically the portal of hopping from an optimistic chain to a zk one and back and forth. But it's hard to keep up with that. And that means that it's harder for us to cater for the users in the same way because we have user demand, but there isn't enough user demand for all the new l two s out there. And right now covering the main relevant ones, it does the deal. But if we're going to stay on the same pace, it will be very hard even for aggregators to keep up with this. So this is how we're seeing this of trying to solve, by aggregating all the liquidity out there and all the deck solutions for one.
02:49:26.312 - 02:50:29.948, Speaker A: And secondly, thereafter is also about making it very seamless. So by seamless I mean the user doesn't only have to bridge and swap, but they can also do some kind of action on the destination chain, like lping, buying a defi token, repaying a loan. And we think at our heart that this should be done in one transaction, abstracting everything away. The user just wants to buy a coin on velodrome and he now is on ZK Singh that we can make it possible. But if more and more chains are going to pop up, it's going to be harder to keep up with that. Yeah, I want to pick up on that, because user fragmentation also means tooling fragmentation. So for us, as a tool that basically tries to understand transactions, decode them, show them to the user and do accounting for them, we also need to rely on some tools.
02:50:29.948 - 02:51:21.650, Speaker A: So if there isn't a way to easily query transactions for etchane or get some specific data for an address on a chain, then it's actually impossible to integrate etsane in Rotky or in any other tracking tool. So when a new l two is deployed, it should come with this stuff. But unfortunately it isn't even for the users. This is not so much rotky specific, but for example, with the new gitcoin stuff, they have moved some things to the public goods network, the PGN. But users are really complaining. They say it's not safe. I cannot use the safe, the multi sig in PGN, there is no infrastructure, so safe has not made their UI work there and users can't create a new safe there.
02:51:21.650 - 02:52:30.420, Speaker A: Also, users don't want to breach to new chains. There are users complaining, why are you switching us from optimism to have to go to a new chain? How do I breach there? So all this user fragmentation is just a problem that we could just avoid by. Well, as everybody said, just have less l two s. We don't need a new l two for donating to public goods I mean, that's silly. I guess I would support the stance that Lifi has that it's going to get increasingly harder for someone like a bridge aggregator and also like infrastructure providers to support all of these needs that you have. And the only thing that we're enabling by supporting all of these routes is actually user fragmentation and liquidity fragmentation. If we enable oracle pricing on another l two popping up, you suddenly have capital moving there, which essentially creates a bit of an issue because you're essentially taking liquidity away from somewhere and rates get worse for other people and it just creates more issues.
02:52:30.420 - 02:53:38.540, Speaker A: But at the same time, we can't ignore it because there's demand or supposed demand being created. And all of this kind of supports the narrative that people are going to have to jump through hurdles and you're going to have infrastructure needs at every chain because people like I also mentioned, are going to abstract the way that you're using multiple chains. That just means that we're going to most likely in the future have to infinitely scale because everybody wants their own stuff and everybody wants their own stuff being supported by multiple infrastructure projects. Yeah, I think it might actually get even worse in some sense. So like the l two s are all their own kind of distinct homes. And then now with smart contract accounts, we're going to have another additional layer of user fragmentation. So I do think there's some responsibility with the l two s and account abstraction builders to really be thinking about how users aren't having this fragmented experience and that their ability to have account management across all these different networks, wallets, applications, et cetera, feels cohesive.
02:53:38.540 - 02:55:17.144, Speaker A: Yeah, it's interesting. So the moment we came from right, where we had all of these different alternate layer ones, we had some emergent layer twos, we had ethereum main net. It was an incredibly fragmented experience, right? Even chains where you'd have to go through so many steps if for some reason you had some tokens over on Phantom and you wanted to get them all the way to cosmos. Right. How frustrating and painful of an experience was that to do? So I think that one interesting thing that I think is happening is that with the decline of l ones, and I think the alternate layer ones, I should say, is that folks sort of realize like, you need to have a compelling reason to exist, right? Why does crypto need a phantom right now? Why does it need a Solana? Why does it need an avalanche? And if they don't have strong answers to those questions, we've seen what people default to, which is we will go to ethereum. And why are they still living on ethereum? Layer one, the biggest answer you get if you talk to major builders or major capital is security guarantees, right? It is simply still the safest place to be. So what happens when, say, the op stack rolls out fault proofs and adds additional security guarantees? We know the user experience is better, we know the rewards and incentive systems are better.
02:55:17.144 - 02:56:36.852, Speaker A: Add on top of that eip four four, which is going to drop costs on these layer twos another ten to 100 times. And I think we could be seeing similar to what we saw with the Altel one boom, the sort of layer two boom, where even in a market where a lot of users or a lot of capital may be migrated off chain, those who remain choose to focus and to concentrate on layer two, ethereum, in the same way. They saw sort of a lot of concentration on these all layer ones early on. Because the goal, right, is that the user base, the capital base, expands exponentially. But in the meantime, in these dark months, it doesn't mean you can't find the pockets of growth. And I think if these layer twos are able to make users and builders feel much more secure, if they're able to continue to improve on speed and cost, even if it's just us sticking around for now, I think we're going to choose a few hubs to focus our attention on, and then the network effects of that will be such that it will sort of accelerate that concentration. So there might be a ton of l two s, just like there were a ton of l ones.
02:56:36.852 - 02:57:18.208, Speaker A: But that doesn't mean everyone's going to be on every single one of them. They're going to be the place where the users are, the builders are, the rewards are, and I don't think that'll be across 100 chains. I think there's going to be some clear winners. Yeah. I would also quickly add, I think, like, off chain computation and off chain liquidity are probably, like, underrated areas to help solve kind of user fragmentation, because even if they're locked kind of in your ecosystem, you're still able to offer a consistent experience in some degree based on the application. Yeah. So just to add one more thing about.
02:57:18.208 - 02:58:25.928, Speaker A: So we see a lot of data about volumes and how users are migrating from one chain to the other. And the question about user fragmentation is being addressed more on the new users and on the users who are basically non ETH wells. And I'm telling you this because when Makerdaw's EDSR was launched, we tracked how the movement of ETH moved from Mainet and from the other chains, from other l two s back to Mainet to basically get the EDSR. And I can tell you that most of the ETH was ETH that was already on Mainet. The amount that was coming from scaling solutions like optimism or arbitrum was negligible, not even an 80 20. So the user fragmentation is definitely for the new users that are coming, not for the big wells, the east buyers from 2016 and so on. But that's who we should have in our target, because the big users, the wells, they don't really care anyways about this.
02:58:25.928 - 02:59:25.336, Speaker A: But the new users, the new wave of 1 billion users will definitely care about costs and how gas efficient a new chain is. I think costs definitely or unanimously underpin a lot of it. As soon as you have a single, as Alex said, sort of extra hoop to jump through, you veto any other considerations. But I do think once you pass that barrier, once you, as you said, are in Ethereum and you feel safe, you feel secure. Of course, amongst the platforms and networks we've discussed, it's the one that has naturally stood the greatest test of time. It's the one where there's greater confidence and even see from the pure staking reward being the quickest TvL growth in any particular defi application, that people take their heart towards that. And that is because it's been there for so many years.
02:59:25.336 - 03:00:18.084, Speaker A: But I think another thing that underpins that, and it's a perfect example, or I guess sort of a counterpoint to give to what lefterist said, is there's a subset of people who take a strong sentiment of community. A lot of what we're building, everything that we've built on from 2009 has arguably been an effort of multiple people who perhaps didn't know one another from around the world coming together. This industry is rife with public grants. To take something like the Gitcoin network. People don't want to move on, understandably, for going on to some multi sig or some m of n and would rather not take those risks. But the underlying grant system itself does very well. It's funded an awful lot of projects.
03:00:18.084 - 03:02:09.404, Speaker A: If you can get past that hoop and you can kind of plateau the security concerns and plateau any kind of extra hurdles to pass, I think community and governance is a strong point for some of the triumphing l ones. Sorry, l two s, also l ones, actually. Ethereum foundation being so well decentralized and still being yet the monolith that attracts and keeps the whales and gives people that security on the point of decentralization governance throughout different l two s, have any of the mechanisms, should we say, the past releases and the cascade effects of that determined and defined your choices, or even perhaps a perspective that you'll have into a user's choice to work with, develop on and for the user's desire to be in and around those platforms. For us personally, for us as rotky, it doesn't really matter because we don't deploy. But as a user, I can tell you that when you look at an l two, you really have to consider the worst case scenario. What happens, who has control, who can stop the network, who can freeze transactions? And then in the optimistic roll ups right now that they have started having. So this progressive decentralization of the sequencer of everything some have introduced security council, you should look at who is in the Security council, who has the ability to veto upgrades, all these, they may sound extreme, but our entire field is built on what do you do? What happens when the unthinkable happens? And it has happened many times in l ones, in l two s.
03:02:09.404 - 03:02:53.980, Speaker A: And when it happens, we need to be prepared. So you need to think, who are you trusting with your money when you breach? Because Mainet is mainet, but once you breach it, there is security concerns and you really need to consider them. And I also want to take that community thing that you spoke about a bit before, just to not be misunderstood. The Gitcoin community is in many, many chains, but lately there has been this push towards trying to make everyone go and breathe into a new network, this PGn network. And there has been a very big pushback by every user that I know of various. So both of rotkey, but also of other grants that they're like, I just don't want to breed to an unknown network to donate. I just want to donate.
03:02:53.980 - 03:03:24.244, Speaker A: And plus, there is no tooling yet for this place. You can't make a safe, there is no nice explorer and so on and so forth. So you have various communities and you can't force them to move somewhere. This is something to really consider. You can't just create a new chain and say, okay, you guys just go there and this will happen. The community tends to be created and stay on a particular place. When you say, okay, we are in l two.
03:03:24.244 - 03:03:47.660, Speaker A: So we went out of Mainnet, we went to Optimus, we went to arbitrum. Then you can't say, okay, just move a bit further. And then further and further, you lose the users. The users say, I don't want to do this, seriously. We had users tell us, just give me a normal donation address in optimism or arbitrum or anywhere, an EVM address and I will just send you money. I don't want to breeze. I don't want to use Gitcoin anymore.
03:03:47.660 - 03:05:21.470, Speaker A: Yeah, well concur with all of that. And to add, and maybe it'll help with the following answers of the same question. Sorry is it was more to Alex's the remaining 1 billion bringing them on, you have that from the developer perspective, what community and governance initiatives and sentiment magnetize you to building on these and for the remaining users, what community and governance pulls them towards? And I think, well, I completely agree that the security concern is of course, or should definitely be the largest one if you're playing around with your own funds. And naturally, the transparent, well governed practice platforms with usually open source approvers, designs that have been building in the open and with other people there to subject to great scrutiny, fall into the line of those that have a more solid community and keep those users. But I agree that you have a center of users growing outwards and not necessarily needing to pluck from others, but it was for new users onboarding one aspects of community governance, magnetize them to different. Yeah, I mean, I think it's incredibly important and it's super interesting. We've been very close to as left Rose has as well, to the evolution of governance on the op side, right? And governance is never easy, it is never clean.
03:05:21.470 - 03:06:20.092, Speaker A: It can be deeply, deeply frustrating. But if you get certain things right and you continue to grow and evolve, it can be incredibly powerful. If you look at the evolution of how optimism has leveraged governance to basically oversee the distribution of grants. So these are both the sort of proactive kind of grants in the terms of governance or growth grants. So for new projects onboarding, for new tools and infrastructure onboarding, as well as retroactive public goods funding, which is like 30 million or something like that, op tokens are about to be distributed purely through the votes of badge holders. I mean, you get this sense of how incredibly powerful these decentralized governance systems can be. And there's a strong incentive, extrinsic incentive, right.
03:06:20.092 - 03:07:49.304, Speaker A: If you are a builder in the space to want to be a part of that, and especially if you're somebody who in your own life, in your own country, hasn't had that type of participatory, I guess, experience, it's incredibly compelling, right, that you can just show up one day on chain, right, start playing around, get some tokens, start talking about things and turn yourself on the optimism network into a delegate, right? Get yourself elected to a committee and before you know it, you are helping to drive major decisions on chain. And I think that will keep you involved, right? That will keep you kind of both intrinsically and extrinsically motivated to support the growth of that network. Like all of our incentives, as Belladrome is the largest exchange on optimism, are aligned to growing that ecosystem. So much so that we have a team member who we focused 100% of his time, this is Jack Anorak, on participating in governance, because our survival depends on the survival of optimism, right. And the broader collective. So when we are trying to onboard new protocols and they're saying there are major infrastructure gaps, we need somebody in governance helping to solve for those. So what does he do? He runs for the infrastructure committee, starts allocating grants towards solving the problems of the protocols we're looking to onboard.
03:07:49.304 - 03:08:25.684, Speaker A: And I think that's incredibly compelling stuff. Right? And that is a whole nother reason to get involved in a particular ecosystem or to stay involved in a particular ecosystem. It can also be very alienating. I think you're actually seeing the flip side of this on arbitrum right now, where by virtue of how they allocated their tokens, so basically control over the massive amount of arbitram funds that they have, they gave it to all of the existing protocols. And what are all of those existing protocols doing right now? Allocating themselves more tokens. Right. It's nuts.
03:08:25.684 - 03:09:43.168, Speaker A: And so when you're a new entrant in the space of arbitrum, what you're seeing right now is yourself being actively boxed out, you're seeing protocols, massive protocols, vote against a competitive Dex, getting a grant or things like that. And that's the kind of flip side of governance where if you don't get these things right, it will turn you off to an ecosystem. And the tough part is, once those tokens are out there, once the sort of parties are aligned, and if it's kind of not distributed, well, that could be the death of community governance on that ecosystem, because people have enough power to keep themselves in power in perpetuity, to box out others. And I think that's going to be the reward for more open ecosystems like optimism, because we'll happily bring those folks over and get them building on op and on base. So I think that governance is about incentives and rules. I really like personally the law of change from optimism. I think that that is doing a very good thing at painting the picture for the next years of how governance should look like that's for one and the other one.
03:09:43.168 - 03:10:27.016, Speaker A: As Alex said, it's about incentivizing the Dapps and the projects and getting the builders there. But in the long run, that's not sustainable. You can't just mint tokens as a new scaling solutions and give them out infinitely. And I think we've seen that pretty well in the cosmos ecosystem that worked. And of course, they have a different mechanism there. But it gets very political at one point where, let's say that the rewards do not make up for the validators or the builders to stay on one chain. So governance done right can help the ecosystem.
03:10:27.016 - 03:11:31.796, Speaker A: However, governance done wrong by incentivizing badly has repercussions in the long run. I think governance is quite important. We personally prefer, let's say layer twos, where there is an open process for things to discuss. And I think if there is an open process, it also opens up just way more possibilities. Like, even if a proposal gets voted down or people argue against it, there is more people willing to put an idea out there that might be discussed than if you have to approach, let's say, a layer two and ask for a grant from a specific person in the foundation that then makes a coin flip on the decision. Like, if it's out there in the open and you can openly discuss things, it is way more likely that you might actually end up adapting something that wouldn't have found the way to your chain otherwise. Yeah, I think for application builders, the best model for me in terms of how to select it is to kind of think of the governance, the community, and the core engineering team.
03:11:31.796 - 03:12:24.950, Speaker A: Almost the way you would view the Apple App Store is, do all the benefits kind of outweigh maybe potential misalignment in terms of values? And if so, that's kind of the framework you need to work through to decide if it's the right place for you. I think having a relationship or developing relationships with those teams is better for your protocol or your application to survive. Because when Apple changes their policies, everybody kind of checks to make sure that their application is okay, and we should all be doing that. So I think it's great that you're actively involved in governance. And I would encourage all application builders to really look through the history of governance on the network that they're looking at deploying to and making sure that it feels like the right place for you. And if you will have questions come up and those teams will respond. So I think that's an important aspect to consider.
03:12:24.950 - 03:13:59.024, Speaker A: I think we have this sort of perpetuated joust between the accountability which are brought up so much, which is just so important, I think I would hopefully and tentatively say that some of the, I shan't name them as they were before. L two platforms that made XYZ governance decisions were in good faith and people are just doing a lot of testing the water were immersed very heavily in a brand new coordination mechanism and trying to figure out how to organize ourselves in smaller groups. And no one's getting it right but iterating towards the right model before even picking on the other side. I mentioned first of the features is having serious accountability, having security councils, and I think also making sure that the holders of large swaths of tokens is really decentralized and they aren't. One thing you'll never be able to do with something like voting in with ZK is colluding off chain and making sure things like this don't happen and don't influence the eventual outcome of our industry. But moving towards that I think is going to be handheld. Well, with just better understanding of long term economic models for token cycles, understanding the best way to ensure the projects which are built on particular l two s or future thinking projects for grants and for dows, et cetera, remain sustainable is not yet figured out.
03:13:59.024 - 03:16:22.964, Speaker A: I still feel in the same area of food for thought for where we have a lot of these governance proposals and I don't know, check a lot of the forums yourselves and you'll see this aggressively ongoing but converging together is something I think we can only do with transparency and public accountability. I guess there would be a couple more questions on the future, but given we have seven minutes, if there are any audience questions for panelists or perhaps myself, we can move to those. Yes, we have one. Ah, you talked about governance and I'm a little concerned about the model of governance in l two s and even defi because all governance mechanism are based on tokens which is going to be centralized because I think, as Vitalik says, trillion dollar companies can buy tokens and write the votes like Abracadabra if, you know, win the governance on curve to list meme token in triples. And I think for mass adoption we need personal vote for consensus mechanism for governance. And we hate central system, but central system accept humans as privilege to vote. Why don't we have such a thing in l two s or other consensus mechanism for governments? I'm not sure that everybody heard you well, but I think you talked about the problem of plutocracy in token governance, right? Okay, it's a difficult problem, but one thing that's actually beautiful is how different l two s are trying to address this.
03:16:22.964 - 03:17:28.600, Speaker A: So one l two that I have our friend from velodrome mentioned is optimism ecosystem. They are trying to create a system of sections, balances. So you have the token house, which is the plutocracy, let's say the token holders, but also the citizens of the ecosystem of optimism, which are so I think they call it by chamber or bicameral, I don't know how you call it governance. So a system of checks and balances between the token holders and a set of people who are the citizens of the ecosystem. And this is supposed to try and help fight against capture by external influences. And also they are trying right now to create an anti capture commission so that the ecosystem doesn't get captured by a specific actor. Isn't going to be some kind of people based voting system instead of token based voting.
03:17:28.600 - 03:18:09.530, Speaker A: So I don't want to speak for the optimist ecosystem because I'm just a very active participant, but I'm not part of the foundation. But I think that this is what the citizen house is supposed to be because they are all recognized people, so you know who they are and that they are real people. Because the problem of per person voting is the civil problem. You have to be able to know that the other person is a real person. But I think that the idea of the citizen house in the optimist ecosystem is exactly this, that they are real people, trusted people in the community. That's how I understand it, at least. Thank you.
03:18:09.530 - 03:19:38.192, Speaker A: Anyone else? I have a question, if that's all right. Is it all right with you guys? Anyone else have a question? Let's say you're someone that is building a DFI protocol on Ethereum and you want to break out into like a layer two situation, but you don't know necessarily which route to go down. You guys were talking about all these different layer two projects that are coming up and all the 200 plus different chains. It's definitely a tough choice. Do you guys recommend a process that you would go down in order to make that decision? Are there pros and cons to each solution? What are your thoughts there? So I think that one is the liquidity that's on the said scaling solution and the other one is obviously about incentivization. So coming back to governance, that means who will give you the biggest grant should be a big part of that decision matrix that you're doing there, obviously as a new DAP looking to launch on a new chain. And besides that it's also your cryptographic preference of ZK roll ups versus optimistic roll ups.
03:19:38.192 - 03:20:09.304, Speaker A: But that's, I guess, like a very research driven decision. If you're making a business research decision, it should be about liquidity mostly. Go ahead. Yeah, I was just going to say, if you're in the position of evaluating, reach out to us. I'm Wagme Alexander on Telegram. Or you can ping us on the velodrome or aerodrome accounts. I think optimism and base is doing something very unique.
03:20:09.304 - 03:20:51.492, Speaker A: Optimism main net in particular right now, we try to serve basically as the front door. So if you are evaluating l two s right now, you're wondering where to deploy, reach out to us. We will walk you through what the governance and grant landscape looks like on optimism. So you can have some idea of what things look like there. We make it very easy by virtue of our protocol to bootstrap initial liquidity. And there are a number of programs that Jack Anorak and others in optimism governance have been working on to make it very easy to deploy as well. So, for instance, one thing a lot of builders struggle with beyond just liquidity is security in the sense of being able to fund an audit.
03:20:51.492 - 03:21:33.768, Speaker A: This is a major pain point that we often heard. So beyond the idea of you getting perhaps initial incentives to deploy on optimism, optimism is now helping to support, fund your audits. So I think there's a ton of compelling reasons to come build on base to come build on optimism mainnet right now. And so if anybody's evaluating, reach out and we will help you walk through it all. Cool. Thank you. Yeah, just to add on that, I mean, obviously you have a base requirement of, if you're a defi adapt, do you have all the tooling on the chains you might want to deploy on? Even though financials might be a big point.
03:21:33.768 - 03:22:24.440, Speaker A: But if you're missing, for instance, you're deploying two oracles, like something like liquidity, and you only have one there, you're obviously not going to be able to. And you're driven more towards it's more, can I do it? And then I would say the financials come into place would rather be how I would define it. Cool. Yeah. And then I would just say once you've identified the problem statement of why you're moving over to the l two, kind of going through the research design and build phase to understand optimistic for CK, is liquidity or a simpler deployment process more important to you, or is privacy on a ZK more important? I think there's like a rush to move to l two. But I think it pays to kind of be tactical about how and why you move over. Because it has real long tail effects on your application.
03:22:24.440 - 03:22:56.016, Speaker A: Right, exactly. And that's the thing. It's like, because once you make that choice, you kind of are stuck going down that alley road, whatever you want to call it, and you don't want to make the bad decision, depending on your situation. Yeah. Thanks for the lengthy answer. I'd love to grab your guys'telegrams after and we can chat in the dms, I guess. But if there's anyone else that has questions, let me just do a time check real quick.
03:22:56.016 - 03:23:10.100, Speaker A: I didn't even. It was sort of two minutes over time. Sweet. All right, well, let's clap it up for this panel. They were awesome, actually. Awesome. You guys kept us awake before lunch, so we have lunch right now.
03:23:10.100 - 03:23:20.584, Speaker A: I believe it's served out in the main hall, which is, like, directly behind us. Thanks, guys. Appreciate it. Appreciate it. Let me grab your telegram in a little bit. All right, see you guys. Thanks.
03:23:20.584 - 03:23:39.220, Speaker A: You guys rocked it. Thank you, man. I'll get yours, too. All right. Oh, someone left their backpack. Gentleman Luke. Yo, can you guys grab Luke, make sure you guys get caffeinated and get your food in.
03:23:39.220 - 04:14:13.230, Speaker A: It's don't Namara. I know there will be a shaman it I hear you. I knew new revolution maybe get again where do we live you wonder when do we leave you stronger it's sa passing by believing was just and then begin again cool. When do we leave you stronger close where do we leave you? When do we leave? It's behind and to my home do my bro me and me that's what I'm too much for me to cry do my promotory to my promise to my pomi close do I thought the dip to you I thought I'm surviving every lonely day when there's got to be no chance for me my life would end and it doesn't matter how I cry my tears so far are a waste of time if I turn away am I strong enough to see it through? I can go crazy is what I will do if I can't have you I don't want nobody, baby doesn't matter how I try I gave it all so easily to you my love to dreams that never will come true I'm strong enough to see it through and go crazy is what I will do if I can't have you I don't want nobody, baby if I can have you if I can have you I don't want nobody baby if I can have you I don't want nobody baby if I can't have you if I can have you I don't want nobody say that if I can have you sa I tried to get through your phone is quiet and so are you the consequence of my demand or what a name we've been together so long the story moved on but we denied the love we felt that risky side of sentiment oh, what a man oh baby, come back to me leave on me come to see that I'm addicted to you that I'm addicted to baby, come back to me please darling can't you see that I'm addicted to you that I'm addicted to you come back oh baby come back to me please don't make it come that I'm addicted to you that I'm addicted to you oh baby come back to me please darling can't you see that I'm addicted to you? That I'm addicted to you sample I can't remember when it started to go but when I look in the mirror tracing lines with a pencil I remember what came before I wanted to think there was endless love until I saw the light dim in your eyes in the dead night I found out sometimes there's love that won't survive New York City, such a beautiful disease New York City locked up in a box behind her closet door she pulled down the blinds and listened to the thunder with no way out from the family stone we all told how things could get better when you just say by I lay awake one more night close beautiful coming and did I mention notes that I found taped to my locked front door? It talked about no regret as it slipped from my hand to the scuffed tile floor a train for hours on end watch the people pass me by it could be that it has no way just an action junkies love I New York City such a beautiful disease New York City such a beautiful such a beautiful New York City such a beautiful New York City such a beautiful such a beautiful I can send no more, no because I can stand it no more no I can stand it no more no I can stand I tell you I can stand I can stand I tell you I can think I can standing no more, no I can stand it no more, no I can stand it no more no touch of stranger time spinning round, round in space then it's gone without a tree, you I wonder where deep in the night when I hear the sound it cross the page trying to spell your name it so I fold it up and I flick it out paper home plane, you won't fly. The cell phone cease to you because it didn't live my room, but it awaits the hand for someone else. It garbage man got Satan, so he opens it up to all his friends amongst the crowd. A heart bright and a heart will men walks on home side from work.
04:14:13.230 - 04:15:27.190, Speaker A: The letter falls from his hand. He reaches out only to catch the sky. It's gone with the wind. Gossip said I spilled it across the back trying to spell your name. Up and down, there it goes. Vape arrow plane. It hasn't flown several seas to you.
04:15:27.190 - 04:17:46.610, Speaker A: But as on and sway. It goes through the hands and someone else to find you, girl. God said me little psycho girl in bed with a boy for the psych cowboys looking for love. All the psycho girls with beautiful words. Always surrounded by crazy ones. Close, don't ducko girl. And there was a boy for the sack cowboy looking for love.
04:17:46.610 - 04:18:59.770, Speaker A: All the psycho girls. Beautiful world. Always surrounded by crazy ones. All the psycho girls are suffering toys giving your body whenever you soul. All the psycho girls with beautiful words. Always surrounded by crazy ones. You jump.
04:18:59.770 - 04:20:18.962, Speaker A: You guys want to turn me on? Yeah. What's up, everyone? How's everyone doing? Yo, get out of here. I know those guys. Yes. This is so cool. Wow. Hey, you guys made my day.
04:20:18.962 - 04:20:44.966, Speaker A: What's up, everyone? Welcome to post lunch afternoon session. You guys feeling energetic and alive? Like? I sure am. I got my red balls here. You best believe I'm getting pumped up for Vitalik coming up next. Yeah. God, you guys are so quiet. Yeah, we'll kick things off.
04:20:44.966 - 04:21:04.734, Speaker A: I think we have modic right now, and then in like 40 minutes after that, we'll have. You could do a q and a, too at the end. Like ten minutes. You know what the spiel is here. I don't need to tell you guys. Whose first time is it here in Istanbul? First time? Me too. Oh, my God.
04:21:04.734 - 04:21:18.898, Speaker A: How crazy. By the way, don't say the word sick. It's a bad word. I've been saying that a lot this morning. So just so you know, it's a bad word in Turkish, but. Oh, my God, look, it's me. This is cool.
04:21:18.898 - 04:21:35.974, Speaker A: All right, we're digging it. We're moving right now. Moving and grooving. I hope you guys have a great time in Istanbul. This is a massive venue. Please take some time to walk around, check out the place downstairs. We have workshops and coworking spaces upstairs.
04:21:35.974 - 04:22:05.060, Speaker A: There's like a chill area, some other stuff. Oh, yeah, there's some more sponsors upstairs, and there's also impact folks downstairs too, so you guys can check out all the folks that kind of got like a free sponsorship. I mean, they're just dope projects that didn't have budget, so they're part of the impact tier. But yeah, I'm going to stop speaking so I don't cut into Modek's time, but this is Madik, everyone. Give it up for Madik. Thank you. Thanks, man.
04:22:05.060 - 04:22:33.454, Speaker A: Awesome. Well, it is so wonderful to be here today. It is also my first time in Istanbul, so it's a real treat. And today we're going to be talking about digging into eigenva from the perspective of a future. L two, I'm going to use a lot of emojis today, and so if you like the format, please let me know. I'm always trying to experiment with different formats, so please let me know. Cool.
04:22:33.454 - 04:22:57.086, Speaker A: And I wanted to start on an interactive note. Kind of curious here. How many people here have a sense of what is the number one chain when it comes to daily active users? What do you guys think? Any guesses? Polygon. It's not polygon. Binance. Smart chains. Not binance.
04:22:57.086 - 04:23:10.482, Speaker A: Smart chain. Tron. A lot of people yelling out Tron. It is in fact, Tron. Tron is. Yeah, this is surprising for a bunch of people. It was surprising to me, too.
04:23:10.482 - 04:23:40.210, Speaker A: Tron has 1.5 million users, daily active users. And I don't know about you, but that makes me sad. I'm here this week to talk about scaling Ethereum. I'm really excited about the Ethereum ecosystem. And so it's a real shame that even when you add all of the l two s combined plus Ethereum, the Ethereum ecosystem still doesn't beat the 1.5 million daily active users.
04:23:40.210 - 04:24:30.642, Speaker A: That's a bit of a shame. I'm not the only one that has noticed this. Anthony Sasano recently tweeted this, pointing out that the biggest payments chain in crypto is tron, even going as far as saying that it's offering real value to real users. And it's hard to dispute that. And that makes me a little sad, because this is the future that we want. We want the whole world to be rushing into the house of Ethereum, right? Coming in and getting all of the decentralization, all of the self sovereignty that Ethereum offers. But instead, you know, it feels like sometimes this is the know, it's just us.
04:24:30.642 - 04:25:27.934, Speaker A: It's just us here in Istanbul, having all the fun, running around in what looks like an abandoned bed, bath and beyond. And I'd much rather get to this world. Right? We all know that l two gas prices will eventually become cheap in the Ethereum ecosystem, and ultimately we think that this is probably what's required to be able to compete against ecosystems like Tron. But it'll take a while, right? We've got protodank sharding is still being worked on. Dank Sharding is going to take a few years. And so in the meantime, I think it's interesting for us to think about ways that we as an ecosystem can offer cheap transaction fees at the l two level. And you know, Vitalik's been writing about this recently.
04:25:27.934 - 04:26:47.360, Speaker A: I don't know who here has seen Vitalik's latest post, different types of l two s, a lot of hands up. He very thoughtfully articulated how there will always be use cases for which ethereum security may not be needed. There will always be use cases where people will want to pay less for transactions. And he also pointed out that in the short term, there may be use cases where, especially when it comes to l ones transitioning to l two s, it may be nice to offer cheaper alternatives so that these l ones can continue to have their momentum while we wait for things like bank sharding to get created. And that definitely speaks to me, given that I work on cello, and cello is an l one that is in the process of transitioning to an l two. And one way to think about cello is it's kind of almost the Ethereum aligned kind of competitor to Tron. And so, just to get everyone on the same page before we talk about data availability, if you haven't heard of cello, or if you're not sure about what makes it unique, cello is an EVM compatible l one that is currently actively transitioning into an l two.
04:26:47.360 - 04:27:34.250, Speaker A: And it's really built for ease of use. It's really built for those same use cases that we just talked about, payments, microfinance. And the way that we do this is by focusing a lot on mobile. And so Sella has a ZK snark based like client, but it also has an ability for folks to receive payments using phone numbers as identifiers in addition to public key derived addresses. And it also has something that's pretty unique, I would say it has the ability to pay for gas with tokens, so you can pay for transaction fees in tokens even without account abstraction. You can do this with eoas today. And so if you're sending a stablecoin, you can pay for that transaction fee in that stablecoin again without account abstraction.
04:27:34.250 - 04:28:12.278, Speaker A: And then we were also the first chain to offset our carbon. Three and a half years ago, there was a governance proposal that passed, and ever since, the chain has been programmatically buying carbon offset credits on chain, making it the first chain to become carbon negative. A lot of chains have followed suit, which we're really excited about. We think it's ultimately really positive for the whole industry, but it has also made cello a really great home for refi projects. And so there's a lot of refi happening on Celo. And so all of this has contributed to a lot of growth in the ecosystem. There's a lot of daily active users that have been onboarding onto Stello over the last nine months especially.
04:28:12.278 - 04:28:59.078, Speaker A: We've now surpassed our all time highs in the last bull market, which is really cool. And if you come back to this chart that we were looking at before, Sello is frequently featured as a top ten chain by daily active users, which is just really cool to see. We've been slowly working our way up over the last year, and it's just really quite amazing, really humbling, that we're finally here. And it's not the chain itself that's driving this. It's all of the amazing projects and wallets built on top of cello, one of which you might be interested to hear about, which is minipay. Who here has heard of opera? A bunch of people. Opera is this browser company.
04:28:59.078 - 04:29:58.214, Speaker A: They used to have a desktop browser that is no longer commonly used, but what they do have is an incredibly popular mobile browser called Opera Mini that has 500 million installs, 100 million active users. It's used primarily in emerging markets. And Opera Mini just added this thing called mini pay, which is a stablecoin based wallet built on top of Stello that's really trying to become either the global Venmo or pan Africa. Venmo built directly into this browser. And so it's really neat, and that's driving a lot of these new users that are coming to the platform, and they're really taking advantage of the ability to send money to phone numbers and the ability to pay for gas with tokens. Valora is another very similar wallet that I'm involved in, so I have to mention it. It's really nice if you have friends for whom crypto is just too confusing.
04:29:58.214 - 04:30:34.694, Speaker A: Metamask is just too confusing. Send them to Valora. It just makes everything really easy again. You can pay for gas with stablecoins, you can send value to phone numbers. It's just a real treat okay, so now that we're all on the same page, now that you know the kind of use cases that are built on top of cello, we can start talking about what we need as we transition cello from an l one to an l two. And in prior talks, you may have seen me talk about one block finality, you may have seen me talk about decentralized sequencing. These are all things that are important to us and things that we're working on.
04:30:34.694 - 04:31:36.410, Speaker A: But today I wanted to talk about data availability and specifically around maintaining the cheap gas fees that exist today on cello that so many of our users have grown to rely on, and that ultimately we have to preserve as we transition cello from an l one to an l two. If we were to suddenly, as part of that transition, dramatically increase our fees, then a lot of our users would simply no longer have the utility from the product that they've been using. And so maintaining cheap gas fees is just really important. And so in the absence of kind of full dank sharding, which I think will deliver really great low fees for everyone, we needed to look at alternatives that exist either now or will be coming soon. And we wanted to look at Ethereum aligned alternatives. We thought this was really important for us. And so that's what took us down the path of discovering Eigen layer.
04:31:36.410 - 04:32:34.522, Speaker A: Who here is familiar with Eigen layer and restaking? Hopefully it's everyone. But for those of you who aren't familiar, you can think of it as a marketplace of trust, for trust, where you can buy basically trust. And the trust that you're buying is the trust that Ethereum validators provide today by staking their eth. And since there's a lot of Ethereum validators, there's a lot of trust the Eigen layer can tap into. And one of the first things that they're offering through this marketplace is something that they're calling eigenva, which is a data availability offering built on top of these restaked Ethereum validators. And I'm really excited today to share that we have actually integrated eigenda support into the op stack base. Cello l two, code base.
04:32:34.522 - 04:33:40.050, Speaker A: It's pretty exciting. And I know what you're thinking, and I know what you're thinking, because this is what Bartek was thinking yesterday when I told him about, he said, hmm, eigenda isn't public. You how much of this am I going to really believe? We have the chief skeptic in the house protecting all of our funds across all the different l two s with his role at l two beat. And for that we are very thankful. But the reality is that, yes, we have been integrating, and Eigenda will be launching relatively soon, we're told. And the team has been very gracious in allowing us to get a preview, a sneak preview of the offering, and to be able to actually build with the code base. And so here's a screenshot of a bedrock batcher that is actually batching blobs to eigenda.
04:33:40.050 - 04:34:30.130, Speaker A: So taking transactions, batching them, and writing them to Eigenda. And yeah, I just wanted to spend a little bit of time talking about our experience doing this. And first things first. A lot of people ask, is it complicated? Does it take time to integrate it? And the answer is, actually, it was quite easy. So in large part because the eigenva team had an integration already with op stack, we were able to get up and running in honestly less than 48 hours. So we were able to port the code that they had written into the cello l two code base relatively quickly. It's ultimately less than 1000 lines of code that it took for us to do this integration.
04:34:30.130 - 04:35:21.060, Speaker A: And we started benchmarking it recently. And it's pretty cool to see that on a single laptop with four eigen da nodes running, we were able to get on the order of 1.4 megabytes per second. Now, you should definitely take this with a grain of salt, because obviously the production version of this will be running on many nodes distributed throughout the whole world. But still, it's interesting just to get a rough sense that this thing has the potential to be able to deliver some really great throughputs. And of course, because data availability is embarrassingly parallel, these numbers should actually only increase as you add more nodes. And so we're excited to see how that pans out.
04:35:21.060 - 04:36:36.390, Speaker A: So what does the integration look like end to end? Here's a little kind of emoji based example of an l two ecosystem that we're building in the cello l two world. So we're working on this decentralized sequencer on the left. In the middle we have the Inda nodes, and on the right we have our l two full nodes. And I wanted to just walk through the lifecycle of a block, which in an l two starts with a sequencer proposing a transaction bundle. In a decentralized sequencer, you have a single leader in a BFT based consensus protocol proposing that transaction bundle and then broadcasting it out to all the other sequencers, and then getting through consensus, an aggregated signature where two thirds or more of those sequencers have effectively signed off on that block, or on that transaction bundle, I should say. And so we call this a sequencer certificate. You can think of it as an aggregated multisig with a bit vector so you can see who signed.
04:36:36.390 - 04:37:58.078, Speaker A: And again, if more than two thirds of the sequencers have signed off on this, then we consider that transaction bundle sequenced invalid. Next, the sequencer shares this data with a eigenva disperser, and the disperser is responsible for dispersing this blob to all the different eigenva nodes, effectively sharding the data and doing all of the Reed Solomon erasure encoding. These eigenda nodes report back with a signature that the disperser aggregates into an eigenda certificate, which it then writes to Ethereum to have it be validated. And one of the nice things about this disperser is that it's doing this not just for this one l two, but it's actually doing it across multiple l two s. And so it's aggregating the signature across multiple blobs that it's getting for multiple l two s, so that you can amortize the cost of verifying the signature on the l one. And I think that's one primary reason why this is potentially interesting for the cellar network, because we can again amortize these costs. The disperser also returns a blob key to the sequencer.
04:37:58.078 - 04:39:00.870, Speaker A: And you can think of this blob key as a pointer to the data that the whole network can then use to access this data in the future. And so the next thing that happens is a sequencer writes this again to the l one. And this is how we effectively finalize these transactions in this bundle. Now, on the full node front full nodes download both the blob key and their certificate. They verify their certificate, and then they use the blob key to access the transaction data and the sequencer certificate, they verify that certificate to make sure that the sequencers have signed off on this transaction bundle, and if so, they execute the transactions, and in effect, they derive the l two block. Pretty simple. Right now you might be looking, there's a lot of nodes on here, there's a lot of duplication of nodes, but there's only one disperser.
04:39:00.870 - 04:39:53.146, Speaker A: And that seems worrisome. In a decentralized ecosystem, the reality is that there would be multiple dispersers. But even with multiple, there's always the possibility that they all go down. And so the question is, what happens here? And in the eigenda design? As far as we understand, when talking to the eigenda team. In this case, the sequencer can actually write directly to eigenda and get the certificate back and the blob key back directly from the eigenva nodes. The sequencer will have to do the aggregation itself, and it won't aggregate this across multiple blobs with other l two s. And so you will end up having to pay more gas when writing the eigenva certificate to the l one.
04:39:53.146 - 04:40:55.326, Speaker A: But at least you're not reliant on this disperser being always available. Now, I should add that we haven't yet implemented this piece, and so we're looking right now at what it would take to do so talking to the team. Okay, so we've done this implementation, and I think a lot of folks are interested. Well, what is the difference, as far as you can tell, between eigenda and Dank sharding and protodyng sharding? And so I created a few slides that talk about some of these differences. The first difference, I would say, is around bandwidth and the target bandwidth that each of these are targeting. So, protodank sharding, from what I've heard most recently, is targeting 32 kb/second dank sharding is targeting longer term, 1.3 megabytes per second, and eigenva is already targeting in excess of that.
04:40:55.326 - 04:41:46.234, Speaker A: Anything between three and ten megabytes per second is what they're quoting us. And the reason that they feel confident that they can achieve a higher throughput is primarily because of the network topology. So ethereum naturally has a p to p network topology. And I think everyone wants to preserve that, even with dank sharding. But eigenda is experimenting with just allowing point to point connections. So every eigenda node connects to every other eigenda node, and that creates a very efficient topology. But it requires every eigenda node to broadcast their ip, something that I think is generally avoided in the Ethereum node ecosystem.
04:41:46.234 - 04:42:40.474, Speaker A: And so I think that's the trade off, ultimately, that allows kind of this higher throughput. And then when it comes to data publishing, I think everyone's targeting the same amount of time, roughly 14 days. Difference number two is around how you pay for data availability. So not surprisingly, for protodank sharding, and for dank sharding, you're paying with ETH. But for Eigenva, one thing that the team is experimenting with that we're particularly excited about as an l one, transitioning to an l two is the fact that they're open to getting paid in other currencies. So not just eth, but also other currencies, including native currencies for the chains that are opting into using eigenva. And so in our case, they're open to allowing us to pay in sello, both in an on demand fashion, but also in a prepayment fashion.
04:42:40.474 - 04:43:39.810, Speaker A: And I think flexibility is certainly interesting for l two s and then number three, security. So this is probably the thing that everyone is the most interested to know about. How does the security compare when it comes to decentralization? As we know, prototype sharding and dank sharding can have anywhere up to tens of thousands to hundreds of thousands of nodes, depending on how you count ethereum nodes. Eigenda initially will start from what we understand in the hundreds of nodes, and then later move to tens of thousands as well. And the security model of eigenva allows for both these nodes and the l two sequencers to provide security. So it's really the combination of both of these sets of nodes. The bottleneck, from what we understand, is around verifying those eigenva certificates.
04:43:39.810 - 04:44:39.750, Speaker A: They're using BLS signature aggregation and to verify these multi sigs on Ethereum. For the BLS twelve, three, eight one curve can get expensive really quickly. And so they're working, from what I understand, on a snark based verifier, and that will unlock the tens of thousands of eigenda nodes once they have them, proof of custody is the next difference. With protodank sharding, there's no proof of custody. Yet. With Dank sharding, it's planned, of course, and with eigendate, it's also planned, likely on a shorter timeline than dank sharding. Proof of custody for those of you who haven't heard the term before, it's effectively a way of slashing lazy data availability nodes that are signing off on having stored data that they haven't in fact even bothered downloading.
04:44:39.750 - 04:45:32.994, Speaker A: And so it's a nice way to prevent lazy nodes from getting rewards without actually doing the work that they're claiming to be doing. And then finally, I think probably the biggest difference is one around what happens in the case of a 51% attack. In the case of protodank sharding, everyone's downloading all the data. So even in a 51% attack, everyone is safe when it comes to data availability. With dank sharding, you may have heard of the term data availability sampling or data availability checks. This is a mechanism by which full nodes basically are checking constantly that the data is available as part of the fork choice rule. And Dang Sharding offers this as a mechanism to protect against these types of 51% attacks.
04:45:32.994 - 04:46:17.930, Speaker A: And in eigenva, you can do the same data availability checks, but you can't verify them at the smart contract level in the l two bridge. And as a result, effectively they can't be used to protect against these types of attacks. And I think this is probably the thing that's taken me the longest to understand and the thing that I think a lot of people really ask about. And so I just have a few more slides to illustrate this difference. And I think the easiest way to see the difference is to just take a simplifying assumption and let's know. Eigenda is run by Ethereum validators. So let's imagine that every Ethereum validator opts into running eigenda.
04:46:17.930 - 04:47:40.666, Speaker A: That would be a lot of nodes. Now, in this imaginary scenario, what is the difference between dank sharding and eigenva when the whole system is being attacked? And I think there's two types of attacks, there's a minority attack. So less than 50% of nodes start acting maliciously. And in this scenario, both eigenda and dank sharding will behave similarly. The data will be available because it is encoded using reed Solomon erasure encoding, and so there will be enough copies of the data for it to be available to everyone with high probability. And then for non malicious but perhaps lazy validators or nodes, proof of custody will protect users from, again, these lazy validators in both cases, in both dank sharding and in eigenda. Now, what happens if you move to a 51% attack? So now over 51% of Ethereum validators start acting maliciously, or over 51% of eigenva nodes start acting maliciously.
04:47:40.666 - 04:48:45.038, Speaker A: And I think, again, this is the difference in eigenva, the data may be withheld, and it's just an unfortunate reality of eigenda. While with Dank sharding, the network will likely need to fork. And so what does that mean? I think this was something that I've really struggled with and really tried to understand, and so I just wanted to quickly illustrate this for you here. Here you have a chain where data is available and blocks are being finalized, and then all of a sudden this 51% attack happens, and the data is not available. What does the future Ethereum network do in this case? And the answer is, it's not yet defined. So I think there's still work to be done to figure this out. One potential option would be for the network to fork down some path of blocks that might be available, but since the network is being attacked, those blocks will probably not be finalized.
04:48:45.038 - 04:49:50.722, Speaker A: And so this may not be the most prudent thing to do. If you look at what Celestia is doing in this case, they simply stall at this point in time and they rely on social consensus to then go and effectively restart the network, kind of maybe figure out who's acting maliciously, put them out of the network, and then recover. And it sounds like this might be the best path for Ethereum as well. And so it's going to be interesting to see how the research continues on this front to see exactly what the resulting fork choice rule will end up looking like to see how the network can protect against these 51% attacks. Now, stalling is not up, is not bad, because ultimately it prevents the network from ever accepting blocks where data is not available. And so from the perspective of an l two, it's kind of nice. They know that their bridges are safe.
04:49:50.722 - 04:51:02.478, Speaker A: Maybe the whole Ethereum community needs to come together and figure this out. But there's no work, there's no multi sig. That needs to be kind of taking action on that bridge. So that's it? Yeah, I think just finishing thoughts, I think I'm pretty excited for the prospect of restaking based data availability solutions. I think they introduce a new tool to the DA toolbox. Ultimately, though, of course, as I think maybe we've heard before, Ethereum data availability will be kind of the gold standard and will be the only DA that will be unconditionally secure even in these 51% attacks. But perhaps, and we'll see how this plays out, there's going to be a trade off that some l two s will make, depending on their security needs or depending on kind of their go to market needs right now, as they wait for dank sharding, perhaps these alternative Das will be appealing to those other l two s.
04:51:02.478 - 04:51:26.258, Speaker A: And with that, yeah, let's work together to overtake Tron. I think that's the only thing I wanted to end with. Thanks, Monic. Great speech as usual. See this guy all the time now. It's cool. It's like we're buddies.
04:51:26.258 - 04:51:44.542, Speaker A: Thank you, sir. Thank you. We would take questions. We don't have any minutes left for Maric, but if you guys have any questions for him, feel free to track him down. I'm sure you're floating around today, tomorrow, yeah, he'll be over here on the right side. You guys can have a chat outside. Thanks, man.
04:51:44.542 - 04:52:16.680, Speaker A: Thank you. Backpacks here. Yeah, give it up for modic. All right, so I need to tell a story before we invite this next guest on stage, because I just want us to appreciate what's about to happen here. Okay? 2017. I found out about Ethereum. I was buying in between, like, a $50 to $400 buy in, let's say.
04:52:16.680 - 04:52:41.262, Speaker A: And I got to a point where I had, like, 32 Ethereum. Life was great. Then the bull market came and I blew it all on nfts. During this whole time, I was doing like, a major investigation on this dude, Vitalik. I'm like, who is this man? And so I watched all these documentaries. There's a bunch of stuff on YouTube. And then I saw him at ETh Denver, and I was sitting there, I remember.
04:52:41.262 - 04:53:15.082, Speaker A: And I'm actually really glad that Bartek and Usina are here. My ex co workers, we were, like, in the front at the vip section in east Denver, looking up at Vitalik. Like, there he was. And by the way, we should not have been in the vip section by any means, but we snuck in there. So I'm totally fanboying right now, clearly. But I hope that you guys are just excited as I am. This is a pretty monumental moment that I get to announce this person for me.
04:53:15.082 - 04:53:34.574, Speaker A: I hope you guys have the starry eyes that I had. But, yeah, it's my pleasure to announce Vitalik. Come on up, bud. Co founder of Ethereum, right here, guys. Hey, sir, it's got a laser there. It's really cool if you want to push it. Right, right.
04:53:34.574 - 04:54:11.820, Speaker A: Lasers for the lazy. Got it. Okay, so today I will be, as you can see, talking about state channels. No, talking about plasma. Who here remembers plasma? Who here remembers state channels? State channels are cool. Plasma is, I think, becoming even cooler. So to remind people, first, what is plasma? Right? So this over here is a diagram of probably the simplest version of plasma to explain on a technical level, which is called plasma cash.
04:54:11.820 - 04:55:12.282, Speaker A: And plasma is basically a layer two scaling solution, right? So it does the same thing that roll ups do, in the sense that it creates a kind of subuniverse where you can do things inside of the subuniverse, you can deposit into it, you can withdraw from it, and it tries to be as secure as possible. Meaning if you have things in the subuniverse, it tries to guarantee the objective that you are always able to take those things and withdraw them back onto Ethereum. Now, the difference between plasma and roll ups is that in a roll up, all of the data is on chain, but in a plasma, the data is off chain. Now, in exchange for this plasma have some more limited security properties that I am going to talk about. Right. But first, let's try to understand how this construction works. So, in plasma cash, we assume that every single coin is an NFT, right? So over here we have a tree.
04:55:12.282 - 04:56:09.318, Speaker A: The tree has eight different nfts. And here I'm just calling them coin zero, coin one, coin two, coin four. But you could imagine, like, one of them is an artvark, one of them is a bat, one of them is a cat, one of them somewhere might be a monkey. So just think of them as being nfts, right? And if you want to extend this to cash, then basically you'd have to split up the cash into tiny pieces and make each of the tiny pieces an NFT. So every plasma block is created by an operator, and the operator creates a Merkel tree where the thing in position x of the Merkel tree is either zero or a transaction spending coin x, right? So over here in the first tree in position one, you have transaction spending coin one from Alice to Bob, and we assume it's got a signature attached. So Alice signed it, and then coin four is going to move from Charlie to David, and then everything else is zeros. Right? So this is a sparse Merkel tree.
04:56:09.318 - 04:56:56.042, Speaker A: Most of it is zeros. Then in the next block, nobody send transactions, so it's just going to be all zeros. Then in third block, let's say Bob just directly decides to send the coin over to Eve. Coin one. And so you have a transaction in position one again, and then you have coin six that gets sent over from Fred to George, right? So coins keep getting transferred around and each coin gets, or each transaction gets put into the appropriate position in the tree, and it has to be put into that position of a block in order to be valid. So plasma also has a set of rules that we call an exit game, right? Basically, let's say that you're Eve and you want to get coin one out. Then to get coin one out, you provide a Merkel branch.
04:56:56.042 - 04:57:27.098, Speaker A: You provide the Merkel branch of the transaction that sent coin one over from Bob to Eve. And you just say, like, here, here's the proof that I got the coin, let me withdraw now. You do not get to withdraw immediately. You start a challenge period, right? Wait a week. And during that week, anyone can challenge you. They can challenge you in basically two different kinds of ways. So one of them is they provide a transaction that transferred the coin from Eve to someone else that comes after coin one.
04:57:27.098 - 04:58:28.906, Speaker A: Right? So basically they're proving that okay, you're exiting with a proof of that you got the coin, but really you already gave up the coin. The other kind of proof that you can have or the other kind of challenge that you can have is an invalid history challenge, right? So let's say for example, instead of that first transaction of coin, one being from Alice to Bob, it's Alice to Charlie, right? So you have a transaction from Alice to Charlie, and then you also have a transaction from Bob to Eve. Then what you do is you challenge the withdrawal by saying, hey, here's a transaction from Alice to Charlie. And guess what? Charlie actually is the latest owner. Bet you can't find someone who's going to spend it. And the original Exeter is not able to come up with a response, right? They're not able to come up with any kind of proof that Charlie is not the latest owner. Meanwhile, if the challenger just submits this transfer from Alice to Bob, then Bob can respond because Bob has the transaction transferring from Bob to Eve.
04:58:28.906 - 04:59:48.658, Speaker A: Right? So basically you have this kind of game where you have the Exeter and then you have a challenger who are providing proofs of what happened based on Merkel branches around a particular coin. And the proving only really happens if the two parties actually disagree, right? In the normal case, Bob just makes the proof wait seven days, or Eve, in this case makes the proof wait seven days, and then Eve disappears with the coin. So it's a surprisingly not too complicated system and it lets you have very high security without requiring data availability layers, in this case, for payments. Now, who here wants to live the rest of their life without fungible currencies and just trading different kinds of cats and dogs and monkeys? Okay, a few people do, right? Maybe that would be cool. But don't know the exact valuation of the cat, the dog and the monkey. You can put whatever you want on the sales tax, right? Unfortunately, in order to have a modern economy, things like that make it a little bit hard. And so we do need to have a nice way to extend this to fungible tokens.
04:59:48.658 - 05:00:19.306, Speaker A: And the problem with just like taking this NFT approach directly is basically fragmentation, right? Like, imagine that you have a plasma system and every cent is going to become its own coin. And then you go and get $50,000. Congrats. You have 5 million coins. Now, if you have 5 million coins that are all beside each other, then you could actually do some fancy stuff and you can exit them all in one transaction. But then here's the problem. Suppose you run a coffee shop and someone's like, yoda I want a coffee.
05:00:19.306 - 05:00:46.774, Speaker A: And he pays you $3. Then someone else is like, yoda, I also want a coffee. And he pays you $3. And then someone else is like, yoda, I want a coffee, but make it a little bit bigger. And then he pays you $4 and then you keep going. And then that's your lighting, right? The problem is you now have many thousands of coins, but they all come from totally different places. And so if he wants to exit them, then you're kind of stuck because you have to make a huge number of exits.
05:00:46.774 - 05:01:06.430, Speaker A: And actually the transaction fees are not even going to be worth it. Right. So this is the fragmentation problem. And it's one of the things that really made plasma very challenging. There is an approach that can do it. So here, basically what you do is we bring back utxos. Like Utxos from bitcoin.
05:01:06.430 - 05:01:41.334, Speaker A: Yay. And it's okay, we can say, yay, bitcoin. Right? Remember, what was it? It's like we were always at war with Solana. Bitcoin is our friend, right? Don't worry, next year it'll switch. You get just like, watch Justin Drake, he's the high priest. He's going to tell you who we're always at war with. Okay, so what you do here is we have a Utxo graph, right? And the way Utxo stands for unspent transaction output.
05:01:41.334 - 05:02:08.258, Speaker A: And basically you have a Utxo just represents some amount of currency, and a transaction consumes utxos and creates utxos. So let's say I have a Utxo that is 0.8 eth, and then I want to give you 0.2 eth. I'm going to create a transaction where that consumes one Utxo, it consumes a 0.8, and then it creates two new utxos. The 0.6
05:02:08.258 - 05:02:48.962, Speaker A: is again owned by me, right? So it's like sending to myself. It's what's called a change output, and the 0.2 is going to be owned by you. So then we just have a graph that consists of all of these transfers, right? Sometimes you have splits, sometimes you also have merges, right? So if you go and receive your $3 each for coffee from 1000 different people, you can do a merge and that turns into one Utxo. If you want to exit the Utxo, then we can sort of trace the history of that Utxo by basically doing what's called this. Follow the Satoshi algorithm, right? You kind of trace the 0.2 e and then we think of it as being the bottom quarter of that 0.8.
05:02:48.962 - 05:03:01.090, Speaker A: And then we trace back, and it's like the bottom 0.2 of the 0.9, and then we go trace it back further. And then because the 0.2 is below the 0.9, it just becomes in the middle, and then the 1.1 comes from those three.
05:03:01.090 - 05:03:51.154, Speaker A: And then it just turns out that the way that things line up, some of that 0.2 comes from the bottom output and some of it comes from the left output. Right? So you can have a bit of divergence, but it's much more restricted than sort of the entire graph of payments that came together to give you the 0.2 eth. So then what you do is when you exit the 0.2 eth, that gets interpreted as exiting all of those coins. And then if someone tries to do an exit of any of those same outputs in that history, then that can be challenged, right? So the goal of the system is basically to maintain a kind of arbitrary conception of what it means to have to talk about the same coins at different points in time and prevent someone from withdrawing the same coin at two different points in time at the same time.
05:03:51.154 - 05:04:27.586, Speaker A: So these are the kinds of ideas that can generalize plasma to fungible tokens. There's like a pretty long and interesting history of different plasma solutions. So there's like a bunch of QR codes. If you guys want to scan them, go ahead and scan. Or you can just control f the words, or you can just photo this entire page and then just tell GPT to tell you what the words are and then give you the links to all of them. But basically, minimal viable plasma was the original plasma cash. Then we had plasma cash flow, which is like a simplification of plasma cash to let you gather many nfts together.
05:04:27.586 - 05:05:39.254, Speaker A: Then plasma prime used RSA accumulators based on prime numbers to try to reduce history size. Lots of really fun ideas, but it always ran into some problems, right? Plasma was eventually sort of de facto deprecated because of two big issues. One of them is client side data costs. Now, what do I mean by client side data costs? Well, if we go back about three slides here, imagine instead of just having these three blocks, you have 10,000 blocks across an entire week, right? And then imagine that because we're talking about coins and you're constantly getting coffees from lots of different people, you have like 1000 different outputs, right? So you have your 1000 different sets of coins. For each of those thousands of coins, you have to download a proof for each block in the last week for each of those coins, right? So it's like 1000 multiplied by 10,000 to prove that when you get those coins, you actually have them and there wasn't some invalid thing to transfer them to someone else. So 1000 coins multiplied by 10,000 blocks. Who here can multiply 1000 by 10,000? Anyone want to shout it out? 10 million.
05:05:39.254 - 05:06:00.574, Speaker A: Now then, these are Merkel branches. Let's suppose in total there is a billion coins. Who here knows what's the length of a Merkel branch out of a billion coin thing? Shout it out. Log 30. Okay, now what's the length of a hash? Shout it out. 32. 32 times 3969.
05:06:00.574 - 05:06:42.246, Speaker A: Hundred and 60 times 10 million. And you have over 9gb. So basically, to sake a plasma system, you need to get like over 9gb of data. Now let's imagine you're someone in, let's say, turkey or even, let's say someplace even like India or Ghana, and you do not have the kind of income that can afford a super fast phone or a super fast Internet connection, right? Some people can. A lot of people can't. Let's say you're one of the people who can't. Then you might have a hard time downloading 9gb.
05:06:42.246 - 05:07:12.834, Speaker A: You might actually get charged for that, right? Even for me, I am roaming. And 9.6gb is like half of the $70 package that Iralo gave me. So 9gb is significant, right? The data costs from the client side are pretty high. So this is the first problem, right. The second problem is that it has difficulty generalizing beyond payments. People don't just want payments, people want an EVM.
05:07:12.834 - 05:07:53.214, Speaker A: And in an EVM, the problem is that these kinds of things are much harder to do because in an EVM, people don't sign off on every transaction that happened. You just have the EVM. And the EVM creates events, and events move coins around. Plasma relies on the assumption that everything is owned by somebody in the EVM who owns Uniswap, in the EVM who owns a Makerdao CDP. Well, maybe the CDP holder has. But then there's also debt. And if you own the CDP, does that give you the right to just get rid of the debt? Well, maybe you don't fully own the CDP, right? So evms are way more tricky than just simple payment based systems.
05:07:53.214 - 05:08:23.146, Speaker A: And so I think for these reasons, we originally sort of gave up on plasma and moved to roll ups. But there is the next chapter of the story, right? What if I tell you that ZK snarks solve both problems. So let's look at the plasma cash exit game before ZK snarks. Right. This is the logic for taking an asset inside of plasma cash. Moving it back to Ethereum that we talked about ten minutes ago. An exit can be.
05:08:23.146 - 05:08:53.690, Speaker A: This is a direct quote from the ether research post. An exit can be challenged in three ways. One, a proof of a transaction spending c. So a proof of a transaction spending the thing you're trying to exit. Two, a proof of a transaction spending the parent of c that appears before c. So basically proving that the thing you're trying to exit is invalid because it's double spend. And three, provide a transaction C star in the coin history before PfC, which you would have to then respond to to prove that there's something spending it after ZK starks.
05:08:53.690 - 05:09:41.518, Speaker A: Two of those three challenge types disappear. Why? Because validity proofs, aka zksnarks, can just prove that a plasma chain is valid. And if you prove that a plasma chain is valid, you prove that every spend that gets included is valid and that there are no double spends. So there's only one type of challenge, a proof of a transaction spending c. So if you try to exit c, then someone can only challenge you by making a proof of something spending c. Now, what happens if in the normal case, the plasma chain operator is working totally fine and you exit by providing a Merkel proof that is based on the most recent block? Right, who can challenge you? Well, no one can because there's nothing after what you're trying to exit. And if no one can challenge you, then what's the point of a challenge, period? You just exit immediately.
05:09:41.518 - 05:10:25.742, Speaker A: Right? So if you ask optimistic roll up developers, then they'll tell you one of their biggest pain points is the one week exit withdrawal window, right? You have to withdraw and then you wait a week and only then you get your money out. Well, now with plasma cash, if you add ZK synops, you can just make exits instant. So this is one really big and powerful thing that validity proofs give you. Also, clients no longer need to download and store history because they do not need to answer history challenges. 9.6gb goes all the way down to basically zero extending to the EVM. So there's been a bunch of attempts over the years to try to extend plasma to the EVM.
05:10:25.742 - 05:11:09.486, Speaker A: And there's actually a couple of different families of constructions. And I just wanted to share another interesting member of this family that I believe people have not really been talking about yet, which is basically, let's create a Utxo graph that just parallels what happens in the EVM. And then let's use validity proofs, aka ZK snarks, to just prove equivalents between the two. Right? So we've got the EVM, and then you also are going to have a Utxo graph. And so in the evm, you have accounts. Accounts have balances, and those balances go up when you get things, and they go down when you spend things. And then separately, you're going to have a Utxo graph that basically mirrors everything that happens in the evm state.
05:11:09.486 - 05:12:15.286, Speaker A: And so if in the evm state I have 50 coins, then you pay me for your coffee, I have 53, you pay me for your coffee, I have 56, you pay me for a venti coffee and I have 60. Then in the Utxo state, I'm going to have four utxos that are like 53, three and four in number, and then maybe at some point it'll just merge them into one when it finally wants to. Right? So you maintain a Utxo graph that says the same thing as what the evm is saying, and then you just do plasma over the Utxo graph. Now, how do validity proofs help? Well, validity proofs basically ensure that everything that happens in this Utxo graph actually is valid and actually does reflect what's happening in the evm. Now, if you have Ethereum blocks up to a certain point, then you can personally regenerate the Utxo blocks, and so you can personally have the information that you need to exit what you want. And then if you want to exit a contract, you would have to provide an exit message with a valid ERC 1271 signature for that contract. Right.
05:12:15.286 - 05:13:09.266, Speaker A: So we just use ERC 1271, which is like the standard for signing with a wallet, and we kind of overload that to just mean the standard for who gets the right to choose when to exit a thing, which is a contract where we don't by default, know who the owner is. So what's interesting about this is that this protects a surprisingly large percentage of uses and use cases. Right? Most people in the EVM just hold coins. Who here holds coins? Who here doesn't hold coins? Interesting. Who here has a position in a complicated defi application and doesn't hold coins? Okay, so holding coins is important, right? Then a surprisingly large percentage of users just hold coins. If you hold coins, you're protected. Even multisig wallets are protected.
05:13:09.266 - 05:14:00.530, Speaker A: If the ERC 1271 reflects the same ownership structure as what can spend even fancy use cases like, for example, the Ethereum Foundation. Multisig four of seven in order to spend unlimited one of seven to spend a much smaller amount. Let's say one of seven decides to spend a much smaller amount, colludes with the plasma operator, just and withdraws those coins and leaves the rest of the information hidden. Well, the four of seven would still be able to withdraw the portion of the previous state that covers all of the coins except for the coins that one of seven withdrew. Right. So even in a complicated setup like that, a default plasma exit game can still provide basically the correct answer. Now here's a situation that plasma does not support.
05:14:00.530 - 05:14:23.600, Speaker A: Let's walk through cdps collateralized debt positions. Who here has had a CDP on Ethereum before? Okay, it's a good crowd. Who here has had a cds on Ethereum before? Okay, good. That was a dummy. I don't even know what a cds on Ethereum is. Just watching. Sometimes you got like the people who raised their hands for everything.
05:14:23.600 - 05:14:51.170, Speaker A: Raise your hands if your birthday is on a day that starts with a four and has two digits. Okay, good. Okay. Amazing blood person. Okay, that's good. So let's say you have a CDP today, which is like basically a smart contract where you hold some amount of ETH and then you have some debt. And you can only grab the ETH if you pay your debt.
05:14:51.170 - 05:15:14.330, Speaker A: Let's say you have one eth -1000 dies. So about two x collateralization. The exit rules will be the owner can exit only if they exit into a layer one contract that enforces the same rules. So even if you exit, you have to pay the same debt. And also we might as well add the rule that other people can exit to force liquidate. Here's the problem. You collude with the plasma operator.
05:15:14.330 - 05:15:51.354, Speaker A: Plasma operator starts making data unavailable. You can exit, but you can also choose not to exit. So what you can do is you can just delay exiting for a very long time. If ETH drops below $1,000, you just walk away. Otherwise you pay the $1,000 and you claim the ETH. So what you've basically done is you've converted a position which is one EtH -1000 die into an option to buy ETH for $1,000. And the option is something that's strictly more valuable than the thing that you've had before, right? Because as time goes on, there is a greater and greater possibility that the price of ETH drops below $1,000, right.
05:15:51.354 - 05:16:32.394, Speaker A: Everyone agrees that there is a possibility that ETH is going to drop below $1,000. Well, good, because there is. In that case, if you follow this strategy, then you've made a profit, right? So basically anything where security is kind of time dependent because it's a financial market, well, plasma is not going to protect you. A roll up is because in a roll up you can have a mechanism where just like anyone can go in and submit blocks. And so you have basically limits on people. You cannot just go and delay people in the same way. Privacy systems, this is a fun one.
05:16:32.394 - 05:17:32.374, Speaker A: So think tornado cache, privacy pools, railgun, any of these, right? The problem is that there's like a mismatch between the sort of real Utxo graph because the real Utxo graph is hidden, right. The whole point in the privacy system is like nobody can tell which output corresponds to which input, right. Because we're trying to keep people private. But then the thing that I described basically involves kind of creating a superimposed UtxO graph, which could just be like a first in, first out system that mismatches with the real UtxO graph. And so one question is like, what if orange, for example, the first withdrawal withdrawer, let's say they're the only withdrawal at some point in time, and then they withdraw the coin after the mixing, but then they also withdraw their coin from before the mixing. And let's say they withdraw both of them, and then the other post mixing state is unavailable. Well, orange might be able to get both of their coins out, right? And if they get both of their coins out, the other people, mathematically speaking, are screwed.
05:17:32.374 - 05:18:19.514, Speaker A: And so it is potentially possible to do something clever to try to get around this, right. Basically what you'd have to do is you'd have to allow the other four depositors, once they see oranges first withdrawal based on the exit, to withdraw the privacy system itself. And then they can define which piece of the privacy system they withdraw by providing a list of all the deposit ids and then proving which index of the deposit ids corresponds to them. Right. And then if they do that, then they might be able to withdraw. So you can do complicated stuff to get around this, but this requires DAP developers to do extra logic and extra thinking in a roll up. DAP developers do not have to do logic and thinking maybe that's a bad thing.
05:18:19.514 - 05:19:16.094, Speaker A: I don't know. And so in a roll up, the story for why things are secure is much simpler. Right? So there's still limitations in plasmas and EVM plasma techniques, I think are not going to be able to protect all users. But EVM plasma techniques plausibly can protect most users, right? And I think there's like this interesting open design space in making application specific exit logic for some of these more complicated cases. Privacy systems, Uniswap, CDPs, anything where there isn't a clear owner, and also making general purpose exit games that can support this kind of logic. And if we have this, then this can be grafted onto existing validiums, right? Lots of people want to do validiums. Like, who here wants to do a validium? Who here wants to do a roll up? Who here wants to pay the fees to have a roll up? Who here is okay with paying the fees to have a validium? It should be more hands.
05:19:16.094 - 05:20:33.830, Speaker A: It's like 100 times lower. I think the big reason for me to be bullish on things like validiums is basically that there are a lot of things that are not connected to Ethereum today, and there's two major categories. One of them is centralized systems. The other one is systems that are currently ALTL ones but really don't need to be, right? Like if you're number 147 on coin market cap and you've been forgotten since 2018, then you probably don't even need to be an one. And you might even benefit from having better security, right? And then on the flip side, if you're a centralized system, let's say you've got a game and you wants to somehow decentralize the game because you learned from your nephew that decentralizing things is cool, right? But you want to decentralize the game in a way that actually makes sense and in a way that doesn't kind of just pointlessly have decentralized and sacrifice on decentralization and sacrifice on convenience. In both of those cases, you're not going to be willing to pay even the fees that are involved in creating a roll up, but the fees involved in creating a validium are much smaller. And so making a validium is something for which tooling basically already exists today.
05:20:33.830 - 05:21:46.122, Speaker A: Like, you just take one of these off the shelf zke VM implementations that are getting increasingly shelfy and increasingly ready for offhang with every passing month, and you just take them and then you got a validium, right? So then if you have a validium, then you can add one of these exit games to it, and that strictly increases the security. And so you can start to have a plasma system, right? So I think there is a lot of room to try to experiment with these constructions, try to make them better and really actually try to realize the vision that a lot of people attempted to realize with the plasma efforts back in 2018. But now that we have ZK snarks, it all is ten times easier and it's actually viable to do things that were impossible before. So hope to see this be a design space that more people try out and explore and basically open up a whole new direction for Ethereum scaling. Check, check. Give it up for Vitalik, everyone. We take a selfie with the crowd.
05:21:46.122 - 05:21:58.020, Speaker A: Okay, guys. Everyone put your hands up. Yeah. Thank you, Vitalik. Thanks for coming, man. That was an awesome speech. Give it up.
05:21:58.020 - 05:22:23.480, Speaker A: I'm so pumped, I forgot who's next. I hope you guys are fueled up because someone from fuel is talking next. Fuel labs, Nick Dodson. We're talking about solving state growth. Oh, there you are. What's up, guy? Matt, pleasure to meet you. Thanks for coming up.
05:22:23.480 - 05:22:37.680, Speaker A: That should be on. There's a laser here. It's pretty cool. You can use it. It doesn't work, though. All right, there we go. All right, we'll just let people go out for a second.
05:22:37.680 - 05:22:53.522, Speaker A: No, please don't. Yeah, that's okay. Got to see Vitalik. Got to get it off the list and then do whatever you need to do. It's a tough act. It's a tough act to follow. All right.
05:22:53.522 - 05:23:15.434, Speaker A: Hey, that's right. We're here and we're staying. I think I can use the laser pointer. Heads up for Hudson over there. All right, well, so for this presentation, I'll try to be quick. I've got a lot of slides to get through. And really this is about fuel, and fuel's Utxo model and how it actually addresses state.
05:23:15.434 - 05:23:58.818, Speaker A: So if you want to learn more about Utxos, this is the presentation for you. So basically, in blockchain, we've been succumbing to a lot of issues with state and verification, and so state growth is a big problem. So for this presentation, I'll try to cover real quick components of a blockchain final boss, which is state growth solution fuel, state philosophy itself. And as well, just some closing thoughts. So just to briefly go over, just fundamentals or basic components. So some things to think about when you're doing blockchain processing effectively state execution and data. So these are core areas that you typically focus on with blockchain.
05:23:58.818 - 05:24:57.580, Speaker A: And there's definitely some, what you could say, definitions, you could place over different definitions, over execution and data. And data can also represent things like holding data for a long time or just over bandwidth in a short period of time. But effectively, you can think about these categories as sort of the fundamental areas that typically bottleneck a client or a blockchain node. So in terms of the actual blockchain components themselves, execution and data are relatively solved in terms of what's been done in the space. And I'll say that for execution, we just have a lot of options, whether it's trying to build more parallel transaction execution, better virtual machines, you have all kinds of different options as far as data. Data availability layers are coming up as well. Higher bandwidth requirements on nodes are also allowing us to use more data.
05:24:57.580 - 05:25:45.206, Speaker A: But as well, we've got a lot of different ways to compress data and move it around. So data is also relatively a solved issue. But state remains something really kind of prickly in terms of client processing itself, and it's sort of always been a problem. And effectively we want to try to find ways to address it. So on the execution side, roll ups don't necessarily solve your execution, but they do open the door to new kinds of execution, new kinds of models as well. Parallel transaction execution is now becoming more commonplace. And for us, and I think definitely many teams, it should be just table stakes in terms of an actual scalable blockchain as well, more efficient machine designs, as I mentioned, using more WaSM, et cetera.
05:25:45.206 - 05:26:28.200, Speaker A: And as well, better pre compiles are allowing us to effectively squeeze out more execution in general. And data availability is also something that's being relatively addressed. 4844 is giving us a little more room, a little more bandwidth, and as well sharding. And then external data availability layers are coming up. So we have a lot of different ideas as to how we want to approach this and how we want to resolve it. But what about state? So state is effectively, when you think about a blockchain, you think about having this giant database of all the different things you need to access right away to validate if the state or the chain is valid. State is a little different in terms of a problem.
05:26:28.200 - 05:26:56.254, Speaker A: Let's see here. I'm not sure what happened there. Okay. Comparing the current state design. So you have things like bitcoin's state, which is effectively like all the unspent transactions, and for that, it's a very significantly large tree. On ethereum, we have a lot of different aspects of state. So we have things like account balances, and then we also have our smart contract state, and then smart contract state.
05:26:56.254 - 05:27:55.860, Speaker A: And code really encompasses all these other things like token balances and approvals and all that kind of stuff. So this is just like a look at what state really is for these blockchains? Peter from Goetheoreum has a lot to say about state, because, go, Ethereum, as the primary execution client, or one of the primary execution clients, has to deal with state on a regular basis. And it's been a massive issue to basically breaking open Ethereum on the throughput side and doing so in a way that's sustainable. So Ethereum, unlike other blockchains, is trying to actually still target a very sustainable set of requirements. And so we're really designing for that all the time. And unfortunately, state, and just being able to access it, being able to increase it, has always been a huge issue. So to give you an idea, this is actually just the blockchain size, which is sort of like an indicator of state, although it's not necessarily the state itself.
05:27:55.860 - 05:28:56.870, Speaker A: As you can see, it's just getting enormous. And if we want Ethereum to be used globally by every person on planet Earth, this particular problem is going to need to be addressed. Now in terms of the blockchain size here, some of this is just printable archive data, but the actual state itself is really growing in a very unhealthy way, and in a way that will restrain us in the future. And so Peter talks about effectively sort of the bane of the Ethereum state tree as it currently exists, and how it really restrains us on different levels of throughput, and just kind of what we're capable of as a blockchain, basically, you know, state because you have to go to the drive and then back. It's actually quite a big trip when you're processing transactions. It would be much better to do something else than having to really do that. So state is really kind of going out of control.
05:28:56.870 - 05:29:35.330, Speaker A: So what are some ways that we could actually address state and address the state growth issue? So we do have a few options that have been discussed for quite a while. Things like state ren statelessness. We could basically, oh, there's a typo there, damn it. We can unmercalize the state. And that would basically be just effectively similar to what Solana does. Just take away all that stuff and effectively we'll just use full notes to validate everything, or we'll just sample things with the light client and just forget about this state tree altogether. Another one is sort of app level compression.
05:29:35.330 - 05:30:34.274, Speaker A: So this would be at the application level just using a lot less layer one state, and using something else, using things like call data, et cetera. There's also an option to just let the state grow and just let it ride that option I don't think is so great because it's really going to restrain the amount of decentralization we could actually continue to have with the network. So probably not a great option. And then there's this last option, which I'm really going to focus on today, which is bandwidth. So on approach one, when we're talking about state rent, effectively the idea here is you rent a state element somewhere in Ethereum, and then at some point, basically if you don't pay to rent it, then we just forget about it. This model is pretty interesting because effectively you can rent out things and maybe we can just forget about certain elements of state. But we do have one big problem, which is effectively something called tree rot.
05:30:34.274 - 05:31:17.714, Speaker A: So if all the state elements in Ethereum are on one giant tree, and you forget about certain leaves, you basically corrupt some of the branches or potentially some of the branching paths. So there's a few things to figure out there. Maybe ZK proofs can help us, but effectively it's not great. And so state ren, not really the best option. Statelessness is also another interesting option, I think Ethereum is really heading towards a stateless future, you could say. So with statelessness, you're really trying to do something different. Where you're trying to say Ethereum is really just sort of signing off on applications, sort of, you could say Merkel root of all the state, and it's really just this giant machine to do that.
05:31:17.714 - 05:32:05.378, Speaker A: And we'll try to move all of the state away from the layer one chain into all kinds of other things, like roll ups and into compressed apps, et cetera. So this is where Ethereum is going, and this is where a lot of things are going, but we're still not there yet. And there's a lot of unanswered questions as to really how efficient this will be and also how maintainable this will be. So things like vertical trees are really interesting patterns to use here, but again, they're still sort of under research, and it's a long way to go until we really get there. So the other option is to unmerkilize the state. So you just rip this thing right open and you go, okay, we're going to forget about mercalizing all this state, and we're just going to have each state element be somewhere in the database. And if you want to verify that that's the case, you really run a full node and then you verify everything, and you just have to get to the spot you're at.
05:32:05.378 - 05:32:48.274, Speaker A: So this is not so great, unfortunately, because then you have to literally run a full node to understand state correctness, and it's very hard to ascertain if state is actually correct. So unfortunately it's also not great. It's also what another chain, not to be named, does and is probably not the best model. So application level state compression, this is also an interesting one. Basically, at the application level, you could basically just use call data, rehydrate the state, which is an interesting idea in and of itself. So the way to think about this is sort of trading state for bandwidth, if you can imagine. So, to give you an example, if anyone here knows the uniswap v.
05:32:48.274 - 05:33:41.326, Speaker A: Three staking contract, they do some nice little things in the contract itself, where effectively they just keep sort of a representation or a hash of a state element, but they don't actually keep everything. And you sort of have to rehydrate the state in order for it to be alive again, and you do that every time, but that ends up being cheaper than having to use state itself. So this technique is actually really interesting in the sense that we're trading one thing off for another. And so you really want to use something like this if you really want to address state in a very wholehearted way. So an example, another application example is sort of compressed nfts. So here you would merklize basically a giant merkel tree of ownership of different nfts. And effectively you keep that merkel root on chain and you just keep everything else sort of off chain.
05:33:41.326 - 05:34:26.858, Speaker A: So Vitalik was mentioning, and his ideas about plasma et know similar ideas to this in a sense. Know, effectively Ethereum doesn't have to store everything, it just stores the root and you rehydrate the state later. So what about roll ups? So roll ups don't actually solve this problem at all, because roll ups just allow Ethereum to open the door to something new. But what that new thing is still needs to actually address the issue. And if it doesn't, then you're back to the same problems. So basically for anyone using the EVM, again just in a roll up, you're not really addressing any of these issues, you're just creating the same problem again, and hence the same issues again. So roll ups don't really solve this issue of state growth.
05:34:26.858 - 05:35:02.794, Speaker A: And again, they're just sort of a door opener to state growth. So just to highlight, again, even on arbitrary and optimism, the address creation is sort of like another indicator of how much state we're having to deal with. And as you can see, the address creation is enormous and state will again balloon and bottleneck as it always does. So even in layer twos, state is becoming still overwhelming for even layer two nodes to process. So with fuel we have a different state philosophy. And for those in the room who don't know what fuel is and what we do. So we're a layer two.
05:35:02.794 - 05:35:18.354, Speaker A: On Ethereum, we were the first layer two to launch. On Ethereum mainnet we use a Utxo model for everything. And I'm super excited that Vitalik is yet again bullish on Utxos. It's a big deal. Yeah, go Utxos. Woo. We're going to do it.
05:35:18.354 - 05:36:02.106, Speaker A: We're going to do it. But yeah, effectively our focus is to really address a lot of these scalability and sustainability issues with blockchain and still give everyone an enormous amount of performance while being a more sustainable design. So when it comes to state, we really care about it and we want to try to make a design that actually allows us to minimize this at the native level. So with fuel we have a different model to approach blockchain processing and verification in general. So because we use Utxo system, we actually don't need a global state tree. So already in Ethereum state model, you have the global state tree and then you have local smart contract state trees. For us, we don't even have a global state tree.
05:36:02.106 - 05:36:37.750, Speaker A: So already you're getting rid of a lot of state just by using the Utxo model alone. The second thing is we have these things called native assets, which I'll explain later. Those don't actually need to be mercurialized at all. So you can have native assets in our system and they only take up one state element. So already just ownership of assets and moving assets, creating assets is already way less state in terms of the net blockchain system as well. You don't have really annoying things like approve and transfer from and all this other stuff. And I'll explain that in the sense of scripts and these other ideas.
05:36:37.750 - 05:37:23.426, Speaker A: The main thing is all of those little additional moves have to be tracked in state, and so they're all additional stuff that creates an enormous amount of state. So we do this while retaining really rich cryptographic like clients and verifiability. So basically we're not losing cryptographic verification or anything like that. We're gaining all of these additional benefits and we're effectively getting everything that we actually want. So with fuel we have a fundamental state philosophy of effectively using more bandwidth and execution and less state. So we're trying to get away from touching the drive and I o and trying to move closer to bandwidth and execution. And in doing so, it opens up a lot more possibilities for how many tps we can actually do.
05:37:23.426 - 05:38:21.210, Speaker A: Not to say that number is important, but it is an indicator of how efficient we are. So comes back to an interesting question, blockchain, is it a database? Interesting one, especially when you're talking about state, and effectively the answer is, it's complicated. Yes and no. But how do we do this? So how with fuel, are we actually going to reduce the state of blockchains in general, keep them sustainable, keep them very performant, and it's basically with an idea called native state rehydration. So effectively, we allow the developer to access a lot of interesting ways to sort of dehydrate their state or to effectively compartmentalize their state. And then things are rehydrated over bandwidth to allow us to basically re access that state. So the conventional approach is just everything's a smart contract, and effectively you have to read and write out of this smart contract to use contracts for everything.
05:38:21.210 - 05:39:20.174, Speaker A: And that's typically how ethereum works. The new approach is, okay, what if we can use a lot more techniques on basically storing just these root hashes and storing only sort of state changes, and then basically presenting data over bandwidth and rehydrating it. So this is a very different way to approach the whole problem, and then you provide many different tools to access this technique. So for us, fuel not only has sort of accounts and smart contracts, but we also have scripts and we also have native account abstraction, and these are different things we can use to effectively apply this technique. So with fuel state philosophy, we're very state minimized in terms of our mechanisms and our design. So again, we give the developer scripts, we give it predicates, which are these lightweight, effectively spending conditions over these assets. And we give developers things like native assets, which use a lot less state.
05:39:20.174 - 05:40:10.042, Speaker A: And we also have a much more flexible transaction model that allows you to do a lot more just from the model itself and not have to use smart contracts or these other very stateful things to get your job done and to have a good user flow. So when we're talking about scripts, effectively, they're just logic, and they don't actually need to have any state in them themselves. They're fed into the system when the transaction is made and they're totally prunable, so they don't actually affect state. They do, however, let you do all of this crazy routing that you need to do to five different contracts with multiple different assets, et cetera. So they already set the developer and the user up for a much better experience. So if you've ever used Uniswap, and you look at the Uniswap code, you can see there's like a router, and the router goes to like 20 different contracts. This is because we just don't have scripts.
05:40:10.042 - 05:40:41.074, Speaker A: So scripts are a really big deal when you're kind of moving the needle forward. You also don't necessarily need proxies to do batch transactions. And you also don't need kind of complex mev searcher contracts. You can just write them in scripts. Native assets, as I was describing, are effectively single state element assets. So in our Utxo model, every asset in fuel can become a native first class citizen. So you don't need to have this model where things are constrained to just ETH as the first class properties.
05:40:41.074 - 05:41:26.200, Speaker A: You can really do every single asset as a first class citizen. So they can use all of the native features of assets that we have with fuel. That includes like non fungible tokens and fungible tokens. And again, it makes the whole system so much more efficient as well. Predicates are a very lightweight, stateless way to do account abstraction. And this allows you to define basically a spending condition or a script, and that allows you to effectively compartmentalize all that code into something that's fed over just bandwidth and then pruned. So we're using more bandwidth and less state, and that allows us to again achieve really high throughput properties without touching state and minimally touching it, if anything.
05:41:26.200 - 05:42:07.650, Speaker A: So in this new transaction model that we have, you can see this is our Utxo design. So you have many inputs, you have predicates, you have scripts, and then you have many outputs. So in fuel's case, inputs are both smart contracts and native coins. And then predicates are spending conditions. So this is the native account abstraction I was talking about. So with predicates you can really put different kinds of spending conditions on coins. And that could be things like I provide this multi sig address, et cetera, or web authent, which we demoed earlier, or it could be anything from BLS to any kind of signature scheme.
05:42:07.650 - 05:43:04.086, Speaker A: And scripts allow you to do these much more prunable and different action oriented kinds of designs, and then there's many outputs. So the Utxo model alone is extremely flexible in the way that it's designed. And it allows you to do a lot more multiparty settlement a lot easier, a lot more efficient, and basically achieve a much stronger model where you don't need to leverage smart contracts every time. Instead you can do things, you can do far more things just with the transaction model alone. So what does this look like in practice? And what does state rehydration really look like if you're really plugging at it and you're really designing something really cool? So one good example case is we can have smart contract wallets with only one state element. So it's the minimal amount of state possible to actually do a smart contract. And then we can include all the different spending conditions and everything else that we need.
05:43:04.086 - 05:44:02.950, Speaker A: And what's really cool about this is right now with ethereum, it's very hard to do anything like this for each person in the room. If you all created a gnosis safe, that would all need to be mercalized into the giant ethereum state tree, and it would be enormous. Every time you make a token transfer, you have to literally go up an entire tree of 256 depths just to move one token in ethereum, or just to have one single contract that allows you to do anything with this, you literally have none of that. And you're going down to just one state element itself. So it's extremely minimized and extremely sustainable in terms of its design. And how this works is effectively in our Utxo model, we mint a Utxo and that Utxo, we effectively graffiti the state hash over it. And this allows us to effectively have a stateless spreading condition that can actually leverage this Utxo as a state element.
05:44:02.950 - 05:45:00.380, Speaker A: And with fuel, when you spend Utxos and you spend these different coins or contracts, you're basically changing the state of them. And so that's where you would actually do the state change itself. So with this, you can basically imagine that smart contract state would effectively come down to one route itself still ensuring light client verifiability. It only requires one I o read, maybe one I o write. And state can be changed with Utxos when they're spent, and it's all done at the native level. So when you're talking about supporting potentially billions of people over a transaction model, you really need to think about every single detail, and every single detail counts in terms of how efficient and how accessible your system actually is. So with state itself today, what we presented is this dehydration and rehydration at the native level.
05:45:00.380 - 05:45:35.480, Speaker A: And again, with state, we haven't really fully defeated everything. State growth is still going to be an issue for any system, but we do have so many tools now to fight it. And embracing new kinds of transaction models and new kinds of ways to do blockchain will allow us to, again, have a very accessible and sustainable system for a long time, and one that can efficiently scale and compete in the market. And again, necessity is the mother of all innovation. It's definitely true here. And yeah, that's basically the presentation. Thank you.
05:45:35.480 - 05:46:03.150, Speaker A: Do you have questions? Yeah, I think we have some time. Okay, sure. Yeah. Happy to take any questions. Yeah, we got five minutes for questions. Okay, anything? Yes, sorry. Yes.
05:46:03.150 - 05:46:20.520, Speaker A: So scripts and predicates are fully programmable. And so every aspect of account abstraction or scripts are programmable. You can do whatever you want in them. Yeah, he's going to run Mike. I see. Okay. Anyone else? I think I saw two hand right here.
05:46:20.520 - 05:47:44.042, Speaker A: Hey. All right. Okay, so the question about the data availability, how did you guys spot the trend in data availability versus the execution? So what do you mean spot the trend? Well, the way I understood is that data availability is becoming more accessible, cheaper to say, and the execution is on the opposite side where it's becoming less available. And it seems like you're trying to adjust for that with your proposed architecture, right? Yeah. So the whole idea is that you want to use execution because that's going to be more available to the node right now. And yeah, data is in many ways becoming more and more solved because both of external data availability solutions, but also just opening up more Ethereum data is allowing us to do this. But as well, there is going to be popular configurations of not using even data availability and just allowing Ethereum to be a verifier of these things and allow people to construct really interesting chains that maybe achieve properties for gaming or something like that, where it's less of a concern.
05:47:44.042 - 05:48:09.666, Speaker A: So all I was trying to say is there's a lot of different great solutions to that for us. We really wanted to focus on trading off the problems of state to execution because you only have so many things to work with. And so in doing that you can achieve a lot more scalability because you're just reducing so much more. Read, write to the hard drive itself. That makes sense. Thank you. Sweet.
05:48:09.666 - 05:49:00.934, Speaker A: And I think Bartek, right? Is that Bartek? I can't see. Yeah, got one more over there. And then I think that's it for this. Did you have one? Another question. Hey, Nick. I guess this is a largely ethereum community, so we might not be as familiar with UdXO models, but I'm kind of curious, how does that compare with Cardano and what they are doing today? And what they plan to do in their next version or whatever. Yeah, so I talked about this at VM day, but effectively, to say it very politely, and I know I'm in an Ethereum crowd, so I can be maybe a little more candid, but some specific project really ruined a lot of great things about Utxos from the branding level.
05:49:00.934 - 05:49:41.470, Speaker A: Basically, Utxos themselves can do a lot of amazing things, but in the way that Cardano designed Utxos, they made the whole system too deterministic. And what this ended up creating was this problem where you had to do every transaction one after the other, and it was extremely serialized and bottlenecked. So their Utxo design was. The issue was not the Utxos, it was the way they were applying determinism to Utxos. So the subtlety is that with Ethereum, state is more malleable. So when you make a transaction to uniswap, you don't necessarily know all the state because it's still forming until the block is actually produced. So state is in a malleable phase.
05:49:41.470 - 05:50:25.140, Speaker A: But with Cardano, state is not in a malleable phase. You have to know the state before you transition it and know it after, which you can imagine is like a really awful design to deal with. So for fuel, we take Ethereum's nice properties like this more state malleable design, but we apply it to Utxos, which means Utxos can act as effectively a state access list. So we can do full parallel processing with Ethereum style smart contracts, but still gain all the benefits of. So effectively, we don't have any of the same issues that Cardano has because we've learned a lot from both Cardano and from Ethereum and from these other models. And we've applied that to the learnings and improved the model significantly. That makes sense.
05:50:25.140 - 05:50:53.714, Speaker A: Yeah, we got one more. Is he good over there? Yeah, there's one more here. And then we got to cap it. Yeah. Cool. To what extent is this focus on state unique to fuse compared to other l two s? Sorry, I need to fuse or fuel? Sorry, fuel. Didn't mean to get your name wrong.
05:50:53.714 - 05:51:08.754, Speaker A: No, it's fine. There's too many projects. Yeah, sorry. And the question was? Sorry, could you repeat the question again? Sorry, now I lost it. Yeah, I won't say fuse this time. It's fine, it's fine. Okay.
05:51:08.754 - 05:51:50.120, Speaker A: Now I'm like all over the place. To what extent is this focus on state unique to fuel compared to other l two s? Yeah, so it's very unique. Basically most layer twos are not thinking about designing something that's sustainable. And so a lot of projects just sort of punt this problem to later. Whereas for us, we wanted to try to holistically address it at the design stages of our virtual machine. And the main reason is because it's not just the cost of verifying things that sucks, it's actually the cost of running all the infrastructure. And so our plan is to have thousands, maybe even millions of fuel roll ups, all gaining the benefits of this system.
05:51:50.120 - 05:52:21.694, Speaker A: But to run the infrastructure for all of those roll ups is enormous. And if you don't actually address these issues of sustainability, state how you use blockchain in general, you're never going to be able to get there. So we wanted to more holistically address these issues at the early design phases and come up with something that we knew would be sustainable. So, yeah, very unique. I would say most teams just punt this problem to later. And by the way, this is still a problem with ZK roll ups, too. Like, you can't get away from it.
05:52:21.694 - 05:52:42.966, Speaker A: You still need to know the state at least to be able to continue the ZK roll up creation. So basically, this is a problem that everyone has, and it's something we're uniquely. Yeah, guys, thanks so much. Thank you, Nick. Thanks, everyone. Give it up. Cheers, man.
05:52:42.966 - 05:53:03.180, Speaker A: Have a great rest of your day. Thank you. So we're going from Nick Dotson to Nick White, from one Nick to the next Nick White, wherever you are. The landscape of DA solutions. Oh, there he is. What's up, man? Give it up for Nick. Here, you can grab this one.
05:53:03.180 - 05:53:29.782, Speaker A: There's your little clicker there. Thank you. Cool. Good luck, man. Hey, guys, I'm Nick White. I'm the COO at Celestia Labs. And the title of my talk is, once it comes up, I'll just give it some time.
05:53:29.782 - 05:54:14.194, Speaker A: Well, basically, I'm going to talk about data availability. And the title of my talk, once it pops up, is, data availability is real, in parentheses, important. So what I want to talk to you about and try to convince you of is that despite what people say, data availability is a real problem that needs to be solved. And it's a really important problem because it is the basis of security for l two s and roll ups and blockchains in general. So we're here at l two days. And so I don't need to tell you guys this, but we have entered the modular era. In the last year, the number of projects building modular blockchain infrastructure has more than doubled and continues to grow.
05:54:14.194 - 05:54:55.774, Speaker A: And bit by bit, we are moving towards a world where it's going to be as easy to deploy a roll up as it is to deploy a smart contract. And so what this means for the crypto ecosystem is that on l two b right now there are 30 some roll ups that are listed, but in the future, I would say sometime next year, that number is going to be in the thousands. So this is dimension. It's a cosmos SDK roll up framework. And in their incentivized testnet, there are over 11,000 roll ups that have been deployed. So the number of l two s are going to grow exponentially. But it's not just the number of them.
05:54:55.774 - 05:55:31.646, Speaker A: It's also the diversity of different kinds of l two s that are going to be built. And this is because there's going to be so many different components that you can combine. You're going to have so many different new combinations that people haven't thought of before. And this is good because we like to say our mantra in the modular ecosystem is build whatever. And this means that people should build whatever they want. They should experiment, they should play around and take advantage of all these new tools and these modular pieces of infrastructure that we're building. But some people will take build whatever too far.
05:55:31.646 - 05:56:25.422, Speaker A: I'm just kidding. But what happens in this new paradigm is you have these novel combinations that are kind of confusing at first glance. So eclipse is a good example of this, where it's kind of baffling the first time you hear it. They're using Solana for execution, they're using Celestia for data availability while settling to Ethereum and also using risk zero as their approving system. And so we're going to see more and more of this kind of pattern. And so it's going to be really important that as a community, we start to develop standard ways of thinking and evaluating the security and the trade offs of the different components in the modular stack from which we build l two s and roll ups. So specifically in this talk, I want to talk about the data availability component and all the different data availability solutions out there and what their trade offs are.
05:56:25.422 - 05:57:10.346, Speaker A: So first, let's talk about what is data availability. So it's kind of a confusing name. A better name might be data publication. And essentially what data availability is about is verifying that some data has been published. In other words, that data is available for anyone to download. And you need this when there's a new block or some kind of update to a roll up or a blockchain because the nodes that want to verify that new block need to know the data that's in it, or at the very least, they need to know that the data is published, that it's available for other people to see. And data availability is not data storage.
05:57:10.346 - 05:58:11.200, Speaker A: Data storage is about retrieving old data from the past, whereas data availability is about verifying new data, fresh data that is being published in regards to some new rollerblock. So as I said at the beginning, it's really important to get this message across, that data availability is critical to the security of l two s. And this is because if the data is withheld, then the roll up operator can freeze your funds or steal your funds, steal bridge assets, and essentially break the rules of the chain. And this is not a world that we want to live in. We want to have roll ups where we can trust that things are going to go as planned and we're not going to be sort of rugged, essentially. And data availability is a critical component of ensuring that. This is an example of sort of what a data withholding attack looks like in practice as taking an optimistic roll up as an example.
05:58:11.200 - 05:59:11.810, Speaker A: So you have a malicious sequencer, they try to push a fraudulent state update to the bridge contract, but the transaction data is also made available. And so you can see that the honest roll up full node is able to generate a fraud proof from the available underlying data and stop that from happening. But if the malicious sequencer is actually able to withhold data, then no fraud proof can be created and the funds in the bridge contract can be stolen. So this is an example to illustrate the point of why data withholding or data availability is important for security. So now I want to talk a little bit about the desired properties for any data availability solution. So the first desired property is you want your DA solution to be scalable. And the way to think about what scalability is in the context of data availability is you want to be able to verify lots and lots of data is available with a minimum amount of work.
05:59:11.810 - 06:00:00.100, Speaker A: So the more data you can verify with less work, the more scalable a DA solution is. And second, we have to think about what is we want our data availability solutions to be secure. And essentially you can think of this as inversely related to the strength of the trust assumptions for that data availability solution. So if you're making really strong trust assumptions, then the security of a DA solution is really not that good. If you have weak and or no trust assumptions, then the solution is very, very secure. And now that we have this background covered, I want to actually go into the more meat of this presentation and talk about the trade offs of different data availability solutions. First, to start off we're going to start with some reference points.
06:00:00.100 - 06:00:48.110, Speaker A: So the first reference point is a full node, also known as the OG data availability solution. And this is how most layer one chains solve data availability today. It's that you simply download all the data that you want to verify and ensure is available. You download all of it. So you don't actually have to make any trust assumptions here because the data is either there you have it on your machine and you can see it, you're holding it in your hands, or it's not, and the data is not available. So this is really good because it's as secure as you can get. You're not making any trust assumptions, but the downside is not scalable because the more data that you want to verify, the amount of work you have to do to verify the data scales linearly with the amount of data you have to verify.
06:00:48.110 - 06:01:55.590, Speaker A: So if a block increases by ten x in size, you have to have ten x the amount of bandwidth to verify the DA. So it's just fundamentally not a scalable solution. And the second reference point is like no DA whatsoever. I call this like JTMB or just trust me bro da, because essentially you're just a handshake agreement with the operator saying hey, I believe that you're not going to rug me. And the good thing is obviously that costs nothing to verify, you're not doing any work, but it's probably maximally insecure in the sense that you are trusting a single counterparty basically on their word not to cheat you. So now with those reference points covered, let's transition to sort of the first class of data availability solutions, the first being data availability committees. So a data availability committee is a group of nodes whose job it is is to basically verify DA on behalf of other people.
06:01:55.590 - 06:03:06.078, Speaker A: So instead of having everyone have to download and verify the data directly themselves, they kind of delegate their trust to a committee of nodes who are supposed to do that and then sign a commitment that the data is available. And once a quorum of this committee is reached, then someone can verify that there's enough signatures and will trust that the data is indeed available. So the benefit here is that it's very scalable because you just have to verify a bunch of signatures and the number of signatures doesn't necessarily change with the amount of data that they're committing to. So it's quite scalable in that sense, but it's really not very secure, because the committee could turn out to be malicious, and there's a lot of incentive for them to be malicious. They could steal funds, they could freeze your funds, all the things we talked about, the whole point of blockchains is to get rid of committee based assumptions and have trust minimized solutions. So a committee, to me, DAC, the C doesn't just stand for committee, it stands for centralized. The next type of DAC is a little bit more interesting.
06:03:06.078 - 06:04:11.822, Speaker A: It's a DAC with crypto economic security. So in this model, again, you have this committee, they sign over a data commitment, but rather than just putting their word when they sign, they also put some kind of economic stake. And what this means is that if they decide to be malicious, then the stake associated with the funds they put at risk will be burned. So there's an economic consequence, a penalty for misbehavior in this model. And so instead of having to assume that the quorum of this committee is honest, you can instead assume that they are rational, meaning that they will not commit this attack if the penalty for doing so is greater than the amount of money that they have to gain. And so this is slightly more secure. But again, the security is limited to the fact that you need to have less funds on the system of l two s on top of such a DAC than the amount staked.
06:04:11.822 - 06:05:22.010, Speaker A: So you always have to have more money staked than that is at risk. And so it really limits the amount of sort of security that you can get out of the system. And this also brings me to a really important point that I think a lot of people are not aware of, which is that any DA solution that purports to have crypto economic security must have its own token. It must have some form of native asset to socially slash. And that is because data withholding has this property that it can't be proven on chain, unlike a fraud proof for execution or ZK proofs for proving validity of execution, you can't prove to on chain actor like a smart contract, that data has been withholded, withheld, and essentially you can only socially slash. There has to be sort of a social consensus among the participants in the DA network to slash those funds when there's data withholding. So as a result, you need to have a native token to be able to actually hold the DAC, or any DA solution accountable for misbehavior.
06:05:22.010 - 06:06:37.070, Speaker A: So, for example, if you wanted to have a DAC that uses ETH for collateral, that ETH is actually not contributing to the underlying security, because you can't prove to the staking contract that hey, you should slash these guys because they just withheld data. There's no objective way to prove that. And this is due to the fact that data withholding is unattributable. And this relates to this old problem called the fisherman's dilemma, which I encourage people to go read if they're interested. Now I want to transition to the next class of data availability solutions, which all use data availability sampling. So data availability sampling is a new cryptographic primitive, where instead of having to download a full block of data to verify that it's available, you can instead sample randomly from the block data and in that way have a very high sort of probabilistic guarantee that the entire block is available. And the nice thing is, not only does this reduce the amount of data you have to download, but it also does not increase very much when the size of the block increases.
06:06:37.070 - 06:07:41.240, Speaker A: So it has a really nice scalability property in the sense of the work you have to do doesn't grow as fast as the data that you can verify. And how it works under the hood is that you take the original data you want to make available, you extend it using erasure, coding the nodes in the network, sample random chunks, and each successful sample is sort of like a coin flip landing on heads. And every time it lands on heads you have a higher confidence that the entire block is available. And you just need to ensure that there's a minimum number of nodes sampling that will be able to reconstruct the original block. And what's cool about this is such a light process that enables a new type of node that's never existed before, called a light node, which you can even run on your phone. So you can actually be gone are the days of running a full node on your laptop or in the cloud. You can actually be running something with full node level security in your pocket or in the palm of your hand.
06:07:41.240 - 06:08:50.380, Speaker A: So now let's talk about the actual solution. So the first DA solution is data availability sampling without reconstruction. So in this world, the light nodes and the full nodes all sample from the block producer and people are able to verify that the data is available when they successfully sample. But you have an assumption that essentially the honest full nodes are able to download enough of the data to be able to reconstruct it amongst themselves. But that might not always be the case because the block producer might know, hey, these are full nodes and these are light nodes. I'm only going to serve samples to light nodes and what happens then is that you have all the light nodes think that the data is available, and in a sense it is because they've all sampled it and collectively they have enough data to reconstruct it, but they have no means of actually recollecting the data and sort of like putting the pieces back together. So in effect, the data is not available.
06:08:50.380 - 06:10:02.770, Speaker A: And so this is significantly more secure than the DACs, but still not as secure as we would like. And so this brings us to the next Das solution, which is data availability sampling with reconstruction. So in this world, even if the block producer makes sure that the honest full nodes are not able to sample data and only light nodes can sample, the light nodes can kind of cooperate with an honest full node to take all their individual samples and reconstitute them back into the original data. So now this attack where you're fooling all the light nodes, that the data is available when it isn't is no longer possible. However, there's still one remaining problematic assumption, which is that you're not among the first nodes to sample. So there's a type of attack called a selective disclosure attack, where the block producer will let a limited number of light nodes sample, but not enough that they have enough data amongst them to reconstruct the original block. So it's not able to fool every light node, but it's able to fool a subset of the first light nodes that sample.
06:10:02.770 - 06:11:08.502, Speaker A: It limits the downside of this attack, and it may be harder to pull off. So it's more secure than one without reconstruction by quite a bit, but it's still not quite as secure as we would like. And this also explains a very important property about data availability, which people might not be aware of, which is that you cannot do data availability sampling on chain because it's trivial to perform a selective disclosure attack on a smart contract. Everyone can observe what samples the smart contract wants, and the block producer can just serve those samples. And then the smart contract thinks, oh, the data is available when it's the only one that has sampled. And so this is unfortunately why you can't have data availability sampling has to happen on its own network at the l one. And so finally, this brings us to the last data availability sampling solution, which is anonymous data availability sampling.
06:11:08.502 - 06:12:17.550, Speaker A: And in this solution, the light nodes and full nodes and everyone are sampling using some kind of private network, maybe like a mixnet, but something that obfuscates the identity of who is actually sampling, so that the block producer cannot distinguish whether it's a sample request coming from a light node or a full node. And in this way the block producer can't target the light nodes to fool them. It gets rid of the selective disclosure attack, however. And the nice thing is that the remaining assumptions are just that you have enough light nodes to reconstruct the block and that you have a synchronous network. And neither of those two assumptions are very strong, they're quite weak. So the security of the system, it gets very, very close to that of a full node. But the difference is that you have this really wonderful scalability property of data availability sampling, because the amount of work that you do does not grow very quickly as the size of the block that you're verifying grows.
06:12:17.550 - 06:13:21.730, Speaker A: And so to sum things up, amongst all the data availability solutions that we talked about today, the only ones that can actually achieve both scalability and security are ones that use data availability sampling. So full nodes are secure but not scalable. DACs are scalable but not secure. And so just to sort of sum things up and come first full circle, I hope I've been able to convince you that data availability is important. It's an important problem because it underpins the security of all the l two s. And also I hope I've been able to communicate why, if any, data availability solution purports to have crypto economic security, it needs to have its own token. And last but not least, data availability sampling is really the gold standard when it comes to dA solutions that can be both scalable and secure.
06:13:21.730 - 06:13:54.880, Speaker A: I would love to continue the conversation on Twitter if you have any questions. And also I'm happy to take any questions now. And thank you guys for your time. Is there anyone that has questions in this moment right now? I got a microphone here. I see you in the back. I'm going to make my way up to you. Do you have a question? Ajit, what's up? Got it.
06:13:54.880 - 06:14:55.586, Speaker A: Just for the recording. Yeah, talk to the mic. So what is the trade off in, let's say, using an optimistic approach versus two dimensional KZG reconstruction? So you're talking about whether to use KZG commitments or to. So the difference there is that. So basically there's another thing that we didn't get into, which is in data availability sampling schemes, there's a potential for the block producer to basically do a bad encoding. This is that they pretend to extend the data, the original data, but they don't. And so what happens is if you wanted to reconstruct the block, you're not able to because it's just bogus and so you have to have a way for the light nodes, so the full nodes can detect this, because once they have enough data, they can see, oh, this doesn't check out, but the light nodes don't have a way to check that.
06:14:55.586 - 06:15:25.690, Speaker A: And so there's two different approaches. One is to use bad encoding fraud proofs, which is what Celestia uses. And essentially what that is is if there's an example of bad encoding, there's a way to send. The full nodes can basically construct a proof. It's a little bit big. It's like one row or one column of the data, and they're able to verify that the encoding is wrong. Or you can use KZG commitments, which are correct.
06:15:25.690 - 06:16:10.262, Speaker A: They're kind of more like the ZK version, which are just like correct by their nature. I think the trade off is essentially we've chosen to opt for the optimistic one. The trade off is the optimistic one. You have a fraud window delay. And I believe the worst case scenario of the amount of data you have to download to verify the fraud proof is like one row or one column, which could get gross with the size of the block. And whereas the ZK version, I think it's like you don't have the fraud delay window to consider a block final. And I think maybe the worst case cost of verifying, you don't have to download that big fraud proof.
06:16:10.262 - 06:17:16.266, Speaker A: But I think the main downside is basically that generating and computing the KZG openings can be really slow. Although there's been a lot of research lately, and I think Dankrad's talk at SBC, he said that there was some new results of hardware acceleration for computing KZG commitments and things like that, that make it a lot faster and efficient. It's sort of similar, I would say, to what's happening between optimistic and ZK rollups, where it's possible to transition between the two. So once it's kind of like what I've heard Ed Felton say, like, well, when ZK is good enough, they would just transition from fraud proofs to ZK proofs. I think there's a similar thing that can be done in regards to data availability. So maybe once the technology is proven and secure and actually scalable fast enough, then I think it would be worth transitioning. But until that time, it has trade offs that are just not desirable from our perspective in terms of the scalability and the speed at which you can compute the KgG openings.
06:17:16.266 - 06:17:49.538, Speaker A: Thank you. Thank you for that. Sweet. Thanks for that. We got one more back here. Hey Nick, great presentation and congratulations on the recent launch. One question is, have you guys looked at this particular implementation of data availability sampling with different consensus algorithms outside of comet? So you mean like, can you explain what you mean? Sure, maybe I'm misremembering, but if I understand correctly, the Celestia fold.
06:17:49.538 - 06:18:34.710, Speaker A: Now let's download all of the data. And then there's a bridge node that translates that data and distributes it to the light nodes. Have you looked at a process of doing that same implementation with a different consensus algorithm where maybe the full nodes operate a little bit differently? Yeah, that's definitely an option. For example, avail uses babe and Grandpa, I think from sort of the Polkadot stack. And so there's different trade offs with liveness and finality and things like that. And so it's something that we've considered. But I think for us, tendermint or comet, BFT is the most battle tested and has a lot of really desirable properties like instant finality.
06:18:34.710 - 06:19:18.920, Speaker A: And so for now, it's sort of like the solution that we think is best in the future. That could change as well. But there is also sort of like this idea that the consensus part of the protocol and the data availability part can be somewhat independent of each other, where you can have the sort of consensus nodes commit to a certain order of the data, have consensus blocks, and in parallel be also generating the extended data availability blocks that people sample. So I think that's probably the more interesting thing to explore on that front. That's cool. Thank you. Thanks man.
06:19:18.920 - 06:20:04.020, Speaker A: I have two microphones I forgot. Almost fell down the stairs, I think. Does anyone else have any questions? I left my phone on the stage. I'm wearing a watch but the battery is dead. His question also reminded me of something else that I think could be an advantage of KZG's, which is that you could actually have a world where the consensus nodes who are voting on the validity of the blocks don't actually have to download all the data to verify it because they can also just sample. So currently in Celestia, all of the consensus nodes actually have to download the full block before they vote on it. Or at least they should do that.
06:20:04.020 - 06:20:46.334, Speaker A: Cool. Going once, going twice. Hey cool, my budy's got you. Hey Nick, congrats on the main net launch. Thank you. Just quick question you mentioned about, I think in your slide there was a point about you need to have a native token in order to enforce slashing. Can you expand on that? I was kind of thinking if there's a way for a smart contract to sort of have a staking key and for them to sort of leverage Ethereum's own token for such purpose.
06:20:46.334 - 06:21:28.062, Speaker A: Yeah, so it's a really nuanced, say, let's use Ethereum as an example. You have ETH staked in a contract, and this is supposed to be stake, that is securing some kind of outside da protocol. What happens is if there's a data withholding attack, you can't prove to the contract. There's nothing you can send to the contract to objectively prove that data withholding happened. You can't just say like, hey, well, here's the missing data. It's not something that is like an objective thing. You have to observe it directly yourself, if that makes sense.
06:21:28.062 - 06:22:03.014, Speaker A: So you have to be a node in the network trying to sample and not succeeding. And that way you know for yourself that the data is unavailable. But I can't tell, my friend, hey, the data is unavailable, trust me. It's something you have to verify directly through the sampling process, if that makes sense. And so when it comes time, let's say that data withholding does happen. Now, in order to slash those validators or those people who are operating that network, you have to prove something to that contract, but you can't. And so there's no way that your hands are tied.
06:22:03.014 - 06:22:43.682, Speaker A: Essentially, the only way to slash for data withholding is through social consensus. So in Celestia, what would happen if there's data withholding is all the nodes in the network would see it. They'd all halt, and then the community would come together and basically be like, yo, we all observed that the data was withheld, right? Okay, we all agree on that. Let's slash all these validators, remove them from the network and restart. So that's just sort of like the fundamental truth of a data withholding attack. And so like the fisherman's dilemma in that diagram, if I could pull it back up, it's like, let's say you're an outside observer. There's the malicious block producer and someone who's raising alarm.
06:22:43.682 - 06:23:38.742, Speaker A: So like the fisherman, he's like, hey, that data is missing in this block, right? He tells you that to you, you're like the third person. What could happen is the block producer could then all of a sudden reveal the data and be like, oh, and then the data is there, and now you can't tell whether is the fisherman lying or was the block producer actually withholding data? You can't distinguish between those two scenarios, essentially. So maybe that's helpful in explaining that social slashing. You can only social slash an asset that lives on your protocol, even if the community of the data availability solution or network outside of ethereum agrees. Hey, we should slash that eth in that contract. They're not ethereum. They can't actually use social consensus to perform that slashing, if that makes sense.
06:23:38.742 - 06:24:13.106, Speaker A: You could maybe if you had in the contract was just like a governance thing, but then when you're staking in there, you're basically staking and putting your funds up for any kind of arbitrary governance to slash you. So that's kind of the conundrum. Did we get the microphone back? You have a question now, do you? Yeah, sure. Yeah, we can get you. I love this, by the way. We're going over, but you're dope. You're rolling with it.
06:24:13.106 - 06:25:17.030, Speaker A: There's a vibe going, I'm not going to stop you, but thank you. Yeah, keep it up, man. So I'm thinking about long term data accessibility. Because you sampled the data, you know that it's been published, but you don't know for sure that it's going to be available from the same celestial full nodes, like, let's say one year from now. But you had a wind of opportunity to download the data yourself. So is the idea that if you're running a roll up or an application on top of celestia, the data is published, that you, as a roll up on top of celestia, it's your responsibility to download that? Or are you thinking that maybe you resample some old data every once in a while? What's sort of the thinking on how you're going to make sure that long term, the data is also available, not just in the initial sampling stage. Yeah, so this touches on a lot of different things, one of them being sort of the difference between data storage or data retrievability and data availability.
06:25:17.030 - 06:26:14.362, Speaker A: And so the way that celestia is designed is that there will be pruning such that after a certain window of time, let's say 30 days, the data of those older blocks will just be removed, or at least no one is supposed to or required to store that data. And so sampling will, you can't sample from the consensus nodes necessarily for that old data. This is fine, because what data availability is trying to do is prevent safety faults or people basically causing invalid state transitions. Right. Or freezing your assets. So that matters in the immediate time when the block is produced or within that window. And then the more time has elapsed, the importance of preventing that is not as great, if that makes sense.
06:26:14.362 - 06:27:13.390, Speaker A: And the difference between data availability and data storage is that data availability is an honest majority kind of attack, and data retrievability is sort of honest, sorry, malicious majority, dishonest majority. And data retrievability is an honest, minority thing. You just need one person to store a copy of it that is willing to save it or, sorry, to serve it to you later. And in general, many chains have kind of adopted this subjectivity window, right where it's like, after a certain amount of time, it's like you just trust that this was what the state of the chain was. And there's a lot of different solutions. You can actually dump all the state on chain if you wanted to, but in general, there's going to be a lot of incentives to store the data because people will be running indexers, people will be running explorers. Or maybe we're also going to implement partial nodes where if you want, you can just listen to a specific namespace and make sure that that's the one that you want to store, for example.
06:27:13.390 - 06:27:42.040, Speaker A: So I think there's going to be enough incentives for at least one copy of the ledger to always be around. It just might not be as easy and abundant to sample from, if that makes sense. Awesome, Nick, awesome presentation, and congratulations on launch. Thank you, Eric, you're the man. I think John has a question. John's over here. Wow.
06:27:42.040 - 06:28:59.360, Speaker A: Thoroughly impressed. Yeah. Congratulations on the lounge. Thank you. This is going to be somewhat of a general question, but what do you think? Now that we have data sampling light nodes for the first time ever, what do you think the things that needs to be done by the ecosystem to increase the number of data sampling nodes? You can answer this from the tooling front, engineering front, but also on the educational front, and also the follow up to that is that what signs should we look up for to track the number of data sampling light nodes? What are some indications of traction there? Yeah, so great question. And my talk at modular summit was about this question, essentially of explaining the importance of light nodes and also talking about how we can ensure that we have a robust and vibrant light node community. And so the first part of it is that is kind of an engineering thing, which is like, how can we make light nodes easy to be run? Like meet the user where they are, right.
06:28:59.360 - 06:29:51.102, Speaker A: I can run a light node right now on command line on my terminal, but the number of people who can do that and or are willing to go through the effort of doing that is very low. So instead, what we need to have are light nodes that can be run in wallets, like in applications, on people's phones, in the browser when they're using different applications. But the thing is, depending on how the node is built and what underlying languages it's sort of built in, it could either be very mobile friendly or browser friendly or not. And so it's about creating sort of like mobile browser friendly versions of the software. And so that's kind of one of the gating things. And it's also working with these wallet teams to actually do the integration. And I actually think avail had something really cool recently where they have, it's just like a website.
06:29:51.102 - 06:30:47.270, Speaker A: Like you just type in a URL and it takes you to this page and you just click a button and then it spins up a light node for you. And also Mina had something similar as well. And so I think that's kind of the future of where things will eventually. So aside from the technology side, there's the social part, right? And we have to a educate people so that they understand the importance of it, right? That your light node is literally securing your digital world. It's how you vote on what you believe in and make sure that the system is not taking advantage of you. And so when people's digital lives are increasingly on chain, or not even the digital lives, just in general, the majority of their wealth, majority of their transactions, the way that they interact with the world is on chain, I think it's going to be a no brainer. It's going to be like, when you leave your house, do you lock the door or do you leave it unlocked? It's not much effort.
06:30:47.270 - 06:31:07.720, Speaker A: It's kind of a pain, but not really. You always make sure to lock it. So it's going to be a similar thing of just building it into the culture. And I like the idea of in the privacy community, there's this notion of private by default. I think we need to have a similar thing in crypto, which is like verify by default. So people don't even think about it. It's just built into the way that all these systems work.
06:31:07.720 - 06:31:56.230, Speaker A: And then in terms of tracking, one of the things that we are encouraging some of the block explorers on celestia to do is to have sort of like telemetry and ways of tracking the number of light nodes. Unfortunately, it will always be possible to sibyl the number of light nodes, but I think given reasonable assumptions, we can always keep things secure and track that number. As far as I know, we're probably in maybe in the hundreds of light nodes right now. That's pretty cool. Yeah, that's a great question and very close to my, just from my perspective, marketing guy don't know anything about tech. What is a Sibyl attack? I keep trying to look it up on Google. So it's essentially a single person trying to pretend to be multiple.
06:31:56.230 - 06:32:49.446, Speaker A: So let's say naively, if you wanted to run a chain and you just said, like, well, the block with the most number of votes is the one that we accept as the next block. The thing is, one person can pretend to be lots and lots of votes. So you have to have some kind of, like, scarcity or some kind of cost to voting to make it uncivilable. So that person can't just become a billion people because that's too expensive for them. In the case of, like, a light node. Light node thing, someone could, let's say, want to inflate what people perceive to be the number of light nodes such that they think, oh, well, we can have this size of a block and be secure because there's enough people sampling and then actually they do a data withholding attack, and all those light nodes turn out to be fake. So you can't actually reconstruct the block.
06:32:49.446 - 06:33:31.000, Speaker A: But, yeah, basically a civil attack is like people pretend someone, some evil person trying to pretend to be multiple people to sort of exploit the system. Yeah. And in terms of exploitation, can that be like, I want to even dumb it down even more? Is it something kind of like bodding similar? Yeah. Okay. That's, like, the only way I've been able to put it in my brain in order for it to work. But it seems like I just read it somewhere in a product that I use and never really understood it beyond just, like, the bot thing. But thanks for explaining that.
06:33:31.000 - 06:33:50.350, Speaker A: My pleasure. And anyone else in the crowd, because I know I just kind of stole the shine there for a little bit, but looks like we're all set. Wow. Thank you, guys. Would love to talk more about data availability. Come find me. Clap it up for Celestia.
06:33:50.350 - 06:34:03.178, Speaker A: Congrats, man. That was really great. Good job. Good job. Yeah, we have data accessibility. Right. Availability.
06:34:03.178 - 06:34:23.110, Speaker A: Excuse me. So I think data accessibility. Yeah, data availability is up next. Your budy Mustafa is going to be here. Yeah. Cool. So if you guys are interested about learning more about Celestia Mustafa will be here very shortly.
06:34:23.110 - 06:42:12.762, Speaker A: We have Bartek running the, moderating the panel. Honestly, you guys should just chill. There's only, like, eight minutes left until we start the next panel. See here? Yeah, we're going to have someone from ETh foundation, avai op labs, and Celestia. Yeah, thanks for queuing up the music. We'll pick it up in, like, not even five minutes. Check.
06:42:12.762 - 06:42:34.550, Speaker A: Check. Sweet. Tesekule it. We're going to start our next section here. It's 401, so we're going to do data availability. This is an hour long panel. It's going to be pretty in depth.
06:42:34.550 - 06:43:00.442, Speaker A: I'm pumped to have these guys up. Bartek Kipushevsky is our moderator from l two beat. So we're really gracious to have him up here. Give it up for Bartek. Feel free to take a seat. Grab a microphone, panelists. There's a pineapple called proto Lambda.
06:43:00.442 - 06:43:23.410, Speaker A: Apparently there's a pineapple coming. Mustafra from Celestia. Give it up for Mustafa Anurag from Avai, which is anurag. Okay, so we got another guy from Avai here. You'll introduce yourself in a second. Dankrad, if I'm not mistaken. Dankrad.
06:43:23.410 - 06:43:40.970, Speaker A: Who's the pineapple? They told me a pineapple's coming, man. I don't know. You don't? Thank. I kind of see it kind of have a pineapple vibe to you. Take it away, guys. Give it up for the panel. Tune in.
06:43:40.970 - 06:44:16.150, Speaker A: Thanks for coming. All right, guys, let's start this amazing panel. It's just incredible opportunity for me to host da stars. I think I hope to learn something in this panel. I hope you will learn, too. And we've got full hour, so we've got plenty of time for, hopefully, a lot of questions. And at the end of the panel, we'll ask the audience for questions as well.
06:44:16.150 - 06:45:23.990, Speaker A: So be prepared. But maybe before we get too spicy or too cocky, let's just start with something nice and sweet. And I'd like to learn a little bit about the backstory of each respective project. So maybe I'll start with Dunkrat and Proto, because when I learned about proto dunk sharding, I kind of assumed in my naivety that it's just a prototype of dunk sharding, right? And by the way, a lot of people are saying dank sharding, which maybe they think is about danking something, but it's like Dankrad, right? So, guys, please introduce yourself, maybe, and tell us a little bit how it all started. Why protodank sharding? Like, what's going on with this name? Dunkrat, proto or however you like. I guess I'll start the proto part first. So while we used to work together on a research team on Ethereum, two on the launch of the beacon chain.
06:45:23.990 - 06:46:00.018, Speaker A: And then as the beacon chain became a thing, there were new challenges that we wanted to start solving. And part of that was data skating. Part of that was the merge. And we started with phase one. We started with this idea, okay, we want 1024 shards. And while the sharding design just looked very different, and then this evolved. In the meantime, there was this pressure to add proof of stake to Ethereum.
06:46:00.018 - 06:46:35.634, Speaker A: So the merge was prioritized. But then in the meantime, this dank sharding design came together. And this is a very ambitious design. I think Dankat can comment on that very well. Where it comes with network challenges, cryptographic challenges. And because it's so ambitious, and we already have roll ups today, we have this pressure on the market for data. We wanted a version like a stepping stone towards dank sharding to not try and roll out everything at once.
06:46:35.634 - 06:47:35.286, Speaker A: And this was then conceptualized with Dunkard and Vitalik and got nicknamed Proto Dank Sharding because I helped productionize it or realize it in an implementation at a hackathon. But really it's the first step towards the field design from Duncan and Vitalik. And how does it feel like for you, Dankrad, to hear your name all the time, mentioned everywhere? Well, I'm not going to comment on that. Yeah, we pretty much, I think, started at the same time in the Ethereum research. So we had a fun few years together and I think to comment on what Proto said on it's a very ambitious design. What I suggested, I think. When was it? End of 2021.
06:47:35.286 - 06:48:37.498, Speaker A: I started bringing together some ideas from what we learned about Mev, that it basically started to be necessary to have more decentralized block building. And what we had learned about the cryptography, that polynomial commitments became practical over the course of the past two years for something like sharding. I started putting these together and actually made, in a way, a greatly simplified sharding design. I think Proto is correct. It is still ambitious. It's probably still ten times easier than what we were working on before, which was like a very fiendishly complex design where everyone was like, oh, given the difficulty of the merge, how are we actually going to get this there? And yeah, that design that I was working on then became later named Dank Sharding. And all of the components were actually there before I just brought them together at that time.
06:48:37.498 - 06:50:00.526, Speaker A: And actually already a few months later, we were like, okay, how do we quickly get something shipped that, to be fair, doesn't actually bring the scaling of sharding, but gets introduced all the cryptographic components so that we can start the scaling very soon after. Thanks so much. Next to you, Mustafa. Well, first of all, congrats for the launch of Celestia. Probably most of the audience knows Mustafa is the founder of Celestia, but I took a liberty of peeking at your Wikipedia page and I was like, what? How many lives have you lived? And that sort of brings me to the question, with your really long journey in all sorts of things, computer related and blockchain related, how eventually you decided to focus your research and work on something as obscure, I guess, as a data availability problem, trying to explain to people what it is when almost nobody understood. So I'm kind of curious because that seems to me like an interesting life choice also. Yeah, I mean, I first got interested in decentralized systems by using bittorrent before bitcoin even existed.
06:50:00.526 - 06:50:54.102, Speaker A: And I thought it was kind of like a really cool example to show that you can create decentralized web. And I was thinking, if you can create decentralized file sharing, how can you do other kinds of decentralized applications? And then bitcoin came along and that was really exciting. And then in 2013, bitcoin block sizes started getting full and the bitcoin transaction fees were extremely high. But the bitcoin community did not want to increase the block size because they thought it would increase the cost for end users to validate the chain. So then I decided to make it my life's goal to figure out how can we increase block sizes on a chain without reducing the cost for end users to verify the chain. And then so I did a PhD at UCL where I was focusing on layer one, scalability, and we were focusing on execution sharding. And this was back around the time when Ethereum 2.0
06:50:54.102 - 06:52:02.006, Speaker A: had this, as Dankrad said, finlify complicated Ethereum 2.06 phase execution sharding roadmap with 1024 execution shards. And the one missing thing from that design that people weren't really discussing is, okay, we can have all these shards, that's great. But what happens if the shards misbehave? Because the whole security model of bitcoin and Ethereum is that you're not supposed to trust the miners. All the validator set, even if they misbehave, you as the user can run a full node and verify the chain. And the solution was, okay, well, what if the shards have fraud proofs or ZK proofs to prove their validity? But then the difficult part was, well, if you have fraud proofs or ZK proofs, what if the shards don't release the data and no one knows what the state of the chain is? And what if no one can create fraud proofs? And then that's when I saw this note from Vitalik on GitHub. It was like a really obscure note proposing a solution to this problem called data availability sampling.
06:52:02.006 - 06:53:02.670, Speaker A: And that was really interesting to me, and I decided to kind of pursue that further. And then I co authored a paper with Vitalik, kind of like fleshing out the design of the system, and I created a prototype. And then I started thinking more about what a blockchain fundamentally is. And I realized, well, this final piece of the puzzle to making sharding work is actually the core component of what a blockchain actually is, just data availability. Fundamentally, a blockchain is just a proof of publication layer that makes data available. So I figured, well, what if we just made a blockchain that just only does that and leave the rest to developers? And that's where the idea of lazy Ledger came about, which is this just a very lazy ledger that only does data availability. Originally I was trying to make it an ethereum l two, but you can't prove data on chain, so you can't make it ethereum l two that has a data availability layer as l two.
06:53:02.670 - 06:54:16.130, Speaker A: So that's why it's now like l one that has a kind of a clean state with no execution environment called celestial. And last but not least, prabal avail. The first time I've heard about Avail was a few years ago when Polygon announced their own data availability solution. It was at the time when polygon, literally, to my mind, wanted to have it all like optimistic change, ZK change data availability. I mean, anything really Polygon wanted to do, right? So we kind know were wondering what's going on there. And then suddenly, if I understood correctly, you guys became essentially an independent team, right? So what's the backstory here? Yeah, so I think I joined Polygon back in 2020, the second half of 2020, and at that time already Polygon had shipped the PoS chain, which was when they started making it. It was a plasma chain, so they started making it as a plasma chain.
06:54:16.130 - 06:55:31.920, Speaker A: And while developing, they realized that data availability is going to be a crucial problem, apart from transaction structure and so on and so forth, which already plagued the plasma scaling solutions. So what they did is they had plasma, but they also had POS to make sure that the data is actually available on the POS chain. And before my joining, I had worked with proof of retrievability and so on during my research days. And that's why when I joined Polygon, one of the main problems that I started working on is data availability. And of course, at that point, Polygon had already realized that data availability is going to be a crucial problem that definitely needs to be solved in order to scale Ethereum. And that's why not only after that we have seen that they acquired a lot of the ZK teams and started building polygon, ZK, VM and so on and so forth, along with other solutions. And so within Polygon, we started working on avail, we started thinking about what is the main theme around which we can make this scaling possible.
06:55:31.920 - 06:56:46.914, Speaker A: We kind of learned that database sampling, as Mustafa and Vitalik had worked on. So we actually read that before even that we had some of our own ideas. Polygon heavily kind of relied on validity proofs rather than fraud proofs. That was one of their core thesis, which they believed on. And hence we relied on something like KZG rather than fraud proofs and so on and so forth. And over time, when we had some solution in hand, when we talked to partners about who will be using it, when we talked to optimism, arbitrum, Starkware, Zksync and others, there was one clear direction that we got is that they will not be using a will because they would not be wanting to use a competitive solution, right? Even if it is good or bad, it doesn't matter at that point. So in order to be credibly neutral, it had to be that we had to come out and create an independent entity so that we had no relation with Polygon, so that we can serve all of the scaling solutions who are working to scale Ethereum and beyond.
06:56:46.914 - 06:57:35.510, Speaker A: Right. So as to kind of make sure that we actually get used not only within polygon, but much beyond. So, yeah, that's roughly the backstory. I'll come back to your point. Whether we scaling Ethereum actually think, you know, that could be an interesting question for the panel, but maybe let's start from the basics so that we are all at the same page. We've just learned in the previous talk from Nick White from celestia that apparently there is something, there's a website called dimension that lists 11,000 raw apps. It's a testnet, but still there are thousands of these constructions.
06:57:35.510 - 06:58:39.020, Speaker A: But I've also read on their website and I quote, data availability networks play similar role to databases in the web applications, providing data as needed. Unlike traditional databases, DA networks are decentralized and hold data for shorter periods. What have I just read? Literally, what did we do? Why we managed to confuse literally everyone. That's why we've been proposing to rename data availability to data publication. Right? I mean, should we do that? Should we all do that, or are we too far down the line? Well, maybe we can all shake hands on it in this panel, and then after the panel, we can go to all documents and rename from data publication. It needs to be a critical mass, I think. Yeah, I'm all for it.
06:58:39.020 - 06:59:22.390, Speaker A: I think it doesn't completely solve the problem. I think the problem is also in the word data itself, even, because I've been invited to give a talk on Friday at this, what is it? Open data event. And they're basically, we want to look at all the different aspects of data in web3. And to me, I was like, it doesn't feel right, because to me, this data that we're talking about here plays such a fundamentally different role to all the other types that I don't like even just, oh, this is like one aspect of data. No, it's not. It's a very fundamentally different aspect. It's this, we have this data that's actually very little data, but that we want everyone to be able to see.
06:59:22.390 - 07:00:45.170, Speaker A: That's the most important aspect that we have. Everyone has a chance for some time to see this data, and that's it. What about you guys? Are we changing it? Should we stop talking about data availability, or should we decide whether you pronounce data or data? Because I'm not a native speaker, so I'm always confused. Yeah. So just to reemphasize the point, availability, storage, custody, they're all different forms of data, but you cannot really compare them because they serve very different purposes. And so data availability is about the permissionless access for other users to see state changes, get, for example, the diffs or the batch data of the roll up to stay in sync with the latest state and to be able to challenge the operator in some form or another. And this is just so different from custody, which guarantees that the data is out there, but it's not available necessarily to the users that challenge, or compared to storage, which is more of a user application for long term purposes, but doesn't serve the challenge period that secures withdrawals, that hold assets of roll ups.
07:00:45.170 - 07:02:38.818, Speaker A: I guess avail is stuck with the name, right? No, I would like to kind of take that vein and kind of say that this is clearly showing that it is such a nuanced concept, as they put it, that it is unfair for us to kind of expect that a user end user would actually read this and understand and try to think of the security implications and so on and so forth. So I'm not saying that the user needs to be completely oblivious to the fact, but the point is that we have to do a lot in terms of tooling and education and creating the right infrastructure so that blockchain applications can scale without them having to understand the difference between custody, availability and such. Of course. Are we doing enough? At least from my perspective, no. But at the same time, the idea is to kind of make sure that if a roll up developer or a roll up infra provider like dimension can write something like that, although we work with them, and out of the 11,000 apps, we also have quite a few of them on avail. But at the same time that if they are unable to represent the facts in a concise manner without all the nuances to their end user, then we have to actually make things much more lucid in order for us to expect that they will be able to talk about security, about whether it's an l two or it's some construction, what is a blockchain, what is a roll up, those kind of things. Are you going to change your name? Yeah, I mean, if that solves a lot of confusion, like for example, I think I'm representing Anurag here, so I won't want to change my name to Anurag to cause more confusion.
07:02:38.818 - 07:04:07.730, Speaker A: Public let's go back to scaling. I think this is actually very interesting and potentially nuanced. So assuming that you guys all understand the difference between data publishing and long term storage, and we'll come back to storage, because I think this is like under talked topic in data availability community, they just brushed the problem away. I don't think they should brush the problem, but let's park it for a moment. Now, I feel like there's a fundamental difference between dunk sharding approach and celestial and the veil. And I would put both solutions in the same camp in a way, in that they're outside of Ethereum in a way. However, this fundamental difference is not widely understood in a sense that almost everyone that I talked to can't really say, well, what is the fundamental difference, price aside and these types of things and potential scalability, what duncred really, in your opinion, differentiates having what I would call enshrined DA versus DA that's essentially implemented outside of Ethereum.
07:04:07.730 - 07:05:11.640, Speaker A: I mean, it depends on what you want. But if what you want is to securely use assets on Ethereum, then enshrined DA has one property that nobody else can provide, which is that you can build roll ups on it that have essentially very close to the same guarantees as the base layer itself in terms of security, censorship, resistance, availability, everything. And you can not get this if you have to bridge data availability, which doesn't mean you should not use it. It depends on your application, right? It depends what you need. Nobody is saying you should never use side chains, but you should probably think about why you're using side chains and is it the right thing that you want to do. So that's from the Ethereum perspective. If what you care about Ethereum scaling, then roll ups built on Ethereum with Ethereum data availability are just in this respect superior to everything else.
07:05:11.640 - 07:06:02.498, Speaker A: If you're talking about building general roll ups, native solutions, then you can just as well build them on any of the other ones. Like for example, if we're talking about sovereign rollups, then of course you can launch them on Celestia or avail. And then your bridge to Ethereum is fundamentally like from a fundamental perspective, it is a side chain bridge. What is still interesting about it is that it potentially allows you to build these solutions with much lower resources than building your own new l one. And I feel like this is actually a name that we're still kind of missing this modularity aspect that doesn't, that has the modularity aspect of a roll up, but doesn't have the security aspect of a roll up. I think it would be interesting to figure out how we want to name these things, which are also very interesting objects on their own. But yeah, that's my opinion on it.
07:06:02.498 - 07:07:30.180, Speaker A: Proto, I'm assuming that you're kind of like splitting somewhat your time into work on product sharding and work in optimism for eight four four. What's your take on this? Well, so after my work at the Ethereum foundation, I've started working more on the layer two side of things. So I've started building roll ups, started building the op stack. Now the op stack has this requirement, this strong requirement of having a DA that's coupled to the settlement layer. The fault proof of the optimistic roll up depends on the data being coupled to the sediment. Now if you decouple this, then you basically have to remodel the roll up like a sovereign roll up where you do not have deposits or withdrawals to a base layer, but rather the roll up is the product itself. And so with sovereign rollups, what Celestia is building towards is to enable rollups to subjectively verify the safety of the other rollups, but to not enshrine this ethm base layer like Ethereum has.
07:07:30.180 - 07:08:33.086, Speaker A: And so the op stack can support an external DA, perhaps, but it would require this bridge that we have been talking about that bridges the data fed ability external to Ethereum. To Ethereum, I think this was previously called the quantum gravity bridge for Celestia, but now it's called blobstream, I think. And so this couples this external data availability to Ethereum. But then as a roll up, if you're coupled to Ethereum, then you also have to assume the assumptions, the security assumptions that come with that bridge. And so that might affect your security model, is that the characterization that you would agree with the side chain bridge. What is your perception on this bridging problem? And I want to dig a little bit deeper into that because I think it's very important and rarely mentioned, right. That we do need a bridge, a bridge that in Celestia's case was used to be called quantum gravity.
07:08:33.086 - 07:09:17.734, Speaker A: Now it's called blobstream. I will ask avail as well, what they think about that. But how do we solve, or is there any way to solve the bridging problem? Yeah, I think you're talking about two types of bridging, right? Roll up bridging and the data bridging, right? I mean, for the data bridging, yeah. Blobstream, for people that don't know, is a data bridge between soleshit and Ethereum. And as Dankrad said, off chain data to a specific chain will never be as secure as on chain data. But we tried our best to make it as secure as possible without it being on chain. And they gave a talk about earlier about this, about the different trade offs there.
07:09:17.734 - 07:10:04.020, Speaker A: But the general idea is that it's basically a data availability committee with crypto economic assumptions such that because celestia is an independent l one with its own token, if the validators relay data that's not available, they can get slashed on the celestia base layer. Now, this fundamentally is impossible with l two or restaking type dacs like eigen layer, because you cannot prove data unavailability on chain. So you can't slash data unavailability on ethereum. You can only slash data unavailability on your own independent chain. So you can't do that. You can't do it using like a restaking based DAC or l two. But in terms of your other question, about.
07:10:04.020 - 07:11:10.310, Speaker A: I mean, that's the spicier question. Is a roll up defined by a bridge, or is it independent chain? And there was a lot of debates about this on Twitter, but my argument or my thought process is, well, it all boils down to upgradability, right? No one wants to build a roll up that can never be upgraded. But then the question is, how can you make a roll up upgradable without breaking the security assumption of a roll up? That users do not have to trust the operator. And so the current thinking around this is that you have some multisig or some Dow that can propose upgrades to a roll up bridge with some delay, let's say like a 30 day delay. And if the upgrader, if the Dow misbehaves and tries to upgrade the bridge to a malicious contract, then users can exit the chain. And so that's the current thinking. You still get the roll up guarantees with a synchrony assumption.
07:11:10.310 - 07:12:17.510, Speaker A: So then the thinking goes, well, if that's the accepted approach, then in theory, you could argue that all roll ups are sovereign roll ups. And that's what Kelvin from optimism has argued, which is that imagine a roll up has multiple bridges. The roll up is not defined by the bridge because the roll up community could choose to hard fork it in such a way that the bridge operator does not agree with it. But if the users who deposited money through that bridge disagree with the way that it's been upgraded, they can exit the bridge and get their funds back. And this is especially useful or the case if most of the funds in that roll up are natively minted on that roll up and not coming through external bridge prabhao yeah, I think they covered all of the points. I don't have much to add. But there is also like one thing is the canonical bridge, and the other thing is the user verified transactions.
07:12:17.510 - 07:14:01.202, Speaker A: So one of the things which people conflate is that how does today a user verify that whether Ethereum roll up has settled on Ethereum or not? More often than not, they just query some centralized provider to know a smart contract state. And if they say that it's verified, then they agree, which means that they not only rely on Ethereum for checking the settlement correctness, but also for data availability and such. Right? So they are relying on the Ethereum crypto economic incentives. One of the things that we are trying to do is that we are trying to say that even if you are running a roll up, and for example, let's say you are running on Starkware, if you post your data why I'm taking Starkware because they post the settlements like maybe six to 8 hours or very infrequently, but within that time all the transactions which were done within the roll up can be verified by the user themselves on their wallets by just verifying the ZK proof along with the ordering assertion that comes from the DA layer which is available in this case. Right? So although the attestation and the bridging and all of this will happen in due course, in time, but that will only be reliant on the bridging aspect. But everything other than that, all the user transactions within the l two and beyond, because people are building l three s and such, all of them can be verified within the user wallets and that's how efficient the ZK roll up constructions can be. Of course with optimistic designs there can be security trade offs that people have to make.
07:14:01.202 - 07:14:55.460, Speaker A: You cannot just say that I will have the DA off chain and be as secure because the security is an overloaded term, right? There is a safety concern and then there is liveness concerns. And we of course take safety concerns highly than liveness. So of course there are many, many trade offs. But at every point in the trade off there will be a cost proposition, there will be a value addition that will come and people have to like people. When I say I hope not the users, but all of these application developers have to choose where exactly in that security paradigm they want to be. Exactly. And what does that offer to them? Right? So I think what you're getting at here is that as a user in verifying transactions, what you're really talking about is the light client that needs to verify the data is available, the execution is correct.
07:14:55.460 - 07:16:16.110, Speaker A: Now for the execution there do exit different constructions off chain, but really only once you have the ordering in place, then you can guarantee that there's only a single form of execution that you have to prove. And that's what really is the definition of a roll up. Strict ordering based on the DA that everybody can reproduce, and proof of execution. Speaking of security, each of the three solutions employ what's called data availability sampling, right? This technique that allows you to check that data is actually available or has been published without needing to download the whole data. But I have a feeling when maybe I'm wrong that the technique is the same, but the actual usage is very different in the sense that Celestia talks a lot about light nodes, right? And users, the end users. And we've just seen in Nick White's presentation, nice pictures from different world locations of users running a light node, not a light client, light node on their phones, right. But at the same time, there's this bridging problem.
07:16:16.110 - 07:17:43.410, Speaker A: So how does that really help? Roll ups on Ethereum, on the other hand, with dunk sharding, we use data availability sampling, but that's actually run by the validators, right? So that we can achieve bigger throughput in the full version of dunk sharding. So is it really the same or is a little bit different? Does it really serve a different purpose? What's going on here make us a little bit more knowledgeable about that? I think that's actually mainly a difference in terminology here. I think what Celestia call light mean, correct me if I'm wrong, I'm assuming that light nodes will eventually verify the full celestia consensus as well. And in that case, I would say it's just what we call actually full nodes in Ethereum, and what Celestia currently calls full nodes, we would call super nodes or super full nodes in some way. More terminology, people. To answer your first question, having data availability, something like clients, is still useful for lester, is still useful for roll ups that sell to Ethereum because it improves the crypto economic assumption of that bridge. Because if the validators relay data over that bridge that's unavailable, then now the light nodes can also participate in slashing the validators.
07:17:43.410 - 07:18:49.086, Speaker A: So it makes it that the crypto economic assumption is more likely to be guaranteed if all the participants on a selection network can do the slashing, and not just the full nodes, but in terms of the terminology of light nodes. So the reason why we call it light node and not light client is because light nodes are kind of more than clients, because they're also contributing to the security of the network by downloading samples and redistributing samples. So that's why we say it's a node. But yes, it is the case that celestial light nodes do not currently verify the state of celestia. They only verify the data availability of celestial. But this is kind of like a similar question to how Ethereum data availability, something like finance, will work, or how will they will verify the state of Ethereum? And I mean, there's several ways you could do that that could also be applicable to Celestia. Either you just run it as a full node where the so called light node, which is actually a full node, just download, verifies all the state.
07:18:49.086 - 07:19:27.814, Speaker A: Now that is something we plan to support in Celestia. But one of the key differences is that because Celestia's state is much more minimal. There's no on chain smart contracting environment, or there's not like a lot of state that's in theory cheaper. That would be cheaper to do. But of course the ideal solution would be to have fraud proofs or ZK proofs on the state so that clients don't even have to verify in the first place. My understanding is that's also Ethereum's goal for data availability, something like clients, which is to have a ZK EVM so that the like clients can just verify ZK proofs of the EVM. So are we converging? Not mean.
07:19:27.814 - 07:20:15.110, Speaker A: You know, we all share the same goal and the end game is the same for all of mean. In that respect. I think it's the same with the type of nodes. I think what the fundamental difference between Celestia and Ethereum is the integrated settlement layer in Ethereum. That currently at least is not a goal of celestia, but from the fundamental data availability perspective, they share the same end goals. Is there anything you want to add here or data availability sampling parties more or less the same? No, I just want to say that we kept the nomenclature the same just to confuse them, like less. We still have light clients who participate in the DHT.
07:20:15.110 - 07:21:24.800, Speaker A: They propagate all the samples on the DHT. That is their main way of getting all the data available, proofs and so on and so forth. We also have full nodes, but they are just very typical full nodes who just not only download the entire data, but also help light clients to kind of populate the DHT in the beginning. But also on top of that, what we are trying to do is that we are trying to make sure that our peer to peer is not only being used for data availability sampling, because a peer to peer network is a tough build, but it is underutilized if it is only for data resampling. Because what about proofs? Like all these sovereign roll up constructions that we are talking about, maybe the peer to peer can be overloaded to kind of use proof propagation. I mean, there are many other use cases that are emerging in which the peer to peer plays a more robust role in terms of how it secures the network. I mean, again, securing is an overloaded term just to make sure that we are on the same page.
07:21:24.800 - 07:23:18.690, Speaker A: Nothing like too much to add there. But does that, I mean, these are nuances which just for our reference, it's also bad to overload terms, but at the same time just creating new terminologies probably confuses a little bit. I've got one more quote before I give floor to the audience. I've just read an announcement from Nier that with Eigen layer, which is a project that is supposedly aligned with Ethereum, they have just announced what they call a fast finality layer that promises to process Ethereum transactions in three to 4 seconds compared to hours or even days on Ethereum. What's going on here? How da impacts finality? Again, any hot take on this, not to confuse Eigen layer and eigenda, we should maybe also discuss eigenda with the differences of sampling. But talking about the pre confirmations from Eigen layer here is that there exists this layer, one and a half kind of product where ahead of the block time or the confirmation ability of confirming large blobs of data for data availability. There is this opportunity for low latency applications for roll ups to confirm their data as a promise with low latency so that other roll ups or other users can incorporate the changes faster and then over seconds or minutes, the data can still go on data fed ability layer for any proofing later on.
07:23:18.690 - 07:24:25.734, Speaker A: Don't I get one slot finality on Celestia? Well, Celestia uses tendermint, so it's got like one block finality, which is 12 seconds. Yeah, if I may add a bit. So basically, I think the restaking service, they give a very good primitive to kind of create committees where economic incentives are embedded, right? And what that allows is that you can create this kind of services where you can give fast finality in the sense that if the finality reverts, then you can maybe slash someone out of that committee and so on. So it's just a committee construction, right? So committees can be used for various services. And this is another service as far as I know. I might be wrong here, but that's what this particular announcement might be about. And what that essentially means is typically you can do similar stuff without embedding it inside the DA layer.
07:24:25.734 - 07:25:19.014, Speaker A: For example, if you want fast confirmations today and you use something like optimism, arbitrum and all of like, you do not wait for da finality, ethereum, block time, none of that, right? You go and submit a transaction, you get a soft confirmation from the explorer, from the RPC. You see the explorer, you see that your transaction is included in a block you don't care about much, right? But if things go wrong, things eventually go wrong, right? There are protocols which safeguard you in terms of how economic slashing works in, let's say, an arbitrum or an optimism, or how these kind of rewards can actually happen in terms of state. So I think all of these are constructions which are coming up. So that's really good. And at the same time, that's what I was trying to say, is that the user will not be knowing all of these nuances. Probably they should or shouldn't. That's another debate.
07:25:19.014 - 07:26:38.754, Speaker A: Would you agree, Dankrad, that eigenda c three maligned or it's more nuanced discussion needed? So I generally don't use that word much. I think what they provide is a great way for projects to get a data availability layer with more scale. Right now, if they are planning to use Ethereum da later, I'm happy about that. I don't think I will go beyond that. Not that it matters, but I think Celestia is more Ethereum aligned than Egme layer because it's important to realize that there's a fundamental difference between data availability committees and data availability sampling based or base layer based data availability layers. They're fundamentally two different categories of things. And as I said, celestia was originally, originally, I was trying really hard how to figure out how can we make on chain data availability proofs on Ethereum so that you can construct something like celestial as Ethereum l two, but the values of Ethereum are more aligned with the celestial than I would say anything else.
07:26:38.754 - 07:27:56.700, Speaker A: Because with things like dank sharding, with full dank sharding and roadmap, the whole point is that end users can verify the chain with data availability sampling. And that's why Celestia has prioritized data availability sampling. Furthermore, and that's why the blobstream bridge is as close as you can get to as secure as an off chain data availability you can get with crypto economic assumptions thanks to the slashing from the data availability sampling. And secondly, it doesn't rely on any liquid staking primitives that might be harmful to the network. Guys, I've got one or two more questions, but maybe there's someone in the audience that would like a panel, something, any brave soul. First one. Oh, we need a micro hi, that's a question for you, Bartek.
07:27:56.700 - 07:28:36.708, Speaker A: Are we going to get a dab to clear up all the confusion and help the teams guess? You know, we have no choice, but what can we do? What do you guys. Here we go again. Another beat website with all g straight offs. I feel like it's already a major role that l two beat does. I don't feel like it's a separate website in a way. I mean, I've tweeted today that I think that we should be more nuanced with how we talk about Da. It's not just on chain versus off chain, it's not roll ups versus validiums.
07:28:36.708 - 07:30:06.644, Speaker A: I mean, we're kind of used to that because that's how we named things, what, two, three years ago? I mean this space is changing all the time, right? Hence we need new names with a better understanding and new constructions that are being built now. I see DA as a spectrum, and I'm much more interested in making sure that users fully understand the trust assumptions and the security that comes with it. Because for me it's not just data publishing, it's also the problem of long term data storage for data availability committees, for example. Right? Like if I have a small committee, we understand it's not super secure, but will they give me data two years after? Right? And by the way, I would love you guys especially avail and Celestia to tell me, well, Ethereum ecosystem is huge, right? We can really hope that there will be someone archiving the data, at least one. I mean there will be a lot of ether scan type of projects because it's just such a big community. How do you want to build such a community for celestial data, for avail data? How do you incentivize such community? I mean, we also need long term data. Mean, one thing that we're looking at, or one thing that a Celestia node developer is looking at, is that this idea of a.
07:30:06.644 - 07:31:37.108, Speaker A: So in Celestia we have this idea of namespaces, so each roll up can have its own namespace and the data is in namespaces. And we have this idea of, we want to do namespace pinning very similar to ipfs pinning. So like on ipfs, if you have like an IPFS URI, you can pin that to your node so that you as your node you can say, I'm going to store that data, I'm going to distribute it to anyone that asks. And we want to have a similar concept in threshold node where you can take your roll up namespace and say, I'm going to pin that data and anyone that can ask me for it over the peer to peer network, I'm going to provide it. And so that way roll ups can be responsible for their own data storage without having to necessarily rely on centralized solutions or centralized RPC endpoints. They can just get the data over the peer to peer network, over anyone that agrees to store it. Yeah, I think it's quite similar to kind of what we are trying to also do is that, as I was telling this point earlier as well, it's about tooling, right? It's not that protocol is ensuring that data, it's about the Ethereum, the tooling, they are so mature that now you can rely on them to keep this data, and they are incentivized enough to keep this data, right? So the ecosystem has to be matured, the tooling has to be mature, it has to be easy, there has to be incentives to people to keep the data, and hopefully we can do that.
07:31:37.108 - 07:32:18.752, Speaker A: Of course, right now we are working with partners to keep this data into ipfs and other long term storage solutions. And that will happen, those kind of projects will happen, but all of this will dried up. If there is no incentive, if there is no community, if there is nothing, there is something vested to kind of keep this data available, right? So I think that's the kind of end goal that we want to say. Like any technical solution that we discuss right now, although we are implementing them, but the technical solution is as good as getting used. So you have to build that ecosystem and community around. There was one more question from the audience. No more.
07:32:18.752 - 07:33:23.766, Speaker A: Yes. So I would say that the data availability industrial solutions are pretty young at this point. And in this regard, I wanted to ask a question. So if we imagine this trend of 11,000 roll ups and many new applications as well, how do you guys imagine the future of data availability, let's say, in five years? The way I see it is like kind of similar to web two, in a sense, that there's going to be a few DA layers, there's going to be more settlement layers, and there's going to be loads, tens of thousands, maybe even millions of roll up or application specific roll ups. That's the kind of general way I see it. The reason why I think there'll be fewer data layers is because data layers have some strong network effects. For a start, if you want to have trust penalized bridging between two roll ups, you need to make sure you use the same data layer.
07:33:23.766 - 07:34:27.692, Speaker A: And that's why, for example, for Ethereum roll ups, like on chain, data is the most secure. And also data availability layers have network effects from number of light nodes. The more light nodes you have, the stronger the data availability sampling guarantees and data availability guarantees that you can get. Yeah, I think one of the points that Mustafa covered, I would kind of echo that, is that data availability is kind of different than, let's say, execution scaling in that sense. So you can actually scale the amount of data that remains available as per the demand, which is the number of light clans that is running in the network and so on and so forth. But just to kind of give some perspective, is that even if arbitrum, optimism, Zksync, Polygon, Zkavm and Starkware put together, put all the data into any one of our DA solution today, they wouldn't even fill it up by some major percent. Right? So it's not even half full or something.
07:34:27.692 - 07:35:35.328, Speaker A: So we not only have to see the number of kind of these roll apps, but the amount of utility that they generate, not only data, because that's easy to generate. Like I can create a roll up today which generates tons of TBS of data, right? That is not the issue. The issue is that whether there is sufficient utility that these roll ups generate and how many of them are generating enough data to kind of fill up the solutions that we are providing. So not only infra, but also applications, right? Well, I would caution there where we are already very close to the Ethereum data throughput, where base, for example, every minute they utilize the maximum transaction size, they create a transaction of 128 kb every minute on Ethereum. Now multiply this times ten for all the other roll ups that we'll see. And this is only growing. That is a lot of data that Ethereum needs to confirm, and that currently has to compete awkwardly with EVM exclusion resources.
07:35:35.328 - 07:36:19.456, Speaker A: Well, it's completely different type of resource that's required here to make data available through Ethereum. And so I'm very glad that we're specializing these resources to optimize. And I would consider this a kind of skating, even though we're not skating to the full version of dunk sharding just yet. We're kind of assuming dankred, that dank sharding will be out there, right in this. I definitely hope that in five years it is live. Is there anything next on the menu? Well, I mean, the good thing about data availability, as it was mentioned, is that it's scalable. So it's not like we have a certain design sort of spec in mind.
07:36:19.456 - 07:37:49.802, Speaker A: But after that, there's no fundamental limit to how much it can scale further. The main limit is that what you want is actually that all your nodes together over sample your data by a certain factor, you can debate what it should be, ten or 100 or something. So ultimately, if Ethereum is used by a billion people, and let's say a million of them run nodes, then we can actually see a very large scale of data on Ethereum. My personal goal is that it should be enough for roll ups to serve human initiated transaction at world scale. I think that will be possible in that time frame and that doesn't mean there will not be any other, there will be much more data hungry transactions. But for this basic financial stuff, I think Ethereum should be able to serve everyone who's got the mic. You? I see a question from the audience or here I want to hear more spicy questions.
07:37:49.802 - 07:38:52.874, Speaker A: We haven't talked about the availability versus custody like with eigenda or pruning data. Spicy one. Do you store your personal data like photos and videos in a DEA? And if not, what should happen for you to store to trust the DEA? So what would need to happen to trust the DEA? Trust it enough to store your personal photos and videos as a primary personal archive. You are like confusing data storage and data availability again. So data availability is really about a temporary resource where we want it to be. The very high guarantees to be accessible by any possible user to get access to the data just so that they can produce a proof or counter the sequencer in some way. Whereas photos and videos are really just about storage.
07:38:52.874 - 07:39:48.590, Speaker A: Nfds are also not about data availability. It's much more in the data storage domain. Yeah, things like filecoin is more in that kind of area. Guys, I have a very quick yes and no question about terminology. Can validium that used to put data outside of Ethereum after dank sharding suddenly can become a roll up by posting data of the new blocks on chain? Essentially, right? Does it make it a roll up? Yes, the roll up property. We don't care about the past. Of course we want that.
07:39:48.590 - 07:40:13.190, Speaker A: If it starts and the past state is not available, well then it's nothing. But assuming it's a validium, the state was available. It has been published like all users have seen. Oh yeah, it's really there. And then from that point it starts posting the state diffs on Ethereum. I would say from that point on it is actually a roll up. So in other words, you're not concerned about the potential data storage problems.
07:40:13.190 - 07:40:43.118, Speaker A: If I wanted to recreate the full state, and from the validium part, I just don't have a full history. Right. I cannot go. But I think this is actually a fundamental property of blockchains. Even a chain like Ethereum today does not actually guarantee you in any way that the history is available. People believe that because it is just a default that full nodes have at the moment, it is not enforced by anything at all. If someone right now created a new version of the node software that just didn't store the old blocks anymore.
07:40:43.118 - 07:41:19.580, Speaker A: You would have the same property, or like, for example, didn't store all the account storage nodes that are never accessed. We also wouldn't notice you could still produce blocks with that. So I think it's a bit of a confusion to think that we even have this property now. I'm not aware of any chain, any users where previous blocks are lost. The only chain I'm aware of is ripple, where apparently in ripple, the first 10,000 blocks are missing. But that wasn't because of a data storage problem. That was because all the nodes were corrupted, had a bug where we were corrupted all at the same time.
07:41:19.580 - 07:41:51.300, Speaker A: Okay, do we have time for one more question from the audience? If it's a quick one. If there's one. Okay, the last one. And we're going to wrap up for the person who was asking about posting videos on the blockchain. One of the first few blobs on celestial is like someone uploaded an mp4 of the SpongeBob, like an episode of SpongeBob. So that you can get glad it was a spongeBob. Yeah.
07:41:51.300 - 07:42:52.074, Speaker A: If you go on modular clouds, there's a link somewhere where you can actually play it in the browser as. Hi. Hi, it's Janice from protocol labs. I would like to ask the data availability problem, what challenges does it bring to the network layer to basically data transfers? I mean, you talked about storage. How about the transferring of data in the other protocols at the peer to peer layer? I take it more as a question about data will be sampling. That has fundamentally new challenges for the networking layer in that all sort of, let's say the traditional blockchain design is just based on gossip, on just telling everyone everything. Like basically in bitcoin and ethereum, you just need as a node, you receive blocks and you just tell all your neighbors about the new block, and that's how everything works.
07:42:52.074 - 07:43:38.446, Speaker A: And it's a very simple design that's very robust. And for Das, we need this new thing where you have to be able to ask particular nodes for certain samples. And that is just a very fundamental change. And getting it to the same robustness as this very simple property is like a major challenge. Yeah, I'd like to separate the network layer problem into the dissemination of data from the builder to those nodes that serve the samples and then the users that query the nodes for the samples. And so this complexity really depends on the size of the validator set or the set of nodes that serves the samples. If you fan out very wide.
07:43:38.446 - 07:44:43.662, Speaker A: And if you divide the data between lots of nodes, then you increase the network traffic and the burden on the builder to distribute this data. But once you do have distributed the data, then you get the benefit of this horizontal scaling where all the nodes contribute to the security by serving samples to these end users. Yeah, that's definitely one of the biggest open problems for data availability sampling. Another open problem right now is how can you do anonymous data availability sampling? Because we need anonymous data availability sampling to maximally to get the most secure possible data availability sampling because right now is the attack. If the samples are not anonymous, then the block producer might only release the first few samples and trick a few light clients, but not release the rest. So we need a way for light nodes to submit their sample requests anonymously in such a way that different sample requests cannot be linked to a specific light client. And that would solve the attack.
07:44:43.662 - 07:45:07.430, Speaker A: So there's some research around this on the celestial forum as well about using mixnets and Tor with delays to figure out to see if that's possible. Thank you very much. Let's give a big round of applause for the panel. Thanks guys. Good job guys. Give it up for the panelists. Stay put.
07:45:07.430 - 07:45:25.926, Speaker A: We're going to keep talking data availability. We have Mohammed from scroll coming up. Come on up, Mohammed. Thanks guys for coming up. Give him a shout again and welcome. Mohammed. Check.
07:45:25.926 - 07:45:41.650, Speaker A: Can you use this? You should have your. Yeah, there's that. Let me get you a clicker. Thank you. You go budy. Good luck. All right everyone, Mohammed here from.
07:45:41.650 - 07:46:38.480, Speaker A: Hello everyone, thanks for coming. So today I'm going to talk about data availability, continue this discussion. So the title is foundation to frontiers and I'm going to first set the stage revisiting the problem of data availability mostly from perspective of Ethereum and roll ups who want to build on top of Ethereum. And after setting the stage, we will get to what are the state of the art in data sampling on Ethereum? What are the ways scroll as a roll up and other roll ups can make their data availability cost even lower. Okay. Data availability is a big piece of puzzle in Ethereum scaling. So if you don't know by now, Ethereum is a decentralized network.
07:46:38.480 - 07:47:11.464, Speaker A: It's secure. But the problem is it's not very scalable. You can't do more than twelve transactions per second on Ethereum, and that's clearly not enough. So one natural idea is why we can't make blocks bigger. And this has been a debate that's been going on for a very long time. So say we make blocks bigger, or in Ethereum terms, we make gas prices go lower. What happens is we are in risk of building another solana.
07:47:11.464 - 07:47:47.908, Speaker A: It becomes centralized. And if we don't want to get centralized, meaning that we want to have many nodes, we risk to have very high propagation delay, which makes the consensus distable. Okay, let's make it a little bit more interesting. Let's say we have big blocks and light client or watchtowers. So this is an idea that has been explored since five years ago or so. Say we have an army of light clients and we have a big block. And any of those light clients is responsible for checking some part of the block.
07:47:47.908 - 07:48:35.560, Speaker A: Like the red one reads the red part, the blue one reads the blue part, something like that. You notice that the whole block has to be available because if some part of the block is unavailable, then, for instance, the red one cannot read the red part. And okay, who's going to check that part? So data availability was already a problem even back then with these simple ideas. And you can think of this simple idea as the early version of optimistic rollups that we have today. Right now, let's get more concrete. So the progression of ideas for scaling Ethereum has been like this. First, people thought about larger blocks or reducing the gas price, and indeed EIP 2028 did this.
07:48:35.560 - 07:49:16.356, Speaker A: What happened was they reduced gas price for empty bytes, and it was okay because they didn't really push it too hard. Like if you make the gas prices too low, your risk blocks be too big and the propagation delay goes up. And this destabilizes the consensus algorithm. In Ethereum, there were other proposals like EIP 4488 and EIP four. Four, four. These two IP were a couple. Like one of them was trying to reduce the gas price for call data, but puts a cap on call data so that the blocks don't get too big.
07:49:16.356 - 07:50:09.670, Speaker A: And the other one was an idea to prune the call data from historical blocks so that nodes don't have to store too much call data from old blocks. But it was never implemented because the core developer team decided that it's too complex and carries too much risk. After that, people start discussing state charting. Maybe not after that. It's a bit complicated. People were talking about the state charting, and state charting is basically creating like, instead of one blockchain, we have a number of blockchains, basically a number of mini ethereums, and they do talk and synchronize from time to time, but the set of validators are somehow separate. So we put 30 people on one of those shards, 30 people on another shard, something like that.
07:50:09.670 - 07:51:02.420, Speaker A: You can think about this and already intuitively run into problems like how these different shards sync, how we move funds around. What about security? Is it going to reduce security? There were many ideas, but at the end the final proposals were too complex and never implemented. Finally, roll ups or earlier versions of roll up like plasma showed up. And the idea was we move the execution off chain and we keep everything else, meaning data availability, consensus and settlement on l one. And this is an idea that has proven to be working. And after some time, Ethereum adopted this roll up centric approach, meaning that roll ups are now a first class citizen of Ethereum. And Ethereum is going to make it easier and easier for roll ups to operate on Ethereum.
07:51:02.420 - 07:51:43.300, Speaker A: And that's how Ethereum users will enjoy scaling. So a roll up has two parts. One is validating the computation that's being done on the roll up. You can think of it validating the estate transitions of the roll up. And the other part is validating that the data of the roll up is available. It basically means that people should be able to reconstruct the full estate of the roll up chain, and it's necessary for reasons that we will get into in a bit. After roll ups were proven to be successful, we ran into a new problem, the data availability.
07:51:43.300 - 07:52:35.270, Speaker A: Using Ethereum as a data availability is not so easy. It's very expensive, and you can't really scale Ethereum to thousands and thousands of transactions with the current levels of data availability that Ethereum offers. So there is this new idea, data sharding. This is very similar to state sharding, meaning that we have smaller groups of people in charge of making sure some part of the data is available, something like that conceptually. And the idea is we want to improve Ethereum usability as a data availability layer. EIP 4844 is the first step to artist vision, and dunk sharding is the next step that's still in the making. The spec for dunk sharding is not fully specified yet, and it's an active area of research.
07:52:35.270 - 07:53:36.232, Speaker A: So as Dunkrad once tweeted, data availability is a kind of misleading name. What we really mean by data availability is data publishing, or that some data is available if it was openly published, and anybody who really wants to fetch it can, that data is available, and a data availability layer is an infrastructure that provides that service for us. A tricky thing about data availability is that unlike validity of computation, we cannot prove data availability, or we cannot prove data unavailability. The reason is this fisherman Dilemma. I took this picture from a 2017 article from Vitalik. So it's basically like this. If someone says I didn't see the data that this other person was supposed to publish, this other person could quickly publish the data.
07:53:36.232 - 07:54:16.400, Speaker A: And from perspective of someone who was not watching the chain, there is no way telling which one is lying. So for that reason, it's a non attributable fault. You can't tell which is which. So the only way forward is data availability test, which we are going to get into in a bit. Basically you can think of EIP four eight four implementing a naive test and dunk sharding implementing an efficient test. That's how I think about it. Okay, before proceeding, roll ups send their data on l one and that's how they take care of data availability.
07:54:16.400 - 07:55:07.910, Speaker A: But why l one blocks are available? The reason is Ethereum consensus ensures data availability of Ethereum blocks, especially new blocks. Meaning that when I'm a proposer, when I propose a block, I need attestation from attestators, from other validators for my block to be finally finalized. And if I withhold my block after a quick deadline like four 8 seconds, attestators won't attest to it anymore. So it won't be finalized. So if something is finalized, it means that it was available. So if you put something on Ethereum blocks, it's guaranteed to be available if it's finalized and part of the canonical chain. Right? As people were mentioning right before me, it doesn't apply to historical data.
07:55:07.910 - 07:55:45.360, Speaker A: Obviously, you can get rid of historical data after it was included in the chain and all. So you cannot really trust Ethereum to store something for you forever. Okay, let's take a look at this data availability problem from perspective of a roll up. Roll ups have to do this one thing. This is very important to roll ups, and it's to make sure the execution is done correctly. People call it integrity of execution. Optimistic roll ups rely on fraud proofs to ensure this.
07:55:45.360 - 07:56:31.308, Speaker A: It means the roll up operator is updating the estate and some people are watching, and if they see a state was updated incorrectly and some transaction was executed incorrectly, they can send a fraud proof showing that this was incorrect. And finally the operator will be a slash, and so on. The important thing here is data availability. Here is crucial to ensure the execution is done correctly. Right? Like if the state of the roll up is not retrievable or reconstructible, you cannot even make fraud proofs. So they really depend on data availability for succinct or ZK roll ups, the situation is a bit different. Like proofs on their own show that the execution was done correctly.
07:56:31.308 - 07:57:33.460, Speaker A: We don't really need data availability for that, but we need data availability for other reasons. A valid state is not available state. Maybe some roll up is operating correctly in the sense that it's executing transactions correctly, but you cannot see the state, so you cannot make claims about the state. For instance, say I'm a user of scroll and I want to withdraw my funds from a scroll, but scroll sequencer is censoring me because they don't like me. If I want to have a mechanism or as a protocol designer to implement a mechanism that allows this user to withdraw their fund, even without collaboration of the sequencer, the user has to be able to make some claims about the state of the roll up. Like make claims like I own this much money in this account and I want to take it out, but this is only possible if the full estate of the roll up is available to them. So censorship, resistance or permissionless withdrawal.
07:57:33.460 - 07:58:03.016, Speaker A: These things are dependent on full retrievability of estate for the users. But that's not the only reason we need data availability. We need data availability for a whole lot of things, like reconstructing a state of roll up. Sure. Forced inclusion, like maybe I want to make a transaction on roll up. The same scenario and the sequencer is ignoring me. There are ways to force sequencer to include your transaction.
07:58:03.016 - 07:58:49.070, Speaker A: If you can show that your transaction was there, it was just not included. We need data availability for that. Other things like running auctions, like if you're running an auction, decentralized auction, and you want to make sure no bidder was censored, the only way for bidders to make sure they are not censored is to put their bids on some data availability solution. And if they were censored, make some claims that show okay. Indeed, the data was available and the action are just ignored and the list goes on. And we are finding more and more applications for this data availability. It's one of the most fundamental properties that blockchain offers and many decentralized applications and protocols rely on this.
07:58:49.070 - 07:59:40.568, Speaker A: Okay, let's take a look at the roll up process. In a ZK roll up, this chart is actually from Scroll's roll up process, but it's kind of similar for every ZK roll up. Every ZK roll up has a commit phase that commits to the data that's going to be executed. It's basically sending a new batch of transactions that are going to be executed in the roll up chain. We call it commit, and after some time the roll up executes those transactions and somebody submits a proof that attests to correct execution of those transactions. And it's finalized in a scroll terminology. Okay, commit is where we take care of data availability, where we send the transaction data unchained.
07:59:40.568 - 08:00:41.772, Speaker A: And so anyone who's interested can look at those transactions, re execute them, and get the fullest data roll up. Right? And these are details I'm going to skip. The most important thing here is that we take care of commit by submitting transactions as call data right now. And it makes up almost 80% of our cost of operating the roll up. Clearly it's like the majority of the cost and we want to reduce that, but we don't want to reduce it with compromising security. Like one thing is we can get rid of data availability altogether, become a validium, right? But that means we are making very strong security assumptions about the roll up operator. Or even if we use data availability solution like Celestia avail or any of those other chains, we're adding a security assumption on top of Ethereum being secure.
08:00:41.772 - 08:01:23.310, Speaker A: We require like Celestia to be secure too, for our system to completely work and hold all its properties, which is not desirable. Okay. EIP 4844 is one step toward improving usability as a data availability solution. What it does, it introduces a new transaction type called blob carrying transaction. A blob carrying transaction can carries a number of blobs, and a blob is just a blob of data, like some raw data. You can think of it that way. The good thing about these blobs is that they are unlike call data.
08:01:23.310 - 08:02:05.848, Speaker A: They are not executable, they are not accessible from the smart contracts. Also, they are just temporarily stored and they get pruned after like three weeks or so. And also you have to pay for blobs in a different gas, like we have regular gas nowadays. And after EIP 44 inclusion, we have data gas, which is priced differently and it has its own market. The benefit is it isolates blob users from fluctuation of execution gas market. It has a lot of good things. And the way I think about it is that it doesn't have any unnecessary feature.
08:02:05.848 - 08:02:54.268, Speaker A: So it's cheaper, right? It's designed with data availability in mind. But why blobs will be available? Like why I make a blob carrying transaction and put some data in a blob and send it to chain and it gets included. Why should I think that the data in the blob is available. The reason is the EIP introduces a change to the fork choice rule to the consensus algorithm. A block is valid only if all the block is valid, only if all the blobs included in the block are valid. How this validity is checked. Consensus clients test availability of blobs and as long as the blobs are available, blobs are available too.
08:02:54.268 - 08:03:23.572, Speaker A: Now let's get into this test. What does test mean? The naive test is download all the blobs. That's what EIP 4844 does. And that's why EIP 4844 is not so efficient, because everybody has to download everything. And if you think about it, when you increase the number of blobs, at some point small node operators in the home have to download terabytes of data. That doesn't make sense. The other way to do it is to make an efficient test.
08:03:23.572 - 08:03:59.712, Speaker A: That's what dunksharding is trying to do. We just sample something from those blobs. Each consensus client samples something. And if those samples are available, then good, they deem the block, I mean the blob available and the block henceforth valid. But it's just not sampling blobs. It requires a lot of technology to make sure that those samples are consistent. The network layer is working properly, and Ethereum is a running chain with billions of assets on it.
08:03:59.712 - 08:04:44.764, Speaker A: We cannot just play around with it. We have to be very confident before deploying anything on it. So it just takes time and it requires a lot of technology. EIP four eight four provides some of those tools that are going to be eventually used for full dunk sharding. Okay, this is a very important slide. EIP 4844 gives us some capacity for data availability. The capacity, I mean, I'm not going into the parameters, is basically 380 block, which you can translate to 100 or 200 transaction per second across all roll ups.
08:04:44.764 - 08:05:28.232, Speaker A: And this is assuming that there is no compression for the transactions. And the transaction I'm talking about is just ERC 20 transfer or something like that. So it's not that many transactions. We are going to need more than that. What's going on? The thing is, EIP 4844 is not the final answer. And we have to do a few things to make situation little bit better. One of those things that we are looking into and other roll ups also are looking into is transaction data compression.
08:05:28.232 - 08:06:29.260, Speaker A: So we have to put this transaction data on chain or on some data availability solution. But we can compress that data before putting it on chain, right? So I talk about three classes of compression. The first one is specialized compression tricks. So there are certain fields in a transaction that we can just drop, because they're part of validating the validity of the transaction, they're part of making sure that a real user signed the transaction, the signature is valid, and this transaction should be executed before that transaction, and so on. You can remove this information from the transaction like nons and signature, and replace it with the signer's address. And if you think about it conceptually, you can still execute everything, so you can recover the state. The only thing that has to be done is that somebody has to verify that there was a valid signature for this transaction that can be done inside circuit.
08:06:29.260 - 08:07:10.620, Speaker A: So effectively we make these fields witness instead of public input. The other idea are caching some of these fields. Basically an account address is too many bytes, but that's too big of a domain. We don't have that many users. For ethereum, maybe four bytes is just enough, because we don't have more than 4 billion users at the time. And I think about it as some sort of caching. Again, this caching is like writing into a merkel tree that has to be done in circuit to make sure it's done properly.
08:07:10.620 - 08:07:57.710, Speaker A: Another class of solution is applying generic compression algorithms. This is what most popular optimistic roll ups do right now, like optimism and arbitram, they apply generic compression algorithms, like compression algorithms that you can compress any file with and yield good compression gain. The only problem is we have to implement either the compression or decompression of the algorithm of our choice in circuit. That's a tricky thing to do. The other idea is posting the state diff. Estate. Diff refers to the effect of a transaction on a state.
08:07:57.710 - 08:08:57.490, Speaker A: If I'm swapping two tokens on uniswap v three, maybe my transaction is routed through a lot of things, but at the end of the day it just changes a few numbers in the pool and my account, like the amount of that token in my account, amount of that token in the pool, maybe the rest of the data is unnecessary to reconstruct the state. Right? So this idea is used by Zk sync. It works very well for certain type of transactions, like swap transactions or transfer of ERC 20 tokens. It's very good for a specific type of transaction. The same goes for specialized compression tricks, because you can look inside the transaction and optimize for that. The generic compression works for everyone and anything. Maybe you have a contract that touches a lot of estate, like for that one estate, if is not good.
08:08:57.490 - 08:09:49.410, Speaker A: The more important thing I want to emphasize is that these two tricks, specialized compression tricks and state diff, they make separation of sequencer and prover more complicated. The reason is think about removing signatures from transactions. When we remove signature from transaction as sequencer and post it on l one, somebody still has to generate a snark proof that there was a transaction with a valid signature that corresponds to this transaction with removed signature. Right? And it was a valid signature, and so on. That witness data is not readily available to the prover. The sequencer and prover, if they are separate entities, they have to talk. And it's not trivial to design a system to make sure that happens.
08:09:49.410 - 08:10:44.796, Speaker A: But for the generic compression algorithm, the witness data remains available. So this is a good thing about this one. So we have to do in circuit data compression, if we use any of these approaches or a combination of these approaches, we have to implement the decompression in circuit. And it's not an easy thing to do. Thanks to lookup tables, it could be done in principle, but we are still actively looking into this to see how effectively we can do this. Okay, conclusion so we have come a long way as a community. We figured out very good ways to scale execution, basically pushing execution off chain to the roll ups.
08:10:44.796 - 08:11:28.290, Speaker A: We've identified the key problems, meaning data availability, and we have some solutions. But EIP 4844, which is going to arrive soon, probably early next year, is not the final solution. The fees probably go down two to five x, but it can be still quite high. The number of transactions per second, as I mentioned, will not exceed 100 200 across all roll ups. So the community has to keep working to our tank sharding. And the thing is, even the thing I say about reduced fees is a speculation. Nobody knows, because the fee market is a new fee market, and you have to wait and see how that pans out.
08:11:28.290 - 08:12:14.608, Speaker A: In the meantime, roll ups cannot wait. Maybe they want to serve their users. They have to resort to two things better, decompression algorithms. Compression and decompression algorithms. Our benchmark says you can get three x to five x smaller in the size of call data you submit or the data you submit to your DA of choice. Another approach is using alternative data availability solutions with DAS data availability sampling already implemented, like Celestia. This is something that can be done, but as I mentioned, introduce new security assumptions to your system.
08:12:14.608 - 08:12:37.140, Speaker A: So it's a trade off. Thank you so much for listening. Thanks, Mohammed. Awesome presentation. I think we are probably at time. Yeah. So no questions.
08:12:37.140 - 08:12:57.576, Speaker A: That's all right. Thank you. Yeah, give it up for Mohammed again from scroll. Thanks for coming up. Up next we have Orkun from Chainway. I'm pretty excited about this talk. Sorry, he's talking about building the first DK roll up on bitcoin.
08:12:57.576 - 08:13:20.588, Speaker A: So that's pretty great. There's a remote. Thank you. Take it away my friend. Thank you. Hi everyone. Thank you for being here today at the last talk of today and like previous from this talk we saw lots of industry leaders in ethereum ecosystem and dA ecosystem talking about DA solutions and how they can benefit from Ethereum and other DA solutions.
08:13:20.588 - 08:13:50.668, Speaker A: And actually Mustafa and we talk a little bit about bitcoin and its properties, but I want to dig deeper in that topic and I want to explain our experience and our product. Bitcoin's first DK roll up. Let's start with a short intro about me. My name is Orkun. I am co founder and CEO of Chainway and I'm also an engineer. And together with my three friends Murates and Ekram we last year co founded Chainway as a venture builder. And since last year we built two different products.
08:13:50.668 - 08:14:31.988, Speaker A: One is proof of innocence. Maybe you know this from Vitalik's private schools paper. We actually did the initial implementation on tornado cash done for Tornado Nova and Railgun. And after that we built ordinal safe, the first bitcoin NFT wallet which I will explain in my presentation. And now we are fully dedicated to building bitcoin's first ZK roll up and we are actually based in Istanbul so we are kind of hosting here and a little bit information about bitcoin auto landscape today. But I can say they are mostly failed. Why? I am saying that because the constructions are like federations, merge mind chains, independent proof of stake and proof of work chains and drive chains.
08:14:31.988 - 08:15:02.496, Speaker A: So you may wonder about drive chains and what are they and federations. So basically I can go one by one. Federation is just basically a permissionless chain. However their block producers are permissions and they eventually decide on what is better or what is not for the chain. So it's a centralized chains basically. And merge mining is also a similar concept that inherits some hash power from bitcoin based layer. But they don't have any bitcoin related security or any main chain security and independent proof of stake.
08:15:02.496 - 08:16:03.992, Speaker A: Maybe we can think this about Polygon to Ethereum with a less security because they are just independent chains with a bridge between bitcoin and Altu. However these breeze are generally multi six and are centralized also and drive chains are new proposal that is not yet accepted and they are basically trusting miners to validate the other chain and then accept or decline the peg request coming from the main chain. So slightly better constructions. We have lightning, and I'm sure you know about lightning, it basically solves the problem of bitcoin to accelerate the payments and make them cheap and instant. And however it has some complexities like routing and channel opening overhead and the implementation complexities really creating a liveness issue on lightning network. And the other thing is client side validated protocols. These are the mainly topic of today's talk and however they mostly failed before this day because of the limited data.
08:16:03.992 - 08:16:48.230, Speaker A: And you see the op return opcode here and you may wonder what is that? Op return is basically an bitcoin script code that allows you to embed eightabytes of data into the chain, but makes the UTX so unspendable and provable. So eightabytes are really limited data in terms of data size compared to the other solutions. And unfortunately the client site validated protocols before this date was not like client Friday. And that makes the verification even harder. So a little bit historian meta protocols of bitcoin. So maybe you know some of this and Mastercoin, later renamed as Omni, it was the first meta protocol on bitcoin. And you see the paper here, it is really bold and says the second bitcoin white paper.
08:16:48.230 - 08:17:26.364, Speaker A: Basically it allows you to embed your transaction data to op return and then track the transactions off chain through some meta protocol called Mastercoin. At the same year we see colored coins which are just using the same logic to create off chain coins on bitcoin network. And later this year we saw counterparty taproot assets and RGB like protocols. And this is the story. So Vitalix was involved with all these efforts like master coin and colored coins. Back to date. He proposed ultimate scripting paper for Mastercoin.
08:17:26.364 - 08:18:09.792, Speaker A: And because the mastercoin rejected his proposal, they said we want to focus on simple things, not programmable layers on bitcoin. And after that we know that Vitalik met with Gevenwood and they started working on EVM and Ethereum blockchain. Actually this proposal was the initial building block for EVM and Ethereum blockchain. So in the meta protocols, I counted the last of meta protocol. But what limited their adoption and their improvement in the meta protocols, because of the op returns limitation, you can only store eight bytes of data. And this creates a new need of off chain data solutions. Like most centralized solutions, I can say.
08:18:09.792 - 08:18:43.390, Speaker A: And because of the initial efforts are really early. There were no chain compression on these efforts. Like you need to send the full history of the coin while sending a coin, which is not ideal for anyone else in the world. And you need to scan every single bitcoin block and validate every single transaction to validate a history of coin. And this makes it not like crying frankly I can say. And the rest is Vitalik also talk a little bit about this Utxo model. It's really hard to add programmability and the early efforts were already using Utxo model.
08:18:43.390 - 08:19:33.140, Speaker A: So we solved the problems and let's think about a solution for this problem. What happens if we had easy verifiability and compression to a meta protocol? Then we get a sovereign ZK roll up actually. So sovereign roll ups are the latest version of meta protocols and meta protocols who are the mother of the sovereign roll ups actually. So sovereign ZK roll up is a metaprotocol that compresses the chain's history and rolls everything together to create a single saxon proof for all the chain's history and validity. So what sovereign ZK rollups allows us to do is basically chain compression. You don't need to send the full history while sending a coin because every state transition is recursively proven inside the proof already. And this enables us to have light clients.
08:19:33.140 - 08:20:22.024, Speaker A: And this light clients means that you can easily verify the full chain even on your mobile phone. You don't need to scan every single block, you don't need to download every single block, you can just run a light client and you can verify the full chain and programmability. Of course, if you are using an account based architecture with any existing vm, just like EVM, you get programmability without introducing any other overhead to the users or developers. Okay, these are the good things, but what happened to the data limitations? Let's go one step beyond and talk a bit more on bitcoin. And what changed on bitcoin since ethereum's launch? Actually there were two upgrades today that helps us to build this DK roll up. One is segwit and the other is steproot segwit. Basically coming from segregated witness.
08:20:22.024 - 08:21:12.216, Speaker A: What that means is it segregated the transaction witness from actual transaction body. And this made actually a single bitcoin block can carry one megabytes of data, but because of the segregated witness, you can put four megabytes of data to a single block. However, just 1 transaction is actually included in the block and the rest three megabytes is carried with the block so also this makes the input witness discounted for forex in segment transactions. And after that, in two years ago. Actually today is the second anniversary of tap root. And tap root upgrade was enabled two years ago today and it enabled efficient span feds for a single Utxo. So you can create a merkel tree of scripts and you can put the roots on chain, and when you span it, you need to span it.
08:21:12.216 - 08:22:00.836, Speaker A: You can just prove the validity of the Merkel pet, then you are spanning the coin. This enables us to have efficient multiple span pads. However, there is one trick that taproot also enables. It removed the script size limit for a transaction and made it equal to a block size limit. Now this means that we can have a four megabytes of scripts in a single Utxo. Besides that, there are schnorr signatures activated on the taproot upgrade, which is not so relevant with our discussion, but important for bitcoin's future, because it enables several other tricks, like discrete log contracts. So early this year, thanks to these upgrades, we see ordinals protocol ordinance protocol is put forward by Casey Rodharmorv, a former bitcoin core developer.
08:22:00.836 - 08:22:31.520, Speaker A: And he found a way to input some data to blockchain and attach the data to individual satoshis. Satoshis are basically the smallest unit of bitcoins. Like waste to Ethereum, Satoshis is what waste to Ethereum, to bitcoin. And you can put four megabytes of data to a single satoshi and then keep track of this data off chain. But the data is actually published on chain. This made nfds real on bitcoin actually. And this was what I was talking about in ordinal safe.
08:22:31.520 - 08:23:13.890, Speaker A: We built the first wallet for ordinal sand, thanks to its technology. It allows us to have four megabytes of data in a single transaction. And it is more way bigger than op return opcode can provide us. And I'm sure you know, bitcoin community and some people didn't like this development and this advancement because they say, stop putting graffiti on my money, because you actually put a graffiti on their money. And however, this is how Taproot and Segwit enabled lots of new features on bitcoin. We have to accept and embrace it, because it allows us to create new innovations on bitcoin. And this is why we are here today.
08:23:13.890 - 08:23:56.732, Speaker A: So let's sum up everything we know. We have a block space, and we can use this bitcoin block space to secure the alto, actually. So the idea is use bitcoin as a laser ledger. So verify on the client side. And if we can make the block space verifiable via zero knowledge proofs then we can get a trustless and instant lite client which is not available before this date. So how we are doing it? Maybe you know sovereign SDK an SDK by sovereign Labs. It's a brilliant SDK to launch modular blockchain sovereign rollups on fast YA layers like avail and Celestia and even on Ethereum and Solana.
08:23:56.732 - 08:24:41.272, Speaker A: However, for our case, because of bitcoin, block times are probabilistic and block times are not sharp like avail and Celestia, we have to optimize it for our use case. So we forked the sovereign SDK and built our own modules on top of the SDK and now we get this architecture. Actually. So briefly, the circuit recursively verifies the inclusion and southness of previous proofs and forced transactions from the previous bitcoin block. And this mechanism ensures that our prover or sequencer doesn't skip any state transition or forced transaction mechanism. So why this is important you may ask? We can just create a batch, roll up proof and put it into onchain. Then anyone can verify this proof.
08:24:41.272 - 08:25:24.468, Speaker A: Okay, that's nice, but let's go back one step beyond and talk about Ethereum roll ups and how they are working today. I'm talking about smart contract roll ups. In that case, smart contract roll ups actually smart contracts on the l one act as a light client for these roll ups. So basically when a roll up patch is created, they sent proof to the Ethereum blockchain. An Ethereum smart contract verifies the proof and checks that the initial state route is matching with the smart contract state. So if it gets matched, then you can add the proof to the latest chain and you can get a new latest state route on the l one smart contract. This mechanism ensures that you are going one by one without skipping any state transition.
08:25:24.468 - 08:26:17.672, Speaker A: But in sovereign Rolloff's case that's not possible because you don't have any lite client running on bitcoin network or bitcoin script. You have to validate and verify the chain off chain in a client site. In order to make this lite client friendly, you need to use recursion and ensure that every step is recursively proven so that you haven't skipped any state transition or forced transaction. And this brings us an equivalent security of a smart contract roll up and we are bringing it to the sovereign rollups. By using this mechanism and doing that we are creating a new type of Zkevm. I know that you know lots about zkevms and everyone building its own ZKE EVM, and in our case the effort is mainly led by sovereign labs. Also, we contributed to it and we implemented rust EVM inside the risk zero ZkvM.
08:26:17.672 - 08:26:54.244, Speaker A: And this results in a start based EVM that is more performant than the circuit based EVM today. Running in the production and proof generation is based on execution cycles on risk zero ZkvM, which makes it highly parallel with a constant cost. And we are truly benefiting from this feature. Why we choose to build a ZkevM on bitcoin because we want every single Ethereum developer to deploy their Ethereum applications on bitcoin without any hassle. So let's sum everything up. We have everything on bitcoin. We have a ZkevM proof, we have a block space proof.
08:26:54.244 - 08:27:24.844, Speaker A: Let's merge them together and get a roll up proof. And the proof outputs a state difference from the previous proof in the proverbs. So the output should be correct or proof generation will fail and the proof is later inscribed into bitcoin. So if you verify the latest proof, you verified all the roll up states since Genesis. And this allows us to support light nodes in a very easy manner. So we have three properties inherited from bitcoin. One is Reorg resistance.
08:27:24.844 - 08:28:16.752, Speaker A: If you want to reorg the roll up, you need to reorg the bitcoin, which is not possible after six blocks exactly, but in one block or two block it is also not possible without any economic power you have. And on the other side, liveness. We are inscribing these proofs to bitcoin and proofs output state difference from the previous proof. This means that if you scan every single bitcoin block, extract our roll up proof from it and apply this output to your genesis state, then you will get the full roll up state without looking to any off chain da solution. So every da step is on bitcoin. And this brings us to bitcoin's equivalent, liveness. And on the other side, of course, censorship resistance is important for us, and we know that bitcoin is the most censorship resistant layer today, and we need to inherit that feature also.
08:28:16.752 - 08:29:18.380, Speaker A: So in RZK circuit we are forcing sequencer to provide the inclusion and soundness proof for all the force transactions. If sequencer skips to include any force transaction, then proof generation will fail and the sequencer bond gets flashed. This is about liveness, but I want to talk a little bit about zerosync you can see in the middle column. Zerosync is a project that compresses all the bitcoin state in a single stark proof because bitcoin supports Utxo model and also Utxo model. Actually we can create a stark proof of the current state of bitcoin and it can be verified on any device. Then later you can download the full Utxo state and continue operating on bitcoin network as an MVP. Right now they developed a header chain proof which also syncs with the bitcoin blockchain in litecoins just by downloading single star proof verifying it and you will get the latest bitcoin header.
08:29:18.380 - 08:30:03.192, Speaker A: Okay, having said that, let's talk about our roll up node types. If you are running a plain roll up light node then you can get the roll up state route trustlessly just by requesting some latest bitcoin block from some provider. And you can also verify it from your regular bitcoin lite client. Then you can extract the proof, verify it and the proof output state route so that you can verify any answer coming from your RPC provider. And if you are running a light node with a zero sync then you get this route instantly. You just download the single proof of bitcoin header chain, you verify it, you get the latest block, then you extract the proof and verify it also. So in your light client you can get the roll up state route instantly and trustlessly.
08:30:03.192 - 08:31:22.676, Speaker A: And if you are running a full node then you can scan every single bitcoin block and you can go through one by one and you can get roll up state route and of course full roll up state. And this brings us an equivalent liveness of bitcoin network. So what is our progress in here? So we have finished our MVP sequencer and full node running on bitcoin. So right now we have a private bitcoin testnet running in our headquarters and we can make transactions using sequencer and full nodes acclo to the latest state of the roll up and starting from this month actually we are integrating our risk zero approver to make this process as similar as the main net version. And hopefully in the quarter one we are targeting to be on bitcoin testnet and on quarter three we are targeting to be on main net. And for the future work we need to minimize the trust in soft confirmations because bitcoin block times are ten minutes we need to have a soft confirmations and our initial plan is to create a sequencer network on bitcoin that can publish soft blocks there. Then later some prover can collect these blocks and prove the blocks validity and then inscribe the proof into bitcoin so that we will be minimizing trust in soft confirmations and decentralizing the sequencer over that way.
08:31:22.676 - 08:31:59.964, Speaker A: So we know that bitcoin DA is limited. It's actually four megabytes per ten minutes. So if you are using state tips like us then it is fine. However, we need to still provide the history and soft confirmations and we need to plan this, and we are planning it through the decentralized sequencer network over the bitcoin for future explorations. This is a paper by Robin Linus from zero sync team. Again, the BitVm BitVM actually allows you to compute anything on bitcoin. So he says bitcoin is not limited.
08:31:59.964 - 08:33:08.740, Speaker A: With bitcoin scripts you can create a program, you can compile the program into circuits, then you can embed your circuits into teproot trees. And if some off chain dispute happens, then verifier can challenge the prover to enable some teproot span pads. And this allows you to compute anything optimistically on bitcoin. So what if we can embed a ZKP verifier in BitVM so we get an optimistic ZKP verifier running on BitVM and in the end this results in a trust minimized BTC breach between our roll up and bitcoin mainnet. And I personally see that this is a pet to op starcore op ZKP. The opcode I mentioned is not existing today, but I believe over the time we see that people are implementing ZKP verifier on bitcoin mainnet through BitVM and it creates a lot of overhead of off chain data transfer and bandwidth and storage. And you can also verify the ZKP by just a single opcode and this enables lots of roll ups to be run on bitcoin.
08:33:08.740 - 08:33:50.150, Speaker A: Hopefully in the future we will see that this kind of opcode will be enabled on the bitcoin main net. So as a result we are creating a new meta on bitcoin and we are envisioning bitcoin as a base layer that is able to verify any off chain computation and we are creating an EVM layer for it. And if you are an Ethereum developer or EVM developer, and you want to deploy your applications directly on bitcoin, please come build with us and we would love to help you any way we can. And that's all about my presentation. Thank you for listening, man. If you have any question, I can answer your questions. It.
08:33:50.150 - 08:34:18.980, Speaker A: Yeah, we have some time here, I'll grab that. And if there's anyone in the crowd that has a question, feel free to raise your hands. Here's one. Oh, you got one. Sweet. Thanks. I think it's not working.
08:34:18.980 - 08:34:45.830, Speaker A: You can take this one. Here. Trade. Hello. I got a question about chain reorganization. You said that it's rare event, but in reality, I'm running cold wallet. And it's kind of every time I see one, two block reorganization and then running again.
08:34:45.830 - 08:35:08.776, Speaker A: How we can be shielded about this? What do you mean by being sure about this? From time to time, there is reorganization. One block. Drop it just often. Sometimes on bitcoin, it's often blocked. A lot of often blocks. Every day. One, two blocks.
08:35:08.776 - 08:35:37.312, Speaker A: I can see. Okay, you say bitcoin can go offline or can go reorgan blocks. The chain is running like this. And sometimes the block is incorrect, and then we go another round. Okay, so, in the roll up sense, you don't need to have the finality on the base layer. It doesn't have to be instant, because the roll ups also settles on their own network. Like its sovereign roll up settles on their own network.
08:35:37.312 - 08:36:09.344, Speaker A: So, in our case, if the bitcoin goes under some reorg or some attack, then the roll up itself is also reorganized that way. So we are strictly tied to the bitcoin. And this applies for every single l two. On Ethereum, for example, Ethereum doesn't have a single slot finality. And if some fork happens on Ethereum, then all the roll ups are also forked. However, for our case, as we are sovereign ZK roll up in the regular reorgs, on bitcoin, we can follow the reorg. And if there is a majority shifted from one fork to another fork, we need to go to that fork and prove everything again.
08:36:09.344 - 08:36:40.568, Speaker A: And inscribe them again to the bitcoin network and full nodes, and rearrange the fork again for debt proofs. However, as a sovereign ZK roll up, we can choose to create our own fork on the layer two. So, for example, after some point, we can say, okay, we are introducing a new kind of vm together with EVM. So we need to hard fork the chain. We can hard fork our roll up without touching to the bitcoin. And the natural reorgs are really natural. And roll up logic itself follows the correct reorg all the time.
08:36:40.568 - 08:37:04.290, Speaker A: But you can also create your own fork on the l two. But if some hard forks also happen on the l one, you can choose or not choose to follow the correct one. Okay, thank you. Good. Anyone else? Hey, my guy's excited. Here you go, buddy. Hi.
08:37:04.290 - 08:37:47.520, Speaker A: If this takes off seriously, because I was very impressed by the sentence that every Ethereum developer would be able to basically deploy on bitcoin if this takes off. How do you envision bitcoin changing in general, if this happens or not? Like introducing an opcode, are you talking about. No, I mean, if people are going to use the system you're building in a massive way, so that now bitcoin is only about money. Basically it's money and now it's ordinals. So nfts. And it's already changing a bit. The landscape, like the marxis are becoming less prominent and I think it's a good thing.
08:37:47.520 - 08:38:27.036, Speaker A: But did you think the consequences of this? It's just a question. I don't have any idea. We really believe that bitcoin is the most decentralized and secure base layer to build some applications on it. And today it is just people think about bitcoin as a store of value and just a simple payment protocol. I agree that Satoshi designed the protocol in this way to have a simple payment in a decentralized and trustless manner, and that works perfectly. However, we have a secure block space on bitcoin that is going empty today. And we need to create new applications to build upon this empty block space.
08:38:27.036 - 08:39:07.144, Speaker A: So we have, like I said, four megabytes of block space. And today blocks are, I don't know, one or two megabytes generally, and we have two or three megabytes to put data on it. And we know that halvenink is coming and bitcoin needs more fee to incentivize miners to operate and people to create nodes and continue to chain. And roll ups are the only solution for that. If you are going to some off chain solution and we can talk about every single bitcoin outweigh. In fact, Ethereum is almost same with all the bitcoin outs, because it is off chain to bitcoin itself. So you are not paying any fees while using the off chain protocols like bitcoin, altos or ethereum or whatever.
08:39:07.144 - 08:39:48.550, Speaker A: However, with roll ups, you are actually paying fees to bitcoin. And this helps bitcoin in the long term. We generally think about this way. We are scaling bitcoin in a sense of programmability and speed, and bitcoin is scaling us through its security. And I hate this word, but we are aligned with bitcoin in terms of fees, because we are paying lots of fees for bitcoin to store the DA to publish the data correctly. And this, I think, matches the incentives with bitcoin miners and users, with roll up users. So in short, yeah, we believe that roll ups didn't get any attention so far in bitcoin community, but in the future, this will change.
08:39:48.550 - 08:40:08.428, Speaker A: Good. Yeah. Anyone else? I think we have time for one more. No? Anyone? Okay, you got one more. Let's keep it going. I'm sure you know Andreas Antonopoulos. Yeah.
08:40:08.428 - 08:40:48.888, Speaker A: He disappeared completely from public stage. And the reason is because he's being violently attacked by bitcoin Maxis, basically, just because he started talking about Ethereum. He wrote a book on ethereum, et cetera. So there is this group of people, the bitcoin Maxis, which can get really religious and violent, and you're doing something which is potentially very annoying to them. You've seen how they reacted to the ordinals. How do you see these playing out? I mean, this is an extra layer of, what can you say? Non pure bitcoin application, whatever that means. Right.
08:40:48.888 - 08:41:27.936, Speaker A: How do you see this going in the future? Yeah. Maxis so far took a lot of place on bitcoin and they prevented lots of innovation happening on bitcoin. And eventually, I think the orna's movement and the culture coming from it proves that this is not sustainable. The maxi culture, like custodians and cults and et cetera. So you are just sticking with the simplest thing that you can do with bitcoin and hoping to bitcoin to scale to billions of users, and everyone should use bitcoin. Everyone should pay with bitcoin, but it is not possible in today's form. You need to innovate on bitcoin.
08:41:27.936 - 08:41:56.960, Speaker A: And these innovations so far happened outside of the bitcoin. And this is not helped bitcoin at all. We see Ethereum and how Vitalik and his friends built the ethereum so far, and how value is created. But in our case, in bitcoin, we prevented lots of development happening. And this is not helping bitcoin, like I said, and I believe it is hard to change, but it will change, like I said. Thank you. Cool, thanks.
08:41:56.960 - 08:42:16.950, Speaker A: Yeah. Thank you. Yeah. And just to piggyback off that, once DFI bitcoin starts lending, there's so much more liquidity there. It's just a matter of time. Obviously, there's anger and resentment along the way there, but in order to believe you will fix it. Yeah.
08:42:16.950 - 08:42:33.500, Speaker A: Okay, guys, give it up for Chainway. Thanks. So much. You did a great job, man. We're done. Day one is done. Drink your waters.
08:42:33.500 - 08:42:52.350, Speaker A: Make sure you guys stay hydrated. I'll be here tomorrow bright and early. You're going to be listening to this voice for an entire extra day. Come back tomorrow, the jokes will keep coming.
