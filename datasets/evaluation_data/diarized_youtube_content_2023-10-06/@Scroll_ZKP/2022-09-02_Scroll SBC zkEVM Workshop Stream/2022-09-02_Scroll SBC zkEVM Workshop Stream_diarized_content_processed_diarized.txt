00:10:01.910 - 00:10:32.780, Speaker A: Okay, there we go. Oh, yeah. A quick note, we're being live streamed, so that's why we have this. So, yes, welcome, everybody. Again, we'll start from the top. Scroll is a Zke EVM project where we're a L2 solution for scaling Ethereum, and we're bico level equivalent, which is really exciting. And part of our vision and goal is to create an experience that is as close to the EVM as possible.
00:10:32.780 - 00:11:26.314, Speaker A: The scroll team is we're super decentralized where we're spread around the world, and we're getting close to almost 40 people now. And going forward, we're looking for more people to join us, particularly if you're creative in nature and have a track record of innovation and have strong execution skills and have exceptional communication skills. We want to hear from you. Not a big ask. So we have a job board on the website, and we'll talk more about that later. So a little bit of high level introduction on how and why we started. So, before founding scroll, I've been working in the industry for a couple of years, and over the last few years, we've seen an exponential growth in the number of active users as well as the number of creative applications in the space.
00:11:26.314 - 00:12:17.258, Speaker A: So when you look at one simple metric, the number of active monthly users, it was just a few hundred thousand just a few years ago, and now we're looking at 30 million active monthly users for metamask alone. And with that level of adoption, scaling has become a major bottleneck for the industry to grow. And it has given rise this problem for a while. And we think it's really needed to support increasing demand from innovators and free users around the world. And without scaling, application layer innovations will also suffer. I think in the last couple of years, I think we saw a huge kind of burst of innovation with the beginning of the DeFi summer. But I think that level of incremental innovation has really slowed down.
00:12:17.258 - 00:13:11.134, Speaker A: And part of the main reason is because of the scaling problem. So when I met yen Haiten two years ago, I think we saw that this problem existed in the space. But at the same time, there was also a ton of innovation that happened in the academic space that we thought we could leverage to solve this problem. And I thought Yin and Hai Chun were both very value aligned. We all want to build something that could be used by real users, and we want to build something for the long term. So with that clear value in mind, we set about this task of building a team and also building a product that can translate very theoretical research into practical and usable and also user friendly product. And that brings us to the demo that we're about to see today.
00:13:11.134 - 00:14:08.954, Speaker A: So we did a lot of research a little bit about what we care about as a group of people. What is scroll and what does scroll stand for? I think one of the core things about who we are is that we really care about innovation and creativity. And I think these things are not stressed enough in our industry. It's part of the DNA of scroll and we want to foster our space and foster this kind of culture within us so that it could permeate into the wider developer community as it builds around us. So we see a future where Ethereum wallets will be owned by everybody on earth. And in order to make that a reality, I think a ton of creativity and value creation attitude is needed. And we've always been open source.
00:14:08.954 - 00:14:52.478, Speaker A: It's partly about the way we were organized and the way scroll started. And we've always been collaborative in culture. And I think this has fed to a very open and transparent as well as collaborative building process that speaks to our design choices as well. And I think as a result, we've had a lot of positive academic feedback and feedback from open source developers around the space. And I think that's the way we can generate even faster innovation in the future. And the last part is integrity and intellectual honesty. We do want to make sure that our users think of the brand as one that delivers on its promise and is not overclaiming and is very, very accurate about what we plan on achieving and what we've done so far.
00:14:52.478 - 00:16:28.470, Speaker A: So thinking long term, so we're very long term oriented. And so when we see reactionary kind of regulation or things that are happening in industry, we try to think about how we can position ourselves in the wrong long run and not be too reactionary to short term impacts that the whole industry is facing. But as a part of that, we do want to have an open call to form working groups to think through some of the aspects of decentralization and how we can build a wider ethereum ecosystem where there's no single point of failure, and how we can articulate part of this kind of technology will achieve on the social layer to policymakers as well as regulators around the world. What scroll is about, we're about kind of innovative thinking and thinking about human like kind of mapping some of the human interactions on chain and delivering very serious academic research and implementing it. That's the whole kind of scroll design and organization process which requires immense effort and a lot of help from all of our community members and also all of our contributors. So going forward, we're going to kind of move our mindset to a little bit about growing the ecosystem, so about bringing more users to space, bringing more developers into space, and fostering more application layer innovations and enable user adoption. So with that in mind, if you like what we do and the way we think about organizing the company and organizing our ecosystem, please apply through our website scroll foundation.
00:16:28.470 - 00:16:52.530, Speaker A: And in particular, we're looking for people who are very good organizers and strong communicators and who have passion for the technology that we're building. We're looking for PMS in particular. And also there's an array of researching roles and engineering roles all advertised on the website. So with that note, I'm going to hand over to my co founder Hai Chen to talk about the scroll architecture and design choices.
00:17:05.710 - 00:17:31.054, Speaker B: Should I click here? Yes. Hello everyone, I'm Hai Chenshen. So before I go into our talk, I just introduce a little bit on our program schedules. You already have the schedule program on your hand. So Sandy just gave like an introduction to the scroll. Next, I'm just going to talk about our infrastructure designs and a little bit of testnet. And I was just trying to do a live demo.
00:17:31.054 - 00:18:12.740, Speaker B: And after that, Yington from the dcache, they're going to talk about the programming in the hello two and then some latest progress. What happens in the hello two? And then Mason is going to talk about our ZKE EVM architecture and how we build a construct SDKe EVM. And then after the break I'll just then come back and talk about some performance optimization we have been done for the ZKE EVM. Okay, so let me get started. And also we don't just want to take this very formal presentation. So you can feel free to ask me or break me like interrupt me with any questions. I will be very welcome.
00:18:12.740 - 00:19:09.346, Speaker B: Okay, so I'm going to talk about Score's architectures. Yeah, just one sentence to summarize what score is building. Score is building an EVM equivalent of Zkrop solution L2 chain to scale the Ethereum and then to bring a lot of more transactions and lower transaction gas cost for the Ethereum so we can bootstrap next billion users. And then before I dive into some technical details and then our architecture designs, I just want to first talk about our design principle that drive the decision of our design and why we choose this approach we want to build. So there's like four principles and the first and foremost, which is the most important thing is what we think. We believe that ensuring the user security should always comes first and then for school. This means that the L2 transactions should have and share the same security level as the transactions that happen in the Ethereum today.
00:19:09.346 - 00:19:58.254, Speaker B: And then without trusting even the L2 operators. And then during the design, we should never trade off any user security for any efficiency or any other reasons for that. Just need to guarantee the user security. User can always maintain access to their phone on the L2 or layer one. And then second, we think an effective EVM equivalent solution, sorry, an effective ethereum scaling solution should allow users and developers like a smooth path, like a seamless migration path from the existing Dapps and then existing developing tools. And I would believe that maintaining the EVM equivalents is the best way to achieve this goal. And then third, the decentralization is one of the key aspects of the blockchain.
00:19:58.254 - 00:20:58.934, Speaker B: Usually it's like the overlooked or improperly traded for some efficiency reasons, but we think like decentralizing is one of the key property and that we shouldn't maintain even at the L2 context. And also the decentralization can guarantee a protocol to be very resilient to any censorship or coordinate attacks like in the school. We consider the decentralization across many different aspects and different layers, including the sequencers, the provers, and then the community of users and developers. And then fourth, for a user to enjoy a great user experience on the L2. So believe that there's two importances we need to achieve. One is that the transaction fee on the L2 should be way cheaper, like orders magnitude cheaper than the base layers. And then the second is that users should experience some instant pre confirmation on the L2s and a reasonably fast finality on the layer ones.
00:20:58.934 - 00:22:04.822, Speaker B: So this both requires a good efficiency you can achieve in the L2, so that to be able to user to have a great user experience. Now next, let's take a look at see how we use these design principles to guide our decisions and our approaches. So first, ensuring the user security and maintaining the EVM equivalents lead us to a ZKE EVM based Zkrop solution. So first, the Zkrof is guaranteed like a very great security by relying on the mass and the Xeonge protocols on the behind. And then the ZKE EVM which achieves the EVM equivalency is the holy grail inside the ZK drop solutions. And then you can maintain the EVM equivalence, meaning that all of the user experience and the developing tools will be just directly compatible with the L2, when you have the ZKE EVM and then Zkem, because it's a Zkrop solution, so you still have to share the security guarantee from the zkrops. And then second, the decentralization leads us to design a decentralized approval network.
00:22:04.822 - 00:23:30.210, Speaker B: So when we started designing the ZKE EVM solutions, we quickly realized that putting the ethereum virtual machine into the ZK proof will lead to a resulting, a great overhead for generating the proofs. So in order to reduce the time to finality on the L2s, we decided, so we should build a decentralized proven network that are going to help us to generate the leaky proofs. So there are two main benefits for having a decentralized proven network. The community. Because we have this permissionless decentralized, proven network, any community member can come and become run a proven node, and then the community will be incentivized to build a better, substantially better solutions, including both hardware solution or software solution to boost up, to help reduce the proving time so that we can have better solution to generate a ZK proof. And then second, when we designing our proving infrastructure, we make sure it's highly parallelizable, meaning that if you want to scale up, we can very easily scale up by just adding, simply adding more proven nodes into our network, so that I can achieve very great throughput for generating the DK proofs. And then third, so for the efficiency, so to improve the efficiency of the L2s.
00:23:30.210 - 00:24:31.190, Speaker B: So our approach is to leverage all of the innovative research driven solutions across all of the community. So our ZKE EVM designs takes advantage of many recent breakthroughs, including the zero notch proof systems, and then the proof aggregation, and then the very innovative hardware acceleration solutions. And then second, so we also focus on to develop in the open source manner, so that this allow us to collaborate with a lot of open source communities, including the PSE team from the Ethernet foundation and other open source collaborators and contributors. And then we believe that all of the community contributed to this open source approach. So can lead to a best and then most effective and efficient solutions for the ZKevM and the Zkrop. Okay, next, I was going to talk about the scroll architectures. So first I was just going to talk about the basic modules and components that happens inside the scroll architecture.
00:24:31.190 - 00:25:30.634, Speaker B: So first you have like layer one chain, which is the Ethereum, and then the L2 is the scroll chain. So in the Ethereum there are mainly two parts. There's two contracts deployed on the layer one. One is the bridge contract mainly for bridging any assets or relaying any messages between the layer one and L2s. And then actually this bridge contract will be also there's one part of bridge contract deployed on the layer one and one part of will be deployed on the L2. And then the second contract deployed on the layer one is the RoB contract which are going to accept those data availability transactions that are going to include some block data and a transaction data inside the RoB contract and also include a solidity verifier to verify the ZK proof that are going to be sent to the Ethereum for the finalized any block. And on the L2, the main functionality is coming from the scroll node which is responsible for generating L2 blocks, accepting RPC requests and also relaying the messages and also generating the proofs.
00:25:30.634 - 00:26:14.890, Speaker B: That will be all happens in responsibility for the scroll node. So inside the scroll node there are three modules. Mainly the sequencer is basically just processing the L2 transactions and generate L2 blocks, also processing any RPC request. So our sequence implementation takes a fork of the Go Ethereum implementation with some minor tweaks for the L2 purpose. And then because the Go Ethereum is the most popular execution client for Ethereum node. So by directly forking based on the Go Ethereum, so that gives us the best compatibility with all of the existing tools. And then all the RPC interfaces will be the same as when you interact with the Ethereum.
00:26:14.890 - 00:27:09.686, Speaker B: And then second, inside the score node there's a relayer which is going to monitor all of the smart contracts we deployed on layer one. For example, if find any deposit event happens on the bridge contract, it's trying to going to relay this deposited message to the L2s. And also the relayer will be responsible for submitting any row up data to the rob contract. And then third, there's a coordinator here inside the score node. The coordinator is going to watch for any new blocks and getting the execution traits from the goy sim. And then once the coordinator receives a new execution trace for a new block and they're going to dispatch this execution trace to one of the rollers inside the roller network. And then just ask the roller to generate the ZK proofs and then the roller.
00:27:09.686 - 00:27:50.758, Speaker B: Then if it finishes the ZK proof generations, it'll just send it back to the coordinator and the coordinator will relay that to the relayer for the roll up, to roll up the validity proof. And then inside the roller. So can talk about roller as well because roller is many contact with the coordinator. So the roller is basically running a ZKE EVM inside. And then we also expect the roller to be run on a hardware accelerators in order to reduce the time to generate a ZK proof. Because the ZK EVM is quite heavy ZK circuits and very complicated. So it's somehow like if you use the accelerators, you can be reduced the total time to generate a proof.
00:27:50.758 - 00:28:34.108, Speaker B: And then right now we are using a GPU prover to generate a proof. But we expect like the community and new hardware companies can come up with new accelerators, can further reduce this proof generation time. And then all of rollers basically is a prover node. And then we have this decentralized approver network where it is run by different community members in the future. Any questions so far? Okay, so we can also take a deep closer look into what happens inside the roller when you're generating the proof. Because it's also just not very simple just running the ZkevM. So first the roller will receive this execution trace from the coordinator.
00:28:34.108 - 00:29:11.788, Speaker B: And what's inside this execution trace, it includes the following things. First yes, the execution step. When you execute a transaction inside the Ethereum machines, for example, you will execute all of different opcodes like add, multiply, or like push and pop to the stack and then read a write to the memories. And including also like you're getting some, any block information from the block headers. And the second thing you will include the block header and the transaction data going to be the input to the execution trace. That's when you need to get any information from the block header. And also what transaction data, like core data you have.
00:29:11.788 - 00:30:17.060, Speaker B: And then third, we also need to load all of the contract bytecodes to make sure that we are actually executing the bytecodes actually from the deployed contracts, not like from arbitrary opcodes. And then last is there's a Merkle proofs. They're going to prove like help you to prove the Merkle tree transition from the old state route to the new state route. And then after the roller received this execution trace, it will just hand this execution trace to a module called circuit input builder. This circuit input builder is going to transform all of the traits output from the go ECM and then assign into the certain circuits, which would be a 2d giant matrix probably I'll get more idea like after Yington and then Mason talk about more about Halo two. So I'm just going to have a few giant two dimensional matrix. And then you're going to expand all of the executing traits and then also generate some auxiliary data that helps to generate the proof and then fit into all of the witness in those circuits.
00:30:17.060 - 00:31:07.492, Speaker B: And then next, when we talk about Zke EVM, actually it's not a single circuit. It includes a set of circuits that works together to generate the proofs. So because if you have everything inside a single circuit, then it will lead to even greater overhead. So that's why we separate them into different parts of circuits and then connect them through some lookup tables, which I think Mason will talk more about that in his talk. So this gives you some flavor of the circuits inside the ZKE EVM. There's like an EVM circuit, it's the main core circuit that proves the transition of this virtual machine state from the previous step into the next state by applying some of the opcodes. And then this ram circuit was going to constrain any read write operations happens inside the ethereum virtual machine, including the stack push and pop, the memory read and write.
00:31:07.492 - 00:32:04.368, Speaker B: And then the storage modification. And then the storage circuit is actually another auxiliary circuit that's going to prove the state translation using the Merko proofs we talk about in the execution chase, apply those merkle like the proofs for every change, including read and write on top of inside the story circuit that can prove the state root exactly transit from the old one to the new one. And there are also a few other auxiliary circuits I won't just talk about here in this talk. And after you run all of the circuits, so each circuit will actually execute one proof. So there will be like a bunch of the proofs you collect from executing the ZkEVM. But if you're uploading all of the circuits into the layer one and then validate them one by one, then it definitely will be resulting a very large gas cost in the layer one. So in order to reduce the gas cost inside the layer one verification.
00:32:04.368 - 00:32:54.118, Speaker B: So we need to further using an aggregation circuit that aggregates all of the proofs from the ZKE EVM and then into a single aggregation proof. And then here we just call it a block proof, which corresponds to a single block. So the aggregating circuit, what's inside aggregating circuits, it just replicates the verification logic and then just represented inside a ZK circuit so that it can be redo everything like you need to do inside a verifier. And then you can do that inside the circuit. And then you'll generate a new proof that once you verify this block proof, you just basically verify all individual proofs inside the Zkevn circuit. Okay, so this is what happens inside a single roller. But usually it takes longer time to generate a single ZK event proof for than generating a block.
00:32:54.118 - 00:33:42.630, Speaker B: That's why we will have multiple rollers that's going to run in parallel to generating the block proof for different blocks. And then if you want to further reduce the overhead of the verification on the layer one, so you can reuse this aggregating circuit and then to aggregate those block proofs again into a single proof. So you can only verify a single aggregation proof that can verify all of the blocks inside in this aggregating proof. Okay, so previously I just described all of the modules inside our architecture. Next I'm just going to show up how that works in actions when to do inside the Zkrop. So in the Zkrop, like the overview is like you have two chains, one is the same chain, the one is the L2 chain. Here is a scope.
00:33:42.630 - 00:34:35.750, Speaker B: The L2 chains can generate more blocks than the layer one. So like the one layer one blocks maybe within that time period, like 15 seconds or 12 seconds after the merge, and then you can generate multiple L2 blocks. And then what you're going to do is you're first going to aggregate all of the blocks within that time of period and then can just send up all of the data like the block data or transaction data, and then rob to the next blocks inside layer ones. And then after a while, because the proof generation takes a longer time, then you can have an aggregation validity proof that aggregates the previous batch of the blocks. And then you can send that proof into even later blocks inside the siM. And then after that the block basically can be finalized. So with that you can actually have the different status of the L2 blocks.
00:34:35.750 - 00:35:28.586, Speaker B: So first, initially the block, once a block is proposed by a sequence and included in the L2 chains, we call it a pre committed status. So when a block is pre committed, this block is not canonical part of the L2 chains, because nothing has been settled like sent to the layer ones. But you can still do some actions optimistically on top of the pre committed blocks based on L2 and the second. So once the block is generated on the L2, so you can collect all of the data you need and then just send that posted to the rob contracts on the deploy on the Ethereum. And after that transaction is confirmed, then this block can be translated from the pre committed status to the committed status. And with that you have more confidence because the data of your transaction means like it already be written into the layer one. So no one can modify that again.
00:35:28.586 - 00:36:11.846, Speaker B: So it just only seems like it's not sure if I committed the status. You are not sure that your transaction has been executed correctly on the L2. So in order to finalize a block, you need to then use the ZKE EVM to generate a validity proof of all of the L2 blocks and all of transaction like steps happen. And then you send that validity proof to layer one and then be verified by a verified contract. Then that means this block has been finalized and then that block has been finalized as canonical part of the L2. So to see more concrete example in the context of a scroll. So first you have the block generated by the sequencers, and then you will roll up the data from this block and then send up to the rob contract.
00:36:11.846 - 00:36:59.958, Speaker B: And then after this block has been confirmed transaction been confirmed, then the block status should be transited from a pre committed block to a committed block. And then meanwhile you also will extract this execution trace from this block and then send that to the coordinator and then being folded to one of the roller for the proof generation. And then you can do that for every like the new blocks you get. And then there are multiple rollers. It's going to generate the proof for each block. And then after a while the coordinator will collect the block proof from those different rollers together. And then you can further create a special task with all of the proof block proof and then send to another roller for this aggregation proof generation.
00:36:59.958 - 00:37:47.942, Speaker B: And then after that the proof generation, the aggregation aggregate proof generation has been down. You receive this aggregation proof and then you can send that proof into the layer one roll up contracts, and then that verification contract will take all our previous data. Like you roll up to the contract as the public input to this verifier contract so that you can then finalize all of the blocks that have been included inside this aggregation. Okay, so I'll probably quickly go through how the bridge works. I think running a little bit late, but I think how does bridge work is quite standard for the layer one to L2 transactions. So the user just interacts with the bridge contract inside the layer one. Deploying Ethereum.
00:37:47.942 - 00:38:31.560, Speaker B: So you do call like deposit token function inside the bridge contract, and then that bridge contract will just emit one event inside the bridge contract. Say like someone just deposited some tokens. And then the layer one bridge contract will just freeze the token, whatever token user transferred to the layer one contract. And I'll just freeze inside that contract. And then the relayer will just monitor all of the events generated from the bridge contract and wait for a few confirmations on the layer one. And after that it will just then send a transaction to the, send a new transaction in the L2 bridge contract. And that bridge contract will then either mint a new token for the ERC or just transfer some Ethereum ether to the user's L2 address.
00:38:31.560 - 00:39:36.246, Speaker B: And then second, on the other hand, if you want to withdraw any tokens from the L2 to layer one, you interact with the L2 bridge contract call to withdraw the tokens and then the L2 contract bridge contract basically will also emit an event that relayer we're going to monitor but at this time. So relayer does not no longer wait for L2 block confirmation, it will wait for this block that, including this with your transaction to be finalized in the layer one. At that time then it will send out this transaction to the layer one bridge contact, which then finally will transfer that token to the user's layer one address. Okay, so for our pre alpha testnet, so we have like score layer one. So we deploy as a fork of the ECM with the proof of authority based consensus and score L2, which is a zero knowledge rock deployed on top of the score layer one. And right now we are running five provers like a small cluster on the back. Right now, as I talked before, we only have the block proof.
00:39:36.246 - 00:40:32.870, Speaker B: So we just, meaning like we just rock every blocks, generate a proof for every block and then just rob that block proof to the layer one. We're having that down finished aggregating multiple block proof into like a single proof. And then we have a few pre deployed applications on top of the pre alpha testnet. So we have faucet application on top of the score layer one. And then second we have a bridge contract on both layer one and L2 for any relaying for transfer tokens between layer one, L2 and then a swap application that is a fork of the uniswap v two that's going to give users like ability to swap tokens and also provide liquidity on the L2. And then we also have developer, a small like rops explorer that's going to show the block status whether it's being pre committed, committed or finalized on the L2. So you can see all of the block numbers and then corresponding block hash and then whether it's finalized or not.
00:40:32.870 - 00:41:18.620, Speaker B: And they're also going to show this transaction hash that's going to like for example the commit transaction hash will corresponding to the transactions that are going to roll up the data and then finalized transaction is going to correspond to the roll up the validity proof to the data one. Okay, so before I go into the demo, so there's like also you have sign up to become an early test and contributor. So there's like a sign up at school IO so you can still sign up. And then we're going to allow every day allow like a few hundred people to come and test this testnet. And also we are hiring, this is like a page for hiring page. You can find all of the open positions on our website. Okay, so let's do the live demo of that.
00:41:18.620 - 00:42:02.860, Speaker B: Let me see inside we have the prerequisite IO. So if you are included inside the wireless, you can be accessed to that. You can go to a guide which give you some instructions of how to use the test hand, how to play with us step by step. And then, so before when the first time you join the test end, you need to going to add all of these configurations to your metamask so that you can be able to switch between different scroll layer one and scroll L2. And then we have some example USDC tokens like deploy on the layer one both on L2. And then the first thing you're going to do is you're going to open this faucet application. Let me see.
00:42:02.860 - 00:42:46.934, Speaker B: Okay, so you can just request one ethereum and a 100 USDC token on the layer ones and it will just show up to that transaction hash like you're going to issue those tokens from the faucet account. So here we already have some of the already requests or something like that. It will show up after about 15 seconds. I'll just receive a new one ethereum and 100 USDC token and then that request only. You can only request once like every 24 hours. If you do that again, I could just show off some arrow and then the next thing you can do is trying to, so that token is issued on the escrow layer one. So the next thing you need to do is you want to experience the L2.
00:42:46.934 - 00:43:38.410, Speaker B: So you just should deposit, transfer that token you receive on the layer one to the L2 which you can use this bridge application are going to see. You can see like I want to transfer two ethereum to Ethereum to the scroll L2 and then click on send. I'm not using the correct, yeah, so now I'm on the scroll layer one. So you can send the scroll, the ether to the L2 and click on send and then confirm. So we're going to see a pending transactions here like after a few seconds. Okay, so you can now see this channel has been confirmed, which you can view on the block explorer. So yeah, sometimes the block explorer just takes a few seconds to show up on the block explorer.
00:43:38.410 - 00:44:20.662, Speaker B: So you can see now this transaction has been confirmed by one block. And after you see that this number goes up to six blocks, then that means the fund you transfer from layer one to L2 will be show up on your school L2 address account. So I previously transferred two Easter tokens that should have not been show up on the L2. So currently I only have 1.3 Easter here on the L2 account. But probably after probably two, three more minutes and then it will just show up like, it will becomes like three tokens, three easters on my L2 account. And then let's see.
00:44:20.662 - 00:44:47.110, Speaker B: So let's not wait for that time. And then we can go back so we can just play with the swap. It's also quite straightforward. You already have some ease and you just swap, sorry. Select a token USDC so I can swap one ether for the USDC. That would be kind of standard as what you do on the unit swaps. So now I will just submit that transaction here.
00:44:47.110 - 00:45:12.438, Speaker B: It'll probably show another pending transaction here. Okay. I think next thing that you can play with, you can see is the Rob Explorer. So there's a few more transactions. Actually the people are playing, still playing with the testnet. So you can see how many transactions here. For the latest blocks there will be like a few transactions.
00:45:12.438 - 00:45:50.560, Speaker B: So you can see the latest two transaction, probably those two blocks like which I was just submitted. So they'll be still at the pre committed status and then there will be, relayer is going to keep updating like sending the new transaction, the commit transaction to the layer ones. And after that transaction has been confirmed, then you can see it becomes committed after a while. So probably still waiting for the layer one to be confirmed those transactions. So these are some transactions already. Send it to the layer because we don't have, oh, there's a lot of transactions. Okay.
00:45:50.560 - 00:46:46.480, Speaker B: I think right now we don't have enough provers on the back end. So there are probably like a few, yeah, you can see like there's a few blocks has been finalized so that you can see this finalized both finalized transaction hash and then the commit transaction hash and that will be switched to the finalized status. And because right now at the testnet we don't have enough freeing power so that we selectively just prove one array, two blocks, that one block will be skipped, skip, just mean, like we just pounce on the, generating the decay proof for that. Okay, yeah, there's probably like one more demo I can show. I don't have time. Actually very soon we're going to support the arbitrary, like the contract deployment. So right now we still have only a few pre deployed contracts, but very soon we have the new functionality support to allow you to deploy any contracts you can.
00:46:46.480 - 00:47:32.062, Speaker B: So right now I can show you some examples. So I'm actually modifying a little bit on the scaffold east, which is a great tutorial that by the Austin did like that. So actually I was running this scaffold east on my own laptop and connect to the school layer too, which means you can deploy contracts. So what you can do, like you just do yarn start in the scaffold east. So I did some modification in a new branch so that it's connected not to the local node, but to our L2 node. So now you can see it will just initialize a website which you can deploy, debug things. I previously tried that.
00:47:32.062 - 00:48:10.610, Speaker B: So you can call the yarn deploy. So currently, like just the original example have inside the scaffold east, which have like a very simple example of contract you can interact with. So now you can call yarn deploy. Okay, so you first compile the contract you have inside the things, and then it's deploying the smart contract. And then waiting for the contract has been confirmed. Okay, now it's deployed. So if we copy this transaction hash and open inside the L2 scans, scroll IO and then you can search for that transaction.
00:48:10.610 - 00:49:01.406, Speaker B: Yeah, it'll probably take a few seconds to actually this boss got usually has some delay to show up the latest transactions. Okay, yeah, see this is the latest contract creation I just deployed on the score L2, which is like already success. And then if you go back to this and then, so you can see this new contract has been deployed on the L2. And then which you can also interact with this contract. You just deploy on the score L2, which you can say I can spot like the, oh, scroll, L2. Okay, now I can just call send and I'll just interact with the, see like I connect to the Scolaya two. Now just sending this transaction through my metamask and I click on confirm.
00:49:01.406 - 00:49:39.470, Speaker B: So I just say like okay, this transaction has been sent and then let's just refresh. Yeah, so you can see like this transaction has been already updated, this purpose like the string inside your smart contract and then later probably show up after the proof of catching up on this, you can then see those blocks will be finalized and generated some CK proof too, around all of those blocks we just generated. Okay. With that, that's all of the demo I have. And then thank you very much. If you have any questions, actually feel free to ask. I can have probably one or two questions.
00:49:39.470 - 00:49:41.246, Speaker B: If you have questions. Yeah.
00:49:41.428 - 00:49:46.100, Speaker C: Have you tested the case where verification fails and you need to roll back?
00:49:47.910 - 00:49:52.450, Speaker B: Usually. I think that means there are some bug inside ZKMA circuit.
00:49:53.110 - 00:50:01.640, Speaker C: Someone just made your block producer did something malicious or something.
00:50:03.370 - 00:50:44.020, Speaker B: Yeah. So I think right now we are still operating under the assumption we have centralized sequences. That means we are not malicious ourselves. But I think in the future, if something has been malicious, then I think we are going to have this proof generation part where we work inside prover, so that prover doesn't need to trust the trace that generated by the sequencers, so that you just need to operate on the transaction data like people send out, and then you generate the trace by the prover itself, so that if the sequence is correct, then you should be able to generate like a validity proof for that. That makes sense. Okay.
00:50:44.390 - 00:50:46.290, Speaker D: Is there any economic consensus?
00:50:50.330 - 00:50:59.450, Speaker B: We're still working on some more details, so we just keep updating on that. So I haven't finalized all of the details of the incentives.
00:51:02.270 - 00:51:14.878, Speaker D: So for the bridging, I guess from l one to l two, trusted relayer, and then from l two to l one. Have you set it up so that you have to Merkel proof to withdraw from?
00:51:14.964 - 00:51:18.670, Speaker B: Yes, there will be some Merkel proof associated with the L2 exit.
00:51:19.890 - 00:51:23.022, Speaker D: Do you have a plan for what you're going to do from layer one to L2?
00:51:23.076 - 00:52:00.760, Speaker B: Yeah, I think we also probably do do that. Once you have the decentralized sequences, you probably cannot trust anything like that. So you should have some proof from layer one to L2 deposit. Yeah, it could be like a little bit tricky because you need to have some data, like you have sync, like some proof, like the state from the layer one to L2. But I think I was just considering the research about that. Yeah, the aggregation is kind of like one way people call it for the recursive. So basically, recursive is basically you verify your proof inside the circuit.
00:52:00.760 - 00:52:46.100, Speaker B: Just people have kind of different naming for that. Batches, actually. So right now, if we process it on the block level like this, so each block you can have, a block could have multiple transactions. Sorry, what transactions? The coordinate will just basically receiving the new executive trace from the sequencer. So once the sequencer generates a new block, it will also generate some execution trace and they will just send to the coordinator l two.
00:52:46.710 - 00:52:48.610, Speaker D: L two to l one bridging.
00:52:49.030 - 00:53:14.134, Speaker B: L two to l one bridging. Okay. Yeah, can switch back a little bit. So the l two to l one is like someone just send a transaction to the l two bridge contract, and that bridge contract will just emit an event. And then also it will send some proof associated with that transaction. And then the relayer basically already received this event from the blocks contract. The bridge contract.
00:53:14.134 - 00:54:11.860, Speaker B: Sorry, the bridge contract. And then we'll wait for it will know the withdrawal transaction will be show up in which block, and we're waiting until that block has been finalized on the layer ones. And then you will send all of the withdrawal transaction, withdrawal messages to the layer one bridge contract along with some of the proof associated with each withdrawal. And then the layer one bridge contract will verify the proof with all of the withdrawals. And then actually for all of multiple, if there are multiple withdrawals within the single block, it will just call multiple transactions to transfer fund from the bridges to users account. Yes, I think that would have a better user experience, but we need to estimate some fees. When you call the withdrawals, estimate like how much fee associated on the layer ones to do the transfer.
00:54:11.860 - 00:55:10.240, Speaker B: Sorry, l two to l one, the frequency? No, it's just like the waiting. Once the block has been confirmed for that contain. If a block in the L2 block contains withdrawal transactions, they'll wait until that block has been finalized on the layer one so that the relayer can send out all of the withdrawal events inside that block to the layer one. Is the block size in L2? That's a good question. Actually, the L2 blocks, the limit is not on the gas. It's usually limited by the circuit size, how many transaction, or how many steps we can include in the circuit. So if you have a larger circuit, that means that you have higher gas limit.
00:55:10.240 - 00:56:11.210, Speaker B: But if I have smaller circuit, then you have smaller gas limit you can process inside the block. Yeah, that's also another great question. So yeah, there will be something like the. So we probably still take most of the gas model from the exchange, but with some minor tricks for some certain simple opcode, that could be resulting a great overhead inside the circuits. We'll probably tune up a little bit on the gas cost for that opcode, but usually I think like that the existing gas cost model is pretty good enough for most of the operations you have. Yes. So it's still kind of like inside of the research.
00:56:11.210 - 00:56:49.506, Speaker B: We have some initial analysis found for some of the Shastri catch of code could be a little bit cheaper in perspective of the decay proofs. And then there will be some external code. Size of code hash could be like a bit cheaper. So you need processing all of the contract code you have and then do some hashing on top of that. But I haven't kind of fully, definitely will be some security concern. We just also need to think about all of those different aspects. Okay, so I think I'll just end up now because we're a little overtime.
00:56:49.506 - 00:57:00.200, Speaker B: We can have like a ten minute break, and then we can come back to the second session where Yington is going to talk about hello two and then Mason going to talk about Zkum architecture. All right, thank you, everyone.
00:57:04.570 - 00:57:05.600, Speaker D: Thank it.
01:06:22.460 - 01:06:23.210, Speaker C: Sa.
01:16:42.140 - 01:16:43.316, Speaker B: Programming a hello cube.
01:16:43.348 - 01:16:43.640, Speaker D: So.
01:16:43.710 - 01:16:49.790, Speaker B: And a hello cube is kind of like a framework. Let's welcome.
01:16:54.560 - 01:17:32.632, Speaker C: Thank you. So, yeah, as Haichun said, halo two is a proof system. And Scrolls ZkVM is built on top of halo two. So this talk will give you a high level intuition for how halo two works, as well as some actual examples of how the API looks and some things you can do with the API. And then Mason is going to come and show you what the scroll team has actually done on top of this API. So halo two proof system can be split into three parts. Conceptually, the front end is arithmetization.
01:17:32.632 - 01:18:33.460, Speaker C: And this is really, as a circuit writer, what you'll interact with the most. So this is where you turn some program and some satisfying witness into a polynomial representation. And often there are different ways to arithmetize some relation. And the one that Halo two uses is called Planck or plonkish arithmetization. So after we've gotten a low degree polynomial that represents our circuit and witness, what we do is commit to it using a polynomial commitment scheme. This is the part you'll interact with less. But I think what's interesting to note here is that this part is also quite modular, and it's quite easy to swap out a polynomial commitment scheme, but keep the front end arithmetization.
01:18:33.460 - 01:19:21.180, Speaker C: So, in fact, what scroll has done is they've swapped out Halo two's inner product argument, and they've replaced it with something called KZG commitments. And the reason they've done this is that KZG proof is easier to verify on Ethereum. Ethereum has a pre compile for pairings on the BN 256 curve, so it's very cheap to verify a KZG proof on Ethereum. And the last component of Halo two has actually not been implemented yet. This is the accumulation scheme that enables proof recursion and proof composition. So I think you saw a preview of that with Haichen's aggregation circuit. It's pretty much the same idea.
01:19:21.180 - 01:20:14.952, Speaker C: It's when you can make a statement about not just one circuit, but many circuits, or like, a trace of execution across many circuits. Also. Interrupt me at any time if you have questions. Okay, so in today's talk, I'll focus more on the front end. I think that will be the most useful and interesting for you. So, a history of what I call plonkish arithmetization. It originated with a paper in 2019 by aztec, and basically it introduced a very flexible proof writing framework that allowed the use of custom constraints as well as things like lookup tables.
01:20:14.952 - 01:20:53.390, Speaker C: So these were extensions on top of the original plank paper. And a few of these, actually. Yeah. Custom constraints and lookup tables are essential for a complex circuit like the ZKE EVM. So being able to define highly expressive relations and capture the state transition rules of the EVM is very important. And being able to look up results across circuits is also important in composing circuits. So you'll see more of this later.
01:20:53.390 - 01:22:09.750, Speaker C: So, to go quickly through planckish arithmetization, the original planck paper defined this, what I call vanilla Planck. They defined this single constraint that includes some linear combination of variables. So by toggling the values of the variables in red, so these are the selectors we can express either an addition gate or a multiplication gate. So, for example, an addition gate is when you set ql and qr to one, q o, the output to minus one, and then qm to zero. And you see that the result of that is you're saying x a plus xb minus x c equals zero, right? So you're saying a plus b equals c. And if you set these cues to a slightly different configuration, you can have a multiplication gate. So over here, what we get is minus xc plus x axb equals zero, right? So a times b equals c.
01:22:09.750 - 01:22:45.552, Speaker C: Yeah. So this is the vanilla planck. And really, it was already quite powerful. With addition and multiplication, you can basically express anything. Now, a further improvement over vanilla planck was called turboplank. And turboplank actually introduced custom gates. So instead of being stuck with a one default constraint, you're now free to define your own gates in whatever linear combination you want.
01:22:45.552 - 01:23:39.330, Speaker C: So I can define an addition gate, I can define a multiplication gate, and I can even define a boolean gate. That's just saying like, a squared minus a equals zero, right? And all you have to do at the end of your protocol is to combine these gates using some random linear combination. So, for example, some power of a random verifier challenge to keep these gates linearly independent. Cool. So those are custom gates. Another sort of superpower of the plonkish arithmetization is permutation. So this is also what people refer to as routing the circuit together.
01:23:39.330 - 01:24:37.220, Speaker C: Essentially, it's a way to enforce equality between two wire values, right? And this is a global sort of constraint. And you can think of it as, after having defined all of your constraints, maybe you want to enforce that the output of one is the input to another. So you could wire them together with an equality constraint or a permutation. And at the end of the whole circuit, this permutation is checked globally. So let's see. So, like, intuitively, you would just have. Intuitively, you would just have one polynomial with your actual value assignments and one polynomial that's permuted, and show that the permuted polynomial evaluates to the same value as the original polynomial.
01:24:37.220 - 01:25:25.664, Speaker C: Yeah, questions? Okay, right now, I'm just kind of speeding through the toolbox that plonkish arithmetization provides us, and we'll see some fun use cases later on. So the last, and I think probably the most heavily used superpower is the lookup. So really the lookup is. Well, let me take you through a use case first. So the lookup becomes interesting when we're trying to perform some snart, unfriendly operations within the circuit. So anything that's, like, operating on bits or anything nonlinear is usually very expensive to express in terms of linear combinations. Right.
01:25:25.664 - 01:26:37.320, Speaker C: So instead of trying to compute, say, sha in the circuit, we could just pre compute outside the circuit, the hashes of some predefined set of some predefined set of pre images. And what we could then do is load the pre images and the hashes into a lookup table in the circuit. And so now, whenever we claim to have performed a successful sha in the circuit, we would just need to prove that it's some subset, that it appears somewhere in the list of legal pre image hash pairs. Yeah. So this is the use case for lookups. Another way you can think of lookups is as a sort of looser permutation argument, where you're not specifying the individual position of each permuted value, but rather you're just saying that these values have to appear somewhere in the original values. So it's a subset argument.
01:26:37.320 - 01:27:30.532, Speaker C: So I think Mason's going to go into this. But one way lookups are used in the ZKE EVM is to communicate between their state circuit, which keeps track of reads and writes. And the EVM circuit, which uses the results of these reads and writes in opcodes. Right? So basically the idea here is that to check the legality of reads and writes. It's more useful to order them in a certain way. Whereas to actually use them in opcodes, it makes more sense to order by opcodes. So we depend on the state circuit to produce a lookup table called read write containing all legal reason rights.
01:27:30.532 - 01:28:11.220, Speaker C: And then the EVM circuit can then use this table as a random access. To basically get the values that it's using when executing opcodes. And. Yeah, all I just explained, and in fact, this cross circuit lookup, it appears not just between the state circuit and EVM circuit, but between a whole bunch of subsurcuits. Linking the ZkaVM mega circuit together. Yeah. So that was the high level overview.
01:28:11.220 - 01:28:40.240, Speaker C: How am I doing on time? Yolo? 15 minutes. Okay. That was the high level overview. So hopefully you got a sense of what you can do with plunk. You can define custom constraints. You can define global permutations between values. And you can define lookup tables that allow you, basically give you a shortcut and allow you to avoid doing expensive operations in circuit.
01:28:40.240 - 01:29:21.176, Speaker C: So now I'll go more into the halo two implementation of plonkish arithmetization. And this will help you understand Mason's stuff later. Yeah. So the mental model of a circuit in halo two is a matrix with m columns and n rows. Now, each cell of the matrix contains a field element from some finite field f. And mathematically, what are these columns and rows mathematically? So each column corresponds to some polynomial PJX. So the columns.
01:29:21.176 - 01:29:53.784, Speaker C: Let's index the columns by J. So PJX is a Lagrange interpolation polynomial. So in other words, it's a polynomial expressed in the Lagrange basis. And PJX has this quality that when evaluated on omega I, I is the row number. It gives you the actual value that you witness there. X-I-J in that cell. And omegas here are the n's primitive root of unity in the field.
01:29:53.784 - 01:30:23.584, Speaker C: What this means is that omega raised to the power of n is the identity element. Yeah. So maybe it's not super important. Yeah. To write a circuit, you maybe don't need to know this, but I would like for you to know this. So it doesn't seem so magical, like, concretely, these are the mathematical objects behind the rows and columns. But if you don't care about this.
01:30:23.584 - 01:31:00.300, Speaker C: We can actually proceed just thinking of them as rows and columns. And what's important for you to know is that there are different types of columns. So the first type is called what we call instance columns. And you can think of them as public inputs, if you're familiar with r one cs. So these columns contain public values, meaning that they are known both to the prover and the verifier. Right. Whereas there's a second class of columns we call advised columns, or you can think of them as witness columns.
01:31:00.300 - 01:31:36.484, Speaker C: So these contain the privileged witness, and this is absolutely secret and not known to the verifier. And the final class of columns actually define the circuit. So these are the fixed columns, and these are baked into the silicon, basically. And these define the custom constraints in the circuit. These define constant values. These define your lookup tables in some cases. And so this is known publicly as well to the proverb and verifier.
01:31:36.484 - 01:32:23.388, Speaker C: And in fact, it's encoded in the proving and verification case. This defines the relation that your circuit is expressing. Yeah, questions? Yes. So the question is, why did I say baked in the silicon? So I was just trying to be cute and draw an analogy to the silicon when we're making chips. And I was trying to make the point that these fixed columns are, they are your circuit definition. And so from one proof instance to another, the fixed values cannot change. Yeah.
01:32:23.388 - 01:33:20.030, Speaker C: Because once they change, you're proving a different statement, whereas from one proof instance to another, your instance and advised values can change. Yeah. Any other questions? Okay, cool. So now we can go into a very simple example of how we would use this mental model to express a circuit that constrains the inputs to the Fibonacci sequence. So, to do this, what I'm going to do is define a custom gate. So the custom gate here is, let me exit here. The Custom gate is this part.
01:33:20.030 - 01:34:21.552, Speaker C: So I'm saying that a zero plus a one minus a two equals zero. Or in other words, I'm saying a zero plus a one equals a two, right? So now there's a Q fib selector that's toggling the custom gate on or off. And the effect of QfIB is if it's set to one, then a zero, a one and a two had better fulfill this relation for this whole thing on the left to evaluate to zero. But if QFIP is set to zero, then this expression evaluates to zero. And that's equivalent to you saying that you don't care, you don't want to check this relation on that particular row where QFIB is zero. So for example, you're doing some other things in your circuit, and on that row, you don't expect the values to be from the Fibonacci sequence. You would set QfIB to zero.
01:34:21.552 - 01:35:44.060, Speaker C: And this is the power of custom gates, that you can create these self contained blocks of logic in your circuit and have a very expressive circuit that makes use of a bunch of different regions. And I will also set over here, a zero plus a one equals a zero on the next row. So what I'm trying to illustrate here is that in defining your custom gates, you do have access to cells on different rows. Yeah, so the planck, because it's defined over the roots of unity, it allows you to rotate through the domain and excess values on different rows in the circuit. And then I've done the same thing, but moved it one column over. And in addition, I could enforce a permutation from the output of one row to the input of the next row. Yeah, this is probably not the best way to, or not the only way to implement the Fibonacci circuit.
01:35:44.060 - 01:36:35.710, Speaker C: So here you see, I can also enforce that the Fibonacci circuit starts at one and ends at 13. And I can do that by demanding equality between my public input and some elements of the witness. Yeah, how am I doing on time? Like 1 minute stop, what? Ten more minutes. Okay, cool. Nice. So that's a toy circuit that makes use of a bunch of tools in the plunge arithmetization. Any questions? Yes.
01:36:36.480 - 01:36:40.750, Speaker D: Is the first column always used just for initialization like this?
01:36:41.920 - 01:37:37.600, Speaker C: So the question was, is the first column always used for initialization? So that's a very good question. In short, no. So it is a degree of freedom provided to the circuit designer of how many columns of which types they want to use. So this is the circuit configuration, and it's set up by the circuit writer. And yeah, I said it's a degree of freedom because it's also a lever for optimization. Heuristically, more columns would produce a larger proof, because each column requires a commitment to the values in that column. However, heuristically, let's say our circuit has some complexity, measured in area and preserving the area.
01:37:37.600 - 01:38:21.980, Speaker C: We chose a configuration with more columns and fewer rows. So fewer rows would mean a faster prover, because it would mean a smaller fft. If you go below the next tower of two, you would have the size of your ffT. So, yeah, in short, the number of columns and types of columns, these are degrees of freedom, and they can be used to optimize your circuit for whatever setting you're interested in. So if you want a fast prover, you would have a wide and short circuit. And if you want a fast verifier, you would go for a narrow and long circuit. Yeah, Jordy.
01:38:21.980 - 01:39:24.352, Speaker C: Yeah. So the question was these optimizations or these choices about number of columns, number of rows, are they made automatically by the halo two back end or do you, the circuit writer, have to do it? So the answer is you have to do it currently, but the halo two back end helps you out to an extent. So this is a good segue to the next slide. Basically, that halo two back end helps you to pack your circuit vertically. Yeah. So right now we have a helper called the layouter that does two passes of your circuit. So in the first pass, it measures all the regions in which you've made assignments.
01:39:24.352 - 01:40:13.540, Speaker C: And in the second pass, it compacts the regions whenever there's free space available. And over here, this is a real circuit. So this circuit is Zcash's orchard circuit. So orchard is the latest payment protocol in Zcash. And the layout tour helped us to get from like almost two to the twelve to below two to the eleven. So it made our preferred twice as fast. Yeah, but I think a lot of manual work is still involved right now in deciding the configuration and some experience is required.
01:40:13.540 - 01:40:41.790, Speaker C: And we are interested in writing tools to automate this. So, like a tool that I would like is you, the circuit writer, tell me what you're optimizing for, like fast prover or fast verifier, and then you tell me some bounds which I cannot cross, and then all the rest should be done for you. Yeah. Any more questions? Yes.
01:40:45.280 - 01:40:46.300, Speaker B: You can do this.
01:40:46.370 - 01:40:48.780, Speaker D: Relative references across rows, right?
01:40:48.850 - 01:40:52.770, Speaker C: Yeah. Oh yeah.
01:40:53.620 - 01:40:55.730, Speaker D: What's the trade off between using more?
01:41:00.010 - 01:41:44.040, Speaker C: Yeah, that's a great question. Someone just asked. So when you're designing custom gates, you can make relative references to other rows. However, this makes the circuit harder to optimize. So what is the trade off between using these relative references versus not using them? So basically, how halo two deals with this trade off is with an abstraction boundary called regions. So a region is defined by a common relative offset. So all cells in the region share the same relative zero.
01:41:44.040 - 01:42:34.950, Speaker C: Right. And so within a region, what you should do is you should define all custom gates that depend on having some cell available at some relative offset. Okay, but let's say this gate, let's say the cell on the next row was in a different region. Then basically halo two does not guarantee that constraint will hold. And the reason here is that halo two considers each region as a self contained block of logic. And after a region is defined, the layouter has complete freedom to move them around like tattoos pieces. Yeah.
01:42:34.950 - 01:43:09.294, Speaker C: So if you defined a custom gate that used some cell in another region, then after one layout or passed, that region might have moved somewhere else and. Yeah, it's your fault. Yeah. So basically, when making relative offsets, do it in this. When referring to relative offsets, do it in the same region. Yeah, that's a good question. Any other questions? Okay, I think I am done.
01:43:09.294 - 01:43:35.900, Speaker C: Ish. Here are some other gadgets we've written in halo two. So the poseidon hash, we made a hash function called sensomia. That's kind of like a Pedersen hash. This is elliptic curve cryptography. And that's shotu 56. Yeah, so I am done.
01:43:35.900 - 01:44:46.868, Speaker C: The remaining slides are, they concern halo two's current state of development, as well as some open problems or some things in our wish list and in our roadmap. You can talk to me about it afterwards or you can join our ecosystem discord to continue the conversation. Yeah, thank you. Any questions? Yes. Oh, that's a good question. Someone asked between powers of two in the FFT, do we see step changes in performance or in short? Yes, we do. So once you cross, let's say two to the eleven, you're at two to the eleven plus one.
01:44:46.868 - 01:45:03.310, Speaker C: Then you need to do a two to the twelve fft. Yeah. So you might as well. Yes, exactly. Jordy.
01:45:06.690 - 01:45:10.360, Speaker B: You mentioned that was not ready. Can you give us a little.
01:45:13.170 - 01:46:08.170, Speaker C: Yeah. So Jordy asked us about the recursion roadmap. I have this super cool thing. Let me see, what is it? Developers Halo to focus. So this is an internal tool that the Zcash team uses to map out our sprints. So this is the recursion specific part of the DAG, and I'm trying to find the critical path. So these three notes are like the critical path for implementing recursion.
01:46:08.170 - 01:46:48.410, Speaker C: Okay. I'm surprised not every team uses this. This has changed our lives. Yeah. So there's some parts that we've already implemented, like parts to do with communicating public inputs from one curve in a cycle to the other curve. But there are parts that can be implemented in parallel and that we're working on. So in fact, let me see on the roadmap.
01:46:48.410 - 01:48:09.012, Speaker C: Yeah, we have plans with filecoin to apply halo two recursion to the file coin like proofs of spacetime and proofs of replication so that those proofs don't take up too much space on the filecoin chain. And we were also talking with Ethereum about using Halo Two for the recursive snark for their VDF. Although I think they've also implemented this in Nova. Yeah. So, Jordy, this is our roadmap. Yeah. Any other questions? Yeah, that's a good question.
01:48:09.012 - 01:48:48.280, Speaker C: So, Ramco asked, in this Fibonacci example, we're using a sparse selector polynomial to sort of toggle the constraints on and off, whereas the startware people use sparse vanishing polynomial instead. And what are their performance trade offs? I actually don't know. I will need to learn more about their approach first. Yeah. Unless you have some idea. Okay, cool. Other questions?
01:48:48.850 - 01:48:49.694, Speaker D: It.
01:48:49.892 - 01:48:51.600, Speaker C: Okay, thank you.
01:49:38.530 - 01:50:11.190, Speaker D: Talk about scroll and the Zkavm that we're building. Just so that we're clear on what we're building, there are a couple of different types of Zkavm. Types of zkavms. And so the top one is, like, language level. So this means that you take a higher level language, like solidity or yule, and you transpile that to a snark friendly vm. And then that's sort of like what you produce is your knowledge. Proof of.
01:50:11.190 - 01:50:56.550, Speaker D: And so this is what matterlabs and snarkware are doing. And then there's bytecode level zkvms, which is what us scroll, Hermes, and consensus are doing. And so in these, you interpret the EVM bytecode directly, and then you sort of prove the bytecodes are executed correctly one by. So, I guess in our case, we have one gadget that proves each, or usually, like, one gadget that proves each EVM opcode. And then in Hermes's case, they, I believe, have micro opcodes. And a bunch of these micro opcodes will correspond to. Will map to each EVM opcode.
01:50:56.550 - 01:51:52.450, Speaker D: But there are still certain differences between this and what happens at a consensus level Zkavm, one of which is we use different state hash function. So the state routes are different. And potentially, we discussed earlier the gas pricing might be different. And then the final one is a consensus level zkavms. And so this one would basically just be a Zkavm proof. A ZK proof that, in fact, that the state route transition is correct according to the Ethereum l one consensus. And so this is what, on the roadmap for Ethereum, what is meant by the ZK snark everything node we're building a bytecode level EVM.
01:51:52.450 - 01:52:35.190, Speaker D: DK. EVM. All right, so let's talk about what we're actually doing. And so high trend had a diagram that was sort of, that showed our workflow. And just to go over it again, so people have transactions, we run them on our forked Geth node, and this produces an execution trace. So there's the execution logs, block headers, the transactions themselves, the bytecode for all the contracts that got created, used, and then Merkel proofs for all the storage slots that got touched. And then this gets fed into our ZKVM circuit.
01:52:35.190 - 01:53:22.786, Speaker D: All the circuits that comprise our ZKVM, those circuits produce a bunch of proofs. All these proofs are then fed to the aggregation circuit. The aggregation circuit produces a proof that all these proofs are in fact valid. And this aggregation proof gets sent to the l one contract, which does all the stuff that high chain talked about earlier. And then at the same time, the geth node provides some data to the l one contract so that everyone else can figure out what it is. The validity proof is a proof of. Okay, sorry, in my talk, I'm just going to be talking about the stuff that goes into the ZKVM circuits and then the stuff that comes out of it.
01:53:22.786 - 01:54:22.354, Speaker D: So the execution traces, and then the Proust. Okay, so at the very top of the RZKVM is the EVM circuit. And so this constrains the operation of the EVM state machine. So what happens here is there's things like the amount of gas that you have left in your execution, the program counter, various call contexts, block context, constraint variables. Let's see what else is there. So sort of just like a lot of the bookkeeping that you do as the EVM sort of goes like bytecode by level, bytecode by opcode by opcode, like executing the transaction and calls. But then to keep the engineers, sorry, I'm one of the engineers at scroll.
01:54:22.354 - 01:55:22.230, Speaker D: And so, to keep us sane, we break this down into a bunch of other components. And so the first one is, there's a bunch of ZKN friendly opcodes, like ketchak is one, and then all the logical operations and or exclusive or not, what have you. And so these are done by lookups? Well, actually, well, I mean, everything else is done by lookups too, but these are also done by lookups. And so in the case of the logical and ors or whatever, these are done by the lookups that Yingtong just described into fixed tables. So rather than write the constraints that show that, hey, a and b is equal to C, we just look it up directly there's a table that has every single input to. And then the corresponding output. And then, because these are fixed, everyone can just see that, in fact, we have filled out this fixed table correctly.
01:55:22.230 - 01:56:18.482, Speaker D: Okay, so, yeah, I'll talk about how that's actually possible later, because. And then, so ketchak has the table that Yington just described earlier, right, where there's a table that has all the pre images in it, along with the ketchak values. In Yington's talk, she called it the state circuit, which is actually what it's called in the code. But I think it's more clear to call it the ram table. And so this verifies that the reads and writes have been done according to how you'd think they would be done, which is so that if I write a value to some slot and then I read a value from it later, that the value hasn't actually changed. Let's see. So then there's a bunch of other lookups.
01:56:18.482 - 01:57:11.802, Speaker D: There's the bytecode lookup, right? Which basically proves that, in fact, the opcodes that the EVM claims it's executing are, in fact the opcodes in the contract itself. There's the transaction lookups, which verify that the call data in the transaction is the call data in the transactions that we're claiming. And then the block context, which is sort of like, has things like the block number, block hash, timestamp, stuff like that. So all these are done by lookups, right. And so this sort of just is like how our circuits sort of talk to each other. There's more circuits on top of all of these that can make sure that the values in these tables are actually correct for the. And.
01:57:11.802 - 01:58:06.400, Speaker D: Or like the logical operation table. These are fixed, so we don't need a circuit to prove that these are correct. For the RaM table, we have a circuit that shows that, in fact, the values aren't changing in between reads and writes. And then there's also the MPT circuit, which is showing that for initial reads, that the values that were initially reading out of, say, storage slots can actually be found in the state tree. There's various things like the bytecode circuit. And there's, like, interdependencies between all of these, right? The bytecode circuit actually has to look up ketchack values. The catchack circuit proves that every row in the ketchack table actually does correspond to the shaw three.
01:58:06.400 - 01:58:49.840, Speaker D: The TX circuit has to look up into the ketchack circuit table because you have to verify that, in fact, these transaction hashes are correct. And then additionally, you also have to verify that we have signatures for all these transactions because we can't just sort of stuff it full of transactions that we're making up. And then the transaction signatures are themselves verified by the ECGSA circuit. So this is sort of how everything depends on each other in the ZKVM circuit. So, yeah, if you guys have any questions, feel free to ask. All right. Yeah.
01:58:49.840 - 01:59:34.906, Speaker D: The verifier will have access to all the. No, so there's the aggregation circuit at the very end. Right. Actually, the verifier has actually access to basically none of these tables other than this one, because this one is fixed. The other ones are assigned. Well, actually, sorry, that's not true. I think some of them are instance columns, like what Ying Chong was saying.
01:59:34.906 - 02:00:19.936, Speaker D: So these are public to the proverb and verifier. And then, so this table is fixed. But then, for example, the Ram circuit is an advice. These are all advice columns, so the verifier doesn't actually see them. The verifier only gets a proof that the prover provides that all the constraints in these different circuits are satisfied. To make sure that the bytecode that you're claiming. Yes, I believe there should be some more dotted lines here, I think.
02:00:19.936 - 02:01:20.790, Speaker D: Yeah, right. For example, you need an MPT lookup to confirm that these bytecodes that we're claiming are actually in the state, I think. Well, there should be an MPT table here that consists of MPT proofs for all sorts of facts. And then the bytecode circuit would look up into it every time it's saying like, hey, this is the bytecode at this address. Okay, so I guess, yeah, so that's sort of like the high level picture. And then now I'm just going to talk about sort of how some of these circuits work. So the EVM circuit, the one at the very top here, basically looks like we have one region for every execution step, I guess.
02:01:20.790 - 02:02:25.350, Speaker D: And usually each execution step corresponds to an opcode. And so in this example, you can imagine that the opcodes that we are interested in proving, the first one is push, and then I add whatever is on the stack, and then I multiply and so on and so on. To talk about slot, I in particular a one through aw, are sort of all advice columns that the prover assigns. And then q op basically is a selector column that just says, basically it's one however many block, I think. I mean, in this case, every six rows is one, which just means that basically every six rows I prove the execution of one opcode. And so there's some context variables. So like PC stands for program counter, SP is the stack pointer.
02:02:25.350 - 02:02:59.890, Speaker D: Gas is how much gas is left. And then there's more. And then there's also these sort of selectors for each of the opcodes themselves. So there's one for addition, one for multiplication, one for shift, right, and so on and so on. And there's also error states of the, there's also error states of the evm rights. Like I'm out of gas, I've overflowed the stack, whatever. And then there's these v zero through vn are sort of operating values.
02:02:59.890 - 02:04:19.742, Speaker D: And so these have basically context dependent values, I guess context dependent names, depending on which opcode I'm currently proving, they have different interpretations. And so let's see. Okay, but first, one thing that we should talk about is how do you handle 256 bit words? So the EVM has 256 bit words, which is unfortunate because the field prime that we use is slightly smaller than 256 bits. And so the way we deal with this is, let's say we have like a word a, and then we break a down into its constituent bytes. So there's 32 of them actually equal to this value here, right? So like the first least significant byte plus 256 times the second least significant byte, and so on and so on. And then what we do is actually when we want to assert the equality of two evM words, rather than check that if we have a and b, rather than check that a is equal to B, we can't do that directly. So instead we compute this random linear combination of them.
02:04:19.742 - 02:05:48.330, Speaker D: And so this is sort of like the random linear combinations that Yington mentioned earlier between custom gates, where so you generate this theta according to some public randomness. And then we compute this sum like a sub RLC, which is instead of using a one times 256, you just have a one times theta and so on and so on. And then this thing mod FP will actually be small enough to fit in the finite field. And so this just means that when we compute our lookups, we compute our lookups, we just compute the RLC of these values instead of just using the bare values. And so the technique of breaking up the words into bytes also comes up when we're computing things like logical operations on the words, right? So if you want to compute the bitwise end of a and b, then you can just do this byte byte and look up the bitwise end of a zero and b zero and a one and b one and so on and so on. So let's say we are back to the add opcode. So we also have these opcode selectors.
02:05:48.330 - 02:07:04.326, Speaker D: And so the constraints for these selectors are basically like what they're supposed to be is zero and binary selectors. And exactly one of them is one. And the one that's one is the opcode that we are trying to prove. And so the constraints that we have in this case are basically the selector times one minus selector is zero, which means that each selector is either zero or one, and that the sum of them is one, which means that exactly one of them has to be one. So this sum here can't overflow because there's not enough terms in this sum to overflow. And then, so in the case of addition, we prove it byte by byte in the way that you do addition in base ten on a whiteboard. So this first constraint says that the first byte plus the second byte of the first byte of a plus the first byte of b is equal to the first byte of c plus potentially the carried bit times 256.
02:07:04.326 - 02:08:08.240, Speaker D: And then this thing, so this thing gets repeated for all 32 bytes. And then we are missing constraints here that say that in fact the carry bits are zero or one. But then if all these constraints hold, then it will hold that the value represented by the bytes of c is actually equal to a plus b. So then there's sort of like, sort of like EVM bookkeeping to do. The program counter here has advanced by one because now we've successfully proven the addition opcode. The stack pointer has gone down by one because the addition opcode pops two values off the stack and then pushes their sum onto it. And then the gas has gone down by three because that's the gas cost of the addition opcode.
02:08:08.240 - 02:08:52.790, Speaker D: And then there's also more like, we have to check that the stack pointer is still less than, still less than 1024 and things like that. Any questions so far? Yeah, is there some way to. Where does the randomness come? Do you need to do an extra step to get like. So I think the randomness would be like basically some hash of the public inputs that you pick. But yeah, you have to be careful about it because if, like. Yeah, right. But yeah, you can't just insert any value for theta in there.
02:08:52.790 - 02:09:25.600, Speaker D: And then there's a lot of carefulness you have to do. Right. You have to actually make sure that none of the values that you're randomly combining are themselves random linear combinations because then issues come up there too. Sorry, you also had a question. Yeah, maybe a naive question, but it's fine. What's preventing you from picking a bigger prime and avoiding. How do you do this randomly? Combination.
02:09:25.600 - 02:09:46.260, Speaker D: Sorry, the reason you have to do. Right, so if the prime are larger than two to the 256, then this is actually like a hardware problem for us, right? Because you have to multiply these larger primes. Now they don't fit into 256 bits.
02:09:53.300 - 02:09:55.168, Speaker B: Okay, let's see.
02:09:55.334 - 02:10:54.420, Speaker D: Okay, so now in our EVM circuit we have c, which is equal to a plus b. But then all this sort of needs to be communicated with, communicated with the ram table. And basically what happens here is that when we pop the a from the stack, what actually happens is that the EVM circuit says this row exists in the ram table. And then, so I think this value corresponds to vb, right? So I'm saying that at position 1023, I got va. At position 1022 I got vb. And then this push goes back to position 1022. And then, so the EVM circuit, all it's really doing in this case, right, is constraining that Vc is equal to va plus vb.
02:10:54.420 - 02:11:35.383, Speaker D: So which brings me to the ram table. Or actually, no, it doesn't. Okay, so far I've all just been talking about the constraints themselves. So the other part of generating the proof is you have to fill in the witness. You have to actually fill in the witness from the geth traces that we have. There's just a lot of sort of data processing to do here where like the, the get, the geth traces come to us in like a JSON form and then we have to process them so that now they're these like rust structs and we fill in the context. Right? So like geth, I, I.
02:11:35.383 - 02:12:44.960, Speaker D: Yeah, believe just tells us, here's a program counter, here's a stack pointer, here's the gas, right? But then these things don't have a corresponding guess value, so we just have to compute them ourselves. But this is just like, okay, put one for the opcode that we're executing right now and then zero for everything else. So then we also have to fill in the intermediate values that were used to prove that, in fact, VA plus Vb equals Vc. So here are all the bytes of Va. Here are all the bytes of Vc, here are all the carry bits, here are all the bytes of UC. Okay, I think Halo two, the main branch right now, only lets you look up from fixed tables, but we've modified it so that you can now look up the lookup argument actually works for into advice columns as well. And so this is very important for us, because a lot of our lookups are not fixed, right? Like the Ram table changes every time for every new transaction.
02:12:44.960 - 02:13:56.524, Speaker D: So it wouldn't work if they had to be fixed. The Ram circuit is the thing that constrains the Ram table. Earlier, when we're executing the VM or the EVM, these are random access, right? So you might read from the stack, and then you read from the memory, then you write to the stack, and then you read from the storage, and then so on and so on. Right? But what we need to prove is that if I write to the stack and then I read from the stack, then that value doesn't change in between. But the order of the reads and writes in the EVM circuit aren't conducive to proving this. The Ram circuit basically has the same contents as the Ram circuit proves this. And the way it proves it is it sorts the reads and writes in a different order.
02:13:56.524 - 02:15:07.444, Speaker D: So instead of them being in execution order, in this case, it sorts them so that all the stack read writes come first, then all the memories, and then all the storage reason writes. And then within the stack reason writes, we sort them additionally by the address and things like the call id, so that we can verify that within a call id, within we have read write consistency for every particular stack address. Let's see in this case. So this read write index sort of tells you how the EVM, I guess. Yeah, like how the EVM, in what order the EVM accessed these elements. But then in the Ram circuit, the Ram circuit constrains that these things appear in this sorted order. And then now we can basically write these simple constraints that say if it's a read, then the values don't change.
02:15:07.444 - 02:16:17.096, Speaker D: And if it's a write, then the value can be whatever you want it to be. And then additionally you also have to add constraints, saying that initial read to a stack should be zero, an initial read to a memory should also be zero, and an initial read to a storage should be like whatever the MPT circuit says it is. And so that's also a constraint that has to show up in the RAm. Yes, well, because you want all the reason rights to a particular address to be like one after another. Because it's like what Yin Kong was saying earlier, you don't want to have these long distance constraints, right? Where like, oh, I'm like you, you actually can't even express it in halo two. You can't say I read it here and then ten rows later, or maybe 100 rows later, depending on what's happening in the EVM I read from it. And then I want these two values have to be the same.
02:16:17.096 - 02:17:22.116, Speaker D: So if they're sorted, then it's actually just like the next one sorted by address, but also sorted the world for each address. Yeah, so the sort order is like the tag, the address, and then the read write index. This is actually a simplified version of it. There's like even more things, because there's other things that are effectively like random access, like different call context variables are in this transaction, log logs, even things like here are the accounts that I've accessed in this transaction. And so all of these are in the Ram circuit, but they're not in the slide for simplicity. Anything else? Okay, these are just some of the constraints in the Ram circuit, right, so that if it's a read, then the value has to equal the previous value, I. E.
02:17:22.116 - 02:17:54.960, Speaker D: It can't change. And then we also have to check that these values are valid. Right, so RW is zero, one. The tag is one of stack memory or storage. So these correspond to just one, two, three. And that in the case of the stack, the address has to be within 1024. And then this is what I was talking about earlier, this constraining the sort order of the keys.
02:17:54.960 - 02:18:32.890, Speaker D: So basically first they're sorted by tag, then they're sorted by address, and then they're sorted by read write index. And there's actually a final constraint that you can't repeat a read write index either. All right, so yeah, just want to thank all the community members who have helped us write this code and are continuing to help us write this code. Are there any questions? Any more questions? Yeah, could you explain a little bit what the advantages are of rather than the transpond?
02:18:35.550 - 02:18:36.300, Speaker B: Well.
02:18:39.810 - 02:18:57.266, Speaker D: Okay, so I guess one is just like there are some implementation things that are easier, I guess. What kind of advantages are you interested in or like, because there are trade offs of course, right?
02:18:57.368 - 02:18:58.020, Speaker B: Yeah.
02:19:00.150 - 02:19:04.180, Speaker D: As far as portability, people generally have their source code.
02:19:08.560 - 02:19:10.380, Speaker B: Are there implementations?
02:19:21.040 - 02:19:27.580, Speaker D: Yeah, I think I'll differ to hyena.
02:19:36.360 - 02:20:01.730, Speaker B: Plugins or like another compiler and also extra compiler infrastructure could be like adding more security concerns, like probably modification by keeping this micro compatibility.
02:20:09.790 - 02:20:12.460, Speaker D: Oh yeah, Ramka also.
02:20:18.100 - 02:20:19.056, Speaker B: Something you need.
02:20:19.078 - 02:20:20.800, Speaker D: To do to go down this route.
02:20:23.070 - 02:20:23.914, Speaker B: It.
02:20:24.112 - 02:20:59.670, Speaker D: Well, yeah, I guess in the case of create. Right. I think you could work around that. Right. But yeah, with bytecode compatible, you don't have to worry about it. Anyone else? Okay, cool.
02:21:03.960 - 02:40:08.112, Speaker B: So we'll have another ten to 15 minutes, right, so I understand you're probably already very tired after the whole workshop. So in the last talk, we are going to talk about some performance optimization for the ZKE EVM. So I think first things, I'm going to describe what happened inside the halo two provers or inside the Plunkish provers with the Kzag commitment. And I'll then talk about what kind of optimization we did and then what's the performance on performance number we have so far achieved. So, first of all, I think previously also Yington showed this kind of diagram of what happens inside this halo two or plunkish circuit studio. So first of all, at the top level, you have the circuit arithmetication where you write the circuits, you define all of your custom gates, the lockup arguments, and then the permutation you're going to do to construe circuits. And then second, we will convert those circuit arithmetic and then define, it becomes into the, convert them into the polynomials, and there will be becomes the relationship between the polynomials or the constraints between the polynomials, quite like the polynomial identities.
02:40:08.112 - 02:40:48.528, Speaker B: So you can see in this step, in the second step. So I just have a lot of equations of all of those polynomials. And those are a bit of constraints that the proof are you trying to constrain and then generate the proof around that. And then last things, what you're going to do is using some of the polynomial commitments scheme to do that. And then in our particular case, you're probably going to commit to your polynomials and then open at a random points that are going to check in the verifier. And what we're using for the polynomial commit scheme is KZG, the commitment scheme. There could be like some other commitment scheme, for example the IPA used in the original halo two.
02:40:48.528 - 02:41:41.584, Speaker B: And then there will be fry as another commitment scheme. But in our proverb we are using for the Zkevm, the scroll or psE zkevm is that we are using the KDG commitment scheme. Okay, so actually Yin Tony has already covered a lot about the plankish arithmetization. So you have different types of columns, you have advised columns that's going to put any arbitrary witness data into those advice columns. And you have fixed columns for some fixed data you can put into that that could be like already preprocessed before in the verification and approving keys. And then there are selectors that control all of the custom gates and also the lookups you're going to do. And then there's some instance column inside your circuit which is going to hold any public input data inside that.
02:41:41.584 - 02:42:30.572, Speaker B: And then there are three major primitives, basic primitives inside the Plunkege arithmetication. So one is the custom gate. So you can define any constraints or relationship between all of different cells with rotations, and you can do the lookup argument with a very powerful thing. So you can prove like a column which had all of the value inside of one column is belong to some of the table columns that you predefined or you can generate through the advice. And then the last thing will be the permutation, which you can copy data from previous rows to the next rows. Those are the primitives you have in the plunketch automatication. Okay, so next I'm going to talk about, by the way, when we're building the zke event circuits, we mainly use the first two, the custom gates, and then the lookup arguments.
02:42:30.572 - 02:43:27.216, Speaker B: So the permutation, we didn't use a lot inside that, because once the EVM circuit has a lot of flexibility inside that, so the permutation need to be fixed per circuit. That's why the permutation is not heavily used. So, I'll just going to talk about more about the custom gates and lockup argument. So, first, the cost of adding a custom gates. So let's take example. You define this t shape custom gate, involve like the three elements in the first row and then the one more elements in the next row, and you can define the constraint is that va multiply vb, multiply vc minus vb equals to zero. And then that will be then translate into this polynomial identity, which is called like a one, which is defined as like the column, like when you commit the column a one, it will become a one x polynomial for this column.
02:43:27.216 - 02:44:21.220, Speaker B: And then multiply a two, and then multiply a three minus a two omega x equal to zero. And here the omega x basically defines like this is the next row with regard to the previous three rows. And then, because this custom gate may not apply to every rows in these three columns, so that's why you have a selector column. And then when you can set selector to one at that rows. So that means this custom gate only check the row we have here, mark as in the red regions, and then that will be also corresponding. Change the polynomial identity here, so that you need to multiply one additional polynomial into the previous polynomial we have. So that means like this polynomial will only check whenever the selector is set to one at that row.
02:44:21.220 - 02:44:44.584, Speaker B: Okay, so then that means in total, in your circuit, if you have Dg custom gate arguments there, then you will end up into Dg number of those polynomial identities in your final proof you need to work on in approver. Okay, next thing, talk about the cost of adding a lookup tables a lookup argument.
02:44:44.632 - 02:44:45.084, Speaker D: Sorry.
02:44:45.202 - 02:45:32.264, Speaker B: So for example, let's say we want to prove all of the value inside the a one column, but belong into the values into the t one column, which is some table column there. For example, see here. For the a one you only have value 1212, or one two three, like there has arbitrary values and you can have duplicate values. And for this table you only have one two three are all the value values you can have for the a one. So what are you going to do? There are some different versions of this lookup argument, so I'm talking about the one. It's specifically used in the hello two. What are you going to like in the hello two is that you will generate two new columns from the a one and then t one into this a one prime and t one prime.
02:45:32.264 - 02:46:31.660, Speaker B: So you're going to do is like for the a one column, you're going to sort every value inside a one column and then create this new columns. And then for the t one, you're kind of like sorting this column into new things. But what you do is you will map this value one into the first appearance value one inside the a one column, and then rest of them will just paddle with zero. And then you will put the value two at the same row as the first value appear in the value two inside the a one prime column, and then so on and so forth. That's how you generate this a one prime and t one prime column. And the second step is that you need to constrain this a one prime column is actually using all of the value inside the original a one column to construct this new a one prime column and same for the t one prime columns. So you need to add this additional multi set check to constrain the a one prime and t one prime from the original a one and t one columns.
02:46:31.660 - 02:47:22.192, Speaker B: That means like these two polynomial identities, you need to add in your final proof. And then third things that you see constrain this is finally the constraint, the value that all of the a one value will belong into the value inside the t one prime. So what you're going to do is you could check all the value inside a one is either equal to your previous row value, or you're equal to the same value in the t one prime column. And then you will have some initial conditioning check, which is like the first value need to be same. That's like how you're adding two more extra polynomial identities inside the proof. So to summarize, for the cause of lookup argument. So first I forgot to do that.
02:47:22.192 - 02:48:11.840, Speaker B: When you do this multi set check, you are going to construct another column like the Zt column, which is like z column here, I forgot to mention here. So this is going to help you to this kind of helper. So I kind of had way beyond that because there's a lot of math behind that. But just you create like a new column to check this running sum check that's going to constrain this permutation check. So that's why in total, if you have Dt lookup arguments inside your circuit, you need to add additional three DT polynomials or columns you need to construct. So those kind of are virtual columns, which doesn't exist in your original circuit. And then you will have additional four DT polynomial identities, those like four constraints you have in your final proof.
02:48:11.840 - 02:49:03.952, Speaker B: Okay, so now let's take a look at what happens when you generate the proof like in the halo two plus the KDG provers. So first of all, in the first phase we define there are four phases in total. So the first phase is that you are going to assign, like given the input data you have, you need to assign all of the values into your giant two dimensional matrix. I would call the witness assignment. And then next things you are going to do is for every column inside your circuit, including all of the advice column, fixed column like instance columns, et cetera, and permutations, you are going to commit them. You first need to convert them into polynomials and then you will do some commitment using whatever commitment scheme you are using inside your approvers. So like we call the commit like fx, a lot of things, those things.
02:49:03.952 - 02:50:23.220, Speaker B: And then the second phase is going to do some extra processing for the lookup arguments, which you need to generate, construct all of the a prime x, t prime x, and the Ztx are going to use to prove constraint this lookup arguments. And then for all of the additional polynomials you generated, you also need to generate some commitment. You need auto commit all of these extra additional columns there. And in the third phase what you're going to do is you need to compute the giant quotient polynomials which you include all of the custom gaze constraints and then all of the lookup arguments, constraints and fermentation arguments there. And then create a giant quotient polynomials and then with some random linear combination of all of the polynomials you have identities you have. And then, so that's like the h prime x, which is the hx divided by the x n minus one, which means that this original x hx should be true for all of the value from the one to n you have. And then because this h prime x, the degree of the h prime x is pretty high depending on the highest degree you have for all of your custom gates and the lookup arguments.
02:50:23.220 - 02:51:34.910, Speaker B: So you are going to slice this h prime x into certain number of the slices and each slice of the polynomial will have the degree up to n like the for each degree, for each polynomials. And here, how many slices? Here is the two to the extended k minus k, where this extended k is kind of the, this difference between the extended k and k is determined by the highest degree for all of the polynomial identities. So in terms like what's the degree sample here, this l zero times zx minus one, the degree of this polynomial identity is two because you have almost like l zero times z. So there's two polynomials multiplied together. So that's kind of defined the degree called the degree of all of the polynomial identities. So here the difference is here is determined by the highest degree of all of the polynomial identities you have. And then you also need to commit all of the sliced hi prime x polynomials here.
02:51:34.910 - 02:52:44.088, Speaker B: And then the last things what you do is going to do the openings of all of the polynomial you have at the random points and all of the rotations you have. The rotating here is meaning you create like at the current row or the next row or the next second rows, those kind of the rotations. And then you will use this multi open protocol at the final to do the pairing check, those kind of things, running some multi urban protocols. So these are the four phases you happened in behind of the approver when you do the proof generation. Any questions so far? No? Okay, so next I'm going to talk about the type of different types of computation you happen inside proof generations. So the first is the wisdom generation, which just only like in the finite field, you do a lot of computations and then filling them into this two dimensional matrixes to do like assignment. And the second, what you're going to do is you need to convert all of the point values between the point values into the polynomial coefficients.
02:52:44.088 - 02:53:25.928, Speaker B: That's how you get a column of values and then convert them into a polynomial, which you apply. The operation you apply here is doing the FFT to do that. And then you do the ifft to convert from point values into the polynomial coefficients and use fft. Or maybe like I wrong about that. So anyway, so you use these two operations to convert between these two things there. And then the next thing is like you need to do some polynomial multiplications and divisions. So those are the vector operations, which means if you have convert your polynomial into those point values, then what you need to do is you multiply those point values together.
02:53:25.928 - 02:54:10.792, Speaker B: So those are like all the vector operations. And then the fourth is do the polynomial commitment here, because we're using the KDG. So we are just using the MSM operations on the elapial curves you have like you choose for the KDG. And the last things I could do is open polynomial and the random points. And then one more thing is you probably run some multi open protocols there. So those, like the open polynomial at the random points is that you're given a scalable value, and then you evaluate your polynomial with all of the coefficients you have inside the polynomial, and you can calculate what value that polynomial evaluates at a certain x you choose, at some random x you choose. So let's see what happens.
02:54:10.792 - 02:54:51.540, Speaker B: What kind of computation is involved in each phases. So, in the phase one, which we do, like the witness assignment, and some preprocessed columns. So that kind of computation you do is witness generations and MSM, Fft and ifft. So MSN, which you need to use to commit a polynomial and FFT. And ifft is which you need to generate some of the, you need to convert the original columns into different values and then different coefficients. And the second, like in the lookup arguments, all the combination you do, the first thing is like, you need to construct these permuted columns, which I described before. And then you also need to do the MSM and IFFT.
02:54:51.540 - 02:55:41.732, Speaker B: And the third things for the computer quotient polynomial. What you need to do a lot of things is to do the polynomial multiplications and divisions so that you can construct this final quotient, hx polynomials, and they also do some fft here inside phase three, and then the phase four, which evaluate the polynomial at the random points, and then run the multi urban protocol. What you need to do is to do the polynomial evaluations, the multi urban protocols, and then some MSN you need to do inside of phase four. Okay, so that's all about all of the proverb, what prover does in behind. So next, I'm going to talk about some optimization for the ZK event we did so far. So this is like the previous graph we did. So actually there are two different types of the circuits here.
02:55:41.732 - 02:56:22.740, Speaker B: So one is all of the circuits inside the Zkevm. So those circuits they just generally have more custom gates, more lookup arguments but fewer rows. And then another type of things is the aggregation circuit which have more rows and then fewer custom gates and then lookups. So take example to compare the EVM circuit we have so far versus the aggregation circuit we wrote. So the EVM circuit we use two to the 18 rows inside our circuit that can handle around about a million gas. You can have to handle that. The gas is inside ESM gas and then you have 116 columns inside that.
02:56:22.740 - 02:56:54.460, Speaker B: But a total like you have almost like 2500 different custom gates and then 50 lookup arguments. And then the highest custom gate degree we have currently have is nine. But there's also adjustable so you can have some config. You can change it to smaller value like five. But currently we just use the config as high custom gate degree is nine. And then if you want to handle more gas inside the event circuit, then you need to have more rows. For example, if you have two to 19 rows then you can probably handle 2 million gas.
02:56:54.460 - 02:57:25.770, Speaker B: And then in comparison, the aggregating circuit we have is two to 25 rows because we need to aggregate all of different circuits inside one aggregating circuit. That requires do a lot of ECC operations inside the aggregating circuit. Right now we need to have the two to 25 rows to aggregate all our proofs we have inside the ZkevM circuit. And then we have 23 columns and one custom gate. Seven lookup arguments and then the highest and custom gate degree is five.
02:57:26.860 - 02:57:43.150, Speaker C: Hi, can we have a question? Yeah, so you have Ryan asked if you have so many custom gates, is there a selector per custom gate? Because it seems like you don't have 2498 columns. Right.
02:57:43.920 - 02:58:07.904, Speaker B: So you have share a lot of selectors across different custom gates. For example, we have one gadget, almost a one gadget for each opcode. But inside each opcode you have different, like you have multiple custom gates corresponding to that gadget and then one gadget usually constrained by one or two selectors to constrain that custom gate gadget.
02:58:07.952 - 02:58:11.988, Speaker C: Yeah, it's like you implemented the opcode switching.
02:58:12.084 - 02:58:12.776, Speaker B: Yes.
02:58:12.958 - 02:58:17.416, Speaker C: Without having to use like 2500 basically.
02:58:17.598 - 02:58:22.600, Speaker D: And then does that columns count also include for lookup arguments?
02:58:24.940 - 02:59:09.514, Speaker B: No, this is the original columns you have. So if you want to see for this, you end up like around 300 columns after you counting down all of the additional columns you have and the points you have. Okay, so now let's look at some performance numbers. So if we run this for the event circuit case study, look at example of the event circuit. So if you do the proof generation on the cpu using the hello two. So right now we are benchmark on the AWS G five tarvx large instance, which have 48 cpu cores and 192gb of cpu ram. And then there's four Amidia a ten gpus.
02:59:09.514 - 03:00:03.182, Speaker B: So if you're running everything inside the cpus, that's the amount of the time you need to spend in each phase. And then the total amount of time you have for the proof generations, about like four and a half minutes to generate for the EVM circuits. And then, so the first off matching we did on top of the CPU is trying to move the MSM FFt iFfT into the GPU kernels, which involve like for example phase one, you have these things like the. So actually every phases will be involved, some of the things that will be happening inside the GPU instead of inside the cpu. And then now let's take a look at what happens after you apply this gpu kernels, where you can see like the phase one, the most of time is spent is doing the MSM or FFT stuff. And then, so you can reduce that from 70 seconds to down to the under 5 seconds. And then, so it counts like all of the things up then.
03:00:03.182 - 03:00:31.798, Speaker B: Now the proving time is about under two minutes. So we can achieve like a 2.52.4. X speed up. And then this is using the four gpus. So you can evenly distribute the different ffts into different gpus and then different MSM to different gpus. So that's like using, utilizing all four gpus on this AWS instance. And then, so next things we realize is now the bottleneck the most time spent is in the phase three, which is the highest time.
03:00:31.798 - 03:01:15.522, Speaker B: What phase three does is not only the FFT, so it also does those polynomial multiplications and divisions. So it also can be very easily to be paralyzed using the gpus. So the second things we optimize we did is to compute those quotient polynomials inside gpus. And then, so now you can see that you can half the time you can spend in the phase three from 33 seconds to 28 seconds, 28.7 seconds. And then the third thing like that. So right now, I think the third thing we did is to, in a phase two, there are certain part of things like construct the permutate columns for all the local arguments that happen, like they're still down in the cpus.
03:01:15.522 - 03:02:09.910, Speaker B: But if you can pipeline the cpu part of the things with the GPU. So when you're doing the MSN or FFT, for the new columns you generated for this lockup argument, then at the cpu you can also to construct permitted the new columns for the next lockup argument. So I'm doing like this pipeline stuff to optimize the phase two. And then what you get is like in the optimizing three, you can decrease the phase two from 23 seconds to 5.9 seconds. And so the total time, like now, if we want to generate the proof for the ZK, the EVM circuit is like around 58 seconds, so it's below 1 minute. And if you compare that all the optimizing we apply with the original cpu, once they achieve almost like five x speed up comparing, they're using the cpus.
03:02:09.910 - 03:02:24.042, Speaker B: Any questions so far? Okay, so next sector, just take a look at it, for example. Sorry, there's one question.
03:02:24.096 - 03:02:24.582, Speaker C: Yeah, sorry.
03:02:24.656 - 03:02:31.530, Speaker D: Is this for proving validation one block of transactions?
03:02:31.610 - 03:03:40.854, Speaker B: Yes. Okay, so next, do you want to look down like they'll look into details how much time you spend inside MSM and fft ifft. If you're in the cpus, you can think of like you can take a look. So that's almost like 64% of the total time will be spending during the MSM FFT iffts, which you can say like this is some rough number there, but if you want to look at the gpus, right? So the gpus takes the latest optimization you have here. You can find like actually after you moving all of FFT and IFFT MSM into the gpu kernels, now the total time you spend in the gpu for those operations is only like around 16% of the total proof generations. And then the rest of them actually now becomes all of the 58% of time. Although here in the phase three, this part of 25 seconds is doing the polynomial multiplication divisions that also mostly happened inside the gpus, but the rest of them you can see.
03:03:40.854 - 03:04:30.838, Speaker B: So currently we haven't moved this like a multi open and then polynomial evaluation into the gpus. So those part of things could be like the next target to further optimize, to reduce the pre generation time. Okay. But on the other case for the aggregation circuit, now we also did the similar optimization to the aggregation circuit. And then now you can find actually, because the aggregation circuit, the differences between aggregating circuit versus the EVM circuits is that aggregating circuit has more rows and then fewer custom gates. Now you can find that the phase one takes now the most of time in all of the proof generations. So it takes almost like two minutes in the phase one.
03:04:30.838 - 03:05:18.920, Speaker B: So what happens like in the MSM and FFT, it doesn't takes a lot of time. That's like almost like two minutes. What it's doing is doing the witness assignment, because you have a very large circuit, and if you feel them like that, one by one, row by row. So it takes a lot of time inside the cpus. Actually, that's something that we're continually optimizing and working on to improve that, to see if we find any way to paralyze this proof generation witness assignment inside the cpus to utilize multicores. And then you can see like in the phase three, which does a lot of polynomial multiplication and activations, that's actually less time than the event circuit, because it has less custom gates, so that you have fewer polynomial modifications. You need to do like operating in the phase three.
03:05:18.920 - 03:05:51.672, Speaker B: Okay, so there's some quick summary. We have some takeaway we have from all of the optimization and then the comparison things. So the FFT and the MSM is still like dominating time if you're using the cpus. Although currently we're not sure. Definitely, probably we haven't heavily optimized the FFT and MSM in the cpu. So that could be like some hand wavy claim we have, because we haven't really optimized our bills. We spend most of time just optimizing the GPU kernels.
03:05:51.672 - 03:06:25.908, Speaker B: But it could be still the case. Like FFT and MSM could be dominating if you don't use the GPU kernels. But that won't be the case actually, for if you have the gpus, it only adds up to 16% of total time. And then, so for the large circuits, the wittiness generation is really kind of a bottleneck for the proof generations you have. And then for different type of circuits, you have different characteristics for different circuits. So you need to do apply some different optimization to different circuits. So, in the aggregating circuit, so more rows lead to higher cost of large ffps.
03:06:25.908 - 03:07:07.280, Speaker B: So you need to have better large FFP kernels in the GPU. And the EVM circuit actually has lots of custom gates, so that you need to do better polynomial multiplications and probably polynomial evaluations better to do that. And then, so that you need to tune the performance based on different circuits layout. Okay, so previous are all about the proof of optimization we did so far, and then we'll continue working on that. So, next, I'm just going to talk about how we want to build our decentralized, proven network. Yeah, so this is the kind of the architecture I showed before. So we have this decentralized proven network so that everyone can join and build that.
03:07:07.280 - 03:08:31.536, Speaker B: But how we are going to reach to that goal, like the final goal there. So I think right now what we are doing in the stage one is we build a GPU solution for generate proof or the ZKE event circuits and then we're going to build like so right now we're using some AWS instances for the proof generations, but we'll also build some private GPU clusters to provide a more stable and then cheaper solutions for building for the proof generation power on our testnet in the stage one. But the next stage we are going to do is we're trying to collaborate with some hardware partners and companies and then to see how we can share our results so far and then share our proof systems. And our organization did, some profile did, and then to see how we can work with them together to build a better customized hardware accelerators. And I would believe that with better custom provers it can still can improve shorten the proof generation time. You can see how we can do better paralyzed on the rest of part we currently spend in the cpus and then how to make the MSM and FFT even faster than the gpus. And then at a stage three, our plans to going to definitely open source our GPU provers we have so far.
03:08:31.536 - 03:09:22.690, Speaker B: And then probably we'll have some add up a little bit more optimization with some permissionless license so that everyone can use as some baseline so you can run the GPU server yourselves to become our proverb, a roller to join our decentralized network. At that time we see if there are some solutions from the hardware company so that people can also buy those custom hardware from those hardware companies and then to use that as their approvers in parallel. That's pretty much like what I want to say about our decentralized proven network. And then thank you if you have any questions. Thank you. Many of the.
03:09:27.510 - 03:09:28.766, Speaker D: According to your results.
03:09:28.798 - 03:09:31.300, Speaker B: They only come to a small portion of it.
03:09:32.070 - 03:09:36.530, Speaker D: So do you think their improvements will actually improve the overall.
03:09:37.670 - 03:10:34.634, Speaker B: So as I said, right now we find the MSM and FFT. So for the smaller circuit, definitely they're a small part of that, but I think for the larger circuit they will be still there's room to improve for the very large circuits. If you have very large number of rows you can still improve that. But definitely there will be also some solutions you can think of to better paralyze the rest of parts which you involve inside the proof generation for the time management of your rest. Management is your gpu fully occupied? Actually, it's not fully occupied. Definitely. There are sometimes, like for example, I think inside aggregation circuit, sometimes the FFT will be waiting for the cpus because cpu takes more time to generate those permitted new columns for the lookup argument, so that sometimes GPU are waiting for that column to be generated before they can start working on the FFT.
03:10:34.634 - 03:10:35.630, Speaker B: And ifft.
03:10:39.420 - 03:11:00.380, Speaker D: Is there a way you can. I'm trying to formalize my question here. So your current gap limit is like l one, right. But since you have the aggregation circuit design, is there a way to just say like, okay, we're going to have a lot of box?
03:11:09.970 - 03:11:50.538, Speaker B: Yeah, we can definitely do that. Also, we are also planning to do that, working towards that. And also there's also plans to have different size of circuits so that you can have larger circuit, you handle blocks with more gas cost you have. And then for a smaller blocks you can have smaller circuits. I think we'll be still flexible. So we're trying to see if we have very fast blocks. And then you don't need to put out a very strict gas cost.
03:11:50.538 - 03:12:09.588, Speaker B: If you have more stuff like you have, you can just use larger circuits. Probably. There's definitely some limit of the maximum you have. Depends on circuits. There will be some limits there, but it will be quite flexible, I would think. Okay, Brian, in a world where you.
03:12:09.594 - 03:12:16.744, Speaker D: Have like a decentralized network of sequencers and reubers, have you thought at all about mev on L2?
03:12:16.942 - 03:12:33.840, Speaker B: Yeah, this is kind of an interesting topic. We're working with all of the researchers on the, working on the MeV, and I think there's also some talk at today's MeV workshop, talking specifically for the L2 meV. I think there'll be some interesting topics we're going to explore.
03:12:38.100 - 03:12:46.284, Speaker C: I did not know that witness generation was such a bottleneck for large circuits. I guess it makes sense, but could you break down which part of witness.
03:12:46.332 - 03:13:18.750, Speaker B: Generation is taking the for that? I don't have specific number for that. But actually we also noticed one thing is like if you're using this dual pass layouter, it has to add more time for the witness generation because you need to go past the circuit twice. So it takes more time. And also because you have the region stuff, you also need to at least go past like twice for the progenitor because you need to first generate all the regions and then to lay out that that could be like one issue that you need to go through.
03:13:19.680 - 03:13:34.140, Speaker C: Like you said, we should try and parallelize it. And one abstraction along which we could parallelize could be region, because each region is pretty self consuming.
03:13:34.220 - 03:14:08.648, Speaker B: Yeah, but sometimes it's not so naive to parallelize this thing because the wittiness generation could be depending on. So if it's step by step or row by row, the next row value could be depending on the previous rows results. So you can generate the value you want to put into the next row. So it's not very naive like a straightforward solution, a general solution at all. But maybe for certain circuits you can have ways so you can predict how many, what's the value you're going to do, or kind of like a trunk by trunk. You can do this within the trunk. You do the sequential things, but in parallel you can do the witness assignment for different trunks.
03:14:08.744 - 03:14:19.040, Speaker C: Yeah, I think this is an interesting problem in general, and maybe we could think of some best practices for which to write your weakness generation.
03:14:22.510 - 03:14:46.470, Speaker B: Yes, I think we haven't actually finalized all of the details of the tokenomic stuff, so I think, yeah, I just don't have a very concrete answer to that.
03:14:47.580 - 03:14:52.490, Speaker D: The decentralized network, are you planning on holding block time back so that.
03:14:58.270 - 03:15:24.446, Speaker B: Yeah, actually we're working like that to packing multiple blocks into a single EVM circuit so that you can, because sometimes what happens in the testnet is usually most blocks just have one transaction, you don't use all of the circuit space you have. So one way to improve is to pack multiple blocks inside a single circuit so that you can have even faster block time inside the layer.
03:15:24.478 - 03:15:33.270, Speaker D: Two. Are you planning on capitalizing on ability of gpus after merge?
03:15:37.210 - 03:15:58.810, Speaker B: We'll see. We'll see how it goes after the merge. Okay, so if there's no more questions, actually for the rest of the time we're just doing some q a not probably related to this. But if more general questions you have to me or Sandy or Mason.
03:16:08.730 - 03:16:09.046, Speaker A: How.
03:16:09.068 - 03:17:28.228, Speaker B: Do you expect the UX to be with commitment and finalization? Yeah, I think so. Like, so user like, I feel like it just, I think for small transactions it should be like probably you can directly act on top of the pre committed, maybe like if it trusted the sequencer. And then I think for the commitment usually happens probably in minutes to be from the pre committed to committed, and then for the finalization to block up to finalize probably, I think can be like the, we still haven't decided how much. That also depends on how much gas cost you need to pay for the finalization, for the proof. So this is kind of a trade off if you have longer finalized time. So that gas cost and the cost on the L2 will be cheaper because you can aggregate more across different transactions so that the final proof verification on the layer one will be cheaper and amortized across more transactions, but you have shorter ones, then you can probably, the transaction fee will be slightly higher. So it's kind of like a trick you can play with different things.
03:17:28.228 - 03:17:59.366, Speaker B: But I think probably like hour, an hour or two could be like a reasonable range. But still they haven't decided to finalize those details yet. Yeah, the gas price could change very, a lot. So definitely there are some prediction. There are two parts of the L2 cost. One probably is like the three parts. One part is the L2 operation cost.
03:17:59.366 - 03:18:36.360, Speaker B: We prove like you pay the electricity to generate those proofs and then the second part will be the data you want to roll up into the layer ones. And then the third part will be the finalized the validity proof verification on the layer ones. So I think so far before the proto dank sharding or dank sharding, the second part, which the data availability will cost more, takes more percent of the part inside your L2 transaction cost, but after the proto dank sharding, so you can think of probably that the cost of two rob data will basically be free. Then you can think of how to optimize the verification cost on the layer one.
03:18:40.010 - 03:18:47.670, Speaker D: Okay scenario, the bottom end becomes gas execution, right?
03:18:47.740 - 03:18:48.360, Speaker B: Yes.
03:18:49.210 - 03:19:02.780, Speaker D: And can you execute in single thread executions that you get after doing gas per second or like the million gas per second?
03:19:05.390 - 03:19:44.986, Speaker B: I think we did some benchmark, like definitely a few thousand transaction per second, but I think that's probably some ERC 20 transfer or like just tip transfer, like easy, can reach a few thousand transactions per second. But yeah, there are definitely some places you can improve for the GOSM because the GoSM is not designed to be very efficient to process like a very high transaction, like the high throughput. So there are definitely room to improve the goy sim to be handled even higher transaction per second, like the highest throughput to do that. But yeah, I think it will be pretty high like the, given the current situations. But definitely we can also improve on.
03:19:45.008 - 03:19:45.580, Speaker D: That.
03:20:04.350 - 03:20:34.200, Speaker B: Matter for the, how much gas. Yeah, yeah, I guess climate. Yeah, actually. So this is like something we consider also look at some arrogant or some other implementations, but it's not the top priority at the moment. So we're having a kit to that bottleneck yet. Bottleneck by the sequencer yet.
03:20:42.120 - 03:20:48.120, Speaker D: Are you referring to contracts specialized in any way or are they kind of existing projects?
03:20:48.700 - 03:20:49.352, Speaker B: For what?
03:20:49.406 - 03:20:52.380, Speaker D: Sorry are they forced for existing projects?
03:20:53.120 - 03:21:10.820, Speaker B: It will be permissionless for the bridge. Sorry, contracts. I think I just follow some existing standard model for the bridge.
03:21:16.400 - 03:21:18.430, Speaker D: If he had an adversary, what.
03:21:22.550 - 03:21:23.300, Speaker B: Create?
03:21:26.550 - 03:21:33.350, Speaker D: He has an idea what your gas prices look like in his system.
03:21:34.200 - 03:22:08.604, Speaker B: Yeah, I think the attacks, one attack we thought about is you can use a lot of extended code hash or code size. That's relatively cheap operations, but you need to load the whole contract into your bico table. That could be easily to eat up all of your circuit space. It could be like constrained by how many contracts you can load in the bicode table. And also you need to compute the code hash. Given all the bytecode, that could be like a one attack. So we're trying to think some way like we can maybe adjust the gas price or make it some optimization how to compute the code hash.
03:22:08.604 - 03:22:58.564, Speaker B: So kind of like way to improve that to avoid those attacks. Yeah, we try to keep as same as close as to the current Ethereum gas model so that we don't change a lot of developer experience from things. And then there could be also some extra security concern if we change the gas model. So if you don't attack the circuit, you can probably attack the sequencer like the side. Sorry, what was the second question? Oh, yeah, that's awesome. Okay, so if there are no more questions, we can end our workshop. And thank you very much to everyone who come and join our workshop.
03:22:58.564 - 03:23:00.250, Speaker B: Thank you so much.
