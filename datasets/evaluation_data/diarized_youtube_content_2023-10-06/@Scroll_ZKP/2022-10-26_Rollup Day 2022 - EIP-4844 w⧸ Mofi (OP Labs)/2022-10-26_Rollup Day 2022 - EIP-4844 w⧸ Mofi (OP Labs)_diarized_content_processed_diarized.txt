00:00:23.850 - 00:01:49.306, Speaker A: Let me start with a quick introduction. So I'm Mophie, also known as at InFi, a discord telegram. I work for Op Labs, which is the engineering arm of optimism, and basically here to tell you about the future of rollups. So what's this talk about? We're going to go over the concept of data availability modular blockchain to eIP four four four, of course, and how that fits into Denk sharding, and give you a little quick update on what the status of EIP four four four development is currently. So, data availability, this is kind of like the problem of having data in your network and making that data available to users in the sense that once you post that data, you should be able to trust that the data will exist and be around the network for some amount of time. This is actually super related to roll ups, because one of the main bottleneck of roll ups is the data that we post back to l one. And right now we use call data for that, which is expensive.
00:01:49.306 - 00:02:58.696, Speaker A: But if we can solve this problem, scale out the data we're posting back to l one really cheaply, then the scaling of roll ups will follow as well. So that availability is also related to the execution in the sense that this data that we're posting back to l one, for the perspective of a roll up, we consume that data to derive the chain, right? And so the execution outputs are also like a NLT concern. So a roll up in a nutshell is the data and the execution check. It doesn't matter what kind of roll up it is, whether it's optimistic or zk roll ups, you can pretty much break it down into these two key intrinsic properties. So for the data availability, like I mentioned, the data that you're posting back to l one, you need to be able to derive it. You need to be able to consume that data to derive the roll up chain. The execution check is the process of actually checking that the data that was posted matches what you expect for the roll up.
00:02:58.696 - 00:03:47.264, Speaker A: And so in the case of like a ZK roll up, it's just a validity proof. For an optimistic roll up, it's a fault proof or fraud proof. So for data availability, I guess you could sort of think about it like data forever available or data once available. Whereas in the former case, the data you're posting back to l one, it needs to be always there or easily reconstructed and trusted forever. And I'll basically tell you how we able to accomplish this with EIP four four. For data, what's available. This is like the minimum requirement for most roll ups whereby they have like a settlement period, at least for optimistic roll ups, whether it's like two weeks or whatever.
00:03:47.264 - 00:04:57.500, Speaker A: If the data is available for longer than that settlement period, then you basically can trust the rollout because you can always derive the state from the data. And this ties back to the execution check I mentioned. The point of that data is so that we can challenge any bad sequencer, which is the entity that's like building your roll up chain and posting it back to l one. So let's do a quick segue into modular blockchains. So Ethereum actually has become quite modular, particularly since the merge that's just happened. And why is this useful? Well, it lets us pretty much encapsulate feature sets and complexity into one thing that's easy to reason about, and then keep things completely separate so that we can scale one thing without possibly introducing complexity in another thing. So it's a way to deal with cycloclimatic complexity overall.
00:04:57.500 - 00:05:58.848, Speaker A: So let's go over the sort of future designs of scaling. In Ethereum today, we have like an l one, which is like a monolithic chain. And then sometime a couple of years ago, we had the beacon chain, which as you've noticed, it's quite a modular addition to the existing execution chain. So we have the EVM, right? And before we had the proof of work, which was used to secure all state transitions in the EVM. But then with the merge, we introduced the concept of a beacon node, and what that does is decouple the proof of work aspect of the execution and let that be handled by the beacon node. So all the beacon node needs to do is interact with the execution chain, the EVM, to determine which blocks needs to be proposed and built. So again, this is reiterating the point.
00:05:58.848 - 00:06:52.720, Speaker A: We introduced the proof of stake on top of the eVM, and this was. I say it's easy, but it took a lot of engineering years to do this. But it's easy to reason about, because all the stuff that we built on the proof of stake chain, the beacon chain, does not really affect the EVM that much. All the proof of stake is just proposing like blocks, right, and asking the EVM to build it. So it's an example of like a clean modular architecture. We can expand the EvM if we need to, we can scale it without affecting the proof of stake and vice versa. Okay, so how does this look like for an l two? That concept of modularity is also pretty useful because you have an l one execution engine, and in an l two roll up.
00:06:52.720 - 00:07:22.670, Speaker A: Optimism. Zk sync or whatever. It's basically just interacting with the l one chain to figure out how to derive the l two chain. Right. And then once it's done with that derivation, it just posts back all the transactions that occur on the l two chain back to l one. And so there's this cycle where transactions are consumed from l one. They get moved to the next state on l two, and then we post it back to l one.
00:07:22.670 - 00:08:01.770, Speaker A: So this is kind of like how it fits in l two s. Today, you have the proof of stake at the very top, the evm. And then l two s. They don't really need to interact with the proof of stake, right, because we've already made things modular enough that all an l two needs are the state that the EVM exposes. Right. So you have multiple l two s interacting with the same l one evm. And again, we can verify execution between the EVM and l two via either fault proof or the validity proof, the execution check that I mentioned.
00:08:01.770 - 00:09:14.050, Speaker A: Okay, so how does this look like in other blockchains, like Salana or avalanche? Well, they introduced the concept of a data layer. And what this layer does is it provides like a common interface for bytes data, if you want to call it that, that several different subchains could share. And it looks nice at first, because solving for data is pretty important for scalability. But the problem with this is that now that you have to break up execution into different subchains, and you lose some security when you do it that way. Ideally, we want to have the EVM and the data being handled by l one. Even though we do it modular, it doesn't matter. But if we can have our cake and eat it, then everyone will be smiling, right? So we sort of solved this problem of bundling data and execution on the same chain via EIP four from four.
00:09:14.050 - 00:10:00.972, Speaker A: It's also called proto dank sharding, because it was like being specked out by protolanda, also another guy from op labs and Dankard from the EF. So you combine them, you get protodank sharding. Not very creative, is it? So, eip four four, what does it entail? Well, we need a KCG ceremony. It's a hard requirement for EIP four four four to work. We need new BLS libraries to implement new KCG crypto cryptography. I'll explain what KCG is in a moment. We have a development devnet that's running implementing a prototype of EIP four four four.
00:10:00.972 - 00:11:04.976, Speaker A: And we also have a consensus specs. So this is actually like the first time. Again, maybe the merge is one of the time, but first time post merge, whereby an EIP really has, like, a dependency on consensus and vice versa. So with EIP four four four, this is sort of how the picture of a modular, ideal, modular blockchain could look like, right? We have the l one evm, which we all trust, and it's secure. We add a data layer, right? Some people dub it the byte space or blob space. And what this gives you is the security of the execution and the assurance that the data that's available on l one is expected and can be used to reconstruct the state. So l two s.
00:11:04.976 - 00:11:57.684, Speaker A: How would l two s use this? All l two s d to do is basically attach to the data layer whenever we're posting back l two outputs. Because if you remember, if we go back a couple of slides here, when a sequencer derives the chain from l one and then applies more transactions in l two, that data that's posted back to l one, it doesn't have to be like l one call data. It can be any l one call m data provider. So what we're doing here is replacing the call data that was in the evm with a new layer. And the idea is that this would make things so much cheaper for lt's. Okay. So the data that we're posting, we refer to it as a blob, right? And this is just think of it as, like, call data.
00:11:57.684 - 00:12:31.596, Speaker A: It doesn't really matter what it is. This blob kind of like, it goes to, like, a lifecycle that this slide kind of summarizes. So the l two transaction that a user posts. Well, a user has a user to interact with l two, like optimism. It generates a transaction at a certain period. An l two sequencer or a roll up operator will bundle up several of these transactions. And then we post these transactions to l one.
00:12:31.596 - 00:13:16.190, Speaker A: Right? Previously. Well, today we post those transactions as call data. The ideal is that we post these transactions as a blob. And the way we do this is by introducing a new type of transaction called a blob transaction, similar to the way we introduced the dynamic fee transaction for EAP 1559. There would also be a new blob transaction for 4844. This transaction, it looks like a regular e one transaction eVM transaction, but it also adds, like, additional data that lets you post l two batch data at the bundle, separate from the call data. And due to pricing mechanics, it can be really cheaply to do so.
00:13:16.190 - 00:14:09.852, Speaker A: Okay, so this blob data eventually ends up in the beacon chain. It's important to note that the EVM does not store blob data. The storage is actually being handled by the beacon chain. So going back here, the slide's kind of misleading, because these blobs are sort of like part of consensus, but it's easier to reason them as like a different data layer for various reasons that I'll get into. And then from the perspective of an l two, if we need to derive the chain, all we need to do is find a beacon client, retrieve the data that we just posted, and then we can just derive the chain that way. Right? So this is another diagram that sort of, like, shows the whole workflow. Basically, same thing you just said.
00:14:09.852 - 00:14:58.712, Speaker A: You have, like an l two sequencer, takes transactions, rolls them up into a roll up, post batch data into an l one. The l one takes those transactions as building a beacon block provides those transactions to the beacon chain. The beacon chain, it gets proposed that data gets gossiped throughout the network. And then an L two verify can take that data and derive the chain to get to the exact same state that we had. Okay, so going to some details about what a blob transaction is, like I mentioned, it's similar to an EIP 1559 transaction. Actually, the pricing mechanics is very similar. Like the way EIP 1559 floats with the base fee.
00:14:58.712 - 00:15:37.480, Speaker A: It's also similar here. Took some inspiration there. And the important thing to note is that the blobs, they're completely separate from the actual transaction body. And the way we're able to do this is that a blob transaction sort of like, has two variants, right? When it's in the mem pool, it contains all the blob data. But once it gets included into a block and it's part of the state tree, we sort of like, strip off that blob data and make it available to the beacon chain. Because like I said, the EVM does not store blob data. That storage happens in the beacon chain.
00:15:37.480 - 00:16:42.620, Speaker A: So it basically looks like an old dynamic fee transaction, but we add a couple of new fields. One thing we add is the blob kcgs. Kcgs in general, what they are, they're basically a commitment to a piece of data. So think of it as like mercury proofs, except that the proofs, the commitments, are very succinct. Kcgs are always 48 bytes in size, no matter how large the data is, which is pretty important property to have, because this blob KCG is going to be part of the beacon block, and we need to be able to easily distribute that data without having really large proofs. KCG commitments also let you prove the valuation of a single point. So say you have, like, the blob data, and you want to prove that blob data at a certain index has a different byte pattern.
00:16:42.620 - 00:17:22.750, Speaker A: Well, you can easily do that using blob kcgs as well. Again, it's very similar to the way, like, mercury proofs work. And one last thing. We added a new field to the transaction called the blob versioned hash. What this is is just a hash of the KCG commitments. The reason why we're using a hash is because it makes it easier for us to compatibility. Basically, if we decide to use a different commitment scheme, then we would just change the way the hash schema works and a different hash can be used there.
00:17:22.750 - 00:18:22.962, Speaker A: All right, and one last thing. So a blob is basically a set of 40, 98 field elements. And these field elements are basically just points on a BLS curve. In particular, for this EIP BLS 380, 112, which means the total size of a blob is 128. Single blob transaction, you can store 128 kb that's completely separate from call data, and it does not get added to the EPM, but it only exists in the beacon chain. One more thing worth pointing out is that since blob data is separate from call data, we need a way to price it. And the fee market structure we came up with is introducing a new type of gas called data gas, or blob gas.
00:18:22.962 - 00:19:08.882, Speaker A: But blob gas, data gas doesn't matter. And this data gas behaves similar to regular gas. And the semantics is very similar to 1559, in the sense that we have a fixed target for data gas for the entire block. And if we notice that we're exceeding that target, then the gas price fee, the data gas fees go up, and if we're below the target, it goes down. The entire point is just a control system to keep data gas at a certain level so that we don't overburden the network with, like, huge blobs. To accommodate this, we added a new field to the block header. In execution, we call it the excess data gas fee.
00:19:08.882 - 00:19:54.164, Speaker A: Honestly, this approach is something we also could have adopted for EIP 1559. But maybe in some future upgrade, we could also do the same thing. This field makes it easy to compute how much we are off of target for the data gas. And for various reasons, it's also very nice to implement, rather than the way EIP 1559 works with the base fee. So again, there is no blob content in the EVM. All the blobs are stored in the beacon chain. So this is kind of like how it looks like you wrap, like a regular transaction version.
00:19:54.164 - 00:21:04.244, Speaker A: Hash. That wrap data is the blobs, but that only exists in EVM while it's in the mempool, but it's not included in the beacon blob. Okay, so how does the beacon chain interact with this blob transactions or new header, whatever. The way we kind of like, have it set right now is to introduce a new execution, sorry, engine API. So the way the engine API works right now is you can propose execution blocks to the EVM, and then you can update for choice. What we're doing is adding a new function that lets you get the payload in addition to getting the blobs bundle. So what does it look like? So remember, if you go through the whole process of a validator proposal, right, it needs to make a request to the EVM to get the blocks that it built, and then it adds those blocks to the beacon chain.
00:21:04.244 - 00:22:03.836, Speaker A: Well, the headers to the beacon chain. So similarly, we need to make a request to the EVM, the execution, to get the sidecar or the blobs. Those blobs are packaged into this new data structure called a sidecar. A sidecar is just a collection of blobs, and for various reasons, we don't want to add that to the beacon, sorry, to the execution header, because it will bloat the header more than it already is. Okay, so how does a roll up take advantage of all of this? That's the entire point of this, right? So first of all, for Ezekiel rollup, and then bear in mind, this is a very simplified model of a ZPR roll up. All ezk roll up needs during its execution and the settlements in l one, it needs to figure out the data that it has containing all that state. It needs to be confident that the data is what it says it is.
00:22:03.836 - 00:23:25.648, Speaker A: And the way it does that is by querying the EVM, given a blob index, a KCG point proof, and some other ZK related proofs, get the actual version hash that's associated with our blob index, and then we can use that version hash and be confident that the blob data that we provided to the contract is correct and it's not being spoofed or anything of the sort. There's one tricky thing about with Zk roll ups is that they may be using a different commitment scheme other than kcgs. They may be using ipas or whatever, but there exists a morphism between any given ZK commitment scheme and a KCG commitment scheme. And amorphism is basically the proof of equivalence. So with the proof of equivalence, you can take the kcgs and do some crazy crypto magic. Honestly, I don't completely understand this part, but with that you can be confident that whatever commitment scheme you're using for your Zk roll up will be compatible with the KCG commitments that are in l one. And so if we go through here, we added a new opcode called the point evaluation recompile.
00:23:25.648 - 00:24:38.270, Speaker A: What that does is takes the version hash that we just retrieved, a point on the blob, what its value should be, and a proof, right? And the opcode will basically in the EVM check that the point that you requested matches the value that you expect. And this is a way to check during your validity proof that a user that's posting data for the proof is not forging blob data. This is all relying on the security of l one kg commitments. So again, this is going forward. Again, proof is the same as the Zk data. We use the pre compile, we can check like multiple points in our blobs, and then for every point that we need during the validity proof, we just use the pre compile, check that it evaluates the right value, and then go forward from there. Okay, so for interactive fraud proofs, how does it look like? It's quite different.
00:24:38.270 - 00:25:46.612, Speaker A: You have like a pre image oracle. So again, this is like with the perspective of a fault proof, whereby you do this bisection game, dispute game, whereby there's a contract and you have like a challenger, and someone disputes data. The challenger disputes data that was posted on l one. So it needs to interact with this contract to figure out whether the data, the dispute has any merit or not. And so here, the period image Oracle doesn't actually know what the blob data should be. All it has access to are the version hashes, which ergo are tied to the KCD commitments. So as a challenger, your job is to provide that blob data, right? And the free image Oracle can just check via l one that the blob data is correct, do the point evaluation pre compile similar to the ZK roll ups case, and trust that the blob data you provided is correct.
00:25:46.612 - 00:27:01.980, Speaker A: And then it can do the state transition itself to verify the proof and the update VM memory. This is in the case of what Vitalik was mentioning earlier, whereby you run your state transition inside of another virtual machine, whether it's mips, but you can kind of ignore that for the purposes of the interactive proof. The main point of this is that you're providing data to the pre image oracle. It knows that it can trust the data using l one, and then it can do the stage resistion from there. Okay, so dank sharding, EIP four four four is sort of like a precursor to dank sharding, hence the name proto dank sharding, in the sense that for full dank sharding, and to recap like dank sharding is the long term solution for Ethereum data availability and data scaling in Ethereum. But one thing we definitely need before we even implement this full dank sharding is the ability to post data back to the beacon chain. And so that's what EId four for four does via blob transactions.
00:27:01.980 - 00:27:54.248, Speaker A: By introducing blobs, we can build the necessary requirements for full data availability. Actually, for this EIP, data availability sampling is not quite implemented. We just make the data available so it's not quite as efficient as full dank sharding, but we'll get there. And the idea is that we introduce blob space of like one megabytes per second. Actually, that should be per slot, not per second, unless this s refers to the slot. So every slot we introduce 1 blob data is basically what roll ups would use to post all your data. So once we do this for full dank sharding, we don't need to touch ed EVM anymore.
00:27:54.248 - 00:28:56.672, Speaker A: Like, we're done. All the changes necessary for full dank sharding would occur in the beacon chain. So this is kind of like how it would look like with full dank sharding, you have like the EVM with multiple data shards, and l two s can just plug into any one of these data shards to get the data they need to derive the chain. Okay, so where are we at with the development of this? So we've been hard of work for almost a year now, just trying to get EIP four four into fruition. And that entails like, building prototypes, doing workshops like updating the specs, making optimizations, and building in devnets, as a matter of fact. So this all started super early this year at Denver proto Lambda, another OB engineer labs engineer was working on this with the EF. There was like an original prototype type.
00:28:56.672 - 00:29:46.956, Speaker A: And then this summer, we really started ramping up development on Eip four four four. And by Berlin, we had our first devnet. The devnet had, like, a very simplified fee model structure, but it sort of, like, demonstrated that vip four four is feasible and client team should definitely look at it. And now today at Devcon Pogata, we have a new Devnet coming up. This devnet will implement the full fee market spec and all the other consensus related changes we've made since then. So this is sort of like a summary of all the work that's gone into, like, eip four four. Implementation wise, we have new specs, execution specs.
00:29:46.956 - 00:30:30.440, Speaker A: We have a geth prototype, we have a prison prototype. Shout out to coinbase for actually helping us with this. This is a collaborative effort between op Labs and Coinbase to build clients that are compliant with EIp four four. We have new execution API specs, though that's kind of, like, stale right now and the KCG ceremony. So let me quickly go over the KCG ceremony. For EIP four four to work, we need what is called, like, a trusted setup. And what a trusted setup is, is basically a way for multiple participants to derive a value that is like a secret and can be used for KCG crypto.
00:30:30.440 - 00:31:43.140, Speaker A: The idea is that no one knows what the secret is as long as the trusted setup was executed correctly. There's a team in EF that's hard at work on the KCG ceremony, but hopefully, by the time we ship EIP four four four KCG ceremony is done and we can proceed from there. All right, so what's left to build first is like a proof of concept that a roll up can take advantage of EIP four four four. And observe that we are actually saving a lot of gas using blob transactions rather than using call data. So for optimism, in this case, it's actually kind of like easy, the bedrock architecture, which is like the next upgrade for optimism. It's a very modular architecture whereby all the steps to derive a chain are completely partitioned. So all we need for EIP four four is to replace where we get l one data, which is currently today, the call data, to just use the data availability layer.
00:31:43.140 - 00:33:07.248, Speaker A: And this is sort of like another block diagram that summarizes how that would look like, again, with the l two node, rather than interacting with l one. To get that call data needed to derive the chain, it interacts with the beacon node at the very top. So that sequence of data just gets fed into the roll up node, and then we can end up with the same state. So the devnet, we've been doing a lot of benchmarking for the devnet, making sure that the EIP four four four does not introduce any new DOS vectors or issues, and we've been getting pretty confident at that. There are still a couple of open issues regarding how do we sync blob data in the beacon chain that we're currently looking into. And of course, last thing is, once the case two ceremony is done, how do we integrate that into EIP four four four in the sense that how do execution clients, beacon chain clients take advantage of the ceremony output? All right, so what implementations do we have? Like I mentioned, we have like a ready prototype for GEF and prism for nethermind. They have interest in implementing EIP four four.
00:33:07.248 - 00:33:42.904, Speaker A: They have an issue open in the repo Teku. They also have a lot of interest in it. For Lighthouse, they've already begun work on a prototype. Still work in progress though. I think it was like at East Berlin. Some folks at Lighthouse, including like a death core dev, started working on a prototype and they're getting there. So for EIP 44, the best resource really to learn more about it is the website EIP 44.
00:33:42.904 - 00:34:44.746, Speaker A: Com. There's also a hack MD written by protolambda that summarizes all the different spec changes we've made over the years. Because right now it's kind of hard to follow EIP four four development because we have the execution layer specs, we have the consensus layer specs, and those are not always in sync. So this meta link in hack MD should really help you figure out where the current status of the specification is and yeah, that's it. Any questions? Yeah, sorry, could you repeat that's hello.
00:34:44.848 - 00:34:54.654, Speaker B: I know the plots get deleted after one month, but can you always prove the KCG commitments in an Ethereum transaction using that opcode? Past one month has been.
00:34:54.772 - 00:35:31.900, Speaker A: Yeah, so like I mentioned, even the opcode doesn't access the blobs. You have to feed it. The blobs and the KCG commitments via the version hash are in the EVM even way past the one month period. So you can always prove it. One possible idea that you could see in the future is that we could have someone create like a bitturn of all the blob transactions that are older than a month, and then you can still provide that to the EVM to verify that they are correct. Yeah, go ahead.
00:35:35.630 - 00:35:47.150, Speaker B: I'm curious how you see that after this proposal has been implemented passed. How does it affect other data availability solutions out there that are like no settlement, just da?
00:35:49.170 - 00:36:39.820, Speaker A: Yeah, I think it reduces their use cases by a lot, because prior to eip four four four, there was talks about roll ups, taking advantage of them and re implementing actually a lot of the KCG commitment stuff as a smart contract. But if we can enshrine that in l one, we don't really need all that. But they would still be useful, like in the bittorrent case whereby they would provide long term access to blob data and there could be some applications, maybe nfts with IPFS hashes or something that might find it useful to have a commitment in l one that they can always use to reference long term lab data. Thank you so much mophie. Give it up for Mophie. Well done.
