00:00:16.650 - 00:00:31.922, Speaker A: Good afternoon, everybody. Thank you, Patrick. The previous talk, it's very interesting. So I guess also thank you to scroll. Thank you to scroll. Thank you to those listed by scroll in the tweetstorm for helping organize. This day is fantastic.
00:00:31.922 - 00:01:18.774, Speaker A: Definitely very, very difficult to do with the long range. So few people originating not even just from this part of the world, but know Bogota specifically. Today we're going to be doing a little panel with quite an eclectic mix on the takes of people who work in and around the rollup space for a variety of different use cases. And I think people who have envisioned the problem and the solution very, very differently. So it's going to be interesting to see what goes on. As a small introduction, we have Oscar, who is the co founder of Empiric Network, who's building the data layer for L two s and zero knowledge roll ups. As a stark enthusiast, he's commenced that deployment on Cairo, but perhaps one day we'll see it moving to some others.
00:01:18.774 - 00:02:47.166, Speaker A: Next in we have Remco, who I, not until today, had the pleasure of meeting, but I do love your technical twitter takes the two PI is quite the blog. He's held a variety of roles, investment, research, and notably, and very recently was at Ox park as a research fellow and now is the head of blockchain at Worldcoin. Next is Brandon, who's the CTO of Manta network, but also now permeating to Poseidon Labs, where they're building a privacy preserving, optimistic roll up. So a little bit different with a permeated account model playing around with those utxo to get that privacy inside of the sort of walled scaled EVM areas. Also fundamentally, I think, a very enthusiastic type theorist and last and definitely not least is Joe Andrews, who by academic training is an engineer and is now the head of product and co founder of Aztec Network. So Aztec, I think, for many, many people, has been at the heart of solutions, on top of or around or at core Ethereum itself and the famous creators of plunk, their proving system that it itself or some modular part likely, and mostly that the permutation is used across an awful lot of solutions. Also, the coiners, if I'm not mistaken, of the Zkzk roll up term.
00:02:47.166 - 00:03:20.910, Speaker A: So those looking for privacy and scalability at the same time. So it'll be very refreshing to see the thought process that comes from that kind of angle. Okay, so the first questions and allowing me to be quiet, something laying the groundwork and I guess paying some homage to the name of the day, why roll ups? Why do you feel that blockchains and distributed ledgers in their form today require this mechanism for scaling and staying true to some underlying base layer?
00:03:22.930 - 00:04:08.060, Speaker B: First of you, I wrote some notes, but I think blockchains today are not really ready for kind of mass adoption. I think you can kind of see one successful NFT mint or a project launch kind of grinds ethereum kind of to a halt through really high gas fees. So we need a way to kind of scale that. And I think it comes down to everyone validating every transaction just doesn't scale. So roll ups, specifically ZK roll ups, let you kind of validate a proof of computation which scales a lot better than validating all the transactions so you don't have to trade off. Security for scalability if you use a roll up is kind of my take.
00:04:10.750 - 00:04:48.780, Speaker C: And even if you have lots of users, in a lot of cases, that does bring the chain to a halt sometimes. Also there's applications that you can't even design on l one, assuming the gas fees that you're going to have to pay anyway. So it also expands the space of possible applications and roll ups and a whole like l stack all the way up, lets you sort of expand your design space and can bring many more developers into the area. And to think about the kinds of apps that they can't even dream of writing right now on l one, for example.
00:04:49.390 - 00:05:44.090, Speaker D: Yeah, and specifically why roll ups and not potential other solutions to the scaling problem. The base layer we want very, very strong security guarantees on that forces you into a corner that really limits your throughput. On top of that, EVM and Ethereum has some design choices that they inherited that also prevent it from scaling very hard. So combined, for anything that requires real throughput, you need to have some solution. There's a number of trade offs you can make there, but generally, if you want good forms of decentralization and trustlessness and sort of an ability to hold the operator accountable, you need some form of data availability. And the only one that we currently know of that has security guarantees that are on par with the base layer is to put the data itself on the base layer. Hence roll ups.
00:05:45.390 - 00:05:46.940, Speaker E: I have nothing to add.
00:05:48.350 - 00:07:12.390, Speaker A: Okay. And without that, we've got some desire for just definitely generic scaling. And what I think is a well justified statement to say that blockchains are awful. No, they're really far from where they need to be if they're going to be this new backbone of civilization that we're all talking about. We have a kind of nice fog of prescience where you could say, who knows what's going to happen now we can only sit and define and guesstimate all the different types of applications that we can have, given these sets of parameters, these awful blockchains. So perhaps once we make them fast and safe, the things that will come, the host of that will be unknown and hopefully, I'm sure, very exciting if the builders can get a shift on. So with these shared security super scaling mechanisms, what are you most excited about? What behaviors, what applications, what even potential new styles of chains do you think that is going to be brought about? Not just in the way we perceive blockchain today, arguably a closed fence or a little bit of a walled paradigm, but really the next stage to cornerly quote, whatever 1 billion users or web3 to the masses, but things that perhaps people aren't quite envisioning.
00:07:14.170 - 00:07:15.526, Speaker B: Down at that end.
00:07:15.708 - 00:08:42.526, Speaker E: Yeah, I think one of the things that we are most excited about is, as we've already pointed out, there are things that change on roll ups beyond just cost and speed. And a big thing that we are really passionate about is cheap computation that you can now do inside of your smart contract. Our protocol, our whole project, is based on that premise, and I can speak a little bit to that. For an oracle, for someone who's bringing data on chain, having computation, cheap computation available is really a game changer, because now you can start really going into things that we call computational feeds, which are very different to just the normal price feeds that we see on the l one or elsewhere. Now, if we just zoom out and think about, what does that mean? We believe, because we have these abilities now, and because we can build these novel data products now, that will enable other people to build much smarter DeFi protocols. And I think you'll see that across, not just defi, you'll see it in gaming, you'll see it in many different areas where these new things that you can do, and computation is one of them, privacy is another, and so on, they will really change what we can offer to the world. And I think that's what we are really excited about, not to speak about the l three s and sort of app specific environments.
00:08:42.526 - 00:08:46.782, Speaker E: I think that will all be very interesting. Curious to hear your thoughts.
00:08:46.926 - 00:09:50.710, Speaker D: Yeah, I can chime in with how we use l two and roll up technology at worldcoin right now. So our goal is to bring a billion people into the web3 world, and we want to target people with, let's say, wallets that hold maybe $10 worth and still give them an experience that is meaningful for them, which doesn't work on Ethereum layer one with the gas fees involved there. And in fact, I would argue it doesn't even quite work with the L2s that are out there right now, which is why I'm pushing really hard for four eight four to get in so we can get good performance there too. We have been experimenting with our own l two one that is transfer specific. It's an optimistic roll up that uses BLS signature aggregation, which basically gets you the best of both worlds. Your call data is very compressed because call data is actually mostly transaction signatures, and now you only have one for a whole batch and you have no verification cost because it's optimistic. So with that we got transactions down to under 400 gas.
00:09:50.710 - 00:10:47.926, Speaker D: Some tricks we can do to get it even further that's exciting. Unfortunately, these really transfer specific chains are not as valuable to the users because the peer to pet transfer use case makes sense. If you have a very broad adoption to bootstrap something, you really need the wider web tree ecosystem with you and let people interact with all the protocols they know and love, like the uniswaps and the compounds and the nfts and you name it. That forced the L two space into EVM compatible chains, because that allowed all these projects to just quickly deploy and reach out to the user base in a now much cheaper environment. It'll be interesting to see how that evolves and if we're really going to cling to the EVM as tightly as we do there. But there is a second use case of l two s that I also want to highlight. One of the things we use is semaphore, which is a protocol to create anonymous sets on chain.
00:10:47.926 - 00:11:38.278, Speaker D: We use this for our privacy protecting proof of prison node right now to insert an identity you call a function in a smart contract that adds your public key to a Merkel tree and it costs a million gas. The reason it needs to be done on chain is to allow for concurrency multiple people to create the transaction at the same time going in in a random order. What we do to scale this better is we have a sequencer that just aligns a couple of these insertions up in a batch and then creates a zero knowledge proof that updates the Merkel tree in one go. This is basically a very primitive minimal roll up already. We create blocks and we zero knowledge proof these blocks and publish them on chain with just the minimal amount of information people need. Basically just the leave values to recover the tree if they needed to. And this is the area where we get into the app specific chains.
00:11:38.278 - 00:12:38.990, Speaker D: This is about as simple as it gets, but you can see this evolving. We can also do something on the claim end where we aggregate batches of claims and allow people to do more complex queries cheaply on chain about their identity. So yeah, I'm excited about where the L two space where whole ecosystems grow and come together and interact with each other work. Plus these use cases we have where we just want scaling for a particular thing. And I guess a third use case for l two s would be when there is value in having different kind of block aggregation mechanisms. You could, for example, think of batch auctions, batch auction markets, where you want to aggregate a whole bunch of transactions and settle them all at once. That has a number of advantages other than that you can find optimal, find more better optima than you would be able to create with sequential transactions.
00:12:38.990 - 00:12:45.470, Speaker D: But you also need this if you want to do things like have perfect privacy in complex transactions.
00:12:46.450 - 00:13:47.560, Speaker C: Yeah, I think you made a lot of great points. One thing certainly that you touched on that I liked was that Merkel tree like minimal l three, sort of. And I think that there's sort of two different independent ways you can go about building roll up type scaling solutions. You can have this kind of thing where you figure out what batching scheme you need for your protocol, and then you build an l three, or however, whatever layer you're on to scale that piece. One thing that, the kind of stuff that I'm working on and my team is working on is trying to not go in that direction, but go in the direction of enabling applications themselves to run with ZK as part of their code. So think like Zk apps, right? So the idea is that for an application where you want to do scaling, you just take, let's say you write some solidity code, you pass it through, you deploy it on l two instead of l one. It's now just cheaper over across the board.
00:13:47.560 - 00:14:41.014, Speaker C: Instead. If we look at the most expensive parts of what makes zcap applications, what's the most expensive part of them is stuff like the really intense cryptography needed for, let's say, verifying proofs on chain, or doing some sort of batch signatures, pairing algorithms, all this kind of stuff. And so what if you built a roll up that was specifically optimized for those kind of computations, right? Then you could have people building ZK apps on top of this roll up, where it's not an app specific roll up, it's sort of a ZK specific roll up. So any application that uses ZK technology in, it would get a speed boost in this special roll up. Right. And then you can build these sort of l three s on top where let's say you're doing something like private transfer, you want to do like millions of transactions at once. You use the l two to speed up any given verification, then use an l three to do batch verification for something like that.
00:14:41.052 - 00:14:41.398, Speaker B: Right.
00:14:41.484 - 00:14:55.658, Speaker C: So these are sort of independent directions you can go in when developing roll ups. And the kind of things that we're focused on is basically giving developers access to ZK in their applications, rather than just figuring out a scaling solution that's specific for their app.
00:14:55.744 - 00:15:37.350, Speaker B: Right. I'd agree with all of that. I think that the most exciting things are kind of thinking about applications that require private state, because that's not possible on most of the ones we have today. So things like consumer finance, ZK games, they all require a developer to think and have these ZK tools. So if you can build a layer which verifies those ZK proofs, I think you get to a really interesting endpoint where kind of applications become private. And that's kind of one of the things which is going to spearhead adoption in our view, at Aztec, it's not just scaling. Like we're kind of in a bear market right now.
00:15:37.350 - 00:15:49.450, Speaker B: Gas costs are kind of nothing and people aren't using Ethereum. So it's kind of the feature set of the roll ups. And privacy is one of those features that we believe quite strongly developers need for adoption.
00:15:50.770 - 00:16:48.574, Speaker A: I think some really excellent answers and also really excellent to, I think, see the takes that were more feature based and more fundamental to looking at different permutations, iterations of the technology, rather than which, of course would have been fine and is exciting. Oh, we're scaled now, now we can do visa or scaled now. And now we can do that was very interesting. From things that are bringing about certain elements of privacy. I'm sure I know from the way Aztec constructs its stack, they forecast a lot of requirements and necessity for privacy in a configurable way, which I think is super interesting. And seeing how that aligns well with scaling as it goes forward to open up new avenues for people to do other. Standardly saying the real world or the previous iteration, the web, two, whatever we all exist in now, and seeing that happen with scalability is very interesting.
00:16:48.574 - 00:17:45.950, Speaker A: Joint between Brandon and Remco. I really liked talking about different ways to find very, very low gas costs in the same way we have, whether something specific without necessarily being app specific, or ZK or Remco's open problem. I think it was from example two of doing optimistic, but with BLS signatures, presumably to stop playing that exhausting gas Tetris you must do when a g two is like a g two bn two five four is like 2 million gas, and the g one is like 15 gas, we've got to sort of part and parcel all these. I do wonder if you can still keep things like that zero knowledge by running a different aggregation scheme, like a two round schnor, and then throw in a KCg ten to mitigate the call data. I think something like that, maybe, but not for 400 gas, sort of around the 50. That's crazy numbers, I think, at this stage, but I like it. And first.
00:17:45.950 - 00:18:34.018, Speaker A: But was the last. Sorry, but was first Oscar speaking about what you can do with sort of app specific and new applications to create l three s, which I guess takes us to our next and very important question as we attempt to have even greater prescience, because it's as the funnel opens up into all of the things we're going to do, predicting the behaviors and the necessities forward such that we can design them in the correct way now is tiresome, I would say, one word. So, computational constraint. Now we're talking about l two s here with general purpose compute. We talk about l three s with sort of app specific l three s. So, general purpose compute about taking this next layer above an l one. Or you could also technically do the same with an l three and keep going up.
00:18:34.018 - 00:19:10.140, Speaker A: But do you feel that we need app specific whether l three app specific layers to run some back end for gaming engines, some extremely high through point of sales? And how do you think those are going to fall into the existing roll up narrative today? And do you think it is possible? I won't yet give my opinion. I'm not sure as a moderator I should be keep the bias to a minimum that we're just going to be able to do away with these and just keep scaling some levels of compute and eventually have 1 billion transactions per second.
00:19:12.030 - 00:19:54.614, Speaker B: I think the l three kind of conversation is a strange one for me personally, just because I think it's more an admission of the l two that their execution environment is missing features. So I think that at some point, there will be an l two that has the features. I'm very biased. I think that's aztec. But I think you should be able to have apps that can run on your l two that don't need to do kind of even more aggregation. Otherwise you've kind of maybe missed the kind of specification of your l two vm. Like you may have focused too much on evm equivalents as an example versus kind of privacy or some other features.
00:19:54.614 - 00:20:22.930, Speaker B: So I'm confident that as technology increases, you'll have kind of apps that do kind of run on an l two, but they shouldn't need to do too much more aggregation to get the scale that's required of that l two. And maybe if the execution environment has programmability, those apps can pretty much do everything on the l two that they need to. And calling them an l 3 may be just confusing.
00:20:24.150 - 00:21:36.246, Speaker C: I think one other thing is that when you have an l two, and of course this is way too early, but when you build l three s on top of the l two, you have. Again, like everything else, you have sort of this ecosystem lock, right? Especially with the partitioning of all of the value that's in the blockchain, you're spreading it out amongst all these l two s then, oh, even worse, you had to spread them all around all these l three s, right? So you have sort of this ecosystem problem where if you're building an l two that's going to do something specific, that's not just going to be evm compatible across the board, you sort of have to have the right ecosystem for it. And I think something that would be really interesting to see is something like gaming companies where a gaming company, let's say, like, what's the. I forget what. Unreal, right? Let's think like the unreal engine, right? Where the company that runs Unreal engine would build an l two, and then games that run on Unreal would be l three s on this l two. And maybe you have, let's say some game where you're playing a round of the game, and once the round is over, you don't care about that state anymore, right? So these l three s, you sort of spawn them freely. You run your game on the l three, then you just commit the final state to l two, and you just forget about the l three again, right? So you sort of want to be able to spawn up.
00:21:36.246 - 00:22:35.210, Speaker C: You want to be able to spawn up random amounts of high intensity compute and then pull it back depending on the sort of demand needed for that application, for example. Whereas l two s can sort of give you this all around scaling in certain instances where you have very high demand. Or you have this ecosystem lock where you might want to build l three s for that purpose, right? Because you have all these mechanisms like Zkp to be able to build l two s that can have arbitrary compute, right? They don't have to run evm, they can be anything. So that sort of, I think leaves the space a little more open. And like you said, aztecs focus on privacy. And so maybe you're making different choices in your vm that are more privacy biased and in certain other instances that might not be the right thing. And so I see l three s more as the place where l two s builder ecosystem, rather than some extra level of sharding.
00:22:35.210 - 00:22:39.840, Speaker C: We have to just deal with more sharding, right? So I think it's more about an ecosystem in that case.
00:22:40.690 - 00:23:15.094, Speaker D: Yeah, I really like the game example. I would say that currently we have these evm compatible l two s. They are inherently limited in the amount of scale that you can provide because EVM itself is inherently limited. Like if EVM could do more, we could also do more on the base layer. The reason we get away with it in the l two s is because we compromise on a couple of things. We mostly compromise on the consensus decentralization, which is kind of fine because by nature of l two s, it's mostly censorship. You're not too worried about consistency things there, because that is guaranteed by the base layer.
00:23:15.094 - 00:24:08.006, Speaker D: But still, we have the Evm there. So we are going to hit a scaling limit with the l two s. Four eight four is going to solve the data availability cost of it, which will give us probably another ten or 100 x more room before we hit that limit. But we will definitely hit that limit with the various l two s out there way before every single in game transaction is on chain. So that necessitates ltrees and the existence of alternative solutions. Now, the l three s for the very same reason, if they are all roll ups, they will not get any more scaling than the l two s get because you still need to accumulate this data at the same place. So what you do there is you compromise even more on the security guarantees, which you can probably get away with, because in an ltree you're working in a very specific, application specific context.
00:24:08.006 - 00:24:54.060, Speaker D: So you can provide, you have usually some level of trust, like in games especially, you would trust the server that you're currently gaming on. So then you can probably move away from data availability and not be technically a roll up, but just be an l three of a different kind. And again, the thing I mentioned earlier where you want to experiment with new mechanics, new ways of new transition rules for your block that are not easily implementable in EVM, things that aggregate over many transactions, things that implement very game specific logic. For example, you need an experimentation space for that. Probably want something that can verify, something complex cheaply. So an l three, there would also be a very natural answer.
00:24:54.910 - 00:25:32.600, Speaker E: Yeah, it's interesting because we're obviously implementing on Kyra right now, so not EVM compatible equivalent. So it's a bit different, but still there. We talk about l three s all the time, and the thing that I think we're excited about there in that ecosystem is that you can do things like privacy is not something that stocknet sort of gives you built in, but you can have that with an l three if you want to build, like a reperformed order book. Right. That's something that you might want to do on an l three. If you have a very specific NFT protocol that you'd want to build. Maybe an l three is a better solution for that for us, certainly, if we want to.
00:25:32.600 - 00:26:03.898, Speaker E: I mean, currently what we're doing is we do kind of simpler things. We construct a yield curve that we compute on chain. We construct sort of market volatility oracles on chain. Those are relatively straightforward computations. You can imagine that we could do very complex things that you couldn't do on the l two. And I think that's what we're excited about. And there's a bunch of great, I think, great teams like slush and so on, thinking about this kind of stuff on Stocknet and elsewhere.
00:26:03.898 - 00:26:10.114, Speaker E: And so we have an interesting different view to you guys because we're sort of trying to build on that.
00:26:10.152 - 00:26:10.498, Speaker A: Right.
00:26:10.584 - 00:26:17.540, Speaker E: And so we're really excited about all the different possibilities that we do see for all the different l two s.
00:26:17.910 - 00:27:16.450, Speaker B: Yeah, I think I'd just kind of maybe count. Well, I think there's a trade off between the kind of lock in requirements that you were saying as you go further down this layer stack to just building a kind of more specific l two that kind of doesn't have those trade offs, because you're basically kind of trading off the ability for a user to get out of the system to somewhere else for kind of being on a particular l two, which has got an incorrect feature set for your actual application that you're trying to build. So I lean more towards seeing, I guess, app specific roll ups, but I think that's in that I can see Starquare's value proposition is like immense throughput, and they've sacrificed EVM compatibility for that. And I think that's a fair trade off to make because it enables a lot of public applications that maybe don't need some of the things you have on Aztec.
00:27:16.610 - 00:27:29.334, Speaker C: I think that's what's really exciting about this whole space, is that people are willing to experiment with all these completely different trade off spaces, and so people aren't just like focused on one thing. So I think it's really exciting.
00:27:29.382 - 00:28:08.806, Speaker A: Either way, excellent answers. Do not have time to summarize. We can hasten to the last question, and Targaryl doesn't start throwing things at us, pacing around the room. I just want to quickly talk. It's been touched on, I think, very, very nicely about, and I shall keep this test we touched on very nicely on privacy. And this feels like the old flame in the ZK roll up from whatever it was in the 60s from gold Vasar et al. Creating these computations, or this kind of concealed computation as part of an arithmetic circuit to seeing blockchains have a kind of serious use case.
00:28:08.806 - 00:29:04.086, Speaker A: When I first had the fortune of jumping on the zero knowledge cryptography train a few years ago, it was witness encryption and it was private encryption. Yeah, I'm coming. It was private encryption for our testing to some certificates and browsers back with God bulletproofs. And since then, it turns out that little arithmeticization, that little witness preserving computation of some program, is a hell of a lot smaller and a hell of a lot cheaper to verify on chain than you would, or we don't necessarily have the necessity to do so. So we don't have mega servers anymore. So to those, and I guess the two panelists on the left, very relevant to their need and desire for privacy in the web3 ecosystem. As we go forwards, do you see that there's any clash between providing privacy and scalability? Not necessarily just for now, because I think it's more arguably, yes, encoding.
00:29:04.086 - 00:30:00.730, Speaker A: And encoding is doubly expensive. You're doing Mercury's and Mercury's like mobile phones, the way you guys do all the extended Utxo with the old spending model, but for sort of the very much long term. Do you see that you have to make a trade off for the two of these? Or is it that privacy preserving applications or privacy preserving behaviors work better, or tessellate even into well scaled platforms? It would be difficult. And then the last quick sub question there, because we talk about this a lot, and it's not brought up enough on Twitter and whatever other outlets that do you think some of the designs and some of the protocols creating and envisioning the roll ups for today, just ZK roll ups of the validium style, non private will have issues retroactively adding privacy.
00:30:01.070 - 00:30:09.070, Speaker D: I think you'll find that the answer to both is the same. It is you need to get rid of global mutable state that allows scaling, that allows privacy.
00:30:10.050 - 00:30:57.200, Speaker B: I think it's very hard to retrofit privacy as a sub question. You kind of always need a Utxo model, and having kind of an account based model makes privacy very hard. You need privacy to be by default, else you just have privacy being used by people who have something to hide, which is kind of not the norm. I think the start of the question in terms of scalability, I think we can compete with Visa Mastercard costs with the current design space. In terms of transactional costs, I think the elephant in the room for ZK privacy roll ups is client side like the current model is download and try and decrypt every single transaction which doesn't scale. So that's like the big elephant in the room, which I think there's not enough people talking about.
00:30:57.570 - 00:31:51.534, Speaker C: Yeah, I think just to end it here, because I think toggle is getting nervous. Yeah, I think there's definitely some tradeoffs in the near term, but I think in the long term, as you see, one trend I think that we're going to start seeing is that optimistic roll ups will be able to, whatever customization they're allowed to make in their vms, you'll be able to find a ZK replacement for it. And so the thing, the sort of design we're trying to get to, to try to get privacy in there faster is do an optimistic roll up that has privacy as a feature app developers can use. And then as ZK technology starts getting better on the scaling side, not on the privacy side, we'll be able to transition into ZK roll up. And so maybe this is the opposite direction of what Aztec does, but I think we're starting as an optimistic ZK and then going into ZkzK in the future. And I think it's going to be very clear that you can replace all optimistic systems with ZK. Maybe it takes ten years, but maybe it takes 20, but definitely going to happen.
00:31:51.534 - 00:31:53.440, Speaker C: I don't see any reason why it wouldn't happen.
00:31:55.650 - 00:32:09.250, Speaker A: Yeah, I guess we're at an end. Yeah. How serendipitous that ZK privacy brought us ZK scalability. Thank you everybody for listening and to the panelists for taking your time. And once again, everybody organizing, it's an absolute pleasure.
