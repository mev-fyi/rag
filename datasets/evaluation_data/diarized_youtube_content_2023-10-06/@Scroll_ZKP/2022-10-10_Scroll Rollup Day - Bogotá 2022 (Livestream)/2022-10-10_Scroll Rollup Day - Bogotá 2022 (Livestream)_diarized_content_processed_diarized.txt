00:04:36.830 - 00:06:42.660, Speaker A: Good to hear. Hello, everyone. Your attention, please. We will be starting in seven minutes, so please prepare and make your way into the main hall. Thank you. Yeah. One.
00:06:42.660 - 00:10:29.830, Speaker A: 2121-212-1212 but in a gap on the little throttle. 1212-3123-1231-2312-3123-123 you didn't know the one, like two minutes, but you didn't want. In two minutes, but it works. Okay. So in one win in 1 minute, close it to be to you, to me to eat. Hello. Hello, everyone.
00:10:29.830 - 00:12:08.950, Speaker A: Please make your way to the main hall. We are starting very soon again. Please make your way to the main hall and take seats now. Thank you. You. Um. And if we.
00:12:08.950 - 00:15:28.090, Speaker A: Hello? Yeah, but yeah, I'll be joining. Guys, I need to. Everybody around. Yeah, I'm also happy to join. Now I need to add another panel. Oh, really? Yeah. So.
00:15:28.090 - 00:16:01.456, Speaker A: No, yeah. David. So, yeah, I think for my. I know. I think a lot of people are sort of like, in a football, so it's more. I mean, it's very concrete. We wanted to avoid, you know, that effect, but no one shows up for the side.
00:16:01.456 - 00:18:07.690, Speaker A: Yeah. It was tailored to, like, research people liked. Okay. Hi, everyone. So we're. Oh, wow. That was very wild, wasn't it? So we're going to get started now.
00:18:07.690 - 00:18:47.988, Speaker A: Let me start with a personal story up front. And if we may have your attention, please. I'd like to start with a personal story because myself, I entered the space of roll ups quite recently, only to find that roll ups are so confusing. And I thought, well, I'm a newbie. Roll ups are quite specific, quite a niche topic. And so I figured, okay, it's going to come with time. And it was only then, after many talks, discussions and events that I found out more and more that roll ups are confusing.
00:18:47.988 - 00:19:18.716, Speaker A: Full stop. It's not just me. And so today we would like to start working even more actively towards making roll ups more comprehensible. And because of that, today we are starting the roll up day with espresso systems and scroll. And we would like to welcome everyone. Today there will be two tracks. You have the program on Rollupday XYZ.
00:19:18.716 - 00:19:44.600, Speaker A: Feel free to move between the two floors as you like. This room and this floor is going to be recorded and live streamed. The first talk of today is going to be one on validating bridges, brought to you by. Patrick. Please give your applause to Patrick. Cool. Awesome.
00:19:44.600 - 00:20:02.904, Speaker A: So welcome, everyone. The bogada. I hope you're all having a wonderful time. I see very little crypto swag, so you've all taken the advice to no dar papea. So awesome. So this talk is more framed towards an introduction. So, as he mentioned, roll ups can be very confusing.
00:20:02.904 - 00:20:58.160, Speaker A: And so I want us just to step back and try to understand what are roll ups trying to solve? What are security properties and why should we care about them? Like, why are they interesting and really under the hood? For roll ups, it's all about bridges. It's just bridge engineering. And really, for the past ten years, this is how we've scaled cryptocurrencies. You have your funds on Ethereum, you lock them into some bridge, and then they magically appear on an off chain system like Coinbase finance, FTX, whatever. And then when you want to withdraw your funds, you typically ping Coinbase to say, coinbase, let me withdraw my funds. And then if you manage to get past customer support, they will send a transaction to the bridge to say, alice can withdraw her 1000 ETH. And the most important bit is, from the bridge's perspective, there's a deposit function, withdrawal function.
00:20:58.160 - 00:21:36.376, Speaker A: And when the bridge gives this message from Coinbase, the bridge has all the assets. And for Coinbase, I think it's like 10% of all crypto assets are under their custody. So there's a lot of assets that are protected by this bridge. And the bridge has to be convinced that the user is entitled to 1000 ETH before it actually sends off the funds. And so in this case, it will trust Coinbase to tell it the truth and to protect the off chain database. And then Alice gets her 1000 ETH. But generically speaking, what's happening under the hood here? Generically speaking, there's an off chain database.
00:21:36.376 - 00:22:14.248, Speaker A: It has account balances, program code, program state. But really this database is recording the liabilities of the system. So if I have like ten coins in arbitrum, really the arbitram database is saying that has ten coins that is liable to me and it should eventually give me back. And the bridge has the assets, and the whole point of roll ups is to make sure that the assets can cover the liabilities. And the bridge is convinced that this off chain database is okay, it's well, and it's alive. So that's the ultimate goal for all of the roll ups. But the trust assumption for these bridges have evolved over time.
00:22:14.248 - 00:22:49.552, Speaker A: So as I mentioned, we have the single authority, the coinbase and the bitstumps. Then we have these multi authority bridges, we have the RSK, we do have liquid by blockstream, but no one really uses it. So it's actually really removed. That example. And then we have these crypto economic bridges as well. But with the crypto economic bridge, the difference is that if I want to be a validator, I have to lock up my funds, have skin in the game, then I appoint myself to protect the bridge on behalf of other people. But at the end of the day, there's still multi authority bridges.
00:22:49.552 - 00:23:27.056, Speaker A: So by the end of 2021, around six authorities controlled, I think it was like 85% of all the modic staked in the system. And so we're really trusting these eight entities to protect the assets held by the bridge. Now, in all of these examples, we're trusting less than ten people to protect our funds. And this sucks, right? The whole point of something like bitcoin was to remove the intermediaries, and now we've just happened to reintroduce them. But it doesn't suck from an ideological perspective. It sucks, practically speaking, because they keep getting hacked. Who's read in the past two weeks about a new bridge hack? Look at this.
00:23:27.056 - 00:23:59.080, Speaker A: Look at all these bridges getting hacked. Did anyone lose any funds? I'm only joking. But when they get hacked, it tends to be large scales. So the biggest hack was McGox. They lost 850,000 bitcoin, which was 6% of all bitcoin that will ever exist today. They just sort of swap it under the counter and pretend it never happened. But basically, it sucks practically, because we have a single authority who has to take a set of human processes, apply the human processes, protect the key, and try to protect billions of dollars.
00:23:59.080 - 00:24:18.748, Speaker A: And clearly that doesn't scale. Humans don't scale. What about multi authority bridges? Well, they're a little better. They get hacked a little less, but they still get hacked. So the run on bridge was a good example. Five out of nine validators get compromised. Under the hood, it was really a single authority bridge because one person controlled five validators.
00:24:18.748 - 00:25:02.510, Speaker A: But we like to have some security there. We've forgot about this old school motto, not your keys, not your coins. We're building these systems where we lock our funds into the bridge, and then we give custody to the off chain system, and we trust the off chain system to protect our funds. So can we do better than this? Can we transact on an off chain system while still allowing users to maintain self custody of their funds? That's sort of the goal for these bridges. And this was the original goal in the side chain paper from 2014. They don't use the word bridge, but they're trying to build a trustless bridge with what they call a two way, Peg. And the idea was fairly simple.
00:25:02.510 - 00:25:47.470, Speaker A: So on a blockchain like Ethereum, you would have a bridge where you could lock your funds into this other layer, one blockchain. Then you would transact there, and eventually you could bring your funds back to Ethereum, or in this case, back to bitcoin. And the important bit is that the bridge on Ethereum is trying to check whether this consensus protocol on this other chain has authorized you to withdraw your funds. So what I mean by that. So if this was Ethereum to bitcoin, Ethereum would be checking the blockheaders in bitcoin and checking the proof of work. If the proof of work passes, then you can get your funds out of the bridge. But the problem here is that I call this a consensus bridge, because ultimately, you're relying on the consensus protocol of the other system.
00:25:47.470 - 00:26:28.120, Speaker A: And if there's a consensus protocol, then you're trusting the judgment of a set of parties. And so we haven't got rid of the parties. And so what's trusted here, so one is if the validators all agree to an invalid transaction, then that could get passed by the bridge. Now, maybe we could use iron all these proofs, and if you go upstairs, you'll learn more about that today. But the ultimate problem with these consensus bridge is liveness. If the other blockchain goes offline, for whatever reason, your funds will get stuck in the bridge on Ethereum. So if bitcoin goes offline, which hopefully it never does, your funds will get stuck in the bridge because you never get it back from bitcoin.
00:26:28.120 - 00:26:58.964, Speaker A: And that's annoying. We should be able to still survive even if the entire off chain system goes offline. So, again, the question, can we really build a bridge that protects us from an all powerful adversary? And this all began with plasma. So plasma in 2017 was the start of this journey for roll ups. There was this paper called plasma. Has anyone here tried to read the paper? No one's tried to read? Okay, one person. There we go.
00:26:58.964 - 00:27:43.184, Speaker A: Three people. Did anyone understand the paper? That's a joke, because it's a really difficult to read paper and asked Joseph poon about it, and he called it aspirational because it was so difficult to read. But plasma was a great idea because it tried to solve that liveness problem, where even if the system goes offline and the operators are fully malicious, you could still eventually get your funds out of the system. But this led to two years of research, plasma, MVP plasma, cash, and all these different schemes. But it wasn't a wasted two years, but nothing viable came out of it. But then Barry came along, Barry Whitehat, and he simplified the design space. Basically he said that know, trying to protect the data and making sure the data is publicly available.
00:27:43.184 - 00:28:25.340, Speaker A: And he came up with this thing called the roll up, where you take the data and you just post it to Ethereum alongside a proof that is valid. And that's where roll ups came from. Ultimately, and I like to call them validating bridges, we say with the whole idea of bridges, what we're really building under the hood is a validating bridge. So the idea is that you still have this off chain database. Someone will come along and propose an update for the database and they'll send that to the bridge. Now, the difference is that the bridge will not blindly accept a random update from someone on the Internet. The bridge must be convinced that this database update is correct and valid.
00:28:25.340 - 00:28:50.490, Speaker A: And then if it's convinced that the update is valid, it will then process any withdrawals that are needed. And then you send the convincing evidence and the bridge will be very happy to say, yep. Is the database safe and alive? Yes, it is. I'll accept the proposed update. And this is what the roll up teams are trying to build. The loop ring. ZK Singh, Garbitrim, Starkware, they're all trying to build a validating bridge in their own way.
00:28:50.490 - 00:29:27.424, Speaker A: And ultimately, if they can achieve this goal, then you can deploy software to protect the funds locked into an off chain system, and something like ethereum can protect you and your funds. This sounds really cool. It's really exciting. Clearly you're all here, so you're excited for roll apps, but this also sounds a bit too good to be true. So how do they work under the hood? So I'm going to cover who are the agents? Who's involved in the protocol? A high level overview of how a validating bridge may work. Then the threat model and some security properties of what it means to actually be secure in this context. So here are the agents.
00:29:27.424 - 00:30:03.400, Speaker A: The first one's an honest user, Kubi Alice. All she wants to do is go on the off chain system and buy and sell her mooncat. She doesn't really care about the system itself, she just wants to be a user. We have the sequencer, and the sequencer's job is to take the off chain transactions, order them, and then pass them on to be executed. Then we have the executor who comes along, he takes the transactions, he'll execute them and then propose an update to the bridge. So let's have a high level overview of how they work together. So Alice is here, and Alice will deposit one coin into the bridge.
00:30:03.400 - 00:30:28.580, Speaker A: And once the bridge gets the deposit, it will show up on that off chain database. Then Bob comes know, hi, Bob. And Alice wants to send one coin to Bob. So Alice will sign a transaction and give it to the sequencer. Now, the sequencer could ping Bob to say, bob, you're going to receive one coin. But right now, that transaction is not public. It's not even pending.
00:30:28.580 - 00:31:00.240, Speaker A: It's just held by the sequencer. The sequencer waits around, I know, collects these off chain transactions, and eventually the sequencer will take the order transactions and pass it on. In this example, the sequencer will give it directly to the bridge. The bridge will take the list of transactions, and it will finalize their ordering. And now they're ordered, but they're not yet executed. So the executor comes along. He'll pick up the transactions from the bridge with his 1990 animation.
00:31:00.240 - 00:31:40.620, Speaker A: He'll execute the transaction, and then he'll post a little checkpoint to say, bridge. Here is the proposed update, alongside evidence that this is valid. And this happens continuously. We order transactions, we execute transactions, and then we convince the bridge that the execution was correct. And so the bridge is continuously convinced that this off chain database is valid. And, well, and that's the idea behind a high level overview of how a validating bridge works. But what's the adversarial model? Who are we trying to fight? Who are we protecting ourselves against? So there's really two things we have to be concerned with.
00:31:40.620 - 00:32:07.732, Speaker A: One, message flow control. So the adversary can view, reorder, and drop every message on the layer two system. The only guarantee we have is that Alice can send a message to the bridge. On Ethereum. That's the only guarantee we have for message delivery. Two, we assume that nearly every single party is corrupted by the adversary. We can only assume there's one honest party and the bridge contract itself.
00:32:07.732 - 00:32:46.608, Speaker A: And that honest party could be the user themselves. And that's the only two we have to be concerned with. But this is actually the most powerful adversary that you can think of. And if you visit l two beats, you'll notice that no roll up today can actually satisfy this beast or fully constrain the beast, because it's a very difficult adversary to defeat. But they're all working towards beating this beast. But what about the security properties? What does it mean to be secure? So the ultimate goal is that there's an off chain database that records the account balances. Program states, smart contracts, and the goal is to protect its safety.
00:32:46.608 - 00:33:25.120, Speaker A: So every update applied to the database is correct and valid and its liveness such that if the entire system is offline, a user can eventually get their transaction executed. And there's three sub goals. One, data availability. Are all the updates to the database publicly available? From the bridge's perspective, the bridge has to be convinced. Can anyone get a copy of the database? Two, the bridge has to be convinced that every update was valid. And three, it's up to the bridge to forcefully include and execute transactions. It's the bridge that enforces censorship resistance.
00:33:25.120 - 00:34:20.832, Speaker A: So let's talk about the data availability problem. What does this mean? So we're going to talk about why does the data need to be public, what data needs to be public, and how do we guarantee it's publicly available? So why does it need to be public? So as I mentioned, we assume there's one honest party who can assist the bridge with protecting the system. This one honest party needs to have a copy of the database. If you don't have a copy of the database, well, you can't propose an update for it. So the reason why the data has to be public is so one honest party can get the database and then eventually propose an update to the breach. What data needs to be public? This differs based on if it's an optimistic or a ZK roll up. But generally speaking, you can either post the transaction history, so you get the list of transactions, the transaction data, and you send that directly to the bridge, or you can get a state diff.
00:34:20.832 - 00:35:00.044, Speaker A: So you have the old database, the new database, you compute the state diff, what stories values are different and you post a state diff to the bridge. And there's pros or cons of both approach, but it's cool. There's two different types of data that can be published. As long as you can recompute the database, the bridge doesn't care and as long as it's valid, of course. And three, how do we guarantee this data is publicly available? One, the first approach was the on chain data availability challenge. That was plasma. Everyone's sort of given up in this approach because it was too hard, but I think nowadays you could probably find a viable solution for it.
00:35:00.044 - 00:35:36.730, Speaker A: But right now everyone's given up on it. The second approach is a committee. This is like any trust or the starknet committee, I forget what they call it, where you have ten or 20 entities that attest to the fact that the data will be available. So law send their signatures to the bridge to say bridge I promise to make the data available. And bingo. The third approach is the roll up, where we just take all the data, basically the update to the database, and we post that directly to the blockchain. So we rely on something like ethereum that guarantee the data is publicly available.
00:35:36.730 - 00:36:12.004, Speaker A: So what about the state transition integrity problem? Now, this is very straightforward. We want to make sure that every update posted to the bridge is correct and valid. If one invalid transaction can be included, then the adversary can steal all the funds from the system and bingo. This is the optimistic versus the ZK, the fraud proof versus the validity proof. I won't go into this today because it's a short talk, but this is like when you hear about the optimistics or the ZK roll ups. They're mostly just solving this one problem. And if they can solve this problem, it's great, but it doesn't mean they're a roll up.
00:36:12.004 - 00:36:56.180, Speaker A: The difference between a side chain and a roll up is really censorship resistance. It's the ability to allow any user to forcefully execute their transaction if the entire system goes offline. So what is censorship resistance? Basically, I send my transaction to the sequencer and they reject it and say, no, I'm not going to include this and execute it for you. So the way we solve this is two components. One, the bridge must be able to accept transactions directly from the user. So the user could send the transaction directly to the bridge, and the bridge will order this for execution. So the bridge has this power, but now that the transactions are ordered, they have to be executed.
00:36:56.180 - 00:37:36.176, Speaker A: So this brings us to the sequencer and the executor. The sequencer has absolutely nothing to do with censorship resistance whatsoever. It could be fully centralized, and the system is still censorship resistant. What really matters is that there's one honest executor out there who will pick up the transactions from the bridge and then execute the transactions. So, to summarize, if we can solve all three problems, maybe we can slay the beast and deploy a secure layer two system. And by the way, this picture is from the bear market in 2014. I replaced the bitcoin shield with the Ethereum shield, because clearly EF is money now.
00:37:36.176 - 00:38:17.304, Speaker A: So anyway, there's other problems that also emerge, but I don't have a lot of time to go into it. But I wanted to show the slide. A couple of things I wanted to highlight was one, oh, by the way, these are resource problems that are really interesting to solve, because while we have the three core problems, these problems surround it and also really do also need to be solved. So obviously MeV is a big issue for the sequencer. Should we have these auctions or some fair ordering protocol? Center chip resistance is nontrivial. Center chip resistance is the hardest thing to build, and clearly not many Perula projects are actually trying to build it. And of course, experimental virtual machines.
00:38:17.304 - 00:39:00.344, Speaker A: We have the Cairo, we have the optimism virtual machine, the AVM, the arbitrary virtual machine. We can actually deploy a crazy virtual machine like WASM, execute the transactions here, but still get the security of Ethereum. So these roll apps allow us to experiment with crazy designs while still retaining Ethereum security. So that's really exciting. Anyway, regardless, is it worth it? Is it worth going through all this bother of trying to build a validating bridge that can protect our funds? Why not just trust Coinbase? Coinbase hasn't been hacked yet. I guess finance did. So that wasn't very good, but it was very unfortunate for them.
00:39:00.344 - 00:39:31.120, Speaker A: They got a bug in their Merkel tree. But anyway, is it worth it? I believe it is worth it. So for me, web three is this buzword that gets thrown around. But my take on it is that in the web two world was where we really have been for the past ten years. We do have the coinbases, the finances and the bitstamps. And the issue here is that we have to give our coins to this off chain system and trust them to protect our funds. They have to take these human processes and apply it to protect our funds.
00:39:31.120 - 00:39:59.950, Speaker A: And that doesn't scale. Empirically speaking, it doesn't scale where in the web three world, ideally I can just deploy software. I deploy the software, the roll up. I offer exactly the same service as someone like Coinbase, but I just don't take custody of your funds. Instead, I can lock it in. And if that goes offline, for whatever reason, I just take my funds back out and go somewhere else. And for me, that's web three, where I can interact with a counterparty, but I don't have to trust them.
00:39:59.950 - 00:40:45.436, Speaker A: And really what this will lead to is that custody will be a liability for most off chain systems. It's very difficult to replicate human processes, to secure billions of dollars, because humans don't scale. If one of these roll up teams can actually build an implementation that works and is bug free, it's easy for developers to build on and it's really easy for people to deploy. That's sort of the layer three goal that anyone could deploy an instance of this roll up, and it'll be the software that scales and the software that protects the funds and not users. So I don't believe users care about custody. I mean, I have a mooncat. Sometimes I lock it in the services, sometimes I don't as a mooncat, but operators do.
00:40:45.436 - 00:41:07.444, Speaker A: If I'm going to do a startup today, I do not want to worry about securing billions of dollars. Who wants that liability? If I could just take someone's software and deploy it. There you go. That entire OPSeC problem disappears. And that's why I think rollops will eventually take off because operators will drive it. So again, thank you for listening. And I believe I'm out of time, so hopefully there's next speakers available.
00:41:07.444 - 00:41:41.684, Speaker A: Gigi. Thank you, Patrick. It was great. It's a good start. Next, we have a panel, and as you may know, roll ups are not only there to extend the throughput of the base layer, they can also add new features. So we have Luke moderating the panel and Oscar from empiric network. Remco from Worldcoin, oh, Joe from Aztec.
00:41:41.684 - 00:42:16.392, Speaker A: And Brandon. Or Shumo. No, it's Brandon. It was supposed to be Schuma, but Schuma unfortunately didn't make it. So we have Brandon instead from Poseidon. Lapse. Is that.
00:42:16.392 - 00:42:35.228, Speaker A: Yeah, audible. Okay, I guess as the moderator, I have to supplicate to the wing chair boys, but I think there's good reason for it. Good afternoon, everybody. Thank you. Patrick. The previous talk, it's very interesting. So I guess also, thank you to scroll.
00:42:35.228 - 00:43:10.228, Speaker A: Thank you to scroll. Thank you to those listed by scroll in the tweetstorm for helping organize. This day is fantastic. Definitely very, very difficult to do with the long range. So few people originating not even just from this part of the world, but just Bogota specifically. Today we're going to be doing a little panel with, I think, quite an eclectic mix on the takes of people who work in and around the rollup space for a variety of different use cases. And I think people who have envisioned the problem and the solution very, very differently.
00:43:10.228 - 00:44:16.380, Speaker A: So it's going to be interesting to see what goes on. As a small introduction, we have Oscar, who is the co founder of Emperor Network, who's building the data layer for l two s and zero knowledge roll ups. As a stark enthusiast, he's commenced that deployment on Cairo, but perhaps one day we'll see it moving to some others. Next in, we have Remco, who I, not until today, had the pleasure of meeting, but I do love your technical Twitter takes the two PI is quite the blog. He's held a variety of roles, investment, research, and notably, and very recently was at Ox park as a research fellow and now is the head of blockchain at Worldcoin. Next is Brandon, who's the CTO of Manta network, but also now permeating to Poseidon Labs, where they're building a privacy preserving, optimistic roll up. So a little bit different with a permeated account model playing around with those Utxo to get that privacy inside of the sort of walled, scaled EVM areas.
00:44:16.380 - 00:45:52.908, Speaker A: Also fundamentally, I think, a very enthusiastic type theorist, and last and definitely not least is Joe Andrews, who by academic training is an engineer and is now the head of product and co founder of Aztec Network. So aztec, I think, for many, many people has been at the heart of solutions on top of or around or at core Ethereum itself and the famous creators of plunk, the approving system that it itself or some modular part likely, and mostly that the permutation is used across an awful lot of solutions. Also the coiners, if I'm not mistaken, of the ZkZk roll up term. So those looking for privacy and scalability at the same time. So it'll be very refreshing to see the thought process that comes from that kind of angle. Okay, so the first questions and allowing me to be quiet, something laying the groundwork and I guess paying some homage to the name of the day, why roll ups? Why do you feel that blockchains and distributed ledgers in their form today require this mechanism for scaling and staying true to some underlying base layer? First of you, I wrote some notes, but I think blockchains today are not really ready for kind of mass adoption. I think you can kind of see one successful NFT mint or a project launch kind of grinds ethereum kind of to a halt through really high gas fees.
00:45:52.908 - 00:46:51.968, Speaker A: So we need a way to kind of scale that. And I think it comes down to everyone. Validating every transaction just doesn't scale. So roll ups, specifically ZK roll ups, let you kind of validate a proof of computation which scales a lot better than validating all the transactions so you don't have to trade off security for scalability if you use a roll up is kind of my take. And even if you have lots of users, in a lot of cases that does bring the chain to a halt sometimes. Also, there's applications that you can't even design on l one, assuming the gas fees that you're going to have to pay anyway. So it also expands the space of possible applications and roll ups and a whole like l stack all the way up, lets you sort of expand your design space and can bring many more developers into the area.
00:46:51.968 - 00:47:53.940, Speaker A: And to think about the kinds of apps that they can't even dream of writing right now on l one, for example. Yeah. And specifically, why roll ups and not potential other solutions to the scaling problem? The base layer, we want very, very strong security guarantees on that forces you into a corner that really limits your throughput. On top of that, EVM and Ethereum has some design choices that they inherited that also prevent it from scaling very hard. So combined, for anything that requires real throughput, you need to have some solution. There's a number of trade offs you can make there, but generally, if you want good forms of decentralization and trustlessness and sort of an ability to hold the operator accountable, you need some form of data availability. And the only one that we currently know of that has security guarantees that are on par with the base layer is to put the data itself on the base layer, hence roll ups.
00:47:53.940 - 00:48:50.216, Speaker A: I have nothing to add. Okay. And without that, we've got some desire for just definitely generic scaling. And what I think is a well justified statement to say that blockchains are awful. No, they're really far from where they need to be if they're going to be this new backbone of civilization that we're all talking about. We have a kind of nice fog of prescience where you could say, who knows what's going to happen now we can only sit and define and guesstimate all the different types of applications that we can have, given these sets of parameters, these awful blockchains. So perhaps once we make them fast and safe, the things that will come, the host of that will be unknown and hopefully, I'm sure, very exciting if the builders can get a shift on.
00:48:50.216 - 00:50:29.472, Speaker A: So with these shared security super scaling mechanisms, what are you most excited about? What behaviors, what applications, what even potential new styles of chains? Do you think that is going to be brought about not just in the way we perceive blockchain today, arguably a closed fence or a little bit of a walled paradigm, but really the next stage to cornerly quote whatever 1 billion users or web three to the masses, but things that perhaps people aren't quite envisioning. Donna, that end? Yeah, I think one of the things that we are most excited about is, as we've already pointed out, there are things that change on rollups beyond just cost and speed. And a big thing that we are really passionate about is cheap computation that you can now do inside of your smart contracts. Our protocol, our whole project is based on that premise. And I can speak a little bit to that. For an oracle, for someone who's bringing data on chain, having computation, cheap computation available is really a game changer, because now you can start really going into things that we call computational feeds, which are very different to just the normal price feeds that we see on the l one or elsewhere. Now, if we just zoom out and think about, what does that mean? We believe because we have these abilities now, and because we can build these novel data products now, that will enable other people to build much smarter DeFi protocols.
00:50:29.472 - 00:51:32.270, Speaker A: And I think you'll see that across, not just defi, you'll see it in gaming, you'll see it in many different areas where these new things that you can do, and computation is one of them, privacy is another, and so on, they will really change what we can offer to the world. And I think that's what we are really excited about, not to speak about the l three s and sort of app specific environments. I think that will all be very interesting. Curious to hear your thoughts. Yeah, I can chime in with how we use l two and roll up technology at Worldcoin right now. So our goal is to bring a billion people into the web three world, and we want to target people with, let's say, wallets that hold maybe $10 worth and still give them an experience that is meaningful for them, which doesn't work on ethereum layer one with the gas fees involved there. And in fact, I would argue it doesn't even quite work with the layer twos that are out there right now, which is why I'm pushing really hard for four eight four to get in, so we can get good performance there, too.
00:51:32.270 - 00:52:15.224, Speaker A: We have been experimenting with our own l two one that is transfer specific. It's an optimistic roll up that uses BLS signature aggregation, which basically gets you the best of both worlds. Your call data is very compressed because call data is actually mostly transaction signatures, and now you only have one for a whole batch, and you have no verification cost because it's optimistic. So with that, we got transactions down to under 400 gas. Some tricks we can do to get it even further. That's exciting. Unfortunately, these really transfer specific chains are not as valuable to the users because that's what the peer to pet transfer use case makes sense.
00:52:15.224 - 00:53:18.190, Speaker A: If you have a very broad adoption to bootstrap something, you really need the wider Webtree ecosystem with you and let people interact with all the protocols they know and love, like the uniswaps and the compounds and the nfTs, and you name it. That forced the l two space into EVM compatible chains, because that allowed all these projects to just quickly deploy and reach out to the user base in a now much cheaper environment. It'll be interesting to see how that evolves and if we're really going to cling to the EVM as tightly as we do there. But there is a second use case of l two s that I also want to highlight. One of the things we use is semaphore, which is a protocol to create anonymous sets on chain. We use this for our privacy protecting proof of prison node right now to insert an identity you call a function in a smart contract that adds your public key to a Merkel tree, and it costs a million gas. The reason it needs to be done on chain is to allow for concurrency multiple people to create the transaction at the same time going in in a random order.
00:53:18.190 - 00:54:02.760, Speaker A: What we do to scale this better is we have a sequencer that just aligns a couple of these insertions up in a batch and then creates a zero knowledge proof that updates the Merkel tree in one go. This is basically a very primitive minimal roll up already. Like we create blocks, we zero knowledge proof these blocks and publish them on chain with just the minimal amount of information people need. Basically just the leave values to recover the tree if they needed to. And this is like the area where we get into the app specific chains. This is about as simple as it gets, but you can see this evolving. We can also do something on the claim end where we aggregate batches of claims and allow people to do more complex queries cheaply on chain about their identity.
00:54:02.760 - 00:54:59.070, Speaker A: So yeah, I'm excited about where the l two space where whole ecosystems grow and come together and interact with each other work. Plus these use cases like we have, where we just want scaling for a particular thing. And I guess a third use case for l two s would be when there is value in having different kind of block aggregation mechanisms. You could, for example, think of batch auctions, batch auction markets, where you want to aggregate a whole bunch of transactions and settle them all at once. That has a number of advantages other than that you can find optimal, find more better optima than you would be able to create with sequential transactions. But you also need this if you want to do things like have perfect privacy in complex transactions. Yeah, I think that's, you made a lot of great points.
00:54:59.070 - 00:56:15.316, Speaker A: One thing certainly that you touched on that I liked was that Merkel tree like minimal l three, sort of. And I think that there's sort of two different independent ways you can go about building roll up type scaling solutions. You can have this kind of thing where you figure out what batching scheme you need for your protocol, and then you build an l three, or however, whatever layer you're on to scale that piece. One thing that, the kind of stuff that I'm working on and my team is working on is trying to not go in that direction, but go in the direction of enabling applications themselves to run with ZK as part of their code. So think like ZK apps, right? So the idea is that for an application where you want to do scaling, you just take, let's say you write some solidity code, you pass it through, you deploy it on l two instead of l one. It's now just cheaper over across the board instead. If we look at the most expensive parts of what makes zcap applications, what's the most expensive part of them is stuff like the really intense cryptography needed for, let's say, verifying proofs on chain, or doing some sort of batch signatures, pairing algorithms, all this kind of stuff.
00:56:15.316 - 00:57:08.484, Speaker A: And so what if you built a roll up that was specifically optimized for those kind of computations, right. Then you could have people building ZK apps on top of this roll up, where it's not an app specific roll up, it's sort of a ZK specific roll up. So any application that uses ZK technology in it would get a speed boost in this special roll up, right? And then you can build these sort of l three s on top where let's say you're doing something like private transfer, you want to do like millions of transactions at once. You use the l two to speed up any given verification, then use an l three to do batch verification for something like that. Right. So these are sort of independent directions you can go in when developing roll ups. And the kind of things that we're focused on is basically giving developers access to ZK in their applications, rather than just figuring out a scaling solution that's specific for their app, right? I'd agree with all of that.
00:57:08.484 - 00:57:53.504, Speaker A: I think that the most exciting things are kind of thinking about applications that require private state, because that's not possible on most of the ones we have today. So things like consumer finance, ZK games, they all require a developer to think and have these ZK tools. So if you can build a layer which verifies those ZK proofs, I think you get to a really interesting endpoint where kind of applications become private. And that's kind of one of the things which is going to spearhead adoption. In our view at Aztec, it's not just scaling like we're kind of in a bear market right now. Gas costs are kind of nothing and people aren't using Ethereum. So it's kind of the feature set of the roll ups.
00:57:53.504 - 00:58:48.970, Speaker A: And privacy is one of those features that we believe quite strongly developers need for adoption. I think some really excellent answers and also really excellent to, I think see the takes that were more feature based and more fundamental to looking at different permutations, iterations of the technology, rather than which of course would have been fine and as exciting. Oh, we're scaled now, now we can do visa or scaled now. And now we can do that was very interesting. From things that are bringing about certain elements of privacy. I'm sure I know from the way Aztec constructs its stack, they forecast a lot of requirements and necessity for privacy in a configurable way, which I think is super interesting. And seeing how that aligns well with scaling as it goes forward to open up new avenues for people to do other.
00:58:48.970 - 00:59:54.572, Speaker A: Standardly saying the real world or the previous iteration, the web two, whatever we all exist in now and seeing that happen with scalability is very interesting. Joint between Brandon and Remco. I really liked talking about different ways to find very, very low gas costs in the same way we have, whether something specific without necessarily being app specific or Zk or Remco's open problem. I think it was from example two of doing optimistic, but with BLS signatures, presumably to stop playing that exhausting gas tetris you must do when a g two is like a g two bn two five four is like 2 million gas and the g one is like 15 gas. We've got to sort of part and parcel all these. I do wonder if you can still keep things like that zero knowledge by running a different aggregation scheme like a two round schnor, and then throw in a KCG ten to mitigate the call data, I think so that maybe, but not for 400 gas, sort of around the 50. That's crazy numbers, I think at this stage, but I like it.
00:59:54.572 - 01:00:37.016, Speaker A: And first. But was the last. Sorry, but was first Oscar speaking about what you can do with sort of app specific and new applications to create l three s, I guess takes us to our next and very important question as we attempt to have even greater prescience, because as the funnel opens up into all of the things we're going to do, predicting the behaviors and the necessities forward such that we can design them in the correct way. Now is tiresome I would say one word. So, computational constraint. Now we're talking about l two s here with general purpose compute. We talk about l three s with sort of app specific l three s.
01:00:37.016 - 01:01:41.736, Speaker A: So, general purpose compute about taking this next layer above an l one. Or you could also technically do the same with an l three and keep going up. But do you feel that we need app specific, whether l three app specific layers to run some back end for gaming engines, some extremely high through point of sales? And how do you think those are going to fall into the existing roll up narrative today? And do you think it is possible? I won't yet give my opinion. I'm not sure as a moderator I should be keep the bias to a minimum that we're just going to be able to do away with these and just keep scaling some levels of compute and eventually have 1 billion transactions per second. I think the l three kind of conversation is a strange one for me personally, just because I think it's more an admission of the l two that their execution environment is missing features. So I think that at some point, there will be an l two that has the features. I'm very biased.
01:01:41.736 - 01:02:32.780, Speaker A: I think that's Hastech. But I think you should be able to have apps that can run on your l two that don't need to do kind of even more aggregation. Otherwise, you've kind of maybe missed the kind of specification of your l two vm. Like, you may have focused too much on evm equivalents as an example versus kind of privacy or some other features. So I'm confident that as technology increases, you'll have kind of apps that do kind of run on an l two, but they shouldn't need to do too much more aggregation to get the scale that's required of that l two. And maybe if the execution environment has programmability, those apps can pretty much do everything on the l two that they need to. And calling them an l 3 may be just confusing.
01:02:32.780 - 01:03:39.420, Speaker A: I think one other thing is that when you have an l two, and of course, this is way too early, but when you build l three s on top of the l two, you have again, like everything else, you have sort of this ecosystem lock, right? Especially with the partitioning of all of the value that's in the blockchain, you're spreading it out amongst all these l two s then, oh, even worse, you had to spread them all around all these l three s, right? So you have sort of this ecosystem problem where if you're building an l two, that's going to do something specific that's not just going to be evm compatible across the board. You sort of have to have the right ecosystem for it. And I think something that would be really interesting to see is something like gaming companies, where a gaming company, let's say, like, what's the. I forget what. Unreal, right? Let's think like the Unreal engine, right? Where the company that runs Unreal Engine would build an l two. And then games that run on Unreal would be l three s on this l two. And maybe you have, let's say some game where you're playing a round of the game, and once the round is over, you don't care about that state anymore, right? So these l three s, you sort of spawn them freely.
01:03:39.420 - 01:04:28.800, Speaker A: You run your game on the l three, then you just commit the final state to l two, and you just forget about the l three again, right? So you sort of want to be able to spawn up. You want to be able to spawn up random amounts of high intensity compute and then pull it back depending on the sort of demand needed for that application, for example. Whereas l two s can sort of give you this all around scaling in certain instances where you have very high demand. Or you have this ecosystem lock where you might want to build l three s for that purpose, right? Because you have all these mechanisms like Zkp to be able to build l two s that can have arbitrary compute, right? They don't have to run evm. They can be anything. So that sort of, I think, leaves the space a little more open. And like you said, aztecs focus on privacy.
01:04:28.800 - 01:04:58.012, Speaker A: And so maybe you're making different choices in your vm that are more privacy biased. And in certain other instances, that might not be the right thing. And so I see l three s more as the place where l two s builder ecosystem, rather than some extra level of sharding. We have to just deal with more sharding. Right? So I think it's more about an ecosystem in that case. Yeah, I really like the game example. I would say that currently we have these evm compatible l two s.
01:04:58.012 - 01:05:40.728, Speaker A: They are inherently limited in the amount of scale that you can provide, because EVM itself is inherently limited. Like, if EVM could do more, we could also do more on the base layer. The reason we get away with it in the l two s is because we compromise on a couple of things. We mostly compromise on the consensus decentralization, which is kind of fine, because by nature of l two s, it's mostly censorship. You're not too worried about consistency things there, because that is guaranteed by the base layer. But still we have the EVM there. So we are going to hit a scaling limit with the l two s four eight four is going to solve the data availability cost of it, which will give us probably another ten or 100 x more room before we hit that limit.
01:05:40.728 - 01:06:48.604, Speaker A: But we will definitely hit that limit with the various l two s out there way before every single in game transaction is on chain. So that necessitates ltrees and the existence of alternative solutions. Now, the l trees, for the very same reason, if they are all roll ups, they will not get any more scaling than the l two s get, because you still need to accumulate this data at the same place. So what you do there is you compromise even more on the security guarantees, which you can probably get away with, because in an ltree, you're working in a very specific, application specific context. So you can provide, you have usually some level of trust, like in games especially, you would trust the server that you're currently gaming on. So then you can probably move away from data availability and not be technically a roll up, but just be an l three of a different kind. And again, the thing I mentioned earlier, where you want to experiment with new mechanics, new ways of new transition rules for your block that are not easily implementable in EVM.
01:06:48.604 - 01:07:15.896, Speaker A: Things that aggregate over many transactions, things that implement very game specific logic. For example, you need an experimentation space for that. Probably want something that can verify, something complex cheaply. So an l three there would also be a very natural answer. Yeah, it's interesting, because we're obviously implemented on Kyra right now, so not EVM compatible equivalent. So it's a bit different, but still there. We talk about l three s all the time.
01:07:15.896 - 01:07:58.340, Speaker A: And the thing that I think we're excited about there, in that ecosystem, is that you can do things like, privacy is not something that stocknet sort of gives you built in, but you can have that with an l three if you want to build like a reperformed order book, right? That's something that you might want to do on an l three. If you have a very specific NFT protocol that you'd want to build, maybe an l three is a better solution for that. For us, certainly, if we want to. I mean, currently, what we're doing is we do kind of simpler things. We construct a yield curve that we compute on chain. We construct sort of market volatility, oracles on chain. Those are relatively straightforward computations.
01:07:58.340 - 01:08:30.336, Speaker A: You can imagine that we could do very complex things that you couldn't do on the l two. And I think that's what we're excited about. And there's a bunch of great, I think, great teams like slush and so on, thinking about this kind of stuff on stocknet and elsewhere. And so we have an interesting different view to you guys because we're sort of trying to build on that. Right. And so we're really excited about all the different possibilities that we do see for all the different l two s. Yeah, I think I'd just kind of maybe count.
01:08:30.336 - 01:09:43.652, Speaker A: Well, I think there's a trade off between the kind of lock in requirements that you were saying as you go further down this layer stack to just building a kind of more specific l two that kind of doesn't have those trade offs because you're basically kind of trading off the ability for a user to get out of the system to somewhere else for kind of being on a particular l two which has got an incorrect feature set for your actual application that you're trying to build. So I lean more towards seeing, I guess, app specific roll ups. But I think that's in that I can see Starquare's value proposition is like immense throughput, and they've sacrificed EVM compatibility for that. And I think that's a fair trade off to make because it enables a lot of public applications that maybe don't need kind of some of the things you have on aztec. I think that's what's really exciting about this whole space is that people are willing to experiment with all these completely different trade off spaces. And so people aren't just like focused on one thing. So I think it's really exciting either way, excellent answers do not have time to summarize.
01:09:43.652 - 01:10:18.676, Speaker A: We can hasten to the last question. And Targor doesn't start throwing things at us, pacing around the room. I just want to quickly talk. It's been touched on, I think, very, very nicely about, and I shall keep this test. We touched on very nicely on privacy. And this feels like the old flame in the ZK roll up from whatever it was in the 60s from gold, Vassar et al. Creating these computations or this kind of concealed computation as part of an arithmetic circuit to seeing blockchains have a kind of serious use case.
01:10:18.676 - 01:11:17.040, Speaker A: When I first had the fortune of jumping on the zero knowledge cryptography train a few years ago, it was witness encryption and it was private encryption. Yeah, I'm coming. It was private encryption for our testing to some certificates and browsers. Back with God bulletproofs and since then, it turns out that little arithmeticization, that little witness preserving computation of some program is a hell of a lot smaller and a hell of a lot cheaper to verify on chain than you would. Or we don't necessarily have the necessity to do so. So we don't have mega servers anymore. So to those, and I guess the two panelists on the left, very, very relevant to their need and desire for privacy in the web three ecosystem as we go forwards, do you see that there's any clash between providing privacy and scalability? Not necessarily just for now, because I think it's more arguably, yes, encoding, and encoding is doubly expensive.
01:11:17.040 - 01:12:29.468, Speaker A: You're doing Merkel trees and Merkel trees like mobile phones, the way you guys do all the extended Utxo with the old spending model, but for sort of the very much long term, do you see that you have to make a trade off for the two of these, or for any cat? Or is it that privacy preserving applications or privacy preserving behaviors work better, or tessellate even into well scaled platforms? That would be difficult. And then the last quick sub question there, because we talk about this a lot and it's not brought up enough on Twitter and whatever other outlets that. Do you think some of the designs and some of the protocols creating and envisioning the roll ups for today, just ZK roll ups of the validium style non private will have issues retroactively adding privacy. I think you'll find that the answer to both is the same. It is you need to get rid of global mutable state that allows scaling, that allows privacy. I think it's very hard to retrofit privacy as a sub question. You kind of always need a Utxo model, and having kind of an account based model makes privacy very hard.
01:12:29.468 - 01:13:10.840, Speaker A: You need privacy to be by default, else you just have privacy being used by people who have something to hide, which is kind of not the norm. I think the start of the question in terms of scalability, I think we can compete with Visa Mastercard costs with the current design space. In terms of transactional costs. I think the elephant in the room for ZK privacy roll ups is client side syncing. The current model is download and try and decrypt every single transaction which doesn't scale. So that's like the big elephant in the room, which I think there's not enough people talking about. Yeah, I think just to end it here, because I think toggle is getting nervous.
01:13:10.840 - 01:14:03.310, Speaker A: Yeah, I think there's definitely some tradeoffs in the near term, but I think in the long term, as you see, one trend I think that we're going to start seeing is that optimistic rolls will be able to, whatever customization they're allowed to make in their vms, you'll be able to find a ZK replacement for it. And so the thing, the sort of design we're trying to get to, to try to get privacy in there faster is do an optimistic roll up that has privacy as a feature app developers can use. And then as ZK technology starts getting better on the scaling side, not on the privacy side, we'll be able to transition into ZK roll up. And so maybe this is the opposite direction of what Aztec does, but I think we're starting as an optimistic ZK and then going into ZkzK in the future. And I think it's going to be very clear that you can replace all optimistic systems with ZK. Maybe it takes ten years, but maybe it takes 20, but definitely going to happen. I don't see any reason why it wouldn't happen.
01:14:03.310 - 01:14:28.470, Speaker A: Yeah, I guess we're at an end. Yeah. How serendipitous that ZK privacy brought us ZK scalability. Thank you, everybody, for listening and to the panelists for taking your time. And once again, everybody organizing. It's an absolute pleasure. Thank you so much.
01:14:28.470 - 01:15:15.696, Speaker A: Targal and I and the scroll team will take it as a compliment that you guys were so enthusiastic about talking. Thank you so much. Now, the next talk, the next panel of today will be about funding public goods. It will be moderated by David Hoffman. Thank you. Give it up for David. And we'd like to welcome Kevin on stage and also Ben with a very special first time appearance that probably for the first time, Ben is not joining a panel with his flute like instruments.
01:15:15.696 - 01:15:42.836, Speaker A: Give it up, everyone. No. Whoa. Hello. There we go. What's up, guys? So here today we're going to talk about public goods funding them, but also with layer twos. And so I'm joined by Kevin from Gitcoin and Ben from optimism.
01:15:42.836 - 01:15:57.868, Speaker A: And you guys are. I'm not going to be able to introduce you better than you can introduce yourselves. So if you could please do that. Kevin, you want to start with yourself? Yeah. I'm Kevin Olson, not Kevin Owaki. Just a quick disabilition. So I'm the work stream lead of the Gitcoin product collective.
01:15:57.868 - 01:16:17.460, Speaker A: We're the product and engineering group inside the Gitcoin Dow. So we're building Gitcoin passport and the Gitcoin grants protocol. And Ben, who are you? What's going on? Y'all, I am Ben from optimism, one of the co founders and on the Optimism foundation now. And we're here to build public goods, baby. Hell, yeah. Who are you? David. I'm David.
01:16:17.460 - 01:16:46.880, Speaker A: I produce content, and I produce a lot of it, and I try and do it every second of the day. Influencer educator first, hopefully influencer second. Okay, I want to start off with a high level question. Layer twos and public goods. Why are these in the same sentence? Why are these. Where is the intersection between these things? Ben? I'll start with layer. Okay, well, first of all, layer twos are public goods, okay? I mean, there's a lot of layers that you can.
01:16:46.880 - 01:17:20.100, Speaker A: No pun intended. There's a lot of aspects to that that you can unpack. Right? So, for one, it is true that, at least for correctly permissioned software, layer two s are public goods, right. These represent code bases, and very, very importantly, they represent very easy to spin up chains because you can borrow the consensus and the security from a layer one. So I would say even more so than a layer one code base in some way, a layer two code base is a public good because they can be used by more people. Yeah. And zeroing in on that a little bit more just to make the question more precise.
01:17:20.100 - 01:18:03.972, Speaker A: Funding public goods and layer twos, how do these conversations intersect? So how does funding public goods and layer twos intersect? So, I mean, that depends on what layer two you're talking about. For us, it's extremely core to what we're doing. I would argue that it is a prerogative or a responsibility that we have, not just in layer twos, which I think is, like, a little too in the weeds. And people in ten years aren't going to think of themselves as on a layer two or a layer three or a layer z. They're just going to think about themselves as using these new forms of applications. But I do think it's very important that we use this opportunity of the new kind of software that's eating the world to do good for the world. So, very broadly, it's, I think, in some sense, a prerogative.
01:18:03.972 - 01:18:47.028, Speaker A: Like, you could make a closed source layer two, but we shouldn't. And just to drill down this on this, a little bit more optimism from day one, has always been leading with public goods and funding public goods, and that's kind of been the optimism brand for what the optimism layer two is here to do. Not only is it here to scale ethereum, but it's also kind of here to scale public goods. And sometimes it feels like scaling of public goods. That's actually the main goal of optimism at times. Why such a strong commitment to funding public goods? Why is that such a core value proposition of a layer two? Yeah. So obviously these things are very related, as we started off with for optimism story in particular.
01:18:47.028 - 01:19:33.280, Speaker A: So, actually, when we very first started off, before optimistic roll ups really existed as a class of thing, there was something called plasma. And this is something that's finally reemerging in the layer three world, which is very exciting. It was a bit ahead of its time in some sense, but we started a nonprofit called Plasma Group, and the goal was basically to build good plasma designs and release them for all the projects that wanted to build scaling solutions. And what we quickly found was that even though it was very compelling thing for us to be doing, and we saw other projects, like directly using our designs and implementing them in their own code bases, we had a really hard time getting funding. Right. We were a nonprofit, and basically we had to go and beg for money in the form of donations. And I kind of felt like this is crap.
01:19:33.280 - 01:20:16.876, Speaker A: There's all this money going to tokens and the application layer. Why is it the case that we're building something that's so fundamental to what everyone else is going to be doing and not able to get funded? So when we made the decision to spin down the nonprofit and go for profit, we said we have to be the last ones to have to make this change. People should get paid to build public goods. So I don't know. It's core to our story, and I think it's, like I said, a responsibility that we have now. Kevin, turning the conversation to gitcoin a little bit, there's going to be many chains, new chains beyond the Ethereum layer one. And Gitcoin is an application layer, so it doesn't really exist on any one particular chain, but there's going to be a cambrian explosion.
01:20:16.876 - 01:20:56.384, Speaker A: We're already kind of seeing it of many, many different layer twos. How is Gitcoin interfacing with all of these different chains, and how does that relate to what Gitcoin's mission is? Yeah, it's an ever expanding surface area. So from an engineering perspective, we're just redeploying the contracts for the protocol on each l two. Ideally, there would be some better tooling, I think, to make cross l two tooling work a little bit better. But right now, it really is just on the burden of the application layer to basically go kind of community by community, two by layer two. And interestingly enough, I think the alignment with optimism has probably been what's driven us to start with that. Right.
01:20:56.384 - 01:21:38.990, Speaker A: So our first deployment is on optimism, working on RPGF two. And I think the plan with kitcoin is to stay values aligned with the groups that we're working with. We're not deploying to be a general service necessarily as we start, but really to work with these individual groups that support public goods, that want to be funding them and, yeah, seeding capital into their ecosystems. Right. To really grow those communities. What lessons can we take from the early days of Gitcoin and, like, rounds one through five, which was very much an experiment for the whole entire ecosystem? Now, Gitcoin rounds are pretty damn, like, polished and refined, but now we're getting into layer two funding. So how are we accelerating what we've already learned, taking the lessons from the layer one? Yeah.
01:21:38.990 - 01:22:10.836, Speaker A: The biggest thing for us is don't build a single monolithic code base, even if it's the fastest way to do something. We're definitely in this process of modularizing gitcoin and bringing it out, really as a protocol. All of this is kind of getting talked about. We're not doing a great job of talking about it. We're mostly just working with partners directly to help kind of bootstrap their first kind of protocol driven grants rounds. Lessons there, I think, are, fuck, I did a lot, man. We've learned so many things.
01:22:10.836 - 01:23:01.140, Speaker A: What's hard about running open grant surrounds, like, with quadratic funding, civil attacks, right? You have to have some sort of stronger sense of identity. So you see all kinds of experiments there over the years with, like, building up trust bonus and now passport grant curation is super hard. Right? People are trying to create fraudulent grants and building up an ecosystem where you can detect that or bring better data signals into the people who have to make decisions, who are curating their grants around and deciding what projects are valid for their community? There's so much that we've learned through that, a lot of it through human effort, and that's going into the protocol and through our tooling to make that bit more easy. I want to say automated, but definitely, like, putting people where they have the right tool to make the right decisions. How does the fact that these layer two teams are just a little bit easier to actually interface with versus Ethereum layer one governance? Right. So you can go talk to the optimism team. Ben's right here.
01:23:01.140 - 01:23:36.748, Speaker A: We can go talk to the arbitrum team. How does that change? Does that change any way of the way that gitcoin operates on these layer twos. I think maybe what's interesting about this is I think most of the layer twos probably are still sort of like the bootstrapping of a community. And I think public goods funding is a great way to do that. Right. I don't want to get too abstract here, but what is funding public goods? The closest corollary you have is sort of taxation, right? You're thinking about actually funding what bridges and roads and clean water. This is like where the public goods term comes from, right? But what you're really trying to do is make sure that you have the sort of infrastructure for your community to thrive.
01:23:36.748 - 01:24:31.664, Speaker A: So public goods funding in this way, if you sort of take that lens, is making sure you have people who don't have to close down their nonprofits to go build something that can get these alternate funding mechanisms and start adopting your technology and getting that virtuous cycle going of users usage fees going back through your system, but then also making sure you're constantly seeding that back out. Right. I don't think any economy should be completely extractive. So I really think the L two s have built this really kind of into their protocols to decide we're going to take some portion of our treasury or our fees or however to make sure there's a constant stream going back out. And then the problem becomes what's the optimal way to allocate that capital? I want to get a little bit out into the frontier here. And that brings us to retroactive public goods funding. And this is, I think in contrast to Gitcoin's model, which is more proactive as in like, hey, I think this thing will be useful in the future, or I know there's this thing that's already been useful to me and I want to see that reward.
01:24:31.664 - 01:25:02.652, Speaker A: And that's like the Gitcoin quadratic funding model. And then there's the optimism. Retroactive public goods model, same but different. Ben, can you kind of walk us through the comparing and contrasting of how are these things the same? But how does the retroactive part change the dynamic of how public goods funding is? Sure. So okay, first of all, major shouts out to Gitcoin, this is like a little on the down low. So this is some alpha. If you're watching this online, if you're the person tuning into roll up day public goods, the new modular code base that Gitcoin has is great.
01:25:02.652 - 01:25:38.036, Speaker A: Some of our retro funding in the future will run off of that. So that's awesome. Okay, but what is retroactive public goods funding? The basic premise is that we should fund things that have contributed towards the impact that we want to see after the fact instead of beforehand. So why do you want to do this? The main argument is that it should be easier to know what had a positive impact after the fact than it is up front. And this is something that we see all the. It's. I think it's actually like an urban legend, but it's still a great analogy of the story of cobras in India.
01:25:38.036 - 01:26:19.616, Speaker A: At one point, the government said, we're going to fund anyone who contributes to solving our cobra infestation by submitting dead snakes to the government. And what people started doing was farming these cobras as a way to get these rewards. Right? So you don't always know up front if the thing that you want to happen or the way that you want to have an impact is going to have the impact that you expect. Right. Because the end result of that was more cobras out there than less. So that's the main argument for why you should do funding public goods retroactively, I would say mechanically. The other concrete difference that you can get is now you basically have a future expectation of funding that is going to come in towards projects.
01:26:19.616 - 01:27:17.000, Speaker A: And the natural question that arises from that is, okay, retroactive funding sounds great. Sure, maybe you can allocate funding more efficiently, but how are these people going to pay their rent and eat in between the period when they're working on these public goods and funding them? So this gets into the concept of basically like, creating vessels for people to invest in public goods up front. And the idea is that a market should actually be better in figuring out what even up front, what are the right public goods, what are the right actions to take to cause the impact that you want to have than any one central would be. So the idea of retroactive funding is basically, it's easier to figure out in the past what worked well than what will work well in the future. And then to get people the money today, the market should actually be able make opinionated bets on what is going to be retro funded and pay for rent and food in the meantime. So there's a little overview. Yeah, I can jump in.
01:27:17.000 - 01:27:53.764, Speaker A: I think they're really kind of two sides of the same marketplace, I think. Right. So if we're helping kind of bootstrap projects and get them off the ground, there's this place where eventually they do need to, they kind of graduate. Right. So we have some projects that have been in round one through 15 now, and they are kind of relying on gitcoin as the sustaining thing. And as we continue to evolve their protocol, it can be a little painful because we're not necessarily built to be someone's always on funding, right? This is where I think something like retroactive would make a lot of sense, right? To come in and say, okay, this project has delivered value over and over and over for the years. Let's let our community turn this kind of always on aqueduct of funding and keep them going.
01:27:53.764 - 01:28:55.288, Speaker A: So I think it's brilliant. I also think there's some corollary to this concept of impact certs and hyper certs that protocol labs has been talking a lot about, which is this idea of if you're like an early investor in a public good, if you're part of a gitcoin round, there should be this market later on, which is saying that thing actually had an outsized impact. Let's start buying these really impactful sort of whatever they might be, social innovations or software projects, fund them later, kind of once the sort of value has increased. And you can kind of create this analogy to sort of vc funding, right? They're sort of like angel all the way up to series CD. And I think that this is kind of what we're talking about is like this graduation of bootstrapping and the early risk. We don't know if this team can pull this off or if this will have much impact, but people want to try, right? So many, many ideas, many small bets, and those graduating into these larger sort of more sustaining funding models, public goods, vcs, that's the golden future that we want to get to now. The tech influencers that are talking about what protocols are good or bad can put their money where their mouth is and benefit everyone in the process.
01:28:55.288 - 01:30:04.300, Speaker A: I really like that analogy of starting with the angels and then at the end of the road are like, is a 16 z to be the ultimate last buyer of the asset or whatever. And there's always been this conversation, and I remember this super clearly in the ICO mania which brought me into the space is like, yes, icos, we fixed the ability to democratize the access for investment. Turns out we didn't do that, but that was the energy, right? And we still see that energy in the crypto space where all these retail investors want to see vcs disrupted and they want to be the early ones. And this has always been a core promise of the crypto space is the individuals are at the frontier, no longer the big institutions. And so is that kind of what you see with his whole retroactive public goods model where you actually are like the individuals who are sifting through alpha on Gitcoin saying like, ooh, that's a really awesome retroactive public goods thing. I'll invest in that one. And this is this new surface area for democratizing access to both upside for public goods and also upside for the small fry that puts in $15 or $100 or something.
01:30:04.300 - 01:30:47.944, Speaker A: Is this kind of the vision that you guys see? I mean, yes, I think if your output is impact, right. We're not talking about necessarily like ten x returns on your 15, whatever. You're a small investment. I think the idea of bootstrapping an innovative project that's going to make some impact in the world or solve a technical need that you have through open source, these things are great to get off the ground and really, truly they don't have other ways of funding. You live this, right? You cannot get funding for open source software for projects like this. So we're building this completely new funding mechanism and that's great to see it graduating into these more sort of institutional spaces of always on or significant funding. How do you see the innovations going on on optimism.
01:30:47.944 - 01:31:11.716, Speaker A: The retroactive public goods innovations and experiments at gitcoin? Is Gitcoin like, hey, that's a cool experiment over there, or are you guys like, hey, we're going to copy that. What's going on the gitcoin side of things? I don't know if we're copying. I mean, we're working together to make it happen. Right. To re ask a question. Retroactive public goods is also going to become a core pillar of gitcoin itself. Yeah.
01:31:11.716 - 01:31:49.810, Speaker A: The goal of the grants protocol to be modular. Right? So you want to use pairwise quadrantic funding and run your own round, just like gitcoin. Be my guest. You've got that ability if you want to change that and do some sort of like token gating quadratic voting, which is actually kind of more the mechanism that we're seeing in RPGF, that's going to be an option for you as well. You might be able to use Macy in the future, a variety of other sort of mechanisms that you can use for how you want to actually do that voting and sort of the initial sort of, what is the wisdom of the crowd? How are you going to source that and how are you going to get that to define where this pool of capital believe allocated goes to? We want to have a variety of mechanisms. It's open. We hope people here will contribute and build their own and build on top of what we're building.
01:31:49.810 - 01:32:33.592, Speaker A: Yeah, maybe that's where I'll stop. Ben, one question I've had about this retroactive public goods model is that I'm worried it might actually bring in a new lobbyist sector of people lobbying for donate to my thing, and all of a sudden, it becomes a little bit more political and a little bit more gameable. Have you guys thought about this and how to prevent this world from happening? Most definitely we have. And there's a few sides to that coin, right? No pun intended. One side of that coin is just like, you got to try something, right? So the fact that there's a risk doesn't mean that you shouldn't make an attempt and you have some possible future you could reach or no chance of reaching it. Right. But there's a lot that you can do in terms of trying to prevent those outcomes.
01:32:33.592 - 01:33:11.160, Speaker A: So there's a couple of things. So a very important thing for optimism was that we decided that our governance system must have a co equal house of governance, which is called the citizens House, which is one person, one vote. Right. So there's a huge vector of plutocratic token voting attacks that you must be able to put in check. And so humans are how we want to do that. Secondly, Kevin just mentioned Macy, and it's true that there's a lot of work that you can do on the infrastructure side to try to minimize the ability of these kinds of outcomes of people trying to manipulate votes. Like bribery is a big one that is specific for Macy.
01:33:11.160 - 01:33:30.128, Speaker A: So Macy stands for minimal anti collusion infrastructure. And the idea is that you should create, basically use cryptographic mechanisms to make it impossible to de anonymize votes. Right. So, like, right now, we see a lot of voting just out in the public. Right. Just like on the chain, you can see how everyone has voted. Right.
01:33:30.128 - 01:34:23.632, Speaker A: You can imagine a future, which I think is important, where you can't see who has voted so far, and the votes are only revealed once it's time to tally everyone's vote up. But even then, it's problematic because you're de anonymizing. So Macy is an even further better instantiation of this that says there are cryptographic mechanisms in place that it's kind of like ZK proofs. We know that the result of this vote was x, but we do not know who voted for and who voted against. And in fact, even if they wanted to try to prove which way they voted, they could not, based on the fundamental cryptographic assumptions. So putting it at the social layer to prevent plutocracy is one of the things. And then just in terms of the infrastructure, making it hard af to break these things with other contracts and other mechanisms are both super important things.
01:34:23.632 - 01:35:01.660, Speaker A: Yeah. So to summarize that, the people that might be the subject of lobbying efforts and bribes, right, these people, the citizens of the optimism house, maybe people are coming to them and say, hey, I'll pay you bajillion dollars to do this thing. And they're like, yes, I'll do it. And then they actually can't prove that they've done it in the first place to anyone. And so if you're going to accept that bribe, whoever's paying that bribe is actually just taking it on faith that you are doing the thing that you do and you can't actually do her. You don't have his ability to prove that you did it. And so you might as well actually just vote with your values and also collect the bribe.
01:35:01.660 - 01:35:13.600, Speaker A: Still corrupt, but at least it doesn't corrupt the protocol. Anticolusion, baby. Exactly right. I think we're coming up on time a little bit. Oh, five minutes. We got more, some more. Cool, tight, tight.
01:35:13.600 - 01:36:03.840, Speaker A: Ben, can you kind of walk us through like a Sci-Fi narrative story? Can you make up a story of a cool, awesome, retroactive public good process? Can you just speed run through a future example of some thing that gets funded with retroactive public goods and start at month one year, one year three? Can you just tell us the story of how it might go? Yeah, sure. Okay. I have a new framework that I've been working on that I'll just quickly show. There's three steps to retroactive public goods funding that you do repeatedly. Step one is impact scoping. It's basically everyone who's going to be controlling where these funds are going says, okay, what is the future impact that we want to see that we will reward retroactively? You achieve social consensus on that. Your goal is to make that as clear as possible so that people will contribute upfront with the expectation of future reward.
01:36:03.840 - 01:36:52.608, Speaker A: Step two is impact scoring. So everyone goes and says, how much impact do we think every project has? Step three is impact settlement. You take all those scores, you aggregate them via quadratic mechanisms or whatever, and you turn those into payouts for those projects. So this is going to be something that we'll see in optimism, puny and in anyone else that does RPGF do as a repeated process, basically. And so I think what you're asking is, what is the Sci-Fi world in the long term future of what is the consensus that we decide around what we're going to fund. And how does that play out? So I think the singularity is obviously where you have to take this if you want a Sci-Fi answer, David, like right now we are seeing an explosion of artificial intelligence. I don't know if y'all have used the dollies and the stable diffusions and all this, but this is crazy.
01:36:52.608 - 01:37:58.436, Speaker A: I used to pride myself on my ability to Photoshop things, and I'm realizing that's just like a dying. No longer do I have exclusive access among my friend group to meme my buddy onto a stupid face or whatever, right? So I think a huge thing that we're going to see in the future is when the super AI comes around, we need to provide a value system that works for both the humans and the ais. And so at the end of the day, there's actually just a paper on this, on the idea that there's a concern that the AI is going to game whatever reward mechanism we give them to short circuit and give itself more reward instead of having the impact that we desire. And so I think there's an argument that in the long term future, optimism has a massive citizens house, globally connected Internet of value with the blockchain. And what we're doing is really laying the groundwork for what are the values that we want to see the AI complete? Because they're going to be the best ones. They're going to dominate the stock market, they're going to dominate everything. And we use the retroactive rewards to keep them in check and to align incentives so that we're not squashed in an effort to create a bajillion paperclips.
01:37:58.436 - 01:38:39.332, Speaker A: I love the long term thinking this. Still long term, but much less long term than that. The idea for public goods funding and the Ethereum ecosystem has always been to like, hey, we need to bootstrap our own ecosystems. But to me, that's always been like, you put on your own oxygen mask first before you help with the others. The idea is to eventually scale republic goods funding to the world, not outside of Ethereum. So maybe you can also tell a story, maybe a shorter time frame story, about how does optimism go from just funding and bootstrapping its own ecosystem to fixing the potholes in some rural town, in some country somewhere. Yeah, so, I mean, that's a very deep philosophical question.
01:38:39.332 - 01:39:39.684, Speaker A: We all have this sense that these blockchains are these sort of digital nation state collective things somewhere between a company and a nation. So there's very philosophical questions embedded in there. I think at the end of the day, when we start to look beyond what is the next frontier, beyond blockchain scaling with public goods funding, I think the next frontier is still going to be software, because we're going to learn a lot about how to fund and incentivize open source software development. And although we in the blockchain community have an easy time forgetting this, there is a huge amount of very important open source and pieces of software out there that are not aligned with the incentives of humans and that dominate our lives. So I think the next step we'll see after getting really good at funding blockchain public goods is going to be other open source alternatives to basically applications that are not web three, but are replacements to the horrible web two systems we have today. Not all those require tokens and money and funding. And so that's going to be the next thing.
01:39:39.684 - 01:40:29.800, Speaker A: Slowly, over time, as more people join the collective and as more people are a part of these online communities, I think we will start to see it permeate into the real world and outside of software. I also think that we'll start to see to the AI point, I think we'll start to see basically hosted non blockchain applications whose impacts are being funded by these mechanisms as well. So slowly over time, we'll go from open source blockchains to open source applications beyond blockchains, and finally we'll permeate into the real world. And exactly how that works is like an incredible political, philosophical question that we could talk for another 5 hours about. It's one of those things where like step one, step two, step three, question mark, question mark, question mark, step four, do it. Yeah. Kevin at Gitcoin, how do you guys think about scaling public goods beyond crypto? I think the biggest thing for us is making that ability for small local communities to fund their needs.
01:40:29.800 - 01:41:22.504, Speaker A: That's really what it comes down to. So I really think one of the major flaws in kind of current nation states, right, is the inability to sense and respond at a local level. So giving this ability for people to decide to fix the pothole in their street, potentially individually by collecting their activities, funding those themselves, or through these sort of online mechanisms, making it where these online communities can have a lower lift to get into funding their own grants programs and sort of plugging into these larger streams of funding. So I think it's sort of fractal getting lots and lots of small communities funding their shared needs and those laddering up into larger and larger projects. So I kind of look at it as sort of a bottoms up kind of view of the world. As we close out here, let's make things like super specific. What's the call to action that you need at Gitcoin to help that world come about faster? What do you need people to do? I need people to look at passport and try to use it.
01:41:22.504 - 01:41:44.416, Speaker A: Tell us how to make it better when the grants protocol is live. Try to use it, try to build on it. Try to use it for your communities. Make that the sort of like basic capital allocation substrate for all of web three. Ben, what do you guys need? Optimism. What do you need people to do? So definitely go use Gitcoin because we're also using their stack for the next thing. Beyond that, I would say don't forget why you are here.
01:41:44.416 - 01:42:13.850, Speaker A: I think time and time again, especially through cycles of crypto, we see a really exciting value proposition and a really exciting piece of the future that we're getting a glimpse at. Be totally distracted and moved offside by desire for quick buck and a desire to cookie cutter copy paste the things that have been working so far. So I would say use gitcoin. And don't forget why the fuck we are here. Pardon my french. You're here. Thanks, guys.
01:42:13.850 - 01:42:41.150, Speaker A: Thank you. Thank you, guys. It was a really interesting talk. Next, we have somebody that probably not a lot of you guys know. Okay, it's fixed. But he contributed a lot to ethereum from the beginning. Please welcome on stage Vitalik.
01:42:41.150 - 01:43:33.646, Speaker A: Great. Okay, so thank you so much. And today I will be talking about multi proofs. So start off introducing the problem, right? So today almost all roll ups are still on what I call training wheels, right? There's still some kind of mechanism that can basically override the proof, and we cause whatever outcome it wants inside of the roll up if it decides that the code has a bug in it. Right? Like there is some kind of multi sig override governance thing, whatever. And in pretty much every roll up that exists today with very few examples. So fuel v one is one of them.
01:43:33.646 - 01:44:35.854, Speaker A: And I think some of starkware's products might be another example. But with very few exceptions, everything is on some kind of training wheel where even though there is some kind of proof system that is theoretically there, the proof system isn't really in charge. Right? So there's this page on l twobeat.com where if you go to the risk analysis tab, it shows you kind of the status of some of these, right? And for all of these, some of the proof techniques are either in development or they're overridable. Right? So if something is upgradable without a delay, that actually means that it's overridable. So basically, every roll up that we have today is not really actually controlled by code. It's still ultimately controlled by some kind of group of humans where you have n of them and m of them can ultimately push through whatever they want, right? So this is a status quo.
01:44:35.854 - 01:45:52.838, Speaker A: And the question that I want to ask is, how can we actually move beyond the status quo? What would it really take to move us to a world where roll ups are actually trustless, or trust minimized, and where the fraud proofs and the ZK snarks that extremely smart people have been spending thousands of hours working on actually mean something? So why is almost every roll up using training wheels today? The answer is basically code risk. Right? So this is just one random sample from the GitHub repo of the privacy and scaling explorations team's ZkevM. And if you just git clone it, and then you go to the circuits repo, and then you do like find pipe, Zarg, WC minus L and fancy Linux stuff to figure out the total number of lines of code in the whole thing. You get this number, 34469. Right? So 34,469 lines of circuit code that is needed, that is in the circuits of the ZKVM. And this doesn't even get into the complexity of the circuit compiler itself. So basically, there's like a lot of black box demons hiding inside this entire ZK circuit to polynomial verification.
01:45:52.838 - 01:47:43.520, Speaker A: Blah blah blah, pipeline, right? And for simple programs, it might be possible to make some kind of proof that is bug free, right? If all you want to do is just prove that something is a polynomial, then, well, okay, fine, you can prove it. You can make kcgs and they're like simple enough, and it's probably fine, right? So EIP 4844, we can do maybe go a step further and start thinking about the shuffle proof in a single secret leader election. Okay, that looks like the circuits are getting a bit more complex, and it's like ten equations instead of one. And it's like maybe on the edge get to proving an entire evM. And it's just crazy, right? So I think 34, 469 lines of code are just not going to be bug free for quite a long time, right? And so the question is, well, if we can't make 34,469 lines of code bug free, then what are we going to do about it, right? Is there a practical near term and a medium term alternative that actually can still get us some degree of trustlessness or trust minimization. So one simple option, and I think this is an option that a lot of people are gravitating to already, is this idea of a high threshold governance override, right? So basically you have some number of guardians and you have some high threshold on it. So here you have a six of eight, you can make it be a twelve of 15, you can make it be 42 of 43, you can make it be 70% of token holders, like whatever, right? You have some kind of high threshold override where if it's very clear that some kind of bug has happened, then the guardians can override it and they can say okay, fine.
01:47:43.520 - 01:49:52.630, Speaker A: This thing that got accepted by the proving system is actually an invalid state route and the governance is going to replace it with some different state route that it decides is valid. But because the threshold is very high, it's very unlikely that it would be able to actually push through something incorrect, right? Because if you want to push through something incorrect, you would have to actually corrupt like 75% of this group of people, right? So this is one approach, right? You basically combine a proving system with a high threshold override and then you get some level of trust minimization, right? So in order for a state route that is correct, according to the code, to pass through, you only need two of three of the eight guardians, to be honest. But then in order for a state route that's incorrect to pass through, according to the fraud proof to pass through, you would need six of eight guardians to be dishonest, right? So you have some degree of trust minimization and you get to the point where the code doesn't have absolute power, but at least the code means something, right? And so the question is like, well, how long is it going to take until the roll ups that are in this room, the roll ups that are in this ecosystem will get to the point where they're comfortable at least doing this, right? At least getting to the point where they're not like purely run by training wheels, but where they're run by some linear combination of training wheels and actual code, that's an attempt to prove the EVM. So this is option one. But option one I think has a couple of weaknesses, right? So one of those weaknesses is that it still does have some vulnerability to governance, right? So the vulnerability to governance is not that high, but you can totally imagine scenarios where you actually do mess up and enough of the governors actually do get corrupted at the same time. And the system does end up actually freezing or doing something really bad. Right.
01:49:52.630 - 01:50:44.730, Speaker A: So that's one issue. Another issue is that I think in general, these kind of like community governance actors, there's a lot of kind of complexity is involved in choosing them. There's the social question of which ones different groups of people will trust, the legal question of who is going to be willing to be a governor, and what even their risk and responsibilities are. Just like a whole bunch of issues that actually come up when you try to actually create a set of guardians. Right. So I think option one, if you have to do it, then I think it's a good idea to do it. And I think it's definitely an improvement over the status quo, which is basically where you have a governance committee that generally can just override the prover and make it lead to whatever result it wants.
01:50:44.730 - 01:51:23.122, Speaker A: But ideally, it would be nice if we could have something other than this. Right, so option two, multipro. Right. So the idea here basically is that instead of having a multi sig of people, you have a multi sig of different proving systems. So the philosophy behind this should be pretty simple. The Ethereum network, to some extent, does something similar, right? Because we have multiple implementations of the Ethereum protocol. And so right now, I think both prism and lighthouse have somewhere around a third of all the validators.
01:51:23.122 - 01:52:52.978, Speaker A: And so if either prism or lighthouse have a bug but the other clients don't, then the worst case is that the chain stops finalizing for a few hours, and then it comes back to normal. And the average case might even be that the chain just actually keeps going and ignores them. Right. So multiple implementations basically allow for a much more resilient network, because if one implementation has a bug in one place, then chances are another implementation will not have a bug in the exact same place, especially if that other implementation is created by a different team that has a different philosophy and even just a fundamentally different architecture strategy. So in this diagram here, the judge hammer and the scales of justice are meant to represent fraud proofs and arbitration, as some of you maybe might have already guessed. And the spooky looking chip there, that stable diffusion generated for me two days ago, is supposed to represent a zero knowledge proving system. So doing a multi between a fraud proof and a ZK roll up is actually a really powerful idea, because fraudproof based systems and zkvms are just designed so fundamentally differently, they rely on such fundamentally different assumptions that basically the level of correlation between them is going to be very low.
01:52:52.978 - 01:54:06.022, Speaker A: Right. Basically, the only way in which you might have some kind of correlation is if either there's some kind of bug or ambiguity in the yellow paper in a particular place, or the same people are involved, or there's some really clever attack against both of them. But the bar to get there is very high, right? You're not just going to get the exact same kind of bug in a fraud prover and a Zkevm by accident. And then even within fraud provers, there's like, a bunch of different approaches, right? So one approach, for example, is that you make a fresh new, like, basically a new implementation of the EVM that's designed around proving one specific computational step. Another approach is you compile the guess source code into some minimal virtual machine, like mips, for example, and you then make a fraud prover for mips, and you just push the entire guest code through that, and you just create a fraud prover of mips and have everything go that way. Right, but instead of guess, maybe you could stick Aragon in there, or maybe you could stick another mind. Or maybe instead of mips, you could use some different machine, right? So there's a lot of different ways to make a fraud prover.
01:54:06.022 - 01:54:40.426, Speaker A: There's also different ways to make a Zkavm. Right? So the pse Zkavm is kind of a direct compilation. The Polygon Hermes team, I believe, is doing this clever thing where they first compile the EVM to an intermediate language, and then proving that intermediate language only takes, like, I think, about 7000 lines of code instead of 37,000. Or maybe. Sorry, it's like the representation of the EVM in their assembly is what takes 7000 lines of code. Right. So there's different approaches, and then there's the ZK sync strategy of just going straight solidity.
01:54:40.426 - 01:55:52.570, Speaker A: So there are different ways to architect these systems. And if you have three different proving systems that are architected very differently, then you might have a lot of redundancy. So this is another approach. Option two, b, more complex variants of the multiprover strategy, right? So one idea is kind of self canceling, right? So if someone submits two conflicting state routes to one particular prover and both state routes pass, then that prover gets turned off, right? So the idea here is basically that if some prover is able to accept multiple state routes, then clearly something is wrong, right, because it's saying yes to two conflicting outputs. And so you shut it off, and either you reduce the size of the multisig, or you replace it with governance that has to choose a different prover. So that's one approach. Another approach is that if no successful message gets passed through a particular approver for seven days, that prover is turned off, right? So if the prover is deadlocked, if the prover ends being unable to accept even things that are valid, then you can shut it off too.
01:55:52.570 - 01:57:28.694, Speaker A: So one of the interesting things about these two ideas is that they're actually kind of inspired by smart contract wallet designs, right? So the concept of self canceling, that's a very close parallel to this concept of vaults that I think Emian Gruncier and a couple of other people were really promoting a few years ago, right, where basically you have a smart contract wallet where you can initiate a withdrawal, but then that withdrawal takes 24 hours to finish. And before those 24 hours come, that exact same key can cancel the withdrawal, right? So the idea basically is that if you get hacked but you still have your key, then you can constantly prevent the hacker from actually taking the money. And then there would be some third override key where if one key clearly keeps canceling itself, then that third key with maybe a one week delay can actually take funds out. Right? So basically this approach is like that exact same idea, except for a personal. Instead of being applied to a personal wallet using private keys, it's applied to a roll up using multiple provers. And then the second idea, basically it's like social recovery for provers, right? If approver is clearly not able to do something, then some other mechanism can switch it, right? So you can actually do some surprisingly interesting and clever stuff here. So this gets us to a third technique, right? So this is a two prover plus governance tiebreak.
01:57:28.694 - 01:58:26.490, Speaker A: So we're going to make two provers and we're going to make them very different. So one of them is going to be ZK and one of them is going to be optimistic. And we have a two of three mechanism, right? Basically. So we have a two of three between the ZK, the optimistic and governance. So there's actually a bunch of different ways to architect this, right? So one of them is the thing that I mentioned on Twitter about a month or two ago, basically the idea being that when you submit a block, there is a 24 hours time window, and after that 24 hours time window, the block gets accepted if there is a snark. And within that 24 hours time window, if someone opens up a fraud proof, then the fraud proofing game and the snarking game, they both run. And then if they agree, then whatever they say is accepted as the result.
01:58:26.490 - 01:59:08.410, Speaker A: And then only if they disagree, the governance has to come in and provide the correct answer, right? So if you want to take an approach that minimizes the governance's role and makes the governance be a more emergency thing, then you'd probably want to do that, right. You'd want to create a system where by default you want to have a kind of two of two between the optimistic and the fraud, between the ZK and the fraud. And the way you do this is by having a time window. And because you have a two of three, you can be more aggressive. Instead of seven days, you can make it 24 hours. And you can say for a block to be accepted, it has to both have a snark and have a 24 hours window pass to make sure fraud proofs didn't come in. Right.
01:59:08.410 - 01:59:55.446, Speaker A: And then if they disagree, then you do some governance thing. But if you are okay with governance being kind of more regularly active, then there is another approach which is for every state route that gets submitted, you just let all three of these games run. And then as soon as two of these games accept a block, then that block gets accepted. Right. And so in the happy case, blocks would actually succeed basically immediately, right. Because when you have a block, then a ZK proof passes for that block and the governance accepts a block and you have a proof, and then the block is finalized within less than an hour. Basically your only limit is going to be how quickly you can make ZK snarks.
01:59:55.446 - 02:00:15.374, Speaker A: Right now it looks like ZK proving EVM blocks is like somewhere in the hours. But in the future, I have a lot of faith in you guys. The technology will improve and we're going to get ZK snarks in 12 seconds. Right? Right. Okay, so there's different options. Right. Is basically what I'm saying.
02:00:15.374 - 02:01:07.550, Speaker A: There's like this large design space of different options that you can choose different trade offs that you can take different depending on how much you value speed versus how much you value minimizing the role of the governance versus a bunch of other considerations. There's a lot of options that you can make, and it's probably worth thinking really deeply through these different options and figuring out which one of these actually makes sense. Right. So advantages of this approach, right. Is that it actually combines all of the advantages together, right. So for this approach, you don't have to trust the governance because even if the governance is completely evil, even if seven of seven get corrupted, it can't contradict the provers. If the provers agree and you're protected from a bug in either one of the two provers.
02:01:07.550 - 02:01:41.738, Speaker A: And ideally if the provers have a very different construction, then the chance of the two having simultaneous bugs is going to be very tiny. Right. So that's the advantage of this kind of design. One other interesting thing that's worth talking about here is what does the code look like for the multi aggregator? Right. Because the goal of this is to try to minimize the number of lines of code that you have to certify. Yes, this is definitely bug free. And if it's not bug free, people are going to lose eleven and a half billion dollars.
02:01:41.738 - 02:02:18.802, Speaker A: Right. So you don't want that to be 34,416 lines of code, but maybe it has to be 100 lines of code, maybe it has to be 200. You want to minimize that as much as possible, formally prove that as much as possible, coordinate on using the same code as much as possible. Right. So the other question is how we minimize the multi aggregators themselves, right. There's a lot of different possibilities. One interesting one is that you just literally use agnosis safe wallet, right? Like you just literally throw coins into a gnosis safe wallet where you have three different keys that are owners.
02:02:18.802 - 02:02:53.666, Speaker A: It's just a plain old two of three agnosis safe and the three wallets just are. One of them is itself agnosis safe of four or seven guardians. Another is an account that pushes through a message if a snark tells it to push through a message. And a third one pushes through a message if it tells a fraud groover to push through a message. Right. So if you do that, then you can even reuse existing code for the thing that actually does the aggregating, which I think is really cool and I think really does reduce the surface area of code that you have to trust unconditionally by quite a bit. Right.
02:02:53.666 - 02:03:26.522, Speaker A: But these are all also things that are worth thinking about. Right? So conclusions here. So I think the big one is that Zke evms are not going to be bug free for a long time. Right. And this is something that's probably worth internalizing and accepting. Right. Basically what's amazing about the Zka space is that I think it's the one part of the crypto space that actually has exceeded expectations in how fast things are going to come.
02:03:26.522 - 02:04:06.650, Speaker A: Right. You got like the merge and it's like, oh, it's going to come in 2015, oh, 2017. And oops, it came in 2022, but then we got zkevms and it's like, oh, they're going to come in 2030, maybe 2027. And then, oops, we have prototypes in 2022. Right? So that's like the good news about zkevms but the bad news about zkevms is that I think we're going to have this long period of time during which they exist, but they're like fairly untested. We don't know if there's bugs in them. There might be bugs in some scary proof systems, there might be bugs in circuit compilers, there might be bugs in the ZKVM code itself.
02:04:06.650 - 02:04:53.046, Speaker A: And so for some number of period of time, which I think will easily last quite a few years, we are going to have these kind of circuits that we trust to a high degree, but we don't trust completely. Right. And the fact that we trust them to a high degree means that we should use them. Right. And it means that we should actually get the benefit from them and not just create systems where we're giving lots of power to a multisig. But the fact that they're not perfect means that we also have to compensate for the possibility that something about them actually breaks. So with multiple implementations, with governance backups, with multiple implementations and governance backups, we can minimize the chance that bugs are going to actually lead to catastrophic outcomes.
02:04:53.046 - 02:05:50.502, Speaker A: There is a trade off space between security versus bugs and security versus bad governance. Right. And I think this year everyone's optimizing for security against bugs, which is probably correct ten years from now. I think everyone should be optimizing for security against bad governance, which is probably going to be correct then and between now and ten years from now. We should kind of slowly move that slider from kind of trusting governance more to trusting the code more as the code becomes more trustworthy. So I think keeping governance involved is a good idea, but it's also a good idea to keep it only involved in emergencies. Now, this is intended to be about kind of a talk about layer twos, but I think one other interesting thing that's worth mentioning here is that there is also a layer one angle to this concept of ZK multi proving.
02:05:50.502 - 02:06:20.434, Speaker A: Right. And the issue here is we want to use ZK vms on layer one. Right. My vision for the future of running an Ethereum node is basically that you should not need to have a piece of fancy hardware. You got your phone, you should be able to have a full Ethereum node staking a million dollars of eth if you want to, running on a phone. And how do you validate an Ethereum block? You get an Ethereum block. That Ethereum block might contain three and a half megabytes of data.
02:06:20.434 - 02:07:06.290, Speaker A: Okay, fine. And a snark, right. Download the three and a half megabytes of data, hash it, stick the hash into a public parameter, verify the snark. Done. Block is correct. It would be lovely if we could get to the point where verifying Ethereum blocks and running an Ethereum full node is as kind of simple and low resource and decentralization friendly as that, right? But in order to get to that future, we need to have a snark that can verify everything, right? We need to have a ZK EVM and a ZK Ethereum consensus layer, and probably recursive ZkzK and like ZK everything. And we're basically going to have to trust everything to a bunch of polynomial maths some weirders in universities invented.
02:07:06.290 - 02:07:49.214, Speaker A: Okay, not everyone's in university. Dropouts contributed a lot to the ZK space. Three cheers for dropouts are contributed to polynomials. Yay. But if we want to actually get there on layer one, then we're also going to go through this period of time where we can't trust one implementation to be infallible. And so then the question is, well, what would a multiple implementations vision for ZK snarks at layer one actually look like? And I think here there's some interesting answers, right? So one possibility for this is that, okay, we have different clients. We have prism and guests and lighthouse.
02:07:49.214 - 02:09:16.558, Speaker A: We have five execution clients. We have five consensus clients. And maybe we're going to also have five ZkeVM engines, right? And so instead of there being 25 client combinations, we're going to go up to 125 client combinations, which makes Ethereum five times more diverse and secure. So then the question is like, okay, somebody creates a block, and then the peer to peer network is just going to generate a proof for a block of each type, right? And like, okay, maybe you have a node running ZkevM engine a that created a block, but then because the data is all in the clear, once it gets out there, someone else can come along and they can make a Zksnark that is compatible with ZkavM engine B. And then people running engine B on their clients are going to be able to verify it. And so you are going to be able to basically get the same benefits of client diversity. But in this ZKVM world, right? Then the question is like, well, how do we actually get there? What is the first part of the Ethereum consensus that we actually are willing to ZK? Is it actually going to be possible to ZK proof things quickly enough? There's a lot of these different issues, but I think realistically, some kind of multi proofing future like that, and possibly even a hybrid future, right? Possibly even.
02:09:16.558 - 02:09:56.298, Speaker A: We might even see a future where, for example, all the institutional stakers still run regular phone odes, but some home stakers run a couple of experimental ZK provers. We might end up going through a couple of those phases. But I do expect that some kind of multi proving and hybrid proving system is going to be the future on layer one as well, and not just layer two. So thank you. Oh no. Did I miss the slide? No, there's a happy giraffe there. Okay.
02:09:56.298 - 02:10:29.400, Speaker A: Yeah, there's a happy giraffe. Thank you so much. Now, Vitalik has been speaking about it. The previous panel on public goods talked about it as well. The very first panel on roll ups mentioned it, and the very first speaker of this morning or afternoon talked about it as well. And here it is, the 4844, also known as the triple four EIP, presented to you by Mophie. Welcome up.
02:10:29.400 - 02:12:33.550, Speaker A: Give people time to settle down. Okay, Eip four four four. Let me start with a quick introduction. So I'm Mophie, also known as ad infi, a discord telegram. I work for Op labs, which is the engineering arm of optimism, and basically here to tell you about the future of roll ups. So what's this talk about? We're going to go over the concept of data availability, modular blockchains, EIP four four four, of course, and how that fits into Denk sharding, and give you a little quick update on what the status of EIP four four development is currently. So, data availability, this is kind of like the problem of having data in your network and making that data available to users, in the sense that once you post that data, you should be able to trust that the data will exist and be around the network for some amount of time.
02:12:33.550 - 02:13:38.432, Speaker A: This is actually super related to roll ups, because one of the main bottleneck of roll ups is the data that we post back to l one. And right now we use call data for that, which is expensive. But if we can solve this problem, scale out the data we're posting back to l one really cheaply, then the scaling of roll ups will follow as well. So that availability is also related to the execution in the sense that this data that we're posting back to l one, for the perspective of a roll up, we consume that data to derive the chain right. And so the execution outputs are also like an LT concern. So a roll up, in a nutshell, is the data and the execution check. It doesn't matter what kind of roll up it is, whether it's optimistic or ZK roll ups, you can pretty much break it down into these two key intrinsic properties.
02:13:38.432 - 02:14:37.850, Speaker A: So for the datability, like I mentioned, the data that you're posting back to l one, you need to be able to derive it, you need to be able to consume that data to derive the roll up chain. The execution check is the process of actually checking that the data that was posted matches what you expect for the roll up. And so in the case of like a Zk roll up, it's just a validity proof. For an optimistic roll up, it's a fault proof or fraud proof. So for data availability, I guess you could sort of think about it like data forever available or data once available. Whereas in the former case, the data you're posting back to l one, it needs to be always there or easily reconstructed and trusted forever. And I'll basically tell you how we're able to accomplish this with EIP four four for data once available.
02:14:37.850 - 02:15:53.664, Speaker A: This is like the minimum requirement for most roll ups whereby they have like a settlement period, at least for optimistic roll ups, whether it's like two weeks or whatever. If the data is available for longer than that settlement period, then you basically can trust the roll up because you can always derive the state from the data. And this ties back to the execution check I mentioned. The point of that data is so that we can challenge any bad sequencer, which is the entity that's like building your roll up chain and posting it back to l one. So let's do a quick segue into modular blockchains. So Ethereum actually has become quite modular, particularly since the merge that's just happened. And why is this useful? Well, it lets us pretty much encapsulate feature sets and complexity into one thing that's easy to reason about, and then keep things completely separate so that we can scale one thing without possibly introducing complexity in another thing.
02:15:53.664 - 02:16:54.950, Speaker A: So it's a way to deal with cyclochomatic complexity overall. So let's go over the sort of future designs of scaling. In ethereum today we have like an l one, which is like a monolithic chain. And then sometime a couple of years ago we had like the beacon chain, which as you've noticed, it's quite a modular addition to the existing execution chain. So we have the EVM, right? And before we had the proof of work, which was used to secure all state transitions in the EVM. But then with the merge, we introduced the concept of a beacon node, and what that does is decouple the proof of work aspect of the execution, and let that be handled by the beacon node. So all the beacon node needs to do is interact with the execution chain, the EVM, to determine which blocks needs to be proposed and built.
02:16:54.950 - 02:17:34.320, Speaker A: So, again, this is reiterating the point. We introduced the proof of stake on top of the EVM, and this was. I say it's easy, but it took a lot of engineering years to do this. But it's easy to reason about, because all the stuff that we built on the proof of stake chain, the beacon chain, does not really affect the EVM that much. All the proof of stake is just proposing, like, blocks, right, and asking the EVM to build it. So it's an example of, like, a clean, modular architecture. We can expand the evm if we need to, we can scale it without affecting the proof of stake and vice versa.
02:17:34.320 - 02:18:50.780, Speaker A: Okay, so how does this look like for an l two? That concept of modularity is also pretty useful because you have an l one execution engine, and in an l two roll up, optimism, zk sync or whatever, it's basically just interacting with the l one chain to figure out how to derive the l two chain, right? And then once it's done with that derivation, it just posts back all the transactions that occur on the l two chain back to l one. And so there's this like this cycle where transactions are consumed from l one. They get moved to the next state on l two, and then we posted back to l one. So this is kind of like how it fits in l two s. Today, you have the proof of stake at the very top, the evm. And then l two s, they don't really need to interact with the proof of stake, right? Because we've already made things modular enough that all an l two needs are the state that the EVM exposes. Right? So you have multiple l two s interacting with the same l one EVM.
02:18:50.780 - 02:19:57.312, Speaker A: And again, we can verify execution between the EVM and l two via either fault proof or the validity proof, the execution check that I mentioned. Okay, so how does this look like in other blockchains, like Solana or Avalanche? Well, they introduced the concept of a data layer. And what this layer does is it provides like a common interface for bytes data, if you want to call it that, that several different subchains could share. And it looks nice at first, because solving for data is pretty important for scalability. But the problem with this is that now that you have to break up execution into different subchains, and you lose some security when you do it, that way. Ideally, we want to have the EVM and the data being handled by l one. Even though we do it modular, it doesn't matter.
02:19:57.312 - 02:20:54.050, Speaker A: But if we can have our cake and eat it, then everyone will be smiling, right? So we sort of solved this problem of like, bundling data and execution on the same chain via EIp four from four. It's also called protodank sharding because it was like being specked out by Protolanda, also another guy from op labs and dank card from the EF. So you combine them, you get protodank sharding. Not very creative, is it? So eip four four, what does it entail? Well, we need a KCG ceremony. It's a hard requirement for EIp four four four to work. We need new BLS libraries to implement new KCG crypto cryptography. I'll explain what KCG is in a moment.
02:20:54.050 - 02:22:05.288, Speaker A: We have a development Devnet that's running, implementing a prototype of EIP four four four. And we also have a consensus specs. So this is actually like the first time again, maybe the merge is one other time, but first time post merge, whereby an EIP really has like a dependency on consensus and vice versa. So with EIP four four four, this is sort of how the picture of a modular, ideal, modular blockchain would look like, right? We have like the l one EVM, which we all trust, and it's secure. We add a data layer, right? Some people call, dub it the byte space or blob space. And what this gives you is the security of the execution and the assurance that the data that's available on l one is expected and can be used to reconstruct the state. So l two s.
02:22:05.288 - 02:22:57.992, Speaker A: How would l two s use this? All l two s needs to do is basically attach to the data layer whenever we're posting back l two outputs. Because remember, if we go back a couple of slides here, when a sequencer derives the chain from l one and then applies more transactions in l two, that data that's posted back to l one. It doesn't have to be like l one call data. It can be any l one call m data provider. So what we're doing here is replacing the call data that was in the evm with a new layer. And the idea is that this would make things so much cheaper for lts. Okay, so the data that we're posting, we refer to it as a blob, right? And this is just think of it as like call data.
02:22:57.992 - 02:23:27.300, Speaker A: It doesn't really matter what it is. This blob kind of like, it goes to like a lifecycle that this slide kind of summarizes. So the l two transaction that a user posts. Well, a user has a user to interact with. L two like optimism. It generates a transaction at a certain period. An l two sequencer or a roll up operator will bundle up several of these transactions.
02:23:27.300 - 02:24:12.320, Speaker A: And then we post these transactions to l one. Right? Previously. Well, today we post those transactions as call data. The ideal is that we post these transactions as a blob. And the way we do this is by introducing a new type of transaction called a blob transaction, similar to the way we introduced the dynamic fee transaction for EAP 1559. There would also be a new blob transaction for 4844. This transaction, it looks like a regular e one transaction evm transaction, but it also adds, like, additional data that lets you post l two batch data at the bundle, separate from the call data.
02:24:12.320 - 02:25:07.620, Speaker A: And due to pricing mechanics, it can be really cheaply to do so. Okay, so this blob data eventually ends up in the beacon chain. It's important to note that the EVM does not store blob data. The storage is actually being handled by the beacon chain. So going back here, the slide's kind of misleading, because these blobs are sort of like part of consensus, but it's easier to reason them as like a different data layer for various reasons that I'll get into. And then from the perspective of NL two, if we need to derive the chain, all we need to do is find a beacon client, retrieve the data that we just posted, and then we can just derive the chain that way. So this is another diagram that sort of, like, shows the whole workflow.
02:25:07.620 - 02:26:02.230, Speaker A: Basically same thing as you said, you have like an l two sequencer takes transactions, rolls them up into a roll up, post batch data into an l one. The l one takes those transactions as is building a beacon block, provides those transactions to the beacon chain. The beacon chain, it gets proposed that data gets gossiped throughout the network, and then an l two verify can take that data and derive the chain to get to the exact same state that we had. Okay, so going to some details about what a blob transaction is, like I mentioned, it's similar to an EIP 1559 transaction. Actually, the pricing mechanics is very similar, like the way EIP 1559 floats with the base fee. It's also similar here. Took some inspiration there.
02:26:02.230 - 02:26:49.540, Speaker A: And the important thing to note is that the blobs, they're completely separate from the actual transaction body. And the way we're able to do this is that a blob transaction sort of like, has two variants, right? When it's in the mem pool, it contains all the blob data. But once it gets included into a block and it's part of the state tree, we sort of like, strip off that blob data and make it available to the beacon chain. Because like I said, the EVM does not store blob data. That storage happens in the beacon chain. So it basically looks like an old dynamic features action, but we add a couple of new fields. One thing we add is the blob kcgs.
02:26:49.540 - 02:27:49.370, Speaker A: Kcgs in general, what they are, they're basically a commitment to a piece of data. So think of it as like mercury proofs, except that the proofs, the commitments, are very succinct. Kcgs are always 48 bytes in size, no matter how large the data is, which is pretty important property to have, because this blob KCG is going to be part of the beacon block, and we need to be able to easily distribute that data without having really large proofs. KCG commitments also let you prove the valuation of a single point. So say you have, like, the blob data, and you want to prove that blob data at a certain index has a different byte pattern. Well, you can easily do that using blob kcgs as well. Again, it's very similar to the way, like, mercury proofs work.
02:27:49.370 - 02:28:39.928, Speaker A: And one last thing. We added a new field to the transaction called the blob versioned hash. What this is, is just a hash of the KCG commitments. The reason why we're using a hash is because it makes it easier for us to compatibility. Basically, if we decide to use a different commitment scheme, then we would just change the way the hash schema works and a different hash can be used there. All right, and one last thing. So a blob is basically a set of 40, 98 field elements.
02:28:39.928 - 02:29:26.172, Speaker A: And these field elements are basically just points on a BLS curve. In particular, for this EIP, BLS 380, 112, which means the total size of a blob is 128 kb. So with a single blob transaction, you can store 128 kb. That's completely separate from call data, and it does not get added to the EVM, but only exists in the beacon chain. One more thing worth pointing out is that since blob data is separate from call data, we need a way to price it. And the fee market structure we came up with is introducing a new type of gas called data gas or blob gas. But Blob gas, data gas doesn't matter.
02:29:26.172 - 02:30:19.244, Speaker A: And this data gas behaves similar to regular gas. And the semantics is very similar to 1559 in the sense that we have a fixed target for data gas for the entire block. And if we notice that we're exceeding that target, then the gas price fee, the data gas fees go up, and if we're below the target, it goes down. The entire point is just a control system to keep data gas at a certain level so that we don't overburden the network with like huge blobs. To accommodate this, we added a new field to the block header in execution. We call it the excess data gas fee. Honestly, this approach is something we also could have adopted for EIP 1559, but maybe in some future upgrade we could also do the same thing.
02:30:19.244 - 02:31:25.160, Speaker A: This field makes it easy to compute how much we are off of target for the data gas. And for various reasons, it's also very nice to implement rather than the way EIP 1559 works with the base fee. So again, there is no blob content in the EVM. All the blobs are stored in the beacon chain. So this is kind of like how it looks like you wrap, like a regular transaction version hash that wrap data is the blobs, but that only exists in EVM while it's in the mempool, but it's not included in the beacon blob. Okay, so how does the beacon chain interact with this blob? Transactions or new header, whatever. The way we kind of like, have it set right now is to introduce a new execution, sorry, engine API.
02:31:25.160 - 02:32:29.150, Speaker A: So the way the engine API works right now is you can propose execution blocks to the EVM, and then you can update for choice. What we're doing is adding a new function that lets you get the payload in addition to getting the blobs bundle. So what does it look like? So remember, if you go through the whole process of a validator proposal, right, it needs to make a request to the EVM to get the blocks that it built, and then it adds those blocks to the beacon chain. Well, the headers to the beacon chain. So similarly, we need to make a request to the EVM, the execution to get the sidecar or the blobs. Those blobs are packaged into this new data structure called a sidecar. A sidecar is just a collection of blobs, and for various reasons, we don't want to add that to the beacon, sorry, to the execution header, because it will bloat the header more than it already is.
02:32:29.150 - 02:33:58.516, Speaker A: Okay, so how does a roll up take advantage of all of this? That's the entire point of this, right? So first of all, for a Zk rollup, and bear in mind, this is a very simplified model of Ezekiel roll up. All EZK roll up needs during its execution and the settlements in l one, it needs to figure out the data that it has containing all that state. It needs to be confident that the data is what it says it is. And the way it does that is by querying the EVM, given a blob index, a KCG point proof, and some other Zk related proofs, get the actual version hash that's associated with that blob index, and then we can use that version hash and be confident that the blob data that we provided to the contract is correct and it's not being spoofed or anything of the sort. There's one tricky thing about with ZK roll ups is that they may be using a different commitment scheme other than kcgs. They may be using ipas or whatever. But there exists a morphism between any given ZK commitment scheme and a KCG commitment scheme, and that morphism is basically the proof of equivalence.
02:33:58.516 - 02:34:48.136, Speaker A: So with the proof of equivalence, you can take the kcgs and do some crazy crypto magic. Honestly, I don't completely understand this part. But with that, you can be confident that whatever commitment scheme you're using for your Zk roll up will be compatible with the KCG commitments that are in l one. And so if we go through here, we added a new opcode called the point evaluation recompile. What that does is takes the version hash that we just retrieved, a point on the blob, what its value should be, and approve. Right. And the opcode will basically, in the EVM, check that the point that you requested matches the value that you expect.
02:34:48.136 - 02:35:43.444, Speaker A: And this is a way to check during your validity proof that a user that's posting data for the proof is not forging blob data. This is all relying on the security of l one Kc commitments. So again, this is going forward. Again, proof is the same as Zk data. We use the pre compile, we can check like multiple points in our blobs, and then for every point that we need during the validity proof, we just use the pre compile, check that it evaluates the right value, and then go forward from there. Okay, so for interactive fraud proofs, how does it look like it's quite different. You have like a pre image oracle.
02:35:43.444 - 02:36:46.892, Speaker A: So again, this is like with the perspective of a fault proof whereby you do this bisection game, dispute game, whereby there is a contract and you have like a challenger and someone disputes data. The challenger disputes data that was posted on l one. So it needs to interact with this contract to figure out whether the data, the dispute has any merit or not. And so here the period image. Oracle doesn't actually know what the blob data should be. All it has access to are the version hashes, which ergo are tied to the KCG commitments. So as a challenger, your job is to provide that blob data, right? And the premium Oracle can just check via l one that the blob data is correct, do the point evaluation pre compile similar to the ZK roll ups case, and trust that the blob data you provided is correct.
02:36:46.892 - 02:38:02.340, Speaker A: And then it can do the state transition itself to verify the proof and the update vm memory. This is in the case of what Vitalik was mentioning earlier, whereby you run your state transition inside of another virtual machine, whether it's mips, but you can kind of ignore that for the purposes of the interactive proof. The main point of this is that you're providing data to the premature oracle. It knows that it can trust the data using l one, and then it can do the stage resistion from there. Okay, so dank sharding eip four four four is sort of like a precursor to dank sharding, hence the name proto dank sharding, in the sense that for full dank sharding, and to recap like dank sharding is the long term solution for Ethereum data availability and data scaling in Ethereum. But one thing we definitely need before we even implement this for dank sharding is the ability to post data back to the beacon chain. And so that's what eip four four four does via blow up transactions.
02:38:02.340 - 02:38:54.528, Speaker A: By introducing blobs, we can build the necessary requirements for full data availability. Actually, for this EIP, data availability sampling is not quite implemented. We just make the data available so it's not quite as efficient as full dank sharding, but we'll get there. And the idea is that we introduce blob space of like one megabytes per second. Actually, that should be per slot, not per second, unless this s refers to slot. So every slot we introduce 1 blob data is basically what roll ups would use to post all their data. So once we do this for full dank sharding, we don't need to touch ed EVM anymore.
02:38:54.528 - 02:39:57.020, Speaker A: Like, we're done. All the changes necessary for full dank sharding would occur on the beacon chain. So this is kind of like how it would look like with full dank sharding, you have, like, the EVM with multiple data shards, and l two s could just plug into any one of these data shards to get the data they need to derive the chain. Okay, so where are we at with the development of this? So, we've been hard of work for almost a year now, just trying to get EIP four four into fruition. And that entails, like, building prototypes, doing workshops, like updating the specs, making optimizations, and building in Devnets, as a matter of fact. So this all started super early this year at Denver Proto Lambda, another OB engineer labs engineer was working on this with the EF. There was, like, an original prototype type.
02:39:57.020 - 02:40:47.284, Speaker A: And then this summer, we really started ramping up development on EIP four four four. And by Berlin, we had our first devnet. The Devnet had, like, a very simplified fee model structure, but it sort of, like, demonstrated that vip four four is feasible and client team should definitely look at it. And now today at Devcon Pogata, we have a new Devnet coming up. This devnet will implement the full fee market spec and all the other consensus related changes we've made since then. So this is sort of like a summary of all the work that's gone into EIP four four implementation wise. We have new specs, execution specs.
02:40:47.284 - 02:41:11.388, Speaker A: We have a geth prototype. We have a prison prototype. Shout out to Coinbase for actually helping us with this. This is a collaborative effort between op labs and Coinbase to build clients that are compliant with EIP four four four. We have new execution API specs, though that's kind of, like, stale right now. And the KCG ceremony. So let me quickly go over the KCG ceremony.
02:41:11.388 - 02:42:14.528, Speaker A: For AIP four four to work, we need what is called, like, a trusted setup. And what a trusted setup is, is basically a way for multiple participants to derive a value that is like a secret and can be used for KCG crypto. The idea is that no one knows what the secret is as long as the trusted setup was executed correctly. There's a team in EF that's hard at work on the KCG ceremony, but hopefully, by the time we ship EIp four four four KCG ceremony is done and we can proceed from there. All right, so what's left to build first is, like, a proof of concept that a roll up can take advantage of. EIP four four four. And observe that we are actually saving a lot of gas using blob transactions rather than using call data.
02:42:14.528 - 02:43:25.470, Speaker A: So for optimism, in this case, it's actually kind of like easy, the bedrock architecture, which is like the next upgrade for optimism. It's a very modular architecture whereby all the steps to derive a chain are completely partitioned. So all we need for EIP four four is to replace where we get l one data, which is currently today, the call data, to just use the data availability layer. And this is sort of like another block diagram that summarizes how that would look like, again with the L two node, rather than interacting with L one. To get that call data needed to derive the chain, it interacts with the beacon node at the very top. So that sequence of data just gets fed into the roll up node, and then we can end up with the same state. So the devnet, we've been doing a lot of benchmarking for the Devnet, making sure that the EIP four four four does not introduce any new DOS vectors or issues.
02:43:25.470 - 02:44:16.472, Speaker A: And we've been getting pretty confident at that. There are still a couple of open issues regarding how do we sync blob data in the beacon chain that we're currently looking into. And of course, last thing is, once the KG ceremony is done, how do we integrate that into EIP four four four, in the sense that how do execution clients, beacon chain clients, take advantage of the ceremony output? All right, so what implementations do we have? Like I mentioned, we have like a ready prototype for GEF and Prism. For nethermind, they have interest in implementing EIP four four. They have an issue open on the repo Teku. They also have a lot of interest in it. For Lighthouse, they've already begun work on a prototype.
02:44:16.472 - 02:45:15.110, Speaker A: Still work in progress, though. I think it was like at East Berlin. Some folks in Lighthouse, including like a geth core dev, started working on a prototype, and they're getting there. So for EIP 44, the best resource really to learn more about it is the website EIP 44. Com. There's also a hack MD written by protolambda that summarizes all the different spec changes we've made over the years, because right now it's kind of hard to follow yet before development, because we have the execution layer specs, we have the consensus layer specs, and those are not always in sync. So this meta link in the hackmd should really help you figure out where the current status of the specification is.
02:45:15.110 - 02:46:11.896, Speaker A: And. Yeah, that's it. Any questions? Yeah, sorry, could you repeat that. Hello, I know the plots get deleted after one month, but can you always prove the KCG commitments in an ethereum transaction using that opcode? Past one month has been yeah, so like I mentioned, even the opcode doesn't access the blobs. You have to feed it. The blobs and the KCG commitments via the version hash are in the EVM even way past the one month period. So you can always prove it.
02:46:11.896 - 02:47:35.540, Speaker A: One possible idea that you could see in the future is that we could have someone create like a bitturn of all the blob transactions that are older than a month and then you can still provide that to the EVM to verify that they are correct. Yeah, go ahead. I'm curious how you see that after this proposal has been implemented passed. How does it affect other data availability solutions out there that are like no settlement, just da? Yeah, I think it reduces their use cases by a lot because prior to eip four four four there was talks about roll ups, taking advantage of them and re implementing actually a lot of the KCG commitment stuff as a smart contract. But if we can enshrine that in l one, we don't really need all of that. But they would still be useful, like in the bittorrent case whereby they would provide long term access to blob data and there could be some applications, maybe nfts with IPFS hashes or something that might find it useful to have a commitment in l one that they can always use to reference long term blob data. Thank you so much Mophie.
02:47:35.540 - 02:50:21.034, Speaker A: Give it up for Mophie. Now, given that we are slightly behind schedule, we will start the next panel on EVM and how much EVM is enough? Five to three. That means we have now another 15 minutes of a break. Thank you. Five to four. We will start in 20 minutes. You say hello to motionarade.com.
02:50:21.034 - 02:59:22.796, Speaker A: You can take your videos from good to the best and easily be better than the rest. Giant fire shot your rocket titles and shows. Go get it. 5ft of drag it windows jacketo perovisto intramonde punto.com m pesara preparate wow emaravija no estomas alone it satan 88 want to know how to create awesome videos? Check out motionarray.com. You can do it too. Titles easy intros easy kittens bridges glitches so easy heartbeats lightning jazz of hop fast and simple drag and drop I'm in love it feels so cheesy video creation can't believe it's so easy.
02:59:22.796 - 02:59:54.250, Speaker A: Music colors tempered too. Every day there's something new. Music clip ads easy. Social stories so easy. Player up, play cool, dive into the motion pool. So go to motion array now to get everything you need to create stunning videos. Todo Salman laso intramonde.
02:59:54.250 - 03:11:45.130, Speaker A: Say hello to motionarade.com. You can take your videos from good to the best and easily be better than the rest. Giant fire shot your rocket titles and ensures go get it. Five bit of that just drag it window generate the festival jacketo pero vesto intramonde puntokomanue formal wow. Make me want to be a better man. Los costos song mas bacos de lo queue de simachinar escomotena quinta bancaria local paracada Rejoncon Lake trabacas itambien puerto recipient siesta sena medicalatina okwalkir parte del mundo trabaca global mente cintiendo de locale sar maybe I can attract a few people from outside to come inside. We're going to continue with the program in like three minutes.
03:11:45.130 - 03:12:34.010, Speaker A: We're going to start with our first panel after the break about how much Mev is enough. I know that everybody is talking about it. Since ECc pairs this year, we have all the speakers already here and I think togral is chasing a few in. Okay, we'll wait like two more minutes and then we'll come h you will be, yes. No. Hi, I'm Patrick. They're it.
03:12:34.010 - 03:13:16.070, Speaker A: Okay. All right. Okay. Yeah, I think so. I mean, as far as sign with that's going to be available like online afterwards and I think everything will be there. Okay, and how long is the panel? This one? We are a tiny bit late. Yeah, I know it's a tiny bit late, but it's like half an hour or 40 minutes.
03:13:16.070 - 03:14:10.790, Speaker A: I think it was very, like one more minute to get like a few more people in and then we'll start, if that's fine for you have. How large is your. Yeah, I know Holly can. Okay, maybe just start and then. Or do you want to wait a bit longer? Yeah, it's for a doctor. Okay, we're going to start now. The panelists are really slowly walking on the stage, so you will have some time to come in.
03:14:10.790 - 03:14:50.624, Speaker A: So the next panel is, as I said, how much mev oh, no, that's the next one. Evm is enough. It will be moderated by Bartek and we'll have Jordy, John, mark, and yi on the panel. And, yeah, maybe we can just start out I can go outside again. Yeah, I think. Yeah, it doesn't matter. I mean, just go ahead.
03:14:50.624 - 03:15:49.760, Speaker A: Maybe I want to have job. Thank you. There's been drinking way too much for this. Does this is. It's a nice load. So I call this company, but yeah. Speaker reception.
03:15:49.760 - 03:17:14.000, Speaker A: Livestream eight. Yeah. Decent turnout for this later. It's flashbox also match. So how spicy do you want it to be? Real spicy. Okay. I'll do my best.
03:17:14.000 - 03:18:04.850, Speaker A: What did you. A door is closed. All right. Thank you very much. Thank you, everyone, and welcome to one of the three panels that we've got today. We've got a little competition. Which panel is going to be the spices? So we'll see how we go about this one.
03:18:04.850 - 03:18:31.610, Speaker A: And I'm Bartek Kipushevsky. I work for Dao. I'm primarily responsible for the multi chain strategy at the Merkeydao, and I'm literally the client of all these four teams, more or less. Right. So they need to convince me that it makes sense to deploy Mercadao on their system. I hope. And so maybe let everybody kind of introduce themselves.
03:18:31.610 - 03:19:15.432, Speaker A: Which projects do you work for? Obviously. And I want one sentence stance on how important is the EVM equivalence to your particular project? The importance of what? How important is the EVM equivalence to your particular project? Just one sentence stance. Right. And we're going to go deep for us, developers should not notice the difference between deploying or working in Ethereum or working in Zikavm. Okay. For us, I think we sacrifice a lot of efficiency just for developer experience to achieve EVM bico level equivalency. So we sacrifice a lot.
03:19:15.432 - 03:19:49.012, Speaker A: So just for this. Hey, my name is Mark and I work at optimism, and I believe that if you don't have EVM equivalents, you're not going to make it. Hey, everyone, my name is John, one of the co founders of fuel labs. Contribute to the fuel project on occasion with some PR reviews. For fuel. If you have EVM equivalents, then you're not going to make it. All right? So I think it's a good start.
03:19:49.012 - 03:20:12.904, Speaker A: So again, this is not a space here we have two ZK EVM projects. Right? Polygon and scroll. We've got optimistic EVM equivalent project and we've got really not EVM equivalent. Right. So it's almost like three against one. And yeah, we're going to see. So maybe.
03:20:12.904 - 03:21:21.250, Speaker A: Let me start then. Let me help John a little bit. I'm going to start with the quote from Nick Dawson, who's probably somewhere here sitting from fuel lab CEO well, he said in one of his tweets that execution layers built on top of Ethereum don't need to be backwards compatible and in fact can do whatever is necessary to deliver global throughput and adoption. For Ethereum, the continued use of the EVM SL one or L two will create an expansive and highly constrained design space for blockchain applications, restricting global accessibility to only a select few who can afford it. So guys, I mean, how do you respond to such a strong critique? We have a huge ecosystem, so we have a lot of applications. A lot of people that's building on top of that has happened since the EBM was launched in 2015, seven years since now. And there is a lot of work done on that side.
03:21:21.250 - 03:22:07.890, Speaker A: I'm not saying that things cannot be improved, but there is a huge ecosystem that's comfortable in how this works, in the tooling and is working on that space. They're doing it from scratch. May have sense. I fully respect it's an option and sometimes these things you have to do it, but it's hard. It's really hard. And maybe when you finish to redoing it again, maybe the ABM is three days, so it's much farther on that. And the thing is why we want to change that.
03:22:07.890 - 03:22:51.372, Speaker A: It works, it's okay, we feel comfortable. Maybe there are things that we can be fixed, but maybe done small things at the EBM level. If this works, what's the problem? Can we move forward? Maybe we have a motivation for changing that. Implementing the IBM in SDK, this is tough, it's hard, but it's done, we did it. So no, I don't see motivation just to switch that and to restart over again. Cool. I think from my point, I remember holiday that definitely we need EVM equivalents, especially on layer two.
03:22:51.372 - 03:23:48.592, Speaker A: Because when you think back like why we are building layer two, we are scaling Ethereum, right? Because Ethereum is congested and it has so many dapps already deployed there. So there need to be some places where you need to safely migrate those apps to some layer two and you need the security, you need a very good developer experience. So that's why you have to build something which for the existing ecosystem. And it doesn't make too much sense for saying, okay, so it's already congested there on EVM, but let's just build something new as an execution layer and just leave it there. So it doesn't make any sense to talk with any decentral application teams and say, oh, just try to learn a new language and redevelop everything. And also because, as Jordy said, eVM is a model which has secured for billions of dollars, and which, for example, a new virtual machine might takes years or another very long period of time for being really secure. And also, as I mentioned, there is ecosystem, there is tooling around.
03:23:48.592 - 03:24:40.912, Speaker A: But another thing which is really interesting is that Ethereum as a community, it has a lot of researchers, and they are proposing eips around all that. And so if you build something which is more equivalent, you can easily apply those eips to layer two and even some eip ahead of time to do something which maybe current Ethereum can't do, because it's driven by your community and you can reuse a lot of work from there. And also just hours ago, like in Vitalik talk, even want to use the EVM for layer one. So which in this sense, we are actually cobbling the future of Ethereum. It's not only for us specifically, but we are cobbling the same code base for the whole Ethereum ecosystem. I think one last point is that EVM, the target for EVM, is not building just for parallelization or execution efficiency. It's actually built for a very small code size.
03:24:40.912 - 03:25:22.510, Speaker A: It's simple. And so that you don't, for example, if you have very complex virtual machine, if you have multiple teams implementing the virtual machine, you don't know whether each team is implementing that correctly. There might be bugs in this very complicated virtual machine implementation, there might be bugs in the compiler, which makes things less secure. And so that's the reason why we choose Ethereum, because we want decentralization, want security. So I think that's similar reason for layer two. So that's why I think, and all the past experience for all the layer ones, because many layer ones, they start with, oh, we want to find with Ethereum, we want to beat Ethereum, but still at some point they become EVM compatible. They just accept this existing community.
03:25:22.510 - 03:25:58.392, Speaker A: So the past experience already told that it's very hard to build your community from scratch, but you should just build something which is equivalent. And yeah, that's my. So the thing about alternative VMs is just let me know when you have foundry for your altVm. The developer tooling for the EVM is just years and years ahead of anything else. And I will admit the EVM is not perfect. There's a lot of things that we need to fix with it. And I do have a lot of respect for the fuel team and what they're building.
03:25:58.392 - 03:26:52.392, Speaker A: I think it's really cool but we love the EVM and we're going to continue contributing to it and making it better. There's actually a lot of cool eips coming up. Hopefully they get into Shanghai or like Cancun after that. That can really make the EVM a lot better. And at this point, the strategy for a lot of applications is to deploy multi chain, and they want that to be as easy as possible. And if you want to get all these applications deployed to your chain, you pretty much need to be EVM equivalent or else it's going to require a lot more work from these teams to actually get them to deploy to your system. Well, the other people have disagreed with Nick, but I agree with any.
03:26:52.392 - 03:27:43.656, Speaker A: There's not really any countering of his points here. I mean, I agree with him wholeheartedly. I will say that to the point that if you start with Ethereum and the EVM, that allows you the opportunity for doing things like improving the layer two, adding parallelism, implementing a bunch of eips that people have wanted for years, right, things like security related. There's address space extension. I don't know if you guys have followed the news last year, before all the merge hype took over, but there was security concerns last year, which means they're even more of a concern now that 20 bytes is not sufficient for security. For addresses, people were suggesting 32 bytes, but it's really hard to do in the EVM because of backwards compatibility, right? There's also a lot of instructions and so on that can't really be tweaked because of backwards compatibility concerns. And these things aren't just for fun.
03:27:43.656 - 03:28:28.100, Speaker A: They're like, if your 20 byte addresses get hacked, all of Ethereum is going to die, right? This is a very significant security concern. Other eips are things like account abstraction. EIP 30 74 by Sam Wilson. Please check it out if you're not familiar with EIP 30 74. Other things like that, guess what that is fuel, what you're describing there, that thing. If we are going to work on these improvements in the future, fuel has been working on for the past year and a half, we are literally plural years ahead of any of these protocols. So I think from what I'm hearing is that the main reason is really the backward compatibility developers experience very mature tooling and whatnot.
03:28:28.100 - 03:29:13.920, Speaker A: But don't we actually build that stuff, what we're building for users and not for the devs? So I'm kind of curious, maybe let's pick one specific example that you've just mentioned. The account abstraction right. This is something that we all know that makes really hard, I mean, the lack of account abstraction to use Ethereum for the retail. Right. Using Ethereum is scary. You have to protect your private keys and whatnot. And account abstraction gives you a nice way to actually build all sorts of very interesting infrastructure that would really allow potentially millions of users to actually use DeFi and roll ups such as Zksync and Starknet.
03:29:13.920 - 03:30:15.160, Speaker A: They don't aim for the equivalence, and they literally have launched with account abstraction. And my understanding is the fuel is going to launch with account abstraction as well. So do you guys think that this is some sort of a constraint for you, like constraining factor to sort of follow EVM and lose on all those new things that you could have done, but now you kind of cannot? I don't think so at all. I think that for us, we want to contribute to proposals that get into l one before they get into l two, and even potentially spin up, say, l three s that have proposals and figure out ways to incentivize usage of those chains to test them out and kind of prove them out. So yeah, I don't think that sticking with l one is a problem at all. Also, as you add more features to your chain, we can see there may be security problems with them. There's also unknown second order effects.
03:30:15.160 - 03:31:12.780, Speaker A: So actually I'm really happy that different chains are experimenting in different ways so we can kind of learn from them as a community. I think for now our focus is still like EVM equivalency with just the same as ECM. We don't have that feature. But I think there are many proposals which you don't need to change your base layer, you don't need to change your whole chain to add that feature, because our mission is to onboard the next billion of users for Ethereum, which they can really easy to use, all the applications, and metamask is much easier to use than any account abstraction based wallet. So that's why we want to make this and introduce more users, because they can keep their even just for metamask, there are so many users they just don't know how to use that. I can't imagine how many users will be stopped by just installing this abstraction wallet. And so that's the first reason, and second reason is that it doesn't need to be a base layer change.
03:31:12.780 - 03:31:35.620, Speaker A: It might be implemented through some smart contract and some additional feature. So we can definitely support that. But maybe in some other ways. I would say it's a matter of specifications. If you want. Look, building approver for Zkvm is a hard topic. It's complex.
03:31:35.620 - 03:32:11.570, Speaker A: That's enough. This is the problem that I want to solve. Of course, there are a lot of topics in the space that needs to be solved. And account abstraction is something really interesting, something to talk about. It's nice that there is proposals and ways to go, compatible or not compatible. But he says, in our case, we're kind of a followers in this case. So our goal is to implement the Ethereum as it is.
03:32:11.570 - 03:32:51.724, Speaker A: We want to scale Ethereum. That's our goal. We want to bring Ethereum to as many people as possible. And Ethereum, what it is ethereum at this point is this. If Ethereum goes to an account abstraction, we will implement account abstraction in the roll up. If Ethereum implements addresses, 32 bytes addresses, or whatever new kind of quantum resistance addresses, we will try to do our best to follow. But this is our dna of the project.
03:32:51.724 - 03:33:31.800, Speaker A: We want to be as compatible as possible to Ethereum because our goal is to scale Ethereum. Okay, so maybe let's sort of move back to why we're doing all this, which is essentially scaling. It's one of the primary reasons. So maybe, John, the question for you, how much moving away from EVM gives you the edge in the scaling wall? It depends on your exact configuration. There's no single answer to this. It depends on your configuration of what is the node that you're running. Right.
03:33:31.800 - 03:34:14.600, Speaker A: What assumptions do you make around security and so on. But it can be very substantial because large parts of the Ethereum virtual machine and also the Ethereum transaction processing model weren't built for performance. They were built for, I don't even want to say simplicity, because it's not exactly simple. They were built because it was the simplest thing that could have been built at the time, because nothing like Ethereum existed. And it was a great, valiant effort when people didn't know better. But since then, we do know better. And you can eke out many additional zeros of performance if you design a virtual machine from the ground up to be performant, in addition to having all the nice features of the EVM.
03:34:14.600 - 03:35:23.224, Speaker A: So what about you guys? I mean, do you kind of feel that sticking to EVM on the other side gives you kind of a theoretical bottleneck that will be constraining for future scaling? Yeah, that's a good question. I think that maybe we can talk about it in two ways. There's kind of the technical scaling of the actual technology itself, and then there's kind of the social side, like the social scalability. And we believe in Ethereum, we want to scale Ethereum both from the technical side and the social side. And we think that sticking with exactly what Ethereum is doing and trying to embody the Ethereum community as closely as possible is a way to scale the social side of Ethereum. Technically, we want to work with the core devs and researchers to figure out ways to technically improve the EVM and work on things like parallelism. There's a bunch of other eips, I mentioned this earlier, that there's like EoF, right? Like a new typed bytecode.
03:35:23.224 - 03:36:17.440, Speaker A: And this would be really helpful because then you could potentially get around the need to be backwards compatible, things like that. So yeah, we don't think that it's a problem. It may take a little bit longer, but we already have a head start with all this value locked up in Ethereum and all these developers that already understand solidity and want to keep using solidity. So for Zikai, the problem is probably slightly different, or not really. I mean, my understanding frankly, was always that it's the proving time, that it's a bit of a constraint, but otherwise, since the cost of validation is so much lower, then maybe it doesn't really matter for you. But I'm very interested in your take. Yeah, I think at the Viki project I can confidently answer that.
03:36:17.440 - 03:36:45.464, Speaker A: It's not a big bottleneck, especially for proverb. I think also Jolie has a number. Our proverbial time is pretty good actually, for our current circuit and setup. We can handle 1 million gas in 60 minutes just using one proverb. And we can actually scale the proverb massively. Like anyone can just run the proverb and we predict that it may be less than 1% of the Ethereum mining machine. You can just run the whole proofing network with a significant improvement on the TPS side.
03:36:45.464 - 03:37:43.464, Speaker A: So we don't think that's a huge problem on the proverb side. But it's just definitely very difficult to build something which is very secure, and it's very complicated code, and you need very strong expertise to build that. And secondly, besides the proverb thing, I think another interesting thing, that just different execution layer or any other thing which is not EVM equivalent, they can just be our layer three, because we do believe that layer two should provide exactly the same experience to be the most secure and most safe. And if you want additional features, you can just build some layer three on top of us and we can verify, approve or whatever. So you can massively improve anything you can do arbitrary on layer three, but if you do that reversely, you use a non EVM equivalent chain as your layer two. And like us, maybe at layer three it doesn't make sense because sometimes layer three sacrifice your lavenders because you have to pass through the layer two and then maybe to layer one. So sometimes it influences your security.
03:37:43.464 - 03:38:17.376, Speaker A: So we want this direct scaling layer to be the most or even exactly the same as ECM. And then other execution environment can definitely build on top of that. And also I don't think if you change EVM to another virtual machine will really improve the current situation for later. Because if you take at the real TPS, even on lavalaunch or some other real chains, it's not really automatic by. Okay, so this chain is like five k tps or this chain is just one k TPS. The real world TPS is just so low. I don't think we really reach that limit.
03:38:17.376 - 03:38:46.964, Speaker A: If we really reach that limit, then you can build something like some fractal scaling and on top of that, just me. Just to be clear, the proverb is not a limitation at all. Prover is a parallel process and you can run as many provers as you want. So you can run as many transactions as you want. So prover is not a problem at all. The only thing of the proverb is the proverb is costly. And then the parameter that you need to check is how much fee.
03:38:46.964 - 03:39:48.720, Speaker A: So you have to build the proof. So how much part of the fee that you are paying in the transaction you need to compute the proof. Okay, at this point with cpu prover, currently the prover that we have at this point, the cost in AWS, which is probably the most expensive cloud service in the world, basic transactions 21 kas, the cost is zero risk, at least one or two orders of magnitude of margin. To improve that prover is not protocol. The scalability limitations comes exactly. They are exactly the same that the optimistic roll ups have. It's processing transactions in a sequential manner.
03:39:48.720 - 03:40:47.460, Speaker A: There are things that you cannot parallelize easily. So you have some limitations in that site. Of course we have that availability, which is another topic. And things I think that at this point we are far away to reaching the points. The other important thing about for scalability is that there is margin. So if you see Geth, for example, Geth is designed to run in a laptop, which is great, but in a roll up system you don't necessarily need a laptop. So you can use different techniques, different computing techniques, you can have a bigger computers and you can have a supercomputers, you can have different techniques to work on that, so you can do a lot of things in there.
03:40:47.460 - 03:41:56.510, Speaker A: Of course, you can always do better. So I agree with John that maybe if you design the things in a different way, it can be a little bit more efficient. But I'm not sure if we have this need yet. Now, any comments, John? Other than a bit of confusion around the fact that it's pretty common understanding that Ethereum's bottleneck today, 15 transactions per second, is really the execution, it's not the consensus. That's why if you take Geth and you just fork it, when you change the consensus protocol to something like avalanche, you're still going to be running at the same TPS. This is like common knowledge. So we are currently bottlenecked by the execution of given a particular cost to run a full node, given a particular specs of hardware, this particular implementation, which happens to be geth, can do this much.
03:41:56.510 - 03:42:37.748, Speaker A: That is the bottleneck. It's not consensus or anything else. So it should be obvious that if we want to go beyond 15 tps, we need to do something. It could be improving the implementation. It could be a whole new architecture, which is conducive to having a more efficient implementation. It could just be increasing the cost to run a full node, something along those lines. So fuel is tackling the fundamental hard problem of rearchitecting the system without increasing the cost to run a full node, so that you can get much more tps without turning into Solana, basically, where it costs thousands and thousands of dollars to run a full node.
03:42:37.748 - 03:43:28.010, Speaker A: No fuel is embodying the ethos of Ethereum, that users should be able to have sovereignty over their money and over their applications, and that they can self verify the chain themselves without having to resort to supernodes or slana style, super expensive nodes. You wanted to add something? Yeah. John raises good points. I think that it goes to show that there are things that the EVM needs, and as a community, I think that we need to come together and add those things to the EVM. That's called fuel. Tell me when you have foundry. Soon, of course.
03:43:28.010 - 03:44:13.992, Speaker A: No, but for real, developing with foundry is the most pleasant thing ever. I really like writing solidity. Yeah, we got some foundry people out here who's written foundry, or written solidity with foundry. See who has written anything for the fuel. I'll counterpoint because there's a few hands raised. Thank you, Cammy. For anyone who's tried foundry, instead of listening to me, go to fork, go to sway and try using fork, which is the fuel orchestrator, which is our equivalent of foundry, try using that, try adding some dependencies, try using the built in formatter, try building some stuff.
03:44:13.992 - 03:44:49.620, Speaker A: And then you'll see the difference between foundry and fork. And you'll see that fork is ahead of foundry even today. And that gap is only going to continue to grow into the future. Try it yourself. Don't believe me? That is a challenge. Okay guys, so let's again shift gears a little bit. One of the very important topics for us at Makerdao is what we essentially, I mean, what everybody calls censorship, resistance of the bridges.
03:44:49.620 - 03:46:08.812, Speaker A: What we found, and this is literally one of the reasons why Makeadao helped funding l two B, is that we had to look very, very deep into every single roll up and ask them really hard questions. Because we want to make sure the DAI, wherever it is, stays censorship resistant, right? So we don't want any sequencers to censor l two transactions. We want to make sure that if you deposit die to a roll up, you will always have this option of taking out if things go like super wrong. Right. And our understanding is that not every single roll up is ready in this regard. And it's actually a tougher problem that most people realize to the point that starknet for example, they're going through a very major upgrade to make sure that you can prove that your transaction failed on l two, which is apparently from the ZK point of view, a very hard problem. And other systems like ZK sync, they're introducing special opcodes so that you can actually prove that something didn't work on l two.
03:46:08.812 - 03:46:59.688, Speaker A: And they call it extensibility. Right? And they need those special opcodes. So again, question for you guys, if you stick with being like 100% EVM equivalent, are you going to build bridges that are sensor resistant and will we be able to use them with no problems? We are sensors resistant from a scratch. And let me answer the questions there, because we have very specific ways to handle those things. First, for proving a nonprovable transaction, which is something that's contradictory by itself, we have a kind of accounters, so we have a set of resources in the proof. And the idea is that we are taking an account. And if we want to execute something that we don't have enough resources, then we just say that this proof is not executable.
03:46:59.688 - 03:48:04.848, Speaker A: So that will be a nop operation on that side. So this is the way we are solving currently in the current textnet CKBM censorship resistance. One of the ways to do is for example, if we have the proof of efficiency proficiency, the idea is that anybody can be a sequencer and the transactions get sequenced when they get in layer one. This is from theoretical perspective is fully decentralized because anybody can sequence that. The problem is that it's hard. The problem is when the projects want to have better, or if you want different functionality or better functionality than the layer one. So you cannot get better in layer two, that in layer one, if you want to have a high finality in layer two, unless you create a new consensus mechanism, you are not going to get better than what you have in layer one.
03:48:04.848 - 03:48:48.156, Speaker A: So you have this limitation, and this is where the sequencer comes to. The sequencer at the end is a consensus layer that you are adding in front of the system, so that temporary, you get a different consensus until this transaction is sequenced. This consensus mechanism can be centralized system. It's a consensus, it's whatever the centralized sequencer says. This is one way to do it. Another way to do it is just putting another chain, another consensus mechanism, a pos or something with different blockchain, trilemma if you want, with different parameters that you want. And this is the consensus that you get temporary.
03:48:48.156 - 03:49:42.192, Speaker A: So this is the idea of how we build the consensus in the case of the centralized sequencer, which is probably the one that you first implement because you get the best of the centralized part. In our case, we have what we call forced transactions. Forced transactions are transactions, so all the users have the right to send a transaction in layer one. So they pay the layer one and those transactions are going to be forced to be included in the next batch with some rules in the next batch. In layer two, these of forced transactions can be any transaction. In general, it's going to be next transaction and a withdrawal transaction. But with this we solve part of the censorship resistance, because there is two kind of censorship resistance here.
03:49:42.192 - 03:50:40.304, Speaker A: One thing is nobody can steal your money or nobody can block your money. That's what you solve. The other thing is that somebody can censor you to use the system so they can remove your funds, but they cannot let you do to normally operate on the system. I think this is, maybe it's less important, but it's also a kind of censorship that needs to be solved. And this is harder to solve with a centralized sequencer. Yeah, I think we have some very similar design where users can just enforce the transaction order like through layer one, and so that your transaction won't be sensor like, won't include, or you can enforce that into the next batch and also we are in parallel exploring the design space for decentralized our sequencer. Because our order is that we want to decentralize proof first, because we want to scale the proving network.
03:50:40.304 - 03:51:19.532, Speaker A: We want to reuse, for example, some mining machines, some gpus, and to run our proverb to provide enough efficiency. And then strategically we will move to this decentralized sequencer. And the reason why, how we are thinking of this is that sequencer and prover might be like different community prover needs very special hardware, like maybe gpus, or even in the future, maybe APIC or IPJs. But for sequencer they might run some different mechanism to reach the consensus. And different from Polygon's proof of efficiency. We are not actually even under the decentralized sequencer setting. Like everyone submitting some batch.
03:51:19.532 - 03:51:39.296, Speaker A: We are not incentivizing. The fast approver who submit the proof first will win. Because we think it might introduce some. Because you rely on the fast prover. And if one day this prover go, then your system become more vulnerable. But we are incentivizing more like the parallelization between different provers. Like we can use some randomness to select a prover.
03:51:39.296 - 03:52:30.504, Speaker A: And then different provers can compute for different batch and with less redundancy. And also you don't rely on one prover, because, for example, if you are running approver, and there will always be some very strong mining machine who will always ring and always just make proof for every batch. And then they just go away because they don't earn anything. So you want to try to avoid this. So that's part of the design problem when we are thinking of this and how we are decentralized sequencer. And we believe that adding this part and some enforcing the transaction order, we might be more censorship reason. Okay, so censorship resistance, sorry to break it to you all, but I think that there's not a single roll up in production today that is actually censorship resistant, except for fuel v.
03:52:30.504 - 03:53:22.040, Speaker A: One. So what are some preconditions? For censorship resistance? For censorship resistance, you need permissionless leader election, right? If the block producers are censoring, you need to have new block producers be able to permissionlessly join the set and be able to start including these transactions that are being censored. No roll up, to my knowledge, has leader election. I think that it's a really interesting research problem. If it's something that you're interested in thinking about, come talk to me. So, another thing that you need for censorship resistance is you need the contracts that are deployed to l. One to be not upgradable.
03:53:22.040 - 03:54:34.392, Speaker A: To my knowledge, all of the roll ups are using upgradable contracts. So even if you have this l one to l two kind of forced transaction, if your bridge on l one that you're sending this forced transaction through is upgradable, then whoever has the owner key can just upgrade that contract and prevent you from sending your transaction to l two. And I am sorry for being the bearer of bad news, but this is just the reality of where we are right now. And I think that it's really important to be very explicit with what the actual security properties of these systems are. I think that we need to, as a community, be a lot stronger on ourselves and hold ourselves to higher, more accountability, higher standards. There's this weird incentive where we don't want to call out the roll ups because we'd rather have people using ethereum things instead of say, avalanche things or solana things. But I think that the reality is we have some way to go before we actually are censorship resistant.
03:54:34.392 - 03:55:31.088, Speaker A: And I do think that we can get there. We just have to work together. I do have one question for you, actually. I remember chatting with you about this on Twitter a while back. But what's your view on the long term? And I guess also the other panelists, what's your view on the long term? How it's going to evolve with upgrade keys? Is there actually a nice, robust way of removing them, despite the very complex nature of roll up bridges? I think something like Vicky thinks, secure console, where you have this delayed time as far as it's long enough for you to upgrade, and I think it will be enough, at least for most case, we have to remove it some point, yes, we want to decentralize. We want to decentralize at some point, the system should not be controlled by anybody. And yeah, this is hard.
03:55:31.088 - 03:56:20.720, Speaker A: And bootstrapping systems, it's a compromise. That's why we do these trick things. And I agree that we need to be very transparent on that. That's another story. But at some point, all the decentralized systems, this is not only exclusive from the roll ups, all the decentralized systems. So I can understand that at the beginning there is some bootstrap phase where there is some upgradability if you want some things that you can tweak somehow, but at some point you need to close the doors, just burn the keys, call it whatever you want. There is a decentralized system.
03:56:20.720 - 03:56:53.364, Speaker A: There is a common good. Everybody can use it. It's not yours anymore. It's just community stuff. And this is hard, but we need to think that this is the goal. One question, what if the key circuit has some bug and you don't have any application key and what will happen there? That's why the brakes, it's very hard. I think that projects with upgrade keys are going to outcompete projects without upgrade keys in the long term.
03:56:53.364 - 03:58:18.004, Speaker A: I think that realistically upgrade keys for some parts of the system are just going to be necessary. And I think that it kind of just comes down to what is your political philosophy around this. Right, do you lean more towards the bitcoiner side of the spectrum or do you lean more towards say like a web three kind of spectrum where you're trying to iterate and make your protocol better over time? So there's like a wide trade off design space here. And I do think that having say governance or some organization that is able to upgrade parts of the system, I do think that it's really good given that there are sufficient checks and balances on the ability to make changes to the system if you can avoid it better. So if you can avoid having some governance based parameters, something that's upgradable under certain. So if you can avoid these things, I think it's a parameter of quality of the system. So a system that bitcoin does not have a key that you can do upgrade something, of course, but these systems should be clean.
03:58:18.004 - 03:58:54.268, Speaker A: And if you are able to design a system that doesn't need governance, it's a better system than a system that requires governance. Guys, we're slowly running out of time. One last very quick question to all of you. When Mainnet and for you guys, when bedrock. Mainnet, when bedrock. Right now we're kind of in the final stages of just testing and kind of adding a few new little features that we need to feel ready to go to production. So we're thinking within a few months to basically upgrade our current system to bedrock.
03:58:54.268 - 03:59:29.724, Speaker A: And with Bedrock, I think that bedrock is kind of. I personally think that bedrock is going to be the standard for how you design an EVM equivalent. Optimistic roll up. And we're designing in a modular way so that it's really easy to kind of use the exact same code and spin up your own roll up. So yeah, a few months. We launched a closed testnet like six months ago, I think, and an open testnet last month. So I guess just extrapolate from there for now.
03:59:29.724 - 04:00:04.504, Speaker A: We have built a permission testnet where users can do interaction and the register like developer can deploy arbitrary smart contracts. We already done that yesterday. East Global, there are several teams building on top of that and we already enable hard hat and fungi especially and people can already use those toolings. It's also a closed testnet, but we still need to wrap up some rest circuit part and do more secure auditing. But soon. We just stopped public testnet this morning. We made the announcement so anybody can test it and it's open.
04:00:04.504 - 04:00:37.080, Speaker A: Maybe it's already done because it's just very new on that. But the problem of saying why commitments and we are not going to do any commitment when there is some security risk in here. So we are not going to launch this day or that day. So we will launch when we think it's ready and it's special. Safe. Okay. And safe, or I would say never 100% safe.
04:00:37.080 - 04:01:09.564, Speaker A: But we feel enough, comfortably safe to launch these things. We are starting the auditing process at this point here with Ethereum foundation, with scroll guys. And it's an effort for auditing this, which is complex. And let's see the beginning of next year. We can launch in one year. It's going to be one year. If it's in, it's going to be ten years.
04:01:09.564 - 04:01:47.844, Speaker A: We are going to launch it when it's comfortable. The only running and we are working really hard this to be as soon as possible. First of all, congratulations on launching the first open source ZKe EVM equipment testnet. It's a very big achievement. I'm wondering what is it licensed under? It's the what of the open source testnet or the open source prover. Sorry. So the prover, there is a version that you can choose between MIT and MIT or Apache.
04:01:47.844 - 04:02:06.530, Speaker A: An ODM itself at this point does not have license. And when we do one of those two, license too. Thank you. We have permission actually developed by the community and anyone can just contribute and. Yeah. Thank you. Very big round of applause to everyone and thank you.
04:02:06.530 - 04:02:36.104, Speaker A: That was a long one. Thank you. Thanks for having it. John, is this one working? Thanks, guys. No matter. Okay, cool. We move on to the next panel right away.
04:02:36.104 - 04:03:01.088, Speaker A: That was enough Evm for today. And we'll move on to Mev. We'll have a panel moderated by tog roll and we'll be joined by Alex, Yannick and Guillermo. And I've heard it's alt Tarun. Yep. Okay, I'm getting this one. This is like step up.
04:03:01.088 - 04:03:16.400, Speaker A: All right. Am I going to look like a dumb ass? Probably. I don't know. Should I? There you go. All right, fine. Got to be separated. Everyone's comfortable.
04:03:16.400 - 04:03:40.540, Speaker A: Can we go pretty good. All right. Yeah. After all the spicy takes of John, I don't really know how to follow up with something even more spicy. My name is Togral. We have Jannik, Guillermo, Alex and Guillermo's alt here. Or as people call him sometimes, Tarun.
04:03:40.540 - 04:04:06.640, Speaker A: Guys, can you introduce yourself, please? Sure. I'm Janik. I work on Shutter network, which is a front running protection Mev minimization system that we hope to integrate into rollups. Hi, I'm Guillermo. I am a glorified paper editor at Bain Capital Crypto. I guess that usually translates ahead of research, but I don't know what that means. Hi, I'm Alex.
04:04:06.640 - 04:04:33.470, Speaker A: I'm part of flashbots. I'm a researcher steward, working on all kind of Mev related things there. I'm Tarun. I found a company called Gauntlet, but I also have written a bunch of papers trying to make sure that people realize Mev isn't always good. Good. All right, first question. Mev has become an established phenomena amongst the community.
04:04:33.470 - 04:05:01.284, Speaker A: Four years ago, it was just one paper, which, like, ten people in the world knew about. And now it's like protocols are being built around it. So, with l two s coming to the scene, what do you see? The future of Mev. Like, who wants to go first? I'm happy to go, but, Yannick, you also work on l two s. If you want to start. I can start maybe with very short answer. I think it won't go away.
04:05:01.284 - 04:05:50.472, Speaker A: We'll stay there, but it will probably different, because in l two, they work very different from produce blocks in different ways. So I see different new phenomena. I actually disagree. Shock? Well, no, I mean. What do you mean looking different? Because, for example, sequence is centralized, and that works very differently. I also think there's space for experimentation. Like, they can at once have to be very conservative in how they design their protocol because there's lots of value, or at least on Ethereum, and roller does not have that problem.
04:05:50.472 - 04:06:13.064, Speaker A: So they can just try out lots of different things. For example, different types of mempools, encrypted mempools, public mempools, maybe. I think you proposed this escalating fee. Escalating, something like that could be implemented, rollout, like, all this kind of stuff. Wait, sorry. Could you have. No, I have not heard of this particular proposal.
04:06:13.064 - 04:06:36.324, Speaker A: And maybe flashpoints better proposed it ourselves. Right. Like, fee escalator is a concept as old as maybe p 15591 of the design was like an escalator fee concept. I think Dan Finley from Metamask is like one person that brought this up. Here's an even number question. Escalator fee. You submit a transaction with a negative fee, say, and settle that transaction for you.
04:06:36.324 - 04:07:25.750, Speaker A: And if no one settles it under a period, the transaction fee escalates to a higher level. There's also any sender from Patrick who's right here that did something similar, but that was more for resubmitting the transaction to the mempool. There's a lot of these concepts, none of this is new and it's about better ux for users. In that case of what was proposed, and it's not something that we're building actively, was to having some type of order flow auction and how auction on chain, that does not suck. Yeah, well that's the problem with escalator fee. It's extremely central. The centralized timestamp that you have is like if you're in block time, which we're slow, and usually you want this auction to go quite fast because you want to be able to update the escalator quite quickly.
04:07:25.750 - 04:07:56.880, Speaker A: But more interesting because they have shorter block times and so you're able to have more fun there. You can also just do more centralized infra and then see what happens. Again, I don't think it's a design we're going to build. There are reasons why it's not as interesting to us. Related leakage of information that auction and other things related to this. And I'm sure you guys worked on Defi and privacy in the context of Amms in particular. So you're familiar.
04:07:56.880 - 04:08:43.340, Speaker A: I have no idea what you're talking about. So I think the one thing to remember, and maybe this is just a sign being a boomer, is that if I look at the Linux, Linux 2.4 to 2.6, that was like one of the biggest sort of open source software upgrades in history. Anything in crypto is one 1,000,000th at best of how big of an upgrade that was. Because every data center in the world to upgrade to this because a, if you wanted to support multicore or you want to support many different processors, different types of heterogeneity in your current all of this shit, you couldn't do it unless you. And it was an upgrade that took like ten years.
04:08:43.340 - 04:09:40.844, Speaker A: And most of the was, it was really fucking hard to schedule processes on multiple cores and to get that right and to scheduling all the cache misses, to scheduling all the cache coherency protocols. There was a lot of went into that from probably hundreds of thousands of engineers so upset have the same problem. The difference is that instead of having a scheduler, you have this set of many auctions that are interacting with each other that act as a scheduler. And that's an even harder problem. So I think we should at least give some credence to the fact that Linux took so long to solve a much easier problem that had to be deployed to 99% of computers. And so I think in general all of the people complain about information leakage. Seen the same thing before where when we moved to the first version of Linux I think the efficiency ratio was like 10%, which was horrible.
04:09:40.844 - 04:10:30.892, Speaker A: 90% of cores were sitting there spinning, doing inspining. Thanks for the Linux history lesson. I have an answer to your question though. Sorry, well, I was just going to say in general, right, centralization is very, you can have all these nice economic efficiency problems and in particular nice thing where you have or don't have this nice thing where you have a bunch of interacting auctions and congratulations. The problem with interacting auctions, as if you remember Milgram from whatever the 90s auction spectra was meant to solve this thing of like you have a bunch of interacting auctions and want to succeed at one and then fail at the other. But you can't guarantee that unless you somehow have additional higher level interaction. This is like kind of repeating Tarune, just more generally with auctions.
04:10:30.892 - 04:11:11.532, Speaker A: So this is why I had a question of even mechanically auctions suck and they'll suck even more because now you have all of themselves don't come into consensus trying to interact in weird ways. So do you have auction and how do you do that? But then you have auctions on auctions you can get really with it, right? And be like, oh, we continue to deal with meta, meta, meta. Was that DMT or weed? What was the implied smoke? Both. Yeah, exactly. Take your pick. Sorry Alex, I have an answer as well. I think both of you are talking about cross roll up stuff happening, right? That's right.
04:11:11.532 - 04:12:00.300, Speaker A: Whereas if I'm pronouncing your name right, Yannick talking about intro roll up, what happens? And actually I take back what I said before about disagreeing with you. I agree. Depending on the design that people go with for their ordering policy, that's going to have, we're going to see different behaviors from people going after opportunities in these places. I don't think these behaviors are going to be novel. There are other chains outside of the ethereum ecosystem that already have different ordering or that don't have a fee market. And we see what happens, right? Solana getting spammed polygon getting spammed for different reasons. Co locating with avalanche foundation validators in order to be faster.
04:12:00.300 - 04:12:56.860, Speaker A: And you see very similar behaviors today with their centralized sequencers. So I think depending on the architectural choice there, you're going to see different behaviors related to MeV. The second thing I will say related to MeV is I think roll ups are going to be much more active with how they decide to harness that value. If you take optimism, for example, I think they're taking a pretty open and public position about like oh, let's redirect public goods funding, right? Other projects might want to do it differently. If you look at Cosmos, for example, accrue value to the atom token using an interchange scheduler, this is kind of like referring to. So I think there's like a big design space there, not only for redistribution, but also for the ordering policy that you choose and how that affects the activity that the searchers have. However, I don't think there's going to be fundamentals.
04:12:56.860 - 04:14:20.920, Speaker A: I'm trying to think is there anyv that specific to l two s comes to mind is like stuff related to exiting the l two or like censoring a commitment, stuff related to escape hatches or stuff related to the l one path that you can take to submit your transaction to an l two, these things are quite specific to layer. And so maybe there's new stuff there that we haven't seen before. There's one thing I think that has not been analyzed, which is the thing that everyone who was running a centralized sequencer is trying to avoid, even though. Which is that if I have two systems which are supposed to synchronize but they have two different fork choice rules, and I can't guarantee some bounded latency difference between the two fork choice rules, what's the probability that they can reconcile under some fixed number of blocks? And the multiple fork choice rule reconciliation actually will probably create a huge amount of MeV. When you go to decentralized sequencers versus single sequencer by multiple fork choice rules you mean like two l two s. Again, one forks and the l two forks and the l two forks a certain time length and the l one forks for a different time length. All of the existence guarantees that exist both formally and numerically for fork choice behavior like ghost or anything like that just go out the way.
04:14:20.920 - 04:15:15.828, Speaker A: So this idea of interacting fork choice was basically making MeV more almost like giving some leverage to MeV between two chains is one of the things I'd be most scared about with a decentralized sequencer and l two. Next. Hey, you asked for different than. I'm not complaining. Since roll ups allow you to design the protocol from the clean. Should we build the protocol from the ground up to mitigate maV, incorporate it into the incentives of the protocol and embrace it? The latter. There's this epistemic fallacy amongst certain people that you actually can come up to consistent ordering across many people with differing utility functions and non trivial differences.
04:15:15.828 - 04:15:44.400, Speaker A: That's sort of a pie in the sky. I don't understand that information theory exists type of team, which is if you build a dex combined with l one or l two, you can do things like market after every transaction. So there cannot be any back running, right. Because by definition there exists no arbitrage if you've routed. Right. So like penumbra is one such case. We have a number of other such projects.
04:15:44.400 - 04:16:30.804, Speaker A: I think some of them are sitting somewhere around here. But the point is there's a whole question of mitigate MeV or something, which is a whole weird other topic that touch, but vaguely looking at the outline of things sound like MeV. You can kind of, there's a lot you can because of the knowledge that MeV that happens on automated market makers has like this very specific right. And you can exploit that pretty carefully to construct things like mitigate MeV in the sense of you have prices for all of your automated market makers at every point in time because you correctly route every. And you do not, that. You do not accept orders that only touch like a specific CFM. Again, but you are making.
04:16:30.804 - 04:16:53.884, Speaker A: It's application specific. Of course it's application specific. Right. That possibility thing like, oh, I'm going to do Fifo ordering for every fucking roll up. Oh no, that's just like ignoring like 50 years of, no, I'm not even talking about fifo order. Just generally like a nonexistence of like, you don't have to, you could do whatever kind of ordering for this. What does that say about general purpose roll ups? Yes.
04:16:53.884 - 04:17:21.216, Speaker A: Do you think as like a mechanism? I think you can generalize it in certain senses. Construct. It's a bullshit answer. Oh no, actually I have a very specific. Well, it's fair, but there are specific classes of protocols for which you can actually use the same trick in a lot of cases. Right? Like what does it mean for things to be consistent? We know what that means in terms of market means in terms of auctions. Right.
04:17:21.216 - 04:17:51.212, Speaker A: There are answers you could make in this case. I'm not saying you can do this for application. I'm sure there's some funny little impossibility result where you construct an application, and that's fine, but, like, 99% of the stuff that we do right now, these two applications, so why not just get good and make them right? That's my answer. But then this begs the question of all those mitigation mechanisms involve consensus somehow interacting with the application. Throw ups do not do that. I don't know, and that's fine. I would say that's a thing.
04:17:51.212 - 04:18:39.710, Speaker A: I think that's a philosophical, that gets back to the debate of the last panel, which is topically, how much do you want your application intertwined with consensus or not? And that's like a religious choice. Yeah, it's like the choice being an atheist or christian. I didn't think theology on this stage. Janik, you guys are building something different. So what's your on this? Yeah, I guess I have to defend it. Yeah, we're building this encrypted mempool thing, which prevents front running, I would say change. And I don't think it has, of course, it has some disadvantages, so it's not.
04:18:39.710 - 04:18:59.428, Speaker A: But it's definitely one approach we should think. There's probably lots of other approaches we should try and roll ups give us to do that. So I don't really see why we should give up in mitigating Mev. I don't think we should embrace it. I think we should do both. We should both embrace. Interesting.
04:18:59.428 - 04:19:50.420, Speaker A: So you're saying that we can do both at the same time. Okay. All right. Anybody has anything to add, maybe comments on? Yeah, I totally agree with him. Right, well, I think lacking, like, rigorous taxonomy for MeV, should we just spend the next hour arguing about what MeV means? But when people say Mev, a lot of the time they think of, which is they think of stuff that's harmful to users. And I think it's evident that anything that's better for the user experience should be considered and should be implemented. Ultimately, you want the system to get to as close to mass adoption as possible and to be as much as user friendly as possible, that there's a lot of other types of extractable value that can be harnessed in the protocol.
04:19:50.420 - 04:20:09.530, Speaker A: Of course, regulatory friendly, which also means, no, let's not go there. Let's not go there. It means to users. Right. The regulators are users. Okay. Of course.
04:20:09.530 - 04:20:59.160, Speaker A: So I guess I would maybe put out one possible. There is the whole, like, if the only thing you have is everything looks like a nail problem. That often happens in crypto. I somehow think the fact that we cannot even decide on even minuscule definition of which types of actions qualify means that any mitigation mechanism you have will actually verify whether it mitigated the correct thing or not, or caused another externality. The fact that we can't even define what an externality to an MEV is formally is a distinct failure of this industry. I mean, we can't even define what a roll up is, right? But that's a distinct failure. That means that we have way to go before we can even talk about mitigation.
04:20:59.160 - 04:21:39.396, Speaker A: Next question. Value extraction is a centralizing force. How should we combat the resulting negative? Talking about externalities. Wait, sorry, what was the question? So some say that MeV can be a centralizing force. And how do we combat the resultant externalities? Negative, like censorship and stuff like that. Well, I think you have a result on some part of this, at least. Actually recently released, still working on.
04:21:39.396 - 04:22:12.690, Speaker A: Yeah, some alpha maybe. No, that's fine. It'll come out when it comes out. All right, anybody else? Sorry, Yannick, go. Not really answering the. But I would also say just because Mev is a central doesn't mean we should give into that force like we could other forces to kind of counteract that. Like for example, simply social rules that we don't accept certain types of behavior, if that's possible at all.
04:22:12.690 - 04:22:44.680, Speaker A: Yeah. Alex, anything to add? I agree with Yannick. And generally, I think you kind of made two distinct points. I think there's like a rich get richer dynamic of MeV and you want that as much as possible. And so this is more about redistribution mechanisms. I think it's relatively underexplored as it's like most welfare maximizing. And again, I guess optimism's position is, well, invest in public goods.
04:22:44.680 - 04:23:48.284, Speaker A: People working on order flow auctions are saying, well, you give a rebate to the user welfare maximizing. I guess the work that these guys did is looking, for example, an instance of saying, well, it can be welfare maximizing for the overall system and maybe not for the particular user. I think the main idea there is if you redistribute the MEV, captured in some manner, that part is constructing the cryptographic mechanism such that the redistribution has very low cost of corruption. But if you could do, then you could basically actually rebate users so much that you can lower the inflation rate of the overall protocol, which MEV as a subsidy to users is more than block rewards, right? And that's sort of a kind of powerful. You have to bootstrap your network with block rewards until the point at which actually redistributing MEV means that you can have to inflate as high. And I think obviously cosmos sort of is to go in that direction. Sort of.
04:23:48.284 - 04:24:29.416, Speaker A: I mean there are too many conflating factors in Adam Toot thing. But I think I have a new name for your paper which is incentive compatible communism question mark. I think the hard part about a lot of these mechanisms, right, is this notion of incentive compatibility. It's very easy to just have an agreement off chain to be like, all right, cool. Yeah, sure, we could redistribute some of the, I could just submit exactly one bid that has zero actual value, but we all perform the auction and redistribute the profits. Yeah, prefacing it with that. I would say that if we ever get to a world even threshold, fhe works on chain.
04:24:29.416 - 04:25:27.504, Speaker A: Now obviously that's pipe dream for anyone who's ever tried to implement these types of things. There are ways to actually achieve this. So I would say that we could actually get to a good redistributionism, but it sort of effectively needs homomorphic encryption. Could you expand on the ways in which you can achieve. So basically if we take kind of like the Eigen layer model one step further, right, where restake their stake and there's some slashing rules that are socially agreed upon for certain applications. One level further for that would actually be proving just like as a ZK proof, but proving that on encrypted inputs that you the correct output for an auction or for subsets of auctions do that. Then you can basically show that the person had to have redistributed because they ran the code that does the redistribution to all the people who restaked to that particular.
04:25:27.504 - 04:26:09.304, Speaker A: But doesn't this go? Now you have an app that is like tightly controlled with consensus. This is why it gets back to religious differences. Yeah. Okay, good. So basically what you're saying is that we should type a couple rather than making, I'll use the popular word, right? I mean the PBS model is sort of in that direction, right? Like ethereum is clearly moving in the direction of coupling to consensus. Now the question is what is the correct way of coupling it to consensus such that it's not affecting extra bad latency for the user. Not that much of an improvement over the off chain auction.
04:26:09.304 - 04:26:42.632, Speaker A: Right. Both improve in user usability and in sort of censorship resistance same time. And if you're not doing that, everyone will just go back to the off chain. It doesn't make sense. Fair enough about PBS. What are your opinions on PBS? Should we go that way or different route? I have no opinions about PBS. I'm happy to talk about it.
04:26:42.632 - 04:27:47.576, Speaker A: Yeah, it depends what you mean by PBS, right? If you mean generally between who constructs a block and who proposes it, or more concrete version, the Ethereum has been people in Ethereum. Okay. So even within Ethereum protocol PBS and effect the out of protocol protocol PBS. I think it's not clear yet whether it, and I think that's at least my personal view. I think there's a lot of more work that needs to be done to understand the market economics. And also I'll point you post from Barnaby that talks about he kind of explains the full block PBS auction as generalizing this to allowing validators to arbitrary preferences on block space. So if you have a credible commitment, a credible mechanism for commitments between builders and one commitment can be I want you to give me a full block.
04:27:47.576 - 04:28:38.060, Speaker A: Another commitment can be I want you to give me a full block with this and this thing, or like a partial block, given some rules. And that I think is quite interesting. Again, it's relatively underexplored, but it makes a lot of sense. Yeah, there's the trade off of adding more overhead on validators. And so that has to be considered. But I think you can have negligible in the kind of templates that they can look at, but giving them a lot more explicitity about what they want, which is ultimately, I think my trouble with the current PBS is currently like full block and it has some benefits. There's a lot of features you can build on top of that as builders, but at the same time it has a lot of drawbacks.
04:28:38.060 - 04:29:38.190, Speaker A: Yeah, so I think the jury is still out of protocol. It's not clear how it's implemented in protocol way that makes sense. There's like a two slot right now that was proposed by Vitalik. I would evolve in the future if Ethereum moves to something like single slot finance. And so there's a lot of these questions that have to be considered with in protocol PBS and also how it interacts with future upgrades of the Ethereum network beyond single slot finality, any other opinions on this? Maybe the general question about PBS, should we do that or not? I would say probably too late for that question. PBS is there and will be very hard to convince to give up a big fraction of their rewards. So what we have to do now, basically is to improve as much as possible.
04:29:38.190 - 04:30:25.772, Speaker A: And that can be either outside or inside of the protocol. The question is more, should we shrine it, or should we keep it separate to the consensus? I wouldn't shrine it right now. To be clear, the work that went behind the IP 1559 to include it in protocol reasoning that went into it like the paper from Tim Rough Garden, the many simulations, the work that the robot did, and compare that to the formal reasoning around PB, no comparison. So I think it deserves a lot more scrutiny. And again, just from that article from Barnaby, there's a big design space for how that should be. Generally, separating a lot of the heavier computing tasks that to do in the past to more specialized parties makes sense. It's like a separation of concerns that makes sense.
04:30:25.772 - 04:31:27.600, Speaker A: How exactly do you separate this to preserve Ethereum? It's still something that needs to be understood. How does this new market that's created interact with the other markets in Ethereum? Fees on Ethereum also has to be understood better. So my opinion, personal opinion, is it should definitely included in protocol right now and requires more. I mean, knowing Ethereum, it's not going to be included anytime soon. I think the last thing I'd say is people in crypto, for some reason, probably just don't seem to pay literatures. But there's this very famous trilemma in the algorithmic game theory literature from May 2000 called the credibility trilemma, which is in some ways flashbots is. The existence of flashbots is proof that ETH has not gone over the credibility trilemma.
04:31:27.600 - 04:32:01.720, Speaker A: It's funny when multiple fields love the same word, because I hate the word trilemma. It's like, wait, the etymology of lemma. So you're saying scalability trilemma is not a real thing. No, trilemma. Right? Like if you think about the etymology of lemma in Latin versus like I in front of it doesn't read correctly. But anyway, the credibility trilemma is the following thing. There exists no auction with unbounded computational bidder auctioneers such that you have the following three properties.
04:32:01.720 - 04:32:32.880, Speaker A: One is credibility that the bidders have to trust whether the auctioneer is lying or not. Like they can actually believe. They don't even need some mechanism by which they can observe the bids, and they never have to actually try to prove that the auctioneer is lying. The second lying, from the perspective of submitting. The simplest example of this is suppose you have a second price auction. Second price auction. The highest bidder pays the bid of the second high.
04:32:32.880 - 04:32:57.408, Speaker A: That bid is really high. Well, the auctioneer can just lie by putting in a bid arbitrarily close to the highest bidder. And then forced auction. So that's a sense in which it's not credible. Like the auctioneer bids, they can be censoring bids, they can be adding bids. So it changes the outcome of the auction property is truthfulness. Like people report the value that they actually, that the object has, that they're bidding on.
04:32:57.408 - 04:33:36.904, Speaker A: And the third thing is bounded communication. The trilemma says you only get two, three of those. So the unique, credible, bounded communication time auction. The unique, truthful, bounded communication auction, auction clock auctions, the unique, credible, not bounded communication auction. And somehow understanding the edge cases of all these things should be very well understood far before you ever install PBS into the protocol. So if not PBS, there's another proposal that is being discussed. It's shutterized blockchain.
04:33:36.904 - 04:34:32.350, Speaker A: So using threshold to basically mitigate mav, if you allow me to. What are your opinions on that, Yannick? Of course, again, that's one of the mitigation minimization. I think the two approaches, they don't really oppose each other. You could implement both. And I would guess that even if we, I think it's very, this will be implemented in Ethereum, but if it would be, then there would probably a PBS of that as well. Why is it unlikely that it would be implemented on Ethereum? Because it adds basically additional security assumptions. You have to trust a fraction of a randomly selected fraction of validators more than others.
04:34:32.350 - 04:35:20.830, Speaker A: And it's also just a very somewhat complicated. Can I ask a quick question? Do you have to use it separately or can it be shared with the consensus as DKG? If consensus had a DKG, no, the DKG would be done out of the consensus. Okay, anything else to add, or. That's it. Next question. Okay, ration of L two s. The behavior of the builders who are going to extract MeV change somehow the behavior and the paths to the most optimal are going to be the same going forward.
04:35:20.830 - 04:36:09.396, Speaker A: So builders don't extract meV, right. MEV is mostly extracted by values who take most of the profits, if not all of it. I think, again, what I mentioned before, right? There's like users on the other side of that equation. And right now some instances of MeV can be exploitative to users. And so thinking about kickback that are welfare maximizing for users of the system is valuable. So I think we'll see more of that for sure. I meant more from the perspective of how important is it going to be in the future in terms of over MEV profits, you're asking 30.
04:36:09.396 - 04:36:51.028, Speaker A: What percentage of MeV profit is cross chain, right? Yeah, 1 January 23rd percentage. I guess only between roll ups. Doesn't have to be between roll ups just in including centralized exchanges. No, I don't. I mean, it's a function of volume. And how adopted are these systems? It's a function of how many mitigation technologies mitigate against the extractable value that is harmful to the user experience. It's a function of like, how much value is lost to the molok.
04:36:51.028 - 04:37:31.030, Speaker A: So how much extractable value is lost to a lack of coordination? How many coordination mechanisms that are not centralized, coordinated by a centralized coordinator, can exist in ways that unlocks values for the system. So part of that. Questions, part of that is understanding better how to unlock coordination between parties in the systems. I don't have a number for you. Yeah, I wasn't expecting. I was just expecting. He wants a distribution supported 100% with no add ins.
04:37:31.030 - 04:38:02.096, Speaker A: I could imagine things, for example, like things that could become very large or very large in terms of volume, is like routing or arbitrage across chains. And right now I think players are fairly unsophisticated for that kind of thing. I don't really know that many players who are truly doing real cross chain shit. Well, very sophisticated players doing cross centralized. Centralized. Oh, no, that's for sure. Right.
04:38:02.096 - 04:38:41.016, Speaker A: That's the lowest hanging fruit. Right. Because it's the most against the largest liquidity pools. So if that happens, like roll ups gain more liquidity and you have more paths to take that are meaningful for these traders, then I think they will do that as well. Right. I think it's unsophisticated, partly because the people that know how to do this right now just have other problems, lower hanging fruits to look at. I mean, one of the, if you talk to searchers who do this Dexarb across bridge, because with Dexarb you kind of know your net loss before you submit the transaction.
04:38:41.016 - 04:39:16.776, Speaker A: But people who are doing liquidations actually have a ton of problems because if they can't repatriate the liquidation profit from the destination chain back to the source chain, they're holding a bag of something that could have gone down 20%. And so hard to hedge anything that you have to do round first, things that you only have to do by unidirectional on bridges. And so that completely changes. You're hedging on the same exchange. Right. It's like my net position on binance or FTX is, I'm short of perp, like long, whatever the collateral I'm selling is. But on bridges, we don't have that.
04:39:16.776 - 04:39:41.970, Speaker A: And so bridges bring services like letting you hedge your liquidation. You may start seeing bridges look like exchanges. I was going to say already today, some of the largest bridges are centralized exchanges. Right, yeah. My favorite bridge on binance and coinbase. Yeah, halfway. Somewhere in the middle.
04:39:41.970 - 04:40:07.830, Speaker A: Yeah. But it's not really a trust bridge there. It's quite centralized as well. Guys, unfortunately, time. I would have loved to continue discussing stuff, but, yeah, thanks, everyone, for listening. Big round of applause to this, guys. There you go.
04:40:07.830 - 04:44:05.840, Speaker A: Arguing for the sake. Confusing people. Yeah, it's some. Somewhat of. Jump, jump, jump, jump, jump, jump, jump, jump, jump. It. Two quick public service announcements, if I may.
04:44:05.840 - 04:44:40.680, Speaker A: Number one, there's a little problem. One guest is missing his black backpack. So if someone took it by mistake, please talk to someone from the staff. And number two, please do not leave forever. Once we wrap up the next panel, there will be cocktails with scroll at seven in a place called Monaco Rooftop, a 20 minutes drive from here. We will start the panel very soon. Sorry.
04:44:40.680 - 04:45:28.390, Speaker A: Yeah, sorry, my brains. And one more question. We have a panelist who's a bit late. Anybody from optimism wants to join instead of your colleague, I think we can stop. Maybe I'll find Ben Jones or something. Please take your seats. And if I may introduce the next and final panel of tonight, it will be a panel with four panelists.
04:45:28.390 - 04:45:56.154, Speaker A: There will be a lot of talk on what was introduced earlier. The EIP 4844 was introduced by Mophi. And I think we will hear even more about that. We will have Sri Ram Kanan from Eigen layer. We will have Josh Bowen from celestia. We also have expected proto Lambda, the proto bit of dunk sharding. Proto dunk sharding.
04:45:56.154 - 04:46:21.430, Speaker A: But he is delayed. Currently. We might have someone from optimism coming. If there is someone, please join. Otherwise, Proto will join later. They're all gone, so we can start. And otherwise, we also have Duncrat Dankrad five, the dunk bit in protodunk sharding, and also, on a personal note, my personal gateway to developing crypto.
04:46:21.430 - 04:47:07.400, Speaker A: And the panel will be moderated by Justin Drake, as I'm told, a kind of mutual gateway between Dankrad and Justin himself, wherein apparently it was Dankrad who hinted at crypto. And then into the rabbit hole did Justin fall. And then years later, it was Justin who dragged Dankrad into the EF crypto hole. And so the loop closes. And now, of all topics, we will be talking about roll ups. So please welcome the panelists. Else is here.
04:47:07.400 - 04:48:01.782, Speaker A: Okay, great. So it looks like, we have three competing data availability layers, and I was told that I need to try and make this a little, I guess, you know, maybe let's do a very quick round of introductions, like who you are and kind of what is your data availability layer? I'm Josh Bowen. I work at Celestia, mostly focused on kind of like the execution layers on top of Celestia. Hi everybody, I'm Sriram. We've been working on a project called Eigen Layer, which essentially enables each stakers to provide other services. The first example of which we're building is a data availability service called Eigenda. Hello, is this working? Yeah.
04:48:01.782 - 04:48:46.322, Speaker A: Good. Yeah. I'm Dan Kras. I'm a researcher at Ethereum foundation. And I mean, the data availability layer that I'm working on is like sharding, which has been part of the Ethereum roadmap for a while, and now it's pivoted to mainly a data availability layer, at least for the next few years. Great. So I guess my first question is, do you think data availability layers are winner take most? What are the network effects? And is it really a very strong competition between the three of you, or are there opportunities to have multiple winners? I think there's a strong argument, just like structurally, that there is benefits for more roll ups to be within a single data availability layer.
04:48:46.322 - 04:49:49.020, Speaker A: The bridging guarantees there are better, but I don't think that necessarily means it's going to be like a winner take all. There will be only one da layer, but I do think the network effects imply there is strength to becoming the dominant DA layer. Right? I think I would agree on that. I think one of the things that builds into this is the idea that data availability can be made scalable. The fact that data availability can be made scalable means that one layer can grow large. But there is another counteracting force to this, which is the native coupling of data availability with the native chain actually gives the highest guarantees always. And so I think the way we will see things evolve is on a given domain, the most secure data availability will be native, and then there will still be a lot more needed, and we are there to supply that.
04:49:49.020 - 04:51:10.914, Speaker A: Yeah, I think I would agree with Sriram there that say, from the Ethereum point of view, there will be the Ethereum data layer and then there will mainly be the others, basically. And I think the Ethereum data layer is unique in that. In there you get very high guarantees and the others give you a different level of guarantees, but in that guarantee they are more or less fungible, right? Okay, so in terms of scalability, it sounds like you say you can have unlimited scalability, but would you agree that you can have unlimited data availability? Why are we not doing that at the Ethereum layer? And what are the trade offs here? So the main limitation about data will be scalability, are that they are strict, sort of like they're not limits of nodes, but they are basically more based on we don't want to lose data. Ultimately, what makes us not lose data is always interest. Want to preserve some piece of data forever. There's no storage medium that lasts forever. There's only people who keep being interested in the data and keep making.
04:51:10.914 - 04:52:26.814, Speaker A: And so fundamentally, I think that will always be the limitation to how far you want to scale your data availability, how far you are willing to compromise, how much you think there's interest for your data availability layer, and how far you think you can push that. So it's not unlimited, it can quite far, and it does scale also with how important your platform becomes ultimately, which is good, but there will be limits. Sriram, do you agree with that, that in order to guarantee that the data will stay there forever, you need interest? And how are you going to build interest? So idea that what we will provide on our layer is only short term data availability, in which case each node does not store all the data, each node small portion of the data for the small period. Both the network bandwidth requirements and the storage requirements are other because you're scaling across the end nodes and whether data needs to be stored perpetually. I don't know. I think a lot of data is avenuescent and needs to be thrown out. Yeah, I disagree with that.
04:52:26.814 - 04:53:31.858, Speaker A: I mean the guarantees we availability are that there was an agreement that the data was a block time. After that everything is kind of altruistic general, in like an existing blockchain, right? You say you have a storage at the time the block was produced. Theoretically all of the nodes could trim that data immediately, right? So we are assuming that there is some period of ability. The bitcoin guys, I think argue about your recomputation stuff, but generally we're with there being data that goes away after some period of time. For applications, your requirements may be like a tax year, right? Like you have to have your records as long as you have to file taxes for it. After that, it's a question of why do you care about the trades you made three years ago? What was the question? I have a small comment on that. The availability guarantees are required, for example, to continue running a roll up, but to do this, the only need is the state.
04:53:31.858 - 04:54:40.770, Speaker A: So as long as you store recent transactions and the state in a scaled data availability layer, whether you need to kind of bootstrap that roll up from history and genesis is a kind of option which is only needed for very high security application, there'll be a lot of other applications for which that will be an overkill. Like you play, you start to do a computation which is evanescent and you just benchmark that state perpetually. That's our right. So I guess one question is, in terms of planning ahead next year, the next decade maybe, like how much demand spec there will be for data availability and can you provide for all? Can you satisfy the world's demand? How large is that? That's going to depend on how long we're in a bear market. There's not a lot of strong demand now. Right, I see. One thing I would say of roll ups have worked very hard in fashion and making sure that very limited data needs to be returned.
04:54:40.770 - 04:55:47.334, Speaker A: And I think the only thing we know how to do scalably provide in a decentralized system is data availability. So it seems like once people realize that there is data availability, there will be a new class of applications that can be enabled. Based on the way I think about it is not just throughput, it is also the so imagine competing with a single server system. What is the comparative cost of your distributed system, downloading and storing that data? We can actually get this to a small factor like a two x for a 50 redundancy or a ten x for 90% redundancy. I think there is a large scope of once the comparison to a centralized server is very minimal, you can start doing things that you would not be not blockchain. So our thesis is we need a lot of data availability. Once you have data availability for cheap, you can start moving application not thought to be possible on a blockchain.
04:55:47.334 - 04:56:26.330, Speaker A: On a blockchain. We would all probably better off if we didn't all have to hire people like t eleven s to gas golf contracts. So more data availability can help us there. So talking about all these challenges to production, the auditing and whatnot, who is going to be the first mover when production soon. This is one thing I learned. Don't make time coming aligned here. So we follow the protocol.
04:56:26.330 - 04:57:15.082, Speaker A: Yeah, I think you have strong opinions Zankrad, in terms of, I mean, I think definitely move very soon. I don't know who's going to be first. Clearly it depends also what exact trade offs you're willing to take, which is I think very different between the different protocols. I think for Ethereum it's sort of this first thing that 4844 that's going to hopefully be within six to twelve months will be live, but full data in a scalable way will probably take more like off the order of two years. Yeah, I mean, going a little deeper than just saying soon. Celestia intends to be kind of the first mover in this space. I think we've been working on this problem to some degree a lot longer than everyone else.
04:57:15.082 - 04:57:39.934, Speaker A: And as Dankrad mentioned, different kind of needs here. It's harder to integrate use case into an existing network. There are obviously benefits for that, right? Once DA is in protocol to Ethereum, you have what, a hundred million dollar market cap kind of securing that. So your security guarantees are much higher. Celestia can't hope to reach that overnight. But we do think we'll be able to be the first mover. We have some prototypes working internally.
04:57:39.934 - 04:58:30.740, Speaker A: We do have net that is live and has been live for several months now. Now I have a worry which is that the way that blockchains secure themselves is by selling block space. That's their source of income. Now it sounds like data availability could potentially be like a commodity which is cheap. And so does this compromise the revenue model and therefore the economic security, the market cap of the, therefore the whole model itself, because they won't, because data availability is by economic security. So how do you see that? Our view on this is, I think fundamentally there are three distinct costs required in maintaining a data availability layer. I think it's very similar for other.
04:58:30.740 - 04:58:56.780, Speaker A: Number one, you have the operational expenses. You have to actually run nodes which download and they have to pay for networking. They have to pay. This is operational costs. There's also economic and economic security that is underwriting whatever service is being provided. And this is based on the cost of capital. So if you want $10 billion taking, you need ten pr or whatever, so you need to pay back $1 billion back to the.
04:58:56.780 - 04:59:28.130, Speaker A: So this is again another important concept. And the is congestion cost, which is you have to pay for the externality you're inflicting on the rest of the network. So as you hit closer to capacity, pay a fee. I think right now what we are seeing is, are fundamentally only charging congestion costs. I think this is not really tenable. We have to have systems charge all three costs. The first one, which is the operational cost, is really in per byte.
04:59:28.130 - 05:00:16.578, Speaker A: The capital cost is in per dollar and cost is in per externality induced and how close you are to the capacity. I think as we grow this industry, block space becomes like cloud space. There is no such thing that is cloud congestion on Amazon. And that's why you're paying for the operational expenses for Amazon doing the thing that it's supposed. And we think about, when we think about pricing in our layers, we take a lot of cash. Sure, there is no congestion. When there is no congestion, and the dominant cost is capital, because operational costs are usually dwarfed by the cost of capital and the dominant cost is the cost of capital, then what happens is the more the roll ups, the lower the per byte fee for the first time.
05:00:16.578 - 05:00:53.994, Speaker A: See positive same side externalities, the more the roll ups come sharing in the same dollars worth of security per byte actually goes down. And this leads to a really powerful flywheel where more and more people want to participate. And that's how we think about pricing in our system. Super interesting. I think congestion costs, and focusing excessively on congestion costs is the only way to be viable, implies you're kind of user hostile to some degree. Like the data layer, to try to be users a high amount and artificially limiting the amount of data it can provide. Like, yes, there's limitations.
05:00:53.994 - 05:01:34.822, Speaker A: Being a commodity, no business really wants to be a commodity if it doesn't have to. We're trying to provide public goods here, right? We want to provide shared data availability. Like many use cases, we don't want to be artificially limiting something that we can technically, simply because we want to be returning a higher profit to validators. We have to demand for block space ahead of what we're currently able to provide. And that if we provide that, if you build it, they will. If we can build very cheap block space, then there will be people who will find useful things to build on top of that cheap block space. Just one thing I want to add is it's not fully fungible.
05:01:34.822 - 05:02:43.060, Speaker A: Like I said earlier, I think when the data with the core settlement layer, you have higher guarantees and people will be paying premium for it. That adds to the other point that Dankrad mentioned. The core philosophy underlying something like Ethereum is you want enough to maintain the ledger, and that is itself a huge in addition to the short term data availability and having these of guarantees bundled into one does provide a certain charging available to others. Josh, one thing you mentioned is that if we were to artificially constrain the block, know that would go to the out. There's a way to have it go to the token holders. Do you think that thing, and in the context of ethereum, we have a lot of income, right? We have so much more income than today. The supply has been decreasing and to our advantage for Ethereum to be like the amount of supply that we provide.
05:02:43.060 - 05:03:48.200, Speaker A: Do you think it'd be acceptable to do that if the income would go to the token holders as opposed to validators? I mean, I think you can compare this to many, many web where you often have a user acquisition growth phase and then you have a period where your vcs show up and they want to get paid, right? So you can say, okay, we have a sufficient number of users that hold Ethereum tokens and we're now going to go and pay dividends to all of these token holders based on our block fees. But that's going to reduce the amount of users inbound because you've artificially priced them out. I guess depends on what your intention is. Do you want more users on your network or do you want reward users on your network? Well, I could argue that there would be more users if the income goes to the token, then you have more economic security that attracts more users. So actually there is a positive feedback loop. But I guess if you're focusing on the security, I feel like security is something that there's a threshold for it per user, right. I think most people would argue that at a point of sufficient security it would be beneficial if it was higher.
05:03:48.200 - 05:04:24.690, Speaker A: Generally, we're assuming with proof of stake there is no individual actor sneak through and own a large portion of the stake. I think many people would know. I don't think that number needs to be a trillion dollars to prevent someone from acquiring all of this stake from a user coming in who says, okay, maybe I want to go day trade some tokens on a roll up on Ethereum. Are they going to say, well, am I willing to spend 1000 because there's only $150,000,000,000 worth of economic security here. That's probably not going to. But if they say okay, it's going to take me trade versus going to take me trade, right. That may be essentially a more limiting factor from an end user's perspective.
05:04:24.690 - 05:05:20.206, Speaker A: Okay, interesting. Dankrad, do you have any thoughts on the economics of data availability? Your question is kind of a little bit biased because it feels like you're thinking that demand that goes down because you have more data availability, the total fees paid will go down, but I actually, the total fees will increase in total. So I'm not really worried about the economic security, at least not at the high level of security that those that would decide to directly settle on Ethereum would be right. I mean, I would tend to agree. It's something we've seen historically with Ethereum. The more we scale the induced demand, the greater the total fees. So the individual keep going down.
05:05:20.206 - 05:06:19.350, Speaker A: But I think the total aggregate fees will go up as we scale. So that is what I'm hoping for. But there is a put forward by Polynaya and his argument is that once each sufficient capacity, sufficient scalability, there's just demand beyond, let's say a million transactions per second or 10 million transactions per second, and then there will be like a huge cliff and it will catastrophically go down. Do you see that as a big cliff or do you continue? The total amount of data availability demand is pretty complicated. This is basically asking how far our industry can grow. And I think we're all built here because we are very bullish about what we can do. But in terms of the pricing model, I think once you this three pronged pricing, which includes operational cost, capital cost and congestion cost, I think this appears extremely.
05:06:19.350 - 05:07:19.642, Speaker A: Second, you can start saying that if you need $20 billion in stake, then you have to return fee and you amortize that fee across all data availability users. So you can calculate this over a running average for the month or something, and then price security fee, which is based on the total amount of dollars at risk. So I think by coming up with innovative economic models, even if there is infinite supply, I think it's able to charge because you are not charging for the data availability, you're charging for the economic security. So the pricing should directly reflect that, as opposed to reflecting generally through congestion costs. I mean, you are basically arguing that I think there will be several data availability markets. There will be one and there will be a premium one, which is because you get direct settlement, like direct interaction with a settlement layer and so on. And I that is the difference.
05:07:19.642 - 05:08:01.794, Speaker A: Like in a market, you charge your cost, but you charge congestion if you are a premium product. But this issue is even true for Ethereum, right? If you suppose make capacity infinity even on a premium layer, the price might go to well, but I argue we can't. I think this is, for example, one of the differences we had. We would argue that on Ethereum should always be preserved. That's not protocol property we provide, but that's a social property we have. And I think that property is one we are not willing to break. So that will provide sort of a certain barrier.
05:08:01.794 - 05:08:41.206, Speaker A: I think that high level of service for which people will be willing to pay exactly Sriram, I back a little bit on your thesis that the capital costs can be amortized. Reason is that there's this concept that I call security ratio. Basically the total value secured divided the economic security. Now on Ethereum today it's about 20 x. We have $0.4 trillion secured in NFTs, EF and ERC twenty s and we have $20 billion in. And I guess if we want to have an invariant, let's say we want to have whatever security ratio of 100.
05:08:41.206 - 05:09:23.218, Speaker A: And anytime someone comes in, they need to contribute towards increasing the economic security. Don't amortize anymore. Would you agree with that? The thing is as coming in. So if you apply this thesis, right, like three, four. Ethereum had like if it was proof of stake, it would have had 100 million security. But today there is $300 billion in your skin is that they're all paying a fee that contributes back value of the token by 1559 and fee payment and staking and other things. So I think the same thing will be true even for a system like is as new users come in and maybe these are users need very high economic security.
05:09:23.218 - 05:10:09.310, Speaker A: Like they're playing a game running a social network, they're doing other things and they're okay with marginally increased capital cost. And as these users come in, they're all paying a fee together, which increases the value of drives forward more staking. So I think the system itself scale. Okay, optimistic. Yeah. I think there's like a general question of like does the use of the data availability, like new users use up more, they try to secure more assets than like the current ratio. But if we see much more data availability at lower cost, I think there's likelihood that we can support less financially valuable transactional, say right now the transaction so high.
05:10:09.310 - 05:10:44.940, Speaker A: We have primarily financial use cases because it's very easy to calculate that it's worthwhile to make this transaction because I get a profit. If you're going and playing a game, say, well, how much economic security did you need for your game? Or how much economic security value were you storing in your 30 minutes ephemeral game. Did that really take away from the security? Probably not. So we can see a lot more use cases that don't from the security budget in your ratio case. Right. Have time for one last question? I guess we can open it up to the audience. Anyone? A very good burning question.
05:10:44.940 - 05:12:17.802, Speaker A: Yeah, go ahead. I can repeat it. The question all the same. So one of the obvious tenets of web three is that we should all be able to control our data control information, but then also have the ability and the freedom to move to another platform if we're not happy with the existing platform that we're on. I think the thing that I'm missing when we talk about data availability and duration of holding data is what happens if I platforms, but all of a sudden the data that I want to take with me longer there. So imagine like you have some social application that's now on blockchain and you have all of your followers and you have your entire community and perhaps like tweets. Is this the data that we're talking about that's not after some period of time? Or are we talking about something else? Anyone want to take that? I have an answer, but I guess, I mean, it depends on the pruning guarantees and I'm not going to the timing of the pruning, right? But it's this question of the guarantee at time of new block and there's fundamentally difference, I think between a data availability chain and a data retrieval market and a data availability chain is guaranteeing that there was security and a consensus around the set of state at some period of state is like a nebulous bag of bytes, we'll say, but guarantee that your data will be retrievable for all time.
05:12:17.802 - 05:13:02.146, Speaker A: But if an active social network or something and you have active usage, the assumption is that state wouldn't be pruned because there is significant economic value type. But I guess in the moving thing, are you thinking like you need to dump all of your data off of it and move it somewhere else? Presumably you'd just query it off of the chain and then do that, and then your application to be continued using at any given time. Right. All of the data you need to be present. It's the difference between state and history, I think. To clarify, I think your question is about data storage and not data availability. So actually the platform you want to go to is ipfs or something like that, and not a data availability layer.
05:13:02.146 - 05:13:31.250, Speaker A: I would argue that Ethereum will be as good, but it will probably be too expensive for your application in practice. In the long term, maybe for the next couple of years it might be fine. All right, thank you guys. That's all time we have. Thank you. Thank you so much everyone. This was the first roll up day and I'm sure it won't be the last one.
05:13:31.250 - 05:13:40.420, Speaker A: So exciting space. There will be many interesting topics to discuss during the Devcon, so find us. Thank you so much.
