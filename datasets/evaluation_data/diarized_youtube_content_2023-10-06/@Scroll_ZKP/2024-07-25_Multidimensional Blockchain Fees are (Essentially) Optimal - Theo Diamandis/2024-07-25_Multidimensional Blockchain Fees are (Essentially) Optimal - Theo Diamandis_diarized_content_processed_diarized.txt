00:00:05.440 - 00:01:02.340, Speaker A: So let me start with thanking everyone who joined today. This is the first of Protocol symposium. It's a weekly seminar series focused on blockchain systems, protocol design, and everything that is in between. The first few talks that we have, like, scheduled so far are very deep towards, like, protocol design stuff. But the focus of this talk series is to talk about both theory and practice of designing protocols for blockchains. So you can expect all range of tags moving forward. If you are interested to give a talk, please reach out and please try to participate and share your insightful comments and questions to make this whole thing more rewarding for everyone.
00:01:02.340 - 00:01:49.856, Speaker A: Our very first speaker is Theo. Theo done his PhD in MIT focused on algorithm design and optimizations, and that's why most of his work in blockchain space also takes a lens of optimization to the problem. And he has done a few very interesting, influential work on multidimensional fee market design. And today we are privileged to have him here to talk about one of his recent works on why how to design multidimensional key markets, and why the design they propose is essentially optimal under center like definition. The floor is yours, Theo.
00:01:50.008 - 00:02:24.412, Speaker B: Thank you. Great. Yeah, thank you so much for inviting me to speak and for the introduction. So, yeah, my name's Theo. I just finished up my PhD in constrained optimization MIT, and I'm currently a research partner at Bain Capital Crypto. This work is based on kind of a series of two papers with some of the usual suspects as collaborators who I'm lucky to work with, including Guillermo Alex Roon and CMF. And here we're going to talk about multidimensional blockchain fee markets, really from the perspective of congestion control.
00:02:24.412 - 00:03:20.218, Speaker B: So you should think of this like your mental model should be, are things like base fees rather than, say, like fees for MeV or something like that. So this is kind of the fee that is paid by everyone that uses the protocol for the protocol to ideally kind of, let's say, allocate resources in an acceptable way. And the other thing is this perspective. Instead of looking at how to design the fee mechanism itself, instead we're going to take a little bit of a different perspective and first ask what we want out of the protocol and kind of how we should think about resources, what we want the capacities of those resources to be, kind of what we want the steady state and the birth states of the protocol. And then out of this design spec, we'll essentially turn a crank and get a fee mechanism. So it's a little bit of the reverse. Instead of starting with the fee mechanism.
00:03:20.218 - 00:04:13.892, Speaker B: We're going to start with what we want and then get a fee mechanism out of that. And I hope to convince you first off, that fee markets with fixed relative prices of resources, so these can be anything from opcodes or at a higher level, like things like storage. Compute bandwidth are relatively inefficient. And so, as I said, this work is going to be a framework to optimally set multidimensional fees for congestion control. Again, we're not really talking about people bidding for certain mev slots. Instead we're talking about, you know, the ability to get a transaction included on chain in a particular block. And just to, you know, first outline this with a really simple example, let's consider a mempool that has a bunch of transactions, and for the sake of argument, one transaction is only going to require compute and the the other is only going to require bandwidth.
00:04:13.892 - 00:05:02.990, Speaker B: And say, the compute transactions and the bandwidth transactions are kind of the same cost. So it's one unit of compute versus one unit of bandwidth, but the utility from the compute transactions is quite a bit higher, let's say it's four, and the utility for the bandwidth transactions is a bit lower, let's say it's two. Let's consider a 1d gas market. So everything, every unit has a price of three. Well, what would you end up doing? Say you have four compute slots in the block, and four bandwidth slots, you would fill all of the compute up because the utility is four. So even after paying the gas fee, this is still a net good thing or positive thing to do, but you wouldn't use bandwidth at all. And this is the type of situation that sometimes you can see with things like NFT mints, where there's a high amount of congestion on one particular resource.
00:05:02.990 - 00:05:58.900, Speaker B: So that's on kind of the EVM land. In say, Solana land, there might be a particular account that has a huge amount of congestion, because that pushes everything else, inclusion fees, much higher. Other stuff won't actually end up being used at all, despite the fact that it's easy to fill up that as well if we wanted to optimally use the block. So if we had a 2d gas market where, say, CPU is priced at three, but bandwidth is one, then what do we do? We fill the block with CPU transactions, and we also fill the block with bandwidth transactions. Now our throughput has effectively doubled. Of course, this is kind of a simple silly example, but we see this type of thing play out in practice all the time. And this leads us to the natural conclusion that orthogonal resources so perhaps accounts that are entirely separate from each other or resources that have minimal interaction with each other should be priced separately.
00:05:58.900 - 00:06:30.320, Speaker B: And so let's talk about like a little bit more formally of what I mean when I talk about transaction resources. So that we can get a little bit into the mathematic. And when I say a resource, I really mean kind of anything that can be metered. So anything that I can say how much of this you're using. So for example, this could be like blobs. It could be big categories of things like how much compute, memory or storage you're using. But it could also be very granular categories like opcodes.
00:06:30.320 - 00:06:54.916, Speaker B: Perhaps you even want to do sequences of opcodes. As an example. If you call a memory slot multiple times. Once that slot is hot, it's cheap. So perhaps you want to actually differentiate between the hot slot and the cold slot. You could do that as well. You could also, if you're trying to do some type of parallelization, have compute on a specific core via particular resource and so on.
00:06:54.916 - 00:07:34.864, Speaker B: So you can imagine this is a very general construction. And this is where a lot of engineering design choices have to be made as to what the resources are. For the remainder of this talk, I'm going to talk about resources in the abstract, just as this thing that can be metered. But this is like a lot of complexity in terms of putting this into a real world system that I'm sweeping under the rug. And I can point to a few examples of where this type of thing is implemented if anybody wants to dig through those code bases. So to formalize this, we're going to say a transaction j consumes a vector of resources. That vector of resources is going to be aj and there's going to be m different resources that we can choose from.
00:07:34.864 - 00:08:33.600, Speaker B: So essentially the I entry of this vector is going to note the amount of resource I that is consumed by transaction j. Then we have a vector x, which is just a zero, one vector which says which of n possible transactions are we going to put in the next block. So if an entry is one, that means that transaction XJ is included and otherwise it's zero, which means that we don't include transaction Jenna. So then we can write out the quantity of resources by simply multiplying XJ, which remember, is a zero one by the vector of resources. So this is going to be zero if we don't include it. If we do include this transaction, we're going to add this vector, sum these all up, and we get this quantity of resources consumed by the block y, which can be written in matrix vector notation. There's just a where each column of a is going to denote a specific transaction.
00:08:33.600 - 00:09:23.396, Speaker B: So then we are talking about, you know, kind of how to do congestion control. So we want to have some notion of, you know, consumption targets and consumption limits perhaps for each resource used. So in this talk, we're going to talk, we're going to denote the resource consumption target by a vector b. And this means that we can write the deviation from the target as just ax minus b. So if we're using too much of a resource ax, which is going to be the amount of resources used by a block, is going to be high, like say the I th component of ax. If we're using too much of resource I more than the target ax minus b, that the ith component of that vector is going to be positive and otherwise it's going to be negative. Then ethereum.
00:09:23.396 - 00:10:19.728, Speaker B: Currently there's, I guess, a 2d gas market, one for blobs, one for gas or blob gas, and regular gas. But b star for regular gas is going to be 15 million. More importantly, perhaps, we also typically have some notion of a resource consumption limit. So some maximum throughput that we want to have of a particular blockchain system or roll up and transactions included must satisfy that the quantity of resources used is less than this limit, which we can write as ax is less than or equal to b. Finally, because we are talking about pricing mechanisms, we want to charge for uses usage of each resource. We're going to denote the prices by a vector p and the j. Essentially this means that the ith component of p is going to be the fee for per unit of resource I.
00:10:19.728 - 00:10:45.720, Speaker B: So that means that for a particular transaction, we charge p transpose aj, which is just the sum of the price of each resource, times how much of that resource is used by a particular transaction. And again, this is kind of between the chain and the people submitting transactions. You can think of this as what's burned. Yeah, there's a question.
00:10:47.140 - 00:10:56.308, Speaker A: Yeah, just, just quickly checking, understanding things correctly. So a, here is a matrix representing transactions in the mempool, right?
00:10:56.484 - 00:10:57.200, Speaker B: Right.
00:10:58.540 - 00:11:02.924, Speaker A: So it's not the space of all transactions, the transactions available in the mempool.
00:11:03.052 - 00:11:13.400, Speaker B: No, it's just we're taking all the transactions in the mempool and we're concatenating them together. So each column of a is, represents a specific transaction. And this is mostly for notational convenience.
00:11:14.510 - 00:11:15.410, Speaker A: Thank you.
00:11:16.950 - 00:12:00.024, Speaker B: And then, so AJ for a particular column is going to be the resources used by some transaction that's in the mempool. And of course it doesn't necessarily. This could be a private mempool, a public mempool. The mempool I use as a term just to indicate all transactions that are, say, a particular block builder can view. And then, so, starting to get to the meat of things like, well, what do we want to do to determine prices? There's a few properties that are quite obvious. First, if we're using the exact amount of resource I, that's our target. So remember, b is the target, then we probably don't want to update.
00:12:00.024 - 00:12:51.392, Speaker B: This means that like our price is pretty good if we're using more of a resource than we want. So more of the resource than the target, that likely means that we would have to increase the price. There's a question of how much to increase the price, but we want to increase the price because this means we're undercharging for that resource and vice versa if we're using too little of a particular resource. So the ith component, the ith resource is under our target bi, then we probably want to decrease the price because we're overcharging for this resource. And so let's consider a proposal. So this is a popular proposal for multi dimensional fees. This is essentially a multidimensional version of 4844, and this essentially just takes the current price and multiplies it by the element wise exponent of some constant times that residual that I mentioned earlier.
00:12:51.392 - 00:13:57.550, Speaker B: You can go through these properties and check pretty easily that essentially it has all these properties. But there are many other proposals that also would have all these properties. So that begs the question, is this a good update rule? Does this do what we want? And as I said at the beginning, we're going to take this inverse approach. Or instead of looking at an update rule and trying to reason about it, we're going to try to specify what we want as an optimization problem. And then we're going to come up with a pricing rule or a price update rule that implicitly solves set optimization problems and the specific choice of that optimization problem, in particular the objective function of that optimization problem, which says what we want is going to then return the crank and give us a specific update rule. So instead of trying to analyze the fee mechanism directly and say, does this give us what we want? We're going to specify what we want and then come up with the fee mechanism. So let's talk about that optimization problem.
00:13:57.550 - 00:14:42.800, Speaker B: We'll start out in this setting, which is unrealistic but we'll see that this doesn't matter. But for the sake of exposition, we're going to consider a network designer that's totally omniscient and determines the transactions in each block. This network designer knows all the utilities of everybody submitting transactions and essentially gets to pick the blocks themselves. I said that we need some way to specify what we want, and we're going to do that by a loss function. So this loss function is going to tell us essentially our unhappiness with resource utilization. And the network designer determines this loss function for the particular resource allocation. So for the resources that a particular block uses.
00:14:42.800 - 00:15:18.702, Speaker B: And we could come up with kind of very silly examples. So this one says, this loss function says that if my resource usage in the block is exactly equal to my target, I'm not unhappy at all. My loss is zero. But if I have any deviation, I am infinitely unhappy. So any deviation lower or higher, this is potentially silly, but you could imagine having some smooth gain and loss as you move away. It's perhaps a quadratic penalty. We could also say we actually don't care about underutilizing things.
00:15:18.702 - 00:15:47.638, Speaker B: We only care about overutilizing things. So if we're underutilizing or if we're under our target, then we have no loss at all. And otherwise we're infinitely unhappy. The next ingredient that we want to introduce is a constraint set. So we're going to say this is a set s, which is going to encode the set of allowable transactions. So this will encode some things like network constraints. So for example, we have to be under the resource consumption limit.
00:15:47.638 - 00:16:44.292, Speaker B: Earlier we encoded this as ax is less than or equal to b, but it also could encode arbitrary and very complicated interactions among transactions. As an example, if two people are bidding for a particular arbitrage, like atomic arbitrage opportunity, only one of them can be included in the block. So these types of things will be encoded in the set s as well. And you can think of this as just some complicated set, um, that essentially says, what are the allowable transactions in a particular block? Or what combinations of transactions in the mempool are allowable? We're going to do a little bit of a sleight of hand here and we'll see that we can kind of undo this later. But instead of considering s directly, we're going to consider the convex hall of us. And what this means is that a transaction can be partially included. So if we have an xj, so remember, that was originally a zero one vector.
00:16:44.292 - 00:17:28.148, Speaker B: But if instead, it's something like one half. That means that that transaction will be included roughly in the next two blocks. So another way to think of this is it's kind of the probability that that transaction gets into the next block. And then the final thing that we have to introduce to have this resource allocation problem is some utility function of the transaction producers. So this is essentially going to be users plus the validators, plus the block builders, everybody interacting with the chain. And this means that, let's say a user has to bid a high amount to get the transaction to the block, but that bid is going to say the validator or the block builder. This is going to net out in this utility.
00:17:28.148 - 00:18:02.016, Speaker B: So this is really just the utility that this entire group of people submitting transactions and everybody in that pipeline before it hits the blockchain gets. And we're just going to specify this as a number. So if transaction j is included, the transaction producers get a joint utility. QJ. Of course, we almost never know this. In practice, you can make an argument that even people submitting transactions don't know what their exact utility is. But we'll see that the network does not need to know this.
00:18:02.016 - 00:18:55.008, Speaker B: So we don't need to have any knowledge of what Q actually is, but we're going to use it as a modeling technique to specify this problem. So then what's the resource allocation problem? The resource allocation problem is to maximize the utility of the included transactions minus the loss incurred by the network. One thing to note here is these two things kind of have to be specified in the same unit. So you can think of pick your favorite numerair and specify these things in units of that numerator. So whether that's, you know, eth dollars, whatever. And then of course subject to the constraints that the utilization, the resource utilization is the exact resource utilization of all the included transactions. So that's y equals ax, which we talked about earlier, and that the set of included transactions x is in the set of allowable transactions.
00:18:55.008 - 00:19:40.862, Speaker B: So essentially that this block is like feasible in some sense, that the consensus algorithm or the consensus protocol that everybody's running won't reject this block. Of course, as we mentioned before, s can be quite complicated, and this is kind of outsourced to the block builders currently on Ethereum. Okay. And I mentioned that we're in the setting of an omniscient network designer. And of course there's like a lot of problems with trying to solve this problem directly in practice. First, the network designer doesn't decide which transactions are included in a block the block builders do this, and the block builders don't care about the loss that's incurred by the network. They only care about the set of allowable transactions because that comes from consensus.
00:19:40.862 - 00:20:14.792, Speaker B: But they don't care if the network is running overheated for a while, as long as that's allowable. Of course, the network designer also doesn't know the utilities of the transactions. And as we said before, we can include fractional transactions. But the convex hall here does allow for fractional transactions. And so this is going to bring us to start turning the crank through duality theory. And duality theory is this very deep result in convex optimization that allows us to construct another problem. And that other problem is going to be something we can actually solve.
00:20:14.792 - 00:21:08.250, Speaker B: And it turns out that this second problem is going to be exactly equivalent to the original. And the core of this theory is taking the constraints and relaxing them to penalties. So one thing to note in this problem is that the network designer cares about utilization. But the network designer doesn't really care about the transactions in each block, except insofar as they affect the utilization. On the other hand, block builders really only care about what transactions they can put in. They don't care about the overall resource utilization of the block, except insofar as it gives them constraints on what they can include. So what we're going to do is we're going to take this constraint y equals ax, and we're going to essentially relax it to a penalty to decouple the utilization of the network.
00:21:08.250 - 00:21:56.900, Speaker B: So that's the loss function, which is a function of y and that of transaction producers, which is that utility term, which is a function of xdem. And duality theory tells us that if we correctly set this penalty, excuse me, the dual problem, or solving the dual problem is essentially equivalent to solving the original problem. And the utilizations are going to be equal. So if we solve the dual problem, essentially the utilization that that would imply is going to be the same as solving this original problem, which is sometimes called the primal problem. So the dual is actually going to be to exactly find the prices. So this is a penalty. So this gives us some notion of prices that minimize a dual function, which we're going to denote as g of p from before.
00:21:56.900 - 00:22:35.236, Speaker B: These prices are going to be these prices, p are going to be the prices for violating that constraint. Y is equal to ax, as you can see now earlier, why I used p in originally introducing kind of fee mechanisms is fee mechanisms are exactly going to be methods of solving this dual problem. So we're going to relax that constraint to a penalty and essentially force people to pay per unit violation. This problem ends up being separable. So, GFP has this form, which looks a little bit complicated, but let's walk through it. There's two terms here. The first is what's called the central conjugate of the loss function p.
00:22:35.236 - 00:23:10.920, Speaker B: This is something that it comes from optimization theory. I would think of it as a function that is in most cases very easy to evaluate and has a closed form solution. As a simple example, if our loss function is quadratic, this conjugate function l star of p is going to also be a quadratic function. So we can kind of turn the crank and go from a closed form of a loss to a closed form of the vengeful conjugate. So just think of it as this like easy to evaluate thing. The second term is a little bit more complicated. So let's look at it.
00:23:10.920 - 00:24:01.286, Speaker B: And the second term can be interpreted as maximizing a net utility. So q, remember, is the utility of including a particular transaction. And then a transpose p is going to be the cost based on prices p of that particular transaction. So that means q minus a transpose p is the net utility of including that transaction. And so what this problem says is choose the x, so maximizing over x. So choose the transactions that maximize net utility, subject to the constraint that I mentioned earlier, that this is in the set of allowable transactions. One thing to note, and this is why I said the convex hall doesn't really matter, is this actually has the same optimal value if we use s instead of the convex hall of us.
00:24:01.286 - 00:24:29.744, Speaker B: So we don't have to say solve this relaxed version of this particular subproblem. We can solve it. Exactly. And it's the exact same. And you know, I, I call this the block building problem, because it is the exact problem solved by the block producers or the block builders. What they're trying to do is they're trying to maximize, find kind of the set of transactions that maximize the net utility. This means that the network can observe the optimal value of this optimization problem.
00:24:29.744 - 00:25:18.316, Speaker B: So the block producers make a block that has some list of transactions, and the network can assume that essentially they are at least approximately solving this maximum utility problem of, based on the prices set by the network, they're including the highest net utility transactions. So, before we talk about the pricing mechanism, let's examine this problem a little bit from kind of the theoretical point of view. So, let's assume that we have the optimal prices. So we'll have p, which is the optimal prices. And that's a minimizer of this dual function. And then we're going to assume that the block builder has come up with kind of an optimal block, which we'll call x star. The optimality conditions are essentially that the gradient of the dual function is zero.
00:25:18.316 - 00:26:02.556, Speaker B: The gradient of, it turns out, is just y ax star. So what is y ax star? Well, remember, that's the residual. That's essentially the resource violation. And that says that that's zero. So this says that at optimality, we essentially get no violation of that constraint. And y here is going to satisfy this equation which says that the, essentially the, the gradient of our loss function evaluated at y is exactly the prices. So what does that mean? That means that the prices that minimize g to minimize the dual problem charge transaction producers exactly the marginal costs faced by the network.
00:26:02.556 - 00:26:43.458, Speaker B: The marginal costs faced by the network are encoded in the loss function. So essentially, as the loss changes based on wherever you are, that's kind of the marginal cost that you're incurring by moving from one resource utilization to another. And those are going to be exactly the prices at optimality. So this is more or less what we want. The loss function should encode how unhappy we are with a particular resource utilization. And we can think of that as kind of encoding these like costs. Furthermore, these prices are exactly those that incentivize the transaction producers to include transactions that maximize welfare generated minus the network loss.
00:26:43.458 - 00:27:32.962, Speaker B: In other words, the things that are the prices that cause us to solve the dual problem, incentivize the transaction producers to implicitly solve the primal problem. Okay, so this is kind of all, these are all good things. These are all more or less like what we want out of this type of fee mechanism. The question is, well, how do we actually minimize this function g of p? Because, as I said, we don't know the utility. So there's no way for us to compute what the utility of a particular block is. But what we can do is we can compute the gradient. So the gradient is just given by y, which is something that's kind of easy to evaluate on chain minus the resource utilization of the last block.
00:27:32.962 - 00:28:11.732, Speaker B: So we actually never have to form a and x here. All we have to do is we have to look at the resource utilization of the last block, which is going to be the ax star terminal. So the network is going to determine this y, which is computationally easy. This comes out of the central conjugate, and then the network is going to observe x star, and therefore the resource utilization from the previous block. So this is the solution to that block building problem. Then, once we have the gradient, the network can apply whatever our favorite first order optimization method is. So, for example, gradient descent or variant thereof to compute a price update.
00:28:11.732 - 00:28:12.920, Speaker B: Yes. Question.
00:28:15.540 - 00:28:46.430, Speaker A: Yeah, I mean, it's about the last slide where you're talking about the gradient of the loss function determining the price. So I'm wondering if you have, like, a block of transactions. Like, think of me as a user making a single transaction. Does the price inside the block changes for me in the. In the model you're thinking about? Or like, say, like we have one x, and then we update the x to include my transaction. Is that how.
00:28:47.010 - 00:29:17.474, Speaker B: No. So we're kind of, prices are fixed for a particular block. Like, we set the prices, we solve the block building problem, we update the prices for the next block. So think of this like, similar to the base fee on Ethereum right now. So right now, I've only talked about the kind of single shot case. So I'm assuming that kind of, you know, like, in this, this slide in particular, I'm assuming that we actually have, like, the optimal prices already. And we're only doing.
00:29:17.474 - 00:29:24.270, Speaker B: We're only constructing one block. We'll extend this to kind of multiple blocks and a few slides.
00:29:25.010 - 00:29:25.506, Speaker A: Got it.
00:29:25.538 - 00:29:30.790, Speaker B: But we have to kind of do the single block thing before we can talk about multiple blocks.
00:29:31.740 - 00:29:46.400, Speaker A: Got it. So, assuming currently the prices are optimal, given the optimal solution for the selection of transactions available in the maple, this is the optimal way to update the prices to get to the next optimal.
00:29:47.260 - 00:30:33.150, Speaker B: Well, this slide doesn't talk about updating the prices at all. So this slide, what this slide says is, let's assume we have the optimal prices. So we don't know anything else. We have optimal prices, and then we charge. So we have the optimal prices, and then we charge essentially these prices. If we're charging these prices, then the block builders will include the optimal transactions, or essentially the transactions that maximize their welfare minus the network's loss. So this is saying, essentially, if we kind of set prices correctly, then we're in a very good situation, because essentially we set the prices correctly, and everything else that we fall out that we want falls out of that.
00:30:33.150 - 00:30:51.390, Speaker B: We have to talk about then how to set prices correctly, which is what the next slide is, is we have to have some method, because we don't know what prices are necessarily. So we have to have some iterative method that tells us how to set prices. And we're going to talk about that in the multi block case soon. Yeah. Second question.
00:30:52.400 - 00:31:24.612, Speaker A: So can you please go to the next slide? I have question. So here you're basically calculating the gradient. Are you calculating this on chain? I mean, my question is that, as far as I understood, the a matrix is representing all of the transactions in the mempool. So when you're calculating something unchained, you don't have any idea what's going on in the mempool. So it means that a should be representing all of the mine transaction, not those transactions.
00:31:24.676 - 00:31:54.610, Speaker B: No. So remember, like I said when I was going through this slide, x star is only the transactions we include. So Ax is just a shorthand for the resource utilization of the block that we observe. So we don't care. We don't actually need to form this matrix at all. All we need to do is we need to observe the resource utilization of the last block, which is something that, of course, as soon as the block is published, we know. Right.
00:31:54.730 - 00:31:56.122, Speaker A: Okay. Okay.
00:31:56.226 - 00:32:26.930, Speaker B: But yeah, it's a little bit. Yeah, it's a little like it is something that. I'm glad you asked to clarify this because we don't actually have to form this. It's just to differentiate. Ax Star here is the resource utilization of the previous transactions, whereas y actually comes directly from that central conjugate function. So y, if, you know, we don't have prices set optimally, then y may not be exactly equal to a times x star.
00:32:27.790 - 00:32:35.890, Speaker A: Yes. And y star is the target usage of the resources. So what was it more?
00:32:37.710 - 00:33:13.446, Speaker B: No, not as well. Sometimes it will be y comes from this equation here. So essentially this just falls out of. If we have. So for some prices, p, maybe not p star, but just for some prices p, we compute the y such that this equation is true. And this, this essentially just comes out of how the form, this ventral conjugate function, it's just the gradient of this thing. And you can think of that on.
00:33:13.478 - 00:33:14.770, Speaker A: Chain or off chain.
00:33:15.910 - 00:33:29.490, Speaker B: It can be calculated on chain. It's easy to like something computationally easy, as I would say, have for the mental model that oftentimes it'll just be like the gradient of a quadratic function, let's say.
00:33:30.400 - 00:33:32.340, Speaker A: Okay, thank you.
00:33:36.280 - 00:34:37.878, Speaker B: So just to go for a few simple examples. These are kind of silly, but as I said earlier, if we kind of have this like zero infinity, where we're only unhappy, we're only happy if we're exactly equal to the target, we get an update rule that looks a lot like gradient descent, where we're just essentially taking the prices and modifying them by the residual and you can check that kind of offline, that this has all the properties we want. If instead we have this loss function where we're actually not unhappy if we're underutilizing the chain, but we're only unhappy if we're over utilizing the chain. We get the same thing, but we have this operator here which says that we only take the positive part of this. So what this means is in the second case, we can't have negative prices, whereas in the first case we can have negative prices. And this comes exactly from how we specified the loss function. So in the first case we actually said we're very unhappy if we're underutilizing the chain as well.
00:34:37.878 - 00:35:19.590, Speaker B: And so of course, what would that imply is that we might want to subsidize usage of the chain. In the second case, we said we don't care if we're under our target. We only care if we're over our target. So then we would never subsidize usage. And our prices would always be non negative at most, or, sorry, at least we would have a zero price for using some resource. And then there's the one I introduced earlier, which unfortunately I don't have time to get into the details of in this talk, but we go through this in the paper. And in fact, if you want to see kind of a in depth exploration of 1559 or 4844, kind of the 4844 version of it, that's in one of the appendices of the most recent paper.
00:35:19.590 - 00:35:58.208, Speaker B: Okay, so let's talk about the multi block case and kind of why gradient descent makes sense to use iteratively, as in multiple blocks of transactions. And for this, let's think about a game with two players. So there's the block producers on one side and the network itself on the other. So what's going to happen in each round? Well, the network is going to publish prices that it shows, say, with gradient descent. So it's going to publish prices for the next block. Then users are going to submit transactions with some utilities and some resource consumption. This could be adversarial.
00:35:58.208 - 00:36:40.592, Speaker B: We don't know how the users are going to submit transactions. They could be trying to denial of service, attack the chain, or they could just be trying to buy their favorite meme coin. Who knows? Then the network is going to receive that payoff. So, from the dual, and notice here that everything is indexed by the block, which I'm going to denote by the index k. And so the network is going to receive a payoff, which is going to be the dual function, and then is going to update the prices accordingly. So we have to have some metric for how well the network is doing in this. And we're going to use this metric called regret, which is from the online learning literature and the online optimization literature.
00:36:40.592 - 00:37:27.170, Speaker B: As you can see, there's two terms here. The first term is essentially the sum of all these network payoffs, well, the average sum of all the network payoffs of every single block from some time one all the way up to some time horizon, which I'm going to denote my capital t. The second term says that instead of choosing these prices with gradient descent, what I'm going to do is I'm going to choose the best fixed prices, assuming I know all the transactions that are going to be submitted for the rest of this time period. So think of this time period as like some epoch. And I'm going to say I actually know every single transaction that's going to be submitted. I'm omniscient, I have a crystal ball, but I have to choose fixed prices. So I'm going to choose the best fixed prices for this.
00:37:27.170 - 00:38:57.010, Speaker B: And then the metric that I look at is essentially how much worse does doing these dynamic prices where users can be very adversarial, they could submit transactions that are trying to screw over the network in any way possible. How much worse is that going to be versus this baseline of choosing the best fixed prices with the crystal ball? The main result of the second paper is that gradient descent with a particular step size chosen. So essentially with the step size that's a constant times one over the square root of the epoch is going to give us a regret that decays as one over the square root of the length of epoch. So what does this mean? Well, as this like epoch gets very long, the difference between kind of the average that you, the how well you do in the case where you're using this dynamic update mechanism, and the case where you have the crystal ball is going to go to zero. And so this is oftentimes referred to as o of one over square root of t is regret because we ignore the constants. And as I said, this goes to zero as t gets large. So one really important thing here is that this result does not assume any model on how people submit transactions.
00:38:57.010 - 00:39:29.692, Speaker B: They could be as adversarial as possible. There's no assumption that there's some distribution people are pulling from. There could be particular agents that are messing with the protocol and the blockchain setting. I think these types of results are very powerful because they are worst case results. They are results that assume an adversarial environment. All of us that work in the space know that these environments can be quite adversarial at times. And this is when I say multidimensional fees or fees set in this way are essentially optimal.
00:39:29.692 - 00:40:10.754, Speaker B: It's because we have this regret that goes to zero as t gets large. This is a result from online convex optimization, which I think there's a lot more to do in applying results from online convex optimization to dynamic mechanisms that are enshrined in a protocol. So things like fee mechanisms, as we saw here, because they have these adversarial guarantees. In addition, one thing that's kind of an interesting side note is this doesn't actually assume that we ever converge to the optimal fixed prices. P. We actually never need to get to the optimal prices. As long as we run this algorithm, we still get this o of one over square root of T.
00:40:10.754 - 00:41:06.370, Speaker B: Regret. And one other thing to note here is that essentially these constants, m and b, roughly come from the size of the block that we allow. So in some sense, this could, this would one way to interpret this. I would be very cautious of interpreting it this way, but this one interpretation is that if we have smaller blocks, this is going to be a smaller constant over here. So smaller blocks, longer time periods are going to give us, or smaller, faster blocks might give us lower regret. That being said, this is a theoretical property. So this more, I would say, as giving intuition as to why gradient descent, or some type of gradient descent type algorithm, is a good thing to do, rather than, I would say, interpreting it super literally as to telling you exactly, you know, how you should set parameters.
00:41:06.370 - 00:41:47.774, Speaker B: Setting parameters, unfortunately, is still a challenge that very much must be done by a lot of trial and error. And that's, that's, you know, part of the design of these systems. So, just to recap, as I said, the scheme is kind of optimal in the sense that at zero grad on average, which comes from online convex optimization, and I kind of prefer these types of results. As was mentioned earlier, I'm an optimization PhD. I kind of view things through these particular lenses, and game theoretic results can be very useful in a lot of cases. But I actually think the online convex optimization results here, because it doesn't require that the adversary is rational. We only assume the adversary has a bounded budget.
00:41:47.774 - 00:42:46.314, Speaker B: And the bounded budget can just be actually from the fact that we have a limit on the block size. See, there's only so many things that you can put in a particular block. You don't need any notion of agents playing to equilibrium. They can kind of do whatever they want. And just to conclude before questions with a really kind of simple example is we can see that if we have a scenario where in this scenario, essentially we had two types of transactions. One type had one resource utilization profile, another type had another resource utilization profile. At block ten over here, we insert a huge number of the second type of transactions, and that use a lot more resource to, as you can see in the multi dimensional case on the left, what happens is that essentially prices will adjust, they will allow for this kind of burst capacity of the second resource, and then we'll get back to a steady state afterwards.
00:42:46.314 - 00:43:29.108, Speaker B: So these dashed lines are our targets and these dotted lines are our limits. So this is the burst up to the limit and so on. You can actually see for the first type of resource, we underutilize it here as we're clearing the second type of transactions, and then we kind of clear the backlog right here, one dimensional. These on the other hand, actually have this kind of chaotic oscillating behavior. And this is something that you can see in practice as well. There's a great paper I have in the references of my paper that kind of does a great study of observing some of these types of phenomena. And we get this oscillatory behavior, and it takes quite a bit longer for the resources to kind of go back to the levels that we want.
00:43:29.108 - 00:44:18.834, Speaker B: So this is why I think multidimensional fees, kind of from the throughput perspective that I mentioned earlier, also are really where we want to go. Just to wrap up, the conclusion here, I think is really that instead of focusing on choosing and carefully crafting the update rules, instead what we should do is choose the objective function. So choose what we want out of the network, then choose the algorithm that we use. And I didn't have as much time to talk about the details of the algorithm, but there's a lot of discussion of that in the paper. And then once we have kind of the objective function and the notion of algorithm, we turn a crank and we get the price update rule via this framework. So we don't actually have to give any thought into the price update rule itself, and we have strong guarantees on how it behaves. Then we saw these price update rules.
00:44:18.834 - 00:45:23.712, Speaker B: Actually, there's basically no difference between correctly fixing prices with an oracle or crystal ball that gives us knowledge of the future and using these online gradient descent type algorithms on average. So kind of in the long run, and that was the main result of this of the second paper, that these things are zero regret. And again, to emphasize these results hold without any assumptions of demand distributions or market clearing prices. Really the only thing that we need is some notion of like a block time and a maximum size of a block, which could be even, it doesn't have to be a block, it could be like a maximum number of transactions or resources that can be consumed per unit time. And from that type of thing, these results essentially all follow. So there's a lot of extensions to this. And just for some of like the academic stuff, I think that there's some interesting kind of, this framework does extend to the throughput in like a per contract or per account sense, which would be, for instance, as in Solana.
00:45:23.712 - 00:46:02.574, Speaker B: So resources, there kind of have a different notion of like resources in the EVM world. And there's some things that I think have to be kind of carefully thought through and applying this framework to that. But the extension is pretty direct. As I mentioned earlier. One really important thing is this framework doesn't tell you what the resources should actually be. And of course, a lot of this complexity ends up being outsourced to the developer and how they should construct their smart contracts. And so there's a question of what the trade off should be between complexity and kind of the granularity at which you can run the chain.
00:46:02.574 - 00:47:01.726, Speaker B: If we could price every sequence of opcodes independently from each other, that would probably be the maximum throughput, but that would not be very easy to use. Also, on the other hand, there's kind of the question of what the loss function should be. So what are the desired performance characteristics of a particular system, and how do we design a loss function that gives us that, or not that gives us that, but that encodes our exact preferences. There's some implementations of these multidimensional fees, in particular by the avalanche and penumbra teams. And so these, they made obviously choices into what the resources should be and so on there, which can provide some insight and actually like blob pricing as well. As I said, 4844 can be directly analyzed in this framework, and I'm happy to talk about that offline. But essentially, 4844 is a particular loss function with a particular choice of online optimization algorithm.
00:47:01.726 - 00:47:50.740, Speaker B: And with that you get the exact form of the 4844 price update rule. That leads us to kind of the next question, which is, which is which update rules or which update algorithms? Online optimization algorithms are most useful. And so there's some trade offs here between, say, convergence behavior and complexity. To what extent should different resources be coupled where. So, for instance, like some resources also, you might want to increase their capacity or increase the price more rapidly than others because they're not able to sustain burst capacities for as long and so on. So there's a lot of design decisions that have to be made even within the context of this framework. But I hope that this framework kind of gives a nice map to start to fill in the lines of.
00:47:50.740 - 00:48:17.556, Speaker B: And as I mentioned earlier, while I talked about key mechanisms, this is kind of likely relevant and useful for many similar mechanisms. So any mechanism that's protocol enshrined, where the protocol has to have a set way of doing things, and some people are paying that protocol. So this, you can imagine there's a lot of Defi applications in this type of work as well. And. Yeah. And so for more info, please check out the paper. There's two papers here.
00:48:17.556 - 00:48:39.662, Speaker B: This is the more recent one, which has all the online optimization results and kind of a summary of the first paper. The first paper goes a little bit more kind of in detail through some of the modeling decisions. And please reach out if you have any questions. Thank you. I'm also happy to take questions for the remainder of the time. Perfect.
00:48:39.726 - 00:48:51.650, Speaker A: Thanks a lot. Amazing talk. I have a lot of questions myself, but I want to let others ask their questions first. So feel free to unmute yourself and ask questions.
00:49:02.440 - 00:49:34.050, Speaker C: I can have a question. Sorry, this is covered. I might have missed it in your talk. I'm curious what your thought is about the contextuality of these fees or resources, because at least in the example, it was like a static. You know, in your early example, it was like a cost of two or four. But for many transactions, especially, for instance, for proving costs, it really depends on what transactions come before the transaction. You know, the overall structure of the block, is that considered in this model?
00:49:34.750 - 00:50:09.864, Speaker B: You can consider that in this model where perhaps like a resource, depending on if. Like there's like, if a slot is like, hot, then that's a different resource than if you're trying to access a cold slot. And that kind of comes down to how you choose the resources. And again, there's kind of a. What level of complexity do you want there? So right now, of course, a lot of that is. Is entirely static. And not, not only static in the sense of, like, that in particular doesn't matter, but like, say, like a s.
00:50:09.864 - 00:51:10.740, Speaker B: Reed will always cost the same amount of gas, and the only thing fluctuating is gas. So in some sense, we can always say something silly like an s read will always cost I don't know, whatever it is, like 250 editions, and it doesn't really matter how else, like, we do it. So it's. I would say there's a question of to what this model does encompass it. There's a question of how much you want to encode in that. And I think that is somewhat of a design question as well. There's also the other hand of, let's say, you know, this, this was talked about in a per block sense, but it doesn't, in a lot of l one's, it makes a lot of sense to think about it in per block, but on roll ups or like anything where there's a sequencer, it doesn't, it doesn't necessarily make sense to, I mean, if there's not a really strong notion of, like, what a block is, we can think of this as being, like, much shorter, like a much shorter time window before you update the prices.
00:51:10.740 - 00:52:26.060, Speaker B: That also would allow you to take some of these context dependent things into account in a little bit of an easier way, because in the per block sense, we would have to have different resources that correspond to different things. And you could even imagine, actually, the way this works in this model is, let's say there's two possible transactions. They're the same transaction, but one uses the resource of hitting a cold slot of storage, and one uses as a resource of hitting a hot slot of storage. And so now there's a constraint in the set s that says you can only include one of these two transactions, but these two transactions obviously pay different things and they're actually the same transaction. So a user submits one transaction, but is duplicated to say, okay, in one scenario, we include it and it uses this one type of resource. In the other scenario, we include it and it uses kind of this other type of cheaper resource. And I'm going to essentially outsource this decision to the block builder, but I'm going to include in that constraint set that I can only include either one or the other because it's really only one transaction that I'm, that I'm submitting.
00:52:26.060 - 00:52:49.190, Speaker B: So this is how you would include some of these context dependent things directly in the model that I presented. But as I said, perhaps if you do this in a way that's like, where there's like a yemenite, much shorter block times, you just update the prices. That's kind of another way of getting around it. Did that make sense? So it's kind of like, it does include it. There's a complexity trade off, though.
00:52:50.370 - 00:53:10.400, Speaker C: Yeah, I guess I'm curious about. Because you can say you can either include one transaction on the other. But there might be cases where transactions a and B. You can include a and after that b. But the other order doesn't work. I guess the only question is that. Do your conclusions hold up with this more complex model?
00:53:10.940 - 00:53:42.312, Speaker B: Yes, it would just be like. That would just be kind of encoded in this constraint set. But like I said, perhaps if you have one of these things where the gas of a transaction is context dependent. Depending on what was included before or after. You would probably have something where you would actually duplicate that transaction into two transactions. The two transactions would have different resource allocations. And you'd have some constraint that says only one of these two.
00:53:42.312 - 00:54:05.282, Speaker B: At most, one of these two transactions can be included. And that would give you kind of the full expressivity. But again, at the cost of additional. Excuse me. Additional complexity. That's a great question. Sorry.
00:54:05.282 - 00:54:05.970, Speaker B: Go ahead.
00:54:06.130 - 00:54:26.940, Speaker D: Sorry. I have two quick questions. I wasn't here for the beginning of the talk. So apologies if the first one is already covered. But when you're talking about multidimensional resources in the blockchain setting. That means something like instead of gas or compute units. You have like storage slots or other.
00:54:26.940 - 00:54:31.540, Speaker D: Is that a good analogy for what the breakdown is like?
00:54:31.920 - 00:54:45.400, Speaker B: Yeah, it just can be anything that can be metered. But, yeah, I mean, sometimes in proposals these are talked about. Oh, you have compute gas. You have memory gas. You have storage gas. You have bandwidth gas, whatever. It could be the opcode level.
00:54:45.400 - 00:55:05.656, Speaker B: It could be on the sequences of opcodes. Where different sequences of opcodes that you call are different. It could be an opcode executed on a particular core of the machine. So it doesn't really matter. Like, this is kind of a big design question. And that's not covered by this framework directly. Just because it depends very specifically.
00:55:05.656 - 00:55:26.796, Speaker B: On the hardware requirements of the system. And kind of a lot of other considerations. But it's as long as you can meter it. So as long as you can tell me how much of this thing I used. Then it can be a resource. And all of these essentially have their own floating price that you're paying per unit of that thing you're using.
00:55:26.988 - 00:55:46.110, Speaker D: So to what degree does the result that you have. Which seems to be that multi dimensional markets are relatively efficient. To what degree does that hold? If the. The resource space that you choose is incomplete or isn't, like, the optimal one.
00:55:47.290 - 00:56:18.080, Speaker B: So the result itself still holds. Like, let's go to this result. And the reason. So, like, looking at this result. Say we're doing a one dimensional theme market. This is going to be the best one dimensional price, and this is going to be this dynamic one dimensional price. So the result doesn't actually give you that much intuition as to what increasing the dimensions get to.
00:56:18.080 - 00:57:10.230, Speaker B: And that's going to be, again, pretty specific to the system, because sometimes you can imagine you add on a dimension, but that dimension is like in some ways redundant to what you already have. So you're actually not going to get anything different here. It's because we're choosing the baseline to have the same dimensionality as this dynamic pricing mechanism. The result is entirely independent of the number of resources that you have. And then choosing these resources is more of like a design question. I tried to give some intuition at the beginning as to where this can be a throughput bottleneck, but I think this is something that really just comes from what does usage of a particular system look like? And then as a result of that usage, what are the mostly orthogonal resources that don't interact with each other, and then choosing something like that.
00:57:10.650 - 00:57:35.330, Speaker D: Gotcha. That makes sense. One quick question on this slide. You say that this doesn't assume any model of the types of transactions that are being submitted. Can you give some intuition for, at least in this theoretical model, why it isn't the case that adversarial transactions could like, I guess, mess up this result?
00:57:38.590 - 00:58:42.880, Speaker B: Yeah, that's a good question. So I guess one caveat with the result is if you do assume, say, like a distribution that the transactions are being chosen from, you get something that's much stronger. Like you get that essentially the regret will go to zero much quicker. So you might get something that's like one over t instead of one over square root t. And of course, like, you know, let's say t is like something like ten to the four, right? One over square root of that is going to be one over 101, over t is going to be another factor of hundred, like less than that. So if you do assume model, you do get a stronger result. The intuition for this in particular kind of comes out of, I would say, the ability for us to construct pretty good lower bounds on convex functions, like linear lower back, like linear model lower bounds that are somewhat independent of the rest of the behavior of it.
00:58:42.880 - 00:59:14.290, Speaker B: And that's why we can get the result at all. But the result itself, in kind of a pure adversarial sense, is somewhat weak because of this. Like one over square root, t isn't actually that fast, but what it says, which is, I think, like very important is that you're not getting like totally screwed. The system is still kind of behaving as best that it could even under this adversarial behavior.
00:59:16.550 - 00:59:34.420, Speaker A: I have a follow up question about like the number of dimensions. So let's do this thought experiment. Let's say we choose our resources are like every, every sequence of lens ten or like 100 of different opcodes, right? So that gives us a lot of dimensions for the resource.
00:59:34.580 - 00:59:35.068, Speaker B: Sure.
00:59:35.164 - 01:00:18.840, Speaker A: And let me try to like try to make this like benchmark behavior a little bit bad looking. Say the transactions come in this way. First everybody sends the first sequence of transactions, then everybody sends the second sequence of transaction. Like throughout time people are using, like in each iteration, people are using a specific, every transaction using a specific sequence of upwards. So what ends up happening, if we know this is the sequence of transaction, is that we probably have to have elevated prices for all these resource types, right? At all times. Like.
01:00:19.860 - 01:00:21.960, Speaker B: Oh yeah, go ahead, sorry.
01:00:24.580 - 01:00:44.170, Speaker A: And like intuitively I think that causes like this, this number like this difference to grow because like now what the, what the fixed prices do is have prices high at all time. But probably, I'm not sure, but my guess is.
01:00:44.250 - 01:01:41.710, Speaker B: So this is like why this result is so powerful is even under the most adversarial conditions, under a long enough time horizon, and depending on kind of the constants m and b there, which I said are kind of related to the size of the block, this will still go to zero. The difference here will go to zero. The best fixed prices might not actually be that good because you can construct very complicated scenarios in which the best fixed prices won't actually be that good. But this still says that. And that's kind of the hour of these types of results is still, it'll go to zero in the long run. And then also even in the scenario described, that's like 200 to the 10th, roughly dimensions. So the number of like that's, I mean, ten to the 20th, which is like something that's like big enough to be somewhat infeasible.
01:01:41.710 - 01:02:54.080, Speaker B: But this is, to get back to the point like the power of online convex optimization results is like against this baseline, which we can argue of like this is how good of a baseline this is. Unfortunately, this is about like the best that one can do in terms of a baseline to compare to just as a large literature that looks at this in particular. So compared to this baseline, you're still going to, on average in the long run not really be doing that differently. One other way of looking at these two is from a machine learning context. And this kind of comes from the online learning baseline, essentially says something like, you know, all the transactions and you train some model offline that says, what are the best prices given? Like all the transactions that you see. So maybe you see everything that everybody's going to submit and you like train a model that tells you the best prices, and then this model is fixed. The second one says that you don't, you know, you see these transactions are revealed to you kind of one at a time or one block at a time, and you have to choose this online algorithm.
01:02:54.080 - 01:03:20.660, Speaker B: And then what this is saying is that the method that we're using in the online sense doesn't do any better than this offline sense. It's offline supervised learning versus an online learning type of scenario. Yeah, great question. Because it is like the result is a result compared to a benchmark. And so you can definitely construct scenarios in which the benchmark is not going to be doing that well.
01:03:23.090 - 01:03:44.026, Speaker A: Right. We are a little bit overtime. Thanks, everyone, for staying a bit longer. Thanks to you for the great talk. Again, we are going to continue this talk series for as long as possible, so please show up whenever you can. I hope we have most of you next week. Thanks a lot.
01:03:44.026 - 01:03:44.738, Speaker A: Thank you.
01:03:44.834 - 01:03:51.622, Speaker B: And thank you so much for inviting me and for all the great questions. Thank you.
01:03:51.806 - 01:03:58.830, Speaker A: Bye bye. Thanks, Aldouse.
