00:00:05.800 - 00:00:15.340, Speaker A: Okay, up next, we have using ZK to enhance the evm with axiom by Yisun at axiom.
00:00:27.560 - 00:01:18.970, Speaker B: Hey everyone. So one of the founders axiom here to tell you how we're going to enable applications on Ethereum to access more data. And so I just want to start by covering how we typically access data on Ethereum as users. And that's through this JSON RPC interface that all full nodes and archive nodes expose. And the purpose of this API is essentially to deliver information about the state of Ethereum to the human eyeball. So what would typically happen is a web front end would query the state of the chain, possibly through an external service, or in rare cases through your own node, and deliver these numbers to your users. And as things have progressed, people demand increasingly more sophisticated ways to view this information.
00:01:18.970 - 00:02:20.700, Speaker B: There are many options for archive nodes now, and there are also indexers that make the information you get from an archive node more semantically meaningful. And finally, once you have that information now there are these fancy wallets that will even simulate your transactions live against the state of the chain. So this situation is extremely different from the situation for smart contracts. From the point of view of a smart contract, they are almost blind. So it might be surprising that smart contracts can't access any historic on chain information. If you go to OpenSea and pull up a listing for this cute pudgy penguin, you'll realize that the only piece of information on the page that's actually accessible on chain is the boxed current owner, which for this penguin is Zac Efron. For better or worse, all the other information on the page, like the transaction history, the previous owners, the previous prices, that's not accessible on chain.
00:02:20.700 - 00:03:26.340, Speaker B: So you might think this is kind of a design flaw of Ethereum. Like why don't we let contracts access more information? But it's actually a fundamental trade off that Ethereum makes to remain decentralized. If we allowed smart contracts to access the full history, then every full node would essentially have to become an archive node and have random access to the entire history of Ethereum, and that would obviously hurt decentralization. Now, one thing that probably everyone knows is that compute on chain is very expensive, so there are certain precompiles that make some operations cheaper, but we're not going to add arbitrary computation as precompiles. The reason for that is to add a precompile to the EVM or any blockchain VM. We have to be very sure that it doesn't open up a DDoS attack vector on transaction validation and it doesn't really make sense to use social consensus to go through a list of all computations. So as a result, we have this very data limited and compute limited on chain compute environment.
00:03:26.340 - 00:04:27.844, Speaker B: So the question we asked at axiom is, how can we scale smart contracts to enable more data rich on chain applications? And our answer is to use something we're calling a ZK coprocessor. So what that means is we allow smart contracts to query into Axiom to do two things. First, to read historic on chain data, and second, do some verified computations on top of that data. After we do both of those things, we come to a result, and we generate a zero knowledge proof that that result was computed correctly. We then verify the proof on chain and deposit the result in a smart contract for any other application to use. So you might ask, what types of things can we do? Well, we can access the full history of Ethereum, and then we want to allow applications to run analytics. Cryptography, or we hear machine learning is very popular these days on this data.
00:04:27.844 - 00:05:36.088, Speaker B: No matter what, we always verify a proof on chain of the data. And what that means is that all the results that you get back from axiom actually have security that's cryptographically equivalent to accessing data from Ethereum itself. Okay, so let me talk through how that's even possible. So if you have a smart contract, then it has access to the current block hash of Ethereum, and that block actually commits to the previous block of Ethereum, which commits to the previous block, which all the way back commits to the Genesis block of Ethereum. And each block of Ethereum commits to the state and all transactions and receipts in that block. So if you were to try to access Ethereum history naively, in a smart contract, you would provide a Merkel proof of, let's say, a transaction into a previous block header. And if that's a block header, a million blocks ago, you would provide all million intermediate block headers, and then you would check the Merkel proof and the chain of ketchack hashes of the block headers.
00:05:36.088 - 00:06:47.056, Speaker B: Now, of course, that's extremely expensive, and you can probably at best, access a really small amount of data in this way. So what we do at axiom is put that whole computation into ZK. So this lets us compress the amount of compute needed on chain to verify the computation, as well as compress the data we have to put on chain to do the computation. So this lets us scale the data access and also compose it with other operations. Okay, so dive a little bit more deeply we don't actually want to prove the full chain of 1 million ketchup hashes to access data from 1 million blocks ago. So what we actually do on chain is we maintain a smart contract that caches the entire history of block hashes on Ethereum back to Genesis. And in order to enable queries into different blocks in one query, let's say an account from one block, a storage slot from another block, and a block header from yet another block, we commit the entire history of block hashes in a data structure called a Merkel mountain range.
00:06:47.056 - 00:08:05.900, Speaker B: And as Ethereum progresses, we actually have to maintain this Merkel mountain range by providing zero knowledge proofs on chain of the past ketchek chain of block headers. We do this every approximately 45 minutes. Now when we actually fulfill queries into the history of Ethereum, we always query against this cache of block hashes. So what that means is we verify Merkel Patricia tri proofs of historic states, addresses and storage slots into the cache of block hashes. And just to show more explicitly what we can handle, in every Ethereum block, we have the block header, which is the block number, the block hash and commitments to the state transactions and receipts. Now, on Mainnet we can allow you to prove any property of an address, namely the nonce balance code hash or storage route, through an account proof or any storage slot into the storage route of any address. And this week we just launched on testnet proofs into any historic transaction or receipt in the history of Ethereum.
00:08:05.900 - 00:09:00.776, Speaker B: So now I want to talk through, okay, all this technical stuff. What are the actual applications that we think it can enable? So I've drawn like a little diagram. On the horizontal axis we have the data complexity, namely how much historic data do you actually need to view in your smart contract to do this? And on the vertical axis, the compute complexity, and they're actually roughly correlated. If you have a large amount of data, you generally need a large amount of compute to handle it. So in the beginning, we actually think that the applications are going to be things that are possible in the EVM, but maybe a little bit inefficient. They might require some additional caching, or they might require some additional trust assumptions. So things like accessing the consensus level randomness from the beacon chain, verifying your account age, computing some average prices, or having a volatility oracle.
00:09:00.776 - 00:10:11.440, Speaker B: These are things that maybe you can work around in your smart contract, but are much more convenient if you can access history as we actually scale. I think there's a new class of applications which are really only possible by using ZK reads into the history of Ethereum. These are things like verifying the entire checkpoints of ERC 20 balances of some tokens, computing on chain reputation by trustlessly proving your actions objectively, slashing participants in a protocol for provable actions which violate some rules in your protocol, or even adjusting the parameters of your defi protocol based on the full onchain history of what happened. So we're really excited about enabling new applications that actually rely on smart contracts reading the history of the chain itself. So let me just give a picture of where we are today and where we're going. So today we've launched on mainnet reads to block headers, accounts and account storage. And on testnet we have transactions and receipts.
00:10:11.440 - 00:11:12.760, Speaker B: And to do compute over those today, you need to deploy a custom ZK circuit. Where we think ZK coprocessing is going is to start by accessing the whole of on chain data, and then to actually do post processing on that data in a way that's most native to smart contract developers. And we think that way is by applying view functions whose execution is proven using the Zkevm. So this might seem like a bit of a pipe dream. I mean, the Zkevm even seemed like a pipe dream a year ago. We actually think this could come sooner than most people expect, and these first two steps together would constitute taking an entire archive node and putting it into ZK. After that's done, we think that the best thing is to actually leave the bounds of the EVM and allow developers to start doing computations that are well beyond what's possible in a blockchain VM, and we think that will be best through a ZK native VM.
00:11:12.760 - 00:11:46.852, Speaker B: So we've started building out this roadmap, and we're pretty excited to see what it can unlock in the future. So we're axiom. We're the ZK coprocessor for Ethereum, and we just launched our main net two weeks ago. So you can check out our developer docs and try us out fully self serve using Docs Axiom XYZ. And at the end of this talk, I just want to put in a plug. We think ZK is a bit too much of a black box for ordinary developers today. So we started something we're calling the Axiom open source program to help change that.
00:11:46.852 - 00:12:16.830, Speaker B: So it's a program where we teach developers how to write ZK circuits and develop ZK applications. In. Our first iteration of that program finished a couple of weeks ago. A couple of the participants are in this room, and they built things like fixed point arithmetic, ed 25519 signatures, and other fundamental ZK primitives. So applications are closing for the second iteration July 20 eigth, and you can apply at the URL on this slide. That's it. Thanks guys.
00:12:16.830 - 00:12:21.070, Speaker B: You.
00:12:22.740 - 00:12:25.890, Speaker A: Do we have any questions? Yes, I can see someone at the back.
00:12:32.580 - 00:13:16.450, Speaker C: Hey, thanks for the talk. You mentioned enforcing good behavior. It's kind of off topic, but something action could actually offer on the Ethereum roadmap, you have this proposal builder separation that is supposed to be enshrined in the protocol. I mean, if you look at the advance of ZK to stack in general, wouldn't it make more sense to, instead of making every single node replicate this kind of computation, have some kind of protocol that relies on ZK and just have a proof of whatever is your constraint set? Basically, just don't enshrine PBS and just think about an entire new design around this.
00:13:19.140 - 00:14:09.040, Speaker B: Yeah, I think when you think about applying ZK to PBS and MeV in general, you have to consider sort of how much latency you're willing to tolerate. I'm not too deep in PBS, but my understanding is that there are certain things that are hyper latency sensitive, and I think ZK today is not so appropriate for those. And there are other things which are almost post hoc where something has happened and you want to punish or reward someone to create an incentive. And I think ZK could be a pretty powerful tool for that. I think the last thing to say there is ZK is really good at enforcing objective statements. So if you want to write down a rule that you're very committed to enforcing, then ZK can do that. I think my understanding in PBS is that you might want some subjectivity, and so I'm not sure to what extent you'd want to codify that using ZK.
00:14:13.300 - 00:14:53.200, Speaker C: Okay. For example, you could have all the messaging between the builder and the proposer. So if you look at Flashbot, the way the builder has this bid trace message, and basically the most important field is the actual bid and the proposer then choose the most valuable block and sign stat. I mean, at the end of the day, all the messaging could happen off chain, and you could just verify that there was a commitment to a block or whatever and verify that part, and basically offload all this messaging shouldn't. I mean, there could be a way in which you could use eks that the messaging itself doesn't happen on chain.
00:14:53.860 - 00:15:02.720, Speaker B: Yeah, I definitely agree, especially if you have a protocol where some sort of bad commitments, according to some criterion you specify, are later punished.
00:15:03.460 - 00:15:04.690, Speaker C: Okay, thank you.
00:15:09.600 - 00:16:04.010, Speaker B: How do you think about resource pricing for the coprocessor? Yeah, to be frank, I think right now it's a little early to commit to anything on resource pricing. A bunch of things are going to enter the mix which aren't really a factor today. So one is pretty large scale aggregation, both within queries and across queries, so that will reduce costs quite dramatically. The second is understanding what the size of queries people want to make, even is particularly on the compute side. So generally speaking, we think that the proving costs should be determined on some sort of market basis just to allow a smooth transition as provers get better. But we generally want to see more usage before understanding what the market structure should look like.
