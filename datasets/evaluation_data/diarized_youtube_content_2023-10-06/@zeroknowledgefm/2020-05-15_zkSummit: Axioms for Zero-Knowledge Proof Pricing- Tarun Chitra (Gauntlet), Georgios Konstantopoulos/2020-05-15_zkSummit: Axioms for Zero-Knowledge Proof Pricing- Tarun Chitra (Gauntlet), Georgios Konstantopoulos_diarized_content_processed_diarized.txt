00:00:06.010 - 00:00:31.320, Speaker A: Hello everybody. I realize I just possibly pulled you out of Oleg's talk mid sentence, so if you want to finish up with him, feel free to go back for a moment. But now I want to present our next speakers, tarun Giorgios, who are going to be talking about pricing ZKPs with their Talk economic equilibria in Zero Knowledge Proof Generation DC. All right, I'll pass it to you. Go ahead.
00:00:32.970 - 00:00:37.080, Speaker B: Cool. Are the slides on full screen?
00:00:39.610 - 00:00:40.454, Speaker C: Cool.
00:00:40.652 - 00:00:41.914, Speaker B: Great. Yeah, they are now.
00:00:41.952 - 00:00:42.860, Speaker C: They are now.
00:00:43.230 - 00:01:37.466, Speaker B: Okay, so thank you everybody for coming to this talk. So, for the past few years, we have seen a kind of Cambrian explosion in the world of succeeding transparent and universal zero knowledge proof systems. And many of these systems were created to help create private, scalable, blockchain based smart contract platforms. And in this context, we have a group of participants that create proofs in order to operate a decentralized network. And they incentivize to do so via some reward. And you can think of it as similar to Bitcoin, where some miners are incentivized to compete to produce the next set of admissible transactions. And it is apparent that incentive choices are crucial to ensure that proofs continue to be produced and the network stays up.
00:01:37.466 - 00:02:21.738, Speaker B: So in this talk, we extend the axiomatic framework from a recent paper to construct what we call like reward allocation rules for zero knowledge proof generation. And we will then use these actions to produce a set of expected and desired properties which your protocol will possess. So, let's start with some examples of the architectures that we have seen today. So let's take Coda, for example. So in Coda, you have a block producer which is kind of similar to M miner that is chosen with some way which we do not investigate. And they choose some transactions from their Mempool. And again, how they choose their transaction, it's of independent interest.
00:02:21.738 - 00:03:22.998, Speaker B: And then they publish these transactions and wait for Snark bids on them. And this is because a transaction Encoda is only valid if it's included in the chain alongside with the Snark, which proves it's validity. And you have these parties called snarkers that are watching for transactions, which then they generate proofs over its transaction and they post a Snark along for its transaction with their offer. And then the block producer chooses like looks at the order book and chooses probably presumably the cheapest transaction from that order book and that transaction gets included. And the whole idea here is that if there is free entry to the Snark market, there will always be at least one proof per transaction. So the system will be censorship resistant, at least from the side of the Snarkets. And just to be clear here, coda's economic model asserts that the block producers and approvers will be separated but will have coupled incentives.
00:03:22.998 - 00:04:03.770, Speaker B: But we cannot really be sure that this will be the case in a permissionless world. It is likely that maybe some block producer will also be approvers. And so we need to evaluate fairness and competitiveness under these assumptions. And just as a note that I have added an asterisk here on the free entry because in coder's case the Snark for one transaction can be cheap. So free entry? You could say that if you have enough participants, yes, free entry is easy. But what if the Snark is costly to produce? Then you'd perhaps have issues with free entry due to the proof size. So you need to start thinking about how you can start distributing large proofs.
00:04:03.770 - 00:05:15.966, Speaker B: So this kind of gets us to the next architecture which we have seen, which is leveraging a paper called Proof of Necessary Work by Catissen Bono, which was released earlier this year, which basically intertwines Nakamoto Consensus with Snarks. So in Nakamoto Consensus you have a header and you grind the nons over it. So what happens here is that instead of grinding over a nons over a block header, you use the nons as an input to the Snark and then you generate the Snark proof and you check that the Snark proof's hash is less than some difficulty. But this Snark proof can be huge. So we can look at some other literature called Disic, which basically is a framework for doing distributed proof generation and a setup for growth 16 in Apache Snark. And if you put these two together, they give you something that starts to look like Snark mining pools. And there is no freelance though, because when we start distributing this, there's some loss in efficiency that occurs from making the marketplace more fair.
00:05:15.966 - 00:06:01.162, Speaker B: So if you look at this from a more in depth perspective, you have the mempool. The mining pool gets the block template and then using the technique from the Proof of Necessary Work, they choose the nons and using the technique from disk, they split the proofing chunks. They will distribute the chunks to the provers the workers. The workers will perform the computation and then they will submit their work back to the mining pool. The mining pool then will assemble the proof, like each chunk into one big proof. They will check if the proof hash is less than T and submit it to the chain. And then the chain should give you some subsidy which could be the block reward and some fees from all these transactions.
00:06:01.162 - 00:07:29.034, Speaker B: And what happens is that the mining pool will take some tax and then the rest will be distributed as a block reward. But the question here is how will they do that? So we start to get some intuition that there is some room for investigation. So our whole question around this is why do people do this? So why do people join a Snarket place like Encoda? Or why would people mine in a Snark pool? And the answer obviously here is that there may be some attractive subsidies on the launch or there might be some protocol fees which dating opportunity, cost and everything else in mind, they will be more attractive. But again, how do we choose this? What's the intuition? So the goal of this work is that we take into account that we have multiple agents who have multiple roles which may overlap. So maybe I spent some of my resources on being a blog producer, some of my resources on being approver and we'll need a simple formalism to look at the design space and see how the revenue should be split. And in economics, the way of doing this is to specify axioms which enforce constraints on agent preferences and these rules and actions, they will provide us with the intuition that we're looking for. And the most important properties that we want to encourage is the proof generation to be competitive and fair.
00:07:29.034 - 00:08:33.898, Speaker B: And then the natural question becomes what is fairness? And our focus on this will be in the incentives for proof miners, the provers at the timescale of generating a single proof. So in Koda's case, generating a single proof is for one transaction, in the mining pool case it's one block. So the design question becomes how do we allocate rewards for our proof system and for our circuit, for the proof generation to be fair? So the desiderata, if you will, in this case the desired properties are we have enumerated the following properties. Some are taken from the paper that we base our work on, some are specific, let's say, for this work. So firstly, we don't want dependence on identity. So if me and Darun commit the same resource as the system, we should have more or less the expected reward. The system should be civil resistant in the sense that I should not be able to split myself in multiple entities and get a higher reward from that splitting and it's opposite.
00:08:33.898 - 00:09:11.350, Speaker B: The system should be collusion proof. So I should not be able to combine pull resources from multiple people to gain more than our collective fair share. There should be no gain to amortization. So formally doing this is a bit tricky. So intuitively you can think of this as that if I've made five proofs in a row, the 6th proof should not take me less time than computing just one proof. So I do not have some sort of caching that I can do to get disproportionate gains. There should be something that we will call witness dependence.
00:09:11.350 - 00:09:47.350, Speaker B: So if I'm a proofer and I'm given a circuit and an input, I should not be able to get the same fee for an input that is easier to compute a proof on versus a harder input. And finally, we want to minimize prover extractable value because clearly the proving marketplaces will have the same problems as front running or censorship in the normal mined markets. So now I will pass it on to Tarun to move on with our formalizations.
00:09:48.730 - 00:10:51.770, Speaker C: Cool. Thanks rudos. So we're basically going to kind of go through some of the mechanism designing game theory. The main sort of tool that's used in the space is kind of an allocation function. Now an allocation function is really a map from a resource space, usually some metrizable measurable space to sort of a space of rewards, which is usually in this case positive reels positive orthan I guess. And so the idea here is that you kind of bake in all the properties you want into your allocation function and then you try to say, okay, does this function represent all of the economic properties that I'm looking for? And the intuition is that at some high level alpha encodes all of the macroeconomic desert data that you're looking for. For instance, it should be encoding this notion of fairness or the notion of civil resistance.
00:10:51.770 - 00:11:57.250, Speaker C: And so historically you might say, okay, well, you just chose a random function. Why should I believe that function is the correct function for ensuring X, Y or Z? Well, if we look at sort of history, the famous sort of Von Norman Bergenstein theorem in 1947 shows that for every quasi convex function of this form, you automatically define a set of constraints on economic preferences that users in the system can have. And what that means is there's a duality between the axioms and or preferences and utilities. Now, what do I mean by axioms and or preferences? Well a simple axiom for instance, is say I have a society and all of them prefer good A to good B. Then in the total societal preference, sort of the average the ranked vote of everyone A has to be preferred to B. So that might be one of the axioms you choose. So that's sort of a constraint that's made, but that constraint translates to a constraint on the function that is allocated.
00:11:57.250 - 00:12:36.874, Speaker C: And in crypto kind of this paper by Chen Ruffkar Andrew basically tried to apply this to proof work and proof of to kind of show that for a certain definition of fairness, there's sort of a unique allocation function. Now protocol designers specify this. It leads to extraordinarily different outcomes. So in Bitcoin, of course we have proportional allocation based on hash power. Of course you don't know the total hash power, so you don't really know the proportion with perfect certainty. But it also has a balanced budget block rewards go out to everyone. Zcash on the other hand, doesn't have a balanced budget.
00:12:36.874 - 00:13:14.458, Speaker C: It has a network tax via the developer fee. And then Ethereum 2.0 for instance, is proof of stake with the square root allocation. And the square root allocation actually quite changes the kind of distribution in and of itself quite a bit. So what is our resource space? Well, if we step back and look at proof of work and proof of stake, both happen to be product simplicities. So they're all the probability synths of things that sum to one but the difference is, in proof of work, you don't actually know the denominator in the sense that you don't know the total. An individual doesn't know the total hash power system.
00:13:14.458 - 00:14:15.502, Speaker C: And in proof of stake, everyone has to know the total money supply. Now, what about zero knowledge proofs? Not sure why my video disappeared, but well, zero knowledge groups have a lot of different sort of resources that end up being used that have very different hardware properties and or algorithmic properties in terms of how well you can parallelize them. They're sort of communication lower bounds or sort of properties of a certain group, like random walks can't mix fast enough, et cetera. But at a high level, we have things like Fourier transforms, which as in Fri based systems and in a lot of systems that basically do any types of error correction that isn't done by a polynomial commitment. You have to do an FFT over an Abelian group. Obviously, character base could be big. There's a lot of ways to make that.
00:14:15.502 - 00:14:53.020, Speaker C: The computational issues with that quite large. You have to do multi exponentiations, which is obviously time consuming on purpose. And in something, say like the prior talk on polynomial commitments, without FFTs, you have way more polynomial evaluations. So those n log n Jacobi polynomial and LaGrange polynomial type estimations are more expensive. And then finally you have these circuit specific computations for calculating witnesses. These costs are extremely nonuniform. So the resource base needs to account for the heterogeneous nature of kind of the resources used.
00:14:53.020 - 00:15:35.970, Speaker C: So what we did is we tried to make the simplest nontrivial model of this. So assume there are K computation types. So computation type one might be Fourier transform, something whose lower bound on complexity is m log n minus communication complexity, of course. And then computational type two might be multi exponentiation, et cetera. We define this vector lambda, which is the relative average cost of computation type I relative to computational type one. Then we define a vector that each agent has, that is, the amount of resources allocated to each type of compute. And then finally we have a sort of lower bound.
00:15:35.970 - 00:16:28.550, Speaker C: In order to do a Fourier transform, you need at least 100 units of multi exponentiation power. This way we sort of choose one computational unit as a numera and we measure everything else relative to that. And a simple example of how this looks, say we have three computational resources, say we have two of them, take double the compute of the first. Then we can measure the total units of C one compute by taking their dot product. So what does this look like formally? Well, we basically again have this kind of graded structure where we're graded by the number of agents. But there is another thing in the Cartesian product, the first component. The first component really corresponds to this lambda vector, which is sort of the relative cost vector.
00:16:28.550 - 00:17:57.650, Speaker C: And then the other component is sort of your resource distribution allocation for each agent. So the naive thing to say might be, hey, doesn't the proportional rule work sense that the proportional rule should be kind of the only fair rule? But the real question is in zero knowledge proof plan, the question is how much compute does a prover I actually compute? So in Georgia's earlier slide, you saw the allocation in the ZK pool. And the interesting thing is, if we try to write the naive version of the proportional rule, what it looks like here is, hey, how many units of compute one compute does agent I have relative to all the other agents. The problem is it doesn't respect this constraint, which is that there's sort of a minimum amount of compute you need to get things to work. So we're going to look at that in pictures. So on the left we have the roughguard paper, which basically is what sort of the three types of principal agents look like proof of we have the risk neutral, which is quasilinear utilities. Again, the von Neumann Morgenstern theorem gives you uniformization proofs for any strictly convex, strictly concave or linear class.
00:17:57.650 - 00:18:56.422, Speaker C: And so the risk neutral ones are defined to be the quasilinear utility. And you can basically see that as a function of resource, the risk seeking users have this convex function. The risk averse have this concave function, risk neutral or linear. The difference is in ZK proof generation on the right, you see that the utility is actually zero until we hit our constraint line. And that constraint line is really important because unlike in proof of work or proof of stake, where if I have a very small amount of stake, there's still technically a non zero allocation to me in infinite time. In zero knowledge proof generation, that just isn't true because of how all these components interact and because of kind of empirical evidence that we have from running these applications. So the idea is that this little constraint causes some differences in how collusion works.
00:18:56.422 - 00:20:22.290, Speaker C: So let's say we have these three malicious users we want to include, and if you notice on the left and the right, one of them is kind of below this threshold. Now, in the normal version of collusion, basically you'll find that if you're risk neutral, risk seeking, you're collusion resistant, but if you're risk averse, you're not, which is kind of what you expect intuitively, because if I'm risk averse, that means I optimize, I want lower variance. If I want lower variance, that means I will pool together with someone else because I lower the variance of expected reward on sampling by doing that, since the sampling is proportional to my stake times one minus my state times the concept. Other hand, the other type of collusion actually does work because if I'm an agent who is below the constraint so I don't have enough resources to meet the minimum, I joined forces with some people such that we get over the minimum. I actually do increase my reward. But that's not necessarily bad because that actually means that we've been able to take participants who were unable to participate at a certain time and by colluding they actually were able to generate new proofs. So there is a sense in which you want collusion up to this constraint and that's the type of stuff you want to define correctly.
00:20:22.290 - 00:21:09.330, Speaker C: So now let's go through the axioms. First axiom no gain to amortization. This basically says if you have a positive expected reward then you need to necessarily have more compute than the L1 norm of the minimum. And why is this? This sort of forces a linearity lower bound because this means that for every proof I generate I have to commit at least the L1 norm of the lower bound. Per proof I have some type of non asymptotic amortization, cannot have asymptotic amortization the second. So this gets back to our constraint because our constraint is somewhat circuit dependent. So what we do is we define a notion of collusion proofness that is circuit dependent.
00:21:09.330 - 00:22:30.854, Speaker C: And the way that the circuit dependency works is take the set of users. Basically in this picture we have three types of users. We have sort of the adversarial ones, we have the honest ones who are the runners and then we have the people who don't have enough compute resources who are in orange. And basically we basically redefine the collusion proof constraint based on a the circuit so there's a circuit dependent lower bound and then B by allowing anyone who's above the circuit dependent lower bound to combine arbitrarily. And so what this says is hey, all of the addressers even if they combine and become you only see 13 public keys instead of 16, the rewards are never greater than if they the other two constraints. Another sort of natural constraint, and this comes from the roughguard paper is budget balance. Budget balance basically corresponds to do I spend all of the block rewards on subsidies or not essential probably every proof of state token to some extent that has slashing is a non budget balance and also burning fees can also these axiomic frameworks don't just for dynamic fees.
00:22:30.854 - 00:23:28.590, Speaker C: So usually you consider the feeble case. Now, the last two things are civil resistance to symmetry which are intuitive. So symmetry basically says take any element of symmetric group on N letters permute the resources and then compare that to muting the output values and it should be the same. Which basically says if I contribute a certain amount of resources and Georgia contributes a certain amount of resources and they're the same amount, then our expected reward should be the same. This formula basically says I can't split up my coins and get more rewards. So what kind of properties do we the first property is sort of the direct analog of the rough garden property, which is the proportional rule has. These is sort of the unique rule that satisfies symmetry, civil resistance, strong budget balance, inclusion briefness.
00:23:28.590 - 00:24:41.658, Speaker C: Now, for satisfying something that is unique to zero knowledge proofs, which is no gain to amortization, the unique rule is kind of this convex optimized version where we constrain. So one is the indicator function, indicator function that restricts to this hat space, which is constrained by the circuit lower bounds of circuit dependence. Again, so we have a circuit dependent amortization rule. And the other thing we prove is that thankfully these are all convex problems because we're constraining it to an optimization, a convex optimization problem over a convex set. So we can construct your standard LaGrange duals and the Jean transform type of things and you can actually show that you have a circuit dependent utility function. So you can view this entire constraint as I have agents who are getting proportionally allocated fees, but their utility function is constrained to a convex set and the convex set is basically defined by the half planes defined by all the circuits and this generalized to non uniform constraints. So you can actually combine multiple circuit constraints and get a polytope sort of like the same you see above.
00:24:41.658 - 00:25:08.560, Speaker C: And the cool part is, if you wanted to compute Nash equilibrium in this setting, thanks to the fact that it's a structured polytope, you can use some standard algorithm like the Lemki Halsen algorithm, which basically walks along the edges of the polytope akin to the kind of simplex algorithm and tells you kind of where the Nash Equilibrium for a particular agent is. So now George is going to present a couple of last few things.
00:25:12.850 - 00:26:35.754, Speaker B: So, okay, so using the things so far, we'll try to extend them in some ways to get some extra properties that we might want from our protocol. So firstly, there was a question in Crowdcast which directly relates to this, which we call witness dependence. Again, so basically this equation says that the reward I get for computing over one witness minus the reward that I get for computing on another witness must scale more or less proportionally to how the cost for computing on each witness respectively changes. And this kind of reminds us of like at least reminded me of Constantine cryptography. And we saw that such an attack on a snark and a timing based attack was seen on a recent paper on the timing based attacks on zikas'snark where basically you had a multi exponentiation and if there was a zero, the person doing the computation could skip some computation. So in the ZCAS attack, if you're skipping a computation, you have a timing attack. So in this case, imagine that you have a multi explanation of say, like 100 elements and you distribute it.
00:26:35.754 - 00:27:13.320, Speaker B: So the first ten elements go to some prover and so on and so forth. But what if approver gets an array? Let's say the array is sparse and the approver gets a segment of the computation which is all zeros. So if it's all zeros the approver may try to say that, hey, I did my computation, I calculated the chance that I was assigned to correctly give me my reward but your protocol should not reward him because basically anytime you're skipping any computation based on whatever, you should be not rewarding. Are my slides not on.
00:27:16.730 - 00:27:17.990, Speaker C: You see the witness?
00:27:18.970 - 00:27:20.294, Speaker A: Sorry about that.
00:27:20.492 - 00:28:22.586, Speaker B: Yeah, thank you. Okay, so if you can skip a computation you should not be rewarded for it. This more or less says you get rewarded per computation step which depends on the witness, not on a circuit level and the second kind of extension that we want to propose is the minimizing the prover extractable value. Yes. So there are some kinds of attacks which miners can do in proof of work systems and it turns out that these attacks can kind of carry over to this sort of proving ground proving mining market. So basically the one is censorship where basically the proverb sees something that is available for proving and they just don't compute a proof on it, which maybe they don't want to make a proof on it because it was some CDP. Maybe that was a transaction which would bite their CDP below some amount and they would lose money.
00:28:22.586 - 00:29:47.126, Speaker B: Or maybe they just want to prevent you from transacting. This is solvable by just having a large number of provers so if there's a lot of people available for generating proofs you avoid censorship and also by distributing the computation if you have a large circuit. But in the case of reordering the proverb basically what they do if you're receiving a block is that you have a bunch of transactions which you receive in a certain order and if these transactions you perhaps may benefit by reordering these transactions. So currently our allocation rule does not take into account the ordering so we must kind of fix this and the solution that we propose for this is that you should basically just analyze the coding by adding an extra term in the reward function and we explain this extra term as a distance metric. So basically this says that your reward will decrease the more the permutation of the transactions that you're computing. The proof over differs from the original transaction order that you received. So if the block sequencer, the block producer gave you transactions zero to nine in an order zero to nine, and you suddenly put transaction nine in the first slot and the Fed slot on the last slot, you should lose some money.
00:29:47.126 - 00:30:10.670, Speaker B: And the practical instantiation of this would be that the moment that you try to reorder anything, just reduce the reward to zero. And any penalties taken from this, they get held by the protocol. And you can choose how you will allocate them. It's of separate interest. So to wrap up this presentation we have some final takeaways.
00:30:13.250 - 00:30:37.560, Speaker A: Just 1 second. Before you do that, I actually need to go into the next session in order to get it started. So you are welcome to stay in this session as long as you need to. I'm just going to let you know that, and I'll see you soon. I'm going to be pulling some of the audience with me though, so if anyone wants to kind of make sure that they're still following this one, just make sure that you're in this particular session. Okay, thanks, guys.
00:30:39.370 - 00:31:11.954, Speaker B: Cool. Yeah. So the final takeaways are that it seems to be that we expect that marketplaces and distributed proving systems for zero knowledge proofs will go to the moon. And it's hard to figure out what is the unit of cost in zero knowledge proofs because the resource space is much more complex. And so we want to have fair entry, ideally to have nice incentives and Tarun, do you want to finish this?
00:31:12.152 - 00:32:28.186, Speaker C: Yeah. So we kind of were inspired by this recent work to kind of give economic understanding via axioms to some of the rules that people are using in protocols. It kind of reverse engineers the allocation rules and makes them kind of much more salient. And in a lot of ways by doing that, you can start thinking about fairness directly, such as how do you prevent centralization, how do you kind of prevent interference with consensus? And fundamentally, the allocation function must match the action you want. And if you want to have a rigorous protocol, you need to kind of construct both of those correctly and sort of the last thing, as George just mentioned, is there's kind of an advantage to having a weak budget balance protocol. In a lot of ways it makes things a little more flexible. For instance, having an insurance fund which is paid for by the minimized proud rejectable value fees that are collected or potentially, I guess, what people seem to like to call public goods, but it doesn't necessarily matter what it gets spent on.
00:32:28.186 - 00:32:42.080, Speaker C: I'd say the insurance fund is probably the most salient version, but I think there's a lot of room in that design space to have the allocated efficiency of these systems grow quite a bit. Thank you.
00:32:42.450 - 00:32:47.910, Speaker B: Thank you very much. Okay.
00:32:52.520 - 00:32:53.270, Speaker C: Anyone?
00:32:55.080 - 00:32:56.500, Speaker B: Tyler Smith?
00:32:58.800 - 00:33:03.710, Speaker C: Deb, is there any reason for penalize? Yeah. Do you want to take that on?
00:33:04.480 - 00:34:12.800, Speaker B: Yeah, sure. That's what I wrote in the comments. But let me just say so by sharp cut off in the talk, what I meant is that the moment that there is some reordering in the transactions, you can just cut the reward to zero. Another approach to this would be to somehow force the ordering of the transactions on the Snark level. So the way you could do that would be that the prover no, sorry, the block producer posts, let's say, to the chain a hash of the transactions so sequences the transaction, serializes them. Generates the hash, posts it, and then the Snark would need to prove or you would add some sort of consensus rule to the Snark that the hash of the transactions given as inputs to the Snark must match the hash, which was published earlier by the provers by the blockload users. So you kind of tie the two actors together in that way, but that increases your Snart complexity.
00:34:12.800 - 00:34:29.496, Speaker B: And if we added that as a rule to these properties, maybe it could be too restrictive. So we just choose to say just penalize and that's it. Yeah.
00:34:29.518 - 00:34:56.590, Speaker C: And the distance used, there's not too many constraints on it. It just needs to be unimodal and monotonically decreasing from the true ordering. It can't be like multimodal, where if it's sort of a semi metric submetric because then you get a lot of weird problems. So there are some constraints on that. D but just pointing out.
00:35:01.420 - 00:35:02.890, Speaker B: Is there another question?
00:35:03.260 - 00:35:06.816, Speaker C: Cool. Okay. I think that's it. All right, thanks.
00:35:06.918 - 00:35:08.910, Speaker B: Okay, cool. Thank you very much, everyone. Yeah.
