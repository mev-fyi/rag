00:00:07.210 - 00:00:54.590, Speaker A: Hi, I'm Alastair. Let me know if I'm talking too quiet, too loud. If I start myself, just let me know, please. Although you can't see because it's completely illegible. I'm Alistair Stewart and this is based on joint work with, with Oanna and with Sergey, who couldn't be here. And I'm going to talk about light clients, sort of the problem of how to follow the consensus of a blockchain efficiently. Now, okay, does this work? Yes, it does.
00:00:54.590 - 00:02:04.240, Speaker A: So, specifically, what is a light client? I'm going to structure this talk. I'm going to try and explain very briefly what this solution does, and then I'm going to say how it does it, sort of what the snark is, and only then am I going to explain why we did this and why it's this way compared to what everyone else does, which I don't know if that will make sense, but I hope so. Broadly speaking, a light client is something you'd be running on a resource contrained device, and you're trying to follow this sort of decentralized consensus system. And part of the problem is that this sort of system may have many consensus participants, particularly thinking of proof of stake blockchains. We have things like validators. We might have a lot of them, sort of on polka dot. We are aiming for 1000, but Ethereum already has like 200,000 validators.
00:02:04.240 - 00:02:58.530, Speaker A: And it's not obvious how I can get a resource contrained device to understand what these guys have agreed on. And the solution that I'm going to talk about is just the problem of verifying that something was signed by many signers. And I'll explain later exactly why this is the problem we need to solve so we can do multi signatures. This allows us to give or sort of aggregateable signatures like BLS. This allows us to give one signature from a lot of signers. But there is still an issue that this. So this means instead of having a thousand signatures, I can have one signature, which is great.
00:02:58.530 - 00:04:16.730, Speaker A: The issue with multi signatures is, of course, that I still need to have the 1000 public keys to verify them. And this set can change the set of guys who we are interested in signing. Something can change frequently, and if it does, then these public keys are a lot of data by themselves. Now, how don't we solve this? Well, the obvious solution would be to use threshold signatures, maybe, and there's certainly some every year. There's interesting progress on this, but it's still generally true that the distributed key generation you need doesn't scale so well to thousands and millions of participants. And also it would actually be useful to know who signed what, but it would be nice to know who signed what and not have quite so much data from this public keys. So what this solution does.
00:04:16.730 - 00:04:50.532, Speaker A: Okay, yeah, I better sort of say how big the problem is. And this is sort of. Yeah, so we have these like two five six or three eight four bit public keys. And if we have two five six bit public keys, then we probably also have two five six bit signatures from everybody. So that doesn't actually save anything. And we're looking at kilobytes and megabytes for every time we need to, frequently, every time we need to update this set, unless we did something sensible. So what our solution works does is this.
00:04:50.532 - 00:05:54.280, Speaker A: So basically the idea is that we're going to commit to a long vector of public keys, and we want to verify that a particular subset of these guys signed something. And we indicate the subset by just giving a bit field, a bit vector of length n. This size is still order of n, but like the public keys were two five six or three eight four bricks. So this is 100 times less data at least than just giving the keys. And this is a significant advantage. And if you do this, it's like it has the same advantages as just doing some sort of aggregable signature scheme like BLS by itself, in that the signers don't have to interact with each other, which is complicated when there's a lot of them. And indeed we know who signed what for, which we kind of need, probably need about a bit per signer.
00:05:54.280 - 00:07:12.310, Speaker A: And so how would we get such a, a street scheme? Well, sort of one of the obvious things to do to do was let's do something like BLS. Everyone has their own BLS public key and proofs to possession, and we just need to compute an aggregate public key from this commitment, or prove that this commitment to the vector and this bitfield give this aggregate public key. And sort of the obvious solution would be let's use a snark for that. So how would we do this with a snark? Well, we could use something like plonk or whatever's popular right now to do this. With plonk, you probably want to use turbo plonk, you probably want to use a custom gate. So the kind of the operation that you're mostly trying to do here is we have a load of elliptic curve points and we want to add a load of them if the bits are worn. And this means we define basically a set of equations which give a conditional incomplete elliptic curve addition gate.
00:07:12.310 - 00:08:43.560, Speaker A: Now, I've seen code out there for an incomplete elliptic curve addition gate for turboplunk, and it looks very similar to this, except without this b, which is the bit that determines whether we add things or not. I'm kind of surprised that they didn't go a degree higher and put this bit in, because when you're doing something like a multiscalar multiplication, you normally want to condition on bits. So if we had such a gate, how would we compute the problem? Well, there's this minor incompleteness issue that if we want the simple formula that this is short wirecraft form incomplete elliptic curve addition, we have to make sure that we never add something to itself or it's opposite, not least because we can't express zero in our fin coordinates. It's a really simple way of doing that. For BLS, which has a cofactor, we just start with the base point outside the subgroup. And then if all the points we're adding are in the subgroup, we never get, the accumulated sum is always outside the subgroup. And so it's never equal to minus the thing we're adding or plus the thing we're adding.
00:08:43.560 - 00:09:34.280, Speaker A: And incompleteness is not a problem. So the circuit is super simple. We just, okay, there should be a b here. Imagine there's a b here, a bi that I'm using to multiply these things. And so, yeah, basically, so the, the way like that, the halo two guys are now talking about how they do their circuits is that they're talking of chips. Now we have sort of two dimensional things. So this is just a table where you imagine that each column stands for some polynomial commitment in Lagrange basis, and this is the value of each of these things at some power of roots of unity.
00:09:34.280 - 00:10:11.008, Speaker A: And so the gate just takes some public key, x and y coordinates this bit, and the last accumulated value and adds and gives the next accumulated value. And more or less the entirety of the circuit is we just repeat this n times. Because what we're doing is straightforward repeating the same thing n times. We don't actually need a permutation argument. And this is all we're doing. So we didn't use plonk, we just did this. And.
00:10:11.008 - 00:10:32.744, Speaker A: Yeah, so in the simple form, this is just the circuit. And it's sort of the simplest circuit you will see today, I'm sure. Probably, unless anyone knows anything faster. And the advantage of a simple circuit is, obviously it's faster. For the proverb in the end, why.
00:10:32.782 - 00:10:39.036, Speaker B: Not use projective coordinates? And then you drop this condition on.
00:10:39.218 - 00:10:47.032, Speaker A: Yes, you could drop this condition, but then you'd have more variables. Bigger or a bigger degree.
00:10:47.096 - 00:10:52.480, Speaker B: It's proven to be more efficient to do addition points on identity curve.
00:10:55.940 - 00:11:34.152, Speaker A: I doubt it's more efficient than this, but yeah, I mean, just having extra variables would make it slower. So. Yeah, the inverse is kind of not a problem. While you're. While we're putting this in a constraint system, it's just we multiply the other side by the inverse. Yes, sure, the inversion is a problem for computing these things, and that would be faster in projective coordinates, but here it's a snob. We have nondeterminism.
00:11:34.152 - 00:12:26.240, Speaker A: We just need to give a constraint and that has fewer variables if you do it in affine coordinates and you just have a pair of points for each thing, except for this incompleteness problem. But you can though. Don't you have that in projective coordinates too? Without. Unless you do something really complicated. And there's a simple hack for that. If you weren't dealing with a subgroup, there's also trivial things you could do to solve this, like using feature mirror to find your point h or doing some hash to curve, which would also work if you have proof of possession for these public keys. Yeah.
00:12:26.240 - 00:12:48.274, Speaker A: Okay. And here are the polynomial constraints, sort of. We have these four expressions and they have to divide x to the n minus one. Obviously, this is incredibly complicated to put on a slide, but it is simpler than plonk and other things. Right. I'm just trying to. Yeah.
00:12:48.274 - 00:13:23.134, Speaker A: Emphasize that this is. We were going for something simplicity. I mean, as you all know, particularly the guys who were like the last talk, we're still in the era of hand optimizing all this stuff, if you really care about prover time. And that's why we didn't use an off the shelf thing. We did our own thing, which admittedly took longer. So there are sort of two things here. So one thing is we did this trivial thing, which in this case is to avoid decommitting our commitment.
00:13:23.134 - 00:14:26.550, Speaker A: In the snark, we use polynomial commitments so that we don't have to do the decommitment. It's already in the right format. This kind of made the proof a little bit more tricky because it breaks this sort of polynomial iop, polynomial program abstraction that everyone is using a bit. But it's not really too bad because this is actually what everyone has been doing with circuit polynomials. If you have a polynomial commitment, kind of in the input which you trust for the personic plunk, Marlin and this is not miles away from that. We trust that someone committed to these public keys correctly, and the verifier doesn't compute that themselves. So there is one thing that's a bit slow if you do it this naive way when you have large numbers of public keys.
00:14:26.550 - 00:15:18.150, Speaker A: So your input is what the commitment to the public keys and this bit field. Now, in the verification equations we have this polynomial B of X, which is your sort of Lagrange basis commitment to this bit field. And the verifier, they have the bit field. They can just evaluate this by biocentric evaluation for Lagrange evaluation for roots of unity. But doing that would cost them order of n field operations. There are other things you can do. So this is sort of like we're imagining that we consider each bit to be its own field element.
00:15:18.150 - 00:16:29.680, Speaker A: Another thing we could do was to split these bits into this bit vector into chunks and consider those to be field elements. We'd have to add a few constraints. So in particular we have this pack scheme where we divide the input into sections of 128 bits or something that fits, it's a power of two that fits nicely into a field element. And then we kind of think of this as order of n over lambda, where lambda is your security parameter. Because your field size is three to of lambda, it ends up being 100 times less field operations to deal with this bit vector at the cost of a few extra, some constant number of extra group elements, because we have to add group operations, because we have to add some more constraints. Or you could just give up on this bit field entry and just count the number of ones. And so your input would be the number of guys who were there, and you could add trivially some constraints to compute that from b of x.
00:16:29.680 - 00:17:40.258, Speaker A: But we are trying to put the bit field in for reasons I'll go into later. And which polynomial commitment and which curves do you use? We have an implementation on BW 6761 Bls twelve three seven, which is the same thing that sello is using in their plumo protocol. But there's a whole load of other ways you could implement this. The construction these guys use to produce this two train is generic. Like they pointed out in an f research post that it would completely work with BLS twelve three eight one. The only difficulty comes with freer transforms and finding a multipitive subgroup of the right size. But because this thing has a nice factor, that the base field is nice and too addict for BLS twelve three eight one, the scalar field isn't too addict, but it is too addict.
00:17:40.258 - 00:18:13.940, Speaker A: But the base field isn't, and the base field is the thing which our coordinates are in and which we're doing addition over. And then we have to use a different subgroup and more complicated FFT. But it will work. For those of you using bls twelve three eight one. In your consensus, you could also use something other than KZG, like non pairing friendly outer curve. We could probably get something smaller if you did some batching, it would probably be faster. If you want to verify loads of these.
00:18:14.710 - 00:18:17.582, Speaker B: What's the embedding dimension, roughly?
00:18:17.726 - 00:18:18.370, Speaker A: What's that?
00:18:18.440 - 00:18:20.930, Speaker B: What's the embedding dimension of the curve.
00:18:22.950 - 00:19:17.884, Speaker A: These guys came up with? Okay, and I'll try and remember if I can pronounce their names. But no, the guys that came up with this curve gave us sort of generic construction for producing a curve from a BLS curve of about twice the size. They wrote a couple their paper, which gave this and then another paper sort of generalizing it to bls twelve and bls 24. So basically you need a bigger curve in order to get the base field of this curve inside it, because we're doing operations on base field things. Well, if you use pairings, you have to get a bigger curve. You can probably not do that if you didn't want to use pairings for the thing, but we have to use pairings for BLS, which embedding of the curve.
00:19:18.012 - 00:19:24.530, Speaker B: So what's the minimum power field, final field, such as.
00:19:26.820 - 00:20:05.350, Speaker A: Right. What we're doing is we're considering, again, always go with the simpler case. We're considering the case when it's g one. We're considering this case. We put the public keys in g one, and so it's in the base field that we have the coordinates, not the g two case, which would definitely have a more complicated circuit. Yes, one. Now, admittedly, there is a bit of a trade off in BLS, actually.
00:20:05.350 - 00:20:49.174, Speaker A: I think you can. I kind of speculate that it will be secure to put your keys in g one and g two, and then you could have your hash in g one and the public keys in g one for the verifier. And that will get round the problem I think you might be seeing. But yeah, the simplest thing was g one, if you want to be fast. That was the trade off we made. So these are the. So we implemented it.
00:20:49.174 - 00:21:25.390, Speaker A: We've had this code for a while. I'm going to give you the link at the end. And these are the sort of benchmarks we got. The thing we were going for with all this customization. Okay, I've completely missed time. This then the thing we're going for with all this customization was to aim to have not only a decent verifier time, but a fast proving time. And you see all these schemes under a second for 1000 validators, like half a second.
00:21:25.390 - 00:22:19.860, Speaker A: And even if we go up to a million, they're sort of five, 6 minutes, which is kind of fast for what you can do with snarks on inputs that big. So this is what we do. How do we use it? Why is this the thing to do? And how does this compare with what everyone else is? So light client, as I said, is software to let a resource contrained device verify what's going on in contents in the blockchain. What resource contraindes are interested in? Well, a browser phone or a blockchain. So one of the chief applications I'm thinking of is for, this is for bridges. We have a light client on another blockchain and often it's just too expensive to put even 1000 public keys, certainly on Ethereum one. So basically a blockchain is more resource constrained than a browser on a phone.
00:22:19.860 - 00:23:19.006, Speaker A: And this sort of affected our model a bit, especially on how much security we want to have from this protocol. So we're looking at like we're thinking of systems where you have 1000 to a million validators, we maybe believe statements assigned by two thirds of them and the set changes like once a day or something, which is quite a lot if you're dealing with an entire history of a blockchain. And how light can we do this? We can know we can do this in constant size if this is all we're trying to do. And Dfinity and Mina have done it both with huge overheads, especially so dfinity does this reshareable threshold signature scheme. So they keep the same key, but it's complicated. And they have to do this complicated thing every time they change a validator set. And Mina just put everything into a snark, which is great because they're not just following consensus, but validity.
00:23:19.006 - 00:23:51.198, Speaker A: But again, there are a bit of downsides in computation. The direct comparable thing is Celo's plumo thing which use the same curves we do to roll up DLS signatures. What's the difference in between us and them? Well, mainly it's the target, the thing we're thinking of aiming at. Celo have like 100 ish validators. We're thinking of thousands to a million. We're thinking of targeting a blockchain rather than a cell phone. And that means that the cell phone you have to sync from scratch many times, almost every time you reach it up.
00:23:51.198 - 00:24:28.550, Speaker A: Maybe you have to sync it from scratch. But if we're doing something stateful on the blockchain, we only need to increment it. We only need to update it gradually. As a result, they only put like 140 something validator set updates inside their snark. And that meant that they had to do pairings inside snarks, loads of hashes inside snarks, and it just ended up being a lot slower than what we're doing. We only do one evoc at a time, which of course would be more data if you were thinking from the start. But on the other hand, if you have very large validator set, the plumer approach already uses huge circuits.
00:24:28.550 - 00:25:10.390, Speaker A: You probably wouldn't be able to roll up many errors if you had that many validators. Their proving is very slow, the hours to our seconds, but that's because they do hashes and pairings in their circuit, lots of hashes. And the bit that all that we do in our circuit is a very small part of what's in their circuit. And this is fine for them because they only have 100 validators. So we just catch up to something very recent and then we give 100 or 200 validators public keys and we can just do this at a time. Whereas in our sort of model, we don't ever want to give the validators the current set of public keys, so we have to do a new proof for everything. And it really helps to be able to do these proofs faster than the blockchain finalizes blocks.
00:25:10.390 - 00:26:14.860, Speaker A: So that's why we're aiming for sort of half a second with 1000 and even the a million validator figure I gave you, the five to 6 minutes is faster than an ethereum two epoch, so it's probably fast enough. The other thing I want to say is that we don't actually, this all works under a thesis adonis assumption, but generally we don't actually want to trust a validator at all. We'd like to somehow assume they're rational. We'd like to punish people who misbehave. And if we have this bit field and we can tell who signed, we can kind of do this. So in particular we have this property called accountability, which if you see the blockchain, you see a likely proof of something false, and if it's on a bridge, it's public, then you can give a proof to your blockchain and we can slash, say a third of validators. And as we've seen recently, bridge security is super important, right? Yeah, we want to be as secured as possible.
00:26:14.860 - 00:26:42.438, Speaker A: So yeah, we have code. There's a paper on its way that I will try and put a draft version of in this repository. Takeaways. This is kind of simple. We can apply it to many existing proof of stake protocols. All they need to do is to use BLS signatures or be adapted to use BLS signatures and have the sort of thing most guys are doing. I just want to say, yeah, if you want a snark proof of that's fast for large instruments, you have to optimize it by hand.
00:26:42.438 - 00:26:48.680, Speaker A: Try and make everything as simple as possible. And we think accountability is super important. Okay.
00:26:55.790 - 00:27:10.570, Speaker B: Thanks. We're actually out of time, but depending on if I get some thumbs up or thumbs down, if we have time for a question. Anyone have a question? So how many constraints or gates does it cost to have a pairing?
00:27:10.730 - 00:27:11.802, Speaker A: How many constraints?
00:27:11.866 - 00:27:16.670, Speaker B: How many constraints or gates does it cost to have a pairing or kg in your case inside a circle?
00:27:17.010 - 00:27:32.470, Speaker A: Don't ask me, we didn't do it. So I think there's actually clever ways around to avoid having a pairing that I could think of that I would be going for if I really wanted to do a recursive version of this. It biggers. I think it's going to be horrible.
00:27:33.530 - 00:27:40.020, Speaker B: Perfect. Thanks. Enjoy your break and then we'll be back close.
