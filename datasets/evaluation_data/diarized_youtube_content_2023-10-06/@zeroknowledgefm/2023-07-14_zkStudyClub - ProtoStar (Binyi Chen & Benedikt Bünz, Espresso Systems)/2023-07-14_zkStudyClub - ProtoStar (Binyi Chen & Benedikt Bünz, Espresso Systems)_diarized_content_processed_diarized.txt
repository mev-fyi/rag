00:00:03.110 - 00:00:18.650, Speaker A: Hello again everyone. Welcome back to another edition of ZK Study Club. This week we're excited to welcome back Ben Yi Chen and Benedict Boons to talk about their recent work protoscadar on generic efficient IBC schemes. So with that, Ben Yi, the floor is yours.
00:00:19.390 - 00:00:47.698, Speaker B: Thank you. Thanks Alex, and thanks for the introduction. And thanks everyone for coming. I'm Binyi and today bandit and I represent Protostar, which is a new generic but also efficient incrementally verifiable computation scheme. Okay, wait, so why it's not okay, it's working. Okay, so let's consider the following problem. Okay, imagine you have initial input z zero and you've done some computation and obtain some output ZM.
00:00:47.698 - 00:02:03.386, Speaker B: What the computation is somewhat some like iterative computation, but in each step you are running some step function given the previous output and some online witness which can be different for each step and finally get this output. So now how could you prove to someone else and efficiently proving it to someone else that CM is actually the correct output rather than some garbage values? More precisely, how could you prove that there do exist some intermediate witness such that each intermediate execution of step function was done correctly and finally the output is the m. So the solution to this problem can be quite useful, for example in blockchain world, because this type of computation actually captures a lot of scenarios. For example, verifiable delay function. Here the step function is just a run function and the number of iterations basically the harness parameter. And also like succinct blockchain like Mina, well, you can understand this intermittent output CI as the ledger state, and you can understand this online witness as some transactions coming in the block. And more importantly, recently there's a really popular application, zkevm, which is basically proving the correct execution of a block of EVM transactions.
00:02:03.386 - 00:02:38.966, Speaker B: Well, here the step function is a little different, but not much. Basically it's a family of functions which consist of multiple op codes and at each step you can pick one of the op code to execute. And given this, it's really useful and meaningful to start this problem. But interestingly, actually this kind of problem has already been studied a long, long time ago. Even like around bitcoin was invented back in 2008. Paul Vellins already introduced this great notion of IVC. Well, in the scheme the proofer can interpretively generate some small proofs.
00:02:38.966 - 00:03:16.102, Speaker B: Given the previous proof which we call IVC proof is the pi m here, pi m minus one here. And given this and given also the online witness, he should be able to generate the next IVC proof. And finally, the verifier only is to take this final output plus this really tiny IVC proof to ensure or to check that all the previous computation were done correctly. And we require that. Suppose these intermediate proofers are honest and they have the right witness intrusively, they should be able to generate correct proof. That passed the verifier check. Finally.
00:03:16.102 - 00:04:44.798, Speaker B: And it's so called a completeness and on the other hand, informally and intuitively, if the verifile has passed the truth check for some proof like Pi M here, it should be the case that all this possibly malicious proverb are actually knowing some valid witness. And intuitively this is also called knowledge soundness and more generally we don't need to restrict to proof like this kind of chain of computation. We can be more general like proving like tree of computation or dag of computation or some more general topology of computations and this is called proof carrying data or PCD. But for simplicity in this talk we are focused on IVC but we know that the techniques in this paper can also be generalized to work for PCB as well. Okay, so it is well known that actually the IVC scheme can be constructed from another really powerful primitive like Snarks, which is primitive like enable you to prove the correct execution of some gigantic circuit of computation using some really small proof which can be easily verified. So here the idea is basically very simple. It's like basically a recursive Snark with IVC prover will just generate a Snark proof for the statements that last execution of the step function was done correctly and the previous IVC proof was verified correctly by the Snark Verifile.
00:04:44.798 - 00:06:11.054, Speaker B: So we have to embed this circuit inside this large circuit as well and this is a really neat and simple idea. However, in practice it's not so performance mainly because in this circuit, in this recursive circuit, the Snark verification part can be really expensive. It can be really takes a lot of constraint to capture the logic of a Snark verifier. Moreover, because of the use of Snark you have to use some expensive parently cycles or trusted setup which is also undesirable. Given this, many researchers were exploring the way whether there exists another way to construct Snarks without using Snarks can we do it more efficiently from some other weaker primitives? And one particular popular and promising direction is the so called split accumulation scheme or folding scheme introduced by Bclms 20, I've seen published in TCC 20 and also the Nova paper in 2021 and basically the idea is the folding. So given some MP relation, the accumulation scheme or the folding scheme, the goal is to reduce the task of proving two MP instances into proving a single instance. How to do that? The idea is that given two instance witness pair will have some way to merge them together into a single one so that we only need to check the folded one rather than the one that we had before.
00:06:11.054 - 00:07:37.974, Speaker B: More specifically, we have these following algorithms in the scheme where the first we have accumulation prover algorithm which will take two instance with this pair like x one W one and x two W two and usually the instance parts, the x one parts are usually small while the witness part can be possibly large. And then taking this as input, the accumulation prover can merge them together and fold it into another instance with spare which is XW. Okay? And then there's another algorithm called accumulation verifile that will check that this folded instance is done correctly, is computed correctly. Basically, it will take some small input x one, x two, and get some meta information from the proverb and check that the output folded instance was the correct one. Okay? And finally, also I want to mention here is that one highlight here is the accumulation verifile is usually much cheaper than the Snark verifile and it can use much smaller circuit to represent this accumulation verifile, which is a key that we can have a much more efficient IVC scheme. Okay? So after having this having this folded instance with spare, there will be another algorithm called Decider that will check that this instance with a spare is inside the relation. Okay? So this is the Cumulation scheme and similarly it needs to satisfy the following requirements.
00:07:37.974 - 00:09:09.010, Speaker B: First, that if the original instance x one x two are satisfiable, it must be the case that folded instance is also satisfiable. On the other hand, if the accumulator approver can generate some witness, some folding witness that passed the final deciding check, it must be the case that he also know the corresponding witness for the original two instances that was folded before and that's the intuition of knowledge thumbnails. Okay, given this, it turns out to be quite easy to generate to build an IVC scheme from accumulation scheme. The general idea, this is already explained in the previous paper, but the general idea is the following that the idea is just to iteratively and continuously folding some running statements about the falling statement. First is the last execution of the step function was done correctly. And second is that the output accumulator, in particular output accumulator instance was folded correctly. So after having this proof, it's okay for the IBC proofer to iteratively generate this accumulator from some knock instances, for example, from some knock proofs, we ensure that this instance was done correctly, given this proof, and we use this accumulation proofer to generate the folded instance with pair.
00:09:09.010 - 00:10:04.890, Speaker B: And after having that we can ensure that as long as the final resulting accumulator is checked correctly by the decider which will be run the verifier, all the previous computation will be done correctly. And also because we have this proof which has been checked, the accumulator is also the correct one that needs to be checked. So that's the general idea. But the highlight here is that in this scheme, the IVC prover only needs to generate some non interactive proof which might not be succinct for some small circuits, because the accumulator verified circuit is much smaller than Snark verified circuit. That's why in a concrete sense it's much more efficient. Okay, so there's one small caveat here is that we don't have any guarantee that this decider is really, really succinct. We don't have a guarantee that it's always constant complexity, and the complexity of it might be linear to the instance size.
00:10:04.890 - 00:10:19.470, Speaker B: But this is easy to resolve by adding another Snark that delegates the computation of decider at the end. So finally, the IBC proof will only check this knock proof rather than running the decider himself.
00:10:20.130 - 00:10:22.880, Speaker C: Okay, so that's just one thing.
00:10:23.570 - 00:10:24.480, Speaker B: Go ahead.
00:10:26.690 - 00:10:58.634, Speaker C: One thing that is important is the decider will always be constant in the number of iterations. So there's two things. There's the size of the function F, and then there's the number of iterations. So both the accumulator and the entire big accumulator. So the witness and the instant and the decider are constant in the size in the number of steps. But some parts here, especially the accumulator witness and the decider do not have to be constant in the function in the step function.
00:10:58.832 - 00:11:21.620, Speaker B: Exactly. In general, it is still satisfied notion of IVC. IVC's notion only require that the verifier complexity is independent of the number of iterations, which here we still satisfy. But it's just in practice, if the circuit is large, maybe the decider complexity can be relatively large compared to checking a Snark proof. Okay, great.
00:11:24.150 - 00:11:35.800, Speaker D: I have a question about how the concrete security depends on the number of iterations. I think that's maybe a complicated subject, so we should perhaps leave that to the end.
00:11:36.330 - 00:12:08.110, Speaker B: Yeah, maybe we can discuss this in the end. I think this is a good question, but yeah, we can discuss later after this talk, I guess, after this presentation. Yeah. Bendig, please help me to remember this question. Great. So given this background, we know that there are already some very efficient construction IVC like Nova and also in Bclms 20. Basically, the accumulation verifier complexity is only around like two or three group exponentiation.
00:12:08.110 - 00:12:57.250, Speaker B: However, there's still a gap for supporting some advanced application like Zkevm. The main reason are twofold. The first is that the existing accumulation scheme usually only support R one CS constraint system. For example Nova or PCL Ms 20. And this R one CS is the type of constraint system that's kind of really traditional and is kind of less expressive than some recent advanced gates like high degree gaze or lookup gaze. By expressive, what I mean is that basically to represent the same type of functionality, for example, elliptic curve addition, you might only need like one or two constraints if you are using high degree gates, but you might need more if you're using R one CS. So we are saying the hydro gate can be more expressive in terms of its representation power.
00:12:57.250 - 00:13:43.018, Speaker B: And this is really useful also in the EVM applications. And we also know that while we have finished this paper, we also know there's also some other really elegant follow up like Sangria and Origami that still support Plunk or lookup gates. But there is some caveats here. The main issue is that when the degree of the gate increases, the folding complexity also grows by a factor of T, which is the degree of the gates. That means you can kind of cancel out the advantage of using these expressive gates because amortized chords per gates can also increase. So you might better just use Nova in that case. So this is not enough, I think, and this is one of the good feature that are missing.
00:13:43.018 - 00:14:46.818, Speaker B: And second one is that we do not have really efficient circuit branching support for most of the scheme, except some recent work called Supernova. What does it mean? Let me illustrate by example. Let's assume there's like we want to simulate EVM execution and EVM has hundreds of op code, right? And in each step what you want to do is just to pick one of them and execute. But if you are using Nova, what you can do is only to embed all this op code gadget inside a single recursive circuit and do some routing when you do the proving. And the folding complexity will consequently be proportional to the sum of the size of this opcode circuits, which is undesirable because there are hundreds of opcodes. So, what we want is to have another scheme where the recursive circuit size is only proportional to the single op code that has been being executed at runtime. That can save us a lot of proving costs because the circuits can be much smaller when the numbering instruction is a lot.
00:14:46.818 - 00:15:59.078, Speaker B: So we highlight that these two requirements are really essential and important for DKE EVM applications because first, DKE EVM are heavily using this recent advanced type of GAIF and second, EVM do have a lot of instructions like hundreds of instructions in their instruction set. Beyond this, another different reason why we want to have a new paper here is because we see that many existing scheme of IBC do have really different flavors of description as well as security proofs. It would be great if we can have a unified and general framework for understanding all this IBC scheme and make the analysis and the developments much easier. So that's another motivation of protestar. So, given this, I want to highlight our contribution here is that first, we have come up with a new general recipe for constructing IVC scheme. More precisely the folding scheme. Basically, given some MP relation R, and given some special sound interact protocol for relation R, we can generically transform it into some folding scheme for this relation and it turns out to be actually quite efficient.
00:15:59.078 - 00:17:11.118, Speaker B: And Nova is just one special case in this modular framework. And second, we also construct some really efficient special sound protocols which I will talk about later. Some special sound protocol for some really expressive relations like high degree gaze, lookup relation, nonuniform circuit selection and also CCS which is a recent advancement on the constraint system which is generalization of R one CS to higher degree gaze and the result and by combining these two parts the first part is general recipe, the second part the special sound protocol. We obtain a new IVC scheme called Protestar. And the highlight here is that dominant cost of IVC proving is only one MSM of size of the witness and also a knock proof for the recursive circuit that's dominated by three group exponentiations. In particular, there's no dependence in terms of the number of group exponentations, there's no dependence on the gate degree as well as the lookup table size. Also we know that there is a concurrent work, Hypernova, which is also really an elegant work and they also achieved some similar results.
00:17:11.118 - 00:18:18.566, Speaker B: And the difference here is that we think that our support to Lookup seems to be much more cheaper almost for us, our support to Lookup is almost for free, while their support of lookup can add a lot of overhead. And second, in this recursive circuit we know that the number of hash operation and non native field operation inside circuits is also significantly smaller in our case. And moreover, because our support to look up the circuit for non native field operation can be even smaller, so gives even more advantage out of that. So we think that for some application like Ckevm, it can be favorable, protostar can be a favorable choice. Okay? So that's the contribution. And now let me go to the detailed part and let me explain how the general recipe works for building a folding scheme. Okay? So recall that our main goal here is that given multiple instance witness pairs, we want to fold them into a single one, right? And we follow the framework as below.
00:18:18.566 - 00:18:56.930, Speaker B: Basically, we have three steps. First for some amp relation R. We first have some multiram special sound protocol for it, which turns out to be usually pretty easy, and we'll load it. Later. And after having this interactive special sound protocol, we can use some really standard trick to transform into non interactive arguments of knowledge. And after that we propose to have a generic and efficient transform to construct a folding. Scheme for the set of verifier check in these large proofs and this is basically a generalization of Nova.
00:18:56.930 - 00:19:46.110, Speaker B: But at this point, steel is still not good enough because as the degree of the gate increases still the number. Of group operations is still proportional to the degree of the gate. So that's another key here, is that we add one more step at the beginning while given some interact protocol for this relation r we add one more round and transform it into another interactive special sound protocol, but with a much more compressed verified check. So the set of verified check is much simpler and has much lower, like mostly has not a lot of high degree. Gates. And after that, we use the same transformation as we did before. And turns out the resulting scheme becomes much more efficient in terms of number of group operations.
00:19:46.110 - 00:21:02.380, Speaker B: In particular, the number of group operations now is independent of the degree of the verified check. So that's the general framework and let me go into more details later for now, any quick questions? Okay, so before coming to the detail of construction, let me quickly recap what a special sound protocol is since I already mentioned it for multiple times. So, intuitively, what special sound protocol means is this following so given some relation R, suppose you have some interact protocol for proving this relation R and we say this protocol is k special sound. If given k difference accepting transcript, we would be able to extract some witness for this relation. Then we say it's case special sum. For example, let's take this really naive example where there's no public inputs, but the goal of proofread is just to prove to the verifier that he knows some really long vectors ABC that satisfies some head and mark product relations and there's a really naive special sound protocol for it. Basically the proofreader just send the entire witness ABC to the verifile and verify just check all the equation need to be checked in order to ensure that it's inside the headermart product relations.
00:21:02.380 - 00:21:36.402, Speaker B: And this is still a special sound protocol. And first is obviously one special sound because every accepting transcript is basically just accepting or bad witness inside the relation. And we know there are three important parameters we care about in an interact protocol. The first is R, which is number of runs that proverb will send messages. So here you see this non interactive, basically just only one move. So r equals one. And second there is a parameter called d, which is the maximal degree of the set of equations checked by the verifile.
00:21:36.402 - 00:22:43.834, Speaker B: And third is another parameter is L, which is the number of checks done by the verifile. So more specifically in this example, the Verifile will conduct n difference equation checks, each of them is just a multiplication, right? So because multiplication is degree two, that means d here equals two and l here equals m because we check n different equations here. And to express in some language of Snark circuits, this basically n is the number of constraints in a circuit and this basically the gates of the degree of the gates. Okay, so that is it, that's special sound protocol. And before coming to the next step, I want to highlight one more thing here, is that usually it's much much easier to design a special sound protocol for some relation. And the main reason I think here is that we don't have any requirements on the complexity of this communication as well as the complexity of the verifier check. So basically prover can send a lot of things, can send a really gigantic, possibly large witness together to the verifier and the verifier can do a lot of checks to check.
00:22:43.834 - 00:23:37.162, Speaker B: This is inside the relation and still is a valid special sound protocol. So that this makes our analysis and design much easier to syncing in the special sound protocol case. Okay? So now suppose we have this interactive protocol which is special sound. We can use some really standard tricks to transform it into a non interactive argument knowledge using commit and open and fierce Amir. So basically, finally what you get is a knock transcript basically consists of two parts. The first part is the short commitments to the messages that's being sent initially in this original protocol and the second part is the big openings of these commitments which is basically the message sent by the approver in the initial protocol. And also you need to come up with some random challenges using fierce Emir.
00:23:37.162 - 00:24:27.322, Speaker B: And it was proved by some previous work that as long as the original protocol is special sound, the resulting knock protocol also have knowledge thunders. So basically now we have obtained some knock proof for the same relation R but also have knowledge thunders and our goal is reduced to fold these knock proofs. Okay? And for now, let's see, we already have this knock proof for this relation. For simplicity, let me just assume the public input is empty. But when the public is not empty, we just put it inside the instance part of this proof. So now in this knock proof we have two parts. The first part is what we called proof instance which consists of short commitments to the messages and challenges and possibly public input which we'll ignore for simplicity.
00:24:27.322 - 00:25:20.122, Speaker B: And second part is this long proven messages and this knock proof is valid if and only if a set of L equations are checked correctly. So if and only these L equations are evaluated zero. And here each equation out of this L equation in FD for simplicity we assume is homogeneous. It means that every term of this algebra formula exactly has degree D in its input. For example, x one times x two plus x one square is homogeneous because every term is degree two while x one plus x one square is not because x one has degree one while x one square has degree two. But for simplicity, let's assume that every term are the same but it's actually without loss of generality. Because suppose we have some other more general formula which is in homogeneous.
00:25:20.122 - 00:26:23.070, Speaker B: We can just add a select variable here u as the input and we can pad every terms to exactly degree D by multiply the power of U. And here in this NOx sims, we just set U to be one, right? So it's without loss of generality and for simplicity let's always consider this case. So now our goal is to fold this knock proof and build accumulator out of it. And the accumulator exactly have really similar form which also have these commitments and challenges and proven message fields. The only difference here is that we add one more arrow vector commitment inside this accumulated instance, which is some short commitment to some long vector arrow vectors. And also the set of checks by the Accumulator is changed a little bit. Well, in each equation, instead of requiring it to be exactly evaluating to zero, it's okay to be evaluating some other non zero values, as long as these final l values are exactly consistent with the commitments inside this accumulated instance.
00:26:23.070 - 00:27:20.378, Speaker B: And this is some trick very similar to, I think similar to Nova and also Bcims 20. Okay? So now our goal is to fold this knock proof onto this Accumulator one by one, okay? So that's our goal. And recall, the main goal here is to want to have a new Accumulator instance, such that if the new one is satisfiable, then all these checks, all this previous check for previous Accumulator check, and this not proof will also be correct. So how to do that? A general idea is that we see that this structure, the two structure are really similar. They all have the same formula FD here. And what we can do, we can interpolate them using polynomial. So let's consider the falling polynomial, while we replace every message by some linear polynomial, that is interpolating both the messages from the Accumulator as well as from the NOC.
00:27:20.378 - 00:28:11.126, Speaker B: So, for example, we use x times m plus m prime to replace M prime and m and similarly use x times r plus r prime to replace those challenges. Why we care about this polynomial? The idea here is that because this FD is homogeneous, it has a really good feature and a property, because basically, if you see the constant coefficient of this set of polynomials, they exactly matches the left hand side of the cumulator's check. And if you see the degree d coefficients of this polynomial, they exactly matches the left hand side of this knock verifier. And let me illustrate by one very simple example. Let's say there's only one equation being checked. Well, L is one, so there's only one polynomial to be considered, and the degree equals two. And this formula is just r one square.
00:28:11.126 - 00:29:07.030, Speaker B: And then we rewrite it in this polynomial representation, replace everything with linear polynomial and expand it. Well, now we replace r one to this and do the square again, and we expand it, and we see some magic happens. It's exactly that constant term, exactly accumulative check and the degree two term exactly not verified check. So this is one simple example, but believe me, it holds it more generally. And given this feature, given this property, it gives us a really good way to check that the previous two checks are done correctly. Basically, this is equivalent to check that this polynomial has the falling form. First, the constant coefficient exactly is some vector being committed in e here, and the degree d coefficient exactly zero, right? So we all need to prove that.
00:29:07.030 - 00:29:36.606, Speaker B: And how to do that. The idea is the falling. So we represent this polynomial in this form, this coefficient form, and the proofer only to commit this polynomial and show that exactly this right hand side polynomial has this form. So the idea is the falling. The prover will just send this intermediate across arrow terms, which will bind this polynomial. And sometimes, because EJ can be long, because there are L number of equation being checked. So we need some Pettixon commitment to make it short.
00:29:36.606 - 00:30:38.210, Speaker B: And after that, we can reduce this polynomial check to some random evaluation check, where the Verifier will just sample some random challenges. And now this kind of check can be reduced to this check, while x is replaced by these random values alpha. And you see that this is exactly a new type of accumulator check. The only difference is that the message becomes the linear combination using alpha and arrow terms also becomes some linear combination of the original cross arrow terms and arrow vector commitments. And so that gives us a really natural way to do folding, which also similar in Nova, is that we just do the random linear combination, for instance, and error commitments and approver will additionally fold this witness as well. Okay, so for now, I'm kind of a little bit cheating because I'm always arguing the soundness, but actually what we need is something called noise soundness. So we should be able to extract the regional witness for these two checks, right? But actually the idea is quite similar.
00:30:38.210 - 00:31:37.570, Speaker B: So proved by previous work is sufficient to only consider special soundness. And we can prove that this protocol satisfies d plus one special soundness. Basically, given d plus one different transcript with different challenges, then first we can pick two accepting transcript and extract out, like solving some equations and extract out Mm prime here. And the second step is to prove that those Mm prime and RR prime are the correct witness. And that is also simple because this FD is a degree d polynomial and this polynomial check are matching for d plus one points. That also means that these two polynomial are exactly identical by some fundamental algebraic theorem. And that means the extracting m and M prime exactly matches these equations because the constant coefficients matches and the degree d coefficient also matches.
00:31:37.570 - 00:32:21.190, Speaker B: So that proves that the extract witness is a valid witness. So that's an intuition, but I refer to paper for more detailed and more formal proof. Okay, so at this point, it's still not very good enough, right? Because you can see when d is good enough, d is large. The number of group operations by prover is actually around d times L, because you have to commit to d minus one cross error terms here, each of which is length L. Similarly, the Verifier needs to combine the error commitments and that accounts for d group operations, which is really expensive if d is large. So. We need to figure out a way to make this much more efficient.
00:32:21.190 - 00:33:23.014, Speaker B: Any questions so far? Okay, so let's see how we can optimize this scheme. You see, the reason that we need this DL group operation is exactly because we need to do some pattern commitments to make it short, right? Well, each vector is of length L and L is usually really large because the number of constraints is really large for circuits. But let's suppose we have some special sum protocol in which there's only one check. Okay? In that ideal case, we don't need to use this Patterson commitment, right? Because the vector itself is already short, we can just send them in the clear without using any group operations. So this gives some idea. The idea is just to given some any general spectrosum protocol, can we transform it into another protocol that's proving the same relation? But we only use single check, basically L equals one. If that's the case, when we do the folding step, we can be much more efficient.
00:33:23.014 - 00:34:18.650, Speaker B: So that's the general idea and the solution to it is also pretty simple. So suppose we have two gates, two equations, we can introduce one random variable and do the linear combination of it and it's easy to check if the final resulting gates is satisfiable, then with hybrid original y is also the satisfiable. And now we successfully merge these two gates into single gates. And more generally, given L different equation, we can interpolate all this L equation into a single equation using the power of beta. Power of beta. And this gives some idea of doing transformation. So more precisely, given some initial protocol which is special sound, what we do is that a verify will sample a random challenge and merge all the checks you have before into a single check, which is basically the random combination of the order checks.
00:34:18.650 - 00:35:04.562, Speaker B: But this special sound protocol is not good enough yet because the gate degree or the degree of the check becomes much higher because in one term we need to multiply by beta to the L. So degree becomes D plus L and L is large. So still we have a really bad scheme as expensive. But this issue is actually not very difficult to solve. The idea is just to see and understand this power of beta as a single value, as a single different message rather than a power of beta. So what we do is the following. So after sample this beta, the proverb will compute the power of beta himself and send the last message as this power of beta, okay? And then the new check becomes the following.
00:35:04.562 - 00:35:46.038, Speaker B: So instead of writing this as beta to the J, we all need to write as a BJ, which is one of the message being sent by prover. And now the degree is only D plus one. So this kind of solved the issue of high degree. The degree blow up. However, they also introduced some two other issues. The first issue is that the proverb needs to send more messages, basically it needs to additionally send L messages, that means later it has to commit to these L terms which can be expensive and second issue is that the Verifier also need to check that these messages are done correctly, which I'll talk about later. So let's focus on the first issue here.
00:35:46.038 - 00:36:44.650, Speaker B: Well, the proofer is more expensive and the solution to that is actually pretty easy. Well, instead of representing a power of beta using a single variable, we can represent it as a product of two variables or even more generally you can represent a product of three, two or even K variables, right? And that means instead of sending all this power for beta, the proofer only is to send this, the only is to send a square root L number of message bi and Bi prime. Well, Bi prime is the beta to the I times square root L and still you can have this check which is only degree D plus two. And more generally if you using K different product, we have less message to be sent but higher degree. But usually we find it a sweet spot to using two because initially already proverb already needs to put some efforts. So using two already decreased from L to square root L. So usually it should be sufficient.
00:36:44.650 - 00:37:47.694, Speaker B: Okay, so this solves the first issue while the proverb complexity doesn't increase much and the second issue is the falling. So recall again that initially what we have is we have L different degree decays, right? And using this trick we transform into a single degree decays. However, we also introduce additionally squared L degree two checks here which seems doesn't solve the problem but the key observation here is that the additional check are low degree. So what we can do is that we separate the checks into two parts. The first part is a single degree D checks which we are using D minus one cross arrow terms, however, these D minus one arrow terms are really short so we can just using field operations rather than Paris and commitments. And second part is this part and we do need to add one more cross error vector commitment to commit these squared L terms. However, because it's low degree we all need to add one more separate commitment for that instead of D of them.
00:37:47.694 - 00:38:51.326, Speaker B: So that solves the issue and finally, what we get is still like the Verifier only to perform R plus two group exponentiations and the plus two here is coming from two parts. The first part is the extra message being sent by the prover and the other one is the extra separate commitment for these squared L checks. And we also note that a great thing here is that when we later support lookup, it really doesn't add any overhead because we have a special sum protocol for Lookup that only involves degree two checks. We can bash all those checks inside this invest cumble checks and we don't need to introduce any extra commitments out of it. So that's also one reason that we can have a really good support for Lookup. Okay? So in summary, the general recipe works as follows given some MP computer relation, suppose we have some interactive protocol for special sum protocol for S. We first transform into some another special sum protocol that has compressed the verifier check and use some standard trick to make it a non interactive argument of knowledge.
00:38:51.326 - 00:40:05.450, Speaker B: And finally we use this generic way to make folding scheme for this NUC verifier. And then we used some previous works idea the IVC compiler to construct an IVC scheme. In particular, if you want to have more efficient construction, you can use the recently proposed work by Dan and Wilson that has using cycle curves to build IVC which is also some idea that has been described in Nova paper. Okay, so basically now we have reduced the problem of constructing IVC for some expressive relation into the problem of building some special sum protocol for some expressive constraint system, right? And it turns out to be actually pretty easy. In particular, we have obtained some only one move special sound protocols for a lot of building block relations like permutation relation, high degree gate relation, non uniform circuit selection and the CCS which is some recent advancement on constraint system. And this scheme is really almost trivial, it's very similar to the naive example I mentioned before. The idea is just let the prover send the entire witness and the verifier will just check the set of algebraic equations that represent this relation.
00:40:05.450 - 00:41:30.062, Speaker B: And the main reason that we can do this is because all these building block relations they can be represented as a set of algebraic equations over the initial witness. So this makes the design of very very easy and this is also justified the claim I had before that building multiron special sum protocol for many interesting relation can be actually pretty easy. And there's one subtle exception here which is Lookup relation because we don't have a really straightforward way to represent this lookup relation as a set of algebraic equation only over the Lookup values. But it turns out we can still have a really simple and efficient special sum prototype and here the interaction will help. Okay, so let me quickly recap what Lookup is. So lookup relation is saying that given some online lookup value which consists of L values, we want to prove that they belong to some preprocessed table of elements T which consists of T elements where T can be really? Large, depending on the table size and like proved by this paper by Hobb in 2022 called Log Derivative for Lookup, which is also a really elegant paper and recommends everyone to read it. It shows that this Lookup relation holds true if and only if this fractional identity holds true.
00:41:30.062 - 00:42:21.618, Speaker B: More precisely, if only there exists some multiplicity Mi, such that these two sum of fractional identity whole chu, where the first one is one over x plus wi, and the terms in the second sum is Mi over x plus ti. So intuitively, what Mi means here is that it is a number of appearance of this elements ti inside this lookup set, right? So basically, if that's the case, you can generate this Mi, if not, then the fractional this polynomial identity on the left hand side won't be equal to the right hand side. So basically, this one holds true. That's the general intuition. And given this, we can come up with a special sum protocol. The idea is also quite similar to what we did before, is that we replace x with some random values by verify. So, the idea is the following.
00:42:21.618 - 00:43:24.054, Speaker B: In the initial round, the proverb will send this witness and the multiplicity which will bind this to a fractional polynomial identity. And after that reduce to check that this fractional identity valuation are equal over some random value r. So verify will sample this random challenge and the prover will compute this fractional identity evaluations and send it back. And now, the verifier only needs to check that these two sum are equal, and also needs to check that this vector being sent by prover are computed correctly using some degree two checks. Okay, so make sure the h and the g are computed correctly. And we note here, that there's one great feature of this special sound protocol, is that even though the proof of message can be quite long, like have length T, they are actually pretty sparse. Basically, the number of nonzero elements inside these messages will be less than L, which is significantly smaller than T.
00:43:24.054 - 00:44:32.346, Speaker B: And that means later, when you want to commit to it, the committing complexity can be much smaller, because we don't need to do anything for those entries that are zero. And this gives us some inspiration, that maybe we can have a really efficient IVC or not IVC for folding scheme for this lookup relation. However, there's one subtle challenge, the reason is folding. So remember that in this folding scheme, in this folding protocol, at the beginning of this folding protocol, the proofer needs to send those intermediate cross error terms, right? And the computation of those error terms actually depends not only on this online knock proof, but also on the accumulated messages. But accumulated messages is actually not sparse, because basically there's some randomly combination of multiple proof of messages. So that means the computation of the error terms can amount to OT number of field operation, which is no longer independent of T. However, we note that actually what we need is just the commitment of these cross error terms, and it turns out we can compute them homomorphically.
00:44:32.346 - 00:45:43.874, Speaker B: Given the accumulated message commitments and accumulator error vector commitments, without bothering to recompute all the vectors on the line. So that's the general idea of saving these tot operations and still have a complexity that's independent of table size. Okay, so I want to note one more thing here, is that this lookup protocol can be easily generalized to support vector lookups. Well, basically every element in this table is just a vector of elements rather than a single elements. And this can be quite useful in application like 32 bit arithmetics, where you just store all those input output arithmetic pairs and you just do lookup in the circuits, which can be really useful and is heavily used in deckevm. Okay, so after having these sub protocols for building block relations, if we want to have some MP complete relation for some expressive constraint system, what we do is just composition, right? We just compose all this subprocol to obtain the final protocol, which is special sound, and it's for this MP compute relation and this field the last piece of our story. And we now have two parts.
00:45:43.874 - 00:46:21.650, Speaker B: First, we have a general recipe for building folding steam given some special sample protocol. Second, we have really simple special sample for some really expressive relation. And by combining these two, we obtain protestar, which is a new IVC scheme in which the recursive circuit is only dominated by three group exponentations. And in particular we can support really high degree gates, really large lookup table, and really a lot of op codes being executed in some instructional set. I think that's basically it. And thanks everyone for listening. And now I'm happy to discuss more and answering questions.
00:46:21.650 - 00:46:24.850, Speaker B: I saw there are some cash questions in the chat.
00:46:25.670 - 00:46:29.730, Speaker D: Okay, I know those are just the papers.
00:46:29.890 - 00:46:41.270, Speaker B: Okay, thank you. Okay, maybe we can first discuss the question on Darwin, right? Bendig, you want to give a comment?
00:46:43.530 - 00:47:08.020, Speaker D: So when you're computing the concrete security that determines what parameters you want to use, do you need to take into account the maximum length of an RBC chain in your application or how does the concrete security depend on?
00:47:11.110 - 00:48:14.466, Speaker C: Well, the interesting thing is that the provable security is actually basically says you can only do IBC for a constant number of steps. So even like constant in the security parameter. And the reason for it is that every time when I do the security proof, I call the extractor and then I call the extractor the previous proof. And these things sort of blow up multiplicatively. So after T steps of IBC recursion, my extractors actually size something like polylamba to the T. And if that gets too large, if that gets larger than polynomial, then at least theoretically this becomes an issue because it's sort of meaningless to say, oh, I now have an extractor which can break some assumption. But if that extractor is exponential size, then it sort of becomes a vacuum statement.
00:48:14.466 - 00:48:31.862, Speaker C: This, however, is only a theoretical issue. I think practically there really is not any let me think. But my interest I don't see any practical implications.
00:48:31.926 - 00:48:40.334, Speaker B: Yeah, I think it's on the security field. Size is large enough, usually concretely support, multiple steps. Go ahead.
00:48:40.372 - 00:49:11.350, Speaker D: Yeah, here's my intuition. Let's say your proof system is such that you need to find a targeted hash collision. So you could build a table that includes all of your possible targets over multiple steps, and then it's not clear that you aren't losing security if you have more targets.
00:49:11.930 - 00:50:11.994, Speaker C: Yeah, I mean, I think another way to look at it is if we at multiple points and this is where the theoretical issues come in at multiple points, we transform interactive protocols to non interactive protocol. And that's absolutely necessary for getting something like IVC. Otherwise the notion sort of becomes meaningless. But you could look at sort of the unrolled, like, underneath it, our interactive protocol, and you could look at the sort of it would be like T times R round interactive protocol, because even the accumulation scheme is derived from an interactive protocol and then applying feature to it. And I don't think we know for special sound protocols, and this only holds true for these special sound protocols, there's not really any issue in sequentially composing them for T rounds. I'm not saying anything wrong, but yeah.
00:50:12.032 - 00:50:31.620, Speaker B: I think the parameter like the R also the degree D also affect the concrete security bonds. But I think for reasonable large number of R or D, usually it shouldn't be an issue in practice, but of course, if D is super high, then we definitely would blow up.
00:50:32.230 - 00:50:46.374, Speaker D: So what you said about unrolling the protocol as though it were one big interactive protocol to which you applied fiat, shame. It sounds as though you might be able to prove something about that. It's probably a yeah, but it's really.
00:50:46.412 - 00:51:22.962, Speaker C: Not like the important thing. So, yeah, you could maybe prove something. The problem is, one thing that is important to understand is that kind of these IVC constructions, it's not the same. We don't just apply Peter to mere once, right. It's not just like you have some big old interactive program. You apply phmeer of the intermediate components, like, for example, the accumulation scheme and the intermediate and then you implement within the circuit. Right.
00:51:22.962 - 00:51:58.698, Speaker C: You build a circuit for that non active protocol. And you cannot build a circuit for an interactive protocol. Right. It doesn't really track. I'm just trying to give an intuition here, right. There isn't really a meaningful thing that you can do or prove security about. I'm just trying to say that the composition of T round or composing special sound protocols sequentially seems to be fine.
00:51:58.698 - 00:52:08.930, Speaker C: And these things are derived from special sound protocols. But that's kind of all. Yeah, it's very heuristic.
00:52:10.550 - 00:52:13.540, Speaker B: What is better is yeah, go ahead. Sorry.
00:52:14.310 - 00:53:03.698, Speaker C: One thing that you can do if you care about the so I think Bingi mentioned this. So we've given these accumulation schemes, they work for kind of you can build basically the IVC chain in a line, but you could also build it in a tree because we give basically a folding argument for you could also fold two accumulators together or stuff like that. And then instead of building a line, you build a tree. And then at least your depth is only logarithmic and in the length of the computation. And then some of these issues become a little bit better.
00:53:03.864 - 00:53:21.980, Speaker B: And another advantage of using a tree is that sometimes it also help with parallelizations. Like if you want to compute all the leaves together right. You can compute all the leaves in parallel and do it layer by layer and it can be even faster. Okay, thank you.
00:53:26.030 - 00:54:23.890, Speaker D: Yeah. There was a thing that I pointed out in, I think, a previous time when you did a similar talk, which is the optimization where user square root of the number of challenges hang on, can we go back to that? It's this one. Yeah. So this square root could be replaced with a K through and then the degree would become D plus k. D plus K. Yes. I think I agree with you that the square root is likely to be sufficient in practice because this o of square root thing that you have to send is probably small enough that that's not the bottleneck.
00:54:24.050 - 00:54:24.422, Speaker B: Yeah.
00:54:24.476 - 00:54:29.034, Speaker C: Suppose you, for example, important to understand sorry, go ahead.
00:54:29.152 - 00:54:46.510, Speaker B: Yeah. Suppose, for example, even you have two to the 30 days, right? The square root is just two to the 15 and it's already very small. And usually you also have another work in the previous runs. Right. So it shouldn't be an big overhead. Usually if you do square roots.
00:54:48.370 - 00:55:40.182, Speaker C: Yeah, I think you can say this, actually pretty generally, because the size of M, the witness is going to be at least L, and you need to at least commit to M. So the only case is so you're. Already paying for the cost of committing to m, which if m is dense, then that's a cost of L. So committing to something that is square root of L should not be an overhead. Of course, there could be extreme scenarios where M is incredibly sparse. So if M has a lot of zeros, maybe it has only square root of L entries, then you might want to go to a cube root. But exactly as you pointed out, Dara, this only adds one to the degree.
00:55:40.182 - 00:55:49.282, Speaker C: So it's really cheap to do that. But in general, square root of square root should be fine.
00:55:49.416 - 00:55:59.830, Speaker B: And the advantage here is always a parameter. Right. Depending on the deep frame application, you can choose your parameter, which you choose what's the optimized one, right? Yep.
00:56:06.950 - 00:56:34.758, Speaker A: I have a question. So I think in the paper that you referenced, the very end which is linked here, describes a soundness vulnerability in the original Nova protocol. And I was just know if you guys had comments on whether or not that vulnerability applied to other schemes that were based know, Nova style IVC schemes and how you address it in this particular construction.
00:56:34.934 - 00:56:56.306, Speaker B: Yeah. So basically I think that paper correct me if I'm wrong, Dan, I think that paper is mostly about correcting some vulnerability in the IVC compiler. Right. This shouldn't be an issue. Actually, there's no issue for folding parts. And basically, for example, in our paper we are just mostly focusing on folding. And given this folding, we just use any IVC compiler that actually works.
00:56:56.306 - 00:57:10.280, Speaker B: For example, we just use the IVC compiler in dense paper. Then I don't think that part will be affected. In particular, the folding part in Nova as well as the star paper will be affected at that point.
00:57:11.930 - 00:57:52.290, Speaker D: Yeah. Shunbo and I looked at this for Halo and we're pretty sure that we didn't make the same mistake. IVC is not fully implemented for Halo Two, but in any case, the mistake seemed to us to be kind of specific to what they did in Nova and they basically did more work than they needed to and included another element in the proof that they didn't need to. If you're looking at it with an engineer's eye, then you would say, I can optimize this out and you wouldn't make the mistake.
00:57:53.190 - 00:57:53.858, Speaker A: I see.
00:57:53.944 - 00:57:54.350, Speaker B: Yeah.
00:57:54.440 - 00:58:43.474, Speaker C: But I think it was essentially like you could say this was a bug in the implementation, not in even the Nova paper. But I think the real issue is that both the Nova and Halo and all the maybe the compiler which when people actually implement this, when you describe it, you describe it over one curve because that the easiest way to do it. When people implement this, they use cycles of curves. They use two curves. And I think that this was just under specified. Nobody had actually written up and thought about formally of how to do this. And when it's under specified, it's easy to make mistakes.
00:58:43.474 - 00:59:01.530, Speaker C: And I think that is exactly what happened here. We're totally at Fall here in Protostar. We don't give a precise compiler, but I think it shows the value of someone really going in and writing up the details.
00:59:02.590 - 00:59:40.040, Speaker D: I agree. I sympathize with paper authors because this is actually really complicated and would double the length of the paper at least. And I think maybe it belongs the form of a paper is probably not the right way to do this. In halo two, we have additional documentation. We have a Halo Two book that is where things like this belong and is not subject to page constraints. Yeah, readable can include lots of background information.
00:59:40.570 - 01:00:21.042, Speaker A: I was going to say a very comprehensive Halo Two book, if my memory serves correctly. But no, in general it's great and I appreciate your all's comments on it because I think it is interesting, the interplay between theory and practice. Right? And obviously where I sit as the CEO of a layer one blockchain trying to implement cryptographic schemes and Pratyush on the call, I think he's still on the know. Can probably speak to this specifically the engineering of a thing versus the theory of a thing. And there's just a whole nother dimension of complexity that gets introduced. Of course, it feels when you're writing a paper, the goal is to be general and to have it be applied in multiple contexts. But of course, in any specific system there's specific implementation.
01:00:21.042 - 01:00:23.880, Speaker A: So feels like a bit of an.
01:00:24.890 - 01:00:39.898, Speaker D: There's also the fact that the cycles of curve were not the focus of most of these papers, it was a focus of the original halo paper. But even that didn't include many of.
01:00:39.904 - 01:00:49.802, Speaker A: The details needed, which I guess we should maybe explicitly recognize as I think being I think at least the beginning of many of these ideas around recursion and accumulation.
01:00:49.866 - 01:00:52.750, Speaker C: So I think that was 100% important.
01:00:52.820 - 01:00:54.400, Speaker A: Moment in the field.
01:00:55.990 - 01:00:57.650, Speaker C: Well, and the naming scheme.
01:01:01.670 - 01:01:41.390, Speaker A: But yeah, no, I think there's also opportunities, of course, when things get implemented and bugs like this get found, it result in new opportunities for research. So I think it's a great I mean, this is one of the great things about this group in particular is I think we have both people who are practitioners as well as theorists. Last question for me and sorry to hog the mic here, but can you guys speak a little bit about the way you see this being applied in practice, particularly with blockchain based systems like the EVM. So how concretely do you view this paper as being applicable to the EVM or maybe making the EVM ZKE EVM is more practical.
01:01:41.890 - 01:03:01.234, Speaker B: Yeah, so I think it's kind of like starting points. But I think it really helps for the Eck EVM applications because as we can see in a lot of ckem applications, they are really heavily used lookup gates first because there are a lot of drain checks and they are simulating arithmetics over like two to 64 or 232 and that's the first thing. And they also heavily use high degree gates and also they do have the problem that they need to support multiple op codes instantaneously. Right. And I think our scheme is really applicable to those area and can make the Zkv application much more efficient. And on the other hand, I think that's not the end of story because for example, I heard from the engineer who implements Zkevm they say one of the really bottleneck is that how to simulate memory? For example, in particular, in our scheme, we still don't have a really good way to simulating memory access beyond just committing this entire memory table inside witness because they are online. So we cannot use the sparse trick for preprocessed table elements in this setting.
01:03:01.234 - 01:03:16.110, Speaker B: So it's still a really interesting open question if we can solve that part, if that part is solved, I think we can come to another stage like well, we have even more efficient application and implementation for Ckvm. And that's my understanding.
01:03:16.450 - 01:03:19.040, Speaker A: So if I can pull out, go ahead.
01:03:19.730 - 01:03:41.478, Speaker D: So my understanding is that the Ckevm, at least the one by the Ethereum Foundation, depends heavily on dynamic tables. The derivative technique, does that support you.
01:03:41.484 - 01:04:16.622, Speaker B: Can support it, but it's not that efficient. Like you can still move this online table as some commitments, but you don't have this trick of sparseness because we need require those to be preprocessed. But this trick is still useful for simulating bit arithmetics, right? Well, you still have some fixed table beforehand. That's also another really heavy part. But I agree for a dynamic table, the complexity is higher if the table size is really large. But still we can support that. Basically.
01:04:16.622 - 01:04:29.460, Speaker B: We can still just instead of sending the W and M, we also send the online table in the first round. That's the only difference. But that means basically you will have more complexity for.
01:04:31.670 - 01:04:32.958, Speaker D: See if I could.
01:04:32.984 - 01:04:51.260, Speaker A: Pull out maybe the last bit of what you said Ben Yi a minute ago. I guess it sounds like an interesting future direction of work here is around memory constrained approvers. Is that right? So, like provers where there's a limit on memory and therefore and trying to figure out how to make it more efficient in that context. Is that right or is that not what you said?
01:04:52.190 - 01:05:11.822, Speaker C: I think it's more the memory within the circuit. Right. You want to prove like the ZKE EVM actually has a memory, right? Like the virtual machine and trying to efficiently prove that the memory accesses reads and writes simulation.
01:05:11.966 - 01:05:13.278, Speaker B: Simulation of the memory.
01:05:13.374 - 01:05:18.110, Speaker A: More efficient representation or simulation of memory in circuit effectively is the direction.
01:05:18.270 - 01:05:33.370, Speaker D: I mean, we could move towards more functional smart contract languages that don't require simulating memory to prove contracts secure.
01:05:34.110 - 01:05:39.366, Speaker C: This is like the yeah, this is their direction.
01:05:39.478 - 01:05:39.994, Speaker A: Exactly.
01:05:40.112 - 01:06:30.886, Speaker C: Yeah. Their direction from, I think, originally started by protocol apps and they're using I think that this was originally built with Nova, but I think everywhere where you use Nova using sort of protestar now would probably give you an improvement. And the main benefit is the lookups and the high degree gates. And the trade off, especially for high degree gates is better than in any other scheme that we've seen before. In Hyperplanc, it's already better than in Plonk, you get better trade offs. But for proto Star you have really this is again, like theory in practice. We have to see this in practice.
01:06:30.886 - 01:06:53.380, Speaker C: But imagine you could support degree, like easily 100 and maybe even 1000 gates and hopefully those are useful. But I think the additional cost for increasing the degree really does not seem that high. That's interesting.
01:06:54.150 - 01:07:24.090, Speaker D: On the simulating memory thing, it's also worth noting that you can write in an imperative language and compile it to. Single assignment form and what you end up with is not very different from a functional program because there was a compiler for a language called Clean which essentially did that and got faster than C on a large range of programs.
01:07:28.260 - 01:07:36.290, Speaker A: Awesome. Well, thank you all very much. This has been a great discussion. Maybe I'll open it up to the floor in case anyone else has any final questions.
01:07:39.440 - 01:07:40.190, Speaker C: Thank.
01:07:40.520 - 01:07:49.828, Speaker A: Okay, well, thank you, Benj and Benedict, very much for being here and for presenting this work. It's really exciting and looking forward to seeing how things progress from here.
01:07:49.994 - 01:07:54.470, Speaker B: Thank you. Thank you, Alex. Thanks for inviting. Yeah, we are also really happy. Thanks for having us.
01:07:55.320 - 01:07:58.320, Speaker D: Thank you. Very enthusiastic about protestar.
01:07:58.400 - 01:07:59.088, Speaker B: Thank you. Bye.
