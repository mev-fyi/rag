00:00:12.050 - 00:00:12.518, Speaker A: Hey, everyone.
00:00:12.604 - 00:00:45.380, Speaker B: Welcome to another episode of the ZK Whiteboard series. Today I'm here with my friend Coworker, applied cryptographer extraordinaire William Borgio. William studied math at EPFL and then has been working on applied cryptography and zero knowledge proof systems for a while. He is, in my opinion, one of the top applied cryptography engineers in the world. So really happy to have William here to talk about Plonke Two.
00:00:46.230 - 00:01:22.830, Speaker A: Hi. Thanks. Glad to be here. So, yeah, going to talk about Planky Two, which is a proving system we've developed at polygon zero. So what is Plank two? So Plank Two is approving system based on plank plus custom gates, which is sometimes called turboplunk. So that's the IOP, and we then need a paranormal commitment scheme, and for that, we use Fry.
00:01:23.170 - 00:01:36.458, Speaker B: So the IOP is like the part of the proving system that basically verifies, that allows us to argue that some proof is valid. And then the polynomial commitment scheme is what encodes that.
00:01:36.564 - 00:01:37.220, Speaker A: Exactly.
00:01:38.470 - 00:01:43.074, Speaker B: Because we're like checking relations about polynomials, and then polynomial commitment scheme allows us to do that.
00:01:43.112 - 00:02:07.500, Speaker A: Succinctly. Yeah, exactly. So this is the information theory of writing a program inside an arithmetic circuit, for example. And this is the crypto how to make it succinct and verifiable. And we built Planky Two with one main goal in mind. It's for fast recursion. So we wanted to build the fastest recursive proof system.
00:02:11.570 - 00:02:16.526, Speaker B: But not just fast recursion, also very fast proofs in general.
00:02:16.628 - 00:02:25.394, Speaker A: Yeah, no, exactly. We wanted to build the fastest prover that was also able to do recursion. Cool.
00:02:25.592 - 00:02:30.946, Speaker B: So what are recursive proofs? When you talk about recursion, what does that mean?
00:02:31.048 - 00:03:21.410, Speaker A: Yeah, good question. So recursion is a technique where you verify a proof inside another proof. So in a proving system, you have two algorithms, the prover P and the verifier V. And this proving system outputs approve Pi, and then you want to produce another proof that Pi is valid. The easiest way to do so is just to write V in circuit. So your proving system is based on Arithmetization, like R one CS or like Planck. And if this Arithmetization is powerful enough, you can write any kind of program in a circuit.
00:03:21.410 - 00:03:42.646, Speaker A: V is an example of a program. So you can write V inside the circuit. And since V accepts if and only if the proof Pi is valid with high probability, if the proof system is sound, then just writing V in a circuit should work for recursion.
00:03:42.838 - 00:03:55.034, Speaker B: Yeah, because the Verifier is some computation. We know that we can express that computation in a circuit, and so we can have a proof that it's valid if and only if the proof that it's verifying is valid.
00:03:55.082 - 00:04:14.600, Speaker A: Exactly. Yeah, you could also that would be a very bad idea. You could also write P inside the circuit. But yeah, that's why zero knowledge proofs are so powerful, is that you can verify them. Succinctly and then also write them succinctly in the circuit. Okay, cool.
00:04:15.850 - 00:04:24.498, Speaker B: So what are the current approaches to recursion? How are people approaching this problem right now? So pre plunky.
00:04:24.514 - 00:04:53.166, Speaker A: Two so pre plunky. Two so pre planky, two even pre plunk. Let's say we had proof system like proof systems like Graph 16 that used Sparing friendly elliptic curves. Okay, so Graph 16 uses an elliptic curve E, which has Base Field FQ, and it's pairing friendly.
00:04:53.358 - 00:05:00.274, Speaker B: That just means it has some additional structure that gives this nice kind of map that helps for exactly.
00:05:00.392 - 00:05:36.800, Speaker A: Yeah. And then the proving system uses this elliptic curve E over FQ. And there's another field that you can define on this elliptic curve. It's FP. So, without going too much into details, p divides the order of the elliptic curve. So the elliptic curve is an abelian group, which has an order, and you take P, a prime factor of this order, and so you can work on FP, which we call the Scalar Field.
00:05:39.010 - 00:05:51.122, Speaker B: So we have the base field and the scalar field. The Base Field is where the elliptic curve points are. And then the Scalar Field has order equal to the order of the group itself.
00:05:51.176 - 00:06:21.950, Speaker A: Exactly. And then the magic of it is that you have an action of FP on E, which takes a scalar x and the point P and where you get the so called scalar multiplication x times P, which belongs to the curve. And so this operation of scalar multiplication is fully FQ operations, even if x is in FP.
00:06:23.250 - 00:06:27.098, Speaker B: So what does this mean for the verifier?
00:06:27.274 - 00:07:08.486, Speaker A: Right, so in a system like Graph 16 or really any proof system using elliptic curves, the verifier v will have to do operation in both FP and FQ. So, yeah, I should mention that the proving system itself is over FP. Okay? So if it's either Planck or R one CS, like, the actual arithmetic circuit that you will build will be over FP.
00:07:08.598 - 00:07:15.802, Speaker B: So, what, I'm writing my programs then, so to speak, or writing my statements, then that I'm trying to prove I'll be using values.
00:07:15.866 - 00:07:21.870, Speaker A: Yeah, exactly. Your scalar values will all be like elements of this scalar field, FP. All right.
00:07:21.940 - 00:07:35.202, Speaker B: But this sounds fine, right? Because on a CPU, I have my really fast MacBook air, and I know that my CPU can do operations in FP and FQ pretty efficiently. So what's the issue with the recursion?
00:07:35.266 - 00:08:40.358, Speaker A: Yeah, okay, so as Brendan said, there's no issue in general like Graph 16, and all Pring system based on apt curves are totally fine. The issue comes up when you try to do recursion. Okay? So we want to write v, so we want to write the verifier inside a circuit. And so, as I wrote before, we'll have to do operations over both FP and FQ. And so the issue for recursion is that if we stay on the same curve, then FP. Those are very cheap because this is the native field of the proving system. But FQ ops are very costly just because the proving system itself is not over FQ.
00:08:40.358 - 00:08:58.560, Speaker A: So we have to do something called non native arithmetic. So there's been some improvements over the year but it's still very costly to do. And so you end up with a verifier circuit that's very large.
00:08:59.090 - 00:09:08.626, Speaker B: It's costly because I can't just express my FQ values in FP because if I do a bunch of FQ multiplications it will overflow at a different point.
00:09:08.728 - 00:09:09.380, Speaker A: Exactly.
00:09:10.150 - 00:09:17.558, Speaker B: So I have to express FQ values. Like I break it up into some number of limbs and then do range checks to check.
00:09:17.644 - 00:09:18.280, Speaker A: Exactly.
00:09:19.370 - 00:09:31.130, Speaker B: Sounds expensive. It is, but there's a solution right, that existing proving systems use, right? There are some maybe not totally optimal approaches.
00:09:31.710 - 00:10:30.974, Speaker A: Exactly. So there's something called two cycles of elliptic curves. So in a two cycle you have your original curve E and second curve E prime. EPriME is defined with base field FP. So the scalar field of E and has a scalar field FQ, which is the base field of E. So you basically switch the base and scalar field compared to E. And what do you get by changing curves? By using E prime, the proof pi is over E, but now if you write your Verifier V over E prime, you write Verifier in the circuit over E prime, then the scalar field of E prime is FQ, which is the base field of E.
00:10:30.974 - 00:11:12.440, Speaker A: So like all the FQ ops will be very cheap. Okay? So now FQ is cheap and then one issue that comes up immediately is that FP operation become expensive. So like FP operation will have to be done non natively. So FP now becomes costly but in general the Verifier does a lot more operations over FQ. For example, pairings are like very expensive FQ operations and so you gain something by changing curves because now all the FQ operation can be done basically for free.
00:11:13.070 - 00:11:22.442, Speaker B: Cool. So the most expensive part of verifying the proof is now pretty cheap. That seems fine, what's the catch?
00:11:22.506 - 00:12:12.730, Speaker A: Okay, so it is fine. The biggest catch is if you want to use a proving system using pairings such as graph 16 or any proving system using KZG, the KZG perennial commitment scheme, then we don't know any such cycles. So E and E prime, both pairing friendly. So we want something like that if we want to have like graph 16 on both curves. But the issue is that we don't know any such cycle with reasonable size. Okay, so I think the size of the best known cycle is between seven and 800 bits.
00:12:14.750 - 00:12:22.586, Speaker B: Which is huge when you consider like your CPU is doing 64 bit operations and so you're doing like a ton of field arithmetic.
00:12:22.698 - 00:12:37.090, Speaker A: Exactly. In that field your operations become way too slow and actually I think you're much better just using something like graph 16 over a way smaller curves and then do all the FQ operation like non natively.
00:12:37.850 - 00:12:40.886, Speaker B: That's an approach that's taken by Aztec, I think.
00:12:40.988 - 00:13:29.270, Speaker A: Exactly, yeah. So if you want to look those up, those are like the MNT curves. Okay. And so that's the big issue with cyclones elliptic curve. If you want to use pouring systems using pairings, if you want to use a pouring system that doesn't use pairings, but that's also based on elliptic curves, there's something called halo. Okay? So one other perennial commitment scheme that works with elliptic curves, and it's based on the discrete logarithm problem is the IPA based PCs.
00:13:29.770 - 00:13:31.640, Speaker B: So what does IPA stand for?
00:13:33.290 - 00:14:19.240, Speaker A: Inner product argument. Okay, that's also known as bullet or butle proofs. So that's also a paranormal commitment scheme using elliptic curves. It's based on the discrete log, as I said. So Dlog as compared to graph 16 or KZG that are based on pairing related security assumptions. And so the discrete log means the discrete log problem means that you can use curves of size 256 bit that will give you 128 bits of security.
00:14:20.490 - 00:14:23.206, Speaker B: So that's much cheaper than our 700 bit.
00:14:23.308 - 00:15:17.370, Speaker A: Exactly. And much cheaper than the 400 bits you need if you actually want security with pairing friendly curves. So one very big issue with IPA based perennial commitment scheme is that the verification is linear. So linear in the size of your circuit, which sounds terrible for recursion, because as I said before, recursion, you want to write the verifier in a circuit. If the verifier is linear, you will end up with a very large circuit, which is bad for recursion. But this really nice technique called halo gives a way to make the verification logarithmic by using something called accumulation. So it's an accumulation scheme.
00:15:17.370 - 00:15:32.160, Speaker A: Okay? And accumulation means instead of verifying the whole proof in recursion, they defer the verification until the end of the process.
00:15:34.530 - 00:15:57.282, Speaker B: That means I'm trying to do trying to generate a recursive proof. And so I'll verify part of the proof in the circuit and then basically say, here's this other part that you have to check later. And you have this strong guarantee that this proof that I'm checking here is valid if and only if the deferred part check sort of passes.
00:15:57.346 - 00:16:19.274, Speaker A: Exactly. So we would have just like a graph like that. That would be something called IVC incrementally verifiable computations. Okay, let's add two more. Okay, so each dot is a proof, and each arrow is like recursion. So this proof is a proof of this. This is a proof of this.
00:16:19.274 - 00:16:59.018, Speaker A: This is a proof of this. So instead of being a full proof, this is like an accumulation step. This is also an accumulation step. And here, if you're done with what you're trying to do with recursion here, you actually verify. Okay? And so this is logarithmic, this is logarithmic two, because it's the accumulation step, and this will be linear. Okay? And so what you gain by that is that every time you want to do recursion, you only pay a logarithmic cost, and then at the end, you have a linear cost, but usually you don't really worry about that because you can verify it natively. You don't really need to do recursion on this one.
00:16:59.104 - 00:17:02.940, Speaker B: Yeah, so this, like, all the checks are on the CPU. Exactly.
00:17:03.310 - 00:17:44.200, Speaker A: Yeah. You don't need to recurse on this. Cool. All right, so Halo uses elliptic curves, so you also need a two cycle because you want to do all the I mean, as I explained before, you want to do all the FQ operations natively. So actually you will have two chains. Okay, I messed it up. Because actually what happens is that this will be like E prime, this will be E, and you will verify it like this.
00:17:44.200 - 00:17:53.020, Speaker A: Something like that. Okay, so, like, you switch curve every time you verify. All right.
00:17:54.990 - 00:18:02.246, Speaker B: So this is actually what Plonky One was based on was Halo, Plonk and Executive.
00:18:02.278 - 00:18:32.440, Speaker A: Exactly. So before plonky two, there was just plonky. It used halo. So, like, the accumulation scheme defined in Halo, also the IPA based quad demo commitment scheme and the Arithmetization was very similar to what we have now in Plunky Two. So, like, also Turboplunk, and I think it's also so Halo is also used by the ECC, by Mina, and by other teams. So it's a really nice system, but we actually found a way to do recursion more efficiently with Blocky Two.
00:18:32.890 - 00:18:40.534, Speaker B: Yeah. So maybe you could talk a little bit about some of the frustrations and complexities around this approach.
00:18:40.582 - 00:19:21.110, Speaker A: To the first bad thing that I mentioned already is that the final verification is linear. So let's say you want to verify it on Ethereum. That would be an issue. Like it will be quite costly to verify on Ethereum or any other L one. Another frustration is just accumulation is a really nice idea in theory. In practice, it's quite annoying to always have to keep the accumulators in your system. And then in the halo paper and the usual halo implementation, they also defer some non native arithmetic.
00:19:21.110 - 00:19:50.718, Speaker A: So even with the accumulation step, there is still some non native arithmetic to do. And they found a technique to defer this non native arithmetic. But that's one more thing you have to keep track of. And in the implementation, you end up with something very complex you've never fully recursed. Like you've never fully proved all the proofs until the end. Right? Because until the end, you only accumulate. Right.
00:19:50.718 - 00:19:56.340, Speaker A: So, like, this proof is like, this one is basically useless without going all the way up.
00:19:57.750 - 00:19:59.634, Speaker B: All right, so planky two.
00:19:59.752 - 00:20:50.500, Speaker A: So Planky Two, our solution to recursion and to the problems in halo was to just not use elliptic curves. So instead of elliptic curves, we use something called Fry. So Fry is something called low degree test that was built by stalker and that's usually used in stocks, but you can actually modify Fry to get a PCs per enable commitment scheme. That's what we need for Plonk. And Fry has a lot of benefits compared to elliptic curve based PCs. First one is that you don't have elliptic curves, so non native stuff. Okay.
00:20:51.030 - 00:20:56.100, Speaker B: So we're just doing everything in FP. Exactly. So the proofs are in.
00:20:56.710 - 00:21:42.450, Speaker A: Like, there's only one field, so you couldn't even think about doing anything in another field. Fry only uses one field, and we do all the arithmetic in this field. Okay, so as I explained before, this was the main pain points with Recursion on Ecbase systems. So not having to deal with non native arithmetic is a real benefit. Another one that's very good for performance is we can use any field and in particular small fields. So with elliptic curve's security assumptions, you need large fields of at least 256 bits, even larger for pairing friendly curves. So with Fry, you don't have any elliptic curves, so you can use any field that you want.
00:21:42.450 - 00:21:46.238, Speaker A: And small fields are in general faster than big fields.
00:21:46.414 - 00:21:56.118, Speaker B: That's just because our CPUs are doing arithmetic natively in 64 bits. So if we want to get really fast, we sort of have to pay attention to what's really fast and hardware.
00:21:56.214 - 00:22:37.270, Speaker A: Exactly. Yeah. And lastly, Fry doesn't have trusted setup and is post quantum secure. Okay, so no trusted setup, that was already the case with Halo, but it's not the case with Graph 16 or KZG. So that's always good to have. And post quantum secure. Any probing system based on elliptic curves or at least a discrete logarithm is not post quantum secure, but Fry is.
00:22:37.270 - 00:22:40.054, Speaker A: So that's a benefit. Like if you care about this stuff.
00:22:40.172 - 00:22:41.654, Speaker B: In 50 years when you get a big one.
00:22:41.692 - 00:23:13.966, Speaker A: Exactly. In 50 years, Plunkey Two will still be there. All right, so that's why we use Fry. Then. As I mentioned at the beginning, we use Turboplunk, which is basically the main arithmetization that new proving systems use because it's so powerful. So yeah, Plunk plus custom gate, very powerful arithmetization, much more powerful than R one CS. And so since we want to do recursion, we want to write the Verifier in a circuit.
00:23:13.966 - 00:24:15.880, Speaker A: So we need like an automatization that makes it easy to write complex operations like a Fry Verifier, and that's basically it. Okay, so let me talk in more details about this small field. So we have a lot of choice. We can choose any field, and so we can take any prime and look at FP. And so how do you find P? What's the best p? Well, we found that there is this prime found by our colleague Hamish that he named the Goldilocks Field or Goldilocks Prime. Let's say goldilocks field with FP. Okay, this prime is really good.
00:24:15.880 - 00:24:20.280, Speaker A: First benefit of this prime is that it fits in 64 bit.
00:24:22.330 - 00:24:25.862, Speaker B: So one element can fit, in a word, exactly.
00:24:25.996 - 00:25:14.482, Speaker A: So that's pretty easy to see. The prime p itself fits in 64 bits. So like, any element smaller than the prime fits also in 64 bits. So that's one of the main reason it's so fast. 64 bit arithmetic is done natively on CPUs, so that makes computations in this field very fast out of the box. And then the special form of these primes, it's not any random prime if you write it in binary, for example, like it has some nice structure and the structure makes it easy to do, like, for example, reductions. So reductions of 128 bit integer mod p is quite cheap.
00:25:14.482 - 00:26:01.030, Speaker A: Okay, why do we need to do 128 bit integers mod p? Well, if we have x times y with x and y 64 bit, then we get 128 bit integer. So this means that multiplication modulo p is fast because you multiply them to 128 bit integer and then you do reduction modulo p and then there's some cool features. There's like a ton of cool features. One that I like is that a mull add of 32 bit integers doesn't overflow.
00:26:02.330 - 00:26:27.754, Speaker B: So these two are more for speed on the hardware side. So like making operations for proving really fast. This is more for optimization on the constraint side in our programs or our statements that we're trying to prove we'll do a lot of 32 bit arithmetic and so this just allows us to express that with fewer constraints.
00:26:27.802 - 00:26:53.080, Speaker A: Yeah, exactly. So, yeah, if mole ad is x times y plus z, if x, y and z are 32 bit integers, then you can compute that the result is more than p. All right, so that's the golden ox field. That's one of the main reason pronunky two is so fast. Cool.
00:26:57.790 - 00:27:17.230, Speaker B: So I think we found that there was like a 40 x improvement or something from moving from a 256 bit field to go deluxe. I could be misremembered. There's a really substantial so who tried.
00:27:17.300 - 00:27:18.560, Speaker A: With a large field?
00:27:19.010 - 00:27:20.506, Speaker B: I think Daniel.
00:27:20.698 - 00:27:21.440, Speaker A: Okay.
00:27:23.490 - 00:27:30.302, Speaker B: Because no, just in terms of field or measuring performance of field operations.
00:27:30.366 - 00:27:32.850, Speaker A: Oh, I see. Yeah, that makes sense.
00:27:32.920 - 00:27:35.266, Speaker B: Yeah. So not a perfect comparison, but you.
00:27:35.288 - 00:27:36.420, Speaker A: Can sort of.
00:27:40.010 - 00:27:55.942, Speaker B: Cool. And I guess another nice property is that if you want to do FPGA or GPU implementations of Plocky two, it's much easier with the goldilocks field because I guess it takes up less space on the chip.
00:27:56.006 - 00:28:20.510, Speaker A: Yeah. Should we continue? Yeah. Okay. So let me talk a bit more about the Arithmetization. So there's a lot of flavors of turboplunk. You can really choose to do it how you want. There's like a basic structure, but then you can be creative on how you do custom gates.
00:28:20.510 - 00:29:09.970, Speaker A: So we do it this way. So first we have constants okay, let me start from the beginning. So we're going to look at a row of the trace. Okay. So in Planck you have a trace which is basically a table or a matrix, and your constraint works your gate works on a row of the table. So how does a row look in plunky two, it starts with a bunch of constants, and then we have two types of wire, x zero to XV, let's say, and then y zero yw. Okay, so those are constants.
00:29:09.970 - 00:29:48.746, Speaker A: What does it mean to be constants? It means that they are part of the secret description, it's not part of the witness. These constants are committed to in the verifier key. So the verifier has access to this information. It's part of the secret description. And then you can choose how many constraints you want. And then these constraints are used in the gate to define the constraints. Those are what we call routable wires, and those are advice wires.
00:29:48.746 - 00:30:07.250, Speaker A: Okay, so this is actually the witness part of the table, and we've split it in two. Okay, so in traditional plank, all the wires are routable. So you can have copy constraints between any two wires in the table.
00:30:09.290 - 00:30:13.910, Speaker B: Basically enforcing that gates. Like the gates sort of share the same value.
00:30:13.980 - 00:30:14.758, Speaker A: Yeah, exactly.
00:30:14.924 - 00:30:17.030, Speaker B: The wires to different gates.
00:30:17.110 - 00:30:56.338, Speaker A: If we have, like, table, we can say, like, okay, this one is the same as this one. We add a copy constraint, but this is not free. In the proving system in Plunk, there's something called the permutation argument to enforce these copy constraints. And this is not for free. You have to pay a cost for that. And we realized that we don't need to be able to have copy constraints between any two wires. We can just restrict the number of wires that can be used in this permutation argument and then the wires that cannot be in the copy constraints.
00:30:56.338 - 00:31:31.086, Speaker A: We call them advice wires, and those are used for intermediate computations. Okay, so let me give you a very simple example. If you want to compute X to the 16 in a gate, the easiest way to do so is maybe like, to compute X to the four and then say that's equal to Y, and then compute Y to the four. Okay? So y you don't need y. That's like an intermediate computation, and so you don't need to route it. And so you would put Y in.
00:31:31.108 - 00:31:38.194, Speaker B: The advice wires, and then you would set some other routable wire to yeah, exactly.
00:31:38.232 - 00:32:27.250, Speaker A: So this is the result that you want. And this would be routable. But the intermediate computation, you don't need it in the perimeter argument. All right, and so how are those constants and wires related? They need to satisfy a polynomial constraint that this is equal to zero. P is a polynomial, and that's the custom part. Right. So when we say we have custom gates that's P, we change P in between every gate.
00:32:27.250 - 00:33:10.990, Speaker A: P could be like defining an arithmetic operation or hash function, something like that. Okay? The cool thing is that you can increase the width. So you can increase or decrease the width of the table to have more wires per row and being able to do more operations and more arithmetic on every row. And you can also increase the degree of P and those two increases will get you more expressiveness.
00:33:12.530 - 00:33:19.502, Speaker B: So this is kind of in contrast to something like R one CS where you have very low degree constraints.
00:33:19.566 - 00:33:49.180, Speaker A: Exactly. So like R one CS is basically just like quadratic type constraints. Here we can be very creative, we can have P be a polynomial, a very high degree paranormal. So of course it's not for free. If you increase the width, you increase the size of the proof. And if you increase the degree, you will also in some sense increase the size of the proof. So it's not for free, but you can play with those two parameters to get the size of the secrets that's better for you.
00:33:49.180 - 00:34:20.520, Speaker A: Cool. Yeah. Let me just give what we use in Plunkey Two as a default for recursion. So we use 135 wires so that's the width of the table and the degree of P, the maximum degree of P is nine. Okay, so that's pretty high degree compared to something like R one CS as we said, which is of degree two. Cool.
00:34:21.450 - 00:34:41.200, Speaker B: And so does using Fry allow us to because that's a way wider table than other plot implementations. Does using Cry allow us to kind of have sort of more degrees of freedom with respect to how we.
00:34:43.890 - 00:35:20.890, Speaker A: Yeah, for sure. So the first thing I can say about that is that usually increasing the width of the table is pretty expensive in EC based systems because you have your table and then you need to commit to each column. And so each column will become a paranormal commitment. So that will be like analytic curve point in IPA based PCs for example, or even KCG. So if you have 135 columns, you will need 135 points as a commitment.
00:35:21.870 - 00:35:23.178, Speaker B: Huge proof size.
00:35:23.344 - 00:36:05.438, Speaker A: That's a huge proof size. Still smaller than Plunky Two. But then also the verification becomes expensive because you have to do something called the MSM in the verification of these type of systems. And so like, this MSM of like 135 curve points will be quite expensive. So in Plunky Two we don't need that because the commitment how do we actually commit to polynomials? We can do that in a batched way. So like we can commit to P, zero to like P and 145. So let's call this PN.
00:36:05.438 - 00:36:19.310, Speaker A: And the way we do that is that we have a Merkle tree whose leaf are the evaluations of all these polynomials at a subgroup element.
00:36:21.010 - 00:36:27.690, Speaker B: And to be clear, these polynomials are interpolated polynomials from each like corresponding to each column.
00:36:27.770 - 00:37:18.560, Speaker A: Yeah, exactly. So that would be like just one root of Merkle. So instead of having like 135 curve points, we just have like 256 bit just like one root of a Merkel tree. So we already gain a lot just by using Fry. That way, of course, it's not for free, because when you open this merkel tree, you will have to open all the columns so we still get an increase in proof size. And so there's two parameters in Fry that we can use to tune the performance of the proving system. Those are called the rate.
00:37:18.560 - 00:37:35.000, Speaker A: Let's call it two to the row. So it's a power of two and the number of queries. Okay. Let's call it Q.
00:37:35.530 - 00:37:41.942, Speaker B: So this is actually my favorite thing about Fry because there's this cool space time trade off, right?
00:37:41.996 - 00:37:42.600, Speaker A: Yeah.
00:37:42.970 - 00:37:44.120, Speaker B: How does that work?
00:37:46.590 - 00:38:21.460, Speaker A: These two parameters are related. Okay? So they're related by the equation lambda, which is your security. The security of the Fry protocol is actually Rho times Q plus a constant. Let's call it G. It's used in grinding. Okay, so I don't think we'll get into that. But basically, you can set this guy to like, I don't know, 16, and then you do some grinding in your protocol, which is basically free.
00:38:21.460 - 00:39:04.370, Speaker A: But so if we ignore this grinding part, then we have basically Rho and Q are inversely proportional. So, like, if you increase one, you have to decrease the other. And so what happens if you actually decrease Rho? For example, like the rate? Decrease the rate. If you decrease the rate, you have to increase the number of queries, increase Q. And what you get by doing that is much faster proofs. So faster proving times. But sorry, larger proofs.
00:39:06.470 - 00:39:12.530, Speaker B: Just because I have to send you if you're the verifier, I have to send you all the data to verify each query exactly.
00:39:12.600 - 00:39:45.440, Speaker A: Yeah, so each query is actually like, you have to open the merkel tree by sending merkel proofs. And so you will have just to send more merkel proofs. Okay. And inversely, if you increase here the rate and decrease the number of queries, you get slower proofs. Okay. But smaller proofs. Okay.
00:39:45.440 - 00:40:16.770, Speaker A: And that's something that's important for planky two, because Fry in general gives you very large proofs, much larger in general than a proving system based on elliptic curves. So having this option of paying the trade off between space and time. So if you have very beefy hardware to computer proof, you can just increase the rate to get smaller proofs.
00:40:16.850 - 00:40:48.018, Speaker B: In the end, to me, this is sort of the beauty of plonky too, right? Because before, we didn't have efficient recursion for Fry. And so you basically had to choose, like, if you were building a ZK roll up or something and posting proofs to Ethereum, you had to choose some point in this spectrum where your proofs couldn't be too big, that they wouldn't be able to fit in a block. But they couldn't also be too slow or else you'd just get killed on proving time.
00:40:48.104 - 00:40:48.594, Speaker A: Yeah.
00:40:48.712 - 00:41:13.020, Speaker B: So with plunky two for the expensive part, we can make that really fast because we don't really care about. Proof size at that point and then we can recurse and we end up maybe you're going to talk about this, but we end up with a really small circuit that's sort of our recursion threshold and it's cheaper to make that a small.
00:41:14.590 - 00:41:42.610, Speaker A: Exactly. So like the way I presented it, you could think that you have to choose between one or the other. But actually using recursion, as Brendan said, you don't have to choose. You can do faster proofs when you don't care about the proving size. And then once you care, for example, when you want to push to L one, you just increase the rate, use recursion to compress the original proofs and then you get like a smaller proof.
00:41:46.730 - 00:41:49.542, Speaker B: So good.
00:41:49.596 - 00:42:22.980, Speaker A: Yeah, good. Okay, so let me give you an example of a custom gate. In planky two, I'm going to take one of the simplest gate. It's the one we call arithmetic base gate. Okay? So there is also an arithmetic gate, but it works in the extension field. So I will not get into that. But like in Plunkey Two, we sometimes work in an extension field of the goldilocks field.
00:42:22.980 - 00:43:19.570, Speaker A: So this one is for base field operations, the operations on the goldilocks field, okay? And the goal of this gate is to do computations like x times y plus z equals W. And we can actually use some constants here. So like c zero times x times y plus c one times z equals W. Okay? That's the kind of operation we want to do often. So why do we need these constants? Well, it makes things easy. For example, if we just want to do like x times y equals W, we just set C zero to one and C one to zero. So you can see how having these constants is helpful to do basic arithmetic operations.
00:43:19.570 - 00:43:52.654, Speaker A: Okay? So if we go back to our custom gate design, we have the constants. Those are like easy, it's C zero and C one. Okay? Then we have routed wires. We will have X-Y-Z-N-W. Okay. So they all need to be routed. There is no intermediate computation here.
00:43:52.654 - 00:44:36.410, Speaker A: It's like a degree two constraint. Okay? So the constraint is very simple. It's W minus c one, x y minus c zero, XY minus c one z. Okay? So that's a degree two polynomial. We don't need any intermediate operations. So all the wires are routable. Okay? And now what's cool is that we have a wide table and here we only use like four wires, so it's a bit wasteful.
00:44:36.410 - 00:45:23.526, Speaker A: So what we've implemented in Plunky Two is that we actually stack operations of this kind in a single row. Okay? So here I will have the first operation. And then what happens is that I can also stack on the side another one of these operations. Okay? And then I continue. So like here I have four wires. The number of routed wires that we use by default in Plunky Two is 80. So I can actually put like 20 such operations in just one row, which is very good compared to something like original plunk.
00:45:23.526 - 00:46:00.760, Speaker A: So in original plank, there wasn't any custom gate. There was like just one gate which looked a lot like this one. And so in original plank, you can do like one operation per row. And we can do 80, we can do 20. So it's basically a 20 times improvement in the length of the trace. It's not for free, but since we need such a white table for poseidon for the hash functions we use, we can use that also for arithmetic operations. So, like arithmetic operations are basically free in Plunky Two.
00:46:00.760 - 00:46:38.130, Speaker A: Very cool. So another optimization, other optimizations we use to make this verifier circuit smaller is on Merkel trees. Okay? Fry basically commits to paranormals using Merkle trees. And then you do a lot of Merkel proof verification. So we need to make that fast. To improve Plunk Two, we use a bunch of optimizations for that. First one is one we call Merkel Caps.
00:46:38.130 - 00:47:29.790, Speaker A: Okay? So Merkel Caps is a version of Merkel roots. In the usual Merkel tree, you have the leaves, let's just do four leaves. And then you have the intermediate layer, and then you have the root of the tree. Okay? And so you get only one root, but you have to go up two times in the miracle proof, basically. So like a Merkel proof for this guy would be like a Merkel proof of this guy will be like this one. And then you go up and then this one. Okay, so your Merkel proof has two elements.
00:47:29.790 - 00:48:10.538, Speaker A: What you can do is instead of giving the root, you can just give this as a root. So this is the cap, okay? And this is the root. So the Merkel cap will be larger than the Merkel root. It will have two elements instead of one. The cap doesn't have to be at the first layer. It could be like at any layer below the root. And then if you want to send a Merkel proof, let's say again for this leaf, you just send this one.
00:48:10.538 - 00:48:41.538, Speaker A: Okay? You just send this element and then let's give them name. And then you have like a hash of X and Y. You verify that this is equal to this guy. And so you need to do some arithmetic to know which one of these or this you need to verify in. But that's quite easy to do. And what we gain by doing that is also like kind of a spacetime trade off. We save on the size of the Merkel proofs.
00:48:41.538 - 00:49:13.658, Speaker A: And then this verification here is actually pretty simple. It's just like a random access in this list. So the cap is a list of hashes, and we just do a random access on these hashes to verify a Merkel proof. Cool. Okay. Now let me talk about something called Starkey. Starkey.
00:49:13.658 - 00:50:16.530, Speaker A: So you will find Starkey on our GitHub alongside plunky two. So star key is the Stark equivalent of planky two. So plank two is based on Planck, but you can realize that Starks, or more precisely, the error Arithmetization, is kind of a subset of Planck. Okay? So Planck is basically error plus copycast rates and the permutation argument and custom gates, and sometimes you don't need either of those. So sometimes you don't need, like, Kapka switch and you don't need custom gates. One such example is for a VM. Okay? So in a VM, you have the state of your VM in a row.
00:50:16.530 - 00:50:35.770, Speaker A: That's the state. That's like a row of your trace, and then you get the next state, which is below it. And what you want to verify is that the state transition is done correctly, the VM state transition.
00:50:36.190 - 00:50:53.326, Speaker B: So just to unpack this a little bit more, because I think it's a cool insight if you think about what Air is, it's basically like the same constraints repeated over and over again, and each row has access to the adjacent row, right?
00:50:53.348 - 00:50:53.870, Speaker A: Yeah, exactly.
00:50:53.940 - 00:51:13.190, Speaker B: And so you're basically saying that plonic is basically like a bunch of errors wired together where you can have gates that impose different constraints and you can have wires that connect them in arbitrary ways as opposed to just like, values and adjacent rows.
00:51:13.610 - 00:52:00.834, Speaker A: Yeah, exactly. So actually, in some versions of Plank, you will not have access to the next state in the constraint because you don't really need to since you have copy constraints. But in Turboplunk, for example, some people implementing elliptic curve operations in Turboplunk do actually include the next state in the constraint to do incremental computations in plunk. So plunk is very like, you can customize it as much as you want. You can include the next state or not. But yeah, if you include the next state, then error is like a subset of plunk. And so to go back to our VM example, so I have the state, I have the next state, and then the only constraint I need is that the transition is done correctly.
00:52:00.834 - 00:52:19.434, Speaker A: Okay? So this is that. That's like the transition. And then I will write a bunch of constraints to encode the transition, the state transition, and then you do the transition again and again and again. It never changes.
00:52:19.552 - 00:52:24.958, Speaker B: Okay, just like your CPU, it's like the same each cycle does something.
00:52:25.124 - 00:52:59.554, Speaker A: Yeah, exactly. So you don't need custom gates because you can just do the same transition on each row. Okay, so that's the reason why the best way to write VMs in a ZK circuit is not with plank, but actually with Starks. Okay? And so that's the reason why we built Starky. Okay, so we wanted to write code for a VM. We realized that plunky Two was a bit overkill for that. Plunky Two is built for recursion, so you need to wire, and you need, like, custom gates.
00:52:59.554 - 00:53:38.326, Speaker A: We use extensively custom gates. But Plunky Two was overkill for the VM, and so we built Starkey, which is basically a Plunky Two, removing all the Plunk specific stuff. And it uses the same framework, so, like the same field, the same hash functions and so on. And it's usually much faster than Plunky Two. One of the reason is these transition constraints. So in Plunky Two, you have some restrictions on the degree of the constraints. You cannot go too low most of the time.
00:53:38.326 - 00:54:37.990, Speaker A: You have to at least have degree four constraints. Okay? But what you can do in degree five, I should say, in Starks, is that you can have degree free constraints of degree free, okay? And what you get by having small degree constraint is that you can get a rate of two. So as I explained before, having a small rate gives you fast proofs. And so that's something we use extensively in Starkey. So rate of two is basically the smallest rate you can have. And so we use the smallest rate possible to have the fastest proof possible, which is something we actually need because VM computations are quite expensive if you have a lot of cycles. So you actually really need to have a fast prover for VM proofs.
00:54:38.730 - 00:54:39.480, Speaker B: Cool.
00:54:45.650 - 00:55:22.722, Speaker A: All right, so maybe as a last thing about Plunky Two is, like, how do we combine all of this together? Like Star key, Plunky Two, and in the ZK roll up context, okay? So in a ZK roll up, you have transactions. You have a bunch of transactions, and you have these transactions, and you want to prove the valid execution of all the transactions.
00:55:22.866 - 00:55:25.174, Speaker B: These would be like Ethereum transactions, Ethereum.
00:55:25.222 - 00:55:58.290, Speaker A: Transactions, any kind of VM. But for us, it would be like EVM transactions. And so you want to prove valid execution of these transactions. The first thing you could try is just batch all these transactions together and have a big proof that all these transactions are valid, okay? So in practice, you wouldn't have just four transactions. You will have maybe, I don't know, 100 or more. And so you'll end up with a really large circuit to do that. That wouldn't be practical.
00:55:58.290 - 00:56:22.890, Speaker A: So that's where you actually use recursion, okay? So instead of proving all the transactions together, you batch them. Let's batch them. So actually, first thing you would do is actually prove them before batching them. So we actually prove them using Star key. Using Star key. Exactly. So, like, these are VM proofs.
00:56:22.890 - 00:57:09.580, Speaker A: As I said before, VM proofs are very fast using Starks. So here we use Star key and a low rate, okay? So to prove the EVM transactions, for example, you will need a lot of cycle and a lot of compute power. So you want to make it as fast as possible. And so you use a low rate, okay? So you have your star key proofs and then what you do is you recursively, verify them to a new proof. Okay? So this is using recursion, and this is using Plunkey Two, okay? Plonky Two is built for recursion. It can recursively verify a star key proof. So here we use star key planky Two sorry.
00:57:09.580 - 00:58:04.366, Speaker A: To aggregate two proofs. Okay? And now we're compressing the number of proofs. Final step, we aggregate these two proofs, okay? So this is using Plonkey Two and also low rate. Okay? We also want these proofs to be very fast because, as I said, let's say we have 256 base transactions. Then here we would have, like, 128 proof to generate, okay? So we still want it to be fast so that we can parallelize it. And then this last proof, we will also use Plunky Two for recursion. And then we use a high rate.
00:58:04.366 - 00:58:55.386, Speaker A: Okay, why do we use a high rate? Well, this proof will be pushed to L One. Okay? So here we have to be careful about the cost of verification, like the gas cost if we push to ethereum, okay? So we want a high rate to make the proof smaller and cheaper to verify. And here, at this step, we only have one proof to generate. So we can just have a very powerful machine to generate it. We don't need to parallelize it. And another cool optimization is that this proof, we will not do recursion on it because we'll push it to L One, and it will be like, natively verified on L One. And so we can actually use Ketchak as a hash function instead of Poseidon.
00:58:55.386 - 00:59:19.800, Speaker A: So the reason we use Poseidon is for recursion, but actually Poseidon is much slower than something like Ketchup. So we can use Ketchack to make proof generation faster and also to make proof verification cheaper on L One, since you have, like, native ketchup UpCodes on Ethereum, for example. Cool.
00:59:21.450 - 00:59:26.760, Speaker B: I guess you could wrap that in KZG if you really wanted to go crazy with exactly.
00:59:27.210 - 00:59:49.040, Speaker A: Yeah. So if the verification is still too expensive on Ethereum, it costs too much gas, for example, and you realize that verifying Graph 16 or Plank and KZG proof is cheaper, you can always recurse on this one using Graph 16, let's say. Cool.
00:59:49.970 - 01:00:42.766, Speaker B: And that's plunky, too. I guess that's plunky, too yeah, I think it's cool because we didn't invent a new PCs or, like, an IOP, but I was personally sort of skeptical when planky Two was being discussed because at that time, I think it took like ten minutes to the fractal was an implementation of recursive Fry, and the proving time was crazy. But I think the core insight in Plunky Two is, like, there are so many ways that you can optimize proving systems, especially with Fry, because it just opens up small fields and different hash functions. I think it's really an amazing achievement in applied cryptography engineering, which I think is really cool.
01:00:42.868 - 01:00:59.190, Speaker A: Yeah. The beauty of punky Two is how customizable it is, and I guess you can see it in this last diagram. We have all the steps in recursion, use different parameters to optimize for what you want in the given layer.
01:01:01.530 - 01:01:33.230, Speaker B: We've seen really insane results for using a super low rate for Starkey, right? Like, 150 Ketchack hashes per second on, like, a MacBook air, which I think it took multiple minutes to do. One ketchack hash. And so so, yeah, I'm obviously really biased. I think that your work really opens up the design space for ZKPs.
01:01:33.890 - 01:01:34.446, Speaker A: Cool.
01:01:34.548 - 01:01:35.386, Speaker B: Congratulations.
01:01:35.498 - 01:01:36.398, Speaker A: Thank you.
01:01:36.564 - 01:01:37.326, Speaker B: Thanks, everyone.
01:01:37.428 - 01:01:37.820, Speaker A: Thank you.
