00:00:07.130 - 00:00:27.000, Speaker A: So next up we have thank you, Tarun Chitra from Gauntlet, presenting ZK mechanism design how privacy can change user guarantees. So welcome, Tarun. Lovely clicker. Oh yeah.
00:00:29.290 - 00:01:10.690, Speaker B: Great. Yeah. First off, sorry for the wrong date. I could not edit my slides last minute, but I think everyone so far has talked more about proving algorithms, trying to focus on optimizing the ability to make things with ZK. I'm going to try to talk a little bit more, maybe somewhat pessimistically and somewhat optimistically, depending on your perspective about applications and limits of those applications. Cool. So I think there's a couple of things that are worth noting.
00:01:10.690 - 00:02:13.014, Speaker B: First off, if we're being very honest, and I mean very blunt, there's only really two applications of ZKPs today that are used in production. The first is effectively state compression, which is roll ups, and the second is private Verifiable compute that's sort of co processors, off chain compute for smart contracts. And this you if you look at Twitter, I don't think the average person who isn't in crypto or in ZK is actually particularly enthralled with those guarantees. This is not to make fun of Andreessen. This is just if you look at this, they tried to make this thing accessible about why people should care about ZK, and instead they got flogged for it. So it's very clear that in some ways we do actually have to also think about applications and applications that are not just roll ups, which is sort of blockchains eating themselves. So I think the key here is to talk about tangible benefits.
00:02:13.014 - 00:03:04.130, Speaker B: So what are tangible benefits? So tangible benefit to a user who's interacting with either ZK system using MPC, using Fhe, I'm using all these interchangeably. So when I say ZK here, oftentimes I mean one of the three, depending on the context. And there sort of need to be certain benefits that the user can get guarantees on. So one, they get better prices, two, they get better UX. And sort of three, they don't have to have a horrible onboarding experience. So I'm going to just talk about three main types of tangible benefits from ZK programs, and then we're going to talk about how mechanism design gives you limitations on what things you can and can't build with that. So the three horsemen, I would say, are provably optimal pricing.
00:03:04.130 - 00:04:03.450, Speaker B: So if you think about why DeFi grew over the last few years, DeFi grew in part because it was easy to price things. You didn't have to think very hard, and a lot of the mechanisms that worked best had the least complexity and it was easy to know whether you got the right price in some sense. The second is what's called auditability, and we'll talk a little bit about the exact definition, but you should think of auditability as being one step further than Verifiability. We're sort of used to Verifiability as, hey, I can verify a state update happened and I don't need to actually replicate the entire state update. But Audibility is the idea of can I pinpoint what actually went wrong when there is an invalid state update? So it's sort of a way of figuring out what caused an error. And the final thing is what I'll call generalized compression, which is adding extra conditions to things like roll ups for giving user guarantees. And I had to add an overheard tweet.
00:04:03.450 - 00:05:22.222, Speaker B: So what's sort of the end game here? There's obviously been this huge, immense R and D effort to kind of get these things to production. And obviously that's been nothing short of heroic. But so far the only use cases people have made are making Zkvms, which is effectively just compression technique in some ways. And so one question you might ask is, and I think a lot of the ZK coprocessing world is kind of trying to ask this, is how can given that we're about to have this huge glut of proving supply over the next 24 to 48 months, how can we actually use that for things other than just compressing roll ups? Okay, so the way we're going to do that, the rest of this talk, we're sort of going to talk about these three properties that are useful and help users that you can guarantee happen with ZK. Then we're going to talk about formalizing that via something that's similar to an ideal functionality in cryptography. So that's what I call commitment collateral games, and then we'll talk about limitations on those. So as I said before, if you think about DeFi's growth, a lot of it came from being very easy to have optimal pricing.
00:05:22.222 - 00:06:10.740, Speaker B: It was easy to figure out what the optimal arbitrage was, it was easy to figure out optimal routing. And now you see this world where people are trying to do off chain aggregation. You see a bunch of different versions of this where it's matching different chains and trying to settle prices, but every user really only wants sort of two things. One is knowing that they're not getting screwed, and second, to not have to work hard to get a good price. So that's sort of optimal pricing doesn't really need much explanation. Men this is a tiny typo of mine. The analogy should actually be Verifiability as to auditability is NP is co NP, but crypto is sort of based on Verifiability, the sort of proofs of state transitions being correct.
00:06:10.740 - 00:07:11.378, Speaker B: But if something is invalid, can I figure out what caused the invalid transition? And this kind of extra step is kind of important, especially when thinking about economic mechanisms. So let's compare sort of a gap, which is the first and second price auctions, where there is a gap in auditability. So just for notational sake, say I have a vector of N elements v one through VN v sub parentheses I is the order statistic. So that's any ordering that guarantees that they're ordered from highest to lowest. So in the first price auction, you get a bunch of bids from bidders, so you have N bids, and the winner pays B one, the first order statistic. Second price auction, you get a vector of N bids. The user who bid B one gets pays b two if there's a cheating auctioneer.
00:07:11.378 - 00:08:00.018, Speaker B: So cheating auctioneer will add fake bids to get the second price auction to pay higher than B sub two, because that's their revenue, right? The auctioneer has incentive to do that. So you might say, okay, let's suppose the users know the auctioneer cheated. How much do they have to communicate with one another to figure out whether the auctioneer cheated? And for the first price auction, you can show that it's a constant number of bids or constant amount of communication. It's very easy to see when the auctioneer cheats. For the second price auction, everyone has to collaborate. And the reason for that is the auctioneer could add as many fake bids as there are users, and they have to all talk to each other to figure out which subset is the actual real bids. So this is a sense in which audibility is different between two economic mechanisms.
00:08:00.018 - 00:09:01.206, Speaker B: It's much harder to tell when there's cheating. Okay, the second type of benefit that CK systems have for users is what I call generalized compression. Generalized compression says, if you think about roll ups in some sense, what is the point of having ZK roll ups? I'm able to prove to you these state updates. Given an input state, an output state, an ordering of transactions, a set of transactions, you don't have to verify the actual execution, right? Everyone here probably knows that. But one thing you might want to do is add extra covenants. So you might say, Actually, I want to know if the set of transactions you gave me is the shortest, is the smallest subset of transactions that goes between the input state and the output state, and you want a certificate of that. Another thing you might say is you might want some type of thing that says, like, hey, the ordering in which these transactions were executed satisfies some constraints.
00:09:01.206 - 00:10:40.534, Speaker B: So I prove that certain types of things weren't front run or were front run, or a particular type of subsets of constraints were respected. Generalized compression is anything that lets you add these extra covenants that proves that not only is the straight transition valid, but that there's some extra special restrictions that are meant to be good for users in, you know, why are these tangible benefits? So I think this is actually something I said on the ZK podcast that Ewa made a meme and posted, which was, there's sort of this trilemma in this world, which is you sort of have this question of, like, can I make something efficient? Can I make it private? And how auditable is it? Or as I said in the episode, how easy is it to tell if I got fucked and you kind of can't get all three but you can get very close with ZK and that's kind of what we're going to give a more formal description of. Okay, so now we can talk a little bit more formally. I tried to just set the stage because again, this is like not a talk about approving system. This is more a talk about if you build an application, what types of things should you be trying to create as a benefit? So instead of thinking about auctions now, let's just think about any type of financial mechanism whether it's a matching market, whether it's a Dex trade, whether it's a game where there's two classes of users. One is the operator. The operator coordinates payments or transfers amongst players and the second is users.
00:10:40.534 - 00:11:42.620, Speaker B: Users who submit state updates and constraints that they want satisfied. The analysis of such mechanisms sort of relies on understanding a the algorithm for computing these payments b incentives and proofs that the algorithm wasn't manipulated. So that in the case of the second price auction you see that figuring out if there's a proof that the auctioneer cheated actually requires a ton of extra state and having those kind of concise proofs is quite important. Third, communication channel for the operators and users and A blockchain is just an asynchronous communication channel for these parties. And fourth, of course, measure of utility financial measurement. And in order to do this analysis you kind of have to blend algorithmic game theory, information theory. So many of you have read a lot of cryptography papers that say things like hey, here's an ideal functionality for how users generate a proof or how an adversary interacts with the API of a ZK system.
00:11:42.620 - 00:12:19.462, Speaker B: For these kind of game theoretic things that you're trying to offer to users you can think of having a very similar thing to an ideal functionality except it's an ideal mechanism. So it's a mechanism that has a bunch of properties that you want for these users and you try to show that this holds. So what are these kind of properties? First, there needs to be an asynchronous public communication channel. That's what a blockchain is. If we strip out all the other things in a blockchain PKI, whatever validators. It's just a bulletin board that people can post things publicly to so posting ZK proofs of things publicly. The second is operator incentive compatibility.
00:12:19.462 - 00:13:33.380, Speaker B: So this is users getting proof that the operator didn't manipulate the outcome of the mechanism. The third is user truthfulness. So this is proof that users aren't colluding and this is sort of so that the operator knows that they're getting the maximum revenue or sort of what revenue they should be getting. And the final thing, and this is where these extra constraints come in is minimal communication. Users do not want to provide a complete description of their preferences and you're seeing this in a lot of things that people are building, right? This whole intent world, this whole like I'm giving you some sort of weak type of thing that says like hey, I want to buy Pepe coin on five chains, I don't really care how you do it. That is sort of this notion that users don't have to exactly prescribe their preferences. So if you look at the economics literature and a lot of the mechanisms that are found in the economics literature, the users have to specify their entire value and preferences for every possible thing in the universe that's obviously not possible, right? Like people want a concise minimal representation and so this notion of minimality is actually quite important.
00:13:33.380 - 00:14:48.694, Speaker B: Realizing these mechanisms relies on ZK and so I'll give you kind of the high level version for incentive compatibility and truthfulness, you need auditability and collateral systems. You need some way of slashing some sort of financial penalty on the operator and some form of way of figuring out whether there should be a slashing penalty or not. The second thing is you need this notion of minimal communication to kind of or you need some notion of extra covenants that are proved to give to satisfy minimal communication. Now, as I said, the idea of me going up to you and asking you to go into a grocery store and tell me the relative preference of every pair of items is very unlikely and yet you go to a grocery store and buy things you want all the time, right? So there's a sense in which many different mechanisms have different ways of specifying the inputs. So a matching market so matching markets might be something like intense kidney exchange. Users specify orderings and preferences for every possible alternative auctions. Users have to provide their distribution of values for every possible item which can be quite large and dynamic pricing the operator reprices with user elasticity.
00:14:48.694 - 00:16:14.094, Speaker B: So one key thing is that any type of description of such mechanisms needs to incorporate all three of these different types of modalities. But again, there's kind of this idea that if you want to have minimal communication from the end user, the user only has to specify that they want PepeCoin, not exactly how much from each chain you actually need to add this extra set of covenants that you need to prove. So a natural question might be why do we need collateral? If we have ZK proofs? Shouldn't we just be able to prove these state transitions perfectly and then you don't really need slashing? Well, the problem is in any permissionless system the mechanism operator can impersonate users, right? In a lot of ways mev exists partially because of this, right? A Sandwich attack is an operator impersonating a user and in some ways the only mitigation is over collateralization so that you can slash the operator proportional to the penalty that they cause the users. But again, collateral isn't the only thing you need. You also need this ability to attribute faults and that's where the ZKP is extremely important because it allows you to prove a fault happened and then slash the operator. So what is a commitment collateral game? So a commitment collateral game is long. So initialize a communication channel where people are able to post DK commitments.
00:16:14.094 - 00:16:50.234, Speaker B: So that might be a public chain like a blockchain. A user who has a valuation for items, some type of utility for these items commits to some properties of their valuation. They don't actually reveal the true valuation. They commit to maybe adding some noise to it and commit to the noise version. They commit to sort of if it's a vector of values, they commit to some subvector. The user also commits to what are called feasibility inequalities. The feasibility inequalities are effectively the user saying hey, I am actually over collateralized.
00:16:50.234 - 00:17:34.330, Speaker B: So this might be the equivalent of saying hey, I put up some collateral and my collateral is greater than my bid. But I'm not revealing to you my bid, I'm only revealing to you that I've over collateralized. And these inequalities are actually extremely important for this auditability piece of being able to commit to those. And this is where you need a ZKP also. And then the user will post collateral to a private pool so that's to like a shielded pool so they can be slashed if they violate their constraints. Finally the operator computes payments and allocations and sends them to users and then the users realize some utility operator realizes revenue. So at a very high level this encapsulates all of the different types of games we talked about earlier.
00:17:34.330 - 00:18:22.990, Speaker B: Auctions, matching markets, dynamic pricing. You can sort of make a ZK version of all of them by mapping them to this set of the system. So how should we interpret this? So one very big difference. So if I look at a fully public game, say an NFT auction or an AMM, the users don't actually commit to their true value. They commit to some approximate sketch or of some properties of it because they don't want to reveal everything. And the proofs that they provide are such that that sketch satisfies some constraints so that people can detect when they're cheating. And the final thing is that the algorithm for choosing the payment has to post a proof so that people can verify that the payments were distributed correctly.
00:18:22.990 - 00:19:18.590, Speaker B: So there's sort of two examples. One is sort of a deferred acceptance auction. This is a commit reveal style auction where people commit their bids, then they post as ZKP that their collateral is greater than their bid and then the auctioneer can and then they reveal on the second step. The other is what's called a commit and run mechanism, which is basically a way of me being able to commit something about my value. It's similar to what I described, but it's specialized for a particular type of auction. It's not general. Okay, so the next question is what is the set of possible mechanisms that these commitment collateral games can be realized for and that's we're going to talk about some of the math for that not fully all of it so naively this commitment collateral game setup is sort of under specified.
00:19:18.590 - 00:20:32.360, Speaker B: Obviously you could construct some types of mechanisms where it shouldn't work, right? Like I could construct a sharp p complete matching game and that thing shouldn't be able to solve it. So clearly there's some sense in which complexity, theory wise we're missing the description of this thing is missing some stuff. And so I think one of the reasons, by the way that I went down this line of work is there's this whole line of work on intents and matching and off chain matching. And one question you might ask is how do you actually do that with some form of privacy? This is sort of working backwards towards that sort of what was talked about earlier. So one question about these commitment collateral games is when do these need to work? Well, first of all, the users need to be able to supply some constraints. There also needs to be some constraint on how much the mechanism changes. If a user changes their value for something by one basis point but the mechanism changes the payments by 200%, there's sort of something that's probably wrong, right? Like there's sort of some notion of the gradient of how the mechanism describes payments should not should be sort of you can bound how much it does that relative to how much user valuations change.
00:20:32.360 - 00:21:32.682, Speaker B: So one way of doing this and this in a black box way is something called sort of a smooth mechanism. So informally, before we write sort of some of the equations, a smooth mechanism makes the following assumptions. Suppose that instead of saying like hey, the users know their exact valuation again, imagine you went to a grocery store and I said give me your value for every single item in the grocery store, you wouldn't be able to do it. But let's say I know up to some error, I know that everyone's valuation falls within they're able to guess what their price is for everything within some marginal error. And then the second thing is assume that the allocation so what the mechanism does very smoothly. So this is what I was talking about, that I can bound the maximum change in prices as a function of the maximum change in user values. This is sufficient to sort of bound how fast people can learn what the optimal revenue is.
00:21:32.682 - 00:22:45.330, Speaker B: And so if we zoom out, the reason you want to do this is you want users to be able to not specify their full valuations but you want the operator to be able to realize maximum revenue or optimal revenue without learning all of the user's properties. This notion of smoothness is sort of a game theory version of smooth complexity. If you've ever heard that from kind of the smooth mechanisms kind of show that auctioneers, if a mechanism is smooth, you can effectively show auctioneers can learn the optimal revenue. Okay? So the idea is you have a bunch of users, they have some distributions, you have some mechanism parameters that can be tuned by the operator. So that might be reserve prices and auctions. So online ad auctions, the sort of parameters there are effectively reserve prices, the minimum price for biding on a particular keyword. And then you have a mechanism which takes in these distributions and these parameters and outputs payments, how much each user pays each other with the idea that there's agent zero, which is the blockchain itself, that you might burn coins.
00:22:45.330 - 00:24:12.726, Speaker B: These are more technical definitions, but you should basically think of like I'll show the first one in pictures in a second, but there's a sort of a notion of a smooth distribution. You can kind of upper bound it on all sort of sets and then there's a notion of the mechanism being smooth, which is the payments are smooth. So ignoring the technical details, both of these things kind of say I don't really know exactly how much the mechanism changes prices, but I can bound it and I don't know how much the user's value change, but I can bound it and I don't know any information to any more precision than that. So I'm not very good at drawing, so I apologize in advance for this. But this is sort of a drawing of this notion of smooth, which basically means that the user's value, which you can think of as the x axis, doesn't have a gradient that's faster than the yellow line. So if the gradients sort of stay below, that sort of means you're relatively smooth, but you can see in the red one it kind of jumps up quickly and that's sort of a not sigma soon. So one important thing to note is that for almost any auction in the generic setting, you can learn after many iterations the optimal revenue for smooth if user valuations are smooth with many rounds.
00:24:12.726 - 00:25:08.378, Speaker B: So what this is sort of saying is things like mev auctions that are these iterated auctions. The auctioneer actually can learn how to get the optimal revenue if the user distributions are smooth. So the TLDR without kind of going into math is the smoothness lets you understand the mechanism performance without knowing the precise utilities values of the users. And it also allows a mechanism designer to trade off the prices they charge versus how much knowledge they have of what users value things at. But all of this stuff I've said so far relies on precise knowledge of the user distributions. And a question is, imagine if the users only commit in ZK some fraction of their valuations. Like I sample random items at the grocery store and I tell you I would pay more for this than this, and I give you a small subset of items.
00:25:08.378 - 00:26:33.500, Speaker B: How many is sufficient to get these same guarantees? So you sort of have two worlds computing the same thing. You have the public mechanism. So this is, say, an NFT auction on Ethereum right now, and you have the commitment mechanism where the users only commit these small set of things. And one question you might ask is, when are they close? When does the commitment version get close to the public version, and do we get all these guarantees? And so, yeah, something that's come up is we've kind of shown that there are ways where the users can commit a very small set of things, and as long as they're doing homorphic commitments, they only be partial. You basically get that the private version of these mechanisms stays close to the public one. So the TLDR of this is you can sort of black box take a public mechanism, one where the operator knows all the information about the user, and construct a private mechanism that gets very similar financial properties, provided the mechanism is smooth. So all of the known commitment collateral commitment games have a reveal step, a public reveal step.
00:26:33.500 - 00:27:47.860, Speaker B: Suppose we were doing this in NPC and the users didn't have to reveal. Can you still get these same guarantees? Like, the users don't have to reveal any of their values? Can we kind of get the matching market case to map to this framework? The matching market case, again, is this intent case of, like, I have many possible things and everyone has different preferences. How do I kind of get an optimal allocation? And is there a stronger notion of approximation? Can you get better? Can these private mechanisms approximate the public mechanisms even more tightly? And with that, the final thoughts are ZK should be providing some tangible benefits, not just compression. These commitment collateral games provide a sort of way of thinking about games where ZK actually provides a tangible benefit and that you can measure in terms of prices. These smooth mechanisms let you do that without knowing anything about the users. And combining ZK with smooth mechanisms lets you have users express minimal preferences while still being able to allow the operators to maximize revenue. And so with that, that's it.
00:27:49.590 - 00:27:56.580, Speaker A: Thank you so much, Tarun. Do we have questions? Okay.
00:28:01.590 - 00:28:22.480, Speaker C: Hi, Tarun. Thank you for the talk. Quick question. When you define smoothness, are you kind of thinking of, like, there's an efficiency loss function when you have this private mechanisms versus a public mechanism, both in terms of smoothening a curve plus the cost of doing computation? The cost of doing computation has two effects. One is loss of time. Second is the real, real cost.
00:28:23.730 - 00:29:15.120, Speaker B: Yeah, that's a good question. One way of thinking about why you need this smoothness is if something is smooth, you could imagine that I have a point on the surface that's sort of like the optimal valuations people have. And if I add a bunch of noise and then I take the average, that should be very close to the true optimal when it's smooth. And you could think of the ZK commitments are adding noise, they're projecting down to a lower dimensional space and adding some noise. But because it's smooth, it's staying in the same neighborhood of the optimum. And that's the idea here, is that any mechanism that can guarantee you these pricing guarantees and the user doesn't have to reveal all their information have to have something like that, because otherwise you have no guarantee that the commitments don't just move you kind of far away from the optimal. That's the idea.
00:29:15.650 - 00:29:17.920, Speaker A: I think we have a question back there. Yeah.
00:29:22.770 - 00:29:47.240, Speaker D: Um yeah, I have the question specifically on sort of the matching markets and how it relates to these commitment games, which is, I think in a lot of cases, you don't want to have these discrete rounds that are kind of required by commitment and reveal schemes. And I'm curious whether you've given fort as to if you can model those in a similar way or if there's a similar equivalence there.
00:29:48.090 - 00:30:24.980, Speaker B: Yeah, it's a good question. I think the matching market case, I suspect, can only really be done in a fixed round with Fhe. I don't think you can do NZK in less than two rounds. Yeah, I think the matching market case is actually the worst case in some sense. But I do actually think there is a sense in which if you had Fhe and you had a way to attribute faults in the matching game, you should be able to do in one round. But yeah, I don't know how to do it without it. I haven't really thought about it enough.
00:30:24.980 - 00:30:30.550, Speaker B: I haven't proved that it has that communication complexity.
00:30:31.850 - 00:30:33.080, Speaker A: All right, thanks.
00:30:35.690 - 00:31:01.710, Speaker E: Yes. One more question, and one maybe comment or answer. My first question is kind of the same as anishes are there any other models besides smoothness that capture kind of expressing people's preferences without them revealing their entire set of values? Or is smoothness the thing we should be studying?
00:31:02.130 - 00:32:07.222, Speaker B: Yeah, so it's a good question. I think smoothness just again, the intuition for why it works, right, is like, if a thing is smooth, I randomly sample the question is, like, for matching markets, for instance, those are very sharp, right? The user utilities might be relatively smooth, but the mechanism, its allocation might jump very discreetly. And for those, you have to have a totally different yeah, I don't know what that is, but I take that. So Guillermo and I wrote a paper recently on things where we give a notion of smoothness to things that look like matching markets, and when that's true, you can apply this same setup. I think the main reason I'm not sure how to deal with anything that is different than smoothness is there's no way to really guarantee, like, if you. Had some notion of a type of metric embedding where I know that post commitment you could think of. Like, I make this.
00:32:07.222 - 00:32:38.398, Speaker B: Imagine I have a vector that defines my preferences that's length N. I project down to a set of commitments that size log N. There are some lower bounds on how much that I have to move distance wise and those kind of give you some hardness results that, other than smoothness, you're not going to get something. But that's kind of like the limit of what I know in that sense but there might be another model. I agree with you, it's just that smoothness is the only one, I think that you can kind of prove some nice properties for.
00:32:38.484 - 00:32:56.920, Speaker E: Yeah, thank you for the intuition, it's really helpful. And as for the open questions, you mentioned something about keeping data private or something or never revealing using MPC, wouldn't the commit and prove paradigm maybe with CK help? Like what if we just commit to the data and never open it, ever?
00:32:57.610 - 00:34:02.140, Speaker B: Yeah so in the known mechanisms, for what it's worth and these ones are in the known mechanisms, usually what happens is you commit to say, a bid and if you don't reveal, you get slashed. That's how we do fault attribution. But if you had a ZK proof of auditability, like, oh, given this set of commitments and some proof of these inequalities I was talking about, so both of these mechanisms don't have crucially proof of some feasibility constraints. If your feasibility constraints are attributable such that you can test whether they're violated by say, your commitments are homorphic and you add them and then you add the constraints and they don't satisfy some expected behavior, then you could do it without the commit wheel. So the kind of crucial thing here is really these feasibility constraints and having homomorphic commitments is extremely important for that. But that's a very good question.
00:34:02.830 - 00:34:03.820, Speaker E: Thank you.
00:34:05.310 - 00:34:06.540, Speaker A: Any other questions?
00:34:15.970 - 00:35:06.366, Speaker F: Thank you. Good talk. Taryn, my question is, in relation to what you just spoke about the commitment, collateral games and the truthfulness, right. So just for simplicity, take one example in the Mev auctions, like you have the per block commitment, let's say multiple bids come from multiple searchers. Is it possible to add some contingent liability mechanism that you have a player coming in for one time and then if they get slashed, you don't really need to have the collateral more than what the bid is. But it could have this kind of condition, liability that if they cheated or if they fail to reveal they get slashed, they will only get back in.
00:35:06.388 - 00:35:59.598, Speaker B: The game once they yeah, I guess this is kind of what they were. Both their questions we're getting at the prior two questions of, like, oh, can I do some temporal type of thing? Yeah. So to be fair, all of these things assume myopic utilities, right? These utilities are stateful for only a single state they don't have time dependence. You probably could show something for time dependence, but I feel like it's actually quite hard. I'm not sure if you're going to get formal proof, but there might be something that kind of works sort of in practice. I think there's a sense in which you could try to relax these incentive compatibility constraints to something that's very approximate and then that is more favorable to do via these temporal mechanisms. Yeah, I haven't seen any.
00:35:59.598 - 00:36:24.760, Speaker B: For what it's worth, an interesting thing about most blockchains is the slashing penalties and the blocker boards are all temporal in some weird way. Like you don't realize your blocker board immediately. For most chains, you have time windows for withdrawals that you receive them, and same with slashing. So there's clearly something to that. I'm just not sure how you formally analyze it.
00:36:25.130 - 00:36:25.880, Speaker F: Thanks.
00:36:29.450 - 00:36:38.280, Speaker A: All right, I think that's all we have time for. Lunch is starting now, and we'll be back on the stage at 02:00 P.m.. Thank you, Tarun. Thanks.
