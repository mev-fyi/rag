00:00:05.450 - 00:00:21.280, Speaker A: Welcome to zero knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralized web, as well as new paradigms that promise to change the way we interact and transact online.
00:00:25.210 - 00:00:25.960, Speaker B: You.
00:00:27.610 - 00:01:16.094, Speaker A: This week, Tarune and I chat with Jill Gunter and Ben Fish from Espresso Systems. Espresso has been in stealth mode for a couple of months, but recently went public with their proof of Stake l one developed with flexible private applications and ZK rollups in mind. We explore the founding and development of the project, the Espresso base chain, and the cape wrapped privacy smart contract application that they've built. We walk through the programming model, the evolution of privacy technology, what configurable or flexible privacy means, and could unlock the Espresso roadmap and more. But before we start in, I want to highlight that the Gitcoin grant Round 13 is on. Now, this means donations made on their platform during this period are actually matched from matching pools. Your donations go a lot further if you donate.
00:01:16.094 - 00:01:31.414, Speaker A: Now I want to highlight the zero Knowledge podcast. Grant. It's a long standing grant on Gitcoin as well as the ZK Hack grant. Both are in the main pool. So this is a great way to support the show and our events. If you want to. I've added the link in the show notes.
00:01:31.414 - 00:02:13.458, Speaker A: I also want to point people to the ZK Tech side round. This is organized by the ZK Validator and Zero X Park, and we've brought together a great group of matching partners from the ecosystem. So there's a 275K matching pool. This goes towards ZK projects, and if you're interested in supporting these kinds of projects or just learning about them, head over to Gitcoin grants and check out the ZK Tech Side round. Something I don't usually do on the show, but wanted to ask if you like the show, be sure to give us a review or a like on any platform where you're listening. Help us share the show to groups, communities or teams you think might get something out of being in the community. And yeah, thanks generally for listening.
00:02:13.458 - 00:02:28.250, Speaker A: I'm meeting lots of new joiners right now through Zkhack on our Telegram channels yeah, and getting to learn about all sorts of new ZK projects coming online. So now Tanya, the podcast producer, will tell you a little bit about this week's sponsor.
00:02:28.750 - 00:03:04.514, Speaker C: Today's episode is sponsored by Anoma. Anoma is a suite of protocols that enables self sovereign coordination. Their unique architecture facilitates efficiently the simplest forms of economic coordination such as two parties transferring an asset to each other as well as more sophisticated ones like an asset agnostic bartering system involving multiple parties without direct coincidence of wants. Or even more complex ones such as end party collective commitments to solve multipolar traps where an interaction can be performed with adjustable zero knowledge. Privacy. Visit Anoma network to learn more. Anoma is also hiring visit.
00:03:04.514 - 00:03:13.610, Speaker C: Heliox dev jobs to find out about their openings. So thank you again anoma now here is Anna and Turun's interview with Espresso Systems.
00:03:17.230 - 00:03:24.458, Speaker A: So I want to welcome Ben Fish and Jill Gunter back to the show. This is the first time I have you both on here together, but yeah, welcome back.
00:03:24.544 - 00:03:25.402, Speaker D: Thank you, Anna.
00:03:25.466 - 00:03:27.726, Speaker B: Yeah, it's great to be back. Great to see you.
00:03:27.828 - 00:03:58.386, Speaker A: So I'm very much looking forward to digging into this project. It has been very much in stealth. For some of the people listening, this might be the first time they hear about a project that you're doing together. It's also one that Tarun, who's also on the call tarun and I have made early investments into him through robot me through Zkv. But I know that a lot has changed. And so for me, this interview is going to be a revisit, and I'm very excited to find out how it's evolved. So the name has also evolved a few times since we first spoke.
00:03:58.386 - 00:04:03.370, Speaker A: So why don't we first start with that? What is the name of this project and how did you get here?
00:04:03.520 - 00:04:35.870, Speaker D: The name is Espresso or Espresso Systems, and we had a bunch of different names that we were thinking about. One of the wonderful things about being at stealth is you can change, have a long time to work through your branding and perfect it. But Espresso was just a really fun name that we all liked, and it evokes kind of speed and compression, and it has a lot of fun memes or puns that you can do with it related to building blockchains that are scalable.
00:04:36.030 - 00:04:46.518, Speaker B: There have been a lot of coffee related dad jokes. Basically, I think, what Ben is getting at, which we're looking forward to sharing those with the general public over the coming weeks as well.
00:04:46.604 - 00:04:52.246, Speaker A: Very cool. So, Espresso, what did you say? Espresso Labs? No, Espresso.
00:04:52.278 - 00:04:53.334, Speaker D: Espresso Systems.
00:04:53.382 - 00:05:00.870, Speaker A: Okay, so, Espresso Systems, tell us a little bit about what this project is all about. What are you focused on?
00:05:01.040 - 00:05:07.594, Speaker D: So Espresso is a single shot scaling and privacy solution for the blockchain ecosystem.
00:05:07.642 - 00:05:09.246, Speaker A: Oh, my God. Was there a pun in there?
00:05:09.348 - 00:05:10.160, Speaker D: Oh, yeah.
00:05:11.010 - 00:05:12.160, Speaker B: Here we go.
00:05:13.250 - 00:05:13.754, Speaker A: All right.
00:05:13.812 - 00:05:19.454, Speaker E: Every generation of programming language needs a coffee reference from well, it's a virtual Espresso machine.
00:05:19.502 - 00:06:04.110, Speaker D: That's what we're building here, right? Not the EVM but the vem. But that aside, Espresso is focused on both scalability and privacy, which obviously we're not the only project focused on. Those are two of the biggest problems in the blockchain space right now, and we can get more into the details about concretely our approach to these two different problems and what we're doing. But I think that the important thing to emphasize is that Espresso is a solution for existing ecosystems. So we are intending it not as a standalone ecosystem, but as it is an independent layer one, but through bridges to the existing EVM ecosystem. It's intended as a scaling and privacy solution for other blockchains, including Ethereum.
00:06:04.190 - 00:06:13.010, Speaker A: Very cool. Let's find out a little bit about the company, the team. So who makes up this team? Espresso Systems. Who's in it?
00:06:13.160 - 00:06:48.718, Speaker B: So Ben is our illustrious CEO. I am head of strategy, which means I'm doing a lot of things on figuring out go to market, figuring out who the user base is, what they care about. I'm also kind of, like, de facto doing a lot of things on marketing and other things that I have no business doing, probably. But that's the nature of early stage startups. And then we have a team of about 25 people, about 16 of whom are on the engineering side. Those numbers might be out of date by the time this airs because we actually have a few people joining over the coming few days.
00:06:48.804 - 00:06:49.886, Speaker D: We are growing quickly.
00:06:49.988 - 00:07:18.054, Speaker B: Yeah. But we've been very lucky to be able to amass a really amazing team on the technical side, both in cryptography and in systems engineering, and then also to find a few really brilliant people on the product side who are working with us as well. So, yeah, it's quite the crew that we've put together even while operating in stealth, and we're excited to continue to grow the team here over the next few weeks.
00:07:18.252 - 00:07:30.460, Speaker A: Very cool. Ben, I think the first time we spoke at the time, you were part of Dan Bonet's group. Is any of the people who came with have you grabbed anyone from there for this team?
00:07:30.850 - 00:07:54.882, Speaker D: Yes. Well, I've worked with Charles and Benedict for a long time. Charles Liu and Benedict Boonez. We all met in the PhD program at Stanford, and we've collaborated on projects both in academia and in industry over the years. Some of those went well. Some of those candidly didn't go well. But we've certainly bonded and gained a lot of experience together over the years.
00:07:54.882 - 00:08:07.430, Speaker D: And so we started this together and brought in Jill very shortly thereafter. And Jill and I actually met in 2017 when she was talking about fat protocols at the MIT.
00:08:08.030 - 00:08:10.140, Speaker A: That's a throwback for you all.
00:08:10.830 - 00:08:15.658, Speaker D: And we've always dreamed of working together. The timing worked out well.
00:08:15.744 - 00:09:18.654, Speaker B: Yeah. And it also turned out that we were down similar rabbit holes in terms of the problems that we were thinking about where I guess I spent a lot of last year and the year prior. Thinking? Well, first thinking about what I really have conviction on in the crypto space, which I think is an important thing for all of us to always be revisiting. And one of the things that I really started paying attention to was just why the Payments use case hadn't taken off yet. And I was looking at a lot of the stuff that was happening with Circle and USDC's massive growth over the course of 2020, but looking at the utility that Stablecoins was offering, it still was not being applied in the payment space, which was kind of the original Satoshi dream right, of peer to peer digital cash. And I started thinking a lot about both scalability and privacy. And I knew that Ben was one of the sort of research luminaries of those worlds and hit him up and was lucky enough to be able to land on the team early on.
00:09:18.772 - 00:09:44.280, Speaker A: Cool. When we first started talking about this project, I felt like it was very, very centered on privacy. And since then, I know it's evolved. Maybe the underlying tech hasn't evolved that much, but the positioning or the way that you're describing it have. So can you tell me a little bit about that journey? Like where you started, what you saw yourselves at the beginning, and where you are now?
00:09:44.810 - 00:10:27.478, Speaker D: Absolutely. Yeah. And the tech has evolved as well, too, since we're a very research and development heavy company and we're always trying to solve the cutting edge problems in the space. But when we started out, we were positioning ourselves as focusing on solving privacy problems. And we knew that building a scalable infrastructure is an important feature. And I think that the shift in positioning is that we actually are building a scalability solution where privacy is an important feature. And that's important when we look at what users we're trying to reach.
00:10:27.478 - 00:10:39.938, Speaker D: We want to be relevant to users who may not care about privacy. Right. But one of the differentiating features of our infrastructure is that it also supports privacy in Web Three apps.
00:10:40.034 - 00:10:41.514, Speaker A: How did you get to that point?
00:10:41.632 - 00:12:04.162, Speaker B: I was just going to go there, actually, Anna, so that's perfect. I wanted to talk a little bit about our process and our product process specifically, and also just the way that we're trying to approach building all of what we're describing here, which is if you look kind of historically I think at it's funny to say, historically, this is only like the last five years, but at the layer one projects of kind of the 20 16, 20 17, 20 18 vintage. A lot of kind of the playbook revolved around coming up with really neat sort of theoretically interesting solution to a problem that existed with ethereum writing a white paper about it and then going away for a period of like one to five years and building that solution and then rolling it out with developer tooling around it and sort of field of dreams approach. If you build it, they will come and they will hopefully build interesting applications that attract users. And I think that's worked out really, really well for those really nascent, sort of bleeding edge at the time projects. But I think if I look around at the industry today, we've actually moved on from that as an industry, which is really great. We are at a point of maturity where we do have products in crypto, in Web Three.
00:12:04.162 - 00:13:07.974, Speaker B: Web Three wasn't even a term right. In 2017, not pervasively used anyway. But we do have these products that are finding real traction, finding real product market fit. And so it's something that we've been really intentional about as we are building infrastructure in a layer, one that we don't want to just go away and bury our heads in the sand and think, okay, this idea that we've come up with is really cool and we're just going to drop it on the market when it's ready in a year or so's time. But rather we wanted to have a much more engaged and ongoing product process with users ranging from developers to DeFi Dgens to NFT creators to Dao admins all the way up to big institutions. And so we've been very intentional over the last several months about conducting user tests and really being engaged with all of those folks to understand user needs. And that's where a lot of this feedback has come from and that's where a lot of the positioning has evolved over time.
00:13:07.974 - 00:13:45.358, Speaker B: And I would anticipate that it will continue to, which is actually the default thing. If you look at most Silicon Valley tech companies, they end up in very different places in terms of what the product is, in terms of positioning from where they started. And as Ben mentioned, we're still very much at the starting line here. And so I think to us it doesn't perhaps feel like it's evolved that much, but it surely will continue to. And you're right to point out that certainly our positioning has been informed by all of those user studies, even just over these past few months.
00:13:45.544 - 00:14:23.762, Speaker A: Going back to what you had said, Ben, about sort of the privacy first, now it's more like scaling solution. With privacy optional. Is there a trend towards just focusing on scaling that you were kind of picking up on? Or do you actually feel like, I almost wonder if the industry had shifted so far to scaling and I almost feel like it's walking back to privacy. I don't know if you picked up on that too. And so that's why I'm curious about how you see yourselves developing from here. Are you going to be doing most of the work into the privacy direction or do you see it more in the scaling or do you see it more in the adoption maybe and let that define it?
00:14:23.896 - 00:15:17.518, Speaker D: I think one of the main reasons to focus on scaling first and then privacy as a feature is that scaling is something that everyone really needs. Everyone needs low cost transactions. And by being a scaling solution, you can attract apps and liquidity and activity onto the system. And that also helps when considering privacy as a feature. If there's no activity on the system, if there's no apps being developed, then if there's no liquidity, then it's hard for anyone who wants privacy to really get any use out of the system. And so we can talk more in more detail on what exactly Espresso is. It's going to be like a high throughput blockchain that has bridges, token bridges to Ethereum and other EVM blockchains and it runs a high throughput Proof of Stake consensus protocol that integrates with roll ups.
00:15:17.518 - 00:15:34.274, Speaker D: But all of this is designed to attract web Three apps to be deployed on Espresso and then it seamlessly integrates with some of the privacy protocols that we've developed there to enable those app developers and users to have optionality around privacy.
00:15:34.402 - 00:16:41.178, Speaker B: One thing that I just wanted to add there as well Anna, is that you really need both to unlock these kind of holy grail use cases in web Three and crypto, you need both scaling and privacy. One thing that really came through as we've been talking to potential users over the last few months is that the scaling issue is a pain point today for today's users of these applications. And so I think that it's important that we are meeting that pain point in any solution that we design, that we bring to market. But equally then privacy, it's not actually so much of a pain point as, for example, scaling is for today's users because today's users all came into this all transparent all the time for the most part world and were clearly kind of comfortable enough to adopt these products despite that. But privacy is a huge barrier to future users, future use cases and innovation. And so that's one framing that's kind of informed our priorities and positioning, I would say.
00:16:41.344 - 00:17:05.140, Speaker A: I think it would be a great moment to actually jump into what the components of the system look like, how they work together. And I sort of want to revisit that like how users could then interact with privacy. But Espresso Systems is the company, but Espresso is also the platform. Maybe let's walk through the components of this system.
00:17:05.690 - 00:18:36.500, Speaker D: Great. Yeah, no, Espresso is the platform is the blockchain that will have bridges to the rest of the EVM ecosystem, but it runs its own high throughput Proof of Stake Consensus protocol. And one of the things that we've been working on really hard is how to integrate that with roll ups to achieve better scalability without compromising on decentralization and specifically decentralized data availability. Roll ups are a very popular scaling solution and technique these days and it really is the only way to scale past thousands of transactions per second because consensus protocols on their own just hit fundamental information theoretic bottlenecks in terms of how fast information can propagate through the network. And so roll ups are a way of compressing many transactions into a smaller sized transaction which is ultimately necessary if we want to keep fees low even in the sense range as demand scales beyond what you're seeing on Ethereum today. But the challenge is, and with a pitfall of a lot of existing all really existing roll up solutions are that they either are posting all the raw transaction data to the consensus protocol and achieving a fairly limited amount of compression. Reducing certain things required for verification, but not overall, significantly reducing the amount of information that needs to be propagated through the system.
00:18:36.500 - 00:19:08.698, Speaker D: Or they're resorting to something called the Data Availability Committee, which is essentially a centralized solution to making sure that data is available to everyone in the system so that they can use that data to build transactions. And so what we've been focused on is how to carefully integrate roll ups with a proof of stake consensus protocol in order to get around that issue. And then I can also talk about this is a lot at once, so maybe we can break this up, but I can also talk about our privacy protocols and how those work.
00:19:08.864 - 00:19:23.926, Speaker A: Well, actually, my next question was actually Cape protocol. Let's actually talk about some of these different pieces and then talk about how they work together. But yeah. What is cape? And this is actually where I got a bit confused. So is Espresso the l one? That's the blockchain.
00:19:23.978 - 00:19:25.490, Speaker D: Espresso is the l one.
00:19:25.560 - 00:19:25.842, Speaker A: Okay.
00:19:25.896 - 00:19:32.738, Speaker D: And Cape, which stands for Configurable Asset Privacy for Ethereum, is a protocol that can run on any EVM platform.
00:19:32.904 - 00:19:40.600, Speaker A: So it's not only attached to Espresso, it's a protocol. It's something that could be deployed in another place.
00:19:41.050 - 00:20:23.854, Speaker D: That's right. It is a protocol that could be deployed in a different place. Although there will be certain advantages of Cape running on Espresso. One of the caveats of running Cape, or really any privacy protocol on Ethereum, is that transaction fees on Ethereum are not private. So, like, for example, Tornado Cash needs a relayer to basically aggregate the transactions and submit them. And that's because in order to submit an Ethereum transaction, you need to have an Ethereum account and pay fees from a non private account. So there are certain advantages like that that running Cape directly on Espresso will have, but it can be deployed on any EVM blockchain.
00:20:23.854 - 00:20:28.446, Speaker D: And in fact, right now, we've deployed it as a demo on Ethereum's testnet.
00:20:28.478 - 00:20:34.914, Speaker A: Rinkbee okay, cool. And is this what's being deployed? Has it been deployed or will it be deployed?
00:20:35.042 - 00:20:41.014, Speaker D: It's being deployed this week, literally, as we speak. So by the time the podcast goes.
00:20:41.052 - 00:20:43.198, Speaker A: Out, it will be deployed.
00:20:43.234 - 00:20:49.500, Speaker B: Yeah. It's a little weird that we're trying to time travel into the future here or the past. Yeah.
00:20:50.110 - 00:21:15.166, Speaker E: Quick question regarding this. So, I deploy a contract on an EVM chain that sort of wraps the local asset, but then when I do certain function calls, it goes across the bridge to Espresso. How does that mechanically work, I guess in terms of user submits transaction to the Cape contract, maybe walking through that.
00:21:15.208 - 00:21:32.250, Speaker D: Would be yeah, no. So Cape is a protocol that can run within an EVM blockchain. So Cape is currently demoed on Ethereum's testnet, but we will migrate Cape the protocol to Espresso when we launch the espresso blockchain.
00:21:32.910 - 00:21:34.250, Speaker A: And that's where it will live?
00:21:34.320 - 00:22:29.402, Speaker D: Primarily, yeah, that's where it will live. Assets will separately can be bridged over from Ethereum to espresso in the same way that they're bridged over to other blockchains that are interoperable with Ethereum. The way that Cape works is that if you have, say, an ERC 20 within the EVM on a given blockchain, then you can wrap that ERC 20 into a Cape token that will have configurable privacy features and it's up to the creator of that wrapper functionality to determine what are those privacy features. Currently, there are viewing policies and even freezing policies that the token wrapper can configure. But in the future this will expand to customized policies using some of the other protocols that we've developed but haven't released yet.
00:22:29.536 - 00:22:42.430, Speaker A: So would you imagine it sort of being that, say you have a token on Ethereum and you'd bridge it over to espresso, you'd wrap it in Cape and then use it in espresso for certain?
00:22:42.500 - 00:22:43.134, Speaker B: That's right.
00:22:43.252 - 00:22:57.330, Speaker A: Okay. I think because you're deploying on Rinkobi, there's this idea that when you talk about Bridging, would you actually ever think of deploying on Ethereum as well and then Bridging through Cape? Can you bridge through the DAP?
00:22:57.410 - 00:23:44.690, Speaker D: Basically, I mean, we actually thought initially about running Cape directly on Ethereum. The problem with running it on Ethereum is twofold, one, high fees. You really need some kind of scaling solution in order to keep those fees down and make it more efficient. And number two, we would anyways need to run it through a relayer similar to tornado cash because of the issue of the privacy of paying transaction fees on Ethereum. So given our agenda, it made more sense to run Cape on espresso. And also we're going to be focusing on the seamless interaction between Cape assets and Smart Contracts or web3 apps that run on espresso. We have a lot more control over that than we do on Ethereum.
00:23:45.510 - 00:24:59.850, Speaker E: Actually, one quick question. So this is a big debate right now in sort of the multi chain world, which is you can have a single asset that has many synthetics represented on another chain like WETH exists in five forms on Solana, which in a lot of ways led to the wormhole hack. So due to the kind of technical fungibility between the different synthetics within DeFi protocols, but then when one gets hacked, you really probably don't want them all to be viewed as equal. So when you're thinking about kind of privacy and sort of side channel attacks, how do you think about a single ERC 20 being minted in multiple synthetics on espresso? Or do you enforce that being done in a single fashion, I guess how are you thinking about because I think this proliferation of multiple representations of the same asset thing has already been a security issue. And I'm just kind of curious if there's like extra privacy sort of side channel attacks from that because I haven't really thought that much about it until you just described your design. So I'm kind of curious if there's actually anything that you're thinking of with regard to that.
00:25:00.000 - 00:25:38.838, Speaker D: Yeah, it's a good question. It's a little hard to address in very broad terms, but it would help, I think, to think through an example. So Cape is particularly suitable. And when developing Cape, we were really targeting creators of assets like stablecoins, which are sort of a bridge between the traditional financial ecosystem and the blockchain ecosystem. Because stablecoins, largely speaking, are created by money service businesses and administrated by money service businesses. They are centrally backed. So if you think about a stablecoin issuer, they can be minting their stablecoin directly on Cape.
00:25:38.838 - 00:26:00.986, Speaker D: On Espresso, they might have units of that stablecoin that are already minted in ERC 20 on Ethereum. And if you think about the interaction where something gets locked in a contract on Ethereum and then minted on Espresso and then moved back, it's not a whole lot different from moving between different representations of the dollars between banks.
00:26:01.098 - 00:26:41.260, Speaker E: Right. But I guess there's still sort of this question of, like, let's suppose there are two synthetics. So I have one ERC 20, I mint two versions of it on Espresso, and then certain people, when they bridge back to Ethereum, tend to bridge in high, actually. Are the quantities revealed on Ethereum upon coming back? I'm just curious if there's some type of statistical thing of, like, I go across on one type of asset, I trade in a curve pool to the other type of asset, and I come back and in that loop I sort of leak information about my identity based on the quantities I sent.
00:26:41.570 - 00:27:20.618, Speaker D: Does that make sense? Yeah, to some degree. It's not really that there exists two different types of synthetics on another blockchain. I would think about it like this. If you have USDC, which is really an asset type right? And you have units of that on ethereum, right. Or even if you have units with that, I think it's simpler to think about if you have units with that within an ERC 20 on Espresso. And then we can think about how the interaction between that and Cape works. Because moving from between Ethereum to Espresso and then from Espresso to kind of a Cape wrapped asset are similar.
00:27:20.618 - 00:28:08.998, Speaker D: So by having multiple versions of Cape wrapped USDC, it doesn't really mean that it's really that you have different ways of encapsulating the same asset. But every unit itself is. When you lock a unit of USDC inside its ERC 20 contract and then create a unit of it within a certain form inside Cape that has certain viewing properties, move it around, burn it, and move it back. Each of those units are they don't simultaneously exist within Cape and then also within the original ERC 20 in a completely independent and movable way, there's an atomic dependence between them.
00:28:09.164 - 00:28:13.002, Speaker A: So you're getting exactly the same token back when you get out of Cape. Basically.
00:28:13.056 - 00:28:14.010, Speaker D: You are, yes.
00:28:14.080 - 00:28:38.050, Speaker A: The sort of take I got here was the idea that if you'd have multiple types of USDC that had been bridged over or wrapped ETH or something, wonder. I don't know, Tarun, if I'm correct here, but it's like, what if you moved USDCs from other EVM compatible chains all towards this one? Is that what you're so one one.
00:28:38.120 - 00:29:38.530, Speaker E: Problem we've seen a lot on Solana and Avalanche is that there are multiple bridges right between Ethan Solana and Ethan Avalanche. And each bridge has its own representation of the ERC. And then basically, people make curve pools or some type of concentrated liquidity pool that says, like, hey, these two things should basically be equal, and there's an arbitrage between them. Now, when the Solana Wormhole attack happened, a thing that maybe was somewhat less reported was that Soland, which is the biggest lender on Solana, had an insane amount of utilization of loans because people the solend sort of contracts effectively said, hey, look, Oracle price. Between Wormhole WETH and Synapse WETH or whatever bridge, any bridge WETH. I forget which ones were all there, but there's like a bunch of them is basically one to one. But one of them actually just had this hack and actually had this mint event.
00:29:38.530 - 00:30:06.810, Speaker E: In theory, it should not be one to one anymore. So people just started adding that as collateral almost instantly. It was actually amazing how fast people are doing this. They added as collateral and then started borrowing the other types of ETH as it was crashing. So until Jump was like, we're going to put the $300 million in, basically people were putting in something that's worth $0.10 on the dollar and borrowing a dollar against it. And this multiple synthetic representation thing has become a big point of contention.
00:30:06.810 - 00:30:36.246, Speaker E: There was a Twitter space with Prestwich and Sunny, and they had a big argument about it a few days ago. Yeah, so it can cause both security issues, but I don't think anyone's ever talked about the privacy issues of having these multiple synthetics because you sort of are leaking some information about yourself. If you go between multiple representations of the same, like on this destination chain, if you have multiple representations of the same source asset, I really don't think.
00:30:36.268 - 00:31:09.246, Speaker D: It'S fundamental to having multiple representations. I mean, we have multiple representations of the dollar, we have a Zillion representations of the dollar, you have bank of America dollars, you have Chase Bank dollars, right. The issue that you're talking about is really more due to independent price oracles that are specific to a particular representation of an asset, and you're talking about forms of arbitrage. I think it's more to do with that than the fact that you have multiple representations, but it is the interaction between these different representations and those price oracles.
00:31:09.358 - 00:31:20.242, Speaker B: Are you asking Tarun, if there are. Issues around the fungibility of these things that get complicated by the privacy element of yeah, yeah.
00:31:20.296 - 00:31:44.734, Speaker E: To some, like on the Espresso side. Right? Like, how do you think about that? Because like I said, it's very clear it has these security issues because if the synthetic can never be rehypothecated due to either a bug like the wormhole attacked or sort of something where a mechanism that's supposed to either be a price oracle or something that is an arbitrage game goes wrong.
00:31:44.852 - 00:31:45.520, Speaker D: Right.
00:31:46.050 - 00:31:54.180, Speaker E: Things happen. But I'm sort of more curious. How do you think about that on the privacy side? Because I don't think anyone's ever actually.
00:31:54.550 - 00:33:03.046, Speaker D: Well, no, it's a really good question. And frankly, I think that with these side channel issues can be so complicated. So you really don't know until you think deeply about it and sometimes things get discovered later and then you have to figure out a fix. My first reaction to this is that privacy is more localized in the sense that let's look very concretely at what it means to if you have a certain wrapped version of USDC and you're using it within Cape, then you're transacting it anonymously or privately within the pool of assets being transferred within this one shielded pool. And so it's more of a localized property of those transactions and how those transactions are indistinguishable from one another than a relationship between what's happening there and other representations of USDC on completely different blockchains. But that said, side channels can creep up in all kinds of unexpected ways, so it's hard to say no, like there aren't side channels that could be exploited and we always have to be vigilant about that.
00:33:03.228 - 00:33:05.030, Speaker B: You're giving us something to think about. True.
00:33:05.100 - 00:33:11.298, Speaker E: And this is just I just think app developers on Espresso will be having to think about this a little bit.
00:33:11.404 - 00:34:15.738, Speaker B: Yeah, I mean, I think that it goes to a great point, though, of just how devilishly tricky it is when you introduce privacy into the mix to design applications, sound secure systems. There's just so much more at play, even this question of fees and what you're using for fees. And I think that one thing I'm super appreciative for Anna and the work that she's done in kind of education and so many others are now kind of picking up the mantle of in this sort of like ZK universe within our little crypto sphere is just getting developers, getting entrepreneurs and everybody else up the curve on these types of questions that yeah, even for us. All day thinking about it, all day long. There are always going to be things that sneak up like this just because these playbooks have not been fully written yet. I was going to say it might be useful to explain a bit more about how Cape works and the functionality of it, to put a lot of what Ben was just saying in context.
00:34:15.914 - 00:34:25.058, Speaker A: Jill, you and I are so synced that was literally what I was about to say. We both know how to do a podcast. You can tell it's like what's the.
00:34:25.064 - 00:34:28.098, Speaker E: Next yes, sorry, I veered us on this.
00:34:28.184 - 00:34:31.380, Speaker B: No, that was awesome.
00:34:31.750 - 00:34:51.690, Speaker A: So yeah, actually I wanted to understand a little bit deeper what is happening inside Cape because I just had this thought as we talked about locking it up. It has these principles, I actually don't know what it does after that. Like do you just transfer it to other people? Do you trade it in Cape? How much can you actually do?
00:34:51.840 - 00:35:37.110, Speaker D: It's a transfer protocol so it doesn't yet have within side of it trading mechanisms. Of course you can have off chain bartering mechanisms and then support direct trades. But it is a transfer protocol, it doesn't run like DFI or Uniswap protocols within it yet. So first of all, you can create assets directly within Cape. You can also wrap assets that have been already created within an ERC 20 within the EVM. And once you have a Cape asset then you can transfer it to other addresses or other users. And the unique thing about Cape is that every asset has certain configurable properties or what we call policies.
00:35:37.110 - 00:36:10.958, Speaker D: We currently support flexible viewing policies and a freezing policy that the asset creator can configure. So for example, a stablecoin issuer could decide that every public viewer should not be able to see any of the details of transactions. It should be completely anonymous and private to the general public. But the stablecoin provider or designated auditor should be able to see details of transactions over a certain amount, certain credential attributes of the sender and receiver addresses and that can be configured.
00:36:11.054 - 00:37:03.186, Speaker B: And just to put a little bit of that in context, we really designed this with specifically stablecoin issuers but really asset creators of all types in. So, you know, the flow is that you go into Cape, we'll have a user interface for it that will stand up here in a few weeks. But you go into it and you can either, as Ben said, create or wrap an asset. And then the next thing that you do if you are the asset creator and not just an end user of the system, is you designate exactly what Ben just described in terms of who can see what about the transactions that are happening within this asset that you're creating and also some of these other policies that are standard to stablecoin issuers. So like freezing and so forth. So that was really kind of the design principles underpinning it.
00:37:03.368 - 00:37:19.130, Speaker A: Are there select assets that you already have in mind? Are you able to actually limit what kinds of assets can come in or is it sort of free for all people? Can bring any ERC 20 wrap it, it's almost like could you whitelist any particular kinds?
00:37:19.950 - 00:37:33.420, Speaker D: Yeah, we do not have control since it is a protocol, we could have control over what's supported in our user interface, but we do not really control what is happening inside the protocol itself.
00:37:34.110 - 00:38:29.390, Speaker B: I think that this actually comes back though, a little bit tarun, kind of obliquely, but to what you were talking about a few minutes ago, just in terms of these questions of fungibility. This is nothing to do with side channels or hacks or privacy. But as you were talking, it did make me think of this challenge that we're thinking through though, with Cape where you could theoretically have many, many people wrapping the same type of asset over with all different configurations of privacy and these different policies. Right. And then you get to this kind of state where you need to, as an ecosystem and a marketplace, get back to having shelling points around. Okay, this is kind of the canonical, most widely used, most liquid version of XYZ stablecoin within this system. But that's kind of going to be an emergent problem.
00:38:29.390 - 00:38:31.854, Speaker B: And property, I think, of the system.
00:38:31.972 - 00:38:41.202, Speaker E: Don't worry, everyone's having the same problem. This is why Preswich and Sonny, who have very different philosophies on how to tackle this, where I had this very large argument the other day.
00:38:41.256 - 00:38:42.820, Speaker A: Now I want it was it recorded?
00:38:43.190 - 00:38:51.640, Speaker E: Yeah, I hope it's huh it's a spaces, so I'm not sure. I think you can record them. I don't know if they did.
00:38:52.330 - 00:38:55.158, Speaker B: I can hear their two voices in my head already.
00:38:55.244 - 00:39:38.130, Speaker A: I feel like I'm like bummed I missed it. Yeah, well, we'll try to dig it up. If we can find it, we'll add it in the show notes for our listeners to have a chance to catch it. I kind of want to understand a little bit like a use case. Maybe we can walk through who is using this and how can you paint a picture of someone who wraps it in? They wrap a token in Cape, within Cape and then they transfer it and then they potentially take it out at some point. Yeah, just maybe walk us through one of these and who is this person? What are they doing? Why are they doing yeah, yeah, I know there's probably many, but maybe you can just help us with all of.
00:39:38.200 - 00:39:47.030, Speaker B: The user interviews and all of the scenarios that we had to come up for. Those are just coming to mind for me here. But Ben, why don't you kick off?
00:39:47.100 - 00:41:27.150, Speaker D: I think that we can continue with the stablecoin example since again, it's an example of a type of asset that is typically issued by a money service business that has risk management responsibilities and the users are well, twofold, right? One, the user of Cape would be the asset creators who are either creating these stablecoins or creating versions of existing stablecoins. You can have existing stablecoin issuers use Cape to basically add privacy as a feature, sort of like an incognito mode to their existing assets so that their users can enjoy that privacy. But while still retaining the level of visibility and control that they have on Ethereum, which is essential for their risk management. In fact, one of the things that's unfortunate about, or perhaps fortunate, but about privacy protocols on blockchains are that when something goes through a privacy protocol and then comes out of it, it's always labeled as having gone through that. So it's very easy for the money service businesses who run some of these DeFi protocols to decide, oh, we'll ban anything that has come from tornado cash. And so what Cape provides is a middle ground solution where those protocols can endorse a Cape wrapped version of these assets so that users can transact it privately, so it's not publicly revealed to everyone, but it is visible at least to certain auditors and therefore won't be banned from the other DeFi protocols sponsored by those auditors.
00:41:27.330 - 00:41:50.558, Speaker A: So this is almost like, would you picture the issuer of the stablecoin as building somehow with this in mind? Are they connected? Are they deploying something on top of it? Are they the person that sort of DAP or user? Would they be providing the user interface where their users could actually use this track if they want to?
00:41:50.724 - 00:42:25.900, Speaker B: So I think just to take an example, let's say one of the big centralized money service business stablecoin issuers, right? Let's say they are looking at kind of the state of the world and the blockchain industry and where it's going, and they say, you know, what's going to be a really important feature for our users is privacy. And maybe not sort of like monero level privacy or tornado cash oriented positioning around it, but just like standard financial services, traditional TradFi level privacy, right?
00:42:26.430 - 00:42:34.938, Speaker A: Like not needing a centralized system, not needing the database, like still on a blockchain, still immutable, still cryptographically proven.
00:42:35.034 - 00:44:18.922, Speaker B: Let's say that they're seeing demand from big businesses of like, hey, we want to explore using your stablecoin to do payroll for contractors in other countries or even to pay suppliers in other countries, right? That stablecoin issuer might say, okay, hey, we want to attract these big sort of next generation users to our stablecoin. We need a privacy solution that's going to balance our need for risk management and reporting with their need, our potential users need for privacy from their competitors and other actors. That stablecoin issuer could then come use the Cape platform as it exists today. I mean, we'd need to have it not be on testnet, but as the user interface, for example, exists today. They could either create a brand new version of their existing stablecoin within Cape that has the parameters that they want, or they could create what we call a wrapped asset type on Cape, which is basically a template for any holder of that existing stablecoin to be able to wrap that ERC, 20 coin into and that wrapped asset type, or indeed that new asset that they're minting on Cape would again be able to have the right parameters that they need in order to meet the needs of those users. And so again, they could do that all within the user interface that we're providing. And those end users, these theoretical businesses that want to pay suppliers overseas could use the interface that we're providing as their wallet, et cetera.
00:44:18.922 - 00:44:32.100, Speaker B: But as Ben has highlighted a few times, cape is fundamentally a protocol. And so if they wanted to stand up their own interfaces around that and so forth for their own users, they could do that.
00:44:32.630 - 00:44:55.434, Speaker D: Typically stablecoin issuers or providers are not providing a user interface to their users, they're creating assets that then exist on a blockchain and we're creating a wallet that users can use to interact with Cape assets similar to MetaMask or buy through wallet, right?
00:44:55.632 - 00:45:28.710, Speaker B: And again, I'm just going to plug. So we're going to be putting up the UI here towards the end of the month. And again, it's just going to be on testnet. It's a demo. It's going to be on Ethereum's testnet. It's not even going to be on the Espresso testnet, which will come with all kinds of benefits as Ben is flagged. But we're really, really looking forward to having that be out in the world and getting asset creators and also end users to go in and play with that wallet and play with the Mint functionality and the configurations and so forth to get more feedback on it.
00:45:28.780 - 00:45:29.350, Speaker A: Cool.
00:45:29.500 - 00:46:02.202, Speaker D: In terms of how it functions as an end consumer app and how asset creators and end consumers interact with it, it's not that different from a product like ZK Money, right? And it just gives more flexibility and control to the asset know, so that it's not just totally anonymous assets within the shielded pools on Cape, they could have certain visibility or freezing properties configured by the asset creators.
00:46:02.346 - 00:46:06.340, Speaker A: Cool. Tarun, you had something you wanted to jump in on?
00:46:06.710 - 00:46:42.446, Speaker E: I guess I'm more curious from more of the development product side, which feature is like when I take my Arc 20, I wrap in Cape and I go across what are sort of the basic view and permissioning functions you're starting with. I guess it sounds like there's this universe of different types of things. But from a token standard looking perspective, what do you view as the initial ones or the more fundamental ones to start with and how are you kind of thinking about the features of that from the perspective? Let's say I'm a prospective developer and I want yeah.
00:46:42.548 - 00:47:22.410, Speaker D: So right now in the current version of Cape, it's somewhat limited because, and this is also for technical reasons, we wanted to have one universal zero knowledge circuit that captures all of these so that you can't distinguish transactions by the type of asset that's being transferred in its policy. In the future, we can give more flexibility to the creators of these assets to program their own arbitrary policy, but right now it's really just any subset of the amount receiver and sender addresses and subset of the attributes of digital credentials associated with the sender and receivers.
00:47:22.750 - 00:47:48.580, Speaker B: Yeah, I think in future though, Tarun, one concrete use case that we're excited about is thinking about how we might be able to support things, for example, implementation of the travel rule here, right, where below a certain threshold reporting requirements are more lax, and above a certain threshold of transfer amount reporting requirements then kick in.
00:47:48.950 - 00:47:59.398, Speaker D: We actually have that as a feature now, so you can configure that you only see information about those fields that I mentioned if the amount is over a certain threshold value.
00:47:59.484 - 00:48:25.070, Speaker B: Yeah, that's implemented in the protocol that won't be on the UI at the outset, but that's, again, I think another example of how we've built this with really like stablecoin issuers and all manner of asset issuers in mind. And we're looking forward to continuing to kind of expand what those use cases and applications look like as we go and think about it more deeply.
00:48:25.730 - 00:48:30.830, Speaker A: What are the actual zero knowledge proofs? What is the Cape protocol using?
00:48:30.980 - 00:48:56.466, Speaker D: We're using Plonk and actually, well, extensions of Plonk. In fact, we have an open source library called Jellyfish that is already open on GitHub and it's based on Plonk, but it has certain extensions of Turboplank and other small optimizations that enabled us to make our circuits really efficient or really our constraint systems really efficient.
00:48:56.578 - 00:49:17.150, Speaker A: Given that you have these different types of like, you can select the privacy settings, does that inform what size of circuit you need? Or does it actually affect the zero knowledge proof? Or is it just like there's a standard ZKP and underneath there's some other stuff that you can configure?
00:49:17.650 - 00:49:38.754, Speaker D: Yeah, there's one constraint system, and that's important so that every transaction looks indistinguishable from another transaction. The viewing policy is an input to the proof. If you have different viewing policies, it doesn't change the proof that's being computed, just change the inputs.
00:49:38.802 - 00:49:39.158, Speaker A: Okay.
00:49:39.244 - 00:50:36.598, Speaker D: If you want to enable custom policies that go beyond this, then it's hard to give just one universal zero knowledge constraint system to capture everything. And so if you want users to be able to program their own constraints, then you need this is getting very technical, but you need like two levels of recursion in order to be able to hide things. And so we have an extension of the Zexi system that is based on integrate using Plonc for the inner proofs and Planck's also for the outer proofs. And we'll be releasing that soon as well. And we have a paper on it that we're going to put out there, which is basically just making a lot of optimizations to Zexi based on Planck. And it has sort of like a ten x improvement in the performance on the state of the art of that.
00:50:36.764 - 00:51:04.606, Speaker E: Are there any other constraints that come with kind of merging those two because they both have sort of different representations of things. So I was just curious, from the perspective of a programmer, do they have to think in terms of writing a Zexi sort of code or do they think in terms of like, oh, I need to actually think of lookups and stuff like that. I'm just kind of curious how that works from the end user standpoint.
00:51:04.718 - 00:52:12.978, Speaker D: Frankly, the way that the Cape Protocol works and the way that Zexi works are actually very similar in terms of their programming model. It's all UTXO based and Zexi is really an extension of the UTXO system where everything is basically encapsulated as a record and every transaction basically deletes records or nullifies records and creates new records. That's how Cape works as well. The additional thing that Zexi does within the Zexi Protocol is that it's proving that it satisfies the custom creation and destruction predicates of any records that a transaction is creating or destroying, and it's hiding the predicates of the records. So all of this is abstracted away from the user. Yeah. We haven't integrated Dexi with Cape into like an end consumer product yet, so it's a little hard to say exactly what it's going to look like to the user in terms of how they write a custom policy and express that using a constraint system or programming in a higher level language.
00:52:12.978 - 00:52:28.602, Speaker D: And then that gets compiled down to a plant constraint system and that entire flow. We haven't really completely integrated that into a consumer product, but it's not that different of a programming model from the way that Cape works itself.
00:52:28.736 - 00:53:10.620, Speaker A: But Zexi actually has this programmable you can actually program within it. At least that's what sort of Alio is working on. And so as I understand it, the Espresso is l one, that's where you would have smart contract type things. That's where the EVM compatibility lives. The Cape lives on top of that, right, as a DAP, as a protocol. But would you then have programmability with and I'm guessing, like, Cape is programmed, but it's sort of pre programmed and then users can interface they can choose their but yeah, like, do you actually also expect people to build again on top of like, would you want them to be able to program on that sort of third level up?
00:53:11.150 - 00:53:54.790, Speaker D: Yeah, that's a really good question. So the way that Dexi enables programmability is that it allows users to basically create these rules for how records can get created or destroyed and that can determine a certain type of application. But Dexi is still quite limited in terms of how it can achieve programmable privacy. In web. Three apps. It's really only handling off chain computation if you want to retain the privacy features that it enables. Zexi would not allow you to build a privacy preserving version of any type of smart contract that requires public data.
00:53:54.790 - 00:54:36.638, Speaker D: Also, there can be fundamental limitations of that because the data needs to be public. Right. So Zexi doesn't allow you to build, like, private uniswap. Also private uniswap is not really possible unless you change the definition of how it works. So there are limitations to that. But what it is really good for is user defined assets that have customized rules in terms of how they work. So you can imagine that every asset has a different zero knowledge proof constraint that needs to be satisfied by the transaction, and you want the transaction to hide what the asset is and what is the circuit that you're proving is satisfied so that you can't distinguish transactions by their types.
00:54:36.638 - 00:55:08.160, Speaker D: That's what Zexi is very good for. Obviously, it's more flexible than that, but we're still trying to figure out how it will again integrate into a developer platform and how users will want to use it, because it's not really clear what developers can do with Zexi in a meaningful way around programmable privacy. That's like a very high level concept that I know is tossed around in the industry. But when you get down to the gory details of how things work, it's not obvious what programmable privacy means.
00:55:09.330 - 00:55:37.094, Speaker A: Okay, so I want to go back to the Espresso layer. I think we've done kind of a cool dive into Cape that sits on top of Espresso. Espresso is an L1. Do you have other protocols also planned that are separate from Cape? Do you picture multiple privacy protocols interacting with that L1, or do you picture Cape as the primary? Are you kind of delivering sort of both together and that makes up the entire system?
00:55:37.212 - 00:55:49.750, Speaker D: Yeah, the two privacy protocols that we worked on were, one, Cape, and then two, this newer version of Zexi, which we call very Zexi for Verifiable Zexi because it's unifizer, it uses universal.
00:55:49.830 - 00:55:50.842, Speaker B: You have an acronym.
00:55:50.906 - 00:55:51.760, Speaker D: Very nice.
00:55:53.890 - 00:55:56.320, Speaker E: That was the sound of 500 size.
00:55:57.570 - 00:56:00.640, Speaker B: No, that was just me. True, that was just me.
00:56:05.270 - 00:56:35.370, Speaker D: And those are the two privacy protocols that we have. And we do not have any other protocols planned. It's a lot already on our roadmap. I'm like more, but yeah, a lot of our focus right now is on the Espresso layer one and how it works in terms of its integration of roll up and consensus and how it achieves higher throughput without compromising on data availability.
00:56:35.870 - 00:57:00.146, Speaker A: Okay, cool. And I think that's what I want to explore now is let's go back to that point now that we understand a little bit more about what's living on top of it. First of all, it's EVM compatible, right? That's the part that's EVM compatible. How are you doing that? Is it like a fork of geth? Are you doing something that compiles down into something else, but you can deploy solidity code on it?
00:57:00.328 - 00:57:23.178, Speaker D: So we're actually developing a zke EVM. So we are building a roll up directly for the EVM we're translating EVM op know to. So that's one exciting thing that we've been working on, just like an efficient implementation of Zkevm, which Zke EVM, by.
00:57:23.184 - 00:57:32.990, Speaker A: The way, are you following which camp? Because there's a few different projects working on that. I know Hermes has one EF is working on something like this. Do you have a unique one?
00:57:33.060 - 00:57:33.642, Speaker E: Scroll.
00:57:33.786 - 00:57:34.686, Speaker A: Scroll, yeah.
00:57:34.708 - 00:58:22.098, Speaker D: So it would be a lot more similar to I would say that the different approaches roughly break down into two different categories. One is build your own custom VM and then have a compilation from the EVM to that VM. And then the other approach is to try to directly build a constraint system that captures the EVM as closely as possible. And I don't want to butcher what other projects are doing specifically. So those are just the two broad categories and we are in the latter category. So we are building a constraint system that directly captures EVM state transitions. And yeah, we're working on using our Turbo Planck constraint system and the various optimizations that we've made to try to make that efficient.
00:58:22.098 - 00:59:09.670, Speaker D: Although I would say that while it's good to make small efficiency optimizations in the representation of the EVM, I think that with the trend in the industry towards having just really high performance provers, I'm sure you're aware of Zprize and this industry, I think you're participating in that, right? So there's an industry initiative to have high performance provers and there's also a lot of startups already that are working on doing that. It's less important to have these small differences in how the constraint system works when you can run roll up servers on a very high performance platform, which is different for roll up than privacy, where privacy requires consumers to produce these zero knowledge proofs like on a phone or on a laptop.
00:59:09.830 - 00:59:10.540, Speaker B: Yeah.
00:59:11.150 - 00:59:47.110, Speaker D: So that's just something to note that betting on really high performance proofers and the fact that the computational cost of producing roll up proofs per transaction is still just so, so much smaller than the costs that are currently due to gas fees, due to congestion on blockchains, that it's not as significant to care about these small differences in the efficiency of the prover. Which is why we're focusing a lot on how rollup integrates with consensus to achieve higher throughput, but while preserving the decentralization of the system, which is so core interesting.
00:59:47.180 - 00:59:53.946, Speaker A: So the Espresso being the L1, that is actually where the Zke EVM works here. That's where that lives.
00:59:54.128 - 01:00:32.146, Speaker D: The Zkevm yes, is a component of the Espresso consensus protocol. And the protocol itself, the provers are a separate logical component. So, like, the servers that are computing these roll up proofs do not need to be consensus nodes themselves, but would be sending proofs and data to consensus nodes. But the way that this integrates with consensus is exactly in who gets to verify proofs and who actually has to receive data and basically sign to say that they've received data and then can serve data to other nodes or users.
01:00:32.258 - 01:00:43.014, Speaker A: So I want to kind of see Cape is deployed on the Ckevm. Those proofs, are they being generated within Cape?
01:00:43.062 - 01:00:44.634, Speaker D: Cape is a higher level application.
01:00:44.752 - 01:01:06.130, Speaker A: Okay. I keep thinking of roll up models where you have smart contracts that do the verification of the proofs. So in the case of Cape is a smart contract deployed on Espresso that's actually doing the verification of the proofs that underlie the Cape protocol.
01:01:06.790 - 01:01:34.854, Speaker D: Yeah, I think let's take a step back. I think that it gets confusing with all these terms being thrown around in the industry, like roll ups or consensus or this or that or the other. Cape is at the application layer, right. Transaction processing itself. That's at the layer one level. Right. And so when you have roll ups, roll ups, you should really think about roll ups and consensus as just combined, defining a new consensus protocol.
01:01:34.854 - 01:02:23.420, Speaker D: Right? A consensus protocol is just a transaction processing system that has some way of ordering transactions. And then different consensus protocols ranging from a centralized system to a decentralized system of different types are having different security properties of what guarantees you have around consistency and liveness and data availability. So when we talk about the rollup and the consensus and Espresso, this is all describing just this abstract transaction processing system. It has nothing to do with the application layer. At the application layer is the fact that you can run protocols like Cape and then you can run EVM transactions. EVM is a virtual machine. It's a way of describing what logical transactions can be processed by the system.
01:02:23.950 - 01:02:46.882, Speaker A: Sorry. And I'll tell you where my mix up comes in because I often am talking to ZK roll up projects, and recently one project mentioned that the ZK roll up, what it looks like is just it looks like a DAP. And that has kind of thrown me for a loop that verification. Smart contracts just look like right.
01:02:46.936 - 01:03:34.006, Speaker D: Because you can have roll ups run just through a smart contract, which gets kind of trippy, right. So you can actually have it's bootstrapped on top of the consensus. So you can actually have a roll up that just looks like just run by a smart contract. And that's actually the core of the problem. That because roll ups are somewhat of an afterthought as a way to scale throughput and there isn't this careful integration of roll up with the underlying consensus protocol. You end up in a situation where either you're posting way too much data on chain still, and therefore you're limited in terms of how much you can scale, or you have a half hearted solution to data availability. And so we want to make sure that we're looking at it as an integrated design with certain security goals.
01:03:34.006 - 01:03:55.590, Speaker D: Right. We want to maintain that if the two thirds of the stake. Is honest or incentivized, right? However you think about that, then you maintain these properties of consistency, liveness and data availability and then that's a security property you want to prove of the overall system, including how roll ups interact with consensus as part of the protocol.
01:03:56.090 - 01:04:12.926, Speaker A: On the data availability side though, is the model then the underlying model? Is it similar to these other data availability solutions? Are you thinking about it in the kind of context of Celestia or is it very, very different because what's built on top has different needs?
01:04:13.108 - 01:05:36.658, Speaker D: Based on my understanding of Celestia, it's very different since Celestia is based on basically proofs of retrievability and it could in retrospect look somewhat similar to different ways of having data availability committees. But to give you a peek into how it works, we have a proof of state consensus protocol which is based on randomized committee elections, which is how some of these proof of state consensus protocols work. So concretely, we have integrated the Hot Stuff consensus protocol with a randomized certificate based proof of stake protocol, kind of similar to Algorand. So you can think of it as Algorand but with Hot Stuff as the internal BFT as opposed to the BFT that Algorand uses. And the way that we integrate roll ups is in terms of how committees get randomly elected to actually receive and then serve the raw transaction data versus how committees get elected to just participate in voting on transaction ordering. And at a very high level. The reason why it is possible to scale throughput and still achieve decentralized data availability is that you do not need to elect as large of a committee to basically function as the data availability committee as you do to participate in voting.
01:05:36.658 - 01:06:50.080, Speaker D: And the reason is that when you sample a random committee, you need two thirds post epsilon of that committee to be honest in order to maintain safety. But as long as one of a random committee member is honest, then you preserve data availability, right? And there's other subtleties as well, such as the fact that due to the possibility of an adversary to adaptively corrupt a committee after it becomes known, you actually need to broadcast blocks that are being voted on through gossip. To everyone in the network before the committee becomes known, because otherwise, an adversary could basically bribe or corrupt those committees and then basically get control of their keys and mess up with the safety of the system. Whereas that's not the same concern with data availability. So that's getting a little bit too much into weeds and we do have a white paper that we're writing on this and we'll be publishing soon. But in a nutshell, it really comes down to a careful look at how roll ups interact with a particular consensus protocol and a careful analysis of the desired security properties that you want in order to be able to achieve this.
01:06:51.330 - 01:07:31.478, Speaker E: I will say one thing, which is that there is some similarity with Celestia here in that data availability sampling still does. Sort of like here you're coupling consensus with a proof of kind of like retrievability but the idea that you randomly select some subset of validators to provide the set of transactions in some non corruptible way and making sure that they don't get revealed until after is actually quite similar. So I'm excited for the paper because there's definitely some overlap. And now that there's three main data availability things going on right now, like E two, Celestia and this, it'll be interesting to compare.
01:07:31.574 - 01:07:33.182, Speaker A: Polygon has one too, by the way.
01:07:33.236 - 01:07:43.594, Speaker E: Oh, yeah, sorry, forgot. But I think Polygon is very similar to Celestia. Actually, two is trying to do one thing that's quite different in how they do the sampling.
01:07:43.642 - 01:07:50.674, Speaker A: They told me it was somewhat different, but there's an episode that I think came out last week. As of now, a few weeks before.
01:07:50.712 - 01:07:57.910, Speaker D: This episode, protocols are somewhat similar at a high level and quite different when you look at the underlying details.
01:07:59.370 - 01:08:22.750, Speaker E: Yeah. The unfortunate thing is there's only one actual sort of fully fleshed out data availability paper that exists, which is the celestial one. Everything else is kind of like a very like the e two and e two one in particular is very hazy. A lot of the details are not there, for instance. So you can't really compare them because they haven't written anything up.
01:08:22.820 - 01:08:28.814, Speaker D: Yeah, and we'll have a paper out soon that explains what we're doing and how it distinguishes from what Celestia is doing.
01:08:29.012 - 01:08:48.486, Speaker A: Cool. Do you imagine other projects deploying on Espresso itself or do you actually see it as like a fully fleshed out system with both sides, with the underlying L1 and Cape and that users are supposed to more like deploy on both of those or kind of work in that system?
01:08:48.668 - 01:09:55.050, Speaker B: I think fundamentally, Espresso is it is a layer one, right. And so we view it very much as that open platform. And we hope that people will transport apps that exist on other chains onto Espresso, that people will use it as an open platform for innovation in all kinds of new ways that haven't been possible to date. But at the same time, we're also aware that, again, to go back to just the state of the industry and how much it's matured and how far it's come, it's not good enough, I think, anymore to just put that out there and say, yes, come one, come all, and build something here. We wanted to make sure that we were seeding it with functionality and designing it with intentionality around what those types of things will be. And so that's a bit of context as to how we came up with this architecture at the outset and Cape both as a protocol and something of an end user product at the outset. But certainly we hope that it goes well past even our wildest dreams.
01:09:55.470 - 01:10:00.282, Speaker A: Cool. Well, on that note, I want to say thank you to both of you for coming on the show.
01:10:00.416 - 01:10:02.826, Speaker D: Thank you so much. It's a pleasure to be here.
01:10:02.928 - 01:10:15.246, Speaker A: Yes, and thanks for sharing with us, Espresso. It sees the light of day. I'm very excited to be able to also talk a little bit more openly about it and now share this episode where we got to explore it.
01:10:15.348 - 01:10:16.190, Speaker B: Thanks so much.
01:10:16.260 - 01:10:17.342, Speaker D: Yeah, great. Thanks.
01:10:17.396 - 01:10:22.110, Speaker E: It was great chatting about this and actually getting things out in the open. Definitely.
01:10:22.260 - 01:10:29.730, Speaker A: I want to say thank you to the podcast producer, Tanya, the podcast editor, Henrik, and to our listeners. Thanks for listening.
