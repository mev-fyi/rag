00:00:05.450 - 00:00:21.280, Speaker A: Welcome to Zero Knowledge. I'm your host, Anna Rose. In this podcast, we will be exploring the latest in zero knowledge research and the decentralized web, as well as new paradigms that promise to change the way we interact and transact online.
00:00:25.210 - 00:00:25.960, Speaker B: You.
00:00:28.250 - 00:01:14.546, Speaker A: This week, Tarun and I chat with Ye Cheng and Hai Chen. Chen from scroll. We discuss the founding of the scroll project, the problems they aim to solve with their native ZKE EVM layer two solution why scaling Ethereum is important, the philosophy of their ZKE EVM, and how this differs from other proposals, as well as potential use cases or products that could benefit from this system. We wrap up with an exploration of how Dapps and products on top of such a ZkeVm may interact with the main chain and with other applications. But before we start in, I want to let you know that the ZK podcast crew is growing. We are currently taking on a number of new projects, and we're looking to hire an additional content producer to join us. There's a job posting for this content producer over on the ZK Jobs board.
00:01:14.546 - 00:01:51.578, Speaker A: Just a reminder, the ZK Jobs board is a spot where you can find jobs from all sorts of top ZK teams, not only the podcast, so you might want to check that out. Either way, for this role of content producer, the job requires you to have at least two years of experience working on regular content production. Ideally, you would be organized, good at project management, and somewhat familiar with the field. There's no need to be an expert on ZK, but some familiarity with our community would be ideal. If you or someone you know fits the bill, please apply. You can find the links in the description and I hope to hear from you. Now I want to invite Tanya to share a little bit about this week's sponsor.
00:01:51.754 - 00:02:44.610, Speaker C: Today's episode is sponsored by Anoma. Anoma is a set of protocols that enable self sovereign coordination. Their unique architecture facilitates sufficiently the simplest forms of economic coordination, such as two parties transferring an asset to each other, as well as more sophisticated ones like an asset agnostic bartering system involving multiple parties without direct coincidence of wants, or even more complex ones such as n party collective commitments to solve multipolar traps where any interaction can be performed with adjustable zero knowledge privacy. Anoma's first fractal instance, Nomada, is planned for later in 2022, and it focuses on enabling shielded transfers for any assets with a few second transaction latency and near zero fees. Visit Anoma net for more information. That's Anoma net, so thanks again, Anoma. Now here is Anna and Tarun's interview with scroll.
00:02:49.030 - 00:02:57.182, Speaker A: So this week, Tarun and I are here with two members of the founding team of scroll. Welcome, Ye Chang and hi Chen. Chen.
00:02:57.326 - 00:02:59.522, Speaker D: Hi Anna. Thanks for having us.
00:02:59.656 - 00:03:03.014, Speaker B: Hi, Anna and Tarun. Glad to be here.
00:03:03.132 - 00:03:03.414, Speaker D: Great.
00:03:03.452 - 00:03:05.014, Speaker E: Definitely excited to have you guys on.
00:03:05.132 - 00:03:30.238, Speaker A: Just a quick disclosure, both Tarun and I were both investors in Skrull kind of early. We invested a while back, myself through the ZK validator and Tarun through robot ventures. But I know that there's know a lot has changed since we did those early meetings, and it would be really great for us to learn a little bit about what's happened since. Let's start off with a little bit of an intro to each of you and to find out how you arrived on this problem or what you were working on.
00:03:30.404 - 00:04:01.094, Speaker D: So, my name is Ye. I'm the co founder of scroll. More specifically, I work on ZK Research. So I work on hardware acceleration for the honor proof and protocol design behind ZK algorithm. So before starting scroll, my background is more in academic. I started to work on the honor proof four years ago, back in 2018, where the biggest bottleneck of using the honor proof in practice is a large proving overhead. And I was doing research in the area of computer architecture and combining with my hardware knowledge.
00:04:01.094 - 00:04:42.690, Speaker D: I started my first research on hardware acceleration for the proof. So the idea is just using IPGO ASIC to make pooling faster and having more hands on experience from the hardware project, I got much better of ZK. I was very lucky to work with Professor Andrew Miller at UIUC and dive deeper into the theoretical side of ZK. It was exactly during that time, the second half of 2019, there were a bunch of new ZK protocols coming out, including sonic plank, Marlin and Halo based on the polynomial commitment. And I was foiling the literature very closely. I become really addicted to those polynomials. It's much more interesting than just hardware.
00:04:42.690 - 00:05:39.746, Speaker D: And so the mass construction behind Licky protocol is just very beautiful and elegant. So besides the hardware and theoretical research, I have also done some application level stuff with my advisor Mike and other crap readers at NYU. So that's my experience before starting scroll. So the chance to start scroll dates back to 2020, when there is an explosion in Defi and I met Sandy and Hai Chen, the two other cofounders. Through some mutual friends in the field of competitive math and also east community, we find that Ethereum needs a scaling solution, but all the existing Zikiro apps have the problem of being too application specific and also the generic proof in a centralized way. Which is very hard to scale. So we want to solve this problem and build a new, totally new layer two with a more general EVM support and a decentralized prover.
00:05:39.746 - 00:05:49.660, Speaker D: So it's also exciting to see some of the research outcome come into practice. Yeah, so that's basically the high level overview of how screw started.
00:05:50.110 - 00:05:53.306, Speaker A: Cool. Hi Chen, what about you? Where did you start?
00:05:53.408 - 00:06:45.098, Speaker B: So previously I didn't like working the web three, so I previously liked working in web two for a few years. I worked in the Amazon AWS for a few years and then we are focusing on working in some AI compiler to optimize the model development. So actually about one and a half year or two years ago, I met with ye, like as ye mentioned, through some common friends, and at that time he was finishing up his hardware accelerator papers. And then that's where he got me intro into this xeonological proof and then teach me a lot about the mass and then the protocol behind that. At that time I was very fascinated by the xeonological proof. I think it's very magic collector, which you can just do like a very small computation to verify some very large computation. So I think I previously know a little bit about that, but I didn't know the details.
00:06:45.098 - 00:07:18.454, Speaker B: After learning more details are getting more and more attracted with that. And then we're kind of discussing what we can do using this zero notch proof. And then it turns out like the ZK Rob and then building the ZK EVM, it's kind of one of the ways you should use that powerful tool in the math. And then before that, before I joining the Amazon, I was getting my phd at the University of Washington. But my background is more in the system optimization. I'm building distributed system and then building the compiler optimization stuff. And then.
00:07:18.454 - 00:07:20.738, Speaker B: Yeah, that's like how I got started.
00:07:20.924 - 00:07:31.466, Speaker A: Nice. It sounds like your background was very well suited to later do this. When you were doing distributed system stuff though, were you touching blockchain or was it like general distributed systems?
00:07:31.578 - 00:08:06.422, Speaker B: It's more general distributed systems, like building some distributed serving system for the machine learning workload. But at that time, during the PhD, we also kind of did some explore some of the blockchains and then the crypto stuff. At that time we were mostly looking at the bitcoin and thinking how we can extend the bitcoin a little bit. And then we were even working on some the prototypes, trying to see we can build some more object oriented language for new blockchains. But I guess that turns out we didn't work as full time during the PhD time.
00:08:06.556 - 00:08:12.810, Speaker A: Got it. Let's go back to the beginning then of scroll the company. So there's three co founders, I guess.
00:08:12.960 - 00:08:13.514, Speaker D: Yes.
00:08:13.632 - 00:08:24.910, Speaker A: Okay. And so you'd all met and you decided to take on a project you wanted to work with zero knowledge proofs. What part of the ecosystem were you in when you decided to sort of start working on a ZKe EVM?
00:08:25.250 - 00:08:58.886, Speaker D: I think we are mostly part of the Ethereum community. For example, my background is more in the ZK stuff, but I have read a lot of research from Ethereum community and I know it's a very creative community. We are pretty value aligned with Ethereum community, this open source and decentralized, and we are pretty value with this. And also our mutual friend are also in the east community and thinking Ethereum is the best settlement layer and especially in e scaling. So that's the reason why we choose to build on top of. Yeah.
00:08:59.068 - 00:09:16.160, Speaker A: Were you connected to the like, did you work closely with them? And the reason I'm asking this is, and we're going to get to this a little bit, a little while later, but there were designs and ideas around ZKevM coming out of there. So that's why I'm curious if that's where some of your original thoughts on it come.
00:09:17.170 - 00:09:55.622, Speaker D: That's a, that's a good question. So it's very interesting. It's also a nice coincidence. Like I was chatting with Barry Whitehead from ECM foundation and seeking help for the first review of our scroll's first version, and it happens that he is also looking to the general purpose problem. And so the collaboration between us and the PSE team, they used to be called apply. The KP team in Etherean foundation happens very organically because they want to build this and we want to build this. At that time, everything we have is just scratch and we are pretty value aligned.
00:09:55.622 - 00:10:17.346, Speaker D: And we also do believe that developing in the open source way, owned by the community, is the best way to really make it. And we are very excited to cobbu this solution together to literally scale ECM. And we are also proud that in this community, nearly half of the contributors are from our team, and we have been co building this effort for around one year now.
00:10:17.528 - 00:10:35.186, Speaker A: Wow, this is super interesting though, that you were actually tackling the problem separately. Met Barry. Turns out they were trying to tackle the same problem. What inspired the problem in the first place? Maybe here we can start to break down what even ZKE EVM means. Why Zke EvM?
00:10:35.378 - 00:11:23.798, Speaker D: I think it's twofold, like one is that why we have to build toward this EVM equivalence approach, because there are some other teams building some ZKVM and some other solutions. So firstly from us, EVM equivalence really matters because it's secure, because you can inherit the security property from EVM model which we all defined years ago and has been test of time, and we have exactly the same behavior as EVM to be very secure. And secondly, that EVM has a very strong network effect. It has the largest developer community and also numerous steps built on that. And also there are a lot of infrastructures and toolings around. We can support all of them seamlessly without any delegate labor. And also like Ethereum has very active research community.
00:11:23.798 - 00:12:36.250, Speaker D: As I mentioned previously, they are proposing a lot of innovations around many eips to improve Ethereum. And if you build something which is EVM equivalent, you can adopt those innovations ahead of time because you have the same environment. And also comparing against there are some other solutions to go into the way of leaky VM, but I think in our case in term of layer two, leaky VM is the best way to really scale ethereum, because from our side the highest priority is not looking for a new virtual machine to support more complete computation, but instead the urgency is migrating existing depths from layer one to layer two. So we think the best way to do that is provide the environment with a seamless migration experience. And I know there are also some complaints about EVM versus other virtual machines, but we believe that EVM is still the best practice for smart contract execution because it has been years of exploration. And I think Vitalik also has some article talking about the design trade off behind EVM and why they would still choose the EVM path. Like even if they know there are a lot of other virtual machines.
00:12:36.250 - 00:13:08.358, Speaker D: And we do believe that some other Ziki VM can be useful in some applications. And we have a research team actively looking into this direction as our next step. But we are thinking about how to add more advanced features in Ziki VM as an extension to our Ziki VM. Also, another technical difference is that building a Ziki VM is much harder than Ziki VM, but we decided to take that using more advanced crypto and hardware optimization and to provide the best developer experience.
00:13:08.524 - 00:14:00.790, Speaker E: I think maybe one thing that would be worth talking through is comparison of the different EVM equivalence methods and sort of what it means to be a fully equivalent CKVM. I think there's obviously a lot of confusion around what qualifies as being fully EVM compatible versus strictly solidity, bytecom compatible versus sort of like the proof generation process is separate from the execution trace directly, stuff like that. So maybe let's walk through what scroll means by EVM equivalents and then how that compares to say some of the other protocols that are talking about EVM equivalents like say matterlabs or other places. Because I do think it can be quite confusing when you first hear ZkeVm because a lot of people kind of were, oh, like someone else already said they were doing that, but then it turns out to have some limitations.
00:14:01.530 - 00:15:00.394, Speaker D: So technically speaking, ZkeVM is simulating the behavior of EVM in circuits. And so in our definition, and also the Ziki Vm we are building with the open source community, we are targeting at bytecode level compatibility, which means the Ziki VM should follow the definition of the ECM virtual machine yellow paper. They define some opcodes. There are lots of definitions in that one, and we think if you are really building a Zik EVM using the term EVM, you should follow their standard and it can provide exactly the same environment as the current EVM on Ethereum. And as for some comparison, I think in term of the technology stack and also the compatibility side, we are pretty close to polygon Hermit. They are incredible team led by Jordy and building something also fantastic. Both of us are targeting at bid code level compatibility and have a very similar architecture, but there are some technical differences from the implementation side.
00:15:00.394 - 00:15:45.622, Speaker D: So we are more close to the native Ethereum implementation. For example, we are directly using a fork of native gas to produce our layer. Two block gas is short for Go Ethereum. It's the most robust and well known Ethereum node implementation and has been used across a lot of places. We also designed some subsurcase to prove for each opcode in gas execution trace, and it's easier to verify that the circuit has exactly the same behavior as native Ethereum. But for polygon Hermes, I think they are rewriting each EVM opcode using a new assembly language and a general proof for their underlying state machine. I think it's more from an implementation side instead of because both of us are targeting at bidcode level compatibility.
00:15:45.622 - 00:16:23.542, Speaker D: I think their approach might need more work to build everything from scratch. As for the comparison between Starquare and Leaky sync, it's more different because as I mentioned previously, we are targeting an EVM equivalents and to achieve bytecode level compatible. But for them, they are building their own virtual machine, which is different from ECM virtual machine and also building a compiler to compel solidity to this underlying VM. So their vm has a totally new set of opcodes, and also they need to build on top of a compiler to achieve language compatible. So I think that's the biggest difference there is, actually.
00:16:23.596 - 00:16:45.434, Speaker A: Like last year I did an episode with Hermes at the time, now Polygon Hermes, where we did do kind of a look deep into Zkevm and the way that they were building it, I remember that they did highlight that each opcode would be equivalent. How is it working for you if it isn't? Like you're not rewriting each opcode. So how do you make it actually work without having to compile?
00:16:45.562 - 00:17:15.474, Speaker D: I think for their approach for each opcode, they will have some relatively small like state machine or some rewrite union assembly code. So what we do is that for each opcode, we will directly build some customized circuit for this opcode. So it's one to one mapping. Like for add, we directly build some circuits for add and instead of running ad as a state machine. And so that's some technical implementation difference. But I think both of us can achieve all the opcodes level compatible.
00:17:15.602 - 00:18:07.026, Speaker B: So the way we're building the ZKE EVM is that we can take each opcode and then write a gadget like a small component, which inside the ZKE circuit that you write a gadget for each opcode and then simulate the behavior according to the implementation, the actual implementation, or from the specs. So you can say like an add opcode. For example, in the EVM you need to pop two values from the spec and then do the addition within the 256 bit range, and then you do push back the results to that. So in the circuit you need to do the exact same things. You need to verify that the two operands to the add offcode is actually popped from the stack. And then you write a constraint to say like the results is actually the addition of these two operands, and then you push back that value back to the stack.
00:18:07.138 - 00:18:26.702, Speaker E: One thing that's important to also remember is that the proof of actually the execution trace versus sort of like something that does a transpile step and you really are proving execution of the transpile thing, not the actual EVM, can be quite different or have a bigger attack surface in some ways.
00:18:26.836 - 00:18:28.314, Speaker A: Why attack surface?
00:18:28.442 - 00:19:38.702, Speaker E: Well, they're more moving parts, right? So let's say I have another language that's compiling to EVM bytecode, but I only prove what happens in the other language's execution environment before it gets converted. There's this problem of like, well, is my translation perfect? And it's not necessarily that there doesn't exist a perfect translation, right? These things are still kind of bounded Turing machines and whatever, but it's more that implementing one that catches every single edge case is actually extremely difficult. An example of this is that in the early days of iPhones and sort of Android phones, there was this huge movement to have people make these transpilers that were like you could just write an app in Android and compile it to iOS and vice versa. And they had way more security problems than the native code in general. And so there is sort of that. From a security standpoint, I do think it is also worth keeping in mind the fewer moving parts, the more you can be confident in the code that's written.
00:19:38.846 - 00:19:39.490, Speaker D: Yeah, exactly.
00:19:39.560 - 00:20:05.394, Speaker B: I think that one importance of the ZKE EVM is that it doesn't require any additional infrastructure to make your smart code, like the smart contract code, to be compatible with the ECM. And also you make sure that your smart contracts execute exactly on the EVM compatible like equivalent environment so that you can make sure all of your code should be behaved as what you expected from the specs of the ECM.
00:20:05.542 - 00:20:22.450, Speaker A: In that case though, why wouldn't everyone do like an EVM? With EVM opcodes, what are the trade offs that you have to face? Is it harder to build? Is it longer to process? Yeah, I'm just curious, why doesn't everyone do that? Is there a faster path if you don't?
00:20:22.790 - 00:21:01.034, Speaker D: I think as mentioned, Ziggy EVM is much harder to build because EVM is a stack based virtual machine. But for the owner proof, it can naturally support a register based virtual machine. So just for this step, it will already add a lot of overhead. And also for EVM, there are a lot of ZKM friendly opcodes like ShA three, and also inefficient data structures. For example, like EVM, word size is not on prime field, it's on like 256 bit. So you need ring check everywhere which just explode. Your circuit adds a very huge overhead to ZK.
00:21:01.034 - 00:21:45.038, Speaker D: And especially for this data storage you need merco potential tree, which is another huge overhead. But for ZKVM you get more flexibility because you can have your own defined instruction set and you can build that to be more Ziki friendly and have a much smaller proving overhead. Ziki EVM was thought to be impossible in the past. I think it's just with recent breakthroughs like advanced circuit optimization, hardware acceleration, and some recursive proof, we can massively improve the performance of prover and finally make that feasible, because even if it has a very large know, people still like it, because it's still very developer friendly. But there are some technical challenges in the past.
00:21:45.204 - 00:21:58.690, Speaker E: Yeah, actually maybe could we walk through, I don't know, a year or two ago, when people thought a ZKVM was not possible? What's sort of the chronology and timeline of the events that changed people's minds?
00:21:59.030 - 00:22:53.694, Speaker D: I think first is from the circuit emulation part, because first in the past we just had r one CSR circuit emulation, and which it's hard to build something which is more customized. And also especially for EVM, like you have 256 beat, it needs very specialized gadgets to prove for range. And I think with some breakthrough, like Aztec proposed Pillowcup, which is a nice way to prove for some zkon friendly primitive. You just need to prove that something is within a table, and this belonging relationship, and that's it. So I think that's very important primitive to deal with those leaky, unfriendly opcodes at the circuitation part. And also another important thing, you can also use this lookup to link different components. For example, in a state circuit, you prove that your read and write are correct over the same elements.
00:22:53.694 - 00:23:26.714, Speaker D: And in another circuit, you prove that your execution is correct. And then you need to prove that the elements in different circuits are the same. So you need lookup to prove that this belonging relationship and plookup can provide a very efficient way to do this. And I think both us and polygon, Hermione, even some team building ZKVM, are using the same approach to link the execution and also the storage. I think that's the biggest improvement. And on the back end side, I think there is hardware acceleration. It's just drawing more and more people's attention.
00:23:26.714 - 00:24:18.206, Speaker D: It just happens simultaneously. We find not only from the front end circuit side, your pruning is still very large and hardware, because even if your proving takes very long time, it's highly parallel, because it only contains some group operation or ability curve, and also ffts. So both two parts are highly parallel. And we have done a lot of previous research around GPU acceleration and also ASIC acceleration. So we exactly know using such kind of technology, you can improve the performance for maybe two order of magnitude to really make that feasible. So I think it just happens. And before that, people don't know this can be leveraged here, because I think internal proof is very hard for you to outsource this proving to a third party, because ZK is mostly in the past, used to be some privacy preserving application.
00:24:18.206 - 00:25:05.722, Speaker D: So you can't give your secret key and your secret information to a hardware provider. But in Zikirap, there is no ZK, there is just validity proof needed. So you can easily outsource this proof generation to a hardware provider. They can run GPU, cluster, IPJ, data center, or even produce ASIC. So I think that's a huge opportunity for this pool to cut in and solve the efficiency problem. And finally, the recursive proof is that I think also using the sum of the optimization, you can generate proof for proofs efficiently, and especially for some non native field operation, you can aggregate that. I think that's also important to reduce the overall verification cost, because the KVM circuits composed of many subsurcates, and for each subsurface you will result in proof.
00:25:05.722 - 00:25:22.100, Speaker D: If you verify all the proof on chain, it will be a very large overhead and you need to aggregate them to make your final proof smaller. I think that's also important, but that's also built on top of the optimization. So I think that's the biggest thing in making this possible.
00:25:22.550 - 00:25:49.690, Speaker A: So you just mentioned that it's circuits and then there's subsurcuits. Would you say, is this still in the realm of a snark that you're using at this core? Or is this some modified semi stark like snark? I mean, I know when I was talking to Jordy from Hermes, there was also stark like techniques used with snarks. So you didn't really know what the word for this thing was. Frank and snark Stark or something?
00:25:49.760 - 00:26:44.698, Speaker D: Yeah, that's a very good question. I think both Snark and Stark are just some Ziki protocol. And if you are describing a Ziki protocol, a better way is just to describe what kind of circuit optimization you are using and what the polynomial commitment you are using, because you can have different combinations for those parts. For Stark, it's usually like aar or air as their circuit optimization and use friar as their polynomial commitment. But for us, we are using both sides on the circle. We are using plunkish authorization implemented in halo two by the Zcash team, and in the back end, polynomial commitment. So the initial version in halo two, they use some bulletproof styled inner product argument, but we replace that with KDG because although the previous one implementing Halo two had many nice properties, but we replace that because we want to verify our proof on Ethereum.
00:26:44.698 - 00:27:06.194, Speaker D: And the pasta curve is not supported directly on Ethereum. So that's the reason why we choose to change that to KZG to make our proof more efficiently verified on Ethereum. So basically it's pluncation optimization plus KDG as polynomial commitment. And for both the EVM circuits and the aggregation circuit, we are not using any stark related technology.
00:27:06.392 - 00:27:08.930, Speaker A: You said plonky or plonky two.
00:27:09.080 - 00:27:14.710, Speaker D: It's plonkish arithmetic. It's a new name proposed by. Yeah.
00:27:14.780 - 00:27:18.630, Speaker A: Wait, is this a different one? This is plonkish. No.
00:27:18.780 - 00:27:29.370, Speaker D: Yes, plonkish. I think it's a word proposed by Dara on Twitter. There are some words to describe what kind of arithmetic you can use. It's called.
00:27:29.440 - 00:27:37.182, Speaker A: Oh, it's a kind of. Okay, okay. But it's not plonk specifically. Or is Planck at the heart of what you're doing as well?
00:27:37.316 - 00:28:13.978, Speaker D: So basically the literature look like this. So basically there is Planck, which only support the addition and the multiplication gate. And then there is plo cup, which is a separate primitive to prove this lookup relationship. But how do you combine this pu cup with the Planck and the ad tech people proposed some semantics called turboplunk to combine two different primitives in one way. And then I think zcash team use a more flexible way to represent know different relationships called ultraplunk. And then they renamed this ultraplunk to plankish optimization. So, yeah, that's how this name.
00:28:14.144 - 00:28:30.766, Speaker A: Oh my gosh. I actually want to do a whole session at some point which just maps all the derivations of the word plonk and what they mean and how they work together. But plunkish. Okay, that was one part. But then you also said halo. And I wanted to just double check if you were saying techniques from halo or halo two.
00:28:30.868 - 00:28:32.734, Speaker D: Yeah, we are using halo two.
00:28:32.852 - 00:28:38.320, Speaker A: Okay. Except that you're using KCG instead of the pasta curve that they're using.
00:28:38.710 - 00:28:39.218, Speaker D: Yes.
00:28:39.304 - 00:28:48.118, Speaker A: Okay. And is there any other variations on this? Like, is there any other additions or changes that you've incorporated into your model?
00:28:48.284 - 00:29:03.178, Speaker D: Yeah, I think that's mostly about the ideas both in ZKVM. So in ZkVM we have multiple subsurcates and these lookup relationships. And in aggregation circuit we handle some non native field operations. And generally proof for that.
00:29:03.344 - 00:29:38.680, Speaker B: I think for the hello two, we also extended their API a little bit. So to be making more flexible use case in the ZKE EVM during our building, we found like there's some limitation from the API and then. So we modify a little bit. And also since we also changed their curve, so we also changed the recursive scheme in our version of the halo two. But we also plan to actually discuss with the zcash team to see how to upstream all of the features. And then I think they're also interested in our way of the hacking the halo two, adding those kind of the new changes.
00:29:39.290 - 00:29:47.430, Speaker A: Cool. Isn't there sort of another camp of thought? I'm kind of just curious, what made you choose to work on this particular direction?
00:29:48.170 - 00:30:18.930, Speaker D: Yeah, I think when we started, we have this discussion with Barry and also the ECM foundations team, and I think because we definitely need the customer gate support and also the lookup argument support. And at that time, Hellotwe is the only implementation that supports both two primitives in the open source way and with a nice license. And also the libraries built by the cache team also usually has a very high security guarantee.
00:30:20.150 - 00:30:24.082, Speaker A: What do you actually use to build this stuff? Like, what languages are you using?
00:30:24.216 - 00:30:27.298, Speaker D: So we are using rust because hello two is written in rust.
00:30:27.394 - 00:30:27.798, Speaker A: Okay.
00:30:27.884 - 00:30:30.946, Speaker D: We need to follow their APIs to write circuits.
00:30:31.058 - 00:30:52.410, Speaker A: And I mean, there's a lot of teams, a lot of ZK teams which are introducing native dsls. Since you are working with EVM, I'm assuming the language to build on your ZKE EVM will be solidity. But do you also need anyone to interact with the underlying ZK circuits in a special way that maybe solidity can't?
00:30:52.830 - 00:31:23.670, Speaker D: I think for now we will only expose some very high level EVM API to the developers. They don't need to touch any low level details. But in the future, if you want to add more advanced features like support something which EVM can't support in our Zik EVM, we might need to hack that a bit and they can add some subsurfaces to our current system and to interact with some solidity contract. And that's one potential direction. But that's not what we are building. For now. What we are building is just the same environment for you to migrate.
00:31:24.010 - 00:31:51.086, Speaker A: Is there a delay? Like if you wanted to move from the l two to the l one with a ZK EVM, is it slower or faster? Or is there any sort of change versus something like optimistic roll up, or the more non ZK EVM ZK roll ups? I'm just trying to picture if these steps of creating the snark in that particular way, if there's any sort of time lag on that.
00:31:51.188 - 00:32:51.490, Speaker D: So in general, Zikirap has a roughly similar delay for withdrawal transaction only. So for those transactions happening within layer two, you get some instant confirmation, because you have some centralized sequencer and as far as your transaction included, you can confirm. But for withdrawing from layer two to layer one, Zikawap usually takes maybe 1 hour or minutes, depending on your TPS. If your TPS is higher, like more transactions, you can just upload your proof faster, because one proof can be amortized over a lot of transactions. So it also depends on how many transactions you have in your system for ZKevm, because your proving overhead is larger. But another interesting design in our current system is that we have a decentralized prover system, which means we will generate multiple blocks in parallel, and for each block there will be some proverb to take this block and generate proof so all those blocks can be generated proof in parallel, and we still get a high throughput amortized. And proving time is not a big issue on our platform.
00:32:51.490 - 00:33:19.338, Speaker D: So the delay will be similar to other Ziko apps, but comparing with optimistic grow up, they take around like seven days to get the finality of withdrawal transaction. They are based on some game theory and economic assumptions, so they basically need an honest node to reexecute the transactions and challenge if wrong. So they need to make sure that at least one honest node capture this. So they need seven days for withdrawal from layer two to layer one.
00:33:19.424 - 00:33:51.458, Speaker A: I want to kind of go a little deeper on this. You just mentioned the decentralized prover. I actually had a question here about the agent that runs the operator of the ZK roll up. I always am trying to figure out what that is in every system. So often it'll be called like a sequencer in certain things, like the starkware setup, it's like they have a committee. I'm just curious, in your case, the decentralized prover, is that acting as the agent that bridges over to the main chain?
00:33:51.634 - 00:34:19.082, Speaker D: Yeah, I think currently we have two roles. One is called sequencer and the other is called roller. So sequencer is for tracking transactions, receiving transactions from a user and a generator. Layer two block. And then after you generate this block, roller, like in our system, is acting as a prover. So it's actually generating a proof. And the sequencer will send this block, or some witness or execution trace to roller and this roller will generate proof and send back the proof.
00:34:19.082 - 00:34:40.150, Speaker D: Because currently the sequencer is centralized, but the proof is decentralized because we can leverage the computation power from the network and then sequencer wirefy the proof, maybe aggregate the proof and submit the proof on chain. So it's still a centralized sequencer to generate block, but it's decentralized proof to generate proof for computation power. Yeah.
00:34:40.220 - 00:34:50.518, Speaker E: Quick question, does the sequencer have to provide some data availability proof also or just strictly a proof of execution?
00:34:50.694 - 00:35:23.490, Speaker B: Yeah, actually. So the sequencer will provide the data availability. So I think you can mimic this in a way of the proposal and a builder. So which the sequencer now is the proposer? It generates the blocks and then also sequence the transactions and generate a block and then provide the data availability to the layer one chains. And then it will send the transaction data or the trace to the roller, which roller kind of generating the validity proof and upload it to the layer one. So to see all of the transaction within that batch.
00:35:23.990 - 00:35:44.522, Speaker E: Right. Okay wait, so the roller actually publishes the proof directly, the sequencer only kind of gives them the transactions, both the data that's sampled as well as the execution call sites. But the roller posts the proof on chain themselves or am I getting the flow wrong of that?
00:35:44.576 - 00:36:05.940, Speaker B: Yeah, I think that's the basic idea. So actually there will be some discussion in that way. The sequencer just send the data availability roller, sends out the validity proof, makes the system very easier to be decentralized in both positions, like in both roles. I think that's very natural way to do that.
00:36:07.030 - 00:36:19.378, Speaker E: Right. So actually one question is if I'm a user of the roll up, do I send my transactions to both the sequencer and the roller or do I send it to only the sequencer?
00:36:19.554 - 00:36:21.218, Speaker B: Only to the sequencer.
00:36:21.394 - 00:36:23.510, Speaker E: Okay cool, that makes sense.
00:36:23.580 - 00:36:27.640, Speaker A: Do you think you will decentralize the sequencer or not?
00:36:28.650 - 00:36:37.354, Speaker B: Yes, we will decentralize the sequencer. So make sector, the whole layer two becomes the trust list. So you don't need to trust any centralized entity and also will be more.
00:36:37.392 - 00:36:50.160, Speaker A: Censorship resistant on the roll up side. On the l two, is there also any sort of validator of the actual underlying data or is that the prover, is the prover sort of acting like that?
00:36:50.770 - 00:37:09.974, Speaker B: Yeah, I think you can think of the prover is acting like a validator, but you don't need to have every of the roller, or we just call it rollers to be verify all of them to participate into validate a block. You just only have a few as a small committee to verify that and then someone can roll up the validity proof to them.
00:37:10.092 - 00:37:44.290, Speaker E: Quick question on the point of decentralizing the sequencer is the long term idea to basically have a pool of machines that can be both rollers and sequencers. Like people can sort of stake in some way and then they get randomly selected to one of the two batches such that yes, there's a single sequencer for one slot, but you have some rotation. Or do you view that as too complicated? I'm just kind of curious what are sort of the different ways you're thinking of decentralizing the sequencer?
00:37:45.030 - 00:38:18.650, Speaker B: I think you cannot have the people randomly selected for the sequencer or become approver or roller. So this is because it has different hardware requirements for different positions. Like the sequencer, you just need to generate the block fast enough and to keep up the throughput. But for the roller, sometimes you may need some accelerators like GPU, FPG or like ASIC to be really able to generate the proof fast enough for that. So actually there's different requirements for different roles. So kind of like just only you can stake for become a sequencer, stick for become a roller.
00:38:19.630 - 00:39:23.038, Speaker E: Yeah, I guess the reason I'm asking is more from the perspective of like, it is likely that to some extent you're going to have quite a huge overlap in the actual entities that are running both of these. Right? Like someone who's already running a roller is already in a data center likely, and could easily run a sequencer node as well. So I guess the main question is how does that sort of change how you think about the sort of allocation to these different parties? And do you think it's like something where you're fine with the election of a sequencer being sort of not unknown? Because at this point we don't really have single secret leader election. There's sort of like some. I mean, if you want to implement the fhe needed for it, then I think you can do it. But I think we're still a little far away from that. So how much do you think that matters here? And sort of like, how are you thinking about this fact that the set of sequencers and the set of rollers will probably have pretty high overlap, at least to start.
00:39:23.124 - 00:39:23.374, Speaker D: Yeah.
00:39:23.412 - 00:39:58.906, Speaker B: Expect to be more rollers than the sequencers because in the sequencer we don't really need to. We just need to have hand for a few number of the sequences to be censorship resistant. But for the rollers you need to keep up all of the throughput. It depends on how fast you can generate a proof and then what's the tps you want to support. So as like ye mentioned before, we have multiple blocks that can be generated in parallel. So that means you need more rollers to generating the proofs for those blocks. So expect there'll be like a ratio we'll keep, we want to keep and then maintain an example.
00:39:58.906 - 00:40:09.050, Speaker B: It's like the ten to one or it depends on the proof generation cost. So that would be like keeping some ratio to have the more rollers to generate the proof in parallel.
00:40:09.210 - 00:40:22.930, Speaker E: How are you thinking about block size as a function of number of rollers that are registered? Do you view this as like a fixed block size type of thing or do you view it as elastic as a function of the capacity of the number of rollers?
00:40:23.430 - 00:40:41.574, Speaker B: I think that it's still a fixed sized block, so it doesn't like the effect at the farm for how many rollers you need to support. So usually the block size is limited by how many steps you can include in the circuit. And also you can translate that into some gas limit you have for the layer two block.
00:40:41.702 - 00:41:14.046, Speaker D: Yeah I can add maybe slightly more because we are using an EVM circuit. So the largest block size you can support actually depends on transaction. For example, you've got some very complicated maybe flash loan transaction. It needs to involve several contract loading and a lot of other opcodes. So this will make your trace, your execution trace longer. So it basically depends on how complicated your transaction have and how many steps need to be included in EVM. And that depends on the largest capacity of a block instead of just a number of transactions.
00:41:14.046 - 00:41:22.630, Speaker D: Because different transactions differ a lot. Yeah, it depends on how many steps you have for this execution on UVM.
00:41:23.370 - 00:42:35.710, Speaker E: Right. So the reason I was asking this is I was like reading the starkware fee model, which has sort of like a thing that it's not as aggressive as EIP 1559, but it does adjust sort of the block size to sort of the number of steps. And they have a notion of step which I'm not sure if that notion is like, it's not as clean as say the EVM notion of a transaction as far as I can tell. But I was just more curious, like in ethereum main net, right? We do have variable size blocks now, right? In the sense of 1559 does sort of force that. So I'm kind of curious, does that impact how provers have to interact with these systems? Because I could imagine something where if the block size goes up, then some provers stop proving because their relative economic share of the network went down. If the block size goes down, maybe it encourages more people. The economics of these types of things I think is very, as we are seeing with starkware, because they've had some interesting fee market kind of things that have been happening in the last maybe like two weeks.
00:42:35.710 - 00:42:56.610, Speaker E: I was just kind of curious, where do you see this evolving, do you see it being something where the economics of these things are static or the economics are kind of like changing as a function of usage? Obviously, it's far out right now, but I think it will be something that will kind of impact how people think about writing code for that they run on scroll.
00:42:56.950 - 00:43:31.418, Speaker B: Yeah, actually, I think that's a very interesting question. I think that we're also making like, they're doing more research of economic models, like how to separate these two rows and then how to keep both rows incentivized. Yeah, I think for sure the block size, I think will be dynamic. But mentioning that the circuit size actually is fixed. So you can have a prefixed circuit size, which, you know, how many upcodes, how many steps you can fit into the circuit. So that's the limiting factor for that. But in terms of economic models, I think we're doing more research to see how to keep incentivized.
00:43:31.418 - 00:43:43.986, Speaker B: You can keep some of the base fee, then you can separate base fee and me values, for example, to use that to reward different positions, different roles. But I think a lot of things, like currently still under some research.
00:43:44.168 - 00:43:57.400, Speaker E: Yeah, I mean, I think it's interesting that by not being the first roll up, you can learn a lot from, say, what's happening on sharkware in the fee market, and it will be kind of interesting.
00:43:58.250 - 00:44:13.454, Speaker B: Yeah, totally agree. We're also kind of learning a lot of things from the existing ones and then see to avoid some of the mistakes to make and then trying to make things work more smoothly here in the scroll, I guess.
00:44:13.492 - 00:44:46.678, Speaker E: One other thing. We've talked a little bit about how hardware acceleration was sort of like a key facet to getting CKVM to work. And hardware can mean commodity hardware, like fpgas or gpus. It could mean specialized hardware, like asics. How do you view that landscape evolving over, say, the next one to two years? And do you think that at launch, when scroll launches, you'll have fpgas ready or gpus ready, or which kind of provers do you think will be there on launch and when you're live?
00:44:46.844 - 00:45:56.826, Speaker D: Yeah, that's a very good question. So basically, we are internally developing a very fast GPU solution because we are a software company. We have a lot of, like, we hire some GPU programmers, and also we started this research for GPU acceleration around like one year ago. So it's basically when we are online, we will definitely have our GPU Pro already to support, to provide very strong computation power already to support a very high tps. So I think in terms of the comparison between GPU, AC and PJ, I think GPU will be the first go to market and they first being practical to be used, and then PJ can win GPU not in term of speed, but in term of the energy consumption at the same speed up like IPG can be more energy efficient. But I think it needs a lot of dedicated work to make IPG really better than GPU, because we have previous experience working with both IPJ, ASIC and GPU. So we know pretty clear that although you can build some customized units using IPG and ASIC, but GPU still, especially our version, we can be ten times faster than the best open source GPU implementation.
00:45:56.826 - 00:46:47.470, Speaker D: So it's significantly faster, and it's very hard to build a new version which can beat our current GPU version. But I know there are some efforts behind this, building some IPG data center, and they can be more energy efficient. And I think also IPG is a very important milestone before you have ASIC. And because there are some specialized operations, for example, multi scalar modification over elidic curve, those primitives are highly pilotable and can be more suitable for ASIC. So I think maybe within two or three years we can have ASIC which can beat both IPG and GPU. And another concern behind this is that the proof algorithm involves in one year you find something which is more efficient. So I think it's just ASIC will chip in a later stage when the algorithm become more and more stable.
00:46:48.370 - 00:47:26.618, Speaker E: Yeah, I guess that's sort of always been the main question. So having worked in building asics, it takes you like two years to tape out. By the time you verified your design, maybe a new processor spec has come out. So I guess my question is, in a space where we can't even agree on the names of the precise architecture, we instead keep adding new suffixes to plonk every two months or whatever. How much do you think that the inflexibility of hardware versus sort of like the flexibility of design, how you look at that trade off as.
00:47:26.704 - 00:48:03.622, Speaker D: Yeah, that's a very good question, actually. So basically, currently most ZK algorithms, either you are plank or even you are growth 16, or like any other algorithms, so they build on top of similar primitives. For example, the mostly commonly adopted one are multiscale multiplication over curve and also ffts over large field. And I think those two are the most important primities. If you get those components working really fast, then you can accelerate most Ziki algorithms. But different algorithms differ a lot. Also, it depends on your circumstance you are using.
00:48:03.622 - 00:49:03.354, Speaker D: Because currently for our Zik EVM, although we already have the kernels for MSM and FFT implemented, but the workflow needs to be trained a lot. For example, we need like 1000 ffts for our current Zke EVM and the data copy between the cpu and also the FPG. That becomes a new bottleneck. So I think with your algorithm evolving, the basic parameter won't change, but it depends on your circuit shape, like how many custom case you are using, and it will influence the workflow, and it might influence your overall efficiency, because some of other stuff can become the new bottleneck. And I think those two primities are commonly used in snark. In Fry, there might be some small differences because they can avoid some group operation. That's also the reason why Stark is considered to be faster than snark, because they don't need either the curve, they don't need any group operation, they only need FFT or a smaller field.
00:49:03.354 - 00:49:19.150, Speaker D: And you know, with the scalability increasing, it's also very hard to accelerate FFT for very large scale. So it's still like a huge trade off, but the parameters underlying doesn't change too much. As far as your bitwise is fixed.
00:49:19.970 - 00:49:56.726, Speaker E: I will say one funny thing about starquare, right, is like the programming language still makes you have to think about field elements and there's no strings. And there's actually some funny things of like you went to all this trouble to avoid group operations, but then you actually still force a developer who may not totally understand what that means, to have to actually reason about some of those things more directly. And I think that's something that I think ZKVM hopefully avoids in the sense that hopefully no one has to actually really understand KZG to write like uniswap.
00:49:56.758 - 00:50:00.886, Speaker D: Yeah, definitely. That's our priority. And to be the most developer friendly.
00:50:00.918 - 00:50:37.426, Speaker A: Zkrop, I have one more question about what you were just saying with the hardware acceleration. If you have these kind of like, you're talking about optimizations, but when you talk about optimizing ZK in this context, and this is actually about EVM speed and gas fees on the roll up, I'm just curious, when you talk about optimizing those circuits, are you talking about making anything in the actual use of the l two faster, or is it literally just in how quickly one could prove how cheaply one could prove how small the proof is? I'm just curious if it has any impact on the actual running of the network.
00:50:37.458 - 00:51:08.218, Speaker D: Yeah, I think that's a good question. So basically when I'm talking about hardware acceleration. So firstly that if you don't have good hardware support, you literally can't generate proof in time. For example, the QM circuit might take you 5 hours to generate the proof. So that's first thing, it's important to make that practical. We already make that practical enough. And second, talking about the cost, I think still with east price still at a very high, I think the data availability cost is still larger than the proving cost.
00:51:08.218 - 00:51:36.346, Speaker D: The cost when you are storing your raw transaction data on chain is still higher than the proving cost. So it's more about the energy consumption for generating the proof and whether you generate that in a more efficient way and with less cost. But for the fee perspective, it's not dominant by this proving cost. And it might change after there are a lot of dunk, sharding and eips implemented on Ethereum, but currently, yeah, I.
00:51:36.368 - 00:52:02.690, Speaker A: Want to actually bring it sort of to a higher level, which is you had mentioned that the ZK and ZkEVM is not about privacy, it's about almost like compressing or acting as the roll up. But do you have any ideas for types of projects that would do best on something like a ZKeVM versus something else? Either like the regular EVM base chain, or like another EVM compatible other chain that's been bridged to Ethereum.
00:52:03.030 - 00:52:03.442, Speaker D: Yeah.
00:52:03.496 - 00:52:52.590, Speaker B: So first of all, all of the existing applications should be compatible with the ZKe EVM. And then I think for those, if you consider comparing some OD layer, one that's EVM compatible. So the bridge I think becomes kind of one of the loophole inside of your security guarantee that sometimes it can be easily attacked, but with the ZKE EVM, so the bridge can be guaranteed with the validity proof. That's a lot of things like more secure than some other outstayer ones. And then I think for those applications that have high stake and then requires those security, and then also want to have high frequency, like the low gas fee, and then with very high frequent transactions and a lot of users, those applications will be very suitable for the ZkeVM.
00:52:52.670 - 00:53:45.298, Speaker D: I think another direction we are also actively research team is actively looking into is that as I mentioned, we definitely want to add some features beyond the key VM. So for example, we can enable some pre compiles or some new parameters, especially on the Keyvm. For example, we build some specialized circuit for some hash function so that you can do that kind of crypto optimization, especially on the key VM in a cheap way. So that's something like we can definitely enable. And also, as you mentioned, you care about privacy, but from our perspective, layer two is built for scalability, for reducing the conjecture problem on Ethereum. Layer one, because you can't get both, especially under the account based model, it's very hard for you to get privacy because even for AlO, for some other company they are building under a Utxo model. So it's very hard to build privacy under the account based model.
00:53:45.298 - 00:54:11.498, Speaker D: So I think privacy like aztec or some other techniques, maybe some other featured apps, specialized, maybe some smaller rops, and we can verify the proofs in our Ziki EVM. And so they can be some specialized rops instead of a general purpose rob. But by adding some more crypto primary support in our ZkevM, we can support those proof of application more and more cheaper and things like that.
00:54:11.664 - 00:54:30.338, Speaker A: Do you ever imagine being used where part of adapt is on the main chain and the other part is on the ZkevM? Something like I've heard sort of the example of maybe the governance module is on the roll up and something that happens rarely, could still be on main net, but they're somehow connected. Do you imagine something like that?
00:54:30.424 - 00:55:00.042, Speaker D: I think for now we haven't thought into that direction. It basically depends on if you are doing something which is more computational heavy, you can move that off chain and use a cheaper proof instead of that. But the actual cost, if you are running Ziki VM or Ziki VM model, it might be higher in verification cost. So it depends on how costly your initial computation can be comparing with your proof verification. So it's a trade off. And yeah, it's an interesting direction, but we haven't looked into that yet.
00:55:00.176 - 00:55:33.458, Speaker A: Yeah, this actually though just brings up another question. So I've been doing a series on bridges and interoperability zones and all that, and one of the things is this idea of general message passing. So I always think of a roll up very focused on data availability and then it moves tokens, but I don't know actually. Do you have messages also going back and forth between the l two and the main net? Are you changing state of account? Can you basically send a message through your roll up bridge?
00:55:33.634 - 00:55:43.162, Speaker B: Yeah, I think that's a good question. I think you can still send a message through some of the specialized contract and then that can be forwarded to the layer one. I think that's totally possible to do.
00:55:43.216 - 00:55:44.634, Speaker A: Could it be executed there?
00:55:44.752 - 00:56:08.530, Speaker B: Yeah, I think you can provide extra functionality in a bridge to having not only sending some tokens, and also you can say, like, I want to invoke some of the smart contract there, like on the layer one, or vice versa. You can say on a layer one, you can send some message on token along with invoking certain contract on the layer two. So you can have this interoperability, I think, between the layer one and the layer two.
00:56:08.600 - 00:56:12.130, Speaker A: Yeah, that's just something I feel like at least I haven't explored enough.
00:56:12.200 - 00:56:58.174, Speaker E: I mean, I do think one thing that gets changed with message passing, if you have an ability to standardize the messages into fixed size proofs, is that people could generate the messages on, say, scroll for some computation they want to do and then send it elsewhere under a fixed size packet, right? So maybe you do the computation on scroll, you do some weird flash loan, you have some kind of complicated thing. You generate the proof, and then you relay the proof to all the other layer twos via some kind of generic message pressing layer that actually is sort of a more full model than say something like wormhole or layer zero or nomad.
00:56:58.222 - 00:56:58.482, Speaker B: Right.
00:56:58.536 - 00:57:14.870, Speaker E: Because they can't actually give you any guarantees on the calculation, unlike a ZK bridge. Right. They can only kind of give you very simple, right now at least simple transactions. And you do rely on kind of like the relayers economic incentives.
00:57:15.210 - 00:57:26.140, Speaker A: In that, though, there's a new agent or a new operator you just mentioned, that's a between l two message passing system, right? Not going through the l one.
00:57:26.830 - 00:57:35.774, Speaker E: I think what you could do is you have the between l two that falls back to l one. I guess that's like closest to nomad in design land right now.
00:57:35.892 - 00:57:36.414, Speaker A: Okay.
00:57:36.532 - 00:57:40.906, Speaker E: But it doesn't require staking on all the chains.
00:57:40.938 - 00:57:41.134, Speaker B: Right.
00:57:41.172 - 00:58:31.166, Speaker E: The problem with something a wormhole is you effectively have to, your capital efficiency is kind of low. You have to stake on every chain, and the amount of stake at every place determines sort of the security. But I think the idea is if you have a single ZK chain that does a computation and generates a proof, and then you only have to send that proof, the sort of economic value of that proof is a lot lower than like, hey, I have to move coins on the other place, right? So there'll be a lot of weird capital efficiency trade offs. I think when you start thinking about ZKL two stuff versus optimistic or versus wormhole sale things where it's like you have synthetic assets on both sides, because the synthetic asset is not free, it does require a bunch of capital to kind of be backing it. Implicitly. Not that there's not a place for all of those. Right.
00:58:31.166 - 00:58:49.750, Speaker E: They can get to production faster. Right. But I think it will be interesting in the long run to see how zks change how much capital you actually need to have on every chain. Right now. It's like quadratic in the number of chains. Right. Like you basically need to have capital on every chain and then the minimum amount depends on every pair of chains.
00:58:49.750 - 00:58:58.326, Speaker E: And in some ways, hopefully, a ZK thing lets that be less, more efficient for message passing. Sorry, that was just my little rant about that.
00:58:58.348 - 00:58:58.534, Speaker B: Sorry.
00:58:58.572 - 00:59:01.978, Speaker E: That wasn't like anything that anyone is doing right now.
00:59:02.144 - 00:59:18.254, Speaker B: No, I think that's a very interesting topic to discuss. I think in the future there could be some interoperability between the Un two ZK row ups, like two ZK layer twos. So I can directly allocate a font in different places and then execute it at a different place. I guess so.
00:59:18.292 - 00:59:25.178, Speaker E: I guess one thing that I think is kind of cool about zkps is that you get almost like an interoperability standard built in.
00:59:25.204 - 00:59:25.442, Speaker D: Right.
00:59:25.496 - 01:00:20.402, Speaker E: You only have to trust the verifiers written correctly, which is a lot lower of a workload than having to trust that the VM translation is written correctly. Right. You can argue that the optimism bug that was found by Sorek and then the wormhole bug both have this problem of like, there's two vms that don't exactly agree, right? They're not bitwise identical. And the translation layer was where the bug of the synthetic thing happened. But the ZK thing, as long as the verifier contract is correct, you have basically perfect interoperability. And so I think that's just lower surface for bugs and errors. I think once the ZK roll ups are live, it'll be way easier to do this type of stuff because you'll have all these networks of provers who are already validating this chain, and you can basically be like, hey, can you generate this proof that I can then relay somewhere else and it doesn't matter who relays it?
01:00:20.536 - 01:00:39.442, Speaker A: That's really interesting. I guess going back to scroll though, do you have ideas of like, I mean, you're still building? We should find out actually where you're at in this build. But I think this kind of brings us to the question of at what stage is the project? What's your timeline for actually having us be able to play with a ZK VM?
01:00:39.586 - 01:01:16.970, Speaker B: So we have the design two phases of our testnets, and then we are quite close to have our phase one testnet, which we are almost like 70% down. And then right now we can already support some ERC 20 transfer on the ZKE EVM. Everything works very smoothly, like I expected. So we'll see like our first launch of Testnet in a few months, one to two months. And then I think in the phase two. So in the phase one we'll probably support some limited off codes and then some of the transactions. And then in the phase two will be like the full compatibility Zkevam testnet.
01:01:16.970 - 01:01:23.522, Speaker B: Those are like every smart contract that supposed to be able to run on the Ethereum should be able to run on that scroll testnet.
01:01:23.666 - 01:01:24.454, Speaker A: Cool.
01:01:24.652 - 01:01:44.400, Speaker E: What applications? Obviously, it's just like early in this space and who knows what the real application was. Like 2016 Ethereum. You couldn't have predicted most of the things that exist now. What are kind of the applications you're most excited about being enabled by scroll that you've kind of heard of or thought through?
01:01:44.930 - 01:02:18.566, Speaker B: I think those example, the consumer facing applications like the targeted for more and then some social applications close to the more users. Not only the Defi applications that's only targeted for some financial applications, but more like the customer facing application for the steppen or those social applications. There'll be very interesting use cases and also to enlarge the whole user space for the blockchain, for the cryptocurrencies to bring more new customers to the blockchain, like to the crypto world. Cool.
01:02:18.668 - 01:02:34.540, Speaker D: Yeah, I think from my side, because I'm a ZK guy, so I want to see more ZK applications and especially on our platform, and they got cheaper proof verification, maybe we can support more stuff to support those interesting ZK applications. Yeah, awesome.
01:02:35.010 - 01:02:46.462, Speaker A: Cool. So I want to say thank you to both of you for coming on the show and sharing with us sort of the journey to scroll and your thoughts about Zkevm, what that could enable, how it's built. Yeah, thanks a lot.
01:02:46.516 - 01:02:47.978, Speaker D: Yeah, thanks for hosting.
01:02:48.154 - 01:02:55.634, Speaker B: Yeah, thank you Anna and Tarun for hosting us and having us to here. It's a very pleasure to talk to you guys on a podcast.
01:02:55.762 - 01:03:01.158, Speaker E: Looking forward to the next one where we learn more about how scroll live is working.
01:03:01.244 - 01:03:11.250, Speaker A: Yeah. So I want to say big thank you to the podcast editor, Henrik, podcast producer Tanya. Thanks to Chris for research and to our listeners, thanks for listening.
