00:00:11.890 - 00:00:39.162, Speaker A: Hey everyone. Welcome to another episode of the ZK Whiteboard series presented by Zkhack. I'm Brendan. I work on ZK stuff at Polygon, and I'm really excited to be joined by Ying Tong, who is a very talented cryptographer and engineer who currently works at the Electric Point Company. She also collaborates with the EF zero x park and is involved in a lot of cool things in the ZK crypto ecosystem. So welcome, Ying Tong.
00:00:39.306 - 00:02:01.610, Speaker B: Thanks, Brandon. Hi, everyone. So today we're going to talk about Halo Two, which is a proof system that allows for recursive composition without a trusted setup. So the first thing we're going to do is a high level overview of Halo Two, and then we'll zoom in on sort of the back end part of it, which is the inner product argument and the accumulation scheme. So Halo Two starts out at the front end with what is called Arithmetization, and this is the process of taking a bunch of values and constraints on these values and encoding them as low degree polynomials. So for Halo Two, we've chosen the Planck Arithmetization, and this is a highly expressive and flexible way to express things like custom constraints, equality constraints, and even lookup arguments. So Planck gives us our circuit in the form of low degree polynomials and this is then input into what we call the multipoint opening argument.
00:02:01.610 - 00:03:38.250, Speaker B: So the multipoint opening argument involves committing to these polynomials, evaluating them at all the required points, and then constructing a polynomial that checks the consistency between the commitments and the evaluations. So it is the polynomial produced by the multipoint opening argument that we then input into the inner product argument. So the inner product argument is a polynomial commitment scheme, and it's a particularly good one for the purpose of building an accumulation scheme. And the reason is because the inner product argument has a succinct accumulation verifier, and we'll see what this means later on. But you can think of it as a verifier that has both a succinct part and an expensive linear time part. So in the context of recursion, we perform the succinct check at every recursive step and we delay the linear time expensive check so that it's amortized across a batch of recursive proofs. Basically, what I just described was an accumulation scheme.
00:03:38.250 - 00:03:59.662, Speaker B: Right now we're using what's called an atomic accumulation scheme. Yeah. So we're going to focus on numbers three and four because I think Planck has been covered very well in another session already.
00:03:59.796 - 00:04:43.482, Speaker A: Yeah, we've looked at Planck. That sounds good. If we think about what Halo Two is, we start with our program or the thing that we want us to prove arithmetize It, commit to it, arithmetize It, perform an IOP that depends on the opening of some limited set of polynomials, prove that those polynomials can be opened and are sort of being evaluated correctly. And then when we want to do recursion, we have this really cool approach that I think we'll obviously get into this later, but it was like this really groundbreaking step forward for recursive proof.
00:04:43.546 - 00:05:58.370, Speaker B: Yes, definitely. So the inner product argument is a polynomial commitment scheme, and we should first start with looking at its API. So what it takes in is some commitment to a polynomial. So purportedly C is a commitment to some degree n polynomial P, and then it also takes in some challenge point at which this commitment is opened. And lastly, it takes in V, which is the value that we're claiming the commitment opens to at x. So we're claiming that PX equals V, and we are also claiming that C is the commitment to. And in Halo two, we use the Pedersen commitment.
00:05:58.370 - 00:06:14.650, Speaker B: So we're claiming that C is the result of an inner product between the coefficients of P and some fixed bases G, which are part of the public parameters.
00:06:15.710 - 00:06:39.220, Speaker A: So you give me a group element and you say, I claim that this is the commitment to this particular polynomial. And I say, okay, prove it. I'm going to give you some random x. And if it is in fact this polynomial, or if it bears this relation to these other polynomials, it should evaluate to. So the IPA just allows us to prove exactly.
00:06:39.590 - 00:08:18.194, Speaker B: And in particular, the proof of the IPA is succinct in the sense that it's logarithmic in the size of the polynomial we're committing to. So, to give you a preview, the proof consists of a vector of group elements L, a vector of group elements R, and each of these are lengths k, where k equals log n, and it further consists of a group element g zero, as well as a field element P zero. And we'll learn more about how this proof was produced as we go through the rounds of the inner product argument. So, yeah, I think we're ready to get into the inner product argument. The inner product argument is trying to convince us of these two statements, and we can rewrite them slightly. So we've written C as an inner product of the coefficients of P with our fixed basis G, and similarly, we can write P as an inner product between the polynomial P. Oh, sorry, this should have been V over here.
00:08:18.194 - 00:08:49.934, Speaker B: Whoops the polynomial P and some polynomial B, and B here is simply the powers of our challenge point x. So, yeah, you can see that the inner product of the coefficients of P with B is the same as just evaluating P at x.
00:08:50.052 - 00:08:57.514, Speaker A: Right, because recall with the inner product, we multiply like sort of element or entry wise or element wise.
00:08:57.562 - 00:09:25.350, Speaker B: Yes, exactly. A dot product. Yeah, okay. So these are the two statements of interest, the two inner products of interest. And we can sort of combine these two expressions into a single one. Let's call it CK. So, reminder, k is log n, and we're going to proceed the inner product argument in k rounds.
00:09:25.350 - 00:10:28.442, Speaker B: In the case round, we start at the KTH round with CK equals C plus vu. Yeah. Now we start, let's start with our polynomial P, which has n terms, and I'm going to index it k. We're in the case round here. All right? And then I'm also going to put here the polynomial b, which is just the powers of x BK. And we also actually have GK, which is the vector of our fixed basis, but I don't have space for it here. And furthermore, it behaves very similarly to b.
00:10:28.442 - 00:11:10.780, Speaker B: They're both publicly known and we're going to treat them very similarly. So in the case round, so our strategy for each round is to gradually try and shrink the size of the polynomials until at the zero th round, we're left with constant polynomials. So single terms. Yeah. So I'm sneakily just reindexing zero indexing things. So how we're going to do this is through the use of a challenge. So at round k minus one, we're going to derive a challenge, UK minus one.
00:11:10.780 - 00:11:55.240, Speaker B: And we can derive this from the commitment c. Yeah. And so the purpose of UK minus one is to separate the high and low halves of our original polynomials. So what I mean by this is that I'm going to cut P into P low and P high, and then I'm going to put a factor of u k minus one here, and then UK minus one inverse here.
00:11:57.450 - 00:12:12.334, Speaker A: So we have these coefficients that we have sort of an order of the degree of the monomial that they multiply. And so we take the top half and put it here and then the bottom half and put it there.
00:12:12.452 - 00:12:12.974, Speaker B: Yes.
00:12:13.092 - 00:12:13.710, Speaker A: Okay.
00:12:13.860 - 00:12:55.500, Speaker B: Yeah. So what we're going to do is we're simply going to add them up. It's a linear combination of the low and high halves. So we're going to in fact define p for our next round of the IPA, PK minus two. And PK minus two is just going to be the sum of these two degree n over two polynomials. Yeah. So P two equals UK minus one, PK low plus UK minus one inverse PK high.
00:12:57.150 - 00:13:03.502, Speaker A: Crucially, they're only because we're like folding them together, they're half the number of terms.
00:13:03.636 - 00:14:26.778, Speaker B: Exactly. Yeah. And similarly, we're going to shrink b, but over here we're going to flip our use, we're going to flip the power of the challenge that we're using. So we'll put UK minus one inverse here for below, and then UK minus one without the inverse for B high. And then again, this gives us BK minus two for, for the next round of the IPA, just a sum of these two halves separated by the challenge, UK minus one. So B-I-K and g is going to be exactly the same as B. So g k minus two, except the elements of g are group elements, so we have to write them.
00:14:26.778 - 00:14:34.170, Speaker B: These are scalar multiplications g low.
00:14:34.320 - 00:14:37.354, Speaker A: So we're using the brackets to denote scalar multiplication yes.
00:14:37.392 - 00:15:30.974, Speaker B: Of additive notation. Yeah. And then you can fill us in now. So if you see here, we started at round K minus one, with the CK minus one equals C plus vu, aka it equals PK minus one, g k minus one. Let's see. Plus p and B k minus one. So just using gates exactly, yeah.
00:15:30.974 - 00:16:11.930, Speaker B: We initialize the K minus one indices to the input vectors. So that was the C that we started with in round K minus one. Now in round K minus two, we can define CK minus two in a similar way, but using these new sort of folded vectors. Right. So this will now be p k minus two, which has only n over two terms now. Yeah.
00:16:12.080 - 00:16:14.614, Speaker A: So we're having the degree at every round.
00:16:14.742 - 00:17:24.790, Speaker B: Yes, exactly. And then plus the corresponding P and B inner product, k minus two. So another interesting thing to note is actually, and I'm just going to assert this now and we can show that later, CK minus two can be also written as CK minus one, plus some terms with the factor UK minus two times some group element, plus some terms with the factor UK minus two inverse squared plus another group element.
00:17:26.410 - 00:17:28.346, Speaker A: Do you want to write that out?
00:17:28.528 - 00:17:34.060, Speaker B: Yes, I do want to write that out. Yeah, let's write that out.
00:17:35.330 - 00:17:39.182, Speaker A: Erase everything. Okay. Everything except the set.
00:17:39.236 - 00:17:39.840, Speaker B: Okay.
00:17:40.770 - 00:17:43.886, Speaker A: Yeah. Because I thought it was cool how you did it in the first one.
00:17:43.988 - 00:18:33.840, Speaker B: Yeah. So just to remind ourselves, the inner product arguments, opening proof consists of these terms and we're going to see how we got them. Right, so we were saying just now that CK minus two can be written as the inner product between PK minus two and GK minus two.
00:18:34.370 - 00:18:43.470, Speaker A: Sorry, don't we need like, minus no, I think that's.
00:18:45.910 - 00:18:49.730, Speaker B: Yeah, I think this.
00:18:49.880 - 00:18:51.654, Speaker A: No, wouldn't it be minus one?
00:18:51.692 - 00:19:25.520, Speaker B: Yeah, I think this is minus one. Yeah. And then GK minus one plus yeah. Inner product. So this scalar here is the result of an inner product between PK minus one and BK minus one. Yeah, actually, no, it's K minus two, I think.
00:19:26.290 - 00:19:39.678, Speaker A: Well, no, but aren't you then going in and then substituting this? Yeah. Sorry. So it's ky two and then you do ky.
00:19:39.774 - 00:21:15.518, Speaker B: Right, okay. This indexing got confusing because indexed yeah, we zero indexed everything. Sort of the punchline of this is that I claim that after we expand all this out, we'll get CK minus one and CK minus two in the following form. So it'll be CK minus one plus some term with the factor U k minus two inverse squared multiplied by some group element, LK minus two. Yeah. And the point here is that these two terms are what's interesting and what we have to retain from the K minus first round. So we need to retain LK minus I'm actually really confused about the indices.
00:21:15.518 - 00:21:16.690, Speaker B: Let's see.
00:21:16.840 - 00:21:20.610, Speaker A: I think that that's right, given how because we're on the next round.
00:21:21.370 - 00:21:23.880, Speaker B: So this should be LK minus one here.
00:21:25.690 - 00:21:29.080, Speaker A: No, because aren't we on the K minus two?
00:21:32.650 - 00:21:33.400, Speaker B: Right.
00:21:37.150 - 00:21:37.818, Speaker A: It's fine.
00:21:37.904 - 00:22:07.460, Speaker B: Okay. It's off by one indices, but maybe possibly, yeah. The spirit of it is that at each of these K rounds, we're collecting two group elements, the LK and RK, and these have to go into our proof. And that's how, in our proof, we get a vector of KL elements and a vector of KR elements. Yeah.
00:22:08.470 - 00:22:10.658, Speaker A: Can we see how that works?
00:22:10.744 - 00:23:39.500, Speaker B: Yes. To see how we get from here to here, all we have to do is expand. So, if you recall what PK minus two is, really, it's just PK minus one low multiplied by some effector, UK minus one plus two, k minus two by some challenge, and then the high half of it is multiplied by the inverse of that challenge. So when I say PK minus two, I mean the folded version of PK minus one. And it's folded using this using different powers of this challenge. And then, similarly for G, except, if you recall, we flipped the powers of the challenges. So this is UK minus two plus UK minus two inverse, g high, GK minus one high.
00:23:44.210 - 00:23:47.534, Speaker A: I think we want to flip these right.
00:23:47.652 - 00:24:05.780, Speaker B: Which is oh, yeah. Flipped. Flipped, yeah. So the powers of the challenges are flipped from those used to compress the P vector. Cool. Yes. And then since B is a very we use it in a very similar way.
00:24:05.780 - 00:24:34.960, Speaker B: It would be tedious to write it out and it's an exercise for the viewer. Yeah. But if we try to multiply these terms, let's see. So we'll notice that some terms cancel out. So this term has a UK minus two. This term has a UK minus two inverse. So that's going to cancel out and give us just P low dot product, g low.
00:24:34.960 - 00:25:26.474, Speaker B: Right. Whereas there's some terms with the same factor, the same power of UK. So these two terms multiplied together, it would give us something with a vector of UK minus two inverse squared. Yeah. So over here, it'd be P high and G low, which is a cross term. Yeah. And similarly, let's see, these two terms are both UK minus two without the inverse.
00:25:26.474 - 00:25:43.874, Speaker B: So if I multiply those together, I would get something with a factor of UK minus two squared, p low and G high, which is, again, a cross term. So I'm pretty sure I left out.
00:25:43.912 - 00:25:47.626, Speaker A: Some just the remaining non.
00:25:47.678 - 00:26:10.570, Speaker B: Oh, yeah. So this one P high and G high. Yeah, p high and G high. So you can see that for those terms whose challenges cancelled out, we're left with basically P low, G low plus P high. G high is the same as saying PNG. Yeah. Right.
00:26:10.570 - 00:26:35.842, Speaker B: And I think if we did PB in this way as well, we would get back PBU. So in other words, we get back PG plus PBU, which is CK minus one, and then we're just left with those terms with actually some factor of the challenge in front. In other words, our cross terms.
00:26:35.976 - 00:26:45.270, Speaker A: Yeah. So the L and the R are just like the cross terms from that round that are kind of like these leftover things we have to account for.
00:26:45.340 - 00:27:13.006, Speaker B: Exactly. Yeah. So you can see that at each round we're really just accumulating cross terms and challenges. Yeah. So that was our first sort of round of compression. And if we keep going, we'll start round k minus two with a vector that's already like N over two with N over two terms. Right.
00:27:13.006 - 00:28:48.800, Speaker B: And then we'll fold it again and that will give us two more cross terms there that we're accumulating and so on, until at the zero th round, we are left with constant polynomials, meaning we are left with just a single term. And by which I mean p zero is a single field element, g zero is a single group element, and B zero is a single field element. Right. And if we write out C zero again, c zero is, um, c zero can be expressed using, like the basically the terms we have left at the end of round zero. So it can be expressed as P zero, g zero plus P zero, b zero scalar multiplied U. Right. But as we saw just now, it can also be expressed as C one plus U.
00:28:48.800 - 00:29:51.410, Speaker B: I don't know what the index is here. Squared l zero one. Okay, forgive the indexing plus U zero inverse square. Yeah, right. And then if we keep going, actually we take C one, we can express it as a C two plus something like that, plus the cross terms from round one. And if we keep on going, we'll end up with something like C zero equals C k minus one plus the sum of all the cross terms from all Krams with their challenges. So UI squared Li plus UI inverse squared Ri.
00:29:51.410 - 00:30:22.540, Speaker B: And these Lis and R eyes are precisely what we've been collecting and putting in the proofs here Li, Ri. Now, so the Verifier at the end of k rounds and looking at the proof, the verifier has access to all the allies and all the Ris. So the verifier is trying to check the equivalence between this expression of C zero and this.
00:30:23.970 - 00:30:31.806, Speaker A: Yeah. So we've shown that these two things are equal and this equality will only hold if, in fact, the prover has.
00:30:31.828 - 00:30:33.520, Speaker B: Been honest yes, exactly.
00:30:35.030 - 00:30:36.962, Speaker A: Evaluated the problem in the middle to reach up.
00:30:37.016 - 00:30:39.860, Speaker B: Right, okay. Exactly.
00:30:40.870 - 00:30:46.162, Speaker A: The verifier has this has lis and r. Yeah.
00:30:46.296 - 00:31:52.538, Speaker B: So using Lis and Ris, the verifier can derive the round challenges, the UIs. Now, the verifier has CK minus one, because this was the original C that we started with. This was the C that purportedly committed to the polynomial that we input into this IPA. Now, the verifier also has p zero because we put it in the proof. The verifier also has g zero and the verifier does not have b zero, right? But something to note about g zero and b zero. Basically, these are the results of folding the GNB vectors over k rounds. And the GNB vectors to begin with were publicly known already.
00:31:52.538 - 00:33:09.058, Speaker B: So what the verifier has to do, what's left for the verifier to do is firstly compute b zero and the verifier is completely capable of doing that because again, b is just the powers of x and this was known to everybody. And secondly, the verifier needs to check that g zero had been correctly constructed, given the starting g vector and given what the verifier knows about the round challenges. So check g zero and yeah, this is what's left for the verifier to do. So remember we said that the IPA has a succinct check and an expensive check. So this number one is a sucksync check. The verifier can compute b zero in log n time and whereas number two here, that's the linear time check. This is a multiscalar multiplication involving n terms.
00:33:09.058 - 00:34:49.842, Speaker B: And this check is the one that we're trying to delay at every step of the recursive verifier and amortize across the batch of proofs. So we'll go now into how the verifier does these two steps. So if you recall, the verifier had to compute b zero where b is just the powers of x and the verifier had to check that g zero was correctly constructed. So now it turns out that basically all the extra information encoded in b zero and g zero, all that information is basically the challenges, the round challenges, because that's what's been used to produce b zero and g zero given the starting b and g vectors. So we can in fact rewrite b zero s, an inner product between some vector S and the starting b vector. And similarly, we can do this for g zero. Now, an S here, the terms of S is a length n vector whose terms are made up of the round challenges arranged in a binary counting order.
00:34:49.842 - 00:36:09.494, Speaker B: So what this means is S is made up of s is made up of n terms and each term is some product of the round challenges, u k minus one. And how we're going to construct each term is that. So for the first column, we're going to alternate this power of the challenge at every row. And so this and this binary counting order is intuitive because of the way we folded things in half at each round. And for a more rigorous and for a proof of this, you should see the halo paper. Yeah, but for now, how we construct S is so we alternate the power for u zero at every row, for u one at every two rows, and for u two, every four rows and so on. And for UK minus one, every k minus two rows.
00:36:09.494 - 00:36:57.946, Speaker B: And in this way we construct the vector S that encodes the round challenges and also retains the structure of the folding that happened at each round. So I claim that the inner product of S and B can be computed in logarithmic time. And I'm not going to prove it here, but just to sketch it out. The inner product of S and B, that's the same thing as saying evaluating S at x, right?
00:36:58.128 - 00:37:02.198, Speaker A: Yeah. Because these are both field elements. Vectors of field elements.
00:37:02.294 - 00:37:37.640, Speaker B: Exactly. Yes. So yeah. So it turns out that we can do this computation in log n time. Yeah. And you can see the halo paper to see how we do that. However, this inner product, which is between a field, it's a multiscalar multiplication here.
00:37:37.640 - 00:38:14.382, Speaker B: There's no shortcut around it. N expensive. It's expensive. In fact, it's n operations because this is n terms. N terms. So this is the linear time check that we're going to delay at each recursive step. So I think with this in mind, if you took away nothing else from the inner product argument, just remember that it has a succinct check and an expensive check, and this is enough for us to go on to the accumulation scheme.
00:38:14.526 - 00:38:37.990, Speaker A: Yeah, I think the explanation of the IPO is really cool. I think what I would take away is just the mechanism by which we reduce something into a logarithmic number of terms. Because obviously, if you're the prover and you're just handing me all the coefficients to the polynomial, that doesn't really help us if we want succinct proofs.
00:38:38.070 - 00:39:15.922, Speaker B: Exactly. Yeah. Our proof is only log n number of terms. Whereas for the Patterson commitment, for example, to fully evaluate a proof, you would need just the coefficients of the polynomial. Yeah. So the atomic accumulation scheme here, we're instantiating it over the inner product argument as the predicate and sort of the structure. Yeah, the diagrams are going to get a lot prettier.
00:39:15.922 - 00:40:54.626, Speaker B: Now, the structure of this is basically let's start with sort of the classical recursion, right? For classical recursion, by which I mean full recursion, you have some application f such that f x zero equals x one. Right. And then you bundle it together with some recursive verifier V, and you call these two circuits together the recursive circuit. So the recursive verifier additionally takes in some proof that proves a previous instance of itself. And this whole recursive circuit in turn, outputs the next proof for the next instance. So you will notice that to fully include a recursive verifier in the recursive circuit, you need the verifier of your proof to be sublinear. Because if, let's say your verifier were linear, the next verifier would be the size of this circuit.
00:40:54.626 - 00:41:01.020, Speaker B: Right. And then you just get blow up. So we don't want that.
00:41:02.110 - 00:41:09.990, Speaker A: And also, just for practical proving times, we would love to have a Succinct verifier.
00:41:10.070 - 00:41:49.670, Speaker B: Exactly. No, the IPA verifier is not sublinear. So if you recall those G terms yeah. What is it? There was like this SB, which is sublinear, and then there is this SG, which is definitely linear. Right. So we can't just do this with the IPA verifier instead. We're going to call this the accumulation verifier.
00:41:49.670 - 00:42:43.770, Speaker B: And in the accumulation verifier, we are only going to do the succinct checks. Right. And what about the linear check? That's what we are accumulating. So that's what we are constantly delaying at each step to the next instance. So the accumulation verifier has to, in addition, take in some accumulator from the previous instance and output the updated accumulator for the next instance. Yeah. So I think what we should do now is go into exactly how we update the accumulator.
00:42:43.770 - 00:43:29.590, Speaker B: Yeah. So if you notice here, what the accumulation verifier is doing is taking two instances and combining them into one. So what I mean by this so Pi zero and the previous accumulator, x zero, are things that have the same shape. They're both instances. So I'll call them QIS. So where Q zero is pi zero and Q one is X zero. So an instance has the following shape.
00:43:29.590 - 00:44:14.038, Speaker B: It's basically an IPA proof shape. So it has some commitment, some challenge point, and some evaluation, and then it has the IPA proof that we saw just now. So the l's r's g zero p zero. Yeah. So both, both of these instances have this shape, right?
00:44:14.124 - 00:44:14.760, Speaker A: Yeah.
00:44:15.770 - 00:45:47.806, Speaker B: So now what the accumulation verifier is going to do is see, first of all, okay, this is the AC verifier. So first of all, it's going to do the Succinct check on each of these instances. So for I n one Succinct check on Qi yeah, I definitely need to refer to my notes here because this gets mind boggling. And then secondly, so now that it's done the Succinct checks, it's now concerned with accumulating these two instances into one new instance. Yeah. And how it's going to do that is a random linear combination. So we sample some randomness alpha that's using, let's see yeah, we sample some randomness alpha.
00:45:47.806 - 00:46:41.638, Speaker B: Let's not fear shamir this random. So using this alpha, we can now accumulate our two instances into one new instance. So what we're going to do is we're going to define a C. So we're going to basically make an inner product argument for this S polynomial that we're not checking. Yeah. So we'll define a C as a linear combination of the G's that we didn't check. So G plus alpha g 10 here, right.
00:46:41.638 - 00:47:25.088, Speaker B: And then this is a commitment to the polynomial S, which is our challenges. Yeah. Because if you recall, G zero is the inner product of S and G. In other words, it is a commitment, a Patterson commitment to S. Right. So C here is a commitment to S zero plus alpha s one.
00:47:25.254 - 00:47:25.970, Speaker A: Sure.
00:47:26.420 - 00:47:27.170, Speaker B: Right.
00:47:29.620 - 00:47:36.800, Speaker A: This is what we mean when we say, like, we're accumulating all of the challenges from that round of IP.
00:47:36.960 - 00:48:34.308, Speaker B: Yeah. We're accumulating these two instances into a new instance. So we're basically instantiating an inner product argument for this new instance. Yeah. So we sample some X. Yeah, let me subscript all these as well. So we sample some X, some random X at which to evaluate this new S, and then we define okay, I'm going to put stars on all the terms that go into our new accumulator and our new accumulator.
00:48:34.308 - 00:49:51.244, Speaker B: Let me just write out what it should look like. It should look like C star, X star, V star, and it should also have an IPA proof L-R-G zero star, P zero star. Yeah, very cool. So V star is just s evaluated at X star. So we have that and then the prover, the accumulation verifier now has C star, X star, and V star, and proceeds to run an IPA for C star, X star, and Vstar, at the end of which it gets this new IPA proof.
00:49:51.372 - 00:49:51.808, Speaker A: Cool.
00:49:51.894 - 00:50:24.584, Speaker B: Yeah. Run IPA for C star, B star, X star, and output this thing. Yeah. So you see that we put in two instances and accumulated them into one new one.
00:50:24.702 - 00:50:25.080, Speaker A: Okay.
00:50:25.150 - 00:50:25.528, Speaker B: Yes.
00:50:25.614 - 00:50:33.112, Speaker A: And we can also generalize that for more instances. Right now we sort of have this chain of recursive proofs.
00:50:33.176 - 00:50:33.452, Speaker B: Right.
00:50:33.506 - 00:50:40.636, Speaker A: We can imagine that if we wanted to batch a lot of proofs, we could take multiple instances and just have a longer linear combination.
00:50:40.748 - 00:50:42.560, Speaker B: Yes, absolutely. Yes.
00:50:42.630 - 00:50:43.250, Speaker A: Cool.
00:50:44.980 - 00:51:48.084, Speaker B: In this way, we keep delaying having to do this on this length and MSM, but eventually we have to do it. And when we do it, that's called the accumulation decider that's doing it. So let's imagine we've had like a long chain of accumulation verifiers, and now we are finally ready to do that expensive check. So what this is going to look like is the accumulator here. Let's see. So we've chained together a bunch of IVC provers, and IVC here stands for Incrementally Verifiable Computation. It's what you said, basically this linear shape that just doesn't deal with like N inputs, but rather just chains one after the other.
00:51:48.084 - 00:52:16.780, Speaker B: At the end of this chain, we have the IBC verifier that actually tells us whether or not this whole chain has been valid. Yeah. Now, in the IBC verifier, the accumulator is given to the act decider. And what the act decider does is checks this.
00:52:16.930 - 00:52:19.432, Speaker A: Yeah. So so it finally checks.
00:52:19.576 - 00:52:20.844, Speaker B: Yes, exactly.
00:52:21.042 - 00:52:29.852, Speaker A: But like the nice not to preempt you, but but the the nice thing is that we can do this check outside the circuit.
00:52:29.916 - 00:52:32.192, Speaker B: Right, right. Yeah, we can do that.
00:52:32.246 - 00:52:41.264, Speaker A: And so it's like, even though it's linear time, not only have we amortized the cost over all of the proofs that we've recursed over, but it's linear.
00:52:41.312 - 00:53:21.464, Speaker B: Time, like on the CPU, which is yes, exactly. And then. We also need to deal with the last new instance and the last sort of output from our application circuit. And so this will be input to the full Snark verifier, which will also do all checks, like even the expensive ones. Yeah, cool. Yeah. So that's how we get atomic accumulation from the inner product argument.
00:53:21.612 - 00:53:22.310, Speaker A: Cool.
00:53:23.080 - 00:55:11.044, Speaker B: So atomic accumulation is when we consider this whole accumulator as a single thing, whereas more recently there has been a scheme called split accumulation that actually allows us to only make a short part of the accumulator public. So we can call this accumulator x is public, and then have a longer part of the accumulator that's private. So if we do split accumulation for the inner product argument so now we're forgetting the IPA. For now, the witness would be literally the polynomial that is committed to by C. Right. And then sort of at each step of split accumulation, at each step of the accumulation verifier, we are verifying that basically we're accumulating two polynomials that evaluate to the correct values at the correct challenges, but we are not verifying that these are the polynomials committed to by the CIS. Because again, that verification is a Patterson commitment, it's an MSM with N terms.
00:55:11.044 - 00:56:03.648, Speaker B: So this final verification is delayed to the decider once again. But I think the point of split accumulation is that basically you no longer need a succinct argument. So the inner product argument was succinct, it was login, and therefore we could input the whole argument into the accumulation verifier. Whereas if you had a super long argument like this that's like N terms plus a few, then all is not lost and you can split up your accumulator and still be able to have a succinct split accumulation verifier.
00:56:03.824 - 00:56:50.416, Speaker A: Yeah, because even though the proof is linear, the incremental verifier is still succinct. So you don't have this problem with the proof circuit complexity blowing up. We don't have to do like when we just give the verifier this polynomial, it's cheaper from the prover time perspective because we don't have to do like the folding and the IPA reduction, we just have this Peterson commitment to the polynomial, which we would have to do in both cases, both the IPA case and this. So it's strictly better from the proverbial.
00:56:50.448 - 00:57:00.328, Speaker B: Yes, yes. I think where it's worse is that the argument size at the end is larger. Yeah, but there are also ways around that.
00:57:00.414 - 00:57:03.592, Speaker A: Well, we can just do the IPA at the end, right?
00:57:03.646 - 00:57:04.970, Speaker B: Exactly. Yes.
00:57:06.080 - 00:57:16.776, Speaker A: Cool. Well, great. This has been really interesting. Do you want to talk a little bit about how this is used in Zcash?
00:57:16.968 - 00:58:22.160, Speaker B: Yeah. So right now the Halo two proof system has been deployed in Zcash without recursion. So it enter the IPA and we're doing the full verifier every time for every proof. Now, when we have recursion implemented, what we're going to do is compose proofs. So in Zcash, every sort of action, which means every group of spent and outputs produces a proof of the validity of that action. And within a transaction and within a block, there are multiple actions. So we can aggregate multiple proofs, multiple action proofs such that we end up with only one proof per block.
00:58:22.160 - 00:58:47.640, Speaker B: And this reduces Zcash's chain state because we don't have to keep so many proofs on chain. So that's batching action proofs within a block, we can also batch block proofs such that the whole chain history can be verified with just one single proof. And that's what Mina is already doing. Yeah.
00:58:47.790 - 00:58:51.944, Speaker A: Very cool. Well, thank you so much for joining, and this has been really interesting.
00:58:52.062 - 00:58:53.510, Speaker B: Yeah, thank you for having me.
