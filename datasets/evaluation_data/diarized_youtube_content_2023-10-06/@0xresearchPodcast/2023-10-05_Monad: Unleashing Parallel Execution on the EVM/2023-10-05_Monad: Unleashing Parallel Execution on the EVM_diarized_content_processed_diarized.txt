00:00:00.090 - 00:00:36.760, Speaker A: If the l one can handle all the transactions, why would you have an l two? That doesn't really make sense to me. Like you're just introducing pain for no gain. Monad also wants to do some innovations and some extensions on chain that really opens up and allows you to build new sorts of applications and that sort of thing. But for us, for v zero, it's just pure performance. It doesn't mean that performance is Monad's only targeted innovation. It's just kind of like, if you don't have the performance, you can't really. Everything else is kind of a non starter, right? Like, if you can't support a large number of users of the chain, then why innovate in other areas? It doesn't even make sense.
00:00:39.690 - 00:01:11.442, Speaker B: What's up everyone? Welcome back to another episode of Zero X Research. Before we dive into today's episode, I want to give a quick shout out to our sponsor, Hexons, one of the most hardcore security teams in web three. Pioneering in ZK and novel cryptography, Hexons is trusted by tier one projects like Polygon, including their work on their new ZK, EVM, Mantle, Risk Zero, Lido, one inch, newbank, and more. You'll hear about them a little bit later in the show. But today is September 20 eigth, and we have an awesome interview lined up with James Huntsaker, the co founder of Monad. James, thanks for coming on.
00:01:11.576 - 00:01:13.026, Speaker A: Thank you for having me.
00:01:13.208 - 00:01:27.106, Speaker B: So I want to start the conversation just around. I guess there's been a lot of effort in scaling the EVM, whether it be l ones, roll ups, side chains, et cetera. How is Monad different than these other solutions?
00:01:27.218 - 00:02:26.054, Speaker A: I think like the early forks of Ethereum, a lot of them were minor sort of tweaks on the consensus. Like Avalanche had a different consensus mechanism. The execution side was usually just a copy of Geth, the Go Ethereum client that was imported, so there wasn't much effort put then. You know, there have been many different iterations of Ethereum clients in different languages, but they are all just basically reimplementations of the same design. So I think where Monad defers is actually we're building something that's fundamentally different in design than the previous iterations. As far as like roll ups and stuff, there's a bunch of trade offs that made with roll ups, and there's still a lot of work to do there. Right now, most roll ups are centralized.
00:02:26.054 - 00:02:57.986, Speaker A: They're not decentralized. Right. They have a single sequencer. There's issues with composability with roll ups apps running on one roll up, and you want to use something, compose it with something that's running on a different roll up, that's really hard to do. So I think roll ups are a fine technology, but there's still a lot of unsolved problems there that I think is probably years of research and effort that needs to really make it like a good user experience and properly decentralized.
00:02:58.098 - 00:03:07.062, Speaker B: Do you mind diving into some of the primary differences between VeVM in terms of Ethereum's design versus your guys'optimized version of it?
00:03:07.116 - 00:03:48.566, Speaker A: Obviously, what we're trying to do is utilize the full power of the machine. We're targeting lower end machines as well, but we want to use all the cores on the machine. We want to utilize the performance of the latest SSD drives. I think a recent number is like $200 to buy a really nice Nvme ssD drive. And those things really like, the performance is really great. A few hundred thousand operations per second. The bandwidth is huge.
00:03:48.566 - 00:04:30.194, Speaker A: Gigabytes of data can be transferred to those per second. Even as a consumer, you have access to this really powerful hardware and the existing co Ethereum, it's single threaded. I'm talking about the execution side of single threaded. There's obviously other things going on, like the RPC and that sort of stuff, but we're just not utilizing the power of the hardware. And that's what Mona is really focused on doing. Our specialty from trading was to build really super optimal software that really utilizes the full power of the hardware. And so that's kind of like we're just taking that.
00:04:30.194 - 00:04:47.014, Speaker A: We've been doing that for a long time. So just taking that and porting that knowledge into the blockchain space by trying to build Mona and just extract as much power as we can out of the CPU and the memory and the SSD drives and that sort of stuff, and just really speed it up. Right.
00:04:47.052 - 00:05:08.910, Speaker C: So I think this kind of gets to the idea of parallel execution, really. And the idea is they're just using multiple cores that the computer has. Even like very basic consumer laptops have multiple cores, they can do multiple things at the same time. And the EVM is very sequential. So can you talk a bit about what parallel execution means in the context of a blockchain and why that's pretty important to scalability?
00:05:10.450 - 00:05:42.300, Speaker A: Yeah. A block is a linearly ordered set of transactions, right? Like transaction one, transaction two, transaction three. And each transaction is basically mapping the previous state to a new state. And then once you string all these transactions together. At the end of a block, you have a whole new state. All these transactions applied to the previous state. Many transactions don't actually interact with each other.
00:05:42.300 - 00:06:24.598, Speaker A: The blockchain state is a bunch of different accounts. Like, maybe you're transferring money to Sam, I'm transferring money to somebody else. Those things don't have anything to do with each other. So even though that they're linearly ordered, the order doesn't matter, right? Like, whether your transaction was before mine in the block or mine was before yours is of no consequence to the actual outcome. In mathematical terms, there's things called partial order, total order. The transactions are really partially ordered. They're not really totally ordered, but we totally order them just because that's what people are used to.
00:06:24.598 - 00:07:16.742, Speaker A: That's what they put in the block, is a linearly ordered set of transactions. So optimistic execution, or parallel execution, is trying to basically untangle that, like, go back from a total order to a partial order on these transactions. And a partial order is really like a graph, and then you're trying to execute that graph and arrive at the same result. Mona's design is basically at its fundamental base. It's just very naive, optimistic execution. Like, I run my transaction. I run your transaction, and we check, and we say, okay, do these two transactions conflict? If they conflict, then we know that, oh, we made a mistake, because let's say you were transferring money to me, and then I was transferring money to somebody else, and I didn't have enough balance for the second transfer, but I did.
00:07:16.742 - 00:07:50.562, Speaker A: With your transfer included, then obviously the ordering matters. It's like the bank, at the end of the day, your paycheck has to come in, and then your mortgage payment goes out, sort of thing. And if your paycheck wasn't in there, if something happened, your mortgage payment would bounce. So it's those sorts of situations where the ordering matters. Of course, there's other situations where the ordering doesn't matter. You go to a restaurant for dinner, and you pay your utility bill. The order that the bank puts those on, your bank statement doesn't matter.
00:07:50.562 - 00:08:25.470, Speaker A: The result is the same, assuming you had enough money for both. So Monad is trying to, optimistic execution is just trying to try different things. The optimistic part is, like, you're naive, you're dumb, and you try different things, and then you check, oh, did what I try actually work? And if it worked, well, that's great. We got to speed up, because we executed these two transactions in parallel. If it didn't work, then we got to go back and fix up what we did. Right. So maybe we have to rerun a transaction that we detected that the order actually did matter.
00:08:25.540 - 00:09:02.490, Speaker C: I want to give a quick shout out to Hexins. As we explore today's blockchain landscape, let's take a moment to recognize them. As a premier cybersecurity provider in web three, Hexons is trusted by tier one projects like Polygon, including a security review on their new polygon, Ezke, EVM, Mantle, risk, zero, lido, one inch, newbank, and more. Get a deep dive into your technology stack. With the most comprehensive analysis and cybersecurity consulting, Hexons not only uses widely known methodologies and flows, but discovers and introduces new ones on a day to day basis. With over $55 billion secured, they cover everything from smart contracts to blockchain to web two pen tests.
00:09:03.150 - 00:09:09.194, Speaker B: Yeah, there's been nearly $7 billion of total value hacked in crypto's nascent history. So it's safe to say your team.
00:09:09.232 - 00:09:10.346, Speaker A: Has a lot on the line.
00:09:10.448 - 00:09:29.662, Speaker B: Don't skimp out. Take your security seriously and reach out to Hexans. Don't forget to mention zero x research for a free web two pen test with your partnership, and reach out to Hexans at Hexins IO. Find them in the links in the show notes, or reach out to them at permissionless. They'll be at booth 832. But without further ado, let's get back to today's episode.
00:09:29.726 - 00:09:39.510, Speaker C: And so on the optimistic execution side, is that build slightly different than the way Salana does parallel execution, or is it kind of the same concept?
00:09:41.210 - 00:10:20.494, Speaker A: Solana has access lists, so Solana knows ahead of time what transactions will conflict. So Ethereum has this, but nobody uses it, I don't think. Didn't use it for a while. I'm not sure the current state, but Ethereum access lists are not required. You would get some discount to. In theory, you would get some discount to specify, I'm going to access this account and you pay a little bit more upfront because you're putting this access list in the transaction. The transaction is bigger, but hopefully the discount that you got on the other side would outweigh that, and you would basically get rewarded.
00:10:20.494 - 00:10:33.334, Speaker A: You would get a discount for specifying this. But with Solana, that's required. With Ethereum, it's not required. So we don't have the information that Solana has when it executes a block. Right.
00:10:33.372 - 00:10:46.646, Speaker C: Okay, so that's why you kind of have to assume, like, all right, let's try to execute these in parallel. And if it works, it works. That's great. We improved. But in the case it doesn't work, do you end up with a slower result? Or is the worst case scenario just doing them sequentially anyways?
00:10:46.758 - 00:11:27.270, Speaker A: Yeah, I mean, the worst case scenario is doing it sequentially. And that could happen even on Solana, right. If you had a Solana block full of transactions that all access the exact same state, then Solana wouldn't be able to execute them in parallel either. So, yeah, the worst case for any parallel system is that all the transactions are dependent upon each other, and you have no choice but to execute them sequentially. So an optimistic execution, the worst case, would be the sequential time. You may execute transactions, but you have to throw away the result. You haven't wasted time, you've wasted electricity.
00:11:27.270 - 00:12:08.530, Speaker A: You've spent a lot of time computing things that get discarded. But it's not like you did that. In lieu of doing productive work, the transactions are always considered sequentially, emerged sequentially. And so when you re execute the transaction the second time, it's guaranteed to succeed. Because at that point, for you to check that there's a conflict, you already have executed all the previous transactions in the block up to that point. And so if you go to execute again, it's definitely going to succeed. You can just think of like one core is always doing sequentially, and then all the other cores are trying to do stuff optimistically.
00:12:09.130 - 00:12:56.298, Speaker C: Okay, that makes a ton of sense. And I think you gave some great example, like real world examples of when sequence matters, and just bring that back to blockchain. Buying an NFT on Opensea doesn't touch the same state as, say, making a trade through Uniswap. And I'm just curious, has anyone done any good research around, let's say, you could have paralyzed the EVM from day one, and Ethereum had this ability. What percentage of blocks or transactions within blocks could actually be done in parallel? Because if we look at what's happening on chain today, it's mostly just trades through Uniswap, and a lot of the time it's like everybody's just trading the same exact meme coin. So I'm curious, what is the total level of optimization that could really exist given the fact that there's not that many things going on on chain and a lot of state overlap?
00:12:56.414 - 00:13:14.006, Speaker A: Yeah. So I don't have any numbers for you right now. We've looked at this. Obviously it varies over time. If you replay early ethereum history without even parallel execution, you can do like 50,000 transactions a second. Right. Because it's all transfers.
00:13:14.006 - 00:13:37.566, Speaker A: So I think people like to quote number of transactions per second a lot on marketing and stuff. But they'll usually be quoting like transfers or something like this. So yeah. Even a single threaded very early ethereum history you can execute 50k transactions per second. But they're all just basic transfers. At that time people were just transferring eth around to each other. There were no really smart contract usage.
00:13:37.566 - 00:14:12.874, Speaker A: And then obviously with time over the years there were more and more smart contracts. People started doing more on chain sort of activity. And the dynamic, the makeup of the transactions changed as the years went on. And so I don't have an overall number for you like what today? But one of our main testing methods that we use is we replay ethereum history. So we just start at the beginning and just replay it and just see how fast can we replay it.
00:14:12.912 - 00:14:44.120, Speaker C: No, that makes sense. And I have one more question on the parallel execution front. Because one thing that just gets a little hairy for me is the definition of touching the same state. It makes sense why trades need to be sequential through a uniswap pool. But what if there were two uniswap pools that both used USDC as the quote asset and two random coins on either end that were different? Does the fact that they both touched the USDC contract make them touching the same state? Or is it different than that?
00:14:45.450 - 00:14:53.206, Speaker A: So just touching the same contract is not enough to say you're touching the same state. You actually have to be touching the same storage.
00:14:53.318 - 00:14:53.690, Speaker C: Okay.
00:14:53.760 - 00:15:24.340, Speaker A: So your balance in like an ERC 20 is basically an entry in a dictionary that says Dan has this much. Right. James has this much. And so if you're doing a transfer or interacting with another smart contract, those are different state even though they're the same account. So yeah, I think that's one popular misconception is that we can't paralyze within accounts. We can certainly paralyze transactions within the same account.
00:15:27.290 - 00:16:02.378, Speaker B: I'm curious about I guess the ordering of transactions as they're executed in a parallel fashion. So it sounds like based on your docs, which by the way you guys released the other day and we can link to in the show notes, but you guys are going to do a priority gas auction and have the consensus client by default. Sorry, the client by default of Monad. Basically order based on that gas auction. Whoever paid the most. The least. Is that like standard or how does this differ from Solana's localized fee markets? I just want a little bit more color on yeah.
00:16:02.484 - 00:16:48.990, Speaker A: So I think it's standard behavior, right, that the people who are willing to pay more for a transaction should get their transaction done first. Right? It's a market, I think the localized fee market thing is interesting for sure, but there's kind of a flip side to this. So just to step back for a second. The most expensive parts of executing a blockchain, like executing transactions, is cryptographic functions like hashes. Those are very expensive. Elliptical curve functions are very expensive. I think like a signature recovery is 70 microseconds.
00:16:48.990 - 00:17:26.346, Speaker A: That's actually a long time, right? That's 70,000 nanoseconds. And on a decently fast cpu, that's a lot of work, right. And then the other part of what makes a blockchain slow is state access. Reading from disk is very slow. Disk turnaround can be a median latency from a disk read can be like hundreds of microseconds. So those are very expensive relative to the other sort of computation. What's going on was just like, okay, I want to add a balance, or I want to transfer this nft to somebody else.
00:17:26.346 - 00:18:45.406, Speaker A: That's extremely cheap computation, right? Compared to the cryptography functions and the state access. I think on the one hand, for parallel execution, you may say, okay, we want to incentivize people to not access the same state, but it's probably actually the opposite, right? We want to incentivize, to an extent, people to access the same state, because the same state is the same disk, and disk reads are very expensive. So we don't have all this fee sort of stuff figured out completely yet. We're still thinking about these things. But if you're going to access something that I'm going to access, like we're both going to trade on the same uniswap pool or something, we probably actually want to make that to an extent cheaper than if you and I traded on different Uniswap pools, because those are two different reads from the disk, and that's just more work, that's just more latency that gets added in. So obviously, if you do too much of that, if everybody does the same exact state, then you can't paralyze anything. But it's definitely a trade off between the performance of paralyzing across cpus, but also the caching sort of behavior that actually happens on the node.
00:18:45.406 - 00:18:49.560, Speaker A: And we want to, to an extent, incentivize people to use the same state.
00:18:52.250 - 00:19:10.118, Speaker C: Right. Okay, that makes a ton of sense here. And I want to talk a bit about Monad BFT and this new consensus mechanism you're bringing. So what are the core offerings? I believe I saw 1 second block times, including single slot finality. So how are you guys achieving that? What's the core thesis behind that?
00:19:10.224 - 00:19:58.262, Speaker A: Yeah, so we started with hot stuff. Hot stuff is just pipeline sort of consensus algorithm, right, where normally you would go through these rounds and each leader would lead each round, and then would lead multiple rounds and a voting, and then the thing would be approved and you would be done, and that block would be done, and then. So the hot stuff and these other sort of innovations were like introduce pipelining, basically, where one leader proposes at the same time. He's also handling the voting for a previous proposal. And so we start with hot stuff. We incorporated DMVfT innovations, Jolteon. There's a lot of papers out there with different innovations.
00:19:58.262 - 00:20:50.140, Speaker A: And so we're just using the latest of all that, all those innovations, our distribution mechanism. We're heavily optimizing the gossip layer, the p two p, the mem pool, looking in the broadcast trees for the proposals. So it's just basically taking the best of what the latest sort of research is out there and assembling it into something that's very performant, innovation wise. On our side, we have a couple of things where maybe we do some sort of probabilistic sort of assembly which results in less data transmission. Obviously, the less data you have to transmit, the more efficient it's going to be. So there's compression. There's sort of probabilistic sort of compression that can happen.
00:20:50.140 - 00:20:58.960, Speaker A: So we're working on all that stuff. But yeah, it's just the latest of BFT research.
00:20:59.330 - 00:21:20.150, Speaker B: Okay, thanks. And I also saw on the doc, so we already talked about parallel execution, but I also saw deferred execution in there. And you guys define that as pipelining between consensus and execution to significantly increase the execution budget. That might have been already touched on when I asked you about kind of like the gas fee market. But do you mind elaborating here just so it's a little bit more clear for. Honestly?
00:21:21.050 - 00:22:00.402, Speaker A: Sure. So in blockchain so far, the leader would execute all the transactions that they're going to put in a block, and they would arrive at a new Merkel route for that, which is basically how you cryptographically prove what the state is, right? It's a Merkel tree, and they would arrive at a new Merkel route, and they would include that in the proposal. And then the voters, they would receive the proposal, they would execute the transactions that are in the proposal. They would calculate their Merkle route, and they would check that against what the leader computed. And if it matches, they would vote yes. And if it didn't match, they would vote no. Right.
00:22:00.402 - 00:22:33.294, Speaker A: So again, that's a sequential sort of sequence of events that's happening. And what we're proposing is that we run these things in parallel, so the leader doesn't need to include the new Merkel route. That doesn't mean, I think some people will infer that that means that the leader didn't execute at the same time it was creating the block. It probably did. Right. Especially with MeV sort of opportunities or that sort of thing. The leader probably did execute the transactions and knows the ordering, but it's not including that.
00:22:33.294 - 00:23:30.442, Speaker A: So the overlap buys you more time to basically pipeline execution and consensus. And so you can use more cores on the machine. So while you're doing consensus for the next round, you can be executing the previous round, the previous proposal, and then at some point, which is TBD in terms of timing, they will include that merkel route on chain, and then people can agree on that or whether it's correct or not. But execution should be deterministic, right? There's no probabilistic sort of behavior happening on chain everything. You should be able to look at a set of transactions and a state and calculate what the new state is. And we should always come to the same agreement on that. If we don't, then there's probably a coding error somewhere, somebody made a mistake.
00:23:30.442 - 00:24:20.480, Speaker A: So execution should be deterministic. So it shouldn't be the case that you ever really disagree on what the result of executing a set of transactions is. So because the proposal doesn't include this, it doesn't preclude you from calculating as fast as you want. If you have a really high end machine, you're a trading firm, like an HLT trading firm, and you want to know what the latest state is from the proposal. Of course you can calculate it as fast as you want if you're doing mev. Of course you want to use a big, powerful machine and throw a ton of hardware at it, and you can calculate all the different possibilities for how you can make money off that block. So it doesn't stop anybody from executing as fast as they want, but it just means that we're not really coming to consensus on the new state route for just a short window of time.
00:24:20.480 - 00:24:46.662, Speaker A: It also sort of allows bursty behavior. You might create a block that takes more than a second to execute. Right. And then at some point they have to catch up, but they can't run the execution, can't trail consensus and just keep building up. We're like, okay, now we're an hour behind. It's not like that. So there's some mechanisms in play to stop that.
00:24:46.662 - 00:24:53.990, Speaker A: But, yeah, it just gives you more budget and allows you to do things in parallel.
00:24:54.070 - 00:25:10.910, Speaker B: Awesome. I appreciate you kind of tying a bow on that concept. For me. It's definitely like a hard thing to grasp. So I appreciate you explaining it to me like I'm five, but how does Monad BD fit into this? I was curious, how is it different than the popular clients on Ethereum?
00:25:11.970 - 00:25:13.630, Speaker A: Do you mean Monadb?
00:25:14.070 - 00:25:15.250, Speaker B: Yeah, sorry about that.
00:25:15.320 - 00:25:35.526, Speaker A: Yeah. There's two main generic sort of key value stores that most clients use. Some use Btree databases. The most popular btree database is called LMDb. I think it's lightning. It's lightning something. Lightning memory map database or something like that.
00:25:35.526 - 00:26:09.646, Speaker A: Very popular key value store. I'll say database and key value store interchangeably. We're not talking about SQL databases, we're talking about key value stores. So some use that. Aragon uses MDBx, which is a fork of LMDB, and then some use LSM databases, key value stores like LevelDB or rocksDB. So these databases have their own structure. Btree is a computer science data structure, LSM log structure.
00:26:09.646 - 00:26:50.830, Speaker A: Mertree is a computer science data structure. And that's how they implement it on disk. There's a number of other issues with implementation, like they're not fully asynchronous. They don't leverage the latest Linux kernel innovations, that sort of thing, but that's kind of a separate thing. And then in Ethereum we have Patricia Merkel trees. So Patricia Merkel Tree is just a data structure of its own that you can cryptographically verify and prove what the situation we end up in is. We're trying to take this one data structure and shoehorn it into another data structure, and that becomes very inefficient.
00:26:50.830 - 00:27:40.426, Speaker A: And so the usage of, I think geth uses level DB. The way that they navigate the Patricia Merkel tree is just a lot of disk access, a lot of lookups, a lot of work that's happening there. And it's just very inefficient. And it's frankly just a waste of the power of, like I said earlier, SSD drives are really powerful these days. You can get a really cheap SSD drive that has a lot of hardware capability. That capability is all lost. That power is all lost with these other databases and that design so knowing that we engaged and we have to build our own database, we did tons of benchmarking even over months and investigation and running all sorts of simulations.
00:27:40.426 - 00:28:06.994, Speaker A: And this was a huge effort actually. I know in a lot of the docs it's like one line, okay, Monadb. But it's actually a huge undertaking, right? There's not very many databases in the world and it's really hard to build it. And there's probably like a couple hundred people in the world that work on key value stores, right? Like Roxyb I think is maintained. Facebook has one. Google created global DB. Facebook I think made rockstb.
00:28:06.994 - 00:28:39.358, Speaker A: I apologize if I'm messing this up, but these are big companies. It's just a really huge undertaking to kind of create this sort of thing. But we felt that we had to because that's the only way that we're going to get the performance that we need. And then on the back side of that, like I said, once you start that undertaking, then you got to make sure that you're using the latest Linux kernel technologies. Make sure you're fully leveraging the power of the SSD. So that's where asynchronous I o comes in. On Linux that's called I-O-U ring.
00:28:39.358 - 00:29:03.100, Speaker A: So there's a lot of internal stuff, low level sort of kernel and hardware sort of stuff that goes into this thinking to really speed it up. But yeah, it's a big part of monitor execution is the database. As I mentioned before, the two major expenses of execution are cryptographic functions and disk access. So this is half of that basically.
00:29:05.150 - 00:29:58.730, Speaker C: Okay, so it does sound like a lot of this as a whole is really a huge undertaking, is taking the EVM and building off of it and really focusing on efficiency, basically taking some of the leading ideas and innovating on those and then putting them all in not only at the execution layer but also with consensus as well. So I'm curious to kind of get your take from more, I guess more of like a business sense in some ways is what was the idea around doing this and building your own separate l one as opposed to doing something that say like eclipse is doing, where they just took the SVM and sat it on top of Ethereum as an L two, so that way they still had this really improved execution environment over to the traditional EVM. Did that ever cross your mind of like hey, let's just focus on just the execution environment for now and maybe we address the consensus later and launch as an l two. Was that ever like a part of the discussion.
00:29:59.550 - 00:30:38.860, Speaker A: Well, if you're going to have a lot of transactions, there's going to be a lot of data. There's a lot of data and a lot of transactions. And so we did look at data availability solutions. We just are going to generate too much data like we're targeting right now, 100 megabit per second connection. A block could be eight megabytes or something per second. You're going to generate eight megabytes of data per second. That's a huge amount of data that compared to what these data availability solutions are supporting right now.
00:30:38.860 - 00:31:15.294, Speaker A: I think I agree with Anatoly. He says, okay, if there ever is a solution that's great, then Salana will convert to an l two. We're not opposed to that. It's just like there's nothing available that can support what we want to do. And if there ever was, then, yeah, we would consider it. But we did look at slussy, we did look at other sort of. I mean, ethereum is kind of a non start because ethereum is really slow, right? Like Ethereum block times are slow, and the amount of data that you can post on Ethereum is very slow.
00:31:15.294 - 00:31:46.286, Speaker A: And it's also going to be expensive. On some of these busy weekends, I've gone and traded on optimism or arbitram on some of the exchanges on the l two s. And the fees skyrocket on the l two s as well, because of the economics, right. It's tied to. So the fees skyrocket on Ethereum, they skyrocket on the l two. And so that's also something we kind of wanted to avoid. But, yeah, if there ever is an opportunity for, if there's a great solution out there that can handle a lot of data, we'd definitely be open to that.
00:31:46.286 - 00:31:48.160, Speaker A: But it just doesn't exist right now.
00:31:48.930 - 00:31:52.894, Speaker C: Just to put that number like eight megabits per second.
00:31:53.012 - 00:31:53.930, Speaker A: Eight megabytes.
00:31:54.010 - 00:32:01.474, Speaker C: Megabyte bytes per second. Into perspective, what is Salana roughly generate per second? If, you know, off the top of your head?
00:32:01.592 - 00:32:34.522, Speaker A: I don't know. Yeah, I don't know off the top of my head what they're putting through. I mean, Solana right now is underutilized. The average number of transactions per second in a given moment is still pretty significantly under what it's capable of. So it's been a while since I've kind of seen like a spike in Solana's sort of activity. But yeah, I don't know the theoretical number of what Solana can do, but I have seen in practice it can do a decent number of transactions as well.
00:32:34.576 - 00:32:45.902, Speaker B: Do you foresee state bloat, like becoming a problem? Because as you mentioned, this is going to be a very data intensive chain. So how are you guys going to circumvent that? And do you expect node requirements to increase over time?
00:32:46.036 - 00:33:19.398, Speaker A: Yeah, state bloat obviously is a big thing. I think Monadb addresses some of that to an extent. Like we can handle a much larger state, local. There's honestly the most worrying thing is like synchronization. So you bring up a new node, you want to sync just like Solana, you can't replay from the beginning of history of Monad because there's just no way you could catch up, right. You would need a supercomputer or something. So you have to sync to some sort of state from another node.
00:33:19.398 - 00:34:14.510, Speaker A: And if that state is hundreds of gigabytes, that's a heck of a lot of data to download. And in the meantime the chain is running in the background, it's continuing to generate new state. Growth is definitely an issue. I'm not worried about it right now from a node requirement perspective or like how much is on disk. I'm more worried about algorithmically, like how do we sync state across nodes and how do we deal with this large object over the network? Less worried about it on the actual SSD drive. I think even Ethereum has put out some proposals on dealing with state growth. I personally would like to see archiving happen as sort of a solution where kind of like just state is deterministically pruned from the tree, but it can be restored later.
00:34:14.510 - 00:34:56.970, Speaker A: So I have a big contract or something and I haven't touched it in a while. We just kind of purge it out of the tree, but it's not lost. You can initiate a transaction that restores the state and then you would prove that that's a valid state. And so that's kind of like an archiving solution. I would like to see that first before we maybe go to storage, rent, or that sort of thing where you're kind of forcing people. It just seems like less intrusive to keep stuff around but keep it archived as opposed to actually deleting it. That's just not a good user experience.
00:34:56.970 - 00:35:33.640, Speaker A: You haven't logged into Google photos for a year and all your photos are gone. That's not a good experience for the user. But if Google said, okay, well, you haven't logged into Google for a while, we're going to archive your photos, and if you want them back, you have to pay a little fee, and we're going to go out to the warehouse and get the tape drive and bring it back in and restore your photos. That seems more reasonable to me. It's an interesting problem and a lot of people have proposals and yeah, we'll be looking at that. But for version zero of Monad, it's not really not a concern right now.
00:35:34.810 - 00:35:44.970, Speaker C: Is there any compatibility with existing EVM contracts today? Can I just pick up a contract I deployed on Eth mainnet and then bring it over to. And would I have any issues doing that?
00:35:45.120 - 00:36:05.178, Speaker A: No issues, no. So we replay our whole test. Like I mentioned, our testing is replaying Ethereum history. So everything is currently active on Ethereum. Should run on Monad. And we also run the Ethereum test suites and all that sort of stuff on Monad code as well. It should be identical.
00:36:05.274 - 00:36:14.450, Speaker C: And was that like some of the core reasoning behind using the EVM and optimizing it, as opposed to taking just a blank slate and kind of building your own execution?
00:36:16.090 - 00:37:00.318, Speaker A: I mean, we did look at WASM early on, so WASm is somewhat source compatible. Like you can compile solidity to WASM, but we just felt that honestly, I think people make too much of a big deal about the vms of these things. They're all pretty standard. I remember doing Java VM back in the day, so they're all pretty similar. There may be some slight architectural things that are different underneath, but EVM is kind of weird. Like it has 32 byte word sizes and this sort of stuff. But besides a little bit of these weird things, it's just a standard stack based VM.
00:37:00.318 - 00:37:30.060, Speaker A: And there have been a lot of vms in history and there's nothing fancy or different about it besides these kind of little quirky sort of things. So there's no really good reason to run away from the ethereum VM. It's a fine vm. So going that route. Yeah. Then we're fully bytecode compatible. How would I replay ethereum history if I did a wasm vm? I wouldn't be able to.
00:37:30.060 - 00:37:39.600, Speaker A: It just made sense to just stick with what's there. It works. It's fine. There's nothing wrong with. Yeah.
00:37:41.010 - 00:38:08.390, Speaker B: So as a question from a guy with zero coding knowledge, for the most part, I'm in like elementary school for SQL and Python. So I saw that the Manad client is written in Rust and C. Like why first of all, and then second of all. And again, this is coming from a not super technical guy, but is there any challenges with getting a bytecode compatible EVM execution environment when you have your client written in these other languages.
00:38:10.410 - 00:38:34.202, Speaker A: So first, as to the why. If you want performance, if you work at a high frequency trading firm, you're going to write your software in C Plus plus. That's just the way it is. See your c plus plus, right? There's no other language. Now, Rust is coming along. But in my experience, people don't use rust in finance, at least not yet. So there's no other language that's going to give you that level of performance.
00:38:34.202 - 00:39:02.310, Speaker A: Not go has garbage collection, memory, memory allocation. It's just not as fast. It's quirky language as well. Obviously, Python and these sorts of things are non starters. They're not fast languages. What's available? What are fast languages? It's basically c, C plus plus. And now Rust has come along, and frankly, that's just like our expertise.
00:39:02.310 - 00:39:28.750, Speaker A: If there was another fast language that we didn't know, then that'd be a different thing. But it's like, that's our expertise. We've been writing c plus plus code for a long time. The team knows it very well. So it just doesn't make sense to try to use a different language in terms of related to the EVM. No, I mean, there's been C plus plus. There are C plus plus implementations of the EVM under the Aragon architecture.
00:39:28.750 - 00:40:01.660, Speaker A: Aragon is in go, but there is silkworm, which is written in c plus plus that's still actively maintained and developed. There was a rust thing, I think paradigm has that ref client that's in rust. And then, yeah, historically there have been other c plus plus limitations. So, yeah, the implementation language doesn't have any bearing on what's implementing the Ethereum VM. That's a totally separate thing.
00:40:02.190 - 00:40:26.760, Speaker B: Okay, thanks. That's super helpful. And then just doubling down on Dan's question, like, okay, if I have uniswap deployed on Ethereum, is it really easy to just copy paste, go on to Manad? Is there any scenario in which that is not the case? Because I've seen a lot of zkvms different. L two S claim Ethereum compatibility, but it's really not quite as out of the box as they kind of advertise it to be.
00:40:28.810 - 00:40:58.350, Speaker A: I'm not aware of any compatibility differences. Like I said, we literally replay Ethereum history to see how fast we can execute. So if somebody had deployed something on Ethereum that wasn't compatible with Monad, that would fail. That exercise would fail. Right. So maybe somebody has a smart contract somewhere that's not deployed on Ethereum and hasn't been used? I don't know. Yeah, but as far as we know, we're 100% compatible.
00:41:00.770 - 00:41:05.540, Speaker C: And then on a percentage basis, when you replay that history, how much faster are you?
00:41:07.190 - 00:41:33.226, Speaker A: Yeah, like I said, it varies by history. So early on, like I said, you can do 50,000 transactions per second on a single core monitor is probably not making much of a difference there. Right. But, yeah, I don't have the latest numbers. It's also kind of funky because Ethereum blocks are small. Right. So we're really targeting 10,000 transactions per second or maybe even more.
00:41:33.226 - 00:41:50.974, Speaker A: And that's one block. Right. So our block time is 1 second. So our blocks will be about 10,000 transactions or more. And Ethereum blocks are 200 transactions or something. 250, I can't remember. It's not really.
00:41:50.974 - 00:42:23.082, Speaker A: Totally. You're trying to calculate the Merkel route after the state, right. So after each block, you have to calculate the new Merkel route. And so when you're replaying Ethereum, you're calculating the numerical route after 250 transactions, whereas for monad's anticipated usage, you would be calculating a numerical route after 10,000 transactions. So there's some things that are kind of like, not fully aligned in terms of making like, a fair performance comparison. But, yeah, we'll do more work on that eventually. At some point we'll publish some numbers.
00:42:23.082 - 00:42:46.894, Speaker A: We'll just say, okay, like I said, the historical stuff is not that interesting. So we'll publish some numbers. We'll say we replayed Ethereum from block 15 million to 16 million. It took this long, that sort of thing. I know a popular metric that people like to post on Twitter and stuff is like sync time or that sort of thing. But again, that's syncing from the beginning of Ethereum. Different transaction makeup.
00:42:46.894 - 00:43:36.202, Speaker A: Those numbers aren't really fair comparisons. And even the Aragon architecture, it uses what they call stage sync, and it doesn't compute the Merkel route until the very end, after you've executed x number of blocks. So it's not really these numbers that people use throw around. I'm sure you guys are aware, like marketing, crypto marketing numbers are kind of baloney half the time. What? No way. Yeah, so these kind of things don't make sense. At some point, we need to produce a rigorous, open source comparison, and it needs to be on recent data on Ethereum and how fast can each client sort of replay these numbers? And then it'll just be open and kind of like untamperable sort of source of truth on performance.
00:43:36.202 - 00:43:39.698, Speaker A: But yeah, I don't have the numbers for you right now.
00:43:39.784 - 00:43:56.966, Speaker B: Is the bet in the long term that kind of similar to the solana bet, hardware is going to improve and therefore Manad throughput will improve as hardware improvements accelerate? Or do you see a world where maybe roll ups do live on Manad? Or is that just like roll ups, aren't it right now?
00:43:57.148 - 00:44:35.010, Speaker A: I mean, if the l one can handle all the transactions, why would you have an l two? That doesn't really make sense to me. Like you're just introducing pain for no gain, right? So we'd have to see maybe somebody else has something that I don't understand, that the use case for an l two. I think one case that actually is mentioned is on an l two. You can introduce, let's say, primitives in the computation that are not on the l one already. So that's an actual real reason to do an l two. It has nothing to do with performance. Right.
00:44:35.010 - 00:45:46.460, Speaker A: So those sorts of cases I would have to look at on a case by case basis and says, like, okay, does that really make sense? Does it not make sense? Monad also wants to do some innovations and some extensions on chain that really opens up and allows you to build new sorts of applications and that sort of thing. But for us, for v zero, it's just pure performance. It doesn't mean that performance is Monad's only targeted innovation. It's just kind of like if you don't have the performance, you can't really, everything else is kind of a non starter, right? Like if you can't support a large number of users of the chain, then why innovate in other areas? It doesn't even make sense. So we have to start with a good fundamental base of a high performance system, and then we can come and we can innovate and we can add new features and new ways to write software for on chain and all that sort of stuff that really allows app teams to build great apps that people want to use, but that's going to come in later versions. So I would have to see those sort of like l two use cases. Like if somebody says, oh, I really want an l two on Monad, well, I would like to have a conversation with them and understand what's missing from Monad that you need an l two.
00:45:46.460 - 00:46:03.600, Speaker A: That would be an interesting thing to talk about. And maybe what's missing is that something that can be added to Monad directly and they don't need to go through the l two route, because as I said, it's not a good experience for composability and those other sort of things.
00:46:04.130 - 00:46:31.958, Speaker C: Yeah, maybe like something around privacy or KYC comes to mind, but we haven't really seen that on Ethereum yet either. So I liked your point as well as about additional computation. That'd be pretty cool. But I think you mentioned this earlier, that this is still kind of in the development phase more so. But I want to talk a bit about transaction fees. So with a lot of the high throughput chains, you're getting super low fees. And that's kind of like one of the features is you can run different types of transactions that wouldn't make any sense on Ethereum.
00:46:31.958 - 00:46:36.860, Speaker C: So different types of trading techniques, for example. Is that what we can expect from an ad as well?
00:46:37.630 - 00:47:35.180, Speaker A: Yeah, so I think the more expensive part of the transaction is really like getting the data around. So again, we don't have the fees worked out, but I think the fees are going to be more about how big is the transaction, how much does it state, does it touch that sort of thing? Less so than. I'm not sure that the gas fees that are currently modeled on Ethereum are really reflecting the total picture of the costs of execution. So we have to evaluate that. But the thing is just getting data around, right? Like sending it from here to Singapore and to London and all over the place. And the bandwidth cost, the network cost, that's a big chunk of know. Preliminarily, I think the gas fee should reflect that more.
00:47:35.180 - 00:48:02.680, Speaker A: Know. I did an expensive computation. That's probably not a big of a deal cost wise to the blockchain. We want to incentivize good computation, right. A point I make often is like blockchain apps are very simple. Think of how much code we're using this website now to stream this, right. Think of how much code is in that website versus like the fanciest on chain program.
00:48:02.680 - 00:48:43.922, Speaker A: There's a magnitude of difference in the number of code. So I think also part of adoption is people building real apps that people want to use. And to do that you have to be able to support a significant amount of computation and a significant sort of features. And I think that stuff will come later, but it's not meant to be like running programs that people can write in third grade sort of thing. They're supposed to be really building good stuff here. So we really want to make sure that monitor is in place to execute that software and build apps that people really want to use.
00:48:44.056 - 00:49:26.930, Speaker C: So I want to hammer down there, because we've seen a large number of high throughput chains come to life of late, right? So obviously Solana Apto Sui say there's starting to be this, like a lot of people are starting to think in that same vein. So what are some of these applications that could exist? I guess right now my issue is all we're seeing is things that were built on Ethereum forked over to these new locations, and it's like, okay, well, great, now it's like a slightly better version of something that already existed. So how's Monad going to really incentivize organic adoption of these new ideas of like, hey, we have this new technology where new things can actually be built, so let's build those things.
00:49:27.000 - 00:50:11.674, Speaker A: If I had a great idea for an app, I'd probably build it, but that's not my specialty. I'm a high frequency trader here. I think our expertise is in trading. Obviously, that's our background. Keoni and I ran an HFT trading team for a long time at, you know, I think we understand that part well. And so we can basically help push the frontier of on chain, that area, that discipline. But in terms of consumer apps or social apps or that sort of stuff, that's not my expertise.
00:50:11.674 - 00:51:01.262, Speaker A: I personally don't know that. But yeah, we have to work with app teams. And I think from my experience that there's not a lot of collaboration with the core chains, sorry, with the chains and the app teams. So I think from a different perspective, I would like Monad to be collaborating with app teams and helping them design, implement their software in a way that's high performant. And really if I can help understand their problems and then I can help them resolve those problems, then that's going to make them more successful. So I do heavily see this as like a collaboration. An example is like the Microsoft Office team has access to the Microsoft Windows team at Microsoft, right.
00:51:01.262 - 00:51:33.494, Speaker A: And that allows them to do a lot of things. And there's been a lot of changes to Windows that allows Office to advance. And so that's kind of an old example. I wish I could think of a more recent example, but having access to the people that are building the technology underneath you is, I think, a very valuable thing. And we would like to collaborate with app teams and really help enable them to build real applications that are going to be desired by users.
00:51:33.622 - 00:52:12.200, Speaker B: I absolutely love that approach. It's pretty funny, actually, because I feel like I talk to so many infra people and it's funny how much they don't really know about what's going on. At the DAP layer. So to hear that you're hoping to foster some collaboration between the two teams, I think is actually a really solid recipe for success. But I do want to jump over a little bit to, I guess, Monad BD and actually BD this time in terms of like bridge integrations. How are people going to get over there once it's live? If you even know yet what Dapps have you talked to from the Ethereum world that are interested in porting over? And honestly, what's the general timeline on what you guys are doing?
00:52:12.650 - 00:52:46.050, Speaker A: Yeah, so for bridging, there's obviously a number of different options, number of different projects out there. We're talking to them. I'm not so much on the BD side of things. My head is in c plus plus code all day. So yeah, I can't provide the latest and greatest updates on where we're at with BD. Technical wise, though, I think a bridge, we have to have some sort of canonical bridge that's integrated with Ethereum. I would like to do something that's very secure.
00:52:46.050 - 00:53:27.114, Speaker A: But then other bridges, other projects will come in and have their bridged assets and all that sort of thing. But we need some sort of like basic level, very secure bridge at the base level. But yes, we're obviously talking to all those teams. I'm not in most of those meetings because like I said, I'm always coding and running simulations and that sort of thing. So I'm not a good person to answer about those. But yeah, we have signed most of our partnerships that you've seen. We announce them on Twitter or whatever, and they're announced and then there's obviously tons of conversations going on behind the scenes.
00:53:27.114 - 00:53:43.220, Speaker A: The BD team is very active, always meeting with people all day, every day. It's just constant. Some, there's going to be a lot of good projects online and I'm pretty confident of that. But yeah, I can't share any specifics right now.
00:53:45.110 - 00:53:54.790, Speaker C: Taking you back into the code, then, what are the biggest challenges that you face and what are the last hurdles that you're like, all right, if we can solve this, this is going to be the best blockchain that exists.
00:53:55.930 - 00:54:25.460, Speaker A: Yeah, it's easy to write something. It's easy to write software, right? You can write an application for the desktop or you can write a web page and if it goes down, it's fine. Maybe somebody's upset, they lose their work or something, or expedia goes down. You can't book a flight for the next five minutes they bring it back up and like, okay, now I can book my flight. These sorts of things. With blockchain, that's not really an option. Right.
00:54:25.460 - 00:55:06.494, Speaker A: So there's reliability in terms of it can't go down. So that's a big piece of engineering to just make sure it shouldn't go down. And then there's hacking, obviously. So are we messing up somewhere? Is somebody going to read the Monad implementation and say, oh, if I send these certain instructions in this transaction, I can destroy the system or I can transfer money to myself or that sort of thing. So you don't have the same level of luxuries that you have with normal applications where, like, okay, it goes down, I'll just restart it. That sort of thing. With the blockchain, you have to be very deliberate about everything you do.
00:55:06.494 - 00:55:29.298, Speaker A: Very careful, super rigorous, and, yeah, that's what keeps me up at night. The performance problems, they don't keep me up at night because I'm confident we can always solve those sorts of issues. But somebody hacking or somebody being able to do a denial service attack or something like that on Monet is what really keeps me up.
00:55:29.384 - 00:55:45.954, Speaker B: Well, I think that's a good spot to close it up. Unless, Dan, you had any other questions, jump in. But James, it's been a pleasure to speak with you. It's awesome to hear someone who's really on the technical side of things and can teach me things. So I learned a lot. Do you mind sharing people, I guess, where they can find you? Learn more about Manad.
00:55:46.002 - 00:55:50.778, Speaker A: Oh, yeah. Our website is Monad XYZ, so XYZ.
00:55:50.874 - 00:55:51.182, Speaker B: Awesome.
00:55:51.236 - 00:55:53.050, Speaker C: Thanks a ton. James, it's been a pleasure, man. Cheers.
