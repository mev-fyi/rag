00:00:00.250 - 00:00:06.560, Speaker A: So at a high level, we're building the best infrastructure for exchanges. The question then becomes, what does that infrastructure look like?
00:00:09.090 - 00:00:33.602, Speaker B: What's up, everyone? Welcome back to another episode of Zero X Research. Today is March 27, and we have a great interview lined up with Jay, the cofounder of say, which is a high performance cosmos chain designed with Dexes in mind. But before we get into the interview, as always, we are joined by two blockworks research analysts, Zero X pibbles and Ren, to discuss the latest market happenings. Ren, I'll kick it over to you first for your hot seat or cool throne.
00:00:33.666 - 00:00:34.038, Speaker A: Sure.
00:00:34.124 - 00:01:13.538, Speaker C: So I have a hot seat this week, and the hot seat lands on Gary Gensler and Sec. So five days ago, SEC sent a wells notice to Coinbase. A wells notice is basically a request for information for an ongoing sort of like, legal process. And it basically is the SEC going hinting you're in trouble. You can kind of own up now and provide us with the information that you think you're in trouble for. Or in Coinbase's case, they just say, screw you, we're going to ignore this and we're going to fire you in court. So Coinbase identified the wells notice was regarding an undefined portion of listed digital assets, the Coinbase earn staking service.
00:01:13.538 - 00:01:45.930, Speaker C: So not Coinbase ETH. That's something else. Coinbase prime and Coinbase wallet. But in reality, the Wells notice gave Coinbase zero clarity on basically what type of infringements they had. And after that, we had a bit of discussion on Twitter. Coinbase's chief legal officer, Paul Grill, stated that Coinbase met with the SEC more than 30 times over the past years. They described their staking and asset listing processes pre the IPO, which the SEC approved, and they've gone the full regulatory compliant vote.
00:01:45.930 - 00:02:32.286, Speaker C: They've acquired two broker dealer licenses in 2018, obtained a dcm and DCO license from the CFTC, and even shut down Coinbase lend towards the end of 2021. So this action from the SEC is pretty disappointing because they have provided zero feedback, they've given zero regulatory clarity, and they have developed zero regulatory frameworks for crypto or digital assets. And suddenly they're going full enforcement and cracking down on Coinbase, even though they've been one of the most quote unquote, well behaved players and centralized exchanges within the space. And furthermore, they've cracked down pretty hard on a lot of stablecoins. For example, Paxos. They cracked down pretty hard on Kraken, the staking program. And it really makes you wonder what their end goal is here.
00:02:32.308 - 00:02:32.494, Speaker A: Right?
00:02:32.532 - 00:02:45.682, Speaker C: Do they just want to shut down everything crypto related within the United States? Is Gary Gensler just a bitcoin maxi and doesn't want anything centralized. But yeah, the SEC is definitely in the hot seat right now.
00:02:45.736 - 00:02:45.906, Speaker A: Yeah.
00:02:45.928 - 00:03:21.038, Speaker D: I guess the ultimate question becomes, I don't know. To me, this feels like the last battle, right? If Coinbase goes down, it does start to look pretty grim at what's left and how we build from there. But I really liked the way that the company, and specifically Brian Armstrong, is kind of positioning themselves to really take on this fight. Like, hey, we don't believe we've done anything wrong. We made an effort to comply with you. We were a publicly traded company on an exchange that you oversee. You've read through our financials, you know what we do.
00:03:21.038 - 00:04:10.170, Speaker D: We want to comply. You're just making it unreasonably hard for us to comply. And if you're just following sound reasoning, that seems like a good argument. And I feel like that makes the average person want to defend Coinbase, to be honest, it just feels like the classic story of government overreach and they seem like the good guys trying to do the right thing, which I guess is super contrasting to the binance news you hear today, where it's just like yet another example of, for some reason, it's so hard for these exchange CEOs to not trade against their customers. But I'm sure we'll get to that later. But I bring that up just to say Coinbase really has tried to do everything by the book as so it seems, especially relative to its peers. And this just.
00:04:10.170 - 00:04:25.598, Speaker D: I don't know, it feels like the last battle and Brian Armstrong had a good tweet thread about how they're preparing to fight this and they believe what they've done is right and they've been by the book. And I was like, I had a response tweet to it. I was like, all right, I'm ready to suit up. Where do you need me? Like, General Armstrong, you're my guy.
00:04:25.684 - 00:04:57.174, Speaker B: Yeah, I don't have a whole lot to add to it. It's just an unfortunate situation, and we just got to sit tight, root on Brian and hope that his baldness prevails. But, yeah, I think just in general speaking, it's going to end up being a good thing. Once we do get the regulatory clarity, that's when the big money institutional capital will actually be able to play around in the space. We'll actually know what's right, what's wrong, what's taxed this way, what's taxed that way. It's regulatory clarity that we've been seeking for the last two, three years at minimum. So I'm just excited.
00:04:57.174 - 00:05:00.246, Speaker B: It's actually starting to get underway and I'm just going to hope for the best outcome.
00:05:00.358 - 00:05:25.058, Speaker D: Yeah, no, I think that makes a ton of sense. And again, if it feels like if we can climb this mountain, then getting that framework kind of sets the precedent of like, here's how you comply in the United States. And I guess assuming the Coinbase win, it kind of pushes us in that direction. But yeah, I'd love to hear somebody talk deeper on the finance side of things. Does anyone have them in the hot seat this week?
00:05:25.144 - 00:06:14.674, Speaker B: Yeah, that's actually my hot seat. And it's kind of ironic because they're getting, or, sorry, CZ is getting sued by the CFTC, which is the US domiciled agency that oversees futures and commodities trading. So basically they're suing CZ for offering investment products like options futures to US customers, which apparently isn't allowed in this jurisdiction. And there was a lot of different messages on signal and encrypted messaging app between the company and apparently they were auto delete conversations and that they were also trading on personal accounts on the exchange that they run, which is clearly a conflict of interest. That does not sound good. And this comes right after all the other regulatory crackdown we've seen in the crypto industry and Paxos with BUSD. There's some rumors that maybe this might be tied to USDT in some way.
00:06:14.674 - 00:06:49.822, Speaker B: I think that's like the absolute worst case scenario and I don't think it's plausible or likely, I should say. But I think the most notable thing to take away from it is the fact that the CFTC is trying to regulate crypto at the same time that the SEC is, who know securities and the CFTC specifically called out bitcoin, ETH, litecoin and a couple other stablecoins, calling them commodities. So hopefully the CFTC gets their way and wins this territory and gets deregulated, honestly, because it seems like they're a little bit more friendly than the SEC and Gary Gensler, but that's kind of the TLDR on this situation.
00:06:49.956 - 00:08:12.250, Speaker D: Yeah, I guess the first thing that pops into my mind on that is, is there any contagion from this? I feel like we're all so scarred of the last twelve months of one bad thing happening leading into four other bad things. So first thing I did was pull up dune and head over to Tom Wan's finance flows dashboard, and they've experienced around a 500 million dollar outflow today, which sounds like a lot on the surface. But if you flash back to the FTX drama, in November 22, there was a single day, I believe it was the 5 November 2022, and there was a $2 billion outflow in one day. So on the grand scheme of things, this is kind of another day for finance, which is good, right? Because if there was some malpractice going on over there and there was all of a sudden large outflows for consecutive days in a row, then I would begin to get concerned of like, okay, well, if they kind of were playing around in a couple bad pools of water, like, were they not backing assets one to one, is this going to lead to another FTX like situation? But the early signs, there's not a mass rush to the exit, which is at least a positive development in my mind. And then I guess the other question is, if we lose binance, what's the impact on the industry? And I guess I'd kind of pose that question to you guys. A, do you think binance goes down? And B, if so, how does that impact the growth of the industry?
00:08:13.550 - 00:08:13.914, Speaker A: Yeah.
00:08:13.952 - 00:08:52.790, Speaker E: So I have a couple thoughts on this. First, I don't think binance is going down off of this, even though CZ tweeted four today, which is associated with death in China, I believe. But I think that they're going to go after finance for having futures trading open to US customers in 2019. But I think that they really circumvented that in 2021. I mean, we can't log on to the finance website right now and go trade like 50 x leverage. So they have complied. It just took them a little while.
00:08:52.790 - 00:09:00.380, Speaker E: So I think, if anything, maybe they pay some fines, but in two months, I think we'll be over this.
00:09:01.550 - 00:09:36.114, Speaker C: Yeah, I would add on that, I would imagine financial CC has a pretty big war chest that is a lot bigger than what we know of. And even if they settle, or they have to shut down everything that they did wrong, or even if they shut down the apparent 300 internal accounts that they used to trade on their exchange against their customers, even if they shut all of those down, at the end of the day, finance is still a dominant central exchange. They have probably more than 85% market share, and they are still a very profitable business as a whole.
00:09:36.152 - 00:09:36.354, Speaker A: Right.
00:09:36.392 - 00:09:58.460, Speaker C: Even if you take out the US portion of their business, they make a lot from trading fees from other countries all over the world and are probably one of the only other companies or products that has found product market fit. Whether it's binance, institutional binance, retail degents on binance, smart chain like, there is still a lot of user activity there that won't go away anytime soon, I believe.
00:10:00.270 - 00:10:25.250, Speaker B: Yeah, agreed, ren. And I guess just in terms of market structure though, they are the place where a lot of people in the world go to trade. So if something would happen to like, it would obviously really hurt the amount of liquidity available for trading, and then I would expect more volatility just in the space in general. So whether that's to the downside or the upside, just less capital is going to move markets in a lot more drastic of a way. So that's something to pay attention to.
00:10:25.320 - 00:11:48.318, Speaker D: There are two other interesting things that I got out of this. One was that the CFTC got a hold of these telegram or signal messages and knew they were set to auto delete. So my question is how. To me it feels like either there was like an internal leak that happened and somebody kind of brought this to the CFC from the inside, which would be kind of interesting, or B, did they somehow force signal to give them those messages? So, yeah, I don't know, does anyone have any takes on how that would happen? And then the other thing is, I guess just the size of the fine is then if they do settle out and don't take this to court, I would assume it'd be like a massive fine that kind of leads to CZ not having to admit guilt in any way, and then binance keeps operating. So the question is, is the size of the fine something binance can pay? And one of the points in that was like any US customer affected by a liquidation, they can be fully reimbursed for that loss because it never should have happened, which is pretty interesting. So if that's true, we were all trading over the last year, and when throwing a 50 x leverage trade on in a down only environment, it pretty much ends one way. So if they have to repay every liquidation that happened to customers that were based in the US, that could be a pretty significant sum of money.
00:11:48.404 - 00:12:36.320, Speaker C: Yeah, I would say that towards the first point in terms of signal is going to be a pretty encrypted peer to peer app, and it's probably the messaging app you want to use if you want to lean on the privacy and secure side of things. I wouldn't be too surprised if it was a disgruntled internal employee, because even though finance is like a 4000 5000 large organization, I have heard their work culture is pretty crazy, and it's pretty intense, like, super long days. And so, yeah, I wouldn't be surprised if it's an employee that says, okay, I've had it with CZ. I've had it with this company. I'm going to leak it because I'm not sure how else the SEC would get auto deleted messages now. And I would assume they were pretty conservative with their deletion timeline. Maybe it's, like, auto deletes after 30 or 60 days.
00:12:36.320 - 00:13:01.014, Speaker C: So, yeah, as far as the second point goes, I don't think the fine will be substantial enough that it will cripple finance. And even in the hypothetical worst case, where the finance starts enough to completely deplete their cash reserves, I think they'll just continue from there, continue operating as a business, because they are still profitable right now, even in a bear market, I would guess. So I think that's fine for them.
00:13:01.132 - 00:13:13.770, Speaker E: Yeah, definitely. I can go next. I have a cool throne for Euler. Euler finance. I'm going to call it Euler. I know, after capital, let's call it Euler, but I think that sounds weird.
00:13:14.990 - 00:13:15.354, Speaker A: Yeah.
00:13:15.392 - 00:13:40.930, Speaker E: So they got hacked for 200 million. That was a week or two ago, and that was a pretty devastating hack for a lot of people. What's interesting is that whoever hacked it sent an on chain message, and they're like, we're not going to keep funds that aren't ours. And also, he did some weird stuff. Like, I know he sent some funds to the Ronan Bridge exploiter.
00:13:43.110 - 00:13:43.714, Speaker B: Who we.
00:13:43.752 - 00:14:23.242, Speaker E: Thought had something to do with the Lazarus group, the north korean hackers. But I don't know. He could have just been trying to muddy up the trail. But what's really neat is over the weekend, they got, like, 5800 ETH back, and as of today, on Monday, they got, like, another 30 mil back. So they've recovered, like, $130,000,000 so far. So that's just things you love to see, because the Euler team is a bunch of great guys, and I think it's pretty critical for DFI that we figure out how to get these funds back to the affected parties.
00:14:23.386 - 00:14:23.694, Speaker A: Yeah.
00:14:23.732 - 00:14:44.790, Speaker B: The thing that was weird to me is just how long it took him to actually send the funds back. And then also, like you said, he sent, like, 100 ETH to three or four different wallets of different prominent tags. So I'm not really sure what the logic there was. I'm sure he just. He, she. They came to the realization that, I can't do anything with this money. I might as well just return it at this point.
00:14:44.790 - 00:15:16.138, Speaker B: And that's why I don't really understand these hacks in the first place, I guess teaching people a lesson, but why not do that and just submit a bug report and hopefully get a nice bounty? I know some of those bounties aren't always the best, but it's better than getting thrown on someone's radar in terms of getting in a lot of trouble and arrested in your own jurisdiction. So, yeah, surprised to see it, but also happy to see the funds get returned. I agree with you, Spencer, that Euler is a good protocol and they were just trying to build a cool, new, innovative product and it just kind of ended, unfortunately.
00:15:16.234 - 00:15:16.494, Speaker A: Yeah.
00:15:16.532 - 00:15:45.820, Speaker D: And the way it happened as well was just super unfortunate. They were very rigorous in their audit structure and there was like one contract with one piece of it that wasn't audited. And of course that was where this bug was living. So really just an unfortunate break for them. And the question is, how do they bounce back? How do you guys view protocols that have had significant hacks in the past, but have since patched that bug and are continuing forward? Is that a protocol you would use again or does it leave like a black mark on them?
00:15:47.310 - 00:15:57.760, Speaker E: They can't get hacked again, right? And Rari finance get hacked like two or three times. What's the other one? Thor chain. Thorchain got hacked multiple times.
00:15:58.850 - 00:16:28.440, Speaker D: Thor chain had some bad beats as well, for sure. But I guess the counterargument is if you do squash out some of these major bugs, then not to say how many can be left, but I don't know, I could kind of see you as that being like a hardening process. Right. The longer you're alive, the further you're battle testing. And that really proves to be true. But it's like, I guess you kind of start over at ground zero now and you lose all that previous battle tested acumen that you've built up.
00:16:29.390 - 00:16:50.302, Speaker C: I want to say that I lean towards it being more bullish, actually. Obviously, it sucks that right now there's $70 million that are lost out there, but at the end of the day, if you do have a good product, there will be new participants that come in, save two or three years down the road, and chances are they will have no clue that you ever got hacked before.
00:16:50.356 - 00:16:50.574, Speaker A: Right.
00:16:50.612 - 00:17:08.482, Speaker C: And they're going to think, wow, this is like a great new lending volume protocol. It's really cool. I have different collateral categories and I'm going to use it. So at the end of the day, it's still how good of a product you are. Obviously, that's a very simplified statement, but I lean towards it being a good event for you, and I think they can bounce back.
00:17:08.536 - 00:17:28.362, Speaker B: Yeah, it's a good reminder, too. Just like, you can't just look at TVL and time in the market and think, all right, I'm completely safe. I can ape my entire balance of stablecoins into here to earn 3%, 4%, whatever it was offering. There's always risk, no matter where you put your money in crypto. So it's kind of a nice wake up call and reminder on that front.
00:17:28.416 - 00:17:49.870, Speaker D: Yes, strong agree there. And I can take us home here with another cool throne. And this one kind of ties into what we talked about last week. Arbitram claimers. The airdrop is still pretty white hot right now. Token price is kind of settling around a buck 20, making the total airdrop value about one and a half billion dollars. That went straight to users of the chain.
00:17:49.870 - 00:18:47.570, Speaker D: So, again, in total is about 625,000 eligible addresses, which there's been some interesting data done on what percentage of those were cyber attackers, and seems like the general estimation is about 20% of those 625,000 wallets seem to be related to cyber entities. That I kind of slipped through that work done by the arbitram and Nanson teams to kind of prevent that, which I feel like that's kind of like an impossible bug to solve, and I'm no cyber expert by any means, but seems like everybody was cybling this for the exact same reason. Like, you're giving out a billion and a half dollars of free money. I'm going to try to see how much of that I can grab. Was kind of like the logic. So they did deter a good number of those, but it does seem like there is still some analysis showing that there were still some cyber attackers that received airdrops. So, nonetheless, token price has kind of based out around a buck 20.
00:18:47.570 - 00:19:12.294, Speaker D: I was kind of thinking fair value in the range of. So it still seems to be in that fair value range. It puts it slightly above optimism in terms of market value. And based on the TVL and daily active users, that seems ballpark fair. It's got a much more vibrant native defi ecosystem, and you see that still really having an impact.
00:19:12.342 - 00:19:12.506, Speaker A: Right?
00:19:12.528 - 00:19:52.038, Speaker D: So if you think about an airdrop, you want to reward users for using your chain, but you also want them to continue the participation of the chain. And net inflows are still in for stablecoin inflows, meaning users are still bridging value to the arbitrum chain. And you also see things like on chain volume still very high. So generally, Ethereum is always really the hub of on chain Dex volume. And you usually see, like, BSc coming in at that number two slot. And right now you see arbitram by a mile. So over the last seven days, there's been about $12 billion of Dex volume on ethereum and 5 billion on arbitrum.
00:19:52.038 - 00:20:39.166, Speaker D: So you really see that change of pace, and people really are trading arbitrum, the token, on the chain, as well as really promoting just broader DFI participation in the ecosystem. So it seems like it was a fairly successful airdrop in terms of rewarding users and getting eyeballs on the chain. And it'll really be interesting to kind of take that next step of, okay, well, we've made this token. How do we put value towards the token? Give it utility. And really that logical progression there is through governance. So they did launch the arbitrum dow, and when you claimed your airdrop, there was sort of like this auto step to delegate your tokens. As we move forward, we're going to see a little bit more involvement on the governance side of things, and that's like a gradual step towards decentralization.
00:20:39.166 - 00:21:07.202, Speaker D: So exciting to see the focus there. And then, of course, the humble shill here. If you are an arbitram token holder and are interested in delegating your tokens, be sure to delegate to blockworks research. We really believe in promoting growth in this ecosystem and excited for the opportunity to kind of hold that governance responsibility. So if you go to our arbitrum dashboard, at the very bottom, there's an Arbs delegation section, and feel free to check out that. And there's a couple of links you can click there.
00:21:07.256 - 00:21:26.886, Speaker B: Yeah, I would take the other side of this one myself. I think the airdrop was a really poorly designed airdrop. I had three or four wallets that got a drop, and I hardly used two or three of them. I had friends who did one thing on Arbitrum and got an airdrop as well, immediately sold it, of course, in order to just cash out and put.
00:21:26.908 - 00:21:27.522, Speaker E: It in the bank.
00:21:27.586 - 00:22:03.230, Speaker B: So, yeah, I don't know. I think they should have done something. I don't know what they should have done, but maybe just, like, looking at people who got liquidated on GMX or something, or different things like that, I think the methodology was a little too straightforward. The only one I thought had a lot of credence was Nova Bridgers, which is their kind of, like, data availability committee chain for gaming and social applications. So I didn't actually get that one too. So I'm actually unbiased here, but I didn't think the actual allocation was super effective. I'm curious if you guys agree or disagree.
00:22:03.390 - 00:22:26.598, Speaker D: I feel you could also make the counterargument to that, because there wasn't a whole lot going on on Nova. The only people who went there were people trying to farm the airdrop because like, oh, let me check this box. So I think that really just proves how hard it really is to kind of weed out who's deserving of this token. It's obviously very easy on a case by case level, but you can't do that with around 625,000 recipients.
00:22:26.694 - 00:22:51.394, Speaker E: I think what would have been nice and very appreciated is an extra reward for the people who bridged Arbitrum day one and were farming Arbane, got rugged on, like, those were the good og days. And it breaks my heart to see someone who plays with Dopex and GMX getting airdropped more than me. They weren't there for the day once.
00:22:51.512 - 00:23:36.210, Speaker C: Rvnian was definitely good times. I think another thing about the airdrop that rubbed me the wrong way was that they included previous civil attackers in the list of civil attackers for this airdrop. With me said, for example, if you civil another airdrop, such as hop protocol, they use their list and included that list for this airdrop. And personally, I feel like, sure, if you civil attacked something in the past, you shouldn't be eligible for that previous airdrop, but this is like a separate instance. If you didn't civil the arbitration airdrop, you shouldn't be penalized just because you siboed and previous airdrop. But that's definitely like a very personal and controversial take, and I'd like to hear your differing opinions.
00:23:37.110 - 00:23:44.430, Speaker B: Wait, Ren, did you say that they did exclude people from the airdrop who had previously cyboled other protocols airdrops?
00:23:44.510 - 00:23:49.010, Speaker C: Yeah, they did. They excluded previous civil attackers for other airdrops.
00:23:49.590 - 00:24:07.290, Speaker B: Okay, see, that makes a ton of sense to me. It's just like the same as a loan. If you go to a bank or something, like they're going to look at your previous credit history. Like, if you've done well, paid your bills, et cetera, then you're going to get a line of credit, but if not, then you're not. So. Makes sense to me. But I could see the other argument as well.
00:24:07.360 - 00:24:32.450, Speaker D: Yeah, I'm in your corner here, Sam. It feels like. I don't think this is probably not the first time. But one of the larger occurrences of your on chain actions are creating an on chain reputation. And that's like, if you talk about decentralized identity, that's kind of building towards that direction. And what you do on chain is creating this whole personal profile. So I don't know, I kind of like it too.
00:24:32.450 - 00:24:33.810, Speaker D: I think it makes sense.
00:24:33.960 - 00:24:34.226, Speaker A: Yeah.
00:24:34.248 - 00:24:36.674, Speaker D: If you tried to cheat one game, you're not going to come cheat my game.
00:24:36.712 - 00:25:11.662, Speaker B: I think that's a good spot to call it over to the interview with Jay. I cannot recommend enough for you guys to all check out blockworksresearch.com. If you go over to the research tab and toggle free research, you're going to get access to some of the best free reports in the industry. And if you want to subscribe to Blockworks research, you can do so using zero x research ten at checkout in order to receive $250 off. And you can also sign up to our free newsletter if you want to just get a little taste where we give alpha on governance, dGen, trade ideas, market commentary, charts of the day, et cetera. Kind of get you caught up to speed on everything you need to know in the market within five to ten minutes.
00:25:11.716 - 00:25:32.982, Speaker D: Give us a follow at Blockworks res. Blockworks res on Twitter. We'll release our new reports during the week. And even if you don't have access to the reports, you're not a paid subscriber. You can still check out the topics we're writing about and get a little bit of a brief insight into what the contents of the report is about. If you want to know a little bit more how we think on the data side of things, head over to our dune public account. We have four dashboards live there for free.
00:25:33.036 - 00:25:38.150, Speaker B: The revolution will not be quarterly reported, so definitely check those out, and let's kick it over to the interview.
00:25:38.490 - 00:26:13.482, Speaker F: All right, everyone, we are joined by Jay, the co founder of say labs, and we are ready to kind of get into the depths of what say is and what the innovations that we've really cranking out in this one. And so, Jay, I'd like toss it over to you. You have a pretty interesting background of kind of how you got into crypto and the things you saw that were wrong in the tradified model, or maybe not necessarily wrong with things you can improve on with blockchains and through decentralization. So I'd love to get a brief little intro of what kind of brought you into crypto, and then we kind of get into the more technical side of things and I think what we both agree is the more fun side of the conversation.
00:26:13.546 - 00:26:31.570, Speaker A: Awesome. Yeah. So thanks for having me, Dan, Sam, and good afternoon to you guys. So my background is that I originally studied computer science, got into crypto back in 2017. At that time, my roommate, he was going through finance Launchpad. So we tinkered on a couple of different projects together. And then afterwards, I ended up joining Robin Hood.
00:26:31.570 - 00:26:56.298, Speaker A: Right, like the Robin Hood. So I spent almost four years over there. I saw the company ten X, and I was an engineering lead when the GameStop saga happened a little bit over two years ago. So I guess for anyone in the audience who doesn't remember, what basically happened is that there was this entire meme stock kind of mania that was taking over the United States, and there were some stocks, like GameStop, AMC, that were straight up mooning.
00:26:56.314 - 00:26:56.446, Speaker C: Right?
00:26:56.468 - 00:27:17.314, Speaker A: Like they were doing exceptionally well, and they were driven by this kind of push from retail. And there were hedge funds that were shorting them. These hedge funds were getting short squeezed. It was a situation where retail was winning, hedge funds were losing, and then just completely out of the blue one day, Robinhood turned off buys. They didn't tell customers why they did that. No one had any idea what was happening. It was just incredibly chaotic, and there was national outrage.
00:27:17.314 - 00:27:45.006, Speaker A: Right. There was this entire kind of, like, true Robinhood moment happening. Robinhood turned off buys, and that kind of stopped that GameStop pump, so to say. So that was exceptionally poorly handled from Robinhood side. They weren't doing anything malicious, but it was just poor communication. And as an insider, it was just really demoralizing to be there at the time, because you put your reputation on the line to join a place like Robinhood. And when they just turn off buys, you get all of your friends asking you, yo, we just opened these GameStop positions last week.
00:27:45.006 - 00:28:23.482, Speaker A: What the hell is happening over here? You have nothing to be telling them, and they're just kind of pissed off, and they are somewhat pissed off at you as well. Your team is definitely not happy, and they're asking you questions. You don't know what to be telling them. So after going through that experience myself, I became much more of a decentralization maxi, because anything that happens on chain is inherently trustless and transparent. So that would have helped avoid a lot of the things that we saw with Robinhood, which is why two years ago, my co founder, Jeff and I, we started building a decentralized exchange. Initially, this then led to us looking into all the infrastructure that we could use to build an exchange. And we ultimately decided that the current infrastructure is lacking, which is why we started building stay.
00:28:23.482 - 00:28:36.530, Speaker A: So super high level stay is a new layer one blockchain. Our mission is to build the best infrastructure for exchanges. So the way that any listers can think about it is, say, is a layer one blockchain with an order matching engine primitive that's built into it.
00:28:36.600 - 00:28:57.894, Speaker B: My question would be, why Cosmos chain as an L one as opposed to, and also going after sector specific? You guys are trying to optimize for Dexes and order books and things of that sort. So why would you go that route as opposed to maybe if you're like a DyDX, going an app specific chain route, of course.
00:28:57.932 - 00:29:25.310, Speaker A: Yeah. So two good questions over there. I'll go through them. First of all, by talking about why did we end up using the Cosmos SDK? So when we initially decided that the infrastructure was lacking, we started considering what would good infrastructure look like? And there were a couple of different, I guess, three prominent verticals that we saw. The first was making use of a roll up. At that time, it was basically building a roll up on Ethereum. The second thing was by making use of some kind of shared security layer one.
00:29:25.310 - 00:30:04.038, Speaker A: So this would be like Polkadot or an avalanche subnet. And the third approach was building a sovereign chain. So the reason that roll ups didn't make sense back then, and even right now in a production setting, they don't really work either, is because they can't scale. So with a roll up, you end up having to have off chain computation. And then you take transactions that happen on chain or off chain, compress them, and then you write them on chain. So if you're writing this to Ethereum l one, each of these transactions, it ends up taking around 16 gas per byte of data that is written to the l one itself. So currently, Ethereum blocks have a target of 15 million gas per block.
00:30:04.038 - 00:30:28.982, Speaker A: So the end result of that is that you can only get in around 6000 TPS. And the bottleneck around that scaling ends up being the data that is written to the chain itself. This isn't solved by protodank sharding either. With protodank sharding, you get closer to 6.7K TPS. So realistically, right now, you can't really scale if you're building any kind of roll up on Ethereum. So the second thing that we then considered was making use of shared security.
00:30:28.982 - 00:31:02.766, Speaker A: So parachain, subnet, something of that flavor. We decided that that wasn't ideal for us either, because we wanted to build the best infrastructure for exchanges. And one part of that is having really, really good performance, specifically around time to finality. The lower the time to finality there is for exchanges, the better. And if we're making use of any kind of shared security approach, we're basically bound by whatever that validator set is and whatever their consensus time is. And we can't get anything better than that in terms of the consensus time. So if, for example, Polkadot has six second block time, we can't have any time to finality faster than that.
00:31:02.766 - 00:31:51.082, Speaker A: So we wanted to customize the validator set, we wanted to customize the consensus that we were using, and we wanted to just have better time to finality, which is why we didn't end up making use of any kind of shared security approach. And then the third thing that we considered was a sovereign chain. And even within sovereign chains, there's two flavors, right? You can either do what, like Solana near Aptos we did, which is you start off and build everything from scratch, or you can make use of something like Cosmos SDK, which has already been battle tested for several years. At this point, you don't need to recreate the wheel. You don't need to really worry about a lot of security issues because those have already been handled. And then you can get something out very quickly, and then you can basically customize the stack for whatever kind of performance optimizations or application level changes that you want to make. So from our side, that was honestly kind of a no brainer.
00:31:51.082 - 00:32:27.882, Speaker A: Like, rather than thinking two years into building an entirely new chain completely from scratch, handling all the cryptography, all these things that have already been done, it made more sense to get started with the Cosmos SDK. We were able to get a working layer, one with the native order matching engine built into it in a couple of months. And then afterwards, we made tweaks around the consensus layer, adding in parallelization at the application layer. And that was really easy to. I mean, the biggest thing about all of this is it's largely been battle tested already. Even when Terra collapsed, the Cosmos SDK and tendermint core side of it was completely unaffected. Yeah, Terra's algorithmic stablecoin mechanism, that ran into issues.
00:32:27.882 - 00:32:51.682, Speaker A: But the Cosmos SDK, that was, like, one of the most clear testaments to how stable it is. So that's why we went down that route to get started building, say. And at this point, I mean, we've been working with the Cosmos creators, so we've been working with Zucky, who's our advisor. We've been working with Marco, who's the product lead for the Cosmos SDK, to push Cosmos and tendermint to their absolute limits. So we can get into that a little bit more in the future. But, yeah, I think Cosmos was a really good way for us to get started.
00:32:51.736 - 00:33:13.670, Speaker F: So it sounds like you kind of wed the Cosmos route because it gave you this level of customization as a developer that you felt like was a great starting point to then build further out into. But it also has this proven track record of being a very comprehensive and capable blockchain network. And that's like the brilliance of the Cosmos SDK as well as tenderbit. But, yeah. Continue.
00:33:13.740 - 00:33:55.478, Speaker A: Yeah, I mean, and to that point, I think the stability aspect of it is actually something that people don't talk about as much. Like, if you look at Solana, for example, the reason Solana has outages is not because they have some fundamental flaw in their mechanism. I think from a theoretical standpoint, Solana is extremely well thought out. I think the issue that they run into is they just build everything from scratch, and then there ends up being application level bugs or, like, consensus level bugs, which then run into. So there's just a lower chance of there being bugs if you're using something that is already this stable. So that was definitely one of the reasons that we chose to go with Cosmos. With regards to the second question that you guys had asked around sector specific chains, the high level way that we think about it is that there's essentially two kind of extremes around layer one infrastructure out there right now.
00:33:55.478 - 00:34:42.210, Speaker A: On one hand, you have general purpose chains like Ethereum and Salana, and on the other hand, you have application specific ones like osmosis and Dydxv Four. We think both of these sides have pretty significant benefits. If you look at application specific chains, the core idea over there is you have one application, and then you customize the entire stack. So the application level, you customize the consensus level, you can customize the storage level, like, every single part of it around that one application. So by definition, you are likely going to be building the best possible infrastructure for that specific application because it's completely built up around that application. That's a really powerful thing because you can have much better user experiences, you can have much better performance, and you can make whatever optimizations you need to. So that's something that we really like about the application specific approach.
00:34:42.210 - 00:35:37.302, Speaker A: With the general purpose approach, you don't get that level of customizability, but what you do get is social coordination. So every single project that is building in that ecosystem. They're all on the same team and they're all working together to help that ecosystem succeed. And that's why if you look at the biggest, essentially the biggest projects out there, they all have this general purpose approach, because it's much easier to scale and grow an ecosystem if it's like, let's say dozens of different teams that are collaborating than if it's just one application that is basically screaming into an abyss, trying to get users trying to grow the community. So because of that, there are kind of two takeaways. The first is that an app chain approach is going to lead to better infrastructure, but it's much more difficult to have it grow and scale from the community side. And we think the community side ends up being one of the most important things, especially when you have open source technology, the technological mode ends up being pretty limited, the community ends up being the biggest thing that ends up leading to long term sustainability for a project.
00:35:37.302 - 00:36:15.970, Speaker A: Right. If you fork Ethereum, you're not going to be able to get the same kind of success as Ethereum because Ethereum has its community already. So the approach that we ended up taking was kind of a middle ground approach, which we termed sector specific. So it benefits from the idea of having customized infrastructure. So we've basically taken the Cosmos SDK and built infrastructure that is customized for exchanges through changing the consensus level, application level, and also adding in a native border matching engine. And the other thing that we've done is we're not building an application ourselves, so we're just building the core base infrastructure, and then we're allowing other applications to come and build on say. So because of that, a couple of things.
00:36:15.970 - 00:37:00.898, Speaker A: One is that we can now have an ecosystem of applications rather than just having an app chain. And the second thing is now there's a sense of credible neutrality. So if you're building a Dex app chain and then a Dex wants to come and deploy a smart contract on you, that Dex is going to be competing against the core app of that app chain. So from basically that other exchange standpoint, it doesn't really make sense strategically to be deploying on that layer one, because you're going to be competing against the core infrastructure that you're building on. So with the sector specific approach, we get credible neutrality, we get social coordination, and we also get the customizability that I was talking about before. So we think that ends up being the best approach specifically for these types of applications that have product market fit, like exchanges, and then need some mechanism to help them scale and have better user experience.
00:37:01.064 - 00:37:25.194, Speaker F: Awesome. Yeah. And I like how you really focused on the fact that communities are remotes, because, again, we see that over and over again throughout the industry. But I want to get a little deeper under the technical side of things. So you took the Cosmos SDK and tendermint as a great starting point, but then you made a number of technological breakthroughs that I think, as you put it, we're pushing the stack to its limits. I'm curious where the divergence comes in. Right.
00:37:25.194 - 00:37:43.642, Speaker F: So I've seen about the twin turbo consensus, optimistic block processing, parallelization. There's a lot of different things to go through here, but I want to pose it to you, however you want to break these down. What are the most exciting technological innovations that, say, was able to push through under the Cosmos SDK in sendermint?
00:37:43.786 - 00:38:12.454, Speaker A: Yeah, absolutely. So at a high level, we're building the best infrastructure for exchanges. The question then becomes, what does that infrastructure look like? So from our side, the way that we answered that question was we went and talked to different exchanges and asked them for feedback around what is actually important to them. After having enough conversations, there were basically two verticals that became clear that these are important exchanges. Right. The first is performance, specifically around throughput and latency, and the second is around user experience. And that's much more of like a vague thing.
00:38:12.454 - 00:38:50.806, Speaker A: That is, different exchanges have different ideas for what user experience looks like, but everyone wants to have this idea of better user experience. So we started initially building by adding in this native order matching engine, which I can talk about a little bit later. And then afterwards, we thought to ourselves, how can we really push the performance aspects of this to be even better? So that's when we started looking into the consensus layers and the application layers. And there were basically two things that we started doing to enable better latencies and better throughputs. So from the latency side, we added in twin turbo consensus. So this is taking tendermint, and we changed the way that block propagation works. We changed the way that block processing works.
00:38:50.806 - 00:39:27.938, Speaker A: And this allowed us to have tendermint that was running as fast as it theoretically could. And the second thing that we did is we added in parallelization. So we're currently the only cosmos chain to be making use of any form of parallelization. So, I mean, super high level. The kind of result of these changes were that we are currently seeing a 500 millisecond time to finality in our latest public testnet, which makes us the fastest chain to finality out there point blank and this is with conditions that mimic main net. And the second thing is we've been able to get in that public testnet up to 20,000 orders per second that we can process. So most ecosystems have around 1000, 3000 orders that they can process every second.
00:39:27.938 - 00:40:00.838, Speaker A: So roughly a magnitude better over there as well, in terms of more of a deep dive into this actual twin turbo consensus parallelization. Twin turbo consensus is basically taking tendermint and then the optimistic block processing and intelligent block propagation were the two things that we did, which is why we call it twin turbo consensus. So I can start off by talking about block processing. The idea of optimistic block processing is honestly pretty simple. If you look at Tendermint, the way that it works is first there'll be a block producer that proposes a block. Then validators will receive that. Then there'll be a pre vote step where they send messages to every other validator.
00:40:00.838 - 00:40:49.482, Speaker A: Then there'll be a pre commit step where they once again send messages to every other validator. And then afterwards they'll actually execute the transactions in the block, and then afterwards they'll commit them. So execution, typically with tendermint and Cosmos SDK, happens after the two voting steps are done. Now, if you think about it, this is really inefficient because validators receive this block and they're just chilling, right? They're not doing anything for the pre vote, pre commit step, and then they only start execution after those are done. And we thought to ourselves, we can definitely optimize this because the core thing over here is you can start having a concurrent process spin up as soon as the validator receives the block and just have it update some kind of candidate state. And that's exactly what optimistic block processing is. A validator will receive the block, and then they'll start executing immediately, and they'll be updating a candidate state.
00:40:49.482 - 00:41:43.750, Speaker A: And then one of two things will happen after the pre commit step is done. Either the block will be approved, in which case you can take that candidate state as a validator, and then you can just commit that and have that be the canonical state of the chain. So in that case, it's really simple and you end up having much better performance. The other scenario would be if that block is rejected by the network. In that case, you can just discard the candidate state, and then you can start moving on to the next block. And the core concern over here is that what if there's malicious validators, like, let's say 30% of the validators are malicious, and then they somehow try to ddos the validators in the network by having them just try to run a bunch of optimistic blocks like one after the other. So the way that we got around that is we only have the first block for any given height be optimistically processed, and after that block fails, then every other block for that specific height will be processed sequentially after the pre commit step is done.
00:41:43.750 - 00:41:57.694, Speaker A: So this helps prevent this ddosing scenario where all the resources just end up getting used because their validator is trying to have ten different on. Steven, more than that, concurrent optimistic processes that are trying to run. Any questions around that piece?
00:41:57.812 - 00:42:14.790, Speaker F: Yeah, I'm assuming, does this affect block times at all? So if the block is optimistically created, then I'm assuming that'd be a faster process. Right. And so that's probably where you get the 500 milliseconds. But what happens if it's rejected and it goes like default to sequential ordering?
00:42:15.210 - 00:42:53.870, Speaker A: Yeah, so a couple of things over there. The first is, yes, in cases where there's higher throughput, this will help reduce the block time. In terms of the lower bound, this is not really going to matter because in lower bound there's actually nothing to process. So that ends up being more or less negligible processing step. But if the actual execution part of it is taking, let's say 500 milliseconds, and then each voting step is taking, let's say 100 milliseconds, then those two voting steps that take 200 milliseconds, you basically have 200 milliseconds worth of that execution happening during the concurrent processing step. So it definitely helps in these higher throughput scenario times. With regards to your question around failure cases.
00:42:53.870 - 00:43:20.090, Speaker A: So if it's rejected by the network, then what happens is we will move on to the next block for that height, and that block will not be, I guess, processed using this optimistic approach. And that's to prevent that DDoS scenario that I was talking about before. So only the first block at any given height will be optimistically processed, and then afterwards, from the next height onwards, you'll start optimistically processing again. So this is kind of something that restarts at every single height.
00:43:21.330 - 00:43:32.826, Speaker B: Now I'm curious too, in comparison to a regular cosmos tendermint chain, how does this affect hardware requirements considering the performance is so much better for validating?
00:43:32.858 - 00:44:14.094, Speaker A: Yeah, so optimistic. Block processing is actually not something that will really result in additional hardware requirements because you're basically taking something that will be happening after the execution set or after pre commit and just running it as soon as the block is received. So there are other things that we do, specifically parallelization, that end up being more resource intensive than, for example, what you would need with Ethereum. But the optimistic block processing is something that other cosmos chains can do as well without increasing their hardware requirements. And that's something we're actually chatting with the teams behind this as well. We're probably going to be upstreaming these changes and then having them be flags that teams can use if they want to, because at the end of the day, this is all open source anyway. Other teams can make use of this if they want to.
00:44:14.094 - 00:44:18.640, Speaker A: So we'll just make it easier for other teams to benefit from the kind of innovation that we've done.
00:44:20.530 - 00:44:34.610, Speaker B: So you mentioned parallelization there, and you hear this term thrown around a lot, like I even hear it with Polygon. They're working on that for their proof of state commit chain. So can you kind of explain the nitty gritty around parallelization and the benefits it provides?
00:44:35.030 - 00:44:51.814, Speaker A: Yeah, I guess before I get into that, I'll hop into intelligent block propagation, which is the other part of twin turbo consensus. And then afterwards we can do aprilization. Deep dive. That's cool. Perfect. Yeah. So from intelligent block propagation standpoint, this was the other core part of twin turbo consensus.
00:44:51.814 - 00:45:30.322, Speaker A: And basically what happens right now with normal tendermint is that a block producer, let's say that you're a user, you want to submit a transaction. The lifecycle of that transaction will be first it goes through a full node, and then it ends up in the mempools for different validators. Then one of these validators will be a block producer. So they will look at their local mempool, and they will construct a block based off that. Once they have that entire block constructed, they will first create a block proposal message, which just has a block hash. And then afterwards they'll send that block proposal message over to the rest of the nodes in the network. Then they'll take that block that they have, they'll break it up into a bunch of small chunks, and then they'll send all those chunks over the network.
00:45:30.322 - 00:46:07.934, Speaker A: So any validator that wants to start voting as part of their pre vote step, they'll need to wait to first receive that block proposal message. Then they will need to receive all of the separate block chunks. They'll need to put all the chunks together, find the hash of that block, make sure it matches the hash from the block proposal message, and then they can proceed with voting. Right now, the thing that's extremely inefficient about this is that the contents of the block that these validators have to wait for. It's just all transactions. And most of these transactions, generally, all of these transactions end up already being there in the mem pools for each of these validators. So you're basically waiting to receive this data that you already have, which is just incredibly inefficient.
00:46:07.934 - 00:46:45.610, Speaker A: So the approach that we started taking instead is that block proposal message that I described before. Instead of just containing the block hash, it also contains transaction hashes. So it contains an ordered list of each of the transaction hashes. And the transaction hashes are much smaller than the actual transaction. So the result of this is the block proposal message will be a little bit bigger, but it's not like massive, and you can still send it as one message to every single validator in the network. Then the validators will receive this block proposal message. They will look at their local mempool, and if they have all of those transactions, because there's already a mapping of transaction hash to transaction in the mempool, so they can just look to see if they have all these transactions.
00:46:45.610 - 00:47:17.954, Speaker A: If they do, then they can just construct the block locally. So the massive benefit there is, previously they were waiting to receive all these chunks over the network, and there's costs associated with this network vacancy, right? Like it might result in 100 milliseconds of additional time it takes to receive that. Now they don't need to wait that anymore. They can just construct it locally. So in the happy path, they can construct it locally, they can begin voting, and in our case, optimistic block processing immediately after that. So that's a huge win. The question then becomes, what happens if they don't have every single transaction from that block proposal message in their mempool.
00:47:17.954 - 00:47:57.330, Speaker A: In that case, we just fall back to using what we were previously using, which is the block proposal will create this bigger block proposal message that has the ordered list of transaction hashes, and then they'll still take the block, break it up into chunks, and send it over the network. So if you as a validator, you do not have all the transactions from that block proposal message, then you can just wait, and you'll eventually receive the transactions from the form of that block, and then you can just go ahead and use that. So in the happy path, it'll lead to substantially improved performance. And in the case where you don't have all the transaction hashes, you can just wait a little bit longer, and then you'll essentially fall back to the same time, the same latency as you were seeing in the normal tenermint approach.
00:47:58.070 - 00:48:21.978, Speaker F: So this is really exciting. It almost feels like the approach you took, and tell me if this is completely off base. But it was like we see opportunities to improve the efficiency of a few of these systems, and so we're going to build these systems, but there are case edge cases where they're not going to work 100% of the time. In the event that happens, then let's just use this already great infrastructure we're building on top of and rely on the ways it's been running in the past. Is that something you'd agree with?
00:48:22.144 - 00:48:35.958, Speaker A: Exactly, yeah. So we're very optimistic in our approach. Like if things are under ideal scenarios, we'll have better performance. And if things fall back to the kind of normal scenarios, then we'll just use what's already been battle tested with Cosmos and tendermint and fall back to using that already reliable approach.
00:48:36.154 - 00:49:00.914, Speaker F: Now that's pretty exciting. And so you guys are already in public testnet, and when you look at some of it, like you look at the data of what's been happening thus far, how often is like intelligent broad propagation or optimistic processing, how often are they being utilized within this testnet or how compared to falling back on the traditional methods?
00:49:01.042 - 00:49:38.986, Speaker A: Yeah, I mean, candidly for this public testnet, we haven't collected metrics on that. In the internal testnet for intelligent block propagation, it was 99.99% of the time that that was applicable. So yeah, we probably should get more metrics around this in the public testnet. But the block times, just given that they are as low as they are right now, even under higher throughput scenarios, we do believe that it's getting applied basically all of the time. And the core thing over here is if you have a smaller validator set, then it's much more likely for these to be beneficial, especially intelligent block propagation. As soon as you start having hundreds of thousands of validators or something, then the intelligent block propagation approach might not really be that helpful.
00:49:38.986 - 00:49:50.714, Speaker A: But if you can make the assumption that you have connections between every single validator which you essentially need to have with the tendermint, if you're using tendermint, then you're able to benefit from those assumptions and have better performance around that.
00:49:50.752 - 00:49:54.218, Speaker F: That makes total sense to me. Let's dive into parallelization. This is an interesting one.
00:49:54.304 - 00:50:35.874, Speaker A: Yes, sir. So parallelization is for anyone who's listening that isn't familiar. The normal way that ethereum, for example, is processing transactions is, let's say there's 100 transactions, it'll sequentially process them so it'll go through every single transaction one by one, and it's not able to do any of them at the same time. Now this is really easy. Logically. It also results in lower hardware requirements because you don't need to have any beefy machines to be able to better process transactions. The downside here is it is really slow and it's not very scalable, which is why on Ethereum, for example, you can't really have super scalable applications that are built on the layer one itself, just because they can't really handle substantial throughput.
00:50:35.874 - 00:51:22.978, Speaker A: So the approach in web two that was taken to help combat sequential processing is this idea of parallelization, which basically means that rather than executing one thing at any given time, you can execute multiple things, and as long as they're not conflicting things that are being processed, you can have separate work streams that are processing each of these, and it'll allow you to finish processing these workloads in a faster period of time. Right. So the core idea is something that is applicable to everything that we use. Like basically anything you do in web two, you're going to be benefiting from some form of parallelization to have better performance. We decided that we should do that on say as well. So we're currently the only cosmos chain to be making use of any form of parallelization. And the approach that we're taking is that developers will need to first of all deploy their smart contracts.
00:51:22.978 - 00:52:00.318, Speaker A: Then afterwards they'll need to execute a transaction that will define the dependencies for each of the functions in those smart contracts. So it'll basically be a template that they need to define. And then at runtime that template would get filled in. And then afterwards, when at runtime transactions are being processed, you'll be able to look at the dependencies that are being basically the state that those transactions are touching. And then you can construct a directed asylum graph of these dependencies. So an access list of these dependencies and then anything that is touching the same state. So if both of us are trying to touch the same order book, then our orders would need to be processed sequentially.
00:52:00.318 - 00:52:52.834, Speaker A: But if we're touching separate order books, for example, or if we're touching completely separate things, then our orders can be processed in parallel. So if there are parallelizable workloads, which basically means workloads that are not all touching the same shared state. So easiest way to think about that would be like different people sending tokens between each other, or different people touching different smart contracts, then parallelization helps significantly improving the amount of transactions that you can process. And if it's a workload where everything is touching the same state, then in that case, parallelization ends up not being as beneficial because you still need to be sequentially ordering or executing those transactions. So the approach that we've taken has helped us get up to around 20,000 orders per second that we can process in the public testnet. So it is quite beneficial from a performance standpoint. And I would say that the approach that we're taking, there's basically a spectrum of approaches to parallelization as well.
00:52:52.834 - 00:53:21.130, Speaker A: On one hand, you have aptos with their optimistic processing approach, and then on the other hand, you have like Solana and twee, where every single transaction you need to include all the accounts that are being touched or all the state that is being touched. And rather than taking either of those approaches, we're honestly taking a middle ground approach where developers just need to define these dependencies once and submit this transaction once, and then afterwards, for every single incoming transaction, they don't need to define which accounts are going to be touched or which state is going to be touched. The chain is able to decipher that at runtime.
00:53:22.910 - 00:54:09.526, Speaker F: That's pretty cool. And so when I think about this, we recently had John Adler on the podcast who's building fuel, and he talked at length about parallelization. And one thing that helped it kind of click for me was like, okay, I'm coming at you from a laptop with an I seven processor, seven cores in there, and the number of cores increases, which allows them, aerialization basically allows the cores to be working at the same time. That kind of helped it click for me, just for the listeners out there as well, it hits the point where it's like, okay, so now we have all this improved tech, but you start to ask the question, where are the trade offs here? Because as we see with a ton of things in this industry, everything seems to be on a spectrum, and we have trade offs everywhere. So where would you define the trade offs with a chain?
00:54:09.558 - 00:54:46.466, Speaker A: Like, say, yeah, that's a great question. And I agree that everything ends up having trade offs. Like anyone that says that they've solved every single problem around the scalability or around the, I guess, trilemma, they're probably exaggerating. So in our sense, the biggest thing that we decided to do is we made use of cosmos and tendermint to get started. And tendermint specifically is the biggest trade off that we made, because tendermint scales n squared, which means, and the reason for that is, in order to basically create a new block. You need to have two thirds consensus between all the validators. So essentially all the validators need to talk to each other before you can add a new block to the network.
00:54:46.466 - 00:55:00.060, Speaker A: What this means is that you can't actually have a lot of validators if you're trying to reach consensus quickly. So that's why Cosmos hub, for example, is an example of the Tenderman chain with the greatest number of validators. Guess how many validators it has.
00:55:00.510 - 00:55:01.834, Speaker F: 150.
00:55:02.032 - 00:55:32.162, Speaker A: Yeah, I think it has 175. So roughly in that range. Right. So you can't really get past 200 validators. I don't think there's an example of a cosmos based chain that is big and that has done that. And as a result of that, we accepted that we're going to be capped on the number of validators unless we make changes to the tendermint, which, honestly, we do have plans for changes that we can make to enable larger number of validators. But assuming we get started with some Manila implementation of tendermint, you're not going to be able to have a large number of validators.
00:55:32.162 - 00:56:13.060, Speaker A: So that's the biggest trade off that we made, and we decided to lean into that, because if you make the assumption that you have n squared connections and every validator can talk to every other validator, then you're able to make performance optimizations that benefit from that. One example of that would be twin turbo consensus, like specifically the intelligent block propagation piece. If you assume that all the validators stable to talk to each other, and they do end up having transactions in their mempool, then you're able to have better performance around that. Whereas if you're assuming that it's like, 100,000 validators, a transaction, being in someone else's mempool is unlikely, then those performance optimizations we made wouldn't really be applicable anymore. So, yeah, I would say the biggest trade off was using tendermint. And, I mean, we do have ideas for how we can tweak tendermint in the future to enable a larger validator set size.
00:56:13.910 - 00:56:32.998, Speaker F: So, staying on the topic of the validator set, what is the size of the validator set in the public test site today, and how many do you plan to go live with at launch? And not to throw too many questions at you at once, but what was the selection process there? Was that a whitelisted validator set or open to the public? Curious about how your approach was on the validator set specifically.
00:56:33.174 - 00:57:04.882, Speaker A: Yeah, so I believe it's 35. That is the target cap right now. For the public testnet, it'll be something in that similar range for main nets, like somewhere in the 30 to 50 validator range. In terms of when we got started we were thinking, holy shit, it's going to be super hard to get validators. There's all these different projects that are trying to offer shared security. The core problem they're solving is it's hard to have a good validator set. It turned out to be pretty easy from our personal experience, and I think that's also because our community was growing pretty quickly at that time.
00:57:04.882 - 00:58:04.470, Speaker A: When we got started with our incentivized testnet, there was probably a pretty high degree of hype around the say project, like more than we had originally anticipated. So we ended up having over 2000 validators that signed up or that wanted to submit an application to be part of the validator set. And then from there, the way that say's validator set works is it's completely permissionless, so anyone can join if they have enough delegations. What say was doing initially for the incentivized testnet is we picked 200 validators that would be receiving delegations, and then we just alternated the delegation to them so that everyone got a turn. And then afterwards the validators that were like really strong performers from that incentivized testnet are now the validators on Atlantic too, but it's completely permissionless so anyone can join if they get a high enough delegation. And on Mainnet it'll be similar. On Mainet the safe foundation will be delegating to some validators that are strong validators, but anyone can join that validator set side or that validator set if they have enough delegations.
00:58:05.770 - 00:58:10.040, Speaker B: And so how will gas costs be calculated and what will they be paid in?
00:58:10.410 - 00:58:46.500, Speaker A: Yeah, so gas will be paid in stay tokens. In the future we might allow for other types of tokens, but right now it's simplest to just start off with, say, tokens. And in terms of the costs around that, we still need to fine tune those settings. But it's going to be extremely cheap, most likely less than a penny for every single transaction. And insane particular, the native order matching engine does allow for market makers to submit batch transactions. So the biggest party that would need to be submitting a lot of transactions would be market makers, and they basically only need to submit one transaction that is composed of all of their orders within that transaction. So we don't really expect anyone to be paying substantial gas costs in the future.
00:58:47.510 - 00:59:04.490, Speaker F: So the token conversation is always fun to save for the end. But now that we kind of got towards that direction, I'm curious, is the say token going to look something similar to like an atom or an osMo, like a standard proof of stake gas token or what are any additional utility around the token?
00:59:05.310 - 01:00:01.510, Speaker A: Yeah, so it's going to be more or less a standard proof of stake gas token. So similar to Solana, it'll be used for three primary things, which will be governance, security, transaction fees. The other thing that's really interesting about say is say is a trading focus ecosystem, right? So there will be realistically a lot more MEV as a percentage of total activity on say than there would be in another ecosystem, like maybe for example, Solana, that doesn't have as much trading happening compared to what we envision would happen with say. So as a result of that, we think that MEV redistribution will be another mechanism for value mean, I guess, high level for listeners. So MEV stands for maximal extractable value. It's the value you can extract as a block producer by changing the ordering of transactions, or including or excluding transactions within a block. And there's this idea that you can't really stop MeV altogether.
01:00:01.510 - 01:01:02.106, Speaker A: So rather than trying to stop it, you can have auctions so that in the case of ethereum with flashbots, it's the validators to receive the money by having people bid to be included in a block. And we're taking this idea, and we're taking it one step further, which is we're going to be having auctions happening where people can submit transactions. And then whoever wins these auctions, they'll be included in the block. And the thing that we're doing that's different though, is that whoever wins these transactions and then submits these orders in the block, these bids will then be taken and then redistributed to validators and stakers of the chain. So through an approach like this, it results in much more avenues for value pro, because MEV is going to, essentially, the value of MEV in the future is going to match whatever the value of MEV redistribution will match the size of that MEV opportunity. Like if it's $100 opportunity, and there's 100 different people that identify that opportunity, they will all competitively bid up the bid that they're making until the bid is as close to that $100 opportunity as it can be. So realistically, it'd be like $99.50
01:01:02.106 - 01:01:15.490, Speaker A: or something for the bid. And the value of that bid will then get redistributed to the network. So we anticipate that to be a pretty interesting source of value accrual in the future. But that's not something that has been built yet that'll be built after mainnet launch.
01:01:16.630 - 01:01:37.420, Speaker B: That's super interesting. I was just going to ask you, so how do you expect to attract liquidity? Because obviously you need a lot of liquidity in order to increase the amount of MEV that's generated. But considering the staking yield will likely be higher, considering all the trades being made have a lot of high MEV, that's an incentive in itself. But is there any other strategies that you have planned there?
01:01:37.950 - 01:02:19.458, Speaker A: Yeah, I mean, at the start there's going to be airdrop incentivized test and rewards. So that'll be a chance for folks to get basically start participating with say pretty early on from Mainet Genesis. Beyond that, I mean, we're going to have all the infrastructure set up to enable people to bridge stuff over to say. So IBC from the get go will have different bridges that are set up as well. But beyond that, we think it's going to end up being kind of a gradual process to start having more and more activity happening on say. I think originally it'll be the dGens from other ecosystems like Ethereum, Solana, Cosmos, dGens that come over and there'll be applications that are built on say, from day one, so they'll be able to partake in those applications. These applications will realistically be offering some kind of incentives to people that are trading.
01:02:19.458 - 01:02:48.974, Speaker A: So we think initially it'll be the DJens that come on. After that, we think that there will be more retail focused applications that are able to get steam on, say. And that's the biggest bet that we're making. If there's no killer app, let's say, then say is an ecosystem. It's probably not going to be a massive ecosystem. But if there is a killer app that is able to take off within the next six months to two years, then there will be a lot of retail folks that come on as well. And I mean, we've seen this happening in the past several years where any kind of killer application ecosystem results in a lot of retail coming on.
01:02:48.974 - 01:03:27.402, Speaker A: And then from there we anticipate that there will be a lot more liquidity coming in. And as soon as there's retail liquidity, then there ends up being institutions that become much more incentivized to participate as well. So we think the third group that will get involved is institutional players. So from our side, the biggest thing that we're focusing on is just doing as good of a job as we can of building the core base infrastructure and then chatting with teams, kind of explaining the benefits of building on say to them. And at this point, we have over 120 projects that are building on say. So I think that overall there has been a lot of interest from different projects around. Initially it was just the core infrastructure, like being able to build an exchange that is potentially much better than an exchange you can build in any other ecosystem.
01:03:27.402 - 01:03:55.670, Speaker A: And now I think it's just becoming more and more strategic for teams to launch on say, because exchanges are one of those things that become kind of magnets for other types of applications to build around them. This is true both off chain and mean. We can talk about how binance has a similar thing happening, how ethereum has a similar thing happening. But in any ecosystem where there's exchanges, other types of applications get built as well. So we're starting to see more applications just come and try to strategically benefit off these exchanges.
01:03:57.370 - 01:04:20.106, Speaker F: Right on. And just going back to quickly to the MEV discussion. So there's like good MeV and bad MeV, right? So like an example, bad MEV is like front running or sandwich attacks, where you're throwing off the price that a user is getting on a trade, but there's also good MEV in the form of like liquidations and whatnot. So say has an interesting front running protection mechanism. Can you walk us through how that's built and the benefits that provides to its users?
01:04:20.298 - 01:04:59.260, Speaker A: Yes, absolutely. So I guess the other core technical innovation that, say, has, besides the twin turbo consensus and parallelization, is a native order matching engine. So this is something that's built into the chain itself. And anyone that wants to build an order book based exchange can make use of this underlying order matching engine to help with every step of order processing. So order placement, order matching, order settlement, order cancellations, that'll all be handled by this order matching engine. And what it actually does that's really interesting is it makes use of frequent batch auctions. So during order processing, it'll aggregate every single order together, and then at the end of the block, it will execute all these orders at the same uniform clearing price.
01:04:59.260 - 01:05:34.982, Speaker A: So one example would be if there's an order book for, let's say, ether right now, and the price of ether is, what is it? It's 1791. So if there's two orders on the order book, one to sell ether for 1791, to sell ether for 1792, and then two market buys come in. So with sequential execution, which happens on other chains, the first buy will get filled at 1790. The second buy will get filled at 1792. So that means the order of transactions within the block does matter. Whoever in that case is first, they'll get a better price. In face case, both users would get the average price, which would be 1791.
01:05:34.982 - 01:06:15.666, Speaker A: Right? 1790 plus 1792 divided by two. So there's kind of a couple of things that happen because of this. First of all, there's price fairness within a block, which just leads to users being happier, because otherwise it is kind of annoying to see other people getting different prices than you within the scope of a block. The second thing is that front running is no longer possible for any transactions related to the order books that are included in the block. Because the way that front running would typically work if you have atomic front running would be someone sees my order to buy ether for 1791 plus, like, let's say I accept 1% slippage. So that would basically go up to 1791 plus roughly $18. So up to 18 nine.
01:06:15.666 - 01:06:45.230, Speaker A: So what you would do in that case, Dan, you would buy ether for 1791, and then you would just have one transaction composed of multiple steps where you buy ether for 1791. Then afterwards you place a sell a limit sell to sell it for 18 nine. And then assuming there's no other liquidity on the order book, I would end up purchasing it from you for 18 nine. So in a riskless way, you made that 1% slippage is profit for yourself. And I just kind of got screwed from that because I paid 1% higher fees when I didn't need to pay that. And it was extremely exploitative. Right.
01:06:45.230 - 01:07:25.610, Speaker A: So in traditional financial markets, that's illegal. But in the case of, say, because we're making use of the frequent batch auction approach, that's not even possible to do. So that kind of ties into our overarching approach to MeV, which is we want to stop bad MeV, like front running, and then for other types of neutral MeV, such as liquidations, ArBs, NFT, mints. First of all, we don't really think we can stop those. And secondly, we don't really see the benefit in stopping them if they're not negative for anyone. So we'll let sophisticated players benefit from having opportunities to basically participate in these types of Nav, and then we'll just have most of the value of those opportunities get redistributed to the chain through the mechanisms I was describing earlier.
01:07:26.910 - 01:08:02.278, Speaker F: No, that makes a ton of sense. And even just thinking about this in my head now, and kind of like stacking it on the previous discussion. Right? So how does paralyzation work when you have multi hop trades? Right. So say I was going like an ETH to bitcoin. I wanted to go ETH to USDC, but for whatever reason that pool didn't exist. So I had to go like ETH to bitcoin to USDC. How does paralyzation work? Even the order batching in that, I feel like if I went ETH to bitcoin to USDC, and then for whatever reason I want to go back to ETH all in the same, like, would I end up with the same amount of ETH there? Can you walk me through how that works?
01:08:02.444 - 01:08:50.098, Speaker A: Yeah, so, I mean, there would be two types of ways that this could happen. One would be through normal smart contracts on say, so like an AmM contract or an order book contract that's not making use of the order matching engine. In that case, it would be identical to what would happen on Ethereum, where all those would be supported. And the end result of that would be like, if you go through that entire cycle, you might not necessarily get that same price again. The other type is for exclusively order book related transactions that are tied to order books that are built using this native order matching engine. In that case, you can define dependencies. So you could technically, like whoever made the markets for you said ether to USDC or ether to bitcoin and then bitcoin to USDT, they could define dependencies between those two markets, and then one of them would run first all the time, and then the second one would always get run afterwards.
01:08:50.098 - 01:09:36.130, Speaker A: In that case, you could have that multi hop trade workout. But realistically, you're not going to have dependencies being defined between every single market out there. So in those cases, you will not be able to have this multi hop trade happening, because the frequent badge approach, that's the trade off that happens with that approach, because you're having everything get executed at the end of a block, and there's only one time that those trades get executed, having something that goes multiple hops across different contracts, that wouldn't necessarily be possible to do. So. Yeah, what we anticipate happening is sophisticated traders that are trying to do the multi hop approaches generally for collecting ARB of some kind. They'll have to take on additional risk to do stuff across multiple blocks, and we think that's totally fine. The main thing we want to focus on is improving the retail user experience.
01:09:36.130 - 01:09:54.090, Speaker A: And if there's retail users in an ecosystem, then sophisticated traders will start kind of partaking in that ecosystem. Even if things are a little bit more difficult for them. And if there's no retail users, then sophisticated traders don't really want to get involved anyway. So that's why it makes more sense to enable the best experience for retail users.
01:09:56.270 - 01:10:10.240, Speaker B: Yeah, that makes a lot of sense. Now in terms of pricing, that's obviously a very important factor for any decks. Can you go into kind of oracle solutions that you guys have thought about, or are you guys leaving that to the actual Dapps building on top?
01:10:10.610 - 01:10:48.694, Speaker A: Yeah. So from safe side, we're supporting a native price oracle. So this is going to be something that is built into the chain itself. And every single validator will need to submit price feeds, every single block for some small list of governance approved assets. So this would be extremely high liquidity assets like bitcoin and ether. And in that case it'll be extremely decentralized and extremely trustworthy as well because it's literally the validator set that is securing the chain that is providing these price feeds. So as any kind of exchange that wants these price feeds, you'll have probably one of the best data sources available for that for other types of applications.
01:10:48.694 - 01:11:21.080, Speaker A: So maybe you want prediction markets and you want the outcome of a tennis match, or maybe you want to offer perps on some asset that is kind of a new, obscure asset that the native price oracle on say is not providing a price feed for. In that case, we already have multiple projects that are going to be deploying oracles on say so. A lot of the bigger names are going to be deploying oracles on say. So you will be able to make use of those price oracles as well to get those kind of long tail asset price feeds or to get those one off kind of prediction market related price feeds as well.
01:11:21.610 - 01:11:32.342, Speaker F: So this part is fascinating to me because the cosmos doesn't really have the chain link of its ecosystem yet. Do you think that these price feeds could really be propagated throughout the cosmos ecosystem as a whole?
01:11:32.396 - 01:12:00.026, Speaker A: They definitely could be, yeah. So, I mean, because of IBC, it's pretty easy to take a transaction that happens on say, and then have the results of that be propagated to other chains. The downside over there would be that they'll be somewhat still like something that gets written to say at time t. It'll take essentially six to 7 seconds to make its way to another chain. Right. Because it'll require whatever the relayer latency is. Plus afterwards it'll need to get included in a transaction that is part of a block on another chain and go through consensus.
01:12:00.026 - 01:12:21.914, Speaker A: So it will be still prices. So if you want prices, if you're like using some kind of twap, in that case it should be fine to have like a little, if you're using like a 30 minutes t wop, then having to wait 7 seconds for each price feed, that's not that big of a deal. If you're not using a t wop and you're looking at literally the latest prices, I think in that case it might not be ideal to have this kind of like IBC set up from safe side.
01:12:22.032 - 01:12:50.770, Speaker F: Yeah, so that makes a ton of sense. And when I think about the price fees. Right. So validators are pulling in external price data to then propagate throughout the network and allow other applications built on say to benefit from. And so of course the validators themselves are financially incentivized to act in the best interest of the chain, as with most proof of stake chains. But how do you promote the decentralization of the external price feeds that these validators are pulling in to then submit throughout the chain?
01:12:51.190 - 01:13:26.446, Speaker A: Yeah, so from kind of a game theory standpoint, we're not really doing anything from safe side to facilitate that in terms of the safe foundation. Realistically we'll be able to get involved and encourage validators. We'll only give delegations. If you're providing price feeds from these separate sources, but from more of like a chain level game theory standpoint, that is not something that is built into say. And for what it's worth, there have been examples of other projects where that ended up being perfectly fine. Like in Terra's case for example, it was something similar where there wasn't really any game theory mechanic that enforced different sources for their data and that ended up being fine as well. So I think for most applications they're not really going to get affected by that.
01:13:26.446 - 01:13:42.642, Speaker A: And if a certain application really does care about having a lot of different sources for whatever price feeds that they're looking at, they can make use of any of the Oracle solutions that are going to be deployed on say as well. So for those specific type of applications there will be solutions as well. Awesome.
01:13:42.696 - 01:14:13.500, Speaker F: And I want to dive a little into the ecosystem as well. Obviously we're in public testnet right now, but I heard that there was over 120 teams committed to building projects on say. Not so much about the brand itself, but which projects or types of projects are you most excited about? I understand the state team itself is not building any of these projects to kind of keep that credible neutrality, but you spend a ton of time with the technology and likely know it better than most. So I'd love to get your insights onto what you're most excited about coming to say.
01:14:13.950 - 01:15:05.478, Speaker A: Yeah. So because of that native order matching engine that we have built into the chain itself, the types of applications that directly benefit the most will be anyone building an order book based exchange that can either build some kind of middleware or build some kind of exchange that leverages this native order matching engine. One example of a project like that would be cargo. So cargo is building prediction markets, middleware, where they're just building the core kind of infrastructure, and then other people can build each of the exchanges and they're leveraging the native order matching engine, and then they're allowing each exchange to be geographically specialized. So rather than them supporting India and Europe and the United States for different types of prediction markets, they're having different teams that are geographically specialized that are building on top of that. So in general, I do think that ends up being the native order matching engine ends up being one of the more interesting use cases. Besides that, anything that benefits from a lower time to finality does benefit from building on stay.
01:15:05.478 - 01:16:01.966, Speaker A: So having this 500 millisecond time to finality results in a vastly improved experience for any type of, I mean, honestly, any type of exchange. Like if you as a market maker are trying to open a position on chain and then have it get hedged on off chain, typically you'll wait until after that trade has actually been finalized so that it doesn't get reorganized away. In that case, if it's like a 15 2nd time to finality, and you're only going to be hedging it after the trade is finalized, then you're essentially taking on that 15 seconds of volatility and 15 seconds of risk, which results in the way that market makers basically account for risk is by offering wider spreads, which then results in a worse trading experience for users. So I would say anything that benefits from lower time to finalities would definitely benefit from today as well. And I think that that's honestly been resonating a lot with teams at this point. We have over 120 projects that are building on today ahead of the main net launch. So when we originally started announcing our first testnet, this was around May of last year, I recall that time.
01:16:01.966 - 01:16:39.020, Speaker A: I think there were around ten projects that were building on say, and then afterwards there's been a lot of kind of activity happening in the crypto ecosystem. There was terra, there was FTX. I guess even in the past couple of weeks there was drama with Circle as well so we started seeing more and more teams become interested in building on, say at this point, 120 projects I think on the ecosystem page and on social media. Over 60 have already announced. So if you want to kind of look through those, you could go to the SayNetwork IO website. And yeah, we're gearing up for a mainet launch coming up pretty soon now. So probably sometime Q two around may to June time frame of this year.
01:16:39.020 - 01:16:46.814, Speaker A: And yeah, we're pretty stoked. Everything's been going pretty well for the public testnet. So yeah, things are finally coming together. All right.
01:16:46.852 - 01:17:02.642, Speaker B: So I'm curious for airdrop mechanisms. I know I saw you tweet the other day that you were asking for the best mechanisms that people have seen throughout the space. So are you thinking of doing something novel or the typical model of give Adam stakers a little piece and then LPs, that kind of thing?
01:17:02.696 - 01:17:43.694, Speaker A: Yeah. So I mean, from safe side, we realistically will be doing something that is broader in scope than just giving it to Adam stakers. I think that some of the mechanisms that, I guess that there's fundamentally two ways of doing an airdrop, one is like a preemptive airdrop, and the second is kind of a retroactive airdrop that we've seen so far. For a preemptive airdrop, I think you don't really know how people are going to be participating. The end objective is to get as many people to get exposure to say, as possible. So I think one of the things we're definitely going to be exploring is having people receive an airdrop. You'll be eligible for some percentage of the airdrop right away, and then afterwards you have to actually do stuff on chain and try out the product and participate in the community and ecosystem to unlock the rest of it.
01:17:43.694 - 01:18:20.630, Speaker A: So I think that's a much better approach than giving 100% of the airdrop out to users right away because then they're just going to dump and they might not ever use the ecosystem. Beyond that, I do think giving retroactive airdrops is a really interesting approach. The only problem with an L one blockchain is you need to get tokens into the hands of the people more or less immediately after genesis. So a retroactive airdrop like that doesn't work as well for an L one blockchain. But with that being said, there's like other ways that you can incentivize user behavior as well and then kind of reward them retroactively. So those are some of the things that we're investigating from our side. The only things that we've kind of publicly announced that first of all, we'll be having an incentivized testnet.
01:18:20.630 - 01:18:56.950, Speaker A: Like incentivized Testnet is going on right now. 1% of the total token supply will be given out as rewards for the incentivized testnet. So anyone that's listing right now that has not played around with his incentivized testnet, you should definitely check it out. It's really easy to get started and you don't need to be like a validator to participate. You could just go ahead and trade on one of the apps that are built on, say, and then you'll be eligible for receiving the incentivized test and rewards. The other thing that we've mentioned is that there will be an airdrop, percentage of the Airdrop is still TBD, but there will be an airdrop. And then folks that receive the airdrop, they'll be able to unlock the tokens and then start using them on, say.
01:18:57.020 - 01:19:02.474, Speaker B: Ideally for mainet Genesis, man, 1% for the incentivized testnet. I got to get on there and try it out.
01:19:02.512 - 01:19:28.450, Speaker A: Huge. Yeah, that's, we're really incentivized people through that. I don't think most projects give out incentivized testnets that's that large. Yeah, definitely. I think that's been one of the reasons why say has been able to get more kind of action happening in the testnet, which has been super helpful for testing out the chain. Right. Otherwise it's really hard to get main net type of traffic and people trying out different kind of spam scenarios and different attack vectors.
01:19:28.450 - 01:19:34.740, Speaker A: So I think the incentive by Cessnet has been pretty helpful for getting both the word out about say and also people to just try it out.
01:19:35.430 - 01:20:08.030, Speaker B: Yeah, makes a ton of sense. Now, I don't want to end on a token related question because that's too cliche. So I got one more question for you. I guess it just makes a lot of sense for me for you guys to be working closely with centralized exchanges in lieu of all the FTX stuff that we saw this past year. And one of my biggest frustrations with the cosmos ecosystem know sending from an exchange to the hub to wherever you want to go next. So are there any plans to partner with an exchange either to actually use say's tech or to at least support say bridging directly from the exchange?
01:20:08.690 - 01:20:32.326, Speaker A: So for centralized exchange listings, yes, that will be set up by Mainet. So it should be a much cleaner user experience than having to go through multiple hops to be able to get tokens on say. Yeah, I mean, that's honestly really important for having good user experience. So that will be ready. The second thing around centralized exchanges, building on say, like building a Dex on say. That's interesting because we actually have had conversations with exchanges around that. Nothing has been finalized, so I can't really talk about that right now.
01:20:32.326 - 01:21:03.114, Speaker A: But it is pretty clear that centralized exchanges, they do see the value proposition of also having a decentralized component or some kind of hybrid component. So there is actually pretty strong interest for them to, I guess right now they're exploring different ways that they can do stuff on chain. Like some of the bigger exchanges, they might just build their own layer one, but a lot of other exchanges they're interested in deploying projects, deploying exchanges on chain and then having that tie into their core centralized exchange somehow. So I actually do think that within the next year, year and a half, there will be at least one centralized exchange that launches something pretty meaningful.
01:21:03.162 - 01:21:13.918, Speaker B: I'm say, that's exciting stuff. Awesome, Jay. Well, I'll hand it over to you to tell people where they can find you. Learn more. But thank you so much for coming on. We'll have to do this again once you guys are fully ramped.
01:21:14.094 - 01:21:40.842, Speaker A: Yeah, thank you for having me on, Sam. And yeah, I mean, for anyone wants to learn more about me about, say, my twitter handle is jandrajog. It's just at jandrajog. So j a y e n d r a j o g on twitter for, say, network, it's s e I n e t w r k. I would highly recommend following say, network, twitter. And then from there you can see the link tree and then access like discord. Learn more about incentivized testnet and participate in the Atlantis program.
01:21:40.842 - 01:22:13.222, Speaker A: So for anyone that doesn't know, the Atlantis program is a fully community led program that has over 2300 participants right now. And it's been pretty interesting to see where there ends up being the strongest community support. It's interestingly in places like Turkey, Ukraine, South Korea, so places that some of them are very well known crypto hubs, others less so. And there's been just organic traction happening from those places for the Atlantis program. So that's a really easy way to get part involved in the state community. So definitely would recommend checking that out as well.
01:22:13.276 - 01:22:20.342, Speaker F: Fantastic. And we'll put all those links in the description for the listeners, that's easy access for you all. But thanks a lot, Jay. This was awesome. Really appreciate you coming out.
01:22:20.396 - 01:22:25.700, Speaker A: Awesome. Thanks, Dan. Yeah, thanks for having me on, guys. This was a ton of fun and anyone that has any questions can just feel free to dm me as well.
