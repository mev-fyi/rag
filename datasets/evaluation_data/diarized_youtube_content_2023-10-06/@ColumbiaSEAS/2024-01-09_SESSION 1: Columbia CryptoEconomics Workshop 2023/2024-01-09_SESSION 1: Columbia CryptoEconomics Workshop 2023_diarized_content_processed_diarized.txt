00:00:10.170 - 00:00:31.258, Speaker A: All right. Good morning, everyone. Welcome to Columbia Crypto Economics 2023. Here's a URL and a QR code for the agenda. Oh, yeah, here's a QR code and URL for the agenda. A couple of housekeeping notes before we get started. No food and drink in the auditorium.
00:00:31.258 - 00:00:56.646, Speaker A: There's going to be lunch and breaks and so on throughout the day. Those all occur back on the first floor where you checked in. So that's where the breaks will be. At the end of today, we have a reception starting at five going till seven. That will also be at that same location. For those of you that are joining us, also tomorrow, you do have to check in again tomorrow. There's going to be sort of a different colored wristband just for security purposes.
00:00:56.646 - 00:01:33.006, Speaker A: Restrooms, for those of you that are looking for them, they're sort of around this wall here. So notice there are these two exits outside of this exit around that corner, you can find the restrooms back there. And then we will be having a group photo. We're going to do it in this room. Just going to have everybody sort of hold up their t shirts and take a big photo. And that will be sort of right at the start of lunch, at the end of the second morning session. Pleased to introduce, I'm very pleased to introduce our opening speaker, the dean of engineering here at Columbia University, Dean Chang.
00:01:33.006 - 00:02:07.162, Speaker A: And before his remarks, I just want to say, know, as we all know, kind of interest sort of in the crypto space, broadly, it waxes and wanes. Right. That's true in the sort of industry side. That's true in the investment side. It's also, for the most part, been true in academia that support at various universities has sort of come and gone. And Columbia has really been kind of amazing and fairly unique, as far as I know, among our peer institutions and really having sort of unflagging support for the advancement of this technology and sort of the science behind it. And that's come largely from the leadership of both Dean Chang on the engineering side and also dean McLares on the business school side.
00:02:07.162 - 00:02:22.080, Speaker A: So that support includes support for this event. But actually there's a lot of other things they've done to support students and faculty at Columbia working in this area. So I just want to sort of acknowledge, I think, the great leadership that Columbia has shown. So please help me welcome Dean Chang.
00:02:26.930 - 00:03:06.750, Speaker B: Good morning, everyone. Just want to welcome everyone to this wonderful event here. I'm Shifu Chang, as TM mentioned, and really wonderful to see CMAC and TM organizing this wonderful event. I think it's the second year has been expanding, attracting wonderful audience and speaker to this event and particularly in this wonderful location. Let me say a few words about this forum here. This building is called a forum, was created open in 2018, probably one of the first building, maybe next to the Green Science center here and this campus here called Manhattanville which is about ten blocks away from the main campus. The last year event was on the main campus there, right? So a really wonderful event here.
00:03:06.750 - 00:03:48.458, Speaker B: The purpose of this forum is served the cross disciplinary collaboration like this one, particularly between engineering, business, economics, social science, finance, industry and foundation Israel collaboration. Thank you Israel foundation for supporting today's event and the forum serve as a gate into this new campus. The campus from 120 5th street to 130 2nd Street. I used to live just off the corner here on 25th street for the faculty house. This area was all mechanical, shop, storage and gas station. Now it's all totally different, supporting cross disciplinary collaboration like this one. The other purpose of this building is serving as a connector between the main campus on 116 to the new campus here.
00:03:48.458 - 00:04:46.074, Speaker B: And the campus right now has business school, two wonderful buildings. CMAC is enjoying his view here, see the river, see the city and also has a new site for engineering school expansion. At Columbia, we see this cross disciplinary collaboration between engineering, STEM, business school, economics, finance, social science, as well as a neuroscience, there's a neuroscience building just behind this one. And next to a neuroscience, there's a school of arts, performance, studio, gallery, so you name it. So this kind of really cross cutting collaboration connecting with the community is our key signature in this new campus here, engineering school. We also really advocate for the cross section convergence collaboration between academics, practitioner and industry. So I see a lot of a wonderful speaker and participants from all these different sectors in today's event.
00:04:46.074 - 00:05:25.266, Speaker B: For example, we use this convergent collaboration model, create several centers recently large scale center, ten years, 15 years. One is on AI, AI and climate, AI and neuroscience, AI and semiconductor and next generation wireless communication as well as AI and smart city. So we're talking to Tim. It would be wonderful to have AI and blockchain and also this crypto economics. I'm sure a lot of topic will be discussed there too. So I don't want to take too much time. I want to welcome everybody and thank you Isram for support and TM and CMAC for organizing this wonderful event.
00:05:25.266 - 00:05:31.420, Speaker B: Wish you have a wonderful discussion, come up with more collaboration to pursue in the next few years. Thank you. Welcome.
00:05:39.760 - 00:06:05.130, Speaker A: All right, thanks very much Duncheng. So let's go ahead and get started with our first session. We have our first two speakers, amazing speakers, like the ones you'll be hearing later today as well. First up, we're going to hear from Ed Felton. You probably know Ed as the co founder of off chain Labs. In previous lives, he was a professor at Princeton, where he's now emeritus. Also the deputy us CTO at the White House.
00:06:05.130 - 00:06:16.030, Speaker A: And after him will be Ben Fish. And I'll say more about him right before his talk. And are we going to bring up Ed's slides or.
00:06:18.080 - 00:06:18.540, Speaker C: Perfect.
00:06:18.610 - 00:06:25.324, Speaker A: And so Ed will talk about policy based transaction ordering for roll ups. So there's that mic. And who's that mic?
00:06:25.522 - 00:07:07.128, Speaker C: Fantastic. Okay, good morning, everyone. Thanks for your time. Today. I'm going to offer a perhaps somewhat opinionated view about this issue of how transaction ordering or sequencing should be done on layer two or layer three roll ups. And I want to start with this question. How should an l two l three chain sequence its incoming transactions? One thing to note here is that unlike the conversation about layer ones, where typically there is a design team or some kind of social consensus or core developer team who decides how it's going to be within a particular technology stack.
00:07:07.128 - 00:08:04.108, Speaker C: In the l two and l three space, we're seeing a proliferation of different chains, many of which use common technology stacks. And so among chains using the arbitrum stack, for example, different chains may choose different solutions. And so we really are in a situation where each chain will choose for itself according to its own governance. If that's a Dao governed chain, the DAO will decide which sequencing approach and which specific technology to use. If it's an app chain that is owned or controlled by a particular application developer, then they will decide. So we really need to think of this not as a group of people are going to go into a room and decide how it's going to be for the l two s or even for the arbitrum stack. Instead, we're thinking about what is it that chains are likely to choose? What will be attractive to them to do their sequencing? Okay, so here's a rough outline of what I want to go through.
00:08:04.108 - 00:08:47.704, Speaker C: I'm going to talk some about the role of sequencing in an l two or l three roll up. I'm going to take a little bit of care to talk about the role of the sequencer and how it fits into the rest of the architecture, because that's going to drive a lot of the rest of the conversation. We'll briefly review how block building operates on Ethereum. In practice, we'll talk about Ethereum's approach. It's an in practice approach of a builder auction more generally. Talk about a bit about the economics of that and about what's required to implement it. And then I'll talk about an alternative way of framing the problem, which we call policy based transaction ordering.
00:08:47.704 - 00:09:16.848, Speaker C: And I'll talk about a specific proposal that we're likely to make for arbitram chains. Again, proposal, because it's the choice of each chain. All right, so let's start with, by looking at a typical l two or l three roll up. This might be an optimistic, might be a Zk roll up. At the level of this discussion today, that distinction doesn't really matter. But we'll start with the job of the sequencer. The job of the sequencer is to produce what we'll call sequencer blocks.
00:09:16.848 - 00:10:28.600, Speaker C: A sequencer blocks is just a set of transactions which the sequencer has deemed to arrive and perhaps deemed to have arrived in some particular order. You then have a path here of execution, producing blocks. Now, execution in all of these roll ups is a deterministic function of the sequencer blocks. So if you're given a sequencer block and you know what the correct state transition function is, that is the correct function that goes from sequencer block to block, you independently can evaluate that function. And so that means that this path, which I've drawn here, is what allows all honest parties to have common knowledge of what the blocks of the chain are, because each honest party can separately and independently evaluate the execution. Or if you prefer, you can rely on someone else to do that and tell you the result. But although this gives common knowledge among all honest parties of what the blocks of the chain are, it doesn't allow those blocks to be settled directly to a base chain like ethereum.
00:10:28.600 - 00:11:33.632, Speaker C: So for that, you have this other path, which involves two steps. First posting, that is, taking the sequencer blocks. Typically, they are aggregated together, concatenated together, and then compressed for efficiency, and then post them either to Ethereum or to some other data availability layer. And then after the sequencer blocks are posted, there's a separate step of validation, which is about somehow proving to the base chain, think ethereum, what the correct blocks are, so that ethereum knows a cryptographic commitment to the correct sequence of blocks. All right, so you might ask, why do it this way? Why separate sequencing from execution and validation? Why have these two left and right paths? Let me explain. There's a couple of reasons for that. The first reason has to do with the differences between what's in a sequencer block and what's in the blocks or validated blocks at the bottom.
00:11:33.632 - 00:12:26.956, Speaker C: A sequencer block contains transactions, whereas the blocks and validated blocks at the bottom, they contain valid transactions. Note the difference in the bottom, the transactions are guaranteed to be valid, meaning that they're properly signed and et cetera. But up at the top, the sequencer just produces transactions. The sequencer can include in its sequencer block transactions that are invalid, and it's actually helpful and opens up the design space to allow the sequencer to do this. Now, a rational sequencer will never do it, because to include an invalid block will increase the sequencer's cost to no benefit. So a rational sequencer will never do this. But the protocol does not assume for correctness that the transactions up at the top are valid.
00:12:26.956 - 00:13:07.452, Speaker C: So the sequencer has a very limited job here. And that's one of the reasons we do this. It means that the sequencer and the validation and posting steps have different security requirements, and that means that we can optimize each one for those security requirements. That's one reason why we separate those functions. But the bigger reason is this. It has to do with the timing on a well designed roll up. This path on the left can be executed in about 1 second, end to end, that is, it's about 1 second from when your transaction arrives at the sequencer until all honest parties know the block that results from your transaction.
00:13:07.452 - 00:13:48.604, Speaker C: And this is very valuable for users. Right? Users love having a 1 second response time faster than the twelve or more seconds you see on, say, ethereum. And so the left hand side, because it involves just sequencing and then concurrent execution by every honest node, can happen very quickly. The path on the right is not so fast. The path on the right is going to take you at least twelve minutes. Why? Because both posting and validation require putting transactions on Ethereum, and those transactions having finality on Ethereum. And by the way, that's true if you use a Zk roll up just as much as an optimistic roll up.
00:13:48.604 - 00:14:52.828, Speaker C: Regardless of how you're doing this, this right hand path is going to take at least twelve minutes. And so the most important reason for dividing things in this way is it allows the user to get that 1 second response time, while the more sort of archival and security critical functions are able to operate at the pace of Ethereum, which is necessary to get the security that comes from Ethereum. This architecture is important. It has to do with the role of the sequencer and the role of what happens below. For the rest of the talk, though I'm just going to talk about the sequencer and how it does what it does. But the notes are that its security requirements are different from a general purpose chain or from a validation type of function, and also that its only job is to emit sequencer blocks that contain transactions that have arrived. Okay, so when we're designing a transaction, ordering or sequencing functionality for a chain, here's a set of goals.
00:14:52.828 - 00:15:23.976, Speaker C: We may not be able to achieve them all, but these at least are the things we'd like to do. First, we want low latency for that left hand path. And by the way, in order to achieve low latency in a practical sense, we need to be able to guarantee that that sequencer chain, or the chain of sequencer blocks almost never reorgs. Because if it might reorg after five or 10 seconds, then you can't really say that users can rely on the 1 second delay. So we want low latency and almost never reorgang. We want low cost of operation. This is, after all, an l two or l three.
00:15:23.976 - 00:16:03.972, Speaker C: And the reason we're doing it, the two main reasons you're using an l two or l three at all are low latency and low cost of operation. We don't want to lose those in our sequencing design. We'd like to resist front running. Front running is extractive and in general harmful to users. We'd like to resist or eliminate it. We'd like to be able to guarantee timely inclusion of transactions so that when you submit a transaction, it will be included in a sequencer block soon within some bound. If we can't do that, we can at least have a sort of warning light system which says that that guarantee holds, except when some red warning light which exists on chain is blinking.
00:16:03.972 - 00:17:11.624, Speaker C: And if you have that kind of warning light that says right now, we can't guarantee that there has been prompt inclusion over the last, say, ten minutes. This is actually a very useful thing in designing protocols to stack on top of this, because then you can have a protocol on top which goes fast when the light is not blinking and slows down when it is. We'd like to efficiently capture the MEV value, the value from transaction ordering activity, and internalize it within our own chain, rather than allowing it to be captured by someone outside. And then finally, we'd like to avoid a centralized concentration of power. But pay attention to the way I phrased that last one, because when we talk about decentralization, there's a pitfall of having, on paper decentralization which is the right to participate. What we really care about is that power be decentralized so that there are not a small number of parties who have disproportionate power in how this function operates. Okay, so in terms of how this is going to work, in practice, there are sort of two approaches of builder auction or policy based ordering.
00:17:11.624 - 00:17:52.484, Speaker C: Let me just say to the technical folks that I'm not seeing a countdown timer up here, and you're welcome to put one on the monitor down there if you like. Okay, so let me talk about the builder auction on Ethereum. So in Ethereum, of course, each block is provided by a randomly selected proposer from among the set of validators. In practice, the proposers almost always auction that right to builders. This was not part of Ethereum's initial design. Ethereum did not set out to say, we're going to have a builder auction. It really emerged as a market consequence of the design that Ethereum chose, Ethereum's large scale proof of stake block proposal protocol.
00:17:52.484 - 00:18:23.552, Speaker C: And in hindsight, it's foreseeable that this would happen. But I don't think that the community had the foresight that this was going to happen. In any case, it was not a deliberate choice. It was more a side effect of choices that were made. So what do we see in that builder market? Well, we see significant concentration in that builder market. To give you an example, there's different ways of measuring market concentration. One that sometimes uses the HHI index, which is essentially taking the sum of the squares of the market shares of the different market players.
00:18:23.552 - 00:19:19.456, Speaker C: And if you look on, say, block building activity on the latest day that was available when I prepared this, which was Monday, what you see is the HHI is 00:22, which by the standards used by at least us competition authorities, is considered a highly concentrated market. So there is concentration, and there's increasing concentration, concentration overall, since the merge is about 00:15 two, which is moderately concentrated. Okay, so we see significant concentration in the builder market. We see increasing concentration in the control of proposers through the rise of staking pools. That's a complicated debate, not my topic today, but arguably more concentration than there was before. We see an increasing amount of out of protocol transaction flow, so called private order flow, where transactions are submitted not through the standard mem pool, but directly to one of the builders. And we see large amounts of value flowing through this ecosystem.
00:19:19.456 - 00:20:09.328, Speaker C: You can ask yourself, what's the equilibrium here? And I don't know about you, but I don't love the equilibrium that this suggests. Now, I talked about the concentration. You can postulate different mechanisms that can drive that concentration. But here's one feedback loop that I think is plausible. If some builder has a better private transaction flow than its rivals, it's able in general to build blocks that are more lucrative than its rivals. It follows that that builder will get more of its blocks included on the chain. So now, if you're a party who wants to sell your transactions privately to one builder, and you'd like your transactions included, soon, you'd prefer to sell to the builder who gets more blocks included, because you're likely to have lower latency to be included, all else equal.
00:20:09.328 - 00:21:04.208, Speaker C: And so this gives you a preference to go to that builder, which gives that builder yet better private transaction flow. There's different mechanisms you can postulate, but there are various mechanisms like this that can be postulated to be driving that concentration. By the way, this graph from flashbots tends to support the mechanism that I suggested. This shows essentially that the builders who build the most blocks do tend to have a higher percentage of privately submitted transactions in their blocks. Okay, so Ethereum is in this mode where it has a builder auction and a very large proof of stake consensus. And I think those two mechanisms actually fit together. That is, if you have a very large proof of stake consensus in your block building process, you'll tend to get a builder auction emerging, as happened on Ethereum.
00:21:04.208 - 00:22:14.236, Speaker C: And if you want a builder auction, one way to do it is by very large proof of stake consensus. So returning to our question of what l two s and l three s should do. Question is, should l two s and l three s operate by having a very large proof of stake consensus for their sequencing function? Well, let's look at the pros and cons of a very large proof of stake consensus. There are two very good pros that are very important for Ethereum. First of all, this gives very strong security if there's a large amount of economic value staked, as there is on Ethereum, and also it can be used at l one. What I mean by that is, if you're designing a layer two technology, you have this huge design advantage over a layer one, namely that you can rely on smart contracts on some other layer one. In other words, if ultimately large scale proof of stake or proof of work is the only solid foundation for a decentralized consensus, if you're a layer two, you can buy that from Ethereum by using an ethereum smart contract rather than having to build another copy of it yourself.
00:22:14.236 - 00:22:46.792, Speaker C: But if you're designing Ethereum or some other layer one, you don't have that option. And so compared to other approaches of doing sequencing, large scale proof of stake consensus has the advantage. You can do it at layer one. And so if you're a layer one, this can make sense to do. The con of this is that it's very expensive. There's a lot of resource and economic value invested in any large scale proof of stake consensus system. You have to lock a lot of economic value, which is then held unproductively in the system.
00:22:46.792 - 00:23:55.928, Speaker C: And you have to, of course, operate a large number of validators. This is all very expensive, and that is revenue which could have been used for other purposes. So there's a high opportunity cost for your chain. And so if someone comes to a chain and says, I would like to give you a large scale proof of stake consensus, but you have to give me all of this value in order to compensate me and my validators, that's not going to look like a good deal to the chain. Okay, so back to this picture. I don't think it makes sense for, given all of that, I don't think that large scale proof of stake consensus is a good match for the sequencing problem for an l two, if we're not going to pay the cost of doing large scale proof of stake consensus down here, which is in the most security critical parts of the system, the parts where a security violation could lose all economic value in the chain. If we're not willing to pay for that down here, then why should we pay for it up there? Okay, so the alternative is policy based transaction ordering.
00:23:55.928 - 00:24:41.548, Speaker C: And the idea here is that the chain is going to set a policy for how blocks should be built. I'll talk in a minute about how you enforce that policy. That's going to be the key question. But the policy basically specifies two things. First, when should we make a block? And second, when we make a block, how do we do it? We specify some function f, some deterministic function f, which, given a mempool, the mempool contents, determines what sequencer block should be emitted. So an example policy might be something like make a block every half second and include in the sequencer block all transactions that are in the mempool sorted into decreasing order by priority fee, but only up to the gas limit. That's a reasonable deterministic function that might make sense to do.
00:24:41.548 - 00:25:51.280, Speaker C: This is of course great if it works. Who wouldn't want to choose exactly the policy of choice? But the question is, how do you ensure that the policy is followed? Well, here's a partial answer, which admittedly is very partial, but can help, namely a conformance check. The idea here is we're going to find some predicate p which holds for all possible valid results of that block building function. And then the execution layer of our chain, as part of the validation, is going to verify p for each sequencer block, and it's going to discard any sequencer block that violates p and then punish the sequencer somehow. Okay, so this at least guarantees that all the blocks that are produced are within a subset of the universe of all possible blocks. So, for example, if we had that previous block building policy which required that transactions be sorted into decreasing priority fee order, the conformance check would simply verify that the transactions within a sequencer block are in sorted order. That's of course trivial to verify as you scan linearly through the sequencer block in order to execute the transactions.
00:25:51.280 - 00:26:37.360, Speaker C: So this is useful. Okay. But beyond that, what you're going to end up doing is having a sequencer committee. So the policy would be implemented by a sequencer committee via some kind of BFT style consensus. This would require more than two thirds of the committee members, to be honest, because of the usual constraint on committee consensus in the presence of byzantine malicious behavior. And so you're going to operate a procedure that goes something like this. First, the committee will reach consensus on the contents of the Mem pool, that is, a set of transactions that are deemed to have arrived as of the time that the policy says to make a block.
00:26:37.360 - 00:27:28.544, Speaker C: Then we can optionally have a step of threshold decryption. If people are worried about front running by committee members, they can submit their transactions in encrypted form, and the committee can do threshold decryption with the decryption threshold requirement being the same as the committee consensus requirement, so that you don't have to make an additional trust requirement for this. And then finally, having decrypted, they have the mem pool. All the members of the committee can concurrently compute the function f to build the block, and then they all sign the block, and you aggregate those signatures. And that's a sequencer block. So this is a rough outline how you can use a consensus committee to follow policy based block building. How large should this committee be? Well, that's a pragmatic decision around trust.
00:27:28.544 - 00:28:08.760, Speaker C: As a practical matter, I'd say maybe twelve to 20 would make sense if you just think about it. Because these parties are chosen based on trust, it's not often, not that you don't want the committee too small, so that the compromise of a small number can destroy your trust. On the other hand, as you start making a longer and longer list of people you trust, your marginal trust in the next person goes down. And so it would be hard, I think, to find a set of, say, 1000 parties who you trust that strongly. Okay, so a medium sized committee consensus via this kind of mechanism. All right, so how is this committee chosen? Thanks. Yep.
00:28:08.760 - 00:28:56.472, Speaker C: How is this committee chosen? Chosen through chain governance. However each chain decides if they decide to adopt this policy, they'll also decide to adopt a particular committee. Maybe that's a Dow vote. Maybe that's a unilateral announcement by a chain operator, depending on the governance for each individual chain. What if the committee colludes to make some other block outside the policy? Well, now your committee is behaving like a profit motivated block builder. And so, although we don't like this, arguably it's not that much worse than what we'd get with a builder auction, because there you also have rationally motivated block builders trying to maximize their own return. And if this happens, governance can replace the committee.
00:28:56.472 - 00:29:51.356, Speaker C: What if the committee doesn't act? Well, you need some kind of fallback liveness mechanism. Okay, so the candidate policy that I actually like for this is time boost. So what this is, is a sealed bid all pay priority gas auction over transactions. Every transaction comes in. And so the policy is to sort transactions into decreasing order by priority fee with a deterministic tiebreak within the block, and then publish the resulting sequencer block, and then wait for a fixed interval, say half a second, to start the next round. Why do you wait? Because you want all of the potential mev searchers to know what the current block is, have time to do a little analysis and submit their transactions for the next round. By doing that, you ensure that as much as possible, the competition to be first in the next block is channeled into this auction and not channeled into latency racing.
00:29:51.356 - 00:31:00.132, Speaker C: There's some interesting analysis of how the interaction is between biding and latency racing, which is ongoing research by our team. Happy to talk about that later. Okay, some simple examples, economics of this. If you imagine that there's a single arbitrage opportunity in a block, such that the first party pursuing that opportunity gets it, what you have is essentially a sealed bid all pay auction for that opportunity, which is a reasonably efficient way to distribute the opportunity and capture the valuation of it. If you have two independent arbitrage opportunities where the first transaction going for each wins a reward, then it operates essentially like two separate sealed bid all pay auctions and because your bid is a priority fee paid per gas, there's not an incentive to bundle different transactions together under a single bid. And so this tends to provide a sort of strategy independence property, which is really nice, which you can see explored more in our paper from the aft conference. Okay, I would add transaction bundles to this.
00:31:00.132 - 00:32:21.520, Speaker C: You accept submission of bundles containing multiple transactions, and the sequencer guarantees that transactions in a bundle will be in the same sequencer block, and also they'll be consecutive in the block if they have the same bid consecutive and in the same order as in the bundle. This gives a submitter some latitude to arrange their transactions as they see fit, but without breaking the basic principle of the auction, revenue collection is relatively easy to do because this is a priority fee, an extra bid per gas, and priority fee is already a concept in the Ethereum stack. Any Ethereum compatible chain already knows how to collect a priority fee, and so the chain will already just automatically collect this fee. That's cheap and convenient. The funds are held and collected on chain as part of the validated execution of the chain, and so you can have high trust that they're collected correctly and also that they'll be distributed according to whatever policy is set by chain governance as to where those fees should go. If you have a shared sequencer across multiple chains, each chain will collect its own priority fees and so you'll get automatically a nice distribution of the fees across the chains. The path forward for this, we're implementing the time boost policy in Arbitrum's centralized sequencer as an option for chains that want to adopt it.
00:32:21.520 - 00:32:57.792, Speaker C: And then there's joint research with the espresso systems team to develop a decentralized version of this. That's it. That's my opinionated take on how to do these things with an offer at the end, at least to arbisram tech chains. And I hope we have time for questions or discussion. Please raise your hand. I'll come on down to the mics to Mike, then microphones on either side. Thanks for the great talk.
00:32:57.792 - 00:33:44.320, Speaker C: I'll give you a super easy basic question first, just checking something. So what is it specifically about proof of work that enables the proposer, the builder auctions? Is it simply the fact that the proposers know ahead of time they're assigned slots relative to proof of work? For example, partly that the proposers know they're assigned slots. It's also the case that there are a lot of proposers, and so it's not really efficient for each proposer to build their own block. Right. Block building is a sophisticated process. At the very least, you need to be able to actually execute all of the transactions and figure out if they're valid. You need to sort through submitted transactions and throw away the invalid ones, which even that is probably too much for every proposer to have to do.
00:33:44.320 - 00:34:46.612, Speaker C: So proposers want their job to be easy. There's a large number of them, whereas builders are, there's a smaller number of builders. That's why I think it makes sense with a smaller committee of twelve to 20, to have them actually doing this work redundantly. Okay, so my question is, is it possible to see the incoming private transactions? And if not, is it possible for relayers like blocksroute and some of the other companies you've listed to see the private transaction contents? Like knowing exactly how much they're trading and things of that nature? Yeah. So let me talk about how this would work in kind of the end game system that I was talking about. So transactions can arrive to the sequencer in either plain text or encrypted form, right. The encrypted ones are not decrypted until after there's already a consensus on what the contents of the block will be.
00:34:46.612 - 00:35:59.368, Speaker C: So those at least are not revealed until essentially the block contents are committed. And so there's not the possibility of seeing the decrypted plain text of those and then injecting a transaction that can get in front of them. That said, the question is, well, how do transactions get from the end user to the sequencer? Right? And so in a world with the encryption of transactions, in practice has to be optional, because legacy wallets don't encrypt transactions, at least don't encrypt them this way. Right? But at any point between, I'd say two things. First of all, at any point between the user's wallet and the sequencer itself, encryption can be added. You'd expect that a standard node in a system like this, like a standard arbitrum node, would know how to and would encrypt incoming transactions. So if a user's wallet rpcs to a node with a plain text transaction, the node would encrypt that transaction and then forward it on to the sequencer.
00:35:59.368 - 00:36:46.680, Speaker C: So that node would get to see, and the user's wallet would get to see the transaction. People who are generating transactions, MeV searchers, are typically generating transactions programmatically, and presumably they would encrypt their transactions. The other question though is, could there be a sort of bundle or a block builder who buys from users private transaction flow, they could. They're not going to help that user get better position within a block. But in principle, yes. You could have a party who buys private order flow from users and then submits on behalf of the users to the sequencer. Those users transactions will typically execute later than they would have otherwise, and with higher latency, that process is not going to help you get your transaction in sooner.
00:36:46.680 - 00:37:48.400, Speaker C: Unlike on Ethereum, where selling your order flow to a builder might help you get your transaction in here, I don't think it necessarily does. Okay, as a quick follow up, is it safe to assume that only builders, if you are a searcher, there's no way to see private transactions unless you ran a large infrastructure company or you were a builder. If you're a large infrastructure company and users are using your wallet, in principle, you could direct the flow from those wallets off to wherever and then extract. Right. That's a known issue that exists today, and that would continue to exist in terms of ability to see private transaction flow. If a user is encrypting their transaction at submission or by some trusted node in transit, then, right. That transaction would not be visible to anybody until its position in the sequence is already committed.
00:37:48.400 - 00:38:25.900, Speaker C: Meaning that everything, not only is its position committed, but everything before it is also committed. Okay, understood. Thank you. Yes, I just had a quick question. So, under this new time boost regime, essentially it does still require a number of trusted parties for the BFT style consensus. And these parties wouldn't necessarily be aligned with people submitting transactions, but rather are governance holders who they're accountable to. And so would we end up potentially with a scenario where they try and bribe arb governance holders to let them front run in exchange for like a staking feedback or something.
00:38:25.900 - 00:38:55.044, Speaker C: Right. I mean, they could try to make a deal. This whole thing is, if we're talking about this operating on say, the arbitram one chain, this whole arrangement would exist because the Dow, which is the token holders, have chosen it. So I think it would be slightly perverse result to choose this and then make an arrangement with this committee to do something different. The token holders would be better off just choosing that other thing instead.
00:38:55.242 - 00:38:55.652, Speaker D: Okay.
00:38:55.706 - 00:39:09.680, Speaker C: So the idea is they would just directly extract value rather than do this, then extract, if they want some other arrangement, they could choose that other arrangement. This is an option they could choose. And I think, yeah, if they wanted something else, I would expect them to choose that directly.
00:39:09.860 - 00:39:10.732, Speaker D: Okay, nice.
00:39:10.786 - 00:39:11.790, Speaker C: Thank you. Yeah.
00:39:14.880 - 00:39:49.640, Speaker E: I have one observation, a question, one observation. I'm an economist, so it seems like there are a lot of diversity in needs and willingness to pay. And so you might have some transactions that have high security requirements but low latency requirements conversely. And so it might be that different chains offer different types of services. That's an observation. I think this is very useful in looking at that on the auction front. I had a comment, more technical.
00:39:49.640 - 00:40:11.580, Speaker E: You weren't clear. It sounds like your auctions for a single block or single transaction. In auction design, at least the auctions I work on multiple blocks. And you think about Google's ad for position, they would have the highest value, gets first position, second highest.
00:40:12.720 - 00:40:14.220, Speaker C: That's what I'm suggesting.
00:40:14.300 - 00:40:14.930, Speaker E: Okay.
00:40:15.540 - 00:40:22.332, Speaker C: Every transaction that's submitted, no matter the bid is included, but they're sorted into decreasing order by their bid.
00:40:22.396 - 00:40:24.432, Speaker E: So it works sort of like Google.
00:40:24.566 - 00:40:25.440, Speaker C: That's the idea.
00:40:25.590 - 00:40:44.888, Speaker E: Because all pay auctions is a revenue equivalence theorem, which you're probably aware of. And all pay auctions are revenue equivalent to first price, second price for single objects only. There are other issues with revenue equivalents that make it harder for bidders. Perhaps you're aware of.
00:40:44.974 - 00:41:35.432, Speaker C: Yeah. So what's interesting here is that different bidders are, in general pursuing different goals. And so that is there's different arbitrage opportunities that may be available or different back running opportunities and different bidders. The bidders are not all competing for the same thing. Or like an ad auction where they're bidding for first position, second position, third position, which are sort of different quality versions of the same good, if you will. It's not quite like that. If Alice, Bob and Charlie are the top three bidders, and Alice and Bob are all seeking one arbitrage opportunity and Charlie is seeking some independent arbitrage opportunity, then Charlie's perfectly happy.
00:41:35.432 - 00:42:04.092, Speaker C: As far as he's concerned, he's a winner because he captures the valuation that he's seeking. So I take your point that this is not exactly that. It's only an all pay auction in the case where there's a single good that's being pursued. But it's a little bit more complicated, I think, even than the ad auction setting. I'd love to have an out of band conversation about this to dig further and also with some of my colleagues who understand this stuff more deeply.
00:42:04.156 - 00:42:04.944, Speaker D: No, because I do.
00:42:04.982 - 00:42:08.524, Speaker E: I work in spectrum auctions and energy auctions, and there's some analog.
00:42:08.572 - 00:42:57.904, Speaker C: Yeah, love to talk. Okay, thanks. Just go over here. Hey, thanks so much for the talk. I'd love to hear a little bit more about some of the design challenges you've run into with the more decentralized implementations of time boost and any interesting research avenues there? Sure, I'd say a couple of things. One of them is just the question of what kind of consensus protocol should you be using? Because the requirements here are a little bit different than the standard kind of BFT blockchain requirements. In particular, there is this requirement to pause for that 500 millisecond analysis and submission window periodically, and that's not a requirement that exists typically.
00:42:57.904 - 00:43:21.150, Speaker C: So often, for example, consensus protocols try to use pipelining in order to have multiple, sort of the construction of multiple blocks or rounds of consensus in flight at the same time. But that's not a goal here. So there are some differences in particular about how this works. I'd be happy to talk more offline about that as well. Thank you.
00:43:21.920 - 00:44:09.770, Speaker D: Yeah, so two part question on the kind of policy you're suggesting. First part is it feels a bit like with sorting just by priority fee. We're going back to some extent to priority gas auctions. How we had it previously on ethereum. So I'm wondering, isn't there a risk of competition if you really want to get your transaction in that there will be network congestion, because then you'll pay like a really high priority fee, and that will be bad for other users of the system. And second part, if a lot of those transactions are trading transactions, which is currently often the case, isn't their economic value lost because you could have a different sequence of those transactions and then basically serve people at different prices and kind of serve their intents better?
00:44:10.380 - 00:44:55.896, Speaker C: A couple of things. One is, let's see, on the first point, ethereum, of course, did have in effect a priority gas auction system for a while, and you saw some perverse behavior there. But one of the reasons you saw perverse behavior is because it was not a sealed bid auction, but because people could see each other's transactions in the mempool. And so it was a kind of iterative auction, and that led to a lot of churn in the mempool, especially at the end as the deadline neared. But with the sealed bid auction, you don't have that situation. Each party, you don't gain information as the time goes on, at least you don't gain information about others bids. And so each party should just compute what is their best bid according to their strategy and submit it once.
00:44:55.896 - 00:45:47.610, Speaker C: So you don't have that kind of churn in terms of is it wasteful to have multiple transactions trying to capture the same opportunity being submitted? I think the answer is yes and no. It is wasteful in a sense that you have multiple bidders who are paying to have their transactions run. There's a kind of pseudo waste because it's an all pay auction, which in some sense is not waste, right under auction theory, because people bid less in an all pay auction. But there's also the question of, is it waste to actually execute those transactions that look and say they look into the market, they say, I'm seeking this arbitrage opportunity. Oh, it's not there anymore. Terminate. Is it waste to run those? And again, I think the answer is yes and no.
00:45:47.610 - 00:46:46.412, Speaker C: This is work that the sequencer has to do. And in a block building model, the block builder has to do work in order to execute and analyze transactions that ultimately don't find their way into the block. So although it may be tempting to say we're going to give these transactions a way to not be included if they don't win their race or they're not the high bidder, for their opportunity to actually do that would require a couple of things. First of all, it opens up a kind of a free riding problem where people can throw large numbers of transactions at the sequencer, which has to process them and only later discover that they're not included. And of course, there's no cost to parties submitting that. So you have a free writing problem, which is a significant one in any l two or l three sequencer, I can tell you from experience. And one way around that free writing problem is to have that actually carry a cost.
00:46:46.412 - 00:47:25.210, Speaker C: If you submit a transaction which the sequencer is going to have to run as part of its block building process, then we actually want you to pay gas because we want you to internalize the cost that you're imposing by submitting that thing. So arguably, it's not waste that. Arguably, it's a cost you're paying that is designed to give you an incentive, which is designed to align the submitter's incentive with the cost that they are imposing. All right, thanks, everybody.
00:47:38.310 - 00:47:55.450, Speaker A: So we've got that mic that Ed was using and also the handheld. Can we get Ben's slides up? And then you may or may not get some time warnings on there.
00:47:55.980 - 00:47:57.692, Speaker D: I don't think I'm going to use the whole time.
00:47:57.746 - 00:48:05.856, Speaker A: Okay. I think it was great. The big Q and a sessions. Can we get on the main screen as well?
00:48:06.038 - 00:48:29.736, Speaker D: Okay, and this is the clicker here?
00:48:29.838 - 00:48:35.340, Speaker A: That's right. Yeah. And there's a laser pointer if you need it. So forward, backward laser pointer.
00:48:43.280 - 00:48:43.964, Speaker C: All right, great.
00:48:44.002 - 00:48:55.200, Speaker A: Let's proceed to the second talk of this first session. Very pleased to welcome Ben Fish, who's a professor at Yale University, also co founder of Espresso Systems. He'll be talking about revenue allocation and shared sequencing.
00:48:57.060 - 00:49:29.950, Speaker D: All right, thank you very much, Tim. So I'll preface this by saying, first of all, I'm not an economist, I'm a cryptographer. So this is mostly an appeal to all the economics students or economists in the audience. I'm hoping to be able to motivate you to think that this problem is interesting and that you can help me or us solve it. Okay, so what is this problem exactly? That green button, right?
00:49:32.760 - 00:49:34.740, Speaker A: Maybe point it upward?
00:49:40.090 - 00:49:41.960, Speaker D: Well, that's the first problem.
00:49:45.690 - 00:49:46.680, Speaker A: There we go.
00:49:50.190 - 00:50:38.854, Speaker D: Juiced. All right. Okay, so let me first introduce, in case there are some of you out there who don't know what roll ups are. So roll ups are this solution that is horizontally scaling Ethereum. The idea is you can add many different servers on the application layer of the blockchain that sort of shard the work of being able to gather transactions and prove how these transactions are updating the collective state of the blockchain. And the key point here is that one computation is being split across different servers. So if you add a new application to Ethereum that is independent from other applications, you can add a new set of servers.
00:50:38.854 - 00:51:44.830, Speaker D: That new set of servers can execute for that application. And therefore, adding that application doesn't burden the layer one nodes, which are trying to reach consensus on the ordering of transactions and on what the state is. And in this case, the layer one nodes can just receive proofs of validity from these application layer servers that prove to them what the result of executing these transactions are. Another key point about roll ups is it's leveraging heterogeneity that naturally exists in a network. So if you want to run this layer one system like Ethereum on 12,000 nodes, or even more, then there's going to be a big gap between the weakest nodes in the system and the most powerful nodes in the system. And so what roll ups allow you to do is to enable the more powerful nodes that are able to execute transactions at a very high rate, to enable the weaker nodes to keep up in some sense. So these are two ways to think about how roll ups are able to scale the throughput of a blockchain like Ethereum.
00:51:44.830 - 00:52:46.500, Speaker D: Now, roll ups, today, they run their own isolated sequencers. The sequencer is a component of the roll up that I'll explain in more detail in a moment. So the sequencers are basically collecting transactions and determining the ordering of transactions for each of these different roll ups. And the main drawbacks of this is that these roll ups are effectively isolated from each other, unlike applications on Ethereum, where one transaction can update multiple applications at once. You could have a transaction that sends money from one party to another and then does some exchange in a decentralized exchange and then transfers some more funds. You could have transactions that have if clauses, they have all kinds of conditional execution clauses, et cetera. This is not possible across applications that now exist on different roll ups because they are not doing their sequencing together.
00:52:46.500 - 00:53:43.378, Speaker D: You can't have a single transaction that updates multiple roll up states at once. So the job of this roll up sequencer is to collect transactions that are submitted by users, determine their order, determine which transactions get included into the next block that's going to be appended to this roll up. And today it also sort of combines the role of also then executing these transactions, determining what the state transition is. If it's a ZK roll up, it would actually post a proof that it executed them correctly. If it's an optimistic roll up, it would just post the state route to the Ethereum contract, and that may be challenged by some other node later. But the most important point here is that it's determining the roll up block. So which transactions are included in the roll up and their order.
00:53:43.378 - 00:55:03.370, Speaker D: In other words, it's functioning like a consensus protocol for the roll up. Now, an important note is that Ethereum also functions as a consensus protocol because once the block is posted to the contract on Ethereum, then Ethereum has finalized the order as well, and the sequencer can't change that later. So Ethereum in some sense acts as a notary service that once the sequencer makes up its mind about what it should post, it can't change its mind later. Once Ethereum says, okay, this is final, right? There is also a detail I'm glossing over here, which won't be so important for the talk. But if the sequencer is censoring some transactions, and users could send their transactions directly to the contract on Ethereum, and there may be a force include mechanism, but that is forcing users to go on an alternative path. So we can still think of the sequencer predominantly as a consensus protocol, and often users will trust what the sequencer says as the next block, as final as well. So additionally, just like leaders in consensus protocols, a sequencer may naturally want to run an auction.
00:55:03.370 - 00:55:57.178, Speaker D: This is a natural market dynamic that arises because the sequencer may not know how to build the most valuable block. It may not know the best way to build the block. So naturally it may run an auction among various specialized services called builders that are competing with each other. And it would run some auction mechanism, and through this it would learn some kind of block from the builder network. Okay, this is the same dynamic that occurs with leaders in consensus protocols like Ethereum. Now a shared sequencer architecture, which also goes by the name as base roll up. For the picture that I have here, because we're going to use the layer one as a shared sequencer, the architecture looks as follows.
00:55:57.178 - 00:56:54.826, Speaker D: It's a subtle change. Now, instead of having each roll up be in charge of ordering its own transactions and posting the next block to the layer one for verification, users would send their transactions directly to the layer one system itself. The layer one would order these transactions and make the transaction data available to roll ups for execution. But the layer one does not execute itself. And I'm glossing over some details, but at a high level, what the roll ups would do is they would read the list of transactions from the layer one. They would execute these transactions to determine the state transition, and then post the result of the state transition back to the layer one for verification. There may be other subtle issues, such as the inability of the layer one to tell if users are paying fees correctly.
00:56:54.826 - 00:57:48.450, Speaker D: So there may be spam issues, but I'm not going to go into any of those hairy details of shared sequencing in this talk. We're just trying to get a picture of what shared sequencing is at a high level compared with isolated sequencing. The key advantages of shared sequencing may be related to security and decentralization. Now we're using the layer one entirely, which is potentially much more secure and decentralized than these individual roll up servers. We're using that for the liveness as well as the safety of the roll up, because it's the first line of defense in terms of determining the ordering. It may have economic advantages as well. If the layer one nodes are sufficiently decentralized so that they're not going to monopolize pricing, that could be economically advantageous for users.
00:57:48.450 - 00:58:40.420, Speaker D: A server that is an isolated sequencer may act as a monopoly and increase prices. The second advantage is around improved interoperability. So now, because one system is ordering all these transactions together, suddenly it can enable atomicity between transactions. So a single transaction that the layer one receives from a user could include multiple transactions for individual roll ups, and the layer one could ensure that they all get sequenced together. You could also have a separate shared sequencing layer, that is not the layer one. There could be various reasons for doing that. We often call this a layer 1.5.
00:58:40.420 - 01:00:33.190, Speaker D: Now, importantly, and related to this enhanced interoperability, this architecture also gives rise to a natural auction where builders may bid on roll up block bundles. Right now, the shared sequencer is playing the role of all these individual sequencers at once. But instead of just auctioning off to builders the right to propose a block for each roll up individually, it can now simultaneously auction off the right to update all roll ups at once, or potentially a subset. And in fact, because builders can now bid on building a super block that affects all these roll ups at once, they can now make promises to users around atomicity of transactions across different applications on different roll ups. For example, a user may ask a builder, I'm willing to pay this if my transactions on roll up a and b execute either together or not at all, and I'm not interested in either of them executing if the other doesn't execute. So a builder can now satisfy that preference if it wins the auction for a super block instead of having to participate in each of these auctions individually. So we have two different one way to summarize the dynamic of shared sequencing versus isolated sequencing is that we have two different types of auctions, right? One is a shared auction where the shared sequencer can auction off a super block of all the roll ups at once, and the other is an individual auctions where each roll up itself is auctioning off its block to a network of specialized builders.
01:00:33.190 - 01:01:53.150, Speaker D: So the shared auction revenue may be higher than the combined isolated auctions. There could also be counterexamples where it's lower, but the shared auction revenue may be higher. But even if it is higher, it's being captured by the shared sequencer instead of the individual roll up sequencers. So the question then becomes, how should the shared auction revenue generated by this shared sequencer be allocated among the individual roll ups? And the goal here is to create some kind of stable mechanism through which the roll ups will participate in this shared auction and be convinced that they're better off running in this shared environment than on their own. Right? Now, certainly the shared sequencer can also simulate the n individual auctions. So there is hope that we should be able to create some kind of stable mechanism in the shared sequencer environment where we're always able to give the roll ups a better deal than they would get running on their own. Okay, so this is the question, is there a stable mechanism through which roll ups can participate in the shared auction and be convinced they're better off than running in isolation.
01:01:53.150 - 01:02:45.250, Speaker D: And the problem is that while they're running on this shared sequencer, they don't know what they would be making if they were running on their own. So they don't know the counterfactual. There are many, many analogies to this problem. So you could have bands playing at a music festival. The festival is selling tickets. How should the ticket revenue be allocated among the bands to convince the bands to keep coming back to this music festival, rather than running their own festivals? You could have a travel agency that's selling tickets, combining flight legs from multiple airlines. How should the ticket revenue be allocated among the airlines? I'll move to a toy example that I think captures the essence of the same problem that we're dealing with in the rollups, in the shared sequencer setting.
01:02:45.250 - 01:04:12.190, Speaker D: Let's consider that we have a company a that manufactures only right shoes, and we have a company b that manufactures only left shoes. Then we have a shoe store that is offering to auction the pair of shoes together. So how should we allocate the auction revenue that the shoe store gets among companies a and b? Also, let's assume that we're only manufacturing one pair of shoes a month. So normally, shoe stores are selling many pairs of shoes. It's not quite an auction, but for the sake of this toy example, we manufacture one left shoe, one right shoe at a time, and then we decide, should we auction off our left shoes and right shoes independently, or should we give them to the store to auction them off as a pair? Okay, so if we give it to the shoe store to auction off as a pair, how should the auction revenue be allocated among the companies a and B? And the goal is that a and B should be convinced that they're better off giving their shoes to the store than auctioning off the shoes separately. And again, the key challenge, which is the same in the roll up setting with shared sequencers, is that companies a and b don't know how much they would make if they ran their own auctions independently. While the shoe is the one, the shoe store, excuse me, is the one that is running the auction.
01:04:12.190 - 01:05:09.802, Speaker D: So let's try to come up with a possible solution. So let's say a and B. And by the way, I'm going to be spitting out naive ideas that, again, I'm not an economist, but we'll go through a few different possible solutions, and hopefully this can give all of you in the audience who have more expertise in this area some feeling for the problem. And maybe you'll pull out some result from the classical literature and say, hey, Ben, this is solved, and send me a paper, which would be an excellent outcome from this talk. So, possible solutions? Well, A and B could sometimes run the auctions independently to learn the so called valuations of the left shoes and right shoes individually. And they can alternate running the auctions independently from giving their shoes to the store. So let's say on Mondays they run the auction independently.
01:05:09.802 - 01:05:53.286, Speaker D: On Tuesdays they give the shoes to the store. On Wednesdays they go back to running it independently, and they're trying to learn. Okay, well, when I do it on my own, how much am I making? Well, there's a couple of issues with this. We can't trust companies A and B to report their individual auction revenues honestly, because the idea would be to use those numbers in order to determine how we can split the shared auction revenue, so we can't trust them to report it honestly. There are other issues as well. The presence of. You would think that, oh, these auctions that are alternating are completely independent, but they're not quite independent.
01:05:53.286 - 01:06:57.854, Speaker D: If I know that on Tuesday I can buy a pair of shoes, and on Monday I'm only able to take the risk of ending up with only a left shoe or a right shoe, I may just wait for Tuesday. Right? So the fact that I know on Tuesday the auction is going to be running differently from on Monday may cause me to wait and not actually participate and bid in the auction on Monday, because again, on Tuesday, I can express my preferences in a richer way. Okay, so the existence of the shared auction and alternating slots affects the valuations in the independent auctions. So here's a possible solution, too. The shoe store could alternate running the left and right shoe auctions independently and auctioning the pair. So every other auction, it auctions left shoe, right shoe independently, and then on the OD slots, it auctions off the pair. So let's let VA denote and Vb denote the revenues from the independent auctions for a and B, respectively.
01:06:57.854 - 01:08:00.760, Speaker D: So when we run the independent auctions, we just distribute VA to the company a, Vb to the company b, and when we run the auction on the pair, then the revenue r could be allocated based on past revenues, Va and VB. Let's say we just take VA and Vb from the last individual auction it was run, and we use that, and we could plug it into some allocation rule, right? There's many different allocation rules. I'm not really sure which one is best for this scenario. You could have a proportional allocation rule, where a gets VA over VA plus Vb times R, you could have a shapley value allocation rule. In this case, a would get one half R minus Vb plus Va. B would get one half R minus VA plus Vb. Or assuming that r is greater than or equal to VA plus Vb, maybe we just want to give both a and b, Va plus epsilon and Vb plus epsilon, respectively, and either burn the surplus or the surplus goes to the shoe store.
01:08:00.760 - 01:08:53.382, Speaker D: In any case, there are still several issues. So, first of all, bidders may still prefer to wait for the shared auction, right? We haven't gotten rid of that issue again. Every other auction, we're auctioning off the pair. So if every hour we're doing this, or even every day, you don't have to wait that long to just buy the pair of shoes, which is what you really want to do, then you may not participate in the individual auctions. So the fact that we're alternating these auctions does create some correlation between the valuations. The fact that we're running the pair auction impacts the valuations measured from the individual auctions. Okay, got stuck again.
01:08:53.382 - 01:09:48.162, Speaker D: Okay, now, another, possibly even bigger issue is shill bidding, right? So company a or B may submit high bids in the individual auctions to inflate their valuation, and this would increase its allocation of revenue from the shared auction. So we've gotten rid of the issue that we can't trust company a and B to report honestly the revenue that they're making. But we haven't prevented them from participating in the auction. So they may end up participating and bidding very high in one of these left or right shoe auctions. And they can do that for free because the payment will go back to them. So we're going to address the shill bidding issue. But let me first address the other issue first.
01:09:48.162 - 01:10:50.074, Speaker D: So we're incrementally building towards what I'll call possible solution three. So instead of alternating the auctions, running first the individual auctions and then running the auction on the pair, let the shoe store simultaneously collect bids for independent left and right shoe auctions and the pair auction. Okay, so let's suppose that we run truthful auctions, let's say second price auctions, for example. And then it uses these bids to calculate hypothetical winners and auction revenues, VA, VB and R. Okay, so it's simultaneously collecting these bids. We're actually going to allow users or bidders, excuse me, to reuse the same funds to bid in both auctions simultaneously. Because what we're going to do is we're only going to end up executing one of the auctions and we'll cancel the other.
01:10:50.074 - 01:11:36.874, Speaker D: So what we do is we then flip a coin and if it's heads, then the winners of the independent left and right shoe auctions are selected and the pair auction is canceled. And then the revenue VA and VB would be distributed to a and B respectively. If it's tails, then the winner of the pair auction is selected. The revenue R is then distributed among a and B using some allocation rule, could be proportional shopli, et cetera. We would use the valuations Va and VB calculated from the bids to the independent left right shoe auctions in order to inform this allocation. So the analysis is as follows. So let's first assume that there's no collusion between the agent's bidding and the company's a and B.
01:11:36.874 - 01:12:30.234, Speaker D: Now it's a very big assumption, right? There's shield bidding. So let's just set that aside for a moment. If there's no collusion, then what we've accomplished here is that because there's a probability one half that you will end up running the independent auctions and a probability one half that you'll end up running the pair auction. Agents should bid truthfully in both auctions, right? And their outcomes in one do not affect their outcomes in the other. In fact, I believe that this would remain true with weighted coin flips with probability heads, any P greater than zero. If P is equal to zero, then it's hard to argue that anyone would take it seriously and actually bid truthfully. But as long as there's some sufficiently high probability that you're going to select the pair auction, then you should bid truthfully in that.
01:12:30.234 - 01:13:30.812, Speaker D: And the same goes for the independent left and right shoe auctions. So that's why we can hope to get correct valuations VNVB and use them in order to inform this allocation rule. Now the problem of course, is that we can't assume that there is no collusion between the agent's bidding and the company's a and b. So how do we solve shill bidding? A typical solution is to burn some fraction of the auction revenue. Okay, so let's say we burned a beta fraction of the auction revenue. How do I go past the five minutes remaining notice? Okay, well, in any case. So how do we solve the shill bidding? Well, let's say that we burned a beta fraction of the auction revenue when we run the individual auctions.
01:13:30.812 - 01:14:33.524, Speaker D: Okay, so in the case that we get heads, the winners of the independent left and right shoe auctions don't get VA and VB. They get one minus beta times VA and one minus beta times VB, respectively. And a beta times VA is burned and beta times VB is burned. So let's say that company a tries to raise Va by some epsilon by submitting some bid to this auction that would say, calls the second price to be va plus epsilon instead of va. So that means that a is going to end up having to burn some beta fraction of va plus epsilon. So its expected payoff change is going to be, well, with probability one minus p over two. Sorry, with probability one minus p.
01:14:33.524 - 01:15:14.344, Speaker D: So I'm just using the Shopley value allocation rule. With probability one minus p, we get tails, and it ends up getting its shopli value distribution of the revenue r. So its increase in revenue would be epsilon over two. In this case, with probability p, we end up selecting the independent left right shoe auctions. And so now it has to pay beta times VA plus epsilon. Okay, so that's its loss in that case. So we would like its expected payoff change to be negative by doing this behavior.
01:15:14.344 - 01:16:04.524, Speaker D: And that would be achieved by setting beta to be greater than one minus p over two p, which is fine, I guess, when p is equal to one half, then beta would be equal to one half. It's not great because we're burning half of the revenue, and that may not be so great for company a and b to want to participate in this at all. One thing that we could do is we could decrease the probability that we select the pair auction. So three quarters of the time, we actually just run the auctions independently, and one quarter of the time, we run the pair auction. And then that would decrease beta to one over six. So that might be better. Okay.
01:16:04.524 - 01:17:04.464, Speaker D: Yeah, perfect. So, from shoes to a shared sequencer auction, so we can apply the same solution to a shared sequencer with n roll ups. We would collect bids for n independent roll up block auctions, and one superblock auction with all the n roll up blocks. And so let's let v one through vn denote the independent block revenues and vs the super block revenue based on these bits. So, with probability p, we would select n winners of the independent auctions, and otherwise we would select a winner of the superblock. If independent then is selected, then we would allocate vi times one minus beta to the ith roll up, and we would burn beta times the sum of vi. On the other hand, if the superblock wins, then we would allocate this revenue vs.
01:17:04.464 - 01:17:33.168, Speaker D: By applying some allocation rule. Now, unfortunately, if we're n greater than two, it's not entirely clear how we would use, for example, a shoplive value allocation rule. We would need to run some kind of combinatorial auction to get the valuations of individual combinations. So we might just stick with something like a proportional rule. Or we could consider giving each roll up. Vi plus epsilon. Again, it's not entirely clear to me.
01:17:33.168 - 01:18:18.152, Speaker D: I think it's an open question how these allocation rules affect the overall analysis. So another consideration is whether to run a lottery instead of an auction. So the main problem with the auction is that it's possible that a single builder may win every auction, and this could potentially lead to monopoly pricing. We could get stuck in an equilibrium where there's a single builder that's charging some revenue maximizing price, which is greater than the market clearing price. It's excluding users that are unwilling to pay this revenue maximizing price. It's leaving blocks below capacity. And yet no other builder is able to win the auction to include these transactions.
01:18:18.152 - 01:19:05.208, Speaker D: That could be a potential issue. So instead, consider what might happen if we operate a lottery. This is similar to proof of work or proof of burn. Here, builders are not participating in auction, but they're buying lottery tickets for the right to produce a block, and a lottery winner is selected at random. So how is this different? Well, unless one builder buys 100% of the tickets, then a second builder can now include transactions at a lower price with some probability. And the fact that I, as a user, could just maybe wait for another builder with some probability, let's say 10%, I may not pay this higher price. So this could potentially break the monopoly.
01:19:05.208 - 01:19:53.976, Speaker D: It's not entirely clear to me. I think this is another open question, sort of studying the differences between lotteries and auctions, and their impact on monopoly dynamics. But it has been observed, for example, in proof of work system like bitcoin, that even without mechanisms like EIP 1559, you achieve some kind of non monopolistic pricing. There's a paper monopoly without a monopolist that goes through that. So finally, we can adopt the solution from the auction to the lottery setting. One problem is adjusting the lottery ticket prices to approximate the clearing price. Here, what we would do is we would run n plus one lotteries.
01:19:53.976 - 01:20:42.984, Speaker D: We would set some cap m on the number of tickets available for sale in each set, and some initial ticket prices. If all the tickets are sold for the ice lottery, we re raise the price. If fewer were sold, then the price would be reduced in each round, with probability p, we select winners of the individual roll up lotteries. Otherwise we would select a winner of the shared lottery, and ticket purchases for the individual lotteries are refunded or canceled if the shared lottery is chosen, and vice versa. And then we would apply some allocation rule. Everything else remains the same. I claim that if lottery tickets are sold at the true clearing price, the revenue for individual roll up lottery, I should be at least as high as the auction revenue.
01:20:42.984 - 01:21:54.780, Speaker D: Vi sort of a simple analysis, but I think I'm running out of time. So I want to mainly leave with this slide here, because I think this talk leaves a lot more questions than I've really answered. So number one, are lotteries better than auctions for avoiding builder monopolization and specifically monopoly pricing? Two, does the lottery version otherwise have the same characteristics as the auction version? Is the analysis exactly the same? What is the best allocation rule in either of these versions? And the requisite value of beta, the amount we have to burn to prevent profitable shill bidding, is quite high when n equals two in particular, and the allocation rule is sharply. And so one question is, could it be better with other allocation rules? Could we make beta smaller? Could we make beta so that it could be decreasing in n as n increases? All kinds of questions here. Are there other mechanisms? How could we do better? Are there impossibility results? So, hopefully I've motivated you to think that this problem is interesting, and I would love to talk to anyone who has any ideas.
01:22:09.180 - 01:22:50.692, Speaker F: Yeah, thanks for the great presentation. So, interestingly enough, this is very much related to some problems we've been studying at cowswap, and there's going to be a research paper soon, and I will share with you. I was wondering two things. The first one is still not fully clear to me why you need p greater than zero. So I can imagine a world where there are two bidders, and one very much likes the right shoes, the other one very much like the left shoe. Of course, as a bidder, I don't know whether my opponent will like one shoe. Therefore, there is always a probability that the most efficient things to do is to allocate each item to one of the two bidder.
01:22:50.692 - 01:23:30.900, Speaker F: And so, even if p set to zero, I still have to want to take seriously my bid for the individual item, because that might become the realized outcome. So that's question number one. Question number two, which perhaps is more broad, does this mean that each of those roll ups and l two will need to essentially get rid of their own token, since all the bidding and all the action is done, all the transactions is done at the kind of l one layer, or we have to add a layer of complexity here where these values may accrue in different tokens.
01:23:31.640 - 01:24:06.800, Speaker D: Okay, great. Yeah. So in reverse order, starting with the question of roll ups and their tokens, I think that the role of roll up tokens is sort of orthogonal from this problem. So there's no reason why roll ups would have to get rid of their tokens or not have a token, if they wanted to have a token in the first place. The token may be used for various reasons, including governance, for example. That's how it's mainly used in many roll ups today. There may be other uses as well that would still have utility.
01:24:06.800 - 01:25:40.390, Speaker D: The key point here is that we're just solving a revenue allocation problem. So the token may represent who to distribute the funds to. Because when we talk about, okay, distributing revenue to a roll up, what is the roll up? Well, the roll up is a set of token holders, most likely. And so if we were distributing some value va, as I was calling it, to roll up a, what that would mean is potentially paying out va proportionally to all the roll up token holders, or burning some number of those tokens, et cetera. Now, on the first question, we might need to have a conversation about this offline, because it sounds interesting, but when you say that you could set p is equal to zero, you mean that P was representing the probability that we selected the individual auctions instead of running the pair auctions? What? Right? Yes, but if p is equal to zero, what does that mean? That means that we're only running the auction on the pair. You're saying that instead of flipping a coin, have the two different auctions compete with each other? That would be different. So you're saying that whichever ends up generating higher revenue, you would select that.
01:25:40.390 - 01:26:28.016, Speaker D: Got it. Yeah. I think that there are potential solutions that could involve that. I think it's harder to see, without looking at a specific example, whether that would be truthful. Though. I think that the reason why I was just using the simple example of flipping a coin is that it's easy to argue that you'll get the correct valuations for the individual left and right shoe if you were doing something else that was allowing the auctions to compete with each other, for example, it's not entirely clear that that would be the outcome. Yeah, we can talk offline.
01:26:28.016 - 01:26:28.790, Speaker D: Yeah.
01:26:32.910 - 01:26:36.118, Speaker G: I have a question around the motivation for revenue sharing.
01:26:36.214 - 01:26:36.860, Speaker B: Yes.
01:26:37.570 - 01:26:44.602, Speaker G: So one possible, very simple way to do it is just to not do any sharing, just give everything to the sequencer.
01:26:44.666 - 01:26:48.330, Speaker D: Yeah, that's what Ethereum does today for applications, in fact. Right.
01:26:48.500 - 01:27:25.070, Speaker G: And that has potential benefits. Like, one is that it's very simple, of course. Another one is that the sequencer is doing something right. It's providing a service. And so it kind of makes sense to compensate them for that service. And especially if the service has network effects, the sequencer might demand to be paid. And then another aspect is that with applications becoming better and better and leaking less and less mev to the proposer, and with more and more tooling like encrypted mempools, maybe the amount of mev will tend to zero anyway.
01:27:25.070 - 01:27:33.274, Speaker G: And so maybe this is not a problem that will be meaningful in the very long term.
01:27:33.322 - 01:28:18.190, Speaker D: In other words, even if you're running on a shared sequencer and you are generating revenue just from including transactions, it's separable. So it's very easy to attribute the revenue split among the different. It becomes more complicated to split it only when you have things like meV. Yes, right. Yeah, that's potentially true. That would make this problem simpler. With regards to the first question, I think it's really up to roll ups in the, I mean, you might ask to flip the question around, why aren't applications on Ethereum demanding a share of the sequencing revenue? Why doesn't uniswap demand a share of the meV? They could, and we would have the same problem that we would need to solve.
01:28:20.610 - 01:28:35.954, Speaker G: So can you shed some more light on the necessity to randomize between the two auctions? What's the difference between this randomized auction and say, an auction that half the time runs the shoes separately, and then half the time runs the pair auction together?
01:28:36.072 - 01:28:39.006, Speaker D: I'm sorry, could you repeat the question? I didn't quite get it right.
01:28:39.048 - 01:28:51.862, Speaker G: So instead of what's the difference between tossing a coin, heads, you auction the shoes separately, tails doing that, versus on half the days I'll do the first auction and then the other.
01:28:51.996 - 01:29:15.290, Speaker D: Right. Well, the problem with if half the days I do the first auction and half the days I do the second, then as a participant, you may choose to just wait to participate in the second. So let's say you want to buy a pair of shoes. You know that on Mondays the shoes are being auctioned separately. On Tuesdays they're being auctioned as a pair. You would rather participate in the auction that's auctioning as a pair. You'd choose not to participate on Monday because you have limited funds.
01:29:15.290 - 01:29:39.080, Speaker D: So this is just an example to show that if you just alternate the auctions. Then it's not true that there's independence on behavior of agents between the two auctions. So the presence of the joint auction may affect the valuations in the first. Whereas if we collect the bids and then select randomly which one we're actually going to execute, you can show that agents will behave truthfully in each.
01:29:44.500 - 01:30:01.916, Speaker H: So I guess it was solution three where you said you run both simultaneously and choose one with. Okay, I'm sorry. So say that you run two auctions simultaneously, but you know you're going to choose the combined one and you use the individual biddings to kind of like.
01:30:01.958 - 01:30:07.776, Speaker D: Do some sort of. That's p equals zero. A little slightly different p equals zero than you were talking about, but that's p equals zero. Yeah.
01:30:07.818 - 01:30:33.336, Speaker H: And then you can somehow also encourage giving correct bids by, instead of burning a p, but distributing that apportion to people who are bidding close things. And I know people who have different mevs because they have access to liquidity and sex tax arbitrage. But I think could be somewhat best of the both worlds.
01:30:33.528 - 01:30:41.836, Speaker D: But if you set p equals zero, and maybe I just misunderstand the question, but if you set P equals zero, then why can you hope that people will bid truthfully?
01:30:42.028 - 01:30:47.748, Speaker H: Well, you can. So P equals zero in the sense that you always choose the combined, like.
01:30:47.754 - 01:30:49.232, Speaker D: If we always choose the combined auction.
01:30:49.296 - 01:31:05.080, Speaker H: And then you'd have another parameter, say 20% of the fees or whatever, distribute to people who have bid close to the median for all roll ups I to n. And that way you could somehow.
01:31:05.900 - 01:31:24.610, Speaker D: You are suggesting that we somehow incorporate the. Basically we run more of a commentary auction. Right. We run a single auction that involves bidding on individuals, individual left shoe, individual right shoe of the pair. And we use those bids to somehow determine an allocation. Like we run ECG, for example. Yeah.
01:31:24.610 - 01:32:00.990, Speaker D: Potentially a combinatorial option may suffice. It's not clear that any of the solution concepts quite solve the problem there, though. And also it could become, especially if we have n roll ups and we are not willing to run an exponential in n. Let's discuss algorithm. Yes, you can talk later. Okay, thank you.
01:32:01.520 - 01:32:11.400, Speaker A: All right, so that concludes the first session. There's a coffee break. Once again, that's downstairs, same place where you checked in. And let's reconvene here for the second session at the top of the hour. Bye.
