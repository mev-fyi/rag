00:00:00.600 - 00:00:37.004, Speaker A: Robots have become quite adept at verbal communication thanks to large language models like CheckGBT, but their nonverbal communication skills lag behind, especially facial expressions. Our goal is to improve nonverbal communication in human robot interaction. The challenge is twofold. Firstly, designing a robot that can make a wide range of facial expressions involves complex mechanical design. A second change is knowing what facial expression to generate so that they appear natural, timely and genuine. We developed the Emo. It is equipped with 26 actuators and a soft skin, allowing for versatile visual expressions.
00:00:37.004 - 00:01:14.724, Speaker A: Emote face can be easily replaced for customization and maintenance. For more lifelike indirections, we integrated high resolution cameras with each eye, enabling emote to make eye contact, which is crucial for nonverbal communication. On the software side, we developed two AI models. One predicts human facial expressions by analyzing subtle changes in their target face. The second model generates mode commands using the corresponding facial expressions. With these two models, Emo can anticipate and replicate a human smile before the human actually smiles. This ability to co express emotions in real time, we needed call expression.
00:01:14.724 - 00:01:51.380, Speaker A: Firstly, we train emo how to make visual expressions. We put the Emo in front of the camera, let it do random movements after a few hours. Imo notes the relationship between their facial expressions and the more commands. This supervised learning framework is like how humans practice facial expressions by looking in the mirror. Secondly, we provide videos of human facial expressions for Imo to learn frame by frame. I think that predicting human facial expressions represents a big step forward in the field of human robot induction. Traditional robots have not been designed to consider humans facial expressions during interaction.
00:01:51.380 - 00:02:25.404, Speaker A: Now the robot can integrate human facial expressions as feedback. Its not just about classifying emotional categories, happy, sad, but specific visual expressions including nuanced change. Therefore, when a robot makes core expressions in real time, it not only improves the interaction quality, but also helps in building trust between humans and robots. In the future, we hope Imo can become an entity of AI or even AGI. You can do conversation and you can perceive the world through the eyes in the cameras, just like a real person.
