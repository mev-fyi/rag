00:00:03.870 - 00:00:04.420, Speaker A: You.
00:00:06.470 - 00:00:39.500, Speaker B: The next session is called l two economics medley. So essentially the goal of this session is a bit different from the other ones. We want to present a few ideas and open problems. So we spend most of the time working on the l one protocol. And already there, there is things related to l two s, in particular, data availability, which Ansgar is working a lot on. But we're also thinking about economics. And there are some initiatives too that maybe Ansgar will mention too.
00:00:39.500 - 00:01:33.722, Speaker B: So we'll give brief introduction, some mental models to think about l two economics. Then I'm going to talk about some problems around dealing with data costs for roll ups. And then Ansgar is going to cover multidimensional pricing, both for l two and for l one. Okay, so first we have the l one chain. You can think Ethereum, and essentially the roll up is this off chain system that latches onto the l one. There is some smart contracts on l one, and then there is some operations on the l two. So there is different roles, I won't go into details, but essentially there is different functions performed by entities.
00:01:33.722 - 00:02:39.314, Speaker B: Sequencer, proposer, prover. The main point is that this system has to interact with the l one, and in particular publish some data at a cost to get their security, and then user interact directly with the l two. So, similar picture, but here we are looking at the economic flows of this system. So it's a platform with a balance sheet. So users pay l two fees, and then the platform or the protocol incurs operational costs and incurs data cost for publishing to l one. So in particular, the l two fees need to cover essentially three things, the operational cost, the data cost, and then there is the congestion cost. We won't talk about mev extraction, things like that.
00:02:39.314 - 00:03:21.210, Speaker B: We'll focus on fees. And then in particular, I'm going to focus on the data cost and interaction with l one fees. And I believe Ansgar will touch on congestion cost and the way you can do better pricing at L two. So, dealing with data cost. Data costs are very important because they're a big portion of l two costs, and they impact both profitability and user experience. So one complaint is that l one fees are high. So roll ups are looking into mechanism to minimize their data costs.
00:03:21.210 - 00:04:30.246, Speaker B: So one is to strategically choose most affordable data markets, times, et cetera. The other one, which I'll talk about a little bit, is around, like sharing data, posting between different roll ups, which is an idea I've been working on earlier this year. So the other thing about l one cost is that l one fees are all over the place, right? So there is a lot of variance in this cost. So I'm going to pose more of a question around. Can there be some market sophistication and maybe some products to edge against these fluctuations? Okay, so maybe just pictorially before I try to describe what we are trying to model in terms of data sharing. So here you can see two different roll ups, l two a and l two b. They independently post data to l one and they pay data cost a and data cost b.
00:04:30.246 - 00:05:18.950, Speaker B: So what we'll explore is when two roll ups may have some incentive to actually join and essentially merge their data and post it together to the l one, this could be done like by a shared service or like by some, maybe like two roll ups of the same family. This is already built protocol. Okay, so I'll present a simple model for this. We have a short paper out, so I refer to that for details. But essentially the model is very simple. So roll ups have two type of costs. So one is the data cost, which is observable.
00:05:18.950 - 00:05:56.802, Speaker B: The other one is the delay cost. So for security, you don't want to delay the publishing of data too much. And there may be different sensitivities for different type of roll ups, but these are the two types of costs. And then data can be also posted in two different markets. So think about even the single roll up case. For now, you can use the regular market, which is what roll ups do today. They post data as call data in normal transactions or after four eight four is live.
00:05:56.802 - 00:06:44.622, Speaker B: They can use specialized data market which is in the form of blob transactions on Ethereum. So you have these blobs with fixed capacity. And essentially like roll ups can put their data into these blobs. And they pay a fixed cost per blob in respective of how much these blobs are utilized, how much of this blob is filled. So the roll up want to minimize posting costs. And there is an inherent trade off between waiting to filling the blob more efficiently if they want to post in the data market and then incur some delay cost. So we find some results first around the single roll up posting strategies.
00:06:44.622 - 00:07:52.786, Speaker B: So if a roll up is small, like the arrival rate of transaction is low enough compared to their delay cost, then they might use a regular market. This is kind of a steady state equilibrium model. Essentially this is the case when the delay cost is too high relative to the efficiency that they can gain from packing blobs. And more interesting results are around when roll up can cooperate together and join in a joint posting strategy so they could do it and essentially share the space of a blob. And we identified some condition on size of roll up and the cost structure that make this profitable. And whether roll ups adopt this or not also depends on the cost sharing scheme that is utilized. So we published the short paper in October and then in November at East Istanbul.
00:07:52.786 - 00:09:06.862, Speaker B: There were actually two Akaton projects that tried to build this already. So one is from Dapplion, who is like core developer, and the other one is someone built it on suave, actually. So this is exciting. I think it's really cool to see this already being built. I think the question is, will there be demand for this? But personally, I'm pretty interested in seeing whether the use cases are, because it's kind of a first step into creating more specialized and sophisticated markets for segmenting block space. Okay, so before leaving this to Ansgar, I want to pose a question. So when we think about this l one data cost for roll up, one question that we are often talking about is whether we'll see the adoption of block space futures, and especially for blobs and for roll up data, it seems that there could be the need.
00:09:06.862 - 00:10:42.300, Speaker B: So essentially, roll up have this commitment that once they're live, they need to buy blob space for their continued operation. The l two fees depend directly on the l one fees, which are very volatile. And yeah, they may want to buy some type of future contract to edge against local l one fee increases. And also from the user's perspective, they may want to give them more predictable l two fees, especially for roll ups that have like l three s on top of them or upchains. So this is maybe just like, I'm throwing out a bunch of questions. We don't have a specific discussion session on l two s in the afternoon, but these are like, ideas that would be interesting to discuss. So if we think about these type of future contracts, how could this look? What would be the units? Would you sell full blobs? Or would you abstract away and essentially fractionalize the blob space? Sell directly kilobytes, maybe with some type of blob merging protocol? Then how do you price these contracts? Like, is there an external market? Is there a mechanism for price discovery? I think this is, in general, it's a very interesting area, because today, essentially, you can think that ethereum is just selling block space and there is only a spot market for it.
00:10:42.300 - 00:11:50.866, Speaker B: But I don't know, if you think about AWS, they have a ton of ways in which they package their compute resources, for example, like reserve instances in the future, et cetera, et cetera. So I think there is definitely space for efficiency. And maybe this is not just a question around future contracts, but it's both like a product design problem. It's like how do we structure the current and future block space? And then the next question is how do we sell it? And then the other thing is that if you start thinking about this problem, it's immediately clear that there is going to be impact on the l one protocol in terms of censorship and congestion pricing manipulation, because we have this mechanism for congestion pricing and could be potentially manipulated. And this could be especially profitable if there is like derivatives on top. So I think with this congestion pricing, I'll stop and maybe it's a good segue into what Anskar will talk about.
00:11:51.048 - 00:12:34.746, Speaker A: Yeah. Hey, I just wanted to briefly touch on another topic, kind of closely related, that I talked about last year as well, a little bit. So just to give it a little bit of a recap, and then just really a small intuition of how this applies in the layer two context. So if you were here last year, maybe you remember this kind of picture. So the idea is basically on Ethereum right now, we basically only have a single dimensional fee market. And that just implicitly under the hood prices a lot of different resource types. And if you look at the resource types a bit more in detail, although this is kind of orthogonal to the rest of the talk, but basically you see that there's actually quite a bit of structure in those resources.
00:12:34.746 - 00:13:06.666, Speaker A: So, for example, a very kind of useful distinction to make between these different resource types is burst limits and sustained limits. So burst limits are the ones where actually you're directly limited on the throughput for each individual block, say bandwidth. Right. You just cannot have blocks that are too big, otherwise your blockchain fails. Sustained limits are limits where you can spike in usage. You're just basically limited in kind of the sustained load that you want to allow for. And actually, the nice thing is that there's quite a bit of overlap in terms of.
00:13:06.666 - 00:14:03.914, Speaker A: So, for example, bandwidth and history growth are kind of two sides of the same coin. Bandwidth consents, the kind of the data as it comes in, and history growth basically as it is stored. So in a way, if we were to go to a future of Ethereum, where you price kind of individual resources independently, you could combine those. And the nice thing that actually falls out of this is that you just immediately get some allowance for a 1559 type mechanism where basically, you have some headway in terms of allowing for increased burst because you can basically go all the way up to the bandwidth limit, but then have a lower target which corresponds to the sustained limit. So this kind of pretty closely matches to the fee market we have today. Just of course, ideally you'd start to split out dimensions. One problem that I also flagged last year already was that of course, it's really hard to go change an existing blockchain, especially with a pretty invasive change like this.
00:14:03.914 - 00:14:37.414, Speaker A: So while this might still come in the future, it's not an area of super active research, but it does inform future changes we make. And one example of that is ERP four eight four, also called protodank sharding. You might have heard of that. So I mean, you already mentioned it, right? That the blob, the blob market, the data market. And of course that basically is related to this one dimension. Although the way we are going to implement it is it only affects a new type of data we introduce. So it does not touch or kind of extract out the existing way we handle existing data.
00:14:37.414 - 00:15:06.734, Speaker A: And that is because specifically the way we want to scale blob data in the future is by treating it different from normal data. Normal data has to always be downloaded by all the nodes in the network because it is accessible during execution. This new type of data that we are going to introduce, it does not have that property. So we can in the future scale it with more efficient sampling mechanisms. And that's why we don't bundle it with the existing kind of data. But we basically treat it as a completely separate dimension. And just one intuition to have from that.
00:15:06.734 - 00:15:45.546, Speaker A: People always are a little confused with ERP four eight four. There is no scaling inherent to the eap. We don't actually increase the throughput of the network directly. All we do is we basically now separate out these two dimensions. You can think of them in this case, basically the resource a in this case would be just like all these existing types of resources, the kind of the combined l one usage, and then the resource b is just this blob data. We just intended for layer two settlement. And so right now, today in Ethereum, if you want to have more layer two settle on ethereum, that directly competes with layer one usage, and so basically has to also pay competitive prices.
00:15:45.546 - 00:16:23.798, Speaker A: And also basically the total throughput is in this sense competing. And then of course, the idea is with before we basically give those separate dimensions. So now you have the full rectangle here and you can go through these more efficient points where you can have full layer one throughput and then at the same time, full layer two settlement throughput in a non competitive way. So this is all kind of more of a recap of what we are doing, why we are structuring ERP four eight four in this two dimensional way. So basically, blob data is a new dimension. We give it its own type of gas. We call it blob gas.
00:16:23.798 - 00:16:58.750, Speaker A: And the interesting thing is, and this is a bit niche, but I still think important once we go to the layer two case in a second. If you think of the unit that is charged in blob gas is a blob. And when a transaction comes in, you immediately see how many blobs does it carry? Right? A blob transaction can come with a single blob, that's 128 kilobytes. It can come with two, three in the future, maybe 510, whatever blobs. And you can immediately just take the cost for a single blob, multiply by number of blobs. It's a very simple computation. You don't have to actually execute the transaction.
00:16:58.750 - 00:17:38.174, Speaker A: It's static per transaction. Now of course, this is just the consumption in data gas, right? Then how much the data gas costs, that's a separate question. But the consumption in data gas, basically you can compute from the outside. And also one extra thing that makes it relatively simple for us to introduce that is that Bob transactions are only intended to be sent by sophisticated actors, layer twos. So actually normal end users don't have to deal with the complexities of a two dimensional transaction type, which makes it a bit of a no brainer, very simple dimension for us to add. All of this will be slightly different now in the layer two case. So that's kind of just as a background.
00:17:38.174 - 00:18:57.046, Speaker A: Now going to the layer two, what's the situation today? Well, there's really interesting kind of aspect of the settlement dimension, right? And so if you send a transaction today on the layer two under the hood, what you have to pay for is for one, you have to pay for the layer two congestion, just as you have to pay for layer one congestion. But then there's this completely separate type of cost that you also incur, which is the layer two has to then take that and put it today in just the normal market, the call letter market in the future, maybe into the blob market, basically. And then also the ZK roll up has to run the ZK proof on the layer one. So there's basically some extra type of cost that is not at all related to the layer two congestion, right? Like the layer two could have very low congestion, but as long as the layer one right now has high pricing, basically this extra dimension still might incur quite high cost. And by the way, just maybe as a brief aside, it's not going to be relevant for the rest here. But I do think, by the way, that this is an understudied area in general, kind of making this distinction more explicitly between congestion, like the part of the cost of a roll up of blockchain that comes from congestion cost versus just actual underlying cost. So usually when people model layer one costs, for example, they often assume that any given transaction causes some cost to the network, to the nodes.
00:18:57.046 - 00:19:35.080, Speaker A: But usually that's not how we think about it. We think about it as pure congestion pricing. We think of it as like a specific amount of block space we just have for free. And then the only reason why a transaction is not free is that we have to manage the fixed supply we have in this case for the layer two. You can think of the layer two internal congestion similarly to the layer one case, but the external cost here, the settlement cost, this is a very different type of cost. And kind of making this distinction more explicitly and thinking about how these different types of costs kind of relate to each other, I think would be an interesting area of study. But of course I'm focused more on just kind of the two dimensionality here.
00:19:35.080 - 00:20:27.666, Speaker A: So how do layer twos do this today? There are different approaches, some the minority of roll ups. I know that from linear, but I think this might be one or two others literally just don't make any kind of specific 2d distinction here and just map this into one dimension. So the way linear charges is basically just like similar to in the US, the cost plus contract. So basically you just take the cost that they incur for the transaction and they add a constant, I think 30% or something on top, and they just cover their own congestion with that. Of course that's quite inefficient because it doesn't accurately kind of map into the actual congestion situation of their roll up. So most layer twos do something slightly more sophisticated and they actually have a two dimensional model already today. It's just that it's purely under the hood and not exposed to the users.
00:20:27.666 - 00:20:48.506, Speaker A: And there are two different flavors. So one of them is to say we hide the settlement dimension completely. That's for example, how the op stack handles it. So if you send a transaction on the op stack, you can actually look at it when you go into your metamask or something. You set a maximum fee for the entire transaction. I did this the other day. I said I want to pay at most two cent for my transaction.
00:20:48.506 - 00:21:23.930, Speaker A: And then metamask was like, yeah, you're fine, only two cent maximum. Then you send the transaction and you look and it charged me. Why is that? Well, they just do not at all basically account for the layer one settlement cost. They just afterwards look at how much did we actually have to pay on layer one, and then they take that directly out of your layer two account, and there is no way for you to set a limit for that. Of course, their fault proving system will ensure that they never overcharge you. They only charge you the actual cost, but you have no way of introspecting that in advance or setting limits to it. Then there's another school of thought here that, at least last time I checked, this is a couple of months ago, though.
00:21:23.930 - 00:22:28.282, Speaker A: For example, how arbitrum does it is that you actually map it into one dimension dynamically. But dynamically means that basically the amount of gas, of layer two gas that you basically deduct from the transaction for layer one settlement depends dynamically on the current price for data on layer one. And so what that means is that it could be that your layer two transaction now runs out of gas just purely for the settlement alone, right? Or it might not have enough gas left for the rest of your transaction. So it means that, in a way, the gas price, you set, a maximum gas price you're willing to pay as a user. But that's kind of now a bit of a meaningless metric, because it no longer has, like a constant mapping to the actual underlying cost of the transaction. And also the gas limit now is overloaded because it no longer just refers to the running cost of the transaction. While this kind of avoids some of these unattended consequences for the other approach, both of them, of course, are not ideal because they try to pit a 2d mechanism into an existing 1d mechanism.
00:22:28.282 - 00:23:37.730, Speaker A: Excuse me, none of the layer twos right now do what? I think the principled approach here would be to actually expose the 2d aspect and nature of the sea market to the users. So, of course, then the question becomes, at least as an era of exploration, would it not maybe make sense to do this, to actually kind of make it a proper two dimensional fee market? And if so, how could we do this? And then kind of analogous to how we did it on layer one, we can think again as like, settlement cost would be its explicit separate dimension. The nice thing here again, and that's why I kind of went into it in a bit more detail. In the four to four transaction type, the settlement cost is also static per transaction. What does it mean, static? Well, of course the actual price you pay does vary because it depends on layer one cost, but as measured in whatever, if you call it settlement gas or something, if you see a transaction, you can immediately compute how much space it will use up on layer one. And then maybe you add some extra kind of overhead for whatever other settlement operation you do on layer one. But you can compute that from the outside.
00:23:37.730 - 00:24:29.746, Speaker A: And of course there's a little caveat here, because the way roll ups actually handle data is that they end up batching multiple transactions and compressing them. And so the actual kind of compressed size might vary a little bit, but actually layer twos already basically just use heuristics for this. So in a way you could still have simple logic that you don't need to account for this dimension dynamically during execution. The difference though to the way we do two dimensional on layer one, of course, would be that now this transaction type would actually be designed to be used by everyday users. And so that does mean that we now have to think about how do we expose this to the users, if at all. Maybe it's just some advanced setting somewhere hidden in your metamask. You don't usually see it, but if you're a power user, you can click and you can separately set like maximum settlement layer cost and maximum layer two congestion cost and all these kind of things.
00:24:29.746 - 00:25:06.320, Speaker A: But of course it's a bit of ux trade off. So ideally you kind of hide it as much as possible. It also means it needs active wallet support. So the way you'd roll this out is you'd just basically add it as an optional transaction type first. So it would be still relatively simple to add, but then over time, basically you shift mod option over to this new transaction type and then phase out the one dimensional transaction type. And Davido already alluded to this a little bit, so I just wanted to give a bit of a shout out. We recently, a colleague of mine and I, Carl and I, we started this IAP process.
00:25:06.320 - 00:25:52.558, Speaker A: The idea, we call it the roll call, the roll up call, it's analogous to the awkwardfs call that we have, the weekly call we have on Ethereum. It's going to be a monthly call. We already are on our second iteration, actually next week, so if any layer two people here haven't heard of it yet, feel free to come join us. And then a transaction type like this, basically taking the four to four transaction type, adopting it for layer twos would be a really good candidate for the standardization process. Because of course, if a single layer two does it, no, none of the wallets will bother to support it. But if all the layer twos buy into a two dimensional approach like this, of course, now all of a sudden the case for wallet support would be really strong. And then this is the happy case of multidimensional pricing, because again, it's pretty simple static cost.
00:25:52.558 - 00:26:24.774, Speaker A: So I want to kind of, in the end, just briefly mention, and that's kind of going to just touch on it, not really go into detail. But there is one more dimension in the layer two case that's actually a bit more tricky, and that's the prover dimension. That of course is only relevant for ZK roll ups. So the way ZK roll ups work is that they, internally, on the kind of congestion side, incur two very different types of cost. The first is just like sequencing. The sequencer just basically has to just do the same thing that any normal layer one node does. So basically just execute transactions, combine them into a block.
00:26:24.774 - 00:27:21.610, Speaker A: So in that case, that is a very similar cost to the one we know, and that's of course the same one that also optimistic roll ups have. But then the ZK rollups have this completely different type of cost, the proving cost, and it has a very different structure because of course that depends on the details of the underlying ZK proving system, and that can be very, very different in terms of the cost that it incurs from normal sequencing. So for example, famously like the catchack hash function that we use in Ethereum, very simple to use to compute on a cpu, very expensive to compute in ZK. And the problem is right now, because we have the EVM equivalent, so all the layer twos, most of the layer twos, all the EVM layer twos, target EVM equivalents. They want to deviate from layer one as little as possible. Otherwise all the tooling, all the users basically out of the box will have to kind of get used to a different system. And so they all just basically copy the pricing model of layer one.
00:27:21.610 - 00:28:04.426, Speaker A: And that means that the gas cost is a very good fit for the sequencing part of their congestion structure, but a very bad fit for the proving cost. And so that means that today what ZK rollups do is basically they all just have relatively sophisticated DOS protection logic, where they manually keep track of how many catches have we done in our block. And if they get close to the limit they stop accepting transactions that consume too many catches. But they can only figure this out by running them. So then they have to sometimes run them and can't include them, so they can't charge them. So it's a really bad situation. I mean, in practice maybe it's good enough, but definitely something where a separate proving cost dimension would of course be the principal thing to do.
00:28:04.426 - 00:28:57.270, Speaker A: The problem here though is if we compare it to kind of to the other case here, that the prover cost is not statically computable. If I give you a transaction, you can't just tell me, hey, the prover cost would be 5000 gas or 5 million gas or something. You actually have to run the transaction, right? And so as you run the transaction, you kind of have to dynamically account for the proving gas cost. And then also the question becomes how do you expose this internally? Usually in the EVM we have mechanisms that if you do subcalls, you can only send a limited amount of gas with that subcall to be able to ensure that once the call returns, you still have enough gas left to do whatever accounting you need to do. This now would need a completely separate mechanism. So it would require a pretty fundamental rework of the EVM. And then of course now we'd have, instead of just exposing a 2d transaction model, fee model, we'd have to now expose a 3d fee model to users.
00:28:57.270 - 00:29:30.626, Speaker A: But especially because of this dynamic aspect of this dynamic dimension and the questions around how to expose that in the EVM, it's a really complex dimension to add and definitely requires further work, but it is as a target still, I think very important to work towards and. Yeah, and that's really all from my side, kind of just a brief overview and thanks it.
00:29:30.728 - 00:29:47.846, Speaker C: So there's a possible future state where the vast majority of roll up users have somebody else paying the gas fee for whatever they're doing, which then means that the fee pairs are actually going to be highly sophisticated. If that becomes true, how does that change your thinking or framework?
00:29:48.038 - 00:30:25.526, Speaker A: Yeah, I think that's a good point. I personally would predict that to probably happen at least to some extent. I think we are already seeing a trend where we just have more and more kind of separation of individual layers and sophistication among these layers. So you could imagine like a very simple confuser to some intermediary that handles your transaction and then a very much more sophisticated boundary between that layer and then the actual chain. Yeah. In that scenario, I think of course it would be easier to add further dimensions. I think 2d we can probably do even with end users being exposed to it.
00:30:25.526 - 00:30:35.770, Speaker A: But once we ever got serious about having all the individual dimensions split out and like a five 6710 dimensional fee market, at that point, probably that's something you cannot show an end user.
00:30:36.910 - 00:31:00.946, Speaker D: So two questions. One is, how about the other flavors of roll ups, like based roll ups, would the cost change in any way? And also another question is, how about the fraud proofs? We talked about ZK proofs, but at some point we're going to have to occasionally or sometimes submit the fraud proofs. What happens to them?
00:31:01.128 - 00:31:54.482, Speaker A: Right, good question. So I think on the base roll up, I mean, obviously, Justin, if you have more sophisticated thoughts there, but I think in principle, it just is more an opinionated approach of who is your sequencer. And so in that way, it doesn't actually touch how your fee market would be structured. You would then just basically have, I mean, of course the idea is that the more complex your fee market, the more sophisticated the sequence and prover as an entity has to be. And so presumably that would mean. But we already see that today anyway, right? With proposal separation and these things, even in a based context, you would actually have some specialized entity doing that for you. And so in that sense, the fee market becoming a bit more sophisticated, I don't think changes much on the ZK versus fault proving side.
00:31:54.482 - 00:32:23.760, Speaker A: That's actually interesting, because the difference is that ZK, the proving itself, you just have to do for every single block, right? And so every transaction incurs this cost, whereas the fault proof, that's something that either just never happens or very occasionally happens. And then you can have special logic kind of charging separately for this. It's not a cost that every single transaction causes and incurs, so you don't have to account for it in your fee market.
00:32:27.010 - 00:32:49.350, Speaker E: Yeah, I had a question about the, so I don't understand. If you say, oh, the gas fee might be wrong for the prover costs, but then adding another dimension to incur this seems like a strictly worse approach than simply actually adopting the gas schedule. That makes things more complicated.
00:32:49.850 - 00:32:53.734, Speaker A: Well, that'd be the right approach, right?
00:32:53.852 - 00:33:06.970, Speaker E: Because it's like, okay, the way I think about it, yes, it might be wrong, but it's strictly within a constant bound of the gas feed. So it still seems better to reflect in that rather than adding another dimension.
00:33:07.310 - 00:34:02.522, Speaker A: Right? So there's a bit of attention here. What you could do is you can basically just make your one dimensional kind of layer two condition cost, just reflect the proverb cost, and then basically say, hey, sequence of cost is just strictly less than that. So it's just covered by this automatically. I have talked to some ZK rollups, I think specifically for example starkware, that were very opinionated that they see some operations where actually the sequencer cost is higher than the prover cost. So for example some sort of lookups where the sequencer has to do the expensive lookup and then passes the result to the prover, who then can just do a very efficient proof there. So they really strongly wanted to have these two separate dimensions to be able to price this markecurately. But I agree with you, an alternative approach could be to just price for the prover or the max.
00:34:02.586 - 00:34:03.200, Speaker B: Good.
00:34:05.650 - 00:34:39.290, Speaker A: Right? Although it depends whether you ever have congestion at the sequencer level. If of course you only ever have this congestion at the prover level, then it makes sense. But if you basically kind of want to efficiently use both all the available throughput of the sequencer and of the prover, then you do need the separate dimensions. Otherwise you're always just binding on one of the two sides if you want to basically be binding on both sides at the same time. But yeah, that would really have to talk with actual ZK roll ups to see. But it's a good point. Infant.
00:34:40.030 - 00:34:41.546, Speaker D: Great, thank you very much.
00:34:41.648 - 00:34:42.870, Speaker A: Going to move on to the next presentation.
