00:00:04.920 - 00:00:24.045, Speaker A: So welcome all. Welcome to this distinguished lecture series in AI. I'm Vishal Mishra, I'm the Vice Dean for Computing and AI in Columbia Engineering. This is the second lecture in our series. We seem to have a reasonably full house, people are still streaming in. So before we start, I'd like to invite Dean Shifu Chang to give some opening remarks.
00:00:24.745 - 00:00:47.181, Speaker B: All right. Good morning everyone. Welcome to our. It's really exciting. This is the first time I see we have an overflow of space used today. Really so exciting about a topic and speaker. I want to thank Michelle and the team for organizing the AI lecture series this semester and throughout the year.
00:00:47.181 - 00:01:28.803, Speaker B: I want to thank our president Katrina Armstrong come to support our event today. And as Michelle mentioned, this is the second in our AI lecture series across the school and is associated with the university initiative in AI. That's one of the priority that President Armstrong is leading us for the school university wide efforts here. Last month we launched this new AI lecture series starting with our faculty member Pierre Jentien to talk about how AI can have an impact in different disciplines. Last month we launched AI and climate projection. Today we're so excited Dr. Yang Likang is here to share his vision, his insight.
00:01:28.803 - 00:01:57.127, Speaker B: I'm very excited you have seen his title. I have seen Yang talking many times in cvpr, icml, Learning Representation. But today's topic is particularly intriguing and his presence as you can see from audience today we have to open up overflow space. The event once is announced, three minutes sold out. You are the lucky ones. Okay. And the lecture series one of the effort around AI University.
00:01:57.127 - 00:02:40.593, Speaker B: We are pursuing advances in fundamental area which is covered by today's lecture. We also pursuing the impact in different disciplines in collaboration among all the 17 schools at Columbia. Climate, business, finance, policy, journalism, you name it. We work with industry communities, create centers on AI and finance, AI on climate, AI on sports, AI and policy. That's our effort. Today we create a new course on AI in context to teach AI in the context of humanity, in literature, in music and philosophy. Today's topic how could machine reach human level intelligence? Just by reading the title make me so intrigued, so excited.
00:02:40.593 - 00:02:48.889, Speaker B: So without further ado, let me invite Vishal, our Vice dean of AI and computing to have introduction of our speaker Yang Likang today.
00:02:49.017 - 00:02:49.965, Speaker C: Michelle.
00:02:52.545 - 00:03:36.583, Speaker A: Thanks Shifu. So Yan of course needs no introduction, but just to embarrass him, I'll give a brief introduction of Jan. Now this may come as a surprise to a lot of you, but it's true and he'll never guess it from his accent, Jan is actually French. He got his PhD from the Sorbonne in 1987. And in his PhD thesis, he proposed an early form of back propagation. Now, back propagation is the way all neural networks are trained now. And it sort of started from his PhD thesis.
00:03:36.583 - 00:04:40.635, Speaker A: He joined A.D. bell Labs in 1988. Before that, he spent a few months or a year with Jeff Hinton working as a postdoc I. There was an alarm, okay, and he joined AT&T Bell Labs in 1988. Next year, he sort of stunned the world with this handwriting recognition system. And you see a video of that. This was absolutely incredible at that time.
00:04:40.635 - 00:05:35.225, Speaker A: And there you see Yan looking slightly different. After that came a long AI and neural nets winter. Jan joined AT&T Research in 1996, but he never gave up. He continued working on convolutional neural networks, CNNs, which were what he used for the handwriting recognition system. Around 2012, the deep learning revolution happened. And now CNNs are everywhere. Whether his friend Elon Musk cars, some people got what I meant.
00:05:35.225 - 00:06:08.899, Speaker A: Or Google Photos, Everywhere. Everyone uses CNNs. In 2013, Jan joined Meta AI as a director of their AI lab. And now he is the chief scientist. In 2018, he also won the Turing Award along with Jeff Hinton and Yoshua Bengio for his work in deep learning and artificial intelligence. In fact, Jeff was here yesterday. He was on campus and he was walking around and people were asking him for selfies, so he wanted to be here.
00:06:08.899 - 00:06:40.585, Speaker A: Unfortunately, something urgent came up, so he couldn't be here. So as I mentioned, Yan won the turing award in 2013-2018. And this is a Turing Award for computer science, not for physics or chemistry, which are also known as Nobel prizes these days. This was the original one, and he won the award in 2018. And you know, he's also big into the selfie game. I took a selfie with him that day. And now with that, I'll invite Jan to tell us about human level intelligence.
00:06:48.525 - 00:07:39.985, Speaker D: Thank you very much for this amazing introduction. A real pleasure to be here. A good thing to come give a talk here is that I didn't have to fly. Although if you ask people from downtown, they rarely go above 23rd Street. So, yeah, I mean, I worked really hard to lose my French accent in the last four decades or so, three and a half decades. But I just recently learned that if you speak English with a French accent, people give you 20 IQ point additional. So perhaps I should speak with a very strong French accent and perhaps a Pierre intelligent.
00:07:39.985 - 00:08:17.769, Speaker D: Ok, what should Appear intelligent is machines. And they do appear intelligent. A lot of people give them iq, whatever that means, that is actually much higher than they deserve. We are nowhere near being able to reach human intelligence or human level intelligence with machines, what some people call AGI, Artificial General Intelligence. I hate that term. I've been trying to fight against it. And the reason is not that it's impossible for a machine to reach human intelligence.
00:08:17.769 - 00:08:47.958, Speaker D: Of course it's possible. There's no question. At some point we'll have machines that are as intelligent as humans in all the domains where humans are intelligent. There's no question that we'll go beyond this. But it's just because human intelligence is not general at all. We are very specialized animals. We have a hard time imagining that we are specialized because all the problems that we can fathom or imagine are problems that we can fathom or imagine.
00:08:47.958 - 00:09:27.075, Speaker D: But there is many, many more problems that we can't even imagine in our wildest dream. And so it makes us appear generally intelligent. We're not. We're specialized. So we should lose that term, Artificial General intelligence. I prefer the term human level intelligence or a code name that we've adopted Inside Meta is an acronym, ami, which means Advanced Machine intelligence, which is kind of a little more loose. Also, we pronounce it ami, which in French means friend.
00:09:27.075 - 00:10:29.551, Speaker D: Makes sense. Okay, so how could we ever reach human level intelligence with machines. Machines that can learn, of course, can remember, understand the physical world, have common sense, can plan, can reason, are behaving properly, not being unruly, dangerous, et cetera. And the first question we should ask ourselves is why would we want to build this? So obviously there is a big scientific question of what is intelligence? And the best way to validate any theory we have about intelligence is to build an artifact that actually implements it. That's a very engineering approach to science, if you want. But there is another good reason, and the other good reason is that we need human level intelligence to amplify human intelligence. There's going to be a future in which we run around with AI assistant with us at all times.
00:10:29.551 - 00:11:04.283, Speaker D: So we can ask any question from them, they can answer any question we have. They can help us in our daily lives, they can solve problems for us. And this will amplify human intelligence, perhaps in the way that the printing press has amplified human intelligence to the 15th century. So we need this for humanity. In fact, I'm wearing a pair of smart glasses right now, and I can ask it questions. It goes through Meta AI, which is the product version of Llama 3 that many of you have heard of. I can ask it various things.
00:11:04.283 - 00:11:36.733, Speaker D: So let me ask you something. I'm not going to use a microphone. Hey, meta, take a picture. You see that little light flash? Okay, you're all on picture. You'll be on social networks soon. So I could ask it more complex questions, obviously. And this thing can also recognize through the camera.
00:11:36.733 - 00:12:20.285, Speaker D: So you can ask it, what am I looking at? What is a species of plant? You can look at a menu in Japanese and will translate it for you. So you know, this kind of assistance are coming. They're still pretty stupid, but they're already useful. But there is a future maybe, you know, 10, 20 years from now where they will be really smart and they will assist us in our daily lives. So we need those systems to have human level intelligence because that's the best way for them to not be frustrating for us to interact with. Okay, so on the one hand, there is the really interesting scientific question of what is intelligence. In the middle there is the technological challenge of building intelligent machines.
00:12:20.285 - 00:13:11.405, Speaker D: And then at the other end, it's actually useful. It would actually be useful for people and for humanity more generally. So all of the conditions, and then the more important condition is that there are people with a lot of resources willing to actually invest to for this to be true, like beta. So the characteristics that we want of those machines is that they need to be able to understand the physical world. Current AI systems do not understand the physical world. They don't understand the physical world nearly as well as your house cat. And so I've been saying, and of course newspapers like this kind of title, Yannukun says AI is stupider than a cat.
00:13:11.405 - 00:13:58.513, Speaker D: And it's true, actually, we need AI systems that have persistent memory. We need them to be able to plan complex action sequences, which current systems are completely incapable of doing. We need them to be able to reason and we need them to be controllable and safe. So basically, and by design, not by fine tuning like it's done at the moment. And that requires essentially new principles that are different from what current AI systems really are based on. So current systems, most of them anyway, perform inference by propagating signals through a bunch of layers of a neural net. Okay, and I'm a big fan of that, obviously, but it's very limited.
00:13:58.513 - 00:14:49.395, Speaker D: There's only a small number of input output functions that can be efficiently represented by feedforward propagation through a bunch of layers in a neural net. There's a much more general approach to inference, which is not just running feedforward to a bunch of layers, but is based on optimization. So basically there's an observation. You give the system a proposal for an output and the system tells you to what extent the output is compatible with the observation. Okay, so give you a picture of an elephant. I put the representation of the label elephant or the text, and the system tells you, yeah, those two things are compatible. The label elephant is a good label for that image.
00:14:49.395 - 00:15:38.813, Speaker D: If you put the picture of a table, it says, no, it's incompatible. Right? So if you have a system that basically measures the compatibility between an input and an output, then through optimization and search, you can find an output that is most compatible with the input. This is intrinsically more powerful as an inference mechanism than just running feedforward through a bunch of layers, because basically any computational problem can be reduced to an optimization problem. So that's the very basic principle on which future AI systems should be built. Not propagating through a bunch of layers, but optimizing the answer so that it's most compatible with the input. And of course this will involve deep learning, system back propagation, all that stuff. But the inference mechanism is very different.
00:15:38.813 - 00:16:24.805, Speaker D: Now, this is not a new idea by all means. This type of inference is what is very standard in probabilistic inference. For example, if you have a graphical model Bayesian network, you know the value of certain variables, you can infer the value of the other variables by minimizing a negative log likelihood or something like that, or with some energy function. And so it's a very standard thing to do. There's nothing innovative about this, but people have forgotten about the fact that this is really much more powerful than feedforward propagation. The framework that I like to explain this is called energy based model. So basically, the function that measures the compatibility between x and why input and output is an energy function that takes low values when input and output are compatible and larger values when they're not.
00:16:24.805 - 00:17:31.177, Speaker D: So the type of inference that can take place to find the output could be a number of different things. If the output or the representation of the output is continuous, and if the modules that we're talking about, the objectives, the all the modules inside of the system are differentiable, you can use gradient based optimization to find the best one good answer. But you can imagine that the output is discrete combinatorial, and then you have to use other types of combinatorial optimization algorithms to figure out the best output. And if that's the case, then you're talking to the wrong lacun. Because my brother is actually he Works at Google. Nobody's perfect, but he works on, he's an expert in combinatorial optimization. So this type of inference gives AI systems kind of zero shot learning ability.
00:17:31.177 - 00:18:16.729, Speaker D: What does that mean? It means you give them a problem and if they can, if you can formulate this problem in terms of an optimization problem, then you get a solution to that problem without the system having to learn anything. Right? That's zero shot you are given. Many of you are students. You're given a new mathematics problem, something you can think about it and perhaps solve it without learning anything new. That's called zero shot skill. And in humans, psychologists also call this system too. So basically you devote your entire attention and consciousness to solving a problem that you concentrate on and you think about it and it might take a long time to solve that problem.
00:18:16.729 - 00:18:47.415, Speaker D: That's system two. System one is when you act reactively, you don't have to think about it. It's become kind of subconscious, automatic. So if you're an experienced driver and you drive on the highway, you don't have to think about it. It's going to become automatic. You can hold a conversation with someone and everything. If you're a beginner, though it's your first time driving a car, you pay close attention, you're using your system to your sort of the entire capacity of your mind.
00:18:47.415 - 00:19:44.809, Speaker D: So that's why we need to adopt this model and this framework of energy based model is sort of the way to understand this at the theoretical level. I'm not going to do a lot of theory here, this is a very diverse audience. But the basic idea is that if you have two variables, X and Y here there are scalars, but you can imagine that they are high dimensional inputs. The energy function is some sort of landscape where pairs of X and Y that are compatible have low energy and then low altitude if you want, and then pairs of X and Y that are not compatible have higher energy. And so the goal of learning now is to shape this energy surface in such a way that it gives low energy to things you observe. Training data, pairs of XY that you observe and then higher energy to everything else. The first part is super easy because we know how to do gradient descent.
00:19:44.809 - 00:20:39.975, Speaker D: So you give a pair of XY that you know are compatible and you tweak the system so that the scalar output, the energy, scalar energy output that it produces goes down. You can tweak the parameters inside your big neural net so that the output goes down easy. The difficulty is how to make sure that the energy is Higher outside of the training sample. The training samples in this diagram are represented by the black dots. And at some level, a lot of literature in machine learning is devoted to that problem. It's not formulated in the way I just did, but it's improbably framework. For example, this problem of making sure the energy of things outside the training data is high is a major issue and it usually encounters intractable mathematical problems.
00:20:39.975 - 00:21:28.965, Speaker D: Okay, let me skip this for now. Okay, so now the whole craze of AI over the last couple of years, three years, let's say, has been around LLMs, large language models. And large language models should be really called autoregressive large language models. So what they do is they're trained on lots of text and they're basically trained to produce the next word, to predict the next word from a sequence of words that precede it. And that's all they've been trained to do. And once the system has been trained, you can of course show it a piece of text and then ask it to predict the next word. And then you inject that next word into the input and ask it to predict the second next word, shift that into the input third word, et cetera.
00:21:28.965 - 00:22:22.891, Speaker D: Right, so that's auto regressive prediction. It's not a new concept that's been around for before I was born, so not recent, but it's system one. It's feedforward propagation through a bunch of layers, right? There is a fixed amount of computation devoted to computing every new token. So if you want a system to spend more resources producing an answer, a system of this type, you basically have to artificially make it produce more tokens, which seems kind of a hack. That's called chain of thought. There's various techniques to do approximate planning or reasoning using this. You basically have the system produce lots and lots of candidate outputs by kind of changing the noise in the way it produces the sequences.
00:22:22.891 - 00:23:05.661, Speaker D: And then within the list of outputs that it produces, you search for a good one, essentially. So there's a little bit of search there, a little bit of optimization, but it's kind of a hack. Okay? So I don't believe those methods will ever lead to true intelligent behavior. And in fact, cognitive scientists agree. Cognitive scientists have been looking at LLMs with a very critical eye and saying that this is not real intelligence. This is nothing like what we observe in people. Similarly, people coming from the non machine learning based AI community, people like Subaru Kambanpati from Arizona State, have been saying, you know, LLMs really cannot plan.
00:23:05.661 - 00:24:31.103, Speaker D: So Rao has a whole bunch of papers, and you could basically talk about the titles of those papers as LLMs can't plan. LLMs, TL can't plan. LLMs really, really can't plan. And even LLMs that claim to be able to plan can't actually plan. So, you know, we have a big problem there that the people who claim that somehow we're going to take the current paradigm, make it bigger, spend trillions on data centers and collect every piece of data in the world and train LLMs, and they're going to reach human neural intelligence, that's completely false in my opinion. Okay, I might be wrong, but in my opinion, that's completely hopeless. So the big question is, what is not hopeless? All right, so if we agree to this basic principle of inference to optimization, how can we so instantiate this in a real intelligence system? And basically doing a little bit of introspection, when we think the way we think is generally independent of the language that we might be able to express this thought in? I'm thinking about saying things here, and it's independent of whether I'm going to give this.
00:24:31.103 - 00:25:12.559, Speaker D: I'm giving this talk in English or French. So there is a thought that is independent of language. And LLMs don't have this capacity, really. And when we think, we have a mental model of the situation that we think of, okay, we're planning a sequence of actions. We have a mental model that allows us to predict what the consequences of our actions are going to be, so that if we set a goal for ourselves, we can figure out a sequence of actions that will satisfy this goal. So instantiation of the model I talked about earlier is one like this, where you observe the world through a perception module. Think of it as a big neural net.
00:25:12.559 - 00:26:02.017, Speaker D: It gives you some idea of the current state of the world. Now, of course, the current state of the world is whatever you can perceive. But your idea of the state of the world also contains stuff that you perceived in the past, stuff that you know, facts that you know about the world. So if I take this bottle of water and I move it from this side to that side of the lectern, your model of the world hasn't changed much, okay? Most of your ideas about the state of the world hasn't changed. What has changed is the content of this lecture and the position of that box, okay? But other than that, not much. So the idea that somehow a perception gives you a complete picture of the state of the world is false. You need to combine this with a memory.
00:26:02.017 - 00:27:01.475, Speaker D: So that's this memory module here, okay? So combine your current perception with the content of your memory, and that gives you an idea of the current state of the world. Now what you're going to do is feed this to a world model, and you're going to hear that phrase many times in the rest of the talk. And the role of this world model is to predict what the outcome of a sequence of actions is going to be. This could be actions that you are planning to take, or this could be, or the agent is planning to take, or actions that someone else may be taking, or some events that may be occurring. Okay? So predicting the outcome of a sequence of actions is what allows us to reason and plan. So you can probably tell that if I take this water bottle and I put it on its head and I lift my finger, you can have some pretty good idea of what's going to happen. It's probably going to fall, right? It's either going to fall on this side or that side.
00:27:01.475 - 00:27:34.183, Speaker D: You may not be able to predict this because I'm balancing it, but it's going to fall on one side or the other. So to some extent, at an abstract level, you can say it's going to fall. I can't tell you exactly in which position in which direction, but I can tell you it's going to fall. You have an intuitive physics model which is in fact, very sophisticated, even though the situation is incredibly simple. So that allows us to plan. This model of the world is what allows us to plan. So then we can have a system like this that has a task objective.
00:27:34.183 - 00:28:30.445, Speaker D: It sets itself an objective for itself, or you set an objective that measures to what extent a task has been accomplished, whether the resulting state of the world matches some condition. You might also have a number of guardrail objectives, things that make sure that whatever actions the agent takes, nobody's gonna get hurt. For example, so those square boxes are cost functions, right? They have an implicit scalar output. And the overall energy of the system is just the sum of the scalar outputs of all the square boxes. The red, the red square boxes, the other modules there, the one with kind of a round shape, are deterministic functions, neural nets, let's say. And the round shapes are variables, okay? The action sequence is a latent variable. It's not observed.
00:28:30.445 - 00:29:01.697, Speaker D: We're going to compute it by optimization. So we're going to try to find a sequence of actions that minimize the sum of the task objective and the guardrail objectives. And that's going to be the output of the system. Okay? And again, that's intrinsically more powerful than just running through a bunch of feed forward layers. So that's kind of the basic architecture. We can specialize this architecture further. For a sequence of actions, I might need to use my world model multiple times.
00:29:01.697 - 00:29:50.985, Speaker D: Okay, so if I move that model from here to here, and then from here to here, that's a sequence of two actions. I don't need to have a separate world model for those two actions. It's the same model that is just applied twice. So that's what's represented here, where action one and action two go into the same world model and it computes kind of the resulting state. And planning a sequence of actions to optimize a cost function according to a world model that you run multiple times is a completely standard method in optimal control called model predictive control. It's been around with us since the early 60s, so it's as old as me. And this is what the entire optimal control community uses to do motion planning.
00:29:50.985 - 00:30:09.805, Speaker D: Robotics uses motion planning. NASA uses motion planning to plan the trajectory of rockets to rendezvous with the space station. It's this type of model. The difference here is that the world model is going to be learned. It's going to be trained. It's not going to be written by hand with a bunch of equations. It's going to be trained from data.
00:30:09.805 - 00:30:51.751, Speaker D: And of course, the question is, how do we do this? I'll come to this in a second. Now, the sad thing about the world is two things. First thing is you cannot run the world faster than real time. And that's a limitation, but we have to deal with that. And the second one is that the world is not deterministic. Or if it is deterministic, as some physicists tell us it is, it's not entirely predictable because we don't have a full observation of the state of the world. So the way you model non deterministic functions out of deterministic functions is that you feed them extra inputs that are latent variables.
00:30:51.751 - 00:31:28.615, Speaker D: So those are variables whose value you don't know. And you can make them swipe through a bunch of to a set, or you can sample them from distributions. And for each value of the latent variable, you get a different prediction from your world model. Okay, So a distribution over the latent variable implies a distribution over the output of the world model. That's the way to handle uncertainty. And of course, you have to plan in the presence of uncertainty. So you want to make sure that your plan will succeed regardless of what the values of the latent variable Will be.
00:31:28.615 - 00:32:10.625, Speaker D: But in fact, humans and animals don't do planning this way. We do hierarchical planning. So hierarchical planning means that we have multiple levels of abstraction for representing the state of the world. We don't represent the world always with the same level of abstraction. Let me take a concrete example here. So let's say I'm sitting in my office in NYU and I want to go to Paris. At a very high abstract level, I can predict that if I decide right now to be in Paris tomorrow morning, I can go to the airport tonight and catch a plane to Paris and fly overnight.
00:32:10.625 - 00:32:39.445, Speaker D: That's a plan. It's a very high level plan. I can't predict all the details of what's going to happen. But at a high level, I know that I need to go to the airport and then catch a plane. Now I have a sub goal. How do I go to the airport? Well, I need to go down on the street and hail a taxi because we're in New York. How do I go down in the street? Well, I need to go to the elevator, push the button, and then walk out the door.
00:32:39.445 - 00:33:23.719, Speaker D: How do I go to the elevator? I need to stand up from my chair, pick up my bag, open the door, close the door, walk to the elevator, avoid all the obstacles that I perceive, push the button. How do I stand up from my chair? So there is a level below which language is insufficient to express what we need to do. You cannot explain someone how you stand up from a chair. You kind of have to know this in your muscle. You need to understand the physical world to be able to do this. So that's the other limitation of LLMs. Their level of abstraction is high because they manipulate language, but they're not grounded on reality.
00:33:23.719 - 00:34:03.053, Speaker D: They have no idea what the physical world is like. And that drives them to make really stupid mistakes and appear very, very stupid in many situations. So we need systems that really kind of go down all the way down to the level. And this is what your husk can do and LLMs cannot do. Which is why I'm saying your housecats is smarter than the smartest LLMs. Of course, housecats don't have nearly as much abstract knowledge stored in their memory as an LLM. But they're really smart in their understanding of the world and their ability to plan.
00:34:03.053 - 00:34:45.821, Speaker D: And they can plan hierarchically as well. So what we need there is world models that are at multiple levels of abstraction. And how to train this is not completely obvious. Okay, so this whole idea, this whole kind of spiel leads to a view of AI that are called Objective Driven AI Systems. It's a recent name. I wrote a vision paper two and a half years ago that I put online at this URL in open review. It's not on archive because I welcome comments so that I can update this paper and it's the groundwork for the talk I'm giving at the moment.
00:34:45.821 - 00:35:40.165, Speaker D: But in the last two and a half years we made progress towards that plan. So I'm going to give you some experimental results and things we built. So the architecture I'm proposing in that paper is a so called cognitive architecture that has the components I just expressed things like a perception module that estimates the state of the world, a memory that you can use, a world model which is kind of a centerpiece a little bit. It costs a bunch of cost modules that are either defining tasks or guardrails. And then an actor and what the actor does is that basically finding, doing this optimization procedure of finding the best sequence of actions to satisfy the objectives. There's this mysterious configurator module at the top which I'm not going to explain, but basically its role would be to set the goal for the current situation. Okay, okay.
00:35:40.165 - 00:36:19.065, Speaker D: So perhaps with an architecture of this type we will have systems that understand the physical world, et cetera, but we have to and have system 2 ability of kind of reasoning. But then how can we learn those water models from sensory inputs? That's really kind of the trick. And the answer to this is self supervised learning. So self supervised learning is something that has been extremely successful in the context of natural language understanding over the last few years. Basically is completely dominating nlp. Every NLP system, LLM et cetera, are trained with self supervised learning. What does that mean? It means that there is no difference between inputs and outputs.
00:36:19.065 - 00:37:08.153, Speaker D: Basically you take a big input, you corrupt it in some way and you train some gigantic neural net to restore the full input if you want. But you know it's not going to be sufficient. We're still, you know, we're missing. Another piece of evidence that we're missing something big about intelligence is that although we have LLMs that can pass the bar exam or some high school exams, maybe not calculus one, I don't know. We still do not have domestic robots. They can accomplish tasks that a 10 year old can learn in one shot or zero shot, the first time you ask a 10 year old clear the dinner table and fill up the dishwasher, they're able to do it. Okay? They don't need to learn.
00:37:08.153 - 00:37:32.155, Speaker D: They can just plan. Any 17 year old can learn to drive a car in about 20 hours of practice. We still do not have level five autonomous start driving cars. We have level two, we have level three. So they're partially autonomous. We have some level five in limited areas, but they are very instrumented and they cheat. They have a map of the entire environment.
00:37:32.155 - 00:38:09.627, Speaker D: So if you think about the Waymo cars, that's what they are. And they certainly didn't need only 20 hours of practice to learn to drive. So that's what we're missing, something big. And that's really a new version of the Moravec paradox, that things that are easy for humans are difficult for AI and vice versa. And we've tended to neglect the complexity of dealing with the real world, like perception and action motor control. Perhaps the reason for this resides in this really simple calculation. A typical LLM of today is trained on 20 trillion tokens.
00:38:09.627 - 00:38:38.525, Speaker D: 2:10 to the 13. That corresponds to a little less than 20 trillion words. Because the token is a subword unit. Each token usually is represented by three bytes or something like that. So that is a volume of training data of 610 to the 13 bytes. That would take a few hundred thousand years for any of us to read through that material. It's basically the entire text available publicly on the Internet.
00:38:38.525 - 00:38:54.165, Speaker D: Now a human child, a four year old, has been awake a total of 16,000 hours. That's what developmental psychologists tell me, which by the way, is not a lot of data. That's 30 minutes of YouTube uploads.
00:38:56.785 - 00:38:57.145, Speaker C: And.
00:38:57.185 - 00:39:52.765, Speaker D: I don't know how much Instagram I should. We have 2 million optical nerve fibers, optic nerve fibers going to our brain through our eyes. The amount of information getting to the eyes is enormous because we have 100 million photo sensors or something like that, but it's being reduced to squeeze down to the optical nerve before it gets to the brain. And that's about 2 million nerve fibers, each carrying a little less than 1 byte per second, a few bits per second. Okay, so the volume of data there is about 10 to the 14 bytes, maybe a little less. It's the same order of magnitude as the bigger set at M. In four years a child has seen more data about the real world than the biggest LLM trained on the entirety of all the publicly available texts on the Internet that would take any of us hundreds of millennia to read through.
00:39:52.765 - 00:40:30.729, Speaker D: So that tells you we're never going to reach human level intelligence by training on text. It's just not happening. We need systems to really understand the world through high bandwidth input like vision or Touch. Okay. Blind people can get smart because they have other senses. And in fact, you know, if you look at how long it takes for children, infants to learn basic concepts about the real world, it takes several months. So a child will learn the difference between animate and inanimate objects within the first three months of life.
00:40:30.729 - 00:41:11.739, Speaker D: Opening their eyes. Object permanence appears really early, maybe around two months. Notions of solidity, rigidity and stability and support, that's in the first six months. So this idea that this is not going to be stable is going to fall. And then notions of intuitive physics like gravity, inertia, conservation of momentum, this kind of stuff that we have an intuitive level that any animal has two that only pops up around nine months in baby humans, much earlier in baby goats and other animals. So it takes a long time. And in the first four months, most of that is through observation.
00:41:11.739 - 00:41:56.791, Speaker D: There's not much interaction. Babies can hardly affect the world in the first four months of life. They do afterwards. If you put an eight month old baby on a high chair with a bunch of toys, the first thing they'll do is throw the toys on the ground, because that's how they do the experiment about gravity. Does it apply to this new thing I'm seeing on my chair? Okay, so there is a very natural idea which is to transpose the stuff that has worked for text to video. Can we just train a generative model to learn to predict video? And then that system will just understand how the world works because it's going to be able to predict what happens in the video. And it's been a bit of my obsession in terms of research for the last at least 15 years, if not more.
00:41:56.791 - 00:42:41.879, Speaker D: Okay, so this predates LLMs and everything. This idea that you can learn by prediction is a very old concept in neuroscience, but it's something I've really been sort of working on with my students, collaborators for many years. And the idea, of course, is to use a generative model, right? Give to a system a piece of video and then train it to predict what's going to happen next in the video. Just the same way that we train LLMs to predict what happens next in the text. Perhaps if you want the system to be kind of a world model, you can feed this world model with an action variable, the A variable here, which in this case would simply be masking, essentially.
00:42:41.927 - 00:42:42.063, Speaker C: Right?
00:42:42.079 - 00:43:15.479, Speaker D: So take a video, mask a piece of it, let's say the second half of it, run it through some big neural net and train it to predict the second half of the full Video we've tried for, yeah, a good part of 15 years. It doesn't work. It doesn't work because there are many, many things that can happen in a video. And a system of this type basically would just predict one thing. And so one way to deal with this problem of predicting one thing. So it's going to predict one thing. So the best thing it can predict is the average of all the possible plausible things that may happen.
00:43:15.479 - 00:43:49.157, Speaker D: And you see an example here, that's an early paper in video prediction. Trying to predict what's going to happen is this really short six frame video with this little girl. The first four frames are observed, the last two are predicted. And what you see is a blurry mess because the system really cannot predict what's going to happen. So we predict the average and you see this at the bottom as well. If I can play that video again, this is sort of a top down view of a highway and the green things are like cars. And the second column are predictions made by neural net trying to predict what's going to happen in that video.
00:43:49.157 - 00:44:29.931, Speaker D: And you see those blurry kind of extending cars because it really cannot predict what's happening. So the columns on the right are a different model that has a latent variable which is designed to kind of capture the variability between the potential prediction and those predictions are not blurry. So we thought that we were, you know, we had a good solution to that problem five years ago with latent variables. But it turns out to not work in for real video. It works for simple videos like this one, but it doesn't for like real world. So we can't train this thing on video. So the solution to that problem is interesting is to abandon the whole idea of generative models.
00:44:29.931 - 00:45:28.433, Speaker D: Okay? Everybody is talking about generative model like it's the, you know, the new messiah. And what I'm telling you today is forget about generating models, okay? The solution to that problem, we think is what we call joint embedding architectures, or more precisely, joint embedding predictive architectures. And this is really the way to build a world model. Okay, so what is this consistent? It's you take that video, you corrupt it, you mask a piece of it, for example, okay? And you run it through a big neural net. But what the big neural net is trained to do is not predict all the pixels in the video. It's trained to predict an abstract representation of the future of that video. Okay? So you take the original video, you take the masked one, you run them through encoders, now you have abstract representations of the full video and the corrupted one, and you train a predictor to predict the representation of the full video.
00:45:28.433 - 00:45:53.045, Speaker D: Found a representation of the corrupted one. Okay. This is called jepa. That means Joint Embedding Predictive Architecture. There's a bunch of papers from the last few years that my collaborators and I have published on this idea. And it solves the problem of having to predict all kinds of details that you really cannot predict. So if I were to take a video of this crowd, in fact, I can take a video of this crowd.
00:45:53.045 - 00:46:19.889, Speaker D: Okay, now I'm taking a video of you guys, okay? And I slowly turn my head towards the right. I'm going to shut down the video now. Certainly a prediction system can predict. This is a room, it's a conference room. There's people sitting everywhere. It may not be able to predict that all the chairs are full. It certainly cannot predict what every single one of you looks like.
00:46:19.889 - 00:46:55.315, Speaker D: There's absolutely no way. It cannot predict what the texture on the wall is going to be or even the color of the side. So there are things that are just completely unpredictable. You don't have the information to do it. And if you train a system to predict all those details, it's going to spend all of its resources predicting irrelevant details. So what a jetpack does when you train it, and I'm going to tell you how you train this, is that it finds a trade off between extracting as much information as possible from the input, but only extracting things that it can predict. And there is an issue with those kinds of architectures.
00:46:55.315 - 00:47:34.205, Speaker D: Here is a contrast between the generative architecture that tried to reproduce Y directly and the joint embedding architecture, which only tries to do prediction in representation space. On the right. There's a problem with the joint embedding architecture, and this is why we've only been working on this in recent years. And it's the fact that if you just train the parameters of those neural nets to minimize the prediction error, it collapses. It basically ignores the inputs X and Y. It makes prediction for SX and sy, the two representations that are constant. And other prediction problem is trivial.
00:47:34.205 - 00:48:15.947, Speaker D: Okay? And that's not a good thing. So that's an example of this energy based framework that I was describing earlier. It gives zero energy to every pair of xy, essentially. But what you want is zero energy for the pairs of xy, you train it on, but higher energy for things that you don't train it on. And that's the hard part. So next I'm going to explain how you make that possible, how you make sure that the pairs of XY that are not compatible have a higher energy. There's variations of those architectures, some of which can be, have latent variables or be action conditioned.
00:48:15.947 - 00:48:41.131, Speaker D: If you want the predictor to be a world model. And there's been papers on this for many years now. The earliest joint embedding architecture actually is from the early 90s. It's a paper of mine about Siamese networks. But we're going to have to train those generic architectures. So how do we do this? So remember this picture. We want to give low energy to stuff that are compatible.
00:48:41.131 - 00:49:08.235, Speaker D: Things that we observe. Training sets, training samples, X and Y, higher energy to everything else. So there are two sets of methods, contrastive methods and what I call regularized methods. So contrastive method consists in basically generating contrastive pairs of X and Y that are not in the training set. So pick an X and pick another Y that's not compatible with it. And that gives you one of those green dots that you see flashing. Okay.
00:49:08.235 - 00:49:49.265, Speaker D: And your loss function is going to consist in pushing down on the energy of the blue dots, which are the training samples, and then pushing up on the energy of the green dots, which are those contrastive samples. Okay, this is a good idea. And there's a bunch of algorithms that people have used to train this. Some of them, for example, for joint embedding between images and text are things like clip from OpenAI. They use contrastive methods. Seem clear from a team at Google that includes Jeff Hinton and then Siamese nets back from the 90s that I used to advocate. The issue with contrasting methods is that the intrinsic dimension of the embedding that they produce is usually fairly low.
00:49:49.265 - 00:50:37.365, Speaker D: And so the representations that are learned by it are kind of degenerate a little bit. So I prefer the regularized method. What is the idea behind regularized method? The idea is that you minimize the volume of space that can take low energy. So you have some sort of regularizer term in your loss function. And that term basically measures the volume of stuff that has low energy and you try to minimize it. So what that means is that whenever you push down the energy of one region of that space, the rest has to go up because there's only a limited amount of low energy volume to go around. And that sounds a little abstract and mysterious, but in practice the way you do this is there's like a handful of methods to do this, which I'm going to explain in a second.
00:50:37.365 - 00:51:39.573, Speaker D: Before that, I'm going to Tell you how you test how well those systems work, right? So in the context of image recognition, you give two images that you know are the same image either. So you take an image and you corrupt it, or you transform it in some way, you change the scale, you rotate it, you change the colors a little bit, maybe you mask parts of it. And then you train an encoder on a predictor so that the predictor predicts the representation of the full image from the representation of the corrupted one. And then once the system is trained, you chop off the predictor, you use the encoder as input to a classifier, and you train a supervised classifier to do things like object recognition or something of that type. So that's a way of measuring the quality of the features that have been learned by the system. And there's been a number of papers on this. And what has been transpiring is that those methods work really well to train a system to extract generic features from images to joint embedding architectures.
00:51:39.573 - 00:52:42.495, Speaker D: There's been a lot of work also on generative architectures like auto encoders, variational autoencoders, VQAEs, masked autoencoder, denoising autoencoders, all kinds of techniques of this type that basically you give a corrupted version of an image and then you train the system to recover the full image at a pixel level. Those methods do not work nearly as well as the joint embedding methods. And we discovered this five or six years ago, not just us, but there was an accumulating amount of evidence showing that joint embedding was really superior to reconstruction based systems, so to generative architectures. And at the time the methods for training were only contrastive. But now we've found some other techniques, and one technique in particular, or one set of techniques that attempt to maximize some measure of information content coming out of the encoder. Okay, so one of the criteria used for training is this minus I, a measure of information content. Since we minimize cost function, there is a minus sign in front, so you maximize information content.
00:52:42.495 - 00:53:51.871, Speaker D: How do we do this? So one simple trick that we've used is something called variance covariance regularization, or in the case where you don't have a predictor, it's vic, reg, variance, invariance, covariance, regularization. And there the ideas you take the representation coming out of the encoder and you say, first of all, you should not collapse to a fixed set of values. So the variance of each variable coming out of the encoder should be at Least one, let's say, okay, now the system can still cheat and not produce very informative outputs by basically producing the same variable or very correlated variable for all the dimensions of the output representation. So another criterion tries to decorrelate those variables. And in fact we use a trick that we expand the dimension, we take the representation, run it through a neural net that expands the dimension, and then decorrelate in that space, and that has the effect of actually making the original variable more independent of each other, not just correlated. So it's a bit of a hack because what we're trying to do here is maximizing information content. And what we should have to be able to do this is a lower bound on information content.
00:53:51.871 - 00:54:23.505, Speaker D: But what I'm describing here is an upper bound on information content. So we're maximizing an upper bound and then we cross our fingers that the actual information content will follow. Okay, and it works. So that's one set of techniques. I'm going to skip the theory. There is another set of method called distillations, and those have proved to be extremely efficient. And it's another hack.
00:54:23.505 - 00:55:08.465, Speaker D: And we only have partial, at least in my opinion, partial theoretical understanding of why it works, but it does work. In there, we share the weights between the two encoders with a technique called exponential moving average. So one encoder has the weights that are basically a temporal average of the weights of the other one, for mysterious reasons. And we train the whole thing, but we don't backpropagate gradient to the one that gets this moving average that gets the full input, and somehow this does not collapse. And it works really well. It's called a distillation method. There's various versions of it, Simcam, Byol from DeepMind, Denov2 from my colleagues in Paris at Meta, I Jepa and V Jepa from the People at Meta who work with me.
00:55:08.465 - 00:55:52.083, Speaker D: And this works amazingly well. It works so well. In fact, the Deno V2 version works incredibly well. It's a generic feature extractor for images. If you have some random computer vision problem and no one has trained a system for that, just download Denov 2. It will extract features from your images and then train a very simple classifier head on top of it with just a few examples, and it will likely solve your vision problem. An example of this is, I'm not going to bore you with tables of results, but example of this is a collaborator at Meta, Camille, who got satellite imaging images of the entire world in various frequency bands, and she also got LIDAR data.
00:55:52.083 - 00:56:34.675, Speaker D: So the LIDAR data gives you for a little piece of the world. LIDAR data gives you the height of the canopy, of the vegetation. She took the DENO features, applied them to the entire world, and then used a trained classifier that was trained on the LIDAR data, on a small amount of data, but applied it to the entire world. And now what she has is an estimate of the height of the canopy for the entire Earth. What that allows to compute is an estimate of the amount of carbon captured in vegetation, which is a very interesting piece of data for climate change. So that's an example. There's other examples in medical imaging, in biological imaging, where DENO has been used with some success.
00:56:34.675 - 00:57:06.789, Speaker D: But this distillation method called IJEPA that I briefly described earlier works extremely well to learn visual features. Again, I'm not going to bore you with details. It's really much better than the methods that are based on reconstruction. And of course, the next thing we did was try to apply this to video. Can we apply this to video? So it turns out if you train a system of this type to make temporal prediction in video, it doesn't work very well. You have to make it do spatial prediction, which is very strange. And there the features that are learned are really great.
00:57:06.789 - 00:58:03.801, Speaker D: You get good performance for that system when you use the representation to classify actions in, in videos and things of that type. We even have tests now that the paper is being completed that show that those systems have some level of common sense and physical intuition. If you show them videos that are impossible because, for example, an object spontaneously disappears or something like that, they say, whoa, something strange happened, their prediction error goes up. And so those systems really are able to learn sort of basic concepts about the world. But then the last thing I want to say is systems of this type that are capable of that basically we can use to train a world model and we can use those world models for planning. So this is new. I haven't presented this yet the paper has been submitted, but this is the first time I talked publicly in English about it.
00:58:03.801 - 00:58:56.865, Speaker D: I gave a talk in French last week about it in Geneva, but. But you get the preview. So this is work by a student, PhD student at NYU Gaoyu, who is co advised by myself and Laurel Pinto. And she did a lot of this work while she was an intern at Meta and Hengai Pan, who's also a student. And the basic architecture here is that we use the features from Denov 2, okay, pre trained and we train a world model on top of it, which is action conditioned. So basically we take a picture of the world or the environment, whatever it is, and then feed an action that we're going to take in that environment. And then observe the result in the environment in terms of denot features.
00:58:56.865 - 00:59:45.291, Speaker D: And then train the predictor to predict the representation after the action as a function of the input, the previous state and the action. Okay, so the predictor function takes the previous state and the action and predicts the next state, essentially. And then once we have that system, we can do this optimization procedure I was telling you about to plan a sequence of actions to arrive at a particular result. And the result is simply a Euclidean distance between a predicted state, end state, and a target state. The way we compute the target state is that we show an image to the encoder and we tell it, this representation is your target representation. Take a sequence of actions so that the predicted state matches that state. So we've tried this on several tasks.
00:59:45.291 - 01:00:16.781, Speaker D: So one of them is just moving a dot to a simple maze. Another one is moving a little. Let me repeat this video. Moving a little T object by pushing on it in various places so that it's in a particular position. That's called a push T problem. And then other task of navigating to an environment, going through a door in a wall, and then pushing on deformable objects so they adopt a particular shape. I'll show you a more impressive example in this one.
01:00:16.781 - 01:00:47.215, Speaker D: So the task, we can collect artificial data because those are virtual environments that we can simulate. And then we experimented with various systems that have been proposed in the past to solve that problem. Dreamer v3 is probably one of the most advanced one. From DeepMind, from Daniel R. Hefner at DeepMind. And what you see here is visualization through a decoder of the predicted state for a sequence of action. So at the top is a ground truth.
01:00:47.215 - 01:01:14.905, Speaker D: You execute a sequence of actions and see the result in the simulator. And then each row is the result of a prediction by one of those models. And what you see is some predictions become blurry, some predictions become kind of weird. Ours is pretty good. Iris is pretty good. Dreamer V3, not so great. This is the most interesting task.
01:01:14.905 - 01:01:48.853, Speaker D: It's called the granular environment. And it's basically a bunch of blue chips on the table. And an action is a motion by a robot arm which goes down on the table, moves by some delta X, delta Y and then lifts. Okay, that's an action. It's four numbers, xy, xy where you touch the table. Delta is delta Y lift. Okay? And the question is so you can train a world model by just putting a bunch of chips in random position and then taking a random action and then observing the results.
01:01:48.853 - 01:02:34.655, Speaker D: And you train the predictor this way. And once the predictor is trained, so those are results of various techniques of planning. So you can use the world model for planning a sequence of actions to arrive at a particular goal. So this is for a point maze pushed wall. But you might want to look at the other one, the granular. So this is what's called a chamfered distance between the, the end state in the image space of all the grains, if you want, and the target measured through chamfer distance. And what you see is our method, which is the blue one, has much, much lower final error than the other methods that we compared it with.
01:02:34.655 - 01:03:01.575, Speaker D: Dreamer V3 and TDMPC2. And TDMPC2 is a method that actually requires needs to be task specific. So it's not as general as world model. So here's a little demo of the system in action for the various tasks. Let me play this again. Look at the push T. Okay? So you see the dot moving in discrete steps.
01:03:01.575 - 01:03:28.507, Speaker D: Because for every tick of the simulation there is the same action is repeated five times. So the actions are only produced like every five time steps. But it gets to the the target. The target is represented on the right and it actually kind of presents. So this is for the granular in particular. So the target is represented at the right. And let me play this again.
01:03:28.507 - 01:04:03.005, Speaker D: We start from a random configuration of the chips and the system kind of pushes the chips using those actions. You don't see the actions, but you only see the result by pushing them so that they look like a square. Now what's interesting about this is that it's completely open loop. So the system basically looks at the initial condition, imagines a sequence of actions, and then executes those actions blindly. And what you see here is a result of executing those actions open loop, closing your eyes. Okay, it's pretty cool. All right, coming to the end now.
01:04:03.005 - 01:04:45.385, Speaker D: So I have five recommendations, okay? Abandoned generative models, all right, in favor of those jepa, abandoned probabilistic models, in favor of those energy based models. So something I haven't said is that in this context you can't really do probabilistic modeling. It's intractable. Abandoned contrastive methods in favor of those regularized methods, and of course abandoned reinforcement learning, but that I've been seeing for 10 years. And so if, if you're interested in human level AI, don't work on LLMs, you're a grad student, you're studying a PhD in AI. Do not work on LLMs. It's not interesting.
01:04:45.385 - 01:05:27.819, Speaker D: I mean, first of all, it's not that interesting because it's not going to be the next revolution in AI. It's not going to help systems understand the physical world and everything. But it's also a very dangerous thing to do because there is enormous teams in industry with billions of dollars of resources working on this. There's nothing you can bring to the table, absolutely nothing. Okay, so do not work on LLMs unless you want to get a job working on LLMs. But the lifetime of this is going to be three years, three, five years from now. My prediction is no one in their right mind would use LLMs in the form that they exist today.
01:05:27.819 - 01:06:13.355, Speaker D: I mean, they would be used as a component of a bigger system, but the main architecture would be different. Okay, There's a lot of problems to solve with this, which I kind of swipe, you know, swept under the rug and I'm not going to go through the laundry list. But we don't know how to do hierarchical planning, for example. So here is a good PhD topic, if you're interested in this. Just try to do, you know, crack the nut of hierarchical planning. There's all kinds of foundation theoretical issues with what I talked about here in energy based models and things like this, how to design objectives for SSL so that those systems are driven to learn the right thing. I've only talked about information maximization, but there is all kinds of other things.
01:06:13.355 - 01:06:53.035, Speaker D: There's a little bit of RL you might need to do for adjusting the world model in real time. But then if we succeed in this program, which may take the better part of the next decade, we might have virtual assistant that has human level AI. What I think though is that those platforms need to be open source. And so this is the political part of the talk, which is going to be very short. You know, we need. Those platforms are Incredibly, you know, LLMs or future AI systems are incredibly expensive to train the basic foundation models. So only a few companies in the world can do it.
01:06:53.035 - 01:08:01.715, Speaker D: And the problem that we're facing now is that the publicly available data on the Internet is not what we want because it's mostly English. I mean, there is other languages obviously, but for various reasons, regulatory reasons, all kinds of problems, you do not have access to all the data in the world. Of every language in the world, there is 4,000 languages or something like that that people use, all the cultures, all the value systems, all the centers of interest, you just don't have all the data available. So the future is one in which those systems would not be trained by a single company. They will be trained in a distributed manner so that you all have big data centers in various parts of the world. They have access to local data, but they all contribute to training a large model that will be worldwide and will eventually constitute the repository of all human knowledge. This is a very lofty goal to try to attain, right? Having a system that basically constitutes the repository of all human knowledge.
01:08:01.715 - 01:09:04.225, Speaker D: But it's a system you can talk to, you can ask questions to, it can serve as a tutor, as a professor, maybe put a lot of us here at our job, you know, it's a thing that we should really work towards. It will amplify human intelligence, you know, improve rational thought perhaps, but it needs to be diverse also. You don't want just, you know, access to just two or three models coming from a handful of companies on the west coast of the us that's completely unacceptable to a lot of governments in the world, democratic governments, right? You need a diversity of AI assistants for the same reason. You need a diversity of newspapers, magazines and the press. You need free press with diversity. And we need free AI with diversity as well. So people in AI, some of them are worried about the dangers of making AI technology available to everyone.
01:09:04.225 - 01:09:53.375, Speaker D: I think the benefits far outweigh the dangers and the risks. In fact, I think the main risk of AI in the future will happen if AI is controlled by a small number of commercial companies that don't reveal how their AI systems work. I think that's very dangerous. So attempts to minimize the risk of AI by basically making open source AI illegal, I think it completely misdirected and it will actually reach the opposite result of the intended one. It will make AI less safe. So open research, open source AI must not be regulated out of existence. A lot of politicians need to understand this.
01:09:53.375 - 01:10:51.205, Speaker D: There's an alliance of various companies that are really kind of subscribed to this model. Meta, IBM, Intel, Sony, a lot of people in academia, a lot of startups, venture capitalists, et cetera, and then a few companies who are kind of advocating for the opposite that will remain nameless. So perhaps if we do it right, we'll have systems that will amplify human intelligence. As I was saying at the beginning of the talk, and this may bring about a new renaissance for humanity similar to what happened with the printing press in the 15th century. And on this cosmic conclusion, I will thank you very much. And by the way, these are pictures I took from my backyard in New Jersey.
01:10:53.755 - 01:11:21.065, Speaker A: Thank you, Jan. So Jan will take a few questions now. And for people who are leaving, please leave from the Broadway entrance. Do not leave from the campus entrance. But yeah, questions, please line up on the mics. If you have questions, start with a question.
01:11:28.685 - 01:11:31.705, Speaker C: No sound. Yeah, it works.
01:11:39.045 - 01:11:48.661, Speaker D: Hi Ayaan, thank you for coming so much. I wanted to ask for 3D vision models, what do you see business applications.
01:11:48.733 - 01:11:50.505, Speaker A: In the next seven, eight years?
01:11:53.405 - 01:12:01.309, Speaker D: Yeah, I haven't talked about 3D. I mean, some of my colleagues think there is something very special about 3D.
01:12:01.397 - 01:12:29.603, Speaker C: I don't necessarily think that's the case. I mean, we're hoping that the next generation of these V JEPA models will basically understand the fact that the world is three dimensional and there are objects in front of others and things like that. Now there are applications for which you need 3D inference and reconstruction in 3D if you want to have virtual objects in virtual environments and things like this. But frankly, I'm not a specialist at this. I think there are specialists of that question here at Columbia.
01:12:29.659 - 01:12:34.891, Speaker D: Actually, just one more question. Do you really see that v Jepa.
01:12:34.923 - 01:12:38.635, Speaker A: Models and Dyno V2 having hierarchical planning.
01:12:38.715 - 01:12:40.655, Speaker D: Like the kind you mentioned earlier?
01:12:41.195 - 01:12:53.135, Speaker C: So it doesn't exist yet. So this is something we're working on. I hope we will get some results about this for in the next year or two, something like that.
01:12:53.515 - 01:12:54.615, Speaker D: Thank you so much.
01:12:55.395 - 01:12:56.615, Speaker A: One question here.
01:12:57.555 - 01:12:58.895, Speaker E: You talked about.
01:13:01.315 - 01:13:02.135, Speaker D: Sorry.
01:13:06.205 - 01:13:26.109, Speaker E: You talked about the benefits of AI and you think it's more beneficial than there are risks to it. But we also have exactly what you said you feared where a handful of companies on the west coast control the most advanced models. So why do you feel that the benefits outweigh the risks?
01:13:26.197 - 01:13:40.697, Speaker C: So that's not entirely true. Meta actually does not subscribe to this model. That AI should be proprietary and kept in its own hands. It releases a series of models called llama. Right. So lama 1, 2, 3, 3.1, 3.2,
01:13:40.697 - 01:14:15.855, Speaker C: which are state of the art or really close to it or better in certain measures. And this is open source. It can be used freely by a lot of people around the world. It can be fine tuned for various languages or vertical applications. And it's llama 3 has been downloaded I think 400 million times or something like this. It's just insane. And every single company I talk to has either deployed it or is about to deploy products based on llama.
01:14:15.855 - 01:14:48.145, Speaker C: There are people in Africa who are using it and training it to provide medical assistance. For example, there's people in India that Meta is collaborating with to so that future versions of LLAMA will speak all 22 official languages of India and perhaps at some point all the 1500 dialects or whatever. So, you know, I think that's the way to make AI widely accessible to everyone in the world. I mean, I'm really happy to be part of that effort. I really wouldn't like to be, you know, part of kind of a closed effort.
01:14:51.685 - 01:15:12.603, Speaker A: Hi Jan, my name is Srikant. I want to ask you, I'm curious to know what you think about the capabilities of Time Series foundation models. Because I see that Amazon, Google, Meta, everyone's trying to work in that domain. But to me, intuitively, it feels like time series predictions are a harder problem than language modeling. What are your thoughts on the capabilities limitations on this?
01:15:12.739 - 01:15:13.755, Speaker D: Yeah, okay.
01:15:13.915 - 01:15:44.445, Speaker C: I think you put your finger on an important point which I forgot to mention. The reason why language modeling works, you know why those predictive models that predict the next word, the reason why they work for natural language and they don't work for images and video, for example, is because language is discrete. So to represent an uncertainty in the prediction, when you have a discrete choice with a few thousand options is easy. You just produce a distribution, probability distribution of all the possible outcomes. Right?
01:15:44.825 - 01:15:46.233, Speaker D: And this is how LLMs work.
01:15:46.289 - 01:16:34.607, Speaker C: They are trained, they actually produce a distribution over the next token. You can't do this with continuous variables, particularly high dimensional continuous variables like video pixels. So there we're not able to represent distributions efficiently in high dimensional continuous spaces beyond simple ones like Gaussians. So my answer to this is don't do it. Do prediction in representation space and then if you need to have actual prediction of the time series, have a decoder that does that separately. But actually training a system to predict high dimensional continuous thing by regression when you have uncertainty simply doesn't work. That's the evidence we have by trying to.
01:16:34.607 - 01:16:43.607, Speaker C: There was a huge project at Meta called Video Mae. So the idea was take a video, max some parts of it and then train some gigantic neural net to predict.
01:16:43.631 - 01:16:46.155, Speaker D: The parts that are missing. It was complete failure.
01:16:46.615 - 01:17:06.965, Speaker C: We abandoned that project. We canceled it because it was going nowhere. And this was really very large scale. A lot of computing resources were devoted to this. It just didn't work. The JPAD stuff though, does work. So my hunch is that for time series there's probably a way to use similar.
01:17:06.965 - 01:17:08.241, Speaker C: Similar idea.
01:17:08.393 - 01:17:09.725, Speaker A: Right? Okay, thank you.
01:17:12.545 - 01:17:42.365, Speaker E: Talk. So my question is, I think I agree with your framework. For you have some world model, and you want to optimize via that world model and how you train the world model. But my question is, how do you get intelligence when the world model is inconsistent with the truth? So as an example, like, let's say your world model only has classical mechanics. How do you discover special relativity? Humans have somehow broken that boundary. But I don't know how you do that when your world model is only based on observed data.
01:17:43.145 - 01:18:27.293, Speaker C: Well, I mean, the type of world model we're talking about here is what I would be happy with before I retire or before my brain turns into a bitch. ML sauce is world models that are of the level of complexity of a catalyst world model, right. Of the physical world, which is pretty sophisticated, actually. I mean, they can plan really complex actions. So that's what we're talking about. Now, you put your finger on something that's really interesting, which is that. Which is a philosophical motivation behind JEPA and this idea that you need to lift the abstraction level to be able to make predictions, right? You cannot make predictions at the level of observation.
01:18:27.293 - 01:18:40.285, Speaker C: You have to find a good representation of reality within which you can make predictions. And that's the hardest problem, really, is to find that good representation space that allows you to make predictions. We do this all the time in science.
01:18:40.365 - 01:18:41.317, Speaker D: We do this all the time in.
01:18:41.341 - 01:18:54.655, Speaker C: Everyday life without realizing. But we do this all the time in science. If we didn't need to do this, we could explain human society with quantum field theory, right?
01:18:54.775 - 01:18:55.415, Speaker E: Right.
01:18:55.575 - 01:19:34.671, Speaker C: But we can't, right. Because the gap in abstraction is so large. Right. So we go from quantum field theory to particle physics and from particles to atoms and from atoms to molecules, from molecules to materials and from chemistry and blah, blah, blah, right? And we go up the chain of abstraction so that at some level we have a representation of physical objects and Newtonian mechanics and for, you know, large scale, it would be relativity, stuff like that. And then, you know, societies, human behavior, animal behavior, ecology, you know, this kind of stuff.
01:19:34.703 - 01:19:35.967, Speaker D: Right? So we have all of those levels.
01:19:35.991 - 01:20:21.745, Speaker C: Of representation for which we have the. For which the crucial insight is to actually find a representation. For example, let's take a planet. Let's take Jupiter, okay? Jupiter is an incredibly complex object. It's got, you know, complicated composition, it's got weather, it's got all kinds of gases swirling around and, you know, very complex object, right? Now, who would have thought that the only thing you need to predict the trajectory of Jupiter is 6 numbers? You need 3 positions, 3 velocities, and you can predict the trajectory of Jupiter for centuries. You know, that's a problem of learning a good representation. Right.
01:20:22.045 - 01:20:27.037, Speaker E: So is the proposal essentially to do this hierarchical planning with hierarchical world models as well?
01:20:27.061 - 01:20:28.421, Speaker C: Yeah, okay, exactly.
01:20:28.493 - 01:20:28.837, Speaker E: Awesome.
01:20:28.901 - 01:20:31.941, Speaker C: Have a system that can build multiple levels of abstractions.
01:20:32.053 - 01:20:32.765, Speaker E: Great, thanks.
01:20:32.845 - 01:20:33.981, Speaker C: Which is really the idea behind deep.
01:20:34.013 - 01:20:35.025, Speaker D: Learning, by the way.
01:20:35.925 - 01:20:39.595, Speaker A: Okay, we'll have two more questions and we'll stop. So we'll take one from there.
01:20:40.415 - 01:21:37.041, Speaker F: Yeah. Hi, my question is about the one type of generative model that you haven't covered, which is the diffusion models, which I believe are quite different from the generative models that you mentioned, because they are more implicit and they don't predict the explicit probability distribution like the LLMs or VAEs or all the other generative one that you mentioned. What are your perspective on the potential of those fusion models? And especially with. It has some attribute related to hierarchical blending, as you said, because when you use it for generating image in the first few timestamps, it actually generates very high level details. Then on the later time step, it fills in the details like those smaller details.
01:21:37.113 - 01:21:37.393, Speaker D: Yeah.
01:21:37.449 - 01:22:16.365, Speaker C: Okay. So diffusion models can be seen as generative or not. But the way to understand them, I think is the following. In a space of representation or images or whatever it is, in a high dimensional space, you have, let's say a manifold of data, let's say natural images, if you want to train an image generation system or perhaps representations that are extracted by an encoder. The type that I talked about, okay. And those basically is a subset within the full space. What a diffusion model does is that you give it a random vector in that space and it will bring you back to that manifold.
01:22:16.365 - 01:23:01.755, Speaker C: Okay. And it will do this by training a vector field so that at every location, random location in that space, there is a vector that basically takes you back to that manifold, perhaps in multiple steps. Okay. That's what it does in the end. It's trained in a particular way by reversing noisification chain, but that's what it does. Now that's actually a particular way of implementing energy based models of the types that I describe, because you could think of this manifold of data as being kind of the minimum of an energy function. And if you had an energy function and you could compute the gradient of that energy function, that gradient of the energy function will take you back to that manifold.
01:23:01.755 - 01:23:31.165, Speaker C: Okay? So that's the energy based view of inference or denoising or restoration or whatever you want. And diffusion models, basically, instead of having an energy function that you compute the gradient of, they Directly learn the vector field that basically would be the gradient of that energy function. Okay. That's the way to understand it. So it's not disconnected from what I talked about. It can be used usefully in the context of what I talked about.
01:23:31.865 - 01:23:34.205, Speaker F: And what about nature?
01:23:36.745 - 01:23:37.105, Speaker C: Good.
01:23:37.145 - 01:24:14.899, Speaker G: Yeah. My name is Leon. I really want to thank you for the talk. My question was sort of about these world models you were talking about, especially in terms of trying to get to actual like cat level or animal type intelligence. So like in terms of like the giraffes, like as soon as it's born, something is in its mind that lets it be able to run or even walk within moments. And I think part of it is because the world model it has constrains the type of actions it takes. And that kind of thing seems to be what you're almost doing with the DINO of trying to do these rule based approaches.
01:24:14.899 - 01:24:23.239, Speaker G: I'm just wondering how do these world models evolve over time? Like how much variability does it have? Because like.
01:24:23.327 - 01:24:24.407, Speaker D: Yeah, yeah, yeah.
01:24:24.431 - 01:24:29.703, Speaker C: I mean, so clearly you need the world model to be adjusted as you go.
01:24:29.799 - 01:24:30.435, Speaker D: Right?
01:24:31.215 - 01:24:50.369, Speaker C: You know, for example, you know, I could be grabbing this object and you know, I think it's full and so I apply a particular force to grab it. But then as I grab it I realize it's not that full. So you know, a slider, I can adjust my world model of that system and then adjust my actions as a function of this very quickly.
01:24:50.417 - 01:24:50.697, Speaker D: Right.
01:24:50.761 - 01:25:31.865, Speaker C: It's not learning actually. It's just a few parameter adjustments. But in other situations you need to learn, you need to adapt your war model for the situation. If you have a powerful war model, you're not going to be able to train it for all possible situations and all possible configurations of the world. And so there are parts of the state space that where your world model is going to be inaccurate. And the system, if you want the system to plan accurately, it needs to be able to detect when that happens. So basically only plan within regions of the space where the prediction of its own world model are good and then adjust its world model as it goes.
01:25:31.865 - 01:25:35.985, Speaker C: If it's not the case, that's where you need reinforcement learning, basically.
01:25:36.515 - 01:25:53.443, Speaker G: Can I just ask a clarification question is I think there's a lot of understanding of. I'm really confident in what I'm able to do. But as soon as like, let's say I throw a ball, the physics of that ball is something really unpredictable. How would you differentiate that in your world model? Out of like parameters.
01:25:53.499 - 01:25:53.787, Speaker D: Yeah, yeah.
01:25:53.811 - 01:26:06.885, Speaker C: So this is adaptation, you know, on the fly of your. Of your world model, or perhaps adjustment of a few latent variables that represent what you don't know about the world, like the wind speed and things like that. So, I mean, there's various mechanisms for this.
01:26:07.625 - 01:26:09.465, Speaker A: Okay, let's thank the speaker again.
