00:00:00.560 - 00:00:31.104, Speaker A: Robots have become quite adept at verbal communication thanks to large language models like ChatGPT. But their nonverbal communication skills lag behind especially facial expressions. Our goal is to improve nonverbal communication in human robot interaction. The change is twofold. Firstly, designing a robot that can make a wide range of facial expressions involves complex mechanical design. The second change is knowing what visual expression to generate so so that they appear natural, timely and genuine. We developed the Emo.
00:00:31.104 - 00:01:09.930, Speaker A: It is equipped with 26 actuators and a soft skin, allowing for versatile facial expressions. Imo's face can be easily replaced for customization and maintenance. For more lifelike interactions, we integrated high resolution cameras within each eye, enabling IMO to make eye contact, which is crucial for non verbal communication. On the software side, we developed two AI models. One predicts human facial expressions by analyzing subtle changes in their target face. The second model generates mode commands using the corresponding facial expressions. With these two models, EMO can anticipate and replicate a human smile before the human actually smiles.
00:01:09.930 - 00:01:45.186, Speaker A: This ability to call express emotions in real time, we need call expression. Firstly, we train EMO how to make visual expressions. We put EMO in front of the camera, let it do random movements. After a few hours, EMO notes the relationship between their facial expressions and the mode commands. This supervised learning framework is like how humans practice facial expressions by looking in the mirror. Secondly, we provide videos of human facial expressions for IMO to learn frame by frame. I think that predicting human facial expressions represents a big step forward in the field of human robot detection.
00:01:45.186 - 00:02:25.500, Speaker A: Traditionally, robots have not been designed to consider human facial expressions during interaction. Now the robot can integrate human facial expressions as feedback. It's not just about classifying emotional categories, happy, sad, but specific facial expressions, including nuanced change. Therefore, when a robot makes code expressions in real time, it not only improves the interaction quality, but also helps in building trust between humans and robots. In the future, we hope EMO can become an entity of AI or even AGI. It can do conversation and it can perceive the world through the eyes in the cameras, just like a real person.
