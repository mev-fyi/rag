00:00:11.080 - 00:00:44.147, Speaker A: I am Luis Ramirez, Founder and CEO of Malwari. Malwari Network is a deep in network that enables real time interactive mixed reality experience at scale. And we orchestrate it through a global network of localized compute nodes. Through this we enable the full pipeline end to end of end user engagement, which is 3D content that can be sourced through an agent versa.
00:00:44.331 - 00:01:19.985, Speaker B: So you've actually got almost all the buzzwords for how we envision convergence in that description, which is great because you've got agents, you've got mixed reality, you've got deep in, you've got localized compute. So you know, I guess former decentralized compute and then you had agents in there. So I know that agents or digital humans was kind of the genesis for worry. Maybe you could talk to that original vision which was several years ago first.
00:01:20.805 - 00:02:37.493, Speaker A: Sure, yeah. Back in 2018, KDDI, which is one of the biggest telco worldwide, came to us with the following problem. They had created a 3D digital human that was fully not sentient per se at the time, was integrated in the API of Watson and was using more primitive stuff. But for 2018 was quite cutting edge. But this, obviously this is a very heavy computation on the back end. But on top of that, to make it realistic like a human that looks natural like us also that took a lot of rendering on the device and it was not possible. So and at the time they had made a contract with a company called Unreal, which is one of the first consumer grade AR glasses and they had this problem, okay, so we want to have this digital human as a customer service agent for the AR glasses, but we can't do it.
00:02:37.493 - 00:03:51.745, Speaker A: So the only way to achieve this is through streaming it. Just like as we have today, Netflix. You press play and you have a high quality movie on your phone or your TV. But for 3D content there was no such technology that was straightforward to stream 3D human into the real world. So that's how we went the rabbit hole and we built core streaming technology to make this happen. Finally, when we started gaining a lot of traction on the technology, we found another bottleneck which is okay, so how do we scale it? We use GPUs, we use different components of the current cloud, but they are not put together in a way that is suitable for this next generation of content. That's when actually I started doing some research and I found the Outlier Ventures thesis interestingly enough and I applied to Basecamp.
00:03:51.745 - 00:04:29.385, Speaker A: We luckily were accepted and I had the buy in from the full team that through Web3 and decentralization was really the way to break that bottleneck versus just waiting for the hyperscalers to solve the problem, which they haven't. Because right now the Demand for full 3D mixed reality applications is not there yet. They will jump into it when the demand is there, but if our plan is to build this from now, so when they want to jump in, it's too late.
00:04:30.405 - 00:04:52.943, Speaker B: Interesting. So maybe just kind of to clarify, the ability to stream an immersive experience, the form of an avatar, an intelligent avatar that you can kind of interact with, that couldn't be done on a local device or the average person's local device.
00:04:53.119 - 00:04:53.875, Speaker A: Correct.
00:04:55.255 - 00:05:46.315, Speaker B: And so you needed kind of this core streaming technology to not be reliant upon the kind of the viewers hardware capability. And so you kind of solve that through kind of the networking technology. This kind of then allows you to serve this kind of next generation content. And maybe we'll come back to this a little bit later that you may or may not think of in the context of the metaverse. Maybe it's the 3D web or something else, but effectively a 3D experience that is interactive, intelligent and seamless. Currently the cloud cannot make that happen. The hyperscalers haven't prioritized that, presumably because they're prioritizing other things, right?
00:05:46.695 - 00:05:48.239, Speaker A: That is correct, yes.
00:05:48.407 - 00:05:55.115, Speaker B: So maybe you kind of had this inflection point. So was it the open metaverse thesis that you read or was it the kind of convergence.
00:05:55.655 - 00:05:59.275, Speaker A: The open Metaverse thesis was the first one that I read. Yes.
00:06:00.655 - 00:06:26.805, Speaker B: So I guess the open metaverse thesis introduced you to the potential for Web3 infrastructure to kind of help solve for some of these problems. Maybe you could talk to that specifically. So how did the web3 make possible this kind of core streaming technologies?
00:06:27.145 - 00:06:45.173, Speaker A: Sure. Okay, so I have the device here. So you know. Okay, so first of all, to. And I have the other device here. To go from this device to this device. There's no processor here.
00:06:45.173 - 00:06:55.701, Speaker A: Right. So you have to plug it in into your smartphone and then the smartphone connects to the network and pulls the processing and the content.
00:06:55.813 - 00:07:04.937, Speaker B: Could you maybe just for people that can't see the video, could you maybe just explain the devices that you've got in your hand? So you've kind of got two. Two devices, right?
00:07:05.121 - 00:08:00.261, Speaker A: Sure. I have the Quest 3, which is a cutting edge device, but it's really thick in its form, it's heavy, it's probably more than half a kilo and it's tiring to wear it. It occludes you from the world. Even if it Has a mixed reality function versus this is just like your raven glasses that had a couple cameras and you just put them on and it looks like you're just wearing sunglasses. And it has a cable that connects into your smartphone. Then the smartphone take care a little bit of the processing UI and other elements and the rest just goes offloaded to an edge server, let's call it not cloud, but an edge server. And that's where the pieces of Open Metaverse made sense to us.
00:08:00.261 - 00:09:07.945, Speaker A: Because the specific requirement for our technology was to have edge rendering and streaming. For example, right now you're in the uk. If I were to use a server in the UK to stream immersive content all the way to Japan, it will have a lot of latency. This type of content requires low latency, otherwise it can create motion sickness because it's real time. It's very different to a 2D experience that is more passive. This is one of the reasons why the hyperscalers haven't jumped into this. Because the way to solve this problem is to build infrastructure locally in the cities of high concentration, which goes against the current architecture that they have, which is highly centralized and just in remote locations where the real state is cheap and they can set up a big server farms there.
00:09:07.945 - 00:09:45.329, Speaker A: So that is an investment of billions or if not trillions of dollars that needs to be done to support this and without any huge demand. CFOs will never approve this. So this is when. Yeah, okay. So we saw the example of Helium that it was a great success story. They were able to bootstrap their supply and everybod to run a hotspot very quickly. And then we also saw the example of render networking which everybody was running a GPU.
00:09:45.329 - 00:10:26.315, Speaker A: In our case we leverage GPUs, we leverage bandwidth, we leverage storage. So it's kind of like a deep in of deep ins. In that sense we are the application layer of deep in because we put together different types of infrastructure for and service. So yeah, just to make it short, the only way to achieve scalability is through the power of community. Having everybody join, live this mission, give their own resources, organize them and scale them. And yeah, blockchain and Web3 is a way to align incentives and keep everyone in check.
00:10:27.375 - 00:11:26.369, Speaker B: And it's interesting because actually there's this. So previously in the past paradigm or current paradigm that you're kind of disrupting. There's a chicken and an egg problem because hyperscalers won't make billions trillions of dollars of investment into new expensive infrastructure if there isn't demand for 3D content or even an expectation of it to be delivered. And therefore 3D content doesn't get made because it doesn't have a way to be distributed to the end user. So in effect the 3D web and metaverse can't happen. It's kind of static. Whilst this really is kind of a breakthrough technology because you're leveraging latent capability within existing infrastructure.
00:11:26.369 - 00:12:16.373, Speaker B: The investment's already been made, somebody's paid for it, it sat there, it has some downtime, or maybe not. Maybe it's been dedicated to this, bootstrapped by some kind of token incentive. But ultimately the hardware is there, it's waiting to be used to be utilized. And as a consequence 3D content can now be delivered, it can be streamed to end users, presumably, then kick starting this kind of flywheel where more content gets delivered, which creates the expectation for immersive 3D and intelligent experiences, which then triggers more investment into the hardware and infrastructure. And presumably hyperscalers might also want to commit some of their resources into these distributed networks.
00:12:16.429 - 00:12:34.653, Speaker A: Yeah, eventually they will. And the truth of the matter is that right now we're very soon in this, at the end of the fall we will announce that we're doing a node sale, which means we're going to bootstrap the network at a higher scale.
00:12:34.789 - 00:12:38.665, Speaker B: So how are you bootstrapping the supply side initially?
00:12:39.285 - 00:13:46.105, Speaker A: Okay, so we've seen that the node sale mechanism is a very fair way actually to get everybody involved and bootstrap a network that is aligned towards the same goal and with the right incentives. So yeah, we will very soon launch a node sale. And so far because of the traction that we've been having already on the demand side, we have institutional buy in from telcos in Asia, Europe, actually also the, the us but not only that, like we also have companies, and this one I can say we have companies like Samsung that actually joined our latest investment round because they fully believe in art thesis and they are actually launching the competitor of the Apple Vision Pro and they think that our way of solving this bottleneck and creating the, the flywheel is the way for them to have a competitive advantage versus Apple.
00:13:46.525 - 00:14:25.095, Speaker B: Interesting. So I mean, the telco one's interesting because of course they've got a vested interest in having this kind of content, like streaming content that requires this kind of bandwidth is directly in their interests. Right. Because that's kind of how they make their money. One, they want to kind of sell hardware or resell hardware and then they want to then sell bandwidth to kind of consume that content so they can act as a distribution channel to tens, hundreds of millions of users for this new kind of 3D experience. Streaming experience.
00:14:25.175 - 00:15:04.821, Speaker A: Right, right. They are the last mile and they are the distribution channel. Telcos for the longest have been wanting to become instead of a service company like a tech COD is the term called a tech company and have a programmable network. But it hasn't been easy for different reasons. But the bottom line is that 5G is a failure. You can do the same things in an LTE connection or with your WI FI connection to what you can do with 5G. The same example, watch Netflix.
00:15:04.821 - 00:15:57.265, Speaker A: But this new type of content does require more advanced and programmable networks. And that's the use cases they're trying to monetize that on demand. If for example, at some point in time this end user really needs a slice of the network that requires priority on the latency or priority on the bandwidth, we have the right to call that API. Or if we go into the agents, I do believe that actually Mawari network will have an agent that will be constantly optimizing the network and orchestrating this arbitrage of resources within the deepin network. And that's completely as you mentioned, an agent that will never face a human is just arbitraging with the APIs.
00:15:57.765 - 00:16:10.665, Speaker B: Yeah, that's interesting. And I'd imagine there's a whole load of RWA well DeFi that you can build on top of that. Right. Because then you made up with a whole futures market that will be kind of pre purchased.
00:16:11.125 - 00:16:42.051, Speaker A: Yes. Put on. So this is my long term vision. Okay. So once malware network we've been compared to the Akamai of mixed reality or xr which Akamai is the biggest content delivery network today. They still own 30% of the current Internet. So yeah, once we become that Akamai of the metaverse, we will have very predictable demand just like oil and any other community.
00:16:42.051 - 00:16:49.455, Speaker A: And that comes down to full defi and futures market that will be thriving in my opinion.
00:16:50.315 - 00:17:13.455, Speaker B: And I think this really highlights this idea that when we're talking about tokens in this context versus say Bitcoin, we are really talking about digital commodities. These are clearly commodities because effectively they are ways of fueling powering these future value chains. These kind of digital virtual value chains. Right?
00:17:14.675 - 00:18:06.025, Speaker A: Totally. As I mentioned, since this is highly dynamic, define makes a lot of sense because when the end user at some point in time they want to have the content served, the state of the network is very different to for example the next day because the usage is always viable. Orchestrating these APIs and also arbitraging and shorting and all those defi things that you can do even by the second now with the ability of blockchain, that's something that it will really be like a fish in the water, per se, for this new ecosystem.
00:18:07.325 - 00:19:19.635, Speaker B: So maybe let's talk about the kind of possibilities when you have this edge capability to kind of stream things to any device. Ultimately, what does that make possible? Like when we imagine the Metaverse, a lot of people think of it in the context of virtual worlds, which of course is one aspect like a form of escapism. But my personal feeling is it's more like the augmentation of reality is going to be perhaps the more daily and more pervasive. Could you maybe talk about the possibility of the possibilities around augmented reality when you have this kind of edge capability? And how quickly might we see that happen? What are the limitations or bottlenecks? Or how do you see that flywheel taking effect? And how do time being able to give us the vision and then tell us how we get there?
00:19:19.675 - 00:20:08.737, Speaker A: Basically, sure. Okay, so basically, just starting from vision with mawari, the name actually in Japanese means your surroundings or look around. So our thesis is that today we have the framed media which is within the boundary of a smartphone or the boundary of a computer monitor. And that boundary just exactly comes straight into your eyes. And you get inside the content, which is a spectrum. Like you see a completely entirely virtual world, but also it's just a completely synthetic reality that is mixed with your daily life. And that's our vision.
00:20:08.737 - 00:21:08.285, Speaker A: That mixed reality will be part of our daily lives. I would say that we are helping to accelerate this. We think this will happen at a mainstream level by 2030 or so. But the inflection point in the Flywheel effect is around 2027, because that's when Meta and Apple somehow, through unofficial sources, have their ender roadmap to release the next generation of devices that are much more consumer friendly. So our mission is, from 2024 all the way to 2027, build that flywheel effect, that community and their infrastructure to make that happen. Now, how to make that happen? I will give some of the examples that are already working for us. But before also is the thesis.
00:21:08.285 - 00:22:17.619, Speaker A: So think about video games per person. So our first interaction with video games, or at least my generation, was at the arcades and it was very big machines, clunky machines that you had to go pay and you had a limited time. But because of that limited time and Scarcity. It started becoming addictive and popular until finally the companies like Atari, Nintendo figured out, okay, so we need to make it small enough so it fits in everybody's home. And then, you know the rest it evolved into, okay, now online gaming and now we have esports and a full subconscious culture around video games. So for mixed reality we see exactly the same that 2024 to 2027, the main catalyzer is location based experiences. And because people do not have the devices at home, so you need to bring them to the experiences and kind of like evangelize them.
00:22:17.619 - 00:23:05.785, Speaker A: And part of our thesis as well is if this first time experience is good enough and it provides value to you, you will come back as a user. Which is this is what we see that has been in XR and the Metaverse a big issue. That because of the technical limitations, the first point of entry is somewhat like, oh yeah, this is kind of like nice, but it's very early stage technology. The graphics are not good enough, et cetera, et cetera. So what Maui is doing within this test is to solve this. So we are working in two verticals. One is entertainment, which is music and art.
00:23:05.785 - 00:24:02.983, Speaker A: And in December we spearheaded an art exhibition with a very famous choreographer in which he was fully digitized in 3D. And then in real time with Unreal Engine morph into a more abstract shape. And going back from human form into this, completely synchronizing with the monitors. It was a mixed reality experience for 10 people that they would go like watch a concert in the end. Because through mixed reality you can see each other's eyes, you can have a social experience. And interestingly enough, our hypothesis of the product market fit was okay. So if end users that came for the first time find value on this, then we are spot on.
00:24:02.983 - 00:24:50.021, Speaker A: And the results were insane. Like, okay, so we sold the tickets for $20 for a seven minute performance. It sold out in less than two days. And 70% of the audience was first time XR users. So we dumped down the UX of course, so that they could just put on the glasses and the show started. And 95% of the D, 70% actually answered on the survey that they found significant value and that they would recommend it to a friend. Another one that this was not produced by Mawari, but that I see also as a very iconic point in time.
00:24:50.021 - 00:25:39.337, Speaker A: And there's this composer called Ryuichi Sakamoto, piano player that he passed away last year, but he was visionary and he said before I pass out, even sick of cancer, he recorded himself in 3D to leave himself as a perpetual digital firm. And last year actually it was in Manchester and at around in London, this concert. And people would just with the glasses, see him playing piano. And that's where technology transcended and into really evoke the emotions of their fans, that people were crying, people were holding hands. Very comfortable. A new type of experience that it really triggered me. This is it.
00:25:39.337 - 00:26:27.979, Speaker A: I mean people here are not thinking, oh, the headset is heavy or it's hot or whatever. They're just enjoying the show. So that's one of the verticals that we're working on. And the second one is more niche to Japan because Japan has an aging population and this has a lot to do with agents. So because of the aging population, there's not a staff for like, if I can call it blue collar jobs. And then all people at the asylums actually do need assistance. So all these agents are being developed in Japan with a lot of trial and error.
00:26:27.979 - 00:27:20.191, Speaker A: This is why KDD since 2018 told us we want to really create this digital assistant. Next year we will launch at full scale a hybrid service of digital assistant. What does that mean? It will be through a realistic avatar that will appear on screens. It will appear on your phone in augmented reality and for people that have the glasses on the glasses and you will be talking to an agent. But the moment that the agent detects that the complexity of the assistance is not suitable or you flag it, then it connects you to a real human. But the human is actually puppeting that avatar. So you never see a real human.
00:27:20.191 - 00:28:33.307, Speaker A: So you still have that sense of presence and connection with your personalized avatar that becomes your confident, your assistance, etc, etc. And I see how this is going to explode. And it's the best use case of agents. And for us is we agents use GPUs as well. So you know, we're here, we're putting like, okay, so the graphics plus the actual inference into one package, then the network and all works well. We have right now a puppeting system that allows for example, and we could potentially do a podcast later in which I will be an avatar and yeah, so that's the point. Through this puppeting system, I see also a new subculture called the virtual YouTubers that started in Asia, but right now in the western world, especially with Gen Z is skyrocketing because the kids that are on the Oculus don't want to show their real face.
00:28:33.307 - 00:28:58.425, Speaker A: Remember we were the same with MySpace for example, but now that MySpace is now okay, you have an avatar and if you can puppet it and it's your digital representation. So those are the two. One is fully AI, the other is hybrid, and then your avatar will learn your likeness and represent your personality online.
00:28:59.085 - 00:29:55.322, Speaker B: Yeah, I mean, I love the hybrid avatar concept. And it's transitory, right? Because as you say, whether it's at an institutional level or a personal level, when you choose to embody the avatar for whatever reason with a human, it then learns from that interaction as well. So it then has. It gets better and better and there's less and less reason for a human to kind of have to a human intervention. So that's super cool. Firstly, do you subscribe to this idea of a post web, as in the web, when all this stuff happens, the web dissolves or is fundamentally transformed. And if so, taking into all the kind of case studies and innovations that you're working on, where do you think that takes us in 10, 20 years?
00:29:55.443 - 00:30:43.067, Speaker A: Well, in 20 years, in a way, the web dissolves in terms of the user experience and user interface. It will be embedded into our daily life and we don't even, maybe even notice the new generations. That is the web. It's just a backbone infrastructure. It's there and everything is mixed. So one day, for example, when we were talking on this type of visions, even makeup, as we potentially will have AR contact lenses, you choose your own makeup and you will never actually will have to do a real makeup. You will always have that ideal self which is starting with the avatars now.
00:30:43.067 - 00:30:57.715, Speaker A: And that will be your presentation to the world. So yeah, we see that it dissolves and transforms into. You are part of the UI and the UX and you don't know you are part of it.
