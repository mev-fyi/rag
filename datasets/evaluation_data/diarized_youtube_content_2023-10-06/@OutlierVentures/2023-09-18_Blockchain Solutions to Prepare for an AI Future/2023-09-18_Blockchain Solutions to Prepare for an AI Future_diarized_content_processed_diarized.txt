00:00:00.090 - 00:00:12.910, Speaker A: There's conceptions of this out there in Sci-Fi and otherwise, that imagine this, where everyone's chasing their own dream and they don't have to worry about how to feed themselves. And they know that if they work hard, they can have access to opportunities like going to Starfleet Academy or otherwise.
00:00:25.210 - 00:00:33.974, Speaker B: You okay? So really happy to welcome back on the show Trent McConaughey, co founder of Ocean Protocol. Trent, it's been a while.
00:00:34.012 - 00:00:36.918, Speaker A: How you doing? I'm well. Good to see you. Good to be on here again.
00:00:37.004 - 00:01:38.618, Speaker B: So obviously the podcast has been going for some time now, and I reckon I've known Trent since 2013, maybe we did have you on the show as one of the first guests several years ago. Hopefully we've grown the audience by now. We got lots of new people that have come into the space, wanted to reintroduce Trent. Ocean protocol actually has a number of announcements, so we're going to talk through some of those in some new releases. But actually, I think it's a very timely moment to bring Trent back because time has proven him right on his thesis about the convergence of blockchain and web3 and AI, and actually a number of the innovations he's brought to market even prior to ocean protocol, all had their moment. He's been right more often than he's been wrong, at least that I'm aware of. Maybe he managed to hide the things he got wrong, certainly the kind of more high profile public ones.
00:01:38.618 - 00:03:01.986, Speaker B: He's pretty much hit the nail on the head. And of course, I think by now everybody's aware of perhaps some of the emerging problems, challenges of AI as it goes mainstream with its killer apps, but then also the potential possibility for web3 to be able to fix some of those, potentially even extend the promise of what's possible with AI, how it's owned, and the data economy. So just very quickly, Trent is a serial entrepreneur that came to blockchains from the perspective of AI and machine learning, had multiple exits of AI companies to synopsis. And Siemens, as I mentioned, has kind of been ahead of his time with a number of initiatives, going back to ascribe, which was kind of doing blockchain based art, I guess we call it nfts now in 2013, big chain DB blockchain database, and then more recently, Ocean Protocol, which he co founded with Bruce Pond. We were very lucky to be an early investor in that. I think technically in the friends and family round, after many beers in Berlin, usually talking about the convergence of blockchain and AI. I don't know how they rank in SEO terms, but certainly some of the earliest, deepest thinking on that subject, Trent was doing, albeit without the level of attention I think, that it deserved.
00:03:01.986 - 00:03:44.578, Speaker B: I think that's kind of coming now. I think Ocean Protocol's time has also come, which is great to see and well done for hanging in, know, being able to kind of finance, continue developing that product across cycles, basically. Trent has also pioneered token engineering as a discipline. So going beyond token design and actually bringing engineering principles, something we've continued to collaborate on, and then also looking at data tokens, I think we call them idos. Idos also have a different meaning now in terms of launching tokens on dexs, but I think we're the first one to use the term ido in terms of initial data offering. And then we're going to talk about predictor, which is the new thing. So we've got a lot to cover.
00:03:44.578 - 00:04:25.454, Speaker B: As interested as I am in predictor, I do think that we need to do your service and allow you time to kind of reflect, I guess, on all those things that you've got right. And I think for founders, hear your founder journey of being early timing is obviously one of the hardest things to get right as a founder and investor. But if you are perhaps too early, sometimes surviving is the next best thing. So would be great to kind of have you give a potted history of what got you to here, I guess. Trent, reflections on some of your early thinking on blockchain and AI. And then, of course, we'll get into predictor.
00:04:25.502 - 00:04:58.870, Speaker A: I was raised in a pig farm in the middle of Canada. Studied electrical engineering, computer science, and for fun, I was always hacking on the side. During undergraduate, I was hacking on AI, et cetera. I managed to get some summer jobs where people would pay me to work on AI, which I couldn't believe at the time. This was the mid, late 90s. So from that, it led me straight out of undergrad to start my first company, which was AI for designing computer chips, specifically for creative design of analog circuits. Initially, we were doing things like using genetic programming to evolve the circuit topology of the structure.
00:04:58.870 - 00:05:44.314, Speaker A: Turns out that was too early for the market, so we had to do a zoom in pivot and focus on just optimizing parameters. That was enough, that was sufficiently advanced for the users of the time. And these days, we hear a lot about AI for creative design and all that, and it's gone mainstream. Back then, we were basically having to do all this heavy lifting, designers seeing what we had and thinking, okay, if this computer is doing the creative design what am I doing? Is this thing going to take my job and stuff? So we had to basically teach them that it was more like a copilot in terms of the phrasing used now. I think that the product was called genius. Unleash your genius, right? That was the phrase for the first company. Started that company officially in 1999, based on about a year of prototyping and hacking before that.
00:05:44.314 - 00:06:15.054, Speaker A: And then this was the height of the.com bubble. So we are a bunch of kids basically straight out of undergrad. We managed to raise a decent amount of money from Silicon Valley and elsewhere, and then grew the company, built it up, went through our pivots, went through a cash crunch because we were a bit early. So this is where we did the zoom in pivot, and we found a way to hang on. And then eventually we started getting real customers, real traction, and the company got acquired in 2004. So a lot of events happening in that five year period, and it was a really great first adventure in the world of startups.
00:06:15.054 - 00:07:08.414, Speaker A: From that, I decided to do a PhD, also for creative AI, focusing on circuit design, because I felt like I hadn't fully cracked that problem for the first startup. And by going into academia on that, you don't have to be constrained by commercial needs and stuff. So I went full force and I came up with something that I was quite proud of, where it drew on a lot of the corpus of engineering knowledge for designing circuits. So it would actually be able to basically think of it like you could draw any random circuit from this well defined distribution of random circuits, and they would all be well formed. They were correct by construction, and that was sort of via this language of circuits, if you will. Just sort of like. Now, if an AI comes up with something, usually it's pretty well formed for an image, say it looks pretty good in its case, it's sort of a lot of training data, but it gives it pretty good constraints in what it comes up with compared to AI.
00:07:08.414 - 00:07:45.886, Speaker A: Coming up with images 15 years ago tended to be a lot more janky than it was good. So anyway, that was the approach I had for the PhD. At the same time as starting the PhD, I started a second company, which was also AI for chip design. In this case, it was for verifying analog circuits and memory circuits and so on. For example, does a memory bit cell, does that hold, which stores a bit of information one bit? So a one gigabit memory chip would hold 1 billion of those bit cells, plus some error correction, et cetera, sense amps, et cetera. So it means, though, that each bitcell has to have a very low failure rate on the order of one in a billion. But it's a huge difference in the failure rate if it's one in a billion versus 1.1
00:07:45.886 - 00:08:15.286, Speaker A: in a billion failure rate. So how do you estimate that you could go and do 50 billion Monte Carlo samples, which means 50 billion simulations, but that would break the bank computationally. So we had to figure out tricks to chop that down where we could get the accuracy, as if we did 50 billion simulations, but do it much faster. So instead of having to run a cluster of 1000 machines for a month, we managed to chop it down to about ten minutes on a laptop or so. So we were pretty happy with that. That was the second company. So the first company was really focused on design of analog circuits.
00:08:15.286 - 00:09:11.142, Speaker A: The second company was verification of analog circuits. And in the world of CAD for chip design, those are really the two big pieces, the two big subfields. There's design and verification for digital and for analog. And as a quick aside, I just gave a talk at ECC, sort of teaching the token engineering community about how the circuit community approaches verification of analog circuits, which is actually very, very similar, and then said, hey, what was done in the world of verification of analog circuits? If we say that a lot of the problems we're seeing right now in token land, such as flash loan attacks, breaking contracts and a lot of bridge attacks, and otherwise, these really are the problem of analog token verification. So it's not the digital side of the smart contracts, it's the analog side. It's the incentives, it's the non on or off. Not true or false, but really the continuous value behavior and continuous in time behavior.
00:09:11.142 - 00:09:39.762, Speaker A: So if we actually take theory, practice and tools from analog circuit verification, bring that into the world of analog token verification, there's some pretty cool wins there. So I'm hopeful for that. It seems that the field is getting there and we keep maturing. And it's really been awesome, by the way, to iterate with outlier on this too. In fact, even at that talk, the speaker right before me achieve from outlier. So it was nice to be back to back with him. So I'll just pause there for a second before I get into the blockchain stuff and see if you have thoughts or comments.
00:09:39.826 - 00:10:43.494, Speaker B: So it's really interesting. Again, you really informed my thinking around token design, bringing engineering rigor. The idea that if we want these systems to be able to handle the economic load of billions, trillions of dollars worth of value transactions, then we need to be sure they're going to hold up as much as we need to be sure an airplane flies or a bridge can take that traffic. And so I think taking that learning from engineering and modeling, designing complex systems, as you say, with incredibly low failure rates, really opened my eyes to the importance of token design. Obviously something we kind of continue to invest in, in thinking about here at outlier. I'm glad we got to talk about that. And I think one of the interesting things, maybe in retrospect, maybe it wasn't by design, but in retrospect, if I look at your career, it seems that you work in one domain, maybe working on something quite specific that leads you to another problem, to identify another problem, which then leads you on to your next startup.
00:10:43.494 - 00:11:36.022, Speaker B: And there's kind of almost this chain which seems to be quite logical in a way, of startups that naturally kind of feed into one another as a narrative all the way to ocean protocol. It would be good to just touch a little bit on ascribe, because I think that was when I first met you. And again, I think the project well ahead of its time. One of the really cool things that I've always loved about Trent is his commitment to open source. So practically everything that's ever been done, as far as I'm aware, in the web3 space has been open source or eventually open sourced, when there might be a particular initiative that might commercially have not taken off that code base, has really helped kind of inform a lot of other innovation going on in web3. But yeah, it'd be great to talk about ascribe and then I guess big chain DB as a stepping stone into ocean protocol.
00:11:36.086 - 00:12:08.534, Speaker A: Yeah, basically to wrap up this verification. So the first company was called Ada, by the way, analog design automation. The second company was called Solito, the verification one. By about 2010, 2011, we had built everything that I'd set out to build. We had become profitable. We had won most of the big semis in the space, the Qualcomm's and Samsung's of the world, and TSMC, which was kind of one of my main goals at the beginning, was if we can get TSMC using this, it means know they're using AI to drive Moore's law, which is always just sort know. As a nerd, it's a very fun thing to sort of aim for.
00:12:08.534 - 00:12:39.710, Speaker A: And I was happily surprised to find TSMC engineers using our software to design the next generation of silicon when I visited them in about 2011 or so. So with Toledo, we had got to the point where we had turned into a scale up. We were past startup, we had gone through harrowing times too. There was cash crunch at one point too. And then we were in a scale up mode and I started basically thinking about other things that I might be interested to do. Like you had mentioned, right? It seems that the dots connect quite logically. Myself.
00:12:39.710 - 00:13:32.750, Speaker A: I'm a big nerd, right? I love to follow lots of fields, AI and VR and crypto and BCI brain computer interfaces and more. At the same time, I recognize that I have a particular set of expertise, that it's wise, if I'm going to jump into something else, to be able to leverage my existing expertise and then learn what I need. And that's in line with a lot of career advice that people have given, right? You want to do something that can pay the bills, ideally, well, something that you're passionate about and something that you're hopefully good at, right? So that intersection of those three things, money, passion, skill. Anyway, that's part of the thread too. And then I was always super passionate about AI. Semiconductors was always interesting because it was a hard problem that was applied and could have a big impact. To be honest, I'd never paid close attention to cryptography, but I was quite interested in the cipher punk stuff happening in the late ninety s.
00:13:32.750 - 00:14:14.702, Speaker A: And then when bitcoin came out, my friends and I would hang out talking about it all the time, right? It was super exciting to us and we only got the bitcoin financial stuff. But then in 2013, I landed in Berlin and started hanging out at room 77 with a bunch of the people there talking about bitcoin, and they were already talking about blockchain. And I'm like, I never thought about this that much. So I went back to the Satoshi white paper, reread it, thought about it a lot, and realized, wow, we've got this general purpose technology. It's as powerful as AI or BCI or these other things. It was sort of an arbitrage. Most things you've seen coming for decades, right? AI people have been talking about since the depends who you ask.
00:14:14.702 - 00:14:52.694, Speaker A: And same thing with the high temperature superconductivity, right, which we just saw emerge as a final technology blockchain, as sort of this decentralized ledger. It wasn't part of this mainstream discussion. People had anticipated it here and there, probably most famously the agoric papers from the 80s with Mark Miller and others. And then also, also some of the early writings leading to the web, things like Xanadu, Ted Nelson and so on, which also affected ascribe of course. But the point is, this wasn't part of the mainstream discussion, the way that AI was. In some way, I learned about bitcoin, got excited about it. 2013, I learned about blockchain, got excited about it, and realized there was even different than AI or other things.
00:14:52.694 - 00:15:30.274, Speaker A: There was also an arbitrage, which is a useful thing, which is realizing that this thing could impact things and most of the world didn't get it yet. To me, extra interesting as an entrepreneur, because it buys you a bit of time to chase new ideas. And it's even more blue ocean, if you will, right? Rather than the Red Sea of shark, it's blue know thinking about different ideas. And my wife, Matt Masha, she is a professional musicologist. She is trained at the Louver School for Musicology. She has a phd in the relationship between art and communists from the sirbon. She had worked at galleries, including in Paris and in Vancouver.
00:15:30.274 - 00:16:13.854, Speaker A: Contemporary art, traditional art. She had also helped to curate, assistant curate at Louver. So she had this very deep art background, right? And whenever we were in whatever cities, we were in Vancouver, Berlin, whatever, she would be spending a lot of time in the art scene, hanging with artists, et cetera. So with that, when we arrived in Berlin, she started to notice this problem around how do you collect digital art? A collector came to her and said, I just bought this dvd for $10,000 from your gallery with this art. There's a scratch, can you replace it, please? And the artist is like, well, no, I sold you the artifact. I didn't sell you the thing. So the collector was kind of like, okay, that's not like both sides were angry with each other, right? Because there was this confusion about what it was that was bought.
00:16:13.854 - 00:16:47.182, Speaker A: But it was actually really hard for collectors to buy digital art. They would actually have to buy some artifact. If you're buying digital art in the, you're probably buying some tv with it or dvd or something, right? Yet at the same time, we are well into the web era, more than 20 years worth of web already. Yet it was really hard to monetize that because things could be copied and pasted. So it was hard for collectors to collect the art. And there, because of that, it was hard for digital artists, artists creating digital art to make any real money from it. So that was a big question.
00:16:47.182 - 00:17:22.310, Speaker A: It was emerging as a big problem. The art world had called an elephant in the room going, one day, mash and I were hanging out after going to some gallery, and she was talking about this elephant in the room problem. How do you collect digital art? And I'd been going on and on about blockchain, and this is a common thing. We're both quite patient and interested in what each other have to say. And then we both sort of looked at each other. It's like, wait a minute, we just asked together, what if you could own digital art the way you own bitcoin? And that turned out to be a wonderful question to ask. And we pulled on the thread and pulled on it, and we're still pulling it ten years later, but just for context.
00:17:22.390 - 00:17:25.158, Speaker B: So what year is this, then? Ascribe as 13?
00:17:25.254 - 00:18:02.038, Speaker A: Yeah, this was probably maybe April, may of 2013, when we asked this question and we started writing, pursuing it shortly after, like June, July, we actually filed a patent for it. I think it got filed officially in August of 2013. We let that expire because we realized this wasn't in line with the web3 ethos at the time. We were coming from the world of enterprise, but we let it lap simply because of that. We didn't want it to get in the way of actual innovation. So by 2014, we were iterating on a beta with many artists and we went live in, I think, January 2015. And what it was basically was, these days you would call it an NFT platform.
00:18:02.038 - 00:18:29.486, Speaker A: It was on bitcoin. Ethereum didn't exist yet. Someone could come along, sign up with their email address and just a regular password. Under the hood, it would automatically compute a private key for bitcoin HD private key. It would get registered on the bitcoin blockchain following a particular protocol we just devised called spool protocol, secure public ownership layer. I'm missing an o, but something like that. And then you could also transfer it.
00:18:29.486 - 00:19:05.514, Speaker A: Right, so once you've registered it, claiming that you have the copyright to it, then you can transfer it, which basically works as a sublicense. And we actually had a lawyer working with us for better part of two years where he devised all the legals around this too. So you not only had technology protection via blockchain, you also had legal protection where you were claiming copyright, and then it was the licensing, sublicensing, et cetera. So it's best of both worlds. You've got the traditional world of legals and all the backing that nations give for that, as well as the technology based protection. And yeah, we built it up, grew it, got to about 10,000 artists and users. 40,000 registered works went to raise money for the series a.
00:19:05.514 - 00:19:27.302, Speaker A: Our numbers weren't good enough for web two. That was the metric at the time. So we basically said, we have to pivot. We did some experiments, led us to big chain DB, decentralized database that was really targeted for enterprises. That went well for a while. We did a lot of pocs, I think like 50 pocs with basically every big name in Europe, and none of them went to production, just enterprises weren't ready for production. Blockchain.
00:19:27.302 - 00:19:38.706, Speaker A: And this is 2015. 2016. So we said, let's go back to permissionless. We prefer that anyway. And I was missing AI. I was starting to think a lot more about AI. You hinted at that already, and I can talk more about that.
00:19:38.706 - 00:20:03.082, Speaker A: So by mid 2016, I was starting to write about how AI and blockchain intersect. Articles on AI dows and how blockchain can help AI, et cetera. And all roads pointed to decentralized data markets. And we realized this would be a really great nut to crack, problem to solve. And that became the inception of ocean, and we spent most the better part of 2017 getting ocean going and then been at it ever since.
00:20:03.136 - 00:20:26.774, Speaker B: Yeah, and we're going to get into ocean protocol in a little bit. One thing I want to go back to, sorry to jump around a little bit, but I think it's an interesting concept for founders, perhaps especially now, right? There's a big capital crunch in the venture world. There's not enough venture capital going around for early stage founders. You mentioned the zoom in pivot. Could you just elaborate on that a little bit?
00:20:26.812 - 00:21:12.594, Speaker A: I'll even give an example. So PayPal started out quite broad. It was basically payments over the Internet, and they had a pretty broad set of customers helping people buy on any website, any shop that people set up, whether it be PayPal or the Amazons of the world or otherwise. As they iterated, they discovered that something like 80% of their traffic was actually on eBay, used goods and collectibles and stuff, right? And that was a big site in the Internet in the early 2000s. Right. So they decided to double down on that and basically do a zoom in pivot where they focused the vast majority of their effort to having a really awesome experience for eBay. And because of that, the eBay experience got better, better more quickly than the rest, and they grew much better.
00:21:12.594 - 00:21:53.838, Speaker A: So my first company, we started off with quite a broad scope, basically doing search through the space of possible analog circuit structures and parameters. So, like, how does this resistor connect to this transistor connect to this resistor? And what is the resistance of this, and what is the width of that transistor, et cetera? But this was too broad. So in our case, the zoom in pivot was simply a more narrower search base. Having a fixed topology where you're not changing the structure, we would just wiggle the parameters. So it's just doing optimization, not synthesis in the parlance of that. So zoom and pivot is basically taking your product offering, focusing on just one aspect of it. And in our case, we stopped shipping the rest, we just focused on the optimizer because it allowed us to do that.
00:21:53.838 - 00:22:24.618, Speaker A: In the case of PayPal, they kept shipping the rest, but they really focused all their energies to make the eBay experience really awesome. So that's a zoom in pivot. There's many types of pivots, right? You can just go from one product offering to the next. My next company also, we had to do a pivot. We started off with doing optimization for analog circuits, but accounting for yield like statistical variation. But it turned out that that was also too far ahead of its time. So we chopped that down to just statistical analysis, but that turned out not to be compelling enough to compare to the offerings out there.
00:22:24.618 - 00:22:51.534, Speaker A: So then we added in high sigma statistical analysis, which turned out to be compelling because it was a big savings in compute time and so on. So we basically did two pivots until we had our first hit product in that second company. And even in the third project, if you will, in blockchain. Right, it all derives from the same company. It was initially ascribed GmbH. And then we did a pretty big pivot within cryptoland to decentralized database, bigchain, DB, think like MongoDB, but with tendermint consensus around it. From that, we did another pivot to ocean.
00:22:51.534 - 00:23:14.342, Speaker A: But crypto was so new that you could go from one offering to another to another and manage to reuse a lot of the skills that you had built up. It's been pretty blue ocean because there's just so much to build. So we are able to go from one thing to the next, the broad brushstrokes and even with an ocean. Right. We've done various tunings over the years, too. Various sort of smaller pivots, if you will, towards traction.
00:23:14.406 - 00:23:53.042, Speaker B: Yeah. And I guess, as you said earlier, considering much of what we were talking about was pre or just post Ethereum launch, in a way, certainly how we looked at the space as an investor. And at that time, we would have really framed ourselves as an incubator, not an accelerator, that it was really deep tech. The expectation was that anything you invest, well, firstly, there's no point investing in anything that wasn't infrastructure, quick realization for us. And then secondly, you would have to expect that to take several years before that thing would even be used. And so that timing thing was so critical. But I think that mindset meant there was a lot more latitude for experimentation.
00:23:53.042 - 00:23:55.910, Speaker B: I think the patience for that's worn out now.
00:23:55.980 - 00:23:56.214, Speaker A: Right.
00:23:56.252 - 00:24:16.634, Speaker B: We're a decade into crypto. People want to see adoption. They want to understand cost proposition for users. The world's moved on. I don't think big infrastructure projects will get financed that much anymore. Right. But it's interesting for people entering a new domain, that kind of opportunity space that's afforded to a founder.
00:24:16.634 - 00:24:31.778, Speaker B: So then let's get into ocean. So the kind of problem statement that kind of triggered ocean protocol, and then I guess the stack and innovations and how that's evolved, what are we now? So that's 17, three, six years. Yeah.
00:24:31.864 - 00:25:03.606, Speaker A: So I'll talk about ocean evolution, et cetera. But I want to loop back just in what you're saying here. I think it's useful for your audience on building infrastructure versus last mile apps, et cetera. There's a challenge here. Anytime you're building infrastructure, it means that it's going to be a longer time to actually get traction, where there's dollars, in a sense, because the traction comes from the last mile app. So you sort of have to have infrastructure and some sort of last mile usage probably, right? So that extends the timeline. And also, if you're kind of visionary trying to build something, you have one or two pivots.
00:25:03.606 - 00:25:35.094, Speaker A: It takes time, but the big challenge is cash, and cash is king in a startup. But most startups maybe have the luxury of one big pivot. In my first startup, we had one big pivot. In my second startup, we had small to medium sized pivots. Right? With ascribe, big chain ocean, we've had two big pivots. Right? So in all of those, though, it was pretty harrowing times, right? Going through the pivots and looking backwards, I realized this, that I kind of had this mantra to myself, but it was subconscious for years, which is, I'm never going to let this thing die. I'm going to make it happen.
00:25:35.094 - 00:26:00.878, Speaker A: I'm going to make it happen. I don't care what's going to happen. Even if everyone else leaves, I'm going to do it myself. And I'm a builder, so that I know that I probably could, even though it'd be slow and stuff. It'd be slow and maybe painful and stuff. But if I can find a way to feed myself and keep going, then other people can join the cause and so on, right? So fortunately, it never ever came to that. But having that resolve that it's like, I'm going to make this happen no matter what, actually makes it easier for other people to join in, in a sense.
00:26:00.878 - 00:26:34.682, Speaker A: You shouldn't always do that, by the way. I have my wife, in a sense, as my safety valve of like, Trent, you're crazy. If I was being lone wolf for four years on something, it's probably time to change. But there's value in that, right? And there is really my favorite example of this in startup land is blogger. So Ev Williams had started blogger just by building this thing for himself on the side. It was the first good blogging site, built it up. It was starting to get traction, and some vcs came around and he took some vc money and he grew the team to, I don't remember, ten people, 30 people.
00:26:34.682 - 00:26:46.590, Speaker A: So they built it out. But then there came a cash Crunch. I think it was the.com implosion or something. I forget exactly which one. They hadn't gotten good enough traction, making enough money to be able to pay for them. So basically everyone left except for EV.
00:26:46.590 - 00:27:08.018, Speaker A: So once again, he was low in wolf, I think, coding for at least a couple of years on his own. But the traction kept growing, growing, growing. I believe that's when he got acquired, I forget Yahoo or Twitter or someone, he got acquired based on just having lots of traction. And so the point is, he slogged it out. He was able to do it having this resolve. He knew he was onto something and he had the resolve and the skills to just make it happen. Right.
00:27:08.018 - 00:27:22.766, Speaker A: So I think that's a useful heuristic. But you should never be sort of 100% absolute in everything. Always give yourself sort of maybe a backup and a backup plan if you need. But that definitely helped. In the first company. We actually shocked our investors. It wasn't just me with this resolve.
00:27:22.766 - 00:27:33.194, Speaker A: All the founders were like this, and all of our earliest employees, et cetera, we were all like this, and we found ways to extend a Runway. I look back, it's like, holy cow. That was pretty cool what we pulled off. Yeah.
00:27:33.232 - 00:28:03.542, Speaker B: And I think this is why it's important for us anyway, that there's always a technical co founder. The technical team aren't just contractors. They're not just employees who will come and go naturally when the uncertainty increases or the risk increases. But you're absolutely right. At some point, sometimes you have to say, okay, I might be right, but I'm too early or whatever other reason why it might not make sense to kind of pursue with that path. And, look, I think that's really useful stuff. Thanks for that.
00:28:03.542 - 00:28:19.238, Speaker B: In terms of pivoting and kind of navigating that space, let's then go to kind of the problem statement, as was for ocean protocol. Maybe it's the same, maybe it's a slight tweak. But as it was in, what, 2017?
00:28:19.334 - 00:28:51.874, Speaker A: In 2016, I wrote this blog post, blockchains for artificial intelligence. And by that time, I'd come up with this heuristic, this set of tools, to identify how blockchain can help any field. I broke it down to four specific benefits. Decentralized, immutable assets and incentives. So, decentralized as in, you can have play coordination games to coordinate a group of people, whether it's ten people or 100 or 10,000. Bitcoin is 10,000 multi, Sega is ten, and dows are somewhere in between. And if it's 10,000, it really becomes sort of a utility across the Internet.
00:28:51.874 - 00:29:26.906, Speaker A: So that's decentralization. Immutable means provenance, provenance of data, asset trails, provenance of money, provenance of art, all that asset means not your key, not your bitcoin, not your key, not your data. You own it if you have the key to it. So this is the bearer asset approach, rather than someone else having some say in what you own. Right? And that's very useful, because that's become the standard. That is the standard in blockchain land, and it's very healthy and helpful. And then the fourth one that I actually only really fully recognized in 2017 is incentives.
00:29:26.906 - 00:30:02.282, Speaker A: And I think that's actually the superpower of blockchains, that this idea that you can get people to do stuff by paying them in tokens. And then the question is, what do you want them to do, and how do you go about designing a system to get them to do this? And that looks a lot like an optimization problem. You basically come up with an objective function and a design space, and the objective function is something that you want to maximize or minimize. So bitcoin says, I want to maximize the security of bitcoin. How I want people to go and contribute to the security by doing hashes in this particular way. Right. So, yeah, that's the overall set of tools that I came up with.
00:30:02.282 - 00:30:41.314, Speaker A: The first three in, I think, 2015 or 2016, and then the fourth one in 2017. Anyway, I applied this to, and I'll summarize once again, decentralized, immutable assets, incentives. So with that tool. I turned the crank in late 2016 and said, what does this look like for AI? And by the way, what does this look like for big data? I found for each. I just said, okay, how can decentralization really help AI? Right? And you can have things like shared ownership of AI algorithms, shared ownership of data feeding into AI public utility networks that are fully decentralized with training ais and otherwise, right? And turned the crank and went through all of this. That blog post. Yeah, did turn out to be quite popular, too.
00:30:41.314 - 00:31:34.186, Speaker A: There was this recurring theme of data throughout that, because data is unreasonably effective, if you want to make your AI more accurate, you have a choice. You could spend four years worth of PhD level research to improve the accuracy, to reduce the error from, say, 20% to 10% or 10% to 1%. Or you can take an engineering approach and throw ten x more data at it, and you'll get roughly the same results, which, by the way, is a bit embarrassing to AI people, because AI people like to be able to be the smartest people in the room and stuff with their algorithms. But now you can just have more data or more compute, and if you have more data, of course you need more compute, and it can be ten x, then 100 x and 1000 x more data. Google started doing this in earnest in the mid 2000s, followed by all the other big guys, right? Given that the vast importance of AI to data. I wrote about that also in that article, and realized that there was these big problems. The Googles of the world had reams and reams of data.
00:31:34.186 - 00:32:26.634, Speaker A: But what about the average AI researcher trying to come up with models that could predict cancer, try to detect early stage cancer? And I remember one friend of mine, he was working with a hospital, and he got access to a data set. It's so bad, it barely deserves the name. I think it was like something like 120 people, and it was DNA data, like snp stuff, with something like 100,000 dimensions per data set per point. So you've got 100,000 dimensions and, like, 100 data points. It's really, really a poorly formed data set. You can barely do anything with it, right? You can apply regularized linear learning, but in the end, it's really hard to do much, right? But imagine that same dimension of problem with 100,000 vectors, but you've got 100 million data points, or a billion, or 10 billion. That starts to get really interesting, right? And that's actually what we've been seeing with AI for the last 20 years.
00:32:26.634 - 00:32:51.998, Speaker A: But it's really hit the mainstream, especially the last couple of years where we see these large language models or very large language models. Right. In circuit land, we used to have large scale integrated circuits, LSI, and then those got really big, so we started calling them VLSI. I think that was the early 80s VLSI. And then they got extra super large, but they didn't come up with acronyms, they just kept calling them VLSI all the way along. So I guess we smartened up. In neural network land, we just called them large language models.
00:32:51.998 - 00:32:54.786, Speaker A: But we'll probably never see VLLMs, just.
00:32:54.808 - 00:33:16.662, Speaker B: To pause on that point. So obviously by now everyone's heard of OpenAI, they probably use chatt, and it's now being reported that there's these diminishing returns. In fact, in some ways, the more it's being used, the stupider it's getting, the worse the answers. Could you maybe give perspective on that? Right. Was that an obvious thing that was going to happen, or is that something that's emergent?
00:33:16.726 - 00:33:52.978, Speaker A: Yeah. So actually, about two or three years ago, someone actually did a very cool study, actually several teams. But there's a paper that is sort of this scaling law for AI for LMS, where if you want to have a particular error with a particular problem domain within language, then here's the size of the AI model you need, here's the size of the data set you need, right. And if you have ten x more data, it won't move the needle. There really is a sweet spot, but it allowed researchers to really calibrate how much data they might need for a certain sub problem they're aiming for. Right. And that was actually like a medium sized breakthrough.
00:33:52.978 - 00:34:17.466, Speaker A: A couple of years ago, what we saw OpenAI, if you recall, a couple of years ago, they raised, what was it? $10 billion for Microsoft, or was it a billion a lot? I think ten. Right. Because then they went and spent most of that on cloud services. Right. I mean, if you've got $10 billion, you're not using that to hire 10,000 employees immediately. Right. So they had relatively small number of employees still, but it was spending most of that on compute.
00:34:17.466 - 00:34:42.150, Speaker A: So training GPD four, which powers chat, GPT, et cetera, was probably, I don't know if it's reported, but at least a billion dollars worth of compute, I'm pretty sure. Right. So the thing is that was for GPT four, but maybe now they want to do GPD five. They're working on it. They've been working on it for a while. Of course, if they had to spend a billion dollars dollars worth of compute and maybe another billion dollars gathering data. So $2 billion V four, GPD four.
00:34:42.150 - 00:35:03.502, Speaker A: Then for V five, maybe they want to go ten x. Now you're looking at spending $20 billion and it just doesn't fly, right. It's too much. So they hit a wall, actually more economics than parameters. And maybe they were seeing diminishing returns. But the thing is, it's just not worth spending $20 billion to find out. Yes or no, right? It was worth spending a billion dollars to find out, which is interesting.
00:35:03.502 - 00:36:07.970, Speaker A: And by the way, this is actually what happened to Moore's law itself too, right? When intel did the first fabs in the would cost 10,000, $20,000 to do that initial fab to manufacture computer chips, right? Like VLSI circuits or back then, extra small integrated circuits. But then with each passing generation, the price would go up in some exponential factor. And roughly speaking, at least from the 80s onwards, it was roughly doubling, right? So a fab might have been one hundred k, and then the next generation was 200k, then 400k, then 1.6 million, et cetera. And by the time of about 2005, it would have been about a billion dollars, if I recall correctly, to do a fab, right? Maybe actually, maybe even by then 5 billion, it was a lot, right? And then within that fab, that's just the factory with all the machines inside. And then to manufacture a given chip, which has basically come up with a set of instructions, it's called a mask set, would cost $50 million, right? So you better get this right, right. That's why we have all this really awesome circuit verification stuff.
00:36:07.970 - 00:36:37.766, Speaker A: So $50 million for that, plus the fab, that's $2 billion. So what happened was, up till that point, up until about the year 2000, there was tons of semiconductor companies which owned their own fab, right? Everyone and their dog had their own fab. Texas Instruments and everyone, right? Not just intel and Samsung, but Sony and the rest. Fast forward to about. From about 1997 ish till about 2005, they all got rid of their fabs. There was basically just a handful left. There was a pure play fab that had emerged called TSMC, Taiwan semiconductor manufacturing.
00:36:37.766 - 00:37:10.594, Speaker A: There was intel, because they were so huge, they won big in the 90s, right? With Windel, Windows, plus Intel and Samsung and basically everyone else. Texas instruments sold their fab, Sony sold their fab, et cetera. They all went fabless. And then there was a bunch of new startups that just started fabless. Like Qualcomm, right? So basically the economics dictated it. And then over the years, even these guys have dropped off. So in the last couple of years, maybe, I think two years ago, three years ago, intel even started using TSMC for the most advanced process nodes just because it was too expensive otherwise, right? $10 billion, $20 billion or more for a new fab.
00:37:10.594 - 00:37:55.886, Speaker A: We're seeing exactly the same thing happen in LLM land. So Moore's law slowed down and then is basically kind of ground to a halt because of economics. And this is the challenge we're seeing right now with LMS, but with circuits, there is research to try to find ways to do the cheaper, cheaper, cheaper. But people aren't as used to it there, because for the last 40 years, there's been this gift that's been giving. Keep on giving, actually 60 years, right? It's kind of amazing in LLM land because everything is open source and there's huge financial incentive to do a good job since GPD four came out. And even before, there's researchers that keep finding ways to get a two x here, a ten x there, a two x here, a ten x there. And these days, now people are getting to the level of GPT, 3.5
00:37:55.886 - 00:38:45.106, Speaker A: level models on a single Nvidia GPU, which is kind of amazing, right on. Just like say, one day to one week of compute processing, right? It's kind of amazing. So that means that then if you're OpenAI, maybe you can use that. You still run ten of these or 1000 of these for a month and you see what you come up with. So we sort of paused by just doing the naive ten x by spending ten x more money, because we hit this economic constraint and instead the open source world stepped in, plus OpenAI, but especially the open source world, the stability Ais and the llamas and all this, and made everything way cheaper. And now everyone's building on that and just racing ahead, right? So AI is going to keep advancing and I don't think we've hit a wall, right. There's so many people out there chasing it.
00:38:45.106 - 00:39:26.958, Speaker A: And the economic gains can be so great that I actually don't see a wall. And a lot of the time, just as another sort of factor for this, every few years there's some famous AI researcher that comes along and says, I don't think AI can go any further. We need to have fancy thing x, fancy thing y, we need to do symbolic modeling, we need to do expert systems again. We need to do this. But in the end, it's just more compute, right? One of my favorite results was the state of the art in, say, 2015 was. Even then they were calling them deep neural networks, right? It went from deep belief networks to deep neural networks, from the likes of Hinton and staff out of Toronto. And this Schmidt Hooper and his team out of Switzerland and Germany.
00:39:26.958 - 00:40:02.830, Speaker A: They took the same neural network architecture and training algorithm from 1989, the plain Jane backprop. And they just took it. They made it larger. Instead of just three or four layers, they made it ten or 20 layers, and they just threw a ton of compute at it, and they got the same results as all the deep learning stuff. So it was not any change to algorithms at all or anything, and it was just like, lots of compute, not really any more compute per se compared to the state of the art. But that was also kind of, once again, this thing that you didn't need to get fancy with algorithms, et cetera. And by the way, the Hinton and all the deep belief stuff was not that much changes anyway, right? It was a tweak here and tweak there, really, if we want to be honest with ourselves.
00:40:02.830 - 00:40:33.494, Speaker A: And since that time, we've had a couple of other slightly bigger changes. We've had transformers changes, and the fine tuning people going nuts on that. But all in all, it hasn't been massive, massive changes to these basic ideas of neural networks, 1989, and nonlinear perceptions of the early 60s. So my point is here, it's still a very blue ocean. It's a broad set of design space that people are exploring, like thousands, probably tens of thousands of people exploring aggressively. Some of them will pop, so we're not going to hit a wall.
00:40:33.532 - 00:41:23.186, Speaker B: And so does this then take us full circle back to the data problem or the data opportunity. Right. Obviously, or my understanding, anyway, is that OpenAI chat, GPT, and GPT-3, et cetera, have been primarily trained on open web data, which, in theory, anybody can access. Of course, there's various claims to whether they should or shouldn't have used that for commercial purposes or Getty Images, suing various new entities for training on their prop assets, I guess. But effectively, that could only take you so far, perhaps at a more generalized level. But as you then need to have deeper specialisms, you then need to access new data. So you need a new economic paradigm which incentivizes and enables that data economy.
00:41:23.186 - 00:41:26.546, Speaker B: And I guess, brings us back to the premise of ocean protocol.
00:41:26.578 - 00:41:51.130, Speaker A: Right. After writing this paper about how blockchains can help AI, we saw this challenge overall. If you weren't a Google or a Facebook of the world, it was really hard to get your hands on. Really good data sets, large data sets, to have accurate enough models. So how do you. Right. And on the flip side, there was all this, tons and tons of data inside enterprises, for example, that could be used for a price, but you have to address the privacy concerns.
00:41:51.130 - 00:42:40.506, Speaker A: And then also there's all this data that individuals have on their smartphones, et cetera. It'd be awesome to be able to access that for a price. But in that case, also, how do you handle the privacy aspects there? And from the enterprises, too, they have to handle the privacy and the liability and stuff, too. So these are the big challenges. How do you sort of connect the potential data suppliers with the potential data consumers in a way that addresses privacy in a way that you can have it where it's priced data or free data, ideally having some form of price discovery, and ideally that there's no centralized middleman that ends up winning it all, right, that's already been a problem with the googles of the world. But as we go forward, especially when we get into things like BCI data, neural data, you really don't want that to be controlled by Google. And Google seen pushback, right? They were trying to do federated learning across dozens of hospitals in the UK.
00:42:40.506 - 00:43:03.202, Speaker A: And the UK kicked them out because of privacy concerns. Right. It was just too much control of users'health data on that one thing. So you really need to have more decentralized management of this. So that led to ocean basically saying, okay, at the heart of it, the heart of the problem is we want to kickstart this data economy. Maybe a way to summarize is level the playing field around data and AI. So make it aware.
00:43:03.202 - 00:43:22.890, Speaker A: It doesn't matter if you're a researcher at Google or an undergrad student trying to build an AI model, you have equal opportunity to access the data. And then on the flip side level, the opportunity to be selling the data. Right. An open marketplace to be selling the data or sharing it. Right. And as needed, where needed, you have provenance of where the data came from. Right.
00:43:22.890 - 00:44:13.146, Speaker A: And the provenance, of course, comes pretty easily with blockchain to track it, just like we did in ascribebase with digital art, where Providence is king in the art world. That was the premise we began with, with ocean, and it's largely held to this day to be able to do the decentralized data marketplace. We kind of implicitly knew, but we really made it explicit very quickly, which was, you need to have really good decentralized access control. So think like when you're using Google Drive right now to share something to someone else, you give their email et, you know you're clicking share share to an email address, whatever. There's this middleman in between called Google, even if the data itself maybe, why do you have to have it being living on Google Drive, et cetera? So ideally it's living on filecoin or rweaV or something decentralized, ideally encrypted, et cetera. You share to the person. And that sharing infrastructure is all decentralized too.
00:44:13.146 - 00:44:51.482, Speaker A: And then if you want to make it really interoperable, why not make it where you can make it easy to manage and share the data the same way that you manage and share bitcoin and ETH and 1000 other ERC 20 tokens and nfts. So that means tokenizing it, right? So that's basically what we ended up building. The heart of it is decentralized access control, where if you hold 1.0 data tokens, your c 20 data tokens for a given data set data service, then it means you can access the thing. You can come in and say, I want to start consuming this, and then you get the file, or you get the access to the rest API, or otherwise. That's the heart of it. And then ocean has infrastructure to publish these things and consume them.
00:44:51.482 - 00:45:31.306, Speaker A: And typically it's static data or rest API. But we also have this thing compute to data. So you can actually have a script that runs next to the data itself inside an enterprise's infrastructure, or a hospital's infrastructure, or on someone's phone that maybe computes the average across the data set or some other statistic, or maybe does a bit of training to update some neural network model or something. So that's the heart of ocean. If you go know the apps on top of know, we have this reference app, the ocean marketplace, ocean market. And then there's a bunch of people building other know, there's a decentralized Kegel competition. It's called d sites.
00:45:31.306 - 00:46:57.710, Speaker A: There's other data marketplaces that people have built. Probably the most famous one is Daimler has a spinoff called Eccentric, where it's starting with automotive data. But they're also targeting other enterprises. So they've done all the things for enterprise compliance, things like making KYC real easy, KYC AML identity related, and a bunch of other needs, and really doubling down on the computed data side too, to handle privacy and liability, making it really easy for enterprises to adopt. So that's where we're at with ocean. And I guess one other thing we kind of just wrapped up, call it phase one of ocean, and we're entering phase two sort of in the last year, phase one was basically building to the white paper everything we dreamed to build, et cetera, keeping our promises to the community, which is decentralized data infrastructure, data markets, all that. And then the second phase that we're entering now is really going for traction, right? Getting off of the version treadmill and instead focusing on how can we really make this data economy happen? How can we drive the data consume volume, the amount of money flowing through this open data economy where people have their own choice of how to share privately, et cetera, and just grow that, right? And with that, the core team spends time working with other teams, maintaining the core stack, tuning it as needed, as well as like hinted in the beginning of this podcast, we have an internal effort around something called Predictor, which as of this podcast coming out, we've just announced.
00:46:57.710 - 00:46:58.834, Speaker A: And I'll get to that.
00:46:58.872 - 00:47:00.718, Speaker B: So let's get into predictor.
00:47:00.814 - 00:47:50.834, Speaker A: Predictor is a stack and an application for prediction feeds. Its main users are defi traders and data scientists. So if you're a dFi trader, you can go to predictor AI, it's two O's. So predictoor AI, predictor AI. And you'll see there that it lists the top ten pairs or top ten tokens by market cap, bitcoin, eth, et cetera. And for each of them, well, for bitcoin, you'll see that it's actually giving a prediction of whether bitcoin will go up or down five minutes from now with a confidence level, and it keeps updating every five minutes. And then below that you'll see one for ETH and XRP and other things, and those it won't say whether it's going up or down yet.
00:47:50.834 - 00:48:37.954, Speaker A: You'll see a little buy button that you can basically choose to buy that feed. And when you buy that feed, then you get access to it for the next 24 hours. So basically what you can do then is you can have one window open while you're seeing the predictions of whether this thing goes up or down five minutes from now. And then you can have your finance open. And this is actually focusing in the initial product on binance pairs, bitcoin, true USD, simply because that has zero fees and eth, true USD, et cetera. So open it, keep your binance open, and if you think it goes up and you're confident in it, great buy if you want or short however you want, or do whatever other actions you want, right? So that's the experience from perspective of a trader. But of course, it's every five minutes, so why not wire it up with a bot? So of course we make it really easy.
00:48:37.954 - 00:49:29.794, Speaker A: We've got readmes where people can fork a repo for running a trading bot in python and then change it as they need with just the predictions as they're coming, and then they can buy and sell as they need, as they choose, et cetera. So that's one side. It's basically prediction feeds, where the main user is traders initially, but then where do these predictions come from? Are they any good? Right. So the first thing we did actually towards this was make sure that we could get accurate predictions internally, and we got to the point where we got accurate predictions, and we've been trading those against those internally with real money because it's accurate. So we've got decent accuracy ourselves. And we could have sold that ourselves as the ocean core team, Ocean Protocol foundation. But that's a bit centralized issue.
00:49:29.794 - 00:50:15.338, Speaker A: We don't really want that. We really want to kickstart a data economy. So we looked around and we know there's different approaches to basically leveraging wisdom of the crowd from an optimization perspective, most notably how cowswap does it, where there's a bunch of different cowswap solvers that every minute or so, there's a bunch of different trades to be matched, orders to be matched in this deck setup. And these cowswap solvers, each one runs an optimization to try to maximize the mode of matching and then spits that back and then does some matching, and the winner wins and they get money. So that's cowswap. So cowswap is a very interesting sort of dex that sort of solves this coincidence of wants among traders. So someone trader, one wants to trade a for b, trader two, b for C, trader c trader three, c for a ABC.
00:50:15.338 - 00:50:41.126, Speaker A: But you can have 100 different trades, and it's sort of spaghetti mess. And these solvers go and solve that. That's what cowswap does. I've been a longtime fan of, it's from the gnosis team, longtime fan of the gnosis team in general. There's other examples, too. Numera is doing predictions where people submit predictions, in this case, prediction. The centralized hedge fund takes those predictions, aggregates them, trade against them, and then shares the proceeds back to the people making the predictions, which I think is really cool.
00:50:41.126 - 00:51:17.446, Speaker A: I'm a big fan of numera as well. So what we're doing here is people can submit predictions for tokens. So in numerai, they submit it for stocks in an obfuscated way. In our case, they submit it for a pertinent will bitcoin go up, yes or no? Five minutes from now? Will eth go up, yes or no? Five minutes from now? They stake against it as well. So they have to put their money where their mouth is, and then predictor aggregates all those different predictions weighted by stake and puts that thing for sale. Right. And so the people that are making the predictions, if they're really confident, if they're really good, they'll be staking a lot.
00:51:17.446 - 00:51:39.002, Speaker A: If they're not confident, if they're not good, they won't stake. And on average, it's going to be correct or it should be. And in our results, you don't want to be 50% correct or 49% correct. That's even worse. Or even 50.1 or 51%. Ideally, you're 52% or 60%, 60% accurate or whatever.
00:51:39.002 - 00:52:18.694, Speaker A: So, yeah, you're going to be wrong some of the time, but on average, you make money, right? On average, you're correct enough to make money accounting for trading fees, et cetera. And that's exactly the case in our internal experiments. We have that, and now we have this emerging community that we call predictors. The people that are submitting predictions will eth go up five minutes from now or not? Will bitcoin go up five minutes or not? And then they get paid based on how much they've staked and whether they are accurate enough. And they get paid on five minute intervals, basically. Right? So those are the two overall communities. Basically, there's prediction feeds of will bitcoin go up, yes or no eth, et cetera.
00:52:18.694 - 00:52:53.806, Speaker A: And that prediction feed is populated in a crowdsourced fashion, where these predictors are submitting predictions and staking against it. And then to kickstart this predictor community, we are running one internally from ocean core team, maybe even two. And then also we've been collaborating with another team to launch one as well. And of course, it's open to anyone. So once again, we have readmes, et cetera, for people to take and download with code, fork it, et cetera. So we hope that a whole community will emerge of ten or 100 or more predictors. And the way that you make money as a predictor is threefold.
00:52:53.806 - 00:53:28.894, Speaker A: First of all, if you're a predictor and you're wrong, your stake gets slashed, that particular prediction, and it goes to the others. So the others get it's a zero sum among those stakes. So the people who are right get that value pro rata to how much they staked. But then also people buying the feeds. The vast majority of the people, the payments to buy the feeds, that goes to the people making predictions, and then a small cut goes to the ocean community and to buy back and burn ocean and a bit more. So that's the second thing. And the third thing is to kickstart it.
00:53:28.894 - 00:53:56.386, Speaker A: You know, we don't know how big the market will be for people buying this, right? Will be $10,000 a week worth of sales will be less. Will it be more? We don't know. Right. So we decided let's have an incentive program for this as part of our data forming program. And we are putting in it's 35,000 ocean a week. So at the time of this recording, that's ocean is at about $0.40. So it's about 10,000 ocean a week, or $10,000 worth of ocean a week to these feeds.
00:53:56.386 - 00:54:31.762, Speaker A: We're launching with ten feeds. So it's $1,000 per feed per week. So basically, as a predictor, you can make money. If you're accurate, you'll make money from slashes of other predictors sake, from revenue coming from the traders buying, as well as from this Kickstart revenue, basically from incentives. So, yeah, we encourage people to come to play with this thing, to trade against it, hopefully make money to predict, to become predictors, to hopefully make money, too. And we're launching with DFI tokens. We're starting with ten tokens, but we're going to expand to this.
00:54:31.762 - 00:54:51.142, Speaker A: We're going to be going to top 100 coins. We're going to be going to more exchanges and more timescales as well. Not just five minutes, but longer. And then stay tuned. Beyond, beyond that, we're going to go beyond defi as well. So we see that the idea of prediction feeds, it's a new sort of category. It's sort of like a prediction market every five minutes.
00:54:51.142 - 00:55:13.102, Speaker A: So call iterated prediction market, if you will. But it's its own unique beast, too, because it's also a data feed. It just happens to be a data feed that's very valuable because it's making a prediction. It's sort of at the tail end of this data supply chain where you can act on it immediately and make money. So we're quite excited about this. We've been working at it for quite a while in the ocean core team and really happy to be sharing it very cool.
00:55:13.156 - 00:55:45.290, Speaker B: So congratulations on the launch. I know dgens will rejoice at that, especially the go to market. I know the team at Outlier will have a lot of fun internally with that. So we'll get some of our own prediction markets going. So we're up on the hour. Very appreciative of your time, but I would be remiss to not kind of close off asking you about your perspective on proof of personhood. So of course we have OpenAI posing the problem that advances now are going to put everybody out of work.
00:55:45.290 - 00:56:13.714, Speaker B: So we need some form of Ubi universal basic income to kind of offset that. The antidote is being positioned as worldcoin, both by Sam Altman, of course. And the core premise to Worldcoin is proof of personhood. Now, you might not necessarily want to go into a critique of worldcoin, but feel free. But I think more around this premise, the problem, and I guess the opportunity of proof of personhood and approaches to solving it.
00:56:13.752 - 00:56:56.766, Speaker A: So I'm going to approach this from an AI perspective. We have one large problem looming in the AI future and then one crazy huge large one looming beyond that. So the large one is that AI could take away all the jobs. And if you disagree, just at least allow for the fact that maybe there's a 10% chance that it could take all the jobs. Okay, so if, even though there's a 10% chance, then we should start to think about what we can do about that, right? So that's the nearer term largeish problem. And the super large problem is that we get AI super intelligence, and what do we do about that? And there's no economic incentive to slow down AI research. And it's kind of crazy to say we can solve it just by having AI researchers, sorry, AI doing research to solve it.
00:56:56.766 - 00:57:23.546, Speaker A: Like you still need some fundamental solution. So for that, my quick answer there is humans needed a competitive substrate. We need human superintelligence. But that's for another day. But so going then to this nearer term large problem in AI, this risk of AI taking all the jobs. And I actually have seen this as a big risk for years, right. I've been doing AI professionally since the 90s, right? So I've written about this in the past, long before worldcoin, and thought about it and talked about with friends and stuff too.
00:57:23.546 - 00:58:16.554, Speaker A: To just pause that for a second. If you think about what, when people work, most people in the world, they don't like their job, so they're working. They're working to feed their family or feed themselves. And probably their family, if they have a family. And maybe 10% of people are getting satisfaction from the job, right? Maybe 20, depending on how you measure, right? But whenever we talk about Ubi and stuff, people always say, yeah, but I needed to feel self fulfilled and all that, right? Well, that's actually just 10%, 20% at most. And the thing is, you can imagine if you can do that even without working, to get paid, right? So the point is, we can take the concept of work and decouple it into two pieces, getting paid enough to feed your family and then chasing self actualization. So I really like the idea of if AI is taking all the jobs, we don't get paid that way, then we should have some sort of safety net, in my view.
00:58:16.554 - 00:58:51.986, Speaker A: If we can pay for it economically, if we can pay for it to meet our basic needs, that's universal, basic income, food, clothing, shelter, water. But then why stop there too, right? Why not also pay for health care and education going up Maslow's hierarchy all the way to the top, self actualization. So for me, it's USA universal self actualization income. And that, to me, is much more interesting, where every person can chase their dreams, whether it's writing the great american novel or playing video games in your mother's basement. Take your pick, right? We shouldn't choose. That's the point of know. And if you think about, like, when you watch Star Trek or something like that, they don't actually have to worry about income.
00:58:51.986 - 00:59:37.438, Speaker A: They can all chase their own, you know, so there's conceptions of this out there in Sci-Fi and otherwise, that imagine this where everyone's chasing their own dream and they don't have to worry about how to feed themselves. And they know that if they work hard, they can have access to opportunities like going to Starfleet Academy or otherwise. So I see, overall, this seems to be the most pragmatic approach to solving the problem of AI taking jobs. Ubi or USI, how do we pay for it? There have been studies out there in Canada, in Europe, elsewhere on this. It's come out actually looking relatively promising to me, though we can also simply hack incentives in AI and blockchain too. Like, AI and blockchain are general purpose technologies. They're creating a lot of wealth.
00:59:37.438 - 01:00:07.946, Speaker A: So why not redirect the wealth that they're creating back towards helping to pay for Ubi and USI? So to me, that's kind of a no brainer. Like, imagine you have a fleet of cars, sort of like uber, but all of them are self driving. And not only self driving, but self owning. Right? So they start off self owning, buying with sort of this mortgage from Daimler or whoever they buy from, and then they run in this uber style fleet, fully decentralized, of course. And maybe they pay themselves off after two or three years. Right. After that, all their surplus.
01:00:07.946 - 01:00:32.742, Speaker A: But there's some for repairs, of course, but all the surplus would go to USI. And this car isn't so smart that it would care about saving for chasing its own dreams. It's not that smart, but it's smart enough to send its surplus to humans. And I like that scenario. And you can do that for cars. Not just one car, but a whole fleet of cars. And then the roads around and the electric grid and everything around.
01:00:32.742 - 01:00:46.266, Speaker A: And it's sort of this new layer of infrastructure for humanity. So we've had the sort of layer 1.0, call it nature 1.0, which is the trees and the sun and oxygen and stuff. Layer 2.0. I've been calling this nature 2.0 for years now.
01:00:46.266 - 01:01:10.126, Speaker A: And it's actually this right where it's all this extra wealth being generated, it's sort of this layer of silicon and steel that then extends this cradle of civilization for humanity. Right. And, yeah, to me, that's pretty exciting. There's an even better label than Nietzsche 2.0, which is called sovereign nature. And there's an initiative around that SNI, sovereign nature initiative, kicked off by Ed Hesse and others. And I really love the initiative.
01:01:10.126 - 01:01:39.478, Speaker A: I think it's great. So they've taken that vision and really ran with it. So going to then proof of personhood and worldcoin. So, to be honest, full kudos for Sam Altman recognizing this. In the early days of OpenAI, he got really worried about this, so he went and started worldcoin soon after OpenAI Elon working OpenAI got really worried about the second big problem and went off and started Neuralink to help know towards human superintelligence, basically. Right. They don't talk about that much, but that's the prime motivation of neuralink.
01:01:39.478 - 01:02:07.458, Speaker A: So kudos to Salmoltman and co. For doing Worldcoin. I think from a marketing perspective, they certainly grabbed the world's attention with these orbs, which immediately repulsed a lot of people. And overall, I think it's actually really useful that they're trying to solve proof of personhood. Obviously, in blockchain land, we have this idea of a civil attack everywhere. So if you have a useful, decentralized proof of personhood, then that can be very useful. Right.
01:02:07.458 - 01:02:35.450, Speaker A: Worldcoin itself is fairly centralized, but they've done, from what I can tell in my cursory review, a pretty good job on the privacy. Right. Everything they store, they do store centralized right now, but it's all encrypted or accessed via zero knowledge proofs, et cetera. So they're doing a pretty good job there. And I was skeptical. And then I was talking with this fellow, remcode Blonin experimenter, who I deeply respect not only from a technical perspective, but also values perspective. And to be honest, he changed my thinking.
01:02:35.450 - 01:03:00.630, Speaker A: He really opened me up, like, just teaching me about what they were up to with worldcoin. So I'm probably more positively biased than a lot of people in crypto. So thank you, Remco, for opening my eyes more. But also, I'm biased because I already believed in the idea of slash USI and all this. So, yes, there's still centralized aspects. Yes, it happens to originate from Silicon Valley, which a lot of people don't like what I tell the crypto world. Get over it, right.
01:03:00.630 - 01:03:21.466, Speaker A: It's a solution, and they're doing a very good go to market, so good for them. And in the end, maybe that'll be the prevailing standard. There's other potential competing standards. We'll see. And overall, it's the sort of thing that could get more fully decentralized over. You know, with the Remco blownins of the world involved in this, I'm more positive that it will be all right.
01:03:21.488 - 01:03:38.462, Speaker B: Well, Trent, look, thanks for that. I mean, we've covered so much. I appreciate your time. I know you are. I think you may be even on vacation at the moment in Latvia. So over an hour of your time on vacation is worth it in gold. But Trent's been great to catch up.
01:03:38.462 - 01:04:06.660, Speaker B: Really looking forward to seeing the success of predictor, continued success of Ocean Pro, this call. Thanks for coming on close.
