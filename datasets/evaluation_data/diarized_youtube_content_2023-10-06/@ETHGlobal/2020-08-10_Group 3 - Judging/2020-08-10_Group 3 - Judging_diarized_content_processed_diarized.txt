00:00:00.170 - 00:00:32.280, Speaker A: Show you. This is the real background. I should have known you were in the Manhattan skyline. Yeah, this is Peter Thiel's penthouse. They give you free access, I think, when you go through a fellowship, right? Yeah. Key for two years. All right, more than two years for you, though.
00:00:32.280 - 00:00:58.160, Speaker A: You did well enough that you could keep the key. I just never returned it. Okay, I'll stop recording, so I'll kind of get started. You're alive. Exciting. Well, we're not going live. We're recording to YouTube and we'll post later.
00:00:58.160 - 00:01:39.660, Speaker A: So it says it's live, but we're just recording. We'll get to the background discussion and the post edit. You guys are all on your judging links, right? Yeah. Cool. Actually, I can't answer that for everyone like that legitimate question. I assume, though, CarTech did a good job going, most likely. Actually, if you guys want to just quickly click on one of the stars for fuzzy ads just to contest something on my end.
00:01:39.660 - 00:01:56.002, Speaker A: Yes, I just clicked. Cool. Clicked, yeah, worked for me. Great. Yeah, we have this whole custom set up and everything. It's pretty cool. I really like this, actually.
00:01:56.002 - 00:02:29.340, Speaker A: Yeah. What did you use to build this? You don't want to know. You don't want to know. Some dayquery and no, it turned out really well, though, even if it was a this was your hackathon project for the hackathon is what I'm gathering. Basically. You that last message in the chat. You want us to turn everything off? Actually, panelists, you can leave your cameras on.
00:02:29.340 - 00:03:13.242, Speaker A: These are messages now to the attendees who will be promoted to panelists when they become when they do their demos. All right, 18 in the room. Let's kick this off. So welcome everybody. Today is our day two of hackathest judging. I'm Kartik, I'm one of the co founders of Meet Global, and I want to welcome all of you on the recording but also on the attendee list for our hackers to our judging day. So as you know, we're going through this whole week of judging for our hackath projects and we're super excited to actually show what everybody's done today.
00:03:13.242 - 00:04:28.270, Speaker A: And for those of you who are not familiar with hack FS, over the past month, we had 470 hackers from 50 different countries across working 19 different time zones, work towards making really interesting projects that use the best of tools and technologies available from the ethereum and the Falcon ecosystem. And they spent the last four weeks playing with what's possible, experimenting with these technologies, and then showcasing and submitting a project that we'll be seeing over the course of today and the rest of the week. And through all that hard work, I'm super proud to announce that we have the 132 project that have been submitted for this hackathon. And we're taking today to showcase you a good chunk of these projects and going through the judging on this call. So before I kind of kick off the actual demos. I want to briefly walk into some of the logistics of how the event was set up and how judging itself is going to work. And kind of the quick highlight here is that there's going to be 13 teams today that will be presenting to our judges where each team is going to get four minutes for demoing what they built, and a four minute time for Q and A from our judges.
00:04:28.270 - 00:05:06.670, Speaker A: And to minimize any AV issues, we've asked all of our teams to pre record their demos. So everything you'll be seeing today will be a video that's going to be played, but the Q A itself will be live with the teams. A quick overview of how the event itself was set up. Every project you're going to see was either worked individually or with a team. If somebody was working with a team, they could have up to five members in total. And as a requirement, all code that they had to build and submit for this event should have been written over the window of the hackathon itself. So everything we're going to see today was done over the past month.
00:05:06.670 - 00:06:06.290, Speaker A: And the only requirement for these projects to qualify for this event was that they must incorporate the tools and technologies, whether it's SDKs from our sponsors or the baseline protocols that enable these two ecosystems to exist from the filecoin. And the data ecosystem should be used for any big project. So a lot of cool things we're going to see today are a unique mix of the best of what decentralized storage and smart contracts can do. We're super excited to see what everybody's done in terms of how the judging is itself going to work. We have five categories here for our judges to rank every project on, and these categories are going to be on how technical, original, practical a project is, along with how easy it is to use in terms of the design and the developer experience for it, if it's a developer tool. And we also have a general category we call the wow factor to do a catch all for something that might have missed in the four categories above. And before I move on to our demos, I want to emphasize that this is not a competition.
00:06:06.290 - 00:06:47.054, Speaker A: These events are purely here for hackers to learn what is possible with the tools that are available for the decentralized web. And the hackers are here to share their excitement for what they've built and learned over the past month. And the judges are here to give feedback to these teams and just to kind of really nail that down. Not everybody here is trying to become a business. We really want this thing to be a place for everybody to experiment and learn. So everything you're going to see is very much done and presented from the filter of this is the creativity aspect of it and not the business applications of what this can be immediately after this event ends. So a quick schedule on how judging is going to go.
00:06:47.054 - 00:07:37.430, Speaker A: We have these 13 teams that are going to be presenting today and doing the hard job for the next 2 hours are our three judges. So I want to welcome Scott Moore from Jcoin. Dietrich from Protocol Labs and Aaron from Textile. They'll be here with us walking through all 13 demos. And with that, I want to call up our very first team, Fuzzy Ads to come on and kick off with their demo. So with that, let's kick off day two. Demo number one, introducing the fuzzy network, a decentralized ad network.
00:07:37.430 - 00:08:03.038, Speaker A: By being decentralized, the fuzzy network can function with less middlemen. Less middlemen means more profit for us and for the publisher. The profit comes from removing many third parties. Unlike traditional networks, we don't record user data. We don't need to. Traditional networks need this data to target ads. We'll use an algorithm to analyze the website's data and traffic to figure out which ad to show.
00:08:03.038 - 00:08:36.978, Speaker A: Publishers and advertisers can also trust our data. The project is open source and the data is recorded to the blockchain. Moving over to the advertisers side of things, I'll just run through the steps that you need to use the platform. The first would be to buy a one day slot from OpenSea, and the second would be to upload edit the information for the ad that you want to display in the slot that you bought. To buy an ad, you would go here. This is really just a list of the running auctions on OpenSea. If you want to buy a slot, you'd click here and you'd be redirected to the auction on OpenSea.
00:08:36.978 - 00:09:17.990, Speaker A: I'm not going to demo this process since I feel like it's pretty self explanatory. Once you've bought a slot, you would go here to visit, basically get a list of the slots that you own and this is how you edit the information for the ad that you want to show within that slot. So here, those were just two slots that I already set up. But yeah, if you wanted to say, change the image that you're displaying that is being displayed by the publishers in this network, this is where you'd do that. And so you'd update the form and then submit the changes. What would happen here is that basically you would edit information that's being held in a smart contract and the smart contract is what the SDK is drawing from. And so this would update what the publishers are showing.
00:09:17.990 - 00:10:08.086, Speaker A: Now, we did not get to setting up tracking views as well as the payment channels, but basically this is what we envisioned you'd be able to use to see the results of past advertisements. Moving on to the publisher side, really there is only one step, and that was to embed the SDK. I'll leave this up to Elijah to talk about a little bit more. What we have here is a link to the SDK, which is hosted on GitHub and shows the steps that you need to take as a publisher. And then again, although we did not get to setting up the payment channels in history, we would imagine this page being basically showing a record of the payments that you've received as a publisher on this network. So imagine you're a publisher and this is your site. It doesn't have a ton of stuff on it right now, but you want to make some money.
00:10:08.086 - 00:11:31.120, Speaker A: So what you'll do is you'll add into the head a script which you'll pull in from Pinata's IPFS gateway. This fuzzy ads SDK should now enable you to pull in to use this web component called Fuzzy ad, through which you would pass in an ad unit ID, which helps us identify the specific ad slot that you've created or that you've minted on our platform. And that ad slot will serve the right image to display to the user who has landed on your site, which is right now the filecoin ignite banner. If we click on it, it should take us to the filecoin homepage, at which point, through a payment gateway, the payment from the advertiser will go directly to the publisher for having a user having clicked on that site. And through this process, we've been able to cut out any middlemen who are siphoning data from the user and from the publisher and return all of that value to the publisher instead. And so, yeah, this is Fuzzy Eyes. Thanks for watching.
00:11:31.120 - 00:11:52.134, Speaker A: Okay, cool. Yeah. Judges, you guys can jump right into Q A, everybody. Thanks. Thanks for the demo. Yeah, very cool. I had a question.
00:11:52.134 - 00:12:16.378, Speaker A: I'm sorry. No, go ahead. Were you jumping? Yeah, I had a question. At the very beginning you said that there's an algorithm to analyze site traffic to be able to determine value. Can you explain that algorithm a little more? Sure, yeah. That's one thing that we envision, creating that in the future. But it kind of is very important to us that the user maintains privacy.
00:12:16.378 - 00:13:31.826, Speaker A: One of the big problems with the current networks is that all of the information that they use to place an ad is based on user information that the user didn't give them, they actually took it from them. So our thinking is that instead of using user information like the user's buying history and their location and their demographics and all of that, we would kind of create an algorithm that would scrape website information and analyze the information so contextually it would know what the website was about. And then we could couple that with information about the traffic that the website gets, which we could also grab and kind of create our own scoring system in that way. So just to clarify, the traffic information is derived from the loading of the ads themselves? No, the traffic information. Well, we could use that information to supplement, but I think we can also get traffic information in other ways. Okay, so out of band you get information about the website and spawn? Yeah. Just to add to that, there's explicit information about the ad and the advertiser.
00:13:31.826 - 00:14:55.060, Speaker A: So the geographic information about the advertiser would be given to us through the platform. And then from the scraping of the website, you'd have some grouping or some sort of clustering of where in this high dimensional space that website belongs in relationship to other websites. And the distance between the relationship between the ad itself and the information about the ad would be placed in that context in the same kind of high dimensional space to help determine the relationship or the closeness between the advertiser and the ad and the actual website that would also contribute to the score. Cool. Nice. I heard quickly in the video you guys are using Pinata to store some JavaScript assets load. Are there any other kind of like underlying platforms or tools that you're using that we should be aware of? Yeah, one of them that we were thinking of hopefully getting to using was Threaddb to help us store the actual traffic information as well.
00:14:55.060 - 00:15:37.326, Speaker A: Is your question more kind of in context of how would you store the SDK? No, just more just like technically, what tools are you using to build this thing? Got you. Just quickly there's about 45 seconds. Yeah. I'm not sure if this can just be appended to that question, but like the smart contract component in particular, how are you leveraging that in the context of the revenue generation and distribution? I'm not sure if you can capture all that in the one question. Right. We didn't get to setting up the payments channel part, but I just want to note that we're also using IPFS to store the metadata for the ad. So the IPFS link is being stored in the contract and then that's what the SDK is drawing from when it pulls up an ad.
00:15:37.326 - 00:15:49.250, Speaker A: Cool. Unfortunately, I got to cut you guys off, but thanks so much presentation. Thank you. Great. Thanks guys. Awesome. So the next team up is Vallast.
00:15:49.250 - 00:16:27.870, Speaker A: You guys are ready, you can share your video. Hi. Hecafest. I'm Alec. I'm Zach. We're Akashic Technologies, and our project is called Vallast, which is a software firmware, really, any arbitrary binary data distribution system that acts as a secure notary. And effectively, we're using the ethereum blockchain as a source of truth for the latest software releases that a company can use if they wanted to store it in a decentralized store rather than hosting it on their own server.
00:16:27.870 - 00:17:23.038, Speaker A: Hi, hecafest using that we can actually in the future filecoin networks. Don't know what happened there. Sorry. So we're effectively using Vallast Core as the bridge between the ethereum blockchain as well as the IPFS in the future filecoin networks, and we're also using Next JS as a Rest relay and a web UI front end. Using that we can actually bridge back to traditional systems via the HTP relay or directly to your Web Three provider using the Web UI. And eventually we will have a CLI that will support CI CD pipelines. We used Open Zeppelin's access control libraries in order to establish the different access control features.
00:17:23.038 - 00:18:11.998, Speaker A: But effectively one is able to create an organization, create repositories under that organization and publish software releases to those repositories. And then clients can subscribe to those repositories to get the latest release. So to check out the API, we can actually go to a URL that looks like this. This is the organization slug, this is the repository slug. And using this we could fetch the latest metadata and it will return to you the IPFS hash or the file coincid that you stored in the future. And if you go to latest on the repository, then you can see the latest release hash that they have published. And in the future we will actually implement tags, filters.
00:18:11.998 - 00:19:32.472, Speaker A: It is almost implemented, but time constraint. And if you actually go to this Uri here on the IPFS gateway, for instance, you can actually see this is our Valis library published here. So if we wanted to actually upload another release here, we could do so upload it to Pinata, copy that hash, use our good old Sid converter, copy that. And we could even do an NPM install. I'm going to switch my screen here, my terminal window, and do just that. And then using this, basically organizations can distribute their software on a decentralized repository and run their own relays that they can use then if they don't want to use our first party relay. And currently our API is actually hosted on Netlify and since it's written in NextJS, all you need to do is run the relay yourself if you don't want to use our front end and plug in your own Web Three provider.
00:19:32.472 - 00:20:15.400, Speaker A: And if you'd like, even your own IPFS node. So as you can see, it was able to successfully pull the NPM package and install it, which is great. And the NPM package allows you to actually tap directly into Lib and talk to the smart contracts in IPFS and abstracts away many of the features. So, thank you. All right, I'm just going to stop that there. The couple of things I wanted to note just real quick, that original architecture is a little bit outdated. We are calling IPFS directly rather than going through powergate and at the end there, actually forgot to submit the transaction and publish a new release which would then show up in the API.
00:20:15.400 - 00:20:59.400, Speaker A: We also deployed IPFS Vallast IO to fleek so you can access the IPFS only version without the API that just talks to MetaMask and other Web Three providers directly. And eventually we're going to implement a redirect to an IPFS gateway so that you don't have to manually enter the hash into a gateway yourself. That's kind of the idea. I really like this idea, actually. This is really cool in terms of and this might be a bit too specific, we can dive into more general questions too. But the Credentialing, how are you planning to, I guess, extend that? Right now it's using open zeppelin. That might be a bit tough for enterprises.
00:20:59.400 - 00:21:55.144, Speaker A: What's your sort of plan for that in the future? Yeah, definitely want to simplify the organization and repository management as much as possible. Right now we just basically all you need to get started is an ethereum address and then you could create an organization yourself, you become the administrator of It, the web UI, we're intending to abstract away all of that. Basically all you would end up needing is ethereum. And then from there you could create an organization, add other keys to the organization. We have different roles, organization level and repository level, and then events for each of those levels as well. So you could subscribe specifically to a repository or specifically to an organization. And we try to follow a similar namespace as like GitHub with organization or usernames repository and then fetch any of the latest data there.
00:21:55.144 - 00:22:36.664, Speaker A: So the idea is if you're an IoT device, all you would need is a simple web3 filter subscribe to the repository that you need and then next time you get an event, you can execute any auto update features. So the kind of idea we have for this is that authentication is one of those things everyone redoes over and over again. So is auto updates and firmware releases and they all tend to be hosted on centralized servers that could eventually go down anyway, so might as well kind of bridge that gap. Awesome. Just a quick note, we're two minutes left. Thanks for the demo. I really appreciate the attention paid to developer experience of being where developers are and how they work.
00:22:36.664 - 00:23:07.170, Speaker A: One of the questions that I had was around there seems like there's a hybrid topology here. I mean, some of that is around meeting developers where they are. One piece of feedback would be to include kind of the overall architecture diagram and kind of topology of the stack and where the commandments are. And I think that would make it easier to see the changes that you said at the end. You said the architecture has changed now and those changes are interesting enough. I'd like to learn more. A visual of that might tell a lot in a short period of time.
00:23:07.170 - 00:23:39.020, Speaker A: Definitely. Thank you. Yeah, we originally came up with that architecture in the beginning and then things kind of changed, so we'll definitely be updating that. Any last questions or things you guys want to highlight? It's about 15 seconds. Sorry, 30 seconds. I had a question about what were your thoughts from a developer adoption standpoint? Things like this exist, but it's really hard to get the mass movement of people to adopt it. Yeah, definitely.
00:23:39.020 - 00:24:13.826, Speaker A: That's kind of why we went with the Core Lib piece to abstract away as much as possible so that we've met with several people who actually have had this kind of use case necessary. And we've seen this kind of thing need to exist a few times. So figure, why not open source it and see where we could go with it. Sweet. Thanks so much, guys. Cool. All right, so the next project after Ballast is Eradinos.
00:24:13.826 - 00:24:51.660, Speaker A: It's a Greek name. Verifiable credentials on ceramic. So whenever you guys are ready, you can play your video. Is there audio on this right now? It yes, there's audio to be there. We tested that. I tried this one. Wait a second.
00:24:51.660 - 00:25:57.776, Speaker A: Start again. Okay. Welcome to the little Arduino demonstration. A project that service protocol not enough for ICS and clients. Electronic, it's just popular because everything else in the browser, of course we want to use a wallet. So what we get private space called visibility. And that space is starting the seat for the identity wallet.
00:25:57.776 - 00:28:51.470, Speaker A: That's because having a custom biofrap Credential, we tried to make this as simple as possible to do. Actually, we only give the Credential subject. And so this person assigned that we're creating the tools in mind in the Genesis document and extend the identity wallets to ID provider method that we're calling because this is not yet ready tomorrow. It's not yet understood. Everybody can use the case. This one is going on. Please, people say about the time, content, checking the public here and this is the first step I would try.
00:28:51.470 - 00:29:34.972, Speaker A: Awesome. Just a quick note. The audio was a bit jumbled, but I think we kind of got a general idea. I'm going to give you guys an extra, like, minute, the Q A to help with any translation issues there. So, yeah, you can continue with Q-A-I saw the repo Phil did. Can you explain what that particular component well, this was the first idea how to name the project, of course, because the first idea was to put filecoin support on Ceramic. But as it turns out, this is what the people do anyway.
00:29:34.972 - 00:30:04.816, Speaker A: And this is why we didn't follow it up. And then we came up with a weird name, Eridanos, because oh, there's a story behind it. It's in the description Greek mythology. Go ahead. I have a quick question. Sorry. The main point of the project is to demonstrate how Verifiable credentials can work on Ceramic.
00:30:04.816 - 00:30:49.524, Speaker A: And maybe it didn't come out in the video that well. Everything is just running in the browser. So Ceramic is currently at the point of building a node JS application to build that ceramic network. But we actually run the node in the browser. So we are losing some parts of functionality because I think the anchoring service is not running right. But it's quite nice because now you can use them to do things like recoverability checks and everything, or you can even revoke Verifiable credit interest from the issuer by themselves because you can prove that the new document is not what has been proven in the first place. In the document, you mentioned a few challenges that you ran into, just like, for example, to clone and build the port sub modules, like the remote system stuff.
00:30:49.524 - 00:31:43.844, Speaker A: I guess I'm curious about can you dive into a bit more about how did you resolve that challenge of making this work on remote systems? You mean like cloning the repos on remote systems or do you mean yeah, exactly. That's quite funny. I mean, of course you use Git sub modules for that and then you bind everything together in your GitHub repo. But then we are using Fleek Co for hosting, and then Fleek has to build all the stuff and Fleek has to get all the dependencies. This is actually quite straightforward, but of course you have to write all the build scripts and you always have to pull all the upstream stuff because ceramics is also proceeding at the same time. This gave us some hard time because they also advance quite a bit in their way that we're doing, for example, in the IDX protocol. This is what we're really excited about.
00:31:43.844 - 00:32:17.584, Speaker A: But I think they're not really concentrating on the VC aspect of it. They're more concentrating on the schemas. And yeah, we tried to take as much of them as we could, but the VC stuff is really unique to us, I guess. Quick note, we're at the eight minute mark, so you have your extra 1 minute now. Yeah. Are there any other questions? Because what was really hard was like, binding. I mean, ceramic is a three box project, right? So it comes from the makers of three box.
00:32:17.584 - 00:32:35.444, Speaker A: And now the three box people are moving over all the stuff to ceramic. But this is not really compatible in that sense. So you cannot just use the identity of three box in ceramic, even though it looks like that. So you could use the identity that's injected. But unfortunately, ceramic is not able to resolve that. This gave us quite a hard time. So this is why we're doing a little trick here.
00:32:35.444 - 00:32:55.630, Speaker A: I explained that in the video in the beginning. This is not supposed to go live at any time, but we're pretty proud that we're doing it that way. So it's completely decentralized in some sense. I like that it nudged it a little bit closer by running the node in the browser. Gives the end usernap developer a little more control. It's nice. Absolutely.
00:32:55.630 - 00:33:08.032, Speaker A: Cool. Awesome. Well, video is decide. Great demo. Thanks, guys. Thank you. Cool.
00:33:08.032 - 00:33:39.576, Speaker A: So the next group up is Cadbury. I'm going to be playing the video for Cadbury, so I've seen bits of the video. The first minute is particularly interesting. Cadbury's open, neutral, borderless decentralized and censorship resistant meetings. So it's kind of a video hack and they have a video that demonstrates their video in the hack. So I'll get it set up and then start the timer. Okay, everyone can see this.
00:33:39.576 - 00:34:36.440, Speaker A: We're good, let's go. Here's to the craving ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes the ones who see things differently. They're not fond of rules and they have no respect for the status quo. You can quote them, disagree with them, glorify or vilify them. About the only thing you can't do is ignore them because they change things. They push the human race forward. And while some may see them as the crazy ones, we seek genius.
00:34:36.440 - 00:35:24.696, Speaker A: Because the people who are crazy enough to think they can change the world are the ones who do. Hi, AYUSH. What's up? So did you find it difficult to join the meeting? Hi Sushmit. No, it was frictionless. Very smooth. Right? So I'll explain you how this meeting is working. So basically what happened is that there is a signaling server that helps my browser to discover your browser and once that event has happened, the WebRTC takes over.
00:35:24.696 - 00:36:24.140, Speaker A: So all your video, audio and your data that is your chat and the artifacts get directly streamed to you. So it's browser to browser and is handled by Rtcps Connect and RTC data channel. I'll send you some chat here and I'll also try to send you one image too so see how seamless it was. So basically what we are making is that whenever four or more peers joins A, mesh networks get set up and these are like every audio and audio are transferred to everyone. We have discovered that we need to come up with a protocol wherein these signaling servers, these FCUs and MCUs, all of these can be decentralized. Once we have this protocol, it can power everything like it can power meeting, it can power streaming, it can power OTT platform and it can even power broadcasting platform and our protocol would be actually complementing the lightbear two and the filecoin protocol itself. It's really amazing product demo.
00:36:24.140 - 00:37:23.660, Speaker A: Can you also explain me the exact mechanics of how it works in the form of slides? Let's go to slides then. Sure then bye. So ayus we will see meeting mechanics here our front end is on React JS and internally uses raw client side WebRTC we establishes mesh network for all our connected peers we have come up with our own custom signaling server that can be linked and deployed to heroku in within a single click. Signaling server handles all of the IC and STP candidate and gives control to WebRTC. The WebRTC then handles peer to peer audio, video or data streaming via RTC peer connect or RTC data channel. We are hosted on IPFS via fleek utilizing unstoppable domain our ratings are handled via Ethereum smart contract and our meeting artifacts goes on the pilecoin via textile. Power gate.
00:37:23.660 - 00:38:06.464, Speaker A: So, future work, we need performance, scalability and decentralization. So we need to come up with a protocol wherein we'll have the signaling servers, MCUs, SFUs, transcode, us Orchesters as an actors complementing the file coin or the lever protocols. Okay, yeah. Open it up for Q and A. Go ahead. Yeah, I'm super excited. That signaling server is often the point that people put to the decentralized service and think about as the afterthoughts the centralized part.
00:38:06.464 - 00:38:59.012, Speaker A: So the fact that you're picking up the signaling server and leaning in hard on how to solve that problem and adding more decentralization, more value to it, maybe even standardizing that pattern, I think is exciting. Can you talk about of those remaining centralized parts? Have you looked at ways how to distribute or break apart the centralization aspect of signaling server? That one point that connects those two pieces, right? So hi guys, it's nice to meet you. Also, basically we have looked into those features. How do we decentralize a centralized component? So it was quite obvious. So if we see to scale up a meeting, we need to have a SFU that is selective forward unit or MCU to scale up the meetings and even the signaling server. So these are kind of central component. And if we had looked into live P two P web RTC also.
00:38:59.012 - 00:39:56.500, Speaker A: So even live P, two P is using some kind of signaling mechanism, right? So we can take inspiration from Lipier protocol. So what Lipier does is that they have an orchestra and this orchestra is directly responsible to handle the transcoders, right? So these transcoders are not protocol aware. So only these orchestras are protocol aware, right? So whatever our suppose, if we keep a mechanism, if we come with a protocol wherein these orchestra would be responsible for the service and the availability of signaling servers and the MCUs or the SFCU. Now, these orchestra can work parallel with the orchestra of the light pair also. And this can also work with the textile power gate also to all of the meeting data that is being generated can be stored for the further kind of meeting, like YouTube for meeting that has already happened. So that can be picked up from those. So it's like kind of orchestra model.
00:39:56.500 - 00:40:56.196, Speaker A: So the orchestra would be like protocol aware, but these signaling servers and all of these would not be protocol aware. Right? But orchestra would have to stake within the protocol and he has to be responsible for the service and the availability of all of these signaling server SAQ and other services. So this is the we are approaching and we are also looking for some suggestions. Also there's about a minute and a half, two minutes left. Did you find working with the WebRTC tools, were they basically just exactly what you needed or did you find that there's anything you need or want to change about that stack of technology in order to make it work better in this decentralized solution? WebRTC has played a very good role because it from browser to browser. So this perfectly goes into our case. Now, for signaling, also, we are using kind of WebSockets internally.
00:40:56.196 - 00:41:11.292, Speaker A: So we want to ship this architecture to the live P. Two P WebRTC. And live P. Two P WebSockets, too. So regarding WebRTC, as of I don't think we require any improvement. It handles pretty much good. That's great.
00:41:11.292 - 00:41:35.990, Speaker A: Yes. So you mentioned you're pursuing, in some sense a similar model to Live here. That's like one of your down the road sort of goals. I think you may have described this a bit, but I'm not sure I caught it. Can you elaborate a bit on that? Sorry, I didn't get your question. I didn't get your question. Yeah.
00:41:35.990 - 00:42:06.896, Speaker A: How are you, I guess, addressing scalability and it sounds like you're pursuing that in a similar way to Live that is that correct? No. So we actually would be complementing Live peer protocol. So if you see, Lipier is actually designed for broadcasting protocol. So in our initial approach, we wanted to build decentralized application on top of Lipier protocol. So we actually contacted the co founder, Eric of the Lipier. So he himself recommended WebRTC. So Lipier is actually designed for one too many broadcasting system.
00:42:06.896 - 00:42:45.932, Speaker A: So it has a bit of a lag and meeting is kind of its dynamics. Like it should be real time. Even a two second lag cannot like it's not up to the mark. So we had incorporated Liver protocol also. So the protocol that I'm dispatching that would be there, it should be like it will be handling the Sapus and the signaling servers. But this orchestra of the MCUs and Sapu can directly talk to the orchestra of the Live Gear protocol, so all of the transporting mechanism can be pushed to we would that's why I said it could complementing Live here and complementing filecam protocol. Awesome.
00:42:45.932 - 00:42:56.210, Speaker A: That's helpful. I'm going to stop things there. Thanks so much, guys, for the demo. Thank you. It was nice meeting you all. Thanks, guys. Awesome.
00:42:56.210 - 00:44:08.820, Speaker A: So next up on the live stream, we have Lee Bartaz censorship resistant, decentralized social network. I'll let them take it away with their video and then we'll do Q and A right after. Just quick check, is it meant to be audio or no audio? That is supposed to be audio. Okay, I'll give you an extra couple of seconds to kind of figure out the audio. Making your world unstoppable. Libidas is a decentralized censorship resistant social network that allows creators to create and share videos and articles, interact with their audiences via live streaming, and talk to anyone, anywhere via P two P calling. Libertas is powered by the community, and it brings decentralized governance in the hands of the creators.
00:44:08.820 - 00:44:52.788, Speaker A: Here they can vote for new proposals, take down videos all by the community. And for the community, Libertas brings a censorship resistance the UI and its data is deployed and replicated across IPFS, fleek, pinata and textile, making it decentralized. It also makes the DNS decentralized, bringing in and deploying it on unstoppable domains. It is also accessible via Tor, making it privacy focused and truly unstoppable. Let's jump right into a demo right off the bat. As soon as we click this call, we see three options videos, live streaming, articles and calls. I'll talk about each one of them.
00:44:52.788 - 00:45:32.928, Speaker A: First, we've got videos. Here we can see the videos that have been created by the creators on the platform. By clicking on any one of them, it will take you directly to the video page. Now, this has been built in a way that there is no direct interaction of web Three until and unless you have to fire a transaction. So as soon as you're watching a video, as soon as you hit play, there should be a transaction that fires that lets you view the video. Now, I've already watched this video, so it won't ask me to pay for that stream again. But what happens in the backend is as soon as I open another video that I haven't watched yet, it loads up that video from IPFS, creates an ad that has been sponsored by the video.
00:45:32.928 - 00:46:17.008, Speaker A: As soon as I hit play, it fires in transaction via MetaMask to create a stream directly from the advertiser to the video creator or the content creator. And that stream is brokered by the Libertas contract. Now, here you can also view the video. You can see the number of views you can subscribe to the creator. As soon as we hit play, it fires a transaction in the backend and it creates a stream via Meta transactions. Now, the second thing, let's talk about articles. All the articles that have been created by creators on the platform, simply click on them and it gives you a full markdown experience, where it fetches the article from IPS and renders it completely without the need of any interaction with Web Three.
00:46:17.008 - 00:47:04.320, Speaker A: So if I were to copy the link of the article and paste it in a non web Three browser, it still works. You can also tip a content creator if you want, and similarly, you can view the entire article experience without the need for any interaction. So it gives you a seamless web Two experience. Now, let's talk about live streaming. Live streaming setup is pretty simple. All you have to do is click Live and you get a control panel wherein you can enter the title of your stream, enter the title duration, the time period, whether it's paid or not, the rate at which you want to earn, and simply hit Update Stream. What happens here is that it connects to a peer JS node in the backend and gives you an ID.
00:47:04.320 - 00:48:00.928, Speaker A: You can simply copy that ID and share it with anyone you want to be allowed to join your live stream. Similarly, you can click on calling works in a way wherein you can in a simple fashion wherein you can connect to obviously appear in the network, and as soon as you connect it to the network, simply copy it. And if I just open up another window again, so this would be another person that you want to talk to, I simply hit Allow, give my permissions, copy my ILT, hit call, accept my call, and boom, we're connected on both sides. So this was libathus, making your word unstoppable. Thank you. Well, nice with you guys. Like I mentioned, I'll give you an extra 30 seconds because of the audio thing.
00:48:00.928 - 00:48:54.028, Speaker A: Say you got four minutes for Q and A. That was a fully featured solution. It's really interesting. I can imagine there's a whole lot behind that. Can you talk a little bit about what the are there centralized components? You mentioned peer JS in there. What were the challenges in putting things like signaling servers or peer JS or some of these other things onto places like fleek or possibly centralized, surely? So the primary issue that I faced with PJs is that once you're actually developing on the platform, they give you a default service, so they have the default stunter service that actually created connections all around. So that actually gave me a very disturbed connection, which wasn't really stable.
00:48:54.028 - 00:49:56.576, Speaker A: So I had to set up my own stuntern servers on Heroku. So I have a couple of them running right now, which actually relays connections all across all the network. So this actually helps me scale up the network also because the live streaming part and the peer to peer calling part is actually powered by the same WebRTC servers, so it allows me to actually connect to N number of people participating in a call for live streaming. But if there is a direct one person to one to one call, we can participate in as many people as you want, but they can also be one. And similarly, one of the things that I primarily wanted to do with us was actually to have a seamless reputable experience, but at the same time having the entire stack decentralized. So as the storage here, the IPFS and the data itself are deployed over IPFS, they're replicated over Pinata. The data is stored in textiles, textile buckets, so we've got the entire storage there decentralized.
00:49:56.576 - 00:50:44.230, Speaker A: We've got the DNS decentralized. So we've got unstoppable domains, we've got accessible through Tor also. So for people who actually want to publish, the entire network allows you to publish, whether it may be videos or articles, completely anonymously in a censorship resistant manner. So let's say even if you don't have to publish a transaction on the blockchain, I'm using Biconomy infrastructure, so you don't even have to worry about having anybody else have questions. I have another one if you don't. Sure, yeah, go ahead. I saw that in your advertisement demo.
00:50:44.230 - 00:51:38.692, Speaker A: The advertisement was shown, but kind of without the user's consent. How does a system like this avoid the weaponization of advertising against dissemination of information problem that we have today? Sure thing. So rather than actually having the advertisement show up in a sort of an algorithmic fashion, we give creators the power to actually choose the advertisement. So if I was an advertiser and I want to show my advertisement on your video, I send a request with my entire ad. So as soon as I create an ad, each ad has a specific rate associated to it and a budget associated to it. All the data with that ad, the link, the entire information is sent to the content creator. Now it's up to the content creator whether or not they actually choose to show that ad in front of their video besides their video.
00:51:38.692 - 00:52:38.090, Speaker A: So what this allows us to do is that let's say if I, as a content creator, decide to take Google as a sponsor and show their ad beside my video now, because that ad cannot be changed once published, I know exactly what is being shown beside my video. And rather than having the accountability with the entire platform, we give creators the power to actually monetize their content. So as soon as you click the video, a stream is created directly from the advertiser's account through the content creator's account using Xavier. So that streams money or at the rate at which the advertiser decides, directly to the content creator for as long as the video is being watched. So as soon as you go away from the video, the stream stops and it sends the transaction behind to stop the transfers and creating the experience. And those components are built as well, right? You built that? Yes. You can actually go right now to the link and try it.
00:52:38.090 - 00:53:38.232, Speaker A: Maybe have time for one final quick question. Is there any concept of discovering your social contacts or do you have any thoughts about how to do that in the future? It's something I've always seen as a challenge when it comes to decentralized social networks. So the way that I tried to do it was I have a smart contract. The main primary levels are smart contract and then there's another contract that the smart contract itself calls. As soon as I, as a content creator, publish a video, the smart contract itself calls another contract, which relays all the event handlers. So what this sort of does is that it anonymizes the subscriptions. In a sense, if I am subscribed to a creator, people can't really see that I am subscribed because the event of subscription was actually fired by a different smart contract.
00:53:38.232 - 00:54:21.356, Speaker A: So it actually uses the message tender problem to its advantage and anonymizes where the actual transaction is coming from. So the discovery happens in a way, is that if I am subscribed to five content creators, whenever those content creators create a video, that event is fired. So I'm able to correlate my press and my subscriptions and with the data that is fired by the smart contract. So as soon as I know that another creator actually published a video or published an article, I can actually get a notification on my phone using DPNS or any other notification services. I got to end it there. But thanks so much, man, for the demo. Thanks for the good questions.
00:54:21.356 - 00:54:37.008, Speaker A: Thank you. Thanks. Cool. So the next group up is building Wrapped Filecoin. So this is actually one. I'm kind of quite personally interested in seeing how this all works. It's something I wanted for a personal project.
00:54:37.008 - 00:54:54.030, Speaker A: So, yeah. Wrapped filecoin. Let you guys start it off. Hi. We are team wrap FS. I'm Nadzareno. And I'm Christian.
00:54:54.030 - 00:56:15.942, Speaker A: We worked on Wrapped Filecoin we thought was a great idea for such many application in DeFi and would be cool to be seen as collateral MakerDAO or to be swapped on uniswap to side a few. For the first iteration we opted for a custodial approach whereby leveraging on textile powergate API we created a custodial wallet on the back end and also we implemented near C 20 token deployed on Coven. Also, we implemented filecoin wallets like MetaMask for filecoin. So now Chris, I will show you a quick demo. Well, starting with our UI, we have this approach to our simple wallet where we can create a new address. We get the token which is like the private key and the address we can check balance which in this case is going to be zero and we can send file coins to any other wallet. Now we can check the balance of another account that we already have created before where we have zero point 99 five file coins.
00:56:15.942 - 00:57:01.350, Speaker A: Right now we are going to use our UI also to grab some of those file coins into Rapid File coin. So here I can say that I can grab 0.1 file coin. The wallet that I'm going to use to grab is the one that I've previously copied and the Ethereum address where I want to receive my file coins, my Rapid file coins is this one. So when I hit Wrap, we gave the user the instructions to send those 0.1 file coins to our custodian wallet. So now we can head to the wallet and send the desired falcon to that wallet.
00:57:01.350 - 00:57:32.034, Speaker A: So we want to send to this wallet the amount 0.1 and the private token of our account, which is this one. When we hit send, we get a success message. Now we have to wait for the transaction to be confirmed. It's just confirmed. We can see the transaction on Ethercan. Also just to mention that the user interface has been deployed on IPFS via Flick.
00:57:32.034 - 00:58:13.010, Speaker A: And also we are leveraging on Lotus to talk to Falcon testnet. Right now we have the 0.1 Rapid Falcon Minted we can see on our MetaMask. We have already received those 0.1 okay, now we can take some of those rapid file coins and unwrap it into file coins again. So in this case we can say that we want to unwrap 0.2 rapid file coins and the address where we want to receive our file coins.
00:58:13.010 - 00:59:14.214, Speaker A: So when I hit on Grab, this is going to say me that I have to connect with MetaMask and what's connected. Now I can sync the transaction on grab those file, we wait for the transactions to be processed and now it's processed we can see also on Ether scan now and we get the success message now if we check our balance and see that it went from 0.8 to 0.6. So now we have unwrapped our 0.2 Falcons and our balance has updated. Thank you for watching. Nice YouTube recommendations at the end.
00:59:14.214 - 01:00:03.346, Speaker A: Yes. You guys got four minutes for Q A. Yeah, I mean, definitely this is super necessary. What sort of are your next steps? Like, the custodial piece obviously is one that comes to mind, but what else is sort of on your so yeah, for the next step, we would like to develop further the project towards a non custodial approach by leveraging on filecoin smart contracts. So having a fully decentralized application, any other UX or major changes that you foresee or is that kind of like sure. After that you're sort of in place? Sure. I forgot to mention that we would like to work also on the wallet filecoin wallet to create like a MetaMask for filecoin.
01:00:03.346 - 01:01:12.274, Speaker A: So to wrap the code into a plugin for Chrome to easily access filecoin testnet and mainnet as well. This UI wallet was very initial approach for testing the wrapping and unwrapping. But our intention also is like a separate those things for the wrap and unwrap interface. Yeah, I think the main question I had was the custodial part of it. It seems like as far as I understand, the filecoin smart contracts are a little ways off getting this is a great way to kind of demonstrate what the potential power is. Can you talk a little bit about what the hard challenging part was from a smart contract development standpoint? From the smart what do you mean? Like Ethereum or in seems pretty the UI seems pretty easy to follow unwrap. Maybe my question is what was the hard part and where's the danger? Sure, the hard part just intro.
01:01:12.274 - 01:02:04.674, Speaker A: The hard part was like to check for transaction on filecoin. So we were first using textile powergate API. We couldn't find a way to watch for transaction and then we used a Lotus node to do that. Yes. Once we get to use the Lotus node and actually read the transactions, we figure out how we can actually listen for a given exact transaction between two address, then get the message and see the details of that message to see the actual value that have been transferred. So we can actually mint the token in Ethereum Network. That was the hard part because at the beginning we were using Powergate, but there is no clear way to find and read all the transactions in that sense.
01:02:04.674 - 01:02:33.680, Speaker A: So we have to take the Powergate for the front end API at the wallet API and use the Lotus node to read the actual blockchain of icon. Cool. Yeah, that's really great. You guys figured out how to do that and it's something I'd like to add to Powergate. So I'll keep in touch with you guys and we'll make that happen. You'll have to teach me more about that. Thanks for your support.
01:02:33.680 - 01:02:58.290, Speaker A: Of course. Awesome. Thanks guys, for the demo. Thanks. All right, so quick note after this next project, we're going to take like a five minute break, give the judges a chance to kind of collect their thoughts and also me a chance to make some coffee. And so just a heads up on that. So, next team up is Shop FS.
01:02:58.290 - 01:03:18.060, Speaker A: They're doing an interactive IPFS. Ethereum marketplace. So if you guys are ready, you can share your screen and get the video going. All right, also a quick note on these guys. The video is a bit longer than usual, so the Q A will be a bit tighter than usual. So just maybe think about the questions a bit more during the video. All right, let's go.
01:03:18.060 - 01:03:50.786, Speaker A: Hello everyone. So today we'll be presenting our submission Shop FS, a decentralized data marketplace. So here is our team. Now we'll straight move into our introduction. Lately we have seen the user adoption across web3 has been phenomenal and there has been a need to have an end to end solution which involves monetizing. And we hope that Shop FS can fill that void. And lately this is a trend that's been for the data market size and as you see from 2011, it rised linearly.
01:03:50.786 - 01:04:46.210, Speaker A: So let's move into our value proposition. Firstly, enable creators to monetize their content using decentralized technologies. Then we have a very easy to use UIUX for both parties and a subscription model on top for a better user visibility and social profiles so that you can identify the sellers and the buyers that you want to interact with. So let's move into the architecture of the tap now. So we start off with our Shop FS Smart Contracts, which are directly in communication with Sabria Pip 1640 contracts to cater our subscription model needs. Then we have seller and buyer, each of them running a Fleet Space Demon instance at their end to cater to our storage needs and then sure that the buyer has private access to the content that he buys. We have a key management service which is monitoring our smart contracts.
01:04:46.210 - 01:05:07.840, Speaker A: So this is our tech stack. We use Fleet Space Demon to store files in private buckets and we use Graph Protocol for querying data. We have three bots for social profiles and a subscription model. We use Sabre contracts. Yeah, so we can move on to the demo. Hello guys. This is the landing page of our application.
01:05:07.840 - 01:05:29.602, Speaker A: We want to make it very easy for our users. They just have to upload a file, describe it, and they can start earning fees for it. The first step is to upload a file. Uploading a book on dows. So in the background the file is being uploaded using freak space. Even file is successfully sold. I can go to Explore page and see all the files for sale.
01:05:29.602 - 01:05:55.002, Speaker A: Then I can go to the File details page and buy the file. This involves two transactions and the file is bought. Now I can download the file. File gets downloaded locally within the space demon and the local file location will be put up. On the display. I can see my files which have a book for sale, the files I have bought and my subscribers. I have no subscribers right now.
01:05:55.002 - 01:06:15.694, Speaker A: My subscriptions, if I have any, and I can update my subscription info. The minimum number of days the user must subscribe is five days and amount per day. So one die per day. So I confirm the transaction. My subscription info has been updated. Any user who subscribed to me can download any of these files without having to go through the steps of buying it. Before that, click on the subscribe to subscribe.
01:06:15.694 - 01:06:37.414, Speaker A: He has to amount of days you want to subscribe to. Minimum number is five. I said before. So let's say I want to subscribe for ten days. That means I have deposited ten die right now. So how it works is we use safe layer contracts in the background. The contract will calculate how much die per second the seller must get and accordingly stream that money over the ten day period subscription is created.
01:06:37.414 - 01:07:11.762, Speaker A: If I cancel my subscription midway, the contract will automatically calculate how much money is the seller owed and how much money I must get back. At that particular point of time, the seller can withdraw money from his subscriptions. Now I'm logged in as the seller, so I can see that this person is subscribing to me for a duration of ten days. And it started on this particular day. To withdraw, I just have to create a transaction. We also have docs where the user can see some usage manual for any support. We have a discord server where users can get their queries resolved.
01:07:11.762 - 01:07:50.866, Speaker A: Thanks for bearing with us throughout the demo. What we plan is to have a Dao which will manage the development of the product. The Dao will also be used to curate content. We also plan to have a fee model where we can charge a small fee which will roast straight into the development. Coming on to the roadmap. So currently we are here a shop fsmpp on Rinkib and we plan to a decentralized key management structure. Currently we are using a centralized service for our team management and Launch Shop FSD and Launch Shop FS on maintenance environment around this.
01:07:50.866 - 01:08:15.050, Speaker A: Thanks a lot. Cool. We got four minutes for Q and A. Actually a bit shorter than I thought. We edited them down. I wanted to say thank you for being the first project so far that's had an architecture diagram. It really helps from being able to figure out what the different pieces are and things like that.
01:08:15.050 - 01:09:22.160, Speaker A: Have a few stack lists, but it's nice to be able to visually see where the flow I'm really interested in the streaming payments part of that, and I haven't really seen that before or read about it from an implementation standpoint. Does this just charge periodically? Does it come back when you say streaming? How often are these transactions? What happens when transactions are delayed or when the wallet runs out? So how it works is we calculate how much the total amount is and then the contract automatically divides it per second of the total duration. And every second it's basically calculating how much. So at any point when the person actually wants to withdraw, that's when the calculation happens. And then based on the number of seconds elapsed, the contract directly sends the money to the person who asks for it. When you cancel it, basically whatever remain at that particular time is transferred. So it's like almost escrowed and then pulled in when the recipient chooses to.
01:09:22.160 - 01:10:26.704, Speaker A: Yes. So we didn't implement that's already there by sabler, we just used it. Yeah. So you just have your funds locked in in a stream for a duration where two parties agree and the recipient can withdraw anytime during the stream or the complete amount after the stream ends. And the seller or the buyer, on the other hand, can cancel it anytime. And based on the amount of the duration that has passed, you get your funds at running. I was just going to ask, is there any aspect to this project when it comes to data validation, data provenance kind of verifying the origin of the data and that the data you're downloading or buying is actually what you think it is? Any thought to that? Yeah.
01:10:26.704 - 01:10:46.500, Speaker A: So what we plan is to have a dow which will help curate the content. We still haven't thought to completely, but we want to have curation as an important part of it. I was actually going to ask about the dow. That's perfect. Yeah. We have a key management service as well, which is monitoring the smart contracts. Twenty four seven.
01:10:46.500 - 01:11:19.152, Speaker A: And whenever a buy or a sale takes place, so the seller and the buyer's signature first is verified and all the files are getting uploaded in an interrupted form to Fleet Space Demon. So the files are secure and only the buyer will have the private address. Once he is signature and everything is verified. We are ensuring that. I did like also thanks you're. The second project that actually had a local. Host URL instead of the centralized HB URL in the demo.
01:11:19.152 - 01:11:57.836, Speaker A: So thanks for that. One of the questions I had was around Space DMN, is that something that's reasonable to expect to end users to install people who are buyers? Yeah, so it's a tricky one. So actually we have faced a lot of issues throughout our hack. There were some issues while on different OS and all that. And currently, if you see, you have to install it locally, right, to run it. But we are in talks with the fleet team and we are in conversation and they said that they are moving towards a remote thing where you don't have to install it. So we are in conversation with them and once it's done, we will move on to that.
01:11:57.836 - 01:12:21.670, Speaker A: We just ship there basically. Yeah. We also tested hosting the Space T one ourselves on a server. It's working, but we haven't demoed it fully. Yeah, we tried to include that option as well, but there was trade off. Yeah. We have time for maybe one quick question.
01:12:21.670 - 01:13:14.964, Speaker A: I guess it's a bit broad, but what are some of the major challenges that you faced in building this particular type of application when it comes to a marketplace, like a decentralized marketplace especially, what were the biggest challenges from your perspective? Keep the answer around like 30 seconds or so. I think the biggest question was sharing the private key with the buyer in a decentralized fashion. We are using a centralized service right now as a centralized Kms to share the key, but we are looking at options how we can share it in a decentralized manner. Yeah, we had planned to use three bots, but there were a lot of syncing issues while joining the confidential trade. Awesome. Thanks so much, guys. Thank you guys.
01:13:14.964 - 01:19:15.720, Speaker A: Bye. Okay, so as I mentioned earlier, we're going to take a short five minute break. I prepared some slides, well, a slide, so I'm going to put that up. But yeah, judges, feel free, take some time to collect your thoughts. Maybe if you need to take a break, whatever. We'll be back at 156 Eastern, so in five minutes. Om sam sam ram dam om sam ram SA sam.
01:19:15.720 - 01:19:57.976, Speaker A: It okay. We'll get started again in just another 30 seconds, minute or two as judges come back. I got my coffee. I don't know if you guys had a chance to get your own kind of snacks or anything. Yeah, I actually just got my coffee. Also definitely needed today. Cool.
01:19:57.976 - 01:20:41.110, Speaker A: One thing, also this quick note for you guys. I know you guys already know this, but for anyone watching and maybe in the call, all these projects are listed on Hack ETHGlobal Co hackfsowcase. So for those interested, want to see more detail, see the code, see a replay of the video, all that stuff is available. You just search by the team name and right now it's pretty bare bones, the view, but in the next day or two, we'll be adding a few more features to that page, so it's a bit more interactive and easier to navigate. Cool. Okay, so it looks like all of our judges are back. Just to confirm you guys are all good to go.
01:20:41.110 - 01:21:20.868, Speaker A: Sweet. Okay, let's get the next team, Kotal so right, the next team is Kotal. Just to confirm. Kotal mustafa, that's you, right? Yes, I'm here. Sweet. So yeah, whenever you're ready. Once you get the video set up, I'll start my timer and we can keep going, shared it and then unshared it.
01:21:20.868 - 01:21:59.490, Speaker A: There we go. Hi everyone. This is Kotal. Kotal is Cloud Agnostic blockchain operator that make it easy to deploy self managing, self healing blockchain infrastructure on any cloud. So what is the problem we are trying to solve? The problem is most DAP developers are using centralized IBFS gateway and noon as a service instead of deploying their own infrastructure in production. So in Ethereum blockchain, for example, most DAP traffic is flowing through Infura and if Infuria goes down or go out of business, this DAP will stop functioning. So what is the solution we are trying to build? The solution is an open source IBFS gateway and Mood as a service.
01:21:59.490 - 01:22:43.410, Speaker A: We are building it in cloud agnostic way so it can run on all cloud out of the box like AWS, Google cloud platform tencent, cloud alibaba cloud and OpenShift. And we are protocol Agnostic. We are supporting ethereum and IBFS and we will continue to support other blockchain protocols. And we are also multi client. So for Ethereum, for example, we support Go ethereum, hyperledger beso and barrity ethereum or open ethereum. Here are some examples of what you can do with quoteL. With quoteL, you can deploy an IBFS swarm on multiple clouds in different regions, and each IBFS node has different profiles applied and different node size.
01:22:43.410 - 01:23:26.872, Speaker A: You can also create a multi client ethereum network that is deployed in multiple clouds in different regions and each node using different client like do ethereum, hybrid bezo or parity ethereum. This is our roadmap. Before hack FS, we have supported ethereum One. During hackfs, we have supported added initial support for IBFS ibfss world. And after hack FS, we'll continue adding more blockchain protocols like filecoin ethereum two and adding user interface and middlewares. And this is time for a demo. So here is an example of a swarm IBFS swarm.
01:23:26.872 - 01:24:06.300, Speaker A: So we have extended Kubernetes to make Kubernetes know what is a swarm, what is ethereum network, et cetera. So this is an IBFS swarm that have three nodes node one, node two, and node three. We have also forked IBFS to make Go IBFS. Start with bridge generated private key to get this identity. So node number one, start with private key and apply two profile server and the flat FS. Node number two, apply different profile low power and the flat FS and use different resources. Node three, start with different storage resources.
01:24:06.300 - 01:24:53.960, Speaker A: We will go ahead and apply this swarm manifest. Once we apply this manifest, Kotal operator will react and will create this three node swarm for us. As you can see, the swarm is up and running. We will forward the traffic from our machine to node number one. We can send an API to node number one. Like, what are the swarm appears and we get the result from node number one. We can send another API, like what are the bootstrap peers? And we will get a list of all the bootstrap peers.
01:24:53.960 - 01:25:23.350, Speaker A: And finally, we can go to the browser and open the web UI of node number one. As you can see, we can see the status of node number one. We can see the files, see the beers, and modify the settings of mode number one. And thank you for watching. Awesome. Great video. We got four minutes for Q and A.
01:25:23.350 - 01:26:02.074, Speaker A: That seems super useful. Not that I want any of my nodes running on those specific cloud providers, but the ability to quickly change between them gives me a little bit more control in those market dynamics. So I think it's a fantastic idea, and I know a bunch of ops that be very interested in learning more about this. A little feedback. Also, thanks for making very clear value proposition at the beginning of your slides. It really helps when judges are trying to figure out what you're doing and why. So that's positive feedback on that too, for you.
01:26:02.074 - 01:26:35.346, Speaker A: What was the challenging part? There's a lot of coordination of distributed topology here. What was the hardest part to implement? Yeah, the hardest part is, like I mentioned, running on multiple cloud providers and scheduling multiple nodes in multiple clouds and multiple cloud providers and establishing the networking between them. This was the hardest part. One quick note. There are a few people, I guess, working on. It's a very important, I think, problem to be solving. It seems like there's a few people working on that.
01:26:35.346 - 01:27:21.954, Speaker A: Are you familiar with any others that I guess maybe this is a question that would require some diligence on my side too, but are there others that are pursuing this? And I guess, do you have a sense of and I know this is not necessarily product related. How this compares and the number of providers is obviously a key point, but are there other major sort of architectural choices that you made that you think stand out in this context? There's lots of solutions. I was there trying to achieve the same goal, like Dabnote, for example. But Dabnote requires specialized hardware. But Qatar has no specialized hardware requirements. And we try to cut down the requirements or abstraction to zero restructuring. We don't need any hardware at all.
01:27:21.954 - 01:28:20.326, Speaker A: We don't have any cloud provider requirements. They just require a Kubernetes cluster. You deploy Kotalabrator and tell it what you want, like how many nodes you want and what is the client running on each node? What is the node side? What is the API you want to activate? And it will do everything from in a self managing, self hearing way. So if node goes down, it will run it back. For example, in Ethereum Two, if validator goes down, it will restart it back and it will make sure it's highly available and running in multiple regions. How critical was the existence of IPFS cluster in order to make this a useful tool? I'm wondering just kind of like thinking forward towards your filecoin implementation, if you imagine there also needs to exist some kind of, like, clustering layer for modus or any filecoin implementation or maybe that exists. I don't know about it.
01:28:20.326 - 01:29:06.866, Speaker A: Just before you answer that, it's about 1 minute left ish so just so you are so we haven't started the filecoin implementation yet, so we don't know what are the challenges we might face. But so far with IBFS, the implementation was quite easy. Documentation is very clear, and it didn't take so much time for us to implement IBFS orchestration. Does the existence of the cluster API and IPFS, is that really for what you're doing? No. So what we have demoed is IBF swarm. IBF cluster. So IBFS Swarm is a bunch of node connecting to each other, but they are not necessarily have the same bin set.
01:29:06.866 - 01:29:30.474, Speaker A: IBF cluster, on the other hand, have the same bin set and have the same yeah, okay. Yeah, thanks for the reminding me of the distinction there. Yeah. So as I mentioned, the demos during hackey phase, we have added IBFS Swarm. But Nikki so what we are working on right now is IBFS cluster. Cool. It's just right at the eight minute mark.
01:29:30.474 - 01:29:54.268, Speaker A: So thanks, Ben, for the demo and for answering any questions. Thank you. All right, so the next team has an interesting name, the Big Announcement. So, yeah, I'll let them go ahead with their video and eight minutes. Video q A. Hi, I'm Brian. Hi, I'm Amy.
01:29:54.268 - 01:30:29.352, Speaker A: We made a project called the Big Announcement. We're new to distribute application development, and we're interested in building distribute publishing platforms. This project is a prototype designed to teach us and others the basics of app development with web3 JS and Jsipfs. In this video, we're going to give a quick demonstration of this simple project. The Big Announcement is designed to be the smallest distributed application possible. That one, uses web3, JS. Two, stores data to IPFS.
01:30:29.352 - 01:31:13.660, Speaker A: Three locates data using unstoppable domains. And the Ethereum name service Four, has no centralized dependencies. It is a media platform that displays a single very important message, and any person in the world may replace that message by bidding more Ether than paid for the previous message. That's the whole thing pays the most displaying a message to its universe. Let's give it a try. Okay, let me load our web page. So right now, it is querying Ethereum for the message and then querying IPFS for the full contents of the message.
01:31:13.660 - 01:32:04.898, Speaker A: All right, so a few seconds it has loaded the contents of our message and now we can go edit the big announcement. And this is the slightly more complex part of the application. And what we have here is we can add a message and it shows us the current value of the big announcement. This is what the last user paid to upload a message to the big announcement. And for us to upload a message, we have to pay a slightly more value in Ethereum. So the message is in Markdown and I'm going to add a message. Now, when I submit this, what's going to happen is it will spin up a Jsips node, upload our message to IPFS.
01:32:04.898 - 01:33:04.780, Speaker A: Then it will take the content hash of that message and call our smart contract and change the value of the content hash in our smart contract while also paying a small amount of ETH to do so. So if I click submit, we will see some status messages as the thing does its work. And now I have to confirm with MetaMask. And now here's the part that might take some time while we wait for Ethereum transaction to go through, even just on the testnet. Yes, we're running this on the Robson testnet. Yes, it is a fast. So now we go back and we can visit the main page again and we'll go through the same process of querying Ethereum for the content ID, then loading the message from IPFS and voila.
01:33:04.780 - 01:33:42.050, Speaker A: It's a very simple publishing platform that is distributed, has no backend server dependencies and is essentially uncensorable. Pretty cool. Yeah. Good. This was your first time building something in this itself isn't much of a product, but more of a teaching tool. We are going to polish its tiny code base such that it can be used as an example for beginner D app developers. We're also going to blog about our experience building this project.
01:33:42.050 - 01:34:51.480, Speaker A: For more about the big announcement, or to try it yourself, check out the link below. I was so convinced that was the end of the video. I'm really curious, what were the major learnings that you had going through this process? If that's correct, that this was your first time sort of doing this, what did you learn? What did you really enjoy about it? And of course, as we've talked about, what are the challenges you ran into? Sure, great question because honestly, this was an extremely frustrating experience and we blogged about it. If you go to our links, you can see exactly our frustrations. But at every step of the way, we found ourselves being a little bit annoyed at something or another. The initial difficulty we had was just understanding which technologies to use for our use case. And as you saw in the video, we ended up using the most basic technologies just the Baird IPFS and Web Three.
01:34:51.480 - 01:36:16.864, Speaker A: We didn't use anything fancy, and for the most part, we didn't use any of the sponsors products sorry. Except for fleek and unstoppable domains. As we were hacking, a big problem we ran into was the documentation for these fundamental libraries like Jsipfs, and Web Three had gaps in them where we had to just use our intuition to figure out what we were supposed to do between one step and the next step. And I think for experienced JavaScript developers, we came in knowing some web development and some JavaScript development. A lot of people would have been able to fill in the gaps themselves, but there were a number of places where we'd have spent hours spinning our wheels, figuring out what to do with some NPM instruction or something like that. And the Jsipfs docs specifically, I think were a little frustrating to us because there's not a lot of tutorial material and the IPFS spec is like a platform language independent markdown document that doesn't really have connected to the Jsipfs library that we were actually using. Ultimately, though, we do feel a lot of satisfaction that we completed our MVP with all the features we intended to.
01:36:16.864 - 01:36:39.610, Speaker A: So we feel like we learned a lot. I've taken so many notes, thank you. And I want to go read that blog post. I have a question. What's the end game? Where does the money go? Who's the final message? Yeah. So this is not really a product. It's more of an art project and a prototype and a test.
01:36:39.610 - 01:37:19.812, Speaker A: Once we get it on the main net, if people find it and start throwing money into it, awesome. That money will probably go to our pockets someday. But if this never gets any eyeballs on it, then who cares? It was a good learning experience in the end. Right now, we don't have a pinning solution. We just throw the message into the network and just pray. So a real product, which we may continue to work on, would have some kind of distributed pinning solution that doesn't require us to sort of directly have a call server API and money would go to that. I had another question.
01:37:19.812 - 01:37:56.832, Speaker A: I really love that it's a teaching tool. That's fantastic. What is your approach to parlaying what you learned into enabling others to be able to cross those hurdles and bridge those gaps in a way that they can learn from your experience? Also, Titus, with this question, maybe keep it in like 30, 45 seconds answer because we're around the endpoint, but go on. Great. What we're going to do is we're going to keep this project as small as possible. We're going to flesh out the source code that it is so that it's beautiful as possible and well commented. Handles the error cases and have a nice README that directs you to all the parts of it.
01:37:56.832 - 01:38:19.108, Speaker A: And then we're going to have basically a blog post or maybe a tutorial that says, like, here is how you build this thing. And then we'll post it to various interested forums, and it should be like a standalone little piece of tutorial. Awesome. Thanks, guys. That was a really fun demo. Thank you. Sweet.
01:38:19.108 - 01:38:49.636, Speaker A: So I think we have four or five projects left, and the next one is called Go Image. It's an image file management platform. So whenever you're ready yeah. Share your screen and I'll start the timer. Actually, Liam, we've switched order. The next project interplanetary container. Okay, sorry about that.
01:38:49.636 - 01:39:12.980, Speaker A: There's a change. Okay, yeah. Interplanetary Container Registry. Like Docker hub, but it's fully decentralized. All right, so, yeah. My name is Yelc Zanko, and I'm excited to introduce you to my Hackathon 2020 project called Interplanetary Container Registry. The ultimate goal of the project is to build a decentralized version of Docker Hub.
01:39:12.980 - 01:39:48.260, Speaker A: During the hackathon, I got handson experience with IPFS and documented a way to push docker images to IPFS and pull images from there. I also built a simple web front end to process images and their tags. Let's jump into the demo. Here you can see the list of the images published to the Container Registry. This list is fetched from IPFS using the JavaScript implementation of the IPFS protocol. When you click on the image, you see a short description and a list of image tags. Description and tags are also fetched from IPFS.
01:39:48.260 - 01:40:21.696, Speaker A: On the right you see command that should be run to pull the image from the Registry. Let's try it out. First, I download the image from IPFS using IPFS get command. Then I load it using Docker, and that's it. Now let's see how to push a Docker image to the Registry. First, let's pull something from Docker Hub. Then I create a directory for the image and tag.
01:40:21.696 - 01:40:53.820, Speaker A: I also save a short description of the image. Finally, I save the Docker image to the tag directory. Now I publish an updated version of the container registry. After that, I have to update the Registry CID on the front end. It is required only because DHT implementation in JavaScript is still incomplete and not stable, and I could not get IPFS result working in a browser. Awesome. Redis docker image is now pushed to interplanetary container registry.
01:40:53.820 - 01:41:43.790, Speaker A: So under the hood, the Container Registry is basically a hierarchy of IPFS directories and files. The registry name you saw during the demo containers duncode Dev is published using IPNs and DNS link. The web front end is built using the JavaScript implementation of the IPFS project. As future work, I want to build a tool called IPCR. This tool will manage container registries and will allow to push and pull docker images easily and effectively. I also realized that it's not necessary to have a dedicated web application to browse the Container Registry. Instead, the IPCR command can render a static website that represents the Registry and its content.
01:41:43.790 - 01:42:16.318, Speaker A: Every time you push an image, that website should be also published to IPFS. It would be also cool to support crypto domains for registry URLs. You can check the live demo on click as well as at the LSA Dunko crypto. I learned a lot and had fun working on this project. Thank you so much for this opportunity. You. That was cool.
01:42:16.318 - 01:43:23.810, Speaker A: Thanks. One of the questions I had was something we asked in a previous one about data provenance and trust. What are your thoughts on a trust model for pulling down images? Yeah, I used DNS Link and IPNs to publish my container registry, so it's effectively tied to my DNS that I own. But to move it further, I think we can use also unstoppable domains and to also publish container register names there and resolve them using the IPCR tool. I just wanted to clarify the command line tool you talked about building in your next step. Yeah, that would kind of replace the need to use the IPFS command line directly. Yeah, sure.
01:43:23.810 - 01:44:43.194, Speaker A: Okay, cool. So my goal was to figure out the simplest way to put images and their metadata to IPFS and without building anything on top by just using existing tools. And then from that I can go and build the dedicated tool on top of docker API and either IPFS lite, I mean embedded version of IPFS, or using the IPFS node that's running on local machine. Do you have any thoughts or concerns about like was mentioned in the last presentation pinning the data on IPFS? Are you doing that yet or I kind of see some really cool opportunities for actual hosts that are downloading images to also pin them to make them available. What are your thoughts there? Yeah, that's a cool idea. I think should to doses. We have another minute and a half or so.
01:44:43.194 - 01:46:28.990, Speaker A: Do you have anyone else have any last questions? It's interesting because there's a private registry space kind of here and the opportunity for public registry space, but there's not really yet coordination around that global registry space. Have you thought about ideas for that? Yeah, initially I was thinking about a global registry space, but then I realized to make this thing fully decentralized, it would be really great to just let individuals and organizations create their registries and publish them. And one good thing about pushing a static website instead of just having the web application is that we already have search engines that are working with IPFS and I hope those static websites will be discovered by these search engines and people can actually find docker images that are on IPFS. I love that little detail too. I think that's great also that gives that static page gives you kind of a vector to be able to do that coordination communication without having decentralized right, pretty neat. Yeah, probably we can also use maybe Textile, the DB or Orbit the way to further track downloads or maybe Stars or something cool. Any last comments? Okay, cool.
01:46:28.990 - 01:47:06.386, Speaker A: Thanks so much, man. Thank you. All right, so we have two projects left pygate padlock, and if we can get Go image back, we'll have them. But up next is pygate, and pygate is building a Python interface and tools for Filecoin using textile. So, yeah, without further ado, I'll let you take it away. This is pygate, a Python interface and tooling for the Filecoin network. It's built with powergate gRPC and flask for the hack.
01:47:06.386 - 01:47:57.350, Speaker A: FS hackathon? The pygate project team has two goals. The first goal is to bring Powergate into the Python community. This is a February 2020 survey of over 65,000 developers, and Python is overwhelmingly in focus. Given these vast possibilities, our team reasoned that there would be an acute need for tooling which will expose Powergate and Filecoin to the enormous and influential Python developer. Community. Data science, machine learning, web applications, and many other innovative software tools rely on the flexibility and consistency of Python to help them solve problems for people around the world. Currently, there exists only JavaScript and Go libraries which are able to interact with Powergate to leverage these new opportunities for content utilization, our second goal is to create tools upon which to build applications and experiences.
01:47:57.350 - 01:48:37.298, Speaker A: pygate provides these developers a set of tools to effectively build their next project, solving big problems on top of the Filecoin ecosystem. In order to achieve these goals, we created three products. The first is the pygate gRPC interface. A python library for powergate. The second is the pygate Web app, a Flask reference client for powergate and filecoin. And third is the pygate API a flask asynchronous Http and WebSocket API for web applications. Building on the gRPC definitions provided by the Textile team, this library allows for Python to drop in support for Filecoin to any Python software.
01:48:37.298 - 01:48:54.220, Speaker A: We're happy to be able to provide this to the community. It has near feature parity with the Powergate interface. It's actively powering the pygate reference client. It has developer quick starts and several examples. And the best part is it's available on Pip right now.
01:48:55.870 - 01:50:11.530, Speaker B: The pygate gRPC package makes the endpoints of the Powergate API to the FileCODE network available as Python methods. These are being used in the pygate Web app to demonstrate basic Powergate functionality in a Python Flask application. Users can upload single files to the FileCODE network via the Powergate API. They can also upload batches of files, or they can select a batch of files and choose to upload it as a tarball package. Users can search for already uploaded files and retrieve them from the network, all the deal negotiations handled in the background. Powergate uses the filecoin file system concept. Users can add new FFSS and make them the default.
01:50:11.530 - 01:50:51.440, Speaker B: They have the ability to change configuration settings and push them to a ready upload of files. Using those configuration settings, users can track wallet balances. A new one is created for each FFS, and they can also review a log of all pygate web app transactions and error reports.
01:50:52.340 - 01:51:30.552, Speaker A: In addition, we'd love to discuss more about our learnings feedback to the Protocol Labs and the textile teams, our team, and the next steps for pygate. Thank you. Okay, somebody who comes from a background in devrel. I don't care about what languages some developers favorite. I care about total addressable market, and I've really been missing Python stuff in this world. So very excited. Thanks for tackling this challenge.
01:51:30.552 - 01:52:40.176, Speaker A: Also, it seems like really a lot of care towards idiomatic Pythonness and developer tools, and just the overall DX seems really nice, like you put a lot of thought into it, so kudos for that. Also, one of the questions I had was around what are the missing piece? I'm not a Python community person at all. What do you think the biggest challenge is from bringing tools like these and platforms like these into Python? Right. So essentially, we have a few next steps, and what we're looking to do is to include this as part of the textile offering so that textile is able to either help us or manage the custody of this library. And generally speaking, that's one of the major points for any software project, is to have reliable documentation and to ensure that it's sustainable going forward. So I'd say that's probably one of the biggest aspects of that. Yeah, very cool.
01:52:40.198 - 01:53:10.428, Speaker B: I think we solved one of the major problems for Python developers. Sorry, can you hear me? I think one of the major problems we solved for Python developers is that they don't have to deal with protobuff messages at all. That's all abstracted away in that gRPC client. So as the one that was working on the web app Reference client, I'm just dealing with Python dictionaries. I don't have to deal with any of that. So we've solved the major kind of hurdle there already, and that's our major contribution. And as far as other pieces like the Reference app is there, it's for the people to build off of.
01:53:10.428 - 01:53:34.770, Speaker B: Flask is the most popular mini web framework out there in the Python community. It's got thousands of thousands of developers, so it speaks to them already. And we've started work on this Web API to solve the issue of long running tasks against powergate and FalcoIn deals, which is also going to be a major, I think, boost for Python developers looking to implement on top of.
01:53:39.780 - 01:54:16.976, Speaker A: There. Well, I guess first, let me just say thank you for doing this work. On the textile side of things, this is exactly the kind of thing we love to see happen with a project. And moving forward, we definitely want to get feedback from you guys about how we can keep this development going and make it as smooth as possible. Thank you for that. One question, I'm just curious. In the web application, is there a JavaScript client or.
01:54:16.976 - 01:54:29.430, Speaker A: Something that's communicating with the Flask back end, or how are all those interactions happening and the communication between the front end and the back end? I'm not familiar with Python web development much.
01:54:29.800 - 01:55:03.588, Speaker B: There's a bit of Jquery in the client, just a little bit for some of the stuff you saw in the demo, just for the upload button and a little bit. But for the most part in Flask, it uses Ginger templating. Ginger is very popular in the Python world as well. It's used in Django as well. So Flask has Templating built in, and so that's all being handled by Flask templates. It's a web framework in that sense. It gives you all the tools to do web templating, and then you can add your own JavaScript on top, whether that's plain JavaScript or most commonly, it's some kind of Jquery.
01:55:03.588 - 01:55:13.040, Speaker B: And we use a little bit of that and Bootstrap. So Bootstrap is just the basic for the CSS layout. We use Bootstrap there, so that combination is very common in the Flask world.
01:55:13.110 - 01:55:14.628, Speaker A: One other thing also, but all the.
01:55:14.634 - 01:55:47.790, Speaker B: Rest of it is all the rest of it is just Python methods in the Flask routing scripts that are then talking to Powergate via that GRBC library. So the gRPC Library is used throughout every time it's making a call and going all that file upload and download is all using Powergate for the deals and checking the wallets and changing that config is actually pushing it through Powergate. Doing that config push, that's all happening through the gRPC client scripts that I can just use plain Python to do that with.
01:55:48.240 - 01:56:09.876, Speaker A: We only have a couple of seconds left, but I know Art, you had a comment, so I'll let you make your comment and then we'll wrap up. Sure, yeah. Thanks. No problem. Basically, it's just that, yeah, we do have to use some JavaScript, especially when we're working with API using the sockets. But, yeah, like Peter was saying, a lot of the heavy lifting is done by the interface to Powergate and through the Python Library. Cool.
01:56:09.876 - 01:56:19.860, Speaker A: Thank you. Awesome. It's a cool demo, guys. Thank you. Thank you so much. Awesome. Next up, I believe, is Padlock.
01:56:19.860 - 01:56:48.572, Speaker A: They're already in the calls, a lot of you guys. So, yeah, I'll let you take it away. Okay, Shannon. I'm just sharing my screen. Hi, everyone. This is Padlock. Secured by secret network enables monetization for content creators on the decentralized web.
01:56:48.572 - 01:58:12.772, Speaker A: It is a tool for a variety of use cases like digital rights management, access control, and key management. In this video, we will demonstrate how privacy preserving secret contract enable unique capabilities with programmable privacy. So, to begin in this hack, we asked ourselves, how can we monetize content on the decentralized web in a trustless way? Because as a content creator, if I have content on IPFS, the only way I can monetize it is by encrypting it. Say, I promise you, as my consumer if you pay me ten die, I'll share my decryption key to my content with you. But this can't be done on Ethereum contract because the decryption key state of the contract will be public. So how can we program sharing the decryption key with privacy after payment with secret contracts? We can, because we can store the decryption key as the contract state. So how does this work? The idea is to use secret keys, which are stored in a privacy preserving secret contract, allowing users to manage or program access control by selling private keys that unlock their encrypted content using the privacy preserving features of secret network and the decentralization of IPFS on filecoin.
01:58:12.772 - 01:59:02.680, Speaker A: There is an opportunity here to create new capabilities without middlemen on Web 20 content platforms. Our demo will now go into how one piece of content can be uploaded and sold to one user at one time. Here's how that will play out. As a creator, I can go to the app using Fleek for hosting the web app, get an auto generated secret address anonymously or create a public profile using threebox for managing profiles, secret keys and secret network credentials. Upload new content on IPFS and filecoin. Leveraging textile's powergate for IPFS. Hosting the graph for Indexing.
01:59:02.680 - 01:59:59.080, Speaker A: Add metadata for the content and have that metadata stored and pinned on IPFS by using Pinata set price for the content. Ethereum is used as the payment and data layer, keeping a log of all creations and buy orders. MetaMask for Ethereum wallet use, the content would be encrypted by a secret network public and private key pair fetched from the app's secret vault contract. Lastly, we added an unstoppable domain for the project. As a buyer, I can see a preview of the content in the browse page. Pay in crypto receive the non fungible token in my Ethereum wallet to access keys on secret network. There's also the Oracle watching purchases and whitelisting buyers on the Padlock secret contract.
01:59:59.080 - 02:01:06.530, Speaker A: Currently, we are exploring various integrations with existing media platforms and publishers. Although we built a web app for this demo, our hope is that Padlock will function as a widget built into those applications. Overall, we are thrilled to release this example of a secret app running on secret network interoperating with Ethereum, and we are excited to continue working on this project. Hopefully, this will empower content creators to realize their potential on the decent centralized web. Awesome. I really like this. I'm curious in terms of integrations with individual platforms, how are you approaching that? And are there challenges with individual types of platforms in terms of those integrations? I guess I'm also curious about how the widget will compare to the web app.
02:01:06.530 - 02:02:07.620, Speaker A: Go ahead. Okay. Basically right now, we have a simple smart contract on Ethereum which provides a receipt in the form of a non fungible token. But you could imagine other non fungible tokens being used as the purchase. So when you buy a non fungible token that would unlock the encrypted content. Other platforms such as the Art tokenization platforms like superAir known origin makers Place, et cetera, or music streaming platforms like Audius, even virtual reality platforms like Crypto Voxels. You can imagine similar applicability for any of those because of the flexibility and generalization of the secret contract we've implemented.
02:02:07.620 - 02:03:02.950, Speaker A: Got it. And then as a follow up, can you talk a bit about that contract and how secret works in that regard? Yeah, so it's a simple way to store and manage private keys in a contract. So the keys are generated using your secret network account, which is a separate key pair, and it enables those keys to be managed in a trustless way, which you can't do in a contract on Ethereum because it's public. Essentially, they have encrypted inputs, outputs and state relying on trusted execution environments within our nodes across the secret network. I can go into more detail. Awesome. No, I'll stop there and let others ask some questions.
02:03:02.950 - 02:03:47.392, Speaker A: Yeah, thanks for the demo. Really nice video and the design of the site is really nice as well. I also was super interested in learning more about the secret network. This is definitely from a feedback perspective, this is something that would a visual, I think, of the overall architecture would really help tell this story a little better. There's a lot of moving pieces and also seems there's some clever pieces in there that I would love to learn more about, but there's not really a lot of time here. But kudos on trying to solve a really difficult problem that I think a lot of content creators and kind of platforms are going to hit. Yeah, that's great feedback.
02:03:47.392 - 02:05:04.696, Speaker A: We have about a minute left. I do have a question. Where does this keep going? It's just keys all the way down, right? Keys to unlock, keys to unlock keys from your learnings in this project, have you thought about either a pattern or protocol or approaching or publishing some type of standard to get, I guess more adoption of this approach with regards to access control and decentralized key management? Not in depth. I haven't, but yeah, the idea has been there, but I don't know the answer to that. Yeah, so actually all the decryption stuff was kind of last minute, so we had kind of a problem with the decrypting data just due to encoding. Okay, so if you see the last few seconds of the video, the output text file had unescaped characters and things like that. That's why we had to test with a text file.
02:05:04.696 - 02:05:41.284, Speaker A: It was easier, a lot more tolerant to that kind of change. With an image file, you just get a broken image. So we just need to do a lot of testing and come up with the appropriate solution for the encoding issue. Other than that, building an open platform using textile tools. So I think we had a lot of challenges there. And textile tools are awesome when you're building something like private spaces. Like, for example, if you're building a decentralized dropbox, they would be really awesome to incorporate.
02:05:41.284 - 02:06:47.036, Speaker A: But the problem we had here is building a platform where you could share one single FFS instance with many users. So then it becomes problematic because anyone would be able to push new CID configs and just ruin the platform for anyone else. So there's an issue here. I think it relates to textile coming up with more complex ACL types to support that kind of complex interaction with multiple tiers of access. But I think there's a lot of room for growth here and becoming a much more full fledged product. If this is something you guys continue working on, definitely get in touch with us because it sounds like the problems you have are things that we are also trying to solve. So definitely collaborating on that.
02:06:47.036 - 02:07:12.756, Speaker A: Good work. Sorry. I read about textile coming up with more complex ICL types, and there is a white paper for that, for threaddb, I think. So it would be very cool if you just get a heads up on that. Awesome. Thanks for the demo, guys. Thanks a lot.
02:07:12.756 - 02:07:37.896, Speaker A: Yeah, thank you. Okay, so I guess that's you with that the one team that wasn't able to make it, unfortunately. Go image. We're done for the day, so yeah. Thanks so much, guys, for taking all the time to judge all these projects. Thanks to all the participants. Any that are still on the call, watching also or watching the reporting, this has been a lot of fun.
02:07:37.896 - 02:07:53.036, Speaker A: We are doing these all week as we judge all of the I don't know what the total count is now, but quite a large number of hack FS projects and yeah, thanks so much. Awesome. Thanks a lot. Thanks, everyone. Bye.
