00:06:13.890 - 00:06:49.706, Speaker A: Good morning, good afternoon, and good evening, everybody. My name is Kartik. I'm one of the co founders of ETHGlobal, and I want to welcome all of you to our ETH Global Scaling and Infrastructure Summit. I'm super excited to welcome all of you here today. We have so many people joining in from different parts of the world. We're monitoring our chat, and it's so great you see so many country flags and cities being represented. So we have an amazing day planned for all of you today, and I'll be your host for the next few hours, along with our few guest hosts.
00:06:49.706 - 00:07:42.894, Speaker A: They'll be coming in and helping us out throughout the day. So what I want to do today is just going to go over ETH Online and kind of what we have planned for the month of October, and then we'll specifically move into what today is about and what our summit looks like moving forward. So all of you are joining us on our live page. I'm sure a lot of you've gotten the chance to play around with this page for us, and this is how we're going to be hosting the rest of the talks as well. So to make this really an interesting experience for all of us, we wanted to really demonstrate what we can do with a global shared world computer. We already know what Ethereum looks like as a global state management system. So we decided to see if we can also replicate an experience that lets all of us share a state for what everybody else is also experiencing.
00:07:42.894 - 00:08:17.802, Speaker A: This kind of lets us do anything from seeing cool broadcasts, to syncing our backgrounds, to represent what's being talked about on the screen. And we hope that you like it. So I encourage you to play around with this whole tool. Join the chat, tell us kind of how the talks are going or what you like to see about this. And obviously, I'd like to request all of you to be respectful in the chat. We have all of our speakers joining in along with the global team. So if you have any questions or if you have any comments or something can be improved on, we'll be able to monitor that and kind of quickly act on it on this chat.
00:08:17.802 - 00:09:04.854, Speaker A: So play around with this tool. And with that, I'd like to introduce what we have planned for today. So, for those of you who don't know, ETH Online is going to be scheduled and designed to be a month long event, which includes a hackathon which runs for three weeks from October 2 to the 22nd. And then we also have summits that are happening every Friday, and today is one of them. This is our biggest event of the year. We have over 700 hackers participating at this hackathon, and we have over 2400 Summit attendees that are signed up to attend the various summits we have planned for this month. And the hackers are here joining us from over 65 different countries spanning 19 different time zones.
00:09:04.854 - 00:10:03.760, Speaker A: I mean, it's absolutely incredible that we try to map all the cities that people are coming from, from the hackathon side and the summits and it just blew us away on how diverse and spread out this audience is. It's really a global audience and a global ecosystem, and we are super proud to have representation from six continents for it online. So that said, I want to move on and talk about what the summits are about. So we're going to be doing a summit every Friday for the rest of October. Today is our scaling and infrastructure summit. That's what you're watching right now. Next Friday we're going to be doing our Summit on Decentralized Finance, or DFI, and we'll have talks and a handful of panels scheduled for next Friday and then the Friday after that on the 23 October is going to be our summit on the future of Ethereum, where we'll cover a lot of what's up and coming on the design for Ethereum and E Two.
00:10:03.760 - 00:10:59.386, Speaker A: And then the last Friday of this month, which is October the 30th, we'll be having a summit that is going to cover Ethereum's impact in the ecosystem so far. And this will also be the hackathon finale where we'll be showcasing our favorite hacks from this month's hackathon Projects submissions. So the hackathon itself will happen from the second to the 22nd. So if you are a hacker joining us on this chat, this is the end of week one of your hackathon. So a lot of this is in a place where you'll be telling us what you're working on and just kind of sharing us where your blocks that we can help you. And then by the end of next week, we hope to see a lot of prototypes that we'll be able to kind of share and give a highlight on. So, scaling an infrastructure summit, what is that about? We have a lot of amazing talks planned for all of us today.
00:10:59.386 - 00:11:52.130, Speaker A: We're going to be starting with our first talk of the day, which will be a state of L2 and kind of what's happening in the L2 ecosystem. Then we'll be moving on to a talk from Ilya from near team and he'll be covering what Sharding and Scalability looks like and how it affects the development experience for developers. Then we have talk from the Aztec team and they'll be talking about their L2 with a privacy focus and kind of how they're thinking about their roll up service. Then we'll have Georgios and Phil talk about mev. This is a really interesting topic that I urge all of you to kind of join and ask them questions on. Then we'll have the Fuel Labs team talk about roll ups and kind of what that state of all rollups looks like right now. Then we have Andrew from State Channels talking about how do we actually do state channels at scale.
00:11:52.130 - 00:12:49.918, Speaker A: And then we'll have Alexe from the Ethereum Foundation talk about how we actually get to improve and make a lot of performance improvements on Turbo Geth. And then we have a panel that will highlight sort of how Web Three J is being used by the enterprise world. And then we have Kevin, who is going to be covering how optimistic roll ups work and sort of how do we actually keep Ethereum half full. And we're going to end the day with the two remaining talks where we'll have Michael talk about how do we actually set up Node infrastructure at scale. And then lastly, we'll have Dietrich talking about how do we actually do user agency on the IPFS ecosystem. So that's a lot of talks that we have planned for the next few hours. And before we move on to our first talk, we actually have a special announcement we want to make for today's summit.
00:12:49.918 - 00:13:54.458, Speaker A: So what we want to do is actually take some time today and highlight how the ecosystem has grown from various parts of the world. So, as we all know from last Friday, we were able to announce that DevCon will be happening in Bogota in 2021. And there's a lot of stuff happening behind the scenes from the DevCon team. And on top of just the planning for DevCon, there has been a lot of efforts in engaging developer communities in Latin America. And we have a few updates we want to share on that. So to do that, I want to welcome Skyler and Natalia from the Defcon team, and they'll be talking about sort of what they've been doing over the past few months and how they're engaging the Latin American developer community and following their kind of remarks. We'll be sharing some videos from some of our people from those communities just talking about how they've actually started and bootstrapped a lot of their local developer communities and ecosystems and just how that's going.
00:13:54.458 - 00:14:16.746, Speaker A: So without further ado, I'd like to welcome Skyler and Natalia to share some updates from Latin America and DevCon. Awesome. Thank you so much, Kartik. Thank you for letting us be here. My name is Skylar. I am the DevCon operations lead for Bogota. And this is Natalia.
00:14:16.746 - 00:14:24.366, Speaker A: You're with us. So hi. Thank you, Skylar. We are very excited about being here. So. Yeah. My name is Natalia Madrid Malo.
00:14:24.366 - 00:14:46.700, Speaker A: I'm based in Bogota. So I'm waiting for you all to come around soon. I'm Devcom production lead, and I have been also supporting community growth initiative for the last month. Great. Which screen can you share? Can you see online? It's the right one. Right one? Yeah. Go ahead and start it up.
00:14:46.700 - 00:15:43.590, Speaker A: Okay, so for those of you guys who watched aya's talk last week, last Friday, these dates will have looked familiar for you. But for those who didn't, we're going to be having DevCon from august 10 to 13th in 2021, and we're super excited about it. And continuing the theme from is Talk last week, which touched on moving Ethereum's focus towards opportunities in emerging communities. We're here today to talk about scaling, but in a little less technical sense, how DevCon aims to help scale the Ethereum community. So, as many of you know, all of you probably know, DevCon will be in Bogota next year. We chose Columbia for many reasons, many of which are outlined in the blog post we made back in May. But one of the big reasons was the opportunity for community growth in Colombia.
00:15:43.590 - 00:16:35.594, Speaker A: We saw the enthusiasm, the infrastructure, and pieces for a great community, but there was a lot of room for improvement. The pieces were there, but not necessarily in place, not unified. So one big reason Bogotza was chosen was to capitalize on Devcon's potential to spur community growth instead of just involving the existing community. In DevCon, like we've done in years past, our focus has been on growing new communities. And I'm not just talking about Bogota, but Colombia, neighboring Venezuela, and as much as we can, Latin America. So to talk a bit more about how we're focused on growing the community and the success thus far, I'm going to pass it off to Natalia. Okay? Thank you, Skylar.
00:16:35.594 - 00:17:21.142, Speaker A: So, yeah, as you just mentioned, our main goal here is to support community growth, and we believe that Meetups are really important in order to achieve this goal. So one wonderful thing that has been happening lately is that before Defcon announcement, there were stereo Meetup within the Andean region. That means Colombia, Venezuela, Peru, Ecuador, and Bolivia. And today these three meetups are a reality. They have had regular meetings so far with in between 50 to 70 attendees each. But there is more. These meetups and some others at the Buenos Aires Meetup have inspired the creation of new meetups in the region.
00:17:21.142 - 00:18:19.760, Speaker A: So Caracas, Venezuela and Medellin, Colombia communities are planning to launch their meetups by the end of this month. Another aspect that I want to highlight is that the collaboration in between meetup leads is also a reality. They are promoting the creation of new community led projects and initiatives, and looks like they will have some nice surprises for all the community on this road to DevCon. It's very important to mention that we from DevCon Team and the Ethereum Foundation have been supporting and facilitating as much as we can all these processes. However, these are all community led and owned initiatives. We believe this is the more sustainable way of supporting the growth and scaling of the communities. And I have to say that we are very impressed by community enthusiasm and collaborative work.
00:18:19.760 - 00:19:10.954, Speaker A: On the other hand, I would like to talk about two DevCon grants that aim to promote access to more good quality content in Spanish. When we started the design of this growth and scalic initiative, we did some research and we found out that there is a huge need of blockchain and ethereum contents in Spanish. The language here is actually a big barrier. I mean, in Latin America. We started talking with the community and researching about education contents available and that's how we ended up supporting these two initiatives. We are evaluating the possibility of supporting more to come, hopefully. So, to start, I'm going to mention the National University course, creation of Intelligent Contracts with Ethereum.
00:19:10.954 - 00:20:22.950, Speaker A: Okracion de contratos intelligentescon ethereum in Spanish. This is an advanced and technical course that will take place from October 23 this year. As the course would ended up being a little bit expensive for independent developers, we decided to support this community by subsidizing 95% of the course cost. With this, we are expecting to support around ten developers and ten Spanish speaking independent developers in the region. All the information is available using the QR you can find there on the screen. On the other hand, and that there was a need for basic content, our local community took the initiative of creating a blockchain ethereum course, descuriento Blockchain. This course will explore the basic concepts and futures of blockchain and ethereum through a process of 17 sessions and with the collaboration of 21 speakers from all over Latin America.
00:20:22.950 - 00:21:10.638, Speaker A: The first session sorry, took play yesterday with Mariano Conti and with more than 100 attendees, was a great success. This is a 100% community led initiative and we really want to highlight that. So if you are in Latin America or if you are a Spanish speaking community member, sign in here for this course using the QR. It's completely free and in Spanish. Also, you can look for the closest meetup and join the conversation. Awesome. Thank you so much, Natalia, for covering those initiatives and we are extremely excited to see all this community growth happening in and around Latin America.
00:21:10.638 - 00:21:53.362, Speaker A: And so this last call to action is for those all over the world, whether you're in Latin America or not. It's to share your ideas for DevCon. So you can join the discussion at our new DevCon forum at forum Devcon.org, check out the new DevCon improvement proposal process. We're requesting and trying to hear and be receptive to feedback from all of you in our community for DevCon. And we even have a couple of requests for proposals out there around artistic, visualizations and possible games at DevCon, et cetera. So, in closing, thank you, ETH Global, for having us on here.
00:21:53.362 - 00:22:26.098, Speaker A: Thank you for everyone listening. Stay tuned for updates and we're excited to see you all at DevCon six. Thank you. Thank you so much, Skuller. And Natalia. Next up, we have three different videos we'd like to share from members of our community. And what I'll do is I'll just queue up those videos for us and we'll just kind of get some of their comments on how they're actually growing developer base out there.
00:22:26.098 - 00:23:28.742, Speaker A: And I'll stop talking now and I'll just go directly to those videos. So first up we are going to have Christian talking about the developer community in Honduras. So I'll let that playalpa. Honduras currently a smart contract developer for Crypt Finance and one of the founders of the Bill Honduras community and Ethereum Tegalpa Meetup. I am a Defcon scholar and I love hacking. As you can see I've been on other hackathons like Eat Denver and Eat New York. If you were there, probably you saw me coding or selling coffee and I hope that soon we can be over there hacking together in physical and I want to share now a little about our community.
00:23:28.742 - 00:24:11.022, Speaker A: It started a couple of years ago with a small event. It was a workshop about Ethereum, the first one that we had and people like it so much that we kept organizing events. By now we have organized over 35 events and started doing some community activities to grow our own community. For example, we started a bounty program for people that create content. It doesn't matter if it's a podcast, an article on Medium or even their own events design contest. We have smart hackathons that only last less than 12 hours. Activities like that or Lattice project.
00:24:11.022 - 00:25:14.922, Speaker A: It was a collaboration with the Ecosystem Support program from the Ethereum Foundation. It was a local grant program and the objective of that was to get people to actually create or fund activities, projects, research or education that will have a great impact in the Honduras community. And it was really amazing. I hope I can talk later about that, but by now what I can say is that the grants had so much potential that even some of them actually have a global impact. One of my favorite for example was a L2 SDK online voting platform and a developer tool for improving testing of smart contracts. Well, I really hope that our community grows more and I really love how the community of Ethereum has helped us to grow. Ethereum community is open and they are always eager to help.
00:25:14.922 - 00:26:21.354, Speaker A: So you are in the best place Ali. I want to wish everyone to have good luck with their hacks and hope that you enjoyed the summit. That was awesome. Christian just kind of talked about how those local grants kind of go a long way for the local developer community out there and we hope to kind of continue those efforts through Lutheran Foundation and also Global and just getting a lot more developers into this ecosystem. So next up we have Luis who is going to be talking about the local developer community in Venezuela and I'll let the next video cover how they're ola Minambres, Diego, Parameterdao and Venezuela. Hello, my name is Alejandro Fernandez and we are maker thou Ambassador for Venezuela. Yama soy bartela cumonia Venezuela e dilover and part of the Tyrannium Venezuela community and also dialogue in Venezuela.
00:26:21.354 - 00:28:01.170, Speaker A: Tenemo mucco epicultas siento aqualmente mufisil accelera gasolina encontavio minimo mahajo tol mundo apenas serono Santao. In Venezuela we have many difficulties. It is currently very difficult to access gasoline and countless power cuts in the country paying the lowest minimum wave in the world, paying less than a dollar de los mismo. All this makes it a bit difficult for communities to meet in forums and better debate their ideas. But this has not slowed down their innovation and development. In MakerDAO, Venezuela, we have generated educational content to understand the most basic functionalities of the Ethereum network and how throughout our sample of a new final system was built on the path of the well known DeFi bemohem Venezuela. Ungran campo Paralucia royal Ethereum Consolato iconcionera ayur unpoco Venezuelano appear much agracias for suatancion.
00:28:01.170 - 00:28:49.230, Speaker A: We see in Venezuela a great field for the use and development of Ethereum with its smart contracts and functions that can a certain way to help the ordinary Venezuelan people. Thank you very much for your attention. Awesome. That was a great overview of how the developer community is developing in Venezuela. And our last video for showcasing our communities around the world is Juan David who's going to be talking about Dogeta and hopefully you all get to meet him next year at Defcon as well. Hi. My name is Emilio Silva.
00:28:49.230 - 00:29:33.690, Speaker A: I belong to the Ethereum Bota community and this is Reed and we're going to talk to you about this community, this wonderful community. And so, yeah, this is what this video is all about. So why don't you start. Yeah, as any meetup, we have monthly gatherings online. As you can see, we are still in the lockdown here in the city. Every week I lead a group of people from different universities doing an introduction to Ethereum and also every Thursday, Emilia is doing something with developers. Yeah, I'm holding a developers meetup, you can say.
00:29:33.690 - 00:30:26.082, Speaker A: And we're building some app together, I mean, adapt and we're having fun and yeah, that's what we're doing. And we are very excited to be part of the East Global community, keep growing the community and also having hackers that could join to the hackathon. Hi, this is us again. We are now in the online world where we've been meeting for the last six months. And here with us is Nadia Albare. She works for Maker and she's also part of the Ethereum community and she's going to introduce herself and also maybe talk a little bit about this app that we've been developing. Hey everyone, I'm Nadia.
00:30:26.082 - 00:31:48.486, Speaker A: Thanks Demi for the intro. So, yeah, I work for Maker to bring Dai adoption in the region, but also I'm part of this community, the Bobata community. And I found out about Coinosis and this is an awesome project because we are using this project not just in the Bobota community, but also in all the regions to teach people more about crypto, Ethereum and what you can do with your crypto once you have it. And for us in Latin America, it's very important to be aware of this kind of information because of our devaluated currencies. And it's important to give people information about different options they have when they need to keep the value of their savings. So with Kainosis, we are working in different courses about not just development, but also about DeFi and what a regular user can do to obtain, for example, Dai and start using that Dai into the DeFi economy. So that's very awesome.
00:31:48.486 - 00:32:45.634, Speaker A: And I think it's super cool because all these material will be available in Spanish and will develop the Spanish speakers community. So that's great. Exactly. And that high quality content in Spanish is also being an applied project because we are going to have real money, but the participants, they don't have to risk their funds. So with the methodology, as you can see, I can send Claps for the introduction, as Nadia did, of the platform and those rewards are going to be matched with the yield of the products that we are going to be learning how to invest. So this is a hands on, like really practical thing where users can be on board on the crypto world and they can see how this work. And also we are into high quality information and trusted projects.
00:32:45.634 - 00:33:40.874, Speaker A: So this is part of our mission, teach people to develop, as we said in the first part of the video, doing our weekly meetups, but also having this type of real experience with crypto and investment and DeFi. So we're excited to be part of ETH online hackathon and always bringing new people to this awesome community. Thank you for having us. Bye. Last but not least, we are waiting for you here in Dafka. Awesome. So those were three amazing overviews of how the local communities in Honduras, Venezuela and Bogota are developing and I can't wait to check out and meet all these people in person next year and just learn a bit more about how we can help increase more people to join the space.
00:33:40.874 - 00:34:40.400, Speaker A: So we're going to be doing a lot more of overview of how ethereum has impact our ecosystem and community in about two weeks with our final Ecosystem Summit. But what I want to do now is actually introduce a special guest and I want to actually welcome Simona Pop here on this call. So Simona, I know you're here, I'll just ask you to turn your camera on and Simona will be joining us as a co MC for the rest of the summit and we want to just use this as an opportunity to get a lot more people to be involved in need online. So Simon has been one of our favorite people in this space and helped us get to where we are. So I'm super excited to welcome her here and I'll let her introduce herself briefly. Hello. Hello everyone.
00:34:40.400 - 00:35:34.970, Speaker A: Great to see you all. Even though I am pretending you are all in front of me, listening intently to what I have to say. It's great to be back at an East Global event, even though it is under these circumstances, and probably why I've chosen to look like a French Steve Jobs, because what else are we going to do with our free time, super exciting lineup today? I'm going to have the pleasure of introducing some of our speakers and then I'm going to hand off to our next MC. But in the meantime, I am very much looking forward to hearing about the state of L2. Hot, hot topic. And Sandeep, co founder and CEO of Matic, is going to take us through all of that. So very exciting.
00:35:34.970 - 00:36:02.310, Speaker A: Sandeep, are you testing? Testing. Perfect. Yeah. Okay, so I can't share my screen. Could you give me the rights to share the screen? Simona? Yeah, it looks like we'll just I got it. Yeah. Okay.
00:36:02.310 - 00:36:53.270, Speaker A: I think I hope my screen is visible. We're good to go. Yeah. Okay. So, yeah, as Simona said, that very hot, hot topic right now. Where exactly is L2 and what exactly is happening in the space, especially with the crazy gas fees on the Ethereum Main net. So primarily the approach or the agenda for the overall discussion would be to have a look at the higher level parameters on which all L2s are evaluated.
00:36:53.270 - 00:37:48.690, Speaker A: And then I would like to go through the individual categories. That what all L2 approaches are being tried out currently and which all projects are kind of following those approach and what is current status in some of those. So this would be as per my current understanding, and I might have missed naming a few projects here and there, it is completely unintentional. Okay, so let's first get into that. What are the primary parameters for identifying L2? So one of the biggest thing that you would always come across is the data availability. By the data availability, I mean that when the off chain execution, L2 chain execution, right. Something that is being executed off Ethereum Main chain.
00:37:48.690 - 00:38:40.074, Speaker A: So the primary question for any L2 approach is where the data is being stored, right? And that's one of the key identifiers for L2. So for example, solutions like Plasma or Validum or like any simple side chains, they would be storing the data off chain. That means the data is being stored by an off chain mechanism. And those mechanisms can also differ in their approaches. So for example, for optimistic roll ups, which are kind of very hot right now, for them, the data is actually stored on chain. That means the transaction data is actually stored on chain. So those are the kind of primary differences in terms of the data availability of approach.
00:38:40.074 - 00:39:54.610, Speaker A: Right. Second thing which is very critical is that how are disputes or frauds detected or prevented in the sense that, let's say you are having $100,000 on the side chain or let's say or the L2 has $1 billion in the total value on the side chain on the L2. And then the L2 actors decide to do something fraudulent or some particular person using or some particular user tries to defraud the L2 and tries to cash out more funds than they own on the L2 from the layer one. That is the ethereum engine. Then how do you solve those disputes? How does the frauds kind of detected and kind of mitigated in that sense? So that is also like the second big for judging a L2. So in that sense, I found a very good this was a very good post from Starquare where they kind of differentiated between validity proofs and fault proof. So fault proofs is nothing but fraud proof, essentially.
00:39:54.610 - 00:40:35.602, Speaker A: And then on the y axis they have the data, whether the data is on chain or data is kept off chain. And on the x axis you can see that whether it's validity proof or fraud proof. And then they have mentioned about where you can identify a particular approach. So, for example, plasma. So your data of the L2 that means whatever transactions are being executed, the data is being kept off chain for a single operator plasma or single operator plasma. Like maybe OmiseGO there the data is being stored by the single operator whoever is running or a few parties that are running optimistic. Sorry.
00:40:35.602 - 00:41:48.546, Speaker A: OmiseGO plasma in case of for example, matic have 100 plus validators who would be running that plasma, right? So they are storing the data and many would ask that what is the need of a multi operator plasma, right? So, like example Omissego, where a few people would be running or like a single operator would be running the side chain where the transactions are happening versus a matic. So the answer lies in the data availability. So when we have since the plasma is storing data off chain, in this scenario, the data availability is being ensured. Like for matic, why we feel that it's a better approach and this can be debatable and have been debated endlessly. But for us, we solve the data availability by building a robust POS set of operators instead of one operator running it, which can go down or can censor your transactions. Here you have 100 plus validators running the plasma over a POS chain, right? So that's one way of thinking about it. Similarly, for example, optimistic roll ups, as I explained.
00:41:48.546 - 00:43:02.938, Speaker A: So there they use fraud proofs and then I'll come to what is the difference between validity proofs and fault proofs also? Like fraud proofs also, but optimistic roll up. So data is as I said, the data is stored on chain because you actually store all the transaction call data onto Ethereum. And then since the transaction data is on Ethereum, the fraud detection or is much easier because you're. Storing the transaction call data and the state route. And then if somebody wanted to sort of withdraw more than what they are owed and then there will be a challenge period of, let's say from one day to seven days, within that challenge period somebody could come and based on the existing data they can try to challenge that transaction, right? That's what happened with, let's say optimistic roll ups. Similarly, another category of these fraud detection or prevention is validity proofs. So in case of fraud proofs, fraud proofs, you can think of it like somebody has run a fraud and then challenge period somebody can kind of come and challenge it and then kind of slash or crypto economically penalize the faulty person who is trying to withdraw in a fraudulent manner.
00:43:02.938 - 00:43:58.254, Speaker A: Instead of that you have validity proof, which is again very much debated that whether validity proofs are better than the fraud proofs and all that. And the reason being that in validity proof rather than having a particular transaction done on the main chain and then somebody challenging it, validity proof is at the time of withdrawal itself, you check whether this transaction or whether this withdrawal has a validity or not. So that is slightly better. In case of ZK rollups, again you have all the transaction data on chain. So in case of validium like Starquare, the data is off chain managed by what they call I think they have a committee system where the data is stored by the off chain participants and then on the main chain they run validity proof. So this is one of the critical areas like fraud proofs and detection mechanism. The third one and the simple one is that who is actually running the execution environment.
00:43:58.254 - 00:45:10.394, Speaker A: Like I gave you an example of let's say somebody running a single operator plasma versus a matic kind of environment where you have 100 plus validators running the L2 execution environment. Similarly, as I mentioned about validum, there also you have a committee of people who are executing or running the L2 apparatus optimistic role where the entire data is being stored on Ethereum. So you have full data guarantee, data availability is completely secure. And in terms of data availability, there are multiple approaches like Lizzie ledger is being tested out by group and all that. So these three are the primary parameters. Then let's go into the approaches of what all approaches have been for the L2. So the most earliest approach that have been tried is state channels where essentially you open a channel between two parties, you have some sort of guarantee on the Ethereum main chain and then these two parties take the execution off chain where let's say there is a chess game.
00:45:10.394 - 00:45:53.490, Speaker A: So you have a five Ether bet on that. So two parties will have a five Ether kind of contract on main chain. Then we take the execution and then we are playing the chess with each other, sharing transactions with each other. So you can see that the transactions like basically just the signatures are being shared peer to peer. So scalability is really high over there. But then if something goes into a dispute then we can bring all those transactions and then put them onto Ethereum and then verify that on Ethereum itself. And the Ethereum smart contract can identify that who actually was the winner or was going to win this thing and they can settle the dispute.
00:45:53.490 - 00:46:45.090, Speaker A: Problem is that the moment you do beyond executions then it's very hard to get all that data and put it into a smart contract execution and then solve the disputes. So that is one problem. Then you have other problems like what if one of the two parties or both parties lose their let's say they are playing a game on the mobile phone and then there is some problem with the mobile phone, then the data is lost and those kind of things. So that's why it has been till now fairly hard to implement state channels in a very significant manner. Like I think counterfactual seller, they have been toying with that approach. Bitcoin Lightning Network like I just mentioned it, just to give an analogy, bitcoin Lightning network is sort of a state channel mechanism. Connects network is doing very good work with the state channels.
00:46:45.090 - 00:47:41.878, Speaker A: They are kind of now focusing on the interoperability between chains. So like for example, we are also working with Connect's team where let's say from layer one to L2 withdrawals and deposits can be used via connect state channels so they can be much faster. Currently if you have to deposit onto let's say matic L2, it takes around seven, eight minutes because you need to have a confirmation on Ethereum main chain. Then the side chain actors need to do their consensus and then you get the credit on the L2, right? So instead of that you could use Connects like state channel which carries some liquidity. Similarly it's actually more prominent on the withdrawal side because the withdrawal you have to wait for at least 30 minutes if you are using simple POS security and around seven days if you are using Plasma. So can ensure almost instant kind of transactions for that. So that is one, I think upcoming use case of state channels.
00:47:41.878 - 00:48:49.146, Speaker A: But apart from that, apart from few games that are there on the seller platform, state channels are still in that evolving approach, evolving kind of mode and still for production ready state channel systems we are still not there. Similarly then there is a side chain kind of mechanism wherein where they can have a simple mechanism of running the consensus on the side chain and then you simply have a bridge between Ethereum and then the assets can move to and fro between side chains. But once the assets are on the side chain the problem is that once you have the assets on the side chain, then the assets are fully in the custody of the side chain actors. Like if side chain actors decide to collude and then decide to do anything fraudulent, then you can't really do much with that. So for example, Loom network had the Plasma chain. POA network was started like two, three years back. But again, because it has a proof of authority, that means seven, eight validators are running the side chain.
00:48:49.146 - 00:49:56.898, Speaker A: Then the system essentially becomes a trusted system. The same goes with XDI system also it's almost like a POA but then it die as the gas fees mechanism which provides slightly better user experience. But then the side chain actors are few in number seven, eight, maybe ten in number and then they have to do a consensus between them. And then if you are doing any deposits withdrawals, your funds are at the custody of seven or ten this thing players, if you consider only the POS approach, like as I said that and I'll cover it slightly later on, for example for Matic network. So if you are developing something which only relies on the proof of stake security of Matic, then also Matic EVM chain will also become a side chain. In that sense you can think of it, but only difference that we feel is there you have 100 plus validators. There's also one more criteria that whether the side chain actors are permissionless and decentralized.
00:49:56.898 - 00:50:38.526, Speaker A: So in case of Matic, anybody can come and become a validator on the side chain. Whereas with POA or XDI it's a proof of authority kind of system or Dao kind of system. So Dow has to decide who can come in as a validator or not. So those are slightly the differences in that sense. This I already covered then plasma. So in terms of plasma, what is actually happening is that you have a side chain environment where the execution is happening on the side chain side chain itself. But then when you are withdrawing, what Plasma has is fraud proof.
00:50:38.526 - 00:51:46.530, Speaker A: So I also showed the same table, right? So when you are withdrawing and in case there is a fraudulent kind of withdrawal or the side chain actors go rogue and try to do fraudulent exits or something like that, then you can go to the Ethereum main chain and actually run fraud proofs, for example, and then try to slash the fraudulent kind of actors. So with plasma these are the two main things like the assets, specifically the assets. Or in case of matic you have something called predicates. So anything that is predicate enabled, whether it's a smart contract or it's a payment, anything that is predicate enabled for that, if you don't trust the side chain, then you can go on to Ethereum and then challenge any particular transaction. And if you are able to successfully challenge it, then you get to slash all the people. Like in case of matic, I told you that you have 100 plus validators and 66% of them have to sign it. So if, let's say, they signed on a fraudulent transaction and it became a checkpoint.
00:51:46.530 - 00:52:30.454, Speaker A: So one important part of plasma is also that whatever is happening on the side chain, the side chain actors keep checkpointing it to the Ethereum main chain. You can also think of them as the commit chains, right? So using that commit, if you don't trust the side chain or you kind of want to challenge it, you can go to Ethereum and you can challenge any particular transaction. And if you are able to prove it, in case of Mandy, you get to slash at least 66% of the participants who actually signed on the fraudulent transaction. So that kind of provides you slightly more security. As I was already mentioning, OmiseGO doesn't have multiple validators over there. It has a few operators. I'm not sure whether only OmiseGO Foundation is running it or there are few external parties also.
00:52:30.454 - 00:53:32.102, Speaker A: But essentially you are like two, three participants are running or few participants are running the side chain. And then if something goes wrong, then essentially there is some level of trust involved in the side chain. Because one problem is that what if the side chain actor simply doesn't even share you the data? Like they blind you? That's called data withholding, right? So if they withhold the block data, then it might be difficult for you also to kind of go and challenge on Ethereum. So those kind of scenarios also emerge with less operators. In case of Matic, again, I might be biased because I'm speaking from Matic network, right? So being one of the co founders, but with Matic, we feel that having multiple hundred plus validators permissionless and WD decentralized, it's very hard for 66 of them to collude and try to do something fraudulent with the network. So, yeah, that's one approach. Other part also is censorship resistance.
00:53:32.102 - 00:54:49.678, Speaker A: So let's say if only few people are running the side chain or the L2 apparatus, then there is a possibility that some of the few actors actually might end up censoring a few transactions, right? So if you have more decentralized actors on the side, on the L2 apparatus, there is a possibility that there would be less degree of censorship, if at all. So that's about plasma. One issue with plasma that is always cited is mass exits. So mass exits is a scenario wherein, let's say, side chain goes rogue. Or maybe like the side chain operator that I said that if it is a single operator rogue, or in case of, let's say, Matic networks, let's assume in a worst case scenario, all 66% of them go rogue, then everyone will have to exit their funds to the main chain. And that scenario is called mass exit, right? And it has been cited as one of the bigger problems in the plasma design. And again, giving the perspective from the Matic side, we believe that if you have larger number like in this scenario, 100 plus validators, the scenario where mass exit can be a reality is almost negligible or highly kind of mitigated.
00:54:49.678 - 00:56:52.202, Speaker A: But let's say if you had less number of people running the L2 apparatus or running as operator, then it becomes like a significant problem, right? So that's why we believe that having kind of secures that and then plasma has fraud proofs, I already said also one more thing is that because you have to prove things on Ethereum, like with the fraud proofs, you essentially want to prove things on Ethereum. So if you have Gen Zig smart contracts, right, like an arbitrary smart contract which let's say some DeFi lending borrowing platform smart contract that you have, you cannot have a direct nobody right now there is no generic fraud proof mechanism for every smart contract. So, for example, in case of Matic, if let's say you have some standard operations, let's say fund transfers, DEXes and all that, for all of them you have something we call predicates, right? So you have those standard fraud proofs for those standard use cases. But let's say you had an arbitrary contract and you wanted them to be fraud proof, that means you wanted them to be verifiable on the main chain, then you need to write a specific predicate for it, right? So for example, we have been experimenting with Auger team, right? So building some fraud proofs alongside them or predicates for the Auger kind of markets and then I think with Eat Waterloo, I think Joey had also mentioned about them. So something like that in terms of for generic smart contract, even on Matic, although you have an EVM, but if you want to have your generic smart contract to be plasma fight, you could use like a simple you could rely on the POS security of these 100 plus validators. But if you wanted to use like you wanted to have the plasma verification available on Ethereum engine, then you would have to write your predicates. That's one criteria over there.
00:56:52.202 - 00:57:48.182, Speaker A: Now the roll ups, which are kind of younger in that whole category of various approaches and they are also picking up a lot of traction because as we mentioned, that mentioned about the data availability, because the data availability is on Ethereum itself. So there is more security in case of fraudulent transactions and things like that. So primarily there are two kinds of roll ups. There is one is optimistic roll up and then you have zero knowledge based roll ups. So in that sense, optimism is doing great work with the optimistic roll ups. And recently many of you guys would have seen the launch of the testnet of optimistic Ethereum. And Arbitrum is also one very important player there and they're also doing some cool work.
00:57:48.182 - 00:58:20.706, Speaker A: Plus you have other participants like Interstate accidentally, I think I've not mentioned Fuel Labs they are also doing pretty cool work on that. So that's there. And in terms of the ZK roll ups, I think Loop ring is one of the biggest example. Again, that is not mentioned over here. Apologies for that. So for ZK rollups, loop ring is there, ZK Sync is also I think ZK Sync is also building on ZK roll ups. So those are the examples.
00:58:20.706 - 00:59:28.300, Speaker A: Loop ring and ZK Sync would be the examples of ZK roll ups. And then from our side also we also are exploring with various approaches around the ZK based solutions. But yeah, more on that later. So with the roll ups you have higher Ethereum security but then since you are putting all the transaction data onto Ethereum itself, then there are scalability limits because you can only put certain amount of data on Ethereum, right? Essentially you are running those transactions and you are putting all the data onto the Ethereum engine. Then as of yet, I feel that the developer experience is also not much sorted because most of these solutions are still in the evolving stage and none of the solutions is production ready except I think Loop Ring which has a few applications but I'm not sure how many third party developers are building on that. But then Loop Ring has also been one of the more production ready solutions. ZK Sync also I hear few teams are using.
00:59:28.300 - 01:00:26.654, Speaker A: And then one of the other things is that till very recently, optimistic roll ups were used to be application specific. That means you have DAP specific roll ups and then that actually kind of hurts the composability side of things. But then if optimistic roll ups are able to evolve into generic shared chain kind of roll ups, that would be very interesting. And I believe Optimism and Arbitrum team both are doing really working hard on that. And if that comes in, I think it would be pretty cool. And then one other part is that since the roll ups are run by few parties like one, two, three, maybe five participants, then there is a higher chance of these parties colluding and then maybe doing censorship. So on the censorship resistance, I'm yet to see more formidable solutions on that.
01:00:26.654 - 01:01:28.450, Speaker A: There are some solutions proposed but I've not seen a production or even theoretically a more kind of sustainable solution on that. Lastly, these were the primary approaches. Lastly, I would take like four or five minutes on explaining what exactly is Matic network and then why it is getting so much traction. I'm pretty sure most of you guys would be hearing about a lot of applications building on Matic. Even now, some of the DeFi protocols are also looking to build on Matic and few of them actually went live recently got some good amount of tens of millions of TVL. So what Matic is like, I was explaining that it is a hybrid proof of stake and plasma. Why I say so because as I said that it's a multi operator plasma.
01:01:28.450 - 01:02:35.218, Speaker A: That means multiple operators like these proof of stake network of 100 plus validators. They are running the plasma automatic. So out of them 66% of them have to sign on a particular checkpoint and then it goes to Ethereum. And then having so many large number of operators, it also helps in terms of censorship, resistance and more decentralization and the data availability because we assume that if more than 66% of like it's very rare that more than 66% of the validators will collude and it's very hard for them to do that. And I think like 100% Cosmos kind of thing which is a layer one. So you have a layer one level of security on the L2 POS. So that's why many developers simply choose to build on the POS system because it's much more developer friendly and user friendly.
01:02:35.218 - 01:03:51.114, Speaker A: But then the solutions which might have high amount of money involved in that and they want Ethereum security, they might use plasma. Okay, so then developer one of the bigger benefits of Matic is that it's a full EVM based solution. So if you are building anything on this thing and I would actually love to request or invite every one of you, instead of let's say using a testnet, you could simply use Matic side chain. Even on the main chain you could run your entire application on the main chain and test it like hundreds of times. Because right now the fees are like I think around 10,000 to 10,0000 times, like 10,000 times less than Ethereum. So basically in one dollars you could do like maybe from 5000 to 10,000 transactions. So you could actually show your application running on main chain or maybe on our Mumbai testnet both are and the good thing is that on Matic network the block confirmation times are 2 seconds only, right? So you could go to Explorer, Matic network there are more than half a million transactions that have been done by various applications and the user experience goes top notch.
01:03:51.114 - 01:04:41.962, Speaker A: So I would love to invite all of you guys to use the Matic testnet as your testnet because the user experience and those things are much higher over there. And then acquiring test ETH from O one is much easier on Matic to acquire for testing, right? So use it as a testnet and see what your developer experience is like. And then if you like it then we would love to help you out. In terms of developer tooling, that is also one of the bigger benefits that most of your developer tooling which include like your wallets, your web, three JS, Oracles. So chain link for example. Matic is going to be the only second chain I think after Ethereum where chainlink is expected to launch their main net like the second. And then your remix.
01:04:41.962 - 01:05:45.582, Speaker A: ID your ganache and your truffle. Everything works out of the box. So that's why I think it would be a great start if many of you wanted to try it on the testnet side and have a better user experience. And then already we are seeing the number is around I think 70 even now and then 50 plus DApps but safely are on the main net right now. The number of transactions also today would have crossed like six hundred K, one hundred and thirty K plus active addresses and around $12 million in the TVL in the bridges. So for all of you who are looking to build DFI based applications also for the hackathon, would love to connect with you guys. And then we have a very solid developer support program where if you have a cool product which you think that you can take it to MVP Stage, we provide that developer from our side and we also have a builder with actually give out for early stage applications, right? So we give out around $50,000 monthly.
01:05:45.582 - 01:06:19.822, Speaker A: One of the DApps and that I think from the Gitcoin donation round and one of the DApps, I think they have got like around $11,000. So it's a good mechanism for you to kind of experiment with your application when you're early stage. And then we also help many of our applications recently, which are ecosystem applications, we help them to raise funds also and then all those things. So if any one of you goes to that stage after your hackathon and you want to reach out, please do reach out. We are there on the discord side. So that's all from my yeah, we'd love to take any questions. Great.
01:06:19.822 - 01:06:51.782, Speaker A: Thank you so much Sandeep. Actually we're going to move the questions over into the chat. So if anybody has any questions for Sandeep after this, you can easily find him in Live Ethonline.org. Thank you so much Sandeep. That was very exciting and complete. So thank you so much. And now we are going to invite on stage Ilya from near protocol.
01:06:51.782 - 01:07:16.660, Speaker A: He has just finished the Neoprotocol guys have just finished their hack, the Rainbow hackathon. That was a huge success. But Ilya is going to chat to us about Sharding for scalability and development experience. Ilya, can you hear us? Yeah. Thanks, Simona, for introduction. Well, it's great to be here and welcome everyone. I hope your hacking has been starting well.
01:07:16.660 - 01:08:21.800, Speaker A: Let me kind of talk through some of the Sharding and I'm sure with all the news around these two there's a lot of questions. So I'll try to kind of COVID some of the topics and surface some of the interesting questions. But if you have any other questions that I didn't cover, please ping them in the chat and we'll respond at the end. So first I wanted to just mention near itself. So near is a pro stake layer, one that's specifically built with experience in mind, which covers user experience and developer experience and scaling. If you think of it, is part of this kind of experience because as your users, as your user base grows, as your application gets more usage, you should not be dealing with scaling your application and also your users should not be dealing with high fees and figuring out what to do with all that. This stuff should be completely transparent and just work.
01:08:21.800 - 01:09:35.902, Speaker A: And so kind of the general idea is to build the experience that matches kind of the web two, right, the existing internet and in such a way that you can bring in non crypto people and they can just use applications. And it's been pretty important to obviously interoperate with Ethereum because this is where kind of most of the existing developers and users are. But we also want to onboard lots of developers who are coming new. So let's talk about scalability. So let's start with how you cannot scale, right? And so there's been a lot of kind of new consensus algorithms where people would be providing an idea how to scale the blockchains with some amazing consensus algorithm and being able to do stuff that is faster. There has been a lot of also like changing from blocks to trees to dags to some other data structures which are presumably allowed to do something faster or switching to different proof of, right? And to be clear, obviously proof of work has inherent limitation. And that limitation actually is in the fact that you first need to organize a block and then you need to mine it.
01:09:35.902 - 01:10:20.790, Speaker A: And that mining time is actually adds on top of your kind of execution time. And even then actually that is possible to pipeline. So in theory it's possible to optimize that and actually have throughput faster with proof of work. But it's still not the bottleneck. The actual bottleneck is not data structures, it's not consensus, it's not a proof of, it's actually just executing. And the reality is we want to run a smart contract platform, right? We want to run generic code and this code needs time to execute. And so this is actually just syncing the Ethereum blockchain with fully downloaded snapshot of all the blocks.
01:10:20.790 - 01:11:14.082, Speaker A: And so there's no network, there's no consensus, there is no data structures. It's just purely EVM execution. And as you see, there's a limit how much gas you can push through in a second. And so EVM itself on a kind of somewhat default hardware, right, maxes out at 200, 300 transactions per second of normal smart contracts. This is like real smart contracts on main net and this is a bottleneck, right? It doesn't matter if you switch to proof of something like proof of authority, it will be the same way. It doesn't matter if you have the magical pipelining and stuff like this. So the only other option like the way to scale is either significantly optimize the transaction processing, so this way includes changing from EVM to another kind of execution environment.
01:11:14.082 - 01:11:58.806, Speaker A: So WebAssembly is faster, but it has its own challenges. There is kind of Berkeley network, the Berkeley system that Solana for example, uses, which is faster, you can put execution on, for example, GPUs and stuff like this. The problem is all of that really works if you have a homogeneous load. Because if you only need to do payments, for example, you can scale pretty dramatically because you can parallelize that. You can have lots of processing power on one machine going really quickly and doing things in parallel. But at the end, actually, one of the core limitations is not an interpreter is actually hard drives, right? Just being able to randomly access hard drives is hard. And again, you can put more hard drives, but there's limit.
01:11:58.806 - 01:12:27.242, Speaker A: And so the other option is just split the work, right? I mean, that's an option that the whole internet is using, right? Google is not running on one machine. Facebook is not running on one machine. There's lots of machines running in parallel, processing things in parallel. And so this is what's called Sharding, right? I mean, this is not a new concept. It's been around for probably two decades, if not more. And the idea we need to scale up, we cannot have network pretty much everybody in the network doing exactly the same work. They need to paralyze.
01:12:27.242 - 01:13:47.306, Speaker A: The problem is this is very hard. And this is hard because there's a lot of kind of technical challenges on how to make sure that everybody's doing the right thing, how to make sure that the chain is correct when you don't actually run all of those things. So I'll just cover kind of like a little bit of a difference, but between the kind of default sharding approach and also what we've been working on. But the general idea, right, is you parallelize the work, which means you kind of need to do a bunch of stuff in parallel, which the kind of naive approach or like the approach everybody starts to take, including us. Is to paralyze the chains itself, run multiple chains in parallel and then in some way to kind of link them together and use this as a way to communicate between them. And so this is approach that originally like e two took it's approach originally we took it's approach that polka dot implemented it's approach that kind of if you think of cosmos as a sharded chain, which it's not and it's a little bit different but in general the concept is the same by scaling by a lot of chains. But one of the core issues with this, and we'll cover a lot of other interesting issues, is just like it's inherently complex and has a lot of kind of repercussions.
01:13:47.306 - 01:14:38.880, Speaker A: And so the approach we're taking is really paralyzing the blocks. And I also draw some parallels with other approaches as well. So what are the kind of technical challenges with sharding? Well, one of the big challenges is if you. Think of any proof of stake system. Usually if you have a one third malicious and network split, you can actually take over the network, right? So either stall it if you have one third plus one or of stake, or if you have network control you can actually do some fork of the network and one sort of stake. Usually you can incentivize growing the stake. It's still sadly in current proof stake systems we still have a pretty big concentration but hopefully over time this will grow.
01:14:38.880 - 01:15:52.790, Speaker A: And you also need to kind of track like you're pretty much saying that because we assume that there is less than one third of malicious actors bin on the network, then pretty much by induction, right, the whole chain is valid because every next set of validators is selected by the validators that were correct. And you need to actually have some intersection between when the validators are rotating because they need to actually download the state. So that's how this works. Sorry. The problem is that if you think when we're doing sharding, right, what happens is that in each shard you need an assumption of flex theory malicious. And so this leads to a problem that if you sample kind of for a long term we just sampled it and just run with that this validators can get corrupted, right? Let's say you have ten shards, 100 shards. Sampling into that will pretty much lead to probability.
01:15:52.790 - 01:16:51.660, Speaker A: If one third or even 20% is malicious, sampling into one of the shards can be higher. But also what's more important is that people can get adaptively corrupted right over time. So you need to actually rotate validators across the stack and make sure that they pretty much cannot know which shard they validating too much in advance. So that's where this thing come in is you pretty much rotate evaluators but you need to give them some heads up to download the state of the next shard. Because compared to a normal single chain, right, where all the nodes have the full state of the whole network, now you only have a partial state. And if you are responsible for validating next shard, you need to go and download the state of that shard before you are able to validate. Otherwise you'll be lagging and I'll mention data availability in a second.
01:16:51.660 - 01:18:08.180, Speaker A: So the other problem is what happens if somebody actually did attack, right? Let's say there is more than one third, like two third of malicious validators in one of the shards because in general the numbers are smaller of stake. And the problem is that the other shard cannot detect this very easily, right? It's not validating that shard, it's not actually validating state transition. It only kind of keeps track of that shard. The kind of attack would be to send, let's say invalid state. Like for example, say, well, all the money are mine on this shard and then send them to another shard to exchange and exit, right? And so maybe network everybody will detect this, but it will be too late and people already stole the money. So the general approach to this is called fisherman. And this is where somebody can pretty much tell the other shard that hey, something is wrong in this shard and they provide a fraud proof, right? And so this is similar to what we've just heard in kind of plasma's approach as well, where if something is wrong on a plasma, right, then somebody can provide a fraud proof to the ethereum and exit pretty much from this.
01:18:08.180 - 01:19:03.422, Speaker A: The problem here is and similar to plasma is data availability, right? So somebody needs to actually have a fraud proof generated in the first place to be able to prove that something went wrong, right? And so if this malicious actors did not produce a block, right, if they did not post the block to the network, then this, let's say honest actor in that shard has no way to really create a fraud proof and prove it to the other chain. The other side is obviously finality, right? So if you send a cross shot transaction and there's a fisherman, right, the fisherman needs some time. So in case of plasma, it's seven days. Big part of it obviously is because of gas issues and transaction censoring and bytes of other stuff. But this change period needs to exist. I mean, it can be shorter. But this is the finality of this cross shot transaction.
01:19:03.422 - 01:20:00.760, Speaker A: We cannot execute it until we make sure that there's no fraud proof coming in and there's no way to not prove there's not going to be a fraud proof, right? So there's no way to speed it up. And so this creates a lot of issues on just kind of creating a fast cross shard transactions between chains. The other thing is if you for example do speed up, then you need to actually roll back a bunch of stuff, right? You need to fork off. If you allow a transaction to go in and you just kind of revert it, you need to revert kind of cascading effect of all the shards that got affected. And this also can be very complicated. And then the data availability part is what I mentioned, right? If validators did not post invalid block, right, there's no way to prove that there is an invalid state transition. And next, validators cannot do anything about it.
01:20:00.760 - 01:21:25.890, Speaker A: So important is obviously to have a mechanism to provide data availability to the network. And so one of the main solutions that pretty much everybody uses, so it's us, polkadot and other folks is to pretty much use erasure codes. And so this is idea that when somebody produces a block, or in our case part of the block, they need to send out parts, they need to send out erasure coded parts of this block to other participants. And only if those other participants saw this parts, they will include this block as a valid, like they don't need to validate it, they don't need to make sure the state transition is correct, they just need to know that there's data availability. So what you do, you weave in into consensus, right? You actually include this into consensus that you can only accept those things that you saw parts of. And if you have enough of these parts, so enough people are tested to having these parts, then it means that actually from these people you will be able to reconstruct this block that was transmitted, right? So even the original block producers, or in our case chunk producers disappear. You'll be able to reconstruct just from these parts the full information.
01:21:25.890 - 01:23:37.802, Speaker A: And so just for context, razor coded means that it's a special encoding that even if you only have like in this case you have six parts to split the data into six parts and any two parts is enough to reconstruct the original data. So it's kind of a three x duplication of information that's like any of the two parts is sufficient. And so this mechanism is kind of at the core of our approach. And polkadot as well uses this to pretty much make sure the data is available across. So I mentioned before, so we transitioned from this idea of having multiple chains running in parallel to having actually one chain and sharding blocks, right? This actually allowed us to do a lot of things including because we have now one blockchain, this allows to really simplify the whole challenging things, right? Because if there is an issues that have happened at some block, we can just roll back the whole chain that contains all the malicious tradition, right? So we don't need to find a bunch of chains and figure out where they went wrong and roll them back. You just have this kind of one finality for all of them and so this provides kind of a better experience and it's also just easier to implement, that being one of the core things because obviously this stuff is very complicated and it's very hard to test. So the other thing is obviously it provides a better cross track transaction functionality and so we can move into this, I think a few things important to mention here, which if you think of it, what roll ups are and what plasmas are in a way is kind of a specific versions of this as well, right? It's all a way to run some kind of computation outside and then settle into the main chain where main chain is providing data availability and it's providing the kind of finality security, the challenging system, right? So in this case, in our case, we're using kind of our full chain to do this.
01:23:37.802 - 01:25:06.860, Speaker A: But you can imagine this full chain would be ethereum and then all the shards are just running as a parallel chains and that's kind of what plasmas and optimistic roll ups are. And then they have all the same problems that I mentioned. They have a seven day challenging periods to do this. If it's optimistic roll ups, fuzzy K roll ups, they can improve the full state transition so they don't have this problem. And this is kind of how it all links, right? In reality, all of those methods really just like different ways of handling these problems that I mentioned. Now the important piece is how to communicate between, right? So if you have a MultiChain environment or multi shard environment, or multi roll up environment, it actually becomes really complicated how to connect them together, right? And specifically, there's a lot of kind of issues around rollbacks, right? So if the other chain rolled back, what do you do? Right? If Ethereum rolled back, if you are optimistic roll up, you actually need to roll back as well, right? Because you're tracking Ethereum. If you are a separate chain and there's a rollback, what do you do? There's gas cost issues, right? It's because if you're trying to send a message from one chain to another chain, if the gas is different there, it's in a different token, right? Like, who's going to pay for it? How are you going to calculate when you're sending in transactions that you prepaid the right amounts and stuff like this? I need to attach another token or something else.
01:25:06.860 - 01:26:46.614, Speaker A: The critical state recovery is around roll ups is around rollbacks, right? If something happened, if a roll up producer stopped, what do you do? It's a mass exit. You roll back some stuff, you prove things, you make sure that you have all the data availability. But what happens with all the transactions that are in flight, right? What happens with all the cross shard? You were calling from here to here, what happened with them? And it's important to figure out what's the order of execution that will happen on the other side if that happens, right? If like optimistic roll up producer stopped everybody's mass exiting, but you actually had a proper exits as well. And they'll not get executed at the proper time because they all like waiting for seven day it's like what's going to be happening in all of those cases? And then the important part is composability, right? If you have two roll ups or two shards or two chains, right? If the communication between them is complicated, it has gas, it has all those seven day waiting periods, or even like a minute waiting periods, like composing contracts, composing applications become super complicated. And especially if you want to also receive a callback and hear back about what happened on Zelda Sharp. So with near approach, we have pretty much been focused on fixing all those issues and making the experience really smooth on how this works. And think of it, because we have one blockchain from a developer perspective, you actually don't need to think about kind of shards and sharding in general.
01:26:46.614 - 01:28:05.546, Speaker A: Like if you go to our Explorer right now, you actually will not see we're running one shard, we have another test network with four shards but you will not see any difference because the sharding is completely hidden from the developer. And the kind of main thing that we propagating is that all the contracts working asynchronously so the communication between them will not happen within one atomic transaction, it will happen within next block, right? So you're pretty much creating a message and you're sending it to another contract and another contract will execute it in the next block. And this is similar to message passing in Erlang or any other similar language. And first of all, it removes any kind of reentrancy problems. Second, if there's any rollbacks, this can be handled cleanly because all those receipts are kind of staying in the block and then you roll back, you restart it and you have all those receipts just coming in and you need to process them. They are mandatory to process before any other transactions coming in that are like all the receipts, like these messages passed from one contract to another kind of coming in in the beginning of the next block. It solves gas costs because we kind of pretty much like one of the core ideas is dynamically balancing contracts between shards.
01:28:05.546 - 01:28:54.538, Speaker A: And so this would allow to kind of even out the gas costs over time. So it will not be like immediate. Within a few hours you may have spikes, but over long term if there is some contracts that are more used than others, it will pretty much even them out and split them between different shards evenly maintaining kind of the balance. But importantly, it kind of has this compatibility property, right? Because now you can rely that contract will be delivered, right? And you'll get a callback. The only thing that you need to handle is the fact that it's not atomic, right? So you do need to handle kind of state reversals somewhat manually. There's like few tools to do that on developer side. But you do need to kind of handle a failure of the execution of sales contract.
01:28:54.538 - 01:29:37.466, Speaker A: The benefit is now you can handle the failure of execution of sales contracts which you actually couldn't on ethereum. So you can actually call contracts, see if it fails or not and execute some logic based on that. So think of it try catch with callbacks. It kind of allows you to attach obviously near as a resource, it allows you to attach information and receive it. So here's an example how this works. So you pretty much this is in Rust for WebAssembly contracts you can call is whitelisted based on some interface you provide. It the kind of contract that you want to call some arguments, no deposit in this case and somewhat of gas.
01:29:37.466 - 01:30:11.734, Speaker A: And then you have a way to attach a callback to this execution on one of the methods on the contract itself. So it pretty much will call it, it will execute something and then return and actually that contract can call something else and then process something. Get callback. Get callback, right? So you can actually have a pretty long chain of calls that processing. Now, this is very useful. You can actually have a very interesting multi shard contracts now built on across this. You can also shard your existing contract.
01:30:11.734 - 01:31:03.210, Speaker A: So for the users, like for example, for a token, we actually have some proposals for sharded tokens because tokens actually don't need to have all the accounts in one shard, right? You can actually shard them as well. And there's like few other applications that kind of need the scale of multiple shards, so going beyond even one shard. But there's also a lot of the DeFi specifically has some issues with Async calls. And this will be very clear because one of the kind of benefits of DeFi and I usually call DeFi as just a programmable exchange, right? So if you think of it, you deposit money into exchange and it gives you a bunch of tools. I'm talking about like Nasdaq or one of those exchanges. They have a bunch of tools already pre built on that exchange. You can trade, you can do stuff.
01:31:03.210 - 01:32:02.794, Speaker A: And within that, right, everything is atomic, everything very fast, nobody can front run you, hopefully. And then you can go to another. So what ethereum with DeFi provides in a way is ability to deploy new contracts, new code into this one kind of one exchange, one place. And so this is provided by ability to execute kind of atomic transactions, atomic across multiple calls. So it does become complicated to do that for Async calls. And one of the main issues is that because if you think of it, if you have two contracts that have Async and you have some kind of logic flying between them, people can try to front run you to go after this contract earlier on, right? And so there's ways to mitigate that in this kind of multi Async environment. But obviously, as the system grows, as you have more and more pieces working together, this becomes more and more complicated.
01:32:02.794 - 01:32:39.270, Speaker A: So there is a value in sync calls. And so we are actually just launching the EVM, what we call the vaults. So in a way, it's like one shard of a mean, it can obviously scale, but goes up to one shard. And within that EVM we have synchronous calls, right? So within that EVM, it behaves like ethereum. So you can think of it as just launching ethereum shards inside Nier. Actually, originally it was done as just a smart contract. So we literally compiled EVM code and just launched it as a smart contract on Nier and had like original initial testing.
01:32:39.270 - 01:33:14.610, Speaker A: But now we pre compiled it and kind of speed it up. So that gives us whatever the raw performance you can get from the EVM. You get that. The interesting thing is you can have many of them, so you can have multiple EVMs in parallel running and have async calls between them. So you can think of it like can have multiple exchanges or some other contracts maybe that are not exchanges, that dealing with in game items or whatever that don't care about specific DeFi stuff. And so you can still async communicate with them within one block. So you have all the same functionality and you can communicate with also other contracts.
01:33:14.610 - 01:34:23.350, Speaker A: And in a way this actually works as like you can call it realistic roll ups, right? Because what optimistic roll ups is you execute something off chain and then you post it on chain as a data and then you don't execute it, but then you wait for anybody pretty much optimistically there, but then somebody wants to challenge, you need to wait for a period here. You actually execute the state transition of EVM right away and it's like valid right away, right? So I call it realistic roll ups. I mean even better was if there's EVM that's running off chain and they just post all the data and gets valid on chain, but because we have 1 second blocks, you don't even need to do that, it pretty much gets the same performance. So each EVM vault will be as fast as optimism or any other kind of EVM engine running as a separate thing, but it will work within this whole environment and this kind of going forward. There's some ideas how to scale them in this app, but this is kind of future work. So before kind of ending, I have one last thing I want to mention. So Simone mentioned about our rainbow hackathon.
01:34:23.350 - 01:35:32.080, Speaker A: And so this idea is in a way from perspective of Ethereum. Near is kind of a small sharded environment in which you can run contracts and with EVMs it's like many EVMs and there's no kind of like with this bridge we're actually connecting these two environments. And from ethereum side. EVM is from ethereum side. Near is a side chain like many side chains connected together with EVM and with WebAssembly from near side, Ethereum is just another shard, right? Because you can actually call into it and get responses as callbacks and kind of have all the same functionality just with a little bit of lag on finality and all the timing. And so this is pretty much facilitated by light client smart contracts on both sides that are providing you pretty much this trustless decentralized way of connecting to chains. And it's done in a very generic way where you have multiple levels of smart contracts that pretty much provide connectivity and as a developer you can link in any way and prove any fact about zaza chain just to finish up.
01:35:32.080 - 01:36:25.422, Speaker A: One of the main things we've been doing is really focusing on education in the space so we actually have two series, one on YouTube and one as a podcast. One on YouTube is called Whiteboard Series, and it's about kind of talking about protocols and talking about kind of how they work on a whiteboard in depth. It has ethereum, two cosmos, polka dot optimism, matic, and I think there's like over 40 protocols now. And then the Open Web Collective is building. The Open Web Podcast is the podcast about entrepreneurs and builders and investors actually coming together and sharing information, sharing kind of insights on how to build businesses and how to build in this open Web. So keep building. Join us at Near Chat.
01:36:25.422 - 01:36:56.906, Speaker A: I mean, if you are interested in kind of expanding your business, we have a protocol agnostic Open Web Collective, which is really an incubator and accelerator for companies in open Web. And if you just want to learn more about Nier, there's Open Chat and I'm open for questions. Awesome. Thank you so much, Ilya. I think my favorite piece was Realistic rollups versus Optimistic. That was very in tune with real life. I love it.
01:36:56.906 - 01:37:27.606, Speaker A: So we did have a couple of questions from YouTube, but just to keep on schedule, what we will do is post those on Live. Eastonline.org and Ilya, if you wouldn't mind joining that so that you can address those questions. I think they're very interesting ones that deserve an answer from you. So the guys will share the link so you can join and hopefully get those answered. But thank you so much. Thank you.
01:37:27.606 - 01:37:57.230, Speaker A: Thank you. And now we are going to hear from Joe Andrews from Aztec, and he is also in the UK and also not drinking yet, even though it is evening here. And he is going to chat to us about a new L2 built with privacy at its core. My favorite, privacy. Joe, are you here? Yep. Let me just see if I can get the video working. Awesome.
01:37:57.230 - 01:38:29.100, Speaker A: There you are. And the stage is yours. Great. I just share my screen. Yeah? Yes. One secondary stuck in full screen mode. I can't get the PowerPoint going.
01:38:29.100 - 01:39:11.062, Speaker A: Love it. Technical difficulties, like in real life. Yes. Great. Has that worked for everyone? Yes, all good. Cool. So, yeah, very excited to be online here.
01:39:11.062 - 01:39:58.920, Speaker A: I think the last time I was at a ETH Global event was in person, but the online format seems to be working really well. So really excited to be a part of this today. My name is Joe, for those of you who don't know me, and I'm the co founder and head of product at Aztec. We haven't been in the community that much over the last six months because we've been in what we call stealth mode, working on the second iteration of our protocol. And I'm really excited to run that through with everyone today and talk about our ZK roll up L2, which is built with privacy at its core. So we only have about 30 minutes. So I'm going to try and keep the slides to about 15 and then do a 510 minutes demo and have some time at the end for questions.
01:39:58.920 - 01:40:43.326, Speaker A: We're just going to run over what we mean by RzK roll up L2. And in that we're going to go over kind of the difference between using Snarks for scaling and using Snarks for privacy. We're also going to talk about our testnet deployment on Robston and a little bit about Noir, which is our stealth contract programming language which can actually be used to program private transactions on the network. And then we'll finish with a demo. I think before we start, I think it's important to sometimes take a step back in the world of Ethereum. It moves very, very fast. It was only a year ago that we released our 1.0
01:40:43.326 - 01:41:28.910, Speaker A: protocol back in January 2020. This was the first time confidential transactions were on Main net. And back then the gas price was what seems astronomical today, but was close to a million gas. Shortly afterwards, Tornado Cash released anonymous transactions. And then in March, our research team came out with a breakthrough of Plonk, which is a universal Snark. This was the first time you could use a sole trusted setup for all Starks. And it's really kind of been the spearhead for all of the research we see in Snarks today and bringing that to kind of production.
01:41:28.910 - 01:42:36.460, Speaker A: And then more recently in June, we were very excited at Aztec just to see the sheer number of submissions for the Reddit Scaling Bake Off. I think it's been a real interesting kind of way for the community to rally behind the scaling cause and see basically the fruits of everyone's labor over the last year. So this week we've actually just gone live with our Testnet it's live on Robston. The public announcements will be going out on Monday, but the network kind of went live on Wednesday this week. And the headlines of the network are it allows you to do private sends of any ERC 20 for around 5000 gas per transaction. This is kind of nearly a 200 times reduction to the Aztec One protocol. And we're really excited just for users to be able to test that out today and kind of bring that to Mainnet over the coming months.
01:42:36.460 - 01:43:49.870, Speaker A: Another important part of the network is we've designed it to mirror the best practice social recovery principles that are kind of baked into most smart contract wallets today. So it's not like going back to Ethereum a few years ago where if you lose your keys, you lose your funds. The network is fully compatible with trusted guardians or social recovery from trusted third parties, which we're really excited about. And we'll talk briefly about how the network can be used to scale DeFi both from kind of simple token swaps with uniswap or more complicated investments on compound. And lastly, just a quick overview of the programming language Noir. So, taking a step back, I wanted to look at how, I guess, Aztec Two is possible. And the main reason for these kind of more extreme gas cost reductions that we've seen from Aztec Two is because of Plonk.
01:43:49.870 - 01:44:41.620, Speaker A: Planck is a universal Snark that our team has been developing for nearly a year now. But the paper, or the seminal paper, came out in February, March of this year. The Snark itself is different to most Snarks in the fact that it's a universal Snark, which means that it doesn't need a trusted setup, or the trusted setup is universal for every single blanc circuit. There doesn't need to be a fresh setup for each instance of a Snark circuit. It's also significantly more efficient for certain operations than traditional R One CS proving systems. This allows us to do recursion inside Snarks, which means that one Snark proof can prove another Snark proof. And this is core to the Aztec network, as we'll see in a second.
01:44:41.620 - 01:46:24.930, Speaker A: And just to bring that back to how this kind of efficiency makes, as we believe, ZK Snarks usable today, it allows us to do things inside Snark circuits that have not really been possible or feasible before. So using Plonk and the Aztec network, we can validate signatures over ethereum signatures and the curve that ethereum is defined by and also curve 25519, which is the standard curve inside iPhones and most TPM modules. This breakthrough, I think, is something that will massively improve the usability of layer Twos and kind of help to make the applications which run on them a lot more trustless. Another on efficiency, we've managed to get the proof construction time down to ten to 20 seconds, even running on most Android phones, which we're really excited about. So taking a step back here, we just wanted 1 second one. At the core of the Aztec network is a private transaction. And what we mean by this is users can send funds privately.
01:46:24.930 - 01:55:13.820, Speaker A: Can everyone hear me? Yes. Is everything okay? Joe? Joe, can you hear us? Hello. I'm just going to try and rejoin. I think my Internet connection is not working. We could see everything okay and we could hear you okay? So we'll just switch to a quick break whilst we sort out Joe's technical problems and we'll be right back. It, can you hear me now? Yes, we can. We'll just resume in two minutes, if that's okay, Joe? Okay, so, bit of a technical difficulty situation.
01:55:13.820 - 01:55:49.218, Speaker A: We're trying to recreate the IRL experience as closely as we can, but we lost Joe momentarily. There was some sort of Internet outage. Nothing to worry about in London, but he will rejoin and do his talk on ASIC 2.0 at 330 Est. So he'll be able to join then. So please tune in then if you were waiting to see Joe, he will be back. We are sorting everything out and he will be joining.
01:55:49.218 - 01:56:32.390, Speaker A: US to chat all of the good stuff at 330 Est. So join in then. In the meantime, we will move to what was on the schedule anyway, so we're not delaying any proceedings. We're going to get started on Phil and Georgia's talking about minor extractable value. Guys, are you here? No, they're still getting prepped. What shall we chat about? I mean this is the final talk that I will be introducing. I will hand over to Kartik because it is much earlier in PST.
01:56:32.390 - 01:57:24.998, Speaker A: It is Friday evening here, so in the spirit of ETH Global events, I will be off to the pub and I wish you could all join me. I miss you all terribly, but there's a lot of good stuff, a lot of good talks happening throughout. So I think we're going for a good another 3 hours or so. Like I say, a lot of exciting things happening. Be sure to again check into Live East Online for any kind of questions for answers. Hopefully Ilya is there already answering the questions that came through to his talk and then what we will do is essentially okay. So they're still not here.
01:57:24.998 - 01:57:49.630, Speaker A: We are rolling. It's all good. Everybody excited for all the other events that are happening. This is a month long event as we know, ETH Online, biggest one so far. And Phil has arrived. Excellent. Georgios, hopefully on his footsteps soon to join us.
01:57:49.630 - 01:59:10.198, Speaker A: But yes, so it's a month long event, there will be these summits happening every Friday again very in the likeness of the East Global live events and then the hacking happens until the end of the month. I'm excited getting to judge again, so I'm very excited about all of the great projects that hopefully, as opposed to let's say standard hackathons that just go on for 48 hours a month tends to be enough time for some really exciting projects and great new ideas coming out of this. Great news about DevCon as well. So hopefully next year we will all be together again, I'm sure. Well, I'm being very optimistic roll up here, but I'm basically hoping that we'll be able to get together even before Defcon. But if we don't, let's aim for that one, I think it's going to be a great event and we're prepping some exciting things in the run up both with if scholars and with other engagements as well. Georgios has just arrived, just bang on time.
01:59:10.198 - 01:59:39.940, Speaker A: So what we will do is we're going to get them up on the virtual stage, get them all ready. Are we all ready? Yes we are. And they will be able to chat to you about minor extractable. Hello. Hello. Welcome. The stage is yours and off you go.
01:59:39.940 - 02:01:02.826, Speaker A: Hello everyone, my name is Georges, I work with Paradigm and I'm here to talk with Phil Diane who will introduce himself really shortly about Minnet extractable value, a topic which I believe we will hear more and more about in Ethereum. So Phil, would you want to introduce yourself and we can get into it? Yeah, I'm Phil. I'm a PhD student at Cornell University. I work on smart contract security and distributed system security in general particularly, I'm interested in systems currently that have sort of economic forms of security as well as traditional sort of computational based notions of security or even that blend the two. And part of this interest has been in the past in Ethereum and in DEXes on Ethereum and many, many other projects on Ethereum that I've studied or helped with in various capacities. And I guess today we're talking about mev, which I first encountered in my work on DEXes in 2017 kind of time period and kind of coined the term as a way to describe this implicit fee that we saw as potentially changing the game theoretic kind of consensus economic game between miners on Ethereum. So yeah, that's my backstory in a nutshell, I guess.
02:01:02.826 - 02:01:32.130, Speaker A: Nice. Yeah. And somebody that reads about the Flashboys 2.0 paper might say that you and your co authors on that paper are kind of like the fathers of mev. So it's very exciting. So would you be able to give us a succinct definition of mev and give us some intuition on why it can be dangerous for Ethereum or destabilizing to some extent? Yeah, absolutely. So in a nutshell, mev is basically the profit or the value.
02:01:32.130 - 02:01:57.670, Speaker A: It stands for minor extractable value. So it's the value that a miner is able. The able is very important. It doesn't matter whether it actually happens or not to extract from the Ethereum network when they create a block. So mev kind of as a whole comes from several different places in the Lucius form. One of them is the block reward and the other one is transaction fees. Those are both kind of extensively studied in the past as sources of fees for cryptocurrency.
02:01:57.670 - 02:02:57.440, Speaker A: But there's two other sources that I think are very interesting and that apply specifically to smart contract platforms like Ethereum. One of them is transaction reordering. So changing the order in which things happen on the network either to prefer kind of certain users transactions over others or assign certain profits to certain trading bots over others or whatever the case might be. Or even for miners to insert their own transactions that actively interfere with or take advantage of or use depending on what the nature of the transactions is these systems. So it kind of looks at the value with all the degrees of freedom a miner has in the protocol that a miner can use to increase their ETH balance in one Ethereum block or across a range of Ethereum blocks. Why is this destabilizing? Well, the simplest intuition is imagine if there's almost no mev from reordering in the system and then one block comes along where suddenly it's really profitable to control the order of that block. Let's say ampleforth has just rebased and created a massive arbitrage opportunity on decentralized exchanges or something like that.
02:02:57.440 - 02:03:40.702, Speaker A: So in this one block, it may be hundreds or thousands of times more valuable in terms of mev than the block that came before it. So now the next miner that comes along has a choice of which block to mine on. Do they kind of ignore this block that just extracted all this mev and try to take that mev for themselves? Or do they extend it like they're supposed to in the honest blockchain protocol and just kind of go for fees and block reward? Typically you call this time banded attack in your papers, if I'm not mistaken. Yes. So time banded attacks specifically are also you can do this over multiple blocks, so you can do an even longer 51% attack. That's a great point. I will tie that in.
02:03:40.702 - 02:04:23.498, Speaker A: But yeah, basically this makes consensus extremely can you still hear me? Sorry. Yeah. Makes consensus potentially unstable. One important thing to note about time banded attacks is because it's hard to figure out the mev in a block, it becomes more profitable. The more transactions you have access to, the more reorderings you can do. And so it's actually like super linear in time, basically the number of profits you can make from a time banded attack, which I think is a super interesting point. So a time banded attack can happen only in a chain that has reorganizations, kind of in a Nakamoto like consensus where you can choose to build on the latest block or you can start a new fork from the past.
02:04:23.498 - 02:04:54.920, Speaker A: So is it the case that mev is less of an issue in a chain, such as something that's built on Cosmos or Tendermint, where they use a Byzantine fault tolerant consensus with no forking? Yeah. I think this is an interesting conversation, especially with you, George Osin. I'd love to hear your thoughts here because you're a POS and an L two kind of guy. Yeah. In some way. Yeah. Let me challenge you a little bit and then I'll throw it to you.
02:04:54.920 - 02:05:40.946, Speaker A: I would say that the high level answer and the one that's in the Flash Boys paper, if you read, like, the last last section, is that maybe long term reorgs are going to be harder. So you won't see huge, like 51% style attacks because of finality, because it's kind of hard to reorg past this finality checkpoint. And the real fundamental reason for that is just almost a social contract of the community. Like if you reorg past finality, the community won't consider that valid. Even if the software says it's valid, I think people will reject that and there's other incentives to sort of prevent that from happening. On the other hand, I think those incentives also kind of are subtly there in proof of work chains too. Like if someone came along with 100 block time bandit attack on bitcoin, it's not clear whether people would accept what the client says and say yes, this is the most proof of work.
02:05:40.946 - 02:06:22.114, Speaker A: Or will they say like no, obviously this was mined retroactively and it's a malicious thing. So I think a big part of it is the social contract of the community. Like POS kind of formalizes that a little bit more and maybe gives you this distinct boundary past which time banded attacks are hard. I still think they're an issue for kind of the interim process of how you actually decide on that finalized checkpoint and that order. There's all sorts of issues there that can be discussed. But yeah, I would posit to you and I would love to hear your take on this, that really they're kind of equivalent in that in both you could possibly have this long term economic incentive. But in both proof of work and proof of stake there's like some reorg point past which the community will just be like no.
02:06:22.114 - 02:07:10.826, Speaker A: And in proof of stake it's a little bit more explicit because there's this finality discrete notion. Right? So I think I can push back on this. So firstly, I think that we have a disconnect in the sense that there's the BFT proof of stake flavor, which is what Tendermint does, and there is whatever we call it proof of stake that ethereum does. So the ethereum two will do. So the proof of stake that ethereum will do, it takes about twelve to 18 minutes to reach finality for a transaction. And that's because you need to get two epochs to finalize to justify, which means that in that period, indeed you can have some short reorgs and indeed the time banded attack can still happen. But in Tendermint, once a block has been signed on by all the validators, it's no longer possible to create a valid block.
02:07:10.826 - 02:08:21.494, Speaker A: In the past it will get rejected by the clients that are synced to the chain. So I think that you do not have time bandit attacks in Turner mint flavored versions. However, there still can be mev until you reach consensus on the next block. So, for example, while we're still gossiping our pre commits and our commits for the next BFT round, maybe a minor validator comes in and says exclude this transaction because it will give you more because I will bribe you for whatever amount and then it allows you to destabilize that. So potentially what I think that it could cause is that instead of reaching consensus in 5 seconds, maybe it takes you eight or 9 seconds to do that. So it's more like a performance deterioration rather than an actual destabilizer to the extent that it created security. So, moving forward from this, how big do you think that the mev market is? How much mev exists and is it bigger than the transaction fee market will it be? Yeah.
02:08:21.494 - 02:09:09.778, Speaker A: So I wish I had a better answer to that question. I mean, we lower bounded it in Flashboys 2.0, but that was a very conservative lower bound where we only looked at one type of transaction on one type of decks. There's a few different ways to answer this question. You can go for just looking at what people are extracting and I think a decent proxy for that is the transaction fee market. I think the mev market is somewhat linearly correlated to the transaction fee market, maybe a little bit more than the transaction fee market in terms of what bots are actually exploiting. But the actual mev that's available to be exploited by bots is much harder to study because you have to basically ask yourself what is the optimal order of transactions a miner can do in this next block to profit? And that's a really hard computer science problem, pretty much intractable.
02:09:09.778 - 02:09:54.934, Speaker A: So it would include all hacks and all ARBs and things like that. So I would expect the mev market in reality, like the real deal, if you measured it that way, what you would see is approximately the size of the DeFi security market, which is probably like one order of magnitude. And this is just me BSing. My intuition here kind of smaller than the DeFi market. We love your intuition, Phil. If we hear your intuition, that's great. So do you think that there would be formal methods, for example, to find all the available mev inside the block or even kind of calculate the optimal permutation of transactions so that we can pull from the mempool to maximize the mev 100%? I do, and I would say stay tuned on that front.
02:09:54.934 - 02:10:32.320, Speaker A: I'm definitely planning on publishing some work related to this and I'm sure I'm not the only person in computer science who's going to do that. So yeah, I think we'll definitely see all sorts of formal techniques. I hope there will be nice space of competition here. I think formal verification historically has been very murky, the incentives to actually do it because it's expensive and hard and the value is not immediately visible up front a lot of the times. So I hope this will actually be like a nice playground for those techniques to become profitable and maybe even be able to fund their own development eventually. Fantastic. So we will all be looking forward to that.
02:10:32.320 - 02:11:40.760, Speaker A: So far, how does the mev market, in your opinion, differ in Ethereum rather than in Bitcoin? Is it that due to the composability available in Ethereum that its mev market will always be larger than Bitcoin? Yeah, it's hard to know. I mean, Bitcoin has some forms of mev because the market is larger. It's obviously profitable. If you're a, let's say, large market maker on coinbase, and you know the addresses of your competition, and you can bribe miners, say, to even delay them by a block with some probability, that might be a worthwhile thing, for you to do because in terms of market shocks and market events you might be able to move a little bit faster and have longer term like a more profitable equilibrium than they do. So I think it exists in bitcoin. It's much more second order and therefore harder to quantify. And it depends on the real economics of bitcoin in a way that I think by design we'll never be able to know or measure unless maybe we see some attacks come out down the line.
02:11:40.760 - 02:12:18.702, Speaker A: Does L2 increase mev? For example, lightning creates a lot more opportunities to censor a transaction or reordered transaction. That's a great point. Yeah, I think L2 hugely increases mev. And I think that's one of the things the lightning skeptics early on in bitcoin pointed out, that, look, you're moving all your security to another layer, but the miners incentives kind of change when you do that. And I think that was a poorly articulated case back then. But yeah, I think we can probably formally calculate how much. And my guess is if lightning is the scaling future of bitcoin, that will probably be substantial.
02:12:18.702 - 02:13:26.586, Speaker A: So that is a great point. How does smart contract composability relate to mev? And I imagine that a lot of our audience will be protocol or smart contract developers. So how would you recommend to them to design their software to minimize mev? What are some best practices? Yeah, those are both great questions. So I'll take the best practices one first because I think it's a little bit easier and more poorly defined. So I think the best practices for mev right now is just to write an economic spec of what you're building and kind of try to reason through the mev and put it out there with your protocol in those. Early stages of having the white paper make it part of the design and part of the awareness of this whole process so that you are thinking about it and that you're showing people that you're thinking about it and that you get feedback from people when they're mev vectors that maybe you haven't thought of. I think soon we're going to see some more formal toolkits that will help people and I hope things will become easier, but I think that's one really good way in the future.
02:13:26.586 - 02:14:10.486, Speaker A: The other question is how mev relates to security and composability. I think they're all kind of the same thing. So in some ways, the miner is the most powerful permissionless enemy of your protocol. If you're designing a protocol, they can simulate any other cartel, any other set of people on the network, they can reorder transactions, they can censor, they can insert. So they're like strictly more powerful than anyone else on the network with your protocol. So the mev to me is kind of like a lower bound for your security and kind of speaks to in which sets and which cases miners kind of can or can't extract value from strategies in your protocol. So I think while you're doing your security analysis, mev will naturally kind of fall out.
02:14:10.486 - 02:14:59.446, Speaker A: And the last thing composability. So I'm hoping that we're going to release a paper on this soon out of Cornell, but to give you a sneak peek, and you can probably look at my twitter for some other sneak peeks. I think composability is also in some ways a function of mev. So composability is in some ways saying when I deploy my contract, the mev of the world is not going to increase that's basically saying that your contract will compose with everything that's out there because it won't substantially increase the degrees of freedom in either attacking other protocols or in taking money out of your protocol. So I think that's one way to reason about security. And I hope that we have a lot more information on that soon and a lot more kind of formal modeling. I know we will have a lot more of that soon.
02:14:59.446 - 02:15:51.746, Speaker A: So yeah, really looking forward to that. You have us all very excited about this. So while we call it mev, it has actually not been used as mev so far. It has been more like front runner extractable value because all the liquidator bots are just bidding up and doing all these priority gas auctions in the timescale of one block. So have there been any real examples of miners reordering transactions to pocket mev or inserting their transactions at the very top of the mempool of the block? Or has it just been or are miners still not awake to this? Yeah, so I've been talking to a lot of pools and reading about a lot of pools recently. So I've got my pools a little bit jumbled. I forgot if it was f two pool or spark pool.
02:15:51.746 - 02:17:02.222, Speaker A: And I don't want to say the wrong guys, I think it was f two pool, but I just want to make sure that I'm not saying the wrong thing. But I think they were the earliest instance of when I saw this a while back. There's an FC paper on this about it's a front running sok so if you google like front running space, sok space FC, you'll find this paper, I think it's by Jeremy Clark is on there along with a few other people, and they talk about how f two pool basically front ran an ICO. And in some ways that was one of the early forms of mev in 2017, was like, if I can get into this ICO pump and then dump it on the people a little bit later in the pump, that's a form of mev in some way. And they did do that. I think more recently, people have been DMing me a bunch of examples of pools that are doing arbitrage transactions that aren't ordered by gas and that are, let's say, preferring certain bots and doing things like that. So I think those trust based networks are currently being built and I think we're going to see more and more of to, if anyone cares to see it and shout out on what you said.
02:17:02.222 - 02:18:21.318, Speaker A: Last week, an anonymous Twitter account called Frank Researcher, who always puts out amazing research out of nowhere on Twitter, put out actually a case study of this where he found out, he or she found out that actually some miners were inserting low gas transactions on top of the block. And it was interesting because it is kind of like you can tell that there is something fishy happening here. Phil, what is the difference between front running and back running? And I think most of our audience possibly knows what front running is, but it would be helpful to get a full definition of both what are the differences and how are they impacting things in a different way? Yeah, so that's a good question. Front running, classically is a really old term. It deals with when people were trading on paper and people would kind of write orders and walk them across the room. And when you saw your competition kind of the fat guy was walking an order pad at 2 mph, you could go run in front of him and you knew this guy was probably buying based on the news you just saw. So you just basically sandwich someone and you take the best orders in the order book before they can or execute the best trade before they can.
02:18:21.318 - 02:19:02.950, Speaker A: Then they come in after. You kind of get whatever is left in the market and then you go and sell back. At now the higher rate of the market conditions they've created by buying. So you basically make a guaranteed profit, just almost guaranteed by wrapping this person's order in Ethereum, this profit kind of is guaranteed because you can do these atomic smart contract transactions where you do this sandwiching and you kind of just throw the whole transaction. So in some ways it's a little bit easier to analyze than even traditional front running. Sorry, and when you say sandwiching, you mean you insert a transaction before somebody does something, what they do happens and then you do another transaction. Typically that's what front running would look like.
02:19:02.950 - 02:19:30.074, Speaker A: And sandwiching is the particular term. Front running just refers to wanting to be there before them. Sandwiching is kind of a case of that where immediately after you also kind of liquidate or you get out. Yeah, exactly. You're trying to piggyback on them or whatever you're trying to do. It's also very common in high frequency trading. Like when people place large orders on Robinhood or something, they're routed through companies that have more direct lines to the exchange.
02:19:30.074 - 02:20:23.322, Speaker A: So they kind of triangulate you out and place a little order right before yours because they know your order is going to come in now and they know they can sell to you basically. So yeah, this is classically. It could either refer to being in front or very often to sandwiching back running is kind of a more crypto specific phenomenon, although I would argue it's also kind of a subset of this same kind of behavior and it refers to kind of getting in right after someone. So let's say after an Oracle update, how do you do that? Usually by spamming at a slightly lower gas price than they would put. So if they're going to do 25, you try to spam a lot of like 25 minus one guay transactions. And if miners sort by gas price, you hope. And what is the effect of these actions? So these are kind of actions that do not have a net benefit to the ethereum ecosystem, yet they have some kind of effect on gas prices.
02:20:23.322 - 02:21:13.994, Speaker A: So could you expand on that? Yeah, so it's hard to measure their effect. I mean, for sure they're paying a lot of gas and for sure they're putting a lot of transactions in that are raising the gas price and leading to this congestion. We've also seen bots that engage in bidding wars when there's just not enough throughput on the whole chain to handle everyone's kind of events that need to happen. So in general, they definitely drive these spikes that we've seen. They also cause a lot of negative externalities on the P to P network. So if you're just running a full node, many of these PGA transactions that we talk about in Flashboys and even some of these back runnings and other kinds of spams, the front running that we see, a lot of this stuff never even makes it to the blockchain. So you have nodes that process maybe 100 or 200 transactions for the one transaction that ends up getting mined.
02:21:13.994 - 02:22:09.842, Speaker A: And miners don't care about that because they get the revenue and they kind of pay the cost once. But if you're just running a node, you're not making that revenue. You're doing this out of your pocket for whatever business reason you have. And in some ways this is a tragedy of the commons that's like these people's bandwidth are what's being used to play these games on the network right now. Right. So Phil, currently how do you think that the liquidator software or the keypad software operators will evolve over time? Will it be like how mining got commoditized and it's now only big players in the game? So will there only be big player liquidators or will there be some more like an HFT kind of situation or will there be more hobbyists around in the future? Yeah, I think that's a great question. I don't know.
02:22:09.842 - 02:22:59.870, Speaker A: I think either future is in the cone of possibilities right now. I would hope that it's not a few big players, but I think the economics are set up to promote that in the same way that a lot of market economics promote a lot of big players just because expertise will kind of naturally concentrate in a few places. And this is going to be a very kind of tight margin, very competitive kind of ecosystem. So I think there is definitely a realm of possibility here that you get a few actors who gain outsized influence on Ethereum through this. And I think as a community how to prevent this is like a conversation that we need to have moving forward on the social side of the ecosystem and also the R and D side. So I don't know. I hope it'll be a lot of small players but I think the economics are set up in the other direction.
02:22:59.870 - 02:24:04.760, Speaker A: I personally think what kinds of mev exist? Is there benign mev? Is there good mev or is there only bad mev or both or neutral? Can you give us some flavors? Yeah, I mean, I would imagine there's good mev, for example. This is the same question, I think, about general trading activity on Wall Street and in general there's a lot of parallels here, a lot of lessons to be learned. Like when is HFT and are these things parasitic and rent seeking versus when are they beneficial? It's a very hot debate. Some people would say they're always beneficial because in some ways they are part of the security model of the network and they should just be maximally exploited and users want to do this, so we should enable this in as little damaging way as possible. But still, it is what it is. Other people would say there's some mev that's more obviously benign. Like for example, providing liquidity providers the ability to respond to news and move their tokens around inside uniswap contracts could be one example of this.
02:24:04.760 - 02:25:21.054, Speaker A: Or another example could be matching orders on Ether Delta making that market more efficient and arbing some centralized exchange, bringing that liquidity into the blockchain. That could be fine if it's done at a reasonable liquidity provider premium that the user is aware of and okay, with no problem. You could on the other end have these hacks like the BZX attack or something like that where a bunch of people just lose their shirts and that's also mev. And everything in between I think also exists. How do you feel about unified approaches towards mev extraction such as the one taken by Kipper Dao? Just using saying that if you try to outbid me I will bid as high as possible so that it's not profitable for anyone. So this creates a kind of a new equilibrium where maybe it is better for everybody to collaborate instead of collude. Do you think that that approach has a future? Yeah, I mean, I think approaches like that are kind of going to have to I think it's interesting that kind of equilibrium is one we talked about a lot in Flashboys, and I think it originally developed out of an interest of these trading bots that are exploiting the mev to basically cheat miners and keep prices as low as possible amongst themselves and.
02:25:21.054 - 02:25:52.194, Speaker A: Then they were enforcing that cooperation. And if you break that, then they kind of make it unprofitable. And that could be a way to kind of socialize profits a little bit more efficiently and keep them off of the miners table. I think we're going to see that mechanism in a lot of different places or something like it. It's kind of a natural thing to evolve. Maybe one place could be in preventing time banded attacks. So like intentionally leaving money on the table for other miners or having some sort of intentional, transparent cartel that's like a good cartel.
02:25:52.194 - 02:26:35.014, Speaker A: I don't know if that such a thing exists. Got it. Yeah. A counterintuitive question as we're slowly running on time, is, is it possible for mev? Can be good for chain security. And this is more like a thought experiment, so let me lay that out. So in the absence of the block reward, you are relying only on transaction fees to align miner incentives. But what mev is, is more revenue going to miners from non block reward sources? Is it possible that you could fund miners with mev to have a chain without Isshan's work? Yeah, I think that's an interesting open question.
02:26:35.014 - 02:27:11.206, Speaker A: When I originally talked to Vitalik about mev was when the original infamous uniswap X times Y equals K post went on ETH research and I emailed a bunch of people and I was horrified. I was like, what is this? This is terrible for users because all these ARBs exist and the system is not very efficient, so they're going to get arbed. It took another bug. Yeah, exactly. And the response I got from at the time at least, was maybe this will be the future block subsidy instead of transaction rewards when they decay to zero. And yes, that is an argument. I think there is some possibility that we will see that equilibrium.
02:27:11.206 - 02:27:45.674, Speaker A: I think it depends on what the mev actually looks like. So if you have this very burst like mev, I think it's much more unlikely because systems in general don't like shocks. They don't like these crazy things that break the norm and suddenly change all the incentives. My worry is that mev looks more like that. On the other hand, if mev kind of becomes this predictable thing we know how to deal with, maybe we can reach that kind of a future and it could be a benign thing and instead of leading to instability, we could actually be a mechanism for stability. So I think the jury is still harnessing the power of chaos. Exactly.
02:27:45.674 - 02:28:19.640, Speaker A: Yeah. Fantastic. So I have a final question for you, Phil, and this comes from our audience. Do you think that mev and the EIP one five nine, an upcoming proposal for changing the block space market for Ethereum, have any relationship? So I think we both kind of agree here that they don't have a direct relationship at all. Like the EIP 1559 still allows you to give a tip, aka a bribe, aka extra gas that still lets you which depends on transaction ordering. Exactly. Yes, exactly.
02:28:19.640 - 02:28:45.290, Speaker A: From a fetch principles approach, mev comes from transaction ordering. The AIP does not affect transaction ordering. So they should be unrelated. Yes, I agree. There might be some edge case interactions like the fact that spam now has to burn more gas because of the way it's charged with the minimum, or weird things like that at the edges. But in the core I don't really see a fundamental difference. Fantastic.
02:28:45.290 - 02:29:11.522, Speaker A: Phil, I think we are at time. Can somebody confirm that we are at time? I think so too. Yes, are a time indeed. Fantastic. Thank you. So on hold on because I am going to bring Kartik up because there are a couple of extra questions from the audience. You guys have just it's been firing with all of the questions, so hang tight.
02:29:11.522 - 02:29:39.578, Speaker A: I'm going to say goodbye. Kartik, step in and I'll see you all soon hopefully. Thank you so much. Simona and Georges and Phil, thanks so much for a great talk. I do have a good camera set up for this event, so thanks for noticing. So we have a lot of questions coming from the chat and for those of you who are joining us now, you can just go to Live Ethonline.org and just see all the questions from the audience on the chat.
02:29:39.578 - 02:31:16.106, Speaker A: And what I'll do is I'll ask a couple of questions now to you both and if you have time, Georges and Phil, you can just go to Live Youth Online and just kind of answer the questions directly in chat for any follow ups. A lot of them are kind of small clarifications. So I'll just kind of ask a small one, which is in one of your comments, Georgia, you said something and one of our attendees would like a clarification, and that is, can you clarify the difference between ETH two, finality and BFT? Finality and kind of how do you differentiate the two? Yeah, so typically tendermint like systems they require so let's say that you have ten validators, they require that, or let's say that you have 100, you require that in order to produce the next block, 67 of the validators, they must sign on the blocks hash let's say. And once they've done that, you can move on to the next block and the previous block is considered final, the one that they just signed on. While in Ethereum Two finality only comes after. So Ethereum Two has one block per 6 seconds and one epoch per twelve per six minutes and it only finalizes a block due to how the fork choice rule works after two epochs. So there is a fundamental difference basically that in the if two kind of model, it takes you two epochs for a block to be finalized, but it has a kind of looser liveness requirement compared to the kind of quote unquote permission or tighter liveness requirements on BFT.
02:31:16.106 - 02:32:20.046, Speaker A: So how do I say there's a trade off between having a tighter consensus algorithm which gives you the finality, versus having something that's more loose while not being able to have the finality. So it's a bit hard to explain this more in detail in this time span, but I can expand offline if you want to email me. Please do. Yeah, I think if you're able to kind of comment on more on the chat, that'll be awesome. And then one final question for Phil, which is do proof of stake system solve mev in any way? And the question also says, my impression is that the system in which some entity has knowledge of the contents of a transaction before it's finalized should still have mev. So does POS help with any of this? Yeah, so we talk about this in the appendix of Flashboys, like we said, kind of. I think maybe the social charter of POS is a little better because of finality, so maybe it's harder to revert past finality and therefore you only care about this short term mev rather than also these long term reorgs.
02:32:20.046 - 02:33:22.226, Speaker A: But the short term mev is still problematic and still has the ability to affect the consensus protocol and slow it down. And I don't think POS or even leaderless protocols like there are claims a lot that protocols like avalanche that are kind of probabilistic and have no definitive finality or even no definitive consensus necessarily even those protocols, there's still some process by which the order is decided. And all mev really is, is saying that if you can influence that process through whatever protocol messages you can send, then that's a form of rent that's going to be economically relevant to how you make decisions in the network. So I don't think that changes you would be able to do to solve mev. You need to kind of separate execution from ordering. And you could do that with some fancy commit reveal scheme, others are proposing to use verifiable delay functions to do it. So you submit your order, encrypted the Miner orders, the transactions, and then after some time your order is revealed.
02:33:22.226 - 02:33:58.318, Speaker A: Instead of using a VDF, you could use threshold encryption. There's some solutions that you could do, but I don't think that these are easily doable at the base layer, so they would require a new system. Awesome. Well, Georgia and Phil, thank you so much for giving us your time today. And if you have time, there's a handful of questions and comments coming on, so feel free to answer them on the website. So with that, we'll move on to our next talk. And for this next talk, we're going to be covering how few labs is going to scale ethereum with their optimistic roll up approach.
02:33:58.318 - 02:34:19.740, Speaker A: And speaking will be Nick from the Fuel team. And I know Nick is already on the chat, so what I'll do is I'll hand this off to Nick and I'll have him share his screen and kick off the talk. Welcome, Nick. Awesome. Can you guys hear me and see me? Loud and clear. You're ready to share? All right. And I will share my screen.
02:34:19.740 - 02:34:45.026, Speaker A: Okay. And this goes to full screen. Is this full screen or am I in semi full screen? It's not a full screen yet, but if you do that, I'll just give you audio feedback. Get this to full screen. Okay. What you want to do is present instead of full. Okay.
02:34:45.026 - 02:35:07.740, Speaker A: On the top right? Okay, let's see here the button on the top right. There we go. Yes. Awesome. All right, so my name is Nick and I am presenting for the Fuel team. And we're doing optimistic roll up scaling. And I'll get into it because it seems like got a half hour, so let's do it.
02:35:07.740 - 02:36:02.380, Speaker A: So yeah, today we're just going to be going through our fuel optimistic roll up. Sort of what it brings to the table what UTXO models can do in terms of scaling and throughput and sort of our take and approach on where scaling is headed and where we think we can add value to the ecosystem in the space. So just to dive right in. So Fuel is a permissionless trustless L2 protocol for ethereum, for low cost, high throughput value transfer and exchange. And our main technique that we use is highly optimized optimistic roll up design. And our focus, which takes top priority over everything, is parallelism. So our designs feature sort of an approach where instead of designing for specifically the language down, we're designing for where we can get throughput up.
02:36:02.380 - 02:37:11.220, Speaker A: And we think we can offer a lot of interesting benefits with minimal trade offs using this technique. And so far we've been doing pretty well. So before we get into too many details, we have one announcement we'd like to make from the Fuel team, which is that currently with some of our most recent tests and thanks to the Hubble project and Barry WhiteHat, we were able to get 4300 CPS for transfers and swaps. So we think this is a significant increase and it's definitely putting us at a high throughput advantage over a lot of different designs. The main thing to bring home here is that it scales on consumer hardware. So we even think we might be able to validate blocks even on mobile phones, validating these large 4300 TPS blocks. This is obviously more fun than it is realistic, but it's something to be said and just given the way that our validation computation works.
02:37:11.220 - 02:38:04.130, Speaker A: So to get into what Fuel can do, because I think there's going to be more presentations on optimistic roll ups and more presentations on L2 and the trade offs. So we just thought we'd focus on what Fuel brings to the table and what it can do. So our current design features native colored coins. So we treat ETH ERC twenty S and NFTs as sort of first class citizens within the L2 itself. And this also falls into approach that we think layer one is still Base Settlement layer for all of these assets and Base Settlement for protocol design. But we think L2 is a good option to scale these assets. And so our focus is to bring a lot of throughput and scale to these assets and provide a service that can get you the most efficient scale.
02:38:04.130 - 02:38:58.260, Speaker A: The second thing that we'd like to focus on is multi user transactions or partially signed transactions. So with Fuel we allow for a trustless exchange between two or many parties within a single transaction. And you can think of this as offering a way to do noncustodial order book exchange or even other models that don't even feature an order book. So it allows for a lot of off chain settlement and a lot of off chain exchange that you wouldn't normally get if your transactions don't allow for multi users and multi tokens. So this is one really interesting aspect of using multi token UTXO model. So the next thing that we'd like to go over is a focus on the oper. So like some protocols like Bitcoin, we have an open opere for data.
02:38:58.260 - 02:40:07.260, Speaker A: And this allows for sort of arbitrary data blobs. And not only arbitrary in the sense that you can put any data there, but also arbitrary in the sense that they can be accessed or validated through our optimistic roll up contract and deployment on Ethereum. Which also means you can build other logic and realities around data posted on Fuel. So one application you could imagine being used for arbitrary data is something like decentralized Twitter where you're getting a sort of signed payload around data and then that's going on eventually into an Ethereum block somewhere. And what Fuel is doing is sort of organizing, publishing and making the process smooth and efficient to produce this data. So you can imagine a decentralized Twitter, you could imagine decentralized social networking or just anywhere you're officiating data using Ethereum and using our particular technique. So this is one really cool sort of aspect of what we can get done with Fuel right now.
02:40:07.260 - 02:40:57.974, Speaker A: The second thing which we think is incredibly important both to the L2 ecosystem and to Ethereum more broadly, is hash time lock contracts. So Ethereum has actually been able to do these since Mainnet was launched. And hash timelock contracts are a very simple mechanism to allow for cross chain atomic swaps, to allow for immediate withdrawals out of different D Five protocols or different L2s. And they allow for migrations as well. So they're a great tool for migrating from one version to another, say your own roll up. So, to talk about this a little more, the hash time lock design is quite simple. It just allows for parties on both sides of a blockchain reality.
02:40:57.974 - 02:41:39.602, Speaker A: So say one party on layer one and one party on L2 to be able to exchange in a way that allows the party on, say, L2 to easily exit to layer one. And you're doing that with just a hash and release function. So it's very simple, very simple contracts. It can be made relatively efficient and allows for bridging of these different protocols. You could also imagine bridging different layer ones. So going between, say, Ethereum and going between Avalanche or going between Ethereum and going between Xdai hash time locks provide a really great way to do this. And you can also imagine going between different optimistic roll ups.
02:41:39.602 - 02:42:47.230, Speaker A: So going between say, Fuel and optimism, or optimism and Arbitrum or Arbitrum and Fuel. So hash time lock contracts give us a lot of utility and they allow us to sort of jump chains where liquidity is available. So we support those natively and we support them across every token that we have. So the next thing that Fuel has a pretty big focus on is mass token airdrops. So Fuel supports pre signed transaction chains and this allows us to actually form interesting pattern using our eight input in eight output transaction formats. So essentially you can start with a single input in say, a deposit or a large amount of liquidity from a single input or deposit. And then effectively we can use that single deposit in a transaction chain and sort of merklize that or it's like a merkel tree and pre sign trees for dispersal such that users in the hundreds of thousands could be dispersed a transaction.
02:42:47.230 - 02:43:33.194, Speaker A: And you would only need to pay for the transactions that are getting picked up. So this allows for a unique pattern of dispersal and pickup for mass token airdrops. So we saw something similar with the uniswap AirDrop, which was really awesome. And this is sort of an extrapolation on that and a more unique or efficient way to do a similar thing. And per pickup, you could imagine with a merkel tree of several depths, a single pickup might cost three or four transactions, but in the end everyone gets dispersed their tokens and it's all relatively efficient. We've proved this out in our benchmarks, which are available in our docs, so you can check that out there. So going on to our next thing.
02:43:33.194 - 02:45:09.974, Speaker A: So, pre signed transaction chains, once again, we also allow for an interesting pattern where pre signed transaction chains can be used to do things like digital subscriptions. And so effectively you can assign say, twelve months of worth of transactions ahead of time using a single output and then only under certain pretenses or under certain circumstances, those transactions can be triggered on chain and then dispersed to whoever you're subscribing to. So this use case was referred to in the Reddit challenge and it was also something we were able to achieve quite easily with pre signed transactions. And so this is just another use case for having this kind of tech inside of our roll up as well. One other interesting aspect to having an eight input in eight output transaction format is that if you want to approach something like a privacy mixer, you can do all sorts of interesting mixing patterns. And we already have a project that's researching and experimenting on this, largely because as well, Bitcoin is just so damn limited that in order to experiment with the UTXO model, we're sort of the next best thing for them. And so you can imagine scenarios where you want some level of privacy and obviously you're not going to guarantee full privacy, you're not going to guarantee something like a bulletproof kind of level privacy.
02:45:09.974 - 02:46:33.750, Speaker A: What you will guarantee is that you're making it much more difficult for, say, your friends or local businesses or other things around you to be able to see into your transaction history easily. And I think this is one great aspect of having this as your base transaction layer, is that you can kind of get this sort of activity done. So in terms of our timeline and how much time do I have left? Do I have 15 minutes left? Yes, you do. Okay, just to check in on that. So in terms of our timeline right now for Fuel and our optimistic role of releases, right now we're targeting mid Q Four for a version one main net launch, and that includes all of the pre signed transaction tech, some really nice deposit and withdrawal tech as well, all the different features you saw today. And basically it'll open up the field a little bit for our type of execution model and sort of add more to the role of space. So that's our current main net launch, and it sits around 500 TPS in terms of throughput, which is still pretty good considering you're not using any new key formats.
02:46:33.750 - 02:47:10.290, Speaker A: So it's just vanilla ethereum keys and it should allow people to get a taste of what Fuel is about and why we're taking the approach we're taking. So that's our current main net launch timeline is around mid Q Four. So it's coming up. Our next major drop will be the end of Q Four. So this will feature our version 1.5 testnet. So this is where we'll be able to really turn up the heat and kind of achieve our highest throughput that we can think of right now, which is 4300 TPS.
02:47:10.290 - 02:47:58.090, Speaker A: And so you'll see sort of our compressed transaction formats, the use of BLS. And once again, thanks to the Hubble team and Barry for all the work they've been doing on BLS. It's been fantastic. And I think we're looking to make sure we have some kind of standards around that key format so that every optimistic roll up can benefit from the same kind of key format and we can kind of get rid of those signatures. So that's going to be our end of Q Four and then Q One 2021, we'll have our specification lined up for our virtual machine, which I'll talk about in a second. And then lastly a testnet for our version 2.0 release.
02:47:58.090 - 02:48:57.138, Speaker A: So that'll feature, hopefully if we can hit all our targets, a new virtual machine specification for Fuel which provide some nice abstractions over all the things we want to get done in the optimistic roll up context as well. It'll also feature how we get it done in parallel with mass throughput and in a sustainable way. So hopefully we'll get to that testnet release so long as we hit all our targets and don't have any huge major setbacks. So just a small word on our version two. So currently our version 2.0 is focused on UTXO based Ethereum style smart contracts. So we're still finalizing our specification for a virtual machine here, but effectively it'll provide one of, if not the most performance options for doing parallel transaction verification and particularly without global state.
02:48:57.138 - 02:50:13.914, Speaker A: So we've constantly made an effort to avoid global states and global state serialization. And so this really gives us a lot of opportunities for really efficient, really performant design patterns. And you can look at John's work on strict access lists and UTXO smart contracts. It's the link below there if you want to read more about what we're doing there. We really think given that the space has a lot of or will have a lot of EVM oriented and account oriented options for people to attempt to scale their work and their efforts, still using Ethereum as a base officiation layer, that UTXO based transactions will provide a huge throughput benefit. And hopefully we'll be able to really demonstrate that with a full virtual machine, basically without losing pretty much most of everything that everyone loves about the EVM and about Ethereum, but with also providing all the benefits of parallelism with minor trade offs. So just to talk about where to find us.
02:50:13.914 - 02:50:58.474, Speaker A: So our Twitter is fuel labs underscore website fuel. Sh GitHub and a discord link there. So yeah, we purposefully left I think, quite a bit of time for some questions and to go back and forth. I think we're pretty curious to see what people want to ask in terms of questions and how we're doing things and yeah, we can probably start that. Thanks Nick. Awesome. I think from our side, given the slight delay in the stream being looked at, our audience have a delay coming in with a question.
02:50:58.474 - 02:52:11.220, Speaker A: So I'll urge all of our audience to send questions as we are kind of talking. In the meantime, I think what will be super helpful for us is that we'll do is all get us started on just kind of comparing fuel to the other roll up approaches, kind of how do you think about what the trade offs are and sort of what's good or bad about the trade offs for each of the other solutions out there? I mean, we can just do that until we get more questions in. Yep. For, you know, from the beginning we decided to go the route of the optimistic roll ups. Our expertise is more around fraud proofs and optimistic roll up design and that's thanks to John's efforts on the team. And so that's the first thing is we're an optimistic roll up, not a ZK roll up. And I'd say the main thing we think about there is that we like the properties of the fraud proofs and the one week delays and we like those trade offs over ZK related trade offs and we think we can really achieve high throughput and block production without really losing much for security and for user experience.
02:52:11.220 - 02:53:07.986, Speaker A: And thanks to the efforts of Barry and the foundation, we get to benefit from aggregate signatures and aggregate signatures allow us to get rid of that troublesome signature data which really is where you're going to get the maximum amount of savings. So for us, yeah, I think our main focus versus the ZK roll ups is that we focus on aggregate signatures and we don't mind the trade offs of fraud proofs. We do think they're as or more secure in that sense. And once again, you can do block creation validation on easy consumer hardware. So it's really accessible and democratized. And then in comparison, just to quickly touch on it from the other optimistic roll ups, I'd say our main focus is that the other roll ups right now are focused on servicing what Ethereum currently is. And I think that's awesome and we need that kind of work.
02:53:07.986 - 02:54:03.640, Speaker A: But for us we're taking sort of a different approach where we think we can add a lot of benefit by showing people sort of another generation of what's possible and potentially trying to approach the problem from bottom up, so approaching it from parallelism up versus code and compiler down to what's possible. So that's more or less, I think, the differentiating factor for us and it's a different kind of work and it's a different kind of approach. But once again we're really happy with the way the ecosystem is developing and we're super excited for all the launches coming up. I think the space is really developing and we'll have lots of options. Awesome. So we have a couple more questions that have come in so I'll kind of just start asking them until we're at time. Next question is what other features besides TPS are most important when designing a roll up service and sort of what other things should we be excited about.
02:54:03.640 - 02:55:04.940, Speaker A: So I think when designing these roll ups, I think you want to really focus on your use case and what we need from the user perspective down. So if the user needs cheap transactions and they need it to be noncustodial and whatever the user needs, we should aim to provide them with. And I think we have a lot in the ecosystem, especially if you think about stablecoin access that we have. To me, stablecoins and lending protocols are still the hugest value add we have in the space. And I love all the fruit coins and everything, it's great. But if we can really scale and provide a lot of throughput and exchangeability in various formats for users, particularly of those stable coins, I think that's like one of the greatest benefits we can add to the world. So for me, I would just say with roll ups you should design them around specific use cases and try to target them specifically to what users need.
02:55:04.940 - 02:56:25.780, Speaker A: It's that simple. And if we're making that process better, then we're doing our jobs. No, it's great. I think this kind of sparks another question, and really fits nicely into it, is if you're imagining a world where there are use case specific role of services, how do you kind of think about solutions that help you do cross L2 transfers and kind of anywhere from frameworks to best practices? Or how should one think about that and how do you imagine that world panning out? Yeah, so I think that's going to be one of, if not the most important questions over the next year or two is what the hell is going to happen with liquidity and where is it going to go and what is it going to do? So I think we should look at ethereum layer one as still base settlement layer for issuing assets and for creating your protocols. I still think it gives you the maximum amount of options, but in terms of having liquidity on different systems, I think we should be open to once again really available hash time lock contract swap systems between these different systems so that we're not too locked into anything. This goes for being locked into not only different roll ups, being locked into different versions. So we will need easy ways to transit between different L2s and layer ones.
02:56:25.780 - 02:57:15.934, Speaker A: And so HTLCs provide basically all we need and I think even state channels might even give us even another level of speed there if we can do it right. So I think hash time lock contracts are just, they're a way to free us from these liquidity questions and we can just get on with experimenting with where development is going to go across the different L2s and layer ones. Because right now it's so early that if we lock up a lot of liquidity on anything, it'll either want to get off or it'll have to get off eventually. So I think we'll just touch and go and we'll see know things like hash time lock contracts can bring to the table. Yeah, got it. And we have another one. And if you don't have enough context, I'll ask Samuel to expand.
02:57:15.934 - 02:57:50.506, Speaker A: But if you do, I think it'll be great for the audience to get some context too. And. The question is, what do you make of the capital efficiency argument that was put forth by Starquare and kind of what do you think about that? And for the audience, if you can also just tell us what that argument is by Starquare before answering, that'd be great as well. Yeah, I have to say I just briefly skimmed that, so I don't know if I have a huge depth yet. I just saw that today. I'm currently off Twitter. No worries getting off the Hype.
02:57:50.506 - 02:58:38.714, Speaker A: You don't have to directly answer that. We'll follow up with a different question there. And the question is how does delayed finality affect composability and how long would it take for fuel or any other layer to kind of help solve this? So not to drag again, and I seem to keep talking about HTLCs, unfortunately. But yeah, with the optimistic roll ups, the trade off is that you can do smart contracts, you can aggregate the signatures and you can get all the throughput you want, and you can do it whatever way you want. And you can do that on consumer hardware generally and without needing ZK proofs in any way. So you get a lot of benefits. The trade off is that there's a week time delay to remove your assets if you don't have regular access to HTLC liquidity.
02:58:38.714 - 02:59:15.606, Speaker A: But the thing is, to exit these chains, you just need any party on Ethereum or any party on any other layer. One, to be sitting with some liquidity and running a liquidity provider, which they'll make fees for and that will be able know basically solve that issue, in my opinion. I think it sort of optimistically solves it, but it's a fantastic solve and it sort of gets the job done from what we can see. And we'll just have to see how it plays out. Yeah, awesome. Well, Nick, thank you so much for your amazing talk and answering all of our questions. I'm sure there's a lot more questions coming in.
02:59:15.606 - 02:59:56.386, Speaker A: Actually, there are a lot more questions coming in, so if you can just head over to Ethanline.org and log into the chat and just follow up with the audience there, we'll appreciate that. And with that, we are ready to move on to our next talk. So the next talk we're going to be doing is going to be by Andrew and Janice. Andrew is from consensus and Janice is from the graph. And so they'll be talking about how do we actually manage state channels at scale? This is something I'm really excited about and I want to hear their thoughts on how do they actually pull this off for the graph and just how do we think about this as a large infrastructure problem. So I see both Andrew and Janice are here and I'll let them kick off with their SlideShare.
02:59:56.386 - 03:00:38.610, Speaker A: Thank you so much and welcome. Thank you. I think Janice is going to be sharing his slides all right? Yeah. Thanks everybody for attending my first webinar. I'm going to be talking about the recent goal of the State Channels team to build a highly scalable State Channels wallet. As Kartik said. I'm Andrew Stewart, I work on the Magmo team at Consensus and we're part of the State Channels community, the State Channels organization which is a community of researchers and developers implementing open source state channel solutions.
03:00:38.610 - 03:01:21.758, Speaker A: I actually have the pleasure of giving my first joint presentation with Janice, who's the tech lead and the co founder of the Graph. We actually started a collaboration with the Graph a couple of months ago to integrate Nitro state Channels into their query payment model. And so Janice is here with me as well. Yeah, welcome. Glad to have you all here. So the structure of our talk will be as follows. We'll talk about the Graph and its use case for State Channels, especially going forward, looking forward to the Graph network.
03:01:21.758 - 03:02:34.890, Speaker A: Then we'll talk about State Channels by introducing the general concept, but also how it applies to the Graph specifically. And then we're going to talk about not really Jetpacks, but we're going to talk about the improvements we made to push the boundaries of State Channels forward in the past couple of months. Let's start with the graph. The Graph, for those of you who don't know what it is, it's an indexing protocol for organizing and efficiently accessing data from blockchains such as ethereum and storage networks such as IPFS. Back in January 2019, we had Graph Day, a one day conference where we launched a hosted service that's a currently centralized solution to basically do everything that we want The Network to do, just in a simpler way. So that everybody could start using it right out of the gate and didn't have to wait another two years or so for the network to develop. That's been in running since January 2019 and has since been picked up by a lot of well known projects.
03:02:34.890 - 03:04:01.430, Speaker A: You can see some of them here, uniswap synthetics, Ave, Dow, Stagnosis, Aragon, ENS, and many, many more are using the Graph in production today. And you can also see that when you look at the daily query volume that we're processing. So right now we're at about 300 million queries served a day for all of the projects that are using us and that comes down to about 3000 to 4000 queries per second served. We recently launched our testnet, or incentivized testnet called Mission Control as we're moving closer towards the Graph network launch. Now the Graph network will replace this centralized hosted service where everything is run by a single company that you have to trust to stay around and keep everything running, and also that you have to trust with your data. We replace that with a decentralized service and decentralized network and a query market around that. The way this looks then is that instead of hitting a centralized service, the consumer will pick from a variety of indexes that are running around the world based on certain criteria and will send these indexes queries for the data.
03:04:01.430 - 03:05:15.610, Speaker A: And there are incentives baked into the protocol to incentivize indexers to. First of all, index these data sets that we call subgraphs, keep them available, serve queries for them and get paid for these queries. And then there are a bunch of other incentives around that to keep the network healthy, such as curating subgraphs or delegating to indexes that are doing good work and so on and so on. If we use the hosted service as oh yeah, one thing I should say. So initially you want to start not with the consumers directly interacting with the indexers and paying them directly. Rather, we'll start with so called gateways that sit in the middle and that the apps send queries to, like the hosted service today or like any other API service where they don't have to pay for the queries. And then these gateways will select indexers based on certain criteria that can be customized and will handle the payments and also the validation of query responses on behalf of these DApps and end users.
03:05:15.610 - 03:05:52.600, Speaker A: So when you see consumer here, you can also for now, think of them as gateways. If we use the hosted service as a reference, then we'll see a decentralized network that processes about 4000 queries and payments per second. Looking at for reference visa, they process about 16 700 payments per second. They can peak at 65,000, but the average is about that. So we're looking at double that average for the hosted service right now. And if we can translate that to the network, then that would be a decentralized system. Processing that.
03:05:52.600 - 03:07:07.470, Speaker A: Now, you just heard 4000 paid queries per second on Ethereum with its block times of 15 seconds and ten transactions per second throughput. How does that work? And for that, I'll pass it over to Andrew. Yeah. The state channels team would naturally ask at this case, can we use state channels to pay for queries? And to start answering that question, I'll first ask what are state channels? Well, state channels are a L2 scaling technique for Ethereum that enables trustless, instant and zero fee transactions amongst a fixed group of users. They're not exactly instant because a payer and a recipient are rarely in the same location. And in addition to be trustless, they do incur a small computational overhead to verify payments, as we'll soon see. But in any case, for graph queries, state channels are a perfect application because once a gateway selects an appropriate indexer, you now have a fixed set of peers who want to exchange some asset such as Ether or some token for some information, such as the query response.
03:07:07.470 - 03:08:20.710, Speaker A: For those who aren't familiar with Nitro state channels, I'm going to walk through the lifecycle of a toy two party state channel that's funded directly with two on chain deposit transactions. So for starters, if we had, say, Alice and Bob here with five tokens each, and they wish to trade tokens between them, they can use an on chain adjudicator to mediate the off chain token transfer using a Nitro state channel. They do so by exchanging signed states which the adjudicator will use to release funds based on states supported by all the parties in the channel. So to start off, they both sign what's called the pre fund state. Here in this case, the initial outcome of the channel, allocates five tokens to Alice and five tokens to Bob. Once Alice has the pre fund state signed by Bob, it's safe for her to deposit into the adjudicator contract. Even if Bob refuses to deposit, the Nitro protocol ensures that Alice can recover her tokens in bounded time, even in the presence of front running attacks.
03:08:20.710 - 03:09:21.446, Speaker A: After Bob sees Alice's deposit, it's safe for him to deposit and the channel can, from there on, be used to make payments by simply updating the outcome in the channel. For instance, if Alice wants to pay one token to Bob, she can sign a state update where the tokens are allocated slightly differently, with four to Alice and six to Bob. In this case, she's given one token to Bob. Now, Nitro state channels are turn based, meaning that if Bob doesn't want to make a payment, he needs to pass the buck back to Alice in order for Alice to be able to make the next payment. I'll note that there are applications of Nitro state channels that can avoid this, but this allows wallets to make simplifying assumptions that enhances the security model. Now, suppose after making a few payments, they decide to finish with a channel and settle the bill. What they do is off chain.
03:09:21.446 - 03:10:07.510, Speaker A: They co sign a final state indicated here by the Asterisk with the final outcome, and they give it to the adjudicator in the on chain transaction. When the adjudicator is given a final state signed by all the parties, the channel is instantly concluded on chain. In other words, if the peers in a channel collaborate, then they can recover the funds without having to wait a timeout period. Once the channel is concluded, the adjudicator distributes the tokens according to the last outcome that it held. In this case, Alice would get eight and Bob would get two. Now, this is just a toy state channel with a simple outcome. In general, you don't want participants to be able to update the outcome arbitrarily.
03:10:07.510 - 03:11:08.570, Speaker A: And for the graph in particular, you might want to make payments conditional on certain features. Yeah, so this example translates pretty well to the graph network as well. One thing that our payments, where our payments are a little bit different from just accepting a new balance, is that they are conditional on the index serving a query response that matches the query. And that's correct for this query. So what in our case, consumers or gateways will do is they will send a query along with a conditional micro payment that can be unlocked with an attestation for the query response. So that is for the indexer to say I've successfully executed this query against the correct data and I'm serving the correct data back. Yeah, and based on that, that's basically the rules of the state transitions and how payments can be made off chain between the consumer or gateway and indexer.
03:11:08.570 - 03:12:20.898, Speaker A: Yeah, there's one thing to note here, these attestations, if they are incorrect, for instance, they can also be disputed later on chain, so these transactions don't have to be final. If it turns out that the data served was not correct, the graph network will most likely see us entering new dimensions in terms of scale. As more adapts adopt the graph as the technology for querying their underlying blockchain data, we will likely see thousands of them. We already see or have I think between one and two, maybe 3000 subgraphs that were created over time on the hosted service. So there's plenty of interest there. And those thousands of apps will likely power millions of users. And to serve that traffic, we'll have dozens of gateways and we'll have hundreds if not thousands of indexes that either index specific data sets or variety of data sets.
03:12:20.898 - 03:13:23.642, Speaker A: And so there's a lot of scale that requires the whole system to be very responsive and to have, for both queries and payments to have high throughput low latency to be reliable that queries don't fail and payments don't. Fail to be trustless as well, because you don't want to trust a gateway that you don't know or an indexer that you don't know, that you don't even interact with. If you interact with it through a gateway, These gateways, if we think of them as deployed around the globe to reduce latency, they are largely independent when it comes to the state channels. So they will all have their own set of state channels that they manage with indexes. They will have potentially their own balances. They may be run by completely different people with different keys. So for looking at the scale of state channels, this overall network is not extremely interesting.
03:13:23.642 - 03:14:33.070, Speaker A: What's more interesting is to look at a specific pair of a consumer or gateway and an indexer and to see how many paid queries we can execute between these two, what bottlenecks we hit, and how we can remove those. So to discuss the issue of the throughput through a single gateway, index or pair, I'm going to first look at the critical path required to make a payment. What does a gateway and indexer actually have to do to process a payment? So the first thing is the gateway needs to construct a payment state s one. It then needs to sign it, store it in its database, send it to the indexer, and then wait for a receipt. State s two from the indexer. Meanwhile, the indexer is waiting for the payment state s one from the gateway. Once it gets it, it's going to push the payment state into its database.
03:14:33.070 - 03:15:30.130, Speaker A: It's going to then construct a receipt state s two, sign it, store it in its database, and send it back to the gateway. Once the gateway receives S two, it then pushes it into the database and the payment is then processed. So, starting out, we could initially hit on the order of one payment per second. Given these steps, it was a bit more than one, but it was that order of magnitude. And why was it so slow? Well, each of these steps from one to six on the left and on the right, they're blocking steps. If you only have a single channel, the gateway can't do anything to process another query until it receives a receipt from the indexer. Likewise, if you're interacting with your database, your application can't do anything until it gets a response back from your database.
03:15:30.130 - 03:16:33.190, Speaker A: So to increase throughput, we definitely need to use more channels. So creating parallel payment channels is where the Nitro protocol really shines, where we can fund a single ledger channel with a single deposit on chain and use it to fund many payment channels via ledger state updates off chain. By using many of these parallel payment channels, we could initially hit about ten payments per second, which is still slow. So why is it slow? Well, Node JS, first of all, node JS is a multithreaded language runtime, but it only provides a single dedicated thread for running both the event loop and your application code. So you can't use more than 100% of a single core for this thread. If you ever hit 100%, you're going to have a bottleneck. So, for instance, signing state updates need to be as quick as possible, and initially they were slow.
03:16:33.190 - 03:17:29.814, Speaker A: Janice came in and helped us with that. So one thing that or the first step of constructing a state for a new payment that you can then send over to the indexer is that you have to encode the application data. And that is very similar to how you encode a lot of data in Ethereum. You use an API encoder and you pass that data in, and if you write a JavaScript or TypeScript application or library, you're likely to use Ethers or Web three JS. In this case, we initially used the Ethers Abi encoder, and it turned out that that was fairly slow because I think it's implemented in JavaScript. Luckily, the application data that kind of represents our state transitions. So like the conditional micropayment or the Attitation, they are fairly simple.
03:17:29.814 - 03:18:33.100, Speaker A: They have a few fields like the payment amount, a request Identifier, response Identifier, a signature by the indexer. So in this case, it was very easy to just write like an almost by hand custom encoding routine that does this in a much faster way. Than a generic encoder would. And I think that boosted the throughput about 25%. After you've encoded your application state, you will put it into the new state or the application data, you put it into the new state and before you can send that over, you have to encode that again and hash it and sign it. And these three different steps were all using slow JavaScript based code. So again, the Ether API encoder to encode the state, a JavaScript catcher, 256 hash and a JavaScript method to sign the outcome of that.
03:18:33.100 - 03:19:51.314, Speaker A: And one thing that's really nice about JavaScript is that you can very easily extend it with, well, initially if you run Node JS, you can initially extend it with maybe native code. But today we also have a WebAssembly which we can run equally in the browser as well as in Node JS. So one thing we did here was that we replaced the encoding of the state, the hashing of the state and the signing of the state, which were initially three different steps in a single call to a WebAssembly module that was written in Rust and compiled to WebAssembly using Wasmbinegen. And that ended up being a JavaScript package that's as convenient as any other JavaScript library to import and to use. And what you can see here is you don't have to encode the state upfront. Also not doing several round trips to WebAssembly, we're doing a single round trip to WebAssembly to perform all these operations there. What we use for this code in Rust is ETH Abi, a library written by initially parity now open Ethereum team, which we're also using pretty heavily at the graph and it's kind of like partial equivalent to Ethos JS in the Rust world.
03:19:51.314 - 03:20:55.050, Speaker A: This gives us another boost of about 20% in throughput. So when it comes to storing states safely, we're using a postgres back end, but we chose to use objection a JavaScript Query builder library to interact with the database. But the result is that if you write JavaScript code like this, you're constructing the same query at runtime over and over again. In this Snippet you want to construct a join statement between the user table and the emails table. And it actually takes a nontrivial amount of time to construct and dispatch these fairly simple SQL queries, which is just wasting our precious main thread CPU cycles. And it's not actually that bad to just write the SQL query that you want by hand as a prepared SQL statement stored in your source code. That means that all you need to do is fill in your parameters and send it off to SQL.
03:20:55.050 - 03:22:16.850, Speaker A: If we switch to a library like a PG promise that embraces this technique, we can actually seemingly boost the throughput by something like 400% in a single threaded application. So with all these optimizations together to hit with 100 payment channels running in parallel, for instance, we can hit on the order of 100 payments per second in a single threaded JavaScript application. That's clearly still not enough. These days you can get a powerful multicore server and we're only using one of those cores. So if we want to increase throughput more, the natural thing is to hire more workers. We can just spawn worker threads in a node JS application and offload as much of the critical code as we can into a worker thread. And then as long as we have enough CPU cycles to schedule more work, and as long as our database can handle more database connections, we can scale throughput linearly by just paying for more CPU cores.
03:22:16.850 - 03:23:03.314, Speaker A: With this strategy, we can then hit the next order of magnitude, around 1000 payments per second. But the main thread bottleneck will always remain. If you only have one main thread that's running on one core that has to do a certain amount of work, you just can't scale any more than that. And the solution here is again obvious. We should hire more managers. And by managers in this case, Andrew means more gateways or more not threads, but processes that can manage more threads. So one thing you can do is horizontally scale the gateways and have them all use their own database connections and their own worker threads to do basically the same thing.
03:23:03.314 - 03:23:53.870, Speaker A: Put them behind a load balancer and you're good, right? There are a few complications here when it comes to state channels. Due to their sequential nature, you can't use the same channel in two different gateways at the same time. So these need to be synchronized a little bit. Otherwise one gateway or two gateways might try to use the same channel at about the same time. One does it, the other realizes that channel is now busy, decides to retry pick a different channel, but maybe one of the other gateways has already picked that. And so there's kind of a race to find a suitable channel which needs to be optimized a little. So a very naive way to do this would be to just have them use completely disjoint channels.
03:23:53.870 - 03:24:57.750, Speaker A: Like a gateway would do that is or like two gateways would do that are deployed to completely different regions on the globe, both managing their own channels, having their own keys, and managing the channels with the index completely separately. That has the downside of having to create a lot of channels in the first place to have to fund these channels. It's a lot of overhead just to get this extra scale. And since they are all able to connect to the same database, all that's really needed is kind of a lookup table that allows all of the gateways to efficiently and atomically pick a channel in the database that's not used yet and at the same time market as used so that nobody else will use it. And then there's very little overhead in picking a channel. Yeah, it makes sense to kind of move the channel management out of the critical path as well. So that doesn't block any cycles in the main thread of any of the gateways.
03:24:57.750 - 03:26:47.180, Speaker A: So one approach that we've taken is to move the channel creation logic and all of that into a separate service that runs alongside these gateways and manages just the channels, and then the gateways just execute against the channels that they find in the database. And so if we run more applications going from starting at 1000 requests payments per second, where does it end? Well, we'll see that's still one of the open points here. We've yet to see where that takes us, but we're very optimistic that we can take it to the numbers that we need to sustain the usage that the hosted service has seen already and carried over to the network. So to try and wrap things up, I'll quickly summarize that we're making significant progress at increasing the throughput of our State Channel wallet by optimizing the critical path, making a multithreaded wallet, and building tools to synchronize the sharing of a pool of payment channels. I'll just add that most of the work outlined above so far is open source. There is some part the channel management software that Janice was talking about, it's still loosely coupled to Graph specific code, but we plan on open sourcing that in the near future. And in conclusion, the Graph is providing a highly scalable and robust network for indexing and querying decentralized data.
03:26:47.180 - 03:27:55.726, Speaker A: And our collaboration with the Graph has been a really welcome boost to prove that State Channels can work well at scale. With the upcoming Graph network mainnet launch expected later this year, state Channels are going to be one of the few highly scalable, decentralized payment solutions used in production. So we're really excited to continue with this collaboration. Yeah, to just wrap it up, I'd like to thank the Ethereum Foundation for their past and their ongoing funding of the State Channels project, as well as ETH Global for organizing this session, and everybody from the Graph for welcoming the State Channels team into their project. Yeah, thanks everybody for your attention and enjoy the remainder of your talks. Andrew Yanis, thank you so much for this amazing talk and also a very beautiful presentation. We have a few questions coming in from our audience and I have a list of angular questions that I'll ask you.
03:27:55.726 - 03:29:05.240, Speaker A: We have a couple of minutes here and luckily Mike Kirsner from C Channels team has been answering a lot of them already. So there may be some repetition here in the answers, but I'll kind of ask the smaller ones for you. I guess my first question here is just for the audience to better understand what's possible here. How programmable are state channels? Are they only supporting just transfers and any conditional logic? Or do you actually get more complexity on programmability here if you use State channels? So the Nitro protocol focused on simplicity with the goal of targeting the majority of applications that we could imagine using without adding too much complexity into the protocol. So therefore we're currently limited to state channels where the state transition logic is a pure function of the existing state. So you can write arbitrary code that determines what transitions are valid in Solidity, but the code must be a pure function of the current state. And that's the only limitation of the Nitro protocol right now.
03:29:05.240 - 03:30:26.634, Speaker A: There is some research into escaping that constraint, but that's without a target use case, it's not considered right now. So maybe just to follow up on your answer then does that I guess maybe it'd be great if you can also answer what additional liveliness assumptions you have to make for Nitro and kind of how does that change? Yeah, so we actually have a pretty good blog post or a blog series about this. So we were able to formally verify at least a portion of not formally verify. We have a TLA plus specification of a part of the Nitro protocol itself that can prove that as long as you have the ability to submit a transaction, then you will not lose your funds. If you can submit a transaction within a block time, like maybe within a 1 minute time or something like that, you're guaranteed not to lose your funds in spite of an attacker who's willing to spend an arbitrary amount of money front running your transactions. Awesome. A question for Janice.
03:30:26.634 - 03:31:26.946, Speaker A: So as a developer and CTO graph, how has the developer experience been for you to sort of integrate such channels into the Wallets API? And I guess it's mostly about how do you think about these libraries just being immediately ready to be imported into your project or just kind of what you're working on as a developer and just kind of how easy they are to use. So we'd love to kind of get some insight into how using Sage Channels in production has been for you. Yeah, they are not used in production just yet, but they will be very soon. Exactly. So the collaboration started out, I think, with the state Channels team adding a server wallet into their repertoire, because our gateways that we're developing, our indexes are both running node JS processes. So that was badly needed. Before that, I think the wallet was primarily targeting the browser, so that was necessary first.
03:31:26.946 - 03:32:12.242, Speaker A: After that, the integration was pretty smooth. The interfaces that we now use to the libraries maintained by the state Channels team to kind of manage the capacity of channels that we need to handle the throughput in terms of queries, are now automatically managed by the state Channels team libraries and that's the code base that Andrew mentioned is loosely coupled to the graph. So we have these Identifiers for which we need to generate a certain number of or ensure a certain number of state channels. And at. Some point, these Identifiers go away and then the state channels need to be closed. So this lifecycle management, I think, is pretty generic. You can imagine that in a lot of other situations as well.
03:32:12.242 - 03:32:59.294, Speaker A: So that should be fairly easy to decouple from the graph and reuse in other places as well. Yeah, the cooperation has been really fruitful and the end result is really good. Or wouldn't say end result yet because we're still working on those horizontal scaling improvements. But, yeah, overall it's been a really good experience. And from the first moments that we talked to the stage analyst team about their design, their approach, and how they wanted to move everything that's potentially slow out of the critical path and so on, everything made sense pretty much from the start. And that was pretty nice. Well, that's a wonderful testimonial for all the work the Sage Town theme has put in the last two years.
03:32:59.294 - 03:33:59.780, Speaker A: And maybe I'll just kind of close off with another question, which is there's a lot of hackers here for the hackathon this month that are planning to do scalability and integrate that in any L2 solutions in their projects. So, kind of having used Sage Channels for the graph, do you think the SDK and the Sage Channels project is at a place where almost everybody can just import that and get a very high thousands of PPS built into their projects? Even if it's for a hackathon? I think that's more a question for Andrew to answer. I think what you will need for that kind of scale would be to have these libraries more generalized. I mean, you can manage your channels manually, you can scale them up yourself. That's not too hard. But yeah, those libraries that manage the channels on both ends, that will be pretty crucial for everyone to reach that kind of scale. I think that's right, yeah.
03:33:59.780 - 03:34:55.266, Speaker A: Unfortunately, it hasn't been able to be a priority to get this stuff out to be usable to the community. We're sort of running at a pretty breakneck pace trying to meet the scale that the graph network is going to run at. And it's our priority once we do that to release these tools to the developer community. So I regret saying it, but I'm not sure that it would be ready for a hackathon that's ongoing this month. No worries. I think this is just a milestone for all of us to kind of achieve, and especially as it gets integrated into the graph at the scale that they're operating at, I think this is just a good recipe for others to take from. So thanks again, Andrew and Janice, for an amazing presentation and answering our questions.
03:34:55.266 - 03:35:33.090, Speaker A: There's still a handful of more questions coming in on our chat, so you can just head over to Live Ed one if you have time, and just answer the rest of them on chat. We'll appreciate that. And with that, we will move on to our next talk. And this is we're going to resume one of our talks from earlier today, and that is Joe Andrews from Aztec, and he'll be talking about the new L2. Aztec is built as their ZK vault service with a privacy at its core. And I know Joe is already here with us on this call, so I'll just let Joe kick off with a screen share and his presentation. Welcome, Joe.
03:35:33.090 - 03:36:04.806, Speaker A: Thanks, Karthik. Sorry, 1 second. I'm going to run through this fairly quickly because I'm on patchy Wi Fi. Hopefully it's all hold up this time and we'll move on to the demo and the code, because I'm sure that's what everyone's here to see. That sounds great. Great. My name is Joe.
03:36:04.806 - 03:36:52.966, Speaker A: I'm one of the co founders of Aztec, and we're very excited today to announce the launch of Aztec Two, a ZK roll up with privacy. So in the half an hour we've got today, we're going to run through what Aztec Two is and how we use ZK Snarks for scaling and privacy in the L2. We're also going to do a quick demo of how private transactions appear on Robston, and we'll do a run through of Noir, our Snark programming language. I also think it's important to take a step back when people are developing an ethereum. Things move very quickly. It was only eight months ago we launched the 1.0 version of our protocol, bringing confidential transactions to mainnet shortly afterwards.
03:36:52.966 - 03:37:38.780, Speaker A: Tornado Cash also brought anonymous transactions in February. And in the same month, our research team upgraded our universal Snark Plonk in February. So what is Aztec Two? Aztec Two is a ZK roll up based L2. It allows users to have private sends by default, averaging around 5000 gas per transaction. It has social key recovery built in and allows scalable private access to DFI. Alongside it, we're also shipping a programming language, Noir, giving developers the keys to programming privacy. Astec Two is powered by Plonk, the cutting edge Snark built by Aztec over the last year.
03:37:38.780 - 03:38:17.158, Speaker A: Plonk has the performance of a traditional Snark, but only needs one trusted setup in certain areas. It's also ten times more efficient, allowing us to do recursion. This efficiency makes Zok Snarks usable on ethereum. Today. Signatures can be validated over the ethereum curve, as well as curve 25519, which is the curve inside most iPhone and Android security modules. Proofs can be constructed in seconds, and clients can benefit from full user privacy. This efficiency is core to the design of the Aztec L2.
03:38:17.158 - 03:38:54.606, Speaker A: It allows us to roll up Snarks inside Snarks. The inner Snark can be anything a private transaction, a uniswap trade, or even another roll up. This design allows our L2 to have scaling and privacy. As private transactions are at the core of the network, users can get opt in privacy for ERC 20 token transfers. Balances are confidential and senders, and recipients are anonymous. This is the most efficient version of Aztec yet. It's over 200 times more efficient than Aztec.
03:38:54.606 - 03:39:39.402, Speaker A: One private sends are around 5000 gas per transaction, and DFI interactions which will be launched in November are around 10,000 gas. In fact, the network can scale elastically up to 1024 transactions in a single block. Alongside this, we're proud to announce a new account model. Each user can now have an alias. You can be Montzuma II and not an ethereum address. We also mirror common social recovery patterns seen on Ethereum mainnet today. Alongside the network as the core is a snark.
03:39:39.402 - 03:40:46.066, Speaker A: We're releasing Noir, a programming language that allows developers to write their own circuits and have those bundled in the Aztec roll up. The language will be released in Developer Preview in November and is open source and written in Rust. Let's move on to a quick demo. And we'll first of all demo the Block Explorer, then a quick terminal view, and then the code. So we have a block explorer live on Explorer Aztec Network. This Explorer updates in real time and allows users to go and see the blocks that are being processed by the network, the transactions on Etherscan and any transactions that are inside the block. We also have a demo UI, which is at Terminal Aztec network, which is a fun command line interface.
03:40:46.066 - 03:41:31.830, Speaker A: We call the Aztec Zero Knowledge terminal. You can use it just by typing help to get a list of commands that are available. And then init in the background here, we have to construct some proving keys which allow the client to construct the inner proofs in our roll up. Once the network synced, we can see my public balance is 655 testi, and I have a private balance of 30. If I look at my list of commands now, I can do a lot more actions. I can mint, I can approve, deposit, withdraw, transfer or register an alias. Let's start with a private transfer.
03:41:31.830 - 03:42:37.330, Speaker A: Then I can transfer to one of my colleagues, Charlie. Just by typing transfer charlie. The proof's generated in seconds and as soon as it's sent to the roll up provider, we can head over to the Block Explorer to see it being mined in the background. Our roll up provider scales elastically based on the demand of transactions. It will pick a roll up size that's suitable based on the number of transactions in the queue. On testnets, we prefer to relay blocks more efficiently and quickly just to aid the developer experience. On mainnet, this will likely be every hour.
03:42:37.330 - 03:43:33.000, Speaker A: We can see here that a block has been processed. It's currently waiting to be mined on Robston, and we can go over and see the transaction. As soon as this is mined, the Aztec SDK, which powers the terminal view, will update, and I'll have a live update of my balance decrementing. This transfer is completely private. No one can see who I'm sending to or the amount that's being sent. In fact, if we look at the transaction itself, the only state changes we can see are gibberish. There's also a command to view status of transactions.
03:43:33.000 - 03:44:40.840, Speaker A: You can see it's still pending here on Ethereum while we're waiting for that to be mined. Oh, there we go, it's been mined. We can also look at some of the use cases that are useful for DFI, such as withdrawals. If I want to withdraw back to layer one, say 20 die, I can specify any withdrawal address here and that withdrawal will be anonymous. I think the transactions are a little bit slower to generate when zoom is running, but usually this takes around ten to 15 seconds. We head back to the block Explorer. We can see that that last block was settled and as soon as the proof sent, we'll have a new transaction in the queue.
03:44:40.840 - 03:45:46.050, Speaker A: There we go. When we look at the next block on Ether scan, we won't be able to see who the sender of the Dai was. We'll just see an anonymous withdrawal from the roll up contract on layer one. While we're waiting for that to be mined, as a lot of you are developers on the call, I'm going to head over to the documentation. So all of this functionality is available today through our JavaScript SDK. The SDK is written in TypeScript and it allows users to interact with the L2. See if this is the mind yet.
03:45:46.050 - 03:46:39.040, Speaker A: The docs are available at developers. Aztec network and the docs have this unique feature of being live, so you can run all code examples in browser without having to set up any testing environments. Let's run through some quick examples around social recovery. The way that social recovery works in the Aztec Network is we have a multiple key model for a user. So a user is identified by an alias and then it has multiple signing keys. When we create an account, we create a throwaway key pair that's attached to the user's account but is single use, so no one knows the private key. Before the keypad's thrown away, it signs a recovery signature for a trusted party.
03:46:39.040 - 03:47:50.072, Speaker A: That signature authorizes the trusted party to take over the account and only that trusted party. That signature can then be shared amongst multiple services or or key sharding services, and it allows, at a later date, the account to be recovered to that trusted party without having to share private keys. So if we look at the code here, we can see we're just going to use random addresses here. But this code could be if you were a wallet provider or adapt, you could use a trusted address that you use to recover your users accounts. Or users could select their own address here to allow them to let their friends recover accounts. Just initialize the SDK on the docs so we can interact and run this code. The docs are also linked to the terminal view.
03:47:50.072 - 03:49:13.140, Speaker A: So you can see that my balance is synced in the SDK across both websites. It just open this in a new tab. We can see the transaction here waiting to be mined. This is an account transaction which is different to the joint split. The more useful thing to probably run here is actually talking about the recovery data and what users get if we run this particular example, if I zoom in we can see there's a recovery payload here which gives us the key that we added as our trusted third party public key. The key we've added to the account, that's the throwaway key and then recovery data. The recovery data here is a signature which can be used at a later date with the add signing key method to add the trusted user into the account and give the user control of their account again should they lose one of their devices.
03:49:13.140 - 03:50:24.276, Speaker A: We also have the standard methods for deposit withdrawal transfer for ERC, 20 tokens and the network has a full emergency withdrawal feature should we disappear as a roll up provider. All of these functions can be run on layer one without the need for us using the emergency withdrawal method. Later this month we'll be expanding the network to work with DeFi. Users will be able to swap token pairs on uniswap or invest in compound. And as I said on the earlier presentation, there'll be a developer preview of Noir which allows users to write their own circuits to be bundled in the roll up. I think if the last transaction has been mined we can go and have a look at the anonymous nature of these transactions. So here we can see as a user, the only transfer that I can see is a transfer from our contract to a user and that user has no link to the original sender of the transaction.
03:50:24.276 - 03:51:24.136, Speaker A: So this is true cryptographic anonymity. I think that's probably sufficient for the demo. I'm happy to go over more things in the questions but maybe it's good to take a couple of minutes just to take some questions and run over any details that I may have missed. That sounds perfect Joe. Thank you so much for that amazing talk and I'm super excited to kind of see that we just found out about the Block Explorer, the fact that you guys are ready to do 1024 transactions that are private by default and also about Noir. A lot of amazing updates bundled in. So we have a few questions coming in from the chat here and I'll just kind of start asking them in order and for the people listening in on the video because of the delay, if you have any questions just type them up in the chat and we'll propagate them up to Joe.
03:51:24.136 - 03:52:16.488, Speaker A: But example of clarifications that I think will be super helpful here. I think the first one that we would like to kind of get a better understanding of is how do addresses work in Aztec too? Are they just the same as an Ethereum one address for a user? What's different about address spaces as you think about the role of service? Sure. So an identity in Aztec, it can be anything. So it could be linked to your Ethereum address and it could be derived from your Ethereum signing key, but it doesn't have to be. So we have the concept of multiple keys per account. In the current testnet, the keys that are available to users are on a Grumpkin elliptic curve. But later in the year we'll be expanding that with our ultraplunk research to allow Ethereum signatures or hardware security module signatures.
03:52:16.488 - 03:53:10.332, Speaker A: So I think to go back to the question, the account model, an account is identified by a unique ID. That unique ID can have an associated alias, so a short name such as Joe, and it can have any number of signing keys. You also don't need to have an initial transaction to generate an account. You can kind of have programmatically generated accounts that can receive funds without the user having to call the create account proof you can just send to a unique user ID. And then at a later date, the user could claim those funds and kind of make a fully formed account by adding further keys and an alias to that account. And then if at any point they want to recover their account, they can use the social recovery features to remove keys that have been lost and add in new keys. Awesome.
03:53:10.332 - 03:54:17.440, Speaker A: Another clarification that we have from the chat is can you talk a bit about what the onboarding and exiting costs are for a user and kind of how do you think about people onboarding and using the service? Sure. So the deposit flow for standard EFE 20 tokens will incur gas on the user's kind of deposit deposits fairly fast. It just basically is a single block time for our main net launch. For tokens that support permit, we'll probably do an initial scheme where we pay the gas for deposits to help kind of bootstrap network liquidity, but that's still to be finalized. Great. The next question we have is we would like to learn a bit more about Noir in general and what's possible, what are the capabilities that we have planned in features and sort of how does it differ right now from something like sync? Sure. Inside Noir, this is a private by default language.
03:54:17.440 - 03:55:06.164, Speaker A: So the programs are private. So unless a developer wishes to expose variables as kind of public inputs to the circuit, everything is private. So I think the main difference here is that noir is built with privacy at its core. In terms of functionality, we have Shah, hashes Pettison, hashes Blake, two hashes Merkel, proofs of inclusion and non inclusion, as well as some other features which are coming later in the year, such as ethereum transactions, but at a high level. It's kind of a language that's very similar to Rust. It's been written to be as usable as possible. It shouldn't have any kind of strange symbols in it.
03:55:06.164 - 03:55:48.210, Speaker A: And the idea here is that most Ethereum developers can write these mini programs that can be bundled into the roll up without kind of zero knowledge expertise. That's awesome. Yeah. I feel like I'm personally excited to play with smart contract languages that are private by default. Just to add to that, I think one of the other main differences is we have a standard library which is kind of bundled alongside Noir. So if we look at if my screens are still being shared here, this is an example of one of our highly optimized implementations that comes with Noir. The screen is no longer being shared, but if you mind doing that, we can have that up for everybody else to see.
03:55:48.210 - 03:56:42.930, Speaker A: Give me 1 second. So this example here is just importing something from the Aztec Standard library, which will be kind of audited versions of common cryptographic primitives. So we're doing a Shardy five, six hash here. So instead of developers having to go and write their own optimized versions of that, we're just bundling with the language highly optimized, efficient versions of common cryptographic primitives. So I think that's also useful for kind of keeping the complexity down and developers not having to worry about efficiency of their Snark code, because in Snark programming, efficiency is measured in gate count, which translates to client proof of time. So you don't want users waiting for ten or 20 seconds for long proofs when it can be a five second circuit. Amazing.
03:56:42.930 - 03:57:44.490, Speaker A: The last question that we have here is a question kind of for me here, just to kind of better understand, we can spend a couple of minutes on this thing. This is clearly a unique approach that you presented here for how a private role works. So if you have time, we'd love to understand how does this actually interface with existing DeFi protocols that are already on main net and kind of how do you think about how is it possible to bridge them? Whereas we look at other scaling solutions that require a different VM to exist and have a different version of those contracts over there. How does actually Aztec or ZkZk rollups enable that to be a lot more seamless and kind of how are you getting the uniswap and the compound integration baked into the roll up service? Sure. So with the November upgrade, we're taking kind of a layer one approach, I would say. We're going to batch transactions in the roll up and we'll kind of, as a group, relay those trades back to uniswap. And there's some discussion here around kind of minor extractable value.
03:57:44.490 - 03:58:48.996, Speaker A: But the initial approach we're taking is instead of having to port an entire D Five protocol to L2, which comes with its own audit risks and kind of just security risks, we're going to for very liquid markets. I think you can quite easily allow users to bundle trades together inside a roll up and have them all executed at once. And yes, there'll be some slippage in that, but for smaller notionals, the trade off between gas price and slippage for kind of end users is pretty worth it in our case. If you're a larger kind of DFI trader you can always decide to pay the layer one fees and go back to mainnet. But for smaller kind of consumer users and kind of bringing this to mainstream for notionals kind of around ten k, the slippage versus gas price that we think we can get should be sufficient. Amazing. I am personally excited about 5000 gas cost transactions.
03:58:48.996 - 03:59:31.192, Speaker A: I'm sure others are too here. So Joe want to thank you for doing this presentation and showing us a really cool demo of a private roll up service that's already available for people to test out. There are a couple of other questions coming in so if you have time I would encourage you to also go on Live Eatonline.org and just kind of answer and address some of these questions if you have time or somebody else from the Aztec team. And with that, I'd like to intro our next talk for today. And next up, we have Alexei talking about troublegeth, and he'll just be covering sort of all the latest improvements and performance updates that have been added to the Triple Get project. And he is in this room.
03:59:31.192 - 04:00:04.352, Speaker A: So I'll let him introduce himself and kick off with the presentation. Welcome Alexey. Hello. Hi Karthik, thank you very much for the introduction and yeah, so let me just start the screen share. I will sort of say a couple of words about let me just figure it out. Yeah. Okay, so first a little bit about myself.
04:00:04.352 - 04:01:21.880, Speaker A: Basically I've been the programmer for pretty much all my professional life and for almost last three years I was working on this particular software just called Turbogeth. And what turbulet is, is basically it started as a fork as a fork of go, ethereum and sort of experimental software with certain goals to basically make it a bit more performant if possible. But now it's becoming more different and we're going to see in our talk how and why. And the title basically is too good to be true is addressing something that I haven't thought about until very recently is that whenever we present some kind of performance numbers for Turbogeth, most of the time the first question I get where's the trade off? Like what's the catch? Because people think, well, if we do something better then we have to do something worse. Basically. That's what I want to address. So here's the picture about the in the databases.
04:01:21.880 - 04:02:08.780, Speaker A: First of all, I look at Ethereum as a database. Essentially it's a distributed data base with certain properties. But in a databases, essentially you have usually like three points between which you're making trade offs. So update efficiency, space efficiency and access efficiency. And sometimes of course, the space efficiency helps all others and you can kind of move around and trade one for another. Usually you might have seen those triangles as well in different areas. And so people normally think about moving in this little plane which is the circle underneath.
04:02:08.780 - 04:03:08.984, Speaker A: And so this is where you're going to get trade off. Like oh, if I get more efficient in access, I must be losing something in some other places. But what actually happens in technology all the time? And if you look at it, there's always technologies which are strictly improving, right? We have seen this many times, for example, move from SSD to HDD. First of all, the SSD were more expensive, but now SSDs for considerable volumes, they're not so expensive anymore and they're pretty much better at everything. So nobody's asking what is a trade off between SSD and HDD, at least not anymore. So this happens all the time in technology. And usually the way I see it is that the previous technology is a bit further apart away from these efficient frontier where you can do trade offs.
04:03:08.984 - 04:03:53.790, Speaker A: And so usually you want to get closer to the efficient frontier and that normally requires some kind of paradigm shift. And the paradigm shift usually also kind of goes against the common intuition. So we're going to look at this. So what kind of the paradigms did we have to shift here? Which basically I posit that we did get closer to the efficient frontier. So there are a little bit of trade offs but not many. So next time that's why every time people ask me the questions what is a trade off, this is what I'm bringing up. So we're going to be talking about the paradigm shift in the data model where we replace trees with a flat data.
04:03:53.790 - 04:04:49.310, Speaker A: We're not going to be talking about history, the data model for history because it's another complex topic and I'm not going to cover it today. But we are going to be talking about other things like concurrency and architecture. So with the concurrency is essentially whether you do homogeneous concurrency, essentially when you're trying to run many things at once but they're pretty similar, or do you want to run like lots and lots and lots of different things at once. That's what I call the heterogeneous concurrency. And in terms of architecture here basically the shift is from monolithic system with a lot of cross cutting concerns done in the name of optimization. We're moving towards the modular system with the separate concerns which I posit as well is more optimal. So we'd start with the data model.
04:04:49.310 - 04:05:55.350, Speaker A: I thought about this for a very long time. Like, where does this come from? Where does this data model come from that everybody is using? And some time ago I saw that it actually probably comes from the yellow paper. And again, it comes from the sort of common intuition that yellow paper essentially calls this particular way of storing data that most implementations do right now. A sensible implementation or a reasonable implementation. And it says that it's mimizing the function C which is essentially function C is mapping the nodes in that sort of state try into their sort of hashes or something like that. But actually it should probably be the inverse. So although this particular paragraphs, these paragraphs are not normative, they already create this sort of they basically based on somebody's intuition or some sort of common kind of expectation and therefore everybody assumes, I guess everybody assumes this is how we should implement this and this is where it goes.
04:05:55.350 - 04:06:43.890, Speaker A: So back in 2017 when I just started this project, I started with Profiling. Go ethereum. And I bumped into this thing, which was 20% of my profile. And this is exactly accessing this memorized inverse of a function C. So essentially, given the hash, you need to figure out what is the node? The tree node is in the previous slide we were talking about recommendation to memoize this function and here we are actually using this memorization. And that was from my point of view, that was a performance bottleneck and this is where all that started. And this is the picture you probably saw many times, some of you.
04:06:43.890 - 04:07:40.916, Speaker A: So on the left is basically is my very simplistic representation of the state try where you have some elements which are v one, V two, V three, so forth. And they are built up into this kind of sophisticated, I would say state try. And the arrows, the black arrows in that tree on the left are those function like memorizations of this inverse function. So given the root hash I can get this root element and so forth and I can navigate through. So for example, if we wanted to find element v five so we need to basically access this memorization 1234 times. So that's basically four access to the database if you use this memorization as a data model. And what I noticed straight away is that I couldn't parallelize this because there is a data dependency there.
04:07:40.916 - 04:08:46.696, Speaker A: So you can don't do the second arrow until you resolve the first one. So there is necessarily data dependency and no possible parallelization. And on the right you can see how this minimalization kind of is laid out in a database if it's a key value store. Here's another point is that essentially the deeper the worse the problem is becoming. But why do we actually have to do this at all? And it turns out this is all about computing the state root hash which we need to either if you are a minor you have to place it in header or if you are just a verifier of the block you have to. Compare what you've computed with what you got in the header. So all this complexity is simply to verify the state root hash.
04:08:46.696 - 04:10:15.300, Speaker A: And this is actually one of the interesting innovation of ethereum compared to bitcoin which essentially commits every block we commit to the state route. And this is actually quite a good interesting feature but it has a certain cost and apparently so I posit that the cost of implementing it in the old way is actually very high and it's unnecessary high. What does the EVM think about it? So if we look at it from the EVM point of anybody, if you looked at what does the EVM opcodes are, they actually have no idea about the try. So they don't actually look at the try, they look at the account addresses and they want to fetch balances bytecodes or storage and stuff like this and also some instructions for storage loading. Again, they look at addresses, they look at storage location. There is no mercurialization here at all. Basically what the hypothesis was that, okay, having the state stored in a radix tree as a primary structure, it tries to satisfy this sort of computation of the state route which I think is quite a minimal thing, but it sacrifices the efficiency of EVM access.
04:10:15.300 - 04:11:26.108, Speaker A: As we saw before, it requires the sequential data dependent access. So is this sacrifice worth it? So essentially we sacrifice EVM efficiency and to be able to compute these state root hashes quickly and that's what yellow paper calls sensible or reasonable implementation. So yeah, that's what I've been working on for last two years is that trying to figure out if this is sensible or reasonable or could it be that this is a false trade off. And now I am convinced that this is a false trade off because it is possible to do both things efficiently to compute the state route and have efficient EVM access. But how do we do that? So we know if we have a flat data structure. I don't need to explain you how to provide efficient access from EVM because it's clear, like if you go back to the slide, if this is exactly how we store the data, EVM simply accesses it in this form. So I don't need to tell you how this works, but I do want to tell you how the other part works.
04:11:26.108 - 04:12:18.876, Speaker A: So given the flat structure, how do we calculate the state root? So yeah, if we know the structure of the tree then we can do that and of course you can try to build up the tree and then compute the root. But obviously that is going to be quite memory intensive. What you could do better is that you can start some kind of stack and then you need to calculate the tree, let's say from the left and then you only keep the stack of 1234 or five hashes and then every time you move on you just append like you are accumulating sort of the hashes on certain level. So you can basically compute this whole thing with a very limited memory. So with about five accumulators for hashes and so forth. But that's still quite large. I mean, I can tell you that.
04:12:18.876 - 04:13:34.048, Speaker A: For example, it says in SSD, it might take you, I don't know, half an hour or something like this, or sometimes an hour to get, depending on the speed, to essentially iterate through entire state and compute the state route. So although it's sort of relatively quick, but it's not as quick as you can't do it for every single block, right? It's too slow. And so the problem then becomes is that, okay, what if we had a root calculated and maybe we cached a part of that state tree in memory, but because we don't store it in database, once we flash the memory it's gone. And then in our block we needed to modify this green thing and then sort of like how do we actually calculate the new state root? How do we do that? So originally this is how I was doing it. I would essentially load the entire region of the flat structure, the entire one, and then I would calculate the route. And that kind of worked as long as my kind of cache of the state was large enough. And in the beginning it was terribly slow like on a startup.
04:13:34.048 - 04:14:15.568, Speaker A: But then it was also if the block started to hit some cold spaces in the state it was quite slow. But I sort of thought, okay, I'm going to solve this problem later. So I was running with it, I was saying, okay, I know that it's really crap great in the moment, but I know we're going to solve this because I had this idea how to solve it. So this next slide shows how we're going to discard this data because we just used it to compute the state root of that little subtree and then we discard it. Okay? So the solution that I was thinking about all that time but I just didn't have time to implement it. But now it's all implemented, by the way. So now it's all done and dusted.
04:14:15.568 - 04:15:03.350, Speaker A: So we going to store what I call the intermediate hashes. So you might think that, oh, but this is actually going back the direction of these trees. Actually no, because in the tree that the yellow paper suggests to do, we are mapping the hashes into the nodes. But here the mapping is kind of opposite not opposite but slightly in reverse. So we're mapping the prefixes in the trees to the hashes. So essentially the function that this particular structure serves is simply to have those hashes and we store them in the database. Yeah, and the storage is also I would say that it has nice properties because it's got to the data locality and things like this and it's also pretty small.
04:15:03.350 - 04:15:47.330, Speaker A: So now if we need to solve exactly the same problem, if we have this green thing that we modified and we don't have the pieces of the trees around it in our cache, so we can use this structure, new structure that we introduced to help us to bridge the gaps. And so that's how we do it. We load only those things that I pointed out with the arrows from the database instead of the entire region. And here we go. And we can basically compute our new state route. So this has been implemented this year quite recently, I think about April or something or May. And it has been optimized multiple times.
04:15:47.330 - 04:16:34.044, Speaker A: So now I show you what is the cost of that, right? And also I'm going to show you how our storage looks like. So the additional structure that I was talking about just now is called intermediate hashes. It's currently for the reasonably recent state is about two something gigabytes. So actually it's not a bad price to pay for this, right? And it is the structure which essentially stores all the intermediate hashes for all the levels of the street and stuff like that. And so you can also notice that there are two structures for the state. That's another innovation which we introduced quite recently. So we store the state twice.
04:16:34.044 - 04:17:06.664, Speaker A: But why? So first we store the state as a plane structure which is currently 9.9gb, 9.9gb, maybe a bit more right now. And then we have a hash state. So some of you may know that in order to keep the tree balanced, we need to apply some hashes to the keys before we put in a structure. So we separate these two things. So EVM actually works with a plane state and then in order to produce the state root again, only then we use the hash state.
04:17:06.664 - 04:17:58.024, Speaker A: So we convert it to the hash state and then we compute the hash there. But we have managed to get rid of the there was another table here which is called pre images. So because we have the two states, we actually don't need pre images and there are some other nice properties about it. For example, it allows us to withstand certain Dos attacks easier than other implementations. As I said, I'm not going to touch on the history storage but it's also pretty efficient. And so all in all, the efficiency in storage actually translates into efficiency of access as well because things are simply smaller and then they're actually more kind of standard so you can look them up quicker. So now we go into the second paradigm which is the concurrency.
04:17:58.024 - 04:18:53.288, Speaker A: And so here is something which kind of was discovered again this year, is that on the top you basically could see how the things were processed before in go ethereum and trubigia before that time. And you can see that when we download the blocks, there are, let's say some sort of stages of processing. For example, we do recover signature, sorry, we recover senders from the signature, so forth and we do certain things like run EVM through it and compute some storage route or whatever it is. And so usually this happens sort of in parallel with this kind of staggered concurrency and lots and lots of things happening in parallel. And so what we did is that we decided to split them up like this. And of course intuition would tell you that this is actually slower. If you just look at this graph diagram, it's like oh no, this is definitely going to be slower.
04:18:53.288 - 04:20:00.172, Speaker A: Actually it probably was a bit slower in the beginning, but then because we managed to split them into more homogeneous pieces, we were able to actually make the individual pieces faster and in the end everything was much faster, maybe like ten times. Again, this is another paradigm shift which you go into the counterintuitive direction and then you arrive at a kind of next level result. And so this is a stage sync. So what basically happens here is that we are trying to process, especially when we do the initial sync, we try to process things in large batches, as large as possible. So for example, if I were to sync right now, my first batch would be more than 11 million blocks. So I first put this 11 million blocks through the first stage and then another million in the whole batch through the second stage and so forth. And so because we do that and so in the red I just put here some text is that because we're putting most of these stages are actually essentially data transformation.
04:20:00.172 - 04:20:42.604, Speaker A: They take the data from one table and database, they do something with it and put it in another table of database. That's why I'm calling Ethereum a database. But with the databases that we use is that it's actually easier, it's actually much more faster to put that stuff in the database if it's pre sorted. So in other words, database are pretty you should not use them for sorting. So there are much better sorting algorithms. What that basically graph means that we can use that property to speed up the sync quite a lot. But if you, let's say process one block at a time, then the time is not going to be so impressive.
04:20:42.604 - 04:21:26.200, Speaker A: But it would be very impressive if you start processing millions block million a block at a time. And this also explains why our implementation actually could work with HDD sync, which hard drives actually one is currently just on a finishing line right now. I started almost 30 years ago sorry, not 30 days ago, sorry. And today is finally catching up with the tip of the chain. But when it catches up, it's not going to be able to process every single block through the stages. What it will do, which I tested before, is that you see this intersection. Here, for example, for my HDD, it will be about 15 blocks.
04:21:26.200 - 04:22:05.924, Speaker A: It means that it cannot process one single block quicker than 13 seconds, but it can process 15 blocks approximately at the same speed as these 16 blocks are produced on the main net. And so this is where we find the intersection. So my HD sync would be always having a lagger of about 15 blocks. But for some application it's probably all right. But as I say, that because SSDs are so much easier now, so much faster. And then because we don't require you to have five terabytes, so you can actually buy a pretty cheap SSD and run it. So you don't need HDD.
04:22:05.924 - 04:22:51.000, Speaker A: So it's pretty much for academic purposes at the moment. And I think by the time our size grows to two terabytes, you're probably going to be able to afford SSD at the same price as it's now for 1, something like that. Anyway, so then we go to the third paradigm shift, is the architecture. So I put the note here about core dev calls in July 2020. Some of you might have listened to those calls. So for three calls in a row, we decided to not discuss the IPS for Berlin, but talk about other issues, about kind of wider issues. And we talked about the developer burnout, we talked about development process and things like this, about the client diversity.
04:22:51.000 - 04:23:51.950, Speaker A: So maybe some people think it was completely useless, but actually I thought it was really good and I learned basically I found it very informative and I made some conclusions out of it. One thing we debated on the call is that whether the modularization and splitting things in component would actually help. And so there were opinions on the call with saying that if you start splitting things up, you will lose the optimizations because there was assumption that optimizations have to be cross cutting and they basically tie everything together and then eventually it becomes a big spaghetti. But my experience is actually the opposite. And there was one person on the call actually agreed with me. And so this is where we are on the journey to make this happen. And so as you can see here is that one thing is already done, two things in progress, one thing will be started and actually on the right here is that the stage sync is also part of that.
04:23:51.950 - 04:25:17.352, Speaker A: So it's also kind of component. One of the interesting things about this architecture is that it hopefully will allow us to bring more developers into the project and into the core developer development in general. It does actually also lead to certain sort of abdication of power from the core developers. But it's a bit larger topic and I would probably discuss it in somewhere else in kind of more long form, whatever chats and podcasts and stuff like that. Then the last slide, which is a bit of a you've probably heard about or seen me talking about ten x improvement in ethereum and today I just realized that, oh, why is it can't we go from ethereum one x to ethereum ten x or some x maybe two x? If we actually take it seriously, then there are, from my point of view, three main challenges here. So it's a nice goal to have, of course, to do ten x on layer one, but there are three main challenges which I sort of described here. I'm probably not going to unpack all of them for you, but they're dos attacks and there are certain dos attacks we know about and some of them are because of the state access, but some of them are computational, some of them are related to pre compiles and that thanks to vitalik to pointing out to me.
04:25:17.352 - 04:25:51.964, Speaker A: So we're currently actually assessing those things. I mean, I know that some groups are doing the same thing, but we're assessing it in the context of Turbogeth. We are finding more attacks and we're trying to figure out how to protect against them. And so we just recently started this effort and then there are other two things that people are worried about, but I think they are also solvable. So on this point, I think I'm going to end my presentation and I don't know if you have any time left. Sorry guys, I think I might have. Thank you so much, Alexi.
04:25:51.964 - 04:26:36.736, Speaker A: We do have a few minutes for questions and we actually have a lot of questions coming in. So what I'll do is I'll just kind of ask a few here and if we end up running out of time, I'll just ask you to join the live chat on live eonline.org and you can just answer them directly there. So a handful of clarifications, a handful of broader, bigger questions. The first one being, does double hashing attempt to address any long term state degradation related issues? I'm not sure about the double hashing, but I'm not sure what it means. What is meant by double hashing, to be honest. Sorry, I think this comment will reach the audience in about 30 seconds, so I'll just have them see and clarify this.
04:26:36.736 - 04:27:35.024, Speaker A: But in the meantime, I'll just move on to the next question and that is you kind of talked about that. It's taken about three years to kind of get where you are. Purely out of curiosity, what was like the most surprising breakthrough in that time from kind of your perspective? I think the stage sync was the most surprising thing because I kind of thought I discovered very early on that data model would be beneficial. It's just that with the data model, essentially the reason why it took so long is because it's sort of like in engineering. You solve one problem, but then you get another two problems to solve and then you solve those two and then there's another three. And so I wasn't sure whether this sort of I will be able to tie all the loose ends in the end. And then I only figured out like last year that we will be able to tie all the loose ends and it will work, but generally I thought it would work, but with a stage sync, for example, or now with a competent component.
04:27:35.024 - 04:28:46.040, Speaker A: That was quite surprising to me that such simple things actually make such a huge impact. That's awesome. I guess maybe a follow up on that. How many people are currently kind of helping you with being a contributor to Turbogap? Right, so it's actually the team has grown quite quickly and so specifically this year, so we currently have about 13 people who are basically working on it like ten full time, three part time. And most of the time I'm losing count because I'm actually inviting people all the time. And the reason I'm inviting people more all the time, because I do want to test this idea that if we do things in components, we can have many more contributors, which are sort of they're not just waiting for the pull request to be merged but they are there to develop their components maybe on multiple different implementations of the same component and so forth. So I'm actually opt for testing the theory that we could have a much wider contribution.
04:28:46.040 - 04:29:58.432, Speaker A: No, that's awesome. It's a wonderful problem to have when you're scoring too fast and you get to track who's fully contributing. So that's wonderful. Another question that sort of came from the answer you just gave, and that is, what was the actual original intuition that sort of made you even try stage sync in the first place? Was it just experimentation? Yeah, that's a good question because what I was so basically I was profiling Turbogast all the time at that some point I got to the place where the rights to the disk were the bottleneck at some point and obviously, as I normally do, that I started to split. Luckily, the database we had allowed us to look at, I've called the right churn, how much data you write for each table rather than on the entire database. And I noticed certain tables were very intensive in terms of writing and so I saw that those were the ones that they had a very sort of randomized keys. So for example, the transaction lookup one where if you have a transaction hash, you can figure out which block this transaction in.
04:29:58.432 - 04:30:48.240, Speaker A: And obviously the keys being a transaction hashes are pretty randomized. And so we were inserting those randomized hashes within the middle of the table and I was scratching my head thinking maybe if we tried to presort them, if we just take the whole bunch of them, like 900 million of them, presort from them by hash and insert them that way, is it going to be faster? And it turned out to be faster. And this whole thing generated essentially the stage sync. It required a bit of a bravery because in order to push it through, we had to delay the release and we had to rip everything apart and then change the architecture entirely in about two months. It was actually kind of risky thing to do because some people said, no, you shouldn't do that. It's just like release and then change it. No, if I release, I'm never going to change this.
04:30:48.240 - 04:31:30.850, Speaker A: I have to do it right now. And I think it sort of paid off. Absolutely high risk, high reward, and I'm glad that you did. Last question for today before we move on to our next talk. And that is it's a pretty big core value in our space that people should be able to run and kind of individually run and validate nodes, and especially from the genesis block. So what are your kind of thoughts on this as a norm? And is one of the motivations behind Trevor Get to actually make this world a lot more possible? Or is it purely it wasn't started for this reason. I mean, I simply wanted to create a better technology.
04:31:30.850 - 04:32:34.896, Speaker A: I don't know if it's going to be the norm, because maybe if we do, like, let's say, E ten X, maybe we will make it impossible again to essentially we improve the technology, but we use it not to let everybody run a node on their Raspberry pi, but maybe we're going to use it to expand the boundaries of the system instead. Right? So, I don't know, it's not for me to kind of make this sort of into ideology. I think I just want to make a better technology and we'll see what it's going to do. Is it going to make people run Raspberry PiS or is it going to make Ethereum ten times bigger? Right, that's wonderful. I think you're taking the stance of being a technology maximalist and that's awesome. So there's a few more questions that are in the chat, so I'd encourage you to just kind of reply in the chat directly. And I want to thank you again for doing this talk and answering all of our questions and excitement on the chat.
04:32:34.896 - 04:33:09.096, Speaker A: So thank you so much, Alexei. Thank you. Bye bye. And next up, we have our panel, and I'm super excited about this panel because we have three amazing people that are going to be talking about how web3 and the world just gets to use Ethereum on the enterprise side. So we have three amazing people for this next talk, and that's Dan Shaw, Connor Spencenson and Peter Robinson. And Dan will be the one moderating it. And I'll let Dan intro our panelists as part of this and I'll kind of have him give him the rest of the context.
04:33:09.096 - 04:33:44.212, Speaker A: So, without further ado, I would like to welcome Dan and Connor and Peter to this chat and I'll let them kick off with our next panel. Welcome. Awesome. Thanks, cardik. Hello. Welcome. All right, so today we're going to be talking to the creator of Web Three J and talk a little bit about Web Three, how that's evolving and how that kind of fits into a broader enterprise context.
04:33:44.212 - 04:34:06.560, Speaker A: I'll be your host. I'm Dan Shaw. I'm advisor to the Ethereum Foundation on Enterprise community. And I'll let Connor and Peter introduce themselves. So Connor, do you want to kick off the introductions? Yeah, sure. Thanks, Dan. So Connor Spenson, the author of Web Three J and also the founder of Web three labs.
04:34:06.560 - 04:34:22.196, Speaker A: I'll leave it at that. Pass over to Peter. Yeah. I'm a technical director. Applied cryptographer at Consensus Protocol Engineering R D. Back to you, Dan. Awesome.
04:34:22.196 - 04:35:01.368, Speaker A: So I want to start with some basics, especially since know not just the total Ethereum community but also the enterprise community. Love to have kind of crisp definition of what we're talking about. So Peter, can you help us lay the groundwork and just remind folks what we're talking about when we're talking about Web Three? Sure. So the simplest way is let's talk about Web One Two, and then we'll get to three. So Web One, I've got a web server and I own the content. I write the content. It's pretty static.
04:35:01.368 - 04:35:43.904, Speaker A: People surf the Internet, come to my site, Web Two, I own a server, Facebook, Twitter, and people come to me and they create the content and I monetize their content, but I own their content. And web3. It's decentralized. And so people own their own content, people own their own business logic, and there's no central body that owns it. So you don't have centralized governance and you don't have centralized servers. So you might have a network of servers that the information is across. The other big thing about Web Three is it's the web of money.
04:35:43.904 - 04:36:10.440, Speaker A: So you can do value transfer as part of things. It's a native part of the whole system. So I don't know. Dan, is that a pretty good introduction to Web Three? That's great conducts. Thanks. Connor, you created Web Three J once you've sort of lit the groundwork there and tell us a little about why you created and what it does. Sure.
04:36:10.440 - 04:36:44.884, Speaker A: So it all started for me back in 2016. Historically, I'd spent many, many years working in enterprise and then finally got out of that world, into the world of startups in 2015. Worked on my first venture. It didn't work out, but then kind of moved sideways and discovered Ethereum in 2016. So I wasn't one of the super early people on the scene, but I'd followed Bitcoin. But Bitcoin had never really captured my imagination. Fundamentally, it was just the store of value.
04:36:44.884 - 04:37:42.772, Speaker A: What I always loved about Ethereum was just the whole concept of having this massively, decentralized world computer, basically to paraphrase other people in their description. And what drew me to Ethereum was that looking at blockchain back then it was the project that really stood out. It was the first of the Blockchain 2.0 projects and the community around it reminded me a lot of Linux. I've been kind of big into Unix and Linux in the unfortunately for me, jumped on the wrong horse, got into open Solaris and this sort of stuff when the Linux community was exploding. And then when it came round to the blockchain and looking at the communities there, I was like, I'm going with the biggest community this time around. I don't care about what I think, the technologies, I mean, don't get me wrong, it was an amazing jump, I think, with going to 2.0,
04:37:42.772 - 04:38:21.700, Speaker A: but fundamentally the community thing was what drew me into it, as well as just what it was all about. And even now, in 2020, I feel exactly the same way. We've had all these other projects emerge. Yeah, they're so called Ethereum Beaters or whatever you want to call them, but I don't think it's any different. We've still got an amazing community in the Ethereum ecosystem. But going back to 2016, I'd spent all these years in enterprise working on the JVM and being mainly a Java developer. And when I started working with Ethereum, it kind of seemed like most of what was happening there was in the JavaScript world.
04:38:21.700 - 04:39:13.360, Speaker A: And coming from that enterprise background, I thought, well, there's all these enterprises talking about the transformative potential of blockchain. Ethereum is the biggest blockchain platform, so surely there's a need there to ensure the JVM can plug in seamlessly to it. And that was the genesis of Web Three. J spent a bit of time researching to see if anyone else had an equivalent that just kind of hooked in at the API layer. There wasn't anything. I thought it would be a simple project, but then got sucked into the rabbit hole, signing and generating smart contract wrappers and all this other stuff you have to do, and released the first version of it just in time for DevCon Two in Shanghai. So kind of from there it's just grown and grown and it's been a lot of fun just having this project and see it emerge over the years and get ever more sophisticated.
04:39:13.360 - 04:40:11.172, Speaker A: Brilliant. So you touched on a little bit maybe, Peter, you could kind of continue the thread. We've got the JVM and Java and Java has been around forever. It's the old dinosaur, the old workhorse. Why should we care as we're sort of innovating and paving out this new universe, why should we care about mean, I've worked at a few companies and one of them was Oracle Labs and Oracle's got a product, I won't say which one it is and it's got 100 million lines of code and they're in Java. Now if someone said, oh, we want to put this new module in and we're going to bang it in in Golang or Rust or something, they're going to. Say, no, it's really simple.
04:40:11.172 - 04:41:09.220, Speaker A: No, we've got 100 million lines of code, and they're not the only company that has a massive code base of Java. So they've got all these devs who know the language, they've got capability, and so trying to have a new little piece that's in a different language makes supporting the overall product much harder. So that is a big reason. Another reason is a lot of companies like to outsource to places such as India, and you've got something like 10 million devs in India who know Java. So that's another reason for why people use Java as well, is you can outsource to all over the world, and there are just a lot of people who know Java. And so if you can use this tool, Web Three J, so you can think of the Ethereum network, you got all these nodes. Web Three J is this library that you integrate into your app, into your Java application to talk to the network.
04:41:09.220 - 04:42:12.424, Speaker A: That's why people use Java is because there are a lot of people who know it and it makes it. And if you've got an existing code base, then you just want to keep on going. Yeah. So Connor, you've built this incredible community. What are some of the ways that folks are leveraging Web Three J? On the JVM, we have emerging languages like Kotlin and the Android ecosystem. How is building the web3 functionality for the JVM? What have you seen the community doing? So I think just building off the back of what Peter was saying, so many companies started investing heavily in their Java infrastructure in the late ninety s. And so you've had basically 20 years of business platforms being built, and these companies have exploded in terms of their size and their global footprint.
04:42:12.424 - 04:43:19.330, Speaker A: And so what that's meant is that for people like them, it's about being able to create some nice integrations there. The classic thing, of course, is these enterprise integration patterns, which is a very well established thing in terms of how you connect disparate systems or you solve different messaging problems, and there's loads and loads of products and very large companies that kind of operate in that space. Obviously nowadays the Kafka message bus is one of the big ones. But then historically you had, well, Ibmq is still massive there, and MuleSoft kind of made it their bread and butter just kind of servicing that community. And so what you see is like a number of different integrations there. That's a very natural place for it. So part of it is having adapters that can write to persistent data stores, but then also, like, say, with the guy Bill Gin over at Red Hat who wrote the Web Three J adapter for Camel, there's kind of a classic example of how it just kind of plugs into it.
04:43:19.330 - 04:44:10.716, Speaker A: What we certainly see is just that within the JVM world, it tends to be quite different the sort of applications and use cases that people have versus the JavaScript world. I mean, don't get me wrong, like the full stack development thing, it's amazing what you can do nowadays with Node on the front end and the back end. But where you have these enterprise services, you're thinking about backend services. Typically you're not going to create a UI and these sorts of things generally using the JVM. You can though, and there are frameworks for that. But it's those sort of serious business apps that are doing a lot more. It's more about the data processing and doing transformations of messages and so on and so forth, which you don't have those sorts of problems when you're writing client side applications.
04:44:10.716 - 04:45:06.512, Speaker A: It's more when you get into much sort of meteor scenarios. But the other point, of course, as you mentioned there, Dan, is the Android community. And this is something which it's been great to see the adoption of Web Three J in that space. Samsung used it in their blockchain SDK which they released last year for people building, wanting to integrate with Ethereum on Android phones and then also Opera kind of they were one of the early people to use it with their crypto wallet as well there. And so I think what's so powerful about the JVM is that it kind of serves these two massive communities. They're not the kind of ones that people will necessarily write a Web app in. But if you want to write a phone app, then of course you've got things like React native, for instance, to Simplify.
04:45:06.512 - 04:45:56.870, Speaker A: But if you want to write it on the native platform, then Web Three J is what you use for that. And likewise as well on the JVM, Kotlin's got a lot of growth in the Android community just because of its backward compatibility. But I won't go into bore people with those details now. But yeah, it's very well embedded in those different communities and consistently you see the biggest programming languages. It kind of alternates depending on which survey it is, whether JavaScript, Python or Java, but very bright. That's excellent. We're at a point where Web Three is beginning to take off and we're heading towards a convergence of Web Two and Web Three.
04:45:56.870 - 04:47:26.160, Speaker A: But there's still kind of the last bit of chasm, especially folks that are coming from the Web Two world and trying to digest what Web Three means to them. Peter, what do you see as some of the key things that folks need to either do or understand to cross that chasm? I think understanding decentralization is if you want to do a good app in the Web Three world because there are a lot of people who've got an application and they said we want to blockchain it, and so they just add a blockchain to it and there's all sorts of central points of failure. If you want to have a credible blockchain app, then there needs to not be those central points of failure. I think that's a big one. Thinking about the design of the application, also thinking about if you're going to be doing something on the blockchain, you really don't want to be having some massive heavyweight complex smart contract. The smart contract should be a minimalist thing with a lot of other business logic. So you got your core business logic on chain and have a lot of other things happening off chain.
04:47:26.160 - 04:48:01.896, Speaker A: But then some people can dip into the Web Three world for micro payments. And I've seen that a lot with say, games. So this is actually a topic, you have to shut me up on this one because this is something where I could talk for hours. But it's a huge field. But I mean, a way that people often dip their toes in is to allow micro payments of their thing that they've got. So people paying, say, with an ERC 20 or just with Ether itself. And that's a way of people who've got a Web Two app integrating Web Three.
04:48:01.896 - 04:49:16.016, Speaker A: Nice. Awesome. So Connor, how is Web Three J evolving to meet the needs and what does the future of Web 3G look like? So we've got some really cool things that we're working on at the moment. And fortunately as well, the Ethereum Foundation has just given us another grant. So we've got to give a big shout out and thank you to them for that as well because it makes a big difference when you can have some funds to really focus on this as opposed to kind of working around other projects to do this stuff. What we have, and we've already got the core framework support for the specific features of the major Ethereum clients and also things like Hyperledge well, sorry, and also the enterprise focused ones like Basil and Quorum too. We have Maven and Gradle plugins, which are the two major build tools used on the Java Android platforms.
04:49:16.016 - 04:50:21.050, Speaker A: And those build plugins are sophisticated enough now to the point where if you have solidity code in your project, it will automatically generate the Java code to talk to them and then it can also generate unit tests as well for those. Something else though, that the guys have just got a release going out tonight is an open API generator. So what's really awesome about this is that this is a build plugin that you basically say you add this build plugin to your project and some solidity code, and what that will do is it actually generates a fully runnable project which spins up open API APIs for every method on your smart contract, enabling you to deploy and manage and whatever else. So what that means is you can basically just take a solidity contract and then you can just spin up an application that exposes it in an open API format. So it massively reduces the barriers because it's one of those things we've seen people do again and again. They create a smart contract, then they write a Java app to basically create a web service to talk to it. And so we've automated all of that now.
04:50:21.050 - 04:51:16.964, Speaker A: But some of the other things though that we are going to be focusing on now, one of them is starting to think about the e two side of things with the beacon chain. It doesn't really change how people are going to be interacting with main net in the near term, but where we feel that there's still a bit of value is having a Web Three J library for talking to the beacon chain client API. We're also starting to go deeper with our debugger as well. So we do have a stack based debugger that uses the hyperledger Basu EVM. So you've got a production grade EVM for running your unit tests against with Web Three J and we can easily specify docker files or an in memory EVM to do this. It makes integration testing really powerful against different clients. But we're going to be beefing up the debugger.
04:51:16.964 - 04:52:31.164, Speaker A: We're looking at support for an IntelliJ so you can actually have a proper debugging experience in IntelliJ. So going from Java code to Solidity and back again. And then the other thing is being able to actually pull down like a lot of projects like Open Zeppelin being the main, the one that people probably use the most, but they typically upload contract artifacts into NPN and so we're going to enable it in Web Three J. So if you put an at annotation like you do in the JavaScript world for referencing a smart contract that lives on an NPM repository, those contracts will get pulled down into your Java project. So you don't have to copy and paste code to use certain interfaces and these sorts of things. And the other thing is a form of migrations and just refreshing the docs and these sorts of things too that we haven't spent as much time on. And to kind of go back to Peter's point there about the whole getting started experience, it's still so hard for people when they first come to the ethereum paradigm, because not only have you got to get your head round a node, you've also got to get your head round a wallet and you've got to get your head around liquidity.
04:52:31.164 - 04:52:58.920, Speaker A: And it's like it's all these things before you even start writing any code. So we're on a quest to kind of simplify that and also give people first class developer experiences. Brilliant. I was just going to say. And one other thing as well in the baseline too, that's the other area as well we're exploring because right now the baseline is using node infrastructure for the smart contracts. We want to add in support using Web Three J for that too. Nice.
04:52:58.920 - 04:54:14.752, Speaker A: Yeah, the enterprise folks that I've been talked to, baseline protocol is central to a lot of their considerations. You mentioned E Two. And as we look forward not just the implementation in Web Three J, but Web Three, maybe. Connor, Peter, sorry, you could kick us off in how does Web Three look like in an East Two era? Yes, so good question. So I think E Two, for those people who've been following it over the years, is an ever changing beast. It has been morphing and changing and new ideas come and go. There are definitely going to be shards of some sort and the latest thought is that they'll be using a technique called roll ups as a way of having massive scalability, but then you might have execution shards and data availability shards.
04:54:14.752 - 04:55:12.080, Speaker A: I think the thing you can think about is though, that you'd be targeting your Web Three J at one of the shards, essentially for your execution environments. And then it's a matter of dealing with how do you do cross shard communications, because you could imagine you're going to have a contract on one shard and another contract on another shard and you want to do a cross contract call. So just working that through. So there are some complexities, but the thing that I think we can all be sure of is one way or another you're going to be able to write Solidity code and it's going to work on the ethereum too. We're not going to throw the baby out with the bathwater. And somehow rather the main net as it is at the moment is going to be a shard in ETH Two that's thought of as the transitionary process is you get that working as part with the beacon chain. So I think that it's just a matter of being able to target multiple shards.
04:55:12.080 - 04:56:58.904, Speaker A: I think that is going to be the big thing, but it's an evolving thing as exactly how the execution layer and data availability layer will work. Beaconchain is due to be launched really soon. Connor, anything you want to add? So I have to say that earlier on this year, when it kind of sounded a bit more certain that we weren't going to be launching brand new execution environments on E Two right from the get go, it's a sigh of relief because it meant that the fundamental infrastructure we've built up with web3 J around transaction signing, creating contract wrappers so you can natively interact from the JVM with smart contracts. It just kind of still works as it always has. And it's something that's always a consideration with any technology that balance in terms of whether you decide to come up, rewrite everything and start from scratch with a kind of more superior approach, or if you kind of stick with what you have because it's well understood and you kind of move across the other side. And I think somewhat probably unintentionally going back to the early considerations with it, but Solidity has kind of become a global standard for smart contracts. It's like you look outside of the Ethereum ecosystem, like projects that can support solidity, say you can run solidity contracts on our blockchain because they know how many people are familiar with that.
04:56:58.904 - 04:57:40.150, Speaker A: And so it's nice anyway to hear that with these two. Anyway, right now. Anyway, it should be going in that direction, which I think will make everyone's life a lot easier when it comes to the actual migration of it. Excellent. Well, Connor, Peter, it's been a great deep dive into this core part of infrastructure for building Web Three. One thing I want to call out to everybody, there's a survey that the Ethereum Foundation and the EA are doing on DevTools. So if you go to bitly.com
04:57:40.150 - 04:58:20.544, Speaker A: etherprise dev is the chunk part of it. Love to get your feedback. And it's been great to support the Web Three J project and make sure that everybody has the resources to build into the future. And by participating in the survey, you help us figure out where we'd spend our time and resources and supporting the community. So really appreciate you all taking the time and it's been a great discussion. Thank you very much. Thank you.
04:58:20.544 - 04:59:07.650, Speaker A: Thank you so much, dan, Peter, and Connor. And even though Dan kind of talked about the URL, we'll put that in the chat in a few seconds as well. So all of you watching can click on that link and fill out the survey. We are at time, but I do have a question for each of you, and we can just end on that question. And the question is kind of as over the past few years, obviously you've interacted with a lot of enterprises trying to explore Web Three and integrating this into their day to day projects. What's kind of the most common thing that you hear that either hasn't still been addressed or, you know, that we as a community have to work on improving and kind of what can people do to address those problems that you hear commonly? Shall I go first? Go for it. That'd be great.
04:59:07.650 - 05:00:08.804, Speaker A: Unfortunately, there's still a lot of misconceptions that if you run Ethereum, it's proof of work, which it's annoying right when you hear that because there's so many other options there. But that's certainly one area that enterprises kind of do struggle with. But the other side, though, and this kind of is by design, really, with the public Ethereum network. It's really about individuals in terms of a wallet is kind of key Identifier on it, and it's an anonymous Identifier. With enterprises, when they want to transact with one another, they absolutely want to have complete certainty on who they're transacting with, and that sort of confidence there. So that's the key one. And it's not to say you can't kind of have abstractions to tie in with whatever, say, directory services that map to an Ethereum wallet, say, to work around that.
05:00:08.804 - 05:00:57.008, Speaker A: But with the platforms that were designed from the ground up for enterprise. It's like they kind of baked a sort of PKI type infrastructure into it. So it was like that just familiar paradigm. And they're using the crypto curves that are kind of the really well known ones, that there's lots of implementations of, that those enterprise security guys are most comfortable with. Peter? Yeah, look, I think one of the big things that we have is that a lot of people don't know that you can run a consortium chain in know, you say Ethereum and they say mainnet or they don't say mainnet. Ethereum is this, as Connor was saying, a proof of work. Public chain, no permissioning.
05:00:57.008 - 05:01:23.996, Speaker A: And a lot of enterprises instantly think, chaos, the world's come to an end. Of course, there is baseline, which is now a way for businesses to use mainnet and feel happy about their data. But I think consortium chains are a big thing, too. And, yeah, people often miss the fact that Ethereum can do that. That's me. Nice. Yeah.
05:01:23.996 - 05:02:44.980, Speaker A: For me, I often get asked the question, what's the killer app for Ethereum? Folks are poking at that the enterprise love to pick winners and in that they're looking for why invalidating, why is Ethereum going to win? And ultimately, I think it's the distributed nature of the platform and for folks to get from where they are today in their adoption to really the maturity and the need and the use cases where distributed collaboration really clicks in. One of the things that I've been directing folks to focus on is something like baseline protocol. It simplifies the surface area of the problem set for an enterprise that is asking the question, what should I do now with Ethereum? So that simple. I love to guide folks with the enterprise. Lots of no's and one or two yeses. So, as Peter was saying, consortium blockchain. Fantastic.
05:02:44.980 - 05:03:23.456, Speaker A: And baseline protocol so we can get that simple distributed point on the eventually mainnet. That's wonderful. Dan, peter Connor, thanks again for this great panel and hope you have a great rest of your day. And with that, we will move over to our next talk. And this is a talk that I am super excited to talk about. Next up, we have Kelvin from the optimism team and he's going to be talking about how optimism is going to help scale Ethereum. We've seen a handful of talks on scaling today and this is going to be another fantastic one in the series.
05:03:23.456 - 05:03:55.596, Speaker A: So Kelvin is here with us on this chat, so I'll let him introduce himself and kick off with his presentation. So welcome, Kelvin. All right. Hello. Super excited to be doing this. My name is Kelvin, I work for optimism and I do a lot of random stuff, working on this extremely cool project. So I'm very excited to be able to talk to you about this.
05:03:55.596 - 05:04:25.544, Speaker A: And it's kind of going to be a fun presentation. It's not going to be too heavy, not going to be too light. So I hope that everyone gets something out of this and let's talk about the very cool things that are happening in the scaling space right now. All right, so I need to share my screen. Let's get that going. All right. And there we go.
05:04:25.544 - 05:04:48.270, Speaker A: I believe we're good to go. Good to go. All right, optimism. Keeping ethereum half full. All right, here we go. All right, so kind of the first thing that we need to understand is why exactly we are even talking about this right now. So in a right, this is how Ethereum works.
05:04:48.270 - 05:05:41.124, Speaker A: You kind of tell Ethereum what you want to do, whatever these people are saying and Ethereum is like this little family business. Ethereum, here's your little transaction and then it processes your transaction and gives you a result. Right? This is the basic flow of Ethereum and this is kind of a way to look at how this execution really works. Sort of a very simplified version of this execution. And the important thing here, you don't really have to understand much about this diagram right now, but the key things are what Ethereum really does is it's storing your transaction data and results and it's also doing the actual transaction execution. And that means running smart contract code. And the thing is that data storage is pretty cheap.
05:05:41.124 - 05:06:32.656, Speaker A: Like hard drives are pretty cheap, but execution on Ethereum is quite expensive. And a majority of the cost on Ethereum right now comes from the fact that we have all of these new smart contracts that are quite complicated and transactions burn gas like crazy. So this is sort of an updated picture of Ethereum today and we kind of just shout loudly and hope for the best. We've got this person with very long legs and one arm saying we uniswap. We've got a large baby screaming for more yams. We have this gold medalist with gold legs and lambos for feet asking us if we've ever heard of bitcoin. And this is sort of what happens to Ethereum in the face of that.
05:06:32.656 - 05:07:13.836, Speaker A: So you see, Ethereum is completely swamped and everyone's really sad because there's so many transactions to process. So how did we solve this dilemma? Well, there is sort of a clear idea, which is child labor, child chain labor. In this case specifically, we don't consume child labor. But you can see we go to the next generation. We offload some of our work to the other people that we have sitting around. In this case, offloading that work means doing the computation somewhere else on optimus. So here is the optimistic execution model.
05:07:13.836 - 05:08:13.120, Speaker A: And again, all you really have to understand in this slide is the transaction data still gets stored on Ethereum. But we stick the transaction on Ethereum and then we read it and we say, okay, what does this transaction want to do? And then we kind of run it in sort of another version of Ethereum, this L2 version of Ethereum. And that's where the execution happens. And then we publish the result back onto Ethereum. And because this execution is not happening on Ethereum anymore, and we don't have to have all of the Ethereum nodes doing this execution, right, so we do the computation outside, then the fee for this transaction is just going to go way, way down because we don't have thousands and thousands of nodes executing these contracts. And instead we just do it on L2. You've got all the kids doing all the work, the child chain doing all the work here.
05:08:13.120 - 05:09:21.060, Speaker A: And the fundamental problem that arises here, sort of this looks very simple in theory, but the fundamental problem is this advent of fraud, right? So what exactly is fraud? Well, we have these contracts that are storing the transaction data and then L2 gets the transaction and executes the transaction. Executes the transaction and then we get this result that gets published on Ethereum. But Ethereum didn't actually run this transaction, so how does Ethereum know about what happened? Ethereum doesn't know whether the result that was published is actually valid. And so on L2, the people who are publishing these results can pretty much claim anything, any possible result. And layer one has no clue. They kind of just have to accept it. Ethereum kind of just has to accept this result because it hasn't done this work for itself.
05:09:21.060 - 05:10:08.080, Speaker A: So in this case, everything is now candy. So what are the solutions to this general problem? Well, you basically have two options here. You either have Moon Mass or not Moon Math, which is just making Ethereum do the transaction again. So fundamentally, what you are trying to achieve in one of these two solutions is make Ethereum know what the correct result was. And so Moon Math is essentially proving to Ethereum that your result is correct. And the not Moon Math way of doing it is making Ethereum just run the transaction again. And these each have their various trade offs.
05:10:08.080 - 05:10:47.472, Speaker A: But I'm going to take a look at the Moon Math route first and then we'll talk about sort of the path that we've taken in tackling this problem. So, option number one, moon Mass, right? You get this kid to do it. And I didn't need to even Photoshop the glasses on this kid. So you just know that they know what they're doing. And essentially what you're trying to say is this result cannot possibly be invalid. So you provide a transaction result to Ethereum and you give some proof that it can't possibly be invalid. And obviously you want Ethereum says, prove it.
05:10:47.472 - 05:11:48.790, Speaker A: And you provide this sort of complex mathematical structure that convinces you on layer one that this result was correct. The trade off space with this sort of approach is that it's quite confusing to a lot of people. It's quite difficult to understand it's not necessarily developer friendly. And it also limits you in sort of the design space of how you can build your applications. You typically cannot just build a solidity smart contract and deploy it to a system like this because these proof models sort of need to be sorry. You need to build your applications in a way that you can then go and prove that the result of executing your application was X or Y, whatever. And oftentimes you have to adapt your application to the proof system rather than the proof system just working for any arbitrary application.
05:11:48.790 - 05:12:19.810, Speaker A: And especially today, I think that this space will definitely grow in the future. But right now it's sort of quite early and quite difficult to use. So what is option two? Well, let's just briefly bring back up our old diagram here. We have this stuff on Ethereum that I called the roll up contracts. And that's what we want to take a look at. So what actually are these roll up contracts? Because I kind of lied. It's not this simple.
05:12:19.810 - 05:13:22.980, Speaker A: Essentially there are three contracts or three sort of main areas of these contracts. And the first two of these, the sort of canonical transaction chain and the state commitment chain are pretty straightforward. So the canonical transaction chain is just where the transactions go when people put them on Ethereum, right? So kind of think of like your mem pool or something like that where this is an append only lock. You put transactions in there, they stay there forever, they can't be deleted or anything. And this is what the L2 nodes then read from in order to know the ordering to execute transactions on L2. Then the state commitment chain, also pretty straightforward, is also an append only log. Except that so this is where the transaction results go after you've executed after the L2 nodes have executed the transaction the transaction results get posted to this state commitment chain.
05:13:22.980 - 05:14:18.800, Speaker A: And this is sort of read only, a pen only log. But there is a case in which this state commitment chain can be deleted and that is as a result of something in this fraud verification contract. And that's exactly what we want it. We want a system where somehow we can say you cheated and therefore the result that was published is not invalid or sorry, it's not valid anymore and we need to delete it and somebody else can publish a different correct result instead. So this is the general flow. I think you cheated, right? And then I say to the fraud, I messed these slides up. This should say they didn't cheat.
05:14:18.800 - 05:14:52.540, Speaker A: Whoops. My animations are quite rushed today. But point being, if the fraud verification contracts recognize that you did not cheat, it won't do anything. It won't touch this state commitment chain. But if the fraud verification contracts say okay, wait a minute, the result that we found when we executed this. Transaction again on layer one was different than the result that was posted or supposedly executed on L2. So that result was fraudulent.
05:14:52.540 - 05:15:48.528, Speaker A: We need to go in and we need to remove that result from the sort of list, not canonical, because we can delete it, but this list of results. And then we have a timeout window that says you must prove that something is fraudulent within a certain amount of time from when the result was published. And we assume that if you don't try to go through this fraud process after that amount of time, then you just figured it was valid and not fraudulent. And after that it can no longer be deleted. So this is great because this is why we kind of call it an optimistic system. In the ideal case that people are just sort of motivated by profit or whatever, they won't try to cheat. And if they do try to cheat, then we can challenge them.
05:15:48.528 - 05:17:00.410, Speaker A: But if they don't try to cheat, we don't have to bother challenging them, right? Because it's only necessary in the case that there is actually fraud. So when people are submitting valid transaction results, it's much, much cheaper because none of these transactions actually have to be executed on Ethereum itself. So now the really fundamental question is how do you make this work? This is all sort of straightforward in theory, getting Ethereum to just do it for you. But how do we actually do that? And the meme answer is this or this, which is basically that we go over the whole transaction together on layer one through sorry, whoops mess up that slide? Whatever. Okay? Yes. The fundamental problem with any of these sort of systems essentially can be described like this. There's a smart contract and it makes decisions based on something like the current Ethereum time.
05:17:00.410 - 05:17:48.760, Speaker A: And Ethereum says the current timestamp of the current block is this. And timestamps are just one example. But there are a lot of things where the result of these sorts of opcodes stuff like timestamp, is going to be fundamentally different between layer one and L2. So the current time on layer one or the current time on Ethereum is going to be different from the current time on L2. And if we execute this transaction in a different block on layer one, the current time will be different. So all of a sudden this transaction is no longer deterministic. And if we run this transaction on layer one, ODS are, it will come up with a different result than what we got on L2.
05:17:48.760 - 05:18:37.640, Speaker A: But not because we did anything wrong. Just because it uses an opcode, a stateful opcode, because it uses a stateful opcode. And this adds a sense of nondeterminism. And so we need to fix this, right? Essentially, we need to figure out what are the things that contracts can access that are dangerous to us. And the answer is storage contract storage, other contracts and information about ethereum, like the block number. And if they are able to arbitrarily access this information, then they could interact with a contract that doesn't exist on layer one when we're doing the verification step or so, it doesn't exist on L2 when we're doing the verification step on layer one. And that would cause this nondeterministic behavior.
05:18:37.640 - 05:19:32.616, Speaker A: So what are the opcodes that we don't really care about? The safe opcodes that we don't have to worry about? Basically the pure opcodes. So add, subtract, push, pop the stack opcodes, M Load, S store. These things don't have anything to do with modifying the state. But what are the tricky opcodes? So we have sort of the ethereum state opcodes like timestamp and number, s Load and S Store for accessing, contract, storage, call, create, and all of their various different forms. So all of these opcodes give us issues. And if we just allowed contracts to arbitrarily use these opcodes, we could essentially never guarantee a deterministic execution of the L2 transaction on l One when we're verifying it. So how do we block them? Well, we need to block them because we can't allow people to use them.
05:19:32.616 - 05:20:27.080, Speaker A: So we have this thing called the Safety Checker. And the Safety Checker is literally a smart contract that statically analyzes contract bytecode and it says, oh, I see that you include an S load, I'm not going to allow you to deploy this contract. This contract is not allowed to exist on L2 because you use one of these banned opcodes. But if you're a good contract and you decide not to use any of these banned opcodes, then you're totally fine. You can deploy on L2 because that means that when we go back and execute it on layer one, there's no chance that you will use one of these sort of opcodes. That give us a headache. But obviously these opcodes, especially things like S load, S Store and call are extremely useful and we can't live without them.
05:20:27.080 - 05:21:54.196, Speaker A: So how do we actually get around this problem? How do we make sure that people still have access to these opcodes, even if we need to ban them? And the answer is that we get around it by taking all these opcodes and we replace them. We replace them with our own sort of custom opcodes. And these are implemented as calls to a special contract called the Execution Manager, that sort of handles all of the logic for these opcodes. And it ensures that these opcodes will always behave deterministically based on sort of some setup phase when you create the initial, when you want to run this fraud verification step. And in order to sort of translate from Solidity code to these special opcodes, these calls to this special contract that handle these opcodes for you, we have a custom compiler, a custom version of the Solidity compiler that will, instead of allowing you to use these opcodes, just immediately translate them into calls. So this is the general flow of the Execution Manager here. If we just had the Execution Manager with no context and a contract wanted to know what the current time is, well, if the Execution Manager hasn't been set up properly, then it doesn't know what time it is.
05:21:54.196 - 05:22:33.620, Speaker A: And then all of a sudden, it doesn't have any clue of what to do. So really what we need is the two step thing where we have these two contracts called the Execution Manager and the State Manager. And the way this works is that users populate the State Manager with what the state was on L2 before this transaction that they wanted, that they want to sort of verify on layer one. And they say, this is exactly what the state was. These are what the contracts are. It's not all of the state, it's only the state that's relevant to this particular transaction. And you load that into this thing called the State Manager.
05:22:33.620 - 05:24:21.110, Speaker A: And then when you actually execute this transaction during this fraud verification process, when these contracts, for instance, ask for what the current timestamp is, the Execution Manager will then go into the State Manager and say, okay, hey, what time is it right now? And it will access that state and return it back to the other contract. So that's sort of the nice, ideal case of a very simple interaction. But let's say that a contract we're trying to interact with a contract that has not been set up on layer one. So a contract that the user who is trying to prove fraud has not sort of initialized set up, made it ready to be used during this fraud verification process, then what would happen is that in order to actually, let's say, do a call to this other contract, the contract would request this call from the Execution Manager. The Execution manager would ask the State Manager, do I know about this account? Has this account been set up? Does this contract have the ability to access this other account? And in this case, if the answer is no, then all of a sudden, everything has to immediately stop. And this is really important because everything has to immediately stop so that the contract cannot make decisions about the presence or absence of information during this fraud verification process. Because on L2, all of this state is always available.
05:24:21.110 - 05:25:28.140, Speaker A: So if it can somehow recognize that, oh, hold up, some of this state is not available, to me, it can make decisions based on whether it's running on L2 or layer one. So we have a lot of very interesting, very interesting problems to sort of face here, where the Execution Manager has to figure out how to immediately halt the entire execution. But it can't just revert because that opcode is a call. And if it reverts, then that revert can be caught by the contract that called it. So what we end up having to do is make sure that the contract that called it also reverts if the execution manager reverts in this way. So there's this really fascinating aspect of having to sort of hijack reverts and figure out how do we carry all these things up? Because we can't allow execution to continue. And this is all sort of reflected in our contracts right now.
05:25:28.140 - 05:26:14.280, Speaker A: But it's an extremely interesting problem. It's somewhat nontrivial and all these sort of things that you would think are quite easy. Running EVM code on Ethereum are actually quite difficult when you're trying to prevent access to state that contracts should not have access to. Of course, everything errors out. All right, last little technical bit. Because we'll do this, one important thing that we all would need is being able to send information between layer one and L2 so that we can, for instance, transfer ERC 20s from Ethereum onto this optimistic roll up system. So doing that from a layer one to L2 is quite easy.
05:26:14.280 - 05:26:44.964, Speaker A: You kind of just tell Ethereum what you want to say. Ethereum puts it in the L2 chain as a transaction and it's just there. And you can access it on L2. That direction is quite simple. It's basically just sending a L2 transaction. The other direction is a little more difficult. The other direction, you say something, you have a message that you want to say on L2, like transfer some number of years through twenty s or just high, whatever it is.
05:26:44.964 - 05:27:32.656, Speaker A: And you sort of acknowledge that you put that in the chain on L2. But what gets posted to layer one is not the full state of L2 because then what's the point? You post the state root of L2 on layer one. So someone actually needs to come in and say, okay, there is a message on L2, and I can prove to you that this message exists on L2. So please relay it to a contract on layer one. So you say, okay, I believe that there's this message. Ethereum tells you to prove it. You give a merkel tree proof that this message was actually included on the L2 chain.
05:27:32.656 - 05:28:22.770, Speaker A: It says, okay, have we waited long enough that we're sure that it's not fraudulent? And then as long as that proof checks out, we send that message off to a contract on layer one and everyone's happy. And to finish this off, other really cool stuff, we can do custom op codes. We have a native account abstraction, so the EOA account or externally owned accounts don't exist anymore. We can do any EIPS that we want, whenever we want, and we have lots of plans to do that. We have geohot commits and it is extremely cool. Just trust me, look at the repo and you will be very excited. So what is the status? Well, the contracts are being audited right now.
05:28:22.770 - 05:29:21.860, Speaker A: You can see the latest contracts in the Contracts v two repo. We have a testnet up and running and we're doing a lot of testing. We are hitting those integrations because we got to make sure that everything works beautifully and we're hiring. So if you are a compassionate person who loves public goods and you know how to use Ms Paint for presentations, then please come talk to us because we are absolutely hiring and absolutely looking to do more cool stuff with cool people. Great. Well, Kevin, thank you so much for that amazing talk and the lovely visuals. There's a lot of people for the hackathon that are working on integrating L2 solutions, so we'll be sure to recommend them to try out optimism and give you feedback directly.
05:29:21.860 - 05:30:30.860, Speaker A: We are at time, but what I will do is I'll just kind of ask a couple of smaller questions that maybe you can just clarify for the audience and if more questions come up on our chat and if you have time, you can just join the Live Eightonline.org page to answer them and chat directly. So you kind of talked about how the Execution Manager works and the off codes that are at least detected and prevented in some two branches to that scenario. The first one being what actually is impossible if you are trying to deploy to OVM in terms of what you can do on main net but you can't do on OVM. And then the second one being just kind of as developers think more about supporting L2s and optimism, what's kind of like a blocker for them or what should they think about differently or what do they have to do very differently than what they're used to that they should keep in mind as they consider optimism? Yeah, cool. Good questions. Well, pretty much anything you can do on layer one, you can do on optimism as well.
05:30:30.860 - 05:31:30.380, Speaker A: I can't off the top of my head think of anything you can't do. You can write your contracts in Solidity, you can basically take the contracts you have right now and just use our custom compiler and deploy it as is. And you can actually do more because we have these sort of custom opcodes and things like that that we can always add quite easily, I guess much more easily than we could on layer one. So we're going to do a lot of experimentation with potential EIPS that we want to try to implement potential new opcodes. So in a lot of ways it's sort of an expansion on what's possible on layer one and the sort of biggest shift from an application design standpoint since you can just run the same Solidity code, that doesn't really change. The biggest shift sort of comes in the message passing. If you want to have applications where part of it runs on Ethereum and part of it runs on this optimistic roll up system.
05:31:30.380 - 05:32:18.110, Speaker A: They need to talk to one another and we have contracts that sort of abstract that for you and let you really easily do that. But you kind of need to understand what's happening to some extent that there's a delay here between it doesn't immediately go to the contracts on layer one or L2 and you have to wait a little bit before those transactions go through. But really, other than that, you can kind of treat it just like you would treat your contracts on Ethereum. Awesome. That sounds like a pretty good sell because you can just directly support scalability in your project. So excited for the testnet to get a lot more usage and bugs cleared out so that it can become a reality. And want to thank you again, Calvin, for doing this awesome talk and we'll see you on the chat for any more questions.
05:32:18.110 - 05:32:48.820, Speaker A: Thanks again. Perfect. I will be happy to answer any and all questions. Wonderful. Next up, what I want to do is introduce Josh Stark from the global team and Josh will be here taking over for me and introducing our remaining talks for the day. So Josh is already here and I want to welcome Josh here to say hi to everybody on the chat and I'll hand it over to Josh. Thanks, Kartik.
05:32:48.820 - 05:33:19.932, Speaker A: Hey, everybody, thank you for joining us today. We just had a couple more talks left this afternoon and I'm really excited to introduce them. Let me just quickly share my screen as I take over here for a moment. All right. Yeah. So I'm Josh, I'm also part of the E Global team and I'll be your MC for just the next hour. So next up, we've got Michael from Pocket Network and he's going to give us a history of node infrastructure.
05:33:19.932 - 05:33:42.104, Speaker A: So Michael, please join the call and take over share your screen. And yeah, it is your stage. Awesome. Thank you, Josh. I'm going to go ahead and share my screen here. Hi, everyone. Hope everyone is doing well.
05:33:42.104 - 05:34:22.740, Speaker A: OK, great. Hopefully everyone is seeing my screen. So with that hi, everyone. My name is Michael O'Rourke. I am one of the co founders of Pocket Network. And Pocket Network is a protocol that incentivizes people to run full nodes for blockchain developers. So when a developer is building an application and needs access to the RPC, Pocket Network provides full access to that RPC in a fully decentralized manner.
05:34:22.740 - 05:35:02.956, Speaker A: For a little bit more context, when your application is using Pocket, the protocol picks five pseudo random nodes to do work for your application for an hour and then swaps out another five set of random nodes for your application. And it pulls from a global set of nodes. So that's a little bit of background on Pocket. And what I'm going to do here is give a brief history on the node infrastructure. So with that, before I do though, I'm going to give you a little bit about myself. I was born in Dominican Republic. I live in Tampa, Florida.
05:35:02.956 - 05:35:29.084, Speaker A: For those of you who know me, I shell Tampa pretty hard. Really awesome place to live. I studied international studies in college. I learned to code as an iOS developer before I graduated. And I work out of a blockchain co work space called block spaces here in Tampa. So I want to preface this talk with this is meant to be a pretty high level talk. I studied basically history in college.
05:35:29.084 - 05:36:13.652, Speaker A: It's something I'm fascinated by. And what I really want to do is give you all a context of where we are today and what the architecture looks like by starting from the very beginning. So with that I'm going to go into part one which is kind of pre internet and basically we're going to go over the first nodes. What are these regional data owners? And effectively pre Internet was very low bandwidth in this respect. So the first node was the interface message processor. This was the kind of first server funded by the US government. This was to actually protect the US in the case of nuclear catastrophe.
05:36:13.652 - 05:36:58.792, Speaker A: And the idea was, okay, what happens if we build a redundant network that if our major cities are bombed, for example, it won't go down. So that's really where this research started. This builds on the ideas of the amazing JCR Lick Lighter, who is frankly a legend and I would recommend reading anything about him, if you're curious, on seeing how kind of these ideas that Ethereum and every one of these decentralized networks is built on this thing. Basically just did packet switching, dynamic routing and some hypertext. It was very basic and the first computers were were connected in 1969. If you want to learn more about this, I really, really recommend Where the Wizards Stay Up Late. This is a really great book.
05:36:58.792 - 05:37:30.684, Speaker A: The narrative is really well done and it really tells you the history of how this barely happened actually. And it's fascinating. So that's the first node in terms of when it comes to history of node infrastructure. So the next kind of phase I want to talk about is this idea of one of the first bulletin board systems or BBS. This is one of the larger ones that occurred. This one's called FidoNet. This was created by someone named Tom Jennings.
05:37:30.684 - 05:38:25.776, Speaker A: And the nodes here are effectively the people running the servers that you would connect to over your modem or your phone. You see the CDs and the floppy disks. That's the database. That's where the data was stored. And these nodes would have massive really frankly piles or organizations of these CDs to upload on these servers so that people could dial in and see this information on these BBS's. And it's really interesting because you can only connect one person per modem at a time, which is kind of crazy thinking about it today and really? Tom Jennings designed this in a way actually he defined it as cooperative anarchy. He wanted it to be cooperative anarchy.
05:38:25.776 - 05:39:19.990, Speaker A: So the idea would be that he would provide minimal cost public access to electronic mail and some other things. It was really important for Tom Jennings that every node to be self sufficient and needing no support from other nodes to operate. And the way this worked was that there was a node list and it would contain the modem telephone number of all the nodes. And this would allow nodes to communicate with any other node without the consent or need to ask any political groups at any level. And there's a lot of kind of interesting history here in terms of governance and how this whole thing kind of came along. But a little bit about the network architecture of the Fido network. And for context, this grew to have 20,000 nodes all around the world.
05:39:19.990 - 05:40:33.000, Speaker A: Really the people who had the most power were the regional coordinators. And the list of all the nodes would be automatically updated and distributed weekly. This list would basically just have all the information of the hosts, the telephone numbers, the geographic location and everything like that, right? Again, this was cooperative anarchy. So if certain random things happened, right, they would cooperate and write some policy in terms of how to manage this network. The regional coordinators actually kind of de facto made themselves the bosses and they would be the ones who would nominate the network coordinators, the zone coordinators and the international coordinators. And for context here, the regional coordinators were like the big city ones, right? So they're the ones who had the most data of their specific location. But what I really want to highlight here is you've got a bunch of self sufficient siloed networks nonetheless dependent on these regional coordinators and these other network zone and international coordinators.
05:40:33.000 - 05:41:15.636, Speaker A: So the way I like to think about it is it was like being like a turtle. They were pretty siloed. The data moved very slowly through shareware and physical mail and they could work together, although it was kind of limited in capacity and how they would work together kind of thought kind of reminded me of a turtle. So that's the first part. Basically, kind of pre Internet things were very slow, not much data. Then the Internet happens in the 90s, BBS's die, they become ISPs. There's exponentially more data being passed around as the Internet, of course grows.
05:41:15.636 - 05:42:04.840, Speaker A: And in this case, large corporations end up owning our data for a really important, in my opinion, user experience trade off, which is a big reason why the internet exists the way it does today. And frankly, we're kind of at the edge of part two and part three right now. But a really important piece here is that this transition left a void in this kind of node hosting environment, right? Because those BBS end up turning into ISPs so as the 90s went on, the Internet started exploding. A lot of startups had to buy their own servers. This was originally known as a Slash dot problem. You'd get posted on Slashdot and your website would break. This was really expensive.
05:42:04.840 - 05:42:47.876, Speaker A: Startups would spend millions of dollars buying servers to just keep up their website, right? This was before we had any idea of sophisticated DevOps and all the amazing tooling we have today. But this really kind of helped push a really big movement in the late 90s, early 2000s, into the cloud. This is really the state of where we are today. The cloud is huge. We have really four big entities that run most of the cloud. Amazon, Microsoft, Google Cloud and IBM. There's a bunch of independent infrastructure providers as well that run their own data centers.
05:42:47.876 - 05:43:42.280, Speaker A: But those have been progressively going down or getting smaller. And the way I like to think about this is we are all basically Mr. Fantastic, I was starting to draw this mr. Or Mrs. Fantastic, I was starting to draw this out and I kind of started to see that we had really long arms because we could really reach anywhere at any time and it's really frankly incredibly powerful. I don't want to discount how amazing things are right now in this context because we are able to communicate with anyone all around the world. But there comes an important trade off here where effectively all our data is siloed amongst just a few companies, right? Some companies like Facebook might that data is their kind of edge, right? That's kind of their competitive edge, if you will.
05:43:42.280 - 05:45:18.920, Speaker A: Whereas you've kind of got these broad infrastructure providers, which is really kind of our big focus more than anything as Pocket, but they provide servers for people to run their startups. So before when startups had to spend millions of dollars to run their servers and build this infrastructure, now they can auto scale in seconds with Amazon Web Services, right? That's kind of the metaphor. I'm going there. So then part three. This is Web Three, this is owner and data, and most importantly for me and for Pocket, is shared, publicly accessible databases. This is a really important concept, particularly in the context of the way blockchain infrastructure is working today because in the Web Two world, just like Facebook and any company, while they might be running on Amazon Web Services or GCP or whatever it might be, their databases that are hosted on these servers are private, right? For most companies. Most companies, this is kind of their edge, right, and their competitive advantage and kind of how they make their secret sauce, right? An important and really important distinction in Web Three is that these are shared and publicly accessible databases, which has changed the dynamics of how this infrastructure gets hosted and served to every single blockchain developer in the world.
05:45:18.920 - 05:46:26.444, Speaker A: It doesn't matter who you are, we're starting to see these trends. So before I do, though, I want to talk about what a node is in the blockchain context, right? In web two, you have an API, you have your server and you have the database. Death is all three in one and frankly is not optimized for rapid retrieval, right, for grabbing data quickly. It's focused on being a redundant peer to peer in this peer to peer network that exists, right? And as a result, we've had lots of solutions above and below geth. When I say below, I mean like the networking layer and above indexing that has helped these sorts of things. The limitations of kind of just a raw geth server by itself, right? Or a raw geth node by itself. But I really want to kind of highlight that a geth node or an open ethereum node or whatever it might be, is really all three of these at once.
05:46:26.444 - 05:47:22.130, Speaker A: Whereas in web Two, the API and the database and the server are all really optimized for their specific tasks. And as a result, it's a lot and many times more difficult, more expensive, more slowly to do what we take for granted in Web Two world. So we've seen a lot of awesome solutions to come in and really help improve those things. So now that we kind of have a note out of the way or what kind of a node is out of the way, I want to talk about the first node I got in crypto in 2013, I managed to download Bitcoin QT for the first time. Took forever to download Bitcoin then on my local computer, bitcoin QT. And it's pretty magical, of course, the first time that you used one of these full nodes. Right, this is a nice UI on top of a Bitcoin node that you'd be running on your local.
05:47:22.130 - 05:48:55.260, Speaker A: Yeah, this is a quick thing and there's been a lot of information on Bitcoin core and what was Bitcoin QT and now is Bitcoin Core. But it's just a really important moment in history, in my opinion. And because it was so hard to run these nodes, no one really wanted to. It kind of limited the growth of our industry right early on. This is a screenshot of Coinbase, I think, in 2012 or 2013, and they had an important insight that wasn't, you know, they weren't the only ones that have it, but they're kind of of course, they're a massive reason of why we are here today. They were like, hey, Brian Armstrong was like, it's hard to run a Bitcoin node, let us run it for you so you can just access your wallet, right? This is a complete departure from kind of the ideals that a lot of us really care about in terms of sovereign ownership, owning your money and being able to verify a lot of these things, right? So this is the first steps, are the first steps to kind of the traditional. Web two, kind of web Three mashup that we're in today, right? And frankly, almost every single application is working on this context in terms of at least the infrastructure.
05:48:55.260 - 05:49:53.760, Speaker A: Bitcoin decided to go the custodial route, right? We have a ton of amazing applications that are not custodial as well, but that's an important piece in my mind. So the first Ethereum clients, I don't know if you all remember if you've used Mist or the original Parity client, but these were also magical for me, just as magical as the Bitcoin nodes. Parity came out on February eigth, 2016, so these were the first ones. And since then we've seen, which I'll show in a little bit, a nice growth and diversity in some of these nodes as well. And in 2016, defcon two inferior was born. So inferior is, in my opinion, one of the most important aspects of the Ethereum ecosystem. And I want to make sure I highlight that inferior did not lead this.
05:49:53.760 - 05:51:09.588, Speaker A: This would have happened anyway, in my opinion, because Ethereum just kept getting bigger and harder to run. And I show these graphs to show that developers, application developers on Ethereum most of the time, don't want to run their own nodes, they want to focus on building their amazing DAP, right, whatever it might be. At the same time we saw the number of nodes going down, right? I want to highlight that I think it's a little bit overblown in terms of the amount of nodes being risked, the amount of nodes being so risky that things can be censored and so on and so forth. But frankly, they've been a really important piece. And this is part of the old paradigm where DAP developers access an API with a bunch of servers controlled by ran by a centralized entity. And this growth has helped us see a massive explosion of these, right? From geth to open ethereum. Of course, Nethermind and Besu are some of the major clients and we've also seen explosion of providers as well infira Alchemy Rivet.
05:51:09.588 - 05:52:14.776, Speaker A: I'm just naming a few here pigment networks. Quicknode. I want to highlight rivet. They've done a really interesting thing where they've forked geth and have allowed for much simpler and easier scaling of your geth nodes, particularly when you're hitting high throughput or when there's an application that just is absolutely hammering your servers as a result. And again, I've just named a few here, there's a ton of them. It's actually led to a massive diversity in my opinion, which is nothing but good, in my opinion. Also, I consider these the kind of next generation of protocols that are really actively trying to decentralize this infrastructure stack, because we see a lot, I don't know if you're on crypto, Twitter, but we see all the know is your DAP really decentralized because they are using a centralized provider, right? I also do think it's a little bit overblown that the government is going to come down and shut down infira or AWS.
05:52:14.776 - 05:54:15.780, Speaker A: But that said, I think it's important to point out that we have these kind of web two infrastructures and architectures kind of placed on top of this web3 new paradigm, right? And it doesn't always exactly fit, right? So we see things like it's expensive to run Ethereum infrastructure because again, they take up a lot of data as peer to peer networks by perspecting in peer to peer networks, but you also need them to provide data to applications at scale. So I just want to highlight for me, of course Pocket, but I kind of want to show how each part of the stack these three projects are in. And if you took that node picture that I showed a few minutes ago, the graph works on the indexing layer and they are incentivizing people to run full nodes in their own way, right? Because the more traction their queries have, right, the more nodes those indexers need to run, right? But they're really focused on kind of this indexing layer and that indexing layer is super important because again, geth these full nodes are not optimized to grab this information, right? Pocket network purely focuses on the full node. We just open up the RPC for anyone to access. And when a node serves some data through the RPC API, they earn our native cryptocurrency and also think Marlin is really interesting on the networking layer. So they're doing their own incentive for full nodes, but specifically through the networking layer, which I think is really fascinating and in my opinion these are kind of three networks that are in my opinion well designed and are really doing pushing kind of this decentralization of the infrastructure stack which in my opinion is really exciting. And this, in my opinion, will lead to a new wave of web3 friendly infrastructure and architecture.
05:54:15.780 - 05:55:40.960, Speaker A: Because instead of kind of going through many layers to access some data, I believe in the future we're going to be accessing many different kinds of nodes through many different ways of paying with all of these things abstracted through all the tooling. And frankly, I do think through all this traction that all these applications on Ethereum are seeing, we're also going to see the nodes go up, I think as well, allowing for really interesting things to happen down the road, in my opinion. For me, I think right now we're at a really interesting, pivotal time because kind of got these new protocols coming out that are really making honest attempts at really decentralizing the stack and I think that's incredibly important. That said, we'll have to solve really hard problems. We don't have billions of dollars like Amazon does and there's a lot of investment and money that needs to go into networking bandwidth and hardware, in my opinion. For me this takes time, a lot of research and frankly a lot of investment and money for open source protocols like the ones I just mentioned to reach kind of the similar economies of scale as previous ones. That said, we are able to tie in these kind of crypto economic incentives really tightly in our protocols, which in my opinion gives us an advantage at least in the short to medium term.
05:55:40.960 - 05:56:35.010, Speaker A: And I think this is changing the narrative because I think this is a really interesting quote that I've always believed, but in my opinion, software has always implied trust in those hosting the hardware. I think networks like ours are a reversal in this trend, providing new reasons for everyday people to run more hardware for themselves, in my opinion, because we're starting to see all these networks that are frankly incentivizing you to run nodes in infrastructure. I'm just amazed at all the people in Pocket who are now becoming DevOps pros who before had no idea about any of this stuff. Right. I just think these incentives are just starting to get baked in. I think it's going to be an incredibly powerful trend. So yeah, that's it.
05:56:35.010 - 05:57:09.980, Speaker A: Thank you. Thank you for having me. Really happy to be sponsoring Global. If you guys have any questions about how Pocket works, please reach out to us in the channel. We'll be more than happy to answer any questions and point in the right way. Great, thanks Michael, that was a great presentation. If you have just a couple minutes, just a couple questions to run, I'll just I'll stop your screen sharing if that's okay too.
05:57:09.980 - 05:58:11.330, Speaker A: Or maybe Andrew can do that behind the scenes. So one question, I mean, you mentioned it seems like a main tension here is well, in the long term, will all of this end up back at centralized infrastructure? Right. There are often efficiencies of scale, whether that's just the expertise to run the nodes or whatever it happens to be, whether it's proof of work, proof stake or something else. How can we avoid that future, I guess? You mentioned at the end here that the opportunity to bake in crypto economic incentives into these networks gives us a powerful toolkit. But I guess tell us more about what are the different levers that we can push to try and create the incentives to avoid just ending up with another set of centralized infrastructure in web3? Yeah, I think the number one thing is tooling, meaning I can do a one click deployment at a local data center, right. And manage my validator node, for example. Right.
05:58:11.330 - 05:58:56.510, Speaker A: I think for me that's probably the biggest inhibitor when it's at least pure proof of stake network. I think one of a really important, actually, line for me is having non delegation, right? Because delegation is easy when you have that hard line right there. People are actually much more incentivized to build this tooling to make it easier for people to run these nodes. And I think that's an important aspect. But for me, what's interesting about these open source networks like the Graph Pocket and Marlin is that through collective efforts and kind of building on these infra Legos, if you will, it will only get better. Right. And I think that's how we kind of avoid that future.
05:58:56.510 - 05:59:28.250, Speaker A: Got it. And one more question. You mentioned at the end. There are hard problems open in this space for anyone watching today who is looking for a hard problem to work on, big or small. What would you suggest is the next hard problem that we have to tackle as an ecosystem? Yeah. Specifically in the infrastructure space, I'd say coordination amongst these turtles, if you will. Right.
05:59:28.250 - 06:00:09.296, Speaker A: So how do these turtles speak to each other quickly, efficiently, and easily? If I'm AWS, I've got infrastructure all around the world and I know exactly where everything is going. Right. I think understanding how these various infrastructure networks play together and interact, I think is going to be increasingly a more difficult problem, especially as they get bigger and it gets more unwieldy. People have different setups and with different bandwidths in different parts of the world. When it's not one person who owns all of it, coordinating all that is going to get increasingly more difficult. Okay. Well, thanks, Michael.
06:00:09.296 - 06:00:40.232, Speaker A: That's all the time we got. We're going to move on next presentation, but thank you so much and I hope everyone watching. If you have any more questions for Michael, you can find Pocket Network in the E Global Discord and of course, you can ask questions in the chat as well. Okay, thank you. Thanks, Michael. All right, so next up, the last talk of the day. I'd like to welcome Dietrich from protocol labs.
06:00:40.232 - 06:01:09.530, Speaker A: Dietrich is going to talk about participatory infrastructure with IPFS. We're very glad to have him here to give a great kind of last talk of the day and wrap it up. So, Dietrich, I'll pass it over to you and let you take it away here. Hello, everybody. Thanks for the intro and thanks for having me here. It sounds like a long day of interesting topics. I cut the last bit of Michael's there.
06:01:09.530 - 06:01:56.016, Speaker A: It's great to be here talking to Ethereum folks once again. So real quick. I'm dietrich ayala I work on that BFS project at actually first I used to work at Mozilla for many, many years. And while I was there, I worked on a project called Lib DWeb, trying to bring some of these technologies and make them enabled at the very edge in the user agent itself, inside browsers and in that library project. I ended up going to Ethereum DevCon Four in Prague, I think it was, and had a talk there. And it was really great to be able to get introduced to some of the folks in the community and meet some of the projects that really get a sense of the values, intent, and excitement of all you all. So now I've been at Protocol Labs for over a year and a half now working on IPFS.
06:01:56.016 - 06:02:44.212, Speaker A: And I'm going to talk to you today. They said there's a mixed audience and people new to the space and people not new to the space. So a quick introduction to IPFS, the problems that we're trying to solve, a bit about the side effects that I see that these types of architectures and infrastructures have on user agency and what it means when we make these types of infrastructure changes. But first, a real quick run through IPFS, the web today. One of the reasons why a lot of us are here and working on these types of projects is because there are a bunch of problems from centralization, which has a bunch of negative side effects on our ability, our user agency on the Internet, how we interact with each other as individuals and as organizations. Some of the fragility and resiliency of the web. Ultimately, we don't end up owning our own experiences so much.
06:02:44.212 - 06:03:20.368, Speaker A: Some of the problems that we're trying to solve are around access from various different places. Might not have the same type of access that we have in Western countries. I'm from the US. Censorship, which is a major problem in many countries. And I've spent some time living around in different parts of the world where it's more or less of a problem that you really do feel on a regular basis. Huge inefficiencies in network access, security models and trust models. That don't necessarily match the use cases that we have developed 30 years ago or plus years ago, organically grown and now not necessarily being growing and adapting to the use cases that we have today.
06:03:20.368 - 06:04:31.896, Speaker A: And then things that don't work offline or even in disconnected networks, like for example, doing this talk, and if I was doing it from a Google deck and that deck went down, or that service went down, everybody following along wouldn't be able to do so. There's no technical reasons why we can't have this type of local network collaboration offline collaboration, but I'm really interested in seeing those capabilities develop. Things like Internet shutoffs around Egypt, Myanmar, and parts of India right now, having really severe access issues. And then censorship, of course, one of the biggest groups of people using IPFS right now, well, in the last couple of years were people trying to access Wikipedia in Turkey. I went to Istanbul, and you can see there are stencils of Wikipedia pages on the streets there, which was kind of fascinating and really shows how much of a public issue it was. And then we have a really big group of users in China Matters News, using IPFS as the last mile solution to make sure that they have access to external news sources, information. And I lived in Thailand for quite some time where you'd regularly see Wikipedia pages that just give you the royal seal of the censorship bureau of that country there.
06:04:31.896 - 06:05:35.016, Speaker A: So it's a real thing that does affect a lot of people. So enter IPFS project and protocol that's used by a lot of programs and projects in the Ethereum community because of its immutable addressing and the storage flexibility that it provides. Some of the hallmarks of this protocol addressing things in the web today is done by location where something is, which server does it reside on, and which ultimately path on that server oftentimes. Whereas IPFS addresses things by the unique signature of the contents of the data that you're asking for. Cryptographic verifiability built into the protocol through that address. When that address is unique, signature of the data that you're requesting is very difficult or not impossible to fake that something is what it is and that's built into the way that you request something itself. So if you trust the source that you got the URL from or the address from, then you can trust that that data will and verify that that data will not be modified in transport.
06:05:35.016 - 06:06:26.280, Speaker A: And then transport agnostic peer to pair networking. So built on top of Lib PDP, although you can swap out other transport libraries, but Lib PDP providing an abstraction layer across whatever transports and common transports like TCP, UDP, et cetera. But also some groups that we're working with who've done things like develop a Lib PDP transport for things like COAP or bluetooth, other ones. And one of the most important ones that differentiates IPFS from the web of today is the idea of a contextual trust model. So trust model that's not enforced necessarily by tied between this intersection of SSL and DNS. Talking really quickly if you're new to these types of architectural network architectures. HP today working requesting from the web browser or native application or whatever the client is to a centralized server.
06:06:26.280 - 06:07:15.230, Speaker A: Whereas IP fest this data is being transferred between the different peers that are participating in the network at any given time. And that number of peers can be determined by and specific to the application that's being used at the time. So the resilience and robustness network travels along with the popularity and the user base itself. The unique Identifiers the way that we create addresses, you may be familiar with URLs where you have a domain name followed by a file path preceded by a scheme. In IPFS, we hash the data of the file that you want to add, say cat PNG, and we get the unique signature of that set of bits. Create a content Identifier that you can use to be able to address or request that data. If the data changes, thus does the unique identifier, so the address itself changes.
06:07:15.230 - 06:07:48.464, Speaker A: These are called CIDs when transferring data. So we take that cat picture and we split it up into a whole bunch of logical bits. And those bits can then be shared by many different participants in the network, as opposed to being only requested and received by one. The location based addressing so today we have request something by where it resides. Through DNS you find out the IP address of where this data that you're looking for resides. That request goes to a remote server. It resolves eventually to a machine that responds with those bytes.
06:07:48.464 - 06:08:44.824, Speaker A: With IPFS, when you're requesting something by its content, a whole bunch of different peers in the network might actually have that data who are not the publisher. But because of the cryptographic, verifiability means that you're okay receiving that data from those other participants in the network, even though they are not the originators of that data. And ultimately even if that publisher goes offline, some of that data might still be available. Some of those other participants who want to continue or getting utility out of whatever the application or service or data was and are keeping it online. So what does this have to do with infrastructure user agency? This infrastructure at a pretty high level. But one of the things that you see here is that there's a direct relationship between the person requesting the data and the person, entity or project that's serving that data. But come on, let's be honest, it actually looks like in the centralized web a whole lot more like this where you're requesting this data from an entity or project.
06:08:44.824 - 06:09:32.676, Speaker A: And the more that the number of requests go up, the more number of users that you actually have in your project. The infrastructure complexity increases and increases and increases. And this has some interesting side effects raising the barrier to entry for innovators, for people who want to enter new markets because they will have to be able to understand what the scale is going to be. They can make projections on that and build up a relevant amount of infrastructure themselves to be able to support that with side effect. Then again, that there are few people as needs, user needs increase for a given type of service. There are fewer people who are able to actually provide that service because of the overhead required to actually be able to get into the game. Which means there's less competition, which then means there is less innovation, a negative feedback loop of a vicious nature.
06:09:32.676 - 06:10:43.712, Speaker A: But one of the other parts of this dynamic means that the power is in the hands ultimately of the publisher. With web Two, it's a control point over the people that are using a service. For example, if that publisher goes out of business and the service goes offline, the users have no choice but to migrate to something else. Or if it gets bought by a bigger fish, you are now a user of maybe a company that you don't agree with, or you don't like their service, or what they're going to do to the service that you previously liked. Aside from all the types of problems that I mentioned earlier which are still subject to ultimately the publishers hold all the control in Web Two they're the source of often the business logic where your data is stored, which opens you up to other problems such as surveillance and tracking that maybe you didn't realize you opted into. And they also have the burden of that infrastructure, but ultimately they pretty much have 100% of the decision making in applications today. In the centralized web, users have only one choice, which is to leave to go use a different service.
06:10:43.712 - 06:11:28.780, Speaker A: If one exists, very little choice over how their data is treated, the ultimate longevity of the service or product. They may have their own business or livelihood it may be dependent upon. With IPFS, aside from the type of things that I talked about before, it does change the nature of this to some extent. So you are, as I said, getting this data from other sources other than the original publisher, and that publisher goes away. That data can still be there. The infrastructure that runs the application can still be there. The lifetime of these applications end up being more dependent on the user than on the publisher.
06:11:28.780 - 06:13:04.140, Speaker A: In this type of model, a lot of the app logic is getting pushed out to the edges more, out to the clients more, and we start to see these layers collapse. As Michael was talking about the last talk a little bit, that as these layers start to mature, they also start to condense a little bit, they get more and more abstracted away and more and more of this ends up being in the hands of end users. The infrastructure both from deployment and delivery ends up being more and more user participatory as solely being executed on the devices of the publisher. This is a change in how we think about things like value capture, how we think about what we invest in, and how we think about what the ultimate impact of our business efforts are on the behalf of our users. We're seeing more cooperative computing come into play as web3 layers start to mature between things. Like one of the things I'm looking at right now is with the advent of IPFS and then Filecoin suite to launch how we create cooperative organizations like DAOs around ensuring that data, valuable data and services can stay online. So where's IPFS at today? A quick run through the IPFS stack is composed of a number of different pieces from the definition of data to name resolution and finding and discovery of data, the addressing of data, and then the network infrastructure to be able to support the delivery of this data and these applications between peers.
06:13:04.140 - 06:13:38.408, Speaker A: And that's what Libbdp does. An IPFS node running node does a number of different things. If you're running a node right now, typically this is a node that runs on your computer or on servers that you're running on behalf of other people. Either a pinning service or you're running it in order to keep the data for your own application and some users online or keep it persisted anyway. Right now in a running node, our default peer count is 900 peer connections at a given time. This is a transient thing, depending on the use case. You can turn that up or down.
06:13:38.408 - 06:14:34.680, Speaker A: Obviously, on mobile devices, keeping 900 persistent TCP connections is not going to be a viable thing to do from a device constraints, bandwidth usage, or battery lifetime. So we turn those kinds of things down. We're seeing a lot of different applications. Experiment with the boundaries between what does Ipfest lite client kind of mean? How do you run as native as possible, a node inside of a mobile app or inside of a web page as well? Nodes can vary a lot depending on how much of the IPFS protocol and toolkit that they have implemented. We have full nodes. Our primary implementation is in Goipfest, which has a full direct connection to the DHD, to the peer to peer network. But we also have JavaScript implementations and a lot of different HTP clients and partial implementations in a number of different languages like Rust and Python and others that various parts of the community are supporting.
06:14:34.680 - 06:15:53.616, Speaker A: The node deals with a few different things. It does things like manage keys for you, manage your communication between different peers, doing things like managing the peer book so identifying which peers better or worse, connectivity with or use often or less often, syncing data between those peers and then supporting the local apps that are running. So in the case of DApps being a node that does the syncing and requesting, pulling, fetching and publishing of the data that the DAP is interacting with, the easy way to get started downloading IPFS desktop this is a desktop electrode application that spins up a Go IPFS node. Gives you a little bit of an easy way to be able to start, stop and see the status of the IPFS node. The IPFS CLI is available in a bunch of different flavors in a bunch of different places, pretty easy to install and get started at this point. The CLI has a number of different commands that can give you a way to explore what the various components the tools in this toolbox from a protocol standpoint offer you as a developer for how to integrate with your applications and be able to understand what type of communication is happening and running on a regular basis. We run HTP gateways, so we'd be able to allow HTP applications to be able to access data, publish data on the IPFS PDP network.
06:15:53.616 - 06:16:43.996, Speaker A: But also some companies like Cloudflare run a large gateway as well. And we have a public gateway checker page where you can see and register a gateway that you're running. Obviously your mileage may vary, so we do post things like capabilities, level of security, verifiability, and also performance for some of these gateways. But anytime you're not running that node yourself, you are delegating that trust to that other organization or other node. IPFS Companion is a browser extension that allows you ease of access from web applications to your local node that you're running. Something that I didn't add any slides for here, but we do have a couple of different interesting things with regards to browsers. In Opera on Android, you can actually use Ipfest pretty much directly.
06:16:43.996 - 06:17:37.190, Speaker A: It talks to an HTP gateway, but you can actually use the IP Fest address there. And in Brave, if you install Brave nightly browser right now and you go to About Flags, you can enable native IPFS support, which works kind of like their Tor support does. So you enable IPFS, you encounter an IPFS address and asks you, would you like to run a local node? And it'll download goipfs and run and manage that node locally for you directly from within the browser. So this is something that we're hoping to ship with them in a release version of Brave sometime later this year. So pretty exciting. JavaScript IP Fest JS IP Fest is one of the ways that you can easily access IPFS and use it in your web applications or node applications. So this doesn't have a full connection to the DHT, but does have a lot of ways to connect to it and to be able to connect to and manage your peers that your applications connect to.
06:17:37.190 - 06:18:26.596, Speaker A: One of the things that we've done recently as applications are starting to use IPFS more and more, especially in Web applications, if you open up two different tabs and each one uses JS IPFS right now, it would by default. Create a IPS fest node inside each one of those tabs, which isn't really sustainable if you have an application where your users open up multiple tabs. So we have a way to be able to use shared workers now so that you can have one Jsip Fest node running in the browser that's shared across multiple tabs of the same origin. We're working on making it work across origin as well. So you basically have a single IPFS service running inside the browser at the JavaScript level that different web pages can share. So that should make things both more performant and less resource intensive. As the ecosystem has grown and adoption of IPFS has grown, there are a number of different toolkits and frameworks you might have seen.
06:18:26.596 - 06:19:22.496, Speaker A: Some of these three box is built on top of IPFS, and it provides things like data storage, identity abstractions. Textile is a set of developer tools on top of IPFS that offer a couple of different ways to be able to utilize higher application level patterns instead of interacting directly at the lower level. Protocol and then Orbitb is a database abstraction layer over IPFS that allows you to work with data more dynamically and also have access to abstractions like appendall, need log and things like that. We've heard a little bit about infira today. Obviously one of. The biggest services, service providers in this space and a group that we work really closely with to be able to make sure that they have the latest versions of IPFS and that we're meeting the needs of their users as well. Pinata and Temporal are other services that provide IPFS pinning.
06:19:22.496 - 06:20:34.456, Speaker A: They will keep your data up and available as well as a number of other different things, providing management tools and different debugging and data archiving tools as well. One thing that we've seen start to emerge is more and more. Again, like Michael said in the last talk, one of the missing pieces are around CI automation and tooling. So how do you make it as easy as possible to be able to deploy our Web Three applications? How do we make it as easy as possible to be able to manage them and to be able to get the same benefits that we get from a much more mature Web Two infrastructure? And Fleek is one of the companies that we've been working a bunch with this year who's really done this and made it easy to be able to hook into your existing CI and CD using existing toolkits and frameworks supporting those as well as a number of different source code management systems as well. Name resolution is again, like as you saw earlier, the Ipfest CID is not nearly as user friendly to look at as a regular textual name in a language that we understand natively. However, there are a bunch of ways to approach this. We have IPS is the Interplanery naming system, which is a way to be able to map a key to a IPFS CID.
06:20:34.456 - 06:22:12.492, Speaker A: And then services like Unstoppable Domains and ENS are innovating in a bunch of different ways, providing different services that allow you to resolve human readable names to a IPFS CID and to be able to manage the dynamic updating of that content. And companies like this are also integrating in a lot of ways some of the ENS and unsolvable services to make it easier for developers and businesses to be able to ship. So if you're interested in learning more, that was a really fast tour through some introductory IPFS stuff, how IPFS works, some of the benefits, why it matters from a larger user agency perspective, one of the things that these transitioning architectures and changing the paradigm to a more distributed and decentralized architecture, how that actually benefits end users in a way that changes both how we do business and both the longevity and availability of the services that we want to provide actual people. So you can follow along on Twitter IPFS our forums are a fantastic place to ask questions if you are building your Web Three application. We monitor the forums pretty closely now to be able to ensure that we have a really fast response. We have folks that are dedicated to answering your questions there. We also have automated monitoring of the IPFS tag on stack overflow as well to be able to make sure that we get back to folks really soon and try to close feedback loops so that if you're building stuff on top of IPFS, and you have questions or you've gone off the rails or you have a problem and you want to be able to figure out how to move forward, that we can get help to you as soon as possible.
06:22:12.492 - 06:22:28.636, Speaker A: So thanks, everybody. Thanks you all for having me and letting me speak a little. At the end of your event, if you have questions, definitely feel free to reach out. Thanks again. Great. Thanks, Deirdre. We actually had a couple of questions that came in from Chat.
06:22:28.636 - 06:24:30.740, Speaker A: If you just got a second, I can do them right here. Yeah, sure. Audio difficulty here with Josh. Just hold on one moment. Yeah, hi. Josh had some technical difficulties. His resilience of the Internet, his laptop had some trouble.
06:24:30.740 - 06:25:26.070, Speaker A: So we're going to see if we can take a final question here. A couple of questions that came in. Stand by. One momentum. Just going to see if we can and thanks for your patience. Yeah, well, it doesn't look like the questions oh, here we go. Sorry about that.
06:25:26.070 - 06:26:08.210, Speaker A: Usually it's me with the robot voice. Okay. Yeah, it doesn't look like we have any questions that I can access at the moment. Continued technical difficulties. So at this point, Dietrich, I just want to thank you for an awesome presentation. This will basically close out the summit for the day. Thank you very much.
06:26:08.210 - 06:26:42.740, Speaker A: Thanks, everyone, for joining. ETH online will continue for several more weeks, and we have another summit coming up next Friday. In the meantime, the hackathon will continue, and we just want to thank everyone for a great event, and we're signing out. Thank you.
