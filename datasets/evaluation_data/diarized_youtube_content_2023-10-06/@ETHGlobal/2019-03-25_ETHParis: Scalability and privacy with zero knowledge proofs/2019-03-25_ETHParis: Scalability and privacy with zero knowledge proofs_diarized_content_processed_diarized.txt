00:00:06.700 - 00:00:43.452, Speaker A: Okay, so I think we will start. Hi everybody. My name is Alex, I'm also from Adelabs, just as another Alex. And I'm going to talk about scaling about with zero knowledge proofs in general, it's going to be a little more high level. Hopefully you will have a better idea what we can do now and answer the question which was asked before what are the killer features? So I want to pre start with the question whether scaling actually matters or why it matters. Many people say that we just need to focus on usability and user adoption. I would argue it's not the case.
00:00:43.452 - 00:01:46.876, Speaker A: You have to think about scaling because if you become successful you're going to face the scaling challenge and you only want to plan for the success case because if you're not planning for success, then what's the point of making the app in the first place? An app or service or whatever you're building. So any successful project will have to scale and it might be too late. But of course it's important to have usability and it's important to be able to develop applications fast. And it's also important, since we're in crypto, to also take into account scalability and security and decentralization which is known as the infamous blockchain trilemma. It's really difficult to fix one of the problems without harming the others. So we have multiple scaling approaches existing today. We can go for centralization with just a few validators which is maybe useful for some applications, but it defeats the very purpose of crypto.
00:01:46.876 - 00:02:53.476, Speaker A: We have some layer one approaches, POS plus sharding, which will still take some time to develop into full featured approaches which are secure, but they are difficult because they make the compromises on security. Then we have some layer two approaches like payment state channels, plasma which can be used today. But we'll come back to them and I'm going to talk about zero knowledge proofs as scaling. So how many of you know what zero knowledge proofs are and how you can use them? Okay, not so many people. So I'm going to go into a little more details. Zero knowledge proofs allow you to prove that you know some secret input to a publicly known function such that the function evaluates to public output, which is known to everybody. For us, we want to scale and there are only two techniques which we can actually use for scaling because of sanctus part.
00:02:53.476 - 00:03:36.692, Speaker A: These are Snarks and Starks which are pretty much the same from the outer perspective if you look at them as a black box. But they differ in the implementation. Most important difference is that Starks are transparent and Snarks are not transparent. They require a trusted setup. We'll talk about this now, but these are the scaling characteristics of both approaches. As you can see, Snarks are better asymptotically in all three dimensions. They are faster to prove, they are much faster to verify and they have way smaller proof size.
00:03:36.692 - 00:04:52.620, Speaker A: So they're actually better in any dimension. And it's especially important for public blockchains because we always have some limitation in blocks, be it block size or the guest limit. And with Starks, with close to 100 KB proof size, it's really difficult to fit them into blockchain and to make it efficient in contrast to Snarks. So the reason why Snarks were not as widely used in the past was the Trusted Setup. It's a one time operation which is required before you start using the proofs in production, where whoever provides the Trusted Setup promises to delete some initial entropy, some information they use to generate the setup, which we refer to as toxic waste. Because if he retains toxic waste, he will be able to fake all the proofs. Now of course, if we only have one person who provides the setup, it's little difficult to believe that this person has deleted the toxic waste because there is also no way to prove that you deleted it, you just promise it and there is no theoretical way to prove it.
00:04:52.620 - 00:06:08.016, Speaker A: That's why what is being done in practice is a multiparty ceremony where we have N participants, each of them contribute some randomness and all of them say we deleted the randomness and if at least one of them is honest, then the entire ceremony is correct. If all of them collude together and put together toxic waste, then they together can produce fake proofs which if you have a lot of participants which are wide known in the community, would not be a big problem because it's really hard to collude, it's hard to cooperate even on normal matters, not on dark matters like this. However, the biggest problem was with Snark, specifically with the constructs which we had before Satchet Crowd 16 is that we need to do the Trusted Setup for each circuit and by circuit we mean the actual program, the function which we evaluate. That means every time we have a new application we need a new Trusted Setup. And every time we do an update of this application we also need a new Trusted Setup. And it's really difficult. And it's also difficult to attract people from broader community to participate just for your application because the Trusted Setup is hard.
00:06:08.016 - 00:06:54.560, Speaker A: You have to organize security measures, you have to do a lot of computations, it's expensive, it's time consuming and so on. And this is the reason Snarks are only used for now in production in Zcash with two Trusted Setup ceremonies haven't taken place. But this year we have a new hope, which Alexander was talking just before me about. It's called sonics. It has the same asymptotic characteristics of scaling, it's also constant size proof and constant verification time. However, it's once and for all universal Trusted Setups for all circuits and all applications. And we also have implementation under development which we're going to publish soon, hopefully benchmarks in Berlin.
00:06:54.560 - 00:07:35.448, Speaker A: So this practically solves the problem. So now we can have really credible organizations and individuals participating, which everybody knows, and this is going to be a solution. How exactly can we use zero knowledge proofs? I'm going to talk about roll up, which is a construct of a side chain secured with zero knowledge proof secured with Snarks. Roll up is a difficult name. We used to call it like plasma on Snarks. But plasma people were not happy about this. So we were brainstorming recently and tried to come up with a better naming.
00:07:35.448 - 00:08:14.700, Speaker A: I proposed RNP, which is like RNP is not plasma. But it was declined. So I don't know what we're going to use, but for now, we use roll up. So how roll up works, let's recap how plasma works in the first place. So plasma is a side chain. How many of you are familiar with plasma? Okay, so, well, plasma is a side chain where we have a main chain with blocks going in sequence in time, and then we want to scale. So we create a off main chain, another blockchain, with each block being committed on the main chain.
00:08:14.700 - 00:09:12.092, Speaker A: In the main chain, we store just a short cryptographic commitment merkel hash of the block of the side chain. And on the side chain, let's say we have a merkle tree representing the state. So we have leaves of this merkle tree are the accounts with our balances, and we have the root hash is the one we commit on the side chain. So how plasma works, the blocks are being generated by an operator. They include transactions in these blocks. They update the state, post the state on the main chain. And you as a user have to monitor these blocks and see whether your transactions were correctly included, and more importantly, that no transactions have been included that modify the state in a non permitted way that somebody just took your money and assigned to their account.
00:09:12.092 - 00:10:13.700, Speaker A: So you have to be online and you have to monitor. And if you see the fraud, you have to object and put a transaction on the main chain saying, hey, this block is not correct and here is the proof because I did not send this transaction and this transaction takes it to my account. So it requires all users to be online, which we call liveness assumption. And it requires a lot of communication. Yeah, what's that? Oh, I see. Okay, 1 second, I'm sorry for this. It's gonna go here.
00:10:13.700 - 00:11:11.860, Speaker A: Okay. It's gonna be faster. So know. So yes, we have these blocks, we have monitoring liveness assumption. Everybody has to check for frauds. So with zero knowledge proofs, we just replace this entire monitoring part with each block must be verified on the main chain. So the operator, when they produce a new block, they also obligated to produce a zero knowledge proof that this new block is correct, that every state like this state transition was made by applying a sequence of transactions each of which is correct.
00:11:11.860 - 00:12:05.620, Speaker A: They have a valid signature, they have valid from balance to balance the amount was transferred is less than the original balance and so on. So the state transition is then this function here the state transition is the function which we prove with zero knowledge. That means we don't need to monitor the chain anymore. We completely rely on the main chain for securing this because the Smart contract is going to check and we have roughly the same security guarantees as the main chain which is pretty awesome. Now, we have a few challenges with this approach. One challenge is that zero knowledge proof generation snark proof generation is still relatively small slow at the moment it takes somewhere between one to 20 minutes. So we need to come up with instant block.
00:12:05.620 - 00:13:25.584, Speaker A: Instant confirmation for transactions is something we have to do workarounds for. But how do we achieve throughput? If we have these blocks and we have to wait 20 minutes till the next block we won't be able even if we fit 1000 transactions in the block and it's still going to be some limit because the larger the block, the longer the proven time. How do we achieve throughput? We do it with roll up by Cascading transactions. So we make commitments in one timeline and then once we made the commitment we start generating the proof and we keep making new commitments on the main chain and then the proofs follow after these commitments. So this way we can cascade and we can just like a conveyor we can fit much more transactions because blocks just follow. The second big challenge which we have with this approach is data availability because we solve the correctness part through application of zero knowledge proofs. However, if an operator submits a new state which is valid which has been correctly transitioned but they don't publish the data for this state then nobody can reconstruct the state, nobody knows we know the Merkel hash but nobody knows what account data is there and they cannot prove that they own some money.
00:13:25.584 - 00:13:52.904, Speaker A: And then if operator stops cooperating, nobody can withdraw money. So we want to avoid this. This is a hard problem. It's all scaling approaches like Sharding faces the same thing plasma faces the same problem in Roll Up we solve it with putting the data on chain. It's a very simple approach. For each transaction, we put the public data. We don't put the entire transaction data.
00:13:52.904 - 00:14:36.996, Speaker A: We don't need to put signature on chain and we don't need to use storage, which is very important because it's much cheaper to just put some data on, like call data, transaction input. And we only need nine bytes, which is 500 gas as opposed to 21,000 gas minimum transaction fee in Ethereum. However, this unfortunately limits us for now to up to 500 transactions per second. Just because of this, if we didn't have data availability issues we could go thousands of transactions per second. This is the current limitation. So let's come back to the scaling approaches and then let's see how they compare to each other. So we have payment channels and state channels and plasma both require liveness.
00:14:36.996 - 00:15:16.230, Speaker A: You have to be online, you have to watch every transaction. As long as your funds are in the side chain, you cannot sleep calmly. You have to be on the watch. The roll up doesn't need this. This laminess assumption is closely tied to security because if you failed to monitor the fraud, you're going to be screwed. And if you discover the fraud but you cannot put your transaction through on the main chain because the chain is congested or because you're being specifically censored by miners, then you're going to lose your money. Plasma is slightly better but it's comparable in this regard.
00:15:16.230 - 00:16:09.552, Speaker A: For roll up, the security is roughly the same as the main chain except for the trusted setup ceremony which again is solvable if we have a lot of participants. Now, a very important thing is capital efficiency. State payment channels approaches used in Lightning and Raiden and similar technologies require that you open this payment channel for every participant and you can only receive as much money as the size of the currently open state channel which means you have to park a lot of money in there. You have to make a lot of capital bound just to be able to operate the system. So this is pretty inefficient in terms of capital. Plasma is way better. However, unfortunately current contracts of plasma require that all tokens are non fungible.
00:16:09.552 - 00:16:52.052, Speaker A: And if you want fungible value transfers then you have something like coins which you have to exchange. So there is some slight capital inefficiency with roll up it's completely absent. You can have accounts with arbitrary amounts of of value in them. So capital efficiency is again here. And as far as throughput is concerned, payment channels are supposed to be theoretically indefinitely scalable because it's completely off chain. However, because of the capital bound requirement they are practically bound to the amount of capital. So if you have one ether or like $100 payment channel open and you receive 100 transactions then it's it you have to reopen it again.
00:16:52.052 - 00:17:37.170, Speaker A: So it's difficult to quantify but it's very, very inefficient. With plasma it's much better. You have bandwidth requirement but it still can handle up to thousands transactions per second, which is really cool. But the Usability is difficult with roll up as we seen before, because of data availability we can only do 500 transactions per second. Is it bad? Well, we have to compare it to something, right? If PayPal handles on average 160 transactions. So until we reach these levels of throughput it's going to be a good problem to have by that. But for now, if we even have millions of users, it's going to be more than sufficient for token transfers for payments and things like this.
00:17:37.170 - 00:17:51.750, Speaker A: Yes. So let's recap. Scaling is important. You want to plan for scaling and you want to understand which approaches are there. All of them have merits. You want to see what's best for your application. Zero knowledge proofs are really interesting.
00:17:51.750 - 00:18:16.700, Speaker A: I encourage you to watch closely. Developments in Snarks, Starks Sonics especially. We're going to see a lot of cool stuff coming out this year. And roll up can be used today. And actually, if we have time let me see. Yeah, we have some more time. So I can show you a live demo of how it works on matter testnet.
00:18:16.700 - 00:18:50.932, Speaker A: Can just log in with my MetaMask. It creates a new account. It creates a new key pair for me. We use a different signature type with different curve for this. And now I have my main chain account where I have a balance of 1.5 Ether on ringbin testnet. And I have my testnet account which is empty, or rather nonexistent.
00:18:50.932 - 00:19:27.820, Speaker A: I don't have yet anything here. So let's deposit some money. So how it works, you have to send money to a smart contract. I'm going to do this. Let's send 0.1 ether. I send a transaction so it goes to this contract confirm, let's wait for transaction to be mined and then the account, then what happens? The operator will notice that there is a transaction on the smart contract and they will start generating proof for this transaction.
00:19:27.820 - 00:20:09.892, Speaker A: In practice, in production, we will have to wait until we have a lot of transactions to batch them all together. But in the testnet, we just wait for one. So let's see what's the state of a transaction still pending. Okay, it's confirmed and we immediately have balance, which is verified because we already computed the proof and submitted it to the ringpay testnet. And I have this money here, so now I can transact, I can send this money inside the testnet to anybody. So we have this nice test account which is eager to receive my cash. And I'm going to send this amount and I make a submit transaction and it's immediately submitted.
00:20:09.892 - 00:20:49.632, Speaker A: And I see a pending balance has decreased and non has increased. Because the state is live, it's monitor constantly. I can make more of these transactions. I'm just going to make eight, which is the number of transactions in the block. And once I have eight, the server will start generating the proof for the block. So we assume that we're going to have much more transactions in real life, or if we don't have too much, we can always accelerate them. So user experience for end users is going to be you make a transaction, you get a confirmation.
00:20:49.632 - 00:21:26.850, Speaker A: You can either trust operator or you have some confirmation that transaction is going to be included. Or you just wait a few minutes till the block is verified on the main chain and then it's final. I mean, of course as final as the main chain itself because we don't have finality in Ethereum. But assuming there is going to be no reorg, it's final. And you can just forget, go offline it's safe. And then anytime, you can come back and return. And then anytime, even without operator, you can just go and make a withdraw, make a partial withdrawal or make a full exit and just close it completely.
00:21:26.850 - 00:22:03.660, Speaker A: Which is going to be on chain transaction. And I'm just going to withdraw all my money from the side chain. Yeah, we will wait for while we're waiting, I think I'm done. So I'm happy to answer any questions. This is the microphone so that people can hear us who are watching live. You just have to speak closely.
00:22:04.880 - 00:22:06.080, Speaker B: Can you hear me?
00:22:06.230 - 00:22:06.930, Speaker A: Yes.
00:22:07.620 - 00:22:23.412, Speaker B: You said that you need to put some data from the transaction to reconstruct initial transaction. In the main chain it was three bytes from three bytes to and in total it's nine bytes you have to store in call data.
00:22:23.466 - 00:22:26.400, Speaker A: Right. It's from to amount and fee.
00:22:26.480 - 00:22:29.620, Speaker B: How did you encode from address in three bytes?
00:22:30.220 - 00:23:06.766, Speaker A: We just have an assumption of two to the power of 24 accounts, which is 16 million accounts going to be sufficient? Yes. So when you register, you get a new slot and it's getting reserved for you. You put your public key in this slot and we have a maximum amount of users of 16 million who can register. And once you register, you have this three byte Identifier for your account.
00:23:06.868 - 00:23:10.314, Speaker B: So you have a special Identifier for that side chain?
00:23:10.442 - 00:23:24.340, Speaker A: Yes. You will have well, as you can see, when I submitted the transaction, I used Ethereum address. But it's because we have a map in the Smart contract which maps to your sidechain ID.
00:23:24.710 - 00:23:25.860, Speaker B: Okay, thanks.
00:23:33.710 - 00:23:59.598, Speaker C: Actually, I'm interested if you can explain about the signatures. You mentioned that you use a particular curve. Because I thought that it's hard to do ECDSA verification in Snark. So your Snark doesn't include the actual verification of the signature, the ECDSA signature, whatever might be using another signature on the transaction. Who is verifying signatures?
00:23:59.694 - 00:24:10.710, Speaker A: They are actually being verified in Snark. It's not hard to verify them in Snark. You just need the curve which is embeddable. So we have Job Curve and Baby Job inside the Snark.
00:24:13.790 - 00:24:21.818, Speaker C: But it's not the same curve which is the one used in pre compiled contracts in Ethereum. So you essentially then have to in.
00:24:21.824 - 00:24:29.818, Speaker A: Pre compiled contracts, we have a curve for the Snarks themselves and inside Snarks we use a different curve which is embeddable in the Snarks.
00:24:29.994 - 00:24:45.998, Speaker C: And another question, I'm just curious. Have you thought in principle you could replace the Snarks maybe by aggregated signature? You're putting all of the transactions in the block anyway, so you could just aggregate the signature.
00:24:46.094 - 00:25:47.470, Speaker A: You could aggregate the signature and say that somebody has signed the transactions, but you cannot prove that they are all correct that each signature for each transaction was from the sender and that sender had correct the sender had amount on their balance more than what they're trying to send at this very moment and so on. So Snarks is your knowledge proof in general prove the entire function, the entire state transition by applying all these transactions, BLS would just prove that somebody signed the data, nothing more, just notarizing data. So there are approaches to data availability with data notaries, but they are still being researched. So for now we use the simplest one because 500 transactions per second is until we get there. We will figure out how to solve it better. I can just repeat the question. It's not a problem.
00:25:47.470 - 00:25:48.270, Speaker A: Sorry.
00:25:48.420 - 00:25:51.194, Speaker C: Is it possible to use it right now as developer?
00:25:51.242 - 00:27:13.074, Speaker A: Your solution? We're planning to release documentation and SDKs soon for developers to be easier. We have very high requirements to quality and we don't want to release you can use it now, you can talk to me and you can immediately use it on hackathon. But we want to release something which is very high quality and easy to use, which is going to be soon. Yes, yes. What do you mean? In 500 PS, DPS would not be acceptable for scaling solution. For full scaling solution. Oh, but this is just one side chain, so you can have multiple side chains.
00:27:13.074 - 00:28:07.828, Speaker A: Yes, you cannot have many side chains with zero knowledge proofs because they are all going to compete for the gas on the main chain and they're going to drive the gas price insane. And you also have other transactions, but the limitation is only because of data availability. There are ways to solve data availability without data on chain, which will make us true plasma because it's going to be sublinear to the number of transactions. It's just not a priority right now because it's much easier to do it this way. Once we hit this limit, we can always switch to something which is slightly less usable but allows more transactions. Hello.
00:28:07.914 - 00:28:18.836, Speaker C: If the limitation of just data availability, can't you try to use something like swarm or something that you can prove that with Vicars and Arcs?
00:28:18.948 - 00:29:08.088, Speaker A: Yes, we can. However, we have a requirement of 100% data availability, so we need to guarantee that every single byte of this data is going to be retrievable. In the worst case, this is the challenge. No existing solution offers that. There are approaches like Airweaf which are promising, but we always need something like we need to guarantee that every single bit is going to be retrievable. So we can do it with proofs of proximity, with erasure, coding, things like this, where we can probabilistically check that we have at least some percentage of the data blocks. And then if we have, let's say, half of the blocks, you can always reconstruct the entire data set.
00:29:08.088 - 00:29:38.280, Speaker A: But no solution currently offers this. So it would be great if somebody can work on this and fulfill it. Until now, we're just taking the lazy approach and putting data on chain because it's just not as critical. But yes, there is work in this direction. Erasure coding is the keyword. You want to check if you are interested? Okay, I think we're done. Thank you very much.
00:29:38.280 - 00:29:52.066, Speaker A: Hey, guys.
00:29:52.088 - 00:30:04.280, Speaker D: So I know there's a lot of questions about this kind of stuff, so if you guys are interested in having an informal workshop, it's going to happen just kind of outside. So, um, just follow Alex and him and Lewis will give you an informal workshop on this.
