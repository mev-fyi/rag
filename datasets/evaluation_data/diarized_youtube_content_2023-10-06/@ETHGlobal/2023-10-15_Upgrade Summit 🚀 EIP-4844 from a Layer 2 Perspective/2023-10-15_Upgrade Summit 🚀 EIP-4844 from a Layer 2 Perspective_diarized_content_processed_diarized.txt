00:00:09.450 - 00:00:33.080, Speaker A: Thank you. Thank you for having me. Thank you everyone for tuning in. So today I'm going to talk about EIP 4844. It's one of the most debated and expected EIPS. It's going to improve Ethereum's utility as a data availability layer. And if any of these terms are new to you, I'm going to explain these terms to you in a bit.
00:00:33.080 - 00:01:09.774, Speaker A: So my name is Mohammed. I'm a researcher at scroll. Scroll is a ZK roll up and one of the main users of this EIP are roll ups. So I'm going to talk a little bit about the perspective of roll ups on this EIP as well. Okay, first of first, EIP 4844 is about data availability. And data availability is a big piece of puzzle in the grand scheme of scaling ethereum. So, quick reminder, ethereum is decentralized blockchain.
00:01:09.774 - 00:02:08.050, Speaker A: It's very secure, but we can't do more than like roughly twelve transactions per second on Ethereum, which is clearly not enough for a worldwide scale of computing. So people have been thinking about how to solving this so called trilemma and making Ethereum scalable too, while preserving decentralization and security. And we are finally close. If you think about progression of ideas for scaling ethereum. First people thought about making blocks larger. And in fact EIP 2028 did this with reducing the gas cost of call data. And there were other attempts to push this even further, but for multiple reasons such as risk of centralization, risk of destabilizing the consensus mechanism, or sometimes just sheer complexity, the IIPS were too complex to implement and block those atoms.
00:02:08.050 - 00:03:18.810, Speaker A: And then people thought about state charting. State charting was this idea that we split the blockchain, the whole ethereum blockchain into smaller blockchains and rotate validators in these smaller blockchains and all stuff like that. That ideas are now abandoned again for them being too complex. And also other solutions like roll ups showing up that proved to be easier to implement and easier to argue about and required less things to be enshrined in protocol. But the important thing here is while people were studying state charting and working toward it, we encountered this problem of data availability, which I'm going to be explaining in a bit, a little bit more. And then roll ups were introduced. The idea of roll ups is to do the computation off chain and prove that the computation was done, the execution of the transactions was done correctly via a proof on chain.
00:03:18.810 - 00:04:05.820, Speaker A: But this proof is not enough. We also need a data availability solution. We also need to make the transactions that were executed available somewhere to the users who want to check. So we have two parts in a roll up. First is a state transition proof and the second part is a data availability solution. And as I said, Ethereum community came together and they decided, okay, the way to scale Ethereum is through roll ups. And they adopted a roll up centric approach to scaling that means roll ups are now first class citizens of Ethereum and people try to make the experience of roll ups better, so that it makes the experience of every user of Ethereum better.
00:04:05.820 - 00:05:00.540, Speaker A: And data sharding is an idea that came after adoption of roll ups. So we abandoned state sharding, but we still want to benefit from this idea of splitting work among different validators. Not everybody has to do everything something like that. And EIP four eight four is the first step toward state charting which we are going to talk about more in upcoming slides. Okay, so the data availability problem is to ensuring some data that was supposed to be openly published, is indeed openly published and everyone who wants to download it can download it. Maybe data availability name is a bit confusing, maybe it was better to call it Data Publishing. As you can see intuits by Donkrat, one of the people who worked on the IP for it for four.
00:05:00.540 - 00:06:01.658, Speaker A: And the interesting thing is you cannot make proofs of unavailability. So if you want to ensure that some data is available, unlike execution of transactions, you cannot rely on fraud proofs because it's impossible to prove some data is unavailable. So everybody has to download every data to make sure it's available, something like that, or at least it's the case now. And I want you to remember this because we will revisit this concept. And a data availability layer is the infrastructure that allows people or other protocols to publish data and it guarantees that this data will be available, meaning that anybody who wants to download it, can download. Okay, now let's focus on roll ups. So you might have heard about optimistic roll ups and ZK roll ups.
00:06:01.658 - 00:06:54.850, Speaker A: This optimistic or ZK refers to the way that the roll up proves its state transitions. And it's the way that the rollups get integrity of execution or guarantee integrity of the execution. Op rollups rely on fraud proofs, and to make fraud proofs, you need to know the whole state of the roll up. So for that reason, in op roll up, data availability is required even for proving integrity of execution. Okay? So if you're optimistic roll up, you have to make your state available, otherwise your proof system is broken. For ZK roll up, it's not necessary. Like you can have a ZK roll up with proofs and users can be sure that everything is executed correctly and data availability is not required.
00:06:54.850 - 00:07:58.230, Speaker A: But integrity of execution is not everything. We need other things in a roll up as well if we want to preserve the level of security decentralization that layer one has. For instance, if data is not available and we just prove the correctness of execution with some ZK proof and the person who has the full state of the roll up never makes it available, then if they don't show up one day, the roll up is bricked. And if you want to make a transaction. They are the only way to make the transaction because they are the only entity that can make a new proof and make the estate of the roll up progress forward. There are other issues like if you want to enable permissionless withdrawals, there is no way because the proverb can just resist including those withdrawal transactions that are enforced. This could result in a bricked estate or censorship depending on the design.
00:07:58.230 - 00:08:41.620, Speaker A: So it's not an ideal situation to be in. So full state of the roll up has to be recoverable. And roll ups do it in two different ways. One is to make all the transactions that are executed available somewhere, most likely on L One or make the estate diff of after execution of a batch of transactions. You have some update to the state of the roll up. Make these updates available. This is just a technicality, but the end goal is to make the full state of the roll up available to anybody who's interested to have it so that we can have things like multiple provers, decentralized provers, permissionless withdrawals and things like that.
00:08:41.620 - 00:09:23.390, Speaker A: Okay? Now, how scroll approaches data availability and what data availability looks like in scroll. The roll up process in scroll is quite complex. It has a lot of steps. But the two steps that are important for us are commit and finalize. So if you look here, the roll up nodes submit transaction batches to roll up contract. This phase is called commit. And the goal of this phase is to publish transactions on chain to get data availability and also commit to the transactions that are executed in this batch and the new estate that results from this batch.
00:09:23.390 - 00:09:57.498, Speaker A: The second thing that we do or we focus on here is finalize. In finalize step, the proverb sends the proof to roll up contract which is verified by the roll up contract. And then the state of the roll up is finalized. The new state is finalized. So, as I marked here, you can think of it this way. Commit is taking care of data availability and finalize is taking care of integrity of execution. So how we commit now in a score, we commit with Call data.
00:09:57.498 - 00:10:32.678, Speaker A: And what I mean is we want to execute a batch of transactions. We want to make this batch of transactions available. So we encode it in Call data or put it in Call data of some function called commit batch of our roll up contract. We just submit it on chain. The contract never stores the whole Call data. It just computes some commitment hash of the Call data, the transactions that are going to be executed and stored that. But the Call data is gone.
00:10:32.678 - 00:11:20.518, Speaker A: Or is it? Actually, it's not stored in Ethereum state, but it's recoverable from Ethereum blocks. So anybody who wants to download it can download it forever. It's there forever until Ethereum is a thing. So it's available. The call data is cheaper than storage, but it's not cheap enough in the sense that currently 80% of our cost, operational cost of paying gas fees on chain is for doing commit and only 20% for verifying proofs. So 80% cost is for data availability. That means we need cheaper data availability solutions, but we don't want to compromise on security.
00:11:20.518 - 00:11:50.750, Speaker A: We could have used a data availability solution other than ethereum, which could be quite cheaper, but it would have been centralized and carry a lot of security assumptions. We don't want that. So EIP 4844 is here to solve that. EIP 4844 has two objects in it. There is one object called Blob. A Blob is a piece of data, like 125 data. You can think about it this way.
00:11:50.750 - 00:12:39.082, Speaker A: If you look closer, it's two to the twelve elements of field elements of BLS. Twelve, three, eight, one. So what can you do with the Blob? You can put your transaction batch or state diff inside the Blob and publish it. And remember, data availability is not equal to permanent storage. While call data is permanently stored, we don't need this data that we make available to be permanently stored. We just want to make sure anybody who wants to download it, anybody who is interested, gets a chance to download it, maybe for a while. A Blob carrying transaction, which is a new type of transaction that's introduced in EIP 44, can carry multiple Blobs.
00:12:39.082 - 00:13:40.690, Speaker A: So in that transaction you can have like two, three Blobs, and the transaction actually has only a commitment to those Blobs. So only a commitment to the Blobs goes to the EVM execution environment and smart contracts. And the Blobs, the content of the Blobs is stored only by consensus clients, consensus nodes for roughly three weeks. Okay, now what does a Blob carrying transaction look like? It's actually same old EIP 1559 transaction with two additional fields. One of them is max fee per Blob gas, which specifies how much you want to pay for each Blob gas, which I'm going to explain in a bit, and a list of commitments to the Blobs you carry. And those commitments are virgin hashes. It's just a type of commitment.
00:13:40.690 - 00:14:30.500, Speaker A: Okay, let's look closer to these two foot. So the max fee per Blob gas is there because EIP 44 introduces a 2D gas market. We have regular gas and Blob gas, and we have to specify how much we want to pay for regular gas like before, and how much we want to pay for Blob gas. So that's why we have this field. And Blob gas also has the EIP 1559 like pricing mechanism. And the mechanism works like this anytime we have more than three Blobs per block, the price is going to be increased and we cannot have more than six Blobs per block. So the mechanism works so that in each Blob we have roughly three.
00:14:30.500 - 00:15:04.794, Speaker A: In each block, we have roughly three Blobs. It's an important number, we will revisit this. Okay? And the second new field is a Blob version hashes field. It's going to store a list of Blob version hashes. Each version hash looks like this one byte version. It specifies the version of the commitment we use and then a hash of a commitment to the block. It's a KCG commitment.
00:15:04.794 - 00:15:58.526, Speaker A: So it's a polynomial commitment scheme. There is a good reason for this, but for now, just focus on how it works. A Blob is just an array of two to the twelve field elements. We can apply a log range, sorry for the typo log range interpolation to it and get a polynomial where the is element of the Blob is encoded in evaluated on omega to the I. Okay, so why we have this version byte? Because we can upgrade this commitment format without breaking the format. Like we can bump up the version and instead of KZG use something else. Right? And why KZG? It's going to be handy when you want to use this commitment scheme in ZK rollups and optimistic rollups, I'll show you that.
00:15:58.526 - 00:16:27.758, Speaker A: But with KZG, we are committing to this polynomial, not the array. Right. And it's useful, actually. Okay, before proceeding, let's compare Blobs with versus Call data. Call data is what we use now and Blob is what we potentially use in future. Blobs are only temporarily stored, call datas are permanently stored. Blobs have no Smart contracts, have no direct access to Blobs, but Call data is accessible from Smart contracts.
00:16:27.758 - 00:17:16.866, Speaker A: And Blob has its own gas, which makes it separated from the fluctuation of the regular gas market. For all these reasons, it seems like Blobs have no unnecessary feature. They only have things that are required for data availability. So we expect them to be cheaper, but for the fact that they have their own market, there is no way you can be sure. Like we have to see these events fold out and see how it goes. But the expectation, and it's a reasonable expectation, is that Blobs be a lot cheaper. Okay, the EIP also introduced as a precompile, the precompile is a point evaluation precompile.
00:17:16.866 - 00:18:14.230, Speaker A: What it does is that, okay, you have the Blob and you make a polynomial out of it and you commit to that polynomial. But the good thing about this polynomial is that you can evaluate this polynomial on any point. Like if you want, you can use the point evaluation polynomial and show that or assert that on point z, the polynomial evaluates to value y. This may look very unreasonable, so why not just query the is value of the Blob, but it will be handy in a bit. And remember, we have to pay for these invocations of pre compile and it costs 50K gas to do so. Okay, back to the roll up process. We saw how we commit to do the commit phase with Call data, but now we want to do it with blobs.
00:18:14.230 - 00:18:45.960, Speaker A: It's very easy. The roll up nodes used to submit batch of transactions as call data. Now they submit them as Blobs. And the roll up contract had to calculate a commitment to the batch of transaction. Calculate the hash of the batch of transaction. We don't have to do it anymore, we just store the Blob version hash of the corresponding Blob. And again, the transaction batch is not stored in Ethereum state, but it's available from consensus clients for roughly three weeks.
00:18:45.960 - 00:19:50.454, Speaker A: So anybody who's interested has enough time to download it. But remember, we are only storing the commitment to these transaction patches and the commitment is not enough for Zkevm to execute, we have to open this commitment and then the Zkevm circuits can execute. There is this Pi or public input subsurface that takes care of opening commitment. So Improver has to provide the pre image of transactions that are actual transactions and Pi circuit checks that these pre images are consistent with whatever was committed to unchained. Why we only have the commitment because it's very costly to have large public inputs for the Verifier. And what Pi circuit does, you can think of it as decompressing public input. And why we should have this, because otherwise Prover can execute any batch of transaction.
00:19:50.454 - 00:20:50.850, Speaker A: Commit to something and don't do the consistency check proverb can do execute another batch of transactions, which is undesirable. Okay, after we have Blobs, Pi circuit is very easy to implement, I mean relatively easy to implement because we have these random point evaluations. So if you want to make sure two polynomials are equal, you can just look at a random point and see if the two polynomials evaluate two equal values on that random point. It's called the short zip element. And with point evaluation pre compile, it's easy to do. So I'm not going to get into details because we don't have time, but you can expect a blog post to be published by scroll in a week or so that explains these things in detail. Okay, back to Blobs and the capacity they introduce.
00:20:50.850 - 00:21:51.230, Speaker A: As I mentioned, Blobs have this EIP 1559 like mechanism which targets three Blobs per block and doesn't allow more than six Blobs per block. What does that mean? It means on average we have 380 KB additional space per block provided by these three Blobs. Okay? And that translates to 100 Ethereum transactions like typical transactions, transactions like swapping and uniswap, which doesn't sound a lot like across all roll ups we only get 100 transactions per second. In contrast, Ethereum has twelve, roughly. So we are just ten xing the number of transactions possible somehow. Okay, why these parameters are set. So why we don't allow more Blobs per block.
00:21:51.230 - 00:22:33.994, Speaker A: So if you think about it, the amount of storage requirements added to consensus clients who have to store these Blobs is only 50GB. It's nothing. We can double it, triple it easily. Clients won't be unhappy, people who run nodes won't be unhappy. But the problem is the larger the blobs or the more blobs we have per block, the network propagation delay for each block will increase. And this is bad because every block has to be attested to in 4 seconds. Like it has to be well propagated across network in 4 seconds.
00:22:33.994 - 00:23:06.998, Speaker A: And if a block is too large attestators or validators, cannot attest to it in time and it would make the consensus mechanism unstable. For that reason, we are very careful with these parameters. We are not pushing these parameters too high. But on the other hand, we don't get much from it. Okay. When EIP Freight four four will hit the main net, it's included in the Dencon upgrade. It is expected, as far as I know, to be here next year, probably.
00:23:06.998 - 00:23:51.506, Speaker A: We need a lot more testing before it's finalized. Okay. I mentioned that there is this other proposal, data sharding and data sharding proposal. It's mostly called dunk sharding. Dunk sharding is the same idea of having blobs, but instead of everybody has to download all the blobs, like every consensus client has to store all the blobs and serve the blobs. And before a block is considered fully downloaded, you have to download all the blobs and such. The idea is we don't enforce everybody to download all the blobs.
00:23:51.506 - 00:24:40.262, Speaker A: People just have to run some tests to make sure that the blobs are available. While we can't make proof of unavailability for blobs, we can efficiently test availability of the blobs. And that's the idea of data availability, sampling and EIP 4844 is a step towards this because it implements some components are going to be reused in dunk sharding. So if we have this, we can have more blobs, like we can have like ten or 100 or even more blobs per block because the cost of efficiently testing a blob is available is far less than just downloading the whole blob. Okay? So that allows us to not worry about the propagation delay that much. Okay. Conclusions? So we have come a long way.
00:24:40.262 - 00:25:17.360, Speaker A: Implementing AIP for it for four was quite hard. Making dunk sharding design finalized was quite hard. And I believe these are that the things we achieved so far. And we can expect very good results in a short time. Like maybe now it's not enough, but in a few months or maybe a year or two, we can expect to hear better news. So the fees with EIP 4844 will go down. I expect them to go down two x to five x.
00:25:17.360 - 00:25:47.060, Speaker A: But it's important to remember it can be still quite high if we have another bull run and a lot of transactions that not low enough for people to transact. And the community has to keep working on dunk sharding. But I can assure you that the design of dunk sharding is almost finalized. EIP freight Perform delivers a lot of components that are reused in dunk sharding so we can expect good things to happen soon. Thank you. That's all?
