00:00:27.920 - 00:01:11.408, Speaker A: Good morning, good afternoon and good evening everybody. Welcome to the optimism future of Ethereum Summits. We're really excited to be going live here with so many awesome talks about the Optimism ecosystem. Optimism is a huge partner of ETH Globals over the last couple of years and we're really excited to be working so closely with them to celebrate an important part of the optimism journey, as well as chat about the newest things that are coming down the pipeline for the scaling solution known as optimism. So thank you everybody for being here. My name is Jacob, I'm part of the ETH Global team. I'm also an Optimism delegate and I'm also going to be joined today as MC by Katie, who'll be joining in just a second.
00:01:11.408 - 00:01:48.188, Speaker A: Thank you everybody for coming online on our ETHGlobal TV platform. This is a really nice way to interact with everybody. Feel free to log in and join the conversation, ask any questions as people are going through their talks and we will relay those to the different speakers. And of course, feel free to enjoy some lo fi beats during any of the breaks throughout the session. So, as I mentioned, Katie's joining me as one of the MCs today. Katie works for the Udhc team and also is an Optimism delegate. So really excited to have representation on the optimism dao side here as well.
00:01:48.188 - 00:02:26.536, Speaker A: And together we'll be popping in today to go through the different sessions. So I just wanted to give a quick little overview of today's schedule. So we have Carl joining us in just a minute to talk about the modular optimism stack op stack. We have Joshua to do an introduction to architecture optimism, as well as chat a little bit about bedrock. The next upgrade, we have North Swap joining us at 01:00 P.m. Eastern to talk about modular sequencing, followed by Protolambda joining us for pluggable data availability at 130. After that we have a really cool panel at 02:00 P.m.
00:02:26.536 - 00:03:10.536, Speaker A: Eastern on the state of EIP 48 44, joined by Tim Bico, Terrence Mophie and Roberto Bayardo of the Coinbase team. And finally, a cool overview of one of the major products that is completely built on optimism, which is Quicks, their NFT marketplace. So Mark Dawson, the founder of Quicks, is going to come and chat at 245 about what they're working on there. So without further ado, would love to welcome Carl onto the stage. So Carl, whenever you're ready, feel free to unmute and share your screen. Carl is listed on this chat as Carl with many op, with many exclamation points, so you know he's excited. There you go.
00:03:10.558 - 00:03:18.840, Speaker B: Welcome to the chat you already know. How could I not be? Oh my goodness. Let me make sure that okay, I got all the things.
00:03:18.990 - 00:03:20.184, Speaker A: You're all set?
00:03:20.382 - 00:03:22.860, Speaker B: I'm all set. We did it, Reddit.
00:03:23.760 - 00:03:24.492, Speaker A: There you go.
00:03:24.546 - 00:03:25.388, Speaker B: Everything looks great.
00:03:25.474 - 00:03:28.780, Speaker A: Well, I'll get out of the way. And excited to hear more about the op stack.
00:03:29.520 - 00:04:01.672, Speaker B: Okie dokie. So, welcome. Everybody going to talk about the Op stack bedrock and the rise of the super chain. And we actually got a solid amount of time. I didn't realize we had so much time, but I got to fill up all the time. So, without further ado, I'm Carl, of course. I work at Op Labs doing this optimism, or whenever I talk about these things, right, these words pop up on the screen.
00:04:01.672 - 00:04:32.400, Speaker B: There is one word that everyone sees. They don't see the rest of the words, they see superchain. That is what they see realistically. So I'm not going to start off by talking about the super chain. I'm not going to talk about how it is all these chains kind of combined into one super duper fantastic thing. I'm not going to talk about that to begin with. Instead, what I'm going to talk about is the problem ethereum congestion back in 2016, 2015.
00:04:32.400 - 00:04:59.240, Speaker B: This has been a problem for way too long. Oh, my God. It's been a long, many years. But we've had too much demand for Ethereum. Ethereum is too good. And so what happened? How did we solve Ethereum congestion, realistically speaking? Well, we copy and pasted it into more chains, all these various chains generated by Dolly two. That is what we did.
00:04:59.240 - 00:05:56.924, Speaker B: That is what we did. And then what did we do? Well, we connected these chains with largely insecure bridges, these largely insecure chains, realistically speaking, and we kind of hacked some together to address this horrible problem of congestion. And in some sense, optimism is kind of one of these things, right? It is a layer two, but from a user experience perspective, it still has a bridge, it still feels a little bit distinct, but it does have the nice quality of being rooted in Ethereum and sharing some of that security. And so this is kind of the state of affairs in some sense. And if I'm a user, this cute little gerbil also generated, most of these photos are generated, actually. Then you're going from one chain to another, trying to deposit, withdraw. It's just kind of chaos, to be honest with you.
00:05:56.924 - 00:06:42.760, Speaker B: And you can kind of visualize this as a crazy market, right? It's like a marketplace and you're going from these different sellers and maybe you go to this person, you're transacting on this chain using this vendor, and behind you there's a bridge that just exploded and another one that just exploded. And then you went to this other chain and now it's actually a rug pull. It is true and utter chaos right now. This is not good. This is not a user experience worth bragging about, right? This is just honestly very sad. But it's better than no one being able to transact, which is the alternative, because Ethereum was just way too congested. But we can do better.
00:06:42.760 - 00:07:14.820, Speaker B: We can do better. And what if I told you? What if I told you that there was a solution to this problem. There was a solution where instead of going from one place to another, everything is all in one place. And you don't have to transact with all these different vendors. You can instead transact with one trusted seller. One trusted seller and you've got seamless atomic checkout in this market. Well, it's the supermarket.
00:07:15.880 - 00:07:16.740, Speaker A: Whoa.
00:07:17.240 - 00:07:49.970, Speaker B: Oh my gosh. Okay, this is a joke, right? We're not here for supermarket technology, but it's just super market super chain anyway. Really though? Really though, I can't help myself. Our chain should have these properties, right? We demand these properties that everything in one place is single trusted seller. So you're not getting hacked all the time, right? We shouldn't have to bridge from one chain to another. We shouldn't have to get hacked. I mean, my goodness, that's a low bar.
00:07:49.970 - 00:08:11.508, Speaker B: And also transaction fees should be cheap and sometimes pretty much free. Like that is legitimately. There is no technical blocker and so there is no reason why we should not achieve this. Don't fear though. Don't fear. The op stack is here. All right? We finally got to the actual meat of the talk to an extent.
00:08:11.508 - 00:08:39.104, Speaker B: Well, state of affairs, kind of crazy. Absolute madness. But we can fix this. How do we fix this? Well, we can build standardized open source libraries that provide shared security so things are actually secured rooted in Ethereum. There are fallback mechanisms. There are things that prevent terrible hacks. Also, just the fact that it's standardized makes it way more secure because now there are way more eyes on it.
00:08:39.104 - 00:09:51.460, Speaker B: It's not just a one off solution because that's how you get hacks. Additionally composable, right? We need these systems to all work together. You can swap out one module, they're compatible with one another and additionally, finally, coordinated sequencing. I won't talk about how incredibly that important that is, but it is actually incredibly important for creating an experience. Now what is this experience that the user, at the end of the day, all of this technology is in service of building something that we can all use, right? I want to use a better product than we have today. And so what is the user experience of this system? Well, with the compatibility that the shared technology enables, we can actually create a virtualized single super chain, right? We can actually interact with the system in a way where you're not having a bunch of user facing bridging. All that kind of stuff can be handled in the background, right? We can have robust modules that ensure security and additionally it should be hella scalable.
00:09:51.460 - 00:10:18.484, Speaker B: Horizontally scalable. Some call it quote, infinitely scalable. But I feel like that's just like a savage marketing term. Horizontally scalable. So we can have near free transactions, transactions that essentially cost the amount of money that it costs to run that transaction on a computer. So it's like renting from AWS as opposed to paying surge pricing on Uber. So we can realize this super chain.
00:10:18.484 - 00:10:59.704, Speaker B: We can realize it with open source technology, shared standards and working together and we can make that happy experience for our happy gerbil. Now, if we want to talk about how exactly the kind of like combination the virtualization of the superchain works, you can kind of take a look at endgame that has a bit of a hint at it and then there's even more talks that we can go into. But I won't do it right now because we don't have enough time, or at least so I thought. But you can ask questions at the end of the presentation. So now we are past the intro. Maybe I needed all the time that we have. Let's talk about the core of the Op stack.
00:10:59.704 - 00:11:32.340, Speaker B: So this is not necessarily everything, but this is the kind of bread and butter. Okay, what is the Op stack again? Standardized open source modules for layer two chains. We are using it for, of course, optimism mainnet, but it is open source, therefore anyone can mess around. Now, there are three key layers to this, right? Any stack has some layers. The consensus layer, the execution layer, and the settlement layer. Consensus. That's how the chain gets constructed.
00:11:32.340 - 00:12:13.088, Speaker B: The execution, that's the virtual machine that is actually being executed over this chain of inputs. And finally the settlement layer. That's how withdrawals occur. So if you look into layer one, if you look closely with a magnifying glass, you will see that layer one is actually made up of its own consensus layer and execution layer. So this is a recent change that we've modularized layer one in the very same way that we're trying to modularize the layer two technology. And the way that that works is that we have Prism. These consensus layer clients, not just Prism, don't just use Prism y'all, we need diversity.
00:12:13.088 - 00:12:54.576, Speaker B: Additionally, we have execution layer clients like Geth and we have that same structure playing out in layer two as well. Now, let's get into the nitty gritty of the consensus layer, execution layer and settlement layer. What are these different things? So first, for the consensus layer, we have two key modules, two options that we can choose from. We can either choose roll up, which basically means that all the transaction data is fully available on chain or plasma. Now, plasma got a bad rap back in the day. It was just too early. And now there's a million different terms for plasma, like Valadium validium all of these things, the various things.
00:12:54.576 - 00:13:37.644, Speaker B: Basically, at the end of the day, we should just call them data availability challenges. We have a consensus layer client that has off chain data with data availability challenges on chain. So those are our two kind of like broad categories of consensus layer type deals. Additionally, we've got execution layers. So that is the EVM, the classic one, right? We've also got extra fancy ones like this weird one in the middle that I'm kind of just like hinting at but not saying because I don't know if I should talk about it yet. But anyway, we've got other execution clients that are special purpose, maybe for some games, maybe not, whatever. And then finally we have the settlement layer.
00:13:37.644 - 00:14:25.072, Speaker B: This is how things actually get withdrawn from the system. This is how you kind of prove the withdrawal. Now, notably, I have here multisig fault and ZK proof. I am writing this as though the settlement layer, if you have a fault proof or a ZK proof that is kind of working, but you still have a fallback to a multisig then because of the weakest link security, at the end of the day you're still multi SIG. And so that's why multisig is here. Obviously no one wants multisig to persist into the future. But the reality of layer two right now is that things are being secured, withdrawals are being secured by the multisigs, which is better than it being secured by an insecure proof system.
00:14:25.072 - 00:15:29.040, Speaker B: So let's be thankful. Anyway, consensus layer, execution layer and settlement layer, right? You can choose each one of these and if you choose this configuration, then you'll end up with an EVM equivalent optimistic roll up, right? This is the perfect flagship for an L two, but it's not necessarily the only configuration you might ever want. And it does have different security trade offs or withdrawal trade offs, right? All of these things are trade offs. Now, additionally, eventually you'd want to plug in potentially a ZK roll up, a ZK proof system, a ZK settlement layer. And so what that means is that we can have an EVM equivalent ZK roll up. And additionally, because it is modular, we can swap it in and swap it out. So the same chain that was running a fault proof can, without changing anything about a developer experience, without changing anything about the nodes that people are running, you can actually swap in a different proof system for withdrawals.
00:15:29.040 - 00:16:20.544, Speaker B: And so that means that we can get these seamless upgrades and write compatible software. Actually, in fact, the key to scaling up the production of software is defining really good abstraction layers. And so this is actually more important than you might imagine because it allows people to work on different things in silos and still be compatible once they have done the incredible amount of work. And ZKPs are a great example of something that's going to take a lot of work to make extremely inexpensive. And let's continue. So we've got the consensus layer, right? We can do things like layer three by layering in plasma with the EVM and fault proofs, right? All of these things are options and even swap out the execution layer. All of these configurations are valid.
00:16:20.544 - 00:17:12.390, Speaker B: But the key is that we should make sure that the way that they communicate, the way that these layers communicate with one another is standardized so that we can all benefit from each other's innovations and build out the blockchain stack candidly in an open source and permissionless way. So that is the core of the opizzle stack ISL so the bedrock release. We are finally at that section. Okay, what is bedrock? Right, bedrock is a major step forward across two primary dimensions. So, first off, obviously it is the next upgrade to optimism mainnet. That is, we are upgrading our main net. If you deposit into optimism, then that thing is going to be using bedrock very soon.
00:17:12.390 - 00:18:02.432, Speaker B: And that's going to make it literally like the most secure, the most scalable roll up that is out there. Let's go. So, bedrock is actually legitimately, extremely dope. The second thing is that it supplies the foundational op stack modules, right? We built it out in this modular way already. And in fact, in the later talks with Josh Et to we'll go into the details of how these things actually fit together and what components we built. So, we have a configuration of the op stack and we have all of these different packages which it is using, right? We have this op node, which is the consensus layer kind of equivalent. We have op geth.
00:18:02.432 - 00:18:39.884, Speaker B: We even have op aragon, right? This is literally two ethereum clients that are compatible. Additionally, we've got all of these other batcher proposer contracts. We have Canon for fault proofs. All of these things are built in this modular way. And so here I'll go over into more specific details of where the major steps forward in modularity are. So, first off, modular execution with the engine API. So, first I started this talk by saying how the consensus layer and the execution layer are actually being built out and were created for layer one development.
00:18:39.884 - 00:19:18.652, Speaker B: Well, what did we do? Well, we swapped out the consensus layer and replaced it with the op node. So now we're coming to consensus by using L one consensus. That's literally like the definition of layer two. Additionally, we have the execution layer. So we have Get, but we can also swap in things like aragon. And it takes very little code to modify these execution layer clients to support layer twos. And that is critical, right? Code reuse is a key factor of security and these systems need to be more secure than any other system you've used.
00:19:18.652 - 00:19:57.172, Speaker B: Let me tell you, these security requirements are insane. And so this also enables us to have a MultiClient ecosystem. We would never have made the merge upgrade we would never have done the merge upgrade without multiple clients because multiple clients catch divergences between them, making the entire system more secure. That's why the merge went through without any critical bug reports, right? That is literally the technology that the security technology they used was a MultiClient ecosystem. And so we need that very same thing for layer two. And that is absolutely critical. If we want to enable fault proofs, for instance.
00:19:57.172 - 00:20:22.428, Speaker B: And I'll talk about that a little bit more later. So, yes, additionally, we can even have more fun. And maybe it's not going to have a multi client ecosystem, but we can input this weird mystery VM and boom. Now we've got even more execution engines that we can use. So that is the first step, right? It's the engine API. The second thing is modular derivation. So first execution.
00:20:22.428 - 00:21:02.136, Speaker B: The second one is derivation or the consensus layer. So back in the good old days, a year and something ago when we were writing our roll ups, we thought that we needed a bunch of smart contracts on layer one to actually derive the roll up. So essentially, you take in L one. This function is like the roll up function. It produces the layer two blocks. And what we were doing is we were saying, okay, you need layer one and you need some smart contracts on layer one to do some management for you and whatever. But this meant that the system was actually really brittle.
00:21:02.136 - 00:21:48.840, Speaker B: It was really hard to upgrade because you always needed to maintain these layer one contracts. But now what we're doing is we're consuming raw layer one blocks, raw layer one block data to produce the layer two chain. So what does that actually mean? It means that we can swap in because it's just like a raw stream of data. We can swap in new data sources like EIP 4844 or even plasma, right? Things like off chain sources of data. So this allows us to both a scale up, be ready for when we're improving layer one, because we're pushing forward EIP 4844, which is a major improvement to layer one. But we're also able to support things like layer three. And finally modular proofs with Canon.
00:21:48.840 - 00:22:11.440, Speaker B: Right, canon, it generates the execution trace over the client and it allows us to prove that execution trace. But we can prove an execution trace with either fault proofs or ZK proofs, right? These two things totally work. The ZK proofs might be too expensive right now, but that's something that we can work on over, importantly.
00:22:11.520 - 00:22:11.860, Speaker C: Right?
00:22:11.930 - 00:22:29.396, Speaker B: Importantly. Bedrock again supports multiple clients. And multiple clients is incredibly important to support production, high security chains, and specifically to support fault proofs and ZK proofs without big bad upgrade keys.
00:22:29.428 - 00:22:29.624, Speaker C: Right?
00:22:29.662 - 00:22:59.136, Speaker B: We need our upgrade keys to be upgrade. And then you wait like two months because you can actually withdraw. And we need them to make sure that the mechanism that allows people to withdraw from layer two in case there's a malicious upgrade, we need to make sure that that mechanism will never break. And the only way to make sure that mechanism will never break is with multi client support. So there we go. More information on that in our Pragmatic path to decentralization. Because we need to get rid of those upgrade keys.
00:22:59.136 - 00:23:02.528, Speaker B: We need to not be multi SIG roll ups.
00:23:02.704 - 00:23:04.790, Speaker D: Anyway, great.
00:23:06.200 - 00:23:33.116, Speaker B: This evolution, right? The state of affairs, this madness with these bridges going crazy and these different chains and this terrible user experience. We're already making progress there, right? Bedrock release is fast approaching. We're about to take a massive step forward in the op stack. Additionally, we already have multiple op based chains, so this is not even like, X. We need more op chains.
00:23:33.148 - 00:23:33.644, Speaker C: Like, literally.
00:23:33.692 - 00:23:48.064, Speaker B: We accidentally had people deploying op chains just because we were writing our software out in the open. And additionally, we've got teams experimenting with bedrock, pushing the boundaries of what was possible on chain.
00:23:48.112 - 00:23:48.516, Speaker C: Right.
00:23:48.618 - 00:24:37.220, Speaker B: We're going to break through. Even the scalability that we have today is not even close to what we're going to have tomorrow. It's like you don't think about the resource constraints on AWS unless you're a massive crazy team, but you don't really think about the scalability constraints on AWS. As an average individual, you should not be thinking about the scalability constraints as you're transacting or deploying DApps to a blockchain. This is just the beginning, right? We are about to take over with the zombie horde of gerbils generated by artificial intelligence. We will take over the game, and soon we will realize the final evolution of blockchain Walmart. No, I'm just kidding.
00:24:37.220 - 00:25:12.704, Speaker B: We will finally realize the super chain thanks to the op stack because we're working together to solve the problems in a secure and scalable way. Hallelujah. We're going to provide that good user experience to us because we're the users too, you know? And the rise of the superchain. That's it. And by the way, this is also a super chain image, but including a gerbil. I just love the way that these AIS, or like, Dolly specifically, cannot spell it's so good.
00:25:12.742 - 00:25:16.050, Speaker C: Anyway, thanks, everybody.
00:25:16.660 - 00:25:37.830, Speaker B: I guess we don't have that much time for questions, but happy to answer them. Let's go. All right, that's it. I mean, I don't know. I'm now in my room. I still feel like I'm in my room. Is there a place I should be looking for for questions? Do we have a couple?
00:25:41.180 - 00:25:41.960, Speaker D: I don't know.
00:25:42.030 - 00:25:53.100, Speaker B: I don't even know. I don't know where I am. Well, I feel so alone. Wait, am I oh, it's because I turned off my mic. Because I can't hear.
00:25:53.170 - 00:25:53.790, Speaker A: Maybe.
00:25:55.760 - 00:25:56.510, Speaker D: Hello?
00:25:57.040 - 00:25:59.068, Speaker B: Okay, fine. I'll just talk then.
00:25:59.154 - 00:26:00.092, Speaker E: Hey, Carl. Sorry.
00:26:00.146 - 00:26:00.652, Speaker B: Oh, great.
00:26:00.706 - 00:26:09.472, Speaker E: No, I think hi, it's me. No, I think I don't see any questions. I think we're good.
00:26:09.606 - 00:26:10.176, Speaker A: Let me check.
00:26:10.198 - 00:26:11.984, Speaker B: All right, sounds good. Well, thank you.
00:26:12.022 - 00:26:16.660, Speaker E: I guess we can open it up really quick. Does anyone have any questions they want to ask? Carl.
00:26:19.880 - 00:26:24.164, Speaker B: How did you find such a cute dog photo? Well, type it.
00:26:24.202 - 00:26:24.932, Speaker E: That would be my question.
00:26:24.986 - 00:26:31.800, Speaker B: Dog on Google. All right, let's move to Josh.
00:26:32.540 - 00:26:38.228, Speaker E: Okay. Awesome. Carl, thank you so much. I love your presentations. You're so enthusiastic.
00:26:38.404 - 00:26:39.080, Speaker A: Thank you.
00:26:39.150 - 00:26:55.068, Speaker E: You're awesome. Okay, cool. Hi, everyone. I'm katie. So up next we have Joshua Gutao who is going to be giving us an introduction to architecture.
00:26:55.244 - 00:26:56.464, Speaker C: Oh, hold on.
00:26:56.502 - 00:27:13.350, Speaker E: Here we go. Here we go. Okay, so we have Joshua Gutao giving us introduction to Architecture. Josh, welcome. Thank you so much for being here. I'll let you take it.
00:27:14.040 - 00:27:14.790, Speaker C: Awesome.
00:27:16.200 - 00:27:16.612, Speaker A: See?
00:27:16.666 - 00:27:45.910, Speaker C: Can I get my slides? Okay, slides up.
00:27:47.960 - 00:27:50.108, Speaker A: Cool. Awesome.
00:27:50.274 - 00:28:34.970, Speaker C: Hello everyone. My name is Joshua and I'm going to be talking about introduction to architecture. This is actually a little less architecture in terms of buildings and cool shapes in the sky, but this is going to be about software architecture and then more bedrocks architecture. And as Carl is talking about the various components of the Op stack, this is going to really highlight how bedrock, the system fits together and all the different pieces there.
00:28:35.740 - 00:28:36.296, Speaker A: Cool.
00:28:36.398 - 00:29:07.332, Speaker C: So the introduction. So today I'm going to be talking about what even is optimism? Bedrock. Carl's talked on that, but I'll expand on that a little bit more. I'm going to give an overview of the system, both of kind of like the components and also the actors. So we have a bunch of different pieces, but some of them do different things. Then we'll go through the roll up node, which is one of the components, the sequencer and the badger. Then the proposer fault proof challenge agent.
00:29:07.332 - 00:29:42.780, Speaker C: So that's all of the stuff that enables withdrawals. We'll go through a couple data flows and then go through some kind of final implementation notes at the end. So what even is optimism? Bedrock. Bedrock is a network upgrade to reduce fees and increase implementation simplicity. So kind of go through a little bit word by word because it's a big complex sentence with a lot underneath of it. So network upgrade. So bedrock is going to be kind of a point in time release.
00:29:42.780 - 00:30:17.140, Speaker C: The thing will have like we have our current system, we'll have the bedrock release and then we'll have more releases in the future. So the network is still optimism. This is just the network upgrade. And as part of a network upgrade, we'll be keeping all of the history with optimism network. This is just one more step in improving the user experience. And specifically the biggest change to the user experience is going to be reduced fees. And then the non changes to the user experience are going to be increased implementation simplicity.
00:30:17.140 - 00:31:01.096, Speaker C: This is something that most users won't see, but really improves the performance, proves the reliability, enables a lot of other things down the so how do we, when we get there, bedrock achieves this by redesigning the roll up architecture. So a lot of the things from the previous system kind of are getting modified. And then we're drawing a lot of new lines between different components inside the system and really making sure that everything fits together on a way we're happy with and also in a way that kind of enables different components to be swapped in, swapped out, how components can be tested by themselves and making sure.
00:31:01.118 - 00:31:03.400, Speaker A: That everything fits together very neatly.
00:31:04.300 - 00:31:29.308, Speaker C: So now we get into the fun part, which is the architecture. So we're going to go through the system overview. So here's kind of the discrete components. You have a roll up node. This is Op node. And then there's the execution engine, Op Get. We put Op in front of everything because why not? The batcher is the Op batcher, the proposer is Op proposer.
00:31:29.308 - 00:31:58.568, Speaker C: And then we don't have a challenge agent. And the fault proof is Canon. So Op Node and Op Geth are kind of the things that produce blocks in that's, like your full node on Ethereum. Op batcher submits L two transactions to l one. Then the proposer submits output routes and this is what enables withdrawals. And then we talked a little bit about Canon and have talked more about Canon in the past. But that is the fault proof system.
00:31:58.568 - 00:32:04.670, Speaker C: And the challenge agent is just a simple agent that interacts with the fault proof system.
00:32:05.520 - 00:32:05.980, Speaker A: Cool.
00:32:06.050 - 00:32:33.140, Speaker C: So those are different code bases and components. How do they translate to the actors? So the actor is someone running. So you can run a multiple code base lots of times. So Verifiers, that's a full node. So there's going to be a lot of those. That's just like running your full node on Ethereum. To do that, you need your Op node, which is like your consensus client, as well as your Op Get, which is your execution client.
00:32:33.140 - 00:33:04.050, Speaker C: Then you have the sequencer, which is a full node that is mining. And so it produces blocks. And then it also gossips them out and does a couple of small things. But under the hood, it's very similar to the Verifiers. And then you have your batcher, which is an actor that just maps straight back to the code base. Same with your proposer and then your challenge agents as well. The sequencer and Verifiers actually have to map to a couple of code bases because they're more complex things.
00:33:04.050 - 00:33:48.568, Speaker C: So giant diagram type. So this is going through how all of these actors interact. So if you want to kind of deposit into the system, we'll just follow that execution flow. So the user sends a deposit transaction to layer one, and then the sequencer reads. The deposit sequencer also then gets transactions kind of separately. Then it creates the new blocks, sequences them, the batcher then picks, and that's kind of how you get deposits. Now, once you have your l two transactions included, a user sends them to the Verifier.
00:33:48.568 - 00:34:30.480, Speaker C: They get to the sequencer, you create your blocks, the batcher submits the batches. Now then the Verifier reads both the deposit and the batches and then derives a chain that matches the sequencer. And so that's kind of the transaction flow. There's a lot of moving parts here, but that's kind of what it is. And we'll go through some of these flows in a little more depth later on. And then when you're doing output proposals, the output proposer reads the state route from the Verifier and then just submits it to l One and that's the op proposer. And then the challenge agent would interact, would kind of use the Verifier.
00:34:30.480 - 00:34:44.720, Speaker C: Again, looking at the state route and comparing it against what got submitted on l One. And so it's reading between the Verifier and l One. Those are, again, all the system actors and how they interact.
00:34:47.300 - 00:34:48.000, Speaker A: Cool.
00:34:48.150 - 00:35:19.800, Speaker C: So time to dig deep into the roll up node. So it does several things. Reads out from l One, has both deposits and batches. As Carl mentioned. It uses the Engine API to talk to kind of the execution client op geth. There's a P to P network and I'll talk a bit more about that later. And then just as an implementation, it's way simpler than the current l Two geth implementation.
00:35:19.800 - 00:36:08.330, Speaker C: It's a lot closer to upstream and it's a lot easier to work with the diagram. So this is basically all of the components that you need to run a replica or a sequencer. Just a full node on optimism network. So on the Ethereum side, you have your consensus client and your execution client. They talk to each other using Engine API. And then your op node, which mirrors the consensus client, uses just your standard ETH JSON RPC. So it does stuff like gets blocks, gets transactions, gets transaction receipts, just kind of essentially reading in the chain data from l One.
00:36:08.330 - 00:36:47.840, Speaker C: And then it talks to the l Two execution engine through a very slightly modified Engine API. It's very similar, but I'll talk through a couple of modifications there. And then the op geth, or l Two geth is a small diff on top of geth. It's actually a little larger than 500 lines right now, but not really that much more. And so that's a lot easier to work with and keep the op geth up to date with upstream. So we can bring in a lot of those improvements a lot faster than if we had done a lot more modifications to that. So this is a single node.
00:36:47.840 - 00:37:31.280, Speaker C: So now what happens when you have multiple of these nodes? So this is where every node is talking to almost every other node. So the vertical lines there basically maps back to the previous diagram and the horizontal lines map back to the previous diagram. And so the top line and the bottom line are two different sets of nodes. So on the left here, you have different consensus clients for Ethereum talking to each other through the beacon chain. Then l One clients on Ethereum have a sync protocol. They have a transaction pool. And then on the l Two side, we gossip out l Two blocks for the opiode.
00:37:31.280 - 00:38:14.172, Speaker C: And then we actually use a lot of the same sync mechanisms. Transaction pools all that when L Two execution engines are talking to each other. So getting into the details of this engine API, which is what connects the Op node to Op Get, it has two extensions, and these extensions are to some of the payload attributes or like fields. So we have two more fields when we're calling for Twist update, but the rest of it's basically the same. And then gen API is actually really simple. There's four calls, one of which we don't use at all because we don't have a transition. So it's just not relevant.
00:38:14.172 - 00:38:48.764, Speaker C: So the new payload call is just insert a new block into the execution layer. Simple fork choice. Update is a little more complex because it does two things at once. It sets a head block, including kind of safe and finalized blocks. And if you give it an optional parameter, it starts the block building process and it returns back kind of reference to that inflight block. And if you want to get that inflight block that it's building, you just call Get. So and that's basically it.
00:38:48.764 - 00:39:27.930, Speaker C: This is pretty standard API. It's specified by Ethereum, and it makes it really easy to include new execution clients. Op Aragon, that's not that much work to port over a thousand line diff. And then eventually we only have one roll up node right now. But it wouldn't be that hard to create a new roll up node that spoke this API and worked with any execution engine. So deposits and batches. So we've briefly spoken about deposits as kind of a way to get money into L Two.
00:39:27.930 - 00:40:21.872, Speaker C: These are going to be a lot faster now because we're reading them a lot closer to the chain TIFF. But they also serve another purpose. And the sequencer basically cannot delay deposits outside of a certain range and has to eventually include them. If you're interested in more details about the specifics of how this works, I gave a talk at ECC that does a really deep dive into bedrock and goes through all this and why this is true. But the net result of this is that the chain advances even if the sequencer doesn't want it to. And then this enables sensor step resistance. And then if you never had a sequencer again, you could still unwind the chain through deposits.
00:40:21.872 - 00:41:04.024, Speaker C: So deposits also do a lot more than just mint money on L Two. They're fully featured transactions. So anything you can do with a normal transaction, you can do with a deposit. And so, typically, you actually wouldn't use deposits that much to do this because deposits just get included on l One kind of one transaction at a time and actually have some state execution on l One versus kind of direct l Two transaction. When we batch those, we take a bunch of those, we compress them, and then we just shove it in call data of l One. So direct L Two transactions tend to be cheaper and that's kind of the typical flow. But if you need to, deposits are.
00:41:04.062 - 00:41:05.050, Speaker A: Always there.
00:41:07.580 - 00:42:05.740, Speaker C: P to P. So here we kind of described how sequencers gossips out blocks. And so the feature that's enabled here is that if you're just a replica on the network and you're subscribing to these unsafe blocks or blocks that the sequencer has just created, your view of the network is incredibly up to date. And so you can opt to choose to insert these blocks. Now, as part of kind of inserting these blocks, we don't actually mark these as fully safe because there's no guarantee that the block that you received will kind of get submitted to l One. So we prefer in the roll up node, whatever gets submitted to l One over what the sequencer says. And kind of as a result of this, it's a very minimal trust assumption to insert these blocks and stay up to sync with the tip of the chain.
00:42:05.740 - 00:43:03.836, Speaker C: They have a really good user experience without actually sacrificing trust. And this is kind of a similar thing to the liveness in Ethereum where you have a head block which could reorg, but then you have your safe block which is assumed not to reorg, and then you have your finalized block which will never reorg. And so we've brought over the same kind of terminology and same usage of the engine API to mark blocks as safe, which are very likely to not reorg finalized, which will never reorg. And then your head block which you've got from the sequencer, which is likely to be included, but it could change. And that's kind of the whole P to P subsystem. And then as part of a result of getting these blocks as they're distributed, it's really easy to hook into a snapseed. It's not yet implemented because you can just re execute all the blocks.
00:43:03.836 - 00:43:16.210, Speaker C: But that's something where this architecture of splitting out the execution engine from the consensus layer, that's something that's really easy to do and something we will be doing in the future.
00:43:18.580 - 00:43:19.136, Speaker A: Cool.
00:43:19.238 - 00:43:53.730, Speaker C: So we've covered the roll up node, talked about Op geth and the op node, how they interact and how different nodes interact with each other on the network. And now we're going to talk about the sequencer and the batcher sequencer and the batch. So what's here is basically talking about the sequencer. It's a normal node, essentially. It runs a mining process. And just like Ethereum, how every full node has mining code in it, every roll up node will also have mining code in it. And it's just one specific one that can run.
00:43:53.730 - 00:44:41.432, Speaker C: And then the batcher is a pretty simple code base. Just takes l Two blocks, transforms them to the data that you expect, does compression, how you get low fees and then just submits them onto L One, easy peasy. And then having that be so easy is basically like small modular architecture that's the theme here. So now the proposer, fault proofs and challenge agents. So this is a group of kind of three, which will be at least three different code bases and a couple of different agents. But what's here is this is everything that's required to enable withdrawals. So the proposer is kind of the entry point into withdrawals.
00:44:41.432 - 00:45:21.820, Speaker C: So it reads State from L two and then puts it onto L one. And then after, if you put it onto L one and no one says, this is invalid, it's then assumed to be valid. That's the optimistic part of our roll up. And that kind of validity then enables withdrawals. And if it's not valid, then you go through the challenge game and the fault proof. So it's not yet live. See some Canon talks that norswap's given for more information about this.
00:45:21.820 - 00:46:30.176, Speaker C: But the fault proof is what secures the bridge. This is how you say someone submits something that's invalid as a proposal. The fault proof is what marks it as invalid. And we time bound. How long you have to say, hey, this is invalid, so you can actually eventually withdraw, actually eventually withdraw, but it still needs some amount of time for the finalization period to basically have enough time that no one can censor the chain for long enough with this withdrawal period. So if it's like a ten second finalization period, you could censor the chain and then get any fake output through with a week. You can't censor the chain for a week, and then if there's ever an invalid output proposal, it will be caught and then so the actual fault proof game is super fun and exciting, and I'll nerd out a little bit over it.
00:46:30.176 - 00:47:14.268, Speaker C: So the entire thing is called Canon, and it has its whole own sub architecture. It's like a lot of interesting things in there, but it's an interactive proof game over our MIF's execution chase. So, like everything here, it's a lot summarized into a short sentence. So the interactive proof game is kind of this binary search like thing. So you start out with some code. You execute the code, and you can imagine the code is like executing an instruction, like an ad, a subtraction, a load from memory. And so what we do is we have kind of an on chain version of this.
00:47:14.268 - 00:48:23.104, Speaker C: And then your two off chain components, the two off chain people, both run their execution chase execution, your challenge and your defender. So the defender is the person that submitted the output proposal, and your challenger is the person that says, no, this is invalid, and you run the execution trace and you agree on the start and disagree on the end. And then you can kind of mercalize the state, do some funky things in there, and then basically run a binary search over, like, okay, we agree at this step, we disagree at this step, and then you can finally narrow it down to single instruction execution and then execute that single instruction on chain. It's super fun, super interesting, and then actually enables you to kind of fault proof really complex things really easily. So anything that's deterministic and compiles to MIPS can be fault proven. So the whole derivation process is going to get imported to Canon. All of the EVM is going to get imported to Canon.
00:48:23.104 - 00:49:01.010, Speaker C: And we'll kind of go through the fault proof. Any compression, we do again, just straight through so we don't have to deal with loading up data. We can load in the raw data that the roll up node uses, which is, again, just transactions and blocks. And so those are really easy to kind of load up on chain and prove that, hey, this is actually the block hash on chain. And so that's basically all of that. Super fun, super interesting. Not yet live, but we're working a bunch on that.
00:49:01.010 - 00:49:34.312, Speaker C: Okay, getting back here. Okay, so that's how we secure the system. So I'm going to go through some data flows in the system. You'll see how kind of different things interact. And maybe I'll make it a little more concrete. So we'll talk through the deposit, l Two transactions, and we'll also go through withdrawals in a little more detail. So the deposit, you send a transaction to a contract.
00:49:34.312 - 00:50:13.450, Speaker C: And the key thing is that the contract does some computation. Make sure you pay your fees, make sure you lock up your ETH, and then it emits the event. And that event is what we're looking for on the Lt node. That's the authorization that, yes, you locked up ETH. Yes, all of this is valid. Then the world node reads the event, creates a special transaction type on l Two, and then we insert these transactions at the start of the block, and then eventually that deposit gets executed on l Two. There's a couple more details in there around, like gas accounting, but not quite relevant for this talk.
00:50:13.450 - 00:51:13.550, Speaker C: So then if you're submitting direct to l Two transaction, you send out your transaction, it gets to the sequencer, and then the sequencer has a set of transactions in kind of its local mem pool. And then it creates a block based on that. Then once the sequencer has created its block, the batcher sees, hey, there's a new block. We need to go submit it to l One. So then it takes it combines it with a bunch of other blocks, does some data transformation, compresses it all, does a little more like data slicing, and then puts it on l One. And then the roll up nodes read all the call data, look for transactions to a specific address, check the authorization on those transactions, and then reassemble the blocks from that submitted data. And then you have voila your transaction on l Two.
00:51:15.520 - 00:51:15.980, Speaker A: Cool.
00:51:16.050 - 00:52:28.784, Speaker C: So withdrawals so withdrawals are a little more complex because you have to manually kind of do the communication across the layers. So to initiate your withdrawal, you send a transaction to l Two to the withdrawal contract. And so the withdrawal contract does several things, basically creates a commitment to how much you're withdrawing, then burns how much ETH you said you withdraw emits event to make it easy to index and track it. And then it also touches some l Two state based on kind of the commitment to the withdrawal. And this touching is really important because we'll use that later to verify that you actually burnt your funds on l Two and can kind of withdraw them from the optimism portal on l One. So after you do this, then you have to wait around for the proposer to say, okay, I've got this new output route, have the proposer submitted on l One. You have to go through the challenge period to say that no one's making a fake withdrawal.
00:52:28.784 - 00:53:15.392, Speaker C: And then finally you can execute your withdrawal on l One. And so what you're doing there is verifying the withdrawal contract or the optimism portal on l One, when you're doing withdrawal, is verifying that you have a valid proof of your kind of commitment to your withdrawal. So, like you have a withdrawal that says, I want ten E back. It says, hey, you actually did have an event or state on l Two. That shows that the only way the state could have been true is that if you had actually burnt ten ETH on l Two. That's like the proof system there. That's basically the overview.
00:53:15.392 - 00:53:54.712, Speaker C: There's a couple more details about assembling all of the proofs, but that's kind of highly mid level overview, not fully high level. So we have a couple data flows. Let's talk about how it's all assembled. So we've been talking about a bunch of different components, and these are a lot of small modular code bases. The op node is about 15,000 lines of code. The op geth, it's about an extra 1000 lines of code on top of normal geth. And we find that's kind of what it takes to get another execution engine onto optimism.
00:53:54.712 - 00:54:33.180, Speaker C: The op batcher and the proposer around 2000 lines of code as we kind of get those more production ready, grow a little bit, but it's likely to remain under 5000. The contracts around the portal, two contracts, all that's around 10,000 lines of code. But that's with all of our tests and supporting code. So none of these are giant hairy code bases that are all intertangled. They're all kind of off in their own little world. They have clean API boundaries between them and they're a lot easy to work with, easy to upgrade. And so that's been one of the big pushes with this new architecture.
00:54:33.180 - 00:55:00.100, Speaker C: So if you're interested in kind of looking at the code, learning more about it, all of our code is in the optimism monorepo. We have the Op node code base, the Op badger code base, the op proposer. Those are just top level folders. If you want to look at the Bedrock contracts it's in packages contracts, bedrock, self explanatory. And then we have specifications for how the entire system should work inside the specs folder.
00:55:01.320 - 00:55:01.924, Speaker A: Cool.
00:55:02.042 - 00:55:34.428, Speaker C: So in summary, bedrock is a network upgrade to reduce fees and increase implementation simplicity. And we basically talked through all of the architecture that enables this. So this was introduction to betterox architecture. Again, I'm Josh Aguto. Find me on Twitter at trianglesphere. And if you're interested on working with optimism, all sorts of positions there, but in particular, we have positions to work on betterrock. So if this is exciting to you, we're hiring at Jobs optimism IO.
00:55:34.604 - 00:55:35.280, Speaker A: Cool.
00:55:35.430 - 00:55:37.650, Speaker C: That's it for me. Have a good day, everyone.
00:55:42.990 - 00:56:00.980, Speaker E: Thanks, Joshua. Great presentation. Really appreciate it. Okay, so up next we have Norswap, who's going to be talking about modular sequencing. Nor swap welcome. Happy to have you. Thanks for being here.
00:56:03.670 - 00:56:33.190, Speaker D: Hey, everyone. There we go. Better. All right. And we are live. Okay, I'm going to be talking to you about modular sequencing. So I want you to know that I did not come up with the title of the talk, otherwise it would have been transaction Ordering is a Social Construct.
00:56:33.190 - 00:56:52.850, Speaker D: Who watches a sequencer? Mev is everywhere. Wow, that's a lot. Let's unpack that. First of all, none of this is future plans. We haven't decided what we're going to do about this stuff. These are just some thoughts and ideas to explore the design space. Okay? So don't make investment decision or whatever based on this.
00:56:52.850 - 00:57:33.866, Speaker D: Just don't do anything stupid, really. So modular sequencing core talked about how the Opie stack is modular, and just like everything else, sequencing is something you can change in the Op stack. Okay, so what's sequencing? Sequencing is picking which transaction to include in blocks and how they are ordered. Okay. And also who sequences these transactions. Right? And so who gets to pick and what do they pick? How do they choose? So first we're going to be talking about who and today it's us. It's Opilabs that runs the sole sequencer.
00:57:33.866 - 00:57:49.940, Speaker D: So we get to pick the order of transactions. In practice, we just pick them in the order Uri. First in, first out. It's very simple. We haven't done a lot of work on this yet. What could go wrong? No comments, but you get some idea. Censorships, all that bad stuff.
00:57:49.940 - 00:58:44.810, Speaker D: So what you want to do really, is decentralize the sequencer, which means having multiple sequencers that cooperate to come up with blocks. Okay, and so what are our goals when we say sequencer decentralization? There are two goals, really. One is liveness, such that if a sequencer were to go down the network, the L two chain keeps progressing anyway. Very important. The other one is censorship resistance. And that's like making sure that we Op labs cannot make your transaction be censored forever and that nobody can force us to do that either, right? The non goal of this is safety. Optimism is safe, but the mechanism that we use to ensure safety is not this, it's default proof.
00:58:44.810 - 00:59:21.940, Speaker D: Okay? So even if there's a single sequencer, then the chain is safe. Nobody can steal money. But decentralization is important nonetheless for questions of censorship, resistance in liveness. So how do we decentralize? Like everything, there's a short term view and a long term view. There are things that are easy to do and things are harder, but maybe more interesting. And so at first the idea that we're playing with is to have a low number of permission sequencers and to have them run round Verbin, round robin. So what that means is that there will be one sequencer, it would take care of a certain number of alt blocks, like ten or 100.
00:59:21.940 - 01:00:09.998, Speaker D: We need to figure that out and see what's feasible. And then it would be the turn of another sequencer who would do the same. Okay? And so if one sequencer is misbehaving or Censoring or things like that, e would only be able to do that for a certain number of blocks before another sequencer takes over. And if we diversify the set of sequencer well enough, we put them in different countries and we make sure that they have some vested interest not to misbehave. So they should be like known entities, they have some reputation or maybe they put a bond that the governance could take from them. If they misbehave, then we ensure that at least one of these sequencers is going to behave correctly and the chain will make progress and your transaction will be included. So there's another model for this which is interesting.
01:00:09.998 - 01:00:41.820, Speaker D: It's called meva. And meva stands for mev. Auctions we'll talk about Mev a whole lot in the rest of this talk. But Mev, if you don't know what it is, you can think of it as the arbitrage profit, right? If some token is cheap on uniswap and it's expensive on Velodrome, well, you buy it. Uniswap sell on Velodrome, you make money. Who gets to make that money? Well, the first person that notices this and does it and who gets to decide who's first while the sequencer. Okay, so sequencer is able to capture these profits if he's aware of them.
01:00:41.820 - 01:01:58.530, Speaker D: And so yeah, and so since there's a lot of money in this, basically one way that you could say that the sequencer could be decided among a set of sequencer is the sequencer that bids the most gets to sequence the blocks. Okay, there are questions here, right? You don't want to make it so that the same sequencer wins every single auction. So you still need some kind of mechanism to ensure that this does not degenerate in a really bad scenario, but otherwise it's a pretty solid one to decentralize. And also make sure that we capture some value through mev. And I'm going to come back to mev later. All right, so another model that's interesting is that we could just go and say, well, optimism will behave like a small blockchain, like a blockchain that's not very decentralized, like some of the blockchains in the Cosmos ecosystem or even solana. And then we'll run some consensus on all these sequencers and they will come up with a block and then that's the block that will propose to the L one chain.
01:02:03.030 - 01:02:03.442, Speaker A: Yes.
01:02:03.496 - 01:02:36.222, Speaker D: Let me see if I need to add something on that. I think I've covered everything. Yeah. And the thing that this small blockchain is not very decentralized doesn't matter because like I said before, the safety comes from the fault proof, okay? So even if you have a 51% attack, it doesn't matter. It's just a way to ensure that again, the network is live. So even if nodes go down, it still works and nobody can censor because this mechanism is going to pick different people to propose the block each time. And there are many existing consensus algorithm we could use for this.
01:02:36.222 - 01:03:21.946, Speaker D: We could even reuse the proof of stake algorithm from ethereum. Or we could use something simpler like tendermint. The reason we don't need to reuse the algorithm from Ethereum is that the ethereum proof of stake algorithm is made for a huge number of validators, right? And we're only dealing with a small number, at least at first. So I've been saying, yeah, sequencer can misbehave, they can do things they shouldn't do. And so I want to list the ways in which the sequencer could misbehave. So a sequencer can lie and just include transactions that don't work. If they do that, if they submit bad transaction, they will just be ignored.
01:03:21.946 - 01:03:55.420, Speaker D: Basically, other sequencer will just read them and say this is bullshit, it doesn't work. And so they'll be ignored. The other role of a sequencer so one role of sequencer is to pick the transaction and propose them. The other role is to submit output routes. Output roots is a commitment to the result of executing your transactions. And if a sequencer submits bad output routes, then the fault proof can prove that these are bad and the sequencer will be slashed and so it will lose the bond that deposited. So it's economically unsound to do this.
01:03:55.420 - 01:04:55.834, Speaker D: The other things that can go wrong, the sequencer can go down, meaning that he will not do his work. What it's supposed to do, it can censor and then it can misbehave in term of mev and we will see later what that means. But say we decide that optimism we don't want sandwiching, okay? If you don't know what it is, I will explain it later. Or think front running if we decide that we don't want that but the sequencer does it anyway because technically he's able to then what we can do in those cases where a sequencer is down all the time, he sandwiches. When we said that we don't want that or he censors, then governance can take some action and just ban the sequencer, basically. And that works if the set of sequencer is permissioned, right? If you had to apply to become a sequencer and you had to be accepted, then we can ban you and then you can reapply. If it's permissionless, we can't really ban you, right? You'll just go to another server, get a new IP and then join again.
01:04:55.834 - 01:05:34.262, Speaker D: In that case, it's necessary to have a bond that governance can slash and basically take your money. Okay, like you saw, there's a lot of ideas on how we can do this. But the point I want to make is that all these models are compatible with our current architecture. We can just slot them in. And so that's very exciting. We have a free rein to come up with the best solutions in the short term and then in the long term. All right, second topic on modular sequencing, what can we do? Sequencing? Well, how do we pick the transaction? And surprise, this is a talk about mev.
01:05:34.262 - 01:06:04.862, Speaker D: Okay? You might have heard about mev, you probably have. But just in case, what is mev? Mev is value that can in theory be extracted by the actor. That's the power of ordering transaction. So in ethereum proof of work, there was the miners in proof of stake, it's the validator. And on layer two, on optimism, it's the sequencer. Some people also call that maximal extractable values, but whatever. So always mev extracted.
01:06:04.862 - 01:06:28.620, Speaker D: And this is a copy. Okay, there we go. So how do you extract mev? So there's different kind of mevs, and we're going to review the different major kinds in a bit that will make it much clearer. But let's take the example that was before of an Arbitrage. If nobody knows that the Arbitrage exists, you can just send a transaction, do the Arbitrage. Boom. You won.
01:06:28.620 - 01:06:54.594, Speaker D: Except people might be watching the mempool. If it's public, they can see your trade. They can see this is a profitable trade. What if I do it before this guy? Or I put higher gas fees so I will be included first? And so people can actually steal your transaction. This is called front running. So in general, you will need to outbid other people. So in ancient times of the HRL Blockchains, people did exactly this.
01:06:54.594 - 01:07:39.230, Speaker D: They did something called Price Gas auction, which means they put a really high gas price to be the first in the block. And the miners would include them based on the gas price. Then came along a little company called Flashbots, and they released a software called Mev Get. And they offered Mev Get to all the big mining pools. And basically how it works is if you are someone that wants to extract mev an Arbitrator, and now we call them Searchers mev searchers, you will send your transaction to Flashbots. Flashbots will relate them to the miners. And miners will include them in their blocks based on how much you pay the miners.
01:07:39.230 - 01:08:19.982, Speaker D: A very important thing the Flashbots does, compared to precious auction, is that it guarantees that if your transaction is included, it will not revert. Which means that basically, you can send a lot of money. You'll basically check if you get the arbitrage, and if you get the arbitrage, you send a bunch of money to the sequencer or the miner, et cetera. Another thing that Flashabot enables is that it enables you to send not only transactions, but also transaction bundles. And so you can see a transaction in the Mempool, and you can put trades around. So before and after that, and that enables you to do something called sandwiching. And we'll see that in a second.
01:08:19.982 - 01:09:00.326, Speaker D: And people don't like that. Usually this system has a flaw that is not often discussed, is that all the mining pools that run Mvvgf, they can see all the trades sent by the searchers, okay? And so if they wanted to, they could front run all the searchers. They could just steal their trades. They don't do this because they know that if they do this, they have a lot of reputation. And then if they're found out, they might be kicked out of Flashbots, which will end up costing them much more money than they can hope to make by basically stealing a transaction. But they could, and this will be important later. Now we switch to proof of stake.
01:09:00.326 - 01:09:46.250, Speaker D: And in proof of stake, flashbots have proposed a new system called Mev Boost. Mev. Boost is something called a proposal builder Separation system or PBS. And basically you get a new entity, which is a block builder that builds the block, and then you got the block proposer, and that's like a validator. So if you're picked as a validator in Ethereum proof of stake, you can ask block builders to bid for the rights to pick the transactions in the block. And then the block builders will now act like the mining pools in the previous model. So they will probably run Mevgaf or something like that to get transaction from the searchers.
01:09:46.250 - 01:10:46.830, Speaker D: So why did we change this model? The reason is that after proof of stake, no matter what people say, validation on Ethereum is much more decentralized. There are a lot of small staking pools, there are a lot of small stakers, and we want them to be able to make money. If they get to pick a block, they should also get the Mev money. Unfortunately, you cannot trust everybody with the transactions, right? Because anybody that can see the transaction from freshbots, from Mvgas, they could steal these transactions. And this is okay if you're relying on like six mining pools, right? It's a very limited set of people. But if you have thousands or even tens of thousands of validator, it becomes almost impossible to say who stole the transactions. And so instead, you will now centralizing the trust into these block builders and all the validators can contract the block builders.
01:10:49.170 - 01:10:49.678, Speaker A: Okay?
01:10:49.764 - 01:11:23.986, Speaker D: So now let's like what's Mev? Let's see a few flavors. Arbitrage. If there's a decentralized exchange where price is low, you can buy there and you can sell it higher on other exchange. Very simple. Back running simply means capturing some financial opportunity right after the transaction that creates it, right? You want to be first basically. And how to be the first is to be right after. And so the biggest example of this is on lending markets, there might be liquidations because the price has dropped, the price of the collateral dropped.
01:11:23.986 - 01:12:05.830, Speaker D: And so these lending platforms want you to take over the collateral for a price in stablecoins or something like that and you will sell it, but they will leave you a margin which will be your profit. And so people will compete to be the person that liquidates these loans. So that's a case of back running simply for NFTs, if it's completely open, mint the first people to mint get the NFT. And so that's a race. You want to be first to mint the NFT. Generalized front training. We saw an example when we say that pools can steal traits from other people.
01:12:05.830 - 01:12:40.594, Speaker D: So basically you'll look at the Mempool, you'll look at all the trades, right, and you'll try to see, well, if I were to do the same transactions, could I make money? And if the answer is yes, then you just do that. You just do exactly the same transaction. You will just bid more than the other guy and then you will take the profit. So that's called generalized front trading. Generalized because you're trying this strategy over every single transaction in Mempool. Then we got sandwiching. Sandwiching is kind of complex, but it goes like this.
01:12:40.594 - 01:13:19.120, Speaker D: A user makes a big trade, say on Uniswap or something like that. So say he wants to buy ETH with USDC. The Mev searcher sees this, sorry. And so what it's going to do is going to do the same trade but beforehand. So it's going to buy ETH for USDC, which pushes the price up. Then the user is going to make the trade, he's going to have a worse price. And so that also pushes the price up, right? So if the initial price was X, after the sandwichers buys it's express A, and after the user buys it's express A plus B.
01:13:19.120 - 01:14:02.026, Speaker D: And then the sandwicher turns around and sells what he bought in the first step, okay? And basically is able to sell that at a higher price that he bought because the user pushed up the price. And so basically he's making the price worse for the user and he's pocketing the difference. And so that just makes life shitty for you. If you're making a big trade and people don't like this a lot. There's also something called just in time liquidity. I'm not sure I'm going to have time to talk about that. It's a little bit complex to get into all the nuances, but this slide is there and I will link the slides later and maybe with time we'll come back to it.
01:14:02.026 - 01:14:49.358, Speaker D: I think I'll leave it if there's some questions about it, basically. So mev is big business. So this is a screenshot from the Flashbots Explorer, which I took in August, I think. So this is not entirely new, but it gives you an order of magnitude on these things, right? Like on a month, eight, 5 million in 24 hours, about six k and in total, and I'm not sure how long this tracking has been going on, but like 600 million. So this is a massive money that's being made here. It's also highly asymmetric. Some blocks will have massive mev and some blocks will have very little mev.
01:14:49.358 - 01:15:21.418, Speaker D: If you multiply 24 hours, the 24 hours number by 30, you don't get the eight 5 million. So it's very asymmetrical. About 33% of this mev is paid to miners. So this is not a lot, right? You would expect people to know the opportunities and basically bid a lot to the miners, like up to 90% in practice. We don't see this yet. Maybe we'll see that in due time. There's been eight introduction extracting more than 1 million in MVV.
01:15:21.418 - 01:15:46.486, Speaker D: It's interesting. And the biggest one has actually occurred on L Two. There was some 5 million plus Arbitrage on Arbitram a while ago. So mev on l two already exists. Even though there's no flashbots, even though there's no auction, there are arbitrages to be made and people will make them. People will make the money. So mev good is mev bad.
01:15:46.486 - 01:16:17.298, Speaker D: People often say mev is bad. That's a naive take. Someone will profit from arbitrage. Like I said, if there's a discrepancy in price, someone will pocket that profit. And I think personally, it might as well be the network that enables those things. And it might go to public goods and to a whole lot of things that benefit everybody, basically, instead of just benefiting some guy that runs algorithmic trading or some kind of mev software at home. He's not redoing.
01:16:17.298 - 01:16:52.818, Speaker D: Like Arbitraging is useful, but there's no reason you should be paid a huge amount of money for it. Right? So sandwiches are almost purely zero sum. Someone that does a sandwich is not providing any value on the network. It is possible. So there's been a paper published that says in some cases it could be better with sandwiching than not. I really doubt it is relevant in practice, but in theory you could construct these scenarios. I mentioned this just to be complete, but my conviction 100%.
01:16:52.818 - 01:17:23.318, Speaker D: The word would be better off without sandwiches. Nobody likes front running, just people. If you smell an opportunity, make a good trade, someone slips from you, it's kind of shitty and just in time, liquidity. I didn't get into details. But basically this one is good for users, but it's bad for liquidity provider. And in turn, that makes it bad for user in the long term. So preventing mev is dumb.
01:17:23.318 - 01:18:12.310, Speaker D: People say we should just prevent mev. You can prevent arbitrages. Basically one way that some people try to prevent mev is through this thing called fair ordering, which is first in, first out. And this is not fair at all because it privileges organization that can build hardware or infrastructure to reduce latency of mev extraction. And so what's fair in that? Right before you had at least at a competition between guys, it could be at home, and you came up with the cleverest solution, and now you have the same thing that's as flawed, but also only rich organizations can do it. This isn't great. And if you do this, you still have a problem that people will try to start spamming to capture opportunities.
01:18:12.310 - 01:18:47.558, Speaker D: This is not hypothetical. It's happened on Ethereum free flashbot, it's happening on Salana, it's happening on Binance smart chain, and probably on L two. To some extent, though, not quite as bad. So knowing all this now, we've had a big picture of mev. It's not good, it's not bad. We need a solution for it. How do we make sure that we capture all this money for the protocol, for the public goods, for the ecosystem? And there are a few solutions for that.
01:18:47.558 - 01:19:25.502, Speaker D: So yeah, the goals capture the value for the protocol, as I just said, make the user experience good for users. And so for me, that means no front running and no sandwiching. This is sometimes something people argue with. They're wrong, like fight me. Maybe we'll get some interesting questions around that, but basically this works pretty well and it is actually feasible to achieve that. And I'll talk briefly about how we can do that and then decentralize effectively. So all the MVV solution that we came up with must be compatible with sequencer decentralization.
01:19:25.502 - 01:19:58.910, Speaker D: As we talked before, it's extremely important to decentralize. So how do we do it? Basically, we want to run some auctions just like flashbots. One thing we could do is just run MVG gas on the sequencers. The thing is that with bedrock, we'll have two second block time. Okay? And so two second is not a lot of time. And if you have to people need to send their transactions to MVV gas, need to simulate the blocks. You'd send that to the sequencer, maybe the latency starts to add up and 2 seconds is a little bit too tight.
01:19:58.910 - 01:20:33.138, Speaker D: What you could do instead is do meva. So that's mev auctions via something like mev boost, mev boost basically does MV auction actually. And then you would sell basically the rights to sequence more than one block. You could sell right to sequence ten blocks or 100 blocks or something like that. So that's for the mechanism. So making auctions make sure the protocol captures value. And then we could also impose some rules on how mev can be captured.
01:20:33.138 - 01:21:11.590, Speaker D: And so the simple and effective rule is no front running. If you can't front run, you can also count sandwich and you can sort of enforce that on the short term, you can have governance oversight. So basically we say to the sequencers guys, it's not allowed to front run. And if they Vlad this rule, then we can slash them or we can ban them just like we discussed before. This is not great, right? Because we have to monitor what the sequencer are doing and make sure that they don't do this. And then we have to do a governance vote. It's a lot of drama and people need to vote, et cetera.
01:21:11.590 - 01:21:44.094, Speaker D: I'm of the opinion that generate governance should be minimized whenever possible. In this case, it is indeed possible via technical solutions. Okay? And these technical solutions basically work in two steps. The first step is to remove the power of the sequencer to order transactions altogether. And there are basically two solutions that are being talked about in the space right now. The first is a first Universal Oracle. So Chainlink has a solution, identical fur sequencing services.
01:21:44.094 - 01:22:05.080, Speaker D: Again, this has nothing to do with Furnace boo. And the other one is called Threshold Encryption. And that's a solution that a company called Shutter Network is working on. And there may be other people working on that. Yes, there is another company whose name I forgot. Entropy. Maybe entropy, but I'm not sure.
01:22:05.080 - 01:23:00.390, Speaker D: But in both cases, basically you have a networker node that provides an ordering for the sequencer to respect, right? In the first in, first out Oracle. How that works is that all the nodes in that network, in the first sequencing network, they will receive a bunch of transactions and they will give their ordering of transactions. And if a majority of them are honest, then it's impossible to front run. Basically, in the Threshold Encryption case, you will send your transaction encrypted to nodes in the network and they will make an ordering of the encrypted transactions. Nobody knows what's doing what the sequencer will sign on the ordering. And then the network, the Shutter Network, for instance, will come together, create a key to decrypt all this transaction. Then you get them decrypted after the sequencer has said yes, for sure, I will use this ordering.
01:23:00.390 - 01:23:40.130, Speaker D: So that's how you take the power away from the sequencer. Now you cannot order anything. And so as a user, if you use this solution, you know that you won't be frontrun or sandwiches or anything like that. The next step is to reintroduce some form of auctions because we still want to capture mev and we still want that value to go through protocol and to public goods. How you do that is that you conduct two kinds of auctions, ideally at top of block auctions for things that should go at the top of the block. So, say last block after the transaction will reveal you're like, oh, there's a massive arbitrage now. Well, you can bid to be on top of the next block.
01:23:40.130 - 01:24:25.666, Speaker D: And then some transaction also creates a massive mev, just like, let's say Oracle updates. And so this could be in a public pool and people could bid to backtrack them. The reason why you could say, well, why don't we just put the Oracle updates encrypted and then we reveal them. And then people will bet to be on top of the next block. That doesn't quite work. The reason why it doesn't quite work is that people will know the Oracle is coming and they will start spamming to be by luck after it, right? They'll just spam a whole lot and then they will get to be in the same block after the encrypted Oracle update. So you have to do something special for them.
01:24:25.666 - 01:25:03.374, Speaker D: You have to allow background auctions, basically. And I think this is the end of okay, indeed the end of our presentation. So some conclusion there. So the Op stack in Bedrock is the first incarnation of the Op stack. They are flexible and you can easily change sequencing and we will indeed work on that in the future. And the goals with changing sequencing would be to decentralize for liveness, for censorship, resistance, and to extract mev so that we accrue value to the protocol and the ecosystem and the public goods. What are best solutions? Well, we're still thinking about that.
01:25:03.374 - 01:25:32.514, Speaker D: We're still talking to people in the industry. If you are smart and you have some ideas there, feel free to message me, I love to talk about that stuff. And there are short term and long term solutions. On short term, we can use governance as a crutch to say, hey guys, behave or else. But in the long run, there are technical solutions that we can implement that make sure that no matter what, you can't misbehave. All right. I've been Norswap.
01:25:32.514 - 01:25:42.300, Speaker D: You can find me basically everywhere. My handle is Norswap, including on Twitter. And yeah, if people have questions, I think we have a tiny bit of time.
01:25:43.470 - 01:25:45.466, Speaker A: Hey, Neuroswap, good to see you, man.
01:25:45.648 - 01:25:47.034, Speaker D: Hey there, how's it going?
01:25:47.152 - 01:26:11.880, Speaker A: Good, yeah. Thanks for your talk. There were a couple of questions, so I'll just kind of ask them to you here directly. First question here is just about kind of the fraud proofing and how that works. Is it somebody individually is submitting that there's kind of a fraudulent thing that's happened? And if that's the case, then how would you go about preventing people from front, running the fraud proof, basically, and getting the reward for doing so?
01:26:13.050 - 01:27:00.786, Speaker D: That is such a good question. I love it. Yeah, that's a difficult question. I think the disappointing answer is that you would go through something like Flashbot that has sort of a trust agreement not to frontrun because otherwise it's really difficult. I've thought about this a little bit, like, we're not quite there yet, we still need to build, actually the technology of the fold proof fully. And this economic part comes even after that. But I've started a little bit with trying to compensate people that contribute, but it's really difficult because then you can just sort of copy what our people are doing without doing your own work and you don't want that either.
01:27:00.786 - 01:27:03.300, Speaker D: So it's a real problem.
01:27:05.030 - 01:27:30.320, Speaker A: Got you. And then another question that came up was and I know we have 1 minute left, which might not be enough time, but you mentioned as one of the ways to deal with cat is here to deal with mev is sort of threshold encryption is one of the kind of the ideas that's been put out for managing this. Can you maybe chat a little bit more about the interaction of Threshold encryption and mev and kind of how that might be a way to mitigate it?
01:27:31.730 - 01:28:29.840, Speaker D: Yeah. So Threshold encryption, basically, alternative encryption, I should first explain is you have this private key and you split it between all a bunch of people and in this case the nodes of a network, but the public key is known to everybody. And so as a user, you encrypt your transaction with the public key and only when the nodes come together to create the private key can you decrypt the transactions. And so it's a way to basically remove the power of the sequencer to order a transaction because now you are ordering encrypted transactions, you have no idea what's going on there, and then the sequencer will commit itself to them and then later you can reveal. And so it's a way to make sure basically you can't front run or sealed opportunities or sandwich and feelings like that. And if you want to extract image, then you need to introduce auction, but you can do that in a way that you can't do these bad things, basically.
01:28:30.930 - 01:28:32.786, Speaker A: Got you. Awesome.
01:28:32.968 - 01:28:33.682, Speaker F: Okay, cool.
01:28:33.736 - 01:28:55.210, Speaker A: Well, thank you so much for being here and running through your talk with us, modular sequencing, which was actually just a secret mev talk in disguise, as almost all of them are. But yeah, thanks for being here, Norswap. We appreciate you taking the time. And if anybody else has any questions, feel free, as Norswap mentioned, to grab them at any platform at Norswap.
01:28:55.870 - 01:28:58.620, Speaker D: Cheers, thanks a lot, love that. See you around.
01:28:59.070 - 01:28:59.930, Speaker F: You too.
01:29:00.080 - 01:29:00.586, Speaker D: Awesome.
01:29:00.688 - 01:29:15.120, Speaker A: Up next, we're going to have Proto come on and chat about pluggable data availability. We're really excited to have the king of the Pineapples here. So, Proto, when you're ready, feel free to turn on your video, unmute yourself and share your screen.
01:29:30.900 - 01:29:32.530, Speaker D: Hey, Proto, are you there?
01:29:47.620 - 01:31:09.476, Speaker A: Looks like Proto just dropped off, so we're just going to go to a quick break while we get them back in here and yeah, we'll be right back. Thanks, everybody. Awesome. I think we're back. I think. Proto zoom crashed. Proto, are you there?
01:31:09.498 - 01:31:10.070, Speaker D: Now.
01:31:13.420 - 01:31:14.840, Speaker A: Free to unmute.
01:31:20.220 - 01:31:20.956, Speaker D: Oh, there we go.
01:31:20.978 - 01:31:38.170, Speaker A: But we can't hear you, unfortunately. Maybe it's the wrong audio source. Still can't hear it. I'll let you know if I can.
01:31:46.920 - 01:31:47.990, Speaker F: There we go.
01:31:48.440 - 01:31:49.190, Speaker D: Test.
01:31:49.720 - 01:31:51.348, Speaker A: Yeah, you're coming in clear.
01:31:51.514 - 01:31:52.132, Speaker F: Awesome.
01:31:52.266 - 01:31:54.872, Speaker A: Feel free to screen when you're ready as well.
01:31:55.006 - 01:31:55.400, Speaker D: Yes.
01:31:55.470 - 01:32:29.614, Speaker F: I hope the zoom does not crash again. You're coming in. There you go.
01:32:29.652 - 01:32:30.014, Speaker A: Looks great.
01:32:30.052 - 01:33:05.418, Speaker F: Hello. All right, let's get started. Sorry for the delay. So my presentation is about applicable data availability. With optimism, we're both stewarding layer one development of increases in data fed ability, as well as building the technology that uses data fed ability called rollups. And so I'm going to expand on Fire for Fire, also called prototype sharding. That's me.
01:33:05.418 - 01:33:58.540, Speaker F: And with Op Labs, we're building this roll up technology and building towards the bedrock release, which uses this data fed ability. So what does this talk about data fed ability? Layer two usage forward for Far and then some layer three design. At the end, you can plug and play and build something new. So what is data fed ability really? Data fed ability is this primary scaling bottleneck of ethereum. Over time, the sharding roadmap of ethereum has changed a lot. Early on, sharding looked like this thing where we had many, many different shards and they would all have data as well as execution. Over time, we realized that having so many shards would create communication problems if you also try to solve the exclusion problem.
01:33:58.540 - 01:35:12.722, Speaker F: And so instead of trying to solve everything at once, we have this realization where on layer one we can just focus on data, securing data and enabling layer twos to create this competitive environment where the execution layer essentially is out of protocol, but secured by layer one. And so how does this work? The inputs to the functions are secured by layer one. The outputs are a layer two concern. And so what we get from securing the inputs is this permissionless ability to reconstruct state. And this allows any Hanas actor on the layer two to compute odd state and then contest what the layer two output is on layer one and then secure the layer two. So a roll up is really just this combination of data availability and this execution check the thing that secures the withdrawals. Going back from layer two to layer one, we have various different types of exclusion checks.
01:35:12.722 - 01:36:29.566, Speaker F: They're also called validity proofs or fault proofs or previously fraud proofs. So if the sequencer makes a mistake or if there's a malicious sequencer, they might post the wrong output. They might post a state that's not actually valid, and then you have to prove them wrong on layer one to secure the layer two outputs and secure withdrawals. Secret roll ups have a validity proof where you prove with secret technology that the roll up performed a certain state transition. Given layer one inputs, optimistic roll ups, they defer the proof and basically only when there's contention in the Pessimistic case, then you play this interactive game on layer one to prove or disprove the output. So with layer two, we only really need this data availability for other honest actors on the network to reconstruct the state. Reconstructing the state is very easy if the state was only ever changing like once per month and very, very slowly, and everybody agreed on the state.
01:36:29.566 - 01:37:28.930, Speaker F: But the challenge is that layer two is quickly changing, new transactions are being confirmed every second. And so all this data that's changing needs to be made available in a permissionless way where we do not rely on the sequencer. And this is what layer one offers. This is data availability, hosting data that's changing and that cannot be relied on from just one actor from the sequencer, but should be available to everybody. So if you think about modular blockchains, there's this narrative coming along, we're taking apart the stack and you see this on layer one with the separation of consensus and execution. And on our blockchains, this enables us to encapsulate complexity and to enable scaling. So, abstractly, this looks like we have a data provider and this layer two for derivation execution.
01:37:28.930 - 01:38:22.260, Speaker F: This is the simplified version, a little bit more complicated is the reality. So we have deposits going or like layer one messages that change layer two state from layer one to layer two. And then there is this referrals. We have proposal transactions which you can think of as an oracle, to state what the layer two state is and to enable withdrawals. So we have both data communication as well as this communication for Liveness and for this messaging across layers. And then the layer two is designed the same way as layer one with a consensus layer and an execution layer. And so if you think about the layer two state transition function, it's really just a process of incorporating more layer one data to extend to layer two.
01:38:22.260 - 01:39:21.506, Speaker F: And then we can prove those layer two outputs on layer one. And so often I see confusion about the proof of stake finalization and the roll up finalization. Before we had proof of stake, we used to use finalized for layer two data, that is proof that is proven on layer one. But now that we have proof of stake, we also have this notion of finalized data on layer one. So if you think about a roll up as a function that consumes inputs well, then the function is not going to refer if the inputs are not going to referred. And so when the inputs are confirmed on layer one, then the layer two can also be finalized. But it will take some more time for the layer two to also be secured on layer one for withdrawals.
01:39:21.506 - 01:40:09.186, Speaker F: This is this light client type of thing that the Oracle I was talking about to enable withdrawals, they're separate things. And we're focusing here on just vendorization of data because we're talking about data availability, securing what is confirmed. I'll give you a little bit more technical fuel. So we've been running testnets of the next petrol upgrade. Here's an example of a testnet with two replicas and one sequencer. And then what we're tracking here over time, over the span of 3 hours, is the traversal of the layer one chain. These are these colored hashes and every color hash is a different layer one block.
01:40:09.186 - 01:41:07.586, Speaker F: And then a little bit down the graph, you also see the layer two chain being traversed and the layer one references of the layer two chain being traversed. And so, over time, we consume new data and I'll talk more about this data availability. As we're consuming more data, we basically need more capacity to host more users. And so how do we get more capacity? We need to scale the layer one data availability. The Sharding roadmap has been split up into incremental phases. This has been done before. And now for the latest sharding roadmap, we have this idea of Dunk sharding based on Dunk Cloud from the Ethereum Foundation, basically championing this idea of data availability sampling, where you can distribute data more nicely between many, many different network nodes.
01:41:07.586 - 01:42:36.310, Speaker F: For the short term, we do want data and data increase, but we are not there yet with the network layer on layer one to host a thousand times increase in data like that. Or we're not ready for data availability sampling yet, but we are ready for a smaller increase. And so this is what Fire for Four is about, is to make this future compatible increase in data availability, going from the approximate 50 to 100 KB data consumed per block now today, to somewhere closer to a megabyte to two megabytes per block of data for roll ups to consume. So this is what it will look like if for it for far with the contents layer exclusion layer. Then we have this new data layer essentially attached to the layer one, where we're hosting these blobs. And these blobs, they function as inputs to host the layer two EVM activity lifecycle of a blob or this piece of data that layer two transactions are confirmed in is like this where we have the user creating a transaction, the roll up operator bundling the transaction. The layer one transaction pool basically propagating the transaction until it can get confirmed.
01:42:36.310 - 01:43:30.840, Speaker F: And then the layer one confirming the bundle of layer two transactions as this blob of layer two content. We think of this as a sidecar. It's something outside of the regular beacon block, it's synced separately. But it's this condition where we ensure that the data is available along with the beacon block itself. And then the excretion payload of the beacon chain stays in layer one. But Blobs, they are pruned after they have been available for a sufficient amount of. Time for layer two to secure their network and then it's up to the layer two to persist the historical state, which basically only changes once per month for other layer two users to sync from.
01:43:30.840 - 01:44:54.988, Speaker F: So how do we adopt this kind of new, more scalable type of data? We have this derivation pipeline which transforms the layer one inputs into layer two outputs into the layer two blocks as they are processed by the excruciation engine. There are different stages that we designed as part of the Bedrock upgrade to modularize this transformation. And so traversal, retrieval and deposit processing, they're all separate processes. And we can swap out the data retrieval process for one that supports the new EIP. With Eve Berlin, we've been doing exactly this, this hackathon of just like a week ago or so, we built a prototype where we took optimism data, or the optimism roll up and implemented the changes necessary to use the four data. And so Prism we extended with a retrieval API to fetch the Blobs. Gaff now combines four and layer two DIVS so we can use all the new changes in the same code base.
01:44:54.988 - 01:45:40.452, Speaker F: And on the op node. We derive data from the blocks and submit new blocks that are being built by the sequencer. Then we have a full layer two deployment that looks like this. We have a layer one beacon node, layer one execution engine, layer two roll up node and a layer two execution engine. You can see the similarities here where they both have an engine API, a separation of consensus and execution. And then there are these helper modules that make the data, the inputs and the outputs available to the higher layer. And we can take this and we can stack it.
01:45:40.452 - 01:46:57.390, Speaker F: So you could think of a layer three that is basically just the same software as we run for layer two, except stacked on top of the layer two instead of the layer one. So this is like a thought experiment where what if we just reuse the same code to build a layer three and to make things even more scalable. Now, what you see here is that this only really works if the Blobs, the things that host data in layer three, are not bubbled up all the way to layer one, but rather hosted by this layer two with a more, even more scalable data layer. And so you could adopt EIP four on layer two to host a layer three. But it would also mean that you'd have to build this alternative, even more scalable data layer. And then, well, how does this compare to plasma? Or how does this compare to these scaling solutions that host their own data? Well, you could separate it from the layer two more where now you have the separate data module to fetch the layer three data from. It's less equivalent to what you would like to see from layer one technology.
01:46:57.390 - 01:48:02.130, Speaker F: But it also does the same job and you can still stack the rot node, the proposer, the exclusion engine. So all this software is the same, but then host the data also. So I've been experimenting with these different types of ways to further scale what we now know as a roll up, but then in the future as this modular system that you can reconfigure to become a layer three or plasma or so on. And this is what applicable data febility is about. Once you have this stack of modules, you can configure it the way you like and then build cheaper, better solutions based on the type of application you want most. If you want to learn more about Bedrock, the modular stack we're building, and the upgrade really of optimism, you can find this on Bedrock optimism IO. We also have a live testnet you could use.
01:48:02.130 - 01:48:21.370, Speaker F: And then there is EP four eight four, this layer one scaling effort which you can contribute to, which you can read more about on Epford for four and then contribute to on GitHub or in Discord. Any questions?
01:48:26.540 - 01:48:58.708, Speaker A: Thank you. Proto just checking the chat to see if there's anything that's come through. Doesn't look like it's so far. Might be worth just as we're going to jump into a panel about 48, 44 following this directly. I don't know, maybe if it makes sense to maybe chat a little bit more just as sort of a preamble to that panel about 48 four, what it's going to maybe enable and also just sort of like any other thing you think that's relevant to know ahead of that panel with Tim and the.
01:48:58.714 - 01:50:29.120, Speaker F: Rest of the team, right? With Fort for four, what we're introducing is this different type of data. I think the context that many people may be lacking is how roll ups today are hosted and how this compares to rollups in the future will be hosted today. We're using Call data, which is this type of data that passes through the EVM and was never designed to be used in this way. It was designed to be used as an input for a smart contract and then it happened to be hijacked in some way for roll ups to host and make their data available. In the end, this just means that this type of data does a lot more and it's a lot harder to maintain than the type of data that we really need. And so what we're trying to design is the type of data that can be hosted and available without being unsustainable, without growing inbounded the current EVM on layer one and all the inputs is this inbounded type of growth where the state, the history and so on is not sustainable to maintain long term. Only when you can prune or reduce the state, then this thing is sustainable to host on regular hardware.
01:50:29.120 - 01:50:52.330, Speaker F: And so with layer two, we're trying to find this balance where we're securing the layer two, but we're not creating this unsustainable resource usage on layer one. And so by redesigning this type of data feedbility, we can make layer one a lot more sustainable. We can secure layer two at lower costs and so we can increase capacity for users as well.
01:50:57.370 - 01:50:58.070, Speaker A: Got you.
01:50:58.140 - 01:50:58.758, Speaker C: Awesome.
01:50:58.924 - 01:51:01.430, Speaker A: Yeah, I mean that's good context, especially.
01:51:01.500 - 01:51:05.206, Speaker D: To know that right now, I think.
01:51:05.228 - 01:51:50.520, Speaker A: The most important takeaway there is that the chain isn't really set up to do a good job of hosting that data right now. And it's being almost kind of hacked in and really kind of the principle behind those change is actually doing it the right way versus the kind of like other way. Maybe a quick question while I have you from the chat that came through, a bit of a Roll up 101 question from Abhishek, just a question around data availability says, I understand that roll ups make a state route commitment to the main net. Then where is the actual data of the smart contract stored? If it's a roll up chain, is it completely down? If it's on roll up, basically, is it completely on the roll up chain? And if so, how do I access that data?
01:51:51.450 - 01:53:04.298, Speaker F: All right, so I think I should introduce this notion of blockchain state versus blockchain data history. What you could think of as a blockchain is just a sequence of transactions that has to be available or accessible to people to reproduce a certain state. And so layer two makes this sequence of transactions available by bundling them, compressing them and submitting them to this layer One, which can host them with higher security guarantees. And then it's up to the users to read all of layer One and then reconstruct the state. And then as you reconstruct the state, you can find your layer two, balances, your layer two interactions, everything you might need on layer two. And then when you want to withdraw from layer two, all you really have to do from here is to make the layer two output, the state route visible on layer One. And Layer One, unlike a regular node, does not have its own view of these things happening outside of the EVM.
01:53:04.298 - 01:54:15.030, Speaker F: And so rather than what happens in a full node where you can run this layer two function to compute the Layer two state, you need to prove the state with a light client. This light client is essentially a smart contract on layer One that can tell you the correct state of layer two to be used by other layer one contracts that can then process things like withdrawals. The Layer two state can be proven in several different ways. This is the differences I talked about at the start of my presentation with Zika roll ups, optimistic roll ups, and the case of optimistic roll ups. What we do is we always post the layer two output. That only when somebody disagrees with the output. Then we build this game where a challenger and the original poster of the output can battle it out on who's posting the right output and who's posting the wrong claim or output.
01:54:15.030 - 01:54:28.410, Speaker F: And so you can secure the claims of layer two output to always be correct and to just secure the Oracle and then secure the withdrawals.
01:54:31.150 - 01:54:57.394, Speaker A: Got you. Okay, awesome. Well, thank you so much, Proto. I think we're going to go to a brief break while we set up the panel for the state of 48 44. But, yeah, thank you for your time, Frodo, and it's always great to have you on Ebobile TV. And we will transition over to a quick break, so everybody will be back at 02:00 P.m., which is only about a couple of minutes from now.
01:54:57.394 - 01:58:43.400, Speaker A: So see you all there sooner.
01:58:43.500 - 01:59:07.630, Speaker E: All right. Hey, everyone, welcome back. So, up next, we have a panel on the SATA four eight four, and we have a really awesome team of experts here to talk to you guys about it. So we've got the legendary Tim baeko, terrence sao Mophi and Robert Bayardo. Welcome, guys. Thanks so much for being here.
01:59:08.000 - 01:59:09.310, Speaker G: Thanks for having us.
01:59:14.800 - 01:59:34.256, Speaker A: Cool. I guess to kick this off, maybe we could just take a couple of minutes doing quick intros, especially because four four four has been interesting in that it's mostly been spearheaded not by client team so far. Obviously, we have Terrence here who's on.
01:59:34.278 - 01:59:35.924, Speaker C: A client team, but yeah.
01:59:36.122 - 01:59:42.230, Speaker A: Do you all want to take just a minute or two and kind of share who you are and how you got working on 4844?
01:59:44.520 - 02:00:00.472, Speaker G: Absolutely. I'm happy to go first. So, my name is Roberto Bayardo. I work at Coinbase. I've been there for about a year now, and I'm not on a client team. Therefore, I was not as overwhelmed by the merge, I think, as everyone else. And we have deep interest in scaling ethereum.
02:00:00.472 - 02:00:11.730, Speaker G: And I was given the opportunity to participate in this for a bit and so jumped at the opportunity and hope to keep getting more involved as things go on.
02:00:16.020 - 02:00:17.568, Speaker A: Mophie, you want to go?
02:00:17.734 - 02:00:51.150, Speaker H: Yeah, go next. Hey, I'm Mofi Taiwo. I work for Op Labs, which is building the optimism roll up. And I started working on EIP four four four, sort of like, out of necessity way back, like, early summer, because it was critical for optimism to have something like this to keep roll up costs pretty low and, yeah, pretty excited, the opportunity to get this out the door and get it done.
02:00:53.440 - 02:00:55.980, Speaker A: Terrence hello.
02:00:56.130 - 02:00:56.492, Speaker I: Yeah.
02:00:56.546 - 02:00:57.004, Speaker C: So, hi.
02:00:57.042 - 02:01:27.864, Speaker I: I'm Terrence from Prism. Matthew Labs. I am mainly a core dev from the consensus layer, so I've been working around this space since 2018. I started working on Proof of Stake now it's known as the Beacon Chain, and last year I've been working on the merge and also a little bit of MVV stuff here and there. And I've been interested in 4844, actually, since. If denver this year, February, and I hacked a little something with Proto Lambda. So that was fun.
02:01:27.864 - 02:01:42.780, Speaker I: But yeah, I've been mostly watching from the side for the last few months because of the merge was slightly more important. But yeah, I'm actually really excited to start picking this up again and hopefully we can get this into Shanghai.
02:01:44.080 - 02:01:44.444, Speaker C: Nice.
02:01:44.482 - 02:01:55.250, Speaker G: Yeah, I think it's worth adding then I think when we say we're interested in the IP 44 four, I mean, we're interested in ETH scaling, right? And this is kind of the natural first step to getting where we want to go.
02:01:56.100 - 02:02:21.608, Speaker A: Yeah, absolutely. Terrence, you mentioned this first got prototyped around Eat Denver. And maybe do you want to actually take a minute or two and walk through how this relates to Dank sharding for people who have no context because we have this plan to scale Ethereum massively, like Roberto was saying, and Ford for Forest kind of one of the.
02:02:21.614 - 02:02:22.856, Speaker C: First steps towards there.
02:02:22.958 - 02:02:31.900, Speaker A: So do you maybe want to just take a minute or two and tell us what 4844 is and how it relates to full scaling on Ethereum?
02:02:32.720 - 02:03:13.130, Speaker I: Sure, yeah, happy to. So how do we scale? Right, so right now what we're doing with Bitcoin and Ethereum is just every node download every single data and to verify every data. And that's obviously now going to scale. Right. And without this rob central roadmap, essentially, with our future roadmap, it sounds more like that we will essentially put the data on chain and then run the execution off chain, basically. But in the event of fraud, the execution needs some sort of data to basically prove that where the fraud exists. Right.
02:03:13.130 - 02:03:51.764, Speaker I: Usually how it works is that you will have sharding basically means that not every node has to download every single data, but with sharding becomes data availability sampling, which may take a few years to resolve. And four a four four is essentially the stepping stone. Basically say, hey, we will essentially black box the data availability sampling aspect and just make every node download the data as well. But it's a nice future stepping stone for the future, right?
02:03:51.802 - 02:04:42.772, Speaker A: So it's almost like today all the nodes download and execute everything. With roll ups, you can have the nodes only download the data, but not execute every single transaction that happens on the roll ups. Eventually we'd like to get to a spot where the nodes don't even have to download all of the data that got it on the network. They can only kind of subsample part of it and know that it was correct. And four four four is saying we're going to add more data capacity for these roll ups but still get everyone to download everything. Is that roughly right? And like you're saying, this kind of came about earlier this year and got hacked on at East Denver. And after that Mophie, you were like one of the people to actually start working on.
02:04:42.772 - 02:05:00.910, Speaker A: Okay, what does the proper prototype look for? This. What do we need to do to get this in? Geth and Prism first and get all the kinks ironed out. So can you kind of walk through what you've been up to over the past few months, what the implementations were like, where things are at now?
02:05:01.920 - 02:05:39.812, Speaker H: Yeah, certainly. It's been a while ride past couple of months. Getting into EIP four four was like something I wasn't even aware of it until around April, even though it was worked on East Denver. But my first thinking was, okay, we really need this for optimism. Like, gas costs are out of control, right. And at the rate of user growth that we're going, we really need this soon as well. So I basically roll up my sleeves and start working on an implementation.
02:05:39.812 - 02:06:32.952, Speaker H: At first, it was basically looked at already existing work, like what Terrence mentioned he and Proto already had, I guess, like a proto proof of concept of EIP four four. There are some things that are missing. First step was basically to take a look at that existing code and extend it, basically. So what I did first was complete the implementation in the execution client. The one that already existed, was Geth. So all I had to do was continue working on Geth and have that done, and the next thing was to have a consensus client that also incorporates EIP four four four. Even though it's not quite evident, if you look at the spec, there are a lot of consensus level changes to UIP four four four.
02:06:32.952 - 02:07:50.370, Speaker H: And so I basically looked at Prism because it's written in Go, and I'm pretty familiar with Go lately. So that was, like, my client of choice for this, but it really could have been any client. So I looked at a Prism basically started implementing what's needed to get EIP 44 working. And, yeah, that's pretty much mostly what I've been doing for the past couple of months. And then there was a point where once both the Geth and Prism implementations were good enough and implemented most of the spec, it made sense to start integrating things, and that's what gave birth to the first DevNet. So the DevNet is basically a small network that's running the EIP 44 four hard fork. It implements both the consensus and execution level changes to the spec, and it's kind of like our playground we're using to test out how would the spec behave in certain conditions and what sort of transactions can we send to it and testing the threshold of the spec itself.
02:07:52.740 - 02:07:53.580, Speaker A: Sweet. Yeah.
02:07:53.670 - 02:07:54.928, Speaker G: That was a great overview.
02:07:55.024 - 02:07:57.908, Speaker A: Roberto, Terrence, anything you guys want to add on that?
02:07:58.074 - 02:08:28.636, Speaker I: I just want to give a shout out to Mophie and Roberto and team. It's like having a Deafnet now. It's a pretty amazing accomplishment, right? Just because we even had a deaf net before the merge, and we had this deaf net before withdraw and everything. And it's just remarkable. And I always find it impressive and possible that people go into other people's code base and then able to make advancements such that so all the props and all the props go to them.
02:08:28.818 - 02:09:13.436, Speaker G: Yeah, I was going to give props to Mophie specifically as well. I mean, he's really pushed this along. Consistently been an amazing help in providing guidance to those of us jumping in new completely cold and being able to get reasonably productive. Anyway, I also wanted a shout out to Michael de Hoog, who did some work on this from the Coinbase side before I did so. I'm a relative newcomer here. I did some work predominantly on the consensus layer side, implemented the new style blob verification for so on, for example, and continuing building from there. Hopefully we're going to get the next version of the DevNet up soon where we have the latest spec changes, for the most part implemented at least.
02:09:13.436 - 02:09:17.390, Speaker G: Still remains a little bit of a moving target, but it's starting to settle down.
02:09:18.080 - 02:09:39.524, Speaker A: Thanks. And yeah, it is, I believe, the first time that Coinbase gets involved in working on a protocol change. At least that I recall. I'm curious, why does Coinbase want to do this? What is the rationale to put engineers on this instead of the thousand other things I suspect you all have to do?
02:09:39.722 - 02:10:35.512, Speaker G: Yeah, it's a great question. And as you know, Tim I've been talking with Tim since I've started at Coinbase trying to figure out how we can contribute. And there are a lot of competing efforts for time for a Coinbase employee. But ultimately I think we realize that we want to raise all boats, we want to build out the ecosystem that's not just purely charitable. We view Coinbase as not just trying to be kind of the most easy to use and secure exchange and custodian for crypto out there, but we view it as a broader onboarding mechanism to crypto as well as all of Web Three. So if you think about it that way and look at the kind of number of users we're dealing with, if we took all the roughly 100 million Ish users of Coinbase and tried to dump them on chain, it wouldn't work out so well right now. And we want to do that right.
02:10:35.512 - 02:11:01.500, Speaker G: We want to start blending Coinbase's centralized experiences with more decentralized experiences. Again, provide this very straightforward, easily usable onboarding mechanism and so on. And so in order to get there, we need to scale the ecosystem. Right? And the best scaling roadmap out there, we believe, is the ethereum scaling roadmap. So that's the natural place to know. Mike got involved at first. I've joined in.
02:11:01.500 - 02:11:07.570, Speaker G: We've got a couple other people we're trying to ramp up and yeah, just really delighted to be a part of this.
02:11:09.300 - 02:11:32.010, Speaker H: I can't emphasize enough how much effort Coinbase is putting into this. We have a very good relationship with Coinbase because we're all both in the same boat here. We really want Ethereum to scale and we also anticipate the huge influx of users as we come in the next couple of years. So it just makes sense for the collaboration effort that we have going on.
02:11:33.420 - 02:12:11.748, Speaker A: Yeah, and maybe to bring it back to how we actually get this done. Terrence, you're on a client team, obviously, and all the client teams have been deep down in the merge for the past year. But some people have started to look over the edge and think about what comes next. What's your general read of how client teams perceive four four now? What do they see in terms of the complexity here, the risks, if people have had time to even look into it. But yeah, what's your general read on that front?
02:12:11.914 - 02:12:12.292, Speaker D: Right?
02:12:12.346 - 02:12:56.436, Speaker I: So I can give my perspective from the consensus layer client. So my first impressions, I really like it. I think it's elegant and I think it's a good stepping stone. And it's funny because if you look at Ethereum history, right, we don't usually just dive into something, we make incremental progress. I think a good example of that is just like we ship proof of stake beacon chain but we didn't just merge, right? We ship this proof of state beacon chain, we let it run in the wild for like one year and then we hard for to a tear. But we didn't merge there, right? Because we want to try to understand how to hard fork in this type of environment. So we take our time, right? And another example is just hybrid PBS, right? We didn't fully ensure PBS first.
02:12:56.436 - 02:13:50.208, Speaker I: We have this hybrid PBS, we use Medboost just because we don't want to commit into something and I think this is a very similar concept. Basically we are exploring ish right. I think my perception is that I think some of the teams also think that I think it's important to get it done, but we want to get it done as safely as possible. We don't want to trade off between quality and speed. For example, I think majority of my time will be spent on testing. And then in terms of just implementation complexity, I do think it's a little complicated just because there is this new cryptography primitive such as KZG, and most of the client teams, unfortunately, don't have as per cryptography, we have to work with someone else. And unfortunately, when you work with other dependencies, it takes longer, it introduces more complexity.
02:13:50.208 - 02:14:34.576, Speaker I: But the good thing is that we are using Bos in the current proof of state. So we do have some experience working with the teams as well. And I think the consensus and the network changes are fairly easy just because we have done that before many times for the hard fork. And then I think majority of the fun part is just like client implementation detail, right? For example, we have this concept of optimistic thinking. What can a client do when the data is not available? Right? I think using the merge as an example, right, there's probably like in Prism there's like 20,000 lines of diff, but there's only a 2000 lines of diff. That's actually important.
02:14:34.678 - 02:14:35.088, Speaker A: Right?
02:14:35.174 - 02:15:05.180, Speaker I: And then we spend the longest time just looking at those 2000 line of deep to make sure, hey, this is good, this is good, this is good, this is good. So I will say definitely lots of time on testing and stuff. And I also know like Mario from Gap has hacked like an EIP four, four branch for Lighthouse at ipadine. It's super awesome. And Enrique from Taiku has also been asking questions on that front. So definitely seeing a lot more progress right now. But overall I am very bullish.
02:15:06.880 - 02:15:54.172, Speaker A: And do you think introducing because we're basically introducing a new layer into Ethereum, right. With this EIP we have like now we like to call it the consensus layer, execution layer. You can almost think of these blocks like a data layer that we're introducing. How does that change the client architecture or the testing infrastructure? Is this as big as the merge? Where the merge? We had no way to test the combination of execution and consensus layer clients and we had to spend months and months building all that infrastructure. Is adding a data layer similar or is it actually a bit simpler because it's still handled by validators on the Beacon chain and we have a bunch of infrastructure there, so yeah, that's a good question.
02:15:54.226 - 02:16:22.916, Speaker I: If you ask me that like a year ago, I probably will be more pessimistic. But if you ask me now, I think we are used to this concept of layering, right? For example, we have consensus layer, we have execution layer. And with all the MVP stuff we have this builder layer. So everything has become more layerish. Every software implementation is very complicated, but they are as complicated as basically the API between them.
02:16:22.938 - 02:16:23.076, Speaker A: Right?
02:16:23.098 - 02:17:01.540, Speaker I: So we have this concept of engine API where consensus layer and execution layer basically posts certain things to each other and that's all they have to understand. And similar with this data layer, whatever we're calling, right, we will essentially expose this API, such as getting blob and stuff like that. So yeah, I would say the complexity is pretty encapsulated. So that's nice. And we have also done this for the merge, so we are quite experienced at this already. And then it's just testing, which we have been testing for the last eight months.
02:17:01.610 - 02:17:01.940, Speaker A: Right.
02:17:02.010 - 02:17:09.140, Speaker I: There's great. People like pairing and everything. We know how to test this with different client combos.
02:17:09.720 - 02:17:33.532, Speaker G: Yeah, I just like to jump in and out. I think this is way simpler than the merge. The protodang Sharding in particular. It's pretty strongly coupled with the consensus layer at this point. It's not like it's adding a completely new independent layer. So I think it really hits a really great balance of taking a step towards that independent data layer without having to go all the way there. So I actually view this as a relatively low risk change.
02:17:33.532 - 02:17:42.290, Speaker G: Again, being somewhat young, new to this project, and probably naive, but compared to what I saw going on with Emerge, this to me, seems like almost a piece of.
02:17:45.220 - 02:17:45.584, Speaker H: Know.
02:17:45.622 - 02:18:02.980, Speaker A: Like, we keep saying how this is what we'll use to scale Ethereum and how roll ups will use it. Mophi, do you want to take maybe like a couple minutes to walk through? Why is this actually better for rollups? Like, how does a roll up use proto day charting to provide lower fees to their users?
02:18:04.380 - 02:18:57.320, Speaker H: Yeah, certainly. So right now, roll ups basically have to, after sending transactions and doing everything in l Two, we need to post that data back to l One. It makes it easy for a new node that's joining the network to derive the chain once that data is available. But posting that data is actually expensive right now, today, Ethereum the main way we do it? Well, the only way I think most roll ups I know does it is by using call data. When we create a transaction on l One, all the transactions and state routes and commitments, we roll that up. Hence roll up into a transaction on l One and that's posted via the call data. But the call data is quite expensive.
02:18:57.320 - 02:20:18.450, Speaker H: Over the past couple of months, a couple of roll ups, including optimism, have implemented a couple techniques to reduce the size of that call data, like compression. But overall, it's still not sufficient, especially with the rate of growth we've seen at l Two zone, it will eventually be more and more expensive. So what Blobs transactions do, which is what EIP 44 brings, is provide, like, an alternative source of that roll up data that we need, and also provide an alternative but also cheap way to post that data. And the way we're doing it with EIB Four Four is rather than just post it back to l One as call data, we would use like, a special transaction type called the Blob transaction. And this kind of goes back to the data layer that Tim mentioned. So the data that we're posting back to l One kind of, like, gets posted to the data layer, not l One in particular directly. And the economics of this makes it much more cheaper for roll ups to do, because Blob space data space is cheaper than just using the l One block space.
02:20:18.450 - 02:20:56.350, Speaker H: So that's kind of like in a nut. Okay, that's one part of how l Two S would be using roll ups. The second part is also in fault proofs, right, as a roll up. Well, for optimistic roll ups in particular, we need to be able to easily dispute any invalid like state route that's been posted on. L one and with EIP four four four. This also makes it easy. All we need to do is just change where we're getting the data used for that fraud dispute to use Blobs instead.
02:20:56.350 - 02:21:13.360, Speaker H: What I like about this design is it's very easy for optimistic roll ups to just plug in what data source you're using for this. It's either from the data layer or from L one call data. It doesn't matter. So, yeah, that's kind of how it works in a nutshell.
02:21:13.700 - 02:21:14.450, Speaker A: Nice.
02:21:15.800 - 02:21:16.804, Speaker G: Sorry. Go ahead.
02:21:16.922 - 02:21:17.984, Speaker A: No, please, Roberto.
02:21:18.032 - 02:22:05.030, Speaker G: I was just going to add right, so then when you can start extending the data later to implementing the sophisticated erasure encoding rather than keeping it with every single consensus layer. But I think one of the things to add for the viewers that aren't intimately familiar with a spec is the Blob data doesn't need to be kept around indefinitely by the consensus layer. It can be thrown out. And I think the discussions I've heard so far is probably after a month or so. Right. Because I think the main need for that data is for these fraud proofs or availability proofs. And so after the dispute period has elapsed, there's no reason for a standard node to keep that data, which is one of the reasons it can be priced cheaper, for example.
02:22:06.040 - 02:22:06.790, Speaker H: Exactly.
02:22:09.960 - 02:22:45.570, Speaker A: Nice. Okay, so we have all this L Two S are going to use it. It's going to be great. It's going to lower their fees. What's left to do to ship this? First of all, this hasn't been accepted by client teams as part of the next network upgrade yet because we frankly haven't had time to discuss it in much detail because of the merge. But from a purely technical perspective right. To bring this to a spot where it would be safe to deploy on mainnet, what are the big things that are missing and that you're all kind of looking into?
02:22:47.140 - 02:23:24.172, Speaker I: I can go first, then maybe people can fill in. From my perspective, I do want to start merging the changes into Prism, which means currently the code is more for DevNet, but it's not so much for production readiness quality, so it will be hard to merge it. So I would definitely want to clean up whatever we have today, merge it with upstream, and at the same time, I do want to sync with the deaf net a little bit, play with the deaf net, try to break it just to make sure it's robust, it is resilient. And I do think it's very important to write more.
02:23:24.226 - 02:23:24.396, Speaker A: Right.
02:23:24.418 - 02:23:40.130, Speaker I: Because right now I type EIP four four on Google and nothing really show up. So I do think we need more nice resource material such so that to bring more attention to the community. And I think that's like a great way to push forward.
02:23:42.920 - 02:24:03.384, Speaker G: Yeah, I think the other thing is just more client implementations. Right. So we've got gas, we've got Prism. They are preliminary. And Terrence is absolutely right. There's a lot more that needs to be done to productionize that code to test that code, to prove to everyone out there that if we launch this on the main net, things won't explode. Right.
02:24:03.384 - 02:24:18.880, Speaker G: So the other thing I think we really need to do is we need to set up an official testnet that I think has EIP 4844 Active, where the developers of L two S can actually start creating their next version that takes advantage of it and seeing what benefits it provides.
02:24:20.420 - 02:25:19.810, Speaker H: Yeah. And to add to that, I think we also need, like, a proof of concept use case of EIP four four four for a roll up. Because maybe I'm a bit salty about access lists, but we don't want to end up in a case where we deploy AIP and people start using it and realize that, oh, the gains are not that much that we expected. We want to show that this thing actually works. It's a valid use case. We would use it once it's available, and it'll be a huge boon for the roll up teams. So I think demonstrating this in particular by the group bedrock, we're actually working with Coinbase to integrate bedrock with the bedrock is like the optimism upgrade that's currently in development, and we want to integrate it with EIP four four four and then tell the client teams that, hey, this thing actually works.
02:25:19.810 - 02:25:30.004, Speaker H: You can see how it works. It's not just vaporware or something. That'll be really, I think, a good way to convince client teams to accept it.
02:25:30.202 - 02:25:43.320, Speaker G: Absolutely. Yeah. I mean, the back of the envelope math looks great. We expect it to scale, help scaling dramatically, but proof is in the pudding. Once we have code, a usable system, I think it'll be a lot more convincing.
02:25:44.540 - 02:26:38.840, Speaker A: Right? Yeah. And I'll plus one mophie, to your point where it's like we've had features in the past that we've deployed on Ethereum where the usage didn't end up being what we expected. So access lists were once there was once another pre compile for interrupt with Zcash that I think Zcash changed the curve a few months after, so didn't end up working great. So seeing something like a cutting edge roll up implementation that's working on a prototype of Four Four that'd be really valuable. And one thing that Four Four also introduces that's kind of new to Ethereum is basically this idea of KZG commitments, which require a trusted setup. So it's like a new cryptography primitive that we're adding to the network.
02:26:39.600 - 02:26:40.796, Speaker C: Mophi, do you want to kind of.
02:26:40.818 - 02:26:50.450, Speaker A: Walk us through what that is at a high level? Like, why do we need to add new cryptography to make this and and just your general view there.
02:26:51.540 - 02:28:09.610, Speaker H: Yeah. So the data that we're posting to the already dubbed the data layer that roll ups will be using, we need a way to be able to prove that a particular piece of that data, we need to be able to prove what that particular piece of the data means. And this is really useful, for example, in fraud proofs, where rather than sending the entire data set to your fraud proof, you would just send little bits of and you can convince the fraud prover that, hey, this data matches the little piece that you've posted. So what KCG lets us do is basically it's like a commitment similar to mercury proofs, but it's different from mercury proofs in the sense that it's easier for you to prove a particular point with the same proof size. And also it's very easy for you to extend it. This goes back to what Roberto was saying about full data availability and erasure codes. So with KCG, it's very like.
02:28:12.060 - 02:28:12.376, Speaker D: You.
02:28:12.398 - 02:29:02.010, Speaker H: Can extend your data with the KCG and then easily make it amenable for data availability. So that kind of gets over the overview of why we need KCG. But it's like new cryptography, like what Tim says that we're introducing to the consensus spec client. And there's some unknown, unknowns a little bit that we're still trying to figure out, in particular what's the most efficient way of verifying these KCGS. And it's still like an open research question there, but I think as more and more people are getting into the spec and taking a look at it, we'll be able to develop better solutions there.
02:29:02.380 - 02:29:44.200, Speaker G: Yeah, I think it's also worth adding that while it is cutting edge crypto, it's not quite as cutting edge as some of the stuff that's going on in the Zkevn world. Right. It's a fairly contained, I wouldn't dare say well understood, but I don't think it's like we're really inventing brand new stuff here. I'm not a crypto expert, by the way, but I was able to follow the spec fairly easily and go and implement it with the benefit of the protolamba work and Go KZG libraries and so on. So, yeah, I guess I don't want people to be too frightened by that work that's going in there because I haven't found it terribly intimidating. Again, as a non crypto expert.
02:29:44.780 - 02:29:59.450, Speaker H: Yeah, true. The math like KCG is based on is pretty old math, like 80s math. It's just recently we're starting to actually implement it in a production ish system and that's where we're trying to figure things out.
02:29:59.840 - 02:30:10.880, Speaker G: Yeah, and Mophie in particular has been doing lots of work on optimizations and benchmarking and things like that. So that's another part of the project, and work remains to be done there, making it go faster.
02:30:11.300 - 02:30:27.920, Speaker I: I guess the fun question is, do you foresee that we just have one crypto library for KZG that all the clients can use? Or do you foresee there will be multiple efforts that every client team will build their own KZG library?
02:30:29.060 - 02:31:02.610, Speaker H: I think there will be one or two. In particular. With Go, we're thinking of using one particular library that is written in C, but there are some particular things with go and interrupt with C code that might prevent us from eventually using that. But I could definitely see other client teams that perhaps using Rust would probably use that as well. And yeah, it's pretty much mostly two implementations at the most. I think.
02:31:04.500 - 02:31:35.450, Speaker A: Two feels like it would be a sweet spot. One has been a contentious topic in the past for implementing stuff on Ethereum because we do have this multi client architecture and the whole purpose is that if there's a bug, then not all the clients are affected. But then if you have the central point where everybody's using the same library in the background for the purposes of those operations, you're back to like a single implementation. So hopefully we get to see two high quality ones emerge over time.
02:31:36.620 - 02:31:42.764, Speaker G: Yeah, it's been interesting. We've been trying to sort of figure out where to set that bar, where the appropriate abstraction layer is, where we start sharing code.
02:31:42.802 - 02:31:43.292, Speaker D: Right.
02:31:43.426 - 02:31:50.770, Speaker G: You want to set that as low as possible, but without requiring people to implement their own very low level bitwilling to get the crypto right.
02:31:51.140 - 02:32:26.280, Speaker A: Yeah, correct. And I guess to kind of go back up a little bit, we talked a lot about 4844 itself and where we're at there. Terrence, do you want to walk us through? If you look at the whole roadmap towards full charding, what percentage of it is done with 4844? What are the boxes that we check off? And then what's the stuff that we would need to do over the coming years to get kind of better scale on Ethereum?
02:32:26.620 - 02:32:52.900, Speaker I: Right, I can list a few points. So what 4844 includes? Right, what 4844 includes essentially this new transaction type, it's going to be exactly the same format that we'll be using in full sharding. And most of the execution layer logic required for full sharding is also going to be there. And most of the execution and consensus layer cross verification checking logic will also be there.
02:32:52.970 - 02:32:53.348, Speaker A: Right.
02:32:53.434 - 02:33:20.424, Speaker I: So those three are essentially getting piggybacked by four a four four. And like you guys said, the layer of separation between this data layer and the consensus layer is also there as well. The concept of sampling blob. Right now essentially you have one beacon node that samples everything, but with full sharding you have this committee of validators and beacon nodes sample portions of it and that's the power of sharding.
02:33:20.472 - 02:33:20.924, Speaker A: Right.
02:33:21.042 - 02:33:33.824, Speaker I: And I think the guest pricing is going to be highly similar as well. So that's very nice. Right, so maybe we can talk about what full sharding has that four four doesn't have.
02:33:33.862 - 02:33:34.208, Speaker A: Right.
02:33:34.294 - 02:33:55.192, Speaker I: Full sharding have this low degree extension for the Blob, basically allow 2D sampling and then it has the actual implementation of the data value sampling. It has builder and proposal separation because you don't want proposer to essentially sample 32 megabytes of Blob. That's just a lot.
02:33:55.246 - 02:33:55.608, Speaker A: Right.
02:33:55.694 - 02:34:07.710, Speaker I: And hopefully some proof of custody stuff as well, to ensure there's no lazy validator. They just pretend to vote on everything. So, yeah, feel free to add on guys if I miss anything.
02:34:09.280 - 02:34:42.630, Speaker G: Yeah, I mean, I think it's to dumb it down a little bit because it's pretty complicated stuff for someone who's just coming to this. But basically, 4844, every node is still downloading every Blob, still storing every Blob, with the exception of what I mentioned earlier, where they can delete it. The full Sharding roadmap. Once we start doing this eraser coding, you have these committees that can be consulted to reconstruct the data. And so no longer does every node have to store every single thing. So that's why it's called Full Sharding rather than protodink Sharding, I guess.
02:34:43.640 - 02:34:44.004, Speaker C: Right.
02:34:44.042 - 02:34:45.076, Speaker A: And I guess if you think of.
02:34:45.098 - 02:34:47.560, Speaker C: It in terms of cost right now.
02:34:47.630 - 02:35:08.512, Speaker A: When you use Call data, you're storing data that is stored forever by every node. And so that's really expensive. 4844 gives us data that's stored temporarily by every node, so it's like, cheaper. And then Full Danksharding gives us data that's stored temporarily by only a subset of nodes, and it can be even cheaper. Does that roughly make sense?
02:35:08.646 - 02:35:09.084, Speaker G: Bingo.
02:35:09.132 - 02:35:09.730, Speaker D: Yeah.
02:35:11.220 - 02:35:40.868, Speaker H: Also a little thing that's a bit different with the I guess the security of assumptions of the data layer. With Proto Dank Sharding is you only need one node that's having the data available to be able to sync up. Whereas with Call data, you need all your peers to have that data available so you can be able to sync up the tip.
02:35:40.964 - 02:35:42.840, Speaker A: Right, yeah.
02:35:42.910 - 02:35:58.300, Speaker H: So you only yeah, I think that relaxed assumption makes things also somewhat contributes to why blobs are cheaper in some sense, indirectly.
02:35:59.700 - 02:36:37.980, Speaker A: Right, that makes sense. And I guess one final thing I wanted to touch on is there's been tons of community enthusiasm for 4844. There's been random people popping out wanting to help. And I'll give a quick shout out to Kane here from synthetics who's funded a lot of those to just come and help out with various things. If someone's like listening to this and they want to get involved in 4844, what are the things that are most needed right now and what are the places that they should go or follow to kind of be informed of the latest developments?
02:36:41.760 - 02:36:44.670, Speaker G: Mophie, you want to take that? You've got the best landing page, I think.
02:36:46.080 - 02:37:34.350, Speaker H: Well, yeah, there's, of course, EIP four. Four. It contains links to several other resources that I think should help anyone get ramped up to speed with the spec. We also have a DevNet, but that's also another good way to contribute, basically, just running a node in the DevNet building, running a node, participating, creating contracts, sending transactions. There's a faucet available into DevNet that makes it easy for you to fund yourself and do things with it. So that's where I think I would start. It makes you get familiar with the network before taking a deep dive into the spec.
02:37:35.680 - 02:37:44.800, Speaker G: Yeah, I was referring to Mophie's HackMD page where as instructions for getting up and running with the DevNet, it's a great resource.
02:37:45.620 - 02:37:52.980, Speaker H: Points to shout out to Gabby and the Etherd discord for setting up the faucet.
02:37:54.120 - 02:38:04.592, Speaker G: Oh yeah, absolutely. And a bunch of us hang out in the Sharded data channel in the discord. That's another good place to interact with us, I guess.
02:38:04.666 - 02:38:29.900, Speaker I: Another one is just to write learning material guide and stuff. Just a lot of people don't know what 44 four is and they're not probably not going to read EIP or consensus layer spec. Bunch of python code, right? They probably want to read something that's more like humanly readable. I guess so. Yeah, I guess more education material and more resources. Those will be lovely.
02:38:31.540 - 02:38:31.952, Speaker D: Yes.
02:38:32.006 - 02:38:40.288, Speaker A: And if people do those, please tweet at me or Mophie or Terrence and we'll add them. We'll link them on the 4844 website for sure.
02:38:40.454 - 02:39:00.570, Speaker G: Yeah. And if any testing experts out there, we have a lot of work to do there. For example, I think that's one area where it's an easier way to get started. Right. You have to understand the entire spec. You can pick a little piece of it and take a look at the code we've written and how poorly tested it is so far and dive in from there.
02:39:03.260 - 02:39:05.690, Speaker H: Yeah, there's a lot of little hanging fruit there.
02:39:06.240 - 02:39:17.484, Speaker A: Yeah. So testing is a big cool. And so if you have testing experience, please give us a shout on Twitter, emails on the website and we'll find.
02:39:17.522 - 02:39:18.528, Speaker D: Something for you to do.
02:39:18.614 - 02:39:37.670, Speaker G: Yeah, and optimization and benchmarking. Critically important here. We've mentioned it a few times already. This new crypto is pretty expensive. If you don't do it right, you introduce denial of service vectors. We want to make sure that doesn't happen. So that's why that area of work is also important and plenty more to do there.
02:39:41.320 - 02:39:55.000, Speaker A: Sweet. I think this is a good place to wrap up. So we've covered what the EIP is, why it's valuable, where we're at, what you can do if you want to get involved. Do any of you have any kind of closing thoughts you wanted to share with folks?
02:39:59.930 - 02:40:12.280, Speaker H: I guess to close we really need roll ups to be cheap. If you want to send your NFTs and mint them really cheaply, this is the way to go.
02:40:13.950 - 02:40:34.960, Speaker G: I think my only closing thoughts is that this is something we'd really love to target for the Shanghai release. I personally believe it's not too ambitious of a change know, slip beyond that. But we'll do our best to make our case through stuff that works. Hopefully we'll get there.
02:40:35.970 - 02:41:00.710, Speaker I: My thoughts is I think besides withdrawals, gelding is probably the most important thing to welcome post merge. Because like Danny said, now we're sustainable and we have security so Scalability is next. And it's funny, when I first started working on this space back in 2018, I wanted to work on Sharding for the longest time. And then finally, now this is the time to work on Sharding.
02:41:03.070 - 02:41:11.740, Speaker A: I think that's a great place to wrap up. Thanks a lot, guys, for coming on. I appreciate you all taking the time. And thanksglobal for hosting us.
02:41:12.990 - 02:41:14.126, Speaker G: Thanks for having us.
02:41:14.228 - 02:41:14.926, Speaker B: Thank you.
02:41:15.028 - 02:41:23.278, Speaker E: Thank you guys so much. Yeah, thanks for all of the information. It was a really awesome panel. Appreciate you guys all being here. Thanks.
02:41:23.444 - 02:41:24.270, Speaker C: Bye.
02:41:25.670 - 02:41:36.820, Speaker E: Okay, up next we have Mark Dawson from Quicks who is going to talk to us next. Hey, Mark, thanks for joining us.
02:41:37.270 - 02:41:41.054, Speaker A: Hey, Katie, great to be thanks for.
02:41:41.112 - 02:41:46.520, Speaker E: Thanks for being here. For sure. All right, I will hand it over to you. You can go ahead and get started.
02:41:47.450 - 02:42:00.106, Speaker A: Okay, great. I will go ahead and do a screen share. Okay, great. Can you see that, Katie? Yes.
02:42:00.128 - 02:42:01.340, Speaker E: We're good to go. Thanks.
02:42:01.950 - 02:42:31.990, Speaker A: Okay, cool. Hey, everybody, my name is Mark Dawson and I'm the co founder of Quicks. And I'm going to talk about the Optimism NFT ecosystem today. So that'll be a little bit of a change of pace from some of the more technical talks. This one will be a little more fun and a little bit less technical. So let me tell you a little bit about Quicks. We're the largest NFT marketplace on optimism.
02:42:31.990 - 02:43:17.622, Speaker A: We're a top five delegate in the Optimism NFT governance system. So you might have heard about the Token House and the Citizens House. We're top five delegate in the Token House, so we'll be voting on proposals for the governance fund. If you're trading on Quicks, the average gas fee is $0.14 compared to openc or like ETH mainnet, which will be today usually about two or $3. A few months ago, you might remember it was close to like $50. So we're probably ten x or 20 x 30 x cheaper than Ethereum main net.
02:43:17.622 - 02:43:56.800, Speaker A: And we've traded about 2500 ethan volume so far. We launched about oh, I think I lost my screen. Let me make sure it comes back. Sorry, everyone. We launched Quicks in December 2021, right around the time that Optimism removed its whitelist for apps. And we are coming back.
02:44:02.290 - 02:44:03.040, Speaker D: U.
02:44:05.670 - 02:44:26.906, Speaker A: And basically I want to spend time today talking about if you're a creator, like how you can get involved in the NFT ecosystem or katie, can you see the screen okay? I'll assume yes. Okay. And it's full screen?
02:44:27.088 - 02:44:28.522, Speaker E: Yes, it sure is.
02:44:28.576 - 02:45:05.910, Speaker A: Okay, great. Thanks so much. So if you're an NFT creator, you probably have used one Ethereum, maybe Salana, maybe Polygon. The Optimism NFT ecosystem is much smaller today than the other ecosystems, but it's very exciting for a few reasons. So why would you be excited to use layer two NFTs? Layer twos are, of course, like faster and cheaper. And that sounds great, but it's actually even more exciting because it's like ten X faster, maybe 100 x cheaper. So it means that you're going to be doing totally different things.
02:45:05.910 - 02:46:10.438, Speaker A: The way I like to say it is that layer one, Ethereum is like you're driving a car, and layer two, optimism, means that you're going to be flying in a plane, so you're going to be taking trips that you didn't take before. So what kind of trips might those be? So the breakout use cases that I'll highlight so far creator own smart contracts. Mirror XYZ is one of the biggest web three blogging platforms out there. And historically they had one big smart contract on Ethereum main net so that people didn't have to deploy a smart contract for every new blog post that they made. That saves on gas. But the problem is that your writing NFTs can be mixed with other people's blog posts, which creates kind of a confusing experience on OpenSea or Marketplace, and it can just make your work harder to find. So earlier this year, Mirror moved over to optimism.
02:46:10.438 - 02:47:17.918, Speaker A: And now when you're creating a blog post on Mirror, you can deploy your blog post as its own contract. On optimism, it's going to cost only maybe like $5, compared to maybe like fifty dollars to one hundred dollars on Main net. And on our marketplace, you can trade the writing NFTs and they're all their own distinct collections and like on Ether scan they're all going to be their own contracts. So that's pretty exciting and pretty creator friendly. Another cool use case of layer two is reputation NFTs. So just about a month ago, Rabbit Hole, which is a big web three education platform, launched some badges on our platform so you can collect these NFTs to prove that maybe you learned a new skill set or that you completed some tasks, so maybe you learned about some DeFi protocol and now you want a reputation NFT. That doesn't really make that much sense to do on Ethereum main net because you're probably not going to make a lot of money off of the badge NFT.
02:47:17.918 - 02:48:09.794, Speaker A: And so you don't want to be paying like $5 to mint it. But on optimism, you can mint it for like ten cents. And what's really exciting for me is these reputation NFTs today, they're mostly about collecting, but eventually they could be about unlocking, maybe some special skill set. I think if we put more things on chain, like if we give you more badges for all the stuff you're doing, that gives developers a richer set of data to build on top of. So my hope is that if I've done a lot of DeFi activities, maybe when I visit a web three site, it'll kind of recommend to me d five activities or something like that. So reputation NFTs are a great use case of layer twos. Composable NFTs are another one that means NFTs that combine with each other.
02:48:09.794 - 02:48:47.614, Speaker A: So dope. Wars is an NFT project that was released actually like mid to late 2021. So they were really early on it. They began on Ethereum and they wanted to have these characters that they could equip items to. So they have these characters called Hustlers and you want to give them some sneakers. And the sneakers are NFTs and the Hustler is an NFT and you want to combine them. The problem on Ethereum Main Net, once again is that you're going to be looking at like a $5 gas fee for doing that.
02:48:47.614 - 02:49:30.010, Speaker A: And really as L One Ethereum gets more expensive, the only things that you're going to want to be doing on L One Ethereum are things that have direct financial benefits. So maybe you'll trade on L One Ethereum, but you're probably not going to want to equip or interact with your NFTs on L One Ethereum. So they built their own bridge, actually, which is pretty cool, and bridged their collection to optimism. And that's just the beginning. There's a lot more I'll say about composable NFTs later. Another great use case is Dragonia. They're an on chain gaming platform, I think one of the first on chain games on optimism.
02:49:30.010 - 02:50:06.406, Speaker A: So with these, you buy these Dragon trainer NFTs and they can battle each other. And it's like a little bit like Pokemon but much more simple. What's cool is that the battle interactions happen on chain. Once again, as you're putting more stuff on chain and doing slightly less financial activity on chain. You can't be doing that on Ethereum Mainnet because it'll be too expensive. So you're going to want a layer two like optimism. These are all live projects today and there's a bunch that I didn't get to talk about.
02:50:06.406 - 02:50:51.560, Speaker A: So let me go ahead and focus on the ecosystem. We have about 40,000 people trading today on optimism. 40,000 unique wallets, I should say, and a lot of those are in Thailand and East Asia, which is pretty cool. Many are preexisting NFT communities. So Crypto Testers, which is this guy with the Opie hat and he's green. The crypto testers is a community that's existed for a couple years and their goal is to test out new crypto protocols. They issued their community membership NFTs on optimism, which we were really excited about.
02:50:51.560 - 02:51:25.754, Speaker A: And those are trading on quits today. The other NFTs I'll point out here, a lot of them are pure art NFTs. So this top one is called and that's from our community in Thailand. They have a very active community and some really strong developers over there. A lot of them will review smart contracts that we put out. And they've created their own launch pad. This middle one that looks like you can't quite tell what it is, that's called Ganland.
02:51:25.754 - 02:52:00.646, Speaker A: And that's AI generated art by a project called Fractal Visions. And that's based in Colorado, I believe, in the United States. We have town in the top right. This is like the animal looking ghost. And that is a collection, again from creators in Thailand. That one's really fun. The Bortown Animal Ghost Project has the honor, I think, of being the first collection that was copied by a scammer and put on OpenSea.
02:52:00.646 - 02:52:47.142, Speaker A: So like a scammer copied all the art and launched their collection on Ethereum main net. And I always take that as a sign that you're doing something right when people want to copy your work and take it to other places. Because early, when you're establishing an ecosystem, of course the first thing that's going to happen is people are going to sort of copy what happens on layer one Ethereum and try it on optimism. But now people are copying what's happening on optimism and trying to put it on layer one. So that's pretty cool. This one of the person in the helmet is called Motorheads, and that is a collection by a designer actually at Op Labs named Jvmi. And this collection is kind of about video games and early 2000s nostalgia.
02:52:47.142 - 02:53:28.770, Speaker A: So there is this really cool Mint page that had a Game Boy on it. And that one's a pure art project that's quickly developing its own community. And then this one that's kind of cut off a little bit. This is a duck called Usagi Duck, and that's from an artist in Japan. And she made this collection of 2000 ducks that all look very funny and whimsical. So this is a taste of some of the communities and artwork on Quicks today that I wanted to give a shout out to. And another cool thing is that many of these communities launched using our Launchpad on Quicks.
02:53:28.770 - 02:54:27.146, Speaker A: So I'll take a minute to talk about that. Our Launchpad is a creator owned smart contract tool that you can use to deploy your collection and will also create a Mint page for you. And you can have your own Allow list and you can also reserve some for your core team and do the drop directly on our website. If you're a little more sophisticated, you can of course make your own smart contracts and make your own Mint page. And we'll also index and pull those collections into the marketplace. So we're just a big secondary marketplace and we're going to pull all the NFTs to pull out an optimism into our marketplace. So out of these ones, Ganland, Motorheads Boardtown and Usagi Duck all used our Launch pad and Aptimism created their own website.
02:54:27.146 - 02:54:58.354, Speaker A: And I think Crypto testers did as well. And Aptimism also created their own Launchpad, I think, which is pretty cool. There's also a bunch of other launchpad tools and creator tools on optimism, so you can find them at Nifty Kit and Mintplex. And there might be another one. But yeah, more and more creator tools coming to optimism, I'll say it's still a very tightly knit and community. So a lot of these collections will work with each other. So, like.
02:54:58.354 - 02:55:43.474, Speaker A: The motorheads will often cross promote with the bored tone. NFTs, for example. So you might be asking what's next for optimism? NFTs because so far the ecosystem is collections that you might see on main net. So like the art ones, the Tradable community pieces, we expect that to continue to grow, but we have these really exciting things in the pipeline. So we have an NFT bridge coming up that allows you to take an NFT on layer one ethereum and bridge it over to optimism. I'm going to discuss that more at length in a couple of minutes. We're excited about these reputation NFTs, so that's something similar to what rabbit Hole is doing.
02:55:43.474 - 02:55:51.150, Speaker A: Sometimes we call them badges. The difference here is that these are often earned rather than purchased.
