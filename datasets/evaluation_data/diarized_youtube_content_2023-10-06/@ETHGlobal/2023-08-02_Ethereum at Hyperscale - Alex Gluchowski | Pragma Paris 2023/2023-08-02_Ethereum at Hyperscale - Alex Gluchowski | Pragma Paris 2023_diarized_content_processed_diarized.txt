00:00:07.290 - 00:00:20.910, Speaker A: So I want to bring on our very first interview for today, and that is Alex. Alex is the co founder CEO of matter Labs. You may know more frequently at ZK Sync. And we're going to have a conversation around how we can scale all of this even better. So welcome, Alex.
00:00:26.810 - 00:00:31.154, Speaker B: Thank you, Katrik. It's an honor to be here and we're really excited for this interview.
00:00:31.282 - 00:00:54.590, Speaker A: I am equally excited to talk to you officially on stage. We've had a lot of these when nobody was listening, so let's change that a little bit. You've been working on scaling ethereum for nearly five years. I want to start off by just kind of having people know about what you're working on, how we got started and sort of what was the actual insight that made you sort of go all in on this being the problem that you needed to solve.
00:00:55.330 - 00:01:39.914, Speaker B: So you guys probably know ZK sync It's a layer two on ethereum using zero knowledge proofs. It's been the first ZK roll up on ethereum. We actually built the very first working ZK roll up. So it's always been in the ZK space. My motivation for doing this was solving problems that we need to solve to get ethereum to the mass adoption. I came into crypto space out of convictions that freedom is the most important thing you can improve in the world. And it was the intersection of the technological, really challenging, interesting technological things and the passion for freedom that I had.
00:01:39.914 - 00:02:06.470, Speaker B: And it was clear that we need to solve scalability, we need to solve a few other things. But scalability seemed like a really hard problem. And it was not until zero knowledge proofs appeared on my radar that I had an idea. How can we do it fully preserving the properties that made ethereum and bitcoin before that really valuable. And if you think about these properties.
00:02:06.970 - 00:02:08.200, Speaker A: That was my question.
00:02:10.170 - 00:02:29.302, Speaker B: The story of building matterlabs actually illustrates what they are. So when I was first talking to investors and I remember talking to you five years ago in San Francisco and to others and trying to sell this thing, ZK was a very little known term, very little known concept.
00:02:29.366 - 00:02:33.838, Speaker A: It was actually Zcash has just came out and figuring it out what to do.
00:02:33.924 - 00:03:40.690, Speaker B: Yeah, but most people perceived it as just the privacy thing and something weird and far from reality and very complex and expensive. But it was very clear to me that this is a breakthrough technology that will transform the world completely. It was clear that we don't have the protocols right now that are capable of doing this, that are production ready. But the pace of the scientific progress in the space was so fast that it was clear, like, maybe in two years, maybe in three years, in five years, but we will get the product, we will get there. It's like the curve is convexing. And then once we get there the properties that this technology give you were really feeling like magic and people were referring to your knowledge proofs that this magic moon math and not without a reason. And so I was trying to convince people and everyone was like man, I don't know where's confirmation? It's hard, like if you're not a scientist, it's very hard to understand this concept and see beyond.
00:03:40.690 - 00:04:20.522, Speaker B: So the great thing about capitalism and ethereum blockchains specifically and the digital world is we have very little regulation, there is a very low barrier of entry and we have this amazing world of VCs who are willing to take risks. Like you don't have to convince them to 100%, you just have to convince them that there is a slight chance that this can become big and then they are willing to support you. And this is very different from a political world where you have to go and convince some powerful mighty party that this is absolutely the right way to go forward and you have to provide evidence and you can't talk in probabilities.
00:04:20.666 - 00:04:23.630, Speaker A: Innovation doesn't happen when it's masses convincing.
00:04:23.710 - 00:05:14.810, Speaker B: But innovation is the most important thing that drives wealth, progress, prosperity, happiness for us as a species. And innovation requires experiments and so we need to lower these barriers of energy. And this is why my conviction was that the most important properties of blockchain systems are permissionlessness. Like this absolute inclusivity. Anyone can come and stop building things, just deploy a contract or download a wallet and start using it without asking anyone for permission and you can start innovating, you can just build products, people can instantly access it and it's all an open system, just like the internet. In fact, for me the web three is just a continuation of the internet. It's this next wave that brings value on internet.
00:05:14.810 - 00:05:32.850, Speaker B: Like blockchain is the value layer of the internet. I'm absolutely convinced that all of the value transactions of the world will come to blockchain with the magic power of zero knowledge proofs because it just gives you something very unique that you don't get from other technologies.
00:05:32.930 - 00:05:53.694, Speaker A: So let's contextualize this a little bit. There's an argument for, okay, well, we kind of saw as a mathematician that this is what's possible and then it's now an efficiency game or sometimes just some better enhancements around it. But concretely, what does it let you do better that helps you actually set any blockchain up for success in the future?
00:05:53.892 - 00:06:50.190, Speaker B: So I'm not so good in information theory to give you an exact definition of this property, but I intuitively feel it and maybe I'll try to explain it's kind of like compression of the integrity of computation. You compress certain information that you pass over and from there you get two properties, essentially. One is scale or even hyperscale as we call it now in ZK sync. So you can extend the computation indefinitely, you can process all of the world's value transactions in parallel, merge them in parallel in 116 proof. And this proof can be verified by any mobile phone, any clock, any raspberry pi in world in just a few milliseconds. This is really powerful. The scale is one thing.
00:06:50.190 - 00:08:17.818, Speaker B: The other thing is the protocols are called zero knowledge proofs, meaning that you don't disclose any additional information about the fact of the computation, the fact that the computation is correct. But in fact, the systems that we can build on top of the zero knowledge proofs, I would call them like partial knowledge systems. So maybe an analogy that will help you understand this is when you get a payment. Let's say, like you have your banking app or a venmo or PayPal or whatever, and someone makes a payment to you and you see on the display of your app that I received this payment. And you know how much you received and you know your new balance and you can see all the payments on your account. What matters to you is to know that this payment really happened, that there is a world consensus that now you have much more money, you can go and use your credit card and buy some goods, and people will actually take this money from you. But you don't have to know anything about all the other world's accounts in this bank or elsewhere, right? So you only care about this computational fact, but you need the partial knowledge of the entire system, because if you had to know all of the world's accounts, that would not be scalable, right? So this is like, practically speaking, if we now go back to practical plane speaking about ZK roll ups, specifically, we solve computation completely.
00:08:17.818 - 00:09:03.210, Speaker B: We can process infinite number of not infinite, unlimited number of transactions. But we still have the data availability problem. All of the ZK roll ups. How many of you guys are familiar with ZK roll ups and how they work? Okay, most of you are, but so you know that all the roll ups, ZK and optimistic combined are using the same fixed throughput of Ethereum data availability layer. So we can have like today, we can process up to, I think, like around 1000 transactions cumulatively across all roll ups. If one roll up does more, others have to do less, or the price will go up and the price will go up. And even after Protodunk Sharding, and even after Dunk Sharding, we won't get to an infinite throughput.
00:09:03.550 - 00:09:04.860, Speaker A: I want to cover that.
00:09:05.230 - 00:09:56.090, Speaker B: Yes, we'll talk about that. But with zero knowledge proofs, you can build systems that use hybrid data availability models such as Volution in ZK Sync, we have an implementation called ZK Porter for this, where a single roll up can have accounts that are like ZK roll up accounts that inherit 100% security from Ethereum. So you have theoretically, like if there is no bug in the implementation, you have exactly the same security properties as though you are on the contract on Ethereum. But in addition to that, you can have this ZK Porter accounts that are elastic. You can grow them much higher than the roll up accounts. And you don't have to publish them to Ethereum, but you still have access to all the other roll up accounts. And you can still, like each of the system, can talk to any other system.
00:09:56.090 - 00:10:23.794, Speaker B: And the systems can trust each other because they have this integrity from zero knowledge. Proofs. And on top of that, you also get privacy. And privacy in ZK rollups implemented in the right way. Something like ZK Sync is doing is going to be extremely cheap. If on ethereum. Now you have to pay dozens of dollars for a single snark to verify on ZK, it will cost you like 0.1
00:10:23.794 - 00:10:29.538, Speaker B: cent, like under $0.01. Extremely cheap. So all the privacy applications are coming to blockchain with this.
00:10:29.704 - 00:11:13.586, Speaker A: That was an awesome primer because it actually sets up us for the next 15 to 20 minutes of other questions. So let's kind of dig into some of the things that you said. Of course, the magic here for this Moon math is that you can do all these things and they're guaranteed to be correct because if they're not, then the rest of the system doesn't kind of proceed. So it's all or nothing kind of actions. And the best thing here is that all these proofs to contextualize what a proof is, at the end of the day you're coming up with a string which is like a hash of all that data and in most cases that hash stays constant. That's kind of the recursion that's coming in. I can have a million transactions or two transactions or 1000 and the size of that proof for any of those will still be the same thing.
00:11:13.586 - 00:11:53.540, Speaker A: Which means it's quick to verify, it's quick to store anywhere and you just have the ability to construct the same proof later if you want to really verify and do the math. Again, all of that is now in a way anywhere from benefit of hindsight to things that we've learned. So my question to you is kind of forwarding and going back and forth between the last five years. What were some of the things that you had to anywhere from invent to figure out to improve to get to where you are now? Can we talk about some of the advancements you've made? Can we talk about some of the breakthroughs or the actual benchmarks to things that you've built and kind of what was the state of the art before and what's happened?
00:11:55.270 - 00:12:44.660, Speaker B: We are I'm incredibly proud to have a really strong team at Matterlabs, the core team of Zksync. It's not going to be the only team working on Zksync, but it's the original one. We made a number of things for the first time on Ethereum. As I said, we've built the very first ZK roll up. And we have brought the very first implementation of recursion to Ethereum. We came up with the idea of Zkvm, that you can actually do it and we built the first working version of Zkvm. It's been a work on the shoulders of giants of a lot of researchers and other developers and other contributors to the space who are like I have extremely high respect for.
00:12:44.660 - 00:13:33.998, Speaker B: Our team's contribution is tiny compared to all the body of knowledge that was created before us. But maybe I can focus on the most recent things that I'm most excited about. This week we announced a new proof system called Bujam, which is an implementation of the proof scheme called Redshift, which our team came up with as a byproduct of the work. I remember that from we didn't actually intend we were always very practical, very applied research. But we saw Plonk coming out and Fry system and my co founder was like, oh, this is interesting. If we combine this and that we can get some new properties. So the combination of Plonk and Fry, we called it Redshift.
00:13:33.998 - 00:14:16.530, Speaker B: It was not practical until the mirror protocols. And now the Polygon team came up with Plunky Two where they applied the smaller fill to it and the Plunky Two is actually an implementation of Redshift. And now we came up with another implementation of Redshift where we made a number of things completely differently and made a lot of improvements. And now we made the benchmark with Seller. We made them somewhat incorrectly. So it's now almost as fast as Starkey, slightly behind Starkey in the benchmarks. But we used the wrong hash function, so we are slightly above Starkey.
00:14:16.530 - 00:14:38.818, Speaker B: So it's like fastest from all the benchmarked systems in the world. We'll update the benchmark soon and it's more than an order of magnitude faster than Plonky Two, orders of magnitude more performant than the existing pro system we use. And this is going to push the boundaries of what's possible computationally on ZK roll ups very far.
00:14:38.984 - 00:14:50.282, Speaker A: What's a better way to understand what this order of magnitude actually looks like? Are we talking about something becomes from a few seconds to a few hundred milliseconds? Or is it like we've reduced the requirements? How do we actually to think about.
00:14:50.336 - 00:15:33.346, Speaker B: Performance of proof systems, we have to remember that the computation is always done in parallel. Zero knowledge proofs are really well parallelizable. So what matters is your performance comes down to the matter of cost. Another important cost of hardware or cost of computation? Yes, which boils down to the cost of hardware that you can rent somewhere or you own, but you probably want to rent it, you want it to be elastic. If there is higher demand, you want more instances of your proverb. If there is lower demand, you want to roll it down. You probably want to use very efficiently parallelizable hardware for this, which currently the best choice is GPUs.
00:15:33.346 - 00:16:09.742, Speaker B: So we actually have also an implementation of GPU prover for bootjam, which is only requiring 6GB of Ram. So it's like compared to all the previous, to the best of my knowledge, to the existing provers out there, it's a massive improvement. I know some of the competitors were running as recently as a few months ago, prover of 500GB of Ram, which you can only run in a cloud. And so you become heavily dependent on this cloud. This is not what you want. You want to decentralize the proofs. You want to decentralize the sequencer.
00:16:09.742 - 00:16:33.814, Speaker B: That's the core of our ethers at ziki sync. Then we're working on both. But the key enabler to decentralizing the prover is getting it on a GPU hardware that is like consumer grade, that can be run in the cloud on any GPU instance that is used for machine learning or whatnot? Or on GPU mining rigs. Or even on home gaming.
00:16:33.862 - 00:16:34.486, Speaker A: Gaming rigs.
00:16:34.518 - 00:17:10.998, Speaker B: Yeah. So this is the really interesting thing. It's already live on Mainet in shadow proofing mode. So we're currently testing it with we're on the proofs on the old proof system and in parallel, we're on the proof for the exact same blocks because they were designed to be future proof with the choice of the hash function and the architecture of the storage. So we can easily switch the proof system, we can easily switch the field used in this proof system and once it's like after a month, we will complete the security audits and we will just switch to only the new proof system.
00:17:11.084 - 00:17:14.738, Speaker A: So the end user doesn't notice any difference because you've just improved the internals.
00:17:14.834 - 00:17:17.718, Speaker B: The users will notice the difference in.
00:17:17.724 - 00:17:20.250, Speaker A: Terms of they don't have to do anything new to sales.
00:17:20.400 - 00:17:55.726, Speaker B: Yeah, so the users don't have to do anything. It's completely transparent, like nothing changes from the user perspective except for the cost. With the new Pro system, we'll be able to make the blocks essentially unlimited. They will only be limited by the throughput of the sequencer, the new construction and the construction we use in the virtual machine now, not in the proof system, but in the virtual machine design enables us to do arbitrary long execution trace. So we don't have a concept of a block limit. In Ethereum, you have a block limit of like 15 million gas. In ziki sync.
00:17:55.726 - 00:18:03.318, Speaker B: You don't have it. You can have 100 million, like 150,000,000 gas in a single block, in a single transaction, whatever, because it's just a.
00:18:03.324 - 00:18:05.240, Speaker A: Proof that matter, doesn't matter how.
00:18:07.210 - 00:18:33.854, Speaker B: Again, the proofs are paralyzable. So once we close the block, we can just spin off number of instances to prove this block and we don't have to worry about it. What we have to worry about is the sequencer, because it's EVM compatible. So we are on the sequencer in the EVM paradigm sequentially. Every transaction is executed after another. Every block access is sorry, storage access is executed after another. So sequencer throughput matters.
00:18:33.854 - 00:19:21.950, Speaker B: We knew this, and this is why we were building. We do not reuse gaff, for example, because we know it has existing limitations that other teams struggle with. We built a sequencer node completely from scratch in Rust, and our current sequencer node enables something like over north of 100 TPS of average transactions. What we see in the mix of different DFI NFT projects, like if you take an average transaction, it's something around, I think, 200K gas. Not sure exactly. I don't want to lie here, but an average transaction, we can do over 130 or something like this per second today. And we're still working on improving the sequencer, and at some point we can parallelize it.
00:19:21.950 - 00:19:29.762, Speaker B: But we have a really high buffer. Right now, ZK Sync is doing around ten TPS on average for the past months.
00:19:29.896 - 00:19:32.014, Speaker A: That's the current usage. You mean not the capacity?
00:19:32.062 - 00:19:50.842, Speaker B: That's the actual demand. And the capacity is like ten x 13 x. More than that. This usage actually makes Ziki Sync one of the most bottle tested L Two S. We are actually the most used L two over the last 30 days average. We're just above our bitroom and like, far above everyone else.
00:19:50.896 - 00:19:51.802, Speaker A: That's an interesting one.
00:19:51.856 - 00:20:03.550, Speaker B: Interesting. All right. I highly recommend L twoBeat info as a resource for all this metrics. For TVL, we're a third L two by TVL and first L two by usage.
00:20:04.210 - 00:20:42.710, Speaker A: That's an interesting point. So let's talk about this. We're obviously seeing multiple approaches to solve scalability. We just talked about ZK Roll Ups here, and that's what you're working on. There's also optimistic Roll Ups, and we have some other kind of brand new ways to think about things like starquare in the same kind of field, but we kind of are seeing a lot more people experiment with Optimistic Roll Ups, or you kind of see anywhere from base to recently cello. I'm going to try to do a version of this for future speakers too. But how do you kind of perceive what it means to kind of use optimistic roll ups here? And what's kind of the framework around when somebody should consider ZK rollups versus optimistic roll ups?
00:20:43.390 - 00:21:13.060, Speaker B: I think one advantage optimistic Roll Ups have today is that you don't have to wait for the proof generation. So your latency could be kind of synced with ethereum, and you can do, like, every block. You can do some checkpoint on ethereum and kind of make it harder to revert this checkpoint. It's not impossible, but it's harder. Right? We can use this in ZK Roll ups as well. It's just not been used so far. But it's easy to add this property.
00:21:13.060 - 00:21:24.534, Speaker B: Apart from that, optimistic Roll ups are currently cheaper. Well, not optimistic Roll ups. The optimistic execution, if you want, would.
00:21:24.572 - 00:21:27.590, Speaker A: Be can you clarify what optimistic execution is here?
00:21:27.740 - 00:22:21.318, Speaker B: That's a really good question. Maybe optimistic Roll ups cannot be detached from the problem of data availability. So we see a rise of this optimistic chains, so called optimistic chains, which is essentially reincarnation of plasma protocol we've been dealing with, experimenting with five years ago. But the security assumptions come down to basically the same properties as a side chain. So it's probably not really a good term. But if you look at the optimistic roll ups systems that publish data availability on Ethereum and derive security from Ethereum, because anyone can challenge the invalid states that the sequencers would be publishing. Which by the way, I don't think we have optimistic Roll Ups that actually have permissionless fraud proofs.
00:22:21.318 - 00:23:04.434, Speaker B: But apart from that, the Optimistic roll ups are fundamentally going to be more expensive than the ZK roll ups. Right now they're cheaper just because they had a few years, had time to implement data compression. This is coming soon to ZK Sync, and I know that other ZK rollups are also working on this. This will lower the costs by 50, 60, 70%. But just like simple compression, instead of the address, you put the idea of the address. Like you use dictionaries, you compress the numbers. Like you don't have to publish 32 zeros, you just publish 10 because some numbers are more compressible.
00:23:04.434 - 00:24:03.930, Speaker B: But once we have that, we will be roughly equivalent. But after that, we add compression uniquely enabled by zero knowledge proofs. For example, reusing of the states ezksync recursion or different ezksync and StarkNet are the only two L, two S that publish state diffs instead of call data. We have an architectural choice where we don't publish every single transaction on chain at the end of the block, we only publish what has changed in this block. So if you have 1000 NFT transfers, then you will have 1000 updates and Optimistic Rollups will have 1000 inputs for all the transactions. But for different transaction types, like let's say Oracle updates, 1000 updates will end up like they constantly update the same storage slots in the same contract over and over again. They just adjust the price.
00:24:03.930 - 00:24:59.318, Speaker B: So at the end of the block, if you do 1000 Oracle updates, you will only have to pay for one. If you're using like ARP approach for Optimistic roll ups, they will have to pay 1000 times. So for applications like this, same with AMM access, like if you are doing an Arbitrage from a single trading account to the same AMM pair at the end of the block, no matter how many transactions you do, you will only pay for one. And we can nicely amortize this across all users. So if some, let's say uniswap trading pairs are more popular, they will become cheaper the more people use them in the batches. And the batches can be large, it can be like 20 minutes, which is like thousands of transactions, right? So this will enable a completely unique cost lowering for ZK roll ups versus Optimistic Roll ups. And on top of that we have volition where the access to retail users will be, like flat at under $0.01.
00:24:59.318 - 00:25:02.400, Speaker B: Nomad out what the gas prices on ethereum are.
00:25:03.090 - 00:25:04.302, Speaker A: How does that work?
00:25:04.436 - 00:25:06.620, Speaker B: Well, you split the state into.
