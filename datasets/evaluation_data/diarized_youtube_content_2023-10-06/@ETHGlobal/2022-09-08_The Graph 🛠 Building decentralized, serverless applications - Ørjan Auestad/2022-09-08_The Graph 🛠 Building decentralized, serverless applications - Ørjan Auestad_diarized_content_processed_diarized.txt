00:00:06.250 - 00:00:25.310, Speaker A: Good morning, good afternoon, good evening. Depending where you're calling from, we have Oyeon from the Graph doing a workshop on building decentralized serverless applications. As always, if you have any questions, they can go into the zoom chat and our host will be more than happy to answer them. And with that being said, I will pass the mic off to our speaker.
00:00:26.010 - 00:00:47.194, Speaker B: Thank you so much, Anna. I am Ariana Risa and today I will tell you why I am excited about Graph. And hopefully by the end of the presentation, you are just as excited. So this is me. You can find me on Twitter. You can reach out to me on Discord. I hang out in the Eat Global Sponsor channel.
00:00:47.194 - 00:01:23.926, Speaker B: I am on the official Graph Discord channel as well, thegraph.com Slash Discord there. We have dedicated channels for questions and support, so feel free to reach out there and some of our support members will help you with anything. And I'll be there as well. So feel free to ping me. So let's dive into it. Many people ask me, why do we need indexed data? The blockchain is public, right? But most blockchains are optimized for writing data and not for reading it.
00:01:23.926 - 00:02:17.350, Speaker B: So you could imagine the blockchain as a huge public library, and every day the library gets a truckload of new books and it just gets added to the end of these scalps without really being sorted. And when you connect to those nodes, you are limited to browsing the library and look up individual titles. So now imagine you go into this library and you want a list of the books or the 100 books with the highest word count. To get this list, you will need to go through the entire library, check every single book for their word count. And this takes a very long time. So let's take an example that is a little bit closer to a practical example. In web three, like, say you want to create a DeFi dashboard.
00:02:17.350 - 00:02:47.650, Speaker B: Most of you might be familiar with Uniswap. It is a decentralized exchange where anyone can fade tokens in a trustless manner. And they also have an analytics dashboard at info. Uniswap.org, where you can find valuable charts and data. And as you know, the price of a token changes every time a trade is made. And that can even be multiple times in a block.
00:02:47.650 - 00:03:39.198, Speaker B: Now, if you wanted to create a price chart simply by querying raw blockchain data from your, for example, front end application, you would need to make millions of call to an Eat node. And not only would this be incredibly resource, heavy with data is expensive. The load time may be minutes to hours. So all of this data needs to be preprocessed or indexed. And all of the data you see on these charts are coming from the graph. So is it only DeFi charts then, that need subgraphs, need index data? Well, it turns out everybody needs it. We currently have over 64,000 subgraphs that have been created on the Graph.
00:03:39.198 - 00:04:32.942, Speaker B: So that's 64,000 open APIs that are gathering and gathering blockchain data and over 640,000,000,000 queries have been served. So let's take a look at a couple of the benefits of using the Graph. Like you have a you can create a customized API that is tailored to your data requirements. You can include any kind of rich, sorted, filtered or aggregated data and that allows for fully decentralized and serverless application. After all, adapt that relies on a single centralized server is just enough. So the Graph is currently have two projects. Projects.
00:04:32.942 - 00:05:32.338, Speaker B: One is the hosted service which it's a centralized service run by Edginote, one of the core developer teams at the Graph. The centralized service currently have support for 38 networks near Polygon BNB and 35 more. And we are currently moving over to the decentralized network that has support for Ethereum Mainnet currently. But we believe that every DAP deserves decentralized and open APIs and that is something we are committed to. And we are currently in the process of adding MultiChain support to the Graph. So we will start the Gnosis chain and we will add support for multiple networks in the months to come. So stay up to date to that because that is very exciting.
00:05:32.338 - 00:06:16.710, Speaker B: We can take a look at a few benefits of using a decentralized service because it's not just a buzzword to be decentralized. Being decentralized means that you have no central point of failure. You never have to call your back end engineer in the middle of the night because your server went down. You're not at the mercy of large centralized service providers. You have a load balance system. The Graph is infinitely scalable and it scales seamlessly. You have a GeoDistributed service and there are indexers in every major continent and even an indexer that isn't on any of the continents.
00:06:16.710 - 00:06:58.130, Speaker B: You have censorship, persistence. You will never wake up to an email saying that your server went down because your service provider have taken a non crypto stance. And least, but not last. The Graph network is an open marketplace where 160 indexes are competing to deliver the highest quality of service to the lowest possible price. So this is why I'm excited about the Graph. We can stop, post that there and see if there's any questions about the Graph.
00:07:04.110 - 00:07:06.966, Speaker C: I have a question about the Graph.
00:07:07.078 - 00:07:08.320, Speaker B: Yeah, go ahead.
00:07:09.570 - 00:08:21.646, Speaker C: Kind of like an application architecture question. So I am building, I'm doing a project that looks at activity on the blockchain and then distributes rewards in some type of token to the users that executed that blockchain that executed those transactions. And so I'm indexing all of the stuff that I want to look at in the Graph, right? But I have a script that I basically need to run every reward period, whether it's like a day or something, every day I need to run this script and I tally up how much each address earned based on the events that I've indexed in the graph. And then I need to update a contract to allow people to claim the corresponding tokens. But what I'm struggling with is the code that I run every 24 hours or whatnot that's not decentralized, that's just like a script that I have running somewhere and I can't figure out how to decentralize that. It's the only piece that's not decentralized.
00:08:21.838 - 00:09:28.570, Speaker B: That is a good question and the short answer is that currently the graph is read oriented. So taking that data inviting to the blockchain would be more of a process that would be centralized in a way. But the long answer is that we have some really exciting research areas in the pipeline such as verifiable queries and Verifiable indexing. So we use zero knowledge proofs to prove that the data reserve is correct. And that together with a technology called a subgraph bridge where we can bridge data back on chain. These are the things that is part of the exciting future for the graph. But currently that would be kind of a centralized feature.
00:09:28.570 - 00:09:33.500, Speaker B: Did that answer your question?
00:09:37.070 - 00:09:42.240, Speaker C: Yeah, the stuff that you mentioned about the new graph functionality is interesting.
00:09:42.770 - 00:10:25.382, Speaker B: Yeah, we can circle back to that for sure. Feel free to reach out to me as well and I can give you some links about that. Yeah, of course. So we want to see some code and build a subgraph as well. Before we do that, I'll just give a short primer on building a subgraph. So subgarf is the open API and we will be mainly working with three files. When you build a subgarp one is the subgarp manifest in this file? It is a YAML file where you define what data sources you want to index.
00:10:25.382 - 00:11:23.280, Speaker B: So what network the contracts on that network? The start block. Because you will define a start block that indicates that you want to index data from that block on the chain and all the way up till chain head and forward. And also what events calls other on chain triggers to listen to. There will also be a subgraph schema. This is where you define your data structure. You define your entities and relations between those entities and most subgraph developers should spend a little bit of time to think ahead and try to make the schema as close as possible to your DAPs data requirements. Because a well structured schema can save you a lot of time down the line.
00:11:23.280 - 00:12:22.462, Speaker B: Then you have the final part is the subgraph mappings and this will map data between your data sources to your subgraph schema. Here you will do like transforming of data, aggregation of data and so on. So let's see how it looks in practice. Here we have the graph, the graph.com we can look at the subgraph studio which is the staging area for the decentralized network. If we create a subgraph here. Like in this example, I will simply index the Board Ape Yacht Club contract that is on Ethereum, Mainet.
00:12:22.462 - 00:12:48.970, Speaker B: Let me call it Basi Workshop. Now I've created it on the Studio in the graph. And here you have some useful information. It's undeployed. Right now I don't have any information uploaded. This is the subgraph slug. This is the Identifier you will be using in commands in your CLI.
00:12:48.970 - 00:13:27.910, Speaker B: And this is the deployment key. That is a authentication key. So only you can deploy subgraphs to this. So here you have some useful commands and you would start by installing the graph CLI. I have already done so, so I won't let you sit through that 1 minute. Instead, I will jump straight to initializing my subgraph. So let's copy that command and open the CLI.
00:13:27.910 - 00:14:08.210, Speaker B: So you would run graph in it in the studio and the subgraph slug. Now I have a very nice little cheat code for you is to add the index events flag. What this will do is it will add some scaffold code to your subgraph. We will see that in a minute. It will ask us what type of contract it is. And it's an EVM based chain. So it's an ethereum.
00:14:08.210 - 00:14:47.786, Speaker B: The subgraph slug is already filled out for us. I wanted to have it in that default folder. And it is on Mainet. Now let me grab the contact address that looks like this. The name of the API or name of the contact? I don't know off the top of my head. So I just call it something that's just what it will be called within the subcarf code. So now it's generating a scaffold.
00:14:47.786 - 00:15:38.264, Speaker B: It's installing the dependencies. Just give it a second. It will ask us if we want to index more than one contract. But for now we say no. Now we enter that folder and open it in VSC. So now if you remember, five minutes ago we went through the three different main files you will be working with as a subgraph developer. The first one is a YAML file.
00:15:38.264 - 00:16:00.000, Speaker B: And here we can see what's going on. We have one data source that's on Ethereum. It's called basey. It's on Mainet. So this is the type EVM base chain. It's on Mainet. This is the address and this is the Abi.
00:16:00.000 - 00:16:59.814, Speaker B: Here we want to add a start block. We don't need to index all the way from the Genesis block because the contract wasn't deployed until block 12 million. So let me paste this here. You can find this for any contract by looking at the contract creation transaction here. And you will find a block that it was created at. Now that we've added the start block, we make sure that it starts indexing when it actually has something to index. You see, since we pass in the index events flag, we already have some code populated.
00:16:59.814 - 00:17:51.180, Speaker B: Let's also look at the schema. The schema has one entity for each of the four events that is on this contract. Approval, approval for all ownership, transferred and transfer. And it also has some of the properties for these events or have the properties for these events. If we look at the in the source folder there will be a mapping file, it's called Base Es in our example since we call the contract basey and you see that there's already generated a boilerplate code for us that handles each and every one of the events. So let's take a look at one of them. The handle transfer one.
00:17:51.180 - 00:19:05.010, Speaker B: Whenever a handle transfer happens on Chain, it will create a new transfer event with an ID that is equal to the transaction hash and then the log index and it will store the event. The event parameter called from in the from field. It will store the two in the two field token ID and the Token ID. And then it will save everything. So this thing is ready. So let's first make sure that we save the file where we change the start block and we run graph code gen. This will generate the code that should be done every time the mappings, I mean the manifest or the schema is changed and then we go back to the subgraph on the studio we want to authenticate.
00:19:05.010 - 00:20:20.120, Speaker B: Note that the full authentication key isn't shown here so you need to use the copy. You would not show this but we can regenerate it afterwards. Then let's deploy it, we give it a version label and it's deployed. Let's go back to the studio. See, it's already syncing, already synced and we can query its data. So now we have approvals, approval for all we can add. If we want to add some more like transfers, we can write GraphQL query for that.
00:20:20.120 - 00:21:19.480, Speaker B: Yeah, now you have a working subgarp that is indexing events one to one for all events that have happened in the Base C contract. That is pretty cool. But what's really great about subgarf is ability to create which APIs that are tailored to your application. So this is where you would go back to your subgar and add additional data. So say for example that you don't just want to have your transfer entity, you might want to have a note down what the timestamp is and the block number. So you could add that. Let me just type it out.
00:21:19.480 - 00:23:21.260, Speaker B: So since we have changed the schema, we would also run graph code gen again that is simply so these two new fields that we created in the schema is available for us in the mapping code. So now we've said that this is what we want in our data structure. Now let's actually add these things to the mapping file as well. And you see now that when I write entity block number you see that property is already there. So let's save that. We can redeploy it and add a number two. Let's see now, both the first version and the second version is available here in the studio.
00:23:21.260 - 00:24:30.660, Speaker B: And this allows us, for example, to do some more interesting queries. Like say we want the first five transfers and we want to order by timestamp. Let me yeah. Did I yeah, not an order direction ascending. This is let me just copy paste it. That's a little bit weird. When I'm zoomed in, I might need to refresh my page.
00:24:30.660 - 00:25:35.460, Speaker B: There we go. It just took a second for the UI to catch up. So with this query, where we order by timestamp in an ascending direction, we are now getting the first ten basi NFDS to ever be transferred. So that shows the loop of how you can create your first subgraph in five minutes. And then you decide you want some more additional data fields in your subgraph. You go back, you make some changes to your code, and then you redeploy. And when you are done with your subgraph, and when you feel that this should be published on the decentralized network, you can do so over here and you can publish it either to Mainnet, which is the decentralized network, or our testnet on curly.
00:25:35.460 - 00:26:43.080, Speaker B: Let me quickly also show you the Graph Explorer, where if you want to not create your own subgraph, but use an existing one, there's the Graph Explorer, where you can explore the decentralized network. And there's the hosted service where you can explore subgraphs that are deployed to the centralized hosted service. As you can see here, there's over 28,000 that has been made open source for you to use, or open for you to use. We have two tracks for PISCES in it global, and one of them is using a subgof, an existing subgof, and another one is creating your own. So happy hacking. And we have two minutes left, so if anyone has any questions, we can take them now.
00:26:44.250 - 00:26:48.090, Speaker A: Yeah, I think that Kashif had a couple questions in the Zoom chat.
00:26:49.630 - 00:28:25.280, Speaker B: Yeah, let me see. So Kashif has a question about like, when you are waiting for a new event to update in your subgraph and what you do on your front end, the recommended approach right now is to just Paul at a reasonable interval. There's currently some exciting works being done on something called Live queries, and you can already start using the Graph Client that already has live queries. With the Graph Client, you can tag a query as a live query and it will do the polling strategy for you. So I highly recommend anyone that is serious about using any GraphQL, the Graph or other GraphQL endpoints to check out the Graph Client. It's not just the best client to use with the Graph, but the best client overall.
00:28:30.280 - 00:28:31.910, Speaker D: I have a quick question.
00:28:32.360 - 00:28:33.590, Speaker B: Yeah, go ahead.
00:28:34.040 - 00:29:32.390, Speaker D: So, I'm actually building a membership marketplace, DAP, where users can actually get to search for communities and memberships they're interested in based on tags. So if you were a dow or something and you wanted to be found, you could publish or broadcast your community by associating it with some tags like public goods, relationship sports, whatnot. And if you were a user and you wanted to find some communities to join or projects that you might be interested in, you could find it by searching for tags. And it's supposed to display communities that were broadcasted with those tags associated, and then you would basically check for the one you were looking for, something like that. So I was kind of curious if that's something that could be done with the graph because that's like the missing piece to what I'm building, like where are the tags and how that sort of fits in.
00:29:32.780 - 00:30:12.870, Speaker B: Yeah, I would definitely use the graph for that. I would probably use a cheap chain. So whenever somebody wants to create a profile with these tags or create these tags, it is not on Ethereum, Mainet. And then I would index that. And when a user goes in and enters their tags, you would send a GraphQL query to the graph and find the broadcasters that correspond with those tags. Is that a good answer?
00:30:13.480 - 00:30:22.680, Speaker D: Yeah. So basically I would index the tags and wouldn't have anything to do with the contracts and like the example you showed?
00:30:23.100 - 00:31:02.900, Speaker B: Yes. So you would probably create just a small contract where you could even create a contact where anyone can pass in a swing. So it could be any kind of tags. You would pass that in the subgarp and add tags or in the subgraph code. You would sort these entries into tags and then when you query it from your content, you would query it based on the same tags.
00:31:03.560 - 00:31:04.310, Speaker D: Okay.
00:31:05.880 - 00:31:41.680, Speaker B: There's also a feature in the graph called Full Text Search which allows you to just really store a set of strings and define these as you want to be able to do full text search based on these strings. And when a user then goes in and he writes down a couple of tags that he is interested in, you could do a full text search in the Savgar for any matching broadcaster.
00:31:42.660 - 00:31:45.232, Speaker D: Yeah, that's really great. What was it again?
00:31:45.286 - 00:32:41.320, Speaker B: Full. So that's called full text search. Okay, let me pull it up here. So here you can find how to define something as Full Text Search fields. In this example we are using a band search where we have like the band and the band name, the band description, the band bio. This is where people can search. And if you also search here for full Text search queries, this defines how you can create a search or how you define your query to search these fields.
00:32:42.400 - 00:32:46.620, Speaker D: Yeah, got it. Thanks a lot. That was really helpful.
00:32:47.200 - 00:32:56.320, Speaker B: Yeah, happy to help. Any other questions?
00:33:02.950 - 00:33:11.546, Speaker A: I think Shiv had a second question about proxy contracts. It's the other contract.
00:33:11.598 - 00:34:28.188, Speaker B: Yeah, sorry about that. Yeah, so that would be a way to do it. There's a feature in the graph called Dynamic Data Sources or templates that allows you to dynamically create a new data source as they get deployed. So that would be an option if you need to make sure that your subgarp whenever a new implementation contract is deployed, your subgarp is always updated to start indexing the new one. But if this is more of a one time epoxy where you don't really do many upgrades, then the simplest way is to use the API that has the events that you are listening for and then listen to the proxy contract. And sorry if you're not noticing, I'm trying to keep both screens up. Thank you so much, sir.
00:34:28.214 - 00:34:29.492, Speaker C: That answers my question.
00:34:29.546 - 00:34:54.270, Speaker B: I sincerely appreciate it. Yeah, happy to help. And if you ever need any help with that, ping us in the Graph channel in Eatglobal or join thegraph.com Discord and we have support and question channels there as well. Yeah, awesome.
00:34:55.200 - 00:35:22.560, Speaker A: Well, thank you so much Oyen, for taking the time to host this workshop for Ethanline. I hope the rest of you enjoyed it and thanks for tuning in. Yeah, as you said, the Graph team can be reachable on the Ethanline discord at sponsor the Graph or in their own Discord channel as well. And with that being said, I hope everyone has a great rest of your day. Bye.
