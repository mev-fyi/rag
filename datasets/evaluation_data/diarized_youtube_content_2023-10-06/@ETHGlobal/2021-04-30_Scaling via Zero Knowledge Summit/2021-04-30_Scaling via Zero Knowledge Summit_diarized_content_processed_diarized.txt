00:04:16.660 - 00:04:43.964, Speaker A: Good morning, good afternoon. Good evening, everybody. My name is Kartek. I'm Anna, and we'll be your co host for today. I am super excited to welcome all of you and also Anna with us. This is well, actually, before we get to the good parts, I was going to say let's talk about at the platform we're using. So if you've been part of our summits before, we're watching all of this on Apeglobal TV.
00:04:43.964 - 00:05:25.900, Speaker A: And this is something that we design custom to make sure that the experience is shared for all of us. So for those of you who are joining in for the first time, be sure to sign in because if you have any questions for our speakers, you can type them into the chat and we'll be able to ask them and relay those questions live to our speakers and get them back to you. So, for those of you who don't know, this event is organized by ETH Global. And ETH Global is an organization with a very simple mission. Our goal is to onboard thousands of developers into the Web Three ecosystem. We do this thing by running hackathons and summits online and all over the world in person. And this event and this month is all about scaling.
00:05:25.900 - 00:06:09.192, Speaker A: So this is a massive event. We have over 600 developers working on things related to scalability with Ethereum. And we have over 800 summit attendees who are interested in learning what's happening in the world of scalability and zero knowledge and everything else around it. For Web Three, we have developers from 58 different countries and 18 different times and participating. So it's super excited for us to welcome a really global, massive, diverse audience. And on top of all of that, there's over $200,000 in prizes being given out for trying out different things in the scalability and layer Two ecosystem. So as part of the hackathon, we're doing three summits on the theme of scalability.
00:06:09.192 - 00:07:07.576, Speaker A: So our first summit was two Fridays ago, which was called a scaling summit, and it was designed all around what's happening in the world of L two S, whether that's side chains or optimistic roll ups to ZK roll ups. And then last Friday, about a week ago, we did our Merge summit, which was about what's happening with the transition from ETH One to ETH Two and the merge from proof of work to proof of stake. And today is all about scalability and scaling via zero knowledge. We're going to look over a lot of things that go into how we can use zero knowledge primitives to help with anywhere from language to contract to transaction scalability. So let's talk about today's summit. We have a lot of amazing talks from a lot of incredible people, and I'll quickly go through who we have on the agenda today, and then we'll jump right into our first talk. So first we'll have Anna Rose, our co host, talk about just the evolution of this entire ecosystem.
00:07:07.576 - 00:08:08.444, Speaker A: This is a very nascent space and has been growing over the past few years. And having her thoughts on how the ecosystem has evolved, especially from the role she's played in making this happen or just telling us about how this has become so big, will start us off at the right place to set the agenda for the rest of the day. And then we'll have Elena talk about and go over an introduction to how Zero Knowledge proofs work under the hood. Then we'll have Alex, who will give us a brief history and introduction to the evolution of circuit languages. You'll see a lot of recurring themes around circuit design and proof for programmability in Zero Knowledge Realm, and we'll kind of see a whole history of how that's evolving. And then we'll have Barry WhiteHat from the Ethereum Foundation go over just the limitations of what you can and cannot do right now with the current primitives and just the privacy in general. Then we have Alex from Matterlabs talk about how Zkebm works for ZKsync.
00:08:08.444 - 00:08:55.952, Speaker A: Then we'll have Friedol talk about just how they're using Snarks to meet the demands of scaling and managing filecoin. Then we'll have Zach from Aztec go over their architecture of how they make programming practical with Noir, their DSL for geoknowledge proofs. And then we'll have Shahar talk about how the Starks and the ecosystem has evolved, going from Asics to now, running these proofs and generating them on a CPU. And then our final two talks of the day will be about the last two talks are first, on understanding how we can handle mev with homomorphic encryption. And we'll have Ravitil from well, I'm blanking on the name.
00:08:56.086 - 00:08:57.132, Speaker B: New cipher.
00:08:57.276 - 00:09:33.624, Speaker A: New Cipher. There you go. I was thinking about the new transition, but the new cipher. And then our last talk of the day is Brian Goo from Dark Forest talking about how they're using ZK Snarks for procedural generation. So it's a really jet packed agenda and I can't wait to learn about how all these things are possible and just also ask my own questions to our speakers. So, all that aside, I think it's time for us to jump into kicking off the summit. And our first talk is going to be from Anna, who's going to give us an amazing introduction of this ecosystem.
00:09:33.624 - 00:09:36.190, Speaker A: So I'll let our co host, Anna take it away.
00:09:37.220 - 00:09:46.240, Speaker B: Thank you very much. So I'll do a quick screen share. Here we go. Are you seeing my screen?
00:09:46.390 - 00:09:47.600, Speaker A: We are all set.
00:09:47.750 - 00:09:57.124, Speaker B: Perfect. Can you hear me okay? I'm guessing yes. Cool. So. Hi. I'm Anna. I want to quickly introduce myself.
00:09:57.124 - 00:11:01.764, Speaker B: I'm the host of a podcast called Zero Knowledge Podcast. I used to and will do again, an in person event. I used to do this bi annually called the ZK Summit. So, since we can't meet in person yet, I am very excited to participate in this event today because it's like the best folks working on zero knowledge are here today with us. So my goal with today's talk is it's quite a short talk and I won't be introducing what zero knowledge proofs are because Elena, who's going to be speaking next, elena is going to be speaking next and we'll actually be giving a proper introduction to what a zero knowledge proof is. Instead, what I want to do with this is actually look at this evolution of ZK use cases and kind of give a sense for what the general landscape looks like and maybe why scaling and zero knowledge is so important. So zero knowledge and blockchains came together and I would say, like, the first big use case for them was privacy.
00:11:01.764 - 00:11:44.564, Speaker B: And the second big use case, which we're going to be talking about today, is scaling. When you think about the sort of privacy context for ZK and blockchains, there's two main ideas that were first introduced and actually maybe one, which is private transactions. Zcash really popularized this idea of private transactions, using zero knowledge proofs to transfer funds from one account to another in a private setting. Another idea that was introduced soon after was this idea of private computation. So actually being able to do computation behind a zero knowledge proof. And that would allow kind of enable a lot of new and maybe more expanded use cases going forward. Now I'm going to give you a list of these are all kind of related still to privacy.
00:11:44.564 - 00:12:19.010, Speaker B: It's taking those first two ideas and pushing them even further. There's a number of new projects that are emerging, all at different stages. So Kartik mentioned the ZK Games Dark Forest. Brian's going to be speaking later about that. There's this idea of using ZK for self sovereign identity, maybe with password management. Instead of having passwords live on a third party server. There's something called selective disclosure, where you're actually sharing one small amount of information, maybe even for auditing purposes or regulatory reasons, without revealing anything else.
00:12:19.010 - 00:12:53.710, Speaker B: This is very new, but there's something like private AMMS that are being proposed. So this is the idea of incorporating privacy into DeFi something that previously people thought would potentially break these models. There's some new ideas around that that are coming out soon. There is private voting in the form of Macy. That's an ethereum based project. And there's also a very new use case that I am very excited about, which is this idea of private staking or private delegation. Henry Devalance from a project called Penumbra has just prepared kind of a first draft of what that looks like.
00:12:53.710 - 00:13:25.928, Speaker B: But yeah, as I mentioned, all of these are at very different levels. You have sort of on the privacy thread, you have theoretical research, proof of concept implementation and adoption. This is not perfect, obviously. These are kind of like moving around quite quickly. But you can see in terms of privacy, there's a lot of use cases at different stages. Now when you talk about scaling, it's a bit of a different picture. I feel like with scaling, what you see is more project building and developing, but maybe less use cases so far.
00:13:25.928 - 00:13:47.068, Speaker B: So here is a version of the L one, L two, L three paradigm. Now I know this from having many conversations. These definitions seem to EB and flow. Like sometimes you'll say that an L two solution, as we're calling it like a Zcube roll up is actually an L 1.5. I don't know. I'm going to go with this. Okay.
00:13:47.068 - 00:14:10.900, Speaker B: I'm going to keep it for now. But anyway, L ones, there's some L ones that are actually using zero knowledge proofs in their core to scale. So I think we're going to hear from filecoin later. You're going to hear about examples of how they're using it. There's examples like Mina where it's actually using recursive Snarks all the way like that. It's all built around zero knowledge proofs. And now when we're going to be talking about L two S right here.
00:14:10.900 - 00:15:15.100, Speaker B: As mentioned, one of the big differences between the privacy side and the scaling side is you just see this kind of explosion, this boom of new projects using zero knowledge proofs for this scaling activity. Yeah, and so like I said, there's not as many maybe, but the whole idea of scaling comes from a need. And the fact that we needed to use zero knowledge proofs for scaling meant that it actually pushed the entire field of zero knowledge forward in a way that privacy alone didn't seem to. It's kind of actually unfortunate. It seems like privacy is a nice to have always, but in this case, it was like a must have and that's really what pushed it forward. And so then my question to everyone is what's next? And I think at events like this, this is where you figure out what is next. What's next for privacy? What's next for scaling? And maybe what is that next stage of zero knowledge research and vision that pushes the space even further.
00:15:15.100 - 00:15:48.000, Speaker B: So if you're interested in getting involved, I'll just share a little bit about the ways that you can basically get involved in the community. If you want to join us, join our Telegram group or YouTube. You can find us on Twitter and all of those links should be there. There's also a newsletter that I'm sending out monthly called ZK Mesh, which is a rundown of all of the zero knowledge stuff. So check that out. Also do stay tuned for the next ZK sessions, which is a monthly event I do. And hopefully one day a proper in person ZK summit.
00:15:48.000 - 00:16:05.850, Speaker B: And you can find all of these links here. I also just want to mention that I've been running these ZK jobs fairs. So if anyone's interested in finding a job, look out for the next one on May 19. Cool. Okay, so I'm at the end of my presentation. I think I might have two minutes. I'm not sure.
00:16:06.220 - 00:16:07.244, Speaker A: Go for it.
00:16:07.362 - 00:16:11.150, Speaker B: Okay, well, I don't have anything more, actually. I just don't know how to fill it.
00:16:11.680 - 00:16:14.136, Speaker A: No, I think then we're back on schedule.
00:16:14.328 - 00:16:14.828, Speaker B: Cool.
00:16:14.914 - 00:16:15.676, Speaker C: Okay, good.
00:16:15.778 - 00:16:27.744, Speaker A: We're good to go. Thank you so much for setting the context, and I think this kind of sets us up for our next talk.
00:16:27.862 - 00:16:46.056, Speaker B: Perfect. So I'd love to introduce and bring on Elena to talk. She's going to be giving us the introduction to zero knowledge proofs. Are you there? Hello? Think so. Hi. Cool. All right.
00:16:46.056 - 00:17:00.748, Speaker B: So can I start sharing my screen? I think so. Should work. Maybe it worked. Cool. Okay. Cool. Take it away.
00:17:00.748 - 00:17:26.784, Speaker B: Cool. Yeah. Hello, everyone. My name is Elena, and I work in a project called ironfish. Ironfish is a layer one privacy coin. We just launched our testnet, actually, earlier this month, and my entire journey actually began with an ETH global event. The way I even got into crypto is because I went to one of the first hackathons at ETH waterloo and actually jumped start my entire interest in crypto.
00:17:26.784 - 00:18:04.880, Speaker B: So I'm so glad to be still part of this community and to give back. Thank you so much. Cool. So I'm going to post all these slides on my Twitter afterwards. And this is kind of meant to give you an overview of what zero knowledge proofs are, but I also added a bunch of links, so if you're interested, you can actually go through them later at your own time. So what is a zero knowledge proof? Or what is a ZKP? And really, it's the ability to prove honest computation without revealing inputs. So the example that I like to use is a where is Waldo? Example for people who are totally new to genealogy proofs.
00:18:04.880 - 00:18:51.344, Speaker B: So the story goes that there was a math professor who was trying to figure out a way to explain what genealogy proofs are in a very simplistic way, and he was reading where is Waldo? With his five year old son. And they're looking at where is Waldo? Page where you're supposed to find Waldo. And the father goes, I know where Waldo is. And the son goes, prove it. So it's kind of a perfect setting for how does the father prove to the son in technically zero knowledge that he knows the answer. And so what the father does is he takes a huge newspaper, he covers the entire book with this newspaper with a small little hole that reveals where Waldo is. And the paper is much larger than the book, so it actually gives the son known information about where Waldo is, but it does give the son the proof that the father does know where Waldo is.
00:18:51.344 - 00:19:42.400, Speaker B: So this is a very simplistic kind of explanation of xenonolic proofs are. But, yeah, at the core, xenonic proofs are honest computation, and they are used for scalability and privacy, like Anna mentioned and there's a ton of projects that kind of demonstrate that. I'll go over so many variants of genetic proofs, but there's three main properties that they must satisfy it's completeness. So meaning that an honest verifier will be convinced by an honest prover soundness, meaning that the reverse is true. So if someone's trying to cheat, an honest verifier can figure that out and zero knowledge. So if the statement is true, then other than the fact that the statement is true, there's no other information that's being leaked. Okay, so brief history because genealogy proofs come from such a rich background.
00:19:42.400 - 00:20:42.720, Speaker B: So xenolog proofs were first coined as a term by this paper in 1985 by Shafi Goldbausser, Sylvia McCauley and Charles Rackoff. And Shafi Goldwater and Sylvia McAuley, I believe, both got the Turing Award in 2000. Something for their work in zero knowledge proofs, which is kind of like one of the highest awards you can get in cryptography and mathematics. So this particular protocol was interactive protocol, meaning that two parties kind of have to go back and forth to make the proof actually work. And zkSNARKs are kind of the first widely used form of a non interactive protocol. And right now if we talk about ZKPs, most of the time people still refer to ZKPs as ZK snarks and obviously there are a ton of different Z launch proofs. But ZK snarks are, I think, still at this point probably the most popular form of ZKP.
00:20:42.720 - 00:21:27.532, Speaker B: And the term of ZK snarks was first coined in 2011. Some of these names and authors you might actually recognize today. In 2013 there was a famous paper that came out called the Pinocchio Paper from researchers from Microsoft and IBM and that was like the first kind of general computing application for SIKI snarks. And only in 2016 with the paper Groff 16 by Jan's Groff was it made fairly efficient for us to actually use today. So I'm not going to go over exactly how zekusnarks work because that's kind of a large topic. However, a ton of people have explained it extremely well. So to break up ZK snark there's fairly five steps.
00:21:27.532 - 00:22:05.980, Speaker B: So first is computation. So how do we break down the computation to arithmetic circuit? There's something called rank one constraint system. Then we transfer that to something called a quadratic arithmetic program. And finally last step is a snark. So if you are interested in learning more, there's a great tutorial that actually explains the first four steps fairly well. And the last step is explained by Maxime Petkis in his tutorial that goes very deeply and very thoroughly into how a snark is constructed. So if you're interested, this tutorial is actually amazing and probably the most readable thing I've ever read on Ziki snarks.
00:22:05.980 - 00:23:10.960, Speaker B: And there's another article for BLS twelve three one Curve. If you ever heard of that one and wondered what is this curve? Why is it such a big deal? There's actually a great article that explains why it's such a big deal and why it's being used in cross 16. Starks so, kind of long story short, there is a statement that gets transferred to a language of polynomials and there's a prover and a verifier and a challenge such that there's a kind of a hard coded common reference string or structured reference string that is used as part of the challenge. And together all these things kind of create this proof system such that approver can create the zero knowledge proof that a verifier can then verify. So this is a very kind of high level of what ZK snarks are. So I kind of mentioned this proof system called gross 16 or a type of ZK snark. And why is it so great? Because it's actually still used today and it's, I think, still probably considered the standard today.
00:23:10.960 - 00:24:09.540, Speaker B: And zoolaunch proofs are typically created on three things prover time, so how long it takes to actually generate the proof, proof size, how physically large the proof is going to be, and verification time. And for graph 16 in particular, the prover time is pretty good. The proof size is constant and it's super small and the verification time is also constant really fast. There's a lot going for graph 16 and by a lot of metrics, it's still actually relatively unbeatable. There are different proof systems for different metrics that have beaten gross 16 in the past, but I think overall it's actually a very good system. There's also a couple other kind of criteria that ZKPs are graded on that I kind of skipped over. So Ezekiel's Narcs are great, except for one big downside that a lot of people talk about very frequently, which is that they require this thing called a trusted setup.
00:24:09.540 - 00:25:24.910, Speaker B: So if you kind of learn more about how Zeke's Narcs work, they do require that common reference train or structured reference train that I mentioned earlier. And to get that to be as part of the challenge, there has to be a trusted setup, meaning that some people have to come together and provide their randomness and if all of them collude, then potentially the system could be flawed. Now, I think practically the chance of that happening is actually pretty small, but people have been kind of questioning whether or not we can have a better proof system that doesn't require the step to be better for a decentralized technology. So in 2017 onward, there's been an explosion of academic research in the Zoom launch proof space to figure out, okay, can we use something else to kind of get rid of the trusted setup or at least replace it with something better? And a lot of these new proof systems that I'm going to kind of mention really quickly actually came out due to this paper of cote polynomial schemes. And I found this amazing tutorial that actually explains them. If you guys want to kind of dive deep into exactly how that works. So 2017, we had a ton of research.
00:25:24.910 - 00:26:00.200, Speaker B: 2018 is when Starks came out. 2019 was an explosive year for new gen launch proof systems. This is where Sonic came out, halo came out to kind of name some of the more famous ones. Plonk obviously, 2020 was a great year as well. 2021, there's only one that I kind of paid attention to. If there are others, please link me, I can update the slides. So there is a ton of these new kind of replacements for Graph 16 that either have no trusted setup or universal setup or some other alternative.
00:26:00.200 - 00:26:34.220, Speaker B: And in terms of all these, I guess the question is, well, which one should I pay attention to? There's so many. And so I would say probably these are the ones that I would pay attention to. So Bulletproofs, I believe they're used in manero to prove the range proof Starks. There's an entire company behind it called Starkware. I think they're going to be actually talking next. Plonk, Astec is a company behind that, and they're going to be talking next as well. Halo Two was developed by Zcash, and I believe their next version of Sapling that they want to do is going to be based on Halo Two.
00:26:34.220 - 00:27:28.576, Speaker B: So these are kind of like the four that I would say out of all of these, if you're curious, I would kind of pay attention to these the most. So, still coming back to Graph 16, it probably has the biggest tool set currently, and I'll kind of brush on this tool set a little bit later on as well. But there are a lot of what's called DSLs or domain specific languages that are written for Graph 16 that make it extremely easy for you today to actually write circuits in a fairly easy way. So it's kind of like the huge power of Graph 16 is this tool set and this community around it. Of all these people that are doing so much research, plonk is another one they want to highlight. And once again, I think Astec is going to be talking next. I don't want to steal their fire, but they're working on something called Noir, which is domain specific language for Plonk.
00:27:28.576 - 00:28:18.390, Speaker B: So this is an entire kind of ecosystem that's still kind of earlier than Graph 16. And so you're probably not going to have as rich of a tool set, but there's a ton of research happening here as well. The thing about Plonk is that for the first time there is a proof system that even though for some metrics it's still slower than Graph 16, there's a ton of promise that with this research, it's going to be seen a lot of improvements. And then Halo is the other one that I mentioned, or Halo Two in particular. This is the proof system that's being developed by CCASH team currently. And I think it's still like a work in progress. I'm not entirely sure how much you can do today with it, but if you want to learn how Plonk works, which is the thing that's actually related to how Halo Two works, there's a great tutorial that I linked by Vitalik if you want to kind of dig more into it.
00:28:18.390 - 00:28:53.984, Speaker B: Okay, so this entire talk so far has been kind of theoretical about all these different proof systems. And I do want to highlight some examples of where Zenolaj proofs are used in Ethereum today to kind of give you inspiration of how achievable it is to actually use them today for different applications or smart contracts. So first category is gaming. So for Ethereum, everything's out in the open. The state is public. There are so many games out there where you might not want that, you might want some privacy. There are certain games where a player's state should be hidden for whatever reason.
00:28:53.984 - 00:29:24.600, Speaker B: And currently it's really hard to do because everything's out in the open now with zero knowledge proofs. There are ways that you can do that. So one obvious example is Dark Forest. And Brian is going to be talking next about this as well. But what's amazing about this game is that a player has some hidden state, primarily the coordinates of their planets, and they're able to do this with zero knowledge proofs. If you haven't tried it, I highly, highly recommend it. Everything is in the browser and so it kind of shows you the power of zero knowledge proofs.
00:29:24.600 - 00:29:58.532, Speaker B: I think there's a misconception that the computation is super expensive, which to be fair, depending on the computation, it is. But there's a lot of these applications don't require really complex operations and so there's a lot you can do with a very good user experience. So Dark Forest happens entirely in the browser. Every time you make a move, you actually make a zero knowledge proof and it's a very good user experience. So the other category obviously is private transactions, which kind of Anna mentioned as well. So again, same question. Ethereum has everything out in the public.
00:29:58.532 - 00:30:33.968, Speaker B: So if you did want to do a privacy layer, how would you do it? Obviously zero knowledge proofs to the rescue. So another notable example is Tornado Cash and it's a privacy layer on top of Ethereum where you can in zero knowledge proof, sort of like mix your coins with somebody else. And then obviously scalability, which is kind of like the theme for today. And there's a ton of Zkurola projects, some of which are actually going to be speaking after me. There's StarkWare's. StarkNet, which uses starks, not starks. There's matterlab ZKsync, and I believe they're going to be talking today.
00:30:33.968 - 00:31:25.490, Speaker B: There's Loop ring, I believe Anna mentioned a lot more others. So this is once again of how you do scalability with your knowledge proofs. So for things that in terms of like, okay, you were Ethereum developer trying to figure out a project you're inspired. What can I use today? So Zocrates is probably one of the earlier projects for how to do a DSL with domain specific language for ZK Snarks. So Zocrates is great because provides really good documentation and kind of handles everything for you. So it's a kind of pseudo language that does certain construction for you in the background, and then it actually auto generates. This would verify a smart contract for you that you can very easily deploy so that you can verify your proofs on Ethereum and then create them somewhere else, like, let's say, in your browser application.
00:31:25.490 - 00:32:07.170, Speaker B: The one that I think is probably most used today is Circum and Snarkjs. So the two previous projects that I mentioned, dark Forest and Tornado Cache, actually use Circum. And Circum is kind of amazing. They've done everything in JavaScript and wasam in fact, most of their implementation is in JavaScript, which I remember when they first came out, there was kind of doubt of whether or not it could work or it could work efficiently. And so it's really cool to see that it's actually becoming a very dominant choice for DSLs for Ziki snarks. On top of Ethereum, they have a great tutorial that I linked there, and it's really easy to get started. They also help you with the trusted setup as well.
00:32:07.170 - 00:32:38.308, Speaker B: Amazing project. Highly recommend if you're inspired to do something with your launch per day. Other notable languages that I kind of mentioned earlier, there's something called Zinc from Matterlabs, and this is kind of a custom language that's a bit more similar to Rust. And this was designed by them primarily for ZK Sync. Then there's another language called Dwar. Once again, I think Astec is going to be giving a talk after me, and it's a domain specific language for Plonk. This is not the casenar.
00:32:38.308 - 00:32:42.250, Speaker B: It's a different genealogical proof system.
00:32:43.820 - 00:32:44.376, Speaker C: Cool.
00:32:44.478 - 00:33:09.270, Speaker B: And that's it. Hopefully, the goal of this talk was to get you guys inspired. So I hope you have questions. I don't know if we have time for questions or if that's a thing that's allowed here. I think we have a little bit of time, actually. I know there's at least one question, and I'm waiting for a few others, but one of the questions was actually about your project, Ironfish. Oh, cool.
00:33:09.270 - 00:34:02.690, Speaker B: Ironfish is focused primarily on Snarks, correct? Like, that's what you've decided to use. That's correct, yes. Is that sort of something you'd consider switching out if some of the other tech gets better? Like even some of that tech we talked about that doesn't need trusted setups? Yeah, totally. I mean, the reason why we went with it is because it has such a rich tooling and because of the metrics for a lot of use cases, it's still fairly unbeatable if you're going to be using something that's going to be saved on a blockchain proof size of under 200 bytes is like a great metric. To have because whenever you're saving something on the blockchain, space is definitely something that is a huge concern. So kind of that, combined with all the other metrics make it, at least for us, it made a pretty clear choice. Nice.
00:34:02.690 - 00:34:39.884, Speaker B: Why do you think you mentioned that there was this like you kind of listed all of these. The research that had come out year by year, 2019 is boom. And I know what you showed was like, the non trusted setup kinds, but the general research was huge. So do you have any sense for why that happened in that year? I'm not sure. I think that year in particular was kind of experimental for different polynomial schemes. And I kind of mentioned Kate, but I believe Kate came out in 2010. And so the question is like, well, why hadn't all this happened earlier? Yeah, and I'm not entirely sure.
00:34:39.884 - 00:35:10.200, Speaker B: I think the ecosystem was kind of like what you were saying during your intro was kind of like ripe for the taking. A lot of people care about privacy, a lot of people care about scalability. And all of a sudden gene launch proofs, which used to be like kind of a niche topic from the 80s, is kind of gaining more and more traction. Totally. Yeah, I think that too. I actually think it might be the scaling. The fact that scaling became such a clear need that just sort of pushed this a little faster, I'm not sure because ZK roll up is using graph 16.
00:35:10.200 - 00:35:54.596, Speaker B: It's like the very first example by Barry WhiteHat, I believe was using graph 16 on a different curve, on the ethereum curve, but it actually relatively worked pretty well. And I think in terms of metrics, I believe it's still probably the fastest one despite all this research. So I don't know, I think academia might have gotten excited that for the first time ever, their research has a much shorter lifecycle. Usually it takes like 30 years for you to write a paper and then see it live in action. And here it's a matter of months for that lifecycle to actually happen. Yeah. One other thought is that maybe there had been some proof that it could exist in blockchain too.
00:35:54.596 - 00:36:11.820, Speaker B: Like that connection point had been made a few years earlier by Zcash. At this point, it was clearly like it was there to stay. I don't know. These are just some ideas. Okay. Another question. How can someone get up to speed if they want to understand the tech and math behind Snarks? That's a great question.
00:36:11.820 - 00:36:36.790, Speaker B: Yeah. So I'll post these slides. The other link that I'll add onto these slides somewhere is a GitHub link I believe actually was started by Matterlabs or somebody at Matter Labs, and it's called awesome ZK snark Links, I believe. And it's a compilation of a ton of tutorials and a ton of reading material for people who really want to dive in. So I'll add that to my slides before posting them on Twitter for those who are interested.
00:36:37.160 - 00:36:37.910, Speaker D: Cool.
00:36:39.080 - 00:37:01.870, Speaker B: So I'm just going to hold off a SEC to see if there's any other are. I think we're good. Okay. So I'm going to invite Alex Osdemir up. And thank you so much, Elena, for the to. I'm excited you shared this with this audience. It's good that they know.
00:37:01.870 - 00:37:06.540, Speaker B: Cool. Hi, Alex.
00:37:07.120 - 00:37:20.708, Speaker C: Hi, Anna. I actually realized I'm doing very well. We have a bit of a logistical issue to deal with, which is that we need to get my iPad connected so that I can present. I see. And I think what I need to.
00:37:20.714 - 00:37:21.924, Speaker E: Do to do that is to figure.
00:37:21.962 - 00:37:25.910, Speaker C: Out what the meeting idea is that the iPad can join it.
00:37:27.000 - 00:37:27.812, Speaker B: All right.
00:37:27.946 - 00:37:29.990, Speaker C: Is that information that you have?
00:37:32.940 - 00:37:38.760, Speaker B: I don't have any insight. I am only looking at a Zoom interface.
00:37:39.980 - 00:37:47.144, Speaker C: Likewise. I can relate. This is the admin, Alex.
00:37:47.192 - 00:37:47.740, Speaker B: Perfect.
00:37:47.890 - 00:37:57.570, Speaker C: Look in the chat there. In the zoom chat. Great. Yeah. Okay. Let me put this in. And thank you for helping this, by the way.
00:37:57.570 - 00:38:15.296, Speaker C: It says that I am waiting.
00:38:15.488 - 00:38:16.230, Speaker B: Okay.
00:38:19.100 - 00:38:27.370, Speaker C: Take one moment to promote it'll. Take just a few seconds, probably. Stand by. Yeah, no problem.
00:38:28.620 - 00:38:52.450, Speaker B: So we're going to be hearing about the evolution of circuit languages, and we actually did an episode on that pretty, I guess two months ago. So that's sort of further listening after this talk. People are curious. Check it out. I meant to talk to you about this, but I want to do an event just on languages very soon. So now I have you live.
00:38:53.860 - 00:39:01.844, Speaker C: Yeah, sounds like a great idea. And it'll actually be exciting to go over this with visuals. The podcast was a lot of fun, of course, but it's a challenge, right? You want to build?
00:39:01.882 - 00:39:14.170, Speaker B: Oh, for sure. Yeah. No, I think this is going to be such like the two together, I think, are going to be fantastic as a resource. Is your tablet showing up? Let's see.
00:39:15.820 - 00:39:17.416, Speaker C: The interface on it tells me that.
00:39:17.438 - 00:39:23.324, Speaker B: It is waiting, so apparently I don't think they're seeing the iPad yet.
00:39:23.522 - 00:39:30.684, Speaker C: Okay. Maybe I should convince it to rejoin or something.
00:39:30.802 - 00:39:35.356, Speaker B: Yeah, luckily, I mean, we are actually a few minutes ahead of schedule, so.
00:39:35.378 - 00:39:36.612, Speaker C: We have time for discussion.
00:39:36.696 - 00:39:45.010, Speaker B: We actually have a bit of time. Sorry to the audience that you're watching Troubleshooting, but at least we're on track still.
00:39:45.460 - 00:39:46.112, Speaker C: Yeah.
00:39:46.246 - 00:39:54.704, Speaker B: So yeah. Andrew's asking. I'm not seeing the iPad. Can you you maybe you should try again. I don't know exactly how to Trouble.
00:39:54.752 - 00:40:00.872, Speaker C: I'm going to restart the iPad and just try to join again. I think that's probably the thing to do here.
00:40:01.006 - 00:40:01.690, Speaker B: Cool.
00:40:07.660 - 00:40:42.390, Speaker C: So how cool? Lana's talk actually seems like gave, like, a pretty good start to the story of the different languages available. Totally. So very much this will just be following up on that. Okay. We're open up Zoom round two. I guess in the event that this doesn't work too, I can actually just present my notes already written, which is not the end of the world.
00:40:50.930 - 00:40:56.834, Speaker B: What's the what is the slide? What are the slides actually on? Are they PDF or are they I.
00:40:56.872 - 00:41:05.750, Speaker C: Was going to handwrite them, I guess kind of like taking inspiration from the style that we teach in. Looks like it's connecting.
00:41:06.410 - 00:41:08.182, Speaker B: There's something showing up.
00:41:08.236 - 00:41:10.440, Speaker C: Yeah, my iPad thinks it's in.
00:41:10.890 - 00:41:13.100, Speaker B: Okay, that's good.
00:41:14.750 - 00:41:17.260, Speaker C: I guess I should try to share content from it now.
00:41:18.110 - 00:41:18.860, Speaker D: Yes.
00:41:19.470 - 00:41:24.810, Speaker C: Okay. Fantastic.
00:41:26.290 - 00:41:32.880, Speaker B: Screen broadcast. Yay. Okay. Very cool.
00:41:34.050 - 00:41:34.750, Speaker C: Yeah.
00:41:34.900 - 00:41:56.200, Speaker B: Okay. Very nice. I want to introduce you then. Let's kick it off. Okay, so we are now going to hear from Alex Osdamir, who's a PhD student at Stanford. He's interested in programming languages and cryptography, and we're going to be hearing from him about the evolution of circuit languages. So please take it.
00:41:56.650 - 00:42:19.550, Speaker C: Thanks, Anna. Yeah. So, as Anna said, I'm kind of like I guess my background is cryptographic, but also in programming languages. And one thing that's been really exciting for me is that what we've seen over the course of the past, maybe eight, nine years or so, is a real explosion of languages for building circuits for use in zero knowledge proofs.
00:42:20.450 - 00:42:21.726, Speaker E: And the result is a sort of.
00:42:21.748 - 00:43:07.006, Speaker C: Extremely rich landscape, a rich ecosystem of different languages that are serving different needs and work in different ways. And what I'm going to try to do in this talk is tell a cohesive story about all the languages that we've created. So let's start by thinking about why languages matter. And essentially the reason that languages for circuits are important is that zero knowledge proofs, as you know, ZK Snarks, they're very powerful and that they allow you to do really cool things, but they're also limited. And so what do we mean by the power? Well, the power is that we have some sort of problem with the pen now. It's exciting. Okay, we're all good.
00:43:07.006 - 00:43:43.686, Speaker C: So the power of these proof systems is that they allow you to prove a very rich language of statements. So what kind of statements do they support? Well, to be frank, you guys probably know this better than I, but I'll list a few examples. Common statements include I have the money. Maybe this is what the Zcash circuit is proving. You're proving that you have some previously unspent coin. Perhaps I know the credential. So if you're trying to take some action to modify some sort of distributed state, you might need the Credential to do that.
00:43:43.686 - 00:44:17.362, Speaker C: You could use a zero knowledge proof to prove that you have that Credential, that you know, it perhaps in some kind of distributed voting application. You might want to prove that we have the votes. That is, some agenda that you're pushing has enough votes to actually affect the global state. And these statements are useful primitives in implementing various kinds of application logic. And this is why zero knowledge proofs are so powerful. But the thing is, they're also limited. And the reason that they're limited is that you can't prove any of these things.
00:44:17.362 - 00:45:14.286, Speaker C: These statements are not what zero knowledge proofs give you. We often think that they are, but they're not. What do zero knowledge proofs actually give you? Well, what they actually give you is I know the solution to some equations and oftentimes we view the equations as a circle. So what do I mean? I mean that if what you have is something like x times y equals z if you're trying to prove that x times y equals z and, you know, values of x and y and z which make this equation true. You can view this also as a circuit where you've got X, you've got Y and you've got Z, and you're multiplying X and Y and you're checking for equality with Z. You can prove that this circuit evaluates to true. You can prove that these equations are satisfied.
00:45:14.286 - 00:45:53.422, Speaker C: And so what we have is we have a compilation problem, essentially, because the ideas that we have around the statements that we would like to make are very high level. So we have these ideas, but in order to get to writing proofs about them, we need to first turn those ideas into a circuit. So we have this problem to solve. And once we solve this problem, then we can use our favorite zero knowledge proof system to actually produce a proof. This is where the terms Grot 16 come into play. Graph 16 is a proof system implementation. Planck, maybe stark.
00:45:53.422 - 00:46:54.690, Speaker C: These are all proof systems. They're all essentially for turning circuits into proofs. But the question that this talk focuses on, and the question that circuit languages address, is how do we do that? First step, how do we go from the idea I have the money, to a circuit that encodes that idea? And what we would like to have in order to solve that problem is a language that we can use to describe these circuits in some kind of higher level fashion that's easier for us to understand and easier for us to express ourselves in. So circuit languages solve this problem, this problem of expression, and they've been undergoing quite an explosion of growth recently. I would even call it a Cambrian explosion. In the same way that life very quickly went from single cell organisms to dolphins, circuit languages are quickly getting more and more complex and more and more numerous. In my mind, the story starts in something like 2013.
00:46:54.690 - 00:47:46.994, Speaker C: This was when the first things that I would call circuit languages for proofs started to arise and it's been continuing through the present and has only been accelerating. So what I'm going to do in the rest of this talk is I'm going to walk us through this timeline from 2013 to now and look at the different ideas that have come to play in various kinds of circuit languages. And I just want to give a warning up front that the timeline that I'm going to give you is a little bit of a lie. A lot of the works that I describe were actually developed concurrently, but we're going to sort of put them in a particular order to try to help us understand the different ideas that they're working with. Let's get into that. So what's the beginning of the story for us? Well, the beginning of the story is what I would call part one, macro assemblers. So the very first circuit languages, perhaps we'll say that the beginning of our story is the Libsnark code base release, which was in 2014.
00:47:46.994 - 00:48:16.666, Speaker C: On June 2 in particular, I went and looked at the commit logs. And Libsnart is dominantly an implementation of proof systems. So it's dominantly an implementation of a zero knowledge proof. But inside of it, there was some machinery for building the circuits that you were trying to write the proofs about. And that machinery was called Gadgetlib. And Gadgetlib was essentially a C plus plus library for building circuits. And the way that it was designed is that it provided gadgets.
00:48:16.666 - 00:49:08.270, Speaker C: So sort of like C plus plus classes that wrap references to the circuit for modeling ideas like booleans, like machine integers, that is, fixed width integers that wrap around and overflow. And then also gadgets for various kinds of cryptography. So implementations of hash functions, implementations of signature verification, that kind of thing. Essentially what Gadgetlib is is it's a representation of this idea that you can use C plus plus abstractions for circuits. So Gadgetlib is saying, we don't need a whole new language. All we need C plus plus is a great language, it's got great abstractions. We're just going to try to use those abstractions to wrap various concepts that we want to embed in our circuit like a boolean.
00:49:08.270 - 00:49:55.520, Speaker C: And we're going to come up with C plus plus functions and methods for combining booleans in the circuit. And this is a good idea. It makes people very productive. But some people look at this, they look at this idea, use C plus plus abstractions, and they start to get nervous because C plus plus is kind of a crufty language. It's very old, perhaps venerable, and it's not known for having particularly solid abstractions. So some people look at this and they ask, what if you don't like C plus plus? What if you're worried about the kind of pervasive unsafety that C plus plus has led to in the development of system software? All this blockchain stuff is very security critical. It's very important that the circuit encodes the right thing.
00:49:55.520 - 00:50:54.500, Speaker C: And perhaps we're nervous about using a language whose abstractions have traditionally allowed programmers to express the wrong thing many times in a great number of damaging ways. So this concern that C, while the idea of having a macro assembler a library that helped you assemble circuits was useful. They were worried that C wasn't the right language for them. And this led the development of two other projects that we'll talk about. Now, the first of these projects is Bellman. So Bellman was developed starting in 2014, and it was built by the folks at Zcash and dominantly that the idea was that they built it in order to construct the Sapling version of the Zcash cryptocurrency. Their goal at a high level was just to take Libsnark and just move it into Rust, move some kind of Gadget description language into Rust, and build out the kinds of high performance gadgets that they needed for Sapling as they went.
00:50:54.500 - 00:51:29.258, Speaker C: And this went well. And Bellman is something that's still used. Matterlabs has continued to add to it and use it. I've used it for various projects myself, various academic projects. It's a great macro assembler, but in the same way that the folks at Zcash thought, hey, gadgetlib is great, but I want it in Rust, other organizations thought, Gadgetlib is great, Bellman is great, but I want a library that's in my other favorite language. So as an example of this, we have another macro assembler, Snarky. So this was built in OCaml.
00:51:29.258 - 00:52:10.570, Speaker C: It was built by the folks at Order One Labs and Development, or at least the first public release was in 2018. It continues to be developed today. It essentially is another another iteration of the same idea. We want to build some library in a different language for constructing circuits. Now we're just going to do it in no camp. So this continues to be useful, but it's not the end of the story, because one issue that you run into when you're building a library in some other language to model circuits is that this language wasn't really designed for circuits in the first place. It might have ideas latent in it that aren't useful for building circuits, and there might be things that are important in circuits that are hard to express through this language.
00:52:10.570 - 00:53:15.200, Speaker C: And you're kind of limited because you're working in this language like Rust or like OCaml, you're working inside some host language. You're just writing a library, so you don't have as much control as you like. And so this drove a push to construct languages explicitly for circuits. Not libraries that were writing on the back of some existing language, but a language for circuits. And the first step in this direction, I would say, was the construction of so called Hardware Description Languages, or HDLs for arithmetic circuits. And in order to understand this idea of using HDLs for circuits, we need to understand HDLs, and that means that we need to take a detour into the world of digital design. So what is digital design about? Well, it's about answering the question, how are computers made? Or really, how are computers designed? So if you open up your computer, inside it, you'll find a collection of chips perhaps Pentium, perhaps something in the Sandy Bridge family.
00:53:15.200 - 00:54:04.558, Speaker C: This is one of Intel's, I guess, now old generations of their architecture, perhaps the recent M One chip that Apple has built. And some people are excited about, maybe too excited about, I don't know. So what digital design is about is it's about building these computer chips. And this is relevant to the design of arithmetic circuits because what computer chips are is they are essentially digital circuits. If you open up one of these, then what you're going to find is you're going to find an and gate, you're going to find or gates, you're going to find these registers maybe connected to clocks. You're going to find all these different components wired together in such a way that you can do things like run assembly instructions. That's what CPUs do, that's what computers do.
00:54:04.558 - 00:55:06.066, Speaker C: And the natural question is, how do people, these things, they have billions and billions of gates. How do people design these chips that have so many gates? And the answer is they use hardware description languages. These are built with HDLs and there's a lot of HDLs, verilog, VHDL, Bluespec, maybe more recently, Chisel, more or less. People are still basically using Verilog system veralog. But what these languages are, is these are languages that were designed not to program a computer, not to wire up different components, but instead to connect gates to one another. And so one question that people ask is, hey, we got these great languages, these HDLs, for building digital circuits. Can we use those to build the kinds of arithmetic circuits that we need in Snarks? And of course, the answer is yes, you can totally do that.
00:55:06.066 - 00:55:18.950, Speaker C: So the first project, the most prominent project in this area is Circom, which was mentioned in the previous talk. This was a HDL, essentially for arithmetic circuits. That was developed by Jordy Balina.
00:55:21.290 - 00:55:21.702, Speaker F: Also.
00:55:21.756 - 00:56:00.094, Speaker C: Starting roughly in 2018. That's when it was first public. And it has a number of big ideas in it that are taken directly from the world of HDLs. And these are the ideas of constructing equations in the digital world. This would have been constructing gates and organizing those equations into modules and then connecting the modules to one another. So all these ideas, these are ideas from the world of hardware description languages. And Circom just brings those to arithmetic circuits.
00:56:00.094 - 00:56:36.826, Speaker C: So now with Circom, you can write a circuit in the same fashion that you can write an arithmetic circuit for zero knowledge in the same way that you would have written a digital circuit for a processor. This is pretty cool because people have been using the hardware description languages for a long time and they're reasonably good at it. But this isn't the end of the road. And the reason it's not the end of the road is that hardware description languages are nice, but they're not programming languages. So programming languages are a little bit different. Programming languages. They don't have this idea of gates being wired together and organized into modules.
00:56:36.826 - 00:57:57.914, Speaker C: No, instead, programming languages have stateful semantics. They have this idea of variables that you can change over time and if statements that you can either execute or not execute depending on some condition, you might have loops, you might have function calls with returns from those function calls and within loops you might have breaks and you might have continues. You have, broadly speaking, this notion of a sequence of instructions that gets executed or not executed depending on the inputs of the program and the systems that we have for expressing ideas in terms of variables, in terms of loose, in terms of if statements. We call these programming languages. It's not hardware description languages, programming languages and programming languages for those who have used both HDLs and Pls, programming languages people like more, they're easier for us to reason about. And so what we would really like is we would really like to be able to write down our circuit in a programming language or write down our idea in a programming language and have it be churned into a circuit for us. And this is a challenging thing to do because there's a core problem we have to overcome, which is that programming languages are fundamentally designed for the Ram register computational model.
00:57:57.914 - 00:58:57.814, Speaker C: So programming languages are designed with this kind of processing and unit in mind that executes instructions over registers, over data, and sometimes takes that data and either puts it in or gets it from some large memory or Ram. So this is the computational model of Turing machines. This is the computational model that our computers actually implement. This computational model is what programming languages are designed to abstract. But what we need in order to use a zero knowledge proof is we need a circuit, which is a computation written in the circuit model where there is no state, there is no notion of time, there is just a whole bunch of values that get combined and recombined into a single answer. The circuit model is much simpler in some sense and HDLs, they're designed to target the circuit model. But like I said, what we'd really like, because we'd like the Ram register model more is we'd like to be able to write programs and then use some kind of compiler to turn our Ram register programs into circuits.
00:58:57.814 - 01:00:06.818, Speaker C: We'd like to take that program that loop and unroll it into a big circuit that we can do cryptography on. And so the question of how you do this conversion or this compilation is a challenging one and this is what needed to be overcome in order to have true programming languages for zero knowledge. And so this project is a challenging one and it's something that people have been working on for a long time. I have here a few of the academic projects that have been working on this listed pinocchio Gapeto, Bikini these were the original people who tried to tackle this problem. And the way they viewed it is they wanted to take some existing programming language, they chose C and compile it or some subset of it into a circuit. And it should be obvious immediately that they can hope to do this completely. Why? Because C is like a super nasty language with really complicated ideas in it that are possible to implement in the Ram register model, but rather incompatible with the circuit model.
01:00:06.818 - 01:00:48.794, Speaker C: What kinds of ideas am I talking about? I'm talking about, for example, function pointers. So in C you can have a pointer to a function, this is a pointer to some code that's not known at compile time and you can call that function during the execution of the program. And it's just not clear what that would mean in a circuit. I guess it would mean like I look at some data and depending on what the results are, I run a different subsurcuit, this starts to get really hard to reason about. A similarly challenging idea is the idea of dynamic memory. So I have some huge chunk of memory and I want to access it at different locations depending on the inputs to my program. That's also pretty hard to express in a circuit.
01:00:48.794 - 01:01:31.360, Speaker C: But nonetheless, the researchers who are working on these projects, they thought that it would be interesting to try. And so they charged down and compiled the largest subset of C that they could manage into circuits and produced some compilers that were really interesting. They developed a lot of great techniques. But fundamentally I think that these projects were just too ambitious because despite their successes, there were still huge chunks of the C language that they don't handle or they handle incorrectly. And programming in the presence of a compiler that doesn't handle the input language fully is really challenging because you worry that you might write something down that the compiler is going to misunderstand. And so fundamentally, I think a less ambitious approach was needed. This was just too hard.
01:01:31.360 - 01:02:21.470, Speaker C: And so this brings us into part four of the evolution of circuit languages, which I would call DSLs. And so the idea here is that this premise that we want to compile programming languages to circuits isn't fundamentally broken, it's just too hard. And so what we want to do is we want to make it easier by co designing the language, the programming language with the compiler that's supposed to turn it into a circuit so we can design the language in a way that makes this problem easier. And there are a number of great works in this space. The oldest of them is Socrates, which has sort of like a Python esque language with built in support for field elements and various restrictions that make it possible to compile perfectly into a circuit. And since Socrates there have been a number of other projects. So there's Noir, which is being developed by Aztec.
01:02:21.470 - 01:03:04.246, Speaker C: There is Leo, which is developed by Alio, and there is Zinc, which is developed by Matter Labs. And this is not an exhaustive list. There are also other examples. But broadly speaking, all of these projects, what they're trying to do is they're trying to design some programming language specifically for zero knowledge that they because that language is restricted and because that language has been specially designed can subsequently compile into a circuit far more easily than compiling something like C. And so, looking at this landscape, I would point out that Zocrates has the best polish. It's the least buggy. So if you're doing something this second, I would probably recommend Zocrates.
01:03:04.246 - 01:03:36.206, Speaker C: But these other languages are much more ambitious in scope and they really do have great potential and they're extremely hot off the presses right now. I think that Noir was released literally like a month or two ago. But there's a lot of potential here. And so I would keep an eye on them. I expect them to long run, they will be quite great tools for expressing ideas that you want proven zero knowledge. So that's kind of the end of the main line of the development of circuit languages. I do want to take a moment to sort of do another detour into Starks.
01:03:36.206 - 01:04:36.730, Speaker C: So Stark is a different proof system. It's a new proof, a new kind of proof. It is, the proponents of it would say, more compatible with Ram register programming, more compatible with the idea of state and iteration. And the reason that people say that it's more compatible with this is it's built on repeated circuit application. So rather than building some big tree that is your circuit, some huge monolithic thing like the Zcash spend circuit, what you do with Stark is instead you come up with a small circuit, C that gets repeatedly applied. And so what you try to do is you try to view the computation, the statement you're trying to check. Instead of just being a standalone circuit, you want to express it instead as the repetition of a smaller circuit.
01:04:36.730 - 01:05:10.162, Speaker C: And so this is obviously a very different programming model. You're looking for common structure. And so that means that there are different programming needs that need to be met. And the people at Starkware have built this language, Cairo, that's trying to capture programs in this alternative model. So that's kind of interesting as well. Okay, so I want to take a moment and just reflect on what we've covered in this talk. The first thing I want you to take away is that there has been an explosion of languages over the past eight years or so.
01:05:10.162 - 01:05:53.970, Speaker C: Languages have gone from being simple things like Gadgetlib to far more complex things like Zocrates and Zinc and Cairo. The second thing that I want you to take away is that in the design of these languages, there are serious compilation challenges. This is harder than. Building a c compiler. This is harder than building L of M because it requires you, for example, to move between the Ram register model and the circuit model. It requires you to get rid of powerful and hard to express notions like variables. And because this problem is so challenging, I think the third thing that you should take away from this talk is that we really need to continue to explore.
01:05:53.970 - 01:07:30.190, Speaker C: We don't know the best language yet, we don't know the best way of writing languages yet. But even as we explore, we don't want to waste effort because this problem is so hard if we try to start it from scratch every time we build a new language, that's going to be a lot of work. And I think more concretely, what will actually happen is that the languages will be bad. They'll be missing huge chunks of things that would be useful because it just takes too much time to build everything again from scratch every time. And so the final reflection I have is I think it's worth asking can we reuse compilation infrastructure? So Zocrates has a great compiler, but if you want a different language, if you want to build on Zocrates, is it possible for you to, for example, reuse the infrastructure that underlies Zocrades? Or that probably isn't doable, but is it possible for you to build on some kind of shared infrastructure that can support building new and exciting languages for Snarks? And I would argue that the answer is yes. This is possible in the same way that the computer science community has constructed LLDM, which is this superpowered Ram register compilation infrastructure that allows people to compile Rust and C plus plus to Arm and X 86. We could build a circuit compiler infrastructure which allows you to go from a language like Socrates or a language like Zinc, or a language like Noir.
01:07:30.190 - 01:08:24.754, Speaker C: You can imagine all these languages building on some kind of shared infrastructure that helps them on the back end, not just produce arithmetic circuits, say, written as R One CS, which is a particular arithmetic circuit format, but also produce the kinds of arithmetic circuits that you need for other proof systems like Plunk. This is like what we would want, I would argue, if we want to continue to explore different languages without reinventing the wheel every time we need some infrastructure that our different languages can share. And I think that working on this is a really interesting thing to do. And I've been spending some time on it with a few friends of mine. We've been building what we call Cersei for circuit compiler. We think that it might be this infrastructure. If you want to learn more about it, you can go back and you can watch the ZK study called Videos.
01:08:24.754 - 01:09:02.686, Speaker C: We did a presentation on it at some point. Or you could go to this URL where we have a paper that talks about these challenges of common infrastructure and talks about our take on how you might build it. Okay, so again, once more to recap. Over the past decade or so, we've seen a real Cambrian explosion of circuit languages. These languages are solving a hard computational challenge, moving from the Ram register model of variables and loops to the circuit model where you don't have those concepts. And the jury is really still out. We're not sure what the best language will be.
01:09:02.686 - 01:09:23.880, Speaker C: We're not sure what the right abstractions are. We're still trying to figure it out. And I think that one thing that we should try to do is as we continue to explore this space, we should try to build as much shared infrastructure as possible so that we don't have to reinvent the wheel every time that we construct a new circuit language. So, thanks for your attention. I'm happy to take any questions now at this point.
01:09:24.490 - 01:09:38.502, Speaker B: Cool. Thank you so much for this talk, and it's been awesome to see it visually. It's very cool. Question from the audience. Why do ZK proofs need to run on circuits and not as programs?
01:09:38.646 - 01:10:14.726, Speaker C: In a nutshell, it's a really great question, and there's two answers to it. The first answer is that to run a ZK proof, what you do is you do a lot of cryptography, you do a lot of math. And one thing that makes doing math easier is if you know what you're working with. And so the problem with a program is you don't know how long it's going to run. This is an object that changes over time. That's something that's hard to do math on. So the reason that we use circuits instead of other kinds of programming models is that circuits have fixed size and they're easier to build proofs for.
01:10:14.726 - 01:10:33.020, Speaker C: If we could build proofs for your CPU for assembly, we would. And this takes me into the second answer, which is that the people who work on Starks say, actually, you can. And I personally, you know, the jury is kind of out. We'll see how well that works. I think that's not going to pan out, but I could be wrong. But if it did work, it would be very important.
01:10:35.950 - 01:10:41.934, Speaker B: One question is actually do you have the slides? Will you be sharing these slides shortly after?
01:10:41.972 - 01:10:45.440, Speaker C: Yeah, I'm happy to. I can send them to, I guess, the ETH people.
01:10:47.970 - 01:11:03.110, Speaker B: Maybe another question here is, so, like, I think you've highlighted something that needs to be fixed and improved. Is creating that sort of center point, are there any other things in the general language ecosystem that you think need work where people could potentially direct their attention?
01:11:04.090 - 01:11:25.930, Speaker C: Yes, I think there's an extremely long list of things. What are these things? Support for a richer set of types. Support for large integers. Support for various kinds of memory embedded in the circuit. Complete support for the primitive things. Having integers of any width. You want having field elements, being able to embed other fields like baby jubjub.
01:11:25.930 - 01:11:43.602, Speaker C: Inside your current circuit, there's a huge list of things. And while this is my wish list, this is what I wish every language supported. I recognize that as long as we sort of try to do everything independently, it's unrealistic to get all these goodies unless we try that to pool efforts. Cool.
01:11:43.736 - 01:11:55.560, Speaker B: Another question. Does research from Snark provers and Stark provers build on each that. Is there some connection point there?
01:11:56.410 - 01:12:30.000, Speaker C: Intellectually, for sure. So when people started figuring out how to build these small circuits that get repeated in Starks, you bet, they looked at how we had been building circuits already to figure out how to do that. And also there's been cross pollination in the other direction as well. So at least some people who think about Planck actually sort of view it as exposing a similar abstraction. It's not an exact match, but they draw on Stark ideas as well. So at an intellectual level, 100% there's been cross pollination and there will continue to be cross pollination at an infrastructural level. I think it's also possible, but it's an interesting question.
01:12:30.000 - 01:12:47.000, Speaker C: So this is not something my friends and I have explored so far, building common infrastructure that can be shared between Starks. Not just like common ideas, but actually a common program that you could run to Starks and other things. But I think it would be an interesting thing to explore. If you're interested in that, come talk to me.
01:12:47.770 - 01:12:57.910, Speaker B: Here's a question from our next panelist, actually, Barry. How can Cersei take advantage of Planck's specific optimizations?
01:12:58.890 - 01:13:32.978, Speaker C: Yeah, this is a great question. So what I actually want to do is scroll down a little bit and look at this picture we have of the infrastructure here. And what I would say is on the back end here, I've sort of just drawn the different kinds of representations you might produce as separate things. But actually what you really are going to have here is you're going to have sort of a branching tree. So what you really want is you wanted to design the back end of this infrastructure so that it starts by doing the things that are common to all proof systems. So for example, all proof systems work in arithmetic over finite fields. So step one is sort of make everything a finite field.
01:13:32.978 - 01:14:05.840, Speaker C: You want to do that whether you're targeting Plonk or whether you're targeting Arg, One, CS or some other format. And then after that, you want to apply increasingly proof system specific optimization. So yeah, at some point you want to run Plonk specific optimizations. And there is space for that in this infrastructure. I guess what I would say is like common compiler infrastructure, the idea isn't that every single thing gets used by every single target application. It's that you try to use as much as possible of what already exists for your target application. And if you need to add more specific things, that's okay, too cool.
01:14:06.450 - 01:14:28.200, Speaker B: All right, so I think we're at time. Thank you so much, Alex, for this great talk. And I want to now invite Barry WhiteHat to the stage. Hey, so Barry's going to be talking about the limitations of privacy. Do take it away.
01:14:28.730 - 01:14:38.040, Speaker F: Oh, yeah. Okay, let me just share my oh, totally. OK. Can you see?
01:14:41.710 - 01:14:48.910, Speaker B: Yes, I think yes. Now I see that I saw something else before. I see the slides.
01:14:49.810 - 01:15:21.570, Speaker F: Good. Yeah. Let me just make it full screen. So hello, everyone. Hello everyone. And thanks, Anna, for introducing me. And today I'm going to talk about some of the limitations of like basically, what I want to talk about here is that when we want to make private applications, we start out thinking that, oh, well, we can just take Ethereum and make everything in Ethereum private, or take a random blockchain and make everything inside it private.
01:15:21.570 - 01:15:49.662, Speaker F: But it's actually more complicated than that. It turns out that making private things is this kind of complicated and private paradigm. And that's what I'm going to try and explore today. So let's dig in. So let's take an example of some things that we want to make private. For example, we want to be able to make private transfers so you can just send private money to people. We want to be able to make private trades on an exchange so other people can see what you're trading.
01:15:49.662 - 01:16:31.054, Speaker F: And we would like to be able to make private smart contracts and more general things. But in this talk, we're going to discuss why that's not really possible without some major cryptographic breakthroughs with the same kind of trust assumptions that we're comfortable with or we currently have. So let's start a bit slowly, and let's take an example of some of the applications that you can use to make private transfers. So, for example, we can use Tornado Cash. Tornado. Cash is a mixer on ethereum. So basically, you can deposit in this picture, it's 0.1
01:16:31.054 - 01:17:03.810, Speaker F: Ether, and you get added to a group of people who've also deposited 0.1 Ether. And then you're able to make a zero knowledge proof to prove that you were one of the people who performed a deposit and also prove that you haven't performed a withdrawal. And if you can make those two proofs, then the smart contract will let you withdraw 0.1 Ether. So that's how tornado cash works. And Aztec and ZK Opera are similar to this, but they have, like, a higher scaling benefit.
01:17:03.810 - 01:17:43.930, Speaker F: So Aztec is a ZK roll up. It's a ZK ZK roll up, which means it uses zero knowledge proofs for privacy, but it also uses zero knowledge proofs for scalability. ZK Optimistic Rollup is a little bit different. ZK Optimistic Rollup uses ZK technology for privacy, and it uses optimistic rollup techniques like what optimism and Arbitrum use for scalability. And all of these support private transfers and ZK Opera also supports private atomic swaps. So you could theoretically build a private exchange using ZK Optimistic roll up. Okay, so we can do transfers.
01:17:43.930 - 01:18:26.514, Speaker F: So let's talk about private trades a little bit more. So we have this idea of maybe people are familiar with dark pools. Dark pool is basically a way to prove it's. Like this idea from the financial world where you have an exchange where all of the orders that you place are private. The traditional markets that they have, all trades are published while they're searching for someone to match with them. And basically the way that it works now is that there's just some kind of central party that will promise not to share your orders with anyone else unless it can execute it, and then it'll just execute your order and give that to you. So we would like to be able to do that with ethereum as well.
01:18:26.514 - 01:19:21.700, Speaker F: But the problem is that you need to find someone to make your trade. And we're not really comfortable having a centralized party that says, oh yeah, it's okay, you can trust us. We're going to make this trade for you, and we're not going to tell anyone what trade you want to make until we're just going to settle it for you. We're just not really comfortable making that assumption because who would we even pick to do that? Who would we pick such that everyone would say, oh yeah, I'm okay to trust that person to do this? So instead we want to do this in a private way, and we know that we're able to settle the trade with ZK Optimistic roll up, for example, or with some other roll up that ZK ZK roll up or something else that we could build. But we're not able to find our trading partner, which is a problem. So one of the ways that we have to do this is to use a multiparty computation methods to try and find the person we're trading with. And I'll talk about that a little bit later.
01:19:21.700 - 01:20:20.230, Speaker F: So basically what we have here is what we've done so far is we've talked about the VK applications where there's like a private state that users are able to kind of okay, so imagine I'm in tornado cash when I deposit my 0.1 ether. I go into this kind of group and I have some secret information that only I know that lets me prove that I'm part of this group. That's my private state. But there's also this idea of global state that we have this kind of global state, which is the list of all of the people who have performed a deposit until now. That's like our global state. But the problem comes when we want to have global private state, right? When we want to have state that nobody knows, but also everyone needs to know it in order to be able to withdraw.
01:20:20.230 - 01:21:25.760, Speaker F: So this becomes a problem when we want to make private, generic smart contracts, because in ethereum or in cryptocurrency in general, all of our smart contracts need to have this kind of global state. Global state is any public variable in your smart contract. Anything that everyone needs to know. The example that I like to use is Uniswap, where we have, like, a global state, which everyone needs to know in order to make a trade, because you need to be able to make a proof about that state in order to prove you're not kind of cheating anyone. Yeah, so that's what global state is. And when we have global state that we want to hide, it becomes really difficult for us to do that because okay, let's imagine again, let's say we have a global state that we don't want anyone to know, and every user in the system needs to know that global state in order to withdraw their coins from uniswap, for example. That just is impossible, right? It's not possible to have both at the same time.
01:21:25.760 - 01:22:21.754, Speaker F: So let's take an example and dig a little bit deeper. So I don't know if everyone knows uniswap, but basically uniswap is an exchange where you have a token pool of pool A, token A and token B. And these tokens have been gathered together in a pool in a smart contract. And you're able to withdraw token B by depositing token A and you're able to withdraw token A by depositing token B. But you need to know the ratio between token A and token B. You need to know the ratio of the amount of token A and the amount of token B in the pool in order to find out how much of token B you're able to withdraw when you deposit a certain amount of token A. This is an example of global state, right? This is a global state that's constant.
01:22:21.754 - 01:22:37.540, Speaker F: Everyone needs to know. You need to know the ratio of the pool in order to be able to make a trade. So if you want to make a private version of this, we can make pool A and pool B private, but we can't make the ratio between pool A and pool B private because.
01:22:40.150 - 01:22:40.466, Speaker C: We.
01:22:40.488 - 01:23:14.880, Speaker F: Need to be able to prove that ratio is correct when we're withdrawing the amount of money that we want. So if you made it private, you can say that. Okay, but, yeah, we can make it private, though. No, you can't really do that. So you can say that, yeah, you can make it private, but then as soon as someone makes an update to the makes a single trade on Uniswap, no one else will be able to because they won't know the new state. You need to know the state in order to be able to make a transaction. Yeah.
01:23:14.880 - 01:23:53.802, Speaker F: So at the start of the call, anna was kind of alluded to this a little bit, and this is kind of something that we maybe have a bit of a disagreement about that Anna said that. Oh, yeah. So there's starting to be AMMS automatic marker makers, which is basically what uniswap is that are private. But the way that those have been built is using multiparty computation. And multiparty computation has a trust model that's a lot different than the trust model that we're used to. So, for example, zero knowledge proofs. In order to maintain privacy, you need to trust only yourself.
01:23:53.802 - 01:24:55.738, Speaker F: You are the only person you need to trust, yourself or one of everyone else, because there's an attack where imagine you're in a private system and everybody else decides to reveal, hey, this wasn't me, or everyone else decides to reveal their private information. Then you, by fact of by just being there as well, being the only person who didn't do that, anyone can look and say, oh, this must have been this person because it couldn't have been anyone else. So for zero knowledge proofs, you need to trust you, yourself and the general rest of the group. They can give up their privacy in order to kind of take your privacy away with multiparty computation, it's much different. With multiparty computation, you need to trust m of n trusted parties in this model. So basically, a multi party computation is where you take a secret and you split it up into N pieces? Yeah. You trust N people to hold a piece of this secret, and every time an execution happens, m of these people needs to come together to perform the computation.
01:24:55.738 - 01:25:39.050, Speaker F: So, for example, in the multiparty computation, uniswap, m of the parties would come together to perform every trade. So there's just a difficult trade off here where we have M parties and we want m to be really big, right? Because we want the number of people who need to come forward to break privacy to be a lot. But at the same time, if M people aren't available at one time, then we're not able to make a trade. Right. The liveness disappears. So we have this difficult trade off where we're choosing between liveness and privacy. So that's why I'm a little bit worried about multiparty computation for this use case.
01:25:39.050 - 01:26:23.610, Speaker F: I will talk a little bit more about how we use multiparty computation for other things. That is in a one of m trust model where just you need to have a single honest party in order to keep your privacy. Yeah, so that's the second method that's kind of used that we're not really talking about here. Mostly what I'm talking about here is your knowledge books, actually a little bit of MPC later. And then there's a trusted hardware model, which is basically I don't know, it's something I'm very skeptical about. It's basically saying that, oh, listen, let's just trust intel to do this. Intel produced this special chip that they say you're not able to inspect exactly what the CPU is doing, so you're not able to tell what's going on inside it.
01:26:23.610 - 01:27:09.210, Speaker F: So you can make some code, you can load it into the chip, and then you're able to execute that code, but you're not able to find out the new state. But I am just very skeptical about this because it just involves trusting some hardware manufacturer and that's just very difficult to validate that that is done correctly. Actually, it's probably impossible. I mean, how can you check a piece of hardware has been built according to specifications. You would need to analyze it on like a nanometer level. I'm not even sure we're able to resolve that layer, that level of detail. But anyway, that's my kind of side round about trusted hardware.
01:27:09.210 - 01:27:43.614, Speaker F: So let's continue. So that's kind of the end of part one. I've kind of talked about the different trust models that we have, and I also talked about the different techniques that we have to do things. And I also talked about this kind of global state, private state paradigm where you can't have both. And if you have both, you start to lose some of your you just basically lose privacy. And for this reason we can't there was talk before that, okay, let's make everything in ethereum private. Let's just spend all of our time to make everything in cryptocurrency private.
01:27:43.614 - 01:28:17.358, Speaker F: Private transfers, private smart contracts. And it's just not possible to do that right, because if you make things private, you lose this very important feature, which is private global state. No, you lose this feature of global state. You can still have private global state, but you can still have private global state. But you can still have private global state. But that's not really global, though. If something is private, it needs more people to kind of you can have global state, but you can't have privacy and global state.
01:28:17.358 - 01:29:04.830, Speaker F: Those two things just don't really go together. And this is something that Zach is going to talk about later because I think that Zach from Aztec, I think that his talk is going to go into more detail about the type of things that he does or that they do to try and get around this limitation. And I'm going to talk about that a little bit too. Let's go. So the first thing I want to talk about is uni rep. So this is like a reputation system that uses Snarks in order to have this ability to both risk and win and lose reputation. So what we wanted to do is, okay, what we're doing is we're building a reddit style social network where you're able to upvote people and you're able to downvote people.
01:29:04.830 - 01:29:50.080, Speaker F: And we have this concept of caramel. And the way that it works is that every week you make a post and you have a certain address. And during this week, people are able to kind of send you attestations they're able to send you kind of upfotes and they're able to send you down votes. And when I make a post, I attach my epoch address and at the end of every epoch I make a zero knowledge proof that gathers together all of these attestations all of my upvotes and down votes, I add them together and I add them to my profile. But this is a private profile. It's not public. Every time I make a post I need to so at the end of every week, we have an epoch transition where we move from the old state to the new state.
01:29:50.080 - 01:30:26.780, Speaker F: Everyone applies their kind of upvotes and downvotes and we start again with a new week. And basically what happens is that you have to have a certain number, a certain amount of karma or upvotes in order to be able to make posts. So this is a kind of a private reputation system that's kind of justing to people's upvotes and downvotes. We're able to have this kind of reputation system. So this is a project I'm pretty excited about and we're working on it now. And there is a lot of room here to do further work. There's more things that we can build with this.
01:30:26.780 - 01:31:20.890, Speaker F: It's a private reputation system. So there's a lot of possible applications. For example, we could make like a private COVID tracker. That was one project we were thinking about doing where every time you go out you would give your epoch address to the restaurant you were going to, for example. And if they have some COVID case, they will get airdrops, they will send you some negative reputation to your profile and then you won't be able to go out for a certain amount of time until you've done a quarantine. We're using this in this example to kind of quarantine people privately because the way that it works right now is if you go to a restaurant, you give them your phone number and they'll phone you if there is some problem. It's a little bit less private.
01:31:20.890 - 01:31:41.540, Speaker F: But anyway, I feel like I shouldn't have used that example. But anyway, that's UniRep. So there's a whole bunch of other things we can use unirap for. This is just the kind of beginning, the first step that I'm excited to see. It's an experiment. I'm excited to see how it progresses. Yeah, so I'm going to talk about the NPC stuff now, basically.
01:31:41.540 - 01:32:25.406, Speaker F: So there's two kinds of NPC and it's defined by the trust model. There's an M of N trust model where you have to trust M of the parties in the multiparty computation. And there's an M minus one multiparty computation where you have to just trust one party. And what we're doing here is we have like a simple NPC protocol where you say where we make a check that A equals B. And in this example we can say that A and B are exchanges that are trades that we want to make. So basically what we'll do is we'll go to some smart contract. We'll advertise that, hey, I want to make a trade on token pair X.
01:32:25.406 - 01:33:08.954, Speaker F: Yeah, I want to trade ETH for Die or something. And you make a post there and you put your IP address. And then what happens is people will contact you and say, hey, I'm also interested in making a trade on that pair. Would you like to perform this multiparty computation to see if our orders are compatible? So basically you agree to that and then you both do this NPC where you do some crypto stuff and you send some information back and forth. And then at the end of it, you see, you get two numbers and you say, does number one equal number two? And if number one equals number two, that means your orders are the same. If number one doesn't equal number two, that means your orders are different. If your orders are the same, you can see, like, okay, yeah, it's time to trade.
01:33:08.954 - 01:33:39.442, Speaker F: We have a trade, a match on our trade. It's time to execute this. So this is a nice project that's kind of coming along. We've been sort of waiting for a way to privately settle the orders, but now we're at the point where we hope that soon we'll have some kind of ZK private way to do atomic swaps. Once we have that, then we will be able to implement. Yeah, so that's fun. It's a nice kind of private way to do order matching.
01:33:39.442 - 01:34:30.680, Speaker F: But some of the limitations here are that you have to do this with a lot of people, right? Let's say you want to make a trade on some token pair and you're basically going out and just doing this thing with everyone to try and find someone who happens to match with you. And there's some attacks that people can do. Like someone can make thousands and thousands and thousands of orders that are fake and they can use that to kind of grind to try and find the order that you want that you have. So they can just set up like a million orders from zero to a million, and they can just repeatedly do NPC with you until they find the price that you want to sell at. So that's one problem. Another problem is that someone can do the NPC with you. It can turn out that, yeah, the orders match and then it's not possible to actually execute the order.
01:34:30.680 - 01:35:28.890, Speaker F: That's another problem that they're able to kind of back out of this. But, yeah, I feel like starting where we are now, it's a good first step and we can sort of try and tackle these other problems later. Another example of some so this is a fun one where, okay, so let's say that we want to build a reputation system that's basically like a friend of a friend thing, right? If someone's a friend of a friend, you can trust them a little bit because you. Know that friend and you trust that friend. So you can kind of transfer some of your trust in that friend to this other friend. So it's a reputation system that people kind of use in their daily life, but it's also hard to do it in a private way. So basically what we have is that we have a network of users that have connections to different people.
01:35:28.890 - 01:36:11.542, Speaker F: And this is kind of our global state, but we want to keep it private for the users. Right? We want everyone to just know who their friends are and not know who their friends or friends are. So imagine that someone is like, okay, I want to find this friend X. So what they do is they reach out to all of their friends and say, hey, I'm looking for a friend, but I don't want to tell you who I'm looking for. Can you perform this NPC with me to see if they're one of your friends? And the friend says okay, of yeah. So let's say we're looking for Bob and I ask Alice, alice, I'm looking for a friend, but I don't want to tell you who I'm looking for. Can you help me? And Alice says, sure, sure.
01:36:11.542 - 01:36:56.162, Speaker F: So Alice takes a list of all of her friends and I take the friend I'm looking for Bob, and we together perform this NPC together, whereby we'll get a single boolean result. We'll either get this Alice is friends with this person or not, based upon just checking each of her friends. And if one of them works out, that means we found a connection. So then we can use ZKPs on top of this to kind of find the connect, to kind of make proofs of these connections. Yeah, so you can make a ZKP and prove that this connection exists. But this is just really complicated because you have to ask everyone, I have to go to Alice and I have to ask her to check every one of her friends against this person I'm searching for, which takes a long time. It doesn't take a long time, but it's annoying.
01:36:56.162 - 01:37:47.846, Speaker F: And then if you don't find a connection, you have a choice between going one layer deeper in the tree and you can say to Alice, hey Alex, will you connect me with each of your friends so I can ask them if they're friends with this friend that I'm searching? Like once you start doing that, it just blows up because you can start searching many times for the same person. So it can be really difficult to track. Yeah. So that's my talk. Yeah. I have a little bit of time for questions now, but let me just wrap things up. So like, at the start we talked about how some of the private stuff that we've been doing, we then explored why global state is really important to smart contracts and why privacy and global state don't really go together.
01:37:47.846 - 01:38:22.660, Speaker F: We talked about some of the other methods that we can use to overcome this multiparty computation and trusted hardware. And I kind of complained about both of them a little bit. And then we talked about, okay, what are some other ways that we can approach this problem? So we discussed that, and here we are. We really jumped through a lot of hoops in order to do the kind of global search in the lab, Project Blindfind. But anyway, that was a talk, and I'm happy to answer any questions.
01:38:24.390 - 01:38:52.918, Speaker B: Sounds good. Okay. You challenged something I said, and so I need to talk to you about that. That's the first question. You did publish a really kind of well known post about how you cannot make private AMMS. This is going back to the AMM question. Private AMMS using ZKPs, and I don't know if you're aware, but there are some projects that are exploring doing that, but there are trade offs.
01:38:52.918 - 01:39:11.634, Speaker B: So they may be able to use ZKPs to shield the identity, but not the amounts. I guess when I put it in the list of potential privacy use cases, it does reference what you talked about, but it may not be the complete privacy that one would hope.
01:39:11.832 - 01:39:34.774, Speaker F: Yeah, I understand. Yeah. So, okay, so there's kind of two directions here. There's one is the NPC, which we've seen some interesting stuff published from Andrew Miller, I think, recently. The other approach is, yeah, that's totally possible too. I guess that I would argue that it's not covering that. It's just hiding the user.
01:39:34.774 - 01:39:39.770, Speaker F: Right. It's not hiding what you're yeah.
01:39:39.840 - 01:39:48.126, Speaker B: Is it a private AMM? That's maybe the question maybe the word private AMM doesn't exactly capture that in that case.
01:39:48.308 - 01:40:22.886, Speaker F: Yeah, possibly. I'd also like to refer those people to a paper by Mary Mallor a couple of years ago about how so this is a really fun paper. So what they did was they tried to find they did an analysis of the Zcash private pool. And you see, the way Zcash works is you can deposit, you can withdraw, and you can transfer. So what happened was a lot of people would deposit, transfer once, and then withdraw. So it was easy to track because you could see the amount that got deposited, and you see ten minutes later it gets withdrawn. So they use this to kind of track a lot of what was going on.
01:40:22.886 - 01:40:34.474, Speaker F: So I would be concerned that that would also be possible with this kind of private unit swap. But I think it's for sure a useful experiment, and I would support people doing that, and if I can help them in any way, I would be happy to help.
01:40:34.592 - 01:41:09.400, Speaker B: Yeah. I also am really curious to hear more about as Aztec and Zach does his talk, because I know that there's some techniques that they're using to try to solve for these problems while still using zero knowledge proofs. It's almost like, I think, what you described, like these MPC type models. Like mixing it in a way, like having both of those things acting at the same time. Or I know that there's this idea of adding kind of pooling trades in order to sort of hide it through that. Like creating little mini, I guess, mixers as you do it.
01:41:10.490 - 01:41:10.950, Speaker F: Okay.
01:41:11.020 - 01:41:32.640, Speaker B: Anyway, I'm excited to find out more. I learned about this pretty recently. So I don't know, maybe is there anything like what's the future? I mean, you've described a few projects that you're working on right now, but are there any long shot projects that you're thinking about and maybe even you could give ideas to the people in the audience to start working on tinkering with?
01:41:33.250 - 01:42:05.978, Speaker F: I don't know. I normally try and be a little bit more conservative on long shot projects. I think that it's kind of cool to yeah, so okay, so one of the things that is, like, a big area to explore is to use the tooling that Aztec have used to make their ZkZk roll up to make some other recursive proofs, some other ZK app that's based upon recursive proofs. And I think there's a lot of room there to build some pretty exciting stuff.
01:42:06.144 - 01:42:19.470, Speaker B: Nice. All right, well, I think that wraps up this block. We're going to be taking a short break, but thank you so much, Barry, for your chat.
01:42:19.890 - 01:42:23.840, Speaker F: No problem. Yeah, happy to thank you for your talk.
01:42:25.810 - 01:42:31.120, Speaker B: We had a chat. There was a talk. Sorry it's late here in.
01:42:33.830 - 01:42:35.540, Speaker F: Yeah, thanks. Thanks so much.
01:42:38.950 - 01:42:59.050, Speaker A: Thanks so much, Barry. And I think this is also where I get to thank Anna for being an amazing co host for today. And you did an amazing job highlighting how this ecosystem has evolved and kind of kick us off with the rest of the day. So thank you so much for giving us your time and can't wait to see you all in person for the next Real ZK Summit.
01:42:59.950 - 01:43:00.890, Speaker B: Totally.
01:43:01.230 - 01:43:02.154, Speaker A: Thanks again.
01:43:02.272 - 01:43:02.940, Speaker B: Cool.
01:43:03.550 - 01:43:23.940, Speaker A: So with that, everybody, we're going to take a quick 15 minutes break and we'll be back on schedule with our next set of talks. So take some time to get some coffee or eat or drink or whatever based on the time zone you're in, and we'll see you all in 15 minutes. And in the meantime, enjoy some nice lo fi beats. So I'll see you all in 15.
01:58:32.140 - 01:58:32.890, Speaker D: Okay.
01:58:36.060 - 01:59:14.404, Speaker A: We'Re getting some feedback. Hope you had some time to stretch and take a quick break. We are ready to resume with the rest of the day. We have a few more talks going on and I'm happy to talk about our next talk. So the next talk is going to be by Alex from Labs, and he's going to be talking about Zkevm. It's a really exciting topic and I'm super excited to have him introduce a handful of new interesting concepts here and talk about how they work under the hood. Because of our time zone, we ended up re recording this talk and I'm going to be playing the pre recorded video.
01:59:14.404 - 01:59:26.090, Speaker A: But Alex is also here with us live and he'll pop over at the end of this talk and answer any questions live with us. So without further ado, I'd like to kick us off with Alex's talk.
01:59:34.320 - 02:00:39.244, Speaker E: Welcome to the talk about Zkavm, the technology that will extend ZK roll ups from application specific functionality to generic programmability. This has been considered a very difficult task and it was in fact impossible up till this year and many people thought that it will not arrive up till a few years from now. But I'm CEO and co founder of Metal Labs, the company behind the ZK sync protocol. And I can attest that Zkvm is coming to Ethereum this May on the testnet and in summer hopefully to production. And in this talk I'm going to go into the history of the ideas behind generic computation and zero knowledge proofs and we'll demonstrate why it was not possible, what changed, what breakthroughs actually make it available for us today. And hopefully I'll be able to give you a good intuitive understanding of its design. I will assume that you have some basic zero knowledge proofs understanding.
02:00:39.244 - 02:01:26.076, Speaker E: If not, please watch other videos. We don't have time in this 20 minutes talk. Let's start by recapping quickly how ZK roll ups work. We have a state of our blockchain or a roll up with some accounts, they have some balances, some data, they are packed in the merkel tree. We have a root hash of this merkle tree serving as cryptographic commitment to the state. And whenever the state changes because we apply one or more transactions to it, the root hash will also change and we can define the transition with a so called state transition function. So whenever we have a block, the new root hash will be result of the state transition function from the previous root hash.
02:01:26.076 - 02:02:29.936, Speaker E: The transactions data that are involved in this block, the accounts that are modified by those transactions and the merkle proofs for these accounts. And this is all information we need to construct a new root hash. This is in fact what you would place in a stateless client to verify a block. So if we only could implement the state transition function as a zero knowledge proof, this would allow us to implement any type of ZK roll up and we actually have this. So, ZK Sync is live on Mainet since June. It's been used for payments very actively and we have the very efficient provers, very efficient arithmeticization techniques, that is frameworks that convert the algorithms in the state transition function into the form which is digestible for zero knowledge probes. We have all the server infrastructure around it, the smart contracts.
02:02:29.936 - 02:03:20.128, Speaker E: So all we need to do in order to support generic compatibility is to make sure that the generic smart contracts can be efficiently arithmetized, and this is a big challenge. The Arithmetization is a process of converting the function into what we call an arithmetic circuit. It's a directed Acyclic graph where the inputs of your function are the initial nodes. And then you go through a sequence through a network of gates. Each of the gates represents an operation of addition or multiplication in a finite field. And eventually you keep accumulating your results and you get to the final output. And you can imagine it can be really complex if you have a big algorithm.
02:03:20.128 - 02:03:54.492, Speaker E: So something like simple arithmetic operations are very cheap. You can do them with one gate. Logical operations will take more gates. Hash such as shutter, five, six or even Ketchup will take a lot more of those gates. But that would be doable. The problem we have, or the challenge we have is how to make it generic. Why is this a challenge? If you're familiar with your knowledge proofs, you know that the Verifier, which in our case is a smart contract on ethereum, must know in advance the circuit.
02:03:54.492 - 02:04:53.590, Speaker E: So the circuit must be fixed. You can't change it on the go, you define it once, then you make some computations to generate a verification key and then you use this verification key to check the proofs. Now, with generic smart contracts, we want the users to be able to define the contracts and deploy them on this chain permissionlessly, which means we cannot rely on a single static rigid Arithmetization of one fixed state transition function. So we need to find a way how to have multiple contracts coexist. So let's go through the history of ideas that were tried in this area. We will start with an approach called Tinyram, which is already eight years old. And this is a really fantastic idea.
02:04:53.590 - 02:05:57.432, Speaker E: It solves the problem by saying let's create a circuit which verifies not an execution of a single algorithm, but let's make a circuit that can verify any algorithm by verifying the execution trace of the program. The way we do it is we assume that we will have N steps of execution. You can think of them as the cycles in the processor. And at each of these steps what we're going to do is we will read the inputs and the opcode from the program counter and we will do all of our operations. And then we will select the result of only the one operation which actually is supposed to be executed at this step. So this gives us a quadratic complexity or like number of steps multiplied by the number of operations. And the number of operations must be relatively small.
02:05:57.432 - 02:06:51.012, Speaker E: So we would go for something like risk architecture with a small number of opcodes which must be relatively equal in cost and we could then prove anything. How It Works to give you a better, more intuitive understanding, I made this model in a spreadsheet. Let me show it imagine that I want to write a program. So I want to turn this spreadsheet into a universal interpreter of a program. I will provide my program here in the yellow columns with the type of operation at each step and the operand. And I want this program to be executed every time, sequentially. So each step must be applied to the result of the previous computation.
02:06:51.012 - 02:07:51.500, Speaker E: We will start with accumulator, with a value zero, for example. And then I'm going to add 13, subtract five, multiply this by three, and so on. So we're going to support three operations, add, sub and move. And as you can see, when I change something here, all of the computation changes starting from this step. And finally, in this cell, we will have the result of our computation, of the application of all of this program. So the interesting thing is I can modify the program and I can modify the inputs, but the spreadsheets itself, the formulas, or essentially the circuit which I use to produce this final outcome remains the same. And if you look at how I constructed this circuit at each step, I do a simple computation.
02:07:51.500 - 02:08:51.228, Speaker E: It's the same computation for all of them. And it's just a simple formula which does the execution of all three of our operations, addition, subtraction and multiplication. And I use this simple naive approach to select just one of them. So if the operator is add, then I will just take this result because I will multiply it by one and not by zero. If the operation is sub, I will take this result, mal this result, and I just add all of them and I get only one outcome. So this is how tiny Ram works. So in this example, we have here only like static problem, without jumps, without comparisons, without memory access, but all of them are possible to implement.
02:08:51.228 - 02:09:45.616, Speaker E: We have some very efficient models that could be used for Tiny Ram. The problem is that it's going to be pretty expensive. If you compare a program written like handwritten as a circuit to a program which runs in a tiny Ram circuit, the tiny Ram must be approximately 1000 times larger in terms of the number of gates in order to accommodate this program. And for some simpler cases, just for simple computations, this will work. And even for some, actually, even for simple hashes, it will be already too expensive. But for hashes that are required for EVM, that would be prohibitively expensive. We won't be able to support it.
02:09:45.616 - 02:10:56.788, Speaker E: The cost will blow up immensely. So this doesn't work. It's been there for eight years, but it remained a very theoretic approach until recently because not usable. So we looked at this two years ago and we thought, okay, what if we use the recursive aggregation? So it's possible with zero knowledge proofs, to construct a circuit which verifies a proof of a different circuit, and we can dynamically load the verification key so we can verify circuits of different types. And there were approaches to this. The main problem was the cost of constructing such a circuit, which was solvable by using cycles of elliptic curves. So we could have each user will define their own circuit, or each developer of a smart contract will define their own circuit, publish it on the roll up, and then the users will take the circuit, provide the proofs, and then the aggregator will take all of them in the block and produce the proofs of intermediate results.
02:10:56.788 - 02:11:41.300, Speaker E: It can be nested many times in a tree like structure until we get to the final proof of the entire execution of the block. The problem was that cycles of elliptic curves were not available for efficient computation on Ethereum because we lacked the pre compiles. We only have a pre compile of BN two five six on Ethereum. This is the name of a curve we use. And BLS is coming soon, but nothing that supports the cycles of curves. So our first solution to this was two years ago to come up with EAP 1962. We implemented it at MetaLabs.
02:11:41.300 - 02:12:42.452, Speaker E: We produced two different independent implementations, one in Rust, one in C, plus plus. We made a lot of testing, fuzzy testing, a lot of discussions with core devs. But eventually the community deemed this to be too risky back then and too big a pre compiled to include. And it never got adopted. And I can understand that for a project like Ethereum being more on the conservative side with regard to what's going on on the main net is perfectly justifiable so okay, we thought, what can we do else? And last year the Aztec team came up with a number of interesting ideas on how you can structure elliptic curve arithmetics in a Plunk circuit in a more efficient way. We looked at that and we thought this is a perfect candidate for us. So we went ahead and implemented the first recursive Snark on Ethereum using Plonk.
02:12:42.452 - 02:13:36.410, Speaker E: We submitted it last summer in August for the Reddit Skating Challenge, and we actually implemented it in production and launched it in January this year for our Ziki sync roll up for payments, which made this roll up indefinitely scalable. So we are now only limited by Ethereum block size for data availability, but not by the zero knowledge proof computations because we can construct this tree with as many blocks, like smaller blocks of roll up as we need to submit on Ethereum and verify in one single check. The problem with this solution is it's not Turing complete. It required us to implement a new programming language. We called it zinc. It was Rust based and.
02:13:39.600 - 02:13:40.124, Speaker C: You would.
02:13:40.162 - 02:14:24.052, Speaker E: Need to rewrite your programs in a certain way. You would need to avoid unbounded loops, avoid recursion, and also think about how you structure your branching, because if you have too many nested branches, all of them would have to be executed and it could blow up the complexity a lot. We thought, okay, for most DeFi applications, this is probably not going to be a problem. They are not as complex, they do not require that many computations. A lot of programs are written in Viper, which is also a non Turing complete language. But then we realized that people actually really want Turing completeness. People wanted to have EVM.
02:14:24.052 - 02:16:25.680, Speaker E: We got this market signals that people really wanted to take existing programs written in solidity, reuse the tooling they have, reuse the audits they already invested heavily into and just not need to learn a new language and rewrite stuff. So we thought okay, can we embrace this challenge and actually implement it? So we looked at the third approach, which would be what if we could optimize tiny Ram to make it more efficient for this specific use case? And what brought us to this idea is this observation. If you write a smart contract in Zinc, like manually handwrite a circuit for it, then the smart contract logic itself would take a very small fraction of the prover cost structure and the bulk of the prover costs will go towards access to memory, access to storage signatures, hashes, other heavy operations. So if we could apply tiny Ram to this small fraction to the actual contractual logic, then even 1000 times overhead would not be a problem because overall it will not make a big impact on the prover costs for this transaction. Maybe we'll just increase it twice, or maybe by 30% by 50%. And the heavy operations, we could write specialized circuits for them. What we could do is take our tiny Ram circuit and split it in parts and have one part for actually tiny Ram with execution of arbitrary trace of opcodes and then have signatures and hashes done as separate parts of the circuit which can be addressed by these operations.
02:16:25.680 - 02:17:31.136, Speaker E: Now, the problem with this approach is you can't really have too many of these specialized operations. Maybe you need to pick one or two because otherwise it's an impossible optimization problem to pick what fraction of the circuit should be reserved for each of the operations. Because if you just reserve equal number of gates for storage access and for Kchak hash and for Shuttle F six, then it would be very inefficient if some block does not utilize all of it, because the blocks and transactions are very heterogeneous like. One transaction might require a lot of storage accesses, another transaction does a lot of EdDSA signature checks. So you need a lot of Kchak hashes. And yet another transaction uses shutter five six for some reason, so it's not clear how to dispose it. And this is why it's not suitable for Zkavm, it is suitable for generic programmability.
02:17:31.136 - 02:18:31.812, Speaker E: So you could write a separate language and force developers to think about it and make certain trade offs and only have hashes, for example, and build everything on top of cryptographic hashes like not Kchak, not Sha, two five six, not Ethereum compatible. And I think this is the only approach which you can do with Starks. And this is very efficient with Starks for the reasons of how Starks are structured, but it doesn't allow us to make Zkvm. And this was the goal, so we could not go for this. And this brings us to the final approach which we actually implement in ZK Sync 2.0, which is a combination of Dynam optimized circuits for specific heavy operations and the recursive aggregation of all of it. And the way it looks like is this we have transactions, like users submit transactions in form of, or like smart contracts in form of bytecodes.
02:18:31.812 - 02:19:27.136, Speaker E: We have separate circuits of different length structured by powers of two for the execution of the tiny Ram part of those contracts. And then we delegate all the heavy operations to specialized circuits which are then aggregated recursively along with all the proofs for the transactions. If we need more of Ketchup hashes, then we just add more of these circuits to the mix. If we need more storage, we just add more storage. If we need more EdDSA signatures, we add more of those circuits and we can efficiently aggregate any number of them the way I described before, to get the proof of the execution of the entire block. And we also heavily optimized the specialized parts over the course of the last year. So we added specific things for two five, six bits.
02:19:27.136 - 02:20:22.990, Speaker E: Arithmetics, we have very efficient Snark friendly hashes. We have pretty efficient non Snark friendly hashes as well, because we take advantage of the Plunks custom gates and lookup tables. And this gives us a boost of ten X, or even more in some cases, so we can do it very efficiently. The way things work with Zkvm is you take your code written in solidity, for example, like the language, you know, you compile it with your normal compiler into U. And from there we have an LLVM based compiler using a lot of optimizations from LLVM. It's a great platform for compilers to produce the Zkvm bytecode, which you can then permissionlessly deploy on the ZK roll up and execute. And we actually have done a simulation on.
02:20:22.990 - 02:21:13.480, Speaker E: We took the transactions from L One for some frequent DFI use cases. We fetched the execution trace and we looked at what the prover cost will be in this case for us, like with overall costs, with recursion, with the heavy operations, with the tiny Ram part of it. And we came to the proverb cost of like one to $0.02 for any transaction we analyzed, which is very doable. And on top of that, you will have to pay the cost of data availability, which depends on the gas price if you use ZK roll up, if you use something like ZK Porter. Our alternative approach to data availability, that's going to be a lot less. So you will overall pay only like, two $0.03
02:21:13.480 - 02:21:34.550, Speaker E: for a transaction on a ZK Porter. But this is your base cost. And yeah, this scales a lot. So, as I said, this is coming to testnet in May and hopefully to production in summer. And we have a few minutes left. I'm happy to answer any questions. Thank you.
02:21:40.920 - 02:22:15.132, Speaker A: All right, minor technical difficulties. Alex, that was an awesome talk. I was really impressed with how visually explainable everything was in terms of how you went through tiny ram. And thanks also for sharing the actual spreadsheet on the chat. So this is awesome. I think we're getting a couple questions in. And the one kind of question, which I feel like may have been slightly answered later on in the talk, but I'll ask it again in general is this is from Maurice.
02:22:15.132 - 02:22:31.508, Speaker A: And the question is, when you were talking about the size of the circuit, why was Ketchax so much worse than shot 2256? And is that a difference in interpretation or what's kind of the background on that?
02:22:31.674 - 02:23:04.480, Speaker E: It's a different algorithm. It uses broader state with more bits, and it has more rotations, more manipulations of those bits. I'm not the best person, by the way, to answer the deep topics in the crypto implementation side. We have a big separate team for that, but I'll just do my best to explain. Yeah, so it's just a different algorithm which is less snack friendly than shot two five six, and both are way less friendly than algebraic hashes.
02:23:05.220 - 02:23:14.500, Speaker A: Hopefully that answered your question. Morris, a couple more questions coming in. The first one would be, are there any opcodes that are not supported by Zkvm?
02:23:16.040 - 02:23:35.560, Speaker E: We transpile the ethereum opcodes into Zkvm, so we need to support not only the opcodes but also the pre compiles. And we will definitely not support all the pre compiles from the beginning. So there will be some limitations, but they will be very rare. I think most contracts should work without difficulties.
02:23:36.800 - 02:23:46.780, Speaker A: Got it. Another question is tinyram can handle data dependent control flow, or can Tinyram handle data dependent control flow, or do you still execute all branches?
02:23:47.840 - 02:24:12.500, Speaker E: It's fully handling the data control flow because you have a program counter and you read the next comment from the program counter, which is controlled by the program itself. So you can make jumps, you can make conditional jumps, and you only execute one step at a time, and your cost is the total length of the execution trace.
02:24:14.200 - 02:24:14.724, Speaker F: Got it.
02:24:14.762 - 02:24:57.376, Speaker A: Hopefully that question was also clear. And if there's any follow ups from Uli, I'm happy to answer that as well. I think this is more of like a slightly technical one. It's referencing a particular slide. So I'll repeat that. And the question is for the slide that refers to the mixing of heterogeneous circuits, are all the functions in the Pi function there different, or are they part of the same circuit? Yeah, I think this is referring to the slide that you had at the end where you talked about you're mixing the storage one and you're mixing the tiny Ram all in the same circuit. And maybe this will help you better if you were to see that on the chat.
02:24:57.376 - 02:25:00.564, Speaker A: Are you able to look at I.
02:25:00.602 - 02:25:44.240, Speaker E: Remember this slide, I don't quite get the question. So all the functions well, tiny Ram is the same circuit. So we have multiple tiny Ram circuits for different length. They are structured by powers of two. So we have a circuit of length, let's say 1000, then 2004 thousand and so on. And we pick each of them depending on what the execution trace for a particular contract call is. Now, the circuits for heavy operations, they also have different sizes and we pick them, we just use as many of them as necessary.
02:25:44.240 - 02:25:51.910, Speaker E: Yeah, not sure I understand the question.
02:25:52.920 - 02:26:02.890, Speaker A: Yeah, in case we get a clarification on that question, we'll ask that. But if not, that's great. I guess my only other question would be actually we got a new question, which is where does the recursion actually happen?
02:26:05.660 - 02:26:25.280, Speaker E: In the intermediate circuits. So we have special circuits that aggregate other circuits. Actually their sole purpose is to take multiple proofs and produce an aggregate proof of those and to connect the inputs in a proper way. So it's a separate circuit dedicated for recursion.
02:26:27.300 - 02:26:58.776, Speaker A: Awesome. And I think there's another question that I got from the other chats. We have a handful of places where people are watching this aggregate all that maybe one last kind of question will be from our end is just generally kind of seems like you've talked about that this is actually fully Turing complete and EVM compatible. Is there any other gotcha at all involved here or kind of what's the level of completeness to what we understand on the EVM right now that can be mapped onto Zkvm?
02:26:58.968 - 02:27:44.264, Speaker E: Well, it's not going to be 100% exactly bitwise compatible with the bytecode. What we mean when we say it's EVM compatible is you can take your existing contracts written in solidity, for example, and transcompile them to this thing. You might require some minor modifications. We expect that for most DeFi protocols, for example, you don't have to change anything for some contracts. Make some assumptions about the specifics of EVM. So for example, you might depend on the specific costs of gas consumption in the transaction, which will of course be different. In L two, the gas matter completely differently than in L one.
02:27:44.264 - 02:27:46.408, Speaker E: You actually only pay for storage access.
02:27:46.494 - 02:27:46.696, Speaker C: Right.
02:27:46.718 - 02:28:07.360, Speaker E: So such things necessarily will change. And this affects all layer two protocols. Probably if they try to simulate it at some great costs, maybe some of them will, but it will not reflect the actual cost structure. So you will be just wasting some gas in such cases.
02:28:09.060 - 02:28:23.620, Speaker A: More small questions coming in. I'll just repeat them for the video. So we got a clarification on the slide question again, which is, are all the functions labeled Pi the same? And it appears that they are different circuits but in the same form, or are they different sizes?
02:28:42.320 - 02:29:20.068, Speaker E: I'm still not sure. Well, you can recursively combine different circuits, if that's the question. So like you, you will have completely different circuits. You will in the circuit itself, you can load the verification key either from some set of predefined constants or maybe even from storage, and then you can verify the proof against these verification keys. So the proofs can be different types and different length. The circuits which were verified will be of different types of different lengths. I hope this answers the question, otherwise I'm happy to jump on the chat.
02:29:20.244 - 02:29:37.096, Speaker A: I feel like you can clarify that at the end and we'll just do two quick questions before we wrap it up. And the second last question is what is the block latency for Sdkvm here? And I'll ask the last one as well, which is what are the curves that you're using for your circuits?
02:29:37.208 - 02:30:27.512, Speaker E: We're using BN two, five, six for now, which is the only curve we have on Ethereum currently with precompiles. And the latency depends on the hardware you use. Our provers are very well parallelizable, especially with this recursive nested aggregation. So let's say we also have hardware accelerator based in FPGA and we can produce a proof for a single transaction, like a single layer of recursion, I think within a second, like maybe 3 seconds. I'm not sure about the latest numbers. So you can imagine that if you want to aggregate 1000 transactions, you have like ten layers. Oh, no, sorry, not ten, because we can aggregate up to, I think, 20 or 40 circuits in a single block.
02:30:27.512 - 02:30:59.880, Speaker E: Again, I'm not sure which number, let's say 20. So that means you only need three or four layers, four layers of recursion. So the block could be produced within 1215 seconds maximum. So if we use the FPGA accelerator, if we use CPUs, then it depends on how many CPUs we have running and so on. The more we add, the faster.
02:31:00.620 - 02:31:15.480, Speaker A: Awesome. Well, thanks again for clarifying that. And I think now there's a bit more discussion on the other question about the slide. So hopefully you're able to hop on and answer those live. But thanks again for making this work with Live coming on here live. And thanks for the presentation.
02:31:15.640 - 02:31:17.310, Speaker E: Thanks everyone for watching.
02:31:19.040 - 02:32:13.690, Speaker A: With that, we are ready to move. There is still a bit of technical echo we're seeing, so I'm really sorry about that, but I will still kind of move on with announcing our next talk. And I want to welcome Friedel from Protocol Labs and he'll be talking about how they're scaling Zkissnarks to actually maintain the demands of FalcoIn. So apologies again for the technical side of this, but welcome to the summit and I'll let you take it from here.
02:32:15.020 - 02:32:41.104, Speaker C: Thank you. Thank you for the introduction. All right, let me share my screen so we can get started. All right. Thank you everyone for having me. Yeah, so I'm free to let go by dignified quieter or dig for short on the Internet. And I work on protocol apps, and I've been working on bringing ZK Snarks to Filecoin for the last three years.
02:32:41.104 - 02:33:44.132, Speaker C: Probably, if you don't know Protocol Labs is we build things like IPFS and Filecoin. And if you don't know Filecoin is filecoin is the decentralized storage network that happens to rely very heavily to actually work on using zkSNARKs. And I'm going to talk a little bit about the challenges that we had making that actually work. All right, so I've been working mostly on the proofs side of things for Filecoin. And if I'm talking about proofs, what I mean here is the part in Filecoin that verifies that storage is actually maintained as is claimed, and it turns out that is a non trivial problem to solve in Filecoin. I'm going to go quickly over this. There's much more information on the LDA proofs of work in detail, so you can find that out on the Internet or ask later.
02:33:44.132 - 02:34:34.550, Speaker C: But I'm just going to go very high level so we can cover some ground. So we have three big proofs that we need to take care of in Filecoin, and all of these use ZK Snarks to actually make it on chain. So if we look at the first one is called Porep, called short for proof of replication. This is a proof that gets generated every time some storage gets onboarded onto the File client network. So storage gets onboarded in sectors which are just chunks of data. They are currently either 32GB or 64GB. And if I'm giving numbers, I'm usually referring to two gigabyte sectors because that's currently the most commonly used size.
02:34:34.550 - 02:35:20.790, Speaker C: The second proof is what is called winning post, short for winning proof spacetime. This is a proof that gets generated on every block. So if a miner mines an actual block, they have to generate this proof, proving that some challenges actually are still valid on their current storage that they're providing. And then there is window post, which is short for window proof spacetime. And this gets generated every 24 hours for all sectors or partitions. And so basically it's a rolling challenge against the storage that a minor provides. Storage minor provides in order to secure that that storage is still available.
02:35:20.790 - 02:35:59.440, Speaker C: Now, all of these proofs by themselves wouldn't actually need to use ZK Snarks. You could just simply generate mocha proofs in most of these cases and just post them and be happy and live happily ever after. But unfortunately, the amount of proof merkel proofs that we would need to generate would make any blockchain explode in terms of size. And so we use ZK Snarks really to compress the amount of information that needs to be posted on chain and to keep up with that demand.
02:36:02.100 - 02:36:02.850, Speaker G: Now.
02:36:04.660 - 02:37:12.276, Speaker C: Turns out when we've looked at the design of these proofs and what the amount of things that need to be proved here in ZK snarks, there were some challenges looking at what we had when we started as a reference. When we started building this, the new hot stuff that was out was the library called Bellman from the folks at Zcash and they were preparing the Sapling update, switching over to this new library, implementing graph 16 in Rust as a very great baseline. But unfortunately, the things that we were trying to do were a little bit more complex. And so we really took us the last years in getting to a place where we can actually do the things that we run today on Falcon mainnet. So the first challenge that we ran into, and this is like demand one that Falcon gave us, is, hey, we would like to have a billion constraints. A billion constraints is a lot of constraints in a graph 16. I want CS based and for reference.
02:37:12.276 - 02:37:41.260, Speaker C: So I looked this up earlier. So the Zcash Sapling spend circuit, which is like what was there back then, was like, this was really cool. And a live deployment of ZK Snarks uses roughly 100,000 constraints. So we are orders of magnitude beyond what was available and had known to actually be done. Now, how did we get a billion constraints put into zkSNARKs?
02:37:43.060 - 02:37:44.544, Speaker E: We looked at a lot of things.
02:37:44.582 - 02:38:51.828, Speaker C: And what we ended up doing is so we had two steps, really, that we needed to deal with. So the first one was we needed to look at just the setup. So for graph 16, you need to run a trusted setup called Powers of Tell. And this defines how many constraints can there actually be in any given circuit that you ever run based on this trusted setup. So, as you can see here, Zcash ran back then a trusted setup based on two to the power of 21 maximum constraints, which is roughly 2 million. Now, we ran a trusted setup a couple of years later to solve our scaling issues, which allows us to construct circuits based on two to the power of 27, which is run roughly 134,000,000 constraints. Now, this is already a great increase, but if you look at the math, 134,000,000 is still not a billion.
02:38:51.828 - 02:39:59.340, Speaker C: So how do we get to a billion? Well, unfortunately, we couldn't grow the trusted setup we could have, but that would have introduced other issues. So this trusted setup generates a lot of output files, and some of these have to share later down the line. If we had increased even more, then we would have run into issues there that we could have not distributed those files easily. Also, the larger you go in this size, the more complex the trusted setup computations become. And so you reduce the amount of participants that really can deal with this, which is really not something you want to do in a multiparty computation where you rely on having a lot of trusted parties participate. So we were stuck somewhat at this two to the power of 27 number and had to come up with something else. So the next thing that we did to solve this was, well, it is not that exciting, we split the stack.
02:39:59.340 - 02:40:52.140, Speaker C: Luckily for us, the prop circuit really is a repetition of the same thing over and over again, proving that a certain challenge is still valid. And so it is very repeatable what we did. We constructed this proof. Instead of just having a single ZK snark, we generate ten ZK snarks each with roughly 100 million constraints. And so if we take that times ten, we end up with 100 million constraints. Cool. Unfortunately this means now you're not putting 192 bytes on chain, you're putting 1922 bytes on chain, which is not great, but way better than if you would put all of those local proofs directly on chain.
02:40:52.140 - 02:42:04.656, Speaker C: So this was how we got to a billion constraints. Now if you have with a billion constraints, kind of obviously what follows is that the proving part of your snarks is going to go a little slower because you're suddenly generating very large circuits and trying to prove them. And the demand that Filecon gave us was really like look, this has to be done in reasonable times and in some cases specifically winning post in very fast times. So winning post has less constraints, but because it needs to be generated per block and we have a block time of 30 seconds, you need to generate this post in under 10 seconds. So you need to go really fast is crucial for every minor. There's no excuse that this is not going fast enough. So the first thing and really the most critical thing to enable fast proving that we did is take all the proving from the CPUs to the GPU.
02:42:04.656 - 02:43:18.350, Speaker C: So not all the proving, but the most expensive parts. So when you generate a ZK schnarc using Roth 16, turns out the most expensive computations that you have to do are FFTs and multi exponentiations. So what we did is implement those first in OpenCL and later in CUDA to really optimize the performance of those. And if you look here at the graph, the amount of speed up that you get is quite considerable, especially when you look at the 100 million constraint size, which is the one that really counts for us for perhaps and the 10 million ballpark for winning post. And so we're seeing between four and six times speed up between the optimized CPU version. So as an explanation here, Bellman is the original library that Zcash gave us and to the world and that we've built on. And then Bellperson is the fork of that library that we optimized over the past three years.
02:43:18.350 - 02:44:11.390, Speaker C: So you have the CPU version which is slightly faster, but it is by no means as fast as the GPU based versions here in OpenCL and the CUDA version. And the OpenCL version is the one that is currently deployed on Mainnet. The CUDA version has been in development for a little bit and we hope to ship it soon. It'll give another nice boost to everyone running on Falcon. For reference, these times are recorded using a machine running RTX 3090 but you get similar results with weaker GPUs. There's some limitations around the amount of Ram you have to have to load things in, but generally you can use this. Well, that helped a lot.
02:44:11.390 - 02:46:14.470, Speaker C: Let's take a look what else? All right, so the other demand is really fast verification, right? So because we have a lot of Snarks that go on chain block validation needs to handle Snark validation and so you need to really make sure this goes as fast as possible because otherwise you will just simply stop the chain which would be very bad. So the first big thing here that we have is Blast and this was a collaboration with the folks at Supranational. We've built this really based on the requirements from the Ethereum community and us. Originally Blast is focused on implementing BLS twelve 381 based signatures, BLS signatures but because when we designed this we really requested that the operations were available to also do everything we need to actually do graph 16 based of this because what you really need there is a fast implementation of B list twelve 381. This is a library implemented where all the critical routines are implemented in assembly and the higher level parts in C and we've created bindings and rust to that and so we can use it as a drop in replacement for the pure rust implementation of Bilast 381. It's been audited by NCC and currently there is a formal verification underway for the full library. As you can see here, this saves again very heavily factor of three on just doing a simple pairing which is one of the biggest bottlenecks when you do verification times both G One and G two BLS twelve.
02:46:14.470 - 02:47:16.836, Speaker C: Multiplication is also considerably faster and for most of other curve operations this is similar and so you end up with considerably overall speed ups. Not necessarily the three X everywhere, but considerably faster. But this is not enough. This was not enough. And so another thing that we did was implement the Grof 16 Bash verification. Bash verification, the first time I've read about it is in the Zkesh spec and as far as I know they hadn't actually implemented it back then but we found it and luckily got it implemented. And this is really great because it allows you to so there are three Miller loops per proof if you do a verification, but this allows you to save two of those three and do them only per batch and not per proof.
02:47:16.836 - 02:48:22.780, Speaker C: And so you can see here, as soon as you start batching a lot of proofs together, the growth and verification times really goes down, which is great. And so we currently don't fully employ this as much as we would like to currently. So we do this for perhaps because as I mentioned, we have ten poor ups per ten Snarks per poor app, sorry. And so we do a full batch verification there and gain the considerable speed ups. Doing batch verification for more proofs is a little bit more tricky because you need to make sure on the execution path that you can actually do this batching. And the batching only gives you everything is valid or invalid. So if something is invalid, you need to go back and do more complex checks, which is a little tricky to deploy.
02:48:22.780 - 02:49:25.184, Speaker C: But this, as you can see, it gives us really good speed ups from factors of six on just eight verifications, which really counts if you look at the amount of verifications that we have to do on mainnet. All right, so we got that in. So seems like we've got already a lot of things in, but there are not more demands. Fuck. One wants to make more Snarks, really the network and really there's more demand and the chain is getting fuller and fuller and so there's the request to make the proof smaller. So, as I mentioned, for poor apps, unfortunately we have not just 192 bytes from a single Snark, we have ten Snarks. And the amount of Snarks and pour ups that go unchained is considerably high.
02:49:25.184 - 02:50:39.690, Speaker C: I'll show the numbers later. And so you really end up with an issue of having currently, if you look at the chain stats, we still need less bandwidth ebus for Snarks. And so one of the latest things that we've been working on is called Snarkpack. This was just put up on Eprint and is a collaboration with a couple of folks and based on some work from Mary Mallor and others that allows taking a graph 16 based Snarks and to take them and aggregate them, there are a couple of restrictions. They have to be based on a power of so the number of proofs you can aggregate have to be a power of two. But is really great because this is really what allows us to take size and verification times to the next level. There's a more detailed there was just a more detailed work at the ZK Proof workshop earlier this week from my colleagues on this, if you want to know more about this.
02:50:39.690 - 02:51:49.200, Speaker C: But just if we look at the numbers here, they're really good. We can see that from going. So the light blue line is what we have as the batch verification and up to 256 we really go we're not faster yet. If you look at the verification times, the difference is not big, but it's not worth it yet. But as soon as you cross the 256 mark, the verification time stays much, much lower. And you have a difference at the end with 8000, when you aggregate 8000 snarks of a difference from 33 milliseconds versus 435 milliseconds, which is a roughly difference of 13 x, which is very considerable. If you look at the size I don't have a graph here, but if you look at the size differences, you have a similar cut over point at aggregation of 256, and the cut over there grows very quickly.
02:51:49.200 - 02:52:34.640, Speaker C: And if you aggregate at the 8000 level, you're looking at 40,000 bytes versus 1.5 million bytes. And so the difference is about 37 x. So really considerable size improvements here. If you can use this, this is unfortunately not yet deployed. There is a FIP open, FIP 13 if you want to look at that for more details on how this exactly is planned to land on Falcon main net. But it should enable a lot of improvements on onboarding storage, which is where it's used for the moment, on improving poor apps.
02:52:34.640 - 02:53:46.260, Speaker C: Yeah. All right, so if we look at this a little bit back, did we get what we needed? So we've launched Mainnet back in October, and it's been running pretty stable so far, so that's great. And people have been making some Snarks. So let's take a look at how many Snarks and what are we looking at? So, currently the Main net roughly produces 30,000 winning posts per day, which is the smallest one. We have 36,000 window posts. So Window Post has the same number of similar number of constraints as a single GPA, snack and porep. So this is roughly 130,000,000.
02:53:46.260 - 02:54:39.160, Speaker C: So just scratching at that top level of how many constraints we can put in there, and this was specifically actually designed such that we're trying to get as much information into a single ZK Snark there as we can get into on chain, of course, and then roughly 5.5 million porks per day. So this is 550,000 PO day that we're currently averaging on Falcon main net. And so this is 5.5 million Snarks there, and each of them, again, roughly 130,000 constraints. So that's a lot of constraints. So I have a fun number here that we calculated.
02:54:39.160 - 02:55:22.912, Speaker C: Where is my constraints per second? Sorry. All right. Yes. So for March 21, we average roughly 4700 Snokes per second. And so this is Mainnet roughly produces one Snark every 0.2 milliseconds. If anybody was curious, we think this is a big number of Snarks.
02:55:22.912 - 02:56:06.224, Speaker C: We think this is a lot of constraints. But one thing that I would love to ask folks is please do reach out to us, to me afterwards or in the comments. I really want to know who's deploying Snarks and how many Snarks are there deployed, and I think it's really cool to see what we can push these systems to today. So I would love to hear some numbers from folks. So far I have not heard anybody claim that they make this much mini Snarks but please prove me you are. Yeah. This is covered over roughly 2000 miners and this proves currently 4.7
02:56:06.224 - 02:57:04.336, Speaker C: exabytes of storage. And so that's a lot of sectors, right? So most of these are 32 gigabyte sectors. So that is a lot of storage that we have there which is a great proof of that. This Snark scaling actually worked and produced what we wanted because it actually runs now. There was always, as always with these systems if you think about them in theory and then actually deploy them, there's still always the fear that might not quite work as you plan to work. All right, so yeah, that's all. If you have things, please get in touch, help us create great things and please send me all your Snark stats of all your systems.
02:57:04.336 - 02:57:19.770, Speaker C: I'm really curious what other systems are currently doing, what other tricks are people using to scale their Snarks and make them faster. Yeah, and if we have time for questions I'm happy to answer questions.
02:57:22.460 - 02:57:55.350, Speaker A: Absolutely. So thank you so much for that awesome talk. I learned a lot of new things personally and kind of looking at all the other chats we have going on where we're streaming this. A lot of excitement about Snarkpack. So I think one question that's more of a follow up from something you said would be great if you're able to share a link to the workshop on Snarkpack so others can catch up and learn more about it. The only other formal question that we have is just generally speaking, what is the impact of Snark verification during the sync on your end?
02:57:56.120 - 02:58:11.036, Speaker C: I don't have exact numbers unfortunately because this varies. I need to reach out to the folks that make these stats. I do know from a very active feedback from those folks it is usually always not as fast as they would.
02:58:11.058 - 02:58:11.932, Speaker G: Like it to be.
02:58:12.066 - 02:59:02.010, Speaker C: So it has high impact and especially it has impact when there is an issue with it. If somebody is actually running into issues then you get really quickly problems with verification. In terms of related to block times. Not the biggest issue is more winning post for the miners because the actual generating that on time is even more critical and it's just more resource intensive and so that is more critical in terms of block times currently on the network. But verification always needs to be faster. But quote unquote lucky for us there are also other parts of the system that need time. So we're not the only one slowing things down.
02:59:02.620 - 02:59:03.320, Speaker F: Awesome.
02:59:03.470 - 02:59:36.470, Speaker A: Well, I think we are slightly overtime so I'm going to have to wrap this up. But I am personally super excited to learn more about Snarkpack and I think to follow up on the Ask You Mate, which is getting you in touch with all the Snark apps, I think we'll be happy to follow this up on the hackathon site. There's a lot of people working on this stuff and I also think you may be interested in the next talk, which ends up covering how to speed up a lot of those things by using Noir and plookup or plokup. So thanks again, Dignified Choir, and really appreciate you giving us the talk today.
02:59:37.480 - 02:59:40.608, Speaker C: Thank you so much. Thank you for listening. Awesome.
02:59:40.794 - 02:59:59.128, Speaker A: So next up, before we kick off our talk, I want to quickly introduce Catherine from the global team and Catherine will be taking over from me for the next little while. So welcome, Catherine. And I'll leave you all in the good hands of Catherine, and I'll see you all shortly.
02:59:59.304 - 03:00:23.270, Speaker B: Hi, everybody. Thank you so much, Kartik. I'm really happy to be here and take part of this day other than behind the scene. All right, so our next speaker is Zach from Aztec, and we'll be going on with his talk, achieving Practical Programmable Privacy with Noir. So without further ado, Zach, please take it away.
03:00:25.320 - 03:00:36.744, Speaker C: Yeah, happy to do so. Cool. Just to check, can people hear me? Just want to make sure my mic isn't muted. Awesome. Great, thanks. Cool.
03:00:36.782 - 03:00:36.936, Speaker B: Yeah.
03:00:36.958 - 03:01:36.732, Speaker C: So, hi, everyone, I'm Zach, I'm the CEO of Aztec and we are a privacy provider on Ethereum. And our mission as we see it, is to both scale Ethereum, but also mainly to deliver strong privacy guarantees to public blockchains. So I'll just share my screen now because to share a few slides that I have presented to present. Cool. So our thesis at ASIC is that the more that blockchain has reached into the economic lives of individuals, the more that privacy is going to move from being just a nice to have to utterly essential to the functioning and purpose of a blockchain. Blockchains can be used, I mean, the great promise of blockchains to be used as the world's Global Settlements layer for payments, microtransactions, and far more than that. But if we live in a world where your salary is streamed on chain, or we live in a world where the majority of your purchases and payments are being settled on a blockchain, people aren't going to be happy.
03:01:36.732 - 03:02:20.008, Speaker C: With their entire payment history being built up and assembled by some analytics firms so that their details can be sold to Advertisers without their consent. People aren't going to be happy with everybody being able to view how much they're getting paid at a glance on chain. And so really, I think that privacy is going to become a major issue in the future. And we want to as the way we see is we want to basically solve this problem. And we're going to solve this problem using highly advanced, state of the art ZK Snark tech plugged into a very advanced programming language that abstracts away all the cryptographic nonsense and presents a very easy to use, private, smart contract, chronic interface to developers. So before. We dive into that.
03:02:20.008 - 03:02:57.776, Speaker C: What might be useful is to provide a bit of context. What are private transactions and how do they work? Because they're not particularly simple, that's for sure. If you want a transaction to be truly private, then you need three things is what we call the trip ticket privacy. And I've completely stolen this from Anna Rose from the ZK Podcast. You need data privacy, obviously values that are being transferred ahead and any sensitive data is encrypted. You need user privacy, you need the identities of senders and recipients and counterparties to be completely anonymous and encrypted. And you also ideally need code privacy so that the actual contract you're calling is not known to observers.
03:02:57.776 - 03:03:36.624, Speaker C: And you can even go on step further and actually have encrypted code itself so that only a privileged number of people can actually see the smart contract code to begin with for kind of specialized use cases where you're using proprietary algorithms. But this presents a lot of problems if you're doing a cryptocurrency transfer. Your transfer now can't produce linkable database updates. And by that I mean if you take the Ethereum use case, you have an account balance on the Ethereum blockchain. It's in a database somewhere, your address has got a value link to it. And that value changes when you send a transaction. Even if you encrypt that value field, the fact that it changes reveals information about the transaction graph and exposes a little bit of what's happening.
03:03:36.624 - 03:04:32.596, Speaker C: And so the way that we solve this and the way that it's commonly solved, for example, I think Zcash really pioneered this for privacy is you represent value in the more Bitcoin style world where you have unspent transaction objects, except this time they're encrypted. So you have some note that represents the value and it has an owner, but the value and the owner are encrypted and your balance is a sum of those owner notes. That way you can describe a private transfer of value using this joint split ZK snark circuit. The idea is you have some input notes that have owners and values and you have some output notes that have owners and values very much like Bitcoin, except everything's encrypted. And the way that this transaction works is you use a zksnart to validate that some of the input notes equal to some of the output notes to validate that the note owners have consented to spending these notes by. Providing a digital signature and basically validating all of the logic that requires sensitive information inside a ZK Snark circuit so that it's all hidden from the world and from observers. And nobody other than the sender really knows what's going on.
03:04:32.596 - 03:05:29.860, Speaker C: And only the recipients can decrypt the output along with the sender because they're creating them. So that's how you do private transaction. But how do you do it at scale? Because that's important, right? It's not sustainable for the Ethereum blockchain to verify every private transaction in a smart contract because ZK smart verification is crazy expensive, costs you up to hundreds of dollars at the high gas prices we've been experiencing. And this is where our ZK ZK roll up architecture comes in. Effectively, deposits and withdrawals into this architecture are public because you're depositing a withdrawal cryptocurrency, but the identities are unlinkable. So the identity of the depositor is not necessarily the same as the identity of the withdrawal. And you can also mix together values to provide even more obfuscation and any internal transactions that you send within the role of architecture.
03:05:29.860 - 03:06:05.692, Speaker C: So any purely private sends are completely obscured. So senders and recipients identities are hidden, values are hidden. The strongest privacy guarantees you can get. So the way this works at scale is through the concept of recursive ZK Snarks. So you have a ZK Snark circuit which defines the individual private transaction. And then you have another larger ZK Snark circuit which aggregates together hundreds, if not thousands of these private transactions and verifies their correctness by basically what you have is you have a ZK Snark circuit that verifies proofs made of other Ziki Snark circuits. That's why it's called precursive Snarks.
03:06:05.692 - 03:06:46.268, Speaker C: And doing this on Ethereum has been something of a difficult problem to solve because the cryptographic primitives available on Ethereum are extremely limited. You can only do basic arithmetic over one elliptic curve, soon to be two. And so what we've done using our turboclock proving system is we provided the first viable algorithm that can be used to perform recursive Snarks on Ethereum. That's how we've achieved this private roll up architecture. And one of the valuable things about this is that this roll up that you need to make the proverb is completely untrusted. It can be delegated to a third party. They don't need any sensitive information because everything is hidden within the inner Snarks.
03:06:46.268 - 03:07:42.800, Speaker C: And it has more advantages over other over other similar roles in layer twos, because the role provider can't mount censorship attacks because from their perspective, all of these transactions, they just look like random numbers because everything's encrypted, so they don't know what to censor. So from that point of perspective, it's a relatively neat architecture. And then to go on further, to get a little bit inceptiony about it, you can make roll roll ups of ups, of roll ups, of roll ups, of roll ups, of roll ups, if you want to paralyze your roll up construction to aggregate extremely large numbers of transactions. So let's say you have a roll up that aggregates 32 private transaction proofs. Well, then you can create another roll up circuit that verifies 32 of the roll ups that verify 32 transaction proofs. And then you got 32 squared transactions compressed down into a single roll up. And you can scale this arbitrarily to produce massive roll ups, which is again another value of this recursive ZK snap construction.
03:07:42.800 - 03:08:03.994, Speaker C: So that's how private transfers can work. They provide strong privacy guarantees. But the title of this talk is Programmable Privacy. And that's not programmable, that's just basic value transfer. So let's go one step up and say ask ourselves, okay, sends and receives. That's cool. You've got private bitcoin, you've got a Zcash type architecture.
03:08:03.994 - 03:09:07.830, Speaker C: Nice. But how do you do something more? For example, how do you tap into DeFi but in a private privacy preserving way? Now, this might seem on the surface of it not too hard, right? We have Snarks, they're programmable program DeFi, job done. But it's a little bit more complicated than that, because when you want privacy protections, then everything changes. Basically, you live in a bit of a more constrained world because state variables, storage variables, the kind that exist in a solidity smart contract in a world of privacy, they're all owned by individuals because they're all encrypted. For example, if you have some AMM that you're creating, well, you probably need some kind of total liquidity supply variable in your AMM. But you can't keep a global variable like that and update it because that releases information about what individual transactions are doing. And so this makes DeFi protocols in particular extraordinarily hard to do in a privacy preserving manner to actually make the DeFi protocol itself private.
03:09:07.830 - 03:10:32.462, Speaker C: Another problem is that if you have a programmable private smart contract system, so that that join split transaction that I just described, you basically make that programmable by anybody. So anybody can fiddle with the rules of that joint split circuit. Well, now you're adding untrusted code execution into a ZK roll up that's privacy preserving. And so from a crypto first perspective, that's quite dangerous because you need to somehow validate that you have strong security guarantees that somebody can't create a malicious smart contract that manipulates or affects or corrupts other smart contracts. So how do we do this? How do we protect users from being defrauded with malicious circuits? And how do we achieve the kind of code privacy so that you don't even know which smart contract you're calling when you're interacting with this crypto system? This architecture that we're putting together, a lot of it is inherited from the Zexi protocol that was published in 2015 with a few modifications. So the way you do it is you have a concept of a kernel snark circuit. A kernel snark circuit, effectively, it's a Zeke snark circuit whose job it is, is to verify the correctness of a single private smart contract transaction.
03:10:32.462 - 03:12:00.146, Speaker C: So you have a private smart contract which is defined by a ZK SNOX circuit that's coded up by some developer, could be anybody using this noir programming language. And then this kernel SNOX circuit is designed to ensure it validate the correctness of it that the user is interacting with a smart contract that's been kind of deployed to the chain that the inputs and the outputs of. The contract are correct that any state variables that need to be modified are following the rules of the chain, basically performing a very similar function to the EVM on Ethereum, but with the difference that this isn't actually an actual virtual machine. It's much higher level than that. So the story so far is we have a way now of doing programmable private smart contracts. You have this recursive abstraction where you have a privacy proof that is user defined and then you have a kernel smart circuit which verifies the correctness of a single private transaction and then you have a roll up circuit which aggregates kernel smart circuits so it's turtles all the way down here. So how do we get from that to DeFi or something a little bit more ambitious than private transaction, just a private value transfer? I mentioned earlier that one of the problems is state has to be private, everything is encrypted, it has to be owned by one or more people therefore you can't do things like track total supply, you can't trigger out transactions algorithmically and by that I mean consider something like Private Makerdaodao.
03:12:00.146 - 03:13:17.190, Speaker C: If you create a CDP, if it comes under collateralized it automatically gets liquidated. Well, if your CDP is encrypted, the value is encrypted and how do you determine it even if the thing is under collateralized and even if you can do that, how do you liquidate it? The only person who knows how to do that is the owner of the CDP and they aren't going to help you because you're hurting them by performing this action. So I guess the TLDR of this is that whilst you can do private DeFi in a completely private way where you actually make the DeFi algorithm private, it's quite hard and it requires very advanced multiparty computation protocols to achieve. And you might need to add in additional trust assumptions and that this is not something which is particularly easy to deploy or even build if you're not a relatively sophisticated cryptographer. So this might sound a little bit weird know at AZEC our goal is programmable privacy. This whole talk is about how do you do programmable privacy and here I'm saying it's kind of hard, you probably can't do it, that's not actually true but we're at the halfway mark where it looks quite hard to do. You can do more meaningful privacy preserving DeFi type interactions without having to have advanced NPC premises and kind of completely getting around the fact that the actual DeFi protocol itself is very hard to encode in a privacy preserving manner.
03:13:17.190 - 03:14:30.218, Speaker C: And the way you do that is you interface private transactions with public DeFi protocols and by that I mean you go to the source of value, you go to the tokens themselves and you ensure that they're private so that the owners of tokens are encrypted their positions of any one token holder that's also unknown. Then you can make your DeFi protocol completely public and you can file a very strong privacy guarantees because if you have people interacting with, say, for example, uniswap, but you don't know the identities of people interacting, you don't know who is depositing into the liquidity pools, you don't know who is performing trades. Then that's privacy preserving. It gives you very strong protections against some of the existing problems in DeFi. For example, right now, if you have a very large position in DeFi, it's very easy to get pretty much inevitable that you'll get front run, right? Because you can use services like chain analytics to completely dissect and understand everybody's relative holdings in DeFi protocol. If somebody has a portfolio with large positions, you can know about it. If somebody starts to unwind that position, they start to make a large trade, everyone knows about it.
03:14:30.218 - 03:15:03.334, Speaker C: And hey, hey, you can frontline those trades, you can do mev shenanigans. But if the actual assets themselves are private, then you can undo this. Because if you have a large position, you can unwind it slowly, you can slowly drip it like 1000 die out or 1 million die trade time. And nobody knows that those 1000 die trades are connected with one another. So if anybody knows that somebody's doing a large trade because from the perspective of the blockchain, they're not. This is kind of what we're doing at Aztec. This is kind of our focus right now.
03:15:03.334 - 03:16:23.066, Speaker C: It's to basically provide a way of interfacing with public DeFi protocols in a privacy preserving manner to provide the kind of programmability which makes this possible without having to basically reinvent the wheel and require these DeFi protocols to reinvent themselves in a completely private way, which requires very advanced multibodtical mutations and advanced cryptography, which most d five teams don't have the ability to do because it's quite a niche and also probably the inclination, because it's extremely difficult to pull off. So the summary of this is basically, if you want to do private D Five, you create a privacy preserving bridge to a layer on DeFi protocol from a layer two private wallet like Aztec. The idea is you can aggregate together fixed denomination notes that don't need to be fixed, but it helps with the anonymity guarantees. So for example, if you want to trade five E and the dominant nomination is one ETH, then you just split it up into five trades because this is a scaling solution that is relatively cheap to do. And then you have an aggregation circuit. And what an aggregation circuit will do is it collects together trades of the same type from different users and bundles together into one megatrade that then gets sent off to the layer one DeFi protocol. And that way the expensive layer one transaction gets amortized across a large number of users.
03:16:23.066 - 03:16:56.282, Speaker C: So hang on, this is somehow skipped together? Skipped about a dozen slides. So we were here, weren't we? Yes. So this is just a little flowchart of how this system will work. So on the left we have the layer two roll up architecture that's privacy preserving. Users have a bunch of ethereum they want to trade it for die you have an aggregation ZK snux circuit. This will be programmed by somebody using Noir. And these ETH notes get aggregated together into a giant pool of ETH that then gets sent out to the public.
03:16:56.282 - 03:17:23.806, Speaker C: Layer one to a bridging contract, which basically acts as the interface between the layer two roll up and the D five protocol. Anyone can write this. This is completely non interactive. The D five protocol itself doesn't have to write this bridge. We don't have to. Like, as long as somebody writes it and it conforms to a standard interface that we're going to be publishing fairly soon, then this just works. And so the idea is then the bridge contract would perform a swap with some DeFi protocol.
03:17:23.806 - 03:18:26.458, Speaker C: Then the proceeds would get sent back to the aggregation circuit inside the roll up. And then from that the circuit using because it's programmable, will be programmed to disseminate the dye that was exchanged. In this example, disseminated it back to the original depositors in the correct ratios. And this is both how you can interact with existing DeFi protocols in a privacy preserving manner without having to rewrite the DeFi protocol and to do it at scale at a price which is cheaper than just a basic layer one transaction itself. So how do you program this all up? Because this is what we're building at Aztec. But there's one thing between having the Aztec team, which has got some very good cryptographers in it, building this is a very different thing to getting the community to build these things, which is the end goal. Privacy can't achieve its full potential unless the users themselves, they're the ones deciding how best privacy can be utilized for their purposes and programming it up themselves.
03:18:26.458 - 03:19:05.742, Speaker C: And so part of this is the proving system, the cryptography. Because when you have a private roller, one of the big constraints, one of the problems is that the privacy proofs that are constructed so these joint split transactions, these kind of these aggregation proofs, they need to be constructed client side. They need to be constructed by the user because they're privacy proofs. If you delegate that proof construction to a third party, then you're leaking information. You have to give them secrets. You don't want to do that. But if you're constructing these proofs client side, then you're constructing them with very wimpy constrained hardware, old laptops, old phones, and worse than that, probably running a DAP in a web browser.
03:19:05.742 - 03:20:02.594, Speaker C: So you can't even have tightly compiled optimized code. You've got to run it in a web browser, probably using WebAssembly. So you have a lot of slowdowns involved and snark proof construction is notoriously expensive. Typically the difference between running a computer program and constructing a ZK smart proof of that computer program, it can scale up to a factor of a million and that's using this state of the art tech, which improves by one or two orders of magnitude on what came before it. But we're very confident with this ultra Plunk proving system that we've developed in house at Aztec that it's fast enough to get the job done. So one of the reasons why is because it's a completely state of the art universal ZK Snug that's built on top of the original Plonk protocol, which Aztec published in 2019, and it's far more efficient than Plonk. It can do recursion in less than 350,000 constraints.
03:20:02.594 - 03:20:49.326, Speaker C: We're going to get the number down to much lower than that in the future. And it can do very snark unfriendly things very efficiently. So if you're familiar with ZK Snarks, you might know that Snark circuit has if you have to represent a program as a Snark circuit, you need to decompose your program into addition and multiplication gates, which is very hard and annoying to do because that's quite primitive. What we can do instead is we've developed a way of very efficiently accessing lookup tables within a CK Snark circuit. So you can do things like, let's say you want to do a shard of x six hash. Well, that's made up of a bunch of exclusive operations. You can create a lookup table of eight bit exclusive operations and just index get values out of pull values out of the lookup table instead of computing these exclusive ors, which is quite hard to do in a Snark.
03:20:49.326 - 03:21:35.710, Speaker C: And similar things like that means that you can absolutely crush down the constraint cost of common cryptographic algorithms that you need for these kinds of advanced ZK Snarks, as this table shows. And the way that we're going to give the community the ability to actually work with this technology is through Noir. Noir is our private smart contract DSL being developed. The lead developer is Kev Vedabar, and he's developing this as a fully open source project. So the goal of this, we're going to be using this to enable users to build state of the art private smart contracts as Plonk circuits. But the language itself can be used, can be, in theory, plugged into any cryptographic backend, not just Plonk. So this is very much a community driven effort.
03:21:35.710 - 03:22:17.054, Speaker C: And the difference of this of novel with other smart programming languages is this one is very much optimized for client side proof construction. It's optimized for the goal is for it to compile them to very, very highly tightly optimized circuits where proofs can be structured on maybe Wimpy hardware, which is quite critical for our use case. So some of the target features are things like efficient dynamic arrays. Now, if you're a software engineer or programmer, you might think, big whoop, dynamic arrays, wow. They've been around in traditional programming languages since what, the 60s, maybe the 50s. But doing this kind of thing as AK Snark is truly revolutionary. To do it efficiently like to do it with one or two constraints per array.
03:22:17.054 - 03:23:21.446, Speaker C: Access is not really been done before. You can have efficient recursive proof composition, which is essential for a private smart contract system, because you need this kernel snark, you need these roll ups, there's lots of recursion going on here. And also the end goal is this to have very efficient, very nice, pleasant private state semantics. So it's very easy to create storage variables, to have them be owned by users and manipulate them without having to deal with the underlying encryption algorithms and the typical cryptographic algorithms that you need to manipulate state inside Cknunk. Which is like merkel tree membership proofs and nullify sets and a lot of horrible messiness that we want to completely abstract away from the user. And also importantly, pleasant call semantics so that private smart contracts can call other private smart contracts and that privacy smart contracts can even make calls out to layer one smart contracts and get return results back, which is needed for, for example, that aggregation example earlier in the talk. So the workflow for Noir is that you have some application that you want.
03:23:21.446 - 03:23:43.790, Speaker C: So here I've got some examples. Like Ziki Money, Tornado Cash. And then you program up your circuit in Noir. And then basically Noir will spit out two things. It spits out a proof of your circuits that you can create on demand. So you can put that in a web based SDK and have users make those proofs. And it also spits out for you a verification smart contract that you can deploy to Ethereum.
03:23:43.790 - 03:24:30.462, Speaker C: So these follow a fairly standard template. The only thing that really changes per circuit is the verification key, but the actual algorithm remains unchanged because it's a universal smart. And the abstraction layer that we're using here to enable Noir to plug into different cryptographic backends is something that we're calling a Sea, which stands for abstract circuit immediate representation. It's a little bit like Llbm for computers. The idea is it breaks down a program into primitive operations that are efficient to do performance. So to give an example of currently how Sea is structured, as much as possible, we try and devolve the basic operations into linear combinations. So a little bit like rank one constraint systems, if you're familiar with that.
03:24:30.462 - 03:25:22.106, Speaker C: So this is basically just additions and multiplications. But then we have black box algorithms for the complicated stuff. So what we've done at Aztec is we've built very, very tightly optimized gadgets, I guess you could say, for common cryptographic algorithms like Sha two by six. So we've got this down to like 3600 constraints and it can be optimized further. So that kind of stuff, we really want users to be able to tap into that without having to write it themselves, because our version is extremely efficient, because we spent a lot of time working on these. Similarly, you can do range crease very efficiently. Other cryptographic primitives and so the way of si works is you have these black box algorithms that you can tap into, and then everything else that can't be covered by the black boxes gets turned into linear combinations, which is a little bit more expensive to turn into plump gates.
03:25:22.106 - 03:26:04.540, Speaker C: But the idea is that for the linear combinations, that's kind of glue logic between the complicated black box algorithms that are kind of effectively can be considered. They're a little bit like Ethereum pre compiles. So I want to give this the roadmap of this, because this has kind of been a very high level. What are private transactions and how do we achieve them? How do we make this programmable? How do you get it done with Noir? But how do you use Noir? Well, Noir has been released. It's in an alpha, so you can download it now, tinker with it. We've got a lot of work to do on it before it's completely feature, complete, and ready to be used inside, like a roll up architecture to create private smart contracts. But the basics are all there.
03:26:04.540 - 03:27:00.830, Speaker C: Our roadmap for Aztec is that, well, we've released our we've released the first fully private roll up on Ethereum. So it's what we call ZK money, which is a way of shielding ethereum and shielding ERC 20 tokens and then sending them around privately to other people. And right now, sorry, I pinch an OD slide, one of my old talks. So that number is actually 128 transactions per block, not eight, nine, six. And it's powered by a slightly older version of our cryptography, because we wanted to get this released fairly quickly, so we haven't yet migrated it to the latest state of the art. But even with this, our privacy proofs can be constructed on a client, like in a web browser in 10 seconds. So this is why we're very confident that we can get the full private smart contract architecture done in such a way that you can still make these proofs efficiently in the web browser.
03:27:00.830 - 03:27:49.930, Speaker C: Phase two, which we're planning on releasing in the summer this year, is this DFI interaction stuff that I was talking about. So because we want to get this into the hazard community as quickly as possible, we're going to be programming these circuits ourselves so that users can interact using our layer two privately with D five protocols, and that users and developers can build these bridge contracts themselves. So that if you have a D five protocol that you particularly want to talk to, or pretty much any layer one smart contract that can be modeled as a token swap. So this includes things like Dow voting. You can write a bridge contract, wire it into via Air two, and then send asset transactions directly to that smart contract. And that'll be powered by our state of the art ultraplot proving system. And finally, the apotheosis of what we're working towards is this final programmable, private smart contract world.
03:27:49.930 - 03:28:48.970, Speaker C: We're planning on launching either late this year or early next year, where we'll be releasing the 1.0 version of Noir and our Re Architected Asset roll up protocol, which will enable users to program up private smart contracts directly in our layer two and then use them for whatever they want. But the idea is to create a very rich ecosystem of private cryptocurrencies, private entities, private financial instruments that interact with both with D Five protocols in a privacy preserving manner, but also more traditional financial products and services of the type that can't even be deployed on blockchain today because they require strong privacy guarantees to begin with. To work. So the status of Noir at the moment is it's an alpha release. That's the link to the source code if you want to check it out and start tinkering around with the writing nois circuits. The current supporter backend is POC, but we're planning on supporting other cryptographic backends eventually.
03:28:48.970 - 03:29:55.922, Speaker C: Things like Marlon and the smart contract verify generation is all working. So you can not only construct your circuit book proofs, but you can spit out smart contracts to verify them. What's on our roadmap is lots of work on developer tools to make this pleasant experience, to have things like a linter debugger, that kind of thing, as well as a developer SDK, basically the asset equivalent of web three JS. Something to wire to act as the interface between making a proof and then turning the output of that proof into an ethereum transaction and getting it sent to a verified smart contract, as well as support for multiple cryptographic backends and the semantics for smart contract transactions and storage state storage that doesn't require the developer to mess around with merkle trees. So just one final slide to give an example of the kind of DApps that we want to incentivize the community to build. This is one we built ourselves with our current technology. It's ZK money, and it's a privacy preserving web wallet for Ethereum, where private cryptocurrency transfers are less than a layer one Ethereum transaction.
03:29:55.922 - 03:30:12.620, Speaker C: Because of our scalable roll up tech and because of our technology, it's all completely privacy preserving. So, yeah, that's a lightning tour of noa how to achieve practical programmable privacy. And yeah, be super happy to if there's time to answer anyone's questions.
03:30:15.230 - 03:30:35.540, Speaker B: Thank you so much for this super interesting talk, Zach. We do have one question that came in mainly, so the question privacy preserving bridge looks like mixednet for DeFi. If there aren't enough users, traffic anonymity is not your entry. Is that the case here?
03:30:37.270 - 03:31:13.914, Speaker C: So actually it's complicated, but you're on the nose where the fewer people using the layer two, the worse your privacy guarantees are. One of the valuable things about the layer two architecture, though, is that basically your anonymity set is pretty much everybody who's deposited into the roll up architecture. So even if you have a not very commonly. Used DeFi protocol. Let's say, for example, you are the only person who's interacted with the DeFi protocol in the last six months. Nobody knows it's you. It could be you, it could be any of the other people that have deposited it in the Aztec.
03:31:13.914 - 03:31:49.722, Speaker C: So as long as the actual high level protocol has a reasonable amount of traffic, then you have a very strong anonymity guarantees. But you'll create that if the protocol is dead and it's like one transaction a day, then you're going to get very poor privacy guarantees. However, we're pretty confident that's not going to be the case. To give an example, we launched about six weeks ago. We have a one ethereum deposit cap on Aztec. Because it's experimental technology, we want to make sure that people don't throw all their ethereum in it in one go. But despite the deposit cap, we have about one point.
03:31:49.722 - 03:32:08.030, Speaker C: We've had $1.7 million of deposits in six weeks. So even just post launch has got a very vibrant community of users, despite the limitations. So I'm very confident that when we deploy our DeFi bridges, our users will enjoy relatively strong anonymity.
03:32:10.210 - 03:32:26.054, Speaker B: That's amazing. Thank you. I hope that answers the questions of where we're asked this. We are running a bit over time, so unfortunately, I won't be able to take much more question. But thank you so much for your time. Zach, that was great. I hope everybody learned a lot.
03:32:26.054 - 03:32:32.120, Speaker B: I did. So we will continue on to our next talk and see you next time.
03:32:32.490 - 03:32:33.910, Speaker C: Thanks. It's been a pleasure.
03:32:35.710 - 03:33:20.982, Speaker B: All right, so coming up next, we are having Shahar Papini from Sarkware, who's co creator of Kero and one of the main engineers there. So this talk basically is from ASIC to CPU. It's a SARC journey. And this will be a recorded video also because of Time Zone. So apologies for that. But the Sarcore team is always super happy to answer any of your questions in the channels we have internally or on their own personal channel. So without further ado, we will be getting on with the video, just setting up the technical side.
03:33:21.036 - 03:34:01.380, Speaker D: Hello, I'm Shahabini. I'm an engineer at Stalker and the co creator of Cairo. Cairo is a language we developed to help other people use DK stocks. Today, I'm going to talk to you about the ASIC to CPU journey which Starkware had regarding to developing Starks. So first I'll explain what I mean by ASIC to CPU. Explain this analogy. I'll then briefly explain how Starks work, how the protocol does what it does.
03:34:01.380 - 03:34:44.740, Speaker D: I'll show the early stages we had while we developed Starks, and the intermediate steps reaching to the so called Holy Grail, the universal machine reading in stock, which is Cairo. And then I'll have some deeper dive into Cairo and look at its features and what's planned ahead. So let's begin. ASIC to CPU, asics are application specific integrated circuits. They're basically chips. They are specialized for the task. They are very fast, they're expensive and very hard to design for.
03:34:44.740 - 03:35:26.138, Speaker D: CPUs on the other end are for general computation. They are a lot slower, they're cheaper and very easy to program for. Software is a lot easier than hardware. CPUs are a multipurpose single architecture, like a single asics you can call it, that can run a lot of things. Basically a trade off between efficiency and flexibility. There are a few intermediate steps, I would say. At one end we have the ASIC.
03:35:26.138 - 03:36:11.070, Speaker D: FPGA is an architecture which is a bit more generic, easier to program for it's still not as a flexible CPU. We then have some GPU, which is a lot more flexible. And the other end we have CPU. So for each of these I'll have an analogy in the Stark word. Obviously it's not a perfect analogy, just an analogy. So starks? What is Stark? Stark is a family of cryptographic proof systems that can be used for both privacy and scalability. This basically lets you prove statements.
03:36:11.070 - 03:37:26.870, Speaker D: For example, for statements we can prove the thousands number in the Fibonacci sequence is some number x something. Another example statement can be I have 100 signed bank transactions, they're all signed correctly. And maybe after I apply them to some state and get some specific other state, basically everything you can do a computation for, then you can make a statement of Stark statement starkware we focus currently on the scalability part of Starks, not on privacy. Starks can do both. But right now the most burning issue I would say is scalability on blockchain. This is our focus now the verification time of Starks, that is the time it takes to verify a proof someone made is exponentially smaller than time it took for the prover and for the length of the computation itself. So they are very well suited for scalability.
03:37:26.870 - 03:38:19.494, Speaker D: So in bird's eye view, how does stock work? We have some statement we want to prove. First step is expressing it as polynomials. Basically we call this representation an error algebraic intermediate representation. And I'll focus on this in just a moment. After we have this error, we just throw it on a bunch of algorithms on Fry, LD, Merkel, which comprise of the Stark protocol. And at the other end we get some proof. So how does this error look like? It basically comprises of a trace, which is a table with some constant number of columns, let's say two.
03:38:19.494 - 03:39:21.914, Speaker D: In this example, the number of rows is some power of two and each cell is a field element. In a Stark friendly field we use some specific field with 250 bits, but any Stark friendly field would be okay. Here alongside this trace we have constraints that we want to prove that the trace holds. Each constraints need to be some polynomial on some local area of trace. For example, here you can see this second constraint works on the x column at the current row, the Y column at the next row. The repetition for these constraints needs to be a power of two. The number of constraints is proportional to the verification time.
03:39:21.914 - 03:40:43.030, Speaker D: So we don't want a lot of constraints we want their degree to be relatively small otherwise it will be very expensive. And how can we use this to express statements? For example, if we took the Fibonacci statement we could have, let's say, 1000 rows here, thousand and 24 could say that the first and second element are one and our constraint will be that x two equals x one plus x zero, which is exactly the Fibonacci statement. If we have these constraints throughout all the trace, then it will guarantee that if the first two cells are eleven, then the thousand cell is the thousand Fibonacci number. So in addition, we can also add some constraints that say this is one, this is one, and this last one is one, two, three something. Now that we have this representation, if we can show we have a trace that holds all the constraints, then we are done. The other person can be sure that the thousands number is indeed what we said. So let's start with the journey.
03:40:43.030 - 03:41:52.410, Speaker D: At the first stage we had some very rough tools we were basically building stocks from scratch we need to design how the trace looks like, how many columns it has what is the meaning of each trace cell in the Fibonacci example? It's pretty easy. Each one is the ice Fibonacci cell. But when you have very complex logics, you have to assign some cells to be, for example, the current balance of an account, the amount we want to transfer the signature, the public key, all kinds of things and need to represent the connections between them. These are the constraints we want. Constraints of low degree. If we need some constraint of high degree, we'll need to break it up using some more auxiliary trace cells. There is a lot of manual optimization work involved here, so it's kind of a puzzle sometimes to do this, but it's also very hard.
03:41:52.410 - 03:42:38.478, Speaker D: And doing complex logic is complex. It's not easy. I do want to state here that the main metrics we use to define how good an error is. It's first of all, the trace space, which is the number of trace cells. We need columns, type, number of rows this affects both approval time and memory and we also have the number of constraints, which affects the verified time in a linear fashion. So we don't want a lot of constraints. The trace space also affects the verifier but the only logarithmic, poly logarithmic is aware.
03:42:38.478 - 03:43:19.910, Speaker D: It's not that important. So this was how we did things at the beginning then we started to develop some better tools. For example, one of them was our visualizer we can see the bottom, how it looks like this is basically the air. You can see it's one with eleven columns and some rows each cell. You can see the constraint that work on it. We can add some visual features to these things like output cells. Some things repeat in other periods.
03:43:19.910 - 03:44:15.090, Speaker D: In addition to tools like the visualizer, it's also important to have some abstraction. The ability to not think of all the components in the entire area at once. But do some abstraction. Like this part does hashes, this does signatures, this does amounts or state transition, things like that. So doing it on top of air requires some things about automatic placement on the trace and referring to some specific parts inside components. So we had some in house framework to do all these things took a while. It really helped developing things, but it was still hard doing some complex logic.
03:44:15.090 - 03:45:11.942, Speaker D: Not easier, but very hard. A third step, which I'd call the GPU the previous step was like the FPGA equivalent. The next step is actually something we didn't do, something we considered a lot doing and we had some designs for it. In the end we chose to skip to the fourth step altogether. But I'll show it anyway and the issues there are in this step. So we could design some domain specific language that compiles to an error. It can look for example this is an example code of this imaginary DSL you can call functions.
03:45:11.942 - 03:45:49.270, Speaker D: You can have branches and force. There are multiple problems. First of all, you always pay for branches. Basically you need to unwrap all the possible flows into the trace. So each possible flow, each possible timestamp needs to have some specific cell in the trace. So you don't gain efficiency by doing branches. You pay trace sales for both the first branch and the second branch.
03:45:49.270 - 03:46:35.378, Speaker D: Similar thing we can do, we can say for for loops or recursion. It's not possible to do variable number of for loops. You must always be bounded because you need to allocate specific trace space for it. You cannot really do recursion for the same reason you don't really know where in the trace you need to work right now. You can only have things that have this constant flow pretty much. You also always pay for all the iterations. Even if you right now only need ten.
03:46:35.378 - 03:47:47.850, Speaker D: You need to pay for all the power of two iterations. And if you sometimes you need only ten, sometimes twelve, then you will always pay the highest. So these are drawbacks we have here. Another drawback is there is no memory or it's very hard to implement memory in this thing. It's obviously not complete. Another issue for Starks in this case is the way Starks I would say not scale, but they are dynamic in a sense that if you have stock that works for thousand rows, you can take the same stock and apply it on 2000 rows to have like twice the number of transactions. For example, if we make a stock that handles a banked transaction, you can put 100 in it, 200.
03:47:47.850 - 03:48:51.874, Speaker D: You can use it even for thousands of transactions because it naturally repeats itself doing this repetition in this D cell. Is not very natural and if you need to do some complex logic, it goes out of the window altogether. For example, if you need to do something at the end, for example doing some transactions and compressing at the end, the output or checking something at the end, then there is repetition goes out of the window. So we don't really want to give up that part of stock that they don't work on an instance of a single size but can work on instances of various sizes. This is something we want to keep. Okay, so what we eventually ended up doing caro. Caro stands for CPU Air.
03:48:51.874 - 03:49:53.686, Speaker D: Like regular CPU is basically an ASIC. That can do. General computation and so is Cairo is specific air that can run a general computation it's a universal machine turn complete it's a funny machine it has a random access memory a nice feature is it only has a single verifier because it's single error. So we can and we do we can put a single Verifier contract on Ethereum, for example, and the entire world can use it to know some statements are true. In Cairo, you don't have to deploy it every time for every project you use. We can audit these contracts only once, and therefore, it will be more secure because we can invest more resources in this auditing. There are a lot of benefits.
03:49:53.686 - 03:50:45.090, Speaker D: I would say to having a single verifier. These are example of how Cairo looks like so Cairo is actually two things it's the error and the instructions it can run. We call the Cairo virtual machine and we have some high language. High language that works on top of it, which you can see here. We don't have the drawbacks we present before for the DSL. We only pay for what we use because each repeating part in the error of Cairo is just executing an instruction, single instruction. So if there is instruction within, run another branch.
03:50:45.090 - 03:51:32.530, Speaker D: It signed the trace. We can have complex, non reparative logic, we can have recursion, we can have basically anything that a fun human machine could do. A nice thing we noticed is it's actually very efficient. When we took the cairo version of handwritten air version, when we tried to translate it to Cairo, it was only about 20% to 30%. More expensive. The main reason for that is a lot of applications. This one in particular uses a lot of some built in components.
03:51:32.530 - 03:52:17.940, Speaker D: In this case, hashes and verifications of signatures which are written in handwritten. Error, but just. All the logic on top of it is written in this virtual machine. So we basically get the best of both worlds. The hard parts are optimized using head reader error components and we get the logic of tune complete machine in a high level language. So yeah, it was all only about 20% to 30% more expensive. However, the expressibility of Cairo lets us do some more complex logical optimizations, which we did.
03:52:17.940 - 03:53:04.962, Speaker D: And X was a lot cheaper than the handwritten version. For example, in the Reddit demo we managed to do 300,000 transaction in single proof. Lately, we even did 600,000. And this is mainly due to these logic optimization that lets us save a lot of instructions and hashes. So hey, it's better and cheaper. More can we ask for? So some of the tooling we have for Kyvo first of all, obviously we have the compiler and virtual machine and these are available today. You can check them out.
03:53:04.962 - 03:53:48.000, Speaker D: Our site we have the Solidity Verifier which is deployed today on Robson and on Mainet. Basically everyone can use it. There is sharp tool you can use so we can prove your statements. Basically I'll touch it later. We have integration with Ides, specifically Visual Studio code which we use, and Vim, which some of us use. Language Server there is a tracer and a profiler not written here. It's similar to Debugger, you can see it up here.
03:53:48.000 - 03:54:43.042, Speaker D: Basically, after you make a specific run, you can check it out and go through all the steps in your computation. And we have the playground which you can use today in our site. Play around with Carol and how it looks like. It's very fun. I recommend okay, so important thing I want to touch is why we designed Carol the way we did. There are some things that may seem unnatural to common developers that come from common languages, which we did differently, mainly because we want Car to be efficient and we did it. So it will be very compatible with the Air architecture, I would say.
03:54:43.042 - 03:55:40.446, Speaker D: So this is a comparison between physical CPUs and the Air CPU and why some things should be one way in this one and another way in this one. So first of all, how we measure efficiency in physical CPUs, it's mostly the execution speed, how much time it took to run some computation. In Air, the main metric we use is number of trace cells. This leads to the fun observation. Whereas in physical CPUs, adding more hardware on the chip can lead to better times like branch prediction and caching. In errors it's not really equivalent when you add things, you add trace cells by definition. So we do not want to add trace cells.
03:55:40.446 - 03:56:45.930, Speaker D: We want to keep the architecture as simple as possible, so we won't have a lot of trace cells in CPUs, having multiple registers relatively cheap, adding some hardware and some links, it's usually okay. In errors it's a bit more expensive to have multiple registers, especially on the Verifier side, because every time we use a register we need to choose between all the registers. This chooser leads to a big constraint or multiple constraints which affect the Verifier side. We want our constraints to be simple so the verifier won't work hard. The native word in CPUs is bits, 64 bits. Usually in Air it's a field element because that's how stocks work. Every cell on the trace is finite field element.
03:56:45.930 - 03:58:17.030, Speaker D: This means that bitwise operations are easy in physical CPUs because they work with bits and they're hard in Air. So we don't have some opcode, for example, to do bitwise operations. We do plan to add some built in in the future, which will be a lot faster and do it manually in Cairo operations, but it will still be a lot more expensive than physical CPUs. A nice thing we have in Ers is non determinism basically because it's a language for the Verifier, which means when I write a program I care that it holds some constraints and not necessarily that it's deterministic. For example, when I want to find some element in a big list in a physical CPU, I would have to go over every place in this list or in this array. I have to go over one by one until I find my element or don't find it. In Air it's a lot easier with non terminism I can sort of guess the index where this element appears and I can just check that this is indeed the right element.
03:58:17.030 - 03:58:38.026, Speaker D: Obviously the proverb will have to do this linear work. The proverb still needs to go over the entire thing to prove but once Approver finds the right one, he can write just a few cover instructions that say oh, I guessed it's in the 500 place. I check, I read from it, I check it's.
03:58:38.058 - 03:58:38.638, Speaker C: Okay?
03:58:38.804 - 03:59:31.070, Speaker D: So the number of trace cells will be a lot smaller. Memory Access in physical CPUs, memory access is usually a lot more expensive. You need to access things that are far from the CPU, obviously, because the metric is execution speed. In Air, memory access is actually quite cheap. I'd say it costs about five trace cells, even less three and something if we have some optimizations, which is very cheap. So instead of using registers in Cairo, we use memory access. Every instruction has some memory accesses and we have a very minimal amount of registers.
03:59:31.070 - 04:00:06.286, Speaker D: An important thing to note is the memory freeing thing. In CPUs you want to free memory. If you don't free memory, memory takes up of the entire capacity of memory you have. When you free, you can add more. So it's very good to free memory in error. There is not a concept of freeing memory. Everything you ever did in your program in your run is in the trace.
04:00:06.286 - 04:01:01.770, Speaker D: It must be on the trace because you need to prove that everything was consistent after the entire run you have this big trace, has everything, there is no point in freeing anything. That also means we don't need garbage collection, for example, which is very nice, can access everything in the past. Our memory is also immutable some mutable memory. You cannot change it again because that's basically how it works in Air. In Air you at the end have this big table. If you have a specific cylinder, has one value, at the end of the proof it has one value. We can simulate mutable memory on top of this, but it will be more expensive and it's not very necessary.
04:01:01.770 - 04:02:01.514, Speaker D: There are a lot of languages today that have the Immutable memory model, functional languages for example, and we implement a lot of things in Cairo and it's not an issue almost at all. When it is, we have some constructs that you can use that act like read, write memory. Like I said, it's basically simulating read write memory on top of this immutable memory. But mostly we don't need it and it gives a very big efficiency bonus, I would say. So Cairo today we are using Cairo live on Mainet. We use it both in diversify, immutable and dYdX. This is our website for the Kyle language.
04:02:01.514 - 04:02:48.938, Speaker D: You can see all the developer tools and playground and documents. Everything you need to know is there. We have the Sharp, which is the shared prover services. It's available on Robson. You can send from the playground, for example, straight to Sharp or using our developer tools to check it out, basically runs on our provers and then the fact of your run is registered on chain. We also have a few and obviously want more and to support community projects, for example for compiling from higher level languages to Cairo. So we want it to be easier for developers to use Cairo.
04:02:48.938 - 04:03:43.402, Speaker D: Cairo is efficient, it lets you do zero knowledge proofs. Sometimes you don't need all this efficiency, sometimes you just want to compile from your favorite language. We want to enable that. The white paper of Cairo is coming soon. Hopefully you would be able to see all the intrinsic of the Air and virtual machine, how things work from the inside. And we're currently working on our StarkNet, which is sort of a side chain which uses the Ethereum for its consistency and the safety. And every developer could just deploy his own Cairo contracts and run them and they will be proved on this chain.
04:03:43.402 - 04:03:52.638, Speaker D: And you can communicate with Ethereum, with L one hopefully to be seamlessly.
04:03:52.814 - 04:03:53.780, Speaker C: Very easy.
04:03:54.470 - 04:04:25.210, Speaker D: It's upcoming in the next few months. So I hope you learned a bit about caro and why it's cool and why it's efficient and why you should use it. I'm Shahava Pini, you can email me, you can tweet, you can see my Twitter handle and if you have some questions, I'll be very happy to answer. So thank you and goodbye.
04:04:28.670 - 04:05:05.180, Speaker B: Thank you to all the Starcourtin and Shahar for this very informative video. It was a great talk. Now next to our next speaker. So we have new cipher Ravnital coming in and she will be talking about how fully homomorphic encryption can help handle front running on exchanges. So, thank you so much for joining us, Ravital. Feel free to take over with your talk. Great.
04:05:05.180 - 04:05:51.536, Speaker B: 1 second. So we'll be looking at front running on exchanges. As we just heard, this was a joint work with Gada. So, to motivate the problem, what does front running look like in traditional markets? Well, a lot of financial applications involve buying and selling goods. The example you might already be familiar with is the stock market. In auctions, potential buyers submit their bids where a bid consists of at least two pieces of information. You need to know the number of items someone's interested in purchasing and the price per item they're willing to pay.
04:05:51.536 - 04:06:51.100, Speaker B: So what is front running? Front running consists of some malicious actor seeing a transaction's details, and this malicious actor reacts before this transaction is executed. So how is front running dealt with in traditional markets? Well, there's dark pools which you can think of as private exchanges. There's sealed bids, which simply hide the bid. And more generally, front running is actually illegal for most securities, so you can threaten people with legal action. What are the different types of front running? Well, to start, we're going to imagine there's Alice, which is some innocent user submitting some order, and there's Eve, the attacker. For the purpose of this talk, we're going to be primarily interested in a type of attack called insertion. So, in an insertion attack, alice needs to still run the original function for this attack to be successful.
04:06:51.100 - 04:08:00.820, Speaker B: So an example might be ticket scalping, where Alice is interested in buying some concert ticket. Eve sees this, goes and buys the ticket first and then sells it back to Alice for a profit. So in this case, we really need Alice to still want the concert ticket for the attack to be successful and for Eve to have gotten some money off of this. There are other types of front running attacks, such as displacement attacks and suppression attacks, but we won't be looking at those today. So what does front running look like on blockchain? Well, we're going to look at the image on the right hand side. So you can imagine some user submits some transaction and they are interested in buying, we'll just say, 1000 shares of something, they have some gas price associated with their transaction. A front runner, who in this case is a full node in the network so they can see transactions before they're confirmed, notices this order, and then submits a new order to buy the same amount of shares, but with a higher gas price, thereby bribing the miners to prioritize his transaction over the initial transaction.
04:08:00.820 - 04:09:01.916, Speaker B: So, as you might guess, this is more of a problem in the account based model. So that's what we'll be focusing on. Why don't the traditional solutions work for front running in blockchain? Well, first of all, these aren't necessarily securities so we don't necessarily have the same regulations as securities markets. And additionally, you might be interested in decentralized solutions where you don't want some trusted party maintaining the order book. What are the different categories of solutions to this problem then? If we're looking at blockchain, there are three major categories and these categories are taken from an Sok from Eskandari at all about front running. The first you might look at is could you enforce order in some way for these transactions? But this introduces some other problems and doesn't necessarily solve front running. An alternative category of solution you could look at is redesigning the entire market itself.
04:09:01.916 - 04:09:46.890, Speaker B: So maybe you could try economically disincentivizing front running. The final category of solution could be adding privacy. That way these malicious users can't even see the bid in the first place. So we can look at private blockchains where we might be interested in hiding the inputs and outputs, we might be interested in hiding the functions themselves, and we might be even interested in hiding the identity of the users involved. Additionally, we can also look at sealed bid auctions. But it's important to note that if you're taking this approach, you need some way to prevent early aborts. And by that I mean some way to prevent people from submitting sealed bids that they have no intention of acting on.
04:09:46.890 - 04:10:42.540, Speaker B: So since I'm a cryptographer, we're going to look at category number three and that is can we design some cryptocurrency scheme that offers privacy, at the very least input output privacy? And maybe this scheme can also protect against front running attacks. There are some designs out there that offer privacy. The main four we're going to look at, just because all of them use zero knowledge proofs are Zether, Zexi, ZK and Hawk. All of them offer at the very least, input output privacy, which is necessary for this problem. They have various issues, none of them are perfect. So starting with Zether, it has very limited functionality in the sense that you cannot get privacy for any function. Privacy is limited to functions that can be expressed via addition.
04:10:42.540 - 04:11:12.864, Speaker B: Zexi and ZK have similar issues. They use trusted setups for the zero knowledge proof. But arguably you're not committed to using zero knowledge proofs. With trusted setups, these can be replaced with transparent zero knowledge proofs. However, these constructions, regardless, are very expensive for the user. We will look at that in a little bit more detail in a second. And finally, Hawk has a semi trusted manager.
04:11:12.864 - 04:11:44.720, Speaker B: So not something we would probably want in this kind of solution. We are proposing a new design that offers privacy. Our design is called smart Fhe. It will have no trusted setups, no trusted parties involved. It's going to differ in construction from the previous four in that it will use zero knowledge proofs. But instead of using partially homomorphic encryption or commitments, it is going to use fully homomorphic encryption. It will offer.
04:11:44.720 - 04:12:29.260, Speaker B: Input output privacy as needed. And the primary issue with our construction is also that it's expensive, but it's expensive in a different way. The primary problem is space, particularly ciphertext growth with fully homomorphic encryption. So what is fully homomorphic encryption? For those people who don't know. You can think of it as a special type of public key encryption scheme that offers two additional properties, namely that it's additively homomorphic. So that adding A and B in the plaintext space and then encrypting that is the same as having encrypted A and B first and adding that in the ciphertext space. A similar relation will hold for multiplication.
04:12:29.260 - 04:13:16.232, Speaker B: Something special about fully homomorphic encryption is that it uses lattice based cryptography, which is different from all the other constructions. And another thing to note is that there are three different categories of fully homomorphic encryption depending on how they model computation. You have boolean, you have arithmetic and you have floating point. And we will be looking at arithmetic. On the bottom is just a diagram kind of illustrating how Fhe works. You have some encrypted inputs, you can perform computation directly on these encrypted inputs and get encrypted outputs. So why might we want to use Fhe? Let's take a step back and kind of look at the different approaches in private smart contracts and private computation for blockchain.
04:13:16.232 - 04:14:06.590, Speaker B: So on the left is an illustration of how ZK and Zexi work. More generally, the idea is that the user wants to run some function and they want privacy, input output privacy. So what are they going to do? They are going to run the function on the plaintext inputs and they're going to get some plaintext outputs, but they obviously don't want to share this plaintext information with everyone else on the network. So what they're going to do is they're going to encrypt their inputs, encrypt their outputs, and they need to produce a zero knowledge proof showing that this offline computation that they did on their own was done correctly. What do the miners do in this kind of paradigm? Well, they just show up and they check the zero knowledge proof. The zero knowledge proof is good, everything's fine. So in this case, you can imagine the user is doing a lot of work.
04:14:06.590 - 04:14:57.368, Speaker B: It is lightweight in the sense that all of this work is being done sort of offline except for the zero knowledge proof part being produced. So it prioritizes space. Our approach is going to differ in that we want the miners to do a bit more work and we don't want the user to have to do quite so much work to do private computation. So instead the user is going to provide encrypted inputs. These inputs will be encrypted with respect to a fully homomorphic encryption scheme and they need to provide some zero knowledge proof showing that some conditions dependent on the application have been satisfied for their encrypted inputs. Then the miner comes along, they're going to check that the zero knowledge proof is good. If so, great, they can run a function directly on these encrypted inputs thanks to Fhe.
04:14:57.368 - 04:15:40.984, Speaker B: So kind of balancing the work between the users and the miners, and in some sense you can think of it as prioritizing the user's time. That all sounds great, but there are quite a few challenges with using fully homomorphic encryption, especially in the blockchain setting. So we're going to look at the three main challenges. The first challenge is that Fhe is very expensive. This is sort of true. The time complexity actually isn't that bad. And we have some numbers that we ran to show that there is a bigger challenge with ciphertext growth, in the sense that the ciphertext grow quite quickly and they're quite large for Fhe.
04:15:40.984 - 04:16:34.460, Speaker B: But there are other avenues that you can explore to potentially solve this problem. Challenge number two is that unlike the other constructions, fhe uses lattice based cryptography. We need a zero knowledge proof that works with lattice based relations and allows us to prove lattice based relations. But a lot of the lattice based zero knowledge proofs themselves are very expensive. So we need to ask what kind of efficient zero knowledge proof can we use? Thankfully, there's a pretty good construction short discrete log proofs construction that uses Patterson commitments and produces decent sized zero knowledge proofs for lattice based relations. And finally, three. So if you're familiar with fully homomorphic encryption, you probably already know that you can only perform computations on ciphertext encrypted under the same key.
04:16:34.460 - 04:17:27.928, Speaker B: How are we going to perform operations on inputs, especially encrypted inputs that belong to different users and therefore are encrypted under different keys? This is going to require some additional logic in the Smart contract, but it's certainly possible. There is a different variant of fully homomorphic encryption called multi key fully homomorphic encryption, and it allows for truly arbitrary computation on inputs that are encrypted with respect to different users and different keys. But we're not going to look at this for now. So what are the cryptographic tools we'll need for the Smart Fhe construction? We need a fully homomorphic encryption scheme. As mentioned earlier, we want one using arithmetic circuits. This kind of narrows us down to BGV and BFB for basic transactions. We don't need the full power of fully homomorphic encryption.
04:17:27.928 - 04:17:53.172, Speaker B: You only need additive homomorphisms. We'll need a zero knowledge proof that helps us prove lattice relations. We're going to use the short discrete logs proof construction. And then additionally, once we have the Patterson commitment, we can use Bulletproofs for other relations. And finally, we'll need a digital signature scheme or some way of signing transactions. But that's not a problem. Just a quick look at Fhe numbers.
04:17:53.172 - 04:18:22.224, Speaker B: We ran some benchmarks of the BFB scheme from Microsoft Seal. We ran it on pretty basic machine. We have the smallest and largest parameters that are supported by default for BFB. And you can see on the right that the numbers are pretty good. The only thing that's a bit expensive is key generation for the largest parameters. But everything else is like under a second. There is, like, as we noted before, an issue with ciphertext size.
04:18:22.224 - 04:18:59.950, Speaker B: So just getting a sense of how do the ciphertext grow and how large are they? After you perform a homomorphic multiplication operation for the smallest default parameter, supported by seals BFE, it's about 13 KB. So decently large. These are some numbers to give you an idea of the short. Discrete logs proof construction. The performance is dependent on the elliptic curve you choose. We did curve two, five 5119. The proof size is pretty small, less than one and a half kilobyte and using multiple threads decently fast.
04:18:59.950 - 04:19:47.320, Speaker B: So how is our construction actually going to work? Well, we're going to look on building on Ethereum's model. So the idea is that we're going to build on top of a smart contract enabled cryptocurrency like Ethereum that supports public transactions and public operations. So we're going to have an account based model. We're going to imagine that there is still signature scheme associated with the public account. You still have a nonce that's incremented with each transaction and you support smart contracts. Every operation is still associated with some cost and there are fees attached to each transaction that affect the priority. What new is going to be added to this construction? Well, in addition to this public account, we're going to imagine we now also have a private account.
04:19:47.320 - 04:20:25.400, Speaker B: And the private account has an Fhe key set associated with it. And this Fhe key set is going to be used to encrypt the balance. That way a user maintains an encrypted or secret balance similar to the public account. We also need a nonce that is unique to the private account and will be incremented with each transaction. And the idea is that we're going to use these private accounts to perform private transactions. In private smart contracts there are some issues when we're looking at accounts with privacy and concurrency. So we're going to introduce some sort of locking mechanism to prevent conflicts.
04:20:25.400 - 04:21:03.372, Speaker B: So what does a private transaction look like in smart Fhe? Well, we have the sender and receiver. Both of them have their own fully homomorphic encryption key pairs. They both have their own encrypted balances. Imagine the senders as B and the receivers as B prime for a private transaction. Our goal at the minimum is to hide the transfer amount and the balances of the users involved. So what would go into a private transaction? We need to know who the recipient is. We're going to encrypt the amount we're interested in sending under the sender and receiver's public keys with respect to the same randomness.
04:21:03.372 - 04:21:47.612, Speaker B: So we'll need to publish the randomness and then we need some proofs. We need a proof that these two CipherTechs actually encrypt the same transfer amount and they use the same randomness. We need a proof that we've sent a non zero transfer amount to somebody. We're not sending somebody a negative amount of currency, and we need a proof that the sender actually has enough money or currency in their account to perform this transaction. And as we stated earlier, you don't need the full power of fully homomorphic encryption to actually do this. You can simply use the additively homomorphic aspects to encrypt the senders and receivers balance because they're addition operations. So again, more generally, how does smart Fhe work? The idea is that the user provides some inputs.
04:21:47.612 - 04:22:59.620, Speaker B: If the inputs are public or unencrypted, a smart contract just proceeds in the same way as it normally would, or a transaction proceeds in the normal way, as it might in Ethereum. If the inputs are instead encrypted, and in this case, encrypted with respect to fully homomorphic encryption, the user probably needs to provide some sort of zero knowledge proof proving some relations on these encrypted inputs that are application dependent. The miners can come along, they then check the zero knowledge proof and if everything's good, they can perform a computation directly on the user's encrypted inputs, thanks to Fhe. So back to our original problem. What is going to happen for sealed bid auctions? So the idea is that a seller will post some items for auction, say that the seller has up to 100 items to sell, a potential buyer comes along. And the idea is that the buyer can bid on a variable number of shares. So anywhere between one to 100 shares, one to 100 items that the seller has, and they will submit a sealed bid and then reveal their bid at the end of the auction.
04:22:59.620 - 04:23:55.610, Speaker B: This isn't anything new. Sealed bid auctions have been around for a while. So in this case, we'll need a zero knowledge proof showing that buyers have enough currency in their account for the total bid value. And possibly, depending on how you want to write this, you could also have conditions that show the number of items are within the posted range, the price is within some specific range. As mentioned earlier, we're going to need a locking mechanism to prevent parties from submitting bids and then just running away with no intention on actually acting on them. And on the right here, you can just kind of see a diagram showing what a bid consists of, as we stated earlier, number of shares, price per share, and different ways of bidding. Whether you're bidding on a fixed number of shares, in which case you need fewer pieces of information, or if you're interested in biding on a variable number of shares and how that might work, depending on if you have fully homomorphic encryption or not.
04:23:55.610 - 04:25:02.540, Speaker B: So back to the kind of comparison of approaches we were looking at previously, the zero knowledge proof based approach taken by ZK and Zexi, for example, and then the fully homomorphic encryption based approach that we're suggesting and proposing here. So in a zero knowledge proof based approach, a user will need to compute a couple of different values. They will need to have the commitment or encryption to the number of items, the price per item and then the total bid value, which is N times P. They are going to need to produce a more complex Urinolog. Proof showing that they've done this update on the encrypted or committed values correctly because there is no multiplicative relation between the ciphertext and the idea is that the user performs all computations. So, as we stated earlier, the idea is we want to rely on zero knowledge proofs to ensure that updates were in fact done correctly. So on the bottom left, you can get a snapshot of all the pieces of information that a user might need to provide to participate in a sealed bit auction.
04:25:02.540 - 04:25:36.120, Speaker B: On the right we have a fully homomorphic encryption based approach, as would be in Smart Fhe. And you can see on the right that there are fewer pieces of information that a user might need to provide, and the zero knowledge proof is a bit simpler. So it's an easier transaction or easier computation for the users. More generally. How do these solutions map out and what do they look like? It's difficult to say. One solution is superior to the other. It really depends on what's most important to you.
04:25:36.120 - 04:27:01.830, Speaker B: There are a lot of trade offs, the main ones being are you okay with additional trust assumptions like trusted setups, semi trusted managers? Does efficiency matter to you? And if so, what kind of efficiency? Do you prioritize time or do you prioritize space? And then finally, do you need full functionality? Are you okay with, for example, Zether, where you can only do private computations on certain kinds of functions? And finally, just some last considerations, is front running best solved by cryptography? I will not offer an opinion on that. There are plenty of different solutions you could be okay with dark pools, maybe economic disincentives make better sense. More generally, should we combine cryptographic solutions with other non cryptographic solutions? And as we suggested before, if you choose to take a cryptographic approach, what's more important to you, time or space? And that's it. Thank you so much, Arita. That was super interesting. Let me see anyone on the chat. Please post your questions if you have anything while I ask one of the questions, as we have a few minutes, maybe if you can clarify how much of this is a proposal versus implementation that has been worked on.
04:27:01.830 - 04:27:40.352, Speaker B: So it is a full paper that's currently being submitted to the conference. There is benchmark numbers to get a sense of the feasibility compared to the other approaches, but there isn't a full working implementation quite yet. Awesome. Second question that we have is what does the zero knowledge ecosystem needs to do to bring this to reality? I know, it's broad. It's like putting on the spot. Here, take it the way you want. If you or your team is doing this.
04:27:40.352 - 04:28:27.150, Speaker B: Maybe we're not working on this actively, but I think the prover times and the verification times just need to be improved significantly. I think the fact is, with whatever papers are kind of currently being written, they all use trusted setups for a reason. And it's because that's what's offering practical efficiency. So until we have transparent zero knowledge proofs with good efficiency that don't have ridiculous hardware requirements, I think we have a problem. Do you think that any of this can be done on Ethereum? I mean, it kind of goes with your question. I'm optimistic about it, but I can't say for sure yet. Thank you.
04:28:27.150 - 04:28:39.750, Speaker B: We have a lot of smart mind on this space and it's great to have you go through it and I hope it inspires other to break their head around this problem. Great. Thank you. Thank you for joining us today.
04:28:42.200 - 04:29:07.324, Speaker A: That was a really awesome talk. I was just in the background looking at it and this is a super interesting and simple explanation of fully homomorphic encryption that I've seen. So thank you so much. Rabido and Catherine, thanks again for taking over for me and being an awesome co host as well. So thank you. And for everybody watching in, we are at the final stages of this summit. This is our last talk of the day and also a talk I'm super excited about.
04:29:07.324 - 04:29:19.490, Speaker A: So without further ado, I'd like to welcome Brian to talk about procedural generation using Snarks and he's going to talk about that from the context of Dark Forest. So Brian, feel free to kick us off.
04:29:22.120 - 04:29:31.990, Speaker G: Awesome. Hey, folks, I'm going to share my screen here and thanks for having me. E global team. And Karthik, it's great to be here.
04:29:32.520 - 04:29:34.150, Speaker A: We're excited to have you.
04:29:37.480 - 04:30:20.256, Speaker G: Sorry, 1 second. I'm trying to figure out how to oh, here we go. All right, can you all see this? Sweet. So, everyone, my name is Brian. I work on Dark Forest and I also do some work on applied zero knowledge research and development with the Ethereum Foundation. Today I'm going to be talking about procedural generation in ZK Snarks, and in particular, some of the challenges we've run into while experimenting with this technique. Cool.
04:30:20.256 - 04:31:32.980, Speaker G: So, to motivate everything that I'm going to be talking about in today's presentation, I want to start off with this question. How could we build minecraft on ethereum? So, I think a lot of people are very excited about the idea of decentralized worlds that are very rich and expressive and that a lot of people can participate in and build on. And one of the best examples, and most familiar examples to many people is the game Minecraft, which is a three dimensional sandbox game with a rich and varied landscape, a ton of different items and blocks and things that you can build with one of the key features of Minecraft is its very rich procedural generation base. So when you spawn into a world in Minecraft, you're going to spawn into this world that has forests and oceans and deserts and all sorts of both local and global scale structures. And with Ethereum and a lot of decentralized computing platforms, we don't quite have the same amount. Like, we have very strong constraints on things like storage. How much can you actually fit into the EVM? We have constraints on things like transaction throughput.
04:31:32.980 - 04:32:35.448, Speaker G: So it seems like today we're very far from being able to build out a complete and rich sandbox game like Minecraft on Ethereum. But our theory from the Dark Forest team is that using some techniques, combining them with ZK snarks, we might be able to get closer to realizing something like this vision. So to dig into a little bit of context for what we're talking about today, we're going to specifically be talking about some of the challenges we've encountered trying to implement perlin noise inside AZK snark. This is a technique we use in Dark Forest and it's something that we're exploring more complex variations of over the last couple of months and throughout the rest of this year. So Purlin Noise is a procedural generation algorithm for generating rich and varied game world terrain. So you can think of it as a function that can map an arbitrary coordinate pair x comma y to say, like a height on a terrain. ZK snarks allow for constant time contract side verification of the execution of arbitrarily complex functions.
04:32:35.448 - 04:33:35.336, Speaker G: So, in other words, if I have a function that would cost a lot of gas to execute in solidity, like, let's say, like, hundreds of thousands or millions of gas, if I can wrap that up inside a zksnarc then the cost that I pay for execution of this function essentially is just the verification cost, which is going to be a few hundred thousand gas on chain. So putting these two things together, if you want rich procedurally generated worlds on the blockchain, one idea is to put perlin noise inside of a ZK snark. So just a quick refresher for folks who might not be as familiar. A zkSNARK is a tool that basically allows you to prove knowledge of inputs of a function that correspond to some known output. So for example, in this slide here, we have some very simple function. F of x equals x one plus x two times x three minus x four. A ZK snark for this function would allow a prover.
04:33:35.336 - 04:34:35.490, Speaker G: To prove that, let's say I'm a prover, I will give you a claimed output to this function f of x that I claim I have the four inputs x one x two x three x four for this output. And then I'm also going to send you over a proof, which you can check, that will convince you that I do know, four inputs which when the function is correctly computed on those four inputs, does result in out. The zero knowledge property of ZK Snarks ensures that I can actually do so without telling you anything about these four inputs. All you're convinced of is that I do have the four inputs and that I've executed them correctly. But it turns out that for the procedural generation stuff we're interested in, this part is actually not as relevant. The important thing is that the verification of the signature takes constant time. So more specifically, what ZK Snarks are doing is that they're proving constraints on what you can think of as kind of like the computation graph associated with this function.
04:34:35.490 - 04:35:48.484, Speaker G: So in this previous example, we have this function f of x equals x one plus x two times x three minus x four. This can basically be thought of as if I want to perform this computation in steps, I can create an intermediate value y one that I set equal to the sum of x one plus x two, and then an intermediate value y two, that's the product of y one and x three. And then out is simply just going to be that second intermediate value minus x four. So what the Snark sees, what I plug into a Snark is going to be the input values x one, x two, x three, x four, those intermediate values y one and y two, and that output value out. And what the Snark is actually doing under the hood is it's going to generate a signature that is valid if and only if constraints corresponding to those intermediate execution steps are satisfied. So when we're thinking about doing the computation, we're thinking about sort of traversing this computation graph forwards, starting with x one, x two, x three, x four, computing the intermediate values and then eventually getting to the output. What the Snark sees is it will sort of digest all seven of these values associated with the computation and produce a valid signature.
04:35:48.484 - 04:36:41.950, Speaker G: If the constraints hold that the first intermediate value is equal to the sum of x one and x two, the second intermediate value is equal to the product of the first intermediate value times x three, and the output which I'm sharing with the world is equal to the second intermediate value minus x four. So the Snark is sort of basically doing this crunching on seven inputs. So just as an example, suppose that I take inputs two, four, eight and five. The result that I'm going to get if I plug all these into the function is 43. The Snark is basically going to see these seven numbers that are produced during the course of this computation. And what the Snark is able to do is it will generate a valid signature of the computation if three constraints on these seven numbers are satisfied. And because these constraints exactly represent what's going on with the computation in this way the snark is essentially proving correct execution of the function.
04:36:41.950 - 04:38:06.228, Speaker G: So one thing that's important to note is that Snarks can only generate these proofs for constraints involving addition and multiplication. So in the previous example with this function, this function is very simple and only involves additions, multiplications and subtractions, which are just reverse additions. But most functions that we find in the wild and that we might care about could involve other more complex operations like division or modulo. So this means that in general, if we're writing a ZK Snark scheme to allow people to prove correct execution of a function, what we're going to have to do is we're going to have to figure out how to express correct execution of that function in correspondence with a set of constraints that only involve addition and multiplication. So in this example, we have a function that looks very similar to that previous function, except this second intermediate value y two is y one divided by x three. Because division is not a valid operation that we can have inside of a constraint for a ZK snark, what we have to do is we have to express this step of the computation as a constraint that only involves multiplication. So here previously we had the constraint y two is equal to y one times x three.
04:38:06.228 - 04:39:34.160, Speaker G: In this case, this expression is satisfied when this constraint y one equals y two times x three is satisfied. So this is sort of an example of a case where the execution of a function doesn't exactly line up with a constraint system that sort of proves its correct execution. But it is hopefully pretty clear to see that a Snark that is digesting these seven values and verifying these three constraints is essentially verifying correct computation of someone who's gone through with these inputs and plugged them into these two intermediate values and ended up with this output. In other words, correct execution of this function corresponds exactly to these three constraints between these seven values being satisfied. Yeah, so I'm not going to go too deeply into this, but when we start getting to more complex operations like convert some integer x into its bitstring in an array representation, we're going to have to use this trick or tricks like these even more heavily. So rather than there's not an easy way to go from an integer to its bit string using plus and times in sort of the ordinary computational way. But what we can do is we can set up a constraint system that is satisfied if and only if what we claim as the bit string representation of an integer x is indeed the bitstring representation.
04:39:34.160 - 04:40:48.342, Speaker G: And a final piece of nuance with all of this is that in the Snark schemes that we use, so we're using circom and snark JS, which are developed by iden three, all of these operations are happening modulo at 254 bit prime. So you can basically think about like there's this problem that addition and multiplication will overflow if you go past the value p that is kind of hard coded into the protocol. This often means that if we are doing operations where we're sensitive to overflow, we're going to need to add constraints that ensure that every step of the computation is not, in fact, overflowing. All right, so that's just a quick refresher on ZK Snarks. Now I'm going to talk a little bit about Perlin Noise. So as we mentioned, Perlin Noise is an algorithm that allows game developers to build rich and varied terrain that looks sort of random globally, but has local structure. In other words, you can see that this terrain here basically is generated by some function, let's say F, that ingests a coordinate pair x comma Y and outputs a height h.
04:40:48.342 - 04:41:54.682, Speaker G: And the guarantee that this function gives is that points that are close to each other are similar heights. So we're not just like this isn't just a random scattering of points, but if you zoom way out, then you get sort of these global features, but also global randomness. So to dig into a little bit at a high level how this perlinoise algorithm works. The idea is essentially, in order to generate the height map of this terrain, the first thing that the perlinoi's algorithm does is it splits the terrain into grid squares, and then it's going to randomly pick a unit vector and assign it to each grid square sorry, grid square corner. So you can see here we have this little table of grid squares. At every corner, we've picked a random unit vector, so a random directional vector with length one pointing in a random direction. These gradient vectors are essentially going to correspond to these unit vectors are essentially going to correspond to the gradients of the terrain at each of these points.
04:41:54.682 - 04:42:43.414, Speaker G: And so what we're going to do is once we've picked all of these essentially slopes of the terrain, at every grid point, we're just going to interpolate to figure out what the value should be at all of the intermediate points. So in this example, we've got this heat map on top, superimposed on top of this grid. Green means higher and purple means lower. So you can kind of see, like, if you look in this grid square, the three vectors that were assigned to each of these three corners were all pointing inwards and correspondingly. What that means is that on the eventual terrain we're generating at those three points, the slope when you're moving towards the center of the grid square is generally going up. So you can see that that corresponds to a darker green region over here. In this square, you see that the four vectors are pointing outwards.
04:42:43.414 - 04:42:45.420, Speaker G: So what that means is that.
04:42:47.410 - 04:42:47.726, Speaker F: The.
04:42:47.748 - 04:43:25.574, Speaker G: Gradient going inwards is going to be sloping down. And this sort of represents like a dip or like a valley in the terrain function. So Perlinoise basically says sample gradient vectors at every corner and then interpolate. And then you're going to get like a reasonable looking terrain function. And there's a couple more tricks, but this is sort of like the base foundation level one part of the algorithm. So what does this actually look like mathematically? Essentially it means that we need to be able to pseudo randomly sample gradients unit vectors, which is hard to do with only plus and times. But we have some tricks that we'll discuss for doing that.
04:43:25.574 - 04:44:20.810, Speaker G: And then the second part is that we're going to have to just perform a lot of dot products because we're doing all this interpolation. So we're basically just multiplying a bunch of vectors together. Unfortunately, this is actually really easy to do with plus and times, which is why this was the algorithm we picked to sort of start exploring snark implementations of. All right, so what I'm going to do now is I'm going to discuss some of the specific challenges of fitting all of this stuff inside of a snark. The first challenge is that you need to be able to pseudo randomly sample these gradient vectors at every grid corner. So why is this hard? Well, essentially what we need to do is we need some function F that I can plug in the coordinates of a grid corner. And as output I'm going to get a random unit vector.
04:44:20.810 - 04:45:43.414, Speaker G: So I need something that's essentially going to say like, hey, for every grid corner here, for every pair of grid corners, regardless of if they're close to each other or far away from each other, the vectors associated with this grid corner are essentially going to be uncorrelated. So how am I going to do that? What I'm going to do is I'm going to use a snark friendly hash function essentially as like a random oracle. So there exists snark friendly hash functions like Mimsi and Poseidon. And what they're optimized for is they're optimized for basically creating these random looking outputs you can think of while being very cheap to verify inside of a snark. So Mimsi, for example, just looks like a bunch of consecutive additions and exponentiations to a fixed power. So by essentially plugging in these grid corner points into a Mimsi function and then doing some massaging, we can get a function that essentially acts like a random oracle from which we can drive the random unit vector that's superimposed onto each grid corner. The second challenge that we encounter is that throughout the process of doing this perlin noise algorithm, oftentimes we find that we have to do divisions and Modulos and operations like this.
04:45:43.414 - 04:46:35.094, Speaker G: For example, one thing that you're going to need to do to do this interpolation is you're going to need to determine which square are you actually inside. And this essentially amounts to doing something that looks like a quotient and modulo operation. Now the reason that this is hard is, well, there's a couple of reasons why this is hard. The first is because you're already implicitly working in this field. Like all of your operations are modulo p for the protocol, baby. Jub jub prime p. And so that means that if you're not careful about things like overflow, then you can open up your modulo ZK proofs to certain attacks where attackers are basically able to or malicious provers are basically able to pretend that a certain value is the result of the modulo operation when it's actually not.
04:46:35.094 - 04:47:52.720, Speaker G: So basically we have this modulo circuit that ends up being something like 100 lines in circom where we're doing overflow checks at each step. The second thing that you have to be careful about is sine. Since we're working in mod p natively there's not really the notion of negative numbers. So you have to enforce some sort of convention where for example, we're going to consider all the residues between p over two and p to be negative and then the remaining residues between zero and p over two to be positive and then you have to do overflow checks associated with that. So something even as simple as just like proving that three is congruent to eight mod five inside of a snark ends up being more challenging than you might expect. The third challenge that we encounter is essentially taking the randomness that we are able to generate using or like the pseudo random values we're able to generate with Mimsi which we're using sort of as a random oracle and turning those into random unit vectors. So inside of the snark we basically have to be able to assign coordinate like a vector that has length one and some arbitrary direction sort of determined by the randomness to each grid point.
04:47:52.720 - 04:49:01.910, Speaker G: This is hard because typically when you're sampling from if you're trying to sample a random direction this is going to involve the use of trigonometric functions like sine and cosine. So the first approach that we tried to this was essentially using the value that we got from the random oracle as the input into a Taylor approximation of sine and cosine for various reasons that actually involved. The fourth challenge that I'll briefly touch on this is actually infeasible. So what we ended up going with was we ended up using a circuit written by some of the folks over at Semaphore that allows you to select from a list of values. So we hard coded in 16 direction vectors and using this selector circuit we're basically able to index into this list and pick from the set of vectors. So for any given grid point that we're trying to sample a unit vector at we're going to take that grid point, plug it into MIMC and get this randomness. We're going to take this random value modulo the length of the list and then we're going to use the selector circuit to basically pick out one of these unit vectors from this list of unit vectors.
04:49:01.910 - 04:49:57.938, Speaker G: And the final difficulty that we ran into here that I'm going to discuss in this talk is that with perlin noise you're going to be dealing with non integer values. So, for example, a unit vector is a vector whose both coordinates are not going to be integers unless you're at 10 negative 10010 negative one. So we tried a couple of approaches to dealing with this problem. The first thing that someone tried was implementing a series of fixed point arithmetic circuits in Circom. These turned out to be, like, way too expensive to actually defeasible. So what we ended up settling on was we ended up deciding to do all of our arithmetic inside of our ZK circuits with fractions. And the way that we'd represent this is that we'd hold as a global constant in our circuits the maximum denominator D that any calculation would ever encounter.
04:49:57.938 - 04:50:57.786, Speaker G: And we calculated this ahead of time by basically running the perlin function in JavaScript and sort of figuring out what the maximum denominator we would encounter was. So we'd hard code that in there and then all of the math inside of the circuit would be done on the numerators, implicitly knowing that these are numerators of fractions that are divided by the denominator. So we're tracking this global constant denominator D and we're doing all this math with numerators. And so what this means is that whenever we add two values in our circuit, we sort of know that we're adding, like, A over D plus B over D, and we're getting the value A plus B over D. And it also means that whenever we're multiplying two values, we have to multiply the values and then divide out the extra factor of D that we're getting due to the fact that A over D times B over D is AB over D squared. So that's how we got around the difficulty with these calculations not being integer calculations. Yeah.
04:50:57.786 - 04:52:16.660, Speaker G: So the result of this, we have a very sort of basic version of the perlin noise function implemented in Dark Forest, which those of you who played in the last two rounds have probably seen. Basically, perlin noise allows us to add texture to the universe by making it so that some regions have richer areas with more resources than others. The goal here, though, would be to get to a point where perlin noise can be used to create very expressive and featureful landscapes, potentially something that could even look someday like the minecraft world. Beyond these difficulties, one kind of meta difficulty with doing all of this is that the development environments and sort of like the dev tooling for ZK application development is currently still quite rudimentary. So I'll just give a quick shout out to Jacob Rosenthal and Blaine Bubblets, who have been working on a lot of tools for ZK development workflows. But all of these challenges are made more difficult by the fact that we're still very early in ZK application development and the sort of workflows that we have are not very developer friendly. So if anyone is interested in applied zero knowledge development, this is definitely an area that a lot of work can be done in and would be very useful for.
04:52:16.660 - 04:52:50.080, Speaker G: Yeah, so that's pretty much it for my talk. If you want to learn more, we have a blog where we post a combination of Dark Force game related content as well as some ZK development content at blog. Zkga me. You can also find us at Twitter at underscore ETH and we will be releasing the next version of Darkforce in a few weeks, targeting around mid May. Finally, if you want to reach out, you can find me on Twitter as well at at BGU 33. Yeah, thanks for having me.
04:52:52.370 - 04:53:33.450, Speaker A: Amazing. Thank you so much Brian, on such an interesting, and I would just say like a very technical but super detailed and important overview of how this is done. I think this is a key part of what makes Dark Forest a thing and adds the actual natural sort of appeal to really relying on the ZK piece here. So this is super cool. If anybody has any questions from the audience, please feel free to ask them on the chat and we'll relay them on this live. In the meantime, I think you've kind of pointed out a handful of recurring themes that I noticed from the talk. I guess my high level question here is I would categorize this right now as cold golfing for lack of a better category.
04:53:33.450 - 04:54:00.200, Speaker A: And given how early we are in anywhere from the tooling to the development side to just the number of people who are aware of how these things work to get something in the application side. What do you think is sort of missing the most here? Is it the lack of like we need a standard library for lack of a better way to think about it, or we just need more standards on somebody should just do this thing and we just all use the same library every time or kind of what does that repeatability or abstraction look like?
04:54:00.970 - 04:55:03.350, Speaker G: Yeah, so one thing that I think about is that ZK application development I feel like is pretty inaccessible unless you actually have some certain level of math background or a basic understanding of number theory, for example. So for example, the particular nuances around the modulo circuit and making that secure are only going to really make sense if you are very familiar with or have a good intuition. For the Chinese remainder theorem, it's like, why do I need to do these overflow checks here? What could a malicious prover actually do? Well, the thing that the malicious prover could do is because of the Chinese remainder theorem, you can do the Euclidean algorithm to sort of figure out what the malicious value would be for the modular thing in short. I think what we need is there's a couple of things. One is a standard kind of library of circuits that people can kind of plug and play into. Another would be a higher level kind of DSL on top of, for example, Circom, which already does like a phenomenal job of making any of this possible at all. It still just blows my mind that it's possible to program ZK snarks at all.
04:55:03.350 - 04:56:07.734, Speaker G: But the next step is that it should be accessible to program them as well. You shouldn't need to be very familiar with number theory in order to build applications like this. Like an example that I've been thinking about a lot is that cryptography under the hood is very complicated, but we've sort of figured out collectively as a space how to correctly abstract away the mathematical operations underneath cryptography and provide an API, like a conceptual API for developers to know. Like, oh, encryption has these guarantees, decryption has these guarantees, message signing looks like this. And this is manifested then in a bunch of the sort of standard libraries and whatever language you're using for these cryptographic operations. And then another thing that I think will be really big, I mean, that will take a while because I think it's definitely the case that the protocols even it's not even clear what the protocol guarantees are going to be in the future. But the other thing that I think about that is always ongoing is devtooling to allow for just faster iteration cycles, better feedback from better feedback loops for developers.
04:56:07.734 - 04:56:17.120, Speaker G: That's a lot of the stuff that Blaine and Jacob have been doing, which I'm really excited about. So it's a combination of a lot of things. But I do think we are very early.
04:56:17.970 - 04:56:30.386, Speaker A: No, I totally can 100% echo that. And I think just kind of this is the natural evolution of new technologies in general. Like, this is the punch cart fortran era and then we want the fortran to python jumping.
04:56:30.418 - 04:56:31.302, Speaker G: Exactly. Yeah.
04:56:31.436 - 04:56:37.270, Speaker A: Just like you don't have to think about how registers work. Somebody needs to abstract the number theory here.
04:56:37.420 - 04:56:38.120, Speaker G: Right.
04:56:39.610 - 04:56:56.890, Speaker A: Kind of having said that, super excited to see that you were announcing the new version of Dark Forest. And maybe just as a clarification, was what you talked about an analysis of what the last version of it was or how you're using these things to now make the next version of Dark Forest more generated?
04:56:57.390 - 04:57:34.626, Speaker G: Yeah, so these were some of the challenges that we've encountered trying to implement just like a very rudimentary version of Perlin Noise for the last couple of versions of Dark Forest. We're still using these and continually working on improving them for the upcoming versions. Specifically, though, what we're also interested in is we think that this technique could be a building block for a game beyond Dark Forest as well, or for games, plural, beyond Dark Forest. And so what we're going to be exploring in the next couple of months is ways to bring this mechanic into other kind of prototypes or proofs of concept of simple blockchain games.
04:57:34.818 - 04:57:59.806, Speaker A: Awesome. Well, with that, I want to thank you so much for taking time on Friday and doing this amazing talk. And if you have any other questions that have not been answered, we'll relieve them to you directly. And for anybody who's going to watch this later or watching this now, we'll have a Snipped video of this for just a 30 minutes talk available online on YouTube later so you can all catch up and check out the slides and the math directly. So thanks again, Brian.
04:57:59.998 - 04:58:02.100, Speaker G: Thanks Karthik. Thanks for having me.
04:58:02.710 - 04:59:10.390, Speaker A: Awesome. So that was the end of today's summit and we saw an amazing and incredible set of talks and conversations and around how Azure Knowledge Proofs are kind of helping with scalability, whether that's on transactions to security, to making it easy for everybody to use more programmability into their use cases. And we kind of saw a handful of DSLs that got into that are ready or almost ready to be used by everybody and abstracting everything we want to do for full on decentralized apps and a lot more topics. That looks like we're going to see a big revolution on in terms of computing and managing how we evolve decentralized systems. So I hope that you all enjoyed today and this was kind of the full close of our Zero Knowledge Scalability summit and kind of with that what comes after. This is interesting. Initially we talked about we're going to be doing three summits as part of Scaling Ethereum, but we've gotten so much interest for so much more amazing topics and content and speakers to be part of this thing.
04:59:10.390 - 04:59:57.874, Speaker A: I'm super happy to announce that next Thursday at 11:30 a.m. Eastern, we'll be hosting a one off final summit for this event and it'll be the Mev Roast. For those of you who are not familiar with Mev Roast, Apecob will be hosting this thing next week. But it's a monthly deep dive into just kind of going into how the minor extractable value ecosystem has evolved and just the general research and what's happening in that era. It's a really promising and a really big part of what's going to really define DeFi over the next little while. And it already has. And this was originally started as a monthly event by Flashbots team and we'll be hosting this next week on behalf of the MEB Roast Collective.
04:59:57.874 - 05:00:33.050, Speaker A: So we'll see how scaling affects MEB and this also jumps really well into what we're going to be announcing next as ETH Global overall. So all the details will be made public very soon in the next day or two on Scaling ETHGlobal Co. So if you're interested in getting more up to date on what's happening with mev, you can check out the ETH Global scaling website. So thanks again for joining in today and looking forward to seeing all of you next week. And good luck to everybody who's finishing up the final week of Hackathons Hackathon Submissions. Thanks, everybody. Bye.
