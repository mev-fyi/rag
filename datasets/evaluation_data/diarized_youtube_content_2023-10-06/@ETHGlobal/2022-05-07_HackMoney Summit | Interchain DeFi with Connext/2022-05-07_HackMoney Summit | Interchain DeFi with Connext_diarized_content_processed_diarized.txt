00:00:04.090 - 00:00:31.906, Speaker A: So next up, I want to invite Arjun to talk about interchange DeFi and how Connects makes that a lot more convenient and simpler. So without further ado, let's welcome Arjun, and I'll let you take it away from here. Awesome. Thanks so much, Kartik. Hi, everyone. My name is Arjun. I'm one of the founders of Connect enables fast, trust minimized communication between blockchains and roll ups.
00:00:31.906 - 00:01:30.710, Speaker A: We've been in the space for a very long time, heavily researching L2s since like, 2018, and then started working on cross chain last year, and right now have a live product that works on transferring funds between chains that is now being upgraded to do more generalized messaging. And I'll kind of go into what that means in a couple of minutes. This talk is about crosschain DeFi. I know a lot of people have been thinking about what DeFi looks like when you're now in this interchange interval of paradigm. And this talk hopes to illuminate a little bit about what kinds of things can be built in this cross chain paradigm and what kinds of pitfalls exist when trying to build projects in this cross chain world. So, yeah, without further ado, let's start by just some basic background on bridges, bridge, trade offs, and connects. One of the key things with building distributed systems is that there are always trade offs.
00:01:30.710 - 00:02:20.154, Speaker A: This is just like the unfortunate nature of being in our space. And this is especially true for bridges because really, with a bridge or an interoperability system, what you want is a combination of three things. You want it to be trust minimized. Obviously, you want to kind of maintain the security properties of the underlying blockchains, if at all possible. You want it to be generalizable, by which I mean you want to be able to take any arbitrary data and pass it between chains. You can build arbitrary applications, and you want it to be extensible, which means that you can take the same system and replicate it on a bunch of different chains and roll ups and other kinds of constructions without having to go and do a bunch of custom work to integrate. And generally what's existed in the past is that any construction that has existed has only ever been able to fulfill a couple of these requirements at any time.
00:02:20.154 - 00:03:02.070, Speaker A: So the way that Connects works currently, for instance, uses atomic swaps. And while it is trust minimized and can be deployed easily to any chain, we cannot handle any arbitrary kind of payload. We can do some types of contract calling across chains, but not every kind of contract calling. Similarly, you have MPC systems, Oracle systems, what a large number of people are now colloquially referring to as like, multisig bridges. And those include things like layer zero and synapse, things like that. And while those are very easy to deploy to many different chains and they support arbitrary message passing, they're not really trust minimized. You're having to trust this external set of actors that is fundamentally going to have a different security model than the underlying blockchain.
00:03:02.070 - 00:03:48.990, Speaker A: Now, in the last few months we've been pushing this idea of optimistic bridges, which was first pioneered by Nomad. Optimistic bridges kind of take another path along this tradeoff space where they actually fulfill all three of these requirements. They are trust and minimized generalized accessible and they do this by relaying data across chains, optimistically using the similar model to orus where they post data to chain on the receiving chain. And then there's a certain timeout window, in this case 30 minutes within which people can prove fraud. Now, as you can tell, there is again another trade off here, which is latency. So while you're able to get some of these highly desirable properties, it now takes 30 minutes to do transactions across chain. Is there a way to beat these trade offs? We think that the best way to do that is through modularity.
00:03:48.990 - 00:05:07.438, Speaker A: So one thing that we are working on now with our new upgrade is layering kinext on top of Nomad so that we offer kind of the liquidity layer of the stack. Nomad offers the messaging layer of the stack and by doing this, it's sort of like a positive substitution where we can offset some of the trade offs of Connect and also some of the trade offs of Nomad. And the outcome is that we can actually end up with something that looks as close to ideal as possible while retaining the trust minimization properties of the underlying chain. So for any kind of transfer of funds or an unpermissioned call, by which I mean a call that goes across chains and doesn't actually need to check TX origin, that can happen with two minutes of latency, which is the normal time that it takes for Connects to make a transaction. And then for permissioned calls where you are checking TX origin, that takes 30 minutes of latency, which is the standard Nomad time. Generally what we've seen is that this is actually okay because in the vast majority of cases, user facing interactions are typically unpermissioned because you don't normally want your broad set of users to be able to call a Mint function on your token or something like that because that's obviously going to be a security vulnerability. And so generally we found that this works well because user facing interactions can happen quickly with good user experience and then the slower interactions happen with a higher degree of security.
00:05:07.438 - 00:05:58.506, Speaker A: Or I guess they happen with a higher degree of latency, but that's necessary because they need a higher degree of security anyway. All right, jumping into crosschain applications, one of the ways that we like to think about what Connect is actually enabling and what this stack enables is that we are pushing Solidity towards becoming asynchronous. So until now, everybody has built Solidity applications in a synchronous environment. Everything happens within a single block. It's sort of like the AP computer science version of development where you are only building programs that run locally on your machine that don't ever interact with the remote resources at all. But as we know from building web applications and for anybody that's written anything in TypeScript, the reality of the internet is that it doesn't really work that way. You have remote resources, you don't really know when you're going to get a response from those resources.
00:05:58.506 - 00:06:56.494, Speaker A: And so you have to think about asynchronous. In short, you have to start thinking about distributed systems. What connects enables is making these kinds of asynchronous calls across chains. So we have a function, the core kind of flow for making this kind of call is a function called Xcall which maps to the lower level solidity call function. And in the same way that you make a call to a contract on the same chain as you with call data, you would make an X call to a contract on a different chain from you with call data and a destination domain or basically chain address. And what's really interesting about this is that you can actually, again similar to things like JavaScript, receive a callback and execute that callback back on the origin chain. So you can go and execute some transaction somewhere else, get data from executing that transaction, bring it back home and then do something else with that data.
00:06:56.494 - 00:08:14.778, Speaker A: Now of course there is latency involved in this so you have to think carefully about what that latency means. But this is a really powerful primitive and there's a lot of really interesting types of protocols that can be built using this mechanism where you're able to access resources in different locations at different times. Now, one of the biggest questions that comes up during this is like what does it look like to actually do this? How do you charge fees? How do you actually make this interaction happen? And what we've tried to do is just mimic the existing flow for building applications as much as possible. So we charge gas fees on the origin chain they're charged in the native asset that you're already spending on the origin chain to make a transaction. And similar to how you handle gas fees with every single chain today the gas cost of execution on the receiving chain of course is variable, but will get executed if you have enough gas to pay for it. And if you don't have enough gas to pay for it, you can bump gas in again the exact same way that you do today. We think that this actually represents the ideal user experience because the development process for building these cross chain applications then just becomes figure out how much it's going to cost to execute a transaction on a receiving chain and just come up with your best guess estimate for this.
00:08:14.778 - 00:08:59.702, Speaker A: Send a transaction on the sending chain passing in the gas and then if it isn't enough funds to make that transaction happen within the time period you want it to bump gas in order to make it happen faster. Pretty much exactly what you would do to build an application today. What does it look like in practice? In practice we've tried to, as I mentioned, map to the existing lower level call interface as much as possible. Now there's obviously limits to this because we're limited by what Solidity can accomplish. And I think in the long run we can work towards doing more interesting mechanisms like perhaps even building in promises into Solidity so that you don't have to deal with some of the annoying things associated with callbacks. But overall it's still a very simple flow. The entire entry point of this flow is just a simple xcall function.
00:08:59.702 - 00:10:03.166, Speaker A: And the idea is that you can xcall things and get return data from x calling things and use this as a core primitive to now start building much more, many more complex MultiChain or cross chain systems. So now we kind of get into what can you actually build with this primitive and this is where things get interesting. So there is an entire domain space of DeFi applications that exist right now. And the big problem associated with the multi chain DeFi world that we live in is that users have to think about what chain they're on and they have to actually go to external interfaces like bridges to be able to go and interact with an application that's running on another chain. And furthermore, you don't have this consolidated, liquidity, consolidated experience between these different applications. So if you use Aave on polygon today, it's actually a completely different application to use Aave on avalanche it looks the same, but you have to go and do a bunch of things separately from the Aave application to actually interact with Ave on avalanche. And that's just really bad user experience.
00:10:03.166 - 00:10:41.474, Speaker A: What we're really interested in is trying to figure out what ways we can basically trying to encourage people to build applications that are just chain agnostic. By default they live across chains. And similar to how you build applications on the web today, they access liquidity and resources from many chains all at once. Examples of these are cross chain DEXes. So allowing you to swap any asset to any other asset across chains, potentially with optimal pricing. And basically either depending on whether you want best pricing or whether you want to go to a specific chain, you can make this an option to the user. You can also build much more interesting yield aggregators.
00:10:41.474 - 00:11:30.066, Speaker A: So for example, being able to zap into a yield aggregator vault from any chain. So as a user you don't really need to think about where am I going to get the best opportunity? And then even beyond that aggregating yield within those vaults from any source on any other chain. Another really interesting one is lending. So being able to lend money into something like Aave, migrate Aave positions across any chains to be able to get the best rate that you can on any chains and do things like interest rate swaps and then do more complex stuff like borrow across chains based on liquidity that you've lent on one chain. There is so much here. There's a whole world of cross chain DeFi that is yet to be explored and if you have original ideas around this, we definitely want to help. Cool last thing that I really not really last thing this is definitely going to be the biggest chunk of this talk.
00:11:30.066 - 00:12:42.878, Speaker A: So what I really would like to talk about is cross chain application design. This is something that not a lot of people have really explored and I think that things start to get a little hairy when we deal with this Asynchronous world. And I want to caution that this is all like early thinking that is still evolving in real time. So as you go about designing your crosschain applications, definitely try to speak to us, reach out to us and that way we can help you think through these ideas and try to be cognizant of the fact that the Asynchronous environment that you're operating in now will have a bunch of pitfalls. What are these pitfalls? Well, one of the first pitfalls to think about when you're building a distributed system is concurrency. When you're building a distributed system outside of the space you have to consciously be aware of the possibility that some resource that you're accessing, for example if you're trying to update a database may need to be accessed from other systems at the same time. So what happens if you have conflicts within your database because you're trying to update two pieces of the same state at the same time from different origins and the same kind of problem exists in asynchronous validity as well.
00:12:42.878 - 00:13:53.320, Speaker A: And this is where things kind of get a little weird. So imagine for example, you have a DeFi application like a crosschain AMM where there is a pool of funds on a destination chain and that pool of funds needs to be accessed from several different origins concurrently. A really simple example of this is something like Synapse where you have a pool of funds sitting on optimism and then you have the ability for people to swap into that pool of funds from Arbitrum and Polygon and Avalanche. What happens in the case that multiple people try to access that pool of funds at the same time? What happens if you actually run out of funds in that pool? Well, then you have to introduce concurrency control and this is one of the things that you traditionally deal with in distributed systems outside of this space. And you can map a lot of the core strategies that you use to deal with concurrency over to this space quite well as well though, as we'll see not all of them are very effective. The first thing is segmentation. You can split up the state within your pool to make it so that only a part of your pool state can be accessed at any time by any origin.
00:13:53.320 - 00:14:37.010, Speaker A: And in this case, out of this single destination pool, you now have three pools. You basically have three destination pools, sub pools that are created which each map to origin one, two, and three. Now, the downside of this is, of course, that you have fragmented your liquidity at the destination and it kind of gets even worse because it's like every time you want to add a new origin, you have to refragment your liquidity further and further. And of course, that means lower pricing, that means more complexity. That means having to think about, oh, well, I have an origin that's just not really being used. Do I need to resegment that pool so then that way I can utilize more of that liquidity elsewhere. These end up being like weird operational questions that you normally don't ever have to deal with in an AMM.
00:14:37.010 - 00:15:43.330, Speaker A: Another option is something called serialization. So outside of the space, sometimes this is called optimistic locking. But the idea is globally order the transactions that are going into your destination pool. So if you have a transaction coming from origin one, two and three at the same time, ensure that these things are ordered within the destination pool based on when they're mined within, basically when miners ordered them within a block. Now, the weird thing about this is that it doesn't really handle the case with an optimistic lock outside of the space. What you usually do is if the state that you're accessing, the state update that you're trying to make on the state that you're accessing, is no longer valid, then you just kind of drop that update and you go back to the origin chain and say, hey, let's just retry this. In this case, that doesn't exactly work because what happens is you end up in this sort of weird place where, say, for example, you've run out of funds in the destination pool entirely.
00:15:43.330 - 00:16:36.374, Speaker A: You end up in this weird case where it's like somebody has paid a transaction on the origin chain to be able to make a transaction to the destination, and now their funds are just kind of stuck. They're just there and it's very unclear what happens. You could potentially make it possible for them to revert the transaction back to the origin chain and then pull their funds out and submit a new transaction. But that's just like horrible, horrible user experience, and it's really unclear what that even looks like in practice. A last option that I think has started to be explored is pessimistic locks. So instead of actually ordering transactions or locking them at the point of the transfer itself, you lock the pool before you start the transfer. This is something that you do with databases where you could set up like a mutex on some part of the database's state while you're executing something over here and accessing that database state repeatedly.
00:16:36.374 - 00:17:15.282, Speaker A: And then you release the mutex at the end of your process. So then that way other processes can now start accessing that state at the same time. There are downsides to this even outside of the space. So one of the big downsides is things like Deadlocks where you have two things now trying to access the same state at the same time. It becomes kind of weird. You're like, okay, well, which one of these things should happen first? Which one of these things should happen later? And that's actually even more weird when you're starting to deal with liquidity rather than at arbitrary states. So in this case, if you have a pessimistic lock, you're now locking sublocking this pool just in time for each transaction.
00:17:15.282 - 00:18:09.240, Speaker A: But in order to do that, you actually need to have a lot more messaging overhead. You need to have a message that goes back and forth from Origin One to the pool to lock a part of the pool, then the actual transaction and then another message to unlock the pool that's, you know, in in a database environment, that's not that bad because you can send these messages within milliseconds if that. But on chain, that latency can add up pretty quickly. Like if you're using something like nomad, that can eventually be like a few hours for a transaction which would be kind of ludicrous. But even otherwise, even if you have very low latency in all of these cases, these are on chain transactions. So if nothing else, you have an incredibly high cost. And beyond that, you also kind of get into some of the weird aspects of like, okay, well, how would you handle a deadlock in this kind of a scheme? And it's not really clear.
00:18:09.240 - 00:19:14.830, Speaker A: You would end up in a situation where if Origin One and Two were interacting with this pool at the same time and Origin Three tried to make a transaction and got deadlocked, then Origin Three would just kind of be stuck until the others figured it out. And whether or not his transaction would go through would again depend on whether or not there was enough liquidity in the pool. The kind of conclusion of this is that for pools and for this specific use case that we're talking about here none of these options are really that great. It kind of sucks. We have a lot of trade offs associated with building on top of blockchains and one of the big trade offs is that they're expensive and slow. And this is especially magnified when you're trying to deal with something that's as complex as, like, concurrency. And so while it's true that you could potentially use these sorts of strategies for concurrency control on non DeFi applications, so on applications that aren't necessarily utilizing liquidity, you are probably going to run into issues with pricing and complexity if you try to use them within DeFi or within cross chain DeFi.
00:19:14.830 - 00:20:27.460, Speaker A: Another really weird thing is synchrony. And this is another kind of assumption that ends up getting made in the space quite a bit when people think about building cross chain applications. So by synchrony I mean the fact that there is asynchrony across chains means that it isn't possible for any one chain to know the state of another chain at the exact time that it knows its own state. A good example of this is say you have again like our straw man, maybe not straw man, our quintessential example of a cross chain AMM and you have a pool of funds on the origin chain made up of asset X and a pool of funds on the destination chain made up of asset Y. In order to calculate the price that you swap into asset Y on on the destination chain, you need to know the state of the pool at the time when the swap occurs. So say when you initiate this swap at T zero, you know that the state of the pool is S zero and this is what gets transported across chains. By the time it gets across chains, the pool state of X is actually changed to S one.
00:20:27.460 - 00:21:11.010, Speaker A: And the reason for this is that of course there are people making transactions going in the other direction. The pool is changing constantly in size. However, at the destination chain this is actually not known. At the destination chain, the pool state at T one is still S zero. And it gets kind of weird because it's like one of the fundamental assumptions of AMMS is that you have a constant product, right? As we know the K and X Y equals K stands for constant. But if you have Asynchrony and you're making the synchrony assumption, then you end up with a curve that doesn't actually look like an Am at all, AMM at all. It just looks like something that is not going to be extremely helpful for pricing because now your constant is only sometimes constant.
00:21:11.010 - 00:21:54.110, Speaker A: How do you synchronize pricing? Well, this one's actually even more difficult. You could use an oracle to have access to the pool state of all pools at all times. You can try to use a decentralized oracle for this, but then once again you're introducing more asynchrony and potentially more assumptions there. If you use a centralized oracle, then now you've just built coinbase, so that's not very helpful. Another option is to actually just have the pricing itself just happen entirely off chain. And this is something that's been explored quite a bit in the past. This is originally how zero X worked and a bunch of other projects have tried to try to build off chain order books that are decentralized.
00:21:54.110 - 00:22:52.580, Speaker A: But now you have some of the same trade offs that you did at that time. Which are that off chain order books, they're just really complex and you have to think about what it means for this off chain order book to achieve consensus and things like that. Once again, none of these options are really that great. Synchronizing things across chains and especially synchronizing pricing is just like a really weird rabbit hole that is difficult to go down and you end up in sort of really unpleasant places. The kind of takeaway from this is that building DeFi's apps is going to be tough and it's not just like building distributed systems outside of space. And the reason for this is that messaging overhead on blockchains and between blockchains is costly. It's not like messaging overhead when you're going between two databases where you can have many many millions of database updates happening per second if depending on how optimized your database is.
00:22:52.580 - 00:23:50.170, Speaker A: Instead you have one update happening every so often and it costs a ton of gas. So you really want to make sure that whatever construction you come up with minimizes the amount of messaging that you're doing between chains to begin with. Secondly, state update ordering actually matters in a database. It only matters sometimes. And in many cases you're able to kind of get away with using other mechanisms like things like Kafka basically queuing mechanisms to be able to handle ordering better and insert things into a database in the correct way. But that doesn't really work as well in this case because again, gas overhead and when you don't get ordering correct or when it is possible to manipulate ordering to change pricing, then you get MEB. Is there a way out of this? I think over time we'll come up with better ways to handle concurrency across chains and to handle some of the design patterns around Asynchronous.
00:23:50.170 - 00:24:58.390, Speaker A: And one of the things that will actually be the impetus for this will be a massive reduction in cost of operating on these chains to begin with. Because really a lot of the bridge costs come from the cost of the underlying chain or roll ups. As we drive down the cost of roll ups very significantly by moving to East Two and things like that, we will end up in situations where you will have a bit more flexibility on the messaging overhead that you can have and probably better mechanisms around ordering as well. However, for now, our recommendation is to try to build replicated instances instead of fragmented ones. By this we mean instead of building a crosschain AMM that has pools on many different chains all at once or a cross chain lending protocol that has borrowing on one chain, lending on another, completely isolated from one another, and then trying to synchronize those states against each other instead actually have replicated AMM. So you have an AMM on chain. A good example of this is something like Sushi, right? You have Sushi deployed on every single chain and then what you do is you make it possible for users to be able to make transactions directly between these AMMS so that it goes swap to transfer to swap.
00:24:58.390 - 00:25:37.266, Speaker A: All of that can happen in a single transaction and then you allow for rebalancing the pricing between these chains using very cheap and efficient Arbitrage. The mental model here is similar to something like IPFS. You have a distributed system that has a bunch of replicated states that achieves eventual consistency. The nice thing about eventual consistency is that it's very, very low overhead. It's driven by market forces. In this case, there's very strong incentives to actually achieve that consistency in the form of Arbitrage. And you get to kind of mitigate a lot of these really weird situations that you end up at when you deal with state itself being fragmented on many, many different chains.
00:25:37.266 - 00:26:30.458, Speaker A: Now of course this is early. There are a lot of really interesting use cases around protocols and things like that. That actually where it does make sense, especially if you're trying to just go and grab some arbitrary state somewhere else that isn't liquidity. It does actually make sense to still try to fragment the state or try to use mechanisms like concurrency control to update that state. A good example is something like PCV in Faye protocol where PCV is used. The protocol's PCV is used as PCV constants are used as a mechanism to decide how much money can be minted against the protocol treasury. And when you are minting against protocol treasury on other chains, it is possible to just update that PCV value on other chains rather than having the liquidity move to other chains as well.
00:26:30.458 - 00:27:28.586, Speaker A: So there are kind of like optimization functions written around this, but generally speaking, I would hesitate to do not try to avoid situations where you have user facing pools and other kinds of user facing interactions that are fragmented on many, many different chains all at once. This is obviously like initial steps into building crosschain DeFi applications. There's still a lot to be learned here and we're pretty excited about the direction a lot of stuff is going. So if you're interested in talking to us about it, definitely message us on our discord. We have a very awesome community that talks a lot about this stuff and we overall are trying to figure out ways to pioneer some of these standards around asynchronous liability. You can also follow our Twitter for updates and if you're interested in building on top of us, check out our docs. We have our new upgrade live on testnet right now and it should be fully possible to build things on top of and then it should be going live to mainnet within the next month or so.
00:27:28.586 - 00:27:57.060, Speaker A: And lastly, if you do build on top of us, and if you're not in the US, apply to our contributor program. We're running a program where people who build projects and otherwise contribute to kinext. Can earn tokens once our token launch happens in again, about a month or so. And we think of this as an incentive mechanism to get some of these early applications, early examples of what it looks like to build cross chain DeFi out in the wild. Cool. Thank you, everybody. I appreciate your time.
