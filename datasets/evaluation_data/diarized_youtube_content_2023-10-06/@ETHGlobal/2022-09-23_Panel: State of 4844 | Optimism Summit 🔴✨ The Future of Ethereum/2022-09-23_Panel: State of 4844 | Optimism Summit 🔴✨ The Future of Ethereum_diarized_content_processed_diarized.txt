00:00:06.490 - 00:00:30.440, Speaker A: All right. Hey, everyone. Welcome back. So up next, we have a panel on the SATA four eight four, and we have a really awesome team of experts here to talk to you guys about it. So we've got the legendary Tim Bayeko, terrence Mophie, and Robert Bayardo. Welcome, guys. Thanks so much for being here.
00:00:30.810 - 00:00:32.120, Speaker B: Thanks for having us.
00:00:37.690 - 00:01:04.980, Speaker C: Cool. I guess to kick this off, maybe we could just take a couple of minutes doing quick intros, especially because four four four has been interesting in that it's mostly been spearheaded not by client teams so far. Obviously, you have Terrence here who's on the client team, but yeah. Do you all want to take just a minute or two and kind of share who you are and how you got working on four eight four?
00:01:07.350 - 00:01:23.282, Speaker B: Absolutely. I'm happy to go first. So my name is Roberto Bayardo. I work at Coinbase. I've been there for about a year now, and I'm not on a client team. Therefore, I was not as overwhelmed by the merge, I think, as everyone else. And we have deep interest in scaling ethereum.
00:01:23.282 - 00:01:34.540, Speaker B: And I was given the opportunity to participate in this for a bit and so jumped at the opportunity and hope to keep getting more involved as things go on.
00:01:38.830 - 00:01:40.342, Speaker C: Mophie, you want to go?
00:01:40.496 - 00:02:13.960, Speaker D: Yeah, go next. Hey, I'm mophie tywell. I work for Op Labs, which is building the optimism roll up. And I started working on EIP four four Four, sort of, like, out of necessity way back, like, early summer, because it was critical for optimism to have something like this to keep roll up costs pretty low. And, yeah, pretty excited, the opportunity to get this out the door and get it done.
00:02:15.850 - 00:02:18.838, Speaker C: Cool. Terrence, hello.
00:02:19.004 - 00:02:19.334, Speaker B: Yeah.
00:02:19.372 - 00:02:58.022, Speaker E: So, hi. I'm Terrence from Prismatic Labs. I am mainly a core dev from the consensus layer, so I've been working around this space since 2018. I started working on Proof of Stake now it's known as the Beacon Chain. And last year I've been working on the merge and also a little bit of MVB stuff here and there. And I've been interested in 4844, actually, since ETH Denver this year, February, and I hacked a little something with protolanda, so that was fun. But, yeah, I've been mostly watching from the side for the last few months because of the merge was slightly more important.
00:02:58.022 - 00:03:05.670, Speaker E: But, yeah, I'm actually really excited to start picking this up again, and hopefully we can get this into Shanghai.
00:03:06.890 - 00:03:07.302, Speaker C: Nice.
00:03:07.356 - 00:03:18.140, Speaker B: Yeah, I think it's worth adding that I think when we say we're interested in the IP 44 four, I mean, we're interested in ETH scaling. Right. And this is kind of the natural first step to getting where we want to go.
00:03:18.910 - 00:03:54.710, Speaker C: Yeah, absolutely. And Terrence, you mentioned this first got prototyped around E denver and maybe do you want to actually take a minute or two and walk through how this relates to Dank Sharding for people who have no context because we have this plan to scale ethereum massively like Roberto was saying, and 4844 is kind of one of the first steps towards there. So do you maybe want to just take a minute or two and tell us what point 44 is and how it relates to full scaling on Ethereum?
00:03:55.610 - 00:04:35.274, Speaker E: Sure, yeah. Happy to. So how do we scale? Right? So right now what we're doing with Bitcoin and Ethereum is just every node download every single data and to verify every data. And that's obviously now going to scale. Right. And without this Rob central roadmap, essentially, with our future roadmap, it sounds more like that we will essentially put the data on chain and then run the execution off chain, basically. But in the event of fraud, the execution needs some sort of data to basically prove that where the fraud exists.
00:04:35.274 - 00:05:12.640, Speaker E: Right. Usually how it works is that you will have sharding basically means that not every node has to download every single data. But with Sharding becomes data availability sampling, which may take a few years to resolve. And four a four four is essentially the stepping stone. Basically say, hey, we will essentially black box the data availability sampling aspect and just make every node download the data as well. But it's a nice future stepping stone for the future.
00:05:14.150 - 00:05:58.810, Speaker C: Right. So it's almost like today all the nodes download and execute everything. With roll ups, you can have the nodes only download the data, but not execute every single transaction that happens on the roll ups. Eventually we'd like to get to a spot where the nodes don't even have to download all of the data that's gossiped on the network. They can only kind of subsample part of it and know that it was correct. And four four four is saying we're going to add more data capacity for these roll ups but still get everyone to download everything. Is that roughly right? And like you're saying, this kind of came about earlier this year and got hacked on at Denver.
00:05:58.810 - 00:06:23.720, Speaker C: And after that Mophie, you were like one of the people to actually start working on, okay, what does the proper prototype look for this? What do we need to do to get this in? Geth and Prism first and get all the kinks ironed out. So can you kind of walk through what you've been up to over the past few months? What the implementations were like, where things are at now?
00:06:24.730 - 00:07:02.622, Speaker D: Yeah, certainly. It's been a wild ride past couple of months. Getting into EIP four four was like something, I wasn't even aware of it until around April, even though it was worked on East Denver. But my first thinking was, okay, we really need this for optimism. Like gas costs are out of control, right. And the rate of user growth that we're going, we really need this soon as well. So I basically roll up my sleeves and start working on an implementation.
00:07:02.622 - 00:07:54.820, Speaker D: At first, it was basically looked at already existing work, like what Terrence mentioned he and Proto already had, I guess like a proto proof of concept of EIP four four. There are some things that are missing. First step was basically to take a look at that existing code and extend it, basically. So what I did first was complete the implementation in the execution client. The one that already existed was Geth. So all I had to do was continue working on Geth and have that done, and the next thing was to have a consensus client that also incorporates EIP four four four. Even though it's not quite evident, if you look at the spec, there are a lot of consensus level changes to.
00:07:54.820 - 00:08:51.670, Speaker D: And so I basically looked at Prism because it's written in Go, and I'm pretty familiar with Go lately. So that was like my client of choice for this, but it really could have been any client. So I looked at a Prism basically started implementing what's needed to get EIP 44 working. And yeah, that's pretty much mostly what I've been doing for the past couple of months. And then there was a point where once both the Geth and Prism implementations were good enough and implemented most of the spec, it made sense to start integrating things. And that's what gave birth to the first DevNet. So the DevNet is basically a small network that's running the EIP 44 four hard fork.
00:08:51.670 - 00:09:13.280, Speaker D: It implements both the consensus and execution level changes to the spec. And it's kind of like our playground we're using to test out how would the spec behave in certain conditions and what sort of transactions can we send to it and testing the threshold of the spec itself.
00:09:15.570 - 00:09:15.994, Speaker B: Sweet.
00:09:16.042 - 00:09:20.718, Speaker C: Yeah, that was a great overview. Roberto, Terrence, anything you guys want to add on that?
00:09:20.884 - 00:09:51.434, Speaker E: I just want to give a shout out to Mophie and Roberto and team. It's like having a Deafnet now. It's a pretty amazing accomplishment, right? Just because we even had the Deafnet before the merge and we had this Deafnet before withdraw and everything. And it's just remarkable. And I always find it impressive and possible that people go into other people's code base and then able to make advancements such that so all the props and all the props go to them.
00:09:51.632 - 00:10:36.294, Speaker B: Yeah, I was going to give props to Mophie specifically as well. I mean, he's really pushed this along. Consistently been an amazing help in providing guidance to those of us jumping in New completely cold and being able to get reasonably productive. Anyway, I also wanted a shout out to Michael De Hoogue, who did some work on this from the coinbase side before I did. So I'm a relative newcomer here. I did some work predominantly in the consensus layer side, implemented the new style Blob verification for so on, for example, and continuing building from there. Hopefully we're going to get the next version of the DevNet up soon, where we have the latest spec changes, for the most part implemented at least.
00:10:36.294 - 00:10:41.322, Speaker B: Still remains a little bit of a moving target, but it's starting to settle down nice.
00:10:41.376 - 00:11:02.414, Speaker C: And yeah, it is, I believe, the first time that Coinbase gets involved in working on a protocol change, at least that I recall. I'm curious, why does Coinbase want to do this? What is the rationale to put engineers on this instead of the thousand other things I suspect we all have to do?
00:11:02.612 - 00:11:57.826, Speaker B: Yeah, it's a great question. And as you know, Tim I've been talking with Tim since I've started at Coinbase trying to figure out how we can contribute. And there are a lot of competing efforts for time for a Coinbase employee. But ultimately, I think we realize that we want to raise all boats, we want to build out the ecosystem that's not just purely charitable. We view Coinbase as not just trying to be kind of the most easy to use and secure exchange and custodian for crypto out there, but we view it as a broader onboarding mechanism to crypto as well as all of Web Three. So if you think about it that way and look at the kind of number of users we're dealing with, if we took all the roughly 100 million Ish users of Coinbase and tried to dump them on chain, it wouldn't work out so well right now. And we want to do that.
00:11:57.826 - 00:12:21.938, Speaker B: Right. We want to start blending Coinbase's centralized experiences with more decentralized experiences. Again, provide this very straightforward, easily usable onboarding mechanism and so on. And so in order to get there, we need to scale the ecosystem. Right. And the best scaling roadmap out there, we believe, is the Ethereum scaling roadmap. So that's the natural place to know.
00:12:21.938 - 00:12:30.380, Speaker B: Mike got involved at first. I've joined in. We've got a couple other people we're trying to ramp up and yeah, just really delighted to be a part of this.
00:12:32.130 - 00:12:54.820, Speaker D: I can't emphasize enough how much effort Coinbase is putting into this. We have a very good relationship with Coinbase because we're all both in the same boat here. We really want Ethereum to scale, and we also anticipate the huge influx of users as we come in the next couple of years. So it just makes sense for the collaboration effort that we have going on.
00:12:56.230 - 00:13:26.270, Speaker C: Yeah. And maybe to bring it back to how we actually get this done. Terrence, you're on a client team, obviously, and all the client teams have been deep down in the merge for the past year. But some people have started to look over the edge and think about what comes next. What's your general read of how client teams perceive 4844 now? What do they see in terms of the complexity here, the risks.
00:13:29.570 - 00:13:30.094, Speaker B: If people.
00:13:30.132 - 00:13:34.558, Speaker C: Have had time to even look into it? But yeah, what's your general read on that front?
00:13:34.724 - 00:14:12.618, Speaker E: Right, so I can give my perspective from the consensus layer client. So my first impressions I really like it. I think it's elegant and I think it's a good stepping stone. And it's funny because if you look at ethereum history, right, we don't usually just dive into something, we make incremental progress. I think a good example of that is just like we ship proof of stake bitch, we didn't just merge, right? We shipped this proof of state bitch. We let it run in the wild for like one year and then we hard for to a tear. But we didn't merge there, right? Because we want to try to understand how to hard fork in this type of environment.
00:14:12.618 - 00:15:55.200, Speaker E: So we take our time, right? And another example is just hybrid PBS, right? We didn't fully ensure PBS first we have this hybrid PBS, we use medboost just because we don't want to commit into something and I think this is a very similar concept. Basically we are exploring ish right? I think my perception is that I think some of the teams also think that I think it's important to get it done, but we want to get it done as safely as possible. We don't want to trade off between quality and speed. For example, I think majority of my time will be spent on testing and then in terms of just implementation complexity, right? I do think it's a little complicated just because there is this new cryptography primitive such as KZG and most of the client teams unfortunately don't have as per cryptography, we have to work with someone else and unfortunately when you work with other dependencies it takes longer, it introduces more complexity. But the good thing is that we are using Bos in the current proof of state so we do have some experience working with the teams as well. And I think the consensus and the network changes are fairly easy just because we have done that before many times for the hard fork. And then I think majority of the fun part is just like client implementation detail, right? For example, we have this concept of optimistic thinking what can a client do when the data is not available? Right? I think using the merge as an example, right, there's probably like in Prism there's like 20,000 lines of diff but there's only 2000 lines of diff.
00:15:55.200 - 00:16:28.070, Speaker E: That's actually important, right? And then we spend the longest time just looking at those 2000 line of diff to make sure hey, this is good, this is good, this is good, this is good. So I will say definitely lots of time on testing and stuff. And I also know like Mario from Gap has hacked like an EIP 44 branch for Lighthouse at IP berdin it's super awesome. And Enrique from Taku has also been asking questions on that front. So definitely seeing a lot more progress right now. But overall I am very bullish.
00:16:29.710 - 00:17:15.494, Speaker C: And do you think introducing because we're basically introducing a new layer into Ethereum, right. With this EIP, we have like now we like to call it the consensus layer, execution layer. You can almost think of these blocks like a data layer that we're introducing. How does that change the client architecture or the testing infrastructure? Is this as big as the merge? Where the merge? We had no way to test the combination of execution and consensus layer clients and we had to spend months and months building all that infrastructure. Is adding a data layer similar or is it actually a bit simpler because it's still handled by validators on the beacon chain and we have a bunch of infrastructure there.
00:17:15.692 - 00:18:07.254, Speaker E: So yeah, it's a good question. If you ask me that, like a year ago, I probably will be more pessimistic. But if you ask me now, I think we are used to this concept of layering, right? For example, we have consensus layer, we have execution layer. And with all the MVB stuff, we have this builder layer. So everything has become more layerish. Every software implementation is very complicated, but they are as complicated as basically the API between them, right? So we have this concept of engine API where consensus layer and execution layer basically expose certain things to each other and that's all they have to understand. And similar with this data layer, whatever we're calling, right, we will essentially expose this API, such as getting Blob and stuff like that.
00:18:07.254 - 00:18:27.358, Speaker E: So, yeah, I will say the complexity is pretty encapsulated. So that's nice. And we have also done this for the merge, so we are quite experienced at this already. And then it's just testing, which we have been testing for the last eight months. Right. There's great. People like pairing and everything.
00:18:27.358 - 00:18:31.950, Speaker E: We know how to test this with different client combos.
00:18:32.530 - 00:18:56.374, Speaker B: Yeah, I just like to jump in and out. I think this is way simpler than the merge. The protodang Sharding in particular. It's pretty strongly coupled with a consensus layer at this point. It's not like it's adding a completely new independent layer. So I think it really hits a really great balance of taking a step towards that independent data layer without having to go all the way there. So I actually view this as a relatively low risk change.
00:18:56.374 - 00:19:05.100, Speaker B: Again, being somewhat young, new to this project, and probably naive, but compared to what I saw going on with Emerge, this to me seems like almost a piece of.
00:19:08.110 - 00:19:08.426, Speaker D: Know.
00:19:08.448 - 00:19:25.730, Speaker C: Like, we keep saying how this is what we'll use to scale Ethereum and how roll ups will use it. Mophie, do you want to take maybe like a couple of minutes to walk through? Why is this actually better for rollups? Like, how does a roll up use prototype charting to provide lower fees to their users?
00:19:27.270 - 00:20:39.302, Speaker D: Yeah, certainly. So right now, roll ups basically have to, after doing sending transactions and doing everything in L two, we need to post that data back to l One, it makes it easy for a new node that's joining the network to derive the chain once that data is available. But posting that data is actually expensive right now, today, ethereum the main way we do it? Well, the only way I think most roll ups I know does it is by using call data. When we create a transaction on L One, all the transactions and state routes and commitments, we roll that up, hence roll up into a transaction on L One, and that's posted via the call data. But the call data is quite expensive. Over the past couple of months, a couple of roll ups, including optimism, have implemented a couple of techniques to reduce the size of that call data, like compression. But overall, it's still not sufficient, especially with the rate of growth we've seen at L Two zone.
00:20:39.302 - 00:21:44.378, Speaker D: It will eventually be more and more expensive. So what Blobs transactions do, which is what EIP 44 Four brings, is provide, like, an alternative source of that roll up data that we need, and also provide an alternative but also cheap way to post that data. And the way we're doing it with EIB four four is rather than just post it back to L One as call data, we would use, like, a special transaction type called the Blob transaction. And this kind of goes back to the data layer that Tim mentioned. So the data that we're posting back to L One kind of gets posted to the data layer, not L One in particular directly. And the economics of this makes it much more cheaper for roll ups to do, because Blob space data space is cheaper than just using the L One block space. So that's kind of like in a nut.
00:21:44.378 - 00:22:28.890, Speaker D: Okay, that's one part of how l Two S would be using roll ups. The second part is also in fault proofs. Right, as a roll up. Well, for optimistic roll ups in particular, we need to be able to easily dispute any invalid like state route that's been posted on L One. And with EIP Four four four, this also makes it easy. All we need to do is just change where we're getting the data used for that fraud dispute to use Blobs instead. What I like about this design is it's very easy for optimistic roll ups to just plug in what data source they're using for this.
00:22:28.890 - 00:22:36.266, Speaker D: It's either from the data layer or from l One. Call data. It doesn't matter. So, yeah, that's kind of how it works in a nutshell.
00:22:36.458 - 00:22:39.614, Speaker B: Nice. Sorry. Go ahead.
00:22:39.732 - 00:22:40.874, Speaker C: No, please, Roberto.
00:22:40.922 - 00:23:27.840, Speaker B: I was just going to add right, so then when you can start extending the data layer to implementing the sophisticated erasure encoding rather than keeping it with every single consensus layer. But I think one of the things to add for the viewers that aren't intimately familiar with a spec is the Blob data doesn't need to be kept around indefinitely by the consensus layer. It can be thrown out. And I think the discussions I've heard so far is probably after a month or so. Right. Because I think the main need for that data is for these fraud proofs or availability proofs. And so after the dispute period has elapsed, there's no reason for a standard no to keep that data, which is one of the reasons it can be priced cheaper, for example.
00:23:28.850 - 00:23:29.600, Speaker D: Exactly.
00:23:32.850 - 00:24:08.460, Speaker C: Nice. Okay, so we have all this L Two S are going to use it, it's going to be great. It's going to lower their fees. What's left to do to ship this? First of all, this hasn't been accepted by client teams as part of the next network upgrade yet because we frankly haven't had time to discuss it in much detail because of the merge. But from a purely technical perspective, right. To bring this to a spot where it would be safe to deploy on mainnet, what are the big things that are missing and that you're all kind of looking into?
00:24:10.050 - 00:24:59.942, Speaker E: I can go first, then maybe people can fill in. From my perspective, I do want to start merging the changes into Prism, which means currently the code is more for DevNet, but it's not so much for production readiness quality, so it will be hard to merge it. So I would definitely want to clean up whatever we have today, merge it with upstream, and at the same time, I do want to think with the deaf net a little bit, play with the deaf net, try to break it just to make sure it's robust, it is resilient. And I do think it's very important to write more. Right. Because right now I type EIP four four on Google and nothing really show up. So I do think we need more nice resource material such so that to bring more attention to the community.
00:24:59.942 - 00:25:02.960, Speaker E: And I think that's like a great way to push forward.
00:25:05.730 - 00:25:41.690, Speaker B: Yeah, I think the other thing is just more client implementations. Right. So we've got Gas, we've got Prism, they are preliminary. And Terrence is absolutely right. There's a lot more that needs to be done to productionize that code to test that code, to prove to everyone out there that if we launch this on the main net, things won't explode. Right. So the other thing I think we really need to do is we need to set up an official test net that I think has EIP 4844 Active, where the developers of L two S can actually start creating their next version that takes advantage of it and seeing what benefits it provides.
00:25:43.230 - 00:26:38.106, Speaker D: Yeah. And to add to that, I think we also need, like, a proof of concept use case of the IP four four four for a roll up. Because maybe I'm a bit salty about access lists, but we don't want to end up in a case where we deploy EIP and people start using it and realize that, oh, the gains are not that much that we expected. We want to show that this thing actually works. It's a valid use case. We would use it once it's available, and it'll be a huge boon for the roll up teams. So I think demonstrating this in particular by bedrock, we're actually working with Coinbase to integrate bedrock with the bedrock is like the optimism upgrade that's currently in development, and we want to integrate it with EIP four four four.
00:26:38.106 - 00:26:52.814, Speaker D: And then tell the client teams that, hey, this thing actually works. You can see how it works. It's not just vaporware or something. That'll be really, I think, a good way to convince client teams to accept it.
00:26:53.012 - 00:27:06.130, Speaker B: Absolutely. Yeah. I mean, the back of the envelope math looks great. We expect it to help scaling dramatically, but proof is in the pudding. Once we have code a usable system, I think it'll be a lot more convincing.
00:27:07.350 - 00:27:07.762, Speaker C: Right?
00:27:07.816 - 00:27:08.130, Speaker E: Yeah.
00:27:08.200 - 00:28:01.670, Speaker C: And I'll plus one mophie, to your point where it's like we've had features in the past that we've deployed on Ethereum where the usage didn't end up being what we expected. So access lists were once there was once another pre compile for interrupt with Zcash that I think Zcash changed the curve a few months after, so didn't end up working great. So seeing something like a cutting edge roll up implementation that's working on a prototype of 4844, that'd be really valuable. And one thing that four four also introduces that's kind of new to Ethereum is basically this idea of KZG commitments, which require a trusted setup. So it's like a new cryptography primitive that we're adding to the network.
00:28:02.490 - 00:28:03.606, Speaker B: Mophi, do you want to kind of.
00:28:03.628 - 00:28:13.260, Speaker C: Walk us through what that is at a high level? Like, why do we need to add new cryptography to make this? Yeah. And just your general view there.
00:28:14.430 - 00:28:57.198, Speaker D: Yeah. So the data that we're posting to the is where we've already dubbed the data layer that roll ups will be using. We need a way to be able to prove that a particular piece of that data. We need to be able to prove what that particular piece of the data means. And this is really useful, for example, in fraud proofs, where rather than sending the entire data set to your fraud proof, you would just send little bits of it. Right. And you can convince the fraud prover that, hey, this data matches the little piece that you've posted.
00:28:57.198 - 00:29:32.420, Speaker D: So what KCG lets us do is basically it's like a commitment similar to Mercury proofs, but it's different from Mercury proofs in the sense that it's easier for you to prove a particular point with the same proof size, and also it's very easy for you to extend it. This goes back to what Roberta was saying about full data availability and erasure codes. So with KCG, it's very like.
00:29:34.970 - 00:29:35.286, Speaker B: You.
00:29:35.308 - 00:30:24.820, Speaker D: Can extend your data with the KCG and then easily make it amenable for data availability. So that kind of gets over the overview of why we need KCG. But it's like new cryptography, like what Tim says that we're introducing to the consensus spec client. And there's some unknown, unknowns a little bit that we're still trying to figure out in particular, like, what's the most efficient way of verifying these KCGS? And it's still like an open research question there, but I think as more and more people are getting into the spec and taking a look at it, we'll be able to develop better solutions there.
00:30:25.270 - 00:31:07.010, Speaker B: Yeah, I think it's also worth adding that while it is cutting edge crypto, it's not quite as cutting edge as some of the stuff that's going on in the Zke EVM world. Right. It's a fairly contained, I wouldn't dare say well understood, but I don't think it's like we're really inventing brand new stuff here. I'm not a crypto expert, by the way, but I was able to follow the spec fairly easily and go and implement it with the benefit of the proto Lambda work and go KZG libraries and so on. So, yeah, I guess I don't want people to be too frightened by that work that's going in there because I haven't found it terribly intimidating again, as a non crypto expert.
00:31:07.590 - 00:31:22.280, Speaker D: Yeah, true. The math like KCG is based on is pretty old math, like 80s math. It's just recently we're starting to actually implement it in a production ish system and that's where we're trying to figure things out.
00:31:22.730 - 00:31:33.770, Speaker B: Yeah, and Mophie in particular has been doing lots of work on optimizations and benchmarking and things like that, so that's another part of the project and work remains to be done there, making it go faster.
00:31:34.110 - 00:31:50.750, Speaker E: I guess a fun question is do you foresee that we just have one crypto library for KZG that all the clients can use? Or do you foresee there will be multiple efforts that every client team will build their own KZG library?
00:31:51.890 - 00:32:25.420, Speaker D: I think there will be one or two in particular with Go. We're thinking of using one particular library that is written in C, but there are some particular things with Go and Interrupt with C code that might prevent us from eventually using that. But I could definitely see other client teams that perhaps using Rust would probably use that as well. And yeah, it's pretty much mostly two implementations at the most. I think.
00:32:27.310 - 00:32:58.260, Speaker C: Two feels like it would be a sweet spot. One has been a contentious topic in the past for implementing stuff on Ethereum because we do have this multi client architecture and the whole purpose is that if there's a bug, then not all the clients are affected. But then if you have the central point where everybody's using the same library in the background for the purposes of those operations, you're back to like a single implementation. So hopefully we get. To see two high quality ones emerge over time.
00:32:59.430 - 00:33:13.580, Speaker B: Yeah, it's been interesting. We've been trying to sort of figure out where to set that bar, where the appropriate abstraction layer is, where we start sharing code. Right. You want to set that as low as possible, but without requiring people to implement their own very low level bitwilling to get the crypto right.
00:33:13.950 - 00:33:49.110, Speaker C: Yeah, correct. And I guess to kind of go back up a little bit. We talked a lot about 4844 itself and where we're at there. Terrence, do you want to walk us through? If you look at the whole roadmap towards full sharding, what percentage of it is done with 4844? What are the boxes that we check off? And then what's the stuff that we would need to do over the coming years to get kind of better scale on ethereum?
00:33:49.450 - 00:34:16.158, Speaker E: Right, I can list a few points. So what four a four four includes right. What four a four four includes essentially this new transaction type. It's going to be exactly the same format that we'll be using in full sharding. And most of the execution layer logic required for full sharding is also going to be there. And most of the execution and consensus layer cross verification checking logic will also be there. Right.
00:34:16.158 - 00:34:30.798, Speaker E: So those three are essentially getting piggybacked by four a four four. And like you guys said, the layer of separation between this data layer and the consensus layer is also there as well. The concept of sampling blob.
00:34:30.894 - 00:34:31.154, Speaker C: Right?
00:34:31.192 - 00:35:08.234, Speaker E: Now, essentially you have one beacon node that samples everything, but with full sharding, you have this committee of validators and beacon nodes sample portions of it, and that's the power of sharding. Right. And I think the guest pricing is going to be highly similar as well. So that's very nice. Right, so maybe we can talk about what full sharding has that four doesn't have. Right. Full sharding have this low degree extension for the blob, basically allow 2D sampling, and then it has the actual implementation of the data value sampling.
00:35:08.234 - 00:35:30.520, Speaker E: It has builder and proposal separation because you don't want proposer to essentially sample 32 megabytes of blob. That's just a lot. Right. And hopefully some proof of custody stuff as well to ensure there's no lazy validator. They just pretend to vote on everything. So, yeah, feel free to add on guys, if I miss anything.
00:35:32.090 - 00:36:05.460, Speaker B: Yeah, I mean, I think it's to dumb it down a little bit because it's pretty complicated stuff for someone who's just coming to this. But basically 4844, every node is still downloading every blob, still storing every blob, with the exception of what I mentioned earlier, where they can delete it. The full sharding roadmap. Once we start doing this erasure coding, you have these committees that can be consulted to reconstruct the data. And so no longer does every node have to store every single thing. So that's why it's called full sharding rather than protoding Sharding, I guess.
00:36:06.470 - 00:36:31.322, Speaker C: Right. And I guess if you think of it in terms of cost right, right. Now, when you use Call data, you're storing data that is stored forever by every node, and so that's really expensive. Four four four gives us data that's stored temporarily by every node, so it's like cheaper. And then Full Dank Sharding gives us data that's stored temporarily by only a subset of nodes, and it can be even cheaper. Does that roughly make sense?
00:36:31.456 - 00:36:32.620, Speaker B: Bingo. Yeah.
00:36:34.030 - 00:37:03.710, Speaker D: Also a little thing that's a bit different with the I guess the security of assumptions of the data layer. With Proto Dank Sharding is you only need one node that's having the data available to be able to sync up. Whereas with Call data, you need all your peers to have that data available so you can be able to sync up the tip.
00:37:03.790 - 00:37:08.294, Speaker C: Right, yeah. So you only yeah.
00:37:08.332 - 00:37:21.130, Speaker D: I think that relaxed assumption makes things also somewhat contributes to why Blobs are cheaper in some sense, like indirectly.
00:37:22.670 - 00:38:00.790, Speaker C: Right, that makes sense. And I guess one final thing I wanted to touch on is there's been tons of community enthusiasm for 4844. There's been random people popping out wanting to help. And I'll give a quick shout out to Kane here from synthetics who's funded a lot of those to just come and help out with various things. If someone's listening to this and they want to get involved in 4844, what are the things that are most needed right now and what are the places that they should go or follow to kind of be informed of the latest developments?
00:38:04.570 - 00:38:07.560, Speaker B: Mophie, you want to take that? You've got the best landing page, I think.
00:38:08.890 - 00:38:57.160, Speaker D: Well, yeah, there's, of course, EIP four four Four. It contains links to several other resources that I think should help anyone get ramped up to speed with the spec. We also have a DevNet, but that's also another good way to contribute. Basically just running a node in the DevNet building, running a node, participating, creating contracts, sending transactions. There's a faucet available in the DevNet that makes it easy for you to fund yourself and do things with it. So that's where I think I would start. It makes you get familiar with the network before taking a deep dive into the spec.
00:38:58.510 - 00:39:10.678, Speaker B: Yeah, I was referring to Mophie's HackMD page, where as instructions for getting up and running with the DevNet, it's a great resource, points to us and so on.
00:39:10.864 - 00:39:15.790, Speaker D: Shout out to Gabby and the Etherd discord for setting up the faucet.
00:39:16.930 - 00:39:27.410, Speaker B: Oh, yeah, absolutely. And a bunch of us hang out in the Sharded data channel in the discord. That's another good place to interact with us, I guess.
00:39:27.480 - 00:39:52.730, Speaker E: Another one is just to write learning material guide and stuff. Just a lot of people don't know what 44 four is, and they're not probably not going to read EIP or consensus layer spec. Bunch of python code, right? They probably want to read something that's more like humanly readable. I guess so. Yeah, I guess more education material and more resources. Those will be lovely.
00:39:54.350 - 00:39:54.762, Speaker B: Yes.
00:39:54.816 - 00:40:03.118, Speaker C: And if people do those, please tweet at me or Mophie or Terrence and we'll add them. We'll link them on the 4844 website for sure.
00:40:03.284 - 00:40:23.380, Speaker B: Yeah. And if any testing experts out there, we have a lot of work to do there. For example, I think that's one area where it's an easier way to get started. Right. You have to understand the entire spec. You can pick a little piece of it and take a look at the code we've written and how poorly tested it is so far and then dive in from there.
00:40:29.110 - 00:40:41.338, Speaker C: Yeah, so testing is a big one. Cool. So if you have testing experience, please give us a shout on Twitter, emails on the website, and we'll find something for you to do.
00:40:41.424 - 00:41:00.560, Speaker B: Yeah, and optimization and benchmarking. Critically important here. We've mentioned it a few times already. This new crypto is pretty expensive. If you don't do it right, you introduce denial of service vectors. We want to make sure that doesn't happen. So that's why that area of work is also important and plenty more to do there.
00:41:04.130 - 00:41:17.830, Speaker C: Sweet. I think this is a good place to wrap up. So we've covered what the EIP is, why it's valuable, where we're at, what you can do if you want to get involved. Do any of you have any kind of closing thoughts you wanted to share with folks?
00:41:22.770 - 00:41:35.060, Speaker D: I guess to close, we really need roll ups to be cheap. If you want to send your NFTs and mint them really cheaply, this is the way to go.
00:41:36.790 - 00:41:57.740, Speaker B: I think my only closing thoughts is that this is something we'd really love to target for the Shanghai release. I personally believe it's not too ambitious of a change know, slip beyond that, but we'll do our best to make our case through stuff that works. Hopefully we'll get there.
00:41:58.750 - 00:42:23.470, Speaker E: My thoughts is, I think besides withdraw, scaling is probably the most important thing to work on post merge. Because like Danny said, we're sustainable and we have security. So scalability is next. And it's funny, when I first started working on this space back in 2018, I wanted to work on Sharding for the longest time. And then finally, now this is the time to work on Sharding.
00:42:25.890 - 00:42:34.600, Speaker C: I think that's a great place to wrap up. Thanks a lot guys, for coming. Yan, I appreciate you all taking the time. And thanksglobal for hosting us.
00:42:35.850 - 00:42:36.966, Speaker B: Thanks for having us.
00:42:37.068 - 00:42:37.734, Speaker E: Thank you.
00:42:37.852 - 00:42:46.530, Speaker A: Thank you guys so much. Yeah, thanks for all of the information. It was a really awesome panel. Appreciate you guys all being here. Thanks. Bye.
