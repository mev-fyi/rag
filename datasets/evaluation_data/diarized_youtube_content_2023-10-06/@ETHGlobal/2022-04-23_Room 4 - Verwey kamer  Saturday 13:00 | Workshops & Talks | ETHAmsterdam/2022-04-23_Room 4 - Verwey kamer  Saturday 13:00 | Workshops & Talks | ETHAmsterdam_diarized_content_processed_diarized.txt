00:01:15.480 - 00:04:44.670, Speaker A: Sam SA Samuel don't even know me. What I've seen, I've liked, and I got a good feeling about that. Only one. In your dreams. And WA sake. 1111 make a wish.
00:06:46.040 - 00:07:31.036, Speaker B: Hello, everyone. My name is Marius, and I'm going to talk about Go Ethereum today. I didn't have much time to prepare my slides, as you can probably see on the side. So I'm just going to wing it, and we're going to go a bit into the code. I'm going to talk a bit about what the different modules of Go Ethereum do. And then I thought we can just make a lot of time for questions and discussion and stuff like that. All right, I wanted to show you how a transaction travels throughout Go Ethereum.
00:07:31.036 - 00:08:20.416, Speaker B: And this is the rough overview. We submit a transaction via RPC and it gets inserted into the transaction pool. And from there, we have the ETH package, which handles a lot of so we have the ETH Protocol, which defines the network messages and stuff like this, and the way we interact with the network. And from there, we take the transactions and we use the peer package. We go into the peer package and send the transaction to another peer. This peer goes the same way back through the ETH package into the transaction pool. All right? I think it's most interesting to actually see it.
00:08:20.416 - 00:09:03.772, Speaker B: So when you send a transaction via MetaMask to your local node, then you will always send a raw transaction. You will sign the transaction not in your node, but you will send the signed transaction to Go Ethereum and it will end up here. And from there. This internal ETH API has a lot of these API methods that you're very familiar with. Like, ETH send transaction. ETH send raw transaction. ETH get block.
00:09:03.772 - 00:10:11.264, Speaker B: All of these are defined in internal ETH API. And if we follow this now, we submit the transaction. And what we do, we do some basic checks before inserting it into the transaction pool. And so we check that we have enough fees and that the fees don't exceed a certain amount, because we had a lot of people mixing up the value of the fee and the gas price. And so they were sending transactions with a really high gas price, and then they were complaining, why did I lose so much money? And so we have some extra checks there to make sure that this doesn't happen. And, yeah, then we go to Send Transaction. Send transaction is actually in the interface from the transaction pool.
00:10:11.264 - 00:10:39.180, Speaker B: So this is implemented by the transaction pool, this interface. And we end up Send transaction send TX something like this. No, not here. We end up in the API backend. Sorry. Yeah, send transaction is in the API backend. The API backend is something that we use for a lot of these APIs.
00:10:39.180 - 00:11:31.600, Speaker B: And it's a collection of methods that are useful to have for creating the RPC APIs on top of that. And one of them is Send transaction. Send Transaction just adds the transaction as a local transaction into the transaction pool. So if we follow this, we come to the transaction pool, add locals, add transactions. So the transaction pool, it has all of the transactions, of course. And we sort them, we sort them by the gas price. And we also have to with 1559, we have like a two dimensional gas price.
00:11:31.600 - 00:12:03.528, Speaker B: Sorry, a two dimensional gas price. And it's a bit more complicated how to sort the transactions. Because some of the transactions might be valid at a certain base fee. Some of the transactions are not valid at a certain base fee. So we have to continuously update which are the valid transactions. Because we built a pending block. The pending block is just the current block that you have.
00:12:03.528 - 00:12:58.860, Speaker B: And you pile all of the transactions on top so that you know roughly how the next block will look like. With the emergence of a lot of this mev stuff, this pending block is basically useless. And we are trying to get rid of it. The problem is that the pending block is used everywhere. And for example, if you create a transaction, you will need the nons, right? And if you already send a transaction to the transaction pool that is not confirmed yet, you will need to know that there's some transaction. So you can increase the nons once more in order not to replace the transaction. If you send the same transaction, if you send a different transaction with the same nons, you actually replace the transaction.
00:12:58.860 - 00:13:49.790, Speaker B: But only if certain conditions are met. And the conditions are the max base fee per gas has to be 10% more than from the transaction that you want to replace. And the max tip per gas also has to be 10% more. And there's some discussion about this if this is actually the right mechanism. But the problem is, if we don't have this, if we let someone just update the transaction without increasing the amount that they pay for it, then people can just spam. The transaction pool always resend the same nons. And we have a lot of churn in the transaction pool, which is bad.
00:13:49.790 - 00:14:49.596, Speaker B: Yeah. So here we're trying to add the transaction to the transaction pool. We extract the sender. So we do the EC recover on the signature for the sender address and we add it to the transaction pool. And here we look, if we already have the transaction in the transaction pool, we validate the transaction. And if it's underpriced, we discard it. And there's a lot of stuff going on here.
00:14:49.596 - 00:15:36.132, Speaker B: And in the end, we NQ the transaction. I wanted to go into detail about the transaction execution later on, but I can do that. No, I'm going to do it later on. So the basic idea is we're adding this transaction into the transaction pool and somehow the transaction pool has all of these transactions. Now we have to if we go back to the picture we started with RPC, we submitted the transaction into the transaction pool. Now we have to send this transaction to our peers. So we have in the ETH protocol, in the ETH packets.
00:15:36.132 - 00:16:21.328, Speaker B: So you can always see the packet that we're in up here, if you don't know. And in the ETH packet, we have the handler. And that handler has the sync transactions function. And whenever a new transaction, so it periodically asks the transaction pool for the pending transactions. Because of someone, we have different concepts in the transaction pool. Pending transactions are the transactions that are actually executable. We also have gapped transactions.
00:16:21.328 - 00:17:26.830, Speaker B: So if I send a transaction that has non 9000 and I didn't send 8999 transactions before that, then this transaction is gapped. It's non scapped because there could be some race conditions about the transaction propagation. We actually don't want to throw away all of these gapped transactions, but we want to store a couple of them. So if someone then sends, like, the missing transactions, we can validate them and add them to the pending transactions. So what we do here is we get the pending transactions and we send the hashes to a subset of our peers. So oh yeah, some of the stuff is a bit hard to follow.
00:17:28.560 - 00:17:28.876, Speaker A: But.
00:17:28.898 - 00:18:25.200, Speaker B: The idea is we shove the transactions into this announce channel and now I have to sorry, here we go. I have to look where these transactions are actually ending up. Sorry, here. Yeah, in the broadcast, it's in ETH protocol. ETH. Sometimes the package names and the packages are not really great. And yeah, so in the handler, we shove the transaction into this queue and we unqueue the transactions at the other end and put them into this queue.
00:18:25.200 - 00:19:14.372, Speaker B: And this is an infinite loop that runs. And if we have transactions in the queue, we get them. So we only have the transaction hashes in the queue. Yes, we actually fetch the transaction from the transaction pool if it's still there by the time that we ended up here, the transaction is not anymore in the transaction pool, we just ignore it. But yeah, we add it to the pending and we send the pool transaction hashes. And here we go back where we started. And this actually sends the transaction hashes to our peers.
00:19:14.372 - 00:20:05.450, Speaker B: So what we do is we announce like, there's two different ways that we send transactions to a subset of our peers, we send the full transactions and to all peers, we send the transaction hashes. And if someone receives a transaction hash that they're curious in, they can fetch it from our node. But we don't want like if we were to send our transactions to everyone, there would be extreme transaction churn and a lot of bandwidth that we don't really need because you would get all the transactions from every peer at all times. And so we don't want to do this.
00:20:05.760 - 00:20:06.364, Speaker A: Yeah.
00:20:06.482 - 00:20:46.330, Speaker B: So now we go to the P, two P packet. And this just writes the message onto the wire. We managed to push the transaction to the pier. Now we have to go to the other side and actually receive the transaction. And this is done in the transaction fetcher. The transaction fetcher initiated here.
00:20:47.260 - 00:20:48.010, Speaker C: And.
00:20:49.900 - 00:21:27.632, Speaker B: We have some loop. Yes. And okay. I've never seen this code before. That's. Yeah. So we let the timeout the wait notify yes.
00:21:27.632 - 00:22:01.490, Speaker B: We get notification of someone sending us transactions. And so we check whether we already got the transaction. We have the announcement from someone else. If not, we actually ask them to send the transaction. So we schedule the fetches. And for each peer, we ask them for the transaction. And if they have the transaction, they send it to us.
00:22:01.490 - 00:22:31.992, Speaker B: If we get a transaction, we actually call the add transaction function. No, sorry. Add transaction. This one. Okay. Sorry. And yeah.
00:22:31.992 - 00:23:01.552, Speaker B: So this includes the transaction, the transaction into the transaction pool. So this add transaction function actually puts them back into the pool. And so we end up in the transaction pool again. We are at remotes. So we also have this concept in the transaction pool, we have the concept of remote transactions. Those are those that we get from our peers. And we have the concept of local transactions.
00:23:01.552 - 00:23:39.810, Speaker B: Local transactions are those that we get over the RPC. We want to handle them differently because we don't want to drop our own transactions. So we always say if you send a transaction over RPC, this transaction will never get dropped. If a transaction from the wire is replaced by something that pays more, then we can drop this transaction. And so we have this notion of local in the transaction book. Sorry.
00:23:40.820 - 00:23:41.840, Speaker C: And yeah.
00:23:41.910 - 00:24:02.580, Speaker B: So that's roughly how transaction propagation works. The next interesting part is how do we create a block? So the miner any questions to the transaction propagation?
00:24:03.160 - 00:24:09.172, Speaker C: Yes. So how is it working? How is the gossiping the transaction further to other peers?
00:24:09.236 - 00:24:10.648, Speaker B: So in one when I'm receiving a.
00:24:10.654 - 00:24:14.056, Speaker C: Transaction, I'm recognizing I didn't saw this transaction yet.
00:24:14.078 - 00:24:15.464, Speaker B: So I'm assuming that then I'm just.
00:24:15.502 - 00:24:19.232, Speaker C: Also doing the same thing. I'm forwarding to other peers.
00:24:19.316 - 00:24:41.040, Speaker B: You're forwarding to the square root of your peers. And then you're forwarding the hash to all the other peers. And if you receive a hash and you have never seen this transaction hash before, then you ask the peer that sent you this hash for the transaction.
00:24:41.120 - 00:24:44.870, Speaker D: Am I forwarding all of the transactions or do I validate them first?
00:24:47.800 - 00:25:36.390, Speaker B: I have to repeat the question. So the question was if we are forwarding all the transactions or if we're validating them first and then forwarding. And the answer is we're validating all the transactions before forwarding. So our transaction pool only has valid looking transactions. So valid looking means the sender has enough money to cover the transaction cost and the nons is roughly what we expect. There might be some other conditions, but I don't know them. I'm quickly going to close the door because there's some sound coming from there.
00:25:36.390 - 00:26:10.384, Speaker B: All right, let's get to creating a block. So now we have a lot of transactions in our transaction pool. We are a miner. We want to seal a block. How does this happen? For that, we have to go to the minor package and the worker. Yes. So in the minor package, there's a lot of stuff going on.
00:26:10.384 - 00:26:58.270, Speaker B: The minor is extremely complicated, so so I already made made sure that we are in the in the right vicinity. We somehow end up in the commit work function. So the minor starts a loop. And whenever a new block comes in, we need to update our work package. So we need to build on top of the block that we just received. And for that, what we do is we prepare the work that generates um yeah, sorry. No.
00:26:58.270 - 00:27:23.744, Speaker B: All right, where were we? No, we were under worker yes. Commit work. We have to prepare the work. We have to yes. Prepare work. All right. Yeah.
00:27:23.744 - 00:28:01.524, Speaker B: We have to set a couple of fields in the block. For example, we have to set the parent hash to the correct hash. We have to set the correct timestamp. We have to calculate the gas limit per block. You can shift the gas limit by a tiny amount, either up or down. And so we have some configuration, the gas ceiling or gas target. This is what we want to end up with.
00:28:01.524 - 00:28:32.830, Speaker B: So currently this is set by default, set to 30 million. All the miners are trying to hit 30 million. But you can also configure this. So we have to calculate the gas limit. We have to set the coinbase. This is where all the transaction fees are ending up. In the end, if we have London chain rules, then we need to do the base fee stuff.
00:28:32.830 - 00:29:15.120, Speaker B: So we have the consensus engine. This can be either Ethash click or the new Beacon Consensus engine. And we have to make some environment. Once we prepared the block, we actually need to fill it. So we need to apply the transactions. Sorry. So what we do is we fetch all the transactions, all the pending transactions from the transaction pool.
00:29:15.120 - 00:30:24.120, Speaker B: So pending just takes the transactions that are actually executable, and then we first take the local transactions, order them, and then we take the remote transactions and order them. And then we commit the transaction to the pending block, to the pending work package that we're trying to build. And so what that does is, like, we have to set the gas limit and stuff like this. We calculate the sender again, we prepare the state. We have the state database, and we have to set some fields in there. So that we can later on query them during the execution of the EVM. And then we call Commit transaction.
00:30:24.120 - 00:30:54.240, Speaker B: And commit transaction. Actually does apply transaction. And with Apply transaction, we go to the core package. So the core package is responsible for most of the core stuff. The TX pool lives there right now, which is not great. We should move that to its own package. But yeah, so all of the interesting DVM stuff is in the core package.
00:30:54.240 - 00:31:39.360, Speaker B: So we're in the state processor now. We're creating a new EVM block context a new EVM and then we call Apply Transactions, apply Transaction does Apply message and Apply message calls. This new state transition transition database. And this is where we actually do the execution. Or we start the execution of the EVM. So we start the execution of the EVM by doing the pre check. The pre check has stuff like get nons.
00:31:39.360 - 00:32:40.148, Speaker B: We verify the nons. We verify that if it's not fake, yes, we verify that we actually have code there. No, we verify that the sender is an EOA. This is a pretty new change. Previously it was theoretical possible to send a transaction from an address that had code deployed. It's not really feasible to do that because you have to find a collision between a code hash and an address that you know the private key for. But if you set code to an address that you know the private key for in the Genesis block, which is possible, then you could end up at some weird state.
00:32:40.148 - 00:33:35.190, Speaker B: So we don't want to have transactions originating from a smart contract. Then we have to verify the base fee and then we have to buy our gas. And buy gas, just calculates how much gas we are, the maximum amount of gas that we are going to use in the EVM execution later on. All right, so we did the pre checks. We have a lot of code here for tracing. So you can trace the transaction, you can trace each upcode, you can see which operations are executed, what's the outcome and stuff. So if you see config debug and something with tracer, you can just ignore it.
00:33:35.190 - 00:34:18.240, Speaker B: It's not really interesting. And then we check the intrinsic gas. We check whether the sender has enough money. And if we post Berlin, we prepare the access lists. So in the Berlin hard fork, we changed the rules a bit so that you can provide an optional access list. And calls to the contracts that are in the access lists are cheaper. You can pre warm a contract.
00:34:18.240 - 00:35:25.880, Speaker B: And we decided that calls to the pre compiles, calls to the sender itself and the recipient should also be warm cost. So the idea was we want to increase the cost for calling a cold address that we have to fetch from the state database, because that's a pretty expensive operation. But that would break some contracts that have some hard coded limits for interacting with another contract. And so these access lists are to unbreak those changes. And so in the future, they're optional. Right now we could make them mandatory. Mandatory access lists is like the concept of mandatory access list is also something that comes up in Stateless.
00:35:25.880 - 00:36:22.750, Speaker B: If you're doing stateless, you will provide the prestate as a kind of access list, execute the transaction and then also provide the post state so that everyone can verify without having to have the state. All right. And then we check if this is a contract creation. Contract creations. If you send a transaction to no address, to zero address with code, then the code gets executed and we call the create operation and otherwise we call the code at the address that we sent to that we're doing the execution to. Are you more interested in Create or call? Create.
00:36:23.120 - 00:36:23.870, Speaker A: Call.
00:36:24.560 - 00:37:41.780, Speaker B: Okay, let's do the call then. All right. In call, we're first checking if when you call a contract, you can attach a value to that. So we're checking whether the address that central transaction originator has enough money to cover that cost. Then we take a snapshot of the state database and it then we checked the pre compiled and there was so that there's some there's some issue here. I don't know how many people actually know this, but back in the day, you could self destruct contracts. Like, there was a hard fork after the Shanghai attacks that destructed all the addresses without code, without storage.
00:37:41.780 - 00:38:27.284, Speaker B: And the problem was that Vitalik wrote a script to do all of this, but unfortunately and without a balance. Unfortunately, one of the precompiles also didn't have like, it doesn't have code because it's a precompile. It also doesn't have storage and it also didn't have a balance. And so he successfully destroyed the precompile. And so this clause actually checks that. We don't hit that. All right, then we're doing the transfer.
00:38:27.284 - 00:39:33.640, Speaker B: We're just sending the money. And if the code that we're sending the money to or that we're calling so we first send the money and then we're starting the code that is on there. If it's a pre compile, we run the pre compiled contract. So in Core VM, we have the pre compiles yes contracts. So this is like the different pre compiles for the different rules. For example, in Istanbul, a new pre compile was added, the Black to F precompiled. So it's like there are different precompiles depending on the chain rules that we have.
00:39:33.640 - 00:40:17.778, Speaker B: And these precompileds have a run function and a required gas function. So if you want to implement a precompile, you have to fulfill this interface. And for example, the EC recover precompile for the required gas. It's a fixed amount. It's always 3000. And if you run, it will validate the signature values, make sure that we're not inputting something really weird. And then does the EC recover of the public key and returns a public key of that signature.
00:40:17.778 - 00:40:52.450, Speaker B: And so this is like a pretty nice file. There's a lot of really cool things there. And I like this code a lot, other than some of the other code that I showed previously. And yeah, we're in the call again. If it's not a pre compile, we get the code of the address that we just called. If there's no code, we are done. Really done.
00:40:52.450 - 00:41:43.440, Speaker B: But we're finished for now. If we have code at that address, we have to trigger that code. And so we set the Call code. You have three different ways of triggering code. You have the Call code, some other stuff, a static call. And then you call run on the contract and run. We, we fetch, basically we fetch in a for loop.
00:41:43.440 - 00:42:38.530, Speaker B: We fetch the next operation and then we look into the jump table to see the program that we have. Program is like what the Opcode actually does and then we execute it. So how does an Opcode look like in the EVM? You can see, I don't know. We're going to for the London instruction set. Are we going for the Merge instruction set? You're probably all interested in the Merge doesn't do that much. Berlin, Istanbul. Okay, here we have some instruction sets where some instructions are actually set.
00:42:38.530 - 00:43:37.030, Speaker B: So in Constantinople, we had a few new Opcodes. We have the shift left, the Shift right, the Shift arithmetic right, xcode hash Create Two. And they all have a Stack requirement, a gas function. And they have a constant gas and a dynamic gas if there's some dynamic gas. So for some of the operations, like Add or sub static gas makes sense. Like you're adding 256 bit elements together, there's not much that can grow out of bounds. But for some operations, you want dynamic gas depending up the amount of work that you actually have to perform.
00:43:37.030 - 00:44:50.684, Speaker B: And so, for example, for Create, you want to spend gas, as much gas that's proportionally to the length of the code that you are going to deploy. And we can now look into one of these operations. So these are functions actually that are in there. And we can look at one of these functions. For example, the Create function takes some stuff from the Stack, it uses some gas and then it calls the Create Two, which just creates an address and then calls Create with that. And yeah, if we started with the Create route, we would have ended up right here, but we wanted to look at Call instead. That's roughly how the EVM works.
00:44:50.684 - 00:45:53.980, Speaker B: You have these operations, maybe something easy. So this is like an easier function in the EVM. You just take two values from the Stack, add them together and that's it. And yeah, so did pre compiled? Yes. And we, we are back in Call if something happens. So if there's some issue, if the contract reverts in the call, then we have to revert the transactions. And so, if you remember up here, we took a snapshot before we applied the transactions.
00:45:53.980 - 00:47:08.608, Speaker B: And if it reverted, then we have to reload this snapshot and throw away all the modifications that we made to the state database. All right? And so, after we executed the transaction, we know how much gas there was, we have some refunds. So, for example, if you clear storage, or if you call the self destruct opcode, you're getting some gas refunded. So we have to refund that. And we also have to pay the coinbase. So this line is where the transaction fee actually goes to the coinbase. And then we return the transaction result back to apply message and apply message.
00:47:08.608 - 00:47:50.560, Speaker B: And now we have the result. And before bycentium we calculated an intermediate state route after every transaction. This is a lot of work. So after bizenchiom, we decided to not do this anymore. And then we also have the receipt where we set if the transaction failed, we are going to set the failed in the receipt. I'll understand this. And we also set the logs and the Bloom.
00:47:50.560 - 00:48:46.640, Speaker B: So the Bloom is a Bloom filter where all the locks are in. So the locks we have the return opcode that you can use to not the return. How do you emit an event, whatever? If you emit an event in your smart contract, then it will end up in the logs. We want to filter these logs. So, for example, you are interested in all the events that originated from your contract. That said, I don't know send successful or whatever. So you have the Send successful event, those are in the logs.
00:48:46.640 - 00:49:49.030, Speaker B: And now you want to quickly filter through all the blocks. And this is where the Bloom filter comes in. So we put all the locks into this Bloom filter. What a Bloom filter is, I know, probably half of the room knows that I don't know if I should start going into this. But basically you take some piece of data, you hash it and you set some bits in this plume filter. And later on you can hash it again and see if these bits are set. And if these bits are set, then you most probably had this event originate from.
00:49:49.030 - 00:51:20.390, Speaker B: You did apply this event to the Bloom filter beforehand. There's a chance that you didn't and there's some hash collision. But it makes it very easy to query for logs to see if like something if these events, if these locks actually are in the in the Blue filter and sorry, it's 200, no 4000 bytes, something like this 2048 bit, it's fixed link. It's not really valuable anymore and we would also like to get rid of it. But it was valuable at some point because you could just go through all the blocks, look at the Bloom filter and quickly decide, okay, this block has none of my stuff in it. So I'm just going to ignore it and look at the next block. If you actually find something in the Bloom filter that looks like what you were looking for, then you need to go into the block, look at the receipts and see if that was actually what you wanted.
00:51:20.390 - 00:52:28.048, Speaker B: And yeah, that's basically it for a Ply message. And, yeah, we just return the logs from the receipts and we have some error conditions where we're back in the worker where we committed a transaction to the pending where we committed a transaction to the pending block. And now we have built a pending block. First of all, we set the fields like the parent hash and timestamp and stuff like that. Then we went through all the transactions, applied them one by one, and now we have a valid block. And now we need to actually finalize it, create the proof of work. But I'm unfortunately out of time, so we're going to do that some other time.
00:52:28.048 - 00:52:29.490, Speaker B: Thank you very much.
00:56:17.800 - 00:56:18.550, Speaker A: SA.
00:56:48.980 - 00:57:48.864, Speaker C: Oh, the music needs to stop. There we go. All right, so I'm here to talk about the privacy preserving proof of personhood protocol that WorldCoin is developing. I'll start with a little bit of background and how we got to solving the civil resistance problem, how we incorporate privacy in that, how we've been testing over the past years our hardware, and how we are now realizing that this is not only just cool for us and our token, this is actually cool for the whole ethereum community and how we are bringing it back to the community in the form of an SDK that everyone can build a civil resistant application of. I'll give some examples of the cool stuff you can build with it and have some little hints on what we're going to do in the future. So a little bit of background about the project. Crypto is awesome, we realized a couple of years ago, but less than 2% of the world has used it or access to it.
00:57:48.864 - 00:59:04.056, Speaker C: And the way it is right now, it seems to be hoarded by a select in crowd and really hard to enter and it's not as growing as it used to be. Part of that is the way tokens are airdropped. What if crypto had started by just airdropping a new coin, let's say, bitcoin when it was first released by airdropping it and giving every human on Earth one token? Like, how would the world have looked like if that was the case? It would have been so much more inclusive, so much bigger, so much different. Can we do that today? That was the project behind WorldCoin. What if we build a token, mint 10 billion of them and give every person on Earth some of that for free? We have a problem to solve there. How do you make sure each person only claims their part once? That is, how do you prevent the army of clones to take like. How do you prevent people from signing up multiple times, creating all these fake accounts or multiple accounts and claiming more tokens than they have a right to? Obviously this would skew the token distribution, therefore the economics and undermine the goal that we're trying to have now.
00:59:04.056 - 00:59:32.656, Speaker C: We need a solution to this and it cannot be any simple solution. We need something that preserves privacy. This is a value that's important to us. We need something that is inclusive. We really do want every single human being on earth to sign up. We don't want there to be arbitrary constraints on that and we need it to be scalable. Turns out there are a lot of humans on this planet so we've considered a whole bunch of options.
00:59:32.656 - 01:00:11.288, Speaker C: We can have people sign up with email addresses or phones. Pretty common way to do civil resistance in your Web 2.0 app. Unfortunately it is completely insecure and easy to anyone can make new email addresses and phone numbers are surprisingly easy to steal, especially for SMS verification. So the other approach, that is the go to approach in the financial industry is to use passports, ID cards, official KYC documents. This is actually not as inclusive as you think it is. A lot of people don't even have these documents.
01:00:11.288 - 01:00:59.720, Speaker C: It is also the complete opposite of private. So that's a no go. Some of the cool solutions that have been tried in the past is Web of Trust where you create a community that authenticates each other and these mutual loops of authentication in the end build a system that can generate a lot of trust, like spontaneously out of the way it is structured. The problem with Web of Trust is that the attempts that have tried have not been particularly successful. It also suffers from a major bootstrapping problem where you really need to kickstart it with a very large community for Web of Trust to be meaningful. So that's not an option for us, at least not right now. So what we settled on is biometrics.
01:00:59.720 - 01:01:31.930, Speaker C: Everyone has biometrics. We can do this in a private way. It is super inclusive. So let's look at that a little bit more. There are a couple of biometric solutions that are popular fingerprinting, palm printing, DNA although not used much yet, there's issues with all of them. Fingerprints surprisingly, are not very unique. I think the match rate on a fingerprint is something like one in a million, if not worse than that.
01:01:31.930 - 01:02:15.872, Speaker C: Similar with for example, face recognition. Now these are good enough to unlock your laptop and unlock your phone because the attack factor there is you have the real user stored in your system and you just need to compare whoever you have in front of you right now against this stored representation. So if you have a success chance of, let's say one in 1000, this is already like a good deterrent. Like an arbitrary person would not be able to get in randomly, the chances are very low. However, for what we're trying to do, we have a much harder problem. We need to compare every new sign up against every sign up in the past. This is quadratically.
01:02:15.872 - 01:02:58.548, Speaker C: Worse, you now have to deal with the birthday paradox, for example, which means that the chances of duplicates accidentally matching goes up quite radically with the size of the group. So we need something that is very, very unique, not just fingerprint unique or palm print unique, but much more unique than that. DNA would solve it, but even that actually has a, I think, 0.7% false positive rate due to identical twins. But other than that, there are just no good ways of doing it. Something that does work and can be used conveniently are irises. Every human eye is unique in a very particular way.
01:02:58.548 - 01:03:33.820, Speaker C: And you have two of them. So that's like double the entropy right there. So irises was the way to go. Then we looked at okay, so how do we do biometrics on irises? Can we use the phone camera? Well, turns out phone cameras are not nearly good enough to take a high resolution picture of an iris. It's surprisingly hard optically to take a good high resolution close up of something that is so small and so far away. Realistically. We've looked at off the shelf biometric equipment, like, for example, the clear scanners that you see on airports.
01:03:33.820 - 01:03:58.712, Speaker C: They are not very convenient to use. You need to hold your head in some weird goggle. The resolution is surprisingly not that good because, again, they just need to compare yourself against a stored representation. They don't have to deal with the quadratic challenge that we're facing. So we needed to improve on the state of art of irises and we needed to do that with custom hardware. So fuck my life. There we are.
01:03:58.712 - 01:04:45.316, Speaker C: That's a big problem to solve for the theme. So, one eternity later and this literally took two and a half years of development, we now have our biometric device that is capable of civil protecting a set of a billion human beings. So what does this device do? It makes sure you're real. This is something I think a lot of people don't really grasp. Fingerprints are not really secret. You leave them on every single glass, on every restaurant you've ever been to, and a fingerprint alone, like the information contained in a fingerprint, is not what identifies you. It's not what authenticates you.
01:04:45.316 - 01:05:31.110, Speaker C: What authenticates you is that you're the only person on Earth, the only human being that has that exact fingerprint on a real life finger. And it's that last part that is critical. You need to make sure that not only are the biometrics what you expect them to be, you need to make sure that they are actually on a real human being. So that's what the op does. It makes sure it's looking at a real human being. It looks at your iris and not like a contact lens that pretends to be an iris or anything like that. And then it takes a picture, it turns that iris into an embedding, essentially a 100 dimensional vector that can then be compared against others.
01:05:31.110 - 01:06:01.150, Speaker C: And it signs that to authenticate that. This is now a verified iris code. So a little bit on the hardware itself because I think it's just very exciting. We'll be open sourcing the hardware soon. So you can actually go through the schematics, go through the design files, go through the firmware, you name it. There's a lot of cool engineering there. It's basically well, there's one right here, actually.
01:06:01.150 - 01:06:53.004, Speaker C: So you can see there is this sensor area in front. It has a lot of biometric security sensors in there, things like, you name it, thermal cameras, whatever. We need to make sure that whatever is in front of us is a real live human being and not a picture that someone's holding up. Then there are different rings of LEDs there in different wavelengths. So we can photograph the iris in I think it's six different wavelengths that we're using currently. And that helps us gather even more entropy on the structure there. And then there is the big hole at the top of the little circle there contains a very high resolution zoom camera with an Aimable mirror that can really efficiently track you while you're moving.
01:06:53.004 - 01:07:27.770, Speaker C: Because we wanted the experience to be smooth. Like, we don't expect people to stand absolutely still. You can just hold it in your hand. And the camera has a focus window that is basically a centimeter sized cube that it will perfectly track while you're holding the thing in front of you. And then it has wireless connectivity so it can connect to the blockchain and you name it. Yes, we'll be open sourcing the entire design. For those who want to know more, you can also check out what sort of things we do to further protect the orb itself.
01:07:27.770 - 01:08:11.972, Speaker C: But let's talk about privacy a little bit because so far we've been talking about taking pictures of people, but we do not want to store images. We do not want to store names, we do not want to store contact info, we do not want to store your KYC info. We don't want any of that. So how does that fix? How do we fix that? We're not quite there yet. Now we have this 100 dimensional iris code, but the naive implementation would just you would basically submit this iris code to the blockchain. It would check if it was already there, and if it wasn't there, it would kind of bless your wallet as being a unique human. But this is not very cool because now you have an address that is directly tied to an iris code.
01:08:11.972 - 01:08:35.084, Speaker C: So, like, in principle, you could go back to an orb and check if this address belonged to this human. We don't want that. And it only allows you to create one wallet. That's not very nice. You should be able to create pseudonyms. You should be able to do a lot of things under your identity. Really? So that's where the next step comes in, which is zero knowledge proofs using an anonymity protocol called Semaphore.
01:08:35.084 - 01:09:08.640, Speaker C: That's basically the anonymizing parts of, say, tornado cache and CCASH, but without all the currency stuff and just the pure anonymity primitive. The way it works is you install the mobile app. The mobile app generates a Semaphore hash, which is essentially a public key. Here you show the public key. Well, the wallet stores the private key. The public key gets shown as a QR. You walk up to an orb, you show the QR code to an orb.
01:09:08.640 - 01:09:45.204, Speaker C: Now the orb knows the public key, your public key. Then it does the biometrics thing and it generates this little factor, the Iris code embedding. It combines these two pieces of information together and authenticates it with the signature. And now you can send this to the blockchain. And when the blockchain verifies that it hasn't seen a matching pair of eyes before, it will add the public key to a big merkel tree. That's step one. Now you've signed up.
01:09:45.204 - 01:10:14.430, Speaker C: Now you're part of the ecosystem of all humans. You're in the merkel tree of all humans. We've signed up. Now, in order to claim the WorldCoin token, you use the private key from your wallet. You generate a zero knowledge proof of a merkel proof that you're in this tree, and you send this to the contract using any address you want. You can generate a new address for it. It doesn't matter.
01:10:14.430 - 01:10:53.564, Speaker C: Then the zero knowledge proof gets verified. It checks that the merkel route matches that you're actually in the set. It does a couple more things that we'll get into in more detail later. And then you receive your WorldCoin tokens in your wallet address. And due to the zero knowledge proof layer, there is like no connection between the original Iris code that you use to sign up and the wallet that you received this token in. At best, you can say that, yes, this was someone who was in this set of users, but that's the end of it. So we have our privacy back.
01:10:53.564 - 01:11:20.672, Speaker C: The only information that is ever attached to the Iris code is that they're now a member of the set of users. And you can really learn nothing else from this data. So that's awesome. It sounds like we have our technological solution ready. We have our hardware developed, we have our blockchain parts solved, we have our privacy, we have our scalability, we have everything. So we started testing this project. We've built some prototype orbs.
01:11:20.672 - 01:12:18.616, Speaker C: We've been going to several different countries around the world to see how people receive it. We've been doing field trials in 25 countries, pretty much uniformly, randomly distributed over the world, just to see how people respond to receiving a crypto token just based on the fact that they're human. Like the original thesis, like, what if we actually airdrop a token on all humans? And the responses have been very exciting and overwhelmingly positive. It depends on where people are in rural communities, in developing countries, people just like the fact that they're included in the financial fabric. They have historically been excluded. They don't really have access to banking. The Unbanked definitely exists and there's definitely a benefit in helping them with technology like this.
01:12:18.616 - 01:13:16.856, Speaker C: But also in European cities, a lot of people are excited about being onboarded into crypto. They have heard of it, they're interested in it, but they don't have a good way of being included in it yet. And the process of sign up and the token drop is a really great way to onboard new users. We also needed to know how well this scales. Like how many of these orbs do we actually need, how quickly can we sign up a billion users? So we've been running an experiment in Chile with at most three orbs at a given time. There's a whole model around how these orbs are distributed, how people can bid on renting an orb and how they get rewarded then for signing people up and so on. And local people even tend to start small businesses, businesses around that.
01:13:16.856 - 01:13:55.556, Speaker C: So at the end, I think there were about 30 plus people active in Chile helping people sign up. And we had an average weekly sign up rate of 1.5 thousand each week up to 2000, mostly limited by some constraints of the early prototype hardware. So as we improved the hardware, the sign up rates actually went up. And the latest orbs are really efficient. Like you can really quickly sign up large groups of people. Yeah, this is actually a cool picture of how you can see how the orb developed along the way.
01:13:55.556 - 01:14:55.480, Speaker C: What's interesting is that in the early iterations, there were actually two cameras that were looking at each eye individually. And as we got better at the optics and the aiming part of the optics, it turns out that we can do with just one very good camera and just keep switching it between both eyes quickly, like the tracking became that good. So the field trials went almost better than expected, really. Like given the small number of orbs, prototype orbs we currently have, we've managed to sign up half a million people in the last couple of months. And currently we are switching our orbs to mass production. So we have about, I think, 30 orbs now that are in the field for the field trials, for the version that we have now, we are scaling up production and we'll be producing 250 of them this month. And by the end of the year it will be, I think, up to 4000 per month that we are going to be producing.
01:14:55.480 - 01:16:16.172, Speaker C: So extrapolate from there and you'll see that it's actually feasible to onboard hundreds of millions, if not a billion people into this civil resistant token AirDrop. So that's cool, we can airdrop a token on a billion people and then the devs ask us, but wait, can I do the same? Can I reuse that verification for my app? Well, it turns out the civil resistance we built is actually really useful primitive for a lot of DAP developers there. So how can we offer this on its own as kind of an independent thing people can use? So let's do this. And one of the key things again is we want to preserve privacy at all costs. So one of the things people always tell us is like, oh, you should do a non transferable NFT that identifies your humanness. But the problem with that is that it's essentially a whitelisted wallet or a public unique Identifier. Yes, it would work for the civil resistance, but now you would just have one wallet address that is blessed as like your unique human wallet.
01:16:16.172 - 01:16:56.288, Speaker C: And if you vote in a Dow and if you then participate in an AirDrop, you can see that oh, this person who received this AirDrop is the same person as the one who voted that on that proposal. You don't want that. You want to be able to have a proof of humanity while not being able to link all the actions you've ever done together. And this is possible thanks to the zero knowledge proof magic that is in Singapore. And we're building a whole SDK and a user frontend and everything around that, that we call World ID. We've just launched the Alpha today in a workshop. Let me quickly go through what it does.
01:16:56.288 - 01:17:39.304, Speaker C: So it's a protocol to anonymously verify that someone is a real person that is performing an action only once. Those are the three key aspects that it does. So for your users it would look like I'm a real person and I've never claimed WorldCoin. That's what we do currently. But you can also make the claim I have never voted in this poll, I have never minted this NFT and so on. And all these claims you can make and they get verified against the merkel tree and another technical thing called a nullifier. But you never do anything more than make this claim.
01:17:39.304 - 01:18:09.408, Speaker C: Like you cannot link these different instances together. The way this works is actually rather complex. It involves a lot of different steps. But it turns out in the end, you can abstract all of this away in the super easy to use API. All you need to do is import World ID from our SDK and just get a little box in your UI. You need to specify an external nullifier. This is sort of a context.
01:18:09.408 - 01:18:52.192, Speaker C: This is the context within which you want to make sure users can only do it once. So if you want to make sure that everyone can only vote once. You need to generate a unique ID for each vote that you can put here. If you want to make sure that anyone can only claim their AirDrop once, you can, for example, take the AirDrop token address. It needs to be some context, something that uniquely identifies the context that you want to allow everyone to do an action in only once. And then when you set it up, you get this little widget here and you just check that, hey, I'm a unique person doing this for the first time. It's a very simple JavaScript widget.
01:18:52.192 - 01:19:36.040, Speaker C: You can integrate it in minutes. It's designed to resemble a captcha because that's something people are familiar with. And there's also quite a bit of conceptual overlap between a captcha and what this is. It's not entirely the same though, because CAPTCHAs you can solve multiple times, it doesn't prevent you. CAPTCHAs are rate limiting and they make sure that there is a human in the loop, but they don't really solve the civil resistance problem, whereas our solution really solves the civil resistance problem. And another fun thing that we're currently not using, but that I kind of like is that it doesn't prevent you from automating your own stuff. Like with a captcha.
01:19:36.040 - 01:20:23.148, Speaker C: I like to write little Python scripts to automate my life, and CAPTCHAs are not good for that because I cannot automate them. But this one you can actually automate and it would still prevent you from doing something more than once. So it solves civil resistance in a way that I think is actually superior to CAPTCHAs because it doesn't stop you from automating. Quick example, let's say you want to do an AirDrop and you get $50 worth in mesha. You have a little claim button and then by integrating the widget, you'll have this little thingy on top. The moment you click Claim, you just press it. Now it uses Wallet Connect to show a little QR code that you can open in your mobile wallet.
01:20:23.148 - 01:20:49.304, Speaker C: You sign off on it and there you go, identity confirmed. And it proceeds. And now what you get back is a zero knowledge proof, which is just eight UN 256s really, that you can pass along to the transaction you're about to make and verify against the contracts we have. And that's it. Now you have a transaction that every human can only do once. It's as simple as that. This is an example we have with a lot of documentation.
01:20:49.304 - 01:21:40.040, Speaker C: You can play around with it, you can find it on Amsterdam. A couple more things. So what is this Flow like? So the users get the World ID using the they sign up you as a developer, you integrate the World ID widget in your DApp. The user verifies the World ID claim in their app. It pops up in there and then as a dev, you can execute whatever action on chain action you want exactly once it's actually super flexible. These are some of the use cases we've thought of. But the cool thing about an SDK and a hackathon is that people come up with all sorts of crazy ideas.
01:21:40.040 - 01:22:28.196, Speaker C: You can do demographic voting very hard right now. Everyone does token voting. Not so much because they like token voting, they do it because it's the only thing we know how to do right now. But if you are able to uniquely recognize people, you can do actual one person, one vote voting, or you can even do quadratic voting. The AirDrop use case, like we do, we think this will be a big early thing because by Airdropping, a token on all humans, you can quickly grow a very large user base. Quadratic funding requires quadratic voting to really execute properly. You can create NFTs that are tied to unique humans.
01:22:28.196 - 01:23:08.040, Speaker C: You can create new accounts on websites and have civil protection in like a web two kind of way. You can use it to replace some of the fraud prevention that you would normally use KYC for, and anonymous registration on, let's say, social media, where you would want an anonymous account, but you don't want all the anons or the alts or the bots. That's why we have the whole SDK. There's a protocol. There is this JavaScript widget that I showed you, lots of documentation that we are currently writing. We have simulators for this stuff, so you can try it out locally easily. And a bunch of example projects.
01:23:08.040 - 01:23:55.652, Speaker C: We have a discord if you want to chat about ideas, get some feedback, get some help with your projects. I want to do a big shout out to the Simaphore project, whose open source solution we use to get the anonymity right. Another project that's cool that we've been developing is Hubble. Hubble is a layer two. It's less well known, but it's basically an optimistic transfer only roll up, which uses BLS signature aggregation. And this allows it to be an order of magnitude cheaper than all of the other LPUs that are out there right now. And for us this is important because we want the world coin token to be usable for all people all over the world.
01:23:55.652 - 01:25:09.150, Speaker C: And right now, ethereum fees are just inaccessible for the vast majority of people on Earth. And even L two fees, fees that are in the order of like one to $2 are just not useful if your balances are in the order of $10. So incomes Hubble, which actually brings it down by another order of magnitude, and we've been developing an open source sequencer for that in Go that is high performance, that can actually handle the transaction volumes of that many people. That's another cool project that we're building on, Semaphore as it is currently, and as we're using it currently, it does a lot of stuff on chain and it does a lot of like every sign up and every claim currently requires an independent interaction with the Smart contract, which costs 400,000 gas to just do the verification bit. Obviously, that doesn't scale very well. For example, for voting use cases, you don't really want an economic hurdle that prevents participation. So for voting, it's important that you really get the cost down.
01:25:09.150 - 01:25:59.452, Speaker C: So for Semaphore, we're working on an L Two style, like an L Two ish solution. It's basically just a sequencer that can do batching and aggregating of these transactions and then use a recursive proof to submit all of this at once. And you would be able to check multiple claims by multiple people in one transaction at a fixed cost. All of this is open source. Actually, yesterday no, wait, last week we did a big post on why open source is important to us, how we are planning to open source things. It turns out open sourcing hardware is still relatively new and kind of difficult in some ways. Like for example, Somebody Optics has drivers that have proprietary source code that we're not allowed to share.
01:25:59.452 - 01:26:46.670, Speaker C: So that part unfortunately won't be open sourcing immediately. We also don't really want to help people create better surveillance hardware. So we decided to open source the ORP hardware under a license that specifically forbids any surveillance or privacy invading use case. Fortunately, irises kind of suck for surveillance purposes. It's much more cost effective to use face recognition because you can do it at a distance without active participation. If you want to get someone's irises, honestly, it's hard enough to get a good image of someone's Irises with their active participation. So there's like this nice consent element to it that I kind of enjoy.
01:26:46.670 - 01:27:18.292, Speaker C: What's our roadmap? So yeah, we're working on batched CKP submissions. If you're excited about working on recursive Snarks, then definitely hit me up. We have a lot of fun stuff to do there. We're working on an SDK for mobile apps. If you're not having a web app but you would be able to integrate it in your mobile wallet. MultiChain support. We'll be constructing the Merkle tree itself on Ethereum L One because of the extra security it provides.
01:27:18.292 - 01:28:13.080, Speaker C: But the SDK we can deploy on any EVM compatible chain easily, and you will be able to use it on polygon or optimism or your favorite chain. What else is there? So there's this concept of signals in the juveniles proofs. I can quickly go into that that's basically for an AirDrop, you just need to claim that you haven't done it before. But if you want to implement something like voting, you not only need to prove that you haven't done it before, you also need to prove that this is the thing you want to vote for. So there's this extra piece of data that can be attached to the proofs, which we call a signal. And yeah, the roadmap. So we're currently in an Alpha.
01:28:13.080 - 01:29:05.556, Speaker C: We have our first proof of concepts of auto technology. We have our technological roadmap. We have the hardware done. So now it's kind of the blockchain side of things that we're developing out. And it's a fast moving space, so we'll probably adjust as we go along. For example, Hubble, the L Two that we're using, I'm kind of hoping that the other L Two S will adopt signature aggregation either through BLS or general proofs, and then we can do away with our own L two and just be nicely on polygon with our token. In the summer, we'll start actively signing up users at a fast pace using the mass production hardware, and we will be iterating on the earlier projects that want to build on our SDK.
01:29:05.556 - 01:29:31.392, Speaker C: And late summer we'll be just opening it up for everyone to use for whatever they want to. So that's what world ID is. Solv civil resistance at scale in a way that preserves privacy. Anonymity is very self custodial. You have your own wallets. It's essentially no different from other wallet management. Other than that your public key is registered in a special way.
01:29:31.392 - 01:30:02.236, Speaker C: It's fully open source. You could even build your own wallet implementation if you wanted to. Easy to use and implement. And like I said, we're opening all of this up. We're trying to release as much of the source code as we can. We're trying to reach out to the communities we use, like Semaphore, contribute back, basically be good citizens of the Ethereum Ecosystem. And yeah, that's the end of the talk.
01:30:02.236 - 01:30:13.020, Speaker C: We still have about five minutes left, so I want to open it for any Q-A-I have this microphone to pass around.
01:30:14.130 - 01:30:34.850, Speaker D: So I had a quick question about what's your approach to dealing with collusion? Like either people getting together and just getting a bunch of votes together, like scanning a bunch of eyes together, but really kind of doing another one person or buying votes later off of the people who have scanned it and things like that.
01:30:34.920 - 01:30:36.082, Speaker E: Does it kind of make sense?
01:30:36.216 - 01:31:16.702, Speaker C: Yes, there is all sorts of fraud mechanisms that we've thought of, and that also we saw in the field trials. That's why we did them, to just see how people respond to it in general. Any kind of imposter attack we feel like we've covered. So yes, you need an actual human being to sign up when it comes to selling your wallet. What we're working on is being able to reclaim your wallet by just resigning up again. So yes, if you signed up and you participated in an AirDrop and then you sell your wallet, then yes, that token will be sold. But that is the self custody aspect of crypto.
01:31:16.702 - 01:31:28.600, Speaker C: If you sell your token, it's someone else's. But you cannot really buy someone's identity reliably because this person can just claim it back at any point in the future.
01:31:29.290 - 01:31:29.894, Speaker B: Got it.
01:31:29.932 - 01:31:30.520, Speaker C: Thanks.
01:31:39.600 - 01:32:02.084, Speaker E: Could you run me through what happens if, say, I have this widget on my website, your. Alpha widget and the user clicks on it. What does he see? What steps does he have to take to verify that he's indeed human and all the conditions are met so that he can participate in the AirDrop? He doesn't need to scan his iris again, right?
01:32:02.202 - 01:32:22.590, Speaker C: No. So the user will have the world ID app on their phone, they can scan the QR code, and then the phone has the private keys that were initially used for the sign up. And that is sufficient to generate a zero knowledge proof that you're a member of this set and that you haven't participated in this particular thing before.
01:32:23.600 - 01:32:48.944, Speaker E: All right, so, yeah, the use case grows as more people get onboarded by scanning their irises. Right. This solution does work, but as you noted in the beginning of your presentation, reason why most people use QYC right now is because everybody has a passport and everybody can participate. Right. Maybe the bottleneck with the orb.
01:32:48.992 - 01:32:54.080, Speaker C: That not everybody claim to assume that everyone has a passport. That's definitely not the case globally.
01:32:54.160 - 01:32:54.790, Speaker B: Oh.
01:32:57.500 - 01:33:16.892, Speaker E: Okay. But then it's still like the scaling. I understand you guys are mass producing the orbs, but yes, like to get everybody scanned. It's quite as you said you did, 1.5 thousand users per day. At a certain point, I thought I per orb. Per orb, yeah.
01:33:16.946 - 01:33:17.308, Speaker C: Yes.
01:33:17.394 - 01:33:27.584, Speaker E: But if you just look at birth rates and stuff, it's quite hard to keep up. Right. I'm just curious how you're going to.
01:33:27.702 - 01:34:09.100, Speaker C: Oh, yeah, I did some math on what our projected sign up. We know what the orb production rates are because that's all under contract with a high quality manufacturer. We know roughly what we can expect in terms of the sign up rates from these orbs. So you can compute a transactions per second that your blockchain stuff needs to handle, and it's between the ten to hundreds of transactions per second. So it's like two orders of magnitude out of realistic for ethereum L one. But if you do some basic aggregation sequencing, use some of the existing L two techniques we've developed over the past two years, you can manage it, so it's feasible.
01:34:10.640 - 01:34:11.390, Speaker E: Okay.
01:34:11.780 - 01:34:13.184, Speaker B: All right. Yeah.
01:34:13.302 - 01:34:34.244, Speaker E: First, just last remark is that I think it's very ambitious and very well done so far. Curious to see where this goes. And if there's a lot of users that are basically verified by you guys, then we would be definitely open to trying your widget out as well.
01:34:34.362 - 01:34:47.370, Speaker C: Awesome. Yeah. We're very curious about how it will go as well. It's not been tried before. This is a very interesting experiment just to see if this is the right approach to do it. And we're pretty confident that this will work.
01:34:47.900 - 01:34:51.852, Speaker E: Yeah, I agree. CAPTCHAs are over, CAPTCHAs over.
01:34:51.986 - 01:35:36.132, Speaker C: And honestly, KYC based identification, that's just another trust mechanism. Like, yes, if you trust your government, it's fine, but not all governments are trustable. And in fact, there have been historically instances of tiny countries that were handing out millions and millions of passports to people for bribes. So it works in the small part of the world that you and I grew up in, but it really doesn't scale well beyond it. And that's not even talking about the privacy issues around it. So that was an excellent presentation.
01:35:36.196 - 01:35:37.016, Speaker A: Thank you for sharing that.
01:35:37.038 - 01:35:38.970, Speaker C: I thought it was very concise and clear.
01:35:41.100 - 01:35:43.420, Speaker A: I guess my question is you've probably.
01:35:43.490 - 01:36:13.172, Speaker C: Spent a lot of time thinking about what could go horribly wrong. Unfortunately, yes. Socially, through a scalability. Technically, culturally. Do you want to share some of the thinkings that you have of the things that could go wrong? I mean, what can go wrong? We've seen people trying to sign up their dogs, for example. We obviously filter for that. We also filter for children.
01:36:13.172 - 01:36:32.110, Speaker C: We want to make sure that people are adults when they sign up. Maybe in the future it will make more sense to be more inclusive here. But right now we just want to take the conservative approach and make sure that people that sign up are well educated, know what they're doing.
01:36:34.910 - 01:37:27.770, Speaker F: I got two questions. First is linked to mobile, which kind of sounds like a weak point because you can lose mobile. Yeah, they can still mobile because once you're signed up, your key is sufficient to do whatever you want with your address. Is there any recovery mechanism or any blocking mechanism? Because now if I lose my private keys, if I quick enough, I just move my funds to another address. Here it is already linked to one private key. So if I lose this private key, it means basically everyone will have access to my identity, basically even to my funds.
01:37:28.270 - 01:38:06.630, Speaker C: Yeah, there's two components to it. So there's the private key that you use to sign the claim, to sign this unique human claim. And then there are the private keys that belong to your wallet. They're different private keys. They're just stored together in the same wallet. And honestly, this is an area where we don't want to be super innovative ourselves because what we do right now is just the same as every other mobile wallet do. And we will do the same thing like seed, trace, backups if you want, back it up in your Icloud or whatever, have it replicated maybe even hardware wallets.
01:38:06.630 - 01:38:35.620, Speaker C: You can just use existing. It's basically just a fairly standard wallet that you can use there for your wallet private keys. All of this is the same for your identity private key. You're right, this is a little bit different. And deridy recovery mechanism will come in handy where you can just go back to an.org and just replace your public key with a fresh one based on that, hey, you're actually matching this particular iris that we saw before.
01:38:36.310 - 01:39:02.090, Speaker F: And another one is especially because hardware will be open source. So when I do scan myself I actually have no way to prove that this is legit hardware, that it's not actually stealing my data, that it's not copying me and sending to everyone or whatever. I don't have any way to verify what's going on inside the orb, actually.
01:39:02.240 - 01:39:34.164, Speaker C: And this is the traditional hardware wallet problem still unsolved. We're open to ideas. We think open sourcing is a good step in this direction. Auditing the supply chains would be another important thing we can do here. I have some fun ideas on how you can create a little bit more self custody around this. But yeah, we're running out of time, so we can talk about that outside of the meeting. All right, I think it's time for our next speaker.
01:39:34.164 - 01:39:37.050, Speaker C: So I'm open to.
01:39:49.260 - 01:39:54.130, Speaker D: Organization for Humanetics. We actually focus on getting people.
01:40:13.940 - 01:42:02.280, Speaker A: Higher for into the sacrifice to a higher point about life. Leave alone the natural house. It about making my way. You belong.
01:42:25.620 - 01:42:48.970, Speaker D: Should we get started? Okay, thank you all for coming. So today we're going to talk about a development framework I've been building with Paradigm and some other open source contributors for the people that don't know me. Hi, I'm Giorgios. I work with paradigm. We're a venture capital firm. I'm the CTO there. And so we're going to talk about this.
01:42:49.420 - 01:42:49.736, Speaker C: Now.
01:42:49.758 - 01:43:44.500, Speaker D: What's the problem? When writing tests for Solidity contracts, developers have been trained to use frameworks like Hard Hat or Truffle or Brownie. And typically the issue there is that you're writing your contracts in one language, but your tests in a different language, and you end up having to remember a lot of things. You switch contexts. You go from Solidity to python back to Solidity all the time. And personally, whenever I context switch a lot, and when compilation takes a long time, the 15 or 20 seconds that I spend waiting, they're not 20 seconds, they're more like two, three minutes because I Altab the Twitter and just start scrolling because I'm waiting for my code to compile. So there's a lot of places where we can improve things. And we identified that the way to get the most benefits is by combining the language that you write your tests.
01:43:44.500 - 01:44:38.356, Speaker D: So you always write your tests in the same place where you write your contracts. So we write tests in Solidity, and we also identified that Fuzzing, which is the act of writing tests which cover more edge cases in your code, to put it very in a very high level way was also hard to do. So we also seeked out for something, some way to get easier test coverage, to get better test coverage in our code. Now, there's also this library which people personally, I dislike a lot, JS big number which has caused a lot of migraine, hypertension, and stress, especially when migrating from Hard Hat or from Truffle or when you use web three JS or ethers JS. So how do we solve this? We write our test in Solidity, as I said. So this saves you from context switching. You always have the same syntax.
01:44:38.356 - 01:45:16.900, Speaker D: It also lets you feedback into Solidity and kind of provide feedback for the language so that the language improves itself over time. And candidly it doesn't make sense for you to be a smart contributor and spending 60 70% of your time writing JavaScript. We have property based testing, I'll get to that in a bit. Basically, instead of testing for one concrete test case, you test for hundreds or thousands of test cases for the same properties. We allow VM. State overriding. This is the equivalent of, let's say you have a storage variable that you want to change without having access to a smart contract that can change it.
01:45:16.900 - 01:46:10.672, Speaker D: You can literally modify a storage slot and an arbitrary address and you can use that if you're mocking, if you're trying to imitate an oracle or anything else that you want. And there's a lot more actually in the VM state overriding category and I'll get to that in a bit. With respect to compilation and testing, we want everything to run blazing fast. So ideally you run all your tests in parallel. Basically the problem is embarrassing parallel, so you should be able to spin up as many threads as your system can support. We use a very fast rust EVM which means that even without parallelization things are very fast and we also want to have very aggressive caching on the dependency graph. And the thing that is the most underrated is runtime observability and debugging which you will see some screenshots in a bit, but basically when a smart contract is executing you want to see every sub call that has happened.
01:46:10.672 - 01:46:55.700, Speaker D: Ideally have a debugger that you can step through things so we can know what the hell is going on. So the name of the tool that solves all of this is called Foundry to see if this matters, if people should care. We started developing around the end of 2021 and in five months we're on our way to meet Hardhead at the top and we'll see what happens. People have started using it a lot. You can see this in Soulmate which is an upgraded open Zeppelin contracts repository. Optimism uses it MakerDAO uses it mapple notably had their tests go from 30 minutes to 30 seconds which is a lot is a big improvement. And these are the guys building the Synapse bridge.
01:46:55.700 - 01:47:42.004, Speaker D: So far we have about 100 something open source contributors in again less than six months which shows that the project is open source. First it is building with the community rather than only for the community. Have very rich docs templates, a GitHub action, a standard library with popular contracts used for testing and for modifying the rest of your code and of course the core code base. So let's see how does this look like? So again I'll take a trivial example. Set function on doubles and initial value one for X. And normally if you were doing hardhat you would write describe, you would import ethers, you would launch, you would get your wallet, blah, blah, blah. It's a big process and you need to remember a lot of things.
01:47:42.004 - 01:48:34.644, Speaker D: Whereas here you write your setup function. This could have been constructor but this is just for to make it more explicit, think of setup as the equivalent of before each in JavaScript and a simple unit test you do require food, x is the initial value. So this should obviously this assertion should pass and then you double it and then you require that it's two and you would expect that this test passes. If you have a test that fails and you want to check that it fails because you're writing a negative test. And it's important that we also write tests that are not in a happy case you write test fail instead of just test and it would expect a revert to happen. And if a revert doesn't happen then it will actually show test failed because it was hoping to fail. And also I realize I'm compressing a lot of things in a very short amount of time.
01:48:34.644 - 01:49:27.300, Speaker D: So in every slide there is this link to the docs which you can read and the book. It's very comprehensive and gets you zero to hero in a weekend. So I recommend it. Now, property based testing is this process where instead of me giving the inputs I make my test generic over the inputs. I just give it an extra argument and it proceeds to run it for tens, hundreds, thousands of different inputs. So this it will get run with one gajillion whatever and we'll keep doing that until either it reaches the max number of iterations or it finds a counterexample. And if it finds a counterexample it gives me the input which found a counterexample and then I can write a unit test for it and guarantee that the bug that was found during the fuzzing process it is never encountered again.
01:49:27.300 - 01:50:23.368, Speaker D: And you can obviously see that this is extra important when building math libraries like in all of DeFi because let's say there's a rounding error, let's say you have a division by say, you know, there could be any kind of edge case in hyper optimized math. So ideally you want to test as much as possible the properties. Now VM state overrides it gets back to the point that I made earlier about being able to hook into the VM. Basically in Hard Hat, whenever you want to do some state of a ride you would send an RPC request called Hard Hat underscore something. One example is impersonate account, another is set code, another is set storage. Here, instead of doing it over RPC with specialized calls we have a cheat code which we call it, which lives at this specific address. And this is for historical reasons because Foundry was developed after a tool called Daptools, which was before it.
01:50:23.368 - 01:50:55.680, Speaker D: And they also had it the same address. So the way that this works is that you have your contract and you define this, you define the interface. You would import the interface, typically from the standard library that I mentioned. You would instantiate it at the chipcode address and then you have access to all the chipcodes and I have the chipcodes here and you can take a look at them. But this very simple chipcode called Warp. And again, for extra context, these are all MakerDAO inspired names. That's why they're like three, four, five letters with very mimetic, let's say, naming.
01:50:55.680 - 01:51:53.700, Speaker D: And you can see here that you can do VM Warp number and we'll override the timestamp to whatever value you want. Now, this, for anyone that has tested governance contracts, using compound, was a big pain in the ass because you had to mine like thousands of blocks to advance two, three days in the future. And there's another cheat code called Roll where you do VM roll and you can skip to whatever block you need without waiting like a minute for 40,000 RPC requests to go through for mining 40,000 blocks. So take this and imagine that there's like 30 cheat codes like this which let you plug into any kind of VM state. It starts to get very, very powerful. The compilation pipeline is the fastest in the industry. We beat hardhat consistently on all benchmarks and across open Zeppelin contracts and V Three core, unisoft V Three core, we see that the more caching involved, the more they converge into the same value.
01:51:53.700 - 01:52:39.856, Speaker D: And that is because simply there's no more salty invocations or there's just no compilation going. It just says cache, don't do anything. But one very interesting thing is that defaults in code bases matter a lot. So open Zeppelin contract is written in JavaScript, which means that it has low overhead of just starting the interpreter. Whereas V three core is written in TypeScript and it has a bunch of plugins which are required to do normal operations on a daily basis, which means that anytime you do yarn test, it takes a long time until it starts to actually compile. It just doesn't reach the code path. So you see that even though this is fully cached, like in Forge, it is instant because it just says run the compiler.
01:52:39.856 - 01:53:20.384, Speaker D: It doesn't see anything. It takes like half a second or even less. Whereas here it launches the interpreter, transpiles, loads the plugins and then gets the code path about the compilation. So the point I want to make here is that the benchmarks that are affected by third party non compilation related stuff, but at the same time, defaults are the most important thing. And if we're looking to benchmark what users experience, like if I need to configure your repo to compile faster, maybe you should do it. So for testing, I think we have very, very impressive results specifically compared to Dab tools. On certain benchmarks, we got a solid speed up.
01:53:20.384 - 01:54:04.428, Speaker D: You can see that going from many minutes to under a second. Your productivity is just completely a different world versus Hard Hat. We rewrote some tests from V three periphery in solidity and we saw an over 16 x improvement. I think this would be more if we had fuzz tests and even more tests because our testing times, they're kind of invariant because we parallelize so much like the test cases, they are maximally parallel. So they scale sublinearly with the amount of tests that you add. Whereas Hard hat and all the other tests, they don't do that because they defer to Mocha, which is not that fast. But the most important thing here was comparing it against hosted services.
01:54:04.428 - 01:54:51.192, Speaker D: So all of this, they give you simulations. You can do it with Block Native, which has a simulation service, you can do it with Tenderly, which also has a simulation service. And these are local. But the most interesting thing was when using a remote node, literally, the way that we fire our forked mode RPC request, let's say that you're testing against mainnet and you want to ensure that some pool is liquid or that the MakerDAO shutdown function works correctly. In this case, the convex shutdown works correctly. A request without caching on a remote node, they're faster by solid amount, I would say, when using a local node. So instead of using infuria, where I'm using localhost 85 45, we're still a few times faster.
01:54:51.192 - 01:55:16.332, Speaker D: And when we have caching, that's when the true speed is shown, where we take half a second. Next fastest is Block native with 3.5 and then tenderly with whatever, and then you see how it goes. So it's very fast. And speed translates into productivity, productivity translates into money made and money saved. So good. Now let's get into runtime introspection, which is very useful when debugging.
01:55:16.332 - 01:55:56.672, Speaker D: So when you add verbosity arguments to your forge test command and M just says run only test owner tests, it will proceed to give you this nice structured call trace. Now, what this means is that a green call, it's like I have a contract called GM has a function and then inside, in blue, it will always highlight cheat code calls. In green it will call our successful calls. This is the return value. Here it has nothing. Here it says Good morning, it highlights it. If it's a static call, if it's a delegate call, if it's a call code, and if it's a reverting function, it will paint it with red.
01:55:56.672 - 01:56:23.470, Speaker D: But one thing to notice here is that even though you see here it reverts, it propagates the reverse message up, but here it doesn't revert. And this is because this specific contract uses try catch under the hood. So the error is handled at a lower level. But if it didn't handle the error, this array would have been red as well, and the test would also have failed, probably. And you can see that this is like ridiculously fast. It's like 500 microseconds. It's nothing.
01:56:23.470 - 01:56:44.140, Speaker D: Now, the interactive debugger is something that many people have asked, and it is what you expect JK to go up and down. You scroll Opcodes. It has a source map. So it goes through the source code and it knows where you are and highlights it. It shows you all the values in the stack. And bear with me for how it looks like here. It's because it's a bad screenshot.
01:56:44.140 - 01:57:09.628, Speaker D: It shows you the memory. And yeah, as you keep stepping through, this is very useful when Gas Optimizing shows you gas reports. So this is a standard feature that has existed for a while in every framework. So we're like, probably we should add it looks quite good. It's very easy to install. You curl this link, you open a new terminal, or you source your file. You run Foundry app.
01:57:09.628 - 01:57:35.372, Speaker D: It downloads about 15 megabytes, approximately. And then you immediately have it installed. There is no cargo requirement, no NPM. Everything is full cross platform. You literally you unpackage a new M one, you install it, and you're set in 30 seconds or less, actually. So the onboarding experience is crazy good. All right, so future features, faster and stabler, most importantly.
01:57:35.372 - 01:57:59.108, Speaker D: So there's obsession with speed. And we're not going to settle because we can just do it better. Stabler because we don't have a weekly or a biweekly release. We actually do nightly releases just because the project is moving very fast. There's a lot of productive developers on it. And it also creates a nice vibe around it that these people ship. We're adding new commands.
01:57:59.108 - 01:58:43.000, Speaker D: The most important probably is this one, the scripting and the deploying. Right now, the deployment experience is not good, and it was not meant to be good because we were focusing on the testing experience. Now that people have started to actually use the framework, we are adding deployments in Solidity, which means that you write a solidity smart contract and you can use the same code to test your deployment in your tests. And then this code can also generate transactions, which you can post on chain and would manage the lifecycle of transactions. It would broadcast them all in parallel. It would escalate gas prices. This is all infrastructure that we already have from Mev traders that use the underlying libraries under Foundry.
01:58:43.000 - 01:59:13.404, Speaker D: Forge script the same, but not for deployment, just for doing chain operations. Let's say I need to deploy a governance proposal, or I need to do a vote, or I need to buy an NFT or something. Forge FMT and Forge Lint are what you would expect. FMT is a formatter. So we're going to replace the prettier and prettier Solid deployment, if I'm not mistaken. Forge Lint, it would be something like it gives you all the lints that Solhand gives you. It would maybe incorporate Slyther from trail of bits.
01:59:13.404 - 01:59:58.860, Speaker D: It can do whatever we want, honestly. So we'll see what we do there forge Doc, it would take the Natspec comments and it would generate documentation pages for it. This would be modeled after Docs Rs, which is I think the greatest documentation website that exists. And now we're also almost releasing Anvil, which is the hard hand node Ganache CLI equivalent because people want to integration know you're writing a front end and you want to test your code with it. So you just want the testnet node. And if we can deliver a testnet node that's better, why not code coverages? I guess everybody in the room knows what code coverage is. So we are just going to add a flag that says Forge coverage.
01:59:58.860 - 02:01:03.220, Speaker D: It runs all the tests and it highlights to you on the editor green lines, red lines and with other frameworks this was not possible because the testing was not fast enough to generate you the coverage report. The coverage report takes like a minute or 30 seconds or whatever to produce on the JavaScript frameworks because it bloats the bytecode to inject to instrument it and ends up taking a ridiculous amount of time. So it's a very hacky way of doing it and that's why it's slow, that's why you cannot have a good UX around it. Other ideas that we have are invariant tests. So instead of fuzzing one function, you fuzz many functions in a row and you check a property each time. So one example would be is the uniswap pool balanced? And you make one transaction with random arguments, it checks the environment. You make another function call with random argument, it checks again and you keep doing that and this is what enables you to explore more states in your smart contract and maybe find a bug if you have written good properties.
02:01:03.220 - 02:01:43.280, Speaker D: Flame graphs are again what you would expect. There's like a nice diagram. Tenderly also has this I believe they're a nice diagram which shows where was most of your gas spent and use this when optimizing so that you know where you should optimize and more languages. Some people have been asking about it like we haven't seen enough demand yet, but maybe for Viper, maybe for Faye, we'll see and that's it. You can find the page here, you can find the here. So you click it, it takes you here and the book, it has all the stuff that you may need. So happy to answer any questions and thank you for the attention.
02:01:43.280 - 02:02:31.744, Speaker D: Yeah, the gas reports for fast tests, we do not print them now because they're going to be different every time. If you feed random inputs to a function, it will generate different gas prices, it will generate different gas values because of how cold data is priced at the minimum. Yeah, when you say that you want.
02:02:31.782 - 02:02:37.972, Speaker B: It stabler because you're shipping every night, is it dramatic change or just a small change?
02:02:38.026 - 02:03:10.560, Speaker D: I mean, I think it's stable enough, but if it's released every night, there can be a bug that goes through. But I prefer it that way rather than having to bother with bug detected. Yeah, we'll fix it in three weeks. No, we'll fix it tonight. So, yeah, you mentioned support for other languages.
02:03:12.600 - 02:03:23.764, Speaker C: Yeah. So you mentioned possibly adding support for other languages such as Viper or Faye. Has there been any consideration to add support for something like Cairo?
02:03:23.892 - 02:03:58.480, Speaker D: Yeah, we've talked about Foundry StarkNet, but it requires more work because we also need a different VM. So we need a rust Cairo VM. There is one, but it's not there yet. So if we get a VM, then writing bindings the compiler is easy now that we've done it once. And then it's a matter of putting the two together. Foundry is not that complex of software. Like, if you have a VM and the compilation pipeline, they get combined, but the VM and the compilation pipeline individually can get complex.
02:03:58.480 - 02:04:02.070, Speaker D: So if somebody gets us a rust VM, it should be easy.
02:04:02.440 - 02:04:03.350, Speaker C: Thank you.
02:04:05.720 - 02:04:09.400, Speaker D: And again, we do the same techniques instrument the VM call traces.
02:04:16.910 - 02:04:23.878, Speaker C: Hey, I was just wondering, what do you think is missing compared to hard hat? Compared to hard hat?
02:04:23.974 - 02:04:25.610, Speaker D: Compared to on what axis?
02:04:26.770 - 02:04:31.646, Speaker C: Just like in terms of DevTools, if you think about anything that is available.
02:04:31.828 - 02:05:11.434, Speaker D: I mean, we built it to give you a better experience than hard hat. So in my view, faster, easier to install, no context switching. I think there is a world where both exist, right, obviously. And right now you can have a repo, which is a hybrid of the two. You have both JavaScript and what do you call it? And solidity tests. Like, if you go to this visible yeah, if you go to this Contracts, you see how it still has all the JavaScript tests that Uniswap has. But if you go to Contract Foundry tests, you see the tests here.
02:05:11.434 - 02:05:56.540, Speaker D: And just to give you an example, when you test libraries, for example, just to give you an example of the UX improvement, you know how you need to write a wrapper contract to library test and then deploy that? And it's like a bunch of boilerplate that you don't want here. You installed, you just import the library and it just runs it. It looks really clean, in my view. No helper libraries, no nothing. So it's quite easy to keep both aside. But I think what most teams I've seen do is that they write the ad Foundry and they start migrating some tests or new tests or new functionality. I think there was one on back.
02:06:02.190 - 02:06:15.870, Speaker C: I just had a couple of questions regarding the Linting and adding support for Slitter. So would it be some sort of extension that we add to Visual Studio code? Or do you have any. Plans to directly replace.
02:06:17.970 - 02:06:49.434, Speaker D: Could. So there's two ways to add the support for something, right? You can either just copy it and re implement it or just shell out to it. So you assume that it is installed and just call it from your system. We've generally avoided introducing runtime dependencies because it ruins the user experience. You just download one thing and battery is included. So if we can do it natively, we'll do it. But I also acknowledge that Slyther is a very sophisticated piece of software right now.
02:06:49.434 - 02:07:23.800, Speaker D: It covers a lot of edge cases that I don't know how to detect right now. So if we were to do, it would be a big project. So we need to think about it. Slither supports Foundry right now, though, and there's a GitHub action for it as well. So you literally do Foundry GitHub action, slider GitHub action, and you get the most out of it. So it might be the case that, like forge linda is stupid, and we should not do it. All right, thank you all.
02:08:05.180 - 02:13:32.350, Speaker A: Hate it so we it so we got it we so we got it mrs. Cole mr it with a die with love with a die but what I can miss with a die with a die with sam SA sam jam.
