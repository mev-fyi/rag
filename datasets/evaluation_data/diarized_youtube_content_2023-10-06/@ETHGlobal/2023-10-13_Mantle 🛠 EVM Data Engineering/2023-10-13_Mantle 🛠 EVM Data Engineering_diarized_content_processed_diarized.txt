00:00:14.760 - 00:00:15.310, Speaker A: You.
00:00:22.330 - 00:00:34.600, Speaker B: Hello everyone, and welcome to the ETH online 2023 Mantle Workshop EVM data Engineering do it yourself edition. Joining us today is Lib, who will be taking us through this session, and with that I'll pass it over to lib to get the session started.
00:00:37.290 - 00:00:57.754, Speaker A: Awesome. Thanks Rory. Yeah, awesome to see everyone and all the ais joined the Zoom meeting. Great to have you all here. Welcome to EVM Data Engineering DIY Edition. So yeah, let's get started. So GM, GM gum gum.
00:00:57.754 - 00:02:01.190, Speaker A: So this is my Twitter handle at lipm. I'm currently an engineer at Mantle, so I get to play around with a couple things, one of them being data generated from the EVM. And so for a bit of context, so I attended the Rust exeterium conference held by paradigm in San Francisco. And so one talk that really caught my attention was the data endgame. So keep in mind this was mostly a rust conference, but the tools introduced were mostly rust based, but they didn't need to be rust. It was just that it was incredibly insightful to me to see how modern data engineering tools are being done. And so I just thought I would share my experience in the last two to three months on how I use the processes of using tool and my experience using it, and how to actually use it, how I found it to be used most effectively.
00:02:01.190 - 00:02:46.846, Speaker A: But anyway, paradigm sufficient for this is that the data endgame should be fast, cheap, and effortless on any data, any format, on any hardware. So in the conference, three tools were introduced, ref, also known as Rust ETH, which is inspired from Go ethereum. So rust. Ethereum is just an execution layer node that you can connect with your consensus layer, something like prism. And then they also introduce a data extraction tool called Cairo, which is super interesting, and we can kind of dive a little bit deeper on that later. And then Paulus. So paradigm didn't actually make Paula's.
00:02:46.846 - 00:04:01.082, Speaker A: Paula's is actually a third party library made by an open source third party library, but it is a tool that will be heavily used in this talk and throughout the examples. So a very interesting thing that came out of paradigm's talk in my point of view, was that a lot of the full archival nodes used to be 14 terabytes or above, but with the introduction of ref and Aragon, this database size has shrunk down to two terabytes, which is almost an order of magnitude. And it will be an order of magnitude, sorry, it is an order of magnitude, which is kind of insane. So even the RPC throughput latency and success rate looks really promising. Now, I've had some issues with it in the past, but you have to keep in mind that ref kind of is in beta version. Yes. So having the database size go from 14.5
00:04:01.082 - 00:05:53.742, Speaker A: terabytes down to something less than two terabytes makes things very interesting, because now running a full archival, and when I say a full archival, I mean the history of the entire blockchain is accessible under two terabytes, is really a big game changer, because now it being under two terabytes means you can host it in the cloud for, if I'm not mistaken, an order of magnitude cheaper, and you can also host it on higher end laptops on your own. So a little bit disclaimer is that ref is actually heavily inspired by Aragon. And you can actually read about this in the acknowledgments in the ref section. So, for example, Aragon introduced MDBX as the database of choice, which, surprisingly, is maintained by one russian guy, I think. But anyway, a lot of these incremental improvements kind of turn that database size, reduced the database size down to less than three terabytes, which is insane. But yeah, the key thing, I guess, the takeaway from that chart is that it is now possible to run an archival Ethereum mode, like a full archival on higher end consumer laptops and relatively cheap as a business on most cloud providers. And I think if you guys are on the ref Telegram right now, there is a discount code for one of the cloud provider companies that is being shared around.
00:05:53.742 - 00:06:55.186, Speaker A: So if you like to just play around and sync up your ref node, I think, yeah, there's a coupon code lying around the telegram, but I don't have links to the telegram, so you're going to have to use your Google skills to figure that out. But yeah, going back to the original point, we are living in the information age. Data is the new oil. But like oil, data is valuable, but useless if unrefined. In my opinion, archival notes are the equivalent of modern day oil reserves. Think of all those UNISoP trades, all those UniSop v three strategies, all those positions on ave long, short, delta, neutral, all those yield farming you've done, those are all information. And with enough information you can get a sense of idea of what's happening on defi land.
00:06:55.186 - 00:07:25.674, Speaker A: But that's impossible to do if you don't have the raw data. And this is how you can refine the raw data. So you have some note. And again, this is all taken from the rust and Ethereum conference. I just happened to be playing around with it and I really liked it. I found it quite confusing, especially because there's so many options here. Like you have your ref node and then you can go from ref to query results through something called alloy.
00:07:25.674 - 00:08:18.750, Speaker A: What is alloy? You have ref DBPY, you have ref indexer, you have Cairo, you have ethers, you have etus Rh, or you can go to something called a sanitizer. Well, it's not sanitized. You can go into like pocket or delta through Cairo and ref indexer. So something I would like to focus about today is just a small section of this. This would be the local files, Cairo as well. As you know, the first time I looked at this slide, it was quite confusing to me. Like what is pocket and what is Delta? But luckily anyway, yeah, pocket and delta, don't worry, we'll kind of go through what these new terminology means together so we won't be navigating in a dark forest.
00:08:18.750 - 00:09:15.950, Speaker A: So let's say hypothetically you decided to try out ref, you downloaded a ref and now you're running ref. Cool. So you have basically achieved the very first step in running and running your own data refinement or your data engineering DIY edition. So we'll give ref roughly about two days, two to three days to sync, depending on your location. And then now that it's synced, hypothetically, we would like to extract it out. We would like to use the tool called Cairo into something to extract it out into Pocket plus delta files. Now, I know that there's a lot of terminology here, like what is a pocket file? What's a delta? Those sounds like some homemade jargon, but good thing for us is that we have a friend called Chat GPT, and this is essentially what it's returning.
00:09:15.950 - 00:10:14.750, Speaker A: If you're wondering what pocket is, pocket is simply another data format in data engineering. And so in these days you would use pocket over. Most companies that deal with scale would use pocket over CSV simply because it's more efficient to store them as a raw format. For example in let's say s three, where you're being charged with by the gigabyte and it's more efficient to query them. And you don't have to worry about escaping commas or any kind of weird things. The only issue with pocket is that it's not human readable and you have to use a tool to consume. Yeah, but this is like the comparison between pocket and most file formats.
00:10:14.750 - 00:11:15.346, Speaker A: For example, we can ignore column compressible splittable. We'll only focus on readable and complex data evolution schema. Evolution is sorry, complex data structure such as like arrays, JSON data if you really wish to obviously unpass. And then you could have more complex data like strings, which is usually quite a pain in csV, especially you have commas. Pocket is not readable, but it fulfills a lot of the other criteria. Again, just to emphasize, packet is just a data format. Something I would like to also emphasize is that a lot of the data scientists out there might be, or data engineers might be quite hesitant to switch to these new tools because it's rust, and I'm familiar with Python.
00:11:15.346 - 00:12:30.846, Speaker A: I don't want to leave my comfortable pythonic land. Something I would like to emphasize is, even though the tooling is mostly rust based, there are Python rackets available. So if you are a Python scrub like me, and you have not been audited by life through the rust borrow tracker, do not fret, because most of the tools apart from Cairo has a python implementation to it, especially the data engineering bit. The data extraction bit requires a bit of rust, but since the hard part of rust, which is dealing with mutexes and asynchronousity, has been handled by the paradigm team, it's quite trivial to extend on Cairo. Anyway, if you haven't been hardened by the borrower checker and you would like to play around python, that's also fine. So let's say hypothetically again, you've got the ref node up and you've decided to run Cairo to extract your data set. You got a couple of data sets available for you, such as logs, blocks, transactions, traces, storage disk and so on.
00:12:30.846 - 00:13:36.900, Speaker A: So these are inbuilt data sets that paradigm has gracefully provided for us to play around with. So for example, each data set correlates to different kind of raw data that you would like to extract and organize, but not sanitize or transform in any way. So these are raw data from your node. For example, the logs only extracts out the logs, and it gives you a bunch of fields such as block number, transaction hash and the topics, and even a transaction index, which is very useful later on if you would like to extract out block data. For example, it just gives you like block number block hash, the builder of the block, sorry, the extra data which usually correlates to the builder. I'm not sure if it gives you fees, but it gives you the base fees. So each data set correlates to a distinctly different kind of data you would like to deal with.
00:13:36.900 - 00:14:39.206, Speaker A: Storage difference gives you the change in slots, internal slots on the EVM and yeah, anyway, so let's say hypothetically we managed to run Cairo and extract it out into a pocket file on a local directory. We're all storing this in local files because we're too cheap to pay for AWS, s three, or even any kind of blob storage from the cloud service providers. Hypothetically, we've got a bunch of bounds, deltas, blocks, logs, prices. Now, this is my own take on this storage difference in transactions. How can you read them? Because it's not human readable. Right? Well, very true view, actually. You just use Paula's, which is actually a rust based tooling, but they have a python wrapper for you to play around with.
00:14:39.206 - 00:15:17.230, Speaker A: So you can import pawlers as usual in Python and then read the pocket file. And this is the kind of data you're looking at now. It's being stored in binary data, but it's fairly trivial to transform them in Python. And just looking at this, we got about 100 blocks from this, and from the 100 blocks we can see about 12,000 transactions. And Cairo pretty much just extracted this in real time. There was barely any wait time. But keep in mind I'm running my own ref node.
00:15:17.230 - 00:16:06.670, Speaker A: If you guys would like to run this against public nodes, you have to be a little bit careful because you will get rate limited. But I've seen this is very new, which is why I haven't added to this slide. But if I remember correctly, I think someone else wrote a load balancer for this, also in rust. So you can effectively point to multiple RPC providers while extracting your data. But personally, I think that I would prefer to extract data from my own node because I know that it's more reliable and I know that it's a full archival node. Okay, so the problem with this, as you guys might have pointed out, is that exploring the data is actually quite cumbersome. I can't just read the first ten lines of the file.
00:16:06.670 - 00:16:24.282, Speaker A: For me to have an idea of what's going on, I have to run a Python interpreter. I have to input pause, and then I have to type out this long s name. Ethereum. Underscore, underscore transactions. Underscore, underscore block number. Exactly what block number two. Underscore two.
00:16:24.282 - 00:17:32.734, Speaker A: Underscore the end number pocket, which is very tedious. I don't know if you guys like typing and you want to bump up your words per minute, that's great. But for me, for a scrub, for a boomer like me, I don't think my fingers are not as agile these days. But luckily not storm, who has graciously provided us with this tool has introduced this alias, PPQ Python parquet I presume, where all you have to do is put this alias into your bash rc file or your profile config file or something, and then you can now instantaneously explore your data. Now the goal of this is to make data exploration as frictionless as possible, because you don't want that slight mental hurdle to prevent you from exploring your data. It's like going to the gym. You don't want to think about it, you just want to do it.
00:17:32.734 - 00:18:35.426, Speaker A: You want to make data exploration as easy as ABC. So there are certain ways to kind of streamline your workflow, and that's with the help of aliases or whatever you guys have been finding useful. So Polis has a very simple API. It composes really well with. It's slightly different from pandas. So for you guys who are coming from pandas, this API might be a bit different, but I highly recommend that you check it out because things are very intuitive, very simple, they're kept especially simple so that things compose really well and they've made a really big effort to try and name things correctly, which is always a big plus because you know what it's doing behind the scenes. So for example, in this case, if you would like to do for you SQL guys out there, if you would like to group everything into a chunk and aggregate things, this is how you do it.
00:18:35.426 - 00:19:44.154, Speaker A: So in this example, I'm grouping this whole transaction, 12,403 transactions, and I'm grouping them by recipients, and then I'm aggregating the transaction hashes into a list of transaction hashes. And then I'm counting the transaction hashes and sorting them by the setting order. So as you can see in this example, the first two address seems to be the most popular transaction in the last hundred blocks between block 18,255,000 to 18,255,099. So that two address, unfortunately I didn't make this human readable yet, but it's fairly trivial to do so. But you can see that the two address had about 1000 transactions to it, and then so and so on. So this API is incredibly easy to compose. And trust me, if you just spend like half a day looking at it, you'll be so productive with it.
00:19:44.154 - 00:20:36.950, Speaker A: It is honestly crazy how productive I've been with pause as opposed to pandas, even though you could both use arrow like, you could both read pocket files behind the scenes. But Paulus is just so much cleaner that I don't have to google a lot of things. I just type it out and autocomplete helps me out anyway. And in this example, you can even do intersections between lists. So, for example, I'm grouping the transactions by the block number, and then I'm calculating the unique addresses within the block. And then I'm trying to find the intersection between the addresses between the block numbers, which is kind of cool. So I can see that some addresses have more collisions than others, which is probably like sex to Dex.
00:20:36.950 - 00:22:20.940, Speaker A: Sorry, sex or Dex transactions. And yeah, Cairo is actually extremely easy to extend for anyone who plans to extend on Cairo. The downside is that, well, you have to write some rust, but the positive side is that you can also just write solidity asterisk tm. So, notice how I showed you guys a custom field, custom pocket file called prices just then? Well, that's actually a custom implementation that I added onto Cairo, and I wrote very minimal rust. In fact, I just copy pasted most of the code. But because Cairo extracts data on the blockchain, what you could do for anyone who's seen my other talks is that you could use something called arbitrary code execution for data retrieval and implement it onto Cairo, whereby you write some solidity code. The TLDI is that in the constructor of your contract, you return data, because normally you don't return data in your constructor, but you can hijack the constructor to return data using assembly, and then you can write some kind of arbitrary, your arbitrary business logic or kind of data engineering pipeline to extract out data from the blockchain and then return it to Cairo of which could be processed without writing any additional stuff.
00:22:20.940 - 00:23:23.598, Speaker A: So the idea is that you write solidity code, compile it down to bytecode, and then you just send the bytecode through Cairo to your node, which returns you a bunch of data. Remember, it's just an ETH core at the end of the day, because you're actually fake deploying the contract to return the data. You don't actually deploy the me from there. You have the raw data, which you can then insert into a pocket file, which paradigm, again, like Cairo, has done the heavy lifting for the asynchronicity, the chunking, the mutexes has all been done. All you have to do is consume data and return data. So extremely easy to extend anyway. Yeah, so TLDR, the custom data retrieval, works by returning data in the constructor, and so you can extract data by deploying, put open quotations on deploying because you're just deploying contracts in e call.
00:23:23.598 - 00:23:57.718, Speaker A: So you're not actually deploying anything. It's a read only contract deployment. This roughly looks something like this. You have a bunch of prices, and then you get the prices, and then you kind of encode the data in some bytes, and then you return the bytes in assembly. I'm not sure how you can do this in Viper though, but this is how you could do it in solidity and you 100% can do it in half. But yeah, with ecore user you can basically extract and organize data in an ad hoc manner. I call this JITD.
00:23:57.718 - 00:24:24.526, Speaker A: Just in time data. So if anyone wants to use this term, please call it JITD. Thanks. Anyway, to summarize, I know we're running a bit late on time. We want to give like a few minutes for questions. We can use ref to have the raw data Cairo to extract them, and we use Paula's to refine it. That's like the simplest method I found, and probably the most effective.
00:24:24.526 - 00:24:47.900, Speaker A: I know there's a lot of tooling out there and you guys might get very overwhelmed by the choice of tooling, but if you would like to dive into data engineering, these are the only three tools I've been using and I've been fairly successful so far. But yeah, thank you everyone for listening to me ramble. I think we have 5 minutes for questions. Yeah.
00:24:51.310 - 00:25:02.400, Speaker B: If anybody has any questions, feel free to type them in the chat or take yourself off mute and ask directly. And yeah, we do have a question. Is it common for someone to build an indexer with ref?
00:25:04.450 - 00:25:52.370, Speaker A: No, it is not common for someone to build an indexer with ref, but you can because the ref API is actually really nice to extend on. So you can essentially extend on ref while getting upstream changes because of how they structured it. So basically it's extremely composable. You can if you want to, but I don't really recommend it. It's not a really good pipeline in my opinion, but you could. I think the better pipeline would be to extract the raw data and transform it somehow with an intermediate step. Anyway, hope that answered your question.
00:25:54.500 - 00:26:23.414, Speaker B: Are there any other questions? If not, well, thank you very much, lib, for the great workshop and presentation. And everybody else, this is the last workshop for the day, so have a great rest of your day, and we will see you tomorrow for a summit, actually, so more information on that coming soon. Cheers everybody. Thank you.
00:26:23.612 - 00:26:25.090, Speaker A: Thank you, bye.
