00:00:06.250 - 00:00:21.040, Speaker A: Awesome. Up next, we're going to have Proto come on and chat about pluggable data availability. We're really excited to have the king of the pineapples here. So, Proto, when you're ready, feel free to turn on your video, unmute yourself and share your screen.
00:00:21.730 - 00:00:55.278, Speaker B: All right, let's get started. Sorry for the delay. So, my presentation is about applicable data availability. With optimism. We're both stewarding layer one development of increases in data availability as well as building the technology that uses data availability called rollups. And so I'm going to expand on Fire for Fire, also called prototype sharding. That's me.
00:00:55.278 - 00:01:48.320, Speaker B: And with Op Labs, we're building this roll up technology and building towards the bedrock release, which uses this data availability. So what does this talk about? Data fed ability? Layer two usage word for far and then some layer three design. At the end, you can plug and play and build something new. So what is data fedbility really? Data fed ability is this primary scaling bottleneck of ethereum. Over time, the sharding roadmap of ethereum has changed a lot. Early on, sharding looked like this thing where we had many, many different shards and they would all have data as well as execution. Over time, we realized that having so many shards would create communication problems if you also try to solve the exclusion problem.
00:01:48.320 - 00:03:07.130, Speaker B: And so instead of trying to solve everything at once, we have this realization where on layer one we can just focus on data, securing data and enabling layer twos to create this competitive environment where the execution layer essentially is out of protocol, but secured by layer one. And so how does this work? The inputs to the functions are secured by layer one. The outputs are a layer two concern. And so what we get from securing the inputs is this permissionless ability to reconstruct state. And this allows any Hanas actor on the layer two to compute autolid state and then contest what the layer two output is on layer one and then secure the layer two. So a roll up is really just this combination of data availability and this execution check the thing that secures the withdrawals going back from layer two to layer one. We have various different types of execution checks that are also called validity proofs or fault proofs or previously fraud proofs.
00:03:07.130 - 00:04:19.458, Speaker B: So if the sequencer makes a mistake or if there's a malicious sequencer, they might post wrong output, they might post a state that's not actually valid. And then you have to prove them wrong on layer one to secure the layer two outputs and secure withdrawals. Secret roll ups have a validity proof where you prove with secret technology that the roll up performed a certain state transition. Given layer one inputs, optimistic roll ups, they defer the proof. And basically only when there's contention in the Pessimistic case, then you play this interactive game on layer one to prove or disprove the output. So with layer two, we only really need this data availability for other honest actors on the network to reconstruct the state. Reconstructing the state is very easy if the state was only ever changing like once per month and very very slowly, and everybody agreed on this date.
00:04:19.458 - 00:05:18.790, Speaker B: But the challenge is that layer two is quickly changing, new transactions are being confirmed every second. And so all this data that's changing needs to be made available in a permissionless way where we do not rely on the sequencer. And this is what layer one offers, this is data availability, hosting data that's changing and that cannot be relied on from just one actor from the sequencer, but should be available to everybody. So if you think about modular blockchains, there's this narrative coming along. We're taking part a stack and you see this on layer one with the separation of consensus and execution and other blockchains. This enables us to encapsulate complexity and to enable scaling. So, abstractly, this looks like we have a data provider and this layer two for derivation execution.
00:05:18.790 - 00:06:16.520, Speaker B: This is the simplified version, a little bit more complicated is the reality. So we have deposits going or like layer one messages that change layer two state from layer one to layer two. And under is this refers we have proposed transactions which you can think of as an oracle, to state what the layer two state is and to enable withdrawals. So we have both data communication as well as this communication for Liveness and for this messaging across layers. And then the layer two is designed the same way as layer one with a consensus layer and an execution layer. And so if you think about the layer two state transition function, it's really just a process of incorporating more layer one data to extend to layer two. And then we can prove those layer two outputs on layer one.
00:06:16.520 - 00:07:19.190, Speaker B: And so often I see confusion about the proof of stake finalization and the roll up finalization. Before we had proof of stake, we used to use finalized for layer two data, that is proof that is proven on layer one. But now that we have proof of stake, we also have this notion of finalized data on layer one. So if you think about a roll up as a function that consumes inputs well, then the function is not going to referred if the inputs are not going to referred. And so when the inputs are confirmed on layer one, then the layer two can also be finalized. But it will take some more time for the layer two to also be secured on layer one for withdrawals. This is this light client type of thing that the oracle I was talking about to enable withdrawals, they're separate things.
00:07:19.190 - 00:08:07.734, Speaker B: And we're focusing here on just vendorization of data because we're talking about data availability, securing what is confirmed. I'll give you a little bit more technical fuel. So we've been running testnets of the next petrol upgrade. Here's an example of a testnet with two replicas and one sequencer. And then what we're tracking here over time, over the span of 3 hours, is the traversal of the layer one chain. These are these colored hashes and every colored hash is a different layer one block. And then a little bit down the graph, you also see the layer two chain being traversed and the layer one references of the layer two chain being traversed.
00:08:07.734 - 00:09:12.190, Speaker B: And so, over time, we consume new data and I'll talk more about this data availability. As we're consuming more data, we basically need more capacity to host more users. So how do we get more capacity? We need to scale the layer one data availability. The sharding roadmap has been split up into incremental phases. This has been done before. And now for the latest sharding roadmap, we have this idea of Dunk sharding based on Dunk Cloud from the Ethereum Foundation, basically championing this idea of data availability sampling, where you can distribute data more nicely between many, many different network nodes. For the short term, we do want data and data increase, but we are not there yet with the network layer on layer one to host a thousand times increase in data like that.
00:09:12.190 - 00:10:26.070, Speaker B: Or we're not ready for data availability sampling yet, but we are ready for a smaller increase. And so this is what Fire for Four is about, is to make this future compatible increase in data availability, going from the approximate 50 to 100 KB data consumed per block now today, to somewhere closer to a megabyte to two megabytes per block of data for roll ups to consume. And so this is what it will look like if we had for four with the consents layer exclusion layer. Then we have this new data layer essentially attached to the layer one where we're hosting these blobs. And these blobs, they function as inputs to host the layer two EVM activity, lifecycle of a blob or this piece of data that layer two transactions are confirmed in is like this, where we have the user creating a transaction. The roll up operator bundling the transaction. The layer one transaction pool basically propagating the transaction until it can get confirmed.
00:10:26.070 - 00:11:05.750, Speaker B: And then the layer one confirming the bundle of layer two transactions as this blob of layer two content. We think of this as a sidecar. It's something outside of the regular beacon block. It's synced separately. But it's this condition where we ensure that the data is available along with the beacon block itself. And then the exclusion payload of the beacon chain stays in layer one. But Blobs, they are pruned after they have been available for a sufficient amount of time for layer two to secure their network.
00:11:05.750 - 00:12:29.790, Speaker B: And then it's up to the layer two to persist the historical state, which basically only changes once per month for other layer two users to sync from. So how do we adopt this kind of new, more scalable type of data. We have this derivation pipeline which transforms the layer one inputs into layer two outputs into the layer two blocks as they are processed by the Exclusion engine. There are different stages that we designed as part of the Bedrock upgrade to modularize this transformation. And so traversal, retrieval and deposit processing, they're all separate processes. And we can swap out the data retrieval process for one that supports the new EIP. With Eve Berlin, we've been doing exactly this, this hackathon of just like a week ago or so, we built a prototype where we took optimism, data, Optimism roll up and implemented the changes necessary to use the four data.
00:12:29.790 - 00:13:26.530, Speaker B: And so Prism we extended with a retrieval API to fetch the Blobs. Gaff now combines four and layer two DIVS so we can use all the new changes in the same code base and on the op node. We derive data from the blocks and submit new blocks that are being built by the sequencer. Then we have a full layer two deployment that looks like this. We have a layer one beacon node, layer one execution engine, layer two roll up node and a layer two execution engine. You can see the similarities here where they both have an engine API, a separation of consensus and execution. And then there are these helper modules that make the data, the inputs and the outputs available to the higher layer.
00:13:26.530 - 00:14:40.940, Speaker B: And we can take this and we can stack it. So you could think of a layer three that is basically just the same software as we run for layer two, except stacked on top of the layer two instead of the layer one. So this is like a thought experiment where what if we just reuse the same code to build a layer three and to make things even more scalable. Now, what you see here is that this only really works if the Blobs, the things that host data in layer three, are not bubbled up all the way to layer one, but rather hosted by this layer two with a more, even more scalable data layer. And so you could adopt EIP four on layer two to host a layer three. But it would also mean that you'd have to build this alternative, even more scalable data layer. And then, well, how does this compare to plasma? Or how does this compare to these scaling solutions that host their own data? Well, you could separate it from the layer two more where now you have the separate data module to fetch the layer three data from.
00:14:40.940 - 00:15:49.390, Speaker B: It's less equivalent to what you would like to see from layer one technology, but it also does the same job. And you can still stack the roll up node, the proposer, the execution engine. So all this software is the same, but then host the data elsewhere. So I've been experimenting with these different types of ways to further scale what we now know as a roll up, but then in the future as this modular system that you can reconfigure to become a layer three or plasma or so on. And this is what applicable data availability is about. Once you have this stack of modules, you can configure it the way you like and then build cheaper, better solutions based on type of application you want most. If you want to learn more about Bedrock, the merger stack we're building, and the upgrade really of optimism, you can find this on Bedrock optimism IO.
00:15:49.390 - 00:16:11.180, Speaker B: We also have a live testnet you could use. And then there is EAP four eight four, this layer one scaling effort which you can contribute to, which you can read more about on EP four and then contribute to on GitHub or in Discord. Any questions?
00:16:16.330 - 00:16:48.398, Speaker A: Thank you. Proto just checking the chat to see if there's anything that's come through. Doesn't look like it so far. Might be worth just as we're going to jump into a panel about 48, 44 following this directly. I don't know, maybe if it makes sense to maybe chat a little bit more just as sort of a preamble to that panel. About 48 four four. What it's going to maybe enable and also just sort of like any other thing you think that's relevant to know ahead of that panel with Tim and.
00:16:48.404 - 00:18:09.930, Speaker B: The rest of the team, right? With 44 four, what we're introducing is this different type of data. I think the context that many people may be lacking is how roll ups today are hosted and how this compares to roll ups in the future will be hosted today. We're using Call data, which is this type of data that passes through the EVM and was never designed to be used in this way. It was designed to be used as an input for a smart contract and then it happened to be hijacked in some way for rollups to host and make their data available. In the end, this just means that this type of data does a lot more and it's a lot harder to maintain than the type of data that we really need. And so what we're trying to design is the type of data that can be hosted and available without being unsustainable, without growing inbounded the current EVM on layer one. And all the inputs is this inbounded type of growth where the state, the history and so on is not sustainable to maintain long term.
00:18:09.930 - 00:18:42.200, Speaker B: Only when you can prune or reduce the strength, then this thing is sustainable to host on regular hardware. And so with layer two, we're trying to find this balance where we're securing the layer two, but we're not creating this unsustainable resource usage on layer one. And so by redesigning this type of data feedbility, we can make layer one a lot more sustainable, we can secure layer two at lower costs and so we can increase capacity for users as well.
00:18:47.220 - 00:19:40.370, Speaker A: Got you. Awesome. Yeah, I mean, that's good context, especially to know that right now, I think the most important takeaway there is that the chain isn't really set up to do a good job of hosting that data right now. And it's being almost kind of hacked in and really kind of the principle behind those change is actually doing it the right way versus the kind of like other way. Maybe a quick question while I have you from the chat that came through, a bit of a roll up 101 question from Abhishek, just a question around data availability says I understand that roll ups make a state route commitment to the main net. Then where is the actual data of the Spark contract stored? If it's a roll up chain, is it completely down? If it's on roll up, basically, is it completely on the roll up chain? And if so, how do I access that data?
00:19:41.300 - 00:20:54.148, Speaker B: All right, so I think I should introduce this notion of blockchain state versus blockchain data history. But you could think of as a blockchain, it's just a sequence of transactions that has to be available or accessible to people to reproduce a certain state. And so layer two makes this sequence of transactions available by bundling them, compressing them and submitting them to this layer One, which can host them with higher security guarantees. And then it's up to the users to read all of layer One and then reconstruct the state. And then as you reconstruct the state, you can find your layer two balances, your layer two interactions, everything you might need on layer two. And then when you want to withdraw from layer Two, all you really have to do from here is to make the layer two output the state root visible on layer One. And layer One, unlike a regular node, does not have its own view of these things happening outside of the EVM.
00:20:54.148 - 00:22:04.940, Speaker B: And so rather than what happens in a full node where you can run this layer two function to compute the layer two state, you need to prove the state with a Light client. This Light client is essentially a smart contract on layer One that can tell you the correct state of layer two to be used by other layer one contracts that can then process things like withdrawals. The layer two state can be proven in several different ways. This is the differences I talked about at the start of my presentation with Zika Roll Ups, optimistic roll ups, and the case of optimistic roll ups. What we do is we always post the layer two output. That only when somebody disagrees with the output. Then we build this game where a challenitor and the original poster of the output can battle out on who's posting the right output and who's posting the wrong claim or output.
00:22:04.940 - 00:22:18.260, Speaker B: And so you can secure the claims of layer two output to always be correct and to just secure the Oracle and then secure are the withdrawals.
00:22:21.000 - 00:22:38.690, Speaker A: Got you. Okay, awesome. Well, thank you so much, Proto. I think we're going to go to a brief break while we set up the panel for the state of 48. 44, but yeah. Thank you for your time, Proto, and it's always great to have you on Ebubble TV. Thanks.
