00:00:01.370 - 00:00:11.790, Speaker A: All right. Hello everybody. Thank you for being here. My name is Eric Tang. I'm the co founder and CTO at Livepeer.
00:00:14.690 - 00:00:18.800, Speaker B: I'm Victor. I'm an engineer at Livepeer Team as well.
00:00:20.130 - 00:01:02.640, Speaker A: Yeah, thank you. Thank you for being at the Live Peer workshop and thanks to ETH Global for putting on this awesome event. Today. We have about a 45 minutes workshop. I'm going to spend a little bit of time talking about video streaming technology just to set the right context, introduce you to Livepeer and the protocol that we've been building. We'll talk about video streaming for Web Three and how that's different from video streaming in Web Two. And finally we'll do some demos and tell you guys what the prizes are.
00:01:02.640 - 00:01:39.740, Speaker A: So just a quick intro about what lifepeer is. Life Peer is building the video streaming layer of Web Three. We've been building this project for the past five years. We've gone through many iterations. Currently the network processes about two and a half to 3 million minutes of video per week. Those videos are viewed by many million number of users. Just a quick overview about video technology.
00:01:39.740 - 00:02:22.954, Speaker A: Video is about 82% of all the data in the world today. 82% of all the data on the internet. Needless to say, that's the majority of the content that we consume on a daily basis. And that's why Life here is excited to be working on this technology and to decentralize the infrastructure. So when we talk about video infrastructure and video streaming technology, how is it done? There's really three steps. There is kind of the ingest and upload step. There is the video playback step, and then there is the video processing step.
00:02:22.954 - 00:03:20.540, Speaker A: We're going to go into each of those steps just quickly so that we can set the right context. So here it's kind of a typical architecture of a video application of today where it looks a little complex. I'll go through each of the steps so it doesn't look so complex anymore. The first step, we talk about video ingest and upload, right? So this is really dependent on if you're building a video on demand application. So something like a YouTube where you upload a file and have it streamable by anyone on the web. Or if you're building a live streaming application, which is something more like a Twitch or Facebook Live, right, where people are watching you in the moment as things are happening. So for live streaming, it's about ingesting a live stream, broadcasting it, and for video on demand, it's about uploading a file into a back end.
00:03:20.540 - 00:04:53.690, Speaker A: On the receiving end, usually there is a media server that's receiving the content. It speaks the language of the video protocol, speak the language for also Http if you want to upload it. And there's many different types of ingest tools that you can use, right? So oftentimes we think of, especially for Live streams, we think about you can either have a desktop based broadcasting software, something like an OBS studio which is really popular amongst Twitch streamers. You can have a mobile app that's broadcasting the video, broadcasting the live stream using the camera on the mobile phone or you can have an in browser broadcasting studio that's only recently starting to get popular and you might have heard of like a Restream or things like that but those are just end user tools. If you're a developer and you're building an application, there are a few SDKs that can help you to build those types of experiences. So for example, there's a react native component called Node Media Client that allows you to build a mobile broadcasting experience. And then we've actually built a tool called Webrtmp that allows you to build an in browser broadcasting experience that can capture the webcam on the laptop.
00:04:53.690 - 00:05:50.970, Speaker A: So that's a little bit about ingest. The ingest is all about putting the video into the backhand and now the video is in the backhand. How do you play that? So here is delivery and playback. Video delivery is oftentimes delivered through a content delivery network or shorthand CDN. And this is because oftentimes there's tens of thousands or hundreds of thousands of people watching the same content. In the case of a live stream, if you're watching the World Cup or something, there's millions of people around the world watching the same stream, right? So in that case you really need a really scalable infrastructure to be able to deliver that kind of content and a content delivery network helps you do that. Or if you don't have that many viewers, sometimes you can deliver directly from a media server and that's also possible.
00:05:50.970 - 00:07:05.262, Speaker A: But when you're delivering it, you're delivering it to a video player on the client, right? So the video player can be either on the mobile phone or it could be in a and in terms of delivery and playback, we think of HLS, which is the video streaming standard defined by Apple. There is adaptive bitrate streaming which we're going to go into later and then there's the video player. So let's first take a look at HLS. What is why, why is this thing interesting? Well, HLS is the file that you give to the video player in order for it to play, right? So the HLS represents the video itself and the format of HLS is there's a playlist that describes kind of all the different small media segments, and then there's the media segments themselves, which is the video cut up into small chunks. And then the video player simply loads each of those chunks in a sequenced way and then they're able to play it back in real time. Or sometimes if you two exit it'll, play it faster than real time. So that's really HLS.
00:07:05.262 - 00:08:14.540, Speaker A: When you're thinking about a video stream, it's actually tiny little files that are making up this big stream. So that leads us to the second topic, which is adaptive bitrate streaming. And this is the secret sauce for how video is able to play on the internet in the first place. Because if you think about this problem, we're streaming video under all kinds of different networking connections from around the world on different types of devices, different sizes of screens, right? There's so many different varieties, but yet everybody expects to be able to watch the video just the same. So in order to do that, what we do is we transcode the video into many different versions of many different bitrates. So that if I'm sitting at home watching a video on my smart TV, I can watch like a really high resolution version of the video. Or if I take out my smartphone and start watching the same video, it's going to be like a shrunken down and lower resolution video, right? And when I'm loading that smaller video, I can load the smaller bitrate video.
00:08:14.540 - 00:09:35.830, Speaker A: And adaptive bitrate streaming is saying that as you're streaming the video, you can actually change the version and the resolution of the video that you're watching without a broken experience. So for example, if you're ever sitting at home and you watch Netflix and it starts out really grainy and then it gets crisper over time, that is adaptive bitrate streaming at play, right? So the smaller version comes on at first because your networking is just kicking in and then as the video plays, the player realizes, oh, you actually have a much higher network throughput and it starts loading the higher resolution version. And this is very crucial for video streaming to work correctly online. And when we talk about video players, it's really a piece of software that you can put in your web application or you can put in your mobile app in order to play that HOS video that I just talked about. Now there are many different SDKs or products that you can use. There's open source video players or proprietary video players here, I'm just kind of sharing a few of them. The most popular open source one is probably Video JS.
00:09:35.830 - 00:11:00.590, Speaker A: Okay? So that is kind of an overview of playback. Now let's talk about video processing. Video processing is something that happens behind in the background that most people don't know but is actually a super important step, right? We talk about that transcoding step and that's how we're able to enable that adaptive bitrate streaming way of streaming video. So usually when the video gets uploaded or ingested into the media server, it gets sent to a transcoding engine that then transcodes the video and then puts it into a CDN or stores it into an object storage. The thing about transcoding is that it is super computationally intense because you can imagine videos are very complex data structures and you have to kind of decode the video, understand what it is, re encode the video into different versions, store them somewhere and that's really expensive process. So transcoding. The workflow looks something like this, right? Where you have that transcoding engine, you can, for example, ingest like a 1080p version of video and you can transcode it down into a they're all appropriate for different devices and different networking conditions.
00:11:00.590 - 00:11:52.670, Speaker A: So that's a little bit overview of video streaming, right? So all of those pieces together makes video streaming work on the internet. So now that's pretty complex for anybody here this weekend, trying to build a hack. You probably don't want to think about all those things and put all those things together. That's way too much work. So at Lifepeer, we built a decentralized solution to make all of those complexities go away and to make it really affordable. So how do we do that? Lifepeer at the core is a set of protocols. It's a protocol that allows people to contribute resources onto a network that then they can also get paid for contributing that resources.
00:11:52.670 - 00:12:49.858, Speaker A: And it's encoded as a set of smart contracts in Solidity. It's deployed on Arbitrum. It was first on ethereum. Recently we migrated to Arbitrum and it acts in a few ways. One of the most important things that it does is that it acts as a global registry of these orchestrators that represent kind of transcoding capacity around the world. So if I am a broadcaster or if I have a video that I need to transcode, I can talk to this global registry and say, hey, tell me a list of people who have capacity that can do this work for me. And through this discovery protocol, I can not only get a list, I can also kind of start testing, oh, who's closer to me, who has a good latency with me, has good connection with me, and I can start sending my video to them and they can start transcoding that for me.
00:12:49.858 - 00:13:44.180, Speaker A: And of course, the protocol also handles micro payment and all these things. So the value transfer also happens and the orchestrators are incentivized to do that transcoding work. And the other thing that the network does is that it's highly redundant. And what that means is there's lots of orchestrators around the world and there's always an overabundant amount of transcoding capacity on the network. So that if one orchestrator all of a sudden goes offline so for example, I'm running an orchestrator in a data center and a data center loses power and everything goes offline. And that's totally fine because the software is resilient enough that it'll just immediately fail over to another orchestrator and it'll continue to work, right? So even for a video live stream, it won't disrupt the experience. And the other thing that you can do is you can double up.
00:13:44.180 - 00:14:36.990, Speaker A: You can use multiple orchestrators at the same time so that if one goes away, it doesn't even matter. In fact, you can just have the two race and whoever gets back to you first, you use that one under the hood. If you look at the Live Here protocol, I'm not going to go into this whole complex graph. I just want to show this graph in that there's a smart contract. We have the broadcasters and the orchestrators, and there's a verification process here to make sure when the broadcasters are working with the orchestrators, they don't need to inherently trust each other. You can just say, I'm sending my video into the network and I can trust that the network will verify the work for me and I will always get the right results back. And if I don't get the right results back, there is heavy economical penalty.
00:14:36.990 - 00:15:34.150, Speaker A: So that it's highly disincentivized for someone to cheat. Very similar to kind of the blockchain design concept, proof of stake concept, right? If a validator cheats, then they get heavily slashed. Therefore, you don't want to cheat. Another important concept here is that there's an on chain portion and an off chain portion. Of course, when we talk about video streaming, there's going to be millions of video streams that are happening on the network, right? We can't be writing transactions for every single one of those streams. So all of the video streaming steps within the lifeture networks happen in an off chain way. And the only thing that happen on the blockchain are the registrations of the nodes, which happens only once in the node's whole lifetime, and the payments which can be batched together and happen asynchronously okay, so here I'm just going to go into a little bit of the token economics.
00:15:34.150 - 00:16:33.558, Speaker A: The way it works is that people stake their lifeyard tokens to the orchestrators. And as the Lifeyer tokens get staked to the orchestrators, the orchestrators can earn live peer rewards. And at the same time, the people who want to transcode their video with the network also paying in ether to transcode the video. So as more demand goes on the network, the more valuable the network becomes because essentially the Live Here token represents the amount of revenue that you can capture for all the revenue that's going through the network. And then it kicks off this flywheel where the more demand on the network there is, the more valuable it becomes. Then it attracts more supply and then that kind of wheel starts going. So that's it.
00:16:33.558 - 00:16:50.734, Speaker A: That's it. A little bit about the intro of live here, a little intro about video streaming. Now Victor is going to show you some exciting demos around video on demand streaming and also around live streaming. Hello.
00:16:50.932 - 00:16:52.960, Speaker B: Let me set this up.
00:16:57.910 - 00:16:58.660, Speaker A: Hello.
00:17:01.510 - 00:17:29.930, Speaker B: Right. So I want to show you the capabilities that we have in our service in our API. And we're going to start with the Live streams. So we have this dashboard page here with the streams you have in your account. I have already registered. I'm logged in here and you can create a stream from here. But you would normally be doing this from your application.
00:17:29.930 - 00:18:17.758, Speaker B: So we have an API that you can use and you got an API key. And then you can create all the objects on demand as your application logic requires. And we can start here, let me increase this so we can start here by creating a new stream. And you give it a name, I'm setting it to record as well. And here we have fully created object and it has the configured renditions that you want for your playback. So it controls how the stream is going to be transcoded. And then here are the important bits right now, which is the stream key and the playback ID.
00:18:17.758 - 00:19:13.250, Speaker B: The stream key is the secret that you give to the user that is doing the streaming. And it's going to give them right access to this stream, to this channel. And you also have the playback ID, which is the one you use for playback in the stream. And it's a little more public in that sense that many people will be watching, but only one will be writing. And here is the stream that we've just created and say this is an example application that can do video the live streaming from the browser using the SDK that we showed. And all you need to do is to copy that stream key here. So if you say your application creates the stream and then it's going to send this key to your application somehow and it can start streaming to that channel.
00:19:13.250 - 00:19:49.370, Speaker B: And then if we see here in the dashboard, the stream should now become active or maybe not. Yeah, it did. So it's still loading, starting the transcoding and everything. And while it does that, I can also show the playback application. So this is just another example that has video JS player. And all we need to do here is create this URL, which is the playback URL. You can also copy it from the dashboard.
00:19:49.370 - 00:20:59.446, Speaker B: So here is the playback URL. And in the same way in a real application, you would actually get this specific playback ID from your server, from your back end and then you inject in the front end and you can see the stream as it is happening. And there's just a little delay of the actual transcoding of the stream, but it's live coming from this web page here. Let me close this. And this was using this webrtmp SDK that we showed, which is made for you to stream directly from the browser. And it's good to do quick demonstrations or start getting started with the live peer platform, right? So the other thing I can show you is the VOD API. So we have here this other tab in the dashboard.
00:20:59.446 - 00:21:32.174, Speaker B: It's not the streams, it's actually assets. And it's where you can see all the files that you have uploaded to the API. And the same way you can create an asset here by giving a URL to import, et cetera. But I can also show that via the API here as well. So the process there is actually done in two requests instead of one. And first you request for an upload URL that you're then going to use to actually upload the file. And this URL here can be called from anywhere.
00:21:32.174 - 00:22:44.198, Speaker B: So the idea is that you create this presigned URL on the back end, give it to the front end application and the user can do the upload directly. So you don't need to do any kind of proxying of the actual file then to use that URL you just do a put with the file as the body and here it's already going. And when you import an asset, you get a task that processes the asset until it has all the metadata and the duration of the video bitrate, all this kind of stuff. And you can also call this other API here to list all the assets in the account you can also read individually, but this is easier for now. And here is the asset we just uploaded and all the specs that we parsed and it also has a playback ID. But let me show you in the dashboard. So here it just showed up, the one we just uploaded and it has download URL here, which is actually how you can play back the file.
00:22:44.198 - 00:23:30.140, Speaker B: So this is just playing an MP4 right now and we are working on adding HLS support for assets as well. But as soon as you upload the file to Live Beer, you can already use it from this download URL. And here you can see that this one also showed up. It actually came from the recording of that stream, the first part of the demo. And the recording also becomes an asset later. And you can use it the same way and play back the recorded stream as a video file or export it, or even create an NFT out of. So let's go into that NFT part exactly.
00:23:30.140 - 00:24:21.290, Speaker B: We have this SDK, the video NFT SDK that builds on top of this VOD API and you can use to easily create video NFTs. So it handles both the uploading of the file, the processing in the live peer network in case it's necessary, and the exporting to APFS and then the actual minting of the NFT from the exported file. It can be used to build any kind of application. You can use it from the front end, from the back end, from a CLI. And we actually have a couple examples using that. And I'm going to show this one, which is just an application on the front end. And if you are logged in the dashboard, you can go to just mint NFT and you're going to see this UI here.
00:24:21.290 - 00:25:31.620, Speaker B: And let's use the same video file. And this is just the smart contract that we have deployed by default, but you can also use a custom ERC 721 that you have. And first step so it's doing the same process that I just showed on postman. It requested the uploaderial, then actually sent the file, then it did some processing and I'm going to explain soon. And then already exported to IPFS and it has this hash here, this CID and it's already injected here and now we can actually mint with that CID and this exporting here. Currently, if you're using Openc, there is a file limit of 100 megabytes. So if the file is higher than that, it's not going to show anything, it's not going to show the preview of the file.
00:25:31.620 - 00:26:48.284, Speaker B: But when you do it through the SDK, it's going to check that and it can transcode the file to a lower quality just so it shows on OpenSea and it's not just a blank NFT over there. And here we can see that it finished. Yeah, so it's already available here on Openc and I did it on testnet, but it works on Mainet just fine. And yeah, back to the processing, we also intend to add support to other kinds of things like if you upload a video that has a codec not supported on the Internet, on the web, on most browsers, we can also offer to change that. So that's exactly where we plan on adding more and more functionality with the power of the live peer network. And we can also go through the smart contract if there's time. So just to go through quickly here, I mentioned that you would be using just the default contract here, but you can actually create your own and just change the address here.
00:26:48.284 - 00:28:11.256, Speaker B: So the SDK calls that separate contract and you can do so following this guide here that is also in the documentation of the SDK. And we have the base code here for the contract. And it's really simple, just inheriting from Open Zeppelin contract and then adding a simple logic on the Mint that I can show here is better. We just import the contracts from Open Zappolin, then have our custom one inheriting from it, one that has the storage for the NFTs and then discounters just keeps track of the IDs of the minted NFTs to always create a new one with a different ID. And then we have this event here which is sent after the mint is done and is what the SDK relies on to show what was the minted NFT, the minted ID of the NFT. And then this is the main Mint function. The SDK also relies on a signature like this and then it basically creates the new token ID, mints it for the respective owner and sets the Uri to what is sent on the request here.
00:28:11.256 - 00:28:52.790, Speaker B: And that comes from that thing we saw here. This is the Token Uri that went on that argument and then that's it. It meets an event which we can use in the front end to show any information about the newly minted NFT. Finally, all the other methods from ERC 721 are already present in this contract just because it inherited from this. So it supports any tool that relies on these interfaces, like Openc itself. So that's how it just shows up there as we mint it. And I think that's it for the demo.
00:28:55.080 - 00:29:28.050, Speaker A: All right. Thank you, Victor. I want to spend just a few more minutes talking about prices and ideas that you can think about building for the hackathons. Today. Live here is offering up $16,000 of total prizes. I'm really excited to be here and working with the hackers here. The first prize is for $6,000, and we're looking for developers to build the killer video centric social media creator or gaming Web Three application.
00:29:28.050 - 00:30:31.430, Speaker A: This is an area that I think is ripe for disruption for Web Three. All of the components from an infrastructure perspectives are here for us to create a Web Three centric social media platform that can be very competitive to today's platforms, like a YouTube or a TikTok. So we're really looking forward to seeing the creativity of hackers here building platforms like this. The second price is for $4,000. That's for the best use of the Life Peer Video NFT Minting SDK that we just showed. We look forward to seeing how people can creatively use this asset of video NFTs to do all kinds of interesting things and think about kind of thinking beyond the speculative use cases. I think there's a lot of really interesting areas that this can go to.
00:30:31.430 - 00:31:17.076, Speaker A: The third prize is for the best video on demand application using Live Peer. So simple. Think about this as the web3 YouTube. How would a YouTube look different if it's built in a Web Three native way? What kind of features would it have? What kind of value proposition would it have for creators to be able to connect directly with their fans, to be able to directly monetize the work that they do? I think there's a lot of really interesting ideas in here. And finally, fourth place, fourth price for $2,000. We have the best applications of Live Peer in the Metaverse. The metaverse can be interpreted in different ways.
00:31:17.076 - 00:31:58.256, Speaker A: I kind of think about the metaverse as this just already deployed and already running decentralized infrastructure in general, instead of I think the more narrow definition would be kind of like a rendered 3D world. Right. So thinking about using video streaming both video on demand and live streaming into the Metaverse or from the Metaverse to show kind of people not participating, what's going on in there. I think there's a lot of interesting use cases there as well. So that's it. We, I think, have a couple of minutes left. If we have any questions from the audience, we're happy to hear that.
00:31:58.256 - 00:31:59.090, Speaker A: Right now.
00:32:03.700 - 00:32:23.800, Speaker C: I have a quick question on the so when you showed the Live video, so the stream that you just created during the demo, so it was also uploaded as an asset. The asset that is displayed there, is it only one of the encodings that is there? Or is that also like in a different formats, like the different kilobit streams and so forth.
00:32:27.340 - 00:32:49.600, Speaker B: So the asset that is created automatically is from the source video. So it's just the highest quality version. But when we also do have a recording that is the same HLS that was made during the Live stream, so it has all the transcoded renditions and you can actually download those as MP4 s as well. But by default, we create only the source.
00:32:54.020 - 00:33:02.070, Speaker A: Any other questions from the audience? All right.
00:33:04.680 - 00:33:26.140, Speaker D: Thank you both for the overview and the demo. Given that you're thinking about video all the time, but building it from an infrastructure perspective, if you had the time to work on a hack, what are some ideas that you would love? Just a spare extra 20 hours to work on using light beer.
00:33:29.920 - 00:33:33.650, Speaker A: Do you want to answer that? Maybe we can search over?
00:33:36.340 - 00:34:29.730, Speaker B: Yeah, so something that would be really cool would be a mobile app using the NFT SDK. And then you could just make a video and immediately make it into an NFT really easily. Could be just like a camera phone that creates NFTs out of every video that you create or something like that. And I don't know, could be also more different video NFT so that you can create different interactions with video NFTs. Like you can maybe split your NFT into each one is one part of the video, then you gift it to someone else and then you can merge them together if you have the continuous parts or, I don't know, some crazy stuff like that, that would be really cool to see.
00:34:33.460 - 00:35:18.290, Speaker A: Yeah, there's so many interesting things that people can work on. I have a couple of ideas. There's been over 100 applications built just in Q, one alone in Live Peer. Some of the things that are really interesting, for example, the video streaming application for DevConnect is actually built using Livepeer and is streamed with Livepeer. It's called, I think, streameth TV. One of the interesting things, it was a collaboration between Livepeer and Ethereum, the Ethereum Foundation. One of the interesting things that we did is not only is it now completely open source, anyone can take that website and just change make improvements on top of it.
00:35:18.290 - 00:36:00.220, Speaker A: For example, adding like a chat function or adding the login function. Logging with Ethereum so people can see your ENS, things like that. But it's also compotentized modularized. So that, for example, the video player that's being used is actually a module. And this video player has some interesting functionalities. One is that you're able to automatically have a primary and a backup stream so that when you're streaming an event, you can have two streams going on. In case the primary stream fails, it automatically switched to the second stream and your user.
00:36:02.400 - 00:36:02.728, Speaker B: Doesn'T.
00:36:02.744 - 00:36:58.370, Speaker A: See any breakage in the experience. But there's so much more that you can add in this video player, right? Just think about what a web3 native video player can look like and what kind of functionality that it can have, right? You can start allowing the viewers to log in with their MetaMask and show the NFTs that they have in their wallet and the NFTs can then that information can be sent to the broadcaster so the streamer themselves so the streamers can know the type of people who are watching their streams. Right, and this kind of like an idea off the top of my head. But I guess the point that I'm trying to make is that there's a lot of really interesting toolkits that are already built within the Lifepeer ecosystem that you can just take and make tweaks on top of it to add interesting functionalities. And I'm pretty excited about that.
00:37:02.500 - 00:37:11.030, Speaker C: Would it be possible to add extra transcoding steps like some kind of post processing or like watermarking, stuff like that?
00:37:13.800 - 00:37:48.016, Speaker A: Yes, absolutely. You can do that. Everything in Life is open source, including the Lifepeer Node itself. The Lifeyer Node currently handles video transcoding for different codecs. It also handles smart AV features. So for example, if you want to transcode but also do scene detection to figure out if someone is streaming adult content on your platform, you can do that. Right, so that kind of gives you an idea of how open ended it can be.
00:37:48.016 - 00:38:11.640, Speaker A: When you talk about just open source and open video processing, you can absolutely add watermarking, you can add compositing to add different kind of artifacts on top of that. So it's not just a watermark, it can become animated. Yeah, all all kinds of cool ideas that can come out of that one.
00:38:11.790 - 00:38:27.470, Speaker B: I like it the other way around. Something is not like too dull but maybe too fast. So really catering towards people who cannot watch too fast stuff. I just took that idea. I don't have a question, sorry.
00:38:28.080 - 00:38:41.728, Speaker A: Thank you for your contribution. Any other questions? You mentioned that there's a penalty similar.
00:38:41.814 - 00:38:42.716, Speaker B: To proof of stake.
00:38:42.748 - 00:39:44.240, Speaker A: I was just wondering what exactly is the penalty? Yeah, that's a really great question. So that's really in the core design of the live peer protocol, right? And the problem that is trying to solve is that imagine I'm subversive and I ran a live peer orchestrator on the network and I say, hey, I provide transcoding services to everybody and you send video to me. And I start transcoding video for you, but I start ingesting weird videos in the middle of your video. Or I can just simply return blank videos to you. Or I just won't do the work at all. Right, so any of those situations are really bad for the quality of the network and we need to have a way to prevent that. So the way to do that is there's a verification mechanism in the protocol that allows the broadcast, allow the person who's using the network to say I want to periodically verify that and make sure the work is done correctly.
00:39:44.240 - 00:40:28.790, Speaker A: But you won't tell me which segment that you're going to verify, right? Because I am on the network and I'm signing cryptographically for every video segment that I give back to you. You have clear proof and evidence that I said I did the work, right? So if the verification fails, you can submit that proof on chain to say hey, Eric cheated and the protocol will automatically slash me by taking some of my stakes here token away. Because in order to participate, I have to have some skin in the game. And this is where kind of the staking mechanism come in. Again, very similar to how kind of ethereum staking works.
00:40:33.560 - 00:40:59.870, Speaker C: A totally different question because not about the technology itself, but how do you envision like conquering the world with Live Peer? So how do you compete with, let's say, Web Two or traditional video transcoding Services? Is it a competition based on pricing? Do you claim that you can do it cheaper than competition or is it more about that you have more features as Web Three enabled? What's the plan there?
00:41:01.440 - 00:42:38.948, Speaker A: Oh man, what's the plan there? There's definitely that cost aspect, right? Live Peer is ten times cheaper than Amazon Web Services from an infrastructure cost perspective. And that's just because there is so much spare capacity laying around the world that people can donate to, people can put on this network and to make a little bit of money back, right? So that's really interesting. The other thing that's interesting from the long term perspective, which I think is a lesson that we've all learned from the bitcoin network ten years ago, is that if you put a simple set of incentive out there and you say this is encoded in the protocol, everybody feel free to do whatever you want with it. People are smart and they figure out how to take advantage, how to figure out how to game the system by improving their performance to make it a little faster for themselves. And when everybody's doing that, that grows organically like crazy, right? So ten years later, the bitcoin network is by far the largest supercomputing network in the world in terms of the power of computation, right? Because people started building GPU mining software and for a couple of months that was profitable and immediately it became FPGAs and then immediately it became Asics. Asics kept getting faster and faster and faster. So we already see that happening in the Live Here network where ASIC miners are coming into the network that are creating video transcoding specific hardware to be able to compete with kind of traditional GPUs.
00:42:38.948 - 00:43:26.588, Speaker A: So that will only get better and better over time and the long term cost advantage and scalability comes in, right? But I think that's just one angle the other angle that's really interesting is this Web Three movement that's happening, right? And the Web Three movement is really about ownership. It's about giving people an opportunity to have a more open and transparent system. And that I think is highly disruptive to the existing world of video platforms. Right. When you use YouTube, YouTube's take rate is about 50%. That means for every dollar that a creator makes on YouTube, YouTube takes $0.50 from that.
00:43:26.588 - 00:44:22.190, Speaker A: That is crazy for a platform that is made up of the videos that people upload, they don't make any videos themselves. Right. So using Web Three, creators essentially get to say, like, I actually own the video myself because it's tied to my ethereum address, which is on the blockchain layer. The application is simply built on top of the blockchain layer. Right. So Live peer is building the video streaming layer for web3 that has all these hooks into other web3 kind of other web3 components that together creates this web3 video application stack that allows people to build these types of web3 native video applications that I think in the long term are going to be just very disruptive. Cool.
00:44:25.460 - 00:45:00.200, Speaker E: Thank you. Two questions. First one is, do you think that the network of transcoders will ever be spread out enough that you won't need CDN networks? And is there like an idea in your mind to build out another set of nodes with a different function that access the CDN or anything like that? And then the second question is, if I just uploaded a file to IPFS and included that address in my metadata for an NFT, would that work? Or does it have to be transposed to mint an NFT?
00:45:02.640 - 00:45:52.284, Speaker A: Cool. First question about CDNS. So Live here actually already contains software that allows you to deploy your own edge node around the world in order to kind of run your own delivery. And for a lot of applications that works extremely well. We have a user of Live Peer who runs an application that has over 80 million minutes a week a month and have over 75,000 streams per month, this pretty popular video streaming network. And they don't use a CDM, they just run Live Peer nodes around the world, like a couple like three or four locations around the world. And they're able to handle a lot of traffic that way.
00:45:52.284 - 00:46:52.424, Speaker A: So that's the current way to kind of scale your delivery. If you don't want to use a centralized CDN. We're also working on a decentralized CDN solution that allows each life peer node to essentially serve as a seed of a swarm of nodes that are living in people's browsers. Right? So what that allows you to do is to run a live stream and for your viewers to watch the video and deliver the video to each other so it's not always loading from the network. And that's the way to really scale out and that's a really good situation for really good solution for when a video all of a sudden gets viral. And that's kind of the worst situation for a centralized video platform, because in centralized planning, you already planned out your capacity. And when something unexpected like that happens, which happens all the time, it can be really disruptive to the network that's already pre provisioned.
00:46:52.424 - 00:47:13.460, Speaker A: Right. But in this world, when the virality happens, it's great because the people who are coming in to watch those streams are just delivering the video to one another. Right. And that kind of protects the network from this almost like DDoS attack. Right. So that's that. Second question is about NFT minting.
00:47:13.460 - 00:48:13.450, Speaker A: Yeah, you absolutely can just use a video and upload it into IFFs and use that hash and mint the video. However, video files come in all kinds of different formats. They come in all kinds of different resolutions. Oftentimes they're not optimized for video streaming on the Internet or especially streaming in the browser. Right. So you can think of Live peer's network as almost like an optimization or standardization layer that just processes the video so that it makes sure when you're minting the video, you have the best file format to do it, right? Yeah. Is it multiple formats to put in the NFT? Currently it is one format that's like the optimal format, but yeah, in future versions, we'll add in kind of flags for people to have a little more flexibility there.
00:48:13.450 - 00:48:22.990, Speaker A: All right, how are we doing on time? All right, last question.
00:48:30.560 - 00:48:55.220, Speaker F: Is it technical possible to make mint out of one NFT video, more NFTs? So, like, for example, editing video, you have one video and some people wants to make some art out of it and make a second NFT out of the small NFT.
00:48:58.380 - 00:48:59.930, Speaker A: Do you want to answer that one?
00:49:04.860 - 00:50:05.404, Speaker B: So from the Live view perspective, the SDK, you could mint the same video multiple times. You can also create the smaller segments of the video and create separate NFTs out of those. But you could also build something, maybe even on chain to do something like that. Maybe you have the NFT which points to the video, but it has an offset in the video as a metadata, and then your NFT application knows that it's not owning the full video only part of it. And then you could do like you could create sub NFTs or smaller NFTs from the same video without needing to upload or reprocess that video just by having that reference. But that would be a custom protocol on top of the existing ones. So you would need like your own application to parse it and all that.
00:50:05.404 - 00:50:10.910, Speaker B: But I don't know if I answered the question. Is that, okay.
00:50:13.860 - 00:50:24.370, Speaker F: When you make out of this existing NFT second one or more small NFTs, that, you know, the small entities are actually from this.
00:50:24.900 - 00:51:01.720, Speaker B: Oh, right. Yeah. So that's one thing. Yeah. The question is if there's a way to know if the smaller file that was minted as the NFT corresponds to the original one before processing. And that's something that we do want to add as well, which is like when we do the NFT, we upload not only the final process file in the right field for the applications to show the video, but we also have a custom property that is like the original video is this. And then you have a different IPFS file.
00:51:01.720 - 00:51:19.590, Speaker B: And with the proper application, you could go and play the full play or download the full file as well. So you can do both. And right now you can customize the NFT metadata as well, so you could even build that on top of the SDK. It's already possible to do so.
00:51:21.480 - 00:51:55.714, Speaker A: Yeah. All right, well, thank you all for the awesome questions, and thanks for the crowd, and thanks to Eve Global for hosting Are.
