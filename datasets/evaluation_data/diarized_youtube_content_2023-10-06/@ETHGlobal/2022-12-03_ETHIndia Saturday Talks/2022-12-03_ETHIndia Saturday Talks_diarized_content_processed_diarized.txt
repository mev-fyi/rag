00:00:52.710 - 00:00:53.260, Speaker A: Five.
00:00:55.390 - 00:01:13.404, Speaker B: Do we need a sound check? Is this working? Yeah, sound because they'll raise it. Do you want to flip through slides first? It's a bit yellow but not terrible. But that's what we got.
00:01:13.442 - 00:01:14.430, Speaker A: That's what we got.
00:01:21.390 - 00:05:28.264, Speaker B: Yeah, no problem. No rush. Yes, current. What do you want me to do? Okay, how? Okay. Yeah, the micro it I have no monitor over here. 1 minute. I don't know which one this is.
00:05:28.264 - 00:05:41.200, Speaker B: Is it this one? Are you cool with mirroring? If I mirror, then I won't be able to see my notes.
00:06:12.090 - 00:06:34.632, Speaker A: You go check. Check.
00:06:34.686 - 00:06:40.650, Speaker B: Okay. Hi everyone. Thanks for joining us.
00:06:41.900 - 00:06:42.312, Speaker A: Today.
00:06:42.366 - 00:07:56.050, Speaker B: I'm going to be talking about how liquidity pools are a really useful tool as DeFi infrastructure for builders. Before we dive into all of that exciting stuff, I want to start actually just by touching on something that's very important to all of us here. I think if there's one thing that we've learned in crypto over the last three months, it's the value of decentralization transparency. Immutability in finance. With the billions of dollars that have been lost of consumer funds by exchange like FTX, lending platforms like BlockFi, Celsius, Voyager, the value proposition of DeFi has been validated now more than ever. And the people in this room are the people who are really bringing this to fruition. So we're not only here to build cool tech, we're actually building a safer, more fair, more transparent world for all users of financial products and services.
00:07:56.050 - 00:08:50.204, Speaker B: And I think that the work that we're doing now, I think has always been important and I think it's more clear than ever how much the world actually needs this. So with that said, I'm Jeremy Musigi. I'm CEO of Orb Collective. Orb collective is a growth team for the balancer. Dao. Our mission is to scale global utilization of the Balancer Protocol and support the resiliency and long term sustainability of Balancer as a community and as a technology. Balancer is DeFi liquidity infrastructure, and Balancer is designed as a very flexible AMM automated market maker that is highly customizable.
00:08:50.204 - 00:09:59.060, Speaker B: So the liquidity pools within Balancer serve as this building block, as this piece for developers that can integrate and build on top of this base layer for many different use cases. And we'll definitely dive into that. Balancer serves today as one of the top decentralized exchanges in DeFi across chainsr, really. And this is what Balancer is, I think historically most well known for it's our exchange, but Balancer is actually way more than that. So first I want to just kind of lay some foundational knowledge about what a liquidity pool is so that we're all on the same page. So a liquidity pool is like a stockpile of tokens that belong to separate individuals or organizations that are held together in one smart contract. And these tokens are pooled together in order to generate some yield.
00:09:59.060 - 00:11:02.220, Speaker B: By putting those tokens to work, utilizing them in some effective efficient way and creating a liquid market using those tokens. So in the case of an AMM or automated market maker, these tokens are pooled together in order to create an exchange market between different pairs of tokens. So you make it so that a buyer or seller can trade with a liquidity pool, basically by providing asset A and getting asset B in return at a given price. And that price is determined by the liquidity pool algorithmically. So I'll go through a couple of examples of liquidity pools. So first example is a single asset pool. And a great example of this is a money market, or like a borrow lend protocol, such as Compound in this case.
00:11:02.220 - 00:11:55.020, Speaker B: So in a single asset pool, there are lenders that are lending a token into the pool, and borrowers are borrowing that same token out of that pool. Lenders are earning interest, borrowers are paying interest. And it's one pool that consists of one token that's being used by both sides of this market. In a dual asset liquidity pool, now, you open up a completely different kind of use case. So if you look at Uniswap, for example, you have a pool with two tokens, let's say it's ETH and Dai. And this pool is maintaining a balance between the two. And as a trader, you can buy ETH from this pool by selling it Dai in exchange or vice versa.
00:11:55.020 - 00:13:06.740, Speaker B: And having this pool containing two different tokens serves as having a liquid market so that anyone who wants to trade between these two can do so at any time. This is not like an order book which happens in traditional financial markets where in order for your trade to be fulfilled, another person on the other side needs to want to do the exact opposite of your trade at the same price. They want to buy what you're selling, and they want to sell what you're buying. Here, in an automated market maker like Uniswap or Balancer, you're trading against a liquidity pool, not another trader. Now, let's talk about multi asset liquidity pools. So this is a concept that was originated by balancer, but one of the great examples of it in practice is the Curve three pool. First of all, a multi asset liquidity pool is a pool that holds more than two assets and provides multidirectional liquidity between all of the assets that are held in the pool.
00:13:06.740 - 00:13:59.750, Speaker B: And in the curve three pool, you have a liquidity pool that contains the top three stablecoins Dai, USDC and USDT. And what purpose does this serve? It serves any trader who is holding one of these stablecoins or two and wants to exchange into a different one. There's a very liquid market for them to be able to do that very efficiently. On the right side of the screen here, we have the Balancer boosted Ave USD Pool, or BBA USD for short. And this serves a similar purpose to the curve three pool in that it's a stable coin pool. If you have USDT, you can trade for USDC or Dai or any of those tokens for each other. But if you look at the screen, you actually see that this pool contains six assets, not three.
00:13:59.750 - 00:14:57.992, Speaker B: And the reason is because this pool consists of three separate two token pools that are nested together into one pool. So you have a pool for USDT, you have a pool for USDC, and you have a pool for Dai. And I'm going to go into this specific type of pool soon. But for now, I'll just mention that the goal of this pool and the reason why there are six assets instead of three, is that these tokens are also at the same time earning a yield outside of balancer for all of the liquidity providers while also serving as an exchange between stablecoins. So we'll go more into that. It's a really exciting product called boosted pools. All right, so what can you build on a liquidity pool? Now, hopefully you have some sense of what a liquidity pool is, if you didn't already.
00:14:57.992 - 00:16:13.236, Speaker B: But now let's go into what can you actually build with this and how does this work as a building block or infrastructural piece for DeFi developers? So the first example that we'll look at is Aracus Finance. Aracus Finance is built on uniswap v three and Arrakis. What it is, is it's an application that reduces the complexity of liquidity provider or LP strategies on uniswap v three by automating them. So if you're familiar with uniswap v three, you'd know that from v two to v three, one thing that really changed and was very innovative is that they offered this concept of concentrated liquidity. Whereas a liquidity provider in a pool, you can allocate capital to specific ranges that are trading between a token pair. And that offers a lot of opportunity to fine tune your strategies. And it really opens the door for highly sophisticated players to use this technology and really have a competitive advantage over unsophisticated players.
00:16:13.236 - 00:17:16.800, Speaker B: But the problem then is, of course, that it's a lot more complex and it's a lot harder to use for your average retail investor who owns tokens and wants to generate some yield on them. So Aracus is a solution built on top of the uniswap v three liquidity pools to make these vault what they call vaults, which are pre programmed strategies, easy to use for anyone. Another example is urine. So Urine has a lot of different vaults as well. And these vaults, similarly but different to curve, are a way to automate like yield farming strategies usually. So each yearn vault is designed around a certain asset. You put the asset into the vault, and then there's a strategy that's designed for that vault that will take that asset and use it in the most profitable investment strategy available in DeFi.
00:17:16.800 - 00:18:14.420, Speaker B: And these are not exclusively built on curve. You can see here on the screen that there's an Ave vault. There are actually a couple of balancer vaults, but there are a lot of curve vaults. So this example right here is using the curve three pool that we were looking at previously. There's a vault that uses that pool, uses the LP token of that pool to invest into the most profitable strategies. What these vaults can do basically, is they could use the LP token itself if there are investment strategies that are useful there, or it can access the underlying liquidity from those tokens and use that and sort of bring it back at the end of the day. So another example here, which I think is the most exciting one, is Fjord Foundry, which is built on balancer.
00:18:14.420 - 00:19:45.340, Speaker B: Now, this, I think, goes to show sort of how balancer is really in a different category compared to these other AMMS because of the types of liquidity pools that you can use and you can create on balancer. So Fjord uses a primitive pool type on balancer called a liquidity bootstrapping pool, and they use this to create the most popular platform for launching a token. So when a project is launching a new token into the market, they can use Fjord to conduct this sale in a way that is very fair, uses a reverse Dutch auction style price discovery mechanism to find the right market price and to give all participants an equal chance to buy into the sale. It also provides protection against bots that sometimes can buy up an entire sale before other people get a chance. And also you can use Fjord to conduct a sale that actually raises capital in more than one asset. So let's say you're launching your token and you want to raise a treasury that consists both of ETH and stable example, you could actually have both of those being accepted by the pool, so that at the end of the day, you've raised capital in both. So let me just talk a little.
00:19:45.350 - 00:19:46.944, Speaker C: Bit about liquidity bootstrapping pools.
00:19:46.992 - 00:21:12.660, Speaker B: The way that this works and the way that this is possible is that an LBP is a balancer liquidity pool that has the ability to change its weights over time while it's live. So if a pool starts where it's 80% ETH and 20% die, it can actually change over time and get to a completely different percentage distribution, such as like 20% ETH, 80% die. If I had a lot of ETH that I wanted to sell, I could use a liquidity bootstrapping pool to do that. While the weights shift over time, it adjusts prices over time, and it incentivizes traders to trade against that pool and basically to move it into the balance that I wanted to eventually get to. Okay, actually, why not? Yeah, so you can see here on this graphic from Dune Analytics that we've had 299 LBPs on balancer. So far, over 90,000 investors have participated in those. And yeah, there's been a lot of money raised and there's been a lot of volume for those projects.
00:21:12.660 - 00:21:57.230, Speaker B: And Fjord is an awesome place to do that. So let's now dive into more of these pool types on balancer, with of course, a lens toward understanding how these are useful for developers. So we kind of talked about how Liquidity Bootstrapping pool is useful for Fjord. They actually built a whole product based on that pool, but there are more. So first, the weighted pool. This is kind of like one of the more basic foundational liquidity pools. On balancer, these pools can hold anywhere between two and eight tokens.
00:21:57.230 - 00:23:06.210, Speaker B: They have fixed weights, the creator of a pool, which anyone can create, can create one can set those weights according to their specifications, 50, 50, 80, 2010, pretty much anything in between. And these can also hold, as I said, up to eight assets. So one example of something cool that I've seen someone build using this product is there was a gaming project. And in their game there were these in game assets that players would earn by playing the game. Now, what the developers of this game needed to do was they needed a way for players to be able to convert between one asset and another while they're playing the game. So let's say you earn a certain amount of a certain in game asset and once you reach a certain level, you actually can level up to this other asset. There's a liquidity pool on balancer that's actually working behind the scenes in this game and those tokens are being traded in that way.
00:23:06.210 - 00:24:18.966, Speaker B: So this weighted pool is sort of the enabling component of that functionality in the game. So a stable pool is another example. And this is a pool that holds two to five tokens that uses a stable swap formula for tokens that contain exact or almost exact value. So again, this could be Dai, USDC, USDT, but it also can be like wrap bitcoin derivatives like WBTC, Ren, BTC, SBTC, and this could be staked ETH derivatives as well. So the advantages of one of these pools is that for traders, there are tighter spreads and lower slippage. For liquidity providers, you earn a competitive yield with very little impermanent loss, which comes as a result of the way the formula balances between tokens of the same value. Boosted pools is something that I was talking about earlier.
00:24:18.966 - 00:25:06.170, Speaker B: So in a boosted pool, what you have is a highly capital efficient liquidity pool that is not only serving as an AMM where you can exchange any token for the other, but at the same time it's lending its assets outside of the protocol into some other yield generating strategy. So the first boosted pool is the balancer ave boosted pool. And in that pool you have di USDC. I'm actually going to go back to that graphic somewhere around here. Yeah. So you have USDT and aUSDT. aUSDT is the ave wrapped version of USDT.
00:25:06.170 - 00:26:08.302, Speaker B: You can see here that there's a percentage of the assets in the pool. The most of the USDT in this pool is actually deposited into Ave and then the Ave, a token is in this pool paired with USDT itself. This pool just needs to have enough USDT, USDC and Dai to fulfill the demand from traders at any given moment. And the rest of the pool can be earning yield on Ave. So if you're a liquidity provider in this pool, you're earning from two different revenue streams. This is something very unique and innovative in DeFi that really isn't, I don't think is possible anywhere else other than on balancer. So for an integrator, for a developer, boosted pools are interesting because they simplify the path for multi pool operations.
00:26:08.302 - 00:27:15.830, Speaker B: So you can keep everything within balancer and it also enables advanced applications. So out of two weighted or metastable pools that contain, for example, USD or Euro, you can create a whole forex application that generates yield and boosted pools. This first one is built on Aave, but we have multiple new ones coming out soon with other partners that are utilizing assets in balancer pools within their protocols to do really interesting things. So a metastable pool is a pool that is designed for tokens that are highly correlated in value but not pegged exactly. So a great example of that is wrap staked ETH and ETH itself. Wrap staked ETH, since it's staked it's earning a yield and it has a different value from ETH, it's always going to have a slightly different value from ETH. But what this is, is a generalized stable pool that can hold these proportional assets.
00:27:15.830 - 00:28:11.340, Speaker B: And another example of that would be like Dai and CDI. So you have a predictable schedule of what the valuation exchange rate is going to be between Dai and CDI because you know how on compound the value of deposited dai is going to appreciate over time in order to account for the interest that it's going to earn. If we talk about Lido as an example. So when users stake ETH onto ETH 2.0 with Lido, they get staked ETH in return. Through this pool, lido makes it possible for anyone to get in and out of staked ETH very easily by just trading with a balancer pool instead of wrapping and unwrapping or staking and unstaking. So it's really the easiest way to do that.
00:28:11.340 - 00:29:05.402, Speaker B: Lastly, here in terms of pool types is something that we're currently developing right now that's coming out soon. It's called a managed pool. Managed pool is designed to optimize itself for sophisticated portfolio strategies and have more fine grained control. One of the main products that you might build with a managed pool is an index fund. So managed pool can hold up to 20 tokens, actually can hold more than 20 tokens. And you can set dynamic weights, you can set dynamic fees, and you have the ability to have an allow list of liquidity providers. Or you have a manager of a pool who can implement a certain strategy for how that pool will invest its assets, and you can change the tokens and the allocations that that pool has at any given time.
00:29:05.402 - 00:30:10.830, Speaker B: Another thing about managed pools is that they have a feature called circuit breakers. So circuit breakers are designed to protect against like a black swan downside event. So DeFi kind of in general doesn't really have that. But in traditional financial markets, when there's like a catastrophic crash, they have circuit breakers that can just kind of stop the markets from trading. So in a managed pool, a pool can at any point decide that it needs to stop trading because let's say, for example, one of the tokens in that pool got hacked. There was some kind of breach there's, some kind of market catastrophe, a rug pull, whatever might have happened, that pool can actually just stop trading so that the rest of the assets in the pool are protected. So not only can you use these really interesting pool configurations on Balancer, but you can also create your own custom AMM pools.
00:30:10.830 - 00:31:35.344, Speaker B: And I'll talk about a few examples of projects that have done this. So there's element finance, sense, protocol and tempest finance. These are three really cool projects that use Balancer to create their own custom AMM pools to make fixed rate markets possible. Gyroscope has built also their own AMM on Balancer, which serves to power a new stablecoin that they're releasing soon. And kind of the point here is not to dive into all the complexity of how Gyroscope works, but just to show that on Balancer, there's a lot of room for building custom solutions and kind of configuring things based on the needs of your application as a developer. Cron Finance and Excess Finance are two separate projects that are using Balancer to build Twams, which is a time weighted average market maker. And the point of this product, it's kind of something that has been discussed in the space for a while, but no one really built it and turned out that Balancer was the perfect place to do so.
00:31:35.344 - 00:32:30.944, Speaker B: So the goal of a TWAM is to execute large trade orders over a longer period of time so that slippage remains low. You can take trades and you can break them up into smaller pieces. So the liquidity required for each of those trades to be executed is less and the slippage is smaller. So this is really important for large scale investors or traders because in DeFi, slippage is a huge issue when you're moving large amounts of capital. So that's kind of a tour of some of the really interesting things that are being built on Balancer. Now I want to talk a little bit about some of the grants and perks that are available to developer teams. So these are a bunch of actually, some of the projects that I've already mentioned today.
00:32:30.944 - 00:33:47.224, Speaker B: As you can see, these are recipients of Grants. They've received grants from Balancer to build what they were building and ended up building something really amazing. So for any developers that are building in DeFi and need some kind of AMM functionality, if there's some kind of swap feature involved in your application, it's a lot better to use a readymade solution that works, that's secure and that's battle tested, that you don't have to build from the ground up rather than kind of doing all that yourself. So, highly recommend checking out our docs and definitely apply for a grant if that's something that you want to do. We also have this really cool partnership with Certora, the guys who are here and are going to be speaking after me, I believe, which is the Balancer Certora security accelerator. So for projects that are building on Balancer, you get access to two weeks of manual code review by Certora, which these guys are awesome smart contract security firm. And this is something that is valuable to any developer in the space.
00:33:47.224 - 00:34:45.070, Speaker B: And it's really hard to get audits, especially when you're a small team in the space. So that's kind of what we wanted to do, is make this more. And through this program you get the two weeks of manual code review, you get set up an introduction of Satora's Formal verification prover, and you get $10,000 worth of credits for Sartora's formal verification. You also get assistance from the Balancer integrations team on your code functionality on business logic. So it's really a good time to get involved. I'm going to quickly just talk a little bit about the SDK because it's been recently revamped and we've been getting really good feedback from developers on that. So, Balancer JS is a JavaScript SDK that provides commonly used utilities for interacting with Balancer v two.
00:34:45.070 - 00:35:33.512, Speaker B: Balpy is these Python tools for interacting with Balancer v two. In Python balancer. Sor stands for smart Order Router. This is a JavaScript off chain linear optimization of routing orders across liquidity pools to get the best price execution. And then we also have two really cool community led SDKs that have been supported by the Balancer Grants program, Delphi, which is in Delferium and also Rust. So this is an example of configuring the SDK to use mainnet subgraph and contracts. So SDK examples and tests can be run against a local fork.
00:35:33.512 - 00:36:34.630, Speaker B: It's easy to hack and experiment without using real funds. And the SDK uses this very easy interface, the Sor to find best swaps, like across all of the liquidity on Balancer, which is around a billion dollars. And using the SDK, you can very easily add liquidity to any pool and you can fetch and precalculate pool data, which is especially useful for front ends. And with that, we can end here. If there are any questions, I'm happy to take those. Thank you. Are we doing? Q a okay, great.
00:36:44.930 - 00:36:52.640, Speaker D: So this might be slightly vague and feel free to not answer this question, but when you spoke about.
00:36:54.370 - 00:36:54.974, Speaker A: There being.
00:36:55.012 - 00:37:07.442, Speaker D: A circuit breaker in a managed pool circuit breaker? Yeah, circuit breaker in your managed pool contracts, who acts as the circuit breaker? Who has that authority? And the larger question, I guess is.
00:37:07.576 - 00:37:09.458, Speaker B: Sorry, who has what? I couldn't hear you.
00:37:09.464 - 00:37:49.380, Speaker D: That one who acts as that circuit breaker, is that again a decentralized role or is that like one person or a group of few people who act as circuit breakers and decide that it's time to shut down the market for the time being? And I guess the broader question is where do you draw the line? In the beginning you stressed about the importance of decentralization in finance, but we know that it's not a very practical thing. Somewhere we need to have some amount of centralization to get it to work properly. So where do you draw the line? If you can articulate a clear answer to such a vague question, it'd be good to hear.
00:37:49.830 - 00:38:05.800, Speaker B: Sure. So I think the second part of your question to make sure I understand because the audio is a little tricky, was that you're saying that while decentralization is great, we still need some centralization as well.
00:38:06.250 - 00:38:07.014, Speaker D: I agree.
00:38:07.132 - 00:39:42.610, Speaker B: I think decentralization depending on it's very case specific for each project, like when it makes sense to be 100% fully decentralized. But I think that when you look at the DFI protocols like the exchanges and the borrowing and lending protocols that have been operating very smoothly throughout all of the insane catastrophes that have happened in the Crypto space, the last couple of months whilst you've seen a whole list of centralized platforms blow up and lose billions of dollars. I think it's a strong indication that the transparency and the verifiability of a decentralized system is very superior to a centralized system. It doesn't mean that there can't be any centralized components, it depends on the project and the system that I don't want to give a one size fits all prescription here, but to me it's very clear that DeFi will eat mean. These are the same issues that have gone on for so long in the financial industry where there are risky lending decisions made or unethical decisions with customer funds as what's been alleged in the case of FTX. These are things that are not possible in an immutable decentralized protocol. I think that with something as crucial as finance we really need to transition.
00:39:42.610 - 00:40:59.322, Speaker B: And I think that we are eventually going to transition from these human led systems where there's a lot of room for either error or greed or bad decisions or misuse of trust into systems that are fully open, fully transparent. I think it's just very clear to me that that's what we need in finance and we don't need to have these billion dollar sort of catastrophes in the future. I can talk a little bit about the manageable circuit breakers. Again, these are currently in development. So whatever we might talk about today can still kind of evolve and change. And it's not a final product right now, but what I can say is that the idea is to set some programmatic guardrails to defend or to protect the pool against a catastrophic situation. So, for example, if you had a liquidity pool that contained Terra USD, and let's say you had Terra USD and you had a few other stablecoins that were all in this pool, and then Terra USD explodes, the price is suddenly dropping.
00:40:59.322 - 00:41:34.300, Speaker B: What would normally happen in an AMM is that the AMM is programmed that when one token goes down, it sells the other tokens and buys more of that token that went down. Right. So in that scenario, every dollar in that pool would be lost. If you have a circuit breaker in place, you can stop that from happening at some threshold that is defined by the creator of the pool. So that in a Black Swan event, you can actually protect liquidity providers as much as possible.
00:41:34.910 - 00:41:40.650, Speaker D: Yeah, I get the reason for plugin.
00:41:44.910 - 00:41:45.418, Speaker A: Right?
00:41:45.504 - 00:41:56.960, Speaker B: Yes, it can be designed that way. Again, it's a work in progress. So I just wanted to give you a preview of the product when it's done. I'll definitely have a lot more to share.
00:42:00.470 - 00:42:23.030, Speaker C: So we talked about the problems in centralized exchanges. We started seeing problems in the DeFi space, also in decentralized exchanges. Two days ago, Anchor's tokens was exploited and they went to Pancake Swap and millions of dollars was drained. Do you have those kind of safeguards in the balancer?
00:42:23.790 - 00:42:27.820, Speaker B: So I couldn't hear everything you said, but I think you were talking about.
00:42:29.070 - 00:42:41.790, Speaker C: Anchor was exploited on December 1. December 1. And then the fund was moved to Pancake Swap. Once again, a DeFi decks and then it's drained from there.
00:42:41.940 - 00:42:42.254, Speaker A: Yeah.
00:42:42.292 - 00:43:26.490, Speaker B: So there's always a risk in an AMM that if a token that is held in a liquidity pool gets exploited, that any liquidity provider that is in that pool, even the one that is providing the token that is kind of trading against that token that got exploited, can lose their funds. So smart contract risk is very real. We need teams like Certora to help keep the space safe as much as possible from those risks. Those are very real. So I think DeFi has its risks that are different from the ones in CFI. Right. So in DeFi, we have more security risk in terms of like code exploits.
00:43:26.490 - 00:44:04.070, Speaker B: In CFI, we have more like human risk, where you have a trusted person who's in charge of potentially billions of dollars in funds that belong to their customers. And we have to trust them to be ethical and moral and fair and honest. And so many times in the past and just recently, that has not worked out well. So what I'm advocating for is trusting more in secure code than in. An individual, or even like a charismatic person who seems very trustworthy.
00:44:07.150 - 00:44:52.310, Speaker C: It looks logical that code will be bit secure, but it also gives opportunity to hackers and everybody else who understands that everybody can go and exploit. Okay? In the previous case, it's like one person which is not able to do good, and we all get suffered because of that. But here the information is available publicly, okay? The kind of exploit happened with Anchor. Anybody can do who understand better for web three. So, yes, we can say that we are shifting our security measures from direction A to direction B, but it's still saying that, okay, DeFi is the better or more secure than CFI seems to be subjective.
00:44:52.970 - 00:45:51.946, Speaker B: Yeah, and I would also just mention, like in the case of the Anchor exploit, so Anchor got exploited, but the AMM pancake swap was not exploited. Right, but the people who provided liquidity in Anchor on PancakeSwap lost their money. So I think in DeFi, there are kind of multi dimensional risks that an investor has to consider. So the investor that bought the Anchor token and put it in and decided to be a liquidity provider on AMM took a risk in the security of that protocol, and unfortunately, that didn't work out. And I don't have anything negative to say about Anchor at all. And that's not my point. My point is, even if a platform is very secure and not exploited, the assets from all over DeFi that can be held on that platform all have their own risks as well.
00:45:51.946 - 00:47:02.540, Speaker B: So there's like really an exponential matrix of risks that most people I think that you would agree, most people who are using these technologies don't fully understand all of those risks either. They know that it's risky and they're doing it and they're taking the risk. Like, some people like to gamble, but I think a lot of people are not fully aware of those risks. And in a decentralized ecosystem, it's really hard to provide good protection against those risks because there is no one there really holding your hand or stopping you from buying something because it's not safe. Yeah, it's very tricky. Sure. Okay, thank you everyone, and appreciate all your questions.
00:47:02.540 - 00:47:10.128, Speaker B: Thanks.
00:47:10.214 - 00:49:42.320, Speaker A: Good job. Should I put it.
00:49:47.170 - 00:49:47.920, Speaker B: Okay.
00:49:52.850 - 01:03:36.610, Speaker A: Set a couple. Okay. It okay. Can you hear me? Okay, fantastic. Thank you. I see you are a small crowd. Thank you for coming.
01:03:36.610 - 01:04:09.340, Speaker A: I'm going to talk now about formal verification. For many of you, it's probably a foreign concept. How many of you of you are developers? Fantastic. That's the talk for you. How many of you know about formal verification? Some. Great. So I've been working on formal verification for a long time, you can see, and I think it's actually an interesting domain for many things, but I think it's a perfect application for DeFi.
01:04:09.340 - 01:04:37.990, Speaker A: We heard Jeremy's talk about balancer, and actually it's related to what we're going to show you it's technology for checking the correctness of DeFi. And I have an hour for the talk. I'm going to use only half of it, and the second half will be a demo with our tool which is publicly available. You can try it and you can get a free version of it. There's also paid version of it. It's a subscription. Hello.
01:04:37.990 - 01:05:27.668, Speaker A: Do I have to do something? Okay, so what is Satora? Satora is a company for checking correctness of code. Satora is a company which is global. We have 90 people on the team. I didn't write the list of all of them. Our team consists of several people, and I think we have about 25 people with PhD in formal verification from top schools in the US. In Europe, Israel, and we combine it with other people who are security experts. I mentioned only few of them here.
01:05:27.668 - 01:05:54.124, Speaker A: And I think we are very, very grateful for collaboration. In particular have a collaboration with Rajiv from Sekureum. And basically this helped us to embod people into this domain. And some of them are here and working with us, like Alex Joseph and others, that actually bringing more people into this space. And we are looking for more. We are looking for more. And there are different ways that you can collaborate with us.
01:05:54.124 - 01:06:33.236, Speaker A: You can work in Satora, but you can work in customers that work with Satora. You can also work in the community. For example, we have a large grant which was just approved. So basically you're going to write correctness ave and we are doing it with other protocols. We are also working with some of the auditors, for example Spearbeat and Cordarina. So basically, Kodarina is a content company, and as part of the content, you would write correctness rules and you will check them with the Satora program. We also, of course, acknowledge to our connection in Israel, specifically to the IDF.
01:06:33.236 - 01:07:11.220, Speaker A: So we have, I think Amit Heavy was a senior person in the IDF for code vulnerability and he leads our security team. So everybody knows about this bug, and we heard about bugs in many places. There are a lot of bugs. I want to actually point out to you the two ones which are my favorite. This is the Nomad act, and the other one is the compound. So these are cases that basically the quad actually was good actually, and was even audited. And for example, in the case of compound, it was even checked compound.
01:07:11.220 - 01:07:58.100, Speaker A: They use a very good auditors and they're also a paying customer of Sartora and they use Sartora. But what happened after they checked the code with Sartora, somebody changes the code without running Satora, and as a result, there was a hack which was exploited. So this is why we want to use this technology to check the code before it is deployed. So in the domain of security, there are different tools. The simplest tool you can use is testing. Basically you test your code and all developers we use testing or we use fuzzing. There are also tools that help you make the fuzzing easier.
01:07:58.100 - 01:08:32.160, Speaker A: In the area of web two, there is AFL. This is a very, very nice tool which was used to find many, many bugs. And in the area of web three, people are developing similar technology. In particular, trail of bit is developing echidna and paradigm is developing foundry. These are very nice tools. They basically test behaviors. The problem with these tools, they are very, very useful, but the problem is them that's hard to find bugs which are somehow happen in real situation.
01:08:32.160 - 01:09:10.664, Speaker A: And I'll give you one like this, but you can see many of them in the Satora Proverb that actually these are bugs that happen after many states and it will be hard to detect them if you start running from initial state. So that's one technology, it's not unique to web three. It was, of course in web two, but it's also useful in web three. It's probably less useful in web three than in web two, because many of the security bugs in web three, they are specific. It's a managed environment. The second technology, which has been around for ages, it's static analysis. So, static analysis, it provides more coverage than fuzzing.
01:09:10.664 - 01:09:57.544, Speaker A: And there are tools available, there are industrial tools mainly for web two, that checks the code for correctness and it covers more behavior than testing. But the problem with these tools that basically they have the ability to basically miss errors and they have false positive and false negative. In this context, Sartora actually has built a very interesting case. In the Sartora Prover, we analyze the bytecode and actually we can prove certain properties automatically without the human. And this is something which is integrated into the Sartora Prover. There is a static analysis which is integrated. The right hand side are techniques which are supposed to provide more coverage, but they are potentially more expensive.
01:09:57.544 - 01:10:27.744, Speaker A: And they are called formal verification. Intuitively, formal verification means showing that your program does what it's supposed to be. And there are two branches of formal verification. The one which I'll be talking today, which is called automatic formal verification. The idea is the human writes the properties and the machine is trying to reason about these things. And this is working by compiling the tool into some kind of mathematical formula. And this is what's integrated into the Sartoraprover.
01:10:27.744 - 01:11:04.690, Speaker A: And there are a lot of open source tools. In particular, the Daphne tool by Microsoft Research, it's a similar tool. It's a tool that basically compile your code into a mathematical formula. The difference is that the Sator approver is more scalable. And you can see, for example, the Satora Prover, you can run it on thousands of lines of code, which is not something, and this is unique not just in web three. The ability to run on such a huge code and we have a proprietary technology that we developed that help us scale this. This is why we have all these talent people that are helping us building this stuff.
01:11:04.690 - 01:11:48.776, Speaker A: So this one, it's to give you effective proof and bug finding. Because it doesn't start, unlike fuzzing, it doesn't start from initial state. But it is expensive, because the problem is computationally expensive. We usually in computer science we call it undecideable, which means that the computer will not always do a perfect job. And the other branch which is also useful, it's called Interactive, sometimes called manual formal verification. So the idea here is that not the computer is doing the job, the human is doing the job and the computer is only checking you. And there are tools in the area of web three there is the K framework.
01:11:48.776 - 01:12:17.864, Speaker A: In web two there is tools like Lean. So these are tools or Croc, these are tools that check your proof, you are writing your proof. So in this case you can never get wrong. You write your proof and from the proof you extract your code. This is a nice tool which has been used in a nice methodology which has been used in many cases. The problem is these kind of approaches is very very hard for human to use. Even for small things like ERC 20.
01:12:17.864 - 01:12:50.690, Speaker A: You may have to spend ten months to prove this property. So this is very very high technology. So maybe just to give you a hint on static analysis so this is one of the most useful static analysis tool, which is called Slitter. How many of you know Sliter? Great. So I just ran Sliter on the bank. Bank is a very very simple code and it has many many red lines. Do you want to guess how many of them are real error? Zero.
01:12:50.690 - 01:13:30.288, Speaker A: And the reason is not because Sliter is such a technology which is bad. Of course you can improve the technology behind Slitter, but the idea that Slitter does not know what the code is supposed to do. The Slitter is basically running a generic test on your code in terms of comparing the tools. So I try to compare you have basically the other extreme. This is the extreme that you get very high coverage. I mentioned K, Hoc, Lean, so these are the tools that get high coverage. And the other extreme, these are the tools which are easy to use.
01:13:30.288 - 01:14:07.368, Speaker A: And I mentioned Foundry, but there are other tools which are easy to use. And Sertora is trying to build something in the middle. We are trying to make something easy to use as maybe not as easy, but close to using a fuzzer, but give you almost the coverage that you get from interactive term proverb. And I think we are there. We actually have a lot of things that we have done. In particular, we are analyzing the executable code, we are not even trusting the compiler. We are checking the executable code.
01:14:07.368 - 01:14:38.932, Speaker A: We also make the tool very easy to use. And you will see if you see, alex will show you, and you can see it with the demo. And the other thing that we do, we involve the programmer in the loop when it's hard for the tool to analyze the code. You can use modularity. You can basically make your code more modular or tell us how your code is modular. And the more your code is modular, it will be easier for our code to analyze. So what do we do in Satora? We do two things.
01:14:38.932 - 01:15:06.012, Speaker A: We have an open source language called CVL. It's a language for writing properties. This is something that we want everybody to use, and it's something that the properties of your code, this is the CVL. And then we are developing technology. We are developing three kind of technologies. The first one, this is the technology for checking the code. At the moment, it's proprietary, but you can use it and you can use it as much as you want.
01:15:06.012 - 01:15:42.280, Speaker A: The second technology, which is coming actually very soon, is developed by Chandranandi. And Chandra what she's leading, she's leading the project for checking the specification. And this is an open source tool. It's essentially a mutation testing for solidity in which you can check if your specification is okay. It's checking that the CVL is okay, and it's making the task of writing CVL easier. And the third technology that we are building now is a monitoring technology. It's the idea is that we want to monitor the CVL when the code is executed and when the transactions are executed.
01:15:42.280 - 01:16:26.692, Speaker A: So what I want you to get from this talk, and hopefully this is what you get when you use the Satora, is the ability of the beauty of invariant. And invariant, these are sort of the essential properties of program. It basically means these are the properties that they have to get to be right, and if they are wrong, something is bad. Okay? So in area of DeFi, the interesting thing is like solvency, it means the bank has enough money to cover the loan. It has some kind of property. And these are the invariants that you have to work when you are using Satora. So let me give you a very simple but interesting case of a solvency.
01:16:26.692 - 01:17:16.456, Speaker A: So the solvency, the most intuitive solvency, says that if everybody goes to the bank, the bank can still pay the money, pay its debts. And solvency is an interesting property for all the DeFi protocols. And what sartora does, as mentioned by Jeremy in the previous talk, sartora works with customer, and usually we work with them either before audit or after audit. These are the cases that we work with customers after audit, which means they completed the audit, and after that, they checked the code with the Satora approver. And you can see here, these are bugs that are found by this technology after the audit was completed. And all of them, these are solvency. I'm going to show you one like this.
01:17:16.456 - 01:17:47.740, Speaker A: But the idea these is bugs which allow you in a rare situation to take the money from the contract. And this was found by our tool. And it was found after a very, very good auditor. Of course, an auditor can also find bugs after us. We are not replacing the auditor, we are complementing the auditor. So how does the Satora approver work? You will see in Alex demo. But the idea is the user is a static analysis like Slitia.
01:17:47.740 - 01:18:17.724, Speaker A: But you are not only writing the code, you are also writing the invariant. And the tool can do two things. It can give you a mathematical proof that the invariant is maintained. That's very interesting. But the most interesting case, and this is what I showed in the previous slide, it show you a violation. It show you a potential rare case that the program starts from a state which satisfies the invariant into a state which violates the environment. This state is not necessarily the initial state.
01:18:17.724 - 01:18:58.148, Speaker A: It can be a state which happens many, many after many steps of execution. And that's the beauty of formal verification. It gives you an inductive reasoning why your program is correct independent of how many times it's executed. So of course, since we are talking about a computationally hard problem, the tool is doomed to fail in certain cases. And in our case, the failure means not false positive or false negative, but it means timeout. It means you run the tool for 2 hours and you don't get a result. And we have a technology that we are bidding to improve it, but it's always doomed to fail in certain cases.
01:18:58.148 - 01:19:34.516, Speaker A: And here you can use modular reasoning. We are doing many, many things, but at the end of the day, if the code is complex enough, the tool is doomed to fail. The technology is very useful. And I already mentioned that as part of your CI, we integrate like tools, like Circle and Jeep. So basically, every time you change your code, you run the tool. And that's the difference with a manual audit, in the sense that this is a continuous process. You run the tool, you change the code, and assuming that the environment remains the same, you basically maintain the same environment throughout code changes.
01:19:34.516 - 01:20:13.312, Speaker A: So this is essentially the tool that you want to integrate into your CI to keep the code safe. The Sartora prover actually is fairly complex technology, and I won't be able to explain this in this talk, but there is a white paper that I encourage you to read which actually explain everything behind the Sartor Approver. I'm just going to give you the basic idea. So basically, we run the compiler, the Solidity compiler, and then we have our own decompiler. We have our own decompiler that decompile the EVM code. Into something that we understand. So this is these three others code.
01:20:13.312 - 01:20:36.676, Speaker A: It's actually something that we understand. At the moment we are supporting EVM, but eBPF is coming and WebAssembly is coming. So we will support other blockchain. So we are basically building this stack to support all the blockchain that is important. Maybe Cairo, maybe others. So this is all in this three others code. And then we have our secret source.
01:20:36.676 - 01:21:11.936, Speaker A: We have actually tools that simplify the code. And that's very interesting. We run some static analysis. But this static analysis is not used to find bugs, it's used to actually simplify the code. And the interesting thing about this, and I don't know if you have seen in the process of simplifying code, we are making certain assumptions about what the compiler generates, but we are of course checking them. So we are not analyzing arbitrary code. We are checking some properties and I don't know if you notice, we actually produce many of the bugs in the Solidity compiler.
01:21:11.936 - 01:21:40.376, Speaker A: So these are bugs which were automatically identified by Oyotool and they are bugs in the Solidity compiler itself. So this is actually an acknowledgement from the Ethereum Foundation. The other thing that we do, we take the code and generate a mathematical formula. Here we are building on Smt. How many of you are familiar with Smt? Few. Fantastic. So Smt is actually a technique.
01:21:40.376 - 01:22:12.920, Speaker A: I mean if you ask people in computer science they think that satisfiability is a hard problem. They're of course right. But the idea is even though it's a hard problem, there are actually tools that actually can solve many many instances of that. And the idea is there are even tools for Smt and we actually use all of them. We contribute open source to them. So basically part of the things that we do in this thing is we make Smt reasoning easier. And if you don't know what's Smt think about linear programming, think of some kind of mathematical reasoning.
01:22:12.920 - 01:22:45.264, Speaker A: So what we do, we actually and we also going to do more. We do a lot of things to reduce the number of timeout, to reduce the failures and in particular we care about financial systems because this is where all our clients are. So I'll give you a very simple example. We'll give you more. So this is a very simple code. You see you transfer and the environment that you want is that the total is equal to the sum of the balance. And you run the tool.
01:22:45.264 - 01:23:21.832, Speaker A: You see, the tool automatically identify a bug in which Ellie's transferred to herself and basically the money is gone. And this is a case sorry, increased. So the sum is increased, my mistake. And these kind of bugs, these are the bugs that we are finding with these kind of techniques. These are the violation of your environment. And so this is a case, it was found by the tool. When you are correcting the code we can actually produce a mathematical proof that the environment is maintained.
01:23:21.832 - 01:24:04.040, Speaker A: It doesn't mean that the code is bug free, but at least it means that the rule that we check is maintained. So that's the idea. Actually we have two type of customers and maybe now three. One of them are security researchers, which is fantastic. They are using the tool which is great like in the spearbeat in the now with us ave code arena. The other one our developers and the third is us. And the interesting thing people that use us, the maker team, they have actually a very very good team, I think about seven or eight solidity developers that are using this tool and they are checking environment about the code.
01:24:04.040 - 01:25:09.552, Speaker A: And the environment that Kurt wanted to check is that the coin is stable. So everybody know what makers does, right? So maker implement a stable coin. So the environment says that the coin is stable and in fact the tool found out an example in which in seven cases it is violated. So this is something that was found by the tool and basically you see there is a need function and the init function the environment that has is that the depths is equal to the sum of the collateral and the tool actually find that in certain cases, you can violate this environment. And when you fix the code, it show you that this environment holds. So this is a very very good use case of this technology finding bugs in your code and these are potentially hard to find bugs that we are finding with this kind of technology. I want to basically close this first part by showing you a critical bug which again found by the tool and Alex will show it later.
01:25:09.552 - 01:25:43.556, Speaker A: So this is the Sushi Swap Trident. It's a code and actually related to the talk that we've seen by Jeremy, it's basically implement what Jeremy said, the dual liquidity pool. And you see the code here there's bear single. This is a code which is in solidity. It's probably hard for you to see the bug, but I mentioned it in red. Basically the code is not using. It's more interesting bug than the bug that we have seen in the previous slide because the code is actually not using correctly the API.
01:25:43.556 - 01:26:29.236, Speaker A: And as a result there is a rare bug in the code and this rare bug is found by our tool and this rare bug will allow you to completely deplete the money in the contract. Okay, so what is the nature of the bug? So you have to say what is the invariant? And the simplest invariant about liquidity pool is that the multiplication is constant. But I even use something simpler. It's basically say if you have two tokens, they cannot be that one of them is zero and the other is not zero. So if one of them is zero, the other one better be zero. And here you see Bob and Alice in the trident. And there is an operation in the code that says burn single in which L is burner holding.
01:26:29.236 - 01:27:24.856, Speaker A: And what happened this actually you see the number is 400 and the number of B is zero. So basically you see that the environment is broken now, okay? So this is a violation which was found by our tool. By basically running our tool, we found this violation. We found this, I have to say, before the code was deployed, most of the bugs that we found are bugs that were found before the code is deployed. When we are finding code bugs after the code is deployed, we are very, very quiet about that. So what's going on? So in fact, what's going on here? This bug is a bug which is very, very you notice that the system also always start from our invariant. So the question you can ask yourself how can you have this invariant? How can you violate this environment and what are the implications of it? So this is the case that you see what happened here.
01:27:24.856 - 01:27:56.372, Speaker A: Alice deposit 100 coins, bob deposit 100 coins and then Alice transferred 200 coins. So basically you A is 400 and the token B is 200. Now the burn single, that basically violates the environment. And finally what happened? Ellie swapped the money. So basically Ellis give one coin and get all the money and Bob lost all his money. So this is a bug which was found by our tool. But our tool doesn't see all of that.
01:27:56.372 - 01:28:18.670, Speaker A: That's the beauty. Our tool only reason inductively. So what does the tool do? It looks into these two states. It looks into one state which satisfies the invariant. There is a transition from this state into a state which violates the environment. And that's the beauty of the induction. Of course, if you work in manual formal verification, it's exactly the same.
01:28:18.670 - 01:28:57.540, Speaker A: You reason about execution. We start from a state satisfy the environment into a state, violate the environment. So that's the idea I want to close before I let Alex present the demo. I want to actually sort of tell you some things. And these are of course not just my own experience. This community has been along for a long time. Formal verification has been used in hardware industry, especially after several high important bug were identified.
01:28:57.540 - 01:29:36.272, Speaker A: So the question is what's the value of formal verification? And this is something that we are trying to understand. Some of them, of course, are not just for Web Three. So the first thing that people think about formal verification in particular in Web Three you see a lot of people speaking about is formal verification is about proofs. But I think the most value of formal verification are bugs. These are hard to find bugs that you are finding during the process. And I think that we see and I try to make it in this talk. And when you use a Satora proof, you can see that it generates a proof.
01:29:36.272 - 01:30:13.196, Speaker A: And of course the proof has some value, especially if your environment are okay. But the most interesting thing, these are bugs that are prevented. The other thing I already alluded to that is that the hardest problem people think about formal verification is go back to Rice theorem and others that say this is a computationally hard problem, and of course it is the case. But we think that the hardest problem is actually coming up with these invariants. And actually that's why we need the secure. That's why we need people like you to help us write interesting invariant. And we are trying to incentivize people to write interesting environment.
01:30:13.196 - 01:30:43.332, Speaker A: Like, for example, with the Ave, we incentivize people with money. If you write a good environment, you get a reward for that. The other thing is that, and that's also people are thinking about it. When you think of formal verification, people say, oh, I formally verified that. And one of the giant of computer science, Knuts. How many of you know Knuts? Great. So Knuts of course, is a fantastic computer scientist at Stanford, which has contributed to many fields, including formal verification.
01:30:43.332 - 01:31:14.304, Speaker A: And one of the joke that he said, I don't believe this code, I formally verified it, and I've never tested. I think this has a very good thing. The idea is that basically, formal verification is just one thing. It doesn't guarantee that your code is correct. It only guarantees that the environment are maintained, which is of course, increase your security. But it's not a bulletproof. The other thing in the Web Three, that people think and they approach us, they say, look, auditing is expensive.
01:31:14.304 - 01:31:42.856, Speaker A: We want you to replace formal auditing. It's not what we are trying to do. We are trying to complement auditing. I already mentioned that there are auditors who use Satora, the human and the computer, they basically help each other. It's not like an automatic, automatic car when you just have it because you need the human. For example, the human can actually find bugs after us if we don't have the right rules. And then we can update the rules.
01:31:42.856 - 01:32:19.000, Speaker A: The human even already we have seen auditors that found bugs in our rules, which is fantastic. It's just another artifact of the code to analyze. And we want the human and the auditor, the human auditor and the tools to help each other. We are not replacing the auditor. And maybe the last thing I want you to take, and that's again, something that formal verification is something that you need to think as early as you can. Whether you engage with Sartora or not, it doesn't matter. But you need to think of formal verification, in particular, formal specification, as early as you can.
01:32:19.000 - 01:32:49.840, Speaker A: And use the tool, even use our tool as early as you can, even when it's feature complete use. The tool. So this is the idea. You want to use this technology as early as you can because it will prevent bugs and it's easier also to use it because the code gets more complex. It's harder to use this technology. So we want you to start use this tool as early as you can. So, in conclusion, I want to basically tell you sort of three things.
01:32:49.840 - 01:33:28.156, Speaker A: One is that bugfinding is hard without a tool. Without our tool, it's hard. But Sertora actually makes it interesting by using this inductive reasoning, by the idea that you write an invariant and we are looking for violation of this invariant. The other thing that I didn't get a chance to explain, but there are talks that we explain that and there are technical people on our team. We are actually producing something which is interesting in terms of new algorithms. We are actually innovating in the area of static analysis and innovating in the area of constraint solving. And we are combining these techniques and you are welcome to listen more.
01:33:28.156 - 01:34:02.040, Speaker A: What I talk to you is the idea of automatic formal verification. This is the idea that you write invariant and it's like a quality assurance tool that checks the invariant and either find a violation and this violation can be rare or prove absence of violation. So this is it. Please. Want to scan the technology paper. I suggest what we do now. I will answer some questions if they are and while I'm asking questions, but maybe just let people scan, alex will set the demo.
01:34:02.040 - 01:34:21.170, Speaker A: Please stay. The demo is interesting, but I'm happy to take questions on my talk on formal verification. Please, if there are questions, I would love to take some maybe while you said you can set them. Everybody scan? Yeah, you have question.
01:34:25.780 - 01:34:55.704, Speaker C: Would former verification still work for systems that are not closed? So, for example, for, let's say, a DeFi application like uniswap, right? You can probably write an invariant where the total USD value of assets in a pool will always remain same after swaps. Right. But say for a system which is not closed, source like a cross chain liquidity pool where tokens are coming in on one chain but they're actually leaving on another chain. So would formal verification still make sense in a scenario like this? What do you suggest here?
01:34:55.822 - 01:35:19.600, Speaker A: Interesting. So the question is, will formal verification make sense in a system that is not closed? So the answer is it makes the most sense when you have a system which is not closed but it's more complex. Okay. And we work with compound. For example, who's calling uniswap? We work with bridges that actually some of the code is not available. So the idea is modularity. You basically have a requirement on one thing.
01:35:19.600 - 01:35:50.200, Speaker A: For example, uniswap is monotone. And then the question is do you trust this thing or do you check it? So you can do two things when we analyze the code of compound. For example, we made certain assumption on the uniswap. And the question is, do we check them or we don't check them. At the moment, we don't check them. We can check them statically or dynamically. So the idea is, when we work with some clients, they actually call other clients and it's actually the beautiful thing of formal verification and you even seen it, that you are finding an API violation.
01:35:50.200 - 01:36:20.052, Speaker A: You are finding a case that I'm calling your code and I'm not able to actually satisfy the precondition for this. So yes, this is actually the most useful application of formal verification is code. And actually there are a lot of interaction between the code and yes, we can do it either statically or dynamically. We can do it statically. It means that we need to analyze both pieces of code. We can do it dynamically. Or sometimes it's just assumption because sometimes it's code that we don't see.
01:36:20.052 - 01:36:57.776, Speaker A: Somebody is calling an oracle or somebody is calling something that is not in something that we can analyze. It's a bridge, for example. So yeah, but it actually is the case. We are finding bugs because if you have a bug with respect to the assumption, now you are implementing the compound money market and it is calling the uniswap. And the question is you are writing these assumptions and then you are checking the correctness of the code, not with respect to the actual code, with respect to the assumption that you made. So yes, it is very, very useful and it is one of the most useful application of our clients. But it's not easy.
01:36:57.878 - 01:36:58.956, Speaker C: That clarifies a lot.
01:36:58.998 - 01:36:59.750, Speaker B: Thank you.
01:37:04.920 - 01:37:06.550, Speaker A: There's another question here.
01:37:08.760 - 01:37:28.768, Speaker D: So when you have these formal verification specs and you say that formal verification is competitionally expensive, do you mean that you're fuzzing the invariant and you're checking all of the states, like in a brute force method? Or does that competition go into calculating a mathematical proof that this state will never occur or is it like a brute force?
01:37:28.884 - 01:37:52.304, Speaker A: So our technology doesn't do fuzzing, we don't enumerate the behaviors. What happened is that we compile your code into a mathematical formula and the mathematical formula enumerates all the behaviors. And the question when it work and when it doesn't work. It's a very, very hard question. In general, for example, when you have linear equation, it's easier to solve. So we are not enumerating the behaviors. That's the beauty of this technique.
01:37:52.304 - 01:38:27.820, Speaker A: It's actually giving you exhaustive think. Like if you have, for example, in the ethereum, you have like an integer which is 256, two to the 256. We model it as an arbitrary integer. So we give you a mathematical proof. And when is it hard? When is it easy? It's very, very hard to know. We actually have a lot of algorithms and sometimes they surprise us. How well and how sometimes it's really surprising, these constraints over that they can actually succeed on things which are very, very complex for a human, but sometimes hard, for example, when you have interest rates.
01:38:27.820 - 01:38:42.496, Speaker A: We came to know that we are not doing fuzzy. We are enumerating all behaviors. That's very, very different. It's, of course, more expensive. But we are doing fuzzing. We're not doing fuzzing. We are basically enumerating all behaviors.
01:38:42.496 - 01:39:29.040, Speaker A: And of course, it's done by this reasoning about formula. You write a formula like say x square is equal to four and then basically the system will find out that x is two. So that's the idea. You write a formula and then we have open source tool that we are using to find a solution to the formula and a solution to the formula. It means you have a bar and if you find out that there are no solution, we can generate what we call a proof tree, which is actually indicate that you don't have a bar. But this proof tree enumerates infinite number of behaviors. We are not looking into finite behaviors that's a very big difference from fuzzing we can do fuzzing for example to get initial answer and we are actually doing it because sometimes it takes 2 hours until you get an answer and the user is frustrated.
01:39:31.800 - 01:39:32.950, Speaker B: Great, thank you.
01:39:36.360 - 01:39:51.272, Speaker D: Which Smt solver have you used and why? Which Smt solver have you used? I'm assuming either Z Three or CVC four and which one do you think has given you more empirical? Empirically can you comment on the performance of these?
01:39:51.326 - 01:40:28.180, Speaker A: Yeah. So that's a very difficult question because I'm friend of all the people, but fortunately, actually it's very surprising all. So it's actually on different benchmarks, on a different code, they behave different and historically Yikes is much older and people have invested less. But we find that in some of our example yikes does fantastic work, z Three does CVC. We are also supporting Satora. We raise money to support here we are supporting of these kind of things and we are improving. We are adding now support for Z Three for large bitwise operation.
01:40:28.180 - 01:41:00.232, Speaker A: We are working with the answers and also we are thinking of combining them because they have this idea of learning lemmas and sometimes even when you have a timeout you learn something and we can actually combine them. So we are agnostic at the moment we are using the three of them and our user I think there's a flag but I don't know alex there's a flag which one to use but I think most users just let the system choose yeah and there are machine learning techniques that we are doing to optimize that.
01:41:00.286 - 01:41:05.416, Speaker D: Yes, so just to extend on mean since I've seen it in your tool.
01:41:05.438 - 01:41:26.592, Speaker A: Chain so just for other people this Smt solver these are techniques that are used under the hood here which are used of course in Microsoft we basically compile the code and the environment into huge mathematical formula sometimes few megabytes actually. It's interesting that it works. And this few megabytes of formula, it gives to the solver and the solver gives the solution. Yeah.
01:41:26.726 - 01:41:33.932, Speaker D: So, tool chain, I've seen that you have a lot of custom software, right, for your certain application, for your certain requirements.
01:41:33.996 - 01:41:34.320, Speaker A: Right.
01:41:34.390 - 01:41:40.708, Speaker D: Similarly, have you thought of just creating your own solver, combining all the strengths of if at all?
01:41:40.794 - 01:42:21.216, Speaker A: Yeah, so that's very interesting. So the question is, have we thought of creating our own Smt solver? We don't know at the moment. We have been around for four years. I think it will be difficult for us, but maybe we are more thinking of contributing small things to existing solvers and supporting them. But yeah, it's very interesting to build something specifically for DeFi. What we are thinking more is sort of thinking something on the opposite, trying to tell like some kind of design pattern, tell people if you write the code at the moment, one of things that we are doing, we used to charge a flat price for using the service, but we are no longer charging a flat price. If you have code which is complex, you pay us more.
01:42:21.216 - 01:42:43.530, Speaker A: But the idea is we want to actually reduce the price by telling you if you write the code this way and if you write it more modular, it would be easier for us and it will be cheaper for you. We are trying to scale up. But yes, down the road maybe developing a server for DeFi. I know the Ethereum Foundation is interested in that, but yeah, it's interesting.
01:42:49.570 - 01:42:56.210, Speaker C: So the challenge now has transferred to writing specific and very tight invariants.
01:42:58.070 - 01:43:02.820, Speaker D: That doesn't seem like an easy task for a complex project.
01:43:03.190 - 01:43:11.222, Speaker C: Invariants might get easily complex and there might be bugs in the specification. So you mentioned there's another project which.
01:43:11.276 - 01:43:13.394, Speaker D: Kind of checks the correctness of invariants.
01:43:13.442 - 01:43:15.382, Speaker C: Could you please tell a little bit.
01:43:15.436 - 01:43:45.360, Speaker A: More about yes, so you're absolutely right. So the issues that if, you know, I mentioned in computer science, we mentioned KNUS, but another giant of computer sciences I'm sure that most of you know is dykstar. And I mentioned that Dykstar, of course invariant go back to a Platon. But Daxter said that you cannot write a program without writing the environment. That's of course correct in principle, but it's very hard to write environment. You are absolutely right. And we also find out that when we are writing environment for our clients, we made them wrong.
01:43:45.360 - 01:44:18.938, Speaker A: But the idea is the question is what does it mean wrong? If it's wrong in a sense that it finds a bug that our tools say and then we check the environment, that's one thing. But the thing that we are worried, we are worried from environment that actually holds and give you a false confidence on your code. And we already have one client who actually use us in addition to auditing. And then basically if somebody found a bug and they didn't accuse the auditor. The customer actually wrote the invariant and there was no failure of the tool. The problem that the environment was topology. He writes something like seven is greater than five.
01:44:18.938 - 01:44:49.070, Speaker A: And the tool of course was able to prove that seven is greater than five. And it's increased the confidence of the protocol. So now, if you look into the Satora, even if you look into the version that you have, this cannot happen. We have basically a vacuity checker. We have something that checks if we have a bug, it's a bug. But if we verify your property, we try to check if it's vacuous. We try to check, for example, this kind of bugs we can avoid.
01:44:49.070 - 01:45:29.220, Speaker A: And what we do, we do mutation testing, we mutate your code, we take your solidity code and you change it. And if we change it and the environment still holds, we suspect that something is bad. And this is very good for us because as a company we are basically engaging with larger community. And the idea is people are submitting environment and we have no idea this environment good or bad. So we are using these tools to evaluate the environment that the community submit. For example, in the average project, I think we have 25 security who are submitting environments. So we are using these tools to check the environment, both the environment that are produced by our team, but which are written by other people.
01:45:33.870 - 01:45:42.074, Speaker D: So you mentioned about correctness preserving transformations. Could you elaborate a bit on that? Like how do they preserve the correctness.
01:45:42.122 - 01:45:45.054, Speaker B: Or how do they work or give.
01:45:45.092 - 01:45:46.640, Speaker D: An example of that maybe.
01:45:48.370 - 01:46:46.050, Speaker A: I mentioned the correctness preserving transformation. So this is of course we basically did wait, so I don't know how much the way code is generated in the EVM, but it's basically what this idea was called, bump allocator. So basically memory is allocated and what we observe certain properties of these bump allocator. And in order for that to simplify the code, this is how we identify many of the bugs in the solidity compiler itself. And we are actually submitted an article about that and we will publish it, of course. So these are results that basically these are techniques called static analysis, which actually infer some property of the code. And intuitively the idea is, for example, you have a load and a store and instead of letting the Smt reason about it, we can reason about it.
01:46:46.050 - 01:47:09.526, Speaker A: And this is a game changer. It takes something from timeout and convert it to seconds. It's a game changer. Of course, the Smt, it's a complex technology. The less you can call it, whether all the Smt solves, the better. And we want to do more like understanding the values and this how we say correctness preserving transformation. But of course we haven't proved that they are correctness preserving transformation.
01:47:09.526 - 01:47:19.222, Speaker A: It's code that we have written in our tool. But of course, up to a bug in our tool, they are correctness. Preserving transformation. Yeah, that clarifies.
01:47:19.286 - 01:47:20.060, Speaker B: Thank you.
01:47:25.230 - 01:47:35.620, Speaker A: I think we let you can you change?
01:48:17.920 - 01:48:51.864, Speaker D: Can you hear it through the speak now? I think you can hear. Can people in the back hear me? All good. Okay, so I'm going to take you through a bug that Muli also spoke about. It's the bug that we found in the Trident constant product liquidity pool. The idea is to give you some sense of how you work with the sorter approver to find. Also, if you want, you can also start working with me. If you go to demo Satora.com,
01:48:51.864 - 01:49:50.040, Speaker D: or just go to Satora.com and look for the demo button on top right, you will land on this page. Just go to the interesting bugs part and click on the custom product broken piece and this page will open up. So on one side, you have the solidity code that we are trying to verify. On the other side, we have the spec that we've written and the tool takes both of them together and mashes them together, does a bunch of optimization, and then comes up with the logical formulas which are then fed into the Smt solver all that stuff that Mulli spoke about. But let's first look at the solidity code and then we'll look at how we went about verifying it's. Essentially, as I said, it's a constant product liquidity pool, which implements an ERC 20 protocol also for the LP tokens that it mints and distributes.
01:49:50.040 - 01:50:32.840, Speaker D: So for those of you who don't know, a liquidity pool is basically where you provide liquidity and people use that liquidity for various applications. In this case, this liquidity pool was going to be used for an automated market maker. And it was a constant product pool where people like you and me, we can supply liquidity to the pool. And as an IOU, we get back some LP tokens. These LP tokens from a liquidity provider standpoint can be redeemed for your share of the liquidity. So you get back some part of the protocol. So you'll get some part of the liquidity pool, so you get back some amount of both the tokens.
01:50:32.840 - 01:50:52.588, Speaker D: The idea here is that I'll talk about the properties later. I'll just quickly run you through the contract here. So it's an ERC 20 contract. On top of that, it builds functionality for the liquidity pool. So we have two tokens. It's a classic two token constant product sort of a pool. So we have two tokens.
01:50:52.588 - 01:51:37.360, Speaker D: The contract also keeps a track of the reserves that we have for each token. It also keeps a track of the product that it adheres to when it's swapping tokens. This product, of course, goes up and down based on the overall liquidity in the pool going up and down. But when you're swapping before the swap and after the swap, the product needs to remain the same. There's a mint function which basically mints the LP tokens whenever you add liquidity to the pool. And these LP tokens are essentially shares that you hold in the liquidity pool. So when you redeem it, you get a part of the liquidity, the tokens that are sitting in the liquidity pool.
01:51:37.360 - 01:52:27.280, Speaker D: There's a bunch of logic in here that is not relevant for this discussion, so I wouldn't go into it. This is the most important or most interesting function here. So typically in a liquidity pool contract, when you're redeeming your liquidity pool tokens for the underlying assets in the pool, you would redeem the tokens and the contract would give you back two tokens, some amount of each of the two tokens in the contract in the pool. But there is a special function here which it's called burn single. What it does is that when you are redeeming your liquidity, it allows you to pick one token that you want to get paid in instead of two tokens. It allows you that extra functionality where if you choose to get paid in just token one or token two, you can do that. So what this function does is it does two things.
01:52:27.280 - 01:53:30.832, Speaker D: First, it withdraws your liquidity based on the amount of tokens that you have, essentially what share of the liquidity pool you own. Based on that, it calculates the amount of each token that you're supposed to get. And then after that, it does a regular constant product AMM sort of a swap where it exchanges one token for the other, the token that you want. So the functionality for finding out the amount that you're owed by the pool is here, where it's calculating the amount of tokens you get for each token by looking at the liquidity and the total supply. So that's your share of the pool that multiplied by the balance for each token, and that gives you the amount. And then when you come further down, this is where it's taking a token and looking at the amount of that token that was calculated in the step before this. And then it's based on the new state of the liquidity pool, where of course now the reserves have gone down because you've withdrawn some amount from each of the tokens.
01:53:30.832 - 01:54:21.620, Speaker D: So based on the new state, it's going to figure out how much tokens you are eligible for of token B for the given token, a amount. And then it adds that amount that is calculating here to the amount that was calculated here. And that's the total amount it'll transfer to you in one single transaction. Okay, now I think I'll go to the spec after this. So the idea here was that as a liquidity provider, there are certain things that you would be wary of. If I'm providing liquidity to a liquidity pool, I would want to be sure that I should be able to withdraw that. So if I have LP tokens, I should at any point in time be able to exchange those LP tokens for the underlying liquidity pool assets.
01:54:21.620 - 01:55:45.936, Speaker D: Similarly, if I'm depositing something into the pool I should definitely get some LP tokens which is a proof that I've supplied liquidity to this pool and so I'm eligible to withdraw the money that I've supplied back. So the idea essentially is that from a liquidity provider standpoint, if you have certain reserves in the pool then there should be liquidity pool LP tokens out there in circulation, otherwise there's no way to withdraw the funds that are in the liquidity pool. Similarly, if there are liquidity pools in circulation, liquidity pool tokens in circulation, then there should be reserves in the liquidity pool itself because otherwise the tokens that you hold, they don't mean anything because you don't get to withdraw any liquidity that you supplied. So all these properties that I just spoke of, these are fairly high level properties. You don't really need to look into the exact implementation of the liquidity pool. Everyone can get that concept that if you've invested, I mean, supplied some liquidity to a liquidity pool, you should be able to withdraw it and you should get liquidity pool tokens which are essentially a proof that you've supply liquidity. So when we paraphrase that into a logical expression, this is what we get.
01:55:45.936 - 01:57:14.940, Speaker D: So this is essentially the invariant that we had written here, where we are saying that the total supply of liquidity pool tokens can be zero if and only if the reserves of the underlying assets are zero for both. The tokens. And when we run this very simple intuitive invariant which doesn't need you to look at any implementation, just a very high level thing that makes sense to you. When we run this against the tool it shows us a specific bug that we found and and yeah, so when you run the tool, essentially I'll tell you how we run the tool. So we invoke the tool and we give it the contract that we want to verify, we specify the contract that we want to verify in the solidity file and then we specify the specs that we want to verify the contract against and any other helper files. Like we would need some ERC, 20 implementations here to model some of the calls that are happening to ERC, 20 contracts and then the compiler that we need and some other flags that I don't think we need to get into right now. But yeah, essentially we are giving the tool the solidity file that we want to verify, the contract that we want to verify and the specs that we want to verify.
01:57:14.940 - 01:58:00.808, Speaker D: And this is what the report looks like. On the left pane here you have a bunch of rules and invariants that you've written. So if you've written multiple rules and invariants that you've run with the tool you would get a list here and for each rule it will either show you a green circle with a tick mark or a red circle with a cross on it that says violated, and it lets you do a deep dive. This was an invariant. And this tells me that this invariant failed in the preserved state of the invariant. So I want to talk a little bit more about how invariants work with Satora, Prover and Muli. Feel free to add if you want.
01:58:00.808 - 01:58:33.220, Speaker D: So, invariant, we prove invariants using induction. Invariants are proved in two stages. First, when the contract is deployed and the constructor is run. After that, the tool verifies whether the invariant that you've written, whether that invariant holds or not. And then if it holds after that, the tool will assume an arbitrary state which still conforms to the invariant. And then from that arbitrary state, it will call any function in the contract. And after that function is executed, it will again check if the state of the contract adheres to the invariant that you've written.
01:58:33.220 - 02:00:12.870, Speaker D: Once both of them are verified, once you established that after the contract has been deployed and constructed, the invariant holds. And after starting from an arbitrary state which conforms to the invariant and running any arbitrary function in the contract, the state of the contract still conforms to the invariant that you're testing against by induction, you can prove that this contract, this code, will always adhere to the invariant that you're trying to verify. So the first stage of the invariant is what you see here, what we refer to as in state. This is what is verified after the deployment and the construction. And we see that the tool tells us that what's happened here. Just give me a minute, guys. Okay, so, instate is, as I said, the first stage of invariant checking, where it checks it right after construction and preserve state is a second state where it checks after that, when it assumes an arbitrary state which conforms with the invariant and then checks against all the functions in the contract.
02:00:12.870 - 02:00:43.280, Speaker D: So the preserve state is what is failing here. So this has to fail for at least one function in the contract. So when we click on it and do a deep dive, it shows us that burn single is the function. I hope everyone can see this is people on this side able to see this. So burn single is the function which is causing an issue for us. So, again, we further click on it, click on the error. And now you start seeing things popping up on the right side.
02:00:43.280 - 02:01:17.398, Speaker D: It is a bit wonky, yeah. So this is a section that shows you all the rules that you've written. Within those rules, if you drill down, it'll show you further exactly what part of the rule or the invariant has failed, and specifically which function has failed you. This part will tell you the call trace. Call trace is essentially showing you a detailed view of the entire execution that the tool has gone through and it'll help you understand exactly where the error is. This section is where we have the variables and call resolution. This section gives you more information about the exact values of the variables in invariant.
02:01:17.398 - 02:01:58.840, Speaker D: Since we don't have many variables, you don't see much here. But if you write a rule and I'll show you what a rule looks like, you've already seen what an invariant looks like. If you write a rule in which you've specifically defined certain variables that you want to track the state against, that you want to track against the state, then all those variables will get populated here. And you will very clearly, quickly, clearly see a snapshot here of what the value was for those variables before and after some function was called. And it makes it a lot easier for you to understand the counterexample. Now I'll take you through the call trace to explain to you what's happening here. I'll get to the preserved block after this.
02:01:58.840 - 02:02:47.960, Speaker D: It's an additional functionality we have on invariants, which makes it a little more useful when we have additional specific preconditions that we want to apply with invariants. But yeah, you see here that the tool says assume invariant in prestate because that's where it starts when it comes to proving specifications, proving invariants in prison state. So it assumes an arbitrary state and it ensures that that arbitrary state conforms with the invariant. And then it runs a function. In this case, it's running the burn single function. We click further and it tells us more details about it. So at this point, let me take you back to the burn single function so it'll be easier for you to understand what's happening here.
02:02:47.960 - 02:04:03.550, Speaker D: Mint burn single. Yeah, so the Burn single is getting us parameters and a token address liquidity, which is essentially the total number of LP tokens that the person has and the address of the recipient. So the address that gets all the tokens that are withdrawn from the liquidity pool, that address. So what it's doing is it's using this liquidity to calculate the amount of tokens for both the tokens that need to be paid out to the user from the liquidity pool. So it's using the liquidity, the total supply, using that fraction on the total balance of the contract for that token, calculating the amount similarly for the other token, and then it's burning the liquidity tokens because now you've withdrawn the liquidity. And then based on the token that you've supplied here, it decides which token needs to be swapped into the other token. And that calculation is done here through the get amount out function where you supply the amount of the token that you want to swap out of.
02:04:03.550 - 02:04:32.258, Speaker D: And you also provide the latest state of the liquidity pool. So this is the reserves of the two tokens after the liquidity has been pulled out. So the amounts are updated by reserve minus whatever. Amount you're withdrawing from the liquidity pool. And then after that, that amount is added to the amount of the token that you want to withdraw. And then we do a simple transfer of that token that amount to the recipient. It seems fairly straightforward.
02:04:32.258 - 02:05:24.566, Speaker D: But where things go wrong is that the specific example that the tool shows us here is that when we call the burn single function with certain amount of liquidity and some recipient that's not important right now, but we see that the tool is showing us in call trace these steps where first the function is checking for the reserves, it's checking for the balances. And these balances are being used to calculate these amounts. So we see that clearly here that it's checking for the reserves. It tells us that it checked for the reserves and the values that it got back were five and four for both the tokens. Similarly, the balance amounts that it got were 15 and four for both the tokens. And then it checks the total supply, that is a certain amount. Bear in mind this total supply and the liquidity that's been given here.
02:05:24.566 - 02:06:13.398, Speaker D: So this liquidity is smaller than the total supply. It's supposed to be a share that you hold in the liquidity pool. And after that we call the burn function. So right now we are here. So we've seen these steps, we've seen this step and in between we've gone past this calculation where amount zero and amount one has been calculated based on the liquidity and the total supply that we've saw, we've seen. So after this we go to the burn function and then we are looking up the tokens for so this is the lookup that you see here for the tokens and eventually we end up calling the why is that?
02:06:13.484 - 02:06:19.596, Speaker A: Call yeah.
02:06:19.618 - 02:07:24.368, Speaker D: So eventually we end up calling the get amount out function and that is this function which is helping you swap from one token to the other. And one peculiar thing that we see here is that basically you're providing it some amount of a token that you want to get rid of and that's this amount and you're telling it that right now the state of the liquidity pool is this. And if you notice that one of the reserves for the liquidity pool has already gone down to zero. And that begs the question as to how this happened. If one person we can clearly see here that this user has a certain amount of liquidity which is clearly less than the total supply, which can be seen here. If somebody's really good with Hexadecimal math, you can see that it's clearly a subset, which means that the person does not own the entire liquidity. So if they've done a withdrawal, it shouldn't be the case that they've completely drained out the reserves for any one token.
02:07:24.368 - 02:08:32.088, Speaker D: But that seems to be the case here. So that takes us back to the code and we want to understand what happened. And that's when we realized that the way these amounts are being calculated is wrong because we are using these liquidity shares and multiplying that with the balance of the contract. And we see here that the tool shows us that the balance of the contract is different from the reserves that it's tracking the liquidity pool against. You can see for token a, the reserves are five and the balance is 15 here. So if you calculate the same small share against an inflated balance amount for a token, it could very well be the case that inflated balance could be equal to or greater than the actual reserves being maintained in the liquidity pool. And here in this specific example, what the tool is telling us that the balance was such that for this given amount of liquidity and total supply, this amount ended up being exactly equal to the reserve of token zero.
02:08:32.088 - 02:09:09.670, Speaker D: In which case when we called the get amount function, this value ended up being zero. I mean this value ended up being zero and the other value was whatever was left after the swap. And if we look at this function in more detail, we figure out that if one of these reserve values is zero, then what happens is that the output value that it's returning, which is basically the number of tokens that you'll be able to swap into, that number is essentially equal to the total reserve. So what's happening here is that.
02:09:11.880 - 02:09:12.196, Speaker A: In.
02:09:12.218 - 02:09:57.908, Speaker D: This particular case, when you call the burn single function, it's allowing you to withdraw the entire liquidity of the second token. So the balance for the first token was inflated because of which the share that was calculated was wrong. It was equal to the reserves of that token. And then when you called the get amount out function, it ended up giving you the entire liquidity that you had for the second token. So at this point we've already broken the variant and that's what the tool tells us. The tool tells us that we've reached a state where one of the reserves has gone down to zero. When the total supply of liquidity pool tokens is still nonzero, one of the reserves has gone down to zero.
02:09:57.908 - 02:11:41.612, Speaker D: So this has told us that that intuitive sort of a property that we had thought of in the beginning, that if there are liquidity pool tokens in circulation, then there shouldn't be a case where you can get to a state where one of the reserves or both of the reserves are drained to zero. But that's really possible. But now the question is how can someone exploit this weakness of the code? And we looked into that and the attack basically is that you take a flash loan, you take a flash loan, send that money over to this contract and in the process you end up inflating the balance of that token in the contract. The way this contract keeps a track of total reserves versus the total balances is that there's a function called update that's called you should see that function yeah, for instance here. So every time it's messing with the reserves of the contract, after it's done messing around with it, it will check what the latest balance is and then update the reserves accordingly. The same thing happens in the Mint function when you've added more liquidity to the pool and it's given you some liquidity tokens at the end of it, it'll make sure that that newly added liquidity is also captured in the reserves that the contract is tracking. If you take a flash loan, send that money over to this contract and jack up the balance for one of the tokens, but you don't end up calling the Mint contract.
02:11:41.612 - 02:12:52.336, Speaker D: Then what you've done is that the reserve? Continues to be what it was but the balance is inflated way beyond the reserve value at this point if you using your tiny share the limited number of LP tokens that you have if you call the burn single function you will be capable of essentially your share gets inflated. So what happens is all these numbers get calculated against these numbers, they get calculated against an inflated balance number. So for the same small share you're getting a much bigger number. And if you manage things such that this big number is exactly equal to the reserve value at that time, then you can make the get amount out function to give you the entire liquidity of the other token. And once you've done that, the next step of the attack is that you call the swap function here and the swap function will again. This time you reverse it. This time you give it the other token with just one amount and it will again because one of the liquidity pool has been drained.
02:12:52.336 - 02:13:42.228, Speaker D: The token B has been drained. So this time around, because that was zero in the computation, it will give you the entire liquidity for token A. So what you've done essentially is you've drained the entire liquidity for both token A and token B. There are still LP tokens in circulation and there are no reserves to back it. So our tool told us that it's possible with the get single function, it's possible to get into a state where you break the invariant. And then looking at that example and thinking a bit more on how to make it a complete exploit, we figure out this overall larger exploit. It should give you some sense of how powerful the tool is, because you've not even you've not even had to look at the implementation of the protocol.
02:13:42.228 - 02:14:42.324, Speaker D: You've just, from a liquidity provider standpoint, thought of the very basic thing that the protocol should give you in terms of security, and that is the ability to withdraw your liquidity at any point of time. And just put that down into a simple, logical formula and run that against the tool and the code and you get this invariant. The tool obviously is much more dynamic, while the strongest properties that you can verify with the tool are invariants, which are as simple as this one, because they cover a large part of the code. An invariant like this, as broad as this, can break because of any small functionality, any small function in the code. So you're not restricting your checking to any specific part of the code, but you're looking at the entire contract. So these are the strongest invariants. If you can think of high level invariants like this, which pertain to any part of the contract, they'll always give you the best results.
02:14:42.324 - 02:15:40.844, Speaker D: But the tool also allows you to write more specific rules. We have something known as you can write rules. Rules are essentially a combination of a prestate, some function execution, a post state, and then an assertion based on the state transition that might have happened. You can write rules for specific functions, you call specific functions, then track what happened in the state change and assert the change that should have happened and see if the contract breaks out of that rule. In any case, you can write more general rules where you can have so what we call parametric rules. These rules are run against every function in the contract. So you write some preconditions about the state before then any function here.
02:15:40.844 - 02:16:29.676, Speaker D: So this call basically means that the tool will call every single function in the contract with any arbitrary arguments. So this call data ARGs is a dynamic data field which the tool populates with all possible ways, in all possible ways to fit all possible function signatures. And it gives you complete coverage in terms of the inputs that you can feed into these function calls. And then it checks the state after that and any assertions that come after that. So you can use some aspects of the tool to write even very specific unit tests for small functions. So like as Mooli mentioned, that sometimes the code, I mean, this tool is bound to fail. It's only a matter of how complex the code is.
02:16:29.676 - 02:16:58.820, Speaker D: And when we encounter very complex codes, sometimes we have to take a more modular approach, which involves looking at the most bottom level contracts, looking at individual functions in those contracts, verifying the functionality of those contracts. And once we are very sure that those functions work exactly the way they're supposed to work, then we summarize those functions and we assume that they work correctly for the more higher level contracts. So that sort of eases the job that the prover has when it comes to verifying the more higher level contracts.
02:16:59.640 - 02:17:00.004, Speaker A: Yeah.
02:17:00.042 - 02:17:47.706, Speaker D: So you can write rules very specific to certain functions, you can write more generic parametric rules to test out function execution, or you can write very high level properties using invariance and the tool will tell you if your code is capable of breaking out of it. So, yeah, that's what I had. Any questions? What's that? The tool is free. Mooli has said this many times, but let me say this again. Go to the demo page, work with this as much as you like. It's free. You can plug in your own code here, your own spec here.
02:17:47.706 - 02:18:06.746, Speaker D: We have tutorials, free tutorials on GitHub. Please learn. I'm very new to it. I went through securium. I learned the tool in a matter of two weeks and then used it on a project. It's very easy. We have a very strong community support internally in Satora.
02:18:06.746 - 02:18:47.828, Speaker D: We give a lot of importance to that. So if you're curious, if you want to learn the tool, our language CVL is very similar to solidity. So if you're familiar with that, it should be very easy for you to learn Satora prover. And yeah, use as much as you like, it's free. And get more used to it and talk to us if you're interested in working more with us. Anything else, Mulli? Yeah, good. No.
02:18:47.828 - 02:18:58.550, Speaker D: Are you asking if we work with multiple contracts? Can you write.
02:19:08.150 - 02:19:10.290, Speaker A: Properties with multiple contracts?
02:19:10.730 - 02:19:53.918, Speaker D: Yeah. So these properties, they don't necessarily have to be from one contract. Like, if you have one higher level contract and that contract interacts with multiple lower level contracts, which then again, go and interact with other lower level contracts, these properties will be verified on the system as a whole. So the invariant is checking every function that it gets from the high level contracts all the way down to the lower level contracts, unless you apply some sort of a function filter. That's, again, another feature that we have. But yeah, it runs across the board. And when it runs on higher level contracts, there are nuances to how the tool models that interaction, that intercontract function calls.
02:19:53.918 - 02:20:53.026, Speaker D: If you like, it can assume the worst case scenario, assume the most arbitrary behavior coming back from that external function call, or if you have specific implementations that you want the tool to work with, you can link specific implementations. Or we have something called dispatcher, which again, adds more nuances as to how these calls are routed. But yeah, our philosophy is that we want to be extra careful, so we always over approximate. When in doubt, we over approximate. So if you don't specify any particular implementation or if there are multiple implementations, but you don't tell the tool how to use those implementations, the tool will go ahead and assume the worst case scenario that this call could return anything. So it will do all that heavy lifting of checking all possible scenarios there. But if you're very sure of the nature of that interaction, the implementation on the other side, then you can make life easier for the tool by giving it the exact implementation.
02:20:53.026 - 02:21:20.320, Speaker D: So again, the tool is bound to fail if the code is complex enough. And if these interactions happen with a lot of what we call havocing havocing, essentially assuming that haywire. So if you give it a lot of havocing, chances are it will fail. But yeah, the art is in finding that balance. How do you get this very powerful tool to work for your project?
02:21:23.890 - 02:21:26.474, Speaker C: The tool can sometimes timeout.
02:21:26.522 - 02:22:28.786, Speaker D: So before writing rules, specification, do we get a sense that this particular rule will timeout or like after running the tool? Only usually what we do is before starting on a project, we do something known as a sanity check. Sanity check is a very simple rule which says that this is the function. It's a parametric call. If you remember seeing the parametric call, where we just do an f ARGs call, which is the call to every single function in the every single external function in the contract and all the contracts in the scene, actually. And we just call the function and do an assert false. So an assert false is bound to fail as long as the prover gets to that point. But if the code that you're trying to verify is extremely complex, so complex that the tool is not able to get to that point where it's just too caught up in all the execution and all the branches and all the loops that are happening in the functions, then it'll never get to the assertion and it'll time out before that.
02:22:28.786 - 02:23:13.380, Speaker D: In that case, your rule will pass because you never got to the assertion and the default is that it passes or it times out. If you do a sanity check, then it times out. So the first thing that we do with any complex project not just complex, any project, unless it's just one single solidity file. And it's very obvious that it's a very simple thing, which is a rarity. I've never seen such a project in my time with Satora. But yeah, the first thing we do is to do a sanity check, which gives us an idea of which functions are passing, which tells you which functions are easy and which functions are failing, or rather timing out, which tells you that it's too complex. So it gives us a sense of where we need to optimize things and how do you need to break it up.
02:23:13.380 - 02:23:30.700, Speaker D: Anything else? Ask away. Guys. Are you guys curious about Satora? Do you want to work with it?
02:23:32.590 - 02:23:33.340, Speaker A: Yeah.
02:23:40.510 - 02:23:41.066, Speaker D: Hello?
02:23:41.168 - 02:23:41.542, Speaker A: Yeah.
02:23:41.616 - 02:24:25.162, Speaker D: If we complete all the challenges, whatever you're given, can we become an official formal verification engineer or something? Yeah, I think Moli would be the right person to answer that. So yeah, we partner with communities like that and we pay people in the community who write rules using our tool, write rules and verify code using our rules. Moli, he's interested to know if I complete all the challenges and everything, will I officially become a formal verification engineer? Certification? I don't know if you do that but that's not the point of these challenges. These challenges are to get you a sense of how the tool works.
02:24:25.296 - 02:24:38.320, Speaker A: Yeah. So we have engagement with secure. We have engagement with securem. Is that the question of how to study? Is that what you're asking? So we have an engagement. I think there is an online.
02:24:41.090 - 02:24:41.502, Speaker C: So.
02:24:41.556 - 02:25:48.280, Speaker D: Sartura has collaborated with, like Alex said, online communities, and one of them is securium. And securium is an online community of ethereum security smart, contract, security focused, interested aspirational experts, the whole thing, right? So the collaboration in this particular case was really to learn. I mean, sertora had a workshop that goes deeper than these challenges. I think Alex's point here was these challenges are really for educational purposes, right? They don't lead to a certification. But sertora has collaborated with securium and ave as well, where once you learn the tool, you can apply it on the code base that is within the scope. And the top performers are given. I think there were certifications as well from Satora, I'm not sure, but secureum has issued NFTs that show that, hey, you've got this Certora knowledge and expertise, and there have been monetary financial incentives as well.
02:25:56.190 - 02:25:56.940, Speaker B: It.
02:25:59.650 - 02:26:02.720, Speaker D: All right, guys. Come talk to us if you want to know more.
03:27:35.820 - 03:27:44.570, Speaker C: Shall we start or sir Kevin, can I start? Okay.
03:27:45.500 - 03:27:46.008, Speaker A: Hey.
03:27:46.094 - 03:27:50.010, Speaker C: Good afternoon, friends. Two minutes.
03:28:08.820 - 03:28:09.730, Speaker D: Two minutes.
03:28:10.540 - 03:28:42.660, Speaker A: Um, just one, two, three.
03:30:01.590 - 03:30:44.014, Speaker C: Hello. Good afternoon, friends. So I'm here quite excited to give a talk. So I would be talking about a technology, right? Something that we use in our everyday life, but something that we don't talk about much. And it is something that we require as a human to build connection with other humans out there. So that's called real time communication technology, right? It's a part of our daily life. So during our prehistoric time, what used to happen was that it was the communication that helped us coordinate to each other.
03:30:44.014 - 03:31:37.994, Speaker C: And when we were able to coordinate, we were able to increase our survival chances, and we were also able to bring more food to our table and distribute it much more better. This is also the core reason, one of the major factor of the evolution of ours. We also know that the money and the crypto that we know is basically a tool, a tool to capture the value. And this value comes from the consensus, which is basically people coming over an agreement, a group of people coming over an agreement. This makes real time communication technology much more critical than ever. Right now, if we see in our web two world, there are a lot of applications like Facebook, instagram, TikTok, twitter, all of them combined, the social media combined, there are 4.48 billion active user, social media user.
03:31:37.994 - 03:33:07.518, Speaker C: And if we compare them to web three, there are only 84 million active wallet users. And these are basically combined with centralized exchanges also, addresses also, and a decentralized exchanges also so you can see the ratio. Like we are still a lot of problems the social app that has to be solved, the web three adoption is still an unsolved problem. Now to get this real time communication basically how do we bring it over the internet, right? So real time communication is basically many to many audio video data transferred, synchronized at sub latency second. But like the current protocol that we have which is like http TCP UDP, they are not optimized to do so because many of them operates on request response mechanism and all of those things. So there is a protocol called WebRTC that is actually used to give this RTC functionality in the today's world but there are a lot of problems with the WebRTC because it was not designed to scale out there and actually this led to a lot of problems out there. If you see this led to the custom servers to scale out there and since it requires custom servers so what happened was that the corporates started accumulating those servers out there and even on the P two P level you required this centralized signaling mechanism to actually establish those call out there which is the problem.
03:33:07.518 - 03:33:43.030, Speaker C: Now what's happened with this is that basically the user's data goes to a server which is practically a black box. So you don't know what's happening with your data, you don't know what's happening with your privacy out there. And also you never know if it's the best performance that they are getting out of the box. And on top of it, the users are actually exposed to rent extractor mechanisms and even the vendor lock in systems out there. And there are a lot of other problems. But if we go into that, I think we can go. So now we have DRTC.
03:33:43.030 - 03:34:35.260, Speaker C: This is something that we were actually working on at Huddle. So Huddle is not just a normal audio video conference, it's incubating this RTC infrastructure of the future. It's called decentralized real time communication infrastructure. So what is it? I will give you a visualization. So basically like how polygon and other are basically the scalability solutions for layer ones. The DRTC is this real time communication layer for L ones out there and what this would enable is that now you can see this web three social kind of DAPs DApps which require this live streaming which required this audio video calls or which require their avatars to chat with each other at real time. Those can be possible with these things.
03:34:35.260 - 03:35:51.282, Speaker C: Now, at Huddle what we did was that basically to get this real time infrastructure up and started, we took this product first approach and with this product first approach you can actually bootstrap this RTC infrastructure layer out there. So yeah, how did we basically and another thing that point to note is that what you see is that the token like what happens in generally the web Three world is that people make token and they dump the token on the community itself. The other thing is that they make the technology and then they dump the technology on the users itself. It's the same thing, right? But what we need to do is that we need to make a product and we need to make that product valuable and then once that is valuable, we need to distribute that value to the users and the community. Now, once we are able to distribute wealth there is higher development among that community members out there. So yeah, at Huddle, what we are able to do is that within a single click you can see, you can access all of the best of Web Three. Like you can live stream, you can record, you can have your ENS name, you can have an unstoppable name and lens protocol out there.
03:35:51.282 - 03:37:14.070, Speaker C: You can even live stream to YouTube, twitch everything out there all at a single click. So we also be able to bring this experience to your web application and to the mobile applications also and these are live, you can just check it out from your App store and play store and start playing around with it. So all of this infrastructure is actually powered by a media infrastructure. This media infrastructure if you see talks directly to the filecoin network to store on IPFS and filecoin it livestreams to live care protocol to live stream which is basically one to many broadcasting stream and it has this media routers which is made on open source libraries. But on top of this media servers there are custom algorithms that we have written such as Last Ten, such as dominant speaker identification codec optimization and all of those things. So this makes the infrastructure much more robust and this is a sufficiently decentralized architecture as of now. Now if you see we have spent like a year or two year making this architecture much more robust and all of those things but what we don't want is that the developers out there to go into that same rabit hole.
03:37:14.070 - 03:37:51.622, Speaker C: So what we did is that we packaged all of this infrastructure into our SDKs. Now, using our SDKs, you can have the same capacity that the Huddles app has. So it's basically you can fork the Huddle application and make it on your own. You can just white label it and you can make other kind of application. You can make Twitter spaces, you can make forecaster spaces, you can make lens calling one to one. All of them. You can also make like a wallet address which is calling other wallet addresses out there and it opens like a huge door out there.
03:37:51.622 - 03:38:51.050, Speaker C: If you're building a MetaWars in the MetaWars, like the Avatars need to chat with the other Avatars. So here is where you can use the Huddle SDKs out there. And there are other things also like the imagination can go wild wherever you require this real time communication infrastructure, you can just plug in the Huddle SDKs and get that up and rolling. These are load aware, region aware, globally distributed server. And I would also like to take you the quality that we have actually what this infrastructure is capable of first is quality so we can see like it does one thousand and eighty p and higher quality out there, so it's much better for the screen. Share we have been able to minimize the packet loss. There's less jitter buffer, there's dynamic bandwidth adaptation and all of this thing we are trying to do in less than 100 millisecond because that's the best VC experience that we get when we are doing less than 100 milliseconds.
03:38:51.050 - 03:39:31.234, Speaker C: So yes, we are end to end encrypted also. So if you can see, there is the call going on. So this is a version that a user sees on the left side. And the other version is, what if the hacker breaks down into the media infrastructure? That is what he sees. It's an end to end encryption call. And even this is how Huddles media infrastructure actually sees. So your data is actually secured out there even like the Huddle servers cannot see other routers, they cannot see the data that you are doing they can only process the meeting at a super high quality out there scalability.
03:39:31.234 - 03:40:44.000, Speaker C: So these end to end calls we were able to scale to up to 30 people out there and here you can see it live out there further activating the last ten algorithm. This help us scale to more than 400 people out there and this is basically the robust infrastructure that we have got and further going on we have also can be horizontal scale so which means such meetings can happen parallelly out there. So can handle any amount of scale there. So the progressive roadmap so what we believe is that the decentralization is not a switch which can be like zero and one, it's progressive in nature. So Huddle was born in ETH Global Hackers 2019, which was sponsored by The Filecoin and Protocol Lab. We went into Techion accelerator and this is the current product that we have right now. So the DRTC we talked about is basically the phase two part of plans and there is an Afra crypto economics which is our phase three part of plan and they both would be working parallel.
03:40:44.000 - 03:41:35.280, Speaker C: So DRTC is basically this will make all the central component into this nodes architecture and the afra crypto economics would be basically this incentive mechanism for all the apps to work together. What we will realize end of the day where we actually want to get to is basically an open neutral borderless decentralized sensitive premises and community owned and governed real time communication infrastructure. Yes so would love to share some of the learnings that we had while building Huddle. So in hackerfs we actually made it as a P, two P. Mess. But we realized that there's a lot of problems with this approach. So the P two P Mess was not scalable after four people, the experience on the user end was not something that can go as a product itself.
03:41:35.280 - 03:42:25.612, Speaker C: There's a lot of less security because in P two B mess if any one of the peer is malicious, the whole thing gets compromised. The data persistent, you cannot persist the data in this typical fashion out there. This is the whole reason like the IPFS actually made the file coin which is basically the same IPFS mechanism with an incentive layer out there. And the other problem is that in this P two B mesh the other peers can send the malicious or illegal files to other peers because if they are contributing through the network so again, that's a problem. And we cannot always rely on P two B mess because they might be available or they might not be available. So they're highly unreliable. What we can do is basically convert this P two B Mess to peer, to node, to node architecture.
03:42:25.612 - 03:43:26.580, Speaker C: Once we are able to do so, what we can do is that these nodes can become have this macro level feedback wherein these macro properties can be decentralized and they have this consensus algorithm taking care of all the micro level competition that is going on. So this is like individual nodes which are optimizing their self game but they're competing with each other, but on the overall macro level they're collaborating with each other. So this makes it very highly scalable out there and you can always have this security and slashing algorithm. This maintains the quality of service that network has and even the security. So afra crypto economics so this is something a network flywheel. Many people ask me a question that how the value accrual would happen on the token itself. So if you can see there are end users and there's a community network effect starts to happen.
03:43:26.580 - 03:44:13.536, Speaker C: Once that's happened, your volume of RTC starts to rise. And once that starts to rise, what happened is that people start building application on top of the Huddle infrastructure or the RTC infrastructure out there. This makes your volume of RTC transaction rise. And once that rise, basically the token value starts to raise and investors generally wants an upside exposure to any value that is going up. So they put in the financial capital. Once there is like a financial capital and the token has this both monetary value, the people start to put forward their computational and bandwidth capital, the Huddle nodes, the nodes provider and in return they get the token itself. So you can see on a global level like the people are putting in more nodes.
03:44:13.536 - 03:45:19.208, Speaker C: So what happens is that the quality increases, the security increases and the pricing goes down and the scalability increases with the more the nodes that get acted out there. The Afra Crypto economics. So in the recent like what used to happen was that the incentive layer was only on the infrastructure layer, but on the application layer the only incentives you have is basically bounties or some other forms of making people to make the apps out there. I think we can have a protocol that can work that has this both incentive layer on the application layer and this has this incentive layer on the infrastructure layer and there is a dynamic between two that collaborately makes the whole ecosystem grow. Yes. So would also love to take you to the metcalfelos and social network theory because real time communication has this value in social network out there. So Metcalfe laws states that in a telecommunication network the value of that system grows by N square if another node comes into the picture out there.
03:45:19.208 - 03:46:41.888, Speaker C: So here you can see in the graph basically first it's like one or two user, but as the node starts to increase, the value of the whole network start to increase and the whole of the value can be captured into this crypto economics out there. So the perfect example of explaining the Metcalfe law is that suppose you have a mobile, right? So in this mobile, if other person has mobile, this mobile gains value, right? The more the number of people you are able to call with this mobile, the more value that the mobile the whole network acts. So for end users the value increases if number of people and the nodes into that infrastructure start to increase. And this is like what the metcalfe and the social network theory actually summarized. And this is how actually the networks like network effects products like Facebook, Instagram and all of those things are actually valuated on the network effects again. So Zoom was able to overtake Skype because what they enabled was that so Zoom only you had to have this email ID to get to a Skype call. But what Zoom did was that even without a Zeal ID you can do a Zoom call.
03:46:41.888 - 03:47:35.930, Speaker C: So this led to more network effects out there. But at Huddle what we are doing is that you can have this like non zoom account and you can also bring in this crypto primitives out there which is on the rise and this again leads to this network effects out there. Yes. So again, I would highlight some of the problems in the webrtcs out there. For a WebRTC to take there's like four things that goes on. One is first step is signaling, other step is like you need to connect the secure out there. First is signaling, then it's connecting, then it's securing that whole infrastructure and then you start communicating out there, but therefore signaling you need to have these servers out there which are centralized in nature again.
03:47:35.930 - 03:48:53.344, Speaker C: So in this, if you can see the signaling is centralized, but if you can convert these servers into a nodes and these can be like multiple nodes which are like chatting with each other. So your SDPs you can now send to this node and these nodes can gossip around this SDP and whichever peers required that SDP, it can subscribe to that SDPs out there. So this is how you can remove this single point of failure out there. Now, we have realized that you can use this not just for putting out the STPs, you can use it for both synchronous communication and you can use it for asynchronous communication, right? So what is synchronous communication? So if you had used Slack, right, there is a Slack button out there. So it is like many people coming at call at that point of time. So zoom meeting is asynchronous out there but Discord and Slack they are asynchronous communication. So with this mechanism of divergent right, wherein you are divergent node wherein you can use this infrastructure for signaling to attain the async communication and simultaneously you can use the infrastructure to have this async communication capacities out there.
03:48:53.344 - 03:49:55.984, Speaker C: So again, what we did was that this is the end to end encryption that we had solved out there. It used NaCl algorithms out there from the private public keys of the user itself. So even this signaling mechanism is end to end encrypted. And yes, so we have packaged this divergent node into the divergent SDKs. Now you can see that this is a WhatsApp clone made on a lens protocol and you can see a starter huddle that uses Huddles media infrastructures, huddles SDKs and all the chat SDKs is like the divergent SDKs out there. So you can see we have achieved both asynchronous capacity of communication and the synchronous capacity of communication and developers can have this imagination and can build any kind of applications out there and the DIDs that is basically a lens protocol out. So like now coming back to DRTC, what is actually a DRTC infrastructure.
03:49:55.984 - 03:50:53.060, Speaker C: So if you see like UPI, right, the UPI is a fintech infrastructure wherein this BMAP phone Pay app and Google Pay app was made. Similarly this DRTC layer enables this web is the spine of this web three social app, upcoming web three social app. So now you can have this decentralized TikTok, you can have this decentralized Twitter, and you can have another kind of new primitives app, which I think would be possible, which is like the social app and the financial merger kind of apps out there. Because think of like a social Twitter leveraging the DeFi infrastructure out there and this can blow up. We just need to productize that very well out there. But for the start, I think the cloning of the existing web three infrastructure would start to happen. Like clone of discord, clone of WhatsApp? Clone of Twitter.
03:50:53.060 - 03:51:22.200, Speaker C: Clone of LinkedIn. They would start to happen and they would require this synchronous capacity and asynchronous capacity also. And even in the metaverse, like in the metaverse, like all the NFTs they have these games, right? The MMORPG system, the massive multiplayer online system. All of this requires this real time infrastructure. Yep. So the Huddles infrastructure is already powering. Lot of other applications out there.
03:51:22.200 - 03:51:48.750, Speaker C: So this is the whole ecosystem that Huddle is actually powering. So there is, like, schedulers. There's accelerators, there's, like, metawatt spaces, there's messagings, there's Web three attack, there's wallets. NFT communities, social. And I think this is a blank canvas which is yet to be filled by other entrepreneurs and other people making new innovations out there. Yep. Thank you.
03:51:48.750 - 03:51:54.620, Speaker C: This is Sushmit, and this is my barcode if someone wants to follow. Yep. Thank you.
