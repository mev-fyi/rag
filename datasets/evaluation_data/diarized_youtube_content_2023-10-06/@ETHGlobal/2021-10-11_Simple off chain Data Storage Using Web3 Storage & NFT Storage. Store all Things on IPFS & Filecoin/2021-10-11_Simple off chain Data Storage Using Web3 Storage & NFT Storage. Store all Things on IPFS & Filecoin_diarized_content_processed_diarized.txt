00:00:05.770 - 00:00:20.430, Speaker A: So our next speaker is Yusuf, and he will be talking about off chain data storage using web3 storage and NFT storage. So without further ado, let's see how we can make NFT storage.
00:00:22.610 - 00:00:36.018, Speaker B: Welcome. Good to here. Thanks. Share my screen here. Cool. So I am Yusuf. I work at Protocol labs where we make IPFS and filecoin.
00:00:36.018 - 00:01:31.340, Speaker B: Those are two really interesting bits of technology that we think can help everybody in the sort of decentralized app ecosystem take advantage of this vast network of storage that's now available. So let me sort of get into the mix here. This is kind of a mashup of two talks about web3 storage and NFT storage. And these are both services that we've built to make using IPFS and Filecoin really easy and accessible, especially for Web Two developers or really anybody that wants sort of a low friction path into this storage network. So first I want to talk a little bit about NFTs. So we became really interested in NFTs. We have been for a while, but obviously this year they sort of take off in the public consciousness and that's when I personally started getting into them.
00:01:31.340 - 00:02:33.098, Speaker B: I was loosely aware of them before that, but basically it was really encouraging for us to see that IPFS was being used to store lots of NFTs. And we think it's a really great fit for this kind of application because first of all, storing data on most blockchains is prohibitively expensive. And therefore to have an NFT that represents something outside of itself, which is the case for many, there needs to be a link between the NFT and data that lives off chain. And IPFS has some really great properties for creating that kind of link. So we'll go into that in a little bit. But we started to realize that the developers that like OpenSea and foundation and Zora and so on, they care about users being able to access the data should they ever disappear. And this is sort of a common thread in these kind of circles, is that nobody wants to trust any individual party to stick around forever.
00:02:33.098 - 00:03:26.000, Speaker B: But we combine, if we all play well together, we can hopefully design a system that outlives any of us individually. Right? It's good for us to kind of build systems with that in mind, even though hopefully, of course, we'll be around for a while. And it's something that users care about, especially as they became aware. That the thing that they're I don't know, maybe people are already aware of this, but I think that people that are new to NFTs don't always understand what specifically lives on the blockchain and what lives elsewhere and what's that relationship. So there's like an educational piece to that that we've been involved in trying to kind of clue people into. What is that link and what's the strong way to make that link. IPFS fits in.
00:03:26.000 - 00:03:57.670, Speaker B: And if you're new to IPFS, it is the Interplanetary File system. It's been around for several years. And the gist of it is basically that first you put your data onto an IPFS node and that node participates in this big peer to peer content sharing network where basically, if somebody wants a piece of content, they'll ask for it. And if I have it, I'll provide it. And everybody does this. And it's actually a pretty efficient way to distribute data across a network in many ways. Anyway, first you put your data into the IPVs node and you get back this string, the Content ID.
00:03:57.670 - 00:04:21.710, Speaker B: And it is a cryptographic hash. By default. It's Sha 2256. But you can use lots of different hash functions and all the rest. It's a very customizable piece of software. But the fundamental point is that this CID will always identify one specific piece of content and any changes to the content would produce a different CID. So given a CID, you can say, hey network, please give me this data.
00:04:21.710 - 00:04:58.262, Speaker B: And when it comes back from anybody, you can always verify that it's correct because you can compare it against the CID that you use to retrieve it. So this means that you don't need to really care who is providing the data or where they come from or anything. You don't need to know anything about them. As long as the protocol works, as long as somebody has the data and they see your request, they can provide it and you'll get it. And it's a pretty cool feature for linking to things, which we'll get into in just a second. So here's the requesting process. Somebody requests the data by the CID, original node responds, and now they both have a copy.
00:04:58.262 - 00:05:32.520, Speaker B: And if a third node comes along asking for it, both of them can respond in parallel and say, oh, I have it. Here's a few pieces, guy on the left gives a few pieces, guy on the right gives a few pieces. And now you get the file potentially faster than you could downloading the whole thing from one. But there's lots of variables there with respect to latency and so on. So it's really hard to make absolute claims about performance characteristics versus other systems. But in general, there's potential for efficiency there. So, yeah, now this third node has it and as a result, they can also participate in the providing process.
00:05:32.520 - 00:05:59.210, Speaker B: And that's great. The original node can go away, but the content is still there. And so we've kind of gotten close to this dream of content outliving us and our organizational guidance and so on. The data is still retrievable from other nodes. And that's kind of the power of peer to peer networking. No individual peer is blessed or special. But there's some caveats to this approach.
00:05:59.210 - 00:06:55.370, Speaker B: With IPFS, if all the nodes that had the content disappeared from the network, then there's nobody there to provide it. It's like an active process where your node needs to be online in order to respond to requests. And if no node is online with that content, it's just not resolvable. Although I should mention that the same is basically true of Http servers like Facebook.com were suddenly to be unresolvable, then nobody could access it. But with IPFS you have kind of an interesting property where if anybody later adds the same content to the network, provided they use the same hashing algorithm and everything, then they'll come up with the same CID and any links to that original content will be resolvable. I guess like a third party can sort of fix a broken link in a way that they can't with Http.
00:06:55.370 - 00:08:07.026, Speaker B: So that's kind of neat. But the second caveat is that there is an automatic garbage collection process for these ephemeral copies that get made as content propagates throughout the network. Each node will keep a little copy for a minute, but eventually they may get disk pressure and we'll have to clean up and throw stuff away. So to prevent that, there's a thing called pinning which is just signaling to your IPFS node please keep this alive, don't garbage collect your stuff. And this has been one of the main ways for people to, practically speaking, get their data into IPFS and made accessible is to use a pinning service that will provide this data for you on your behalf so you don't have to run your own infrastructure and so on. And as we were looking at the NFT space and thinking about what can we contribute, we saw, well, we can make this process a little easier for people by basically more or less becoming a pinning service, but doing so using some of our other technology, namely FileCOMING, which we'll talk about in just a SEC. So the way we present this is as NFT Storage.
00:08:07.026 - 00:09:29.554, Speaker B: This is a website it is free to use. You log in, you get an API token and then you can upload your stuff through us and where we provide it on IPFS. And also for long term storage, it's all getting sent to Filecoin. So the way this works is, yeah, you take your NFT, it's a very simple upload interface, it's just an Http request, there's a few different variations on that depending on what your needs are, which endpoint you might want to use. But then anyway, we get your data through some request and then NFT Storage is going to store it on an IPFS cluster that we manage and then we also redundantly store it with a third party provider just for safekeeping. And behind the scenes though, we are also negotiating deals with Filecoin storage providers to store data for the long term and they're actually getting paid by the network in the form of block rewards, which is kind of an interesting sort of economic angle that we can talk about in a second. But for now the advantage of Filecoin is that it fits this property of allowing members of the network to disappear, but still giving you some solid assurances about your data.
00:09:29.554 - 00:10:29.834, Speaker B: So now if NFT Storage were to go away completely, and all the IPFS nodes that we manage or that other people run, lost all the content, the data is still just there on filecoin, being continually challenged by the protocol to prove its existence. Basically in the worst case scenario where we just fold up shop completely and can't pay the bills anymore. Everybody has this sort of public verifiable escape hatch, as it were. Or a seed vault, maybe, is another way to think of it, where the data is preserved and it's just right there and we provide all the information for retrieving. It in our API and stuff. So to use it, you go to NFT Storage, which is a top level domain now, so that's cool. And it's really simple and free and you should check it's.
00:10:29.834 - 00:11:33.210, Speaker B: This is the http API It's not very big. The main endpoint is this upload endpoint for storing things, but there's also stuff for retrieving status information about your upload to who is storing it on filecoin and IPFS and so on. So you can kind of get that granular information if you need it. And if you're in JavaScript, we have this JavaScript client that makes things a little simpler and will help you format data according to ERC 1155 standards. So if you are building an ERC 1155 contract or one that can be compatible with one with that, use this client store method and it will do some nice stuff for you. Basically you can give it a file object and it will add the file to IPFS and create a hash link for you in your metadata JSON. And then also it wraps the whole thing up into this thing called a Dag Seabore Graph.
00:11:33.210 - 00:12:32.598, Speaker B: It's not super important what Dag Seabor is, but it's basically just a compact binary JSON object. But it's a nice way of addressing hash length data that we like to play with as part of the IPLD project if you're interested. So I'm going to quickly just show you guys how to use NFT Storage and then we can talk a little bit about our other storage service here's, the site itself. And you log in with GitHub or email, it will send you a magic link to click on. If I log into my profile, you'll see a few things I've stored, mostly just testing things out and so on. And we can go here and poke at it a little bit and get an API token that we can authenticate with. I should also mention that my home Internet is very slow, so things that are slow for me may not be slow for you when you're using them, especially when I'm uploading things.
00:12:32.598 - 00:13:15.450, Speaker B: It's kind of sad. But anyway, so here we are at the API keys and I made this one a few minutes ago. You can make and delete them here. And there's my key for later. I'll probably come back here in a minute. I've got an empty project folder here with some images that maybe we could store on NFT storage and let's make an NPM project to write some code in. So if I do NPM in it and I just going to accept all the defaults now I have a package JSON file and I can say NPM install NFT storage that gets me the JavaScript client library.
00:13:15.450 - 00:14:27.010, Speaker B: Yeah, I'm also going to pull in a couple of other libraries once this is done to make building a simple little example app easier. So minimist is for pulling out command line arguments kind of nice. And then Mime types for figuring out what kind of file you've got because the store function kind of likes you to provide it with a Mime type. And this method that formats according to ERC 1155 expects it to be an image file type when you provide it the image field, well, you can actually give it an MP4 now for compatibility with platforms that use videos in the image field. So anyhow, we have now installed some dependencies and head over to an editor and make ourselves a file. I'll call this index MJS. Something that's relatively new in node land is if you name your file with a MJS extension or if you put type module in your package JSON, you can use import syntax, import NFT storage from NFT storage.
00:14:27.010 - 00:15:11.530, Speaker B: I'll also pull in the file object since the API uses the file object that you would see in a web browser context. And then it provides an implementation for node if you're working on node JS. So I can import that here. And then in order to read the file from my local disk, I'll import node's FS module. So import FS from FS promises so I can use the Async await and we'll pull in Minimist that we installed earlier and my pipes. That's all the stuff we'll need in a second. And let's do a function for storing an NFT.
00:15:11.530 - 00:16:24.134, Speaker B: So just call this Async function store NFT and it'll take an image file name and then maybe the name and description metadata we'll tag our NFT with. So to do this we're going to make an NFT storage client and for that we'll need a token which I'm going to be a naughty boy and hard code into my source code. Obviously this is actually an area that we'll be improving upon very shortly. Right now the best practice for protecting your API token for NFT storage is probably to have it interact with NFT storage from a backend service and spundling it into a purely decentralized DApp. Front end would expose it to the client. But we're working on solutions for that which would involve direct authentication through a wallet and using a decentralized identity and also sort of user scope tokens so that you can manage authorization on your user's behalf. I am mentioning this because it's just a common issue that we are heads down working on.
00:16:24.134 - 00:17:19.674, Speaker B: So stay tuned. In the meantime, I'm going to grab a token from here and paste it in here. So I got that and now I can make a client. So I'll say const client is new NFT storage and you give it a token argument and now I can do stuff with it. But first I'll need to read my file in so I can say file data equals FS read file sorry, I need to await this. It's async and image file name and type is going to be Mime lookup image file name because it just pulls the file type from the file extension, not super fancy. And then we're going to say, okay, I have the data and the file type.
00:17:19.674 - 00:18:00.040, Speaker B: So now I can make a file file object that takes an array of Blob parts which is basically just things that can be turned into binary data. So the stuff we slurped up from our image file name here will fit that just fine. Actually. Let me see. Let me quickly make this bigger so everybody can this is a little easier to read. There we go. So I'm making a new file and I'm giving it the file date, the file name.
00:18:00.040 - 00:18:55.206, Speaker B: Maybe we'll trim that down to just be the last part of the file path import voice name. You can say this is image file name. Okay. And then the last part of the file constructor is the optional arguments where you can pass in the Mime type. So we looked that up earlier and now we can pass it in and now we have a file object that I'm going to call this image because that's what the store method expects, image. Now this is a proper file that we can hand off to the client. So now I'm going to say I'm just going to return client store image name description and there's a few other you can add optional properties if you want.
00:18:55.206 - 00:19:58.134, Speaker B: You can have properties with any key value stuff you want in there for custom use case, which if you are doing the ERC 1155. We do recommend sticking custom properties in properties if you can just because it makes it easier. We're trying to beat the drum of standardization a little bit on the receiving end. We're also scraping ethereum chains and finding all kinds of NFTs in the wild to kind of see what people are doing. There's a lot of interesting metadata variations out there but at any rate, now we have this and we can say we can call it. So let's make up a main function name and this is where we can grab our command line ARGs. We don't need this complete getting even dirty ARGs equals minimist and for this you pass in the process argument.
00:19:58.134 - 00:20:41.740, Speaker B: So process argv pull off the first two for node reasons. And now minimis is kind of simple. It takes whatever arguments it gets and parses them out and gives you a dictionary or JS object rather. So now I'm going to just assert that you gave me the right stuff. So we want there to be an image, a name and description. So if there's no image complain. This may be a little fussy for live coding, but still, I mean, error handling is important.
00:20:41.740 - 00:22:10.350, Speaker B: We'll let the description be optional, I guess. So then we'll say just pull them out of this object and then pass them to the store NFT function. So image here's the image file name and we can just toss that over here and say metadata equals await store NFT and then give it the image file name, name and description. And now let's log it to the console and see what happens. So now if I call the main function, we should be able to call this thing as a node script and see what we see. So let's see, here we go. If I do node index JS and then say image, I can give it images and name, et cetera.
00:22:10.350 - 00:22:50.686, Speaker B: And yeah, I probably should have added a little console log saying hey, we're doing a thing now. But there it goes, it's already done. So what happens? The result you get back at this is you have an IPFS URL, which is in the IPFS format. If you want, you can pretty easily turn this into an Http gateway URL. So this part here is the CID, and then there's a file path. So if I wanted to make a gateway URL out of this, I can say CID, sorry, copying things accidentally here. CID IPFS DWeb Link.
00:22:50.686 - 00:23:57.780, Speaker B: There's lots of other gateways, but that's the one that sticks in my head and then add the file path at the end. So I'll show you what that looks like real quick. If I do CID IPFS web link metadata JSON I spelled that wrong. The metadata, yeah, there's basically a little bundle, it makes a little graph structure for you and then you can traverse that graph using this pathing syntax. But basically it's just there's the metadata that we stored. And you can see here, if it's big enough that the image field has this same IPFS reference to the image itself. So you can make another gateway link and pull it out.
00:23:57.780 - 00:25:06.370, Speaker B: If you have brave installed it'll, follow these links automatically. But I for some reason didn't think to use it for this demo link. So that will chase up the image itself. And if you want a sort of machine readable or traversible form of this without having to parse links and so on, that wrapper object, like the root object of this graph, where if you just have the CID without the metadata. JSON it's actually that Dag Seabore thing I was talking about earlier, where you can fetch this object and then use our IPLD tooling to traverse into the metadata and find the image itself and sort of all bundled into this graph. Not sure why this is taking so long to resolve, but I blame my terrible internet. So we'll see.
00:25:06.370 - 00:25:57.900, Speaker B: Anyway, let's maybe take a break from all this stuff and we'll talk about Web Three storage. We'll come back and hopefully it will have loaded by then. All right, there's something I want to mention before we get onto Web three storage, which is that we have this thing called there's a format called a car, which is a content archive. And you can send us content archives if you want to pre format your IPFS data, like if you have specific layout that you want, or also if you want to send big files over 100 megabytes. I should mention too that we're going to be doing this for you automatically soon. So you may not have to do it yourself, but it's kind of nice to understand what's going on behind under the scene. So you have a mental model what's working.
00:25:57.900 - 00:26:45.890, Speaker B: So the advantage of using a car is that you can know the CID before you upload, so you're not waiting on our server to tell you what the Identifier is and you can verify that it's correct, that we're storing exactly what you're giving us. And like I say, it's Chunkable and you can split the cars into chunks here. So I have one big graph and I can split it into little graphs that all have the same route and our back end will piece them all together for you. And yeah, that's pretty cool. So I'll talk a little bit more about cars in a second because we have this other service and I was quickly running low on time, so I should get into the weeds of Web Three Storage. It works very much like NFT storage, especially under the hood. They are very similar.
00:26:45.890 - 00:27:50.980, Speaker B: If you remember this slide from before, it's the same thing but for non NFT data. So any data that you have that you want to be stored on IPFS and filecoin, you can just upload through Web Three Storage and we will store it for you and it has the cars baked in. This will be coming to NFT Storage soon. But right now Web Three Storage has it already in the JavaScript client where essentially you give it a file of any size and it will create the car file for you and do the chunking and send everything off and it gives you access to the root CID as soon as we compute it on the client. So you can display it in your UI and say, oh, here's the thing now in parallel, send it off to Web Three Storage and it'll get stored for you and you get progress callbacks and stuff like that. Very briefly here's kind of what the architecture looks I'm not going to talk a bunch about it, but there's some cloud workers we maintain some state in a database. But then the interesting bit is the storage broker service.
00:27:50.980 - 00:28:48.004, Speaker B: It will take your data and batch it up into, I think we usually do 32 gigabyte chunks and store those each with a filecoin provider since that's the most efficient format for getting data directly into a filecoin sort of pipeline. But parallel to that, we've already added it to our IPFS cluster and to a redundant Pinata cluster. So it's available for retrieval pretty much as soon as you upload. But then there's maybe I think we're at like 18 hours now in terms of delay for getting it onto filecoin. I'm sorry, it might actually be eight, but I have to actually ask people what the real number is. Sorry, but it is a few hours because we have this aggregation process going on. But as a result of this, by meeting the miners where they are, the storage providers as they were calling, we were able to offer this service for free.
00:28:48.004 - 00:30:01.404, Speaker B: So here's something that comes up a lot when we talk about the service. It's like how is this free? And it basically is because the primary economic driver for filecoin providers right now is block rewards. So they get rewarded first for committing capacity to the network. So if you bring drives online into the network and prove that you've done so, then you get potential block rewards relative to your overall storage power, the capacity that you're adding. But if you are storing real data, boom, you get a ten x multiplier, basically. And the way that this works is that there's a system called Filecoin Plus in which a notary allocates this resource called Datacap and Datacap essentially is sort of like a voucher that says we believe this is real data and not just some random garbage that somebody generated to pump up their numbers, I guess. So the idea is that by flowing your data through web3 storage and NFT storage, we have a reasonable assurance that you're a real person or that there's a real sort of social need behind the data.
00:30:01.404 - 00:30:54.930, Speaker B: That's kind of what we're getting at because fundamentally our mission is data preservation and all that. So anyway, verified data increases the chance of block rewards by ten x and so as a result the actual cost of storing the data and there's some collateral that the providers put up front to ensure that they're going to store it, that is all basically kind of negligible compared to the increased reward. So it works out for them to provide long term storage for free and so we're not charging users anything for storage as well. There's a minimal operational cost to us to keep the IPFS side of things running, but that's in the grand scheme not a big deal and we're super into providing it to make things easier for people. So that's what I got. Thanks very much and if anybody has questions, please let me know.
00:30:56.500 - 00:31:01.584, Speaker A: So that was awesome. Also, nice recovery. I feel like we're going to go over time, but this worked out.
00:31:01.622 - 00:31:06.752, Speaker B: Yeah, totally folded in, I think two.
00:31:06.806 - 00:31:42.856, Speaker A: Kind of common questions, one actually already addressed, which was the nuance between Web Three and NFD Storage, just the formatting and the Car Archive formats. A common question that I think we get, especially during the hackathon, is what are some best practices that you recommend? When people want to think about storing metadata or even updating it, how do we think about that? How does NFT sort of come in handy for these types of things? And I want to question into what's next after this. Is this done? Is this a great service you're offering it? It works. How do you actually improve this thing or what are future features?
00:31:43.048 - 00:32:49.652, Speaker B: Actually, there are some reliability things that we want to bake in right now. Like we're doing a database migration that should make everything faster and more solid. But feature wise, I think those two questions are kind of related. Where right now? Especially Michael and Gazala. I don't know if you guys have met them yet, but they're heads down trying to figure out a sort of a workable plan for extensible NFTs, or not exactly mutating, but keeping track of assertions about an NFT over time. The way that we kind of see that happening now is using events to send out basically entity value triples, where you would say like the entity is an identifier for the NFT itself and then an attribute is some assertion about it, like it has an alternate representation here or it was remixed into something else. You'd imagine lots of scenarios, but then eventually you fold those into you basically compact a big log and then you have a view of the evolved NFT over time.
00:32:49.652 - 00:33:53.140, Speaker B: But that's all kind of speculative right now. We're still trying to figure out how practical can we make this and also is this something that requires a lot of buy in from people up front or is there a way we can kind of apply this to NFTs that already exist? And I think that there is, but there's a lot of details there. So I think that that's kind of where our focus is. NFT wise right now for NFT Storage is to try to figure I'm not sure if it'll actually come through NFT Storage directly, but we will support it there. But I think that that's kind of a broader effort of trying to figure out what the data model should look like in terms of the actual experience of using it. Right now we have two separate services and we're going to end up making NFT Storage a client of Web Three storage just for less stuff for us to manage and for the token management stuff is the big feature that's going to be coming to both platforms soon. I think there'll be a way to have a sort of admin token that you can then create a user scope token and then users will be able to manage and delete their own stuff without having access to anybody else's.
00:33:53.140 - 00:34:14.270, Speaker B: But then we're also going to have a fully decentralized path of using a decentralized Identifier document to assert ownership of a key. And then that gets you. We just want to know that be able to identify you somehow. We don't have to know who specifically you are. So we're building that out and should be coming. I mean, I can't promise timelines, but soon.
00:34:15.840 - 00:34:23.096, Speaker A: Awesome. Well, vids do sound pretty incredible here. That's great. Well, I think that's all the time we have today. Thank you so much for doing that presentation.
00:34:23.208 - 00:34:23.644, Speaker B: Yeah, thanks.
00:34:23.682 - 00:34:28.050, Speaker A: That was great, but perfect. So with that, we are ready to move on to.
