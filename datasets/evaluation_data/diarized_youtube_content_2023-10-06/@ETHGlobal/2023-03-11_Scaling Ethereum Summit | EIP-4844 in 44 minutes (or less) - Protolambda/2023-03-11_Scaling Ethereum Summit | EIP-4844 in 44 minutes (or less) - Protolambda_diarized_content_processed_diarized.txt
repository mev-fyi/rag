00:00:06.410 - 00:00:46.150, Speaker A: Next up, I want to talk well, I want Porto to come on and talk about EIP, 48 44. And there's a lot of 48 44 stuff going on here. Some of you may know how this helps us get to really, really cheap data and transactions. And there's a lot more that people don't talk about. And we're going to take about 44 minutes, maybe less, to cover all those specifics. So we'll get Porto to join us in a second as we get to load up the slides. And in the meantime, if you have any other questions that come up for Proto or Jesse or anybody else, please let us know and we'll be able to kind of relate those questions to you and to the speakers and get those answers.
00:00:46.150 - 00:00:52.000, Speaker A: So without further ado, let's welcome Proto and hi, how's it going?
00:00:52.690 - 00:01:04.740, Speaker B: It's going very well. Thank you for the introduction. Absolutely. Hello, everybody. I'm proto Lamta. I work at Op labs. I'm going to present about EP four.
00:01:04.740 - 00:01:23.960, Speaker B: Okay, so let's jump into this. Ford for four is an EIP. It's about a year old now, so started during eve tenfur. Let me get started with this actual slides. This works. One moment. There we go.
00:01:23.960 - 00:01:39.366, Speaker B: It's technical issues. It's like a curse. Starting with the presentation of Golfing. So. Hello. I'm proto work at Opilabs. Protozink.
00:01:39.366 - 00:02:10.694, Speaker B: Sharding is kind of a meme. It's this name that's been attached to Fort for Four. Fort for four. What it really is, it's the first iteration of the Sharding Roadmap of Ethereum. And this longer term vision is inspired by the work by Dunkard and by Vitalek and others at the research team. However, we cannot wait. We want Sharding on ethereum today.
00:02:10.694 - 00:03:37.250, Speaker B: Like, these are multi year research processes. And so instead of trying to do everything at once, as engineers, we are looking to build this first iteration. And this is a collaboration between Op Labs, Coinbase, as well as all the different layer one client teams and the research team. Of course, Flick Foundation, that's putting together resources to implement the transaction pool changes, the Blob storage, the syncing, everything that's involved in this first iteration of Sharding. And so what I'm going to tell you about is really like, what is this thing? What is fort refer? Why are we doing this? How does this work on layer one? How does this work on layer two? And then I'll talk a little bit about the fee structure of Ford for Four and about the clients and the progress we've made so far. So what does EP four do? EP Four increases the data availability for layer two to about half a megabyte per layer one block. For context, the current data fed ability is around 75, 70 ish kilobytes.
00:03:37.250 - 00:04:25.470, Speaker B: And so this jump up in data fed ability increases the capacity that is there for roll ups to post their data to. If you've watched the talk from Golfin about how these roll ups work then. You are now familiar with the concept of a data fedability layer. The data fed ability layer is kind of hijacked on top of the legacy Ethereum. We use call data, which is meant to be an input to the Ethm. But instead of using this call data, what we intend to use is Blob data. Blob data is like a specialized resource so we can have more of this without putting a lot of more strain on these layer one nuts.
00:04:25.470 - 00:05:24.290, Speaker B: And as you can see, roller popularity has been rising like around end. 2020 is when the idea of a roll up centered roadmap started to really take off around the launch of the beacon chain. And since then, the rollups have taken more and more of data off layer one. So half of the current capacity, like half of the data inputs to the EVM are not actually inputs to the EVM. They're wall up contents. So we want to change this and optimize so we can have more of these contents because why? Well, 97% of the fees on layer two today are actually just paying for the call data on layer one and they're basically misusing this resource. This resource is not efficiently utilized to derive the layer two chain.
00:05:24.290 - 00:06:04.580, Speaker B: Well, this is kind of what it looks like. So this is where transaction fees are going and the thing we want to try and optimize. This total could be much, much smaller if we tackle the largest cost factor here. Like as a layer two, we're not asking a lot of fees. It's primarily just data fees. And we'd rather have a lot more users that can adopt Ethereum through layer two. And we can do so by utilizing shared data to host all the content.
00:06:04.580 - 00:06:51.840, Speaker B: So this is what layer one looks like today. Merge is complete. Got to remove one slide leading up to this in like the modular blockchain steps. We have a consensus layer and execution layer now, but it's only the start of modular blockchain. What we are looking at today on Etherm really is kind of this hack where we have layer two and it's attached to the call data of the ECM. There's no clean separation and although we have these fault proofs and validity proofs to verify outputs of the EFM, we kind of have this strong coupling idea or like how our wallet should be secured. It's not efficiently attached to layer one.
00:06:51.840 - 00:07:41.354, Speaker B: So what we want to do with Ford before is to model it like this, where we have a consensus layer, we have an execution layer and there's this layer one data which technically speaking, it's part of the consensus layer. The consensus layer holds onto the data and votes on which data is available or not. And then through the execution layer, the data is registered and paid for. But this data is really meant to provide for these roll ups for layer two to scale. And this is only the start, like four eight. Four is an incremental step towards full sharding. What we're looking at after four is full dank sharding, which basically means we can support a lot more of these data plumps.
00:07:41.354 - 00:08:55.042, Speaker B: And how do we plan to support this? This can be supported with data availability sampling, which is this security technique for validators on the network to randomly poke others on the network to see if data is available. And the poking here, the sampling can be dramatically improved using redundancy and using these efficient proofs of the data. So I'll talk a little bit more about how these blobs are structured. They are structured in a very particular way, where they are future compatible with this full sharding vision. Okay, so how does EAP four really work now? There is this new blob transaction type and the blobs that are paid for this content. The data is held into the beacon out in the consensus layer for two weeks approximately. And what this means is that we separate this ID of syncing and this ID of security syncing.
00:08:55.042 - 00:10:17.214, Speaker B: You should be able to fetch the layer two state from any place. Just as with layer one, the critical part really is this last two weeks of data. And so, as intuition you could think about ACDN or some service where it's much, much cheaper to provide, host and cache the data and distribute it if the data doesn't change versus changing data. And similarly, this recent amount of data, it's more unstable, it's still being voted on, it's uncertain the layer one might propose it, but not actually make it available to those that rely on it. And what we want to secure really is that any verifier of layer two is able to get the data and then able to use the data on top of this older state of layer two to get the very latest state the tip of the chain. And then using the later state, they can produce any with filled data, they can produce any proof that they might need to counter any false claims on layer One or to produce a validity proof themselves on layer one. And this is what secures the roll ups.
00:10:17.214 - 00:11:17.974, Speaker B: It's really this recent data that's important to make available and then the older data we can optimize. So this task of Ethereum changes to how can we make the data available to these layer two networks, not rely on it to secure all their full assets? Right, this is special data. It's not just that we cannot use regular storage services, it has to be available. And this availability, I'll talk more about this. It's just a strong feature of sharding where this type of resource that Ethereum was lacking is dramatically increasing. And then you ask, well, who's paying for this resource? How does this work? There's this new transaction type. And similar to how you can run the ECM, you can now also attach data to ethereum to make it available.
00:11:17.974 - 00:12:05.038, Speaker B: And what this means is that there's a regular EP One Five nine transaction and the transaction contains some pointers. And these pointers, they point to these separated Blobs. And the idea here is that the transaction registers the fee payment and is processed by the ECM. Whereas the Blobs, they are separated and they go into the consensus layer, into the beacon node. So here's the transaction in more detail. The nice thing about this transaction is that it's structured using the latest encoding format SSD that the beacon chain also uses. And then the functionality is mostly the same as with EP One Five nine.
00:12:05.038 - 00:13:36.094, Speaker B: It's just strictly a small extension to express the fee payment and to register the data. And so there's this max fee per data gas that's added and there is a list of hashes that refer to the commitments that then refer to the full contents of of the Blob. And the pretty thing here is that the data hashes, they are easily embedded into the transaction. They don't take a lot of space, they are 32 bytes long, so they fit in the ECM very nicely. Whereas these commitments, these are 48 bytes, they go into the beacon chain and they are KCG commitments. KCG is this special commitment time type, similar to what a merkel tree does, except instead of a merkel proof, you have an opening to prove any particular point or multiple points in this set of Blob data. And this allows you to build an efficient proof for the contents of the Blob when you need it in the EVM, but otherwise basically optimizes away the proof data.
00:13:36.094 - 00:14:27.970, Speaker B: And this is the key part for full dank sharding, for this full fission of data availability sampling to have this type of commitment in there. And so what this really means is that the transaction goes into the EVM. Like at first it's generalized, it's abstracted as this message, just like any other transaction, the EVM doesn't really know about transaction types. But this additional wrapper data, the commitments and the Blobs, they don't go into the EPM, they leave the execution layer and they go into the consensus layer. So how does this lifecycle work? Right, I'm this layer two user. I want to utilize the shared data. What does this mean? Well, you just use Ethereum as you would use it today.
00:14:27.970 - 00:15:46.140, Speaker B: It's a lot like just sending a layer one transaction, except it's on layer two, it's on our roll up and then the roll up operator or the sequencer, they bundle the transactions and then the transaction bundle. What really happens there is that needs to be submitted to layer one to make it available for Verifiers to see and reproduce the layer two state. And so this bundle of transactions, it's encoded, it might be split across multiple layer one transactions. But really the key here is that the data is encoded into Blobs. And these Blobs paid for with Blob transactions go into the layer one transaction pool and then the beacon chain proposers can pick up on transactions from the transaction pool. They can build their exclusion payload and then along with their exclusion payload, there's now this new addition of a Blob sidecar. This is additional data that comes with the beacon block and there's a separate syncing method to distribute the sidecar and the beacon chain distributes the data.
00:15:46.140 - 00:16:46.190, Speaker B: And then everybody on the beacon chain, they hold onto it for a certain amount of time. So they hold onto it for like two weeks, maybe a month, depends on the exact parameters in EAP for it for firm. And then the execution payload stays in layer one with the fee payments. But then the plops eventually get pruned from layer one, meaning that the resource usage on layer one is bounded. Bounded resource usage is like this long term goal of Ethereum where we shouldn't always keep growing the state and growing the chain history. We want to make this sustainable. And so this is a really important feature of four to four and of Sharding in general, that once we start adding a lot of Shard data that we don't rapidly increase the resource requirements of regular layer one nodes and then it's up to the layer two to persist data long term.
00:16:46.190 - 00:18:00.322, Speaker B: And this is not at risk anymore. After the proofs that security assets assets have already largely been completed, it's then up to the layer two community, the stakeholders of that roll up that the now fully agreed upon layer two state is there for others to sync and join this layer two ecosystem similar to what we do on layer one. So this is what it looks like in terms of the network. We have the sequencer in this transaction pool you could think of it just this cloud of suggestions for the layer one proposer to build a perfect block. Then the Blobs and execution payload make their way into the beacon chain. Then the other nodes on the network, they verify the fee payments and they verify the data is there. But then the data is retrieved by another layer two node, the layer two verifier to reproduce the layer two state using this data as an input to the rollup.
00:18:00.322 - 00:19:27.700, Speaker B: Again parts to talk back golfing to learn all about how the rollup processes this data. Now, how does this work between the layer one consensus layer and the layer one execution layer? We used to have this API called the Engine API and we have the same API right now, except we are adding one method and one type. We're adding this Blobs bundle so that everything that comes with the transactions can be dispayed for by the transactions which then bundled together and separated from the regular execution payload. And so the consensus layer can retrieve the execution payload to embed in the beacon block and can retrieve the bundle of Blobs. The sidecar to then distribute to other beacon nodes for the period that data needs to be available. So we talked about how fort four works on layer one. And to complete the picture, we also need to talk about how EP four works on layer two, because this data would be kind of useless if there were not all these many different roll ups, building ecosystems, extensions of ethereum on top of this data.
00:19:27.700 - 00:20:56.342, Speaker B: And so what it means to update layer two, it means to update the proof that the inputs that we now change about the layer two make their way through the layer two state transition to produce the new layer two output. This transition has to be proven on layer one. And then we change the Verifier code to actually pick up on this data to process it in the first place. And we change the bedsmitter to even submit the data before the Verifier can pick up on it. So it's really bed submitter first, then Verifier, and then the proof shows the full transition of processing Blob data, running the layer two state transition, and then producing the output that then is claimed or validated on show. I'll show this simplified ZK validity proof, ZK roll ups, they all work a little bit different, but if you generalize them a little bit, what it looks like is that there is some form of computation on layer one that verifies business data to show one state route on claim going to another claim. And the proof shows that this transition is failed.
00:20:56.342 - 00:22:33.950, Speaker B: And so how this works is that we first want to prove that we have the correct data in our system. And since not every roll up is every ZK rollup is built with KCG, what some will have to do is this proof of equivalence to show that the KCG commitment over the data is equivalent to the commitment that they prefer to use. Luckily, because this is a polynomial commitment, this can be done rather efficiently as part of the ZK proof in a way. And so there's this precompile that allows you to evaluate a polynomial, do a single point evaluation, and this can be used to do a random evaluation and random in a way that you can securely verify it against another F elevation in another different kind of curve or some system that the ZK rollup prefers to use. And then once you have verified the inputs, you can run your regular ZK validity proof using the same form of input that the ZK rollup is already built for, and then produce the output or verify the output really. And then all you need to do at the very end of this is to just persist the outcome. Now, we also have optimistic rollups and they work differently.
00:22:33.950 - 00:23:12.154, Speaker B: However, we use the same precompile. So the additions to the layer one EVM when we want to verify Blob data for optimistic runups or for seeker rollups, they are the same. We can have this common utility to verify a single point of Blob data. Except that we use the pre compiled slightly differently. Where like an optimistic roll up, they have the notion of this pre image. Oracle. And the pre image oracle is used to access any history in the layer one which could include Blob data.
00:23:12.154 - 00:24:27.570, Speaker B: And then depending if it's a regular hash or the special cache commitment, we use a hash function to verify the pre image matches the claimed hash. Or we use the point of iteration precompile to verify a piece of Blob data at a certain position matches the commitment. To set Blob data and this extended premiums oracle we can plug into the existing single step verifier. So it doesn't matter if you're like an WASM proof system or a MEPs proof system or RISC five proof system if you have this general ID about loading inputs through pre image verification you can plug in this new type of pre image backed by Blob data. So this is kind of what it looks like to integrate this into the Verifier. So we've talked about the proof part of layer two. We also have to modify the Verifier and then the batch smither is really just the reverse system of the Verifier.
00:24:27.570 - 00:25:06.880, Speaker B: In the talk of coffin you have seen that there's this input stream and there's output stream. Now this is the process that takes the input stream and converts it into the output stream in op stack. It's called the layer two derivation pipeline. What it really does, it traverses the layer one chain, the chain of inputs, it retrieves the data, it processes the data. It might use an layer two execution engine to process the data. That's really only the retrieval that we have to adapt to this new type of data. And then the batch emitter, which does the reverse process.
00:25:06.880 - 00:25:43.290, Speaker B: Then it takes a layer two block that has been sequenced. It is going from the layer two block. It extracts the inputs, encodes the inputs in a batch. The batch needs to be encoded in layer one transactions. And if layer one transaction is published now with block data rather than call data okay, so we talked about how layer one works. Now, layer two works if you really want to slot them together. We need the layer two operator to pay for this resource of layer one.
00:25:43.290 - 00:26:42.042, Speaker B: This new type of resource. And what this requires is new type of gas metering. Except that I wouldn't even call it metering at this point because the EVM does all these complex computations whereas data is just static. It's very easy to meter. We look at the amount of data, we can translate it to the amount of data gas that's required to pay for the data. And now this data gas, it works very similar to regular gas, very similar to EAB one five nine where we burn the funds, we burn the fees if there is an excess in usage. And then in the block header of the execution layer one block, we maintain a special variable that can track what kind of excess there is in ERP one five nine.
00:26:42.042 - 00:27:20.346, Speaker B: We know this as the base fee. The base fee goes up if there's an excess. It goes down if there's not an excess. You could express this in many different ways. It doesn't have to be a floating point like variable that moves around. It could just be more discreet and say you could say, oh, there's this much gas that's being utilized over the expected amount. And so what this looks like is that the more excess there is, the higher the price goes.
00:27:20.346 - 00:28:11.670, Speaker B: And note the y axis here. It's a logarithmic y axis. And so exponentially, the cost goes up if there's more excess usage. And so it balances itself out. If the excess usage goes up, prices go up. And then either users are willing to continue to pay elevated prices to continue to use the target usage, or they compensate and they start using less for a little while, and then the prices go down again. And so the base fee in EPM Five five Nine or the excess data gas in EP Four help balance out the resource utilization of the chain.
00:28:11.670 - 00:29:21.614, Speaker B: So we target half, and we have a maximum capacity of twice the target, similar to EP One five Nine, except now with data gas rather than EVM gas, this is expressed in the EIP. It's a little bit hard to parse because the formula is different, but it has the same properties. So just at the same rate as EP one five nine, a rate of 1.125, and it goes up exponentially this way or it goes down exponentially. This depends on the research usage, and the limit is twice as much as the target. And the only addition that we need to maintain this state is this one field addition to the blockader to track the success. Okay, things are going well so far.
00:29:21.614 - 00:30:07.150, Speaker B: We've talked about how forage four works, both on layer one, in layer two, how the fee structure works. But if you are interested in diving deeper into this to really understand it, you should look at these specifications. This is an EAP that spans across multiple layers of ethereum. So there are consensus specs and then there are excretion specs. The consensus specs, they're scheduled for per hard fork. They're isolated per hard fork, whereas on the exclusion specs in our EIP system, they are targeting just specific features of EIPS. Now, there is a new exclusion specs process that describes the EVM in Python.
00:30:07.150 - 00:30:52.334, Speaker B: Expressing further, I think, will take a little bit longer, but the specs are complete. The EIP describes the execution changes, and the consensus specs have all the consensus layer peak changes. And then what would things be if there were not other specs repositories? So we talked about the state transition changes with consensus and execution. But there are also APIs that have to be extended. So there are the Execution APIs to support a new transaction type. There's the engine API to pass along the blocks, or sorry, the blobs between the engine and the beacon node. And then there's the Beacon API that just been modified to fetch the Blob data.
00:30:52.334 - 00:32:16.146, Speaker B: This is the API that layer two clients verifiers would now start to use to pull the Blobs from the beacon node into their layer two node. And then they can cross verify it against the data hashes that have been registered in the transactions that they already parse as part of the regular wallet traversal process. Now, implementing all these specs is not easy, but we're getting there. We have five different consensus layer teams and five different excretion layer teams, actually more than this, even implementing the specifications and participating in testnet. So we started with just Prism and GAF as prototype a year ago during Eve Denver. And since then we have improved the syncing methods, improved the transaction pool, and we're still polishing various different things about the EIP, but the specs are stable and so clients are moving forward. And now we have all clients participating on the same testnets, we have this client interoperability and then it doesn't stop with just client modifications.
00:32:16.146 - 00:32:52.370, Speaker B: Part of EIP four to four requires this new KCG commitment to really be Ford compatible with the long term Sharding vision. And this vision requires a trusted setup or this KCG ceremony output. And this thing has been running for almost two months now, I believe. And also there are about only two days left. So if you still want to participate, there's still a chance. This thing is hosted online. You can use various different clients to participate.
00:32:52.370 - 00:33:43.534, Speaker B: I am showing you this server graph because it's kind of insane that so far we've had over 70,000 contributions. And when I was sharing about 44 just two weeks ago, I think we were at 50,000. This number is rapidly increasing. And the more contributions we have, the better, because the more we distribute this responsibility over not refeeding all the inputs to the ceremony. And just one of the inputs has to stay like private, hidden, it's like toxic waste to secure the final output. And then this final output is used to secure the Bob data effectively. And there are various funnel clients.
00:33:43.534 - 00:34:24.626, Speaker B: There's not just the official clients, there are also, like meme clients. So there's this front end that's this derschcorn teamed. I recommend just browsing around the KCD clients and to try and participate in film New. And after this, like two months of regular participation is over. There's also a special time allocated for teams and participants to do special contributions to mix in their randomness in different ways to create more diversity, to reduce the risk even further. But yes, like 70,000 participants. It's kind of crazy.
00:34:24.626 - 00:35:23.220, Speaker B: This is like already ten times, if not 100 times more than what most ceremonies are getting. Okay, let's talk about progress. So this EIP has been around for almost a year and since last year in Denver we've achieved a lot. We've worked on prototypes, we've worked on the EIP draft, then transformed into more specs. Then there were workshops, more education DevNet started. This is something where I really appreciate the help of Coinbase contributors and other birdie folks like Terence from Prism who put a lot of time into these birdie prototypes to show that it works, to show the viability to explore optimizations. But now we're at the point where we're running devnets more regularly and the KCG ceremony is almost coming to an end.
00:35:23.220 - 00:36:19.880, Speaker B: And in defnet, in defcon we started DevNet or Sergio. After defcon, we started DevNet three. And then earlier this year at the interope event, there was this DevNet that we pulled in all the clients where you can see that it's viable to ship in a hard fork. There's this interoperability problem with many EAP changes where not all clients always implement the same thing. The clients are agreeing on the same pop data and with more testing, more polishing, we can ship this as part of a future layer one hard fork. So currently this is scheduled for the denap artwork, which is the one after the withdrawals. Right now we have various different testnets behind us.
00:36:19.880 - 00:37:24.060, Speaker B: Started with version zero, the hackathon, then there were various different devnets. And increasingly we have been extending this from prototypes to something that's like adopting all the spec changes and the latest features as discussed during this interop testing and the latest features really there are the transaction type, the transaction pool hardening and a decoupling of Blobs. And once these things are finished up, then we're looking at the final stretch, the final milestone towards a production ready for it for and then hopefully you can go live with the next hard fork after the withdrawals. So DevNet five is coming up. This updates the transaction type primarily. This is somewhat of a breaking change. The update here is that we want to perfect the way we structure SSC transactions and get it right.
00:37:24.060 - 00:38:33.700, Speaker B: If this doesn't work out, we might refer to an RLP transaction. But right now there are strong proposals for SSE transactions to shape the way we sign over transaction data in a more structured format, which will be very good for layer two long term to prove any data about any transaction. Then we have some network optimization decoupling of Blobs. What this really means is that the Blobs and the beacon blocks are gossiped separately and so the smaller the messages, the more effective gossip sub is at propagating the data efficiently and without duplication. So this all saves us more benches and then we have transaction pool hardening, meaning that we don't want Blobs to have a negative influence on the regular Ethm transaction pool. So. We're trying to isolate the properties of these Blob transactions as much as possible, have a really strong implementation that can handle the data.
00:38:33.700 - 00:39:18.800, Speaker B: And so this likely will be like a pool only mechanism where we avoid putting in large amount of data if we already have the data. And then there is more testing, there's always more testing. If you want to contribute, this is probably the best place to contribute. We are building more test vectors. There's work on Hives, and there are various other things to benchmark virtual for to make sure it will run well on layer one. So if this is still not enough for you, look at Epfordshore four. We have a question and answer section with vitalik answering things about forage four.
00:39:18.800 - 00:40:09.242, Speaker B: There are various different resources you could read more about for it four. And then if you want to get into contact, you want to really contribute actively to the implementation, to analysis of the KCG ceremony or like other post processing. Or perhaps you just want to visualize the Blob data. Or maybe you want to contribute to the new Blob Explorer that visualizes Blobs. There are all these tooling efforts as well. It's not exclusively client development. During Devcom, there was a team building a Block Explorer, like an experimental Block Explorer specifically for this data and during the Eve online hackathon.
00:40:09.242 - 00:40:33.240, Speaker B: I also hope that more teams can build cool and new interesting tools that utilize this new concept of Blob data. So, yes. Any questions? If there's time left, we can go through some things. Lorenz. Let's scale ethereum together. I'm looking forward to everything that's being built during this echo. Thank you.
00:40:36.090 - 00:40:56.880, Speaker A: Thanks, Proto. There's a lot of time left for questions, but you were surprisingly on 44 minutes mark, so just verify this again after we shot this video, but this was actually very informative, and I think this is the only resource now that exists on the full depth of, what, 48, 44 is. So thank you so much, and this has been awesome.
00:40:57.410 - 00:41:00.480, Speaker B: Yes, my pleasure. Thank you for hosting this. Absolutely.
00:41:00.930 - 00:41:03.050, Speaker A: Take care. Bye.
