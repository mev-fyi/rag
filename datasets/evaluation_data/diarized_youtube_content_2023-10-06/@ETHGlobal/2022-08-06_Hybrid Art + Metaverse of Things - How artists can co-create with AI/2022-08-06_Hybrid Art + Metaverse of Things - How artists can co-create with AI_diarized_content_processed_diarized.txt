00:00:06.090 - 00:00:26.120, Speaker A: Hey everyone, welcome. So we have Portrait XO here, Thomas and Ivy, and they're going to give you a session on hybrid art and metaverse of things. We can take questions as we go. Just put them in the chat and if you have any questions, they'll answer them towards the end. I'll let you guys take the thanks.
00:00:26.890 - 00:01:17.286, Speaker B: Oh, hi everyone. Thanks for joining and thank you to Zora for having us. So I am Pochette XO. I do a lot of hybridized art with human machine co creation. And as an independent researcher, I work a lot with data scientists. And so I wanted to put this workshop together that will give a little insight of what it's been for me as an artist to work with machine learning as well as have perspectives from Thomas Hafferlack, who is founder of Pollination AI, who's a data scientist and a new media artist. And then a little glimpse into our future from our gen Z prodigy, Ivy, who is also studying computer science and transdisciplinary research.
00:01:17.286 - 00:01:31.830, Speaker B: So I'll just get started with our first slide. So Pollinations AI is an open source platform that thomas, I'll let you do a brief introduction.
00:01:31.910 - 00:02:09.986, Speaker C: Brief introduction? Yes. Yeah. First of all, thank you that we were already talking about Pollinations AI on the first slide. I'm very happy to get this introduction. Yeah, pollinations AI was a project that me and a few colleagues created initially, more as a tool for ourselves because we were very fascinated with the developments around generative deep learning. So deep learning that's focused on generating media. First of the first kind of application where these models have become very good is generating images.
00:02:09.986 - 00:03:06.120, Speaker C: And I was so fascinated with the explosive kind of development we had in the last, say, two years in terms of creative possibilities, image quality, and also types of media and types of ways we can process. Me, I was so fascinated that I wanted to make this also available to friends and artists around me who I know and who were really keen to experiment with these techniques. And so it was kind of more of a platform to make it more accessible initially. And now we are turning it into an open source based business with some funding. But yeah, the idea is to really work closely with the open source community and make it easy for people to experiment with these new ways of, let's say, generating and manipulating all kinds of media.
00:03:06.730 - 00:04:17.040, Speaker B: Yeah. Join us on Discord, on Pollination Discord, we have a lot of really fun and meaningful conversations about how we can sustain and work with web three technologies to create these kind of like new ways of, I guess, co creating and doing things on chain with smart contracts and stuff like that. So, yeah, I'll just show some examples here we go of some of the latest experiments. So this is from the new Zora trailer that just went out and Thomas introduced me to this disco diffusion warp model, which was my first stab at it. And what's really interesting to me is what you see is the same text prompt taking one video of a talking head but in different sizes. And this kind of stuff just fascinates me because you get such different results. So it ends up leading me to way more questions of why does this behave the way it does.
00:04:17.040 - 00:05:26.642, Speaker B: And then, so Ivy created some other custom AI visuals and we just submitted all this stuff that we created together. And then Sir shrugI then took the content and then did some really cool analog effects to it. So that is also available in Pollinations. And to the right that is disco diffusion as well. And then to the left is a text to Image Vqgan clip approach that I've used mainly for a lot of my experiments in the last couple of years. And one of my most, I guess obsessive way of working is obsessive obsessed way of working with all of this has been like trying one AI model and then taking the result and then feeding it into another one. And so what's been really fun is I discovered Pollinations AI from Thomish just after I finished creating this neural vocal duet AI album in collaboration with Databots.
00:05:26.642 - 00:06:50.160, Speaker B: And so then I started doing some experiments with taking that music and seeing what kind of visuals I could create. So this is Lucionic, which allows you to do things like control the parameters of your treble, your mid and your low frequencies and set these very granular ways of how the visuals move in latent space so it just gives you even more control. And so this was using a few different parameters with the wildlife data set. And then I decided to experiment with taking that visual into text to Image VK again because I wanted to customize it a bit more to aesthetics that I was imagining at the time. And I've been doing like I'm always doing these deep dive research of related to identity and stuff. So this what I'll just volume text to Image VK gang clip. I think the text prompt was hyperspace Mongolia melting gold and holograms because people always think I'm from Mongolia but I'm not.
00:06:50.160 - 00:07:55.620, Speaker B: But I've learned that apparently. So my background is South Korean and apparently there was a lot of Mongolians that migrated to Korea at some point, so I was really pleased with the results. Also to play with what happens when you take like a square format of a video and then try different formats and aspect ratio as the output I thought was really beautiful. And there's also a super resolution model on Pollination AI so you can upscale your videos. So this was really cool and I've performed live with these visuals because I managed to get it up to 4K. So the next project that I think is really exciting to talk about is how artists can use art as data for creating even more customized AI art. And Thomas, I'll let you explain more detail of what Ruby's and Diamonds project was.
00:07:59.350 - 00:09:09.338, Speaker C: Kind of what I find exciting is not necessarily the idea of AI replacing artists. I mean, there's a lot of quite heated debate at the moment about what the impact of, let's say, Dalia Two is on certain types of art. But I've always been quite happy with the idea of the collaboration between AI and human and I find that one of the most exciting kind of ways of working in this space. And so I was in an artist residency in the Dominican Republic and got together with a contemporary artist, Nicola Rubenstein, from Berlin, and she paints in a certain style and she was also making videos. So we were filming the eyes of different participants. And her idea or dream, or our dream, was that we could apply her painting style to these videos that she filmed. And, yeah, we used different techniques to reach this result.
00:09:09.338 - 00:10:11.774, Speaker C: But she was very happy because she really felt like that. The machine learning models managed to capture her aesthetics in a very interesting way and then allowed her to do something that it would be impossible for her to do without, or it would be a lot of labor, a very labor intensive process to paint all of these video frames. So it was kind of a very interesting mix. And it was also interesting to see how excited Nicola was to collaborate with the machine in such a way. And we're planning to do many other projects together in the future. And yeah, the idea then is somehow you could also create presets together with artists that can then generate series of media that could be used in all kinds of ways and possibly even monetized through NFTs or through some other ways where one would also share profits and so on. Yes, so cool.
00:10:11.812 - 00:10:47.100, Speaker B: It's a really beautiful project. So, last but not least, I am going to pass it over to Ivy, who is the youngest AI artist and researcher that I know of. Ivy has a scholarship to Tufts University starting soon. He's spending some time with us here in Berlin for a while. And yeah, I'll let you talk about your latest experiment with two D to three D or I think it's just so amazing, like how fast you work with everything.
00:10:47.630 - 00:11:41.334, Speaker D: Yeah, I mean, introducing to this technique, coming from the pollinations and seeing pollinations, very exciting for me. But it's also a project that is trying to implement user friendly interface for working with the AI models and it takes some time to implement every single model. But in the world of machine learning, you get new networks like almost every week. And it's interesting to experiment like latest technology there is. So this is a clip guided neural radiant field model, which is quite new and it's not yet implemented anywhere. So if you want to research it, you have to follow GitHub and follow the instruction to install it. But it's also interesting just to work with the networks which are not implemented.
00:11:41.334 - 00:12:48.478, Speaker D: So if you want to try something new, try to find networks which just brand new and which you might be able to somehow use in your project and so on. And I started working with the stuff almost two years ago and at that moment the field was quite empty. There were not a lot of models which were capable of working with the because it's not Euclidean data, so it's very hard to work with it and it's very exciting for me to see that. Just two years in it and we already get this awesome results which are capable of producing 3d models from images, from text. And this is the model which is called Dream Fields by Google. It was originally developed by Google and it's capable of creating 3d models based on text or image data. And it's quite easy to use it, but it's very exciting.
00:12:48.478 - 00:14:23.940, Speaker D: And for example, this is the video of a spaceship. So it created a 3D model from the text spaceship and it has its certain style, it's not like photorealistic and it's realistic, but it's still exciting to see and work with it. And this is just the user interface for the network itself and this is an image of a text input and if you go further and this model is also capable of working with images and is capable of transforming image data to 3D models. And for example, this is I took an image generated by Mid journey which was like Neural flower. I guess if I remember correctly, the text prompt was Neural flower and then I put it into this model and it produced this 3D model but working with the you can always modify it in different 3D software, 3D modeling software. So for example, it generated this flower which I found very interesting because it doesn't look the same as the image, it has certain features same as image, but it's not 100% same as image. And I liked it because it created something new, it added its own features to the 3D model and then I modified it in a blender to look better.
00:14:23.940 - 00:15:03.626, Speaker D: I cleaned it, I modified it. So it still requires some artistic input from you. And then you can use those 3D models like almost anywhere. You can generate some kind of worlds with them, you can use them, you can 3D print them, you can really do anything you can with just normal 3D models and it also generates the texture. So it's also exciting, it generates the shape as well as texture. And on that note, I think it's everything from my side.
00:15:03.808 - 00:15:08.398, Speaker B: That's so cool. I want all of these in my metaverses, in my 3D world.
00:15:08.564 - 00:15:12.240, Speaker C: I want you to show me how you made these last ones.
00:15:14.210 - 00:15:42.954, Speaker B: Really nice. But yeah, that's our Flash presentation. Wow. We finished right on time. We thought we'd leave ten minutes for Q and A, and we're happy to go deeper into the Flash presentation that we've just presented. These are our handles on Instagram and Twitter. If you want to tweet to us or message us on Instagram directly because maybe you come up with questions after this.
00:15:42.954 - 00:16:07.540, Speaker B: Or we don't get around to answering questions, but yeah, that is us for now. Are there any questions from anyone that says talking, but I don't know if I see anything. Let me open okay.
00:16:10.070 - 00:16:16.200, Speaker A: There is no questions in the chat, but if you have any questions now, feel free to unmute yourself.
00:16:20.730 - 00:17:22.490, Speaker B: Yeah, this is a lot of content to go through in a short span of 30 minutes. But what is really exciting to me is that we have all of these tools available. And in the context of Web Three, pollinations have been experimenting with hybrid models, right? With using things like IPFS for data storage, and also having conversations like, what is the most streamlined way of doing this kind of stuff, being aware of how computationally heavy it is, how much time it takes to do all of these things. And so I think it becomes really meaningful when artists become part of this dialogue of where we're going with all of this and get creative with potential problem solving. Thank you. I see a message from Pierre. Thank you, Pierre.
00:17:22.490 - 00:17:30.990, Speaker B: So, yeah, at the moment, Pollinations works connected to Google Colab.
00:17:31.650 - 00:18:34.050, Speaker C: Yeah, we're in a hybrid state at the moment. We have our own GPUs running in our back end since a little while, and we're kind of slowly migrating the models that previously were relying on Google Collab, which is a service which allows you to use GPUs for free in the cloud. Probably people who are in the field know about it. But what we did with Pollinations initially was kind of bootstrap because we didn't have money, which was initially we didn't have money, so we didn't have any way to pay these GPUs. So we kind of connected to Google Collab, which made it a little bit more difficult to use, but was kind of an interesting way to start our project. And now we're slowly making it more user friendly by migrating these models now onto our own GPU back end. And we're planning also to keep it free as long as we can for end users.
00:18:34.050 - 00:19:33.220, Speaker C: So I think if you start using Pollinations in the next few days, it's going to be quite a nice user experience. Maybe we're still ironing out some bugs, but yeah, we have our own back end, so it will allow you to kind of for free, use all of these techniques without too much difficulty, hopefully. And you also have kind of bots in our Discord or on Twitter, so you can also kind of talk to a bot like midjourney for example, also is doing in a very interesting way. So we're experimenting also with different ways you can kind of talk to these models. And the bots have also been quite an interesting direction to take. But you still have also our website, Pollinations AI, where you can create and it's going to get easier to use and hopefully still just as interesting.
00:19:33.830 - 00:19:39.734, Speaker B: Yeah, people can get an image generated based on their Twitter profile information.
00:19:39.852 - 00:20:30.550, Speaker C: Yes, that's going. If you follow Pollination's AI on Twitter, you will get a response with a portrait drawn based on your Twitter bio. Yeah, so these are some of the fun we're having with the AI models. And it's kind of interesting. We have one new idea each day, like a daily generative Tarot card generator or your horoscope or games, which involve generating images and guessing what's on the images. Yeah. Now, kind of these models are usable in this kind of modular way where maybe you can kind of, let's say, feed the input of a text generation model into a model that generates images.
00:20:30.550 - 00:20:48.620, Speaker C: I think we're only going to start kind of having all these fun ideas and kind of applications. This is just kind of a first step in that direction, generating people's portraits without them even saying I want it.
00:20:51.010 - 00:21:47.230, Speaker B: It's really exciting. Interesting times with creative AI, I think. And Ivy is going to come out with a whole series of these new experiments, some of our own collaborations as well, that we've been working on. And I don't know, it's just a really exciting space to be involved in. And I love the idea of having AI become more democratized so that there is a real diverse narrative around what it means to work with these technologies. And I don't know, art to me is like any chance we can abstract something out into some kind of experience or something tactile, gives us something tangible to either look at, feel, or experience. And we can have some kind of an understanding of how all of this works, addressing things like bias.
00:21:47.230 - 00:22:58.342, Speaker B: And it's up to artists to create with these things and talk about these narratives in interesting and immersive and meaningful ways. So, yeah, thank you for making this all happen. I always get really excited when, I don't know, people show some new weird experiment that they've done. And we've had a lot of really fun what the fuck? Moments during our co creative process of just, I don't know, I think artists are really good at accidentally breaking stuff and you've pointed that out also where I've accidentally broken stuff. Totally unintended. But then that's because probably me not being a programmer and how I find it so valuable to have these conversations with data scientists, programmers, artists like yourselves. And it's this combination of different perspectives coming together that really excite me.
00:22:58.342 - 00:23:10.490, Speaker B: And so, yeah, if any of you ever have questions or want to get involved or collaborate or anything like that. We're all pretty open.
00:23:10.640 - 00:23:59.718, Speaker C: Yeah. Also discord is a very easy way to talk, to get in touch. Personally, it's always a real pleasure to have an exchange with people, enthusiasts also in the field. I think the thing with the errors is really nice because I think that's what makes them artistically also so interesting is that the kind of failure states are still interesting. When you have an MP3 that's kind of broken, the error sounds quite annoying. They're very digital, whereas when the AI fails, it fails in kind of funny organic ways. I think that makes it yeah, totally.
00:23:59.814 - 00:25:01.760, Speaker B: We have a nice comment in the chat hash Haron, but yes, I'm just equally excited about the custom AI methods and approaches that artists can harness by doing things like that, like what you did with rubies and diamonds. It's just an example. And I've been sharing that project with all of my visual artist friends because I think that process also of seeing the machine take your own data, your own artistic work and try to create something new with it, that creates a really interesting relationship, I think, with your own work. And I think it opens up a way for artists to become even more intimate with your own process and your practice. Yeah, I'd love to see more of that. So you're going to get pinged by a ton of artists now.
00:25:02.850 - 00:26:08.718, Speaker C: That's exactly what I'm open for. Yes. Also, thanks to a little bit of help and pointers from Ivy, we will also and of course, we've been also kind of researching 3D model generation, so 3D object generation. So we want to also integrate these models that Ivy has been working on and we've also been kind of researching so maybe if you will be able to generate 3D objects or avatars. There's been quite a lot of development in terms of avatar generation from text. So you can describe like I want a forest witch who's overweight and maybe has freckles and you can write it all in text. And these models are doing quite a good job at generating avatars that can already also be embedded into games and imported into unity because they come in this rigged format.
00:26:08.718 - 00:27:21.660, Speaker C: But this is like one of the models does it. So this is something where we see kind of a little bit a big future in terms of generating 3D objects and then also thinking about the metaverse and generating possibly whole immersive environments. And it looks like a goal that's very reachable and possibly also democratizes the access people have to creating their own content in the MetaWars because 3D modeling is not such an easy to acquire skill. Yes, so this is I see a lot of future there and we're kind of also focusing a big part of our research now on this idea of generating 3D models. And also if some people are interested in talking about that, we're also collaborating with ivy. Hopefully we are integrating some of the things he found already. So, yeah, it's an interesting space because there's just so much new stuff happening each month.
00:27:25.070 - 00:27:36.080, Speaker B: I'd love to see 3D artists get involved in that space also of what they think about all of this and how it could implement maybe a new workflow for them as well.
00:27:37.570 - 00:27:40.910, Speaker C: We're actually investigating that with some kind of partners.
00:27:43.010 - 00:28:34.814, Speaker B: Because I think that's the other thing about AI art is that there's this other side of it where people are really scared of it replacing jobs or replacing roles and things like that. But I don't see it that way. I think when there are emerging technologies, this is also why it's important to make sure that the creatives who are working with these methods and approaches like 3D art and 3D artists, to be part of that dialogue and to play with the technologies and feedback and how does that enhance their practice. And I don't see these things as replacing there's nothing that will replace the craftsmanship of an artist's own skill set. I'm just going to quickly answer. There's a question that came in the chat. Pierre, I will message you directly.
00:28:34.814 - 00:29:38.440, Speaker B: If you can message me either on Instagram or Twitter, I can share some musical AI resources. There's some open source stuff that I've used. There's also two process documents I have on my website, which I'll just put in here. And I have some free AI generated audio samples using DDSP auto encoder model, which has been really fun to experiment. And yeah, I think we are at the end of the workshop. I'm just being mindful of time. If anyone has questions or I don't know, shall we end it now and continue in the metaverse of whichever metaverse people want to choose to engage with us? It stop screen share.
00:29:39.290 - 00:29:44.070, Speaker C: I think that sounds good. Engage in the metaverses.
00:29:45.210 - 00:29:48.370, Speaker A: Yeah. Thanks, guys. Bye.
