00:00:06.170 - 00:00:41.290, Speaker A: Hello. Welcome to the getting Started with storage workshop. My name is Dan Willoughby, and I am a developer advocate for storage. Let's get into it. Storage is a decentralized cloud object, storage for video, images, backups, NFTs, DApps, et cetera. We're also known for our high performance. What this workshop is going to show you is the types of web two storage compared to web3, what storage is and how it works.
00:00:41.290 - 00:01:50.910, Speaker A: And then I'll give you some demos of our simple IPFS pinning for NFTs, our Uplink CLI demo for direct Web three storage access, and our AWS s three compatible demo. So in web two, we had multiple options we could have on prem or different types of durability in the cloud, and then we have web3, which is decentralized. So on prem means it's a hard drive in your laptop or in a Nas device. It's relatively fast, but it can be a little bit costly, and you always have the maintenance and upgrades to deal with. Of course, it's quite risky because you only have one copy of your data in single site. You rent hard drives from backblaze and wasabi, and you rely on them to make copies of your data. They use Raid technology, so they'll have one to three copies of your data.
00:01:50.910 - 00:02:59.908, Speaker A: And it's generally inexpensive, but they have minimum storage day policies and free egress, which if you fit within their fair use policies. So one of these hard drives may fail, but they'll build it back up. So your risk level here is relatively low, but their entire data center could burn down and you will lose your data. Single region is what most people are used to, in which a lot of the web uses nowadays. Amazon, S Three, Google and Azure store your data in multiple data centers within one region, such as Europe or like Virginia or Oregon. You'll pay a lot more for this storage, about triple, and they tack on hefty egress fees, and they also charge for APA requests. So some people like to try and get around those hefty fees, and they use like a glacier option, which is very low cost if you don't retrieve your data.
00:02:59.908 - 00:04:00.030, Speaker A: And what most people don't realize is the retrieval fees often outpace what you would pay on just the standard AWS tier at the top of the line. Azure offers a multi or Amazon offers a multi region where you can have your data, say, in Virginia, Oregon, or Europe, and you pay for each of those different regions. It's really durable, great reliability, but it's also very expensive. The risk here is you'll burn through your money really fast. So decentralized is also multi regioned. So when you upload a file, it'll be stored globally in, say, Europe, United States and Asia. Your file is broken into little pieces and stored across the globe.
00:04:00.030 - 00:04:45.120, Speaker A: You'll pay much less here. Egress is very reasonable. There's no charge for API requests, and the code is open source. Also, you notice there's a little tree there. It's more environmentally friendly because we at Storage encourage people to use existing unutilized hard drives when they join our network. Let's zoom in a little bit on the numbers. For a multi region comparable to the highest tier of Amazon, you will pay $4 per terabyte per month, whereas Amazon is 23 for one region and you double that for each region that you go up.
00:04:45.120 - 00:05:38.110, Speaker A: Also, bandwidth is about ten x cheaper. You'll notice availability and durability is very similar. How about the cheaper options? Well, Wasabi says free egress, but that means you can only download your entire file once per month and Backplace has bandwidth of $10. They also tack on other fees. And of course, multiregion is five to $6 per region, whereas on Storage it is free. So, in summary, if you want to protect your data, you're going to pay more money. But with our decentralized technology, you don't.
00:05:38.110 - 00:06:46.258, Speaker A: So how does storage work? Storage allows anyone with unutilized hardware to join our network called Storage Node Operators, when they store our customers files. They're compensated fairly for doing this. So we generate demand by creating applications such as our S Three API compatibility layer. We also have a library called Uplink, which allows users to interact directly with the storage nodes. And then we have satellites that kind of does a little bit of the in between work to make it a little easier to use. That ensures that access controls are managed, ways to pay the storage Node operators and ensure the data is reliable. So what happens to a file when it's uploaded? It's broken into a little first it's encrypted and then it's broken into a bunch of little pieces, like 80.
00:06:46.258 - 00:07:24.110, Speaker A: Those 80 pieces are distributed around the globe to independent Storage Node Operators. On its way back, those nodes are identified, the segments are downloaded, put back together and decrypted and you get your file again. Multiregion is included by default. Performance up to nine to 24 gigabits per second. You'll see availability. Durability similar to AWS s three. But let's not forget security and privacy.
00:07:24.110 - 00:08:12.930, Speaker A: You'll have end encryption on edge or in the client libraries. Not even Storage can read the contents of your data. So how does it work? You see in this little demo I have here that pieces of this file is stored throughout the globe. This one happens to be a FPS video of Big Buck Bunny. So let's go ahead and see it stream. I see I've hit play here and it's now streaming. But I can also hop around and it will reconnect to those various storage nodes in the network and start streaming those pieces and reconstitute this file.
00:08:12.930 - 00:09:07.474, Speaker A: And then over here is a little picture of the graph of where these pieces of the file are coming from it. So we offer a few ways for people to easily get started. We have our s three compatible API, which acts as a great web Two bridge of the tools everyone is familiar with. We support server side encryption and it's globally distributed and highly available. The client side integrations is more of what web Three is used to. With end to end encryption, you own the private key and you encrypt your data and it also ensures maximum privacy. So let's get into some demos here.
00:09:07.474 - 00:09:42.162, Speaker A: I want to show you our decentralized NFT storage using the IPFS protocol. To get started. You go to storage. IO IPFS. On this website you can see a quick demo of how it works. I'm going to upload a simple tree picture and there you see it's already done. And here is our IPFS gateway.
00:09:42.162 - 00:10:10.234, Speaker A: So you can see that here your file. Your image is displayed. So this works great for NFTs or any other files that you need to store in your DApps. Or say you have a video stream. So let's also load this in Brave browser. There it is. But you can see it also works with curl.
00:10:10.234 - 00:10:45.260, Speaker A: So I will also do that. I have a picture of myself here. So let's get this code and I am going to upload this file. Here it comes back. So let's view that in IPFS browser and see if it works. There you go. It does.
00:10:45.260 - 00:11:51.278, Speaker A: So what if I wanted to use these files, say in my react app. So let's go ahead and do create a new react app. This is going to get us a bootstrapped react app really fast with the things that we'll need to show this here. And as that's playing, we will make sure we have the file. So you notice that I just copied this URL here, which isn't our IPFS gateway URL. And then I'm going to start a TMX so that I can run the create act the create react app. All right.
00:11:51.278 - 00:12:23.286, Speaker A: And it is on my other screen. So let's pull that up here. So let's say I want to I want to load this image into here. So let's say image source equals that URL I just copied and save that. Bam. Wow, that's a big tree. So let's go ahead and add some style.
00:12:23.286 - 00:13:07.270, Speaker A: Say width equals 100%. That's a little bit better. And let's get rid of some of this other boilerplate. I'm going to say this is my storage tree on storage IPFS. Great. So that's how you get started with storage IPFS. So next, let's see, I want to show you how to interact directly with our web3 storage nodes.
00:13:07.270 - 00:14:13.210, Speaker A: And that is done with the uplink CLI. So if you go to Docs storage IO, you'll notice we have a quick start. So what you'll need to do is first sign up for storage and confirm your email. And this just helps to have a nice interface to the storage notes. And then next we'll want to install the uplink CLI to install the binaries and I've created a Linux instance here where I'm going to run these commands. So terrahaya have downloaded the first binary and then I will unzip it. And then let's go ahead and install Uplink to our user bin.
00:14:13.210 - 00:15:05.802, Speaker A: So that puts it in my path. All right, so next we will want to create a bucket. So uplink make bucket SJ for storage. And let's call this enyce web3. Oh, so I haven't configured my access yet. So in order to do that, I will need to go to storage IO sign in and I'll sign in here. And I'm going to go ahead and do this quick, start using upload using the CLI.
00:15:05.802 - 00:15:55.514, Speaker A: So let's call this ethnyc. I'm going to give all permissions, but you can limit the permissions to certain buckets or to download upload or delete and specify how long it'll last. So here is my secret keys. Don't worry, I'm going to revoke these. But I'm going to go ahead and copy these to a file on my server for future access. And then I'm going to run uplink configure config setup. Go ahead and name these credentials.
00:15:55.514 - 00:16:27.160, Speaker A: The default is Main and then this is where the API grant comes in. Satellite I address is here. And then a passphrase to encrypt this data client side. All right. And then it's also asking me if I want to generate S three backwards compatible with gateway credentials. I'm going to say no, we will do that later. All right, perfect.
00:16:27.160 - 00:16:56.134, Speaker A: So we've already installed our uplink. We've called setup. So now let's create our bucket. Uplink make bucket. Let's call this ETH NYC. Web three. All right, so now I can LS see what's in that bucket.
00:16:56.134 - 00:17:23.420, Speaker A: There's nothing. So what I want to do is that Big Buck Bunny video that we just watched. I want to upload that. So let's do that. So uplink copy Big Buck Bunny to SJ e NYC. Web three. Oh, and I want to time this.
00:17:23.420 - 00:18:08.040, Speaker A: All right, let's go ahead. There it goes. It's taking a little bit, but let's say I want to make it go a little bit faster. So what I'll need to do is depending on how much CPUs I have, I can add some parallelism. So let's go ahead and say parallelism. Well, I have four CPUs, so let's say eight. So there it goes again.
00:18:08.040 - 00:19:11.420, Speaker A: Much better. So the parallelism comes in handy when you're uploading very large files. In this case, we are uploading a 642 megabyte file, which took about 28 seconds, which is fantastic. So now let's go ahead and get a link to that file. The way we do that is we call uplink Share and I want to make this public. So I'm going to say URL. And then I'll need this file path and I'll tack it on there.
00:19:11.420 - 00:19:38.960, Speaker A: There I have it. A URL. So let's go ahead and look at that. And it looks like it was stored on 833 storage nodes. And there you have the distribution on the right here of all those nodes. And let's say click play. There it goes.
00:19:38.960 - 00:20:08.940, Speaker A: So here we go. A freshly uploaded file on the network, streaming four K sixty frames per second in real time. Pretty awesome. Uplink also has various other commands. You can remove buckets, you can list your files. So let's see, that the contents of our file or our bucket. Sorry.
00:20:08.940 - 00:21:01.018, Speaker A: So there's our big buck bunny file. You can server side move and you can remove objects so very similar to s three. So that's a great segue into our last demo. Our web two bridge AWS s three compatible API. So in order to get started with AWS, the tools that you're used to go to docs storage IO again and click on this AWS CLI quickstart. And we'll start again by generating a access key. So let me find where I put that.
00:21:01.018 - 00:21:53.960, Speaker A: Okay, so I want to create this time I'm going to call it AWS CLI. Oh, I've already done one of those. Let's do EC. And I want to continue in the CLI. So let's go ahead and say uplink setup. Let's call this AWS CLI. We'll copy that API key in here, satellite and our passphrase to encrypt the data.
00:21:53.960 - 00:22:49.080, Speaker A: And this time I'm going to say yes, I would like to generate these. And these are my credentials, which are secret. So I'm going to copy those into this very secure file on my server. You'll of course, want to make sure your secrets are encrypted and stored securely, but for demoing purposes, this is okay. All right, so now I'll want to set up the AWS SDK. So here, I've gone to this link, but you can easily find by saying how to install AWS CLI. I am going to download the latest version and unzip it.
00:22:49.080 - 00:23:44.984, Speaker A: Lots of files and go ahead and install. All right, so I should have AWS. So let's go ahead and open tmux so that I have multiple windows. I'm going to run AWS configure and I want to cat my credentials. File access key ID is that first value generated. The secret key is the next one region name. You can type in whichever and output format of none is.
00:23:44.984 - 00:24:56.030, Speaker A: Okay here. So now that we've done that, we can go ahead and look at our buckets. So in order to use the AWS CLI, you'll need to specify the endpoint URL with httpsgateways storageshare IO and then we can do ALS command. Okay, maybe we can't. So let's go ahead and make a bucket. Let's run AWS configure again, make sure I copied those correctly. Sometimes I copy them in the wrong order.
00:24:56.030 - 00:26:22.850, Speaker A: Oh, you know what? I have done the wrong thing there just now API key. So we will want to make sure that these are correct. And now we should be good ethnyc AWS. Okay, I must have fat fingered copying that in there. So you'll see that it created that bucket. And I should be able to list the contents up there, which are none. So let's go ahead and copy that big Buck Bunny video into there and let's shorten the name this time to BBB MP4.
00:26:22.850 - 00:28:09.218, Speaker A: And now it is going to copy that file through our S three compatible gateway web two bridge into our distributed decentralized network. Now, it's not fully utilizing that parallelism here, so it's not quite as fast as before, but with some configuration, you can get the speed a little faster. Once this is complete, I will show you this file in the browser. Okay, so we'll go ahead and view that bucket, and I always have to type my encryption password to see the contents of my files. And there you have it, the uploaded file. So there's a lot of other things that you can do on storage. Just head to Docs Storage.
00:28:09.218 - 00:28:36.410, Speaker A: IO DCs. We have very many detailed tutorials on how to get started. Here you can see our IPFS pinning sample code. So I hope that you have a fun time hacking and I will see you when it's all over. Thank you so much. Bye.
