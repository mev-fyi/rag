00:00:06.970 - 00:00:50.854, Speaker A: All right, I'm going to get started. Thanks everyone, for joining me tonight. My name is Yonden Fu and I am one of the co founders of the Livepeer Project. So Live Peer is a decentralized video streaming and processing network. And I'm going to talk to you today about unleashing Web three video experiences with Live Pure Studio. But before I dive into Live Pure Studio and all the different video experiences that you can build with it, I want to talk a little bit about open video infrastructure and what we've been building at Live Peer and how that enables Live Peer Studio in the first place. So just to kick things off, today 80% of all Internet bandwidth is consumed by video streaming.
00:00:50.854 - 00:01:38.118, Speaker A: And this shouldn't be too surprising to many of the folks in this room that are used to consuming content online on various social platforms. And this number is really only going to increase in the future. So 80% is really just the start of this trend. However, if we take a look at the status quo and the landscape of how Internet video works today, it looks a little something like this. So we have this closed box that's powered by web two applications and Web two infrastructure. And many of these names on this slide should look familiar to folks. So in Internet video we have dominant players like YouTube for on demand video, we have Twitch for Live Streams.
00:01:38.118 - 00:02:03.518, Speaker A: And increasingly, as of late, we have Instagram around short form video with their reels feature. So TikTok is also in that category as well. At the same time, when we think about the infrastructure that powers all of these experiences, we have some familiar names. We have AWS. We have Google Cloud. We have Azure. So a lot of these hyperscalers and cloud service providers.
00:02:03.518 - 00:03:14.698, Speaker A: And you might ask the question, well, why highlight this? Is this really a problem? And I would make this claim, which is closed infrastructure. So that box that I just showed on that last slide leads to closed walled gardens. And this is particularly important when it comes to video. I would state that because video only exists and can be experienced within the boundaries dictated by whoever controls the infrastructure. So imagine the scenario where you do have portability around all the video data on Instagram or all the video data on all these different platforms. If you can't actually play back that video and you can't actually distribute that video effectively to a global audience, then you're still hamstrung, right? And it's really only these small number of giants and these small number of cloud service providers that are effectively able to provide that infrastructure at scale. So your ability for your content to exist and be experienced by the rest of the world is dictated by the terms that they set.
00:03:14.698 - 00:04:04.662, Speaker A: And some of the implications that I think are interesting here are one, developers are unable to freely experiment with building new video experiences without either one of the following exorbitant costs. So this infrastructure is really expensive to run. So either you pay a gatekeeper for access to it or you build it yourself. But in both cases, you're paying a lot of money. Or in the case where you're reliant on a gatekeeper for access to said infrastructure, you're subject to the threat of a single party that may or may not have any accountability mechanisms to flip a kill switch in order to make your content available and accessible to the rest of the world. Or not. And second, what I think is interesting is video creators are actually defined by the platforms that they use instead of being self sovereign.
00:04:04.662 - 00:05:06.590, Speaker A: So I have a colleague that likes saying that he wouldn't want to be a Twitch streamer or wouldn't want to be a YouTube streamer, but he would just want to be a streamer and he's defined by the act that he performs and the content that he creates. But that's really not possible today, right? Because you can only be a streamer if a platform like Twitch allows you to be a streamer. You can only be a streamer on YouTube if YouTube allows you to be a streamer. So the situation that we're currently in is because infrastructure is not particularly accessible, you have to play on the terms and the playing field dictated by whoever controls said infrastructure. So we think a lot about this at Livepeer and what we think the path forward is, is open video data and open video infrastructure. And an example that I like going through of the possibility of what the future could look like in terms of unlocking new and rich video experiences. I often cite a pair of applications from the Lens ecosystem actually.
00:05:06.590 - 00:06:01.838, Speaker A: So for those that are not familiar with Lens, lens is a decentralized social graph protocol that's built on the polygon blockchain. And what's interesting about their approach is that they've created portable social graph data. And I'll get to why this is relevant for video momentarily. But before getting to that, what I have on this first slide is a screenshot of an application called Lens Tube. So you can think of it as a decentralized YouTube where the social graph data and all the social post data is portable and shared as common infrastructure amongst the same set of applications. And we can see here that we have this video experience that looks very much like YouTube except it's built in Lens Tube. But if we look into this next slide we have another application from the same ecosystem called Lenster and this is actually the same exact social post data and it's surfaced in just a slightly different way.
00:06:01.838 - 00:06:55.454, Speaker A: So it's the same video content, it's the same text content, but it's presented in different user interface. And the reason this is interesting is because we have the advantage of data portability which allows developers now to build specifically for custom preferences depending on a user base. So it could be the case that in scenario number one, it makes more sense to build this video consumption centric application experience. So something like Lens Tube might fit that user type a little bit better, but something like Lenster, if you're less focused on consuming, say, long form video content, you're probably not going to want that video to be taking up the entirety of your screen. If you want to scroll through a feed really quickly, that's really not optimal for that. So something more along the lines of what Lenster has built might be a better fit for that. So what's really cool here is that this is the same social post data that's shared and interoperable.
00:06:55.454 - 00:08:14.038, Speaker A: And now these developers can be really creative around how they build custom experiences and new experiences for different audiences, instead of a single monolithic application having a single developer team trying to cater towards many different audiences simultaneously, but never truly fulfilling the needs of any of the audiences particularly well. So where does video come into play? The reason that I like highlighting this example is that in both of these cases, video is still featured prominently here. And imagine a scenario where you're unable to watch the video in Leninster as well as you can watch the video on Lens Tube, and that would detract a lot from the experience, right? So even though you've created this new interesting product experience for Lenster, you're kind of hamstrung because someone tries watching the video content for your social post and it just doesn't work particularly well. Maybe it looks a little bit squished, maybe it keeps buffering. You're not really going to paint a very compelling picture of your product if that's the case. So this is where a guarantee of experience and portability of experience comes into play, where we think that the future of rich media applications is going to be the combination of these two things. One is data portability.
00:08:14.038 - 00:09:58.300, Speaker A: So this is something provided by projects like Lens where you can share social graph data. But second is this guarantee of experience and this portability of experience where you can go to these different applications and you can be guaranteed the same high quality video consumption experience as some other application in that ecosystem. And we think that really the only way that you can do that is with shared public open video infrastructure to provide these guarantees to more developers so they can spend less time worrying about, well, is my video going to buffer? And more time on how unique can I make my product experience? So many of the people in this room might already be familiar with decentralized storage, and you might be asking the question, well, if I already have decentralized storage and I'm able to upload, say, an MP4 file to IPFS, Rweave, so on and so forth, isn't that enough? We've already seen examples of applications that upload MP4 files to these systems and then play back in the browser. Don't get me wrong, you can get pretty far with that approach. However, the reason that I'm choosing to highlight this right now is that if you take a look at the existing web, two experiences that consumers know and love, there's a lot going under the hood and a lot of complex plumbing that makes that YouTube experience, that Netflix experience, that twitch experience so seamless, that low latency, that interaction that you can have in a chat box because the latency is so low. There's so much that's happening in order to make that work. And while decentralized storage is certainly going to be a part of the picture when it comes to building these applications, we think we need more.
00:09:58.300 - 00:10:35.906, Speaker A: And why do we need more? Because we want to avoid this. We want to avoid this constant loading indicator because this is what kills product experiences. And to just illustrate the flow of how this buffering and this loading indicator might manifest itself is let's just say we have three people, and person one wants to play a particular video. And let's say that they're just using IPFS or they're just using some decentralized storage system. They're going to have some CID, they're going to have a hash to reference the video and that provides data integrity. That's great. But let's say person A, person one wants to watch it on their iPhone.
00:10:35.906 - 00:11:10.126, Speaker A: Person two wants to watch it on their Android, different shape, different form factor. Person three wants to watch that same video on a low bandwidth connection. And everyone in their household is also simultaneously watching Netflix, so they don't exactly have a lot of bandwidth to spare. So the basic approach here would be your device would send a request to said storage system, fetch the file back, and try to start playing it. This is probably what's going to happen. Someone is going to say that the video keeps buffering. Someone's going to say that the video takes so long to start playing.
00:11:10.126 - 00:12:16.562, Speaker A: You press play and seconds might pass by and nothing happens. And then it starts and someone might say that the video looks really squished and it just doesn't look right because the video that you're playing back wasn't created for the particular device type and the format that you're playing back on. So all of these are just examples of how that experience isn't just quite right in that scenario. And this is particularly problematic when we think about what the experiences of the future are going to look like, where if we want to be truly creative with how we can leverage media on the Internet, we want to encourage people to experiment with new ways of consuming that content. So regardless of what you think about, say, TikTok or the recent trends around recommended algorithmic video feeds, what was particularly novel was this vertical scroll autoplay mechanism. And that was one of the first times. That when people started using that in TikTok where they experienced this product, where videos just continuously played one after another short form and it was pretty seamless.
00:12:16.562 - 00:13:06.370, Speaker A: And because of the recommendation algorithms, it was also really engaging. But the only reason that format works is because you can watch those videos seamlessly, one after the other. Just imagine a scenario where you watch video one, video two, and then all of a sudden video three just halts. You're instantly removed from that product experience. You're instantly losing engagement there. And I highlight this because it's going to be challenging to create these new product experiences if you can't guarantee the experience of any individual video at the same time. So just to round things out here, I highlight all these things because why not just upload and playback MP4 files? Well, some of the things that we want here, we want support for short time to playback that can be hard to achieve with big MP4 files.
00:13:06.370 - 00:14:11.494, Speaker A: We want to support things like adaptive bitrate streaming. So this is actually a term that refers to what services like YouTube, Netflix, and all these web two platforms that we're accustomed to use. It basically means that I'm going to take a video that you upload and I'm going to create all these different versions of it so that no matter what device you're on, and no matter what internet connection speed you're on, you're going to get the best possible version of this video. You can only do that if you have some level of processing involved. The next one, I think, is pretty forward looking and interesting and actually not too far off, but we want support for things like video enhancements. So I don't know if anyone follows kind of like the latest trends in artificial intelligence, but OpenAI, a major R D lab in this space, released their whisper model recently, which is really effective at automatically transcribing the audio contents of a given video or generally a media asset. And then what you can do with that transcription is now you can also insert that as metadata into your video and now you have automatic closed captioning.
00:14:11.494 - 00:14:58.886, Speaker A: You didn't have to do anything yourself, and that just works. Once again, that's not going to work unless you do some level of processing. And lastly, if we want to support things like live streaming, unlike static file content, live streaming, it's dynamic files being constantly generated. So that's why the industry has specific protocols to support this, such as RTMP and HLS, and lower latency variants such as Llhls. And all of these things need to be supported in order to truly create a good live streaming experience. So in summary, you can just upload and playback MP4 files, but you probably want a little bit more. So I mention all this because what we've been working on at Livepeer is what we refer to as the Open Video Protocol.
00:14:58.886 - 00:15:40.460, Speaker A: And what this Open video protocol enables is tying it back to some of the things that I was just talking about. It allows anyone to generate videos with the best codec selection for a desired video experience. So going back to adaptive bitrate streaming, it allows anyone to generate the best possible quality versions of videos. So doesn't matter where you are in the world, you can get a good playback experience. Allows anyone to generate videos with the best container selection for a desired video experience without getting into the nitty gritty details. Videos are often packaged up into different containers depending on how they're going to be consumed. So this is enabled on the network by media servers that provide this service.
00:15:40.460 - 00:16:48.842, Speaker A: And lastly, it allows anyone to deliver these generated live streams and or generated on demand videos with low, latency and short time to playback for viewers. And this is all supported by media servers that participate in this protocol and in this network. So, putting this all together, we have this video infrastructure network, and this is a screenshot from our Explorer application that just gives us a snapshot of some of the relevant stats that you might care about around this network. Such as how many fees are being paid in in order to access all these services, how much actual minutes of video are flowing in in order to access these services. And that brings me to live peer Studio. So Live Peer Studio, which I think is going to be most relevant for anyone hacking this weekend, is what we refer to as your gateway into the Live Peer Protocol and the network and is also the video toolkit for web three applications that want to build these rich video experiences that I mentioned. Some of the features that Studio gives you includes end to end live streaming.
00:16:48.842 - 00:17:41.182, Speaker A: So starting from ingestion, so you have a streamer that wants to stream that video, you can ingest it, you can transcode it on the fly on the network in order to generate all those different renditions that I mentioned. And then you can also play back all those renditions seamlessly in your browser in your application. We support video on demand, also referred to as VOD, similar to live streaming. This is end to end. So you can upload a file, have it be transcoded into all these generated versions and then play it back seamlessly in your application. And then a couple other cool things that we support here that hackers might be interested include viewership metrics. Have you ever wondered how many people are actually watching the content that I put out there? How engaged are they? Can I use that information to update how I build my product experience? And lastly, video.
00:17:41.182 - 00:18:45.582, Speaker A: NFT minting. So today we've seen this explosion of activity around NFT Marketplaces and people using NFTs as an interesting monetization tool for creators. And a lot of these NFTs have been linked with interesting text assets, image assets, and we think that the next wave is going to be around video, similar to how in Web Two, we went from image only sharing to this explosion of video sharing. So what we've built are a set of SDKs and tools that allow you to easily go through the Video NFT Minting process without a lot of pre existing knowledge. So just to walk through some of the things that you'll be able to do with live streaming, with a single curl command that looks something like this on the left, you can create a stream so that you're ready to actually start live streaming. And then from there, you can use any standard broadcasting tool, maybe OBS, which is a pretty popular tool. All you need to do is enter the RTMP URL supported by Studio.
00:18:45.582 - 00:19:58.470, Speaker A: You enter a stream key that's generated for you. When you start using the API and you're off to the races, you can start streaming. We've built this dashboard into Studio, which gives you a lot of high level information about your stream. So this includes the health of your stream, what the metrics look like. It includes a preview of the actual playback of the stream so you can actually determine for yourself, is this working properly? Is this actually being presented properly to the viewers? For VOD, for VOD, we wanted to make it really easy to have you take your existing assets, regardless where they are, and have them be transcoded via Live Peer so that you can get the best possible quality of experience in as few lines of code as possible. So if you have an asset that's already, say, on decentralized storage, whether it's an Rweave gateway or an IPFS gateway, doesn't really matter. If your asset is already there, you can submit a single simple JavaScript query to one of the API endpoints for Studio, and have Studio pull that data in, transcode it, and make it available for seamless playback.
00:19:58.470 - 00:20:42.214, Speaker A: If you don't have the asset living in another storage system already, you can do direct upload as well. You can simply upload and have one of your users paste in a file, and then that will be uploaded to Studio for transcoding and playback. Viewership metrics, simple JavaScript query to an API endpoint for Studio, and given the Identifier for an asset that you've already uploaded, you'll be able to get viewership information. You'll get a sense of what engagement looks like. And lastly, video. NFT minting. So for Video NFT Minting, we've created an SDK and tooling that will simplify this process so that given an asset that you want to create a Video NFT for, it can be exported to IPFS.
00:20:42.214 - 00:21:35.318, Speaker A: It'll be uploaded and transcoded and made available for playback. With Live Peer, all of that metadata information can be packaged up into ERC, 721 compatible metadata objects. And then at the very end, once all of that is ready, we also provide a really simple template for you to try out minting that video NFT on polygon. So this is a template, right? So this is really just meant for you to get started and play around with the NFT Minting feature. Of course you're likely going to want to customize this experience for yourself and you can do that using the SDK and tools that I mentioned. But this template can help get you started just so that you can go through the flow, get accustomed to what the flow looks like. So going into this weekend, we have a bunch of features in studio that we think will be really helpful to hackers that want to embed video in their applications.
00:21:35.318 - 00:22:51.778, Speaker A: And there's a lot of hacks that we're excited about and some of the things that I'll mention here really are just guidance and for inspiration. Of course we look forward to seeing what everyone will build on their own. But just some of the things that we think would be cool for people to explore and think a little bit about include live stream experiences around NFT drops. So NFT drops today oftentimes are kind of individual experiences where an NFT drops and you might participate in the drop, you might try to get a particular NFT in a collection. What if you can actually have a live stream experience where people can participate in that drop together in a live fashion and create a more social and communal experience around that? Live streams around NFT pack openings. So I don't know if anyone here is familiar with the concept of an NFT pack. If you're not, you might be familiar with this other genre of videos or content that you might have found on YouTube or elsewhere, which is, I think it's called like unboxings, where these content creators will have mystery boxes that they'll open live or in the content that they share and then they don't know what's going to be in that box.
00:22:51.778 - 00:23:46.946, Speaker A: But it ends up being this really entertaining and interesting experience for their audience because their audience gets to watch them go through the process of opening that box. It might seem a little bit silly if you've never kind of dove into that rabbit hole of content yourself, but it is really interesting how engaged people are with those types of experience. And the reason that I mentioned that is because, well, physical unboxings make sense. You have a physical box, you'll open it up, you don't know what's inside. What's the digitally native version of that? What's the web three native version of that? And we've already seen a lot of experimentation with NFTs and some interesting concepts around being able to create an NFT and then call a smart contract function in order to generate additional NFTs that you don't know what they will be ahead of time. And once again, this is very easy to be an individual experience. But what if you can make it a social and communal experience? What if you can live stream that.
00:23:46.946 - 00:24:23.354, Speaker A: What if you can have people participate live? An extension of that would be what if you had people say Live Bid and participate in the pack opening process so that you can see a content creator that you really enjoy. You can support them by bidding on the packs that they're opening. And that's part of the experience for everyone. A couple of others to round out the list. Video messaging apps. Messaging apps that use video as the native content form factor in order to communicate with either another individual or in a group. Video recording tools.
00:24:23.354 - 00:26:05.374, Speaker A: So think something like Loom except Web Three Native, allowing you to record and own the content that you create and then monetize that directly with your audience. Something else that might be interesting is Web three video newsletter subscription apps. So we've seen this huge explosion of newsletter writers on platforms like Substac in the past year, two years, and Text is a great form for that, right? But we also know that Vloggers on platforms like YouTube or Streamers on Twitch, video is their native content form. So they might not be the most well suited to write a written newsletter, but they can create really interesting video content on a recurring basis. And what if we created Web Three Native video newsletter subscription apps for them so that they can regularly schedule content to be released to their audience and have a direct monetization relationship with their users, with their audience? And lastly, just coming back to the theme around NFTs, we've predominantly focused on static content for NFTs, but live streams are a great example of dynamic content. So what would it look like when we start embedding live streams into NFTs and having collection of live stream NFTs? And once you start exploring that, having an Explorer that can surface all of these live stream NFTs? So that what's interesting is that I can go on Twitch today and see what streams are live there, I can go to YouTube Live today and see what streams are live there, but there's not a very easy and good way to have a composition of the two and to have those two services be interoperable. But what's interesting about NFTs is that they can be a shared data layer across all these different applications.
00:26:05.374 - 00:26:50.366, Speaker A: So what if you built an Explorer that allows you to surface all these live streams across different applications, and then you could build experiences around that. So those are just some of the things that might be interesting for people to think about and hack on this weekend. But of course, everyone here is probably way more creative than I am. So I'm sure that there are much better ideas out there than what I listed. So encourage people to just poke around. So to round things out for ETH Bogota this weekend, we have a set of prizes for people that integrate the Live Your Studio API. First place $5,000 2nd place $3,000 3rd place $2,000 we have this hacker Quick start guide, so feel free to scan the QR code and that'll take you to this notion page.
00:26:50.366 - 00:27:33.420, Speaker A: That should contain all the relevant resources for you to get started poking around with Studio and also exploring some of these other hack ideas. And last but not least, join our community. So once again, scan the QR code and that will take you to a portal that will give you access to the various community channels that we use. And this is a community where we're not only technical and we're building out this protocol and network. And if that's what you're interested in, of course we welcome that. But it's also a community that's exploring this question of what are the rich media and video experiences of the future going to look like not only for Web Three, but for the Internet as a whole. And if that's interesting to you, definitely jump into the conversation there.
00:27:33.420 - 00:28:01.196, Speaker A: So that's all that I have for today. Thanks for your time. And if we have time, I can take some questions, but I'm not sure how much time we have left. Was there I don't know. Yeah, we'll start with him. We'll come back here. Yeah.
00:28:01.196 - 00:28:33.770, Speaker A: Do you have any temples now, like temple? Yeah, great question. So I think the first example that I would cite is the one that I already cited in the slides. This application called Lens Tube, which is built in the lens ecosystem. So that application is live. You can use it today. I think it's at like Lenstube XYZ. If you go to the Livepeer Studio page and scroll down a little bit, there should be a few other references to applications that are live using Livepeer today.
00:28:33.770 - 00:29:18.856, Speaker A: A lot of them, you'll see the trend is user generated content. So we have applications building user generated content platforms focused on filmmakers or Lens Tube, I already mentioned, so a lot of user generated content. So I would start there. You had a question? Yes. So the Studio LightView studio also supports recording, so you can automatically record the live streams so that they're available afterwards. And I believe you can choose the storage location of your choice. So if you want to use centralized storage, go for it.
00:29:18.856 - 00:30:15.272, Speaker A: If you want to use something more decentralized, you can do that as well. But that is supported in the service. Yeah, can I store sorry, could you repeat the question? For example? Okay, custom link. So I believe that short answer, yes, I believe that you can do that today already, but it might require a little bit additional development work on your end. Something that actually one of the team members that is here this weekend is playing around with is making that easier. So you can easily create, say, like subdomain URLs. So let's say that you have a domain that you want the playback URL to be hosted at.
00:30:15.272 - 00:30:38.380, Speaker A: Right. Now, you can do that, but you have to do more work yourself. And I think there's a couple of things that we can do to make that easier. If you have a particular use case that's important for, definitely come find us. Talk to me or someone on the team, because we'd love to hear what you need for that. Yeah, you all right, well, if that's it, thanks so much for your time, and happy hacking.
