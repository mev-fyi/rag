00:00:02.650 - 00:01:01.098, Speaker A: Well, I suppose we could start in alphabetical order. We are working on a system for distributed key generation and threshold decryption called Fervio which is a collaboration with Sika. So Dave could probably speak as to any parts that I miss or perhaps parts that I don't even know about yet. One it sounds like you've already covered as you referenced in the slide threshold decryption. I'll just note that the way we integrate closely into the consensus algorithm in this case tendermint although the approach should generalize to any sort of multi phase BFT, consensus allows us to more or less avoid the one block delay. There's still latency insofar as the validator set, which is the same as this set, which has to provide threshold decryption shares in our case has to compute the shares and combine them. But because we can just put the threshold decryption shares for each transaction in the pre commits and tendermint.
00:01:01.098 - 00:01:37.210, Speaker A: So in the last phase of the BFT consensus round, we can execute the transactions in the same block decrypt and execute the transactions in the same block even though the proposer who has to propose before the pre vote? Round, which happens before the pre commit round has to commit to an ordering of encrypted transactions by the time we get to pre commits. If a block is finalized, because we have the same set for two thirds of the quorum requirement to finalize block and two thirds of the share requirement to decrypt transactions. Those two coincide and so we can decrypt and execute the transactions in the same block in which they were committed.
00:01:40.350 - 00:02:38.330, Speaker B: Maybe I can take the next one. Hi everyone, this is Sterli, I'm the co founder of Automata Network. So for those of you who heard about us for the first time, we are building privacy middlewares for DApps across multiple chains using SGX and Oram. So our team has several researchers who has been doing SGX research since 2014 and some of us were previously working at a security research lab in NUS National University of Singapore. And just to let you know, before autonomous network, this research lab already had a few successful sipping of crypto projects such as Kyber Network and Zelika. So under this context of Mev, we are actually developing decentralized privacy middleware called conveyor to help dexers to minimize mev. And also if you happen to know that we actually also released a small free tool called Mev tags for people to inspect if they have been sandwich attacked previously.
00:02:38.330 - 00:03:49.546, Speaker B: So our approach is we are using trusted hardware te or Intel SCX, whatever you think the term is, to try to just hype the transaction temporarily and determine the transaction orderings before it's published and to preserve the maximum compatibility with existing blockchains, it will review the transaction only after the ordering is locked, decided and locked. So that means layer one, block producer, and also L2. sequentials will just process it as normal transactions, but they don't have the ability to change the order anymore. So this would allow us to directly work with Dexis on many chains and to protect existing users with a non intrusive integration with them and maybe a little more details about it. So the ordering we are providing here is actually particular to it's, very application specific. So we can even just provide a lot order only for a particular trading pair and we will have multiple trading pairs running in powerlife. So it's kind of just a scalable way for us to support DEXs.
00:03:49.546 - 00:05:02.570, Speaker B: And the other technique we are using is called Oblivious Wan which kind of directly enhances the privacy of trusted hardware and it also raises the bar of breaking dee through side channel attacks. So we know trusted hardware can provide these safe rooms or enclaves but it actually doesn't solve all issues for example, the traces left by this enclave when interacting with the outside world could leak information even your data is encrypted. Before leaving this enclave, the access pattern that it reviews could be collected and learned by malicious node operators and then used to infer the computation. Inside your enclave. There have been papers that successfully apply these sidechain attacks and also extracted private keys from Tee, so it's pretty dangerous in some sense. And this kind of attack involves many runs of profiling, probing and analyzing. And the sensitive information is actually reconstructed, sometimes bit by bit, based on the access pattern that were exposed.
00:05:02.570 - 00:05:49.450, Speaker B: Maybe I can give an example of this. So if something looks like a duck and swims like a duck and quacks like a duck, then it probably is a duck. So if some observations of the access pattern could be associated with certain computation with very high probability, then the observer would know what you are doing. Even you are using traffic hardware. So Oram is trying to change the access pattern so that even the traces are observed, it will be useless because the access pattern will not make sense anymore to those observers. And this line of research actually has been there for several decades. And the idea is just to providing just to kind of shuffle the access pattern, just doing more useless work.
00:05:49.450 - 00:06:07.090, Speaker B: But it ended up just doesn't allow the observer to get any useful information from that. That's kind of the technologies we are using to try to safeguard the privacy of the transaction orderings.
00:06:09.750 - 00:06:12.340, Speaker C: Okay, cool, who's next?
00:06:12.790 - 00:07:22.218, Speaker D: I think I can go next. This is John from Secret Network. Secret network is a Cosmos SDK based layer one chain where every validator runs an SGX and as a result, the privacy as every validator runs an SGX. What happens is we can ensure that all contracts on the network are privacy preserving contracts. And the way this works is there's one private key that's shared across all the validators that's generated within the SGX or the trusted execution environment. And when a user is interacting with the network, they create a symmetric key by using their private key and the network key and encrypted inputs are stored in the mempool. That's why no one can really see what's going on as to what the inputs are.
00:07:22.218 - 00:08:17.980, Speaker D: And then validators take those encrypted inputs, they can decrypt them inside their enclaves because they have the network key. The computation takes place inside each validator's enclave and then validators share hashes of the computation results on chain and form consensus based on those hashes. And assuming there's consensus, the state updates the state is also always encrypted and stored on chain. So that's how secret network deals with front running attacks. And we've had secret contracts live on our main net since September of 2020 and we have a functioning AMM that is front running resistant since February of 2021.
00:08:22.850 - 00:08:23.486, Speaker E: Okay, cool.
00:08:23.508 - 00:08:36.130, Speaker C: I guess throughout the panel you'll probably have the time to go into details about how you default from automata number. So let's see who's next. Dave.
00:08:36.550 - 00:08:37.300, Speaker A: Yeah.
00:08:37.670 - 00:09:26.414, Speaker F: Hello. I'm Dave. So I work on project called Osmosis. And one of the things we're doing to front running, as Chris mentioned, it's like working very closely with them is to use special kind of chris kind of summarized this earlier. So I think what I'll talk about instead as a brief overview is how we want to handle fair ordering, which I think is also an important question. So with Threshold decryption on its own, a proposer can still try to get their transaction as the first transaction or the last one. So on top of that, we kind of imagine our take on fair ordering is not the same as kind of the Wendy or equidastic it's instead, which is like this kind of time, this take on when something arrives in imagined global mempool across all nodes.
00:09:26.414 - 00:10:40.840, Speaker F: And instead we're trying to take that everything in a block should ideally be treated as though it came at the same time. So the way you can imagine this for something like trades is then you could imagine you had a random permutation from the trades. You take the average across the random permutation for how your trade would have been executed in like sort of a batch environment. And then for things where you can't really white box like that, where it's not clear, like transaction sends for instance, then you take a random permutation. So the idea is then summarize for orderings and take for things you can whitebox, like analyze, do some white box analysis as though everything came at the same exact time, or you have random permutation of them all, which applies for trades on uniswap, for instance. And then for things you can't take a random permutation based off of decrypted content and then do some other techniques to ensure that there's some decrypted content, there's some transaction content in there that the validator who's proposing the block doesn't control that's I guess we do for fair ordering on top of special decryption things.
00:10:42.730 - 00:10:49.450, Speaker C: Interesting. Yeah. I guess I have more time to go into details. Who's next? Barry.
00:10:51.650 - 00:10:52.734, Speaker E: Can you hear me?
00:10:52.852 - 00:10:53.520, Speaker C: Yeah.
00:10:54.130 - 00:11:38.310, Speaker E: Hey everybody. Yeah. So my take on mev prevention is that there's two kind of approaches. The first approach is where you limit the amount of the group who are able to take advantage of minor extractable value. And that's where you see solutions like encryption and threshold decryption. And the second approach is where you make it kind of impossible for you kind of randomly order transactions based upon you just randomly order them so that even if someone is kind of front running, you're not able to get as much value out of it. But there is a certain class of mev that we're not really able to prevent with either mechanism.
00:11:38.310 - 00:12:10.520, Speaker E: Like for example, if someone is getting their collateral slashed, there's going to be mev there. And even if you order the transactions randomly, someone is going to get that. And this will actually produce this kind of weird incentive game where people are creating thousands and thousands of transactions because they just want to be the first one ordered by the block. So there's these weird kind of incentives that yeah, so that's my take and maybe there's some interesting things to discuss about that.
00:12:12.250 - 00:12:14.662, Speaker C: Okay, cool. So Christopher went.
00:12:14.716 - 00:12:19.074, Speaker G: Okay, so, Yannick, I guess I'm the last one. Can you hear me?
00:12:19.212 - 00:12:19.802, Speaker F: Yeah.
00:12:19.936 - 00:13:02.630, Speaker G: Cool. So, yeah, I'm from Shutter and we are one of these projects that use threshold encryption to prevent front running. It's very similar to, for example, what Sika does. I think the general principle is very similar. The main difference is that we don't see this as a new layer one blockchain, but we try to interpret this as a more general technique that can use and apply at different layers. And the first layer that we applied it on is very close to the application layer. So we implemented this as a kind of a smart contract system on top of Ethereum so that users can send encrypted transactions to Ethereum wrapped, of course, in a normal Ethereum transaction.
00:13:02.630 - 00:13:21.420, Speaker G: And then those would be at some point decrypted and passed on to another smart contract, which would then be front running, protected basically as long as they only accept transactions through this mechanism. Yeah, that's the main idea of Shutter at the moment.
00:13:22.350 - 00:14:08.700, Speaker C: Okay, yeah, I think that is everyone, I guess from the introductions, everybody's working on different solutions and they're meant to be deployed in different ways. So some are working on layer one solutions and then others are working on drop in solutions for developers as sort of a plugin. And so the main point of this question is sort of what are the pros and cons of using your respective technique at these different layers, at layer one or a L2, an application layer, however you'd like to answer that question. Yeah, I'll leave it for you guys. To choose your order.
00:14:10.990 - 00:15:48.970, Speaker B: Maybe. Can I start first? Our solution is actually the pros about solution is fully compatible with any kinds of layer one and L2 because we are really just trying to provide this solution as a plugin to the existing system so that we can help the users and also the DEXes. And most important thing is we are not modifying any layer one, L2 protocols and also at the application layer we are able to offer kind of different variants of implementations that balances between easier integration, better gas efficiency or even higher fault tolerance. For example, if you just need a very quick patch to get rid of mev so you could just build off a single trusted relayer that does a very simple FIFO ordering that will just kind of solve that, but for sure that introduces single point of failure. So maybe a more decentralized relayer network that runs a little bit consensus algorithm to ensure the ordering is better. And in terms of the integration, actually since we are directly working with Dexis at the application layer so they could just adopt this approach very easily as it only needs to kind of acknowledge the ordering coming out from the trusted relayers. And also this kind of meta transaction approach also brings another quite interesting design point where we can actually make the transaction gasless and user won't need to pay in the native tokens for the gas fees.
00:15:48.970 - 00:16:57.570, Speaker B: And also since we have this great flexibility, our design also make it possible to work with other mev solutions as well. For example, we can work with flashboard in order to just ensure the order transactions are delivered without being kind of censorship by the miners talking about the cons. I think it's really about when you build this kind of relayer network it really takes time to fully decentralize your entire system. But actually since we are using trusted hardware, each individual node in the system has a slightly different trust model than just a random node that hosted by anyone. So we sort of have a slightly better confidence on this te node. So in that sense, even we're starting with a smaller number of nodes, the system overall is also secure. Yeah, I guess that's my take on this.
00:17:00.100 - 00:17:49.810, Speaker D: I guess I can go next. For us. When you have smart contract privacy on layer one, the things that you can address with that is much more than mev. You could have NFDS where the content is encrypted and you could get into more subscription and content access or content monetization like use cases in the Web three, which to me is very exciting. And also building things are easier. But then the problem is then you have to build a new ecosystem and have new people build applications on this new layer one rather than plugging into another network. To me, that's the big trade off.
00:17:49.810 - 00:19:09.560, Speaker D: And I think one thing that we are doing more and more is to explore some sort of an operator model whereas in order to target someone who's on ethereum and just like a simple use case say want to do a mix. One thing that we are working towards is how to allow the user to do that just with a MetaMask transaction rather than getting into the whole cosmos ecosystem. Wallets and tooling. And I believe that will be something that will allow us to target more people where we can have the user get a key from the secret network through the help of an operator and then when they're making their Ethereum deposit transaction they can also create an address where they want to get their funds back. That address can be decrypted in secret network. I guess that's similar to what you have been mentioning Delhi and then sent back to Ethereum to make a new transaction. But that may not be as exciting in the mev index cases because it does add some latency.
00:19:12.700 - 00:20:16.328, Speaker A: We're also building anoma is also building a layer one and I would agree with the primary disadvantage of simply adoption from zero. We're building an ecosystem up from scratch. Although I think some of that can be mitigated with good interoperability protocols. The other point I would note specifically with relation to MAV is that it seems to me like for some of the protocols involved which use randomization or which use threshold decryption in ways which are where there are like a bunch of transactions being grouped together and somehow combined, as Dave was mentioning, or being randomly reordered. There's a little bit of a sort of network effect or like, the larger the set is, the better the guarantees or the better the kind of unpredictability individual transactions have. And it seems to me that's easier to achieve in practice when you can make it the default in a system as we can much more easily do at a layer one than perhaps in kind of an add on. Solution just because the whole system can be architected to integrate it as like the default pipeline for submitting transactions.
00:20:16.328 - 00:20:44.810, Speaker A: And we can change generic concerns of surrounds like I mentioned. So it's very easy to achieve a kind of holistic integration such that end users can really use threshold decryption without knowing it, and such that it's so easy that even transactions which don't need it and probably aren't going to want to pay additional costs, either in gas fees or in user experience to get it, can get it for free and thus contribute to the public good of more transactions being randomly reordered or whatever.
00:20:48.220 - 00:21:42.788, Speaker F: I can go next, I think Chris summarizes pretty well. I do want to say on the point of mev resistance tactics, I don't think there's actually a meaningful difference in pros and cons between L1 and L two. Really what you're doing is you control your own chain, your own consensus and L two or really roll ups really just means that you have some data that needs to be posted to ETH or whatever base chain and some priority for how getting transactions from ETH onto your chain. That's not really like an mev concern. That's like a fund transfer thing, which didn't really have Toache mev, that really had like latency and sensor resistance concerns. So I think the key questions to ask is just your own chain, like L1, l two is one category versus the app layer. And then for something like Threshold Decryption, the guarantees actually differ a lot here, actually really for all the mempool privacy techniques except SGX.
00:21:42.788 - 00:22:58.560, Speaker F: So I guess mempool of Encryption and Threshold Encryption, sorry, Time Block Encryption and Threshold Encryption, the guarantees differ a decent amount because you're doing in the App layer. To get these strong guarantees, you kind of have to make your own subset of the chain that's only taking in transactions from this encrypted layer. Otherwise you get these issues where since the decryptors can never guarantee their transactions going to get like when it's going to get included, they could broadcast the decryption. And then if there was a path for someone who's not using decryptions to get their transaction in, first they'd do is they'd see the decryption being broadcasted over the PP layer and then they'd front run that before the decryption gets posted on chain, then defeating the point. When you're in the app layer, it's actually pretty hard. You have to kind of make your own subsystem, which is not going to be interoperable with the rest of the ecosystem. So I think that's a key difficulty you have with really these member privacy techniques when you're on Apple layer versus your own chain.
00:22:59.300 - 00:23:40.800, Speaker E: Yeah, sorry to interrupt you dev yeah, I completely agree with what you're saying. That was the point that I was trying to make and maybe to drive the point home just to drag it out a little bit more. So the example here is that, okay, if you take uniswap and you say we want to make a mev resistant version of uniswap, there's always going to be a place where the mev resistant version goes to the smart contract to tell it. These are the orders to place. And at that point you're able to front run. And this is the concern that you end up just building everything in a silo where you have like front run resistant uniswap, and then you have uniswap and then you have front run resistant compound, and then you have compound. They can't really be interoperable between each other and this is a big concern.
00:23:44.340 - 00:24:49.172, Speaker G: I guess I definitely agree with these concerns. That's also the main issue that we see, that this application layer, front running protection breaks composability in some sense the reason why we still started there is that it's just much easier to implement and you can just focus on this particular problem of front running protection and you don't have to worry about all the other stuff. I mean, building a new blockchain is very hard, building a new L2 is very hard. It takes a long time. So this is one reason why I think it makes sense to start there. And the other reason is, I think it has already been mentioned that the problem exists at the moment on layer one for for many applications and it doesn't really help them if there's an Mev or front ring resistant layer One or layer Two somewhere else. If the application is on that layer that is not protected against this.
00:24:49.172 - 00:24:54.520, Speaker G: So that's the two reasons I see for pro of application layer.
00:24:57.340 - 00:25:41.240, Speaker C: I guess to build upon this is depending on the application, you might be willing to take some amount of, I guess latency. So I guess asking you guys for your respective techniques, do you think some categories of applications are better suited for your technique or is there another technique that's probably better? Or maybe you think isolation is good for all techniques sorry, for all applications. Any thoughts?
00:25:43.500 - 00:27:03.410, Speaker F: I think a lot of difference is based on the security assumption you're willing to take. I've been working on tendermint things for years now where in tendermint this proof stake algorithm where you say I trust that over that two thirds of this validator set is honest, but then I can detect their faults and then if they committed some fault, slash them. And if you're willing to take that assumption, then I think the threshold decryption security works for all the kind of really all applications I can think of which are kind of siloed to one chain, then you do have some cross chain concerns, but these are really inherent to all of the techniques. And so I think the reason I think SGX comes interesting for when you want more than just MEMP level encryption, but you then take the cost of what happens if SGX breaks time lock is I think interesting if you're willing to make hardware assumptions, but not these multi party assumptions that you typically do proof of stake. So I think it's really around the security assumption for timelock versus threshold and then for SGX it's like well, okay, we're taking the lower security because it's cost break on SGX and we're instead pivoting to like okay, what else can we do on top?
00:27:06.660 - 00:27:41.336, Speaker E: I think my argument on that point is that it's a different trust assumption if you're trusting the majority of the chain to not do something that can potentially get them slashed. It's very different from trusting the majority of a chain to do something that they can never be slashed for. You can never prove that they did this. So there's no punishment for the validators if they kind of front run people. There's no way to even know that. Well, you can kind of know, but not really, but yeah, that's my sorry, definitely agreed.
00:27:41.368 - 00:27:53.360, Speaker F: I spent a while trying to think about this. To what extent is this slashable? And I feel like the best way to get slashing abilities is to have the decryptor shares or the decryptors.
00:27:55.380 - 00:27:55.744, Speaker C: Do.
00:27:55.782 - 00:28:25.470, Speaker F: The decryption work in an SGX, so that way they're still forced to be behaving the protocol correctly. But everyone only needs a cryptographic assumption to know that their protocol was done correctly, but then as a defense in depth. So it's like, okay, well now you need to break trust enclave and break cryptography and break this two thirds assumption. Yeah, agreed. This is definitely hard.
00:28:26.800 - 00:28:34.670, Speaker D: There's no solution to that problem. Yeah, if there's an offline collusion attack, there's no way you can know it.
00:28:35.120 - 00:29:07.690, Speaker F: Well, what you need is you want to be able to have a defector report. So in proof of stake, the reason we can do double sign protection is that a light client at the end of the day needs to get the double sign a different signature than the rest of the world. And this can be reported. So if I'm running the problem is that only the Validators actually got these decryption shares. So to get a similar guarantee, you'd need that any member of the Validator set can defect and prove that, hey, look, this cabal tried to decrypt something when they shouldn't have.
00:29:12.060 - 00:29:13.950, Speaker E: Okay, I understand. Yeah.
00:29:14.320 - 00:29:17.580, Speaker F: I don't know how to actually do this without more assumptions.
00:29:18.880 - 00:29:22.990, Speaker E: Interesting point, but I don't want to take all the time. But let's discuss another time.
00:29:24.400 - 00:30:08.700, Speaker C: I think there's about five minutes left if you want to answer this question. Or we can quickly go on to the next question. We can quickly go over to the next question. So yes, the last question is we're all dealing with either hardware assumptions like SGX failing, or trust assumptions such as some quorum of nodes being honest or at least incentivized. So how do you guys handle failure scenarios for your technique? How does it affect the user? How can they recover?
00:30:11.760 - 00:31:45.690, Speaker G: So as already has been mentioned, it's very hard to prevent these key shareholders to collude with each other. And therefore I think it's very important to make sure that they're well incentivized to not do this, mainly by having a lot of them. Because if there's a lot of shareholders, it's much harder to collude and it's easier for a single person to defect and notify the world about this. And the other way I think this should happen is that they are selected in a good way, so not just randomly and not just the spots shouldn't be sold to the highest bidder. Or that's at least what we're trying not to do, but instead select them from different organizations, from different sets of people to make sure that they're not all not a single person. And lastly, to give them very long term incentives so that they have a financial incentive for keeping the system honest and keeping users trusting the system kind of and to do that, we probably want to have a kind of dow that does the selection process because it's very subjective and you can't really write a cryptographic or economic protocol to do this.
00:31:46.860 - 00:32:23.450, Speaker A: Well, I would call with that a little bit. I mean I think in the case of proof of stake systems you have already a naturally incentive aligned group of participants in the validator set who are because they're staked aligned in some sense with the long term success of the protocol and who potentially have sort of out of band or chain external reputation on the line. Plus they have the ability if they are able to coordinate and collude in large enough groups to commit more serious faults potentially than extracting mev. So it seems to me like that's a case where there's a group that already exists and you wouldn't need a separate selection process.
00:32:25.740 - 00:33:27.950, Speaker E: I think on that point I kind of think about the kind of validator set as this economic group and this is a group that are selected based upon whoever is able to make the most profit from what they're doing. And I see exploiting this kind of minor extractable value opportunities is just like another potential revenue stream for them. Like you've seen this on ethereum when the ethereum miners started on mass to use flashpods or front running methods to increase their rewards. And I'm also super worried about the stakers on ethereum doing this. When we move to proof of stake it seems like just such a natural thing for them to do. I agree that having these mechanisms in place, some of these mechanisms will reduce the amount of money they're able to extract but I feel like they will get more and more sophisticated because that's just how they make their most money. That feels like the natural place for them to do to be.
00:33:29.760 - 00:34:02.980, Speaker D: For us. Because of the hardware guarantees this is assuming the validate I mean side channel attacks are going to be expensive because you have to pay gas and then actual physical way to hack the tes is assuming most of these run on data centers is not very, I think, practical. So that's how we think about it. But worse comes to worst you end up with exactly in a normal blockchain kind of scenario.
00:34:05.560 - 00:34:20.440, Speaker C: Okay, cool. So I think that ends this panel. I'd like to thank everybody for taking the time to answer these questions and for going through the trade offs on your respective techniques. I guess Tina can take the floor.
