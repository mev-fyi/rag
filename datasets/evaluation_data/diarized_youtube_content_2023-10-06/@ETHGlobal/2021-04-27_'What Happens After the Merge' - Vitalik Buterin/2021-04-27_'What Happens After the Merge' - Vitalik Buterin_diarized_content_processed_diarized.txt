00:00:00.650 - 00:00:29.602, Speaker A: Thank you so much. Thank you for having us. With that, we are ready for our final talk of the day. I am super excited to be welcoming our next speaker. And in anticipation of this last talk, we've seen a really big surge in the number of live audience viewers. So to anybody who's joining in, just want to remind everybody that this is a live chat and a live conversation. So if you have any questions for our next speaker, you can ask them out in the chat and we'll relate them to Vitalik after the talk finishes.
00:00:29.602 - 00:00:57.866, Speaker A: So if you're tuning in right now, be sure to log in, say hi and ask any questions and we'll be able to relay those questions directly to our speakers. So that said, I am super excited to welcome our next speaker. The next speaker is Vitalik, Buterin Vitalik is going to be talking about what happens after the merge. We've been on a long road from e one to e two, but let's talk about what's next. Welcome italic.
00:00:58.058 - 00:01:01.518, Speaker B: Thank you. Cardik. Are you hearing me okay?
00:01:01.684 - 00:01:03.018, Speaker A: Everything is great.
00:01:03.204 - 00:01:27.742, Speaker B: Okay, great. Then I am going to just turn on my slides and we'll get straight into talking about fun stuff. Okay. Share. Okay, what's the second word happens. Okay. And what's the second word now we've excellent.
00:01:27.742 - 00:02:19.714, Speaker B: Okay, thank you very much for doing the check verdict. All right. So I guess I'll just start talking. So far we've had a lot of excellent presentations about the process of the merge, about all sorts of things kind of having to do with the merge around the merge. Right now, I wanted to kind of finish off by going a bit into the far future. So where is Ethereum going from a technical perspective after the merge? So this is a kind of quick diagram of the roadmap that we know so far. Right? So we have the execution layer.
00:02:19.714 - 00:02:55.374, Speaker B: Currently it's a proof of work chain, but the proof of work part is going to be removed at some point fairly soon. And then we also have the proof of stake inside this layer, which also exists. As of December, the Berlin hard fork already happened. The genesis of the proof of stake side already happened. So next up we have the Altair hard fork on the proof of stake side and the London hard fork on the execution side. That's going to introduce our beloved 1559. And then after that there's a possibility of a feature fork, maybe a bit with like one EIP either before the merge or after the merge.
00:02:55.374 - 00:04:29.134, Speaker B: The Core Dev skulls are still discussing this, but much smaller than say, 1559, for example. But the next big thing is just the merge itself, right? And the merge is this kind of big topic. Then there is the question of, well, what happens after the merge? Right? So after the merge, basically the execution layer is no longer an independent chain. The execution layer is a thing that lives inside of the chain and the chain is run by the proof of stake consensus layer. So once that's done and proof of work is behind us, where do we go from an ethereum consensus layer point of view? From there, the first big thing that we'll need to have is a post merge cleanup fork, right? So basically the idea here is that we've rearranged the roadmap so that the merge is done in this very minimalistic and simple way where you basically just kind of take the existing proof of work chain, strip out the proof of work and then just make it live as a chain inside a chain inside of the proof of stake chain. But it's like a very no frills merge, right? So like, for example, the ETH One voting data mechanism continues to exist even though the chain is basically just voting on the chain, which is completely stupid because, well, the chain knows the history of the chain, right? Withdrawals are also not enabled. There's also this kind of awkward mix of RLP and SSD.
00:04:29.134 - 00:05:41.350, Speaker B: There's a lot of kind of format issues there's theoretically the ability to read the execution chain, but you don't have any opcodes that can actually take advantage of that. So the post merge cleanup fork is this fork that's expected to happen very soon after the merge that just fixes all of those issues, right? So some kind of top priority items that could happen around here. One of them is to remove the ETH One data voting mechanism. So the ETH One data voting mechanism is how currently the beacon chain is aware of the proof of work chain and it has to be aware of the proof of work chain to process deposits. But we're not going to need this because the execution layer will just be able to read the consensus layer that it's living inside or the consensus layer will be able to read the execution layer that's living inside it directly, right? So that simplifies the protocol a bit and it removes an honest majority assumption, which is good, adding withdrawals. So withdrawals will not be available immediately after the merge, but they will be available after the cleanup fork. And that's just like a simple mechanism.
00:05:41.350 - 00:05:57.390, Speaker B: Just need to figure out exactly the way to process withdrawals. But not very technically hard. Potentially fully moving. Actually. This should not be the application chain. That's outdated terminology. It's not politically correct.
00:05:57.390 - 00:06:22.722, Speaker B: As of April. Sorry, guys. Fully move the execution chain to SSD. So RLP and SSD are these serialization formats. The existing ethereum execution chain runs on RLP. The beacon chain runs on SSZ. And SSZ is like in my opinion, as the inventor of RLP, RLP is kind of trash and SSD is better, so it's just more convenient.
00:06:22.722 - 00:07:41.562, Speaker B: It has these really nice properties in that you can prove merkel paths going into SSZ objects that are inside of SSD objects very easily. It doesn't require any kind of weird dynamic positioning for most things. So it has these nice properties. And the way that the merge will work is basically that the execution block that is embedded inside of a beacon block is going to have transactions and those transactions will just be Blobs, right? And they'll still be RLP. Now you can't change them immediately because the format of the transaction really matters because the thing that is signed in the transaction signature is a serialization of the transaction. And so if you change the serialization, unless you add some awkward backwards compatibility layer that sort of reformats the transaction when you're checking the signature, you just want to have a new transaction format and that's something that needs to be added later. Right? And this would be a good opportunity to kind of clean up transaction formats a bit, add something that supports access lists, supports the chain ID in a very kind of nice and native way, supports EIP 2929 stuff, potentially.
00:07:41.562 - 00:08:28.922, Speaker B: Supports account abstraction, related stuff. Just supports everything that we know transaction formats need to contain. And eventually, all of the previous transaction formats can be retired. Beacon State Access opcodes. So opcodes to access the state of the Beacon chain is also something that's important. So they're basically things like the Randow opcode or potentially remapping an existing opcode to Randow so you can access the Randow value inside of the chain and that can be used by applications for on chain randomness. Also the beacon block root opcode.
00:08:28.922 - 00:09:36.606, Speaker B: So a block hash opcode that actually points to a beacon block. And this is nice because it allows the chain to be aware or it allows things in the execution layer and applications to be aware of what's happening in the consensus layer. It also allows them to be aware of history. You can prove history more easily. And it's important for when we add Sharding because you have to prove that some Shard data blob actually got included. And then the last thing is this kind of somewhat controversial thing of agree that we need to agree that at some point, hopefully clients just stop trying to download the pre merge proof of work chain, right? So at some point I think it's a good idea to break the invariance that the base ethereum protocol is responsible for trying to or for giving you the entire history all the way back to Genesis. And the first simple step to that is to just at some point agree that we stop kind of providing the proof of work chain before the merge as part of the protocol.
00:09:36.606 - 00:10:35.330, Speaker B: And if you want that stuff, then you can go to the graph or you can go to some other protocol that someone comes up with and it'll always be possible to access history, but it would just kind of stop being the responsibility of the ETH protocol that's mandatory for consensus nodes. No. Apologize again for calling it the application chain. In my writing, it's called the execution chain now. So that's the first big thing, right? It's not very kind of featureful, not very sexy, but cleanup has to be done and it is sort of the technical debt that has to be paid once this accelerated merge is finished. And the reward is that the merge can come half a year earlier and the ecosystem can save potentially many billions of dollars in resources from just moving over to proof of stake more quickly. So after that sharding.
00:10:35.330 - 00:11:41.078, Speaker B: Right? So for those of us who have been around in the ethereum ecosystem a bit longer, you'll probably remember that the original plan was actually to do sharding before the merge, but now, because we're prioritizing the merge, we want to do sharding after the merge. Right? Of course it'll be developed in parallel, but it'll likely actually be implemented and turned on at some point after the merge is done, just so we have a bit of separation and we don't want to do all of the potentially dangerous things at the exact same time. You want to do first one and then the other so that developers can kind of pay attention and focus. So sharding, as we've been talking about for a year, starts off as being just data sharding. So no execution on shards. Instead the shards are blobs of data. And the reason we care about having blobs of data is so that those blobs of data can be used by roll ups.
00:11:41.078 - 00:12:42.030, Speaker B: Right? So 64 data shards on each shard targeting, I think an average of, I forget if this is an average or the max, but an average of either 256 or 512 kilobytes in a shard block per 12 seconds. And then the security of the shard blocks will start off being committee based. So just a randomly selected committee will vote on it and if it signs off, it's accepted. But then over time we'll add higher levels of security with things like proof of custody and through this big thing called data availability sampling. And at some point we also want to or potentially even immediately have staggered shard blocks. So you don't want the block of every shard to come at the same time during a salat. You want say, shards one to ten to come early in a slot, shards eleven to 20 to come later in a slot, and then shards like 60 to 63 to come near the very end of a swad and have some way of some incentive mechanism to ensure this actually happens.
00:12:42.030 - 00:14:21.294, Speaker B: And so roll ups that are willing to kind of walk between multiple shards will basically be able to just keep on using these new roll up blocks immediately as soon as they come. And they'll be able to have the lock times that are much faster than the twelve second kind of tick time of the beacon chain itself. Right? So this is kind of currently the most promising potential strategy for basically making the ETH Two system and roll ups on top of ETH Two kind of have initial confirmation times that are competitive with more centralized chains, basically without actually incurring the risks of being centralized. Data availability sampling. So this is this really important security technology for Sharding, where basically the goal is to have clients verify data availability guarantees in this probabilistic way. And the goal of this is to allow clients to detect data availability failures or reject Shard blocks that are not available, even if the honest majority assumption for committees is broken, right? So this sort of preserves the property that we're used to with Ethereum as it is today and with Bitcoin as it is today, that basically says even if there is a 51% attacker, they can revert the chain, but they can't force you to accept invalid or unavailable junk. And in a shorted system we want to maintain this invariant, but obviously without requiring every client to personally download and check everything.
00:14:21.294 - 00:15:07.018, Speaker B: And data availability sampling is this lovely and wonderful technology for actually doing that. So every block gets redundantly encoded with polynomial commitments. And the idea is that if you have any 50% of the block, so the block, let's say, gets split into 1000 pieces. If you have any 500 of those pieces, you can recover those other 500, right? Because let's say, kind of degree 500 polynomials. So if you evaluate it at some points, you can extract the polynomial, then you evaluate it at all the other points. If he wants to know more of the fancy math like this has been written about in, I think even the post on Das on notes Ethereum.org that actually contains this picture.
00:15:07.018 - 00:16:51.658, Speaker B: And then once you have this mechanism where you only need half a block to be available for the entire block to be available, you can randomly sample to check that enough of the block probably actually is there. So a lot of work has gone into this, a lot of thinking has already gone into some kind of starting some prototypes, coming up with optimized algorithms for polynomial commitments, thinking through the Shard or the peer to peer network design. But it is still lower priority than just the merge and Sharding because we do recognize the kind of urgent need for security or sorry, for scalability, and the need to just get users, get some kind of scalability in the hands of users first, and then kind of push up the robustness of the system second. Now, the system still is fairly robust even without data availability sampling, right? It just relies on an honest majority assumption and data availability sampling basically removes the honest majority assumption, at least for Sharded data, and then more security improvements, right? So these are things that Justin really loves. Single secret leader election. So basically ensuring that the proposers of upcoming beacon blocks and possibly Shard blocks are not publicly visible anymore and that mitigates Dos issues and mitigates collusion risks. It basically kind of replicates many of the benefits of kind of the way proof of work works, where block publication is more anonymous in a proof of stake context VDFS verifiable delay functions.
00:16:51.658 - 00:17:32.398, Speaker B: So these once again, Justin has talked about these a lot. Basically verifiable very secure randomness for choosing committees. So committees become more difficult to attack, which in practice means that the security of the system goes up. Or you can rely on the committee. Before VDFS, if say, more than 70% of nodes are honest. But with a VDF, that requirement might go down to, I don't know, somewhere around like 57%, maybe 60%. And then proof of custody is basically just a further mechanic that forces nodes to actually download, keep and validate block data.
00:17:32.398 - 00:18:29.674, Speaker B: And it's basically a kind of anti centralization measure. So those are some kind of short term consensus layer focused things. Well, I shouldn't say short term because this is after the merge, but some medium term consensus layer focused things that I expect that will be the major focus after the merge. And these are all ideas that we've been working on for quite some time, right? So for a lot of them, there's basically a complete kind of proto spec that's just been sitting around and at some point he just needs to turn the proto spec into a false spec and then turn it into an implementation executional layer improvements. So the execution layer does have some needs to continue evolving. So one is address extension. So this is a small thing, but it's probably important for the long term.
00:18:29.674 - 00:19:16.730, Speaker B: Basically increase the length of addresses from 20 bytes to 32 bytes. And this is an important security improvement. 20 bytes is in the long term not safe for collision resistance. And this gives 26 bytes for collision resistance, which actually is safe. Add some version numbers for future compatibility and it's needed for state expiry proposals, which is the next thing here. So basically stateless and state expiry, this is a really important thing for the ethereum execution layer post merge. So right now the main thing that is making further increases to the gas limit not safe is actually not even things to do with execution.
00:19:16.730 - 00:20:15.598, Speaker B: It is state size, right? The state size is growing by something like 30GB a year potentially. And then with the recent gas limit increase, it'll probably go up to about 35. And then if there's more gas limit increases, it'll go up even more. And there's this just permanent ever growing state. And this is really inconvenient because it means that just syncing to the network for the first time is something that would just permanently become more and more inconvenient. It also means that the hard disk space requirements of a full node just keep going up and up. And it also actually kind of interacts with the Dos issues because the larger the state size is and the more amount of data he needs to have on database, the more time each individual database read will take and so the more vulnerable the chain becomes to denial of service attacks.
00:20:15.598 - 00:21:12.862, Speaker B: So state size kind of management is this really important issue and I've talked about and really the Ethereum community in general has talked about two big categories of strategy to address this. One of them is statelessness and the other is state expiry. So the idea behind statelessness is to basically say well, we're going to create two classes of nodes. One class of nodes continues to require the state and the other class of nodes instead does not have to store any state at all. And in order to verify blocks they have a witness that basically provides the portion of the state accessed by a particular transaction plus a cryptographic proof that shows that the values that are provided are actually correct. Right? So it's also been called stateless clients and vertical trees which we've been talking about a lot, and code verticalization which we've been also talking about a lot. It's going to become code virtualization.
00:21:12.862 - 00:22:13.560, Speaker B: But these are all really important to make the witness sizes actually small enough for this to be viable. EIP 2929, which happens in Berlin, also contributed to making statelessness viable because it established a nice low bound on the number of state accesses that can happen inside a block. It also made my uniswap transactions about 1% cheaper, which is interesting. So if you have a DAP, there's a chance that it got 1% cheaper and there's a chance it got 1% more expensive. But I think a lot of things did, especially more expensive things did end up getting slightly cheaper, which is interesting. But basically it was a reworking of gas costs that just ensured that there's this bound on the number of storage reads, which means that there's a bound on the witness size. And so there isn't a risk that there's going to be blocks where just the size of a witness is too big to propagate to everyone in time.
00:22:13.560 - 00:23:39.346, Speaker B: There is one thing that is still required to kind of finish that task, one more gas cost change which is at the same time as covert virtualization. We will need to add a gas rule that charges some amount of gas for every chunk of code that gets accessed. And I'm just warning the community this is something that is going to cause some medium amount of pain to at least some application developers. So I'm hoping that fairly soon we'll have tools that will kind of help us understand more exactly what transactions will have to consume more gas as a result of this. But this is something that we do need to kind of keep an eye on and just make sure that it's not a surprise to people. So basically a gas cost of roughly 400 gas for every chunk of 32 bytes in the code that gets accessed as the thing that will need to be done. So that's statelessness, state expiry is a different strategy, right? State expiry is a strategy that basically says, well, we are going to have the state and objects that are not touched for a long time get kind of pushed out of the state and they instead become part of the old state.
00:23:39.346 - 00:24:44.150, Speaker B: And if you have an object that's part of the old state and you wants to access it with a transaction, then you have to provide a witness. And when you provide a witness, then that object gets renewed and it gets added to the more recent state. So the idea is basically that the state that clients, including miners, including full clients, everyone has to store, would only be the set of accounts that were accessed roughly within the last year. And older things are just going to be stored by just archive nodes, block explorers, the graph like other decentralized projects and you'll need to basically get those witnesses through some other protocol. They're not the responsibility of the core Ethereum consensus protocol. So important point of clarification, if you have an account that has a million wrapped doge and you hibernate for two years, your wrap stage is not going to disappear, right? Nobody is going to lose money just because they forget. What's going to happen is your wrap stage is going to become part of the old state.
00:24:44.150 - 00:26:01.002, Speaker B: And if he wants to do something with your wrap stowish, all you have to do is just go through one of these. Secondary protocols, grab a witness for the state and pay a bit of extra gas to make a transaction that contains a witness and sort of brings the account and the storage slot that contain your wrapped doge balance back into the updated state. This is done using this scheme that basically kind of creates a new epoch every year and adds a new state tree every epoch and clients effectively store the most recent two state trees. There's a long post I wrote once again you can probably go find it and I'm sure somebody will hopefully provide a link to it. But the current likely path is to actually do both of these things at the same time. So statelessness and state expiry at the same time and it turns out that this sounds crazy, but it's actually easier to do both at the same time than it is to do either just statelessness or just state expiry, which is interesting. So big project, it does have quite a bit of complexity, but it has a lot of value and a lot of potential to do some important good for the ethereum ecosystem.
00:26:01.002 - 00:26:50.366, Speaker B: It really does remove what's probably the main impediment to just further increasing the base chain capacity. And once this happens, it'll also simultaneously become considerably easier to run a full node that verifies the ethereum execution layer. Excellent. Someone actually did provide some links in the chat. Yeah, right. Let's see. Another thing, by the way, is that I've been talking about this idea of kind of forgetting old data in a couple of contexts, right? I talked about forgetting the proof of work chain history and I talked about forgetting these expired states.
00:26:50.366 - 00:28:11.514, Speaker B: One really nice thing about is that when either the proof of work chain or these expired states expire, they're static objects, right? And having a protocol for backing up and retrieving a static object is much easier than doing it for a dynamic object like the state, right? So for the current state, developers like Piper and Jason Carver and his whole lovely crew over in Colorado have just been doing some amazing work in trying to figure out how to make a good protocol for kind of extracting this and giving clients this data in real time. But it's hard because the state just keeps changing. But older states and the proof work chain history, they're just static files, right? Like once they become old, they're just fixed things and they never change. And what that means is that you can just potentially even stick them on bittorg, right? You can stick them on one of a whole bunch of archiving solutions. And so I actually don't expect archiving and extracting from archives and even doing that in a fully decentralized way to be a problem. Right? I saw two maybe dumb questions from NN. When will the merge go live for the public? From what I optimists are saying before the end of the year.
00:28:11.514 - 00:29:19.840, Speaker B: Pessimists are saying after the end of the year. But I think we don't want to give kind of good, really precise numbers, especially until the whole Rainism thing ends and we kind of have our learnings from that. If everything is open source, how does the team protect from other groups just copying everything and building their own centralized business? If other people wants to fork ethereum and make a centralized version, they're perfectly free to. That's actually happened multiple times already. I'm not worried. I think the ethereum ecosystem's kind of main value proposition is the decentralization and the ecosystem and the public legitimacy that comes with the ecosystem. And the technology is important, but the role of the technology is just to be good enough to actually make sure that people are able to do all of these things that they want to do, right? Like if someone comes up with copies the technology and then builds 5% better technology, it's not like everyone's just going to move to them.
00:29:19.840 - 00:30:26.630, Speaker B: Okay, so some more execution layer things, account abstraction. So this is like one of my personal big favorite kind of pet projects, of course. So basically moving away from requiring every transaction to start in an ECDSA wallet or sorry, in an externally owned account which runs on ECDSA and allowing transactions to start by just being calls to contracts. And this has a lot of benefits. One of them is that multisigs can become simpler because a transaction could just go directly into a multisig, and the multi SIG itself can pay for gas. Without this complicated scheme where the transaction goes into an EOA, then the EOA pays for gas, then the EOA calls a multisig, the multisig refunds the EOA, and then the EOA needs to hold some ETH. And what if gas prices go up by a factor of ten and it adds that the EOA is not holding enough ETH and then you're stuck basically getting rid of this and moving to this future better world where multisigs and social recovery wallets, which I'm a big fan of and all these other things just work as efficiently as regular accounts.
00:30:26.630 - 00:31:18.934, Speaker B: There's multiple paths to this. So EIP 29 38, this EIP that myself, Anzgardrics, and some other people have worked on a while back, is one example of a path. Another example of a path is to just rely on Flashbots. So Flashbots can create a system by which they detect these transactions that are just zero gas price direct calls to an EOA, and that's just a forwarder to a contract. And if they decide that the transaction will pay a fee, then they could just put it in a bundle and miners will accept it. Or possibly some kind of convergence, right? Like possibly there's some set of modifications you can make to EAP 29 38, some set of optimizations that you'll have to do to Flash bots. And these two routes will actually maybe end up more similar than different in half a year's time.
00:31:18.934 - 00:31:59.140, Speaker B: I don't know. But this problem is really important to keep working on EVM improvements. So Evmx is a big one that I'm personally excited about. Just modular operations going for 384 bits and above. This is really good for cryptography. Also you can look at bit sizes below 256 bits. And this is one way to get back some of the benefits of AVM that's 64 bit based instead of being 256 bit based, and potentially other kind of improvements to the ethereum virtual machine subroutines, potentially some other things down the line.
00:31:59.140 - 00:33:11.878, Speaker B: So that is kind of the medium term future. The next question is the long term future, right? So one thing in the long term future that I think is really interesting is a CBC Casper. So there's these two flavors of Casper, right? Casper Ffge and CBC Casper. CBC Casper is kind of sitting in this state where a version of CBC Casper that's fairly simple and that's theoretically implementable is there. But the problem with that version of CBC Casper is that there's just efficiency issues with it, right? CBC Casper requires you to do much more logic that has to do with keeping track of complicated data that has to do with individual messages, whereas FFG is just kind of counting the messages up, right? And CBC Casper is likely more secure because there's fewer. Kind of awkward interaction issues with LMD Ghost CBC. Casper just runs on LMD Ghost, so it's much simpler in a conceptual way, more flexible finality thresholds.
00:33:11.878 - 00:34:02.950, Speaker B: So like Casper FFG, if less than two thirds are online, you just lose finality completely. With Casper CBC, as long as more than 50% are online, you have at least a little bit of finality. Here's this kind of current best effort from two years ago, issue 701, that tries to talk about how to Cbcfi the Ethereum chain. But there might be better ways, and it's possible that the better ways actually depend on just waiting for better technology, like ZK snarks, for example. And that brings us to snarking everything. So we can snark the beacon chain, we can snark the EVM. We could also potentially snark a better VM, but have some kind of pathway by which EVM code can be automatically compiled to the better VM.
00:34:02.950 - 00:34:58.220, Speaker B: But if you write smart contracts in the better VM directly, you get more efficiency. So the two of these things together basically mean that it just becomes much easier to run a node, because you don't have to expend barely any effort to verify other people's blocks. You're just verifying a proof and downloading some data. The main effort you'd have to expand is on creating your own blocks, so the load would just be much lower and potentially Starking. The EVM also just makes it much easier to add native execution to shards if that ends up being something that we want to do. Right? It's just much easier to figure out the crypto economics around adding EVM execution to shards if you have Synacs. And so you don't even have to think about the possibility of one of these executions being wrong and then you having to revert it.
00:34:58.220 - 00:35:38.390, Speaker B: So stacking everything is really nice. It could add a lot of efficiencies. It adds this kind of Holy grail of a chain that requires a very low level of computing power to keep up with. You could potentially have a super powerful light client protocol where the light client just does some data availability, sampling, and then verifies one proof and that's it. And you've basically verified that you have the current Ethereum chain and it's up to date and it's correct. The benefits of this are huge, right? If we want to, for marketing purposes, we could just brands this Ethereum 3.0. But that's like the community's decision.
00:35:38.390 - 00:36:06.738, Speaker B: I don't know, it's like your choice. Like, fight it out in your forums, people. Quantum security is nice. So basically, quantum computers are going to come at some point. And to be quantum proof, we basically just have to replace every elliptic curve thing with a hash thing. So replace Synacs with Starks, replace BLS signatures with aggregate signatures. And we know of Stark based aggregate signature schemes that are pretty good.
00:36:06.738 - 00:37:10.086, Speaker B: Starkware had some really nice ones that they have some examples of. Replace the Verkel tree with a Starked merkel tree and potentially replace the VDF with a Stark VDF if we end up doing a VDF that's not stark from day one, just stark everything and do everything based off of hashes and starks. So at this point, arguably we're done, right? Basically the first set of things here is some security improvements, some economic sustainability improvements and some features. And the far future is just about really nailing down and improving and having extremely strong guarantees about the security of the system. And then once you have the features and once you have the security, if you want, core dev can go on vacation and go into maintenance mode. Right? And I think there's a lot to be said for that. Basically anything further, if needed can be done at layer two.
00:37:10.086 - 00:38:13.962, Speaker B: And if layer one just kind of stops moving quickly after this point, then that's really nice because it reduces the load on client developers. It reduces the risk of centralization in client development. You have this kind of one finite piece of code and anyone would easily be able or one finite piece of spec and anyone at least who has the knowledge to implement CK snark things would be able to just implement a client for it. And also at this point, backwards compatibility is probably something that becomes much more important. Right? So there's a lot to be said, I think, for just kind of really reducing the speed of functionality changes even after this set of things is done, and then reducing the speed of just changes in general after some key security improvements are done. Now, I don't think these are the only three key security improvements that we'll have in the far future. I'm sure that there is going to be others.
00:38:13.962 - 00:39:21.550, Speaker B: There is also the possibility of just tweaks to economic logic. So one thing that I did not mention, for example, for the medium term is a tweak that just bounds the number of active validators just to kind of reduce the risk that the chain will end up being really or unexpectedly really expensive to process. But I do expect basically that sort of the pace of change for ethereum can decrease. Right now there is still the possibility of experimenting with new VMs, and if we want that's, something that can be done at protocol layer, but that's also something that can be done at the roll up layer. Right. Or potentially there's routes in the middle where the protocol layer adds protocol features that are specifically designed to basically make it really easy to make roll ups with different VMs and provide a kind of excellent and very smooth user experience for just jumping between them. But these are still more kind of far future questions and even different people on the team have different opinions on these.
00:39:21.550 - 00:39:48.440, Speaker B: But that's basically where sort of the current known roadmap is going. Um, well, yeah, again, if people have questions, I'm happy to answer them. I'm also happy to scroll through the chat box.
00:39:49.450 - 00:40:17.860, Speaker A: We'll do a bunch of moderation so we have a list of questions that have not been answered yet and thank you so much for kind of dropping this whole list of amazing things that are upcoming in the future. For everybody who's going to look at this in 30 seconds, please post your questions on the chat and I'll relate them directly on this to Vitalik. I'll start with a few that have not been answered. I guess the first one obvious question is why is statelessness and expiry kind of management easier to do at the same time versus independently? And what's kind of the rationale behind that?
00:40:18.230 - 00:40:33.080, Speaker B: Yes, that's a very good and important question. Basically the idea is that the way the state expiry scheme works is that you go away from having one state tree and you instead have a state tree per epoch, right? Are you still there?
00:40:34.650 - 00:40:37.254, Speaker A: Yeah, everything's good.
00:40:37.292 - 00:42:24.954, Speaker B: Sorry, I was just worried that Zoom suddenly did something wonky. Okay, so basically state expiry involves moving from one state tree to this model of one state tree per epoch. And the really nice thing about that model is that that model actually by itself makes it easy to upgrade the state tree format, right? Because if you upgrade the state tree format, you could just make the upgraded version apply to new epochs and that's it, you're done. Whereas with the current model, if you want to update the state tree today, you have to have this really complicated protocol where first you kind of snapshot the current state, then you have everyone recompute that state, then you keep track of what the delta is while you're doing the recomputing, and then you apply the new state and then you start taking that delta and then kind of moving it over into the new state tree that you've created. It's this fairly complicated five step protocol and the complexity of doing that is already quite possibly half the complexity of just making a state xpiry solution. So if we do state expiry, basically that just automatically gives us the easiest possible way to upgrade the state tree because all we have to do is we just have to say, well, everything starting from epoch one is going to use Berkele trees instead of HEXO and Patricia trees. And then if we want, we can make one single hard fork about a month later after epoch one begins where we all just calculate the equivalent vertical route of epoch zero and we just kind of swap it out, right? And that's just a one time thing.
00:42:24.954 - 00:42:39.440, Speaker B: So basically moving to this model where you have epochs and old epochs expire and become frozen just happens to as a byproduct, make it easier to do kind of all of this stage B upgrading that makes stateless as possible.
00:42:39.970 - 00:43:02.600, Speaker A: Got it. Awesome. I also want to just quickly thank Trent and Marius on the chat. They've been doing an excellent job annotating some of the points that you've made and kind of providing additional resources and also just commenting on what's happening. So just thanks so much for making this community awesome. From what you just said, there was another question that's related and that is I lost the question. Let's see.
00:43:02.600 - 00:43:10.490, Speaker A: There you go. So when will vertical trees be live? Will it be before after the merge? Just a quick clarification there.
00:43:10.560 - 00:43:15.750, Speaker B: Okay. Everything I talked about in this post, including merkel trees, is after the merge.
00:43:15.910 - 00:43:27.630, Speaker A: Awesome. The next question is what will decide if after the merge, that sharding. After merge and sharding, we will focus on data availability as opposed to further layer one scaling.
00:43:28.130 - 00:44:19.454, Speaker B: Okay, basically the reason why we're doing this data availability centric sharding roadmap is because, first of all, roll ups are just like the scaling strategy that's the closest to being released. They have uncertainty, but everything else has even more uncertainty. And data only sharding does give them a scalability boost immediately. But the second thing is that having data sharding is in some ways just by itself a necessary stepping stone to having data and execution sharding. In order to solve the problems that you have to solve to have data and execution sharding, you have to first solve the problems that have to do with data sharding. And then on top of that, you have to solve some extra problems that are execution specific. So it's on the way anyway.
00:44:19.454 - 00:44:24.580, Speaker B: And so it's just obvious that this makes sense as the short term goal to focus on.
00:44:25.030 - 00:44:33.970, Speaker A: Awesome. That makes a lot of sense. Slightly related to the last question, how can technical debt be reduced pre merge?
00:44:35.350 - 00:45:52.474, Speaker B: So my expectation is that pre merge, there's still going to be quite a bit of technical, quite a bit of technical debt and it's not really going to go down before the merge. I think a big part of the reason is that before the merge we're just continuing to have this kind of social norm that says that in order to have something that's quote a legit ethereum client, the Ethereum client has to be able to process everything from genesis up until the current point. And I think in the long run that's a formula that's worth moving away from, right? Like in the long run, I think we should be possible. A client should only have to be able to verify, say, the last year of history, for example. And so if we do that, then kind of older versions of the protocol before recent hard forks can just sort of stop being considered and stop being part of the necessary code base over time. And in particular, probably the biggest gain that we're going to get is when we do kind of make this important decision of kind of removing the ability to sync the pre merge proof of work history from the core Ethereum protocol. Right.
00:45:52.474 - 00:46:18.100, Speaker B: Because once that happens, then there really is not going to be a need to do any of this to do anything with the proof worksheet. And you'll go down to just having this one chain and the execution clients can just become much simpler. You can just completely remove fork choice code and kind of separate syncing code from execution clients. So a lot of good things can happen. Yeah.
00:46:21.610 - 00:46:52.720, Speaker A: Cool. To correct myself there, I also want to thank a couple other people who've done a great job moderating this community. So ETH, memes and Sambuza, thank you so much for also being super active. I didn't mean to exclude anybody there. The next question is what is a world of, in a way, layer three look like where you have ETH l two and L one working together and just is that a valid question? What would that mean in a future case?
00:46:53.650 - 00:47:24.906, Speaker B: I'm not sure what layer three means. I think layer three is one of those terms that hasn't really been socially defined yet and so it can mean different things to different people. Does it mean the application layer? Does it potentially mean layer two protocols that have to do with reading state from the chain? Does it have to do with application layer constructions like Oracles? I don't know. I think all of those are important problems. They're kind of worthy of having their own teams working hard to solve them.
00:47:25.088 - 00:47:33.760, Speaker A: Makes a lot of sense. You touched on a lot of new things that are up and coming. What about EVM and eWASM? Where does that fit in?
00:47:35.330 - 00:49:27.426, Speaker B: Right, so my personal opinion is definitely that I personally have become less interested in eWASM over time. I think a big reason for this is that my excitement for eWASM initially was about the possibility that this is this advanced VM that has a lot of existing professional work going into making optimized implementations, and they can even be compilers instead of being interpreters, so they could be super fast and you can implement cryptography in them and you don't need pre compiles anymore. But the research since then has shown that that's not the reality. Right? And the reality is that the Evosm compilers all have worst case bugs where you can make worst case code that attacks them and it makes them very slow and interpreters are not that big an improvement over the EVM. And also that it's possible to extend the EVM and improve it, to make it basically good enough to do cryptography or to do a lot of cryptography with maybe a two to three x penalty with things like EVM 384, I guess it's being renamed EVM X now and so that path is looking better. The realistic kind of a long term VM upgrade possibility is like this idea that I mentioned, where in the long term we can make this VM that's more explicitly snark friendly and that could be a time during which that EVM Can Introduce Some More Features that Kind Of Take Advantage Of The Ability To Do Faster Execution without Bothering With 256 Bits For Everything. But the important thing is that it will have to be backwards compatible with the EVM, right? It'll have to have some kind of compilation path from the EVM to that new VM because there's just existing applications that are using the EVM and we're not going to be able to make them go away.
00:49:27.426 - 00:49:32.340, Speaker B: And if we want the chain to be provable then those things would have to be provable as well.
00:49:33.510 - 00:49:46.710, Speaker A: Absolutely. No, that makes a lot of sense. Especially as you pointed out the Evmx feature set. It does kind of supersede a lot of those trade offs. Another kind of quick question. Is there an optimal number of active validators to be expected for the merge?
00:49:48.170 - 00:50:02.734, Speaker B: I feel like the amount of validators that we have already is more than enough. Obviously if there's three times more then that's great too. But I'm not worried about there being too few. I'm also not worried about there being too many.
00:50:02.932 - 00:50:15.060, Speaker A: Got it. Next question is how will contract composability work post merge in terms of just post merge and Sharding in Async environment? Just what does that look like for all this?
00:50:15.750 - 00:51:00.670, Speaker B: Sure. So I think the merge itself is not going to affect anything, right? Because the merge itself is just a change to the consensus engine. I expect we will learn much more about Async interaction from the roll up world. The roll up world is really starting to come online this year and we already have bring in Zksync and diversify and we're going to see optimism and Arbitrum and I think believe Seller has some roll up thing as well. There are going to be applications on each of these systems and there's going to be a need to have interoperability at the ability to jump between them easily. And this is a problem that the ecosystem will just have to figure out. And I feel like from the roll up world we'll just learn a lot.
00:51:00.670 - 00:51:23.720, Speaker B: There is a big chance that it's just not a problem, right? Like you have this concept of zones and you can do synchronous things within one zone, but if you want you can jump between zones and jumping between them is fairly easy but you'll just not be able to do synchronous things across multiple zones directly and that'll end up being fine. So that's my kind of number one prediction. But we could also learn other things, right?
00:51:24.330 - 00:51:37.770, Speaker A: Absolutely. And sort of kind of attaching to that answer, I guess. Are you excited about any particular positive effects after the merge for more consumer facing use cases on top of ETH?
00:51:38.190 - 00:52:31.686, Speaker B: I'm excited not so much for the merge but more for in that case, but more for roll ups and Sharding. The reason I am excited for roll ups and Sharding is basically that roll ups and sharding are going to increase scalability and so they're going to massively reduce transaction fees. And the massive reduction in transaction fees is something that we are going to need to enable non financial applications. Right? There's a lot of really exciting non financial applications. There's ENS, there's the NFT stuff, there's the Proof of Humanity stuff, there's like Colony and all the Dow stuff. But for those things to run, transactions just have to be significantly cheaper and to orders of magnitude cheaper. And roll Ups will provide that and Sharding will provide that, even in the context of a much larger community and much higher demand.
00:52:31.686 - 00:52:45.600, Speaker B: So those are definitely things that I am excited about and I am just excited about the Ethereum Ecosystem being able to sort of turn back on as a platform for kind of deployment of all of those amazing non financial things.
00:52:47.010 - 00:53:18.780, Speaker A: I personally look forward to that world just for everybody listening. And we'll do a final call of the last few questions coming in. So if you have anything that has not been answered, please type that again on the chat and I'll relay that over here. One other kind of question I have is you kind of talked a lot about what the post merge world looks like, the far out future, and there's a lot of emphasis on just Snarks and Starks and just kind of talking of everything, how far out is that timeline? And if you were to kind of give estimates, when does that work?
00:53:20.530 - 00:54:32.382, Speaker B: Possibly somewhere around three to five years. It's important to note that it cannot happen too quickly because doing synarcs well requires using Synarc friendly hash functions. And the reason basically is that existing hash functions like shot 256 are just insanely bad from a narc efficiency perspective. Like they take many tens of thousands of constraints, whereas the arithmetically optimized ones can work with only either a couple of hundred constraints or even there's fancy stuff like GKR that if it works, would make it even cheaper. But these fancy hash functions require some amount of time in order to just be derisked and academically tested and crypt analyzed and all of that stuff. But once we have that, and once we have just better Stark technology another thing is that just making a Snarked version of the EVM is just this complex kind of engineering task and probably a research task and that's just something that will take a lot of time to do. So yeah, I expect it to be a multi year effort.
00:54:32.382 - 00:54:54.140, Speaker B: And also just realistically, I do expect it to be a kind of lower priority effort for implementation than say, state expiry just because state expiry just is this really important kind of high priority item for sustainability and to allow more gas growth. But it will happen. We've already started doing quite a lot of things to prepare for it happening.
00:54:54.750 - 00:55:20.980, Speaker A: No, that's awesome. Maybe I'll end with a question here related to scaling Ethereum the hackathon. There's over 600 developers hacking on cool stuff to make everything possible on L2 and just more scalable L One solutions. Do you have a list of any cool ideas or things you'd want to see? Whether it's tooling or applications or just anything that would drastically improve the experience for what it is now that people can hack on.
00:55:22.870 - 00:55:28.306, Speaker B: Any tooling that people can build that could drastically improve the experience or tooling that exists?
00:55:28.418 - 00:55:34.390, Speaker A: Your wish list of scaling related hackathons or ideas or tooling that you think could improve this ecosystem.
00:55:37.610 - 00:56:20.760, Speaker B: Just like zooming out a bit and just random ideas. One social Recovery Wallets inside of Roll Ups I'm a big fan of Social Recovery Wallets for reasons that I talked about in my blog and other places. And roll ups just seem like a really good opportunity to kind of actually do them and get people into the habit of using them. So that's like an example of something that would be nice to build. Another example of something that would be nice to build is actually building scalable infrastructure for reading data from the Ethereum blockchain. Ethereum history, roll up chains, shard chains, like everything. I think it's about the same either way.
00:56:20.760 - 00:57:15.766, Speaker B: The reason why this is important is because we are moving away from a world in which regular users will have nodes that are actually processing all of the data. And because of all of these techniques that we talked about, including roll ups and including erasure coding and data availability sampling, we're still going to have security. But users are going to needs to have alternate ways to be able to grab the data that they need on demand, either because they wants to read it or because they want to create transactions with it. And so creating protocols that actually can kind of basically replace a full nodes kind of API as much as possible, but do it in a decentralized way that doesn't require any individual node to store anything is something that would be really valuable to make as well.
00:57:15.948 - 00:57:19.482, Speaker A: How would somebody go about implementing something like that or designing something like that?
00:57:19.536 - 00:57:33.920, Speaker B: I guess it's not that hard. I think step one is markle proofs and step two is discovery and step three is incentives. You just have to kind of combine those pieces in the right way and I think you can make something great.
00:57:38.930 - 00:57:51.090, Speaker A: Well, if there's nothing else, I want to thank you so much today for doing this amazing talk. Hopefully we'll make those slides in the video available later on for everybody to catch up. And thanks so much again.
00:57:51.240 - 00:57:52.034, Speaker B: Thank you very much.
00:57:52.072 - 00:58:33.866, Speaker A: Kartik great. So thanks everybody for sticking us with all day today. I know this is interestingly vital again on a really cool note because our next week's summit is actually all about zero knowledge and how that world is going to make anything from scaling to just new primitives will allow more applications to be possible. So please tune in for next Friday for our Zero Knowledge Summit and we'll walk into what programmability with Snarks and starts looks like along with cool applications of these technologies. All of that information is available on scaling ETHGlobal co. So for those of you who are watching the Summit, I'll see you all next Friday. And for everybody hacking, wish you all the best for your current hacks and I will see you on our discord.
00:58:33.866 - 00:58:35.200, Speaker A: So thanks again.
00:58:35.810 - 00:58:37.450, Speaker B: Thank you. Bye.
