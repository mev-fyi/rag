00:00:07.530 - 00:00:48.490, Speaker A: Hello everyone. Good morning. Afternoon. Not sure what your status is in the hackathon so today I want to talk about something new that's coming in Ethereum. Specifically. If you think back about past Ethereum hard forks or past Ethereum EIPS most of them were all about features, features, features which is super nice but we kind of reached a point where Ethereum kind of works except it's super expensive. So nowadays people are much more focusing on how can we make things cheaper or faster? Or we have this word scaling as basically a bag of features that we want to work on.
00:00:48.490 - 00:01:34.582, Speaker A: And one specific solution that's coming up will be EIP 4844 and I would like to talk about some very small aspect of it but before we dive into that maybe we should let me just do no peeking on the next slide. So first of all, what is EIP 4844? Well talk about that. Maybe we should just mention very, very briefly what L2s are. And layer ones essentially Ethereum is a layer one network. Bitcoin is a layer one network avalanche, et cetera, et cetera. Layer one networks are kind of defined as self contained networks. You have transaction handling, networking, security all kinds of everything that the blockchain needs to do is just self contained wrapped into this single global network.
00:01:34.582 - 00:02:12.002, Speaker A: That's kind of a layer one. Layer two networks are almost like it. It's kind of almost like a completely self sustaining blockchain but it is somewhat dependent on somebody else. For example, if you look at Optimism or Polygon or the other L2s all of them run transactions concurrently to Ethereum, have their own networking, state management, et cetera, et cetera. But at some point, usually at the security side they kind of delegate that out to some other blockchain. For example Polygon, Optimism they don't have 10,000 nodes globally running some complex proof of work or proof of stake chain. Rather they just piggyback.
00:02:12.002 - 00:03:09.078, Speaker A: And the way they piggyback is that they just run all the transactions that they have aggregate the results and from time to time submit those results to Ethereum as proofs. And why is this good? Well, this has the potential to scale Ethereum. Why could it scale ethereum? Well, if Optimism for example, runs transactions and submits the proofs only every 15 minutes then it means that even though one of those proofs might be super expensive since they only submit it every 15 minutes a lot of that expensive fee gets broken down across all those transactions. So that's the scaling part. Now the potential part is that the drawback everything is a trade off in Ethereum world. The drawback is that these proofs since they are submitted only let's say every 15 minutes yeah, so I submit my transaction, it gets included. But will it be included? Will I be sure that it ends up on chain? I don't know.
00:03:09.078 - 00:03:42.242, Speaker A: I have to wait for 15 minutes. So the security is a bit wonky. Furthermore, it gets an even more wonkier thing that these commitments can be challenged. So anything that's run on Ethereum is immediately proven. But anything that's run on optimism, it can be challenged for a week. So it means that if you sell your house on optimism maybe you won't wait for a week before you just hand over the keys. But after a week passes, the commitments, let's say nobody challenged it, everything is fine, life goes on.
00:03:42.242 - 00:04:21.598, Speaker A: But what happens to these commitments? Well, currently these commitments stay on chain but they are kind of the week passed so they are completely useless now. And that's a problem. And why is that a problem? Well, specifically because data on Ethereum generally lives forever. Which means that it is very expensive. For example, just to take a round number of 128 KB submitting just 128 Ethereum costs about, give or take $100 at current prices. But that's just sending the data to Ethereum. If you want to actually store it in a smart contract, good luck, that's $4,000.
00:04:21.598 - 00:04:55.450, Speaker A: And if you want to emit a log, that's maybe $80. Now obviously L2 solutions will have their own weird ways of what data they submit, how they store it. So I can't really say that, okay, optimism will cost this much or arbitram will cost that much. Just the idea is that the costs are insane and if this data would be something we want to actually rely on long term then we could maybe rationalize the cost. But the moment this data needs to be thrown away after a week, that's insanity. We need something better. And this is where 4844 comes in.
00:04:55.450 - 00:05:44.540, Speaker A: Specifically, 4844 introduces a new type of transaction called blob transactions where you kind of have a normal Ethereum transaction but you can have this ephemeral data blob attached and this blob will stay in the network for a month or a week. It's kind of not decided yet. The spec is not final and then this data blob will get dropped out of the network. And a very, very interesting property is that this means that the long term usage or long term price for storing this data is essentially zero. We have to store it for a month but then we can delete it. And of course 1559 was very, very popular because it solved a lot of issues. And 4844 introduces something similar for data blobs where the fees for these data can go dynamically up and down.
00:05:44.540 - 00:06:38.278, Speaker A: Cool. Now there's an interesting aspect, a political aspect here that I want to mention specifically that 4844 introduces somewhat a parallel universe for these block transactions. So whereas currently you have this gas that you just pay for EVM execution and you have the base fee which is the cost for this gas. As I mentioned previously, 4844 introduces a data fee which is completely parallel to the base fee. And that kind of means that if a L2 submits a Blob transaction, it doesn't need to pay the base fee, it needs to pay the data fee. And these move dynamically independently from one another. So what, we could end up with users having to pay a lot of money for large transactions and L2s having to pay very little money for large transactions.
00:06:38.278 - 00:07:20.342, Speaker A: So that might be a bit wonky. Is that fair? Is that good? And well, first up is definitely somewhat of a preferential treatment for L2 S. But it turns out that it's not that bad. Because Blobs consume this data gas, it means that it doesn't matter how many data Blobs we submit to the blocks, they won't consume the normal gas. So users, they won't simply so the assumption is that L2s will always have a lot of money, they can always afford the transactions. So by moving their gas usage outside of the block space, they don't compete with users. So all of a sudden users can run their own transactions.
00:07:20.342 - 00:07:57.800, Speaker A: They don't get pushed out because of the L2s and the fee wise. For sure, data fees move independently and that means that they can actually be cheaper than the base fee. But it also means that if L2s start having a little war between themselves on who gets to publish the next data Blob, that doesn't impact normal users. So the base fee remains completely independent from the data fee. So all in all, yes, 4844 is preferential treatment of L2 S. But from the global perspective of a system, it is probably better for everybody like this. Cool.
00:07:57.800 - 00:08:26.478, Speaker A: So this was just a little background on 4844, why it's useful and why we think that it will be useful. But let's go into a little bit of technicalities and maybe I should look at that screen. And that is Blob transaction pooling. Now, the 4844 is a big EIP. It has a lot of different aspects. It touches consensus, it touches cryptography, it touches execution layer. There's a lot of stuff to talk about.
00:08:26.478 - 00:09:00.230, Speaker A: What interests me personally or what keeps me up at night is if the network crashes, denial of service stuff. If a smart contract gets hacked, I sleep well, I can't do anything about it. That's not my problem. If the network goes down, that's my problem. So that's what I'm focused on. And that's why this talk is about what it takes to handle these transactions. And I will try to contrast them with the normal Ethereum transactions and the normal Ethereum transaction pool to maybe highlight why they are different and what we need to do to handle them differently.
00:09:00.230 - 00:09:40.870, Speaker A: First up, transaction sizes. While Ethereum transactions are really small, probably a value transfer is 100 bytes or 200 bytes, I can't really say. Most contract interactions are less than 1 KB in size for sure if you want to deploy a new contract or if you want to do something very fancy, certainly some transactions might be higher in size, but generally 1 KB below 1. Good estimate. So that means that networking wise, we don't care about how you propagate these transactions through the network. You can just send it to everybody. You can just send it to a subset of your peers.
00:09:40.870 - 00:10:25.714, Speaker A: The impact is so small that it's not that relevant. So what we currently do is we just broadcast it to a square root number of our peers. It's just a random number. We could broadcast it to five peers, it would be the same thing. It doesn't matter in the grand scheme of things, and usually, as long as we have a well connected network, like the first little graph there, sending it to a couple of our peers is enough. There are certain parts of the theorem network which are kind of degenerate, where you have these spokes, probably corporate networks there, it actually might make sense to have a backup mechanism. And what we do there is we not only broadcast it to a few of our peers, we just announce it to everybody else.
00:10:25.714 - 00:10:47.306, Speaker A: Just say that, hey, I have this transaction with Hash. Whatever. If you want it, come and take it. So that's kind of the entire transaction propagation for legacy classical ethereum transactions, block transactions are different. However, they are big. The smallest Blob transaction will be 128 KB in size. Depending on what the final spec will be.
00:10:47.306 - 00:11:22.822, Speaker A: It can be half a megabyte, two megabyte. It can really grow, and that's an issue. So even if I take 128 KB, propagating that across the network where every single node just gets it once and sends it once to somebody else, that's already, I think, either 1.3gb or something of data traffic for the network. So it's large. So if you all of a sudden start passing around these Blob transactions all over the place and sending it to many peers, it's going to be a huge hit on the networking. So we don't want that to happen.
00:11:22.822 - 00:11:51.834, Speaker A: So, first up, first rule of Blob transactions, no broadcasts. Broadcasts are forbidden. And on networking layer, the only thing we must allow is we can announce it to other peers and they can retrieve it if they want to. This has some implications that latency of block transactions will be a bit larger, but we're willing to live with that. And the other thing we need to take care of is that fetching transaction. The concurrency for normal transactions. Transaction spools are dumb.
00:11:51.834 - 00:12:18.780, Speaker A: Hey, here's a new transaction. Awesome, give it to me. Hey, here's another transaction. Awesome, give it to me. For block transactions, we need to kind of be a bit smarter and make sure that if we start requesting random transactions, we don't overload ourselves. So that kind of requires a bit of extensions on the networking layer. So when something is announced, we also know how large it is, what that is, so we can be a bit smarter and that actually is the E 68 protocol that was deployed a couple of months ago.
00:12:18.780 - 00:12:59.154, Speaker A: Okay, transaction size Blobs are big rule number one. The other interesting aspect is block space. Now, an Ethereum block contains I mean, it's targeted to contain 15 million gas, and it's capped at 30 million gas. That's quite a lot of transactions. If I were to fill it up with plain value transfers, that would be a bit over 1400 transactions that could fit in a single block. Now, the implication of this is that there's a very, very high rate of Churn on the transaction pool. By churn, I mean just new transactions arrive, all transactions get popped out, people cancel, transactions replace.
00:12:59.154 - 00:13:44.830, Speaker A: So it's just this big pile of random transactions shooting in the network. And there's a big problem with these things. We cannot really persist the transaction pool to disk. There are so many transactions coming and going all the time in the network that pushing them to disk is problematic. Okay, if you have a very fast SSD, you can probably do it, but it's not obvious what the implications are. So what currently, at least Gath, I would assume other clients also do is they just keep everything in memory and don't care about disk. But if you keep everything in memory, the implications are that you can only keep a limited amount of transactions in memory.
00:13:44.830 - 00:14:19.642, Speaker A: So all of a sudden you open up to these it's called resend and Eviction attacks where somebody just swarms you with transactions, then your pool gets emptied out. Then they cancel those transactions, and it's just a horrible, constant patching of the pool and tweaking it so that different weird attacks cannot happen. Now, opposed to this, Blobs are much better. The Blob space in a block is capped to four. In the current spec, it was 16 in a previous spec. My assumption is that it will go back to 16 in a future spec. Just kind of a rollout.
00:14:19.642 - 00:15:01.594, Speaker A: Just start slow and ramp it up afterwards. Now, the interesting thing about having four of these transactions in a block is that the Churn is very, very slow. So if I need one of these blocks arrives every 16 seconds, or sorry, 12 seconds, and it contains only four of these transactions, I can just save it to disk, load it from disk. All of a sudden, disk storage becomes possible. And this allows us a very interesting thing to increase the capacity very significantly. The exact number is kind of anybody's guess. If we were to increase it to ten gigabyte of disk storage, we can probably store about 80,000 transactions block transactions there probably.
00:15:01.594 - 00:15:38.438, Speaker A: It doesn't really make sense to increase it that much. We can push it even more, even less doesn't really matter. That was just a random number. But the idea is that the moment we have a disk back end for the transactions, evicting legit transactions become kind of unreasonable. Meaning that if somebody wanted to attack a miner's transaction pool, they would actually need to fill up 10GB worth of block transactions. And you cannot really cancel that so fast, so you will probably end up paying a lot to do that. However, the moment we want to introduce a disk back end, we need to take care of artificial churn, meaning pool wars.
00:15:38.438 - 00:16:14.034, Speaker A: Currently, transaction pools constantly allow replacements and miners constantly try to or mev bots constantly try to somehow beat each other and sandwich each other. We need to prevent that. And before we get to that .1 more thing I want to discuss. What is the purpose of transaction? Ethereum transactions are super generic. It's kind of we deliberately didn't want to choose what you can do with a transaction. It's just there, just go do whatever you want with it.
00:16:14.034 - 00:17:12.722, Speaker A: And that's nice, because for users, they can just sub it on transactions, then replace the fifth one, then cancel them because their DAP, I don't know, requires that. And that's super nice. And we are really happy that we can support that use case. But it is a nightmare on our side because so many bad things can happen and any constraint, any limitation that we want to introduce, all of a sudden we break somebody's DAP or somebody's use case and then we have a big pile of issues and it's just head scratching on how to make it work. Now, with Blob transactions, the purpose is L2s. So all of a sudden, we very, very well know what we want to do or what we want to use these Blob transactions for, and specifically that the Blob transactions are made for commitments and these commitments are independent of Ethereum. It doesn't matter what happens on Ethereum.
00:17:12.722 - 00:18:07.670, Speaker A: Optimism still needs to prove that their 10,000 transactions did this so these Blob transactions cannot become stale. There's no, absolutely no reason for optimism to say that I want to cancel that transaction. It just doesn't happen or it shouldn't happen. So furthermore, these L2s need to commit regularly. So every 15 minutes. So not only cancellations don't really happen, but the past transaction problem needs to go through in that 15 minutes window before the next one comes along. And the other reason, the other interesting thing is that since L2s are meant to essentially since these block commitments cannot really change, there's no reason to replace one transaction versus the other, except if the fee really goes up for whatever reason.
00:18:07.670 - 00:18:34.066, Speaker A: The network fee, the base fee, the data fee. So we would like to allow replacements, but not that often. And the reason for that is the cost of replacements. Now, a normal Ethereum transaction can be replaced very, very cheaply. Currently, there's no minimum price required for an Ethereum transaction. It's one way. And the only reason there's one way is that you should not spam the network if you want to replace it.
00:18:34.066 - 00:19:05.546, Speaker A: We'd require a 10% fee bump again, just a tiny random number. So to disincentivize spam quote sorry, parentheses, it doesn't work. So no spam disincentivization there anyway. But the idea is that these transactions are small, so it doesn't matter if somebody replaces it, we just repropagate it through the network. It's fine. And validating one of these normal Ethereum transactions is cheap. Well, there's some signature recovery, some state lookups, but otherwise it's okay.
00:19:05.546 - 00:19:43.270, Speaker A: However, Blob transactions are a complete different story. Rebroadcasting them is very, very expensive. So as I said, just sending one single Blob transaction through the network incurs gigabytes of transfer cost for the entire network. So if people would start fighting with each other and replacing one another's transactions, then the network would probably grind to hold. So we really don't want it processing time. I kind of found a benchmark that it takes about two milliseconds to verify one of these Blobs via KZG. That's the cryptography behind the Blob transactions.
00:19:43.270 - 00:19:57.494, Speaker A: I don't know about that number. I haven't tried it myself. So let's take that with a grain of salt. That two milliseconds doesn't seem like that big of a number. Kind of. It boils down to how many of we get how many of these transactions arrive. But the idea is that the bandwidth is horrible.
00:19:57.494 - 00:20:50.166, Speaker A: So we need to prevent people from replacing transactions. And essentially my solution is just penalize it heavily, meaning that the starting price is fairly high. And if you want to replace something, the fee that you have to bump up is again, very high. Now note, before 1559, if you bumped something by twice the price, essentially had to pay twice as much. But with the dynamic fees, it's not necessarily paying twice as much. It's just that you allow the base fee to go up a bit further. So requiring a 100% fee bum doesn't mean that we're going to be evil and you have to pay, or L2s, have to pay twice as much money, just that they have to accommodate to the fee potentially going twice as high just to make it fair and just to prevent them from having to resubmit, resubmit, resubmit with these 10% raises.
00:20:50.166 - 00:21:34.970, Speaker A: So Blob transactions should be replaceable, but they should be relatively expensive to replace. On the other hand, cancellations are a completely different story. Ethereum transactions can be canceled. People use it. For example, they submit a uniswap swap and then something changes and oh shit, I want to cancel it because all of a sudden some liquidity provider changed and all of a sudden I need to pay more. And then they can just submit a self transfer, which just does nothing at a bit higher fee, just cancels the other transaction. Or I think we've also seen where some people submit multiple transactions and then they try to cancel all of them by just sweeping their funds with a new transaction.
00:21:34.970 - 00:22:07.954, Speaker A: Maybe that second option is a bit more of an attack vector. I don't know if there's a legitimate use case for it, but either way it can be done. And as I said, the problem is you want to remain flexible. And it's very hard after ten years of running ethereum to just say, okay, from today onwards, you cannot do that anymore. That's kind of far to say. However, with Blob transactions, we can be the evil ones and we can say that okay, no, you're not allowed to cancel them. And that has a few aspects.
00:22:07.954 - 00:22:58.754, Speaker A: Specifically, you can replace it, as I said previously, because it's normal that you I don't know, the base fee rises, some fee rises and you want to replace it, but the replacement would only be accepted if that doesn't invalidate any future ones. So you cannot just sweep your account. That's not allowed anymore for block transactions. And again, the reason is that if you propagate five megabytes worth of block transactions through the network that's, I don't know, hundred gigs, give or take, of load for the network, then you're not allowed to cancel them. You're not allowed to just go, no, undo everything. So, rule number one is that you can replace a transaction, but it cannot invalidate any future ones. The other one is same vein that non gaps are not allowed.
00:22:58.754 - 00:23:45.678, Speaker A: So the normal transactions, I can submit transaction with non zero, then five, then ten, and eventually maybe fill it up, the gaps or not. This is kind of useful because people submit transactions randomly, they propagate randomly, it's a useful feature, but it's also an attack vector for Blob transactions. We don't want that to allow that simply because the bandwidth cannot take it. So, rule number two, no nons gaps. And rule number three, which is a bit interesting, is that if you have a Blob transaction skewed up, you're not allowed to submit a normal transaction until that Blob transaction goes through. Again, the reason is to prevent you from wasting bandwidth with the Blob transaction. Again, as long as it's used by L2s, they submit one transaction every 15 minutes.
00:23:45.678 - 00:24:42.930, Speaker A: These rules are perfectly acceptable. So again, the rules were designed for the use case in mind, and the rules allow us to create a much more robust pool. Now, locality, I'm going to speed up a tiny bit, noble transaction pools has this notion of locality where we allowed pre 1559 was meaningful for local transactions to exist, where it can be mined with zero gas price. Or if the more important thing for local transactions was that any transaction that you submitted yourself was kept in your pool always, even if it was too cheap. It was all kind of hacks to somehow try to prevent, try to work around the fact that the pool was too small to fit everything inside. But with Blob transactions, we store everything on disk. So in theory, we can make the disk.
00:24:42.930 - 00:25:15.170, Speaker A: In theory. Again, we need to measure this in live in production so until then, everything is in theory. But in theory, if the pool is large enough, then there's no reason for anybody to be evicted. So there's no reason to say that, okay, this is my transaction, I need to retain this. And that simplifies quite a lot. Plus, since the base fee was introduced in 1559 and now a new data fee in 4844, this whole zero price thing is just eliminated. So there's again no need for this notion of locality.
00:25:15.170 - 00:25:49.730, Speaker A: Now, another interesting aspect with block transactions is that in every single transaction, ethereum was always a superset of the previous one. The access list was extend. Homeset transactions were extended with access lists. Then the accesses transactions was extended with 1559 base fees. And there's a very, very serious push from the research side to have Blob fees extend. This this is something we debate. The idea is that ideally, in my world, Blob transactions should be standalone.
00:25:49.730 - 00:26:31.302, Speaker A: Because the problem with generic stuff is that every time something you make something more generic, implementing it and handling it is getting more and more complicated. Because you just need to do everything that you did until now. Plus a whole lot of other stuff. And as we've seen previously, there are a lot of problems with the existing transaction pool. So if we make something even more generic, then instead of making something more robust, we're making it even wonkier. So that's why personally we push towards making Blob transactions standalone. Meaning that we want to forbid a Blob transaction that does not have a Blob.
00:26:31.302 - 00:27:02.302, Speaker A: So that's the big debate. On the research side. Do we allow zero Blobs or don't we allow zero Blobs? And ideally, we wouldn't allow it. And I guess lifecycle of Transaction I renamed the slide an hour ago. Yes. So there are a few work quirks for Blob transactions. Normal transaction Ethereum transactions are standalone.
00:27:02.302 - 00:27:30.150, Speaker A: This is the beginning of the transaction, the end of transaction. If it's sent over the network, it's completely sent over the network. If it's included in a block, it's completely included in a block. If there is a reorg, I can just say that, okay, block X was reorganed. I can pull out the transactions and stuff them into the next block. It's easy to handle. Blob transactions are a bit more notorious because these Blobs are kind of dangling outside of the transactions.
00:27:30.150 - 00:28:08.854, Speaker A: And that makes a lot of problems. Because one of the problems is that if there's a reorg, then I can pull the transaction itself back from the chain. But the blobs no. And that's a big problem. Which means that anybody working on implementing Blob transaction or transaction pool needs to take care of this fact that they have a very, very different lifecycle. And the other thing that's very interesting with block transactions, exactly the same reason is that the point of entry of block transaction is not clear. Normal transactions arrive into the pool and you have it.
00:28:08.854 - 00:29:11.978, Speaker A: Or as I said previously, maybe they arrive in a block, but you have it. Whereas the problem with Blob transactions is that if we have an Mev bot that mines the Blob transaction and we just receive a block which contains three block transactions, but we don't have the Blobs, okay, what do we do then? And it turns out that this is a complication that need to be handled on the consensus L2. Ha. Because all of a sudden it's just an interesting aspect. I wanted to highlight that there are implications for everybody for Blob transactions. Now, to close up the whole thing, why did I talk about all this? What's the whole point of all these constraints and details and whatnot? Well, we only have a prototype implementation, but our current legacy pool is kept at a very, very low number at 4000 transactions. In theory it consumes 200 megabytes Ram, in practice it consumes a lot more.
00:29:11.978 - 00:30:03.770, Speaker A: The problem is there are a lot of problems because of this small number. And we wanted to introduce as many constraints as possible to blow up this number as much as possible. And with this design it kind of allows us to arbitrarily blow up the disk usage while the memory usage is fairly insignificant. So I just random benchmark that we did for 80,000 transactions it consumes 20 megabytes of memory, so that's not relevant and the disk churn is not relevant either. However, the problem, everything is a trade off. So whereas the legacy Blob pool, whenever a new block arrives, we can reshuffle it very fast in one millisecond for the blocks, for the data Blobs, we have a two dimensional data fee, we have the base fee and the Blob fee. That makes everything insanely complicated.
00:30:03.770 - 00:30:45.014, Speaker A: So for this number it takes a lot more bigger hit, but still, 75 milliseconds after a block is processed before the next block arrives, it's acceptable. All in all, what I really want so what the takeaway could be is that everything will be a trade off. So introducing a new block type of transaction is a trade off. You have to very, very carefully pick what you want to support and what you want to forbid. And the more things you forbid, the better it is for everybody except the user, except L2. But yeah, they all have to live with it. And finally, that kind of metrics is king.
00:30:45.014 - 00:31:15.970, Speaker A: So measure everything because it's very, very easy to say that something should work. But as you can see there, 75 milliseconds is completely acceptable, but it's not a small number. So you need to always be aware of what the implications will be. And kind of that would be a very short intro into the challenges of Blob transactions, at least on the execution layer side. Thank you very much. Thank you. Rapid.
