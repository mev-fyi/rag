00:00:05.850 - 00:00:27.600, Speaker A: You. Welcome to the East Boston first workshop on a Saturday morning. My name is Austin Roberts. I'm here to talk to you about Rivet. So what is Rivet? Rivet is a web3 RPC endpoint. We have a strong open source Ethos. Our service, Rivet Cloud is our service.
00:00:27.600 - 00:01:04.160, Speaker A: It's based on an open source project that we had going for quite a while called the Ether Cattle Initiative, which we'll talk about both in a minute. But Rivet has a strong privacy policy. We will not sell your data. We will not sell your users data. We sell RPC services. And we don't even track the information we get from our users, except, obviously, the transactions that they tell us to send off to the network. And Ribbit is designed for operational maintainability and horizontal scalability, which lets us hit a fairly low price point.
00:01:04.160 - 00:01:35.000, Speaker A: So we're going to walk through the sign up process real quick. If you go to Rivet Cloud slash signup and enter the promo code Boston, we'll get to the promo code part in a minute. So hey, guys. So again, we're talking about Rivet, rivet Cloud slash sign up. We have two plans. We have the build plan, which is a free plan. All we need is your organization name and an email address.
00:01:35.000 - 00:02:21.020, Speaker A: Then we have our scale plan, where you'll have to put in a credit card. The build plan gives you 500,000 free RPC requests a month. The scale plan also gives you that 500,000 free every month. But then you have essentially unlimited RPC requests at one dollars per 100,000 requests. So for now, we're going to go through the process of setting up a build plan. So we're just going to call this demo. So that's going to send me a confirmation email, which I'm going to have to jump over here and buy my confirmation email.
00:02:23.870 - 00:02:34.160, Speaker B: And while he's doing that, one of the key important points here is that the endurance rate plans right now do rate limiting for some of the lower level accounts. And we don't do any of that.
00:02:36.290 - 00:03:12.294, Speaker A: So, again, you put in the promo code Boston, and you will get free RPC requests all weekend. And now we're logged in at the dashboard and we have an API key that is working on the Rivet network. So we can copy the endpoint URL just by clicking this button. It's now on our clipboard, and you can drop that into your application. Or real quick, we're going to go through the process of setting up MetaMask to use Rivet instead of its default. So we're just going to call this mainnet Rivet Drop. In the URL chain.
00:03:12.294 - 00:03:54.154, Speaker A: ID is one symbol is ETH. And now we're using Rivet instead of the defaults. And if we go to a DAP like Fling is a DAP that Open Relay wrote a while back. You can see here it populates quickly with my ETH balance. If I go down to Ambigu, we can see my balance can set an allowance. So now we're going to Rivet instead of MetaMask's default. So back over to back over to the presentation again.
00:03:54.154 - 00:04:30.034, Speaker A: Promo code, Boston gets you free RPC request for the weekend. So how does this work? Behind the scenes? With traditional Ethereum nodes, you have a peer to peer network with a bunch of people running the Ethereum client on their network or on their local system. And it's a peer to peer network. So all the peers share information with each other, but all of that information has to be verified. It's not like Facebook where you go contact a server and they send you information. You trust it because it's their server. All of these peers are sending you information that you have no reason out of the gate to assume is correct.
00:04:30.034 - 00:05:17.570, Speaker A: So you have to verify that information through some computationally insensitive processes that can eat up a lot of the CPU for a server that's running one of those nodes. So when you want to run DApps, you connect your DAPs, whether they're mobile DApps or something running on a server or something running on a desktop. Those connect to your node using the RPC protocol. Each node has fairly limited capacity. On a typical computer with, say, two cores and eight gigs of Ram, you configure on getting somewhere between 150 and 250 requests per second off of a single server. And that single server is also a single point of failure. So if that server goes down, none of those DApps can connect to the blockchain.
00:05:17.570 - 00:06:14.630, Speaker A: So real quick, what are RPC queries? RPC queries allow DApps to get Ethereum balances, read contract code, get token balances, find out what color your cryptokitty's eyes are, transfer tokens, monitor asset transfers, dex trades, basically any information that is there on the blockchain. If you want it, you're going to run RPC queries to get it. So we talked about some of the challenges of you just have that one server with limited capacity and it's a single point of failure. So if you want to solve those two problems, you can load balance across multiple RPC nodes. But this still has some problems. So each node must verify data from its peers. So here now we're running two nodes, and each of those nodes is spending a decent chunk of its CPU verifying information that comes from all of these other peers.
00:06:14.630 - 00:07:01.990, Speaker A: You can possibly get inconsistencies because this node might get a block a full second or two before this node gets that same block. And so if you're sitting here running queries against those two nodes, you might ask this node a question and it might resolve a block number N. And then a second later you ask this node a question and it resolves a block N minus one. So you're getting inconsistent information just by load balancing across a pool of nodes. The other problem is that if you have a sudden surge in capacity. New instances can take hours to sync. Even if you have a snapshot that's, like, taken in the last 24 hours, it can take a few hours to find enough peers, grab information off the blockchain, and write it out to local disk.
00:07:01.990 - 00:07:47.506, Speaker A: So this is not a great way to scale up an Ethereum infrastructure project. So Rivet's solution, and this is part of our open source Ether Cattle Initiative, is we introduced replications. So the basic idea is that you have one master node that connects to peers and verifies data from the peers, and we can actually support more than one master. You need one for this to work, but you can have more than one for redundancy. So that, again, you don't have that single point of failure at your master. And then the data is streamed through a system called Kafka to servers that we call Replicas. Now, the Replicas, they're not verifying the information.
00:07:47.506 - 00:08:40.798, Speaker A: They trust the master because they're operated by the same people. And they just pull down the write operations that were sent to them from the master that gets written to their local disk. And then they're able to serve the RPC requests off of their local disk. The benefits of this are that Replicas don't have to do the verification overhead, so they can dedicate 100% of their resources to or 98% because they got to pull stuff from Kafka and write it to disk. But they can dedicate the vast majority of their resources to serving RPC requests. So where we could get 150 requests per second before we started seeing an increase in response time on a conventional node, we could get about 850 requests per second on a Replica node with the same hardware. It also means that we have minimal inconsistency across pools and Replicas in a pool.
00:08:40.798 - 00:09:21.518, Speaker A: Since they're pulling from the same Kafka server, that Kafka server is going to notify them, as soon as information is ready, they write it out to their local disk. The inconsistencies between two servers might be about 20 milliseconds as opposed to upwards of 2 seconds that we would see with two individual servers pulling block information. And the other great benefit of this is that Replicas start in minutes. So at Rivet, we take snapshots every 24 hours. And then that means that we're pulling up to 24 hours worth of snapshot data from Kafka. All we have to do is pull that data and write it to disk. It's a sequential disk read from Kafka.
00:09:21.518 - 00:09:50.806, Speaker A: It's a fairly simple write to our Replica servers. So we can be up and running within about five minutes. If we need to add capacity, we can add 850 requests per second in about five minutes. And we can add more than one Replica in that five minute period. So we can scale up as quickly as we need to. Just about as far as testnets, we support Gorely and Robson and Rinkbee. Unfortunately, we don't support covan.
00:09:50.806 - 00:10:47.854, Speaker A: Covan is only supported by the Parity client, and Rivet is based on GEF. Our streaming replication system works with a light touch fork to geth we keep it up to date with the mainstream branch, but we have not made similar changes to Parity, which would be required to support Covan. We don't have any immediate plans to do so, but if we get a whole lot of feedback that Covan support is absolutely critical, that could be in our future. We have just, in the last 24 hours, gotten WebSocket support up and running. You won't find that on the dashboard yet. We are not confident enough in our solution to say it's a production ready feature. But for hackers in East Denver, if you want to give it a Whirl, you can plug in East WS Rivet Cloud as your endpoint and put your API key after that.
00:10:47.854 - 00:11:30.780, Speaker A: And you can connect to WebSockets and give it a Whirl. So the benefits of WebSockets are that you can get immediate notification of new blocks, notifications of things like token transfers, or really any other on chain events. You can be notified of that instead of having to pull periodically to see if something new has happened. So here we are in ETH Boston. As sponsors, we are sponsoring a prize for the best project built with Rivet. Really, just about any hackathon project out there could use Rivet as its RPC provider and be eligible for this prize. We are offering 100 million RPC requests for as long as it takes you to use those.
00:11:30.780 - 00:12:12.040, Speaker A: We are going to consider prizes, take into consideration our values when evaluating the project. So we're looking for something that hits the values of open source. And also, we're very privacy centric. So something that is privacy centric is going to get bonus points. So, again, sign up at Rivet Cloud slash, sign up the promo code. Boston get you free RPC requests for all weekend. And we have a dev resources page here where we have documentation on how to use browser plugins, depending on what language you're using.
00:12:12.040 - 00:13:01.720, Speaker A: We've got documentation for different libraries in different languages. One thing to note real quick here is this documentation includes your API key. So you can copy and paste these snippets and it will just work for you. Then we've also got some documentation on the various test nets. We talk about proof of work versus proof of authority. We give a link to a faucet so you can get started with some testnet ether on each of these testnets, as well as some other parameters of these testnets and our recommendations. So these development resources are out there for anybody hacking today, trying to figure out how they use this stuff.
00:13:01.720 - 00:13:13.340, Speaker A: Any questions? All right, yeah.
00:13:13.710 - 00:13:28.400, Speaker C: So you say privacy in particular is stuff that you'd like to see. I wonder, is there a way that you guys are trying to investigate that client side? I'm obscured from what you guys are seeing my calls for. So you couldn't identify a particular.
00:13:32.070 - 00:14:03.100, Speaker A: There'S been a fair bit of conversation at ETH Boston about don't be evil versus can't be evil. Right now, this is a service that people need. So we're taking the don't be evil approach. We are exploring a number of options for the can't be evil approach. So right now, we are recording how many requests we got from a given API key, but that's it. We're not keeping track of IP addresses. We're not keeping track of what the request was for.
00:14:03.100 - 00:14:29.314, Speaker A: Right now, you kind of got to take our word for that. We've got it in our privacy policy. You could sue us if it wasn't true. I know. These are parts of the conventional ecosystem. Again, we are exploring options for how we could make those things technically impossible for us to do, as opposed to us just asking you to trust us that we're not. But we didn't want to wait until we could answer that question to put the service up.
00:14:29.352 - 00:14:29.940, Speaker C: Yeah.
00:14:35.270 - 00:14:55.420, Speaker B: And to be fair, no one else in the space right now, privacy, according to their policies, is not important to them. Right. And so that's really what's critical for us is that we're taking that stance. Right. Because one of the competitors in the space actually is their other business is business intelligence. So they're taking that data and actively mining it.
00:14:56.110 - 00:14:58.170, Speaker C: I wouldn't be surprised. It's a pretty big name.
00:14:58.320 - 00:14:59.340, Speaker B: Yeah, it is.
00:15:02.910 - 00:15:16.980, Speaker C: Like paying for this. That makes a lot of sense because you guys are not want to be equal. I also wonder you said it's open source. So, for instance, if I did have a server cluster that I wanted to run this on and could manage it right?
00:15:18.150 - 00:15:50.090, Speaker A: Yeah. We started this out. Rivet is a product of open relay. And so we run an open source zero X order book, and that goes back to over two years ago that we started working on that order book. And one of the big challenges that we found in running this whole system, we built it with a microservices architecture. Any piece was disposable, it was all redundant. If a server was misbehaving, we just kill that server and the auto scaler replaces it, and everything comes back up and it's happy except for our node infrastructure.
00:15:50.090 - 00:16:20.694, Speaker A: And so we wanted to get there with our node infrastructure, too. And by the time we implemented all the stuff I just talked about, in order to have that, you got to have a Kafka cluster, which is redundant. You got to have multiple masters, you got to have multiple replicas. And by the time you're there, you can handle 80 million requests a day. And we're using a few hundred thousand requests a day. So we've got all this extra capacity. So, yes, absolutely.
00:16:20.694 - 00:16:52.640, Speaker A: We have an open source project if you want to run it on AWS. We have cloud formation templates. You can deploy it with a few clicks, but the cost of that is several hundred bucks a month. And then you have capacity that's way more than you need. So that led us to think, well, we've got all this extra capacity, we might as well set up a business around that. And the competition right now is Alchemy, that you can't just go sign up for. You've got to talk to somebody and talk to them about your use case.
00:16:52.640 - 00:17:47.330, Speaker A: Then there's infuria which when we started working on this was free, but we knew they weren't going to be free forever. And we also knew from some of the other projects that we've worked with that even when they were free, they weren't free for everybody. And when they became not free for you, you didn't know when you were going to hit that limit or how hard it was going to hit you. And of course, they've announced a pricing plan. The first three months are free, but next month their charging starts and our one dollars per 100,000 request price point is going to be better for about 80% of users. If you're in the top of a tier and you can make sure that you don't go over and need to go to the next tier, their price point will probably be better for you. But if you're anywhere near the bottom 75% to 80% of a tier, our price point is going to come out better.
00:17:47.400 - 00:17:50.590, Speaker C: Makes sense. And you said right now it's through credit cards.
00:17:50.750 - 00:18:03.814, Speaker A: Yes. Again, being able to take crypto for this is a high priority, but we didn't want to wait to put this out there until we had that.
00:18:03.852 - 00:18:08.506, Speaker C: I'd be really excited to see micro payments through payment channel.
00:18:08.608 - 00:18:47.734, Speaker A: Yeah, that is an idea we've kind of been exploring. I think initially we will probably take larger chunk payments. One of the challenges is that the RPC request protocol doesn't include any mechanism for doing a microtransaction with that RPC request. But we're also exploring some other potential protocols where maybe there's a Web Three provider that you could drop into your JavaScript application that could do that, but it's not just using the straight RPC protocol. Yeah. Cool. Any other questions? Yes.
00:18:47.932 - 00:19:23.762, Speaker D: So back on the privacy stance thing, kind of reminds me of some of the email provider horror stories from the Snowden area. So do you have any sort of strategy in mind for how you're going to protect yourself if a government comes after you, for instance, where they would be far more comfortable with you working in a conventional manner, where you take more of that data so they can turn around and compel you to hand it over? Do you have a plan?
00:19:23.816 - 00:19:27.300, Speaker A: Greg, do you want to speak to that? Yeah, Greg's our legal mind.
00:19:27.670 - 00:19:46.140, Speaker E: I used to practice law and one of my old hats and business litigation. Of that I can promise that we will fight tooth in that, and we will die on our sword if we have to. I'd rather shut the business down than have somebody be taken advantage of, have anybody be taken advantage of by a nation state.
00:19:47.550 - 00:20:19.538, Speaker A: And one idea that we have tossed around but not yet implemented is putting out some kind of dead man switch. Canary. This is something I've always thought would be a great use for the blockchain, where we could go out and make an RPC call, make a transaction once a week to say we haven't been served with any warrants or something along those lines. And if you just don't see that, then maybe we can't tell you about it.
00:20:19.704 - 00:20:21.140, Speaker C: But it's there.
00:20:21.830 - 00:20:25.960, Speaker E: The other piece of it will be down the line when we can.
00:20:28.090 - 00:20:28.594, Speaker A: Engage.
00:20:28.642 - 00:20:40.246, Speaker E: In some more philanthropic exercises. Then we will be financing things like right to privacy, campaigning as much as we can. It's something that's really kind of deeply.
00:20:40.278 - 00:20:41.450, Speaker C: Ingrained in all of us.
00:20:41.520 - 00:20:52.830, Speaker E: All three of us have been have been. I mean, I think that's true of a lot of people in the space, but a lot of the reason we came to even be here is because of those kinds of ideas.
00:20:54.210 - 00:21:13.460, Speaker C: So one thing I really want to see in this space and I know a lot of people be hungry for, is effectively a private, completely discreet Block Explorer. Like, for instance, if I'm a whale and I want to check my balances and not let anybody know that I have any access to these accounts or care about them at all, right, that'd be something I'd really like to see.
00:21:14.390 - 00:21:16.754, Speaker A: Like a tour based block explorer. Yeah.
00:21:16.792 - 00:21:28.700, Speaker C: So I think hosting is a Secret Service, but then also obscuring from the Secret Service itself, those calls that I'm making. So you don't even know who I am or what I'm looking for. I've been thinking about this, and you can talk.
00:21:31.150 - 00:22:00.610, Speaker A: One idea that we're exploring for a number of reasons. I mentioned that we've got this kafka stream of where our master writes it and our replicas read it and write it out to their local system. One thing we've been exploring is exposing that to customers. So you could just run one little replica. A replica could be a small light server pull down from our kafka stream, write it out to your local system. We have no idea what you're getting. You're getting everything.
00:22:00.610 - 00:22:31.414, Speaker A: We have no idea what you're using. Right. Of course, just to do a quick balance check, that's pretty extreme overkill. But if you're running an application and maybe not in a position to host the full infrastructure yourself, but want to get that level of privacy by taking our kafka streams, you can get that level of redundancy that you need. But all we know is you're taking the same information as everybody else who's taking our kafka streams.
00:22:31.462 - 00:22:40.674, Speaker C: Yeah. My guess is that's a pretty heavy data load for you guys. So it might be a reasonable monetary incentive to people out.
