00:03:53.880 - 00:04:38.916, Speaker A: Good morning, good afternoon and good evening everybody. My name is Kartik, one of the co founders of Eevee Global, and want to welcome all of you to the Polygon ZK Day. You are all watching this thing on Eepglobal TV, so just a quick reminder for everybody who's coming in for the first time, we encourage all of you to log in and say hi to all of us. This is the live platform we're going to be using for the rest of the day. And if you have any questions for our speakers or if you have any comments you want to relay to any of our audience or our speakers, you can write them in the chat there and we'll be able to relay them directly to our speakers. And also, for everybody who logs in, you will be getting a NFT POAP token, so stay tuned for that after the end of this summit. So this event is brought to you by ETH Global.
00:04:38.916 - 00:05:02.272, Speaker A: And for those of you who don't know, ETH Global is an organization with a very simple goal. Our goal is to get more web3 developers into this ecosystem. And we do this by running hackathons and summits. And this is our last event of the year, and we're super excited to be hosting this day talking about all things CK. So let's get into what today is going to look like. So we've got an amazing day planned. We're going to talk about a handful of things.
00:05:02.272 - 00:05:50.124, Speaker A: We're going to do a quick update on what is happening in the Polygon ecosystem. And with respect to zero knowledge world, there's going to be an exciting announcement from the Polygon team, which I can't wait for everybody to also find out about. Then we're going to go into a few demos and talks on how Polygon, Nightfall, Maiden and Hermes are set up. So we're going to go into the polynomial identity language by the Zkvm team. And then lastly, we're going to have a panel discussion with all of our speakers and vitalik, Buterin to just cover what we can do to move this entire ecosystem forward. So, without further ado, let's jump right in. I want to invite Sandeep, Genti and Mihalo, the founders of Polygon, to come on stage and share some of their thoughts.
00:05:50.124 - 00:06:09.220, Speaker A: And without further ado, let's bring all of them on to the screen. Welcome, everybody. Thank you, Karthik. Hello, everyone. Hello, Sandeep. Jerry, do we have also yes. Awesome.
00:06:09.220 - 00:06:46.564, Speaker A: So yes, mihalo, you're audible. So, yeah, let's begin with a small intro from all of us. So, I'm Sandeep, I am one of the co founders at Polygon and I look at the business side of things. That means the business development, marketing, and all the things related to JD. Hi. Jayanti here, founder of Polygon, started out of Matic here, started tech and looking after the tech and research nowadays. Hello, everyone.
00:06:46.564 - 00:07:32.892, Speaker A: I'm Kylo, one of the co founders of Polygon. I'm mainly interested in scaling in our research effort, strategic side of things. And yeah, thank you so much for attending this event in such a large number. We are very humbled and honored to see you guys. You know, let's start this event off with basically brief about the purpose of polygon and then we can move on to different things. We'll walk you through our timeline and about our GK things and things like that. But the talk or the discussion about Polygon cannot start without the discussion of Ethereum.
00:07:32.892 - 00:09:07.920, Speaker A: Right? We wanted to discuss about first what is the core thesis of polygon here, what exactly polygon is trying to achieve here, right? So basically this whole web3 space and and what blockchains are trying to do is to provide basically this internet of value, right? Web two was Internet of information and web3 is basically internet of value. And our strong belief is that Ethereum has strongly emerged as the value settlement layer, which is highly decentralized, highly secure environment for the value to transact globally and probably even beyond the globe once human race starts growing multiplanetary and things like that. But our belief is very strong that Ethereum is that settlement layer. And then on top of that, once web3 starts expanding into the mass adoption, we believe that the layer two solutions or secondary layer solutions on top of Ethereum will be the avenues where the end users will interact, where the business activity will happen, but all of these different avenues will connect back into Ethereum. And that's the core thesis of Polygon. And it includes other ecosystem projects of Ethereum also. But at Polygon we want to truly be one of the most dominant player in the transaction layer.
00:09:07.920 - 00:10:13.168, Speaker A: Similarly, many a time people ask that, okay, what solutions Polygon is providing, right? You might have heard that debate of whether it's a layer two, it's this and that. First of all, most of the people, whenever they are talking about this, they are talking about one or the other solution of Polygon, right? But as a whole, what exactly is Polygon is that if you see what we believe is that Ethereum scaling is a spectrum. We don't believe in a narrow definition of layer two. We believe that all the different kinds of different flavors of solution add value to ethereum, add scalability to Ethereum and add network effects to Ethereum. So this is a spectrum that we believe that Ethereum scalability can happen. And you can see across this spectrum on the left extreme you see standalone chains where the security by Ethereum is very limited. And on the right extreme you see fully secured layer two chains which have both their data as well as dispute resolution.
00:10:13.168 - 00:11:13.636, Speaker A: Everything going back to Ethereum, which is mentioned in this arrow with DA and dispute resolution and data availability. So Polygon wants to be present across this solution spectrum. On the left extreme you have Polygon SDK, which provides you capability to have these EVM compatible sovereign chains. Then you have Polygon POS which is like kind of in the middle. It is actually a chain proof of stake chain built on top of Ethereum and it is currently one of the most popular chains. And many of you guys might know that Polygon solutions have the dau even higher than currently the Ethereum main chain and many of the users are using and it's undoubtedly the highest used platform globally right now. And then as I was talking about the spectrum on the right spectrum you have the zero knowledge solutions, zero knowledge roll ups and EVM compatible roll ups which are being built by various different solutions.
00:11:13.636 - 00:11:53.492, Speaker A: And then Polygon avail as a data availability solution which E 2.0 data shards also are going to be when it comes they can give birth to validium solution. So with that base in mind that what exactly is Polygon trying to achieve? Let's go through the journey of Polygon and I would request JD to take us all through the journey of Polygon and then move forward from there. JD. Yeah. Thanks, Sandeep. So journey started with the idea of Idea to scale Ethereum in late 2017 when I was exploring multiple projects and the same crypto kitty happened at the same time.
00:11:53.492 - 00:13:12.276, Speaker A: And then we realized that there is a need for the scaling solutions for the Ethereum. And at the same time we started coding about the Matic and founded Matic around 218. We made it official, sandeep and Rag and I made it official that we are going to kind of build Matic EVM based scaling solutions. At the time there were few scaling solutions, but they were so complex that people, the developers not going to use that properly or if they use that there won't be a tool after the production, after the maintenance, right? So we thought EVM based chain would be better suited here for the developers to use and move from Ethereum to that specific chain in five minutes or ten minutes. Simple, easy to migrate. And then we kind of did our IEO in 219 on finance. It was very successful IO so far on the finance, as you can see, June, after multiple iterations, DevNet and testnet, we launched Main Net in early June of 2020.
00:13:12.276 - 00:14:30.570, Speaker A: And since then we have been grown so well that top games onboarded end of 2020. In February, Mihilo joined between late 2020, Mihalo Joined is a co founder, became Polygon with Idea with a vision to have a multiple kind of solutions instead of one monolithic kind of POS chain solution. And you can see after 2021 Ave major D Five protocols as well came on Polygon, Susie Swap Away and multiple others. Right? In May 2021 Mark Cuban invested in us and we reached 10 billion marks in a market cape. June 2021. SDK, we launched studio. Studio has been successful in our journey and we invested like many NFT projects, gaming projects so far helped them to launch them and it has been successful so far.
00:14:30.570 - 00:15:28.410, Speaker A: In August 2020, we announced Hermes Network and we are building EVM based CK roll ups. We also launched 1 billion grant for 1 billion commitment from our side for the CK based Technologies Scaling Solutions. Q Three EY came on board with Nightfall, everything in mind and privacy in mind and Q Four 2021 we just announced like a few days back we announced my polygamide with Start based EVM solutions, ZK Solutions. And one more thing, we are going to do that, but after some time wait for that. You to mihilo. Thanks JD and thanks andeep yeah. So let me briefly cover what we call the polygon ZK thesis here.
00:15:28.410 - 00:16:39.568, Speaker A: And this thesis, basically the blog that covers it in more depth is published several months ago and basically we keep publishing it with every ZK related announcement and I guess we can share it also after the event or during, but let's just briefly cover it. And some of the things that are part of these EQTs are relevant to what Sandeep and JD said. So I'll just try to run through these things as quickly as I can. So, as Sandeep said, rightly, of course we believe Ethereum is our best chance to build this global network of value and web free. We are fully convinced in that right now. And we are considering polygon an integral part of the Ethereum ecosystem, of course. And this whole promise of this global permissionless internet of value is really amazing and really supposed to complement this internet of information that we have been creating for the past decades and create a huge value add basically for the humanity, if you will.
00:16:39.568 - 00:17:38.260, Speaker A: And we are fascinated by that idea and we decided to pursue it and commit all of our resources to help reach that amazing and grand vision. That being said, again, Ethereum equals web3 in our minds, but we need to scale it first of all. So let us first cover where polygon was like polygon beginnings and polygon where it is now basically, and the wider context of the industry, I guess when we started versus where we are now. So when we started, as I said, there was this pressing need to scale. The Ethereum minute was heavily congested. Ethereum user base started to kind of deteriorate in a sense that alternative chains started popping up with some normally trade offs and offering maybe additional throughput and users started leaving and whatnot. So we understood there is this really pressing need to scale Ethereum.
00:17:38.260 - 00:18:58.956, Speaker A: At the same time, scaling Ethereum was kind of a niche topic which was very relevant in the Ethereum community, but in the wider context of the crypto industry, it wasn't that visible, or premium at least. And there were a lot of opinions, maybe even coming from VCs or influential people, that it is not really favorable to focus on scaling ethereum and positioning yourself as a project on top of kind of ethereum instead of maybe competing with ethereum. And it was kind of an accepted opinion that can bring bigger benefits in terms of, I don't know, value capture and things like that. On top of that, it's really important to understand that it is very hard to build, quote unquote, perfect solutions for ethereum scaling because Ethereum as a community really does not accept compromises when it comes to these core values or core engineering principles which are mainly decentralization and security. First of all, on top of it we have EVM as the standard that is kind of already established. All these things or requirements combined together make it pretty challenging to build kind of perfect or ideal solution or solutions. On top of that.
00:18:58.956 - 00:20:33.452, Speaker A: Polygon when we started, we have really modest resources, as JD already said, there was this small crowdfunding campaign and even though the treasury was managed really well, we still basically because market cap was very low, we had really modest resources at our disposal. And where we are now like ten months later, I guess or so since we announced Polygon, we are now by far the most adopted scaling platform in the industry. And moreover, apart from Ethereum, we are very likely, to the best of our knowledge, the most adopted blockchain network or platform in the world apart from Ethereum, which is, I guess we all agree, a huge success in such a short period of time. One additional thing that significantly changed since then is that scaling now is kind of a mainstream and everyone's aware of it and fortunately everyone's aware of Polygon now. And we think with Polygon we managed to set an important precedent in a sense that I think we kind of proven that we managed to prove that it's possible to build on ethereum with ethereum on top of ethereum and still capture value and still add value back to ethereum instead of competing kind of with ethereum. So we think this is one very important thing that will maybe set an example for future founders to think about when they're starting their projects. As a consequence, I guess, of this high level of adoption that we have seen.
00:20:33.452 - 00:21:33.384, Speaker A: So we right now have more than 3000 applications deployed on Polygon. We have more than 100 million user wallets, we have processed more than 1 billion transactions. We have more than five or 6 billion even now in user funds, completely organic without any incentives from our side. This huge level of adoption has subsequently led to the growth of the price of the token. We're not here to talk about the price, of course, but I'm just saying that subsequently increased dramatically resources, financial resources that we have at our disposal and that of course put us in a much better position for what we are about to do in the next chapter of Polygon. And now we can sandeep, if you can go to the next slide please so we can actually get to the thing. So how did we achieve how did we achieve this success and this growth in such a short period? We had clear understanding and we still have that.
00:21:33.384 - 00:22:37.760, Speaker A: We have two major focuses that are driving us and driving everything we do at Polygon. One is shipping and the second one is innovation. Innovation because we are still in very early stage when it comes to building infrastructure for web free and this internet of value. So right now in Ethereum we have millions of users but we really want to onboard 1 billion plus or even the whole world. Which means we need to scale 1000 X from this point without again, as I said, sacrificing these major principles major design or engineering principles of Ethereum to do that a lot of innovation it still has to be done. A lot of work on that front is ahead of us and we understood it and we are willing to accept it as one of our focuses and the other one is shipping. For us, we role, I guess, engineers by education and training and we are shipping oriented and for us, unfortunately, innovation without shipping is just an academic basically exercise.
00:22:37.760 - 00:23:30.656, Speaker A: So yes, starting from there and realizing this pressing need to scale, we realized that we for now at least have to be very focused on shipping. In this first period, as JD mentioned, we delivered Polygon POS which turned out to be a huge success. We intentionally offered it as a solution that has certain trade offs. Not a single solution is perfect, of course, but we just realized that the community really needs solution today. EVM compatible solution today without any delays. That was a very good decision in hindsight. And after all this growth and considering where we are now, which I just covered we believe at Polygon now we need to focus even more and commit.
00:23:30.656 - 00:24:17.510, Speaker A: We are in a great position to commit much more resources, all sorts of resources, financial, human resources, et cetera to innovation. I think something happened with the slides. If we can just go back maybe to where we were. So yes, we are focusing back on innovation without losing focus on shipping. Shipping is still one of our of course major, major focuses but we just believe that we are now in a very good position and we are thankful for that and we want to commit significant resources basically to the innovation activity happening on Polygon. If you can just get back the slides that would be great. I think something happened.
00:24:17.510 - 00:25:14.400, Speaker A: Just a moment, just give me one SEC. There seems to be some issue with my this one's. Sorry for this disruption guys. Okay, are we back? Guys, can you see my okay, okay, awesome. Mihalo, you can proceed. Just a moment. I tried to open the presentation myself.
00:25:14.400 - 00:25:48.086, Speaker A: Okay, yes, excellent. Okay, sorry for these guys. Okay, so speaking of innovation and our focus now on it. As many of you will know, there are basically two major families of innovative solutions built on top of ethereum, which are commonly called layer two solutions. One group are fraud proof based solutions, which are the most prominent ones currently at least are optimistic roll ups. And plasma is also part of that same family of solutions. The other one, the second group, are ZK based solutions.
00:25:48.086 - 00:27:10.770, Speaker A: As JD said, we all started researching and experimenting with these solutions pretty early in 2017, I guess ever since plasma was introduced. And that's basically how JD and I met in the first place. And throughout all this time we analyzed all these solutions and we came basically to a conclusion that ZK based solutions are probably, I don't want to say the way to go, but we definitely became biased towards this family of solutions because of several major reasons. We don't have, I guess, the capacity or the time to go into all the details and compare these two families. But in a nutshell, basically, ZK based solutions offer significant advantages in terms of scaling, first of all, in terms of security, first of all, formal security, provable security, and in terms of user experience. And these three things are, as we, I guess all can agree, extremely, extremely important or critically important when building scaling solutions. That's why we decided that ZK basically is a strategic resource for us moving forward and that we want to significantly focus and commit serious or significant funds from our treasury to experiment with these exciting solutions.
00:27:10.770 - 00:28:34.142, Speaker A: So ZKPs, or zero knowledge proofs or zero knowledge cryptography is basically, I would say, a fascinating field of cryptography. It was introduced in mid eighty s and it didn't really have a lot of applications until blockchains came by. Zero knowledge cryptography, sorry, uses very interesting cryptography primitives and methods to prove certain statements, normally without revealing most of the parts of the statement and normally or very often in a very succinct way. So it's possible to, with very little computation, potentially prove huge amount of computation or large amount of statements or transactions if you will. And it became obvious along the way when blockchains were introduced and when the scaling pressing needs to scale became obvious. It became also kind of obvious that ZK and their special properties might be really interesting tool basically to address these major challenges. That's why we have seen in the past, I guess, two years, we have seen a true Cambrian explosion of innovation in the ZK field.
00:28:34.142 - 00:29:19.742, Speaker A: A lot of new protocols, a lot of new primitives, a lot of new projects, and we are very happy to see that and we are even more happy to be able now to seriously join this evolution, basically of ZKS. As I said, we decided ZKP is our strategic resource for us and we decided to support this effort and innovation in this field. With 1 billion fund or 1 billion commitment from our treasury. If we can just go to the next slide. Okay? And just I want to just briefly, briefly cover what we have done so far before I hand it over to Sandeep again. In our ZKR scenario, we already have. So we announced this ZK thesis, I believe, three or four months ago, something like that.
00:29:19.742 - 00:29:54.902, Speaker A: And in the meantime, we are very proud and happy that we have already significant results from this commitment. And so far we have three active projects, three very exciting projects under the Polygon umbrella. First one is Polygon Hermes. As JD mentioned, it is basically EVM compatible roll up. One interesting thing, or specific, or let's say two interesting things about Hermes, out of many, are that Hermes is the first decentralized rollup. So rollups are currently normally centralized. In a sense, they normally have centralized operator.
00:29:54.902 - 00:30:38.258, Speaker A: Hermes team has built the first and only, to the best of our knowledge at this moment, decentralized rollup. And they are now building Zkevm, which is basically the re implementation of the Ethereum virtual machine in a ZK friendly manner, so to say. And we are very excited, of course, about this effort. Polygon Nightfall is the second project. It is done in collaboration with people from Evi. It is basically a continuation of their work on Nightfall, the library for privacy that introduces privacy on Ethereum mainly targeted towards enterprises. We are now building this interesting hybrid roll up, basically, which is the hybrid of optimistic and ZK roll up, which is privacy focused.
00:30:38.258 - 00:31:35.580, Speaker A: So this is the first application of ZKS targeted towards privacy. So that's the main reason, I guess, why we're excited about this. And the third one that we have announced so far is Polygonmiden. Polygonmiden is a Stark based effort, which is basically a generic ZK or Stark based roll up, which is going to be Ethereum compatible. So, in a sense, you can draw some parallels, I guess, with Starkware effort between these projects, with the difference that this project is fully open source, it's mainly community driven, and there are some, of course, differences in the architecture, which we find very interesting, but we don't really have the time to cover those differences. Now. What is very important for us, critically important, that as you can see, there are already free projects here.
00:31:35.580 - 00:32:20.498, Speaker A: And some might ask, okay, how will these projects interact? Will they be competing with each other? What is your idea there for us? We are extremely happy that we managed to establish a really good atmosphere between these projects. We are all aware that this is again, very early stage of innovation. These projects are working very closely together. They are constantly in touch. I mean, we are all constantly in touch. We are exchanging knowledge updating each other, helping each other, and we are moving towards together, towards this hopefully brighter future of scaling. That being said, this arsenal is already very powerful.
00:32:20.498 - 00:33:17.260, Speaker A: I would say but it might even be better or might even complete the puzzle if we just can add something more to it. So what can we possibly add? I will leave it now to Sandeep. But can we imagine that we have the fastest ZK scaling technology in the world that in some way would probably complete or really add a lot of value to this arsenal that we have now. And yeah with this I will just leave it to Sandeep please. Yep. Karthik you want to bring up your part of the presentation for the announcement at least the placeholder. So yeah, I mean guys while Karthik is bringing that.
00:33:17.260 - 00:35:07.770, Speaker A: So basically as Mihailo mentioned about our ZK thesis and why we feel that the announcement that we are going to do today is very big is that there are a few ZK solutions out there. But one of the biggest problems with ZK solutions have been always about performance. And the second thing is that because it requires a large amount of resources for ZK provers to create ZK proofs, it makes it essentially very hard to decentralize. Right. So imagine like as Mihalo was saying, that if we had a very extremely fast, probably world's fastest ZK recursive proving technology and extremely efficient also which could run there, where you could run, ZK proves on a simple machine like a laptop. That would be the game changer for this industry, where a large number of nodes can be run in a network, which can validate the transactions and things like that, which will actually truly make or truly enable us to create decentralized layer two network for polygon. For that you I'm extremely happy to announce on behalf of our entire team everybody is that Polygon is very excited to welcome, excited to welcome Polygon Zero which will be led by a really brilliant team of cryptographers and engineers, primarily Brandon and Daniel.
00:35:07.770 - 00:36:14.610, Speaker A: And they were previously doing it as Me protocol which has now decided to join Polygon and focus on ZK scaling. Previously they were building like a ZK enabled chain but now they have decided to join Polygon and build a decentralized ZK roll up on top of Polygon. So with that I will request Brandon and Daniel to take the stage and take us through. But yeah, for Polygon community we are extremely happy that this essentially now puts Polygon right in the lead in this entire ZK scaling effort. We truly believe that ZK is the ultimate frontier which can bring in internet level scale to blockchains. And with Me team also joining already an amazing suit of solutions on Polygon, this puts Polygon in the lead globally in terms of the ZK solution providers. So over to you Brandon and Daniel.
00:36:15.510 - 00:37:00.900, Speaker B: Hey everyone, I'm Brendan, I'm one of the co founders of Mir. You may not have heard of us but hopefully we can give detail on what we've been working on. And so I'd just like to say that we're really, really excited to be joining Polygon. Obviously, Polygon has had incredible success in attracting users and developers. But I think what's really impressed us is their vision for the future and their commitment to ZK scaling. And so over the past few months, we've had the privilege of working with Hermes and with Maiden and it's already been a really fruitful collaboration. And we're really, really excited to be Polygon Zero.
00:37:00.900 - 00:37:52.926, Speaker B: I think on behalf of me and Daniel, we're also really excited to be coming back to the Ethereum community. Ethereum is how we both got into crypto in the first place. And it's a privilege to work alongside so many great teams to scale Ethereum. So I'm going to give a little overview of what we've been working on and then I'm going to hand it over to Daniel and some of our team members to give a little bit more technical detail. But I'd like to open with this. So, in the history of computing, there's this thing that happens, which is when you increase computing speed by ten x, you unlock radically new applications. So this has been true across platforms.
00:37:52.926 - 00:39:19.898, Speaker B: So for the PC, for gaming, for mobile, every time you deliver processors that are ten x faster, the space of things that you can build and the possibilities increases dramatically. I guess from that perspective, we view ZKPs as sort of a new computing platform. And we'd like to pose a question to the Ethereum community, which is what can you do with ZKPs that are 100 x faster? Specifically recursive ZKPs, that will allow us to build things like more decentralized, more performant roll ups, new ways to provide privacy to users. And so we're just going to talk a little bit about how we've done this. What we mean by Recursive Zero knowledge proofs and sort of go from there. So recursion is really important for scalable ZK rollups because as we know, the premise behind a ZK roll up is that we can take a large number of transactions that would be too expensive to verify on the Ethereum main chain. And instead we can process them off chain and generate a succinct proof that shows that all of those transactions are valid.
00:39:19.898 - 00:40:15.390, Speaker B: We can provide sort of the necessary state updates or depending on the data availability scheme. And that allows us to scale transaction throughput while still maintaining the properties that we love about Ethereum, that it's decentralized and that's secure. So the issue with ZK rollups right now is that it's actually really expensive to generate a proof showing that a large number of transactions are valid. This is especially true for ZK rollups that support general applications, and even more so for ZK rollups that sort of support EVM compatibility. And so what recursion allows us to do is instead of taking a single proof that verifies, say, 10,000 transactions, we can instead generate 10,000 proofs that each verify one transaction. We can do. All this in parallel, and then we can recursively aggregate.
00:40:15.390 - 00:41:09.566, Speaker B: And so we're able to generate the final proof that we post to ethereum more efficiently. So we view sort of this breakthrough in recursive proof generation as being really important for the future of ZK roll ups on ethereum. So for some context on how we got here, recursive proofs have only been available in practice since about 2014. Before that, they existed only in academic papers. So when we started Mir in 2019, it took two minutes to generate a recursive proof. And so we realized very early that this would never work if we wanted to provide high throughput and something that could scale. We needed to fundamentally improve proof generation time for recursion.
00:41:09.566 - 00:41:59.940, Speaker B: Because even if we were sort of taking advantage of parallelism, that only gets us so far. This still represents way too much latency for the performance characteristics that we'd like. So in 2022, things happened. So the brilliant team at Aztec developed the first implementation of recursion on ethereum based on plank and the KZG polynomial commitment scheme. So proof times were about 60 seconds on a desktop. And so this was a huge leap forward for roll ups. At the same time, we developed Plonky, which is a combination of Plonk and halo, and it allowed us to achieve 15 2nd proving times.
00:41:59.940 - 00:43:01.330, Speaker B: But the problem was that this wasn't ethereum compatible, so we couldn't use these faster provers on ethereum. And so now I guess we were really happy to announce Plunky Two, which is a new proving system based on Plonk, Fry, and some wizardry from our own team. And so I think it's really important to sort of give credit where credit's due. And this advance is based a lot on really brilliant work from Zach and Ariel at Aztec, who developed Plunk, and then the extremely talented scientists at Starkware who developed Fry. And so this represents a 100 x speed up for recursive proofs on ethereum. So Plonky Two is fully transparent. There's no trusted setup, no toxic waste.
00:43:01.330 - 00:43:42.050, Speaker B: It's natively ethereum compatible. So it takes about a million gas to verify a planky two proof. And this is constant. We could have a recursive proof that verifies a million transactions. It still only costs 1 million gas to verify an ethereum. And so I think what we're really excited about is that this is a big step forward for the space, because now we're measuring recursive proof generation time not in seconds, but in milliseconds. And so what we have achieved is 170 millisecond recursive proof generation on a MacBook Pro.
00:43:42.050 - 00:44:10.140, Speaker B: So on my MacBook air, it's a little bit slower, it's about 300 milliseconds. But this is still a huge step forward for this space. It's the fastest implementation of recursive proofs ever. And it works natively on ethereum. And so actually, I think that I can show this we'll see on zoom if it slows it down a little bit.
00:44:11.150 - 00:44:11.514, Speaker A: Yeah.
00:44:11.552 - 00:45:11.870, Speaker B: So about 355 milliseconds right now. So yeah, we're proud of that. And so all of this is sort of in service to developing Polygon Zero, which is the most scalable Zke EVM powered by Plunky Two. And so we believe that ZK roll ups are going to compete on speed and on cost. And we believe that we're building the most scalable EVM compatible ZK layer two. So the goal is to allow developers to compile their existing solidity code to run effectively unchanged on a Zkvm. And we think that this is really important for Ethereum and for the crypto space in general because our goal is to provide scalability without compromising on the things that make Ethereum so important and so special for this space.
00:45:11.870 - 00:45:33.430, Speaker B: So we view this as sort of a route to scaling throughput without compromising on decentralization or on security. So we're really excited to talk a little bit more about this in greater technical detail. And so I'm going to hand it over to Daniel.
00:45:35.770 - 00:46:14.398, Speaker A: Okay. Thanks, Brendan. So why are we so focused on performance here? Generating these proofs can be very expensive, especially if we're proving something like an EVM program, which is a really conventional design. It wasn't designed for Snarks, so it has features like arbitrary control flow, random access and all kinds of binary arithmetic. These aren't features that are natively supported by Snark primitives. We can simulate them, but there's quite a bit of overhead. So on the right here, I took this benchmark from one of the old tiny Ram papers.
00:46:14.398 - 00:47:08.370, Speaker A: Tiny Ram is this virtual machine that we can prove the execution of. And the authors measured basically the simulated clock rate of this VM. And as you can see, it took about 33 seconds just to prove one cycle of execution. So if we compare this to a real hardware CPU, which would typically run at around 3.3, is a performance gap of eleven orders of magnitude. To be fair, this is an older paper and we do have some modern techniques that can help to narrow this gap. But this is still a big challenge to say the least, especially with the EVM, because this was a 16 bit machine, whereas the EVM is fundamentally a 256 bit machine.
00:47:08.370 - 00:48:10.670, Speaker A: Cool. So when we started this project we wanted to collect some real data about what sort of computations we're dealing with. So we spun up an Ethereum node and we recorded a bunch of blocks and we looked at basically what's going on in terms of for example, we found that the average transaction executes about 3400 instructions in total. Which doesn't sound like that much, but some of these instructions are relatively expensive. So for example, we have the average transaction executes 88 of these and instructions this is a bitwise and involving 256 bits. And on hardware this would be pretty trivial. But in Snarks, since it's not natively supported, we have to either do it bit by bit or we can use lookup tables or other techniques, but there's still a lot of overhead either way.
00:48:10.670 - 00:48:52.180, Speaker A: The other thing that concerned us a bit here is this Shaw Three instruction. The Shaw Three is this Ketchak hash, and it involves hundreds of thousands of bit operations just to evaluate a single hash, and the average transaction does 13 of them. So this might sound pretty negative so far, but it's okay, we can still make this fast. We just have to be incredibly focused on performance. So with that in mind, let's look at some of the proof schemes that we could consider using. We basically considered three options. The first is schemes based on KCG, like Plonc.
00:48:52.180 - 00:49:35.560, Speaker A: And KCG is this polynomial commitment scheme based on pairings. It has some nice properties, it has very small opening proofs, so a whole argument can be less than a kilobyte. It's easy to verify on Ethereum. However, the challenge here is that it's hard to do recursion with pairings. There are a few different approaches here, but none of the results look really good for performance. So next, we considered these schemes based on Halo. Halo is this idea that came out a couple of years ago, and it provides this really clever way to do recursive proofs without pairings, using elliptic curves that are not pairing friendly.
00:49:35.560 - 00:50:29.690, Speaker A: And we actually implemented this, as Brendan mentioned, in our library, Plunke, and we were able to get decent prover speed and a decent recursion threshold this way. But the problem, as Brendan mentioned, is that Halo takes linear time to verify. So it just really isn't practical to verify on Ethereum unless we combine this with other techniques. And finally, we have schemes based on Fry, like Starks. And these are particularly interesting because there isn't really one set of performance numbers for these proofs. It really depends on the settings that we use them with. So Fry has this parameter called the blow up factor, which is basically a measure of how much redundancy we add to a polynomial before we generate the commitment to it.
00:50:29.690 - 00:51:26.924, Speaker A: And if we want a really fast prover, we can use a small blow up factor, which means less redundancy, so we have less data that we were committing to, and proving will be faster. However, the caveat here is that a smaller blow factor reduces the security of the Fry protocol, and we have to compensate for this by running more queries, and this increases proof size. So we have this dilemma where we can either choose a fast prover or small proofs, but we can't really have both at the same time. Luckily, recursion helps us here. So with recursion, we can take a larger proof and we can shrink it by wrapping it in a recursive proof with a larger blow up factor. And that's exactly what we do in our ZK roll up. So we start with transaction proofs that we want to be as fast as possible.
00:51:26.924 - 00:52:10.076, Speaker A: And here we use a blow up factor of two to really maximize the prover speed. And at first we have these really large proofs because of that, but that doesn't really matter because they just stay on my computer. We don't send them to anybody. Instead, we immediately shrink them by wrapping them in a recursive proof with a larger blow up factor of eight. This brings the size down to about 115 KB, which is more manageable. But if we're going to submit approved to Ethereum, which charges 16 gas per byte, then we'd still like them to be smaller. So before sending approved to Ethereum, we apply the same technique but more aggressively with a blow up factor of 256.
00:52:10.076 - 00:52:25.660, Speaker A: And this brings the size down to about 45 KB, which is great. These proofs do take a bit longer to generate, but it doesn't really matter because we only have a small number of proofs that have to be sent to Ethereum.
00:52:31.860 - 00:52:32.610, Speaker C: Okay?
00:52:33.380 - 00:53:27.760, Speaker A: So the other thing we can do to really maximize the proverb speed is instead of using a 256 bit field like most Snarks do, we can encode the witness in a 64 bit field. And this is much faster, especially for field multiplication, because typically quadratic algorithms are used there. So we can make it about 30 40 times faster by using a smaller field. There are a couple of complications here. One is that the Fry protocol assumes a larger field of at least 100 something bits. So here we borrowed this idea from ETH Stark, which is we include the witness in the small field, and then when we run the Fry protocol, we run it in an extension field of this 64 bit field. And that way we can get the same security of having a larger field.
00:53:27.760 - 00:53:58.860, Speaker A: It's just not a prime field. But that's fine. Fry doesn't require that. There are also a couple of complications with the Plonk protocol, where in certain places Plonk assumes a larger field for security. So we could apply the same technique and run those protocols in this extension field. In some cases we do that, but in other cases we use another workaround, which is to keep using the small field. But we repeat some of the checks in the Plunk protocol in order to boost soundness.
00:53:58.860 - 00:54:22.108, Speaker A: And that's it for me. Now I'll hand it to William. Okay. Thanks, Daniel. So, yeah, I'll talk about the recursive circuit optimizations we use in Plunkey two. So first, a quick intro on proofrecursion. The basic idea is that in a proof system we have both approver and a Verifier.
00:54:22.108 - 00:55:14.144, Speaker A: And for proofrecursion, what we do is we write the Verifier inside a circuit and that allows us to verify a proof inside another proof. This sounds easy, but historically it's been a really hard problem to design efficient recursive proof systems. And recently there's been some research on that, in particular on accumulation schemes with halo. But those have many drawbacks, as we saw in terms of performance. And so our solution to this problem is to use plank with a Fry based perinable commitment. But now if you try to write naively the verifier in a circuit, you will end up with quite a large circuit with two to the 16 gates. So around 60,000 gates, which is okay, but the proverb is a bit slow, so around 10 seconds.
00:55:14.144 - 00:56:03.316, Speaker A: And so we really focused on bringing this number down. So first a quick recap on plank. So in Plunk you have a table of field elements, and on each row of the table you have a polynomial constraint that evaluates to zero. And each row can have different constraints. And so if you go and look at their original plank paper, you'll see that they use a table of width free, so only three columns, and most Plunk implementations use between ten and 20 columns. But we decided to use a much wider table with a width of 135. And this may sounds like a lot at first, but the design of our system is so that it's really okay.
00:56:03.316 - 00:56:47.724, Speaker A: For example, we can commit to all the columns with just one merkel route. With other printable commitments, you would have to commit to each column individually, which would be terrible for performance, but we can just do it with one merkel root. But so why do we need such a wide table is to design complex gate. So, for example, in the Fry verifier, there is this complex operation which is interpolation. So we have eight pairs of points x zero y zero to x seven, y seven, and another point z. And we have to interpolate a polynomial of degree seven on these pairs and evaluate it at z. So that's quite a complex operation.
00:56:47.724 - 00:57:29.410, Speaker A: That would take a lot of gates if we did it with classic Planck. But with our white table, we can actually do it in one gate. So how do we do so we start by finding the interpolation polynomial f outside the circuit, and then we evaluate it at z to get the value v. And then we put all these variables in a single row, as you can see, on the bottom right. And then we have a bunch of constraints on this row. The first set of constraints checks that f is indeed the interpolation polynomial of the pairs. And then we add a constraint to check that f of z equals v.
00:57:29.410 - 00:58:19.840, Speaker A: Another thing we can do with our white table is hashing in one row. So the Fry verifier uses a lot of hashes, like currently they make up 75% of the circuit. So it's crucial to us to have hashing as efficient as possible inside the circuit. And we decided to use poseidon, which is an algebraic hash. So a hash designed for arithmetic circuits, and it's quite a popular hash, and it's used by other teams in the space. But what's special about plunky two is that we can actually do one poseidon permutation in just one row. Okay? So in some sense, Plunky Two is as optimal as possible in terms of hashing.
00:58:19.840 - 00:59:11.132, Speaker A: And so this is like the most common optimization we use in Plunky Two is exactly this. So we have a complex operation in the five verifier and then we use our white table to write complex gate to do these operations. One other nice optimization we use is what we call Merkel caps. So the Fry verifier does a lot of Merkel proof verifications. So on the left here you have classic Merkel proof verification. So you start at a leaf and then you hash your node with its siblings until you go up to the Merkel root and you compare the hash to get with the Merkel root and accept if they are equal. But what you can do is you can save hashes by stopping at the second layer.
00:59:11.132 - 00:59:57.900, Speaker A: So on the right we stop at the second layer and we compare the hash with the second element of the second layer. And so this layer, we call it the Merkel cap. So that's a cap of height one, and instead of committing to the root, we commit to this Merkel cap. You can also have a Merkel cap of height two or three, and those save two or three hashes. This sounds like a small optimization, but since we do so many miracle proof verification in the circuit, they add up to a nice improvement. And what's cool is that this cap height is a parameter in the code that we can tune to optimize either for proof speed or proof size. And we have quite a few of these parameters.
00:59:57.900 - 01:00:42.220, Speaker A: Daniel mentioned the Fry rate, and so we can tune these parameters to optimize for speed or proof size. So this makes Plunk Two quite a versatile proving system. And all these optimizations add up to a nice result that we're really proud of, is that our recursive secret is like really small at two to the twelve gates. So that's around 4000 gates. And this is one of the main reasons why we can have such fast proving times. Yeah, and now I'll let Jacob talk about low level optimizations. All righty, so let's talk about speed.
01:00:42.220 - 01:01:30.008, Speaker A: We talked about high level optimizations, we talked about optimizations to the circuits. Now let's talk about how we implement the very lowest level things. I'm talking like multiplication really, really fast. How do you make math fast? To make math fast, you need both mathematical insight and technical expertise. An engineer can take a spec and write code that runs really, really fast on whatever hardware you're using. An excellent mathematician can take whatever work needs to be done and minimize the number of some fundamental operations like multiplication. But finite field multiplication is fundamental mathematically, not physically.
01:01:30.008 - 01:02:20.780, Speaker A: There is no logic gate for finite field multiplication. So the mathematician only really sees a proxy. On the other hand, for the engineer the spec is immutable. There's only so far that software engineering tricks can take you, and a spec can only run so fast. To make math fast, you need to flip the causation, right? The work that you're doing needs to be determined by the physical reality of the hardware that you are doing it on. Plunky Two is built for performance, right? When we were designing the protocol, we were already thinking about how fast it would be running on commodity hardware. The earliest of decisions in Plunky Two were already guided by performance.
01:02:20.780 - 01:02:59.210, Speaker A: Let me give you an example. When you're doing mathematics in these kinds of proofs, you're working with a prime field, FP, where P is some sort of a prime number. What's a good choice for P? That's a very fundamental question. Well, firstly, it needs to fit within 64 bits, like Daniel's already mentioned. And the reason why is that computers, modern computers, are 64 bit machines, and once you've crossed the 64 bit boundary, everything suddenly gets so much more expensive. But it also can't be much smaller than 64 bits for security reasons. So you want it to be almost 64 bits, but not quite.
01:02:59.210 - 01:03:54.250, Speaker A: And different choices of P will also have different algorithms for multiplication and addition with different performance characteristics. So we want to choose P such that multiplication and addition, those fundamental operations are really fast. So we went through a lot of candidates. Here's a GitHub thread where we documented our research. We considered many fields, each one with their own algorithms for arithmetic operations. We derived those algorithms, we examined the compiled assembly code, we counted cycles and micro ops. And what we ended up settling on is this really neat prime number that has a frankly beautiful mathematical structure that just happens to play really well with how modern CPUs and Isas work.
01:03:54.250 - 01:04:39.172, Speaker A: Let me give you another example. As Williams already mentioned, we spend a lot of time doing hashing, and the hashing algorithm we use is called poseidon. Within poseidon, we use an MDS matrix for a particular step, and the MDS matrix is up to us to choose as long as it meets particular criteria. And we spend a lot of time doing matrix vector multiplication with this matrix. So it's important to get the matrix right to make that particular matrix multiplication pass. There's a few tricks that we use in order to choose a good value. So the choice for the MDS matrix is, firstly, circulant.
01:04:39.172 - 01:05:19.590, Speaker A: What that means is that every row is just a row above it, but shifted by one position. It means that there's fewer numbers for us to have to remember. We're spending a lot less time just loading constants from memory, which make our code faster. Secondly, this matrix is composed of powers of two, and multiplication by powers of two is way faster on a computer because you're just shifting the bits by some amount, as opposed to actually having to go through all the steps of multiplication. These powers are small, which makes the finalization at the end, like, way cheaper. And a lot of these elements are one, which is like multiplication by one is free. Like you just don't do anything.
01:05:19.590 - 01:06:02.100, Speaker A: We make extensive use of vector operations, so most of the code that your computer runs is scalar. The computer kind of takes, let's say, addition. You have two numbers and there's an addition instruction that takes two numbers and returns the sum as one number. But your computer also has what we call vector instructions. So these do more with one instruction as long as you're doing the same thing for multiple values. So you might have a vector of four numbers and there's an instruction to add that to another vector of four numbers. Getting four sums as the result in one vector.
01:06:02.100 - 01:06:44.720, Speaker A: Compilers are generally bad at generating these kinds of instructions. So if you really want to make use of this, that's a lot of work for the program. In Plunky Two, we make extensive use of vector instructions. This was a lot of work for us, but it's really worth it. So vector instructions give us a two times improvement on Poseidon. In addition to that, there's a bunch of ASMS scattered in the code base. So, for example, the top one made finite field multiplication 10% faster.
01:06:44.720 - 01:07:40.900, Speaker A: The bottom screenshots are from parts of our Poseidon implementations on Arm and on X 86. Look. The result of all this work is lightning fast zero knowledge proofs. Right? And you've seen the impressive results from Brendan, but we're only getting faster. There's still no GPUs involved. For example, we didn't think that we wanted our users to need GPUs to run Plunky Two, and I'm looking forward to how fast we can get in the future. The reason that we were able to do this at Polygon Zero is that we are both excellent mathematicians and also hardcore computer scientists.
01:07:40.900 - 01:08:54.088, Speaker A: It's not often that you get to work on something truly groundbreaking. And I feel extremely lucky to have had the chance to work with such a bloody fantastic team. Right. I really can't believe that I get to work with my coworkers because they're just amazing and I am really excited for what we will be able to achieve in the future and we're happy to take questions. Jacob and team, thank you so much for that amazing presentation and just kind of walking through all these improvements. We do kind of have a small caveat, which is we are running a little bit behind, so we'll have to cut the Q a short. But there's one question that came from the audience and that question was just overall kind of how do you think about now new applications to sort of enable more things with just the Polygon ecosystem and also the ethereum side with these new improvements, with these new performance metrics? How do you think about just the EVM world and what would not be possible here.
01:08:54.088 - 01:10:06.400, Speaker A: Just giving that taste, probably I should take this disclosure question and you just also wanted to put a closing note also that how this adds an immense amount of value to the overall Ethereum ecosystem. And Polygon ecosystem is that this prover system, like many who understands the ZK technology, they know that this is the most efficient and the fastest ZK prover system. And this will enable us to create a very high performant, highly decentralized ZK roll up or EVM compatible ZK roll up, which will you can already imagine that if this will provide full security by Ethereum. And then imagine you have a ZK roll up where it provides full security by Ethereum and DApps can use that roll up at a much low cost transactions by using validium kind of systems. Or later on with E 2.0 data shards it has a potential to provide massive scalability to the DAP ecosystem. And Polygon already has a huge DAP ecosystem.
01:10:06.400 - 01:11:16.680, Speaker A: We keep getting these demands that we need to have more Ethereum secured and decentralized layer two. And I think this takes Polygon community very close to that dream. And then what Brandon and team and Daniel and team are building would also provide this is a bit technical getting into it, but might also provide a virtual machine which has more opcodes to use to the developers relevant to the question. The apps on this layer two can actually run more opcodes. That means more functionality, more features on this one while still be fully approvable on Ethereum. So this is really massive for Polygon. And with that closing note, I want to congratulate the entire Polygon community that this establishes Polygon at one of the topmost players in the ZK space and let's build a highly adopted Ethereum layer two and bring the mass adoption to Ethereum.
01:11:16.680 - 01:11:40.944, Speaker A: Thanks a lot. Karthik, over to you. Thanks everybody and congrats again to the Polygon Zero team. I think with that we are ready to move on to our next talk. And for this one, I'd like to invite Paul and Titania for talking to talk about Polygon Nightfall. So without further ado, let's welcome Paul and team. Hello.
01:11:40.944 - 01:12:04.308, Speaker A: Hi everybody. Paul Brody here. So I'm the global blockchain leader at EY and wow. So just I was watching the last presentation. That is a very tough act to follow, but we are going to do our best. I'm only going to take like two minutes before I introduce my colleague Chitanya, who is incredible. But I want to talk just a little bit about the history of Nightfall and how it became Polygon Nightfall at EY.
01:12:04.308 - 01:13:16.864, Speaker A: There's a couple of things that we spent a lot of time thinking about. One is what is it that enterprises need? Right? And there are a lot of brilliant people like Sandeep and others who have been thinking very heavily about scalability. But one of the big challenges for enterprises is enterprises can't just use a mixer, right? Because enterprise tokens and assets are often there, unique, they're often attached to business contracts that represent kind of complex business relationships. And so we have spent a great deal of our time over the last five years, really pretty much nonstop over the last five years from a research perspective, thinking about how to enable privacy at scale. Now, the first version of Nightfall, I think back when Ether was $100 apiece, we spent $3,000 worth of Ether just to build our prototype ZKP based supply chain and move some tokens around. And that was an amazing piece of work, and it proved the case out for us, but we kept iterating. And where we ended up earlier this year was with nightfall three, which is a ZK optimistic roll up technology, and we built that out.
01:13:16.864 - 01:14:15.756, Speaker A: And in cooperation with Polygon, we're really turning this into a layer two system where everyone can access private transfers and payments at scale with very efficient gas costs directly on the Ethereum main net. So that is kind of critical to our goal. And when we started talking to the different layer two providers and there's a lot of them out there, and they're all populated by incredibly brilliant teams. But the relationship that we struck up with Polygon, with Sandeep and the team was incredible because at heart they understood a couple of really critical things that we shared with them a huge commitment to public blockchains, an unshakable commitment to the ethereum ecosystem. EY we are without a doubt the biggest sort of large enterprise investor in the Ethereum ecosystem. We pour our energy into this ecosystem. And then thirdly, we're both very committed to ZKP technology, both for scalability and for privacy.
01:14:15.756 - 01:14:55.388, Speaker A: And so that match the relationship that we built over, that has been the foundation of the work that we've done. We are so excited about kind of where this is going. I believe we are all underinvested in privacy that enterprises need it, for sure. It's an absolute requirement. But I think far too many consumers and individuals are underselling their own value by not investing more in their own privacy. So I very much look forward to that. And I am incredibly delighted to hand over to my brilliant and talented colleague Chitanya Konda to actually explain what polygon nightfall does and give us a quick demo.
01:14:55.388 - 01:15:02.380, Speaker A: Chitanya, over to you. Don't forget to unmute.
01:15:03.440 - 01:15:44.376, Speaker C: Thank you, Paul. Yes, I'm Titania, and I work with EY's blockchain R D team. Today I'm going to show you a live demonstration of what polygon nightfall does. And hopefully a very wise man once told that live demonstrations, especially of tech nature, are a courier seaside one that I repeat time and again, because I always end up with the short end of the straw. I hope today is not one of those days for me. Let me share my screen as I walk you through. Okay, now I.
01:15:44.376 - 01:16:46.620, Speaker C: Hope you all can see a PowerPoint presentation. So I'll give you a bit of context about what Polygon Nightfall does before I give you the Nightfall demonstration itself. What is Polygon nightfall? You all must be pretty much aware the very first iteration of Nightfall was built as a privacy protocol, or it still is a privacy protocol. What it enables us to do is to do private transfers of ERC twenty, s seven twenty ones, or 1155s between two parties, such that the contents of that transfer is private, so nobody can see the value or the token ID at the same time the recipient is also anonymous. So it's essentially a privacy protocol. Now, just to give you a high level summary of how it works, because this is not the point of our talk here, you have three kinds of transactions. You have deposits, transfers and withdrawals.
01:16:46.620 - 01:17:29.524, Speaker C: With a deposit, what you do is you take a token out of L one and you put it into L two. When I say L two here, it's really a shield contract that sits on L one. And you take any tokens of ERC, 20 of a certain value. When you submit this proof, called a deposit proof, which then provides the equivalent commitment that holds the value you've just deposited. A commitment is just one way to obfuscate the contents of a token in terms of a deposit, that's really not private. Because when you move tokens from layer one to layer layer two or back to layer one, that's not where we get privacy. We get privacy when we're within layer two itself.
01:17:29.524 - 01:18:17.848, Speaker C: So now, moving on to the transfers, where we actually get private transfers itself, what we're going to do is in order to spend a token, a user will first have to create something called a nullifier. A nullifier essentially spends the commitment they own. And there's no way to connect a nullifier to the commitment it belongs to. So all someone who's watching the blockchain can see is that something has been spent, but they can't see what has been spent and how much is within it. At the same time, when you're transferring something to some other person, what you're going to do is create another commitment of equivalent value that you've just spent. And obviously, because it's obsocated inside a commitment, the contents of the commitment itself are not visible. So the value is hidden once again.
01:18:17.848 - 01:18:58.330, Speaker C: And also the other piece of data that would be as part of this commitment is the public keys that the new recipient owns. So this new commitment now belongs to somebody else. Anyone who's watching the blockchain can't really see who that recipient is or who received this new commitment. Now, withdrawals are pretty much the opposite of deposits, pretty similar to transfer. You're going to spend a commitment. When you create a nullifier for a commitment, there's no way to associate what commitment was just nullified. So no one can see what commitment was spent and you have an equivalent value being released from layer two to layer one.
01:18:58.330 - 01:19:31.280, Speaker C: The layer one part again, is visible for everyone to see. So you would really want to do transfers with privacy inside layer two. That's the whole point of how you'd want to use Polygon Netfall. Now, this bit of work that I've just described about has existed for some time now. It was first released back in 2018. So what's different between the version then and now is that it does optimistic roll ups. The very beginning, the cost of the transactions that we've done were quite high.
01:19:31.280 - 01:20:39.076, Speaker C: They ranged about in the $100 nature back in the 2018. The reason why it's quite high is there's generally a lot of data that's been stored on the Smart contract in terms of what's held in a merkel tree, all the commitments and nullifiers and you're just sending one transaction at a time, you're not really batching up. Now, both of these changes have already been modified where the Smart contracts are optimized such that we really don't store any data that's not really required to be stored on any Smart contract. And two, we also bat. But what optimistic rollup does, it takes it to a whole different level where you're really not doing a computation itself on chain anymore. You're really providing what the end state of a computation should be and in case that is wrong, you're expecting a challenger to challenge that the end state that you just provided was wrong. By doing this, you're saving all of the gas costs of computation itself and at the same time you're able to scale a lot more in terms of having higher transaction throughput because you can fit more data into a block.
01:20:39.076 - 01:21:40.220, Speaker C: Now, so that's how Polygonite Four uses optimistic rollups. Polygonite for uses zero knowledge proofs for privacy. So that's the ZK part and it uses optimism for scalability, both in terms of higher throughput but most importantly, lower transaction fees. Now, just to give you where this sits in the current ecosystem of layer two solutions, layer two solutions exist in the form of both off chain solutions and on chain solutions. With onchain solutions, there's usually two approaches one tends to take lately, you either use zero knowledge proofs Snarks or Starks of some kind, or you use optimistic roll ups. Using both of these, you can either get the benefit of privacy with some very good scalability or you can directly register as just purely a scalability solution. Where Polygon Nightfall is it's an optimistic solution that uses scalability but also provides privacy.
01:21:40.220 - 01:22:31.388, Speaker C: You would know of other optimistic solutions such as optimism and Arbitrum today, which are built to be scalability solutions from scratch. So, moving on. Now, just to give you a comparison of private transactions, if you were to do an optimistic version of a private transaction as opposed to a ZK's version of a private transaction, what I mean is using Optimism for scalability versus using ZK for privacy, ZK for scalability. There's pros and cons to both of these. A very good pro for an optimistic solution is that the gas costs are way lesser. So the gas cost of Polygon Nightfall is about 9000 gas per transaction. That comes down to about $$2.02.5
01:22:31.388 - 01:23:32.672, Speaker C: today based on various gas prices. Of course, that number is calculated based off of 75 as opposed to ZkZk solutions which tend to be more expensive in terms of gas. Now, on the other hand, with Aztec like solutions, which is ZkZk solutions, they have finality much faster than optimistic solutions. They have finality pretty similar to what a layer one would have. Optimistic solutions tend to have a one week wait period predominantly because you would want to give enough time for challengers to challenge bad blocks. If there exists any one way that polygon night force circumvents this is it uses the concept of instant withdrawals a feature that is provided by using liquidity providers for fungible tokens such as ERC 20s. So that using liquidity providers with instant withdrawals which is a built in mechanism, you wouldn't have to wait one week.
01:23:32.672 - 01:24:29.888, Speaker C: It could just be the layer one finality as well. The other big differences between the two approaches is that Polyvin Nitro, being an optimistic solution uses something like a fraud proof. Fraud proofs tend to be ones that are only submitted in case an invalid transaction ends up on the blockchain. ZK solutions, on the other hand, work on valid proofs which means you can't submit an invalid transaction to the blockchain to begin with. You've got to have a valid transaction that will verify the proof. Generally with Zhao solutions as well, you have computational resources that both of these use. With a polygon nitrogen solution, it's relatively lighter in terms of the kind of zero knowledge proofs that you would do as opposed to an entire ZK ZK solution that is on a very high level what Polygon Nitfold does.
01:24:29.888 - 01:25:15.200, Speaker C: But before I move on to showing you a demonstration, I just want to give you a quick overview of what are the various players involved and what do they do inside a polygon Naxo system. So here you can see three players. You see a transactor, a block proposer and a challenger. Transactors are users who generally create transactions such as deposits, transfers and withdrawals. And then they submit them either directly to the blockchain or they can also submit these transactions to the block proposals. The transactions that you submit to the block proposals usually are transfer and withdrawal transactions, not deposit transactions. Those will have to be submitted directly and always to the blockchain.
01:25:15.200 - 01:25:59.372, Speaker C: Block Proposers then take all of these transactions that they receive from various users and they put them into different L two blocks and they submit these L two blocks into the layer one, which is ethereum. In this case, each layer one block. An ethereum can hold more than one layer two block, depending upon how big these layer two blocks are. And that would exist on the state of L two blocks. The final state they hold would exist in layer one, inside layer one blocks. Now you have a third player called Challengers who are continuously monitoring the system. What they do is that they will take these blocks that are put up, these l two blocks that are put inside the main net.
01:25:59.372 - 01:26:39.928, Speaker C: They will check the validity of both the blocks as well as the transactions within the blocks. And if everything's fine with those blocks, they do nothing. But in case they do find a wrong transaction or a block that was created wrong, they will then create something called a challenge. This challenge will basically prove how the block that is on the main net is actually wrong. When I say mainnet sorry, Ethereum is actually wrong. And if that proof verifies, then Challengers will succeed and two things will happen. One, the block proposer, who just proposed the wrong block will be slashed.
01:26:39.928 - 01:27:19.748, Speaker C: And two, the block proposal would have submitted something called a stake. With every L two block that they create, that stake will be released to challenges as payment for the work they've done. So that's the incentive mechanism that the challenges would have. And block proposals. Their incentive mechanism is that Transactors will pay them fee with the transactions they submit to be put up into a block. So those are the different participants. And finally, I will not go into further details about off chain transactions, how block proposals are registered, how challenges are submitted.
01:27:19.748 - 01:28:10.356, Speaker C: Just to give you a bit of a roadmap before I move on to the demonstration, currently this protocol that I was just discussing about has been deployed into the testnet and we're testing it rigorously to catch any obvious or not obvious and all sorts of bugs that we could think of. Once that's done, the next step would be is to have a proper security audit of the protocol. And then there would be a restricted value mainnet deployment. When I say restricted value, it would be the amount of transfers would be pretty small originally. There might probably also be a bug bounty program run simultaneously after the main deployment. And finally there will be after the main deployment has been successfully run for quite some time. Of course, the restriction will just be lifted.
01:28:10.356 - 01:29:17.144, Speaker C: And for there to be no restriction in terms of the transfer at all, that's everything in terms of context. Now I will move on to the fun part for you, but the scary part to do a tech demo of me. This here, which you see is a Polygon Nightfall wallet. It's very preliminary looking just because we're much more focused on having this tested itself rather than having it part of Polygon's website already. But you can imagine it to be as sophisticated and as beautiful as Polygon has their wallet today and it will be part of the website too. So this polygon Nightfall wallet is run by any user who wants to use this layer two solution to do transfers of private nature. All of the keys, both that pertain to the keys that will be used to send transactions to the Ethereum blockchain, or the keys that correspond to the layer two itself, which is the secret keys that will give you ownership of commitments.
01:29:17.144 - 01:29:57.930, Speaker C: All of those are stored within users browser. None of this will be inside a server anywhere. So this will have proper privacy for a user and everything in terms of privacy sits at the user's end in their browser. Now, another thing is you'd notice that here we have ERC twenty S and ERC 720 Ones and obviously deposit transfers and withdrawals for all of these. Something else to notice, ERC 1155 is also possible. It's just not part of this demonstration right now. And yes, that's pretty much everything before I go on to give you a demo.
01:29:57.930 - 01:31:12.460, Speaker C: So firstly, I will start off by doing a deposit of, ERC 20 to myself, which means when I create this transaction and I sign this transaction, what's happening is that the very first transaction was to approve the Shield contract to take some ERC 20 balance from my ERC 20 contract and send it to itself. The second transaction is to actually send it to the blockchain. So what you see here is I have an L One balance of a million I had and now I moved ten of that to layer two. But it says pending deposit, so that's interesting, right? Why is it not part of L2 already? We've got to remember that it's a roll up solution where lots of transactions are required to be put into a block. At the moment, all I have is just one transaction. Before I go any further, I just want you to show what other players exist here. So the one who's running the browser, this wallet here, I'm the user, I'm one of the participants in this network.
01:31:12.460 - 01:31:57.404, Speaker C: The other users are sitting here. You have a block proposal and you can see that a block proposal was started and they've registered themselves onto the layer one contract and they're listening for any sort of incoming events. And then you would have a second player called a liquidity provider who provides the services if an instant withdrawal is required. And then you have a challenger who is continuously monitoring the system to see if there's any bad block proposed. Other than that, this is just the transactions being run up and a bunch of logging that's not very interesting. So today, right as we speak, there's four players all up and running the system. So one transaction was created, it's still pending.
01:31:57.404 - 01:32:38.728, Speaker C: So now let's create a second transaction. This time, let's do an ERC 721 transaction. I'm picking a token ID one I'm submitting this from. I'm approving this first in the ERC 721 contract and then I'm actually submitting the transaction. So what happens now is that this ERC 721 token has to move from l One to l Two since everything is pretty quick. What you have not noticed is that initially it was a pending deposit, but as soon as a block was created, what that means is that it is now in L2. So the l Two balances have been updated.
01:32:38.728 - 01:33:10.920, Speaker C: So we've created one deposit of ERC 20, that was ten and one other deposit of ERC 721. So that's why you see these balances in L2. The current block size is just two. For the sake of demonstration, I did not want to create 20 transactions as you were all looking at it quite boringly. But obviously these block sizes can be much bigger than that. Very quickly, let's see what our other players are doing. So we have a block proposer block proposal just said, oh, I created a good block and this is the hash of that block that I've just created.
01:33:10.920 - 01:33:44.020, Speaker C: And this is the current balances for the user myself. So both of us are doing our jobs quite well and we're both pretty happy. Now what I want to do is I want to do a transfer. I want to do a transfer to often ERC 721, let's say to somebody else. The default address is to myself. But that's not fun, is it? So I'm going to do a transfer to somebody else and let's pick a token. There's just one token in layer two for the moment in Yasi 721.
01:33:44.020 - 01:34:19.150, Speaker C: I pick that and we'll submit this transaction. I'm signing this transaction to be sent to the blockchain and pretty soon you will see that this l Two transaction that I have goes into pending outflow. Once again, it's pending because it's not been proposed as an l Two block onto l One yet. We need a second transaction to create some blocks. So I'm just going to create another transaction. In this case, let's say a deposit of ten of ERC 20.
01:34:21.060 - 01:34:21.596, Speaker A: And I'm.
01:34:21.628 - 01:34:51.006, Speaker C: Going to sign that as well. Okay, now you will see that oh well, let's update again. Now you will see two things have happened. A block must have been created. Yes, indeed it was. A Git block was created. So I now have a balance of 20 because I had ten originally and I just added ten more and I'm less by one ERC 721 token.
01:34:51.006 - 01:35:33.694, Speaker C: So just to quickly give you an overview, there is a second account here, obviously, that I used to send it to, and that second account has a balance of ERC 721 one. Just to give you a quick understanding of the second account, for the moment, I'm just running the second account in a browser myself. The keys, the level layer two keys that we're using are very different here. But layer one, they're both the same people. Because I'm the same person. So my layer one balance according to that ethereum address is still the same, but under layer two, I have two different accounts and these are the balances of those two different accounts. But you can extend this logic to anybody out there in the world.
01:35:33.694 - 01:36:10.586, Speaker C: It does not necessarily have to be me. It would run the same way. Now, just going back to my original account, I'm just going to quickly log out and log in again. Okay, now let me retrieve my balances. Now I'm going to do something else here, which is to do a withdrawal of an ERC 20. I will click on Withdraw and I'll submit ten. But you've got to remember, if I just click Submit right now, there's the problem of finality being one week.
01:36:10.586 - 01:36:49.010, Speaker C: And I really don't want to wait one week. I just want my transaction straight away because I need to use it for something badly. So I have the option here called Instant withdrawal. So I'm going to click on Instant Withdraw and I will click on Submit and then I will confirm that transaction. So now you see, I have a pending outflow of ten, which means I should pretty soon withdraw ten out of layer two, and that should end up in my layer one balance. At the same time, I need a second transaction for this to be blocked. So I'm just going to create an ERC 721 transaction.
01:36:49.010 - 01:37:24.370, Speaker C: And of token ID two, I will sign that too. And then let's look for the balance in a minute. Okay? So you'd notice that well, another step. Actually, you notice two things here. One, I've just given myself a new RC 721 token again into L two. So there's that. But there's also an update to my L One balance.
01:37:24.370 - 01:38:08.530, Speaker C: I moved ten out of my pending outflow into L One balance, which meant I was able to instantly withdraw the $10 instead of waiting for a finality of one week. Now let's just see what the liquidity provider saw. So the liquidity provider received an instant withdrawal request for this transaction, hash and he provided or she provided me with the money straight away. The mechanism behind this is this. When I create an instant withdrawal, what I'm doing is submitting a request that I'd like to instantly withdraw this token. And this is the fee I'm willing to pay. Liquidity providers are listening to these events.
01:38:08.530 - 01:39:03.986, Speaker C: Those are the events that they're listening to. And as soon as they see an event, what they're going to then do is to basically create accept this request I just made and transfer the subsequent amount for me while taking on the wait time. For the wait time, I would have gone through of a week period onto themselves. So liquidity providers like Challengers are continuously monitoring the network. So they would know if they give me the money straight away today, is that safe or not? They would know that because they're continuously checking all the blocks and all the transactions and all the blocks. By doing so, they know that eventually after a week, this transaction will be finalized for sure. Because there's no bad block before this transaction, there's no way for this transaction to be removed from the network, considering that they will do this verification themselves.
01:39:03.986 - 01:39:39.054, Speaker C: Provide me the liquidity that's required straight away and they will receive the money that I would have received a week later. So that's what's happening with the liquidity provider. There's just one final interesting case I'd like to show you. I'm going to do just two deposits for this of ERC, 20. Okay, that's pending. I need a secondary two. You will see there's a pending deposit of 20.
01:39:39.054 - 01:40:19.926, Speaker C: You will see that I now have an elder balance of 20. But let's just give it a few minutes and we'll see something pretty interesting. Okay, now, you saw I had a balance of 20, a 30 area, but now I only have ten. And 20 were moved back to pending deposit. So what just happened with something naughty? Let's look at it. You see there's a block proposal here and clearly, so far block proposals has submitted three good blocks. But the last block that he just created with my two deposit, he purposefully created a bad block of type incorrect root and he submitted onto the blockchain.
01:40:19.926 - 01:40:57.990, Speaker C: So what happened was as soon as this block was submitted, because that was a block that's part of Ethereum. Now in layer one, my balance got updated saying, well, your balance is now 30, but clearly somebody was monitoring the network, which was a challenger. A challenger was monitoring my network, the network of Ethereum. And then they realized, OOH, this is actually a bad block. Let's take a look at that. They notice that I just received a block here. Q new block proposed event.
01:40:57.990 - 01:41:55.594, Speaker C: I've just received a block proposed. I'm going to verify the validity of the block proposed. That's what they would have done. So as soon as they verified there was a warning, they said block checker, block invalid with so and so issue. As soon as that error was picked up, it then tries to create a commitment for the challenge, which is challenging incorrect route and eventually it would have submitted the challenge to layer one. Our challenge commitment has been mined and sending reveal, which is yes, reveal is to reveal the challenge itself and the challenge would have been provided to layer one. Layer one will then do the computational steps of the challenge itself and on successful challenge, a rollback of the entire state until the first bad block was until where that bad block was.
01:41:55.594 - 01:42:35.430, Speaker C: When that happens, a rollback of all of the transaction state is done and a rollback was received by everyone, including the challenger. So that's what happened. Our state was rolled back where we still have 20 waiting to be deposited. And I don't have that balance yet. Now, what happened was I only have one proposal, as you can see, and as soon as this bad block was submitted, this proposal was basically slashed. So they're no more the proposal. Now, on a good day, if I were to have another block proposal right now, these transactions would have ended up back in the mempool for this second block proposal to have picked up and they would have submitted for me.
01:42:35.430 - 01:43:00.700, Speaker C: So these transactions would have eventually made it back to l Two without me doing anything. That's just the way the entire network works and the different players are interacting with to give a proper, valid transaction state to the system. That's pretty much everything, really, in terms of demonstration. I'm happy to take any questions now.
01:43:01.070 - 01:43:32.966, Speaker A: Tony, I got to say, you keep saying that live demos are career suicide, but the reason you keep getting picked for them is that you always execute them and then look like a rock star. So thank you. Thank you so much. I'm sure Karthik's going to tell us that because of the time constraints, there are no questions, but really, truly brilliant. I'm so proud of the work that you have done, the R D team that has done and the work that we are doing in cooperation with Polygon to make this real. Paul and Zitanya, that's mostly correct. There is one question that I do want to ask, a quick one that came in.
01:43:32.966 - 01:44:14.082, Speaker A: But before I ask that question, I want to just say I've just been looking at hundreds of comments and everybody's just been extremely happy with how easy you made that to understand and follow and of course, the demo work. So I just want to make sure that I point that out. It was a super conveniently, well made, easy to follow presentation that covered really technical topics. So thanks for that. The question is a simple clarification that came in from one of the listeners. And the question is, would challengers be able to reverse transactions made a mistake, such as kind of giving an invalid wallet, address or any other keys and kind of just how does the if it's possible to reverse a transaction upon finality?
01:44:14.146 - 01:44:45.070, Speaker C: I guess so, yes. There's a one week finality period. You really have to do any sort of challenge within that period. So there's various ways a transaction or a block could be wrong. Either the transaction that was submitted to begin with was invalid. The block browser still went ahead and submitted it as part of a block, or the block itself that was created with these various transactions was malformed and that was submitted onto the network. What then happens is that block now is part of layer one.
01:44:45.070 - 01:45:30.320, Speaker C: Let's say other blocks were continuously proposed on top of these blocks as well. At this point, a challenger who is looking monitoring the system, they would pick up this very first block proposed that was wrong block, they will then submit a challenge that challenges that very block. And when that happens, all of the blocks along with this block and the ones that come after will be rolled back. So that entire state is basically rolled back. And when that rollback happens, that rollback is not just happening in layer one of the blockchain. Everybody who's keeping track of their own state in terms of commitments or transfers or transactions, they will also have to update their own state. So block proposers, challengers and users, they will all update their own state according to the rollback that happened.
01:45:30.320 - 01:45:33.370, Speaker C: I'm not sure if that answered your question, karsik.
01:45:33.450 - 01:45:49.940, Speaker A: I hope so too. We'll see if there's a follow up question that comes in. And if there's still any confusion or follow up questions, I'll just ask them directly over email and connect you both. So thank you so much with that amazing presentation and we are ready for our next talk.
01:45:50.310 - 01:45:51.220, Speaker C: Thank you.
01:45:52.870 - 01:46:14.880, Speaker A: So next up, I want to invite Bobbin to talk about Polygon Midin. So, Bobbin, whenever you're ready, feel free to get started. Welcome. Thank you, Kartik. And hello everyone. Very excited to be presenting amongst such brilliant people and talk about polygonmiden. Let me just share my screen.
01:46:14.880 - 01:47:05.980, Speaker A: All right, I hope everybody can see the presentation. Everything is good. All right, so before I explain what polygonmiden is, let me give you guys a bit of history. So I've actually started working with Zero Knowledge Tech and Stark, specifically in spring of 2019. And the first thing I did was build this gen Stark library, which was a Stark prover for aiming to be able to generate proofs for kind of any kind of computations. And once I built it, I realized that it's extremely difficult for regular developers to just pick up and start using something like this. And I can spend a few months trying to figure out how to make it easier and try to develop it first.
01:47:05.980 - 01:47:41.730, Speaker A: Kind of that means specific languages that would make it very simple to create Stark proofs. But even that turned out to be quite complicated. So the next thing I try to do is I start to think about Stark based virtual machines. And I actually made this E Three research post early in February 2020 describing kind of my thoughts on it. And at the time, I actually didn't think it was going to be very powerful machine. I didn't think it was going to be Turing complete. But as I started working on it, I realized that there are a lot of things that we can unlock and there is actually a way to build a practical start based virtual machine.
01:47:41.730 - 01:48:48.986, Speaker A: And a few months later, I built the first version of, you know, improved it over the following months. And this VM is actually one of the foundations for polygonmiden. The other foundation for polygonmiden is Winterfell, which after I worked on VM, I joined Novi. And as part of being there built this winterfell stark prover it's in a way very similar to Genstark but it is much much more performant, modular and up to date implementation of Stark protocol. And now polygon midnvm is kind of a combination of Dstaf VM and winterfell and in a way it is a culmination of what I've been working on for the last two years, so to say. So I'm very excited to kind of talk about it. So what exactly is polygonmiden? Polygonmiden is general purpose Stark based ZK roll up and I know a lot of people here are already familiar with all of these terms but for those who are not let's go through these terms one by one and start with ZK rollup.
01:48:48.986 - 01:50:08.786, Speaker A: So what is a ZK roll up? In a ZK roll up we have users, operators and then we have an on chain contract which is on Ethereum main chain. And the idea is that users send transactions to operators and there are many users and operators aggregate those transactions into batches and then they submit kind of the state change or whatever has changed in the internal state of the ledger that the operators are managing together with the ZK proof that they verified all of those transactions to the on chain contract. Which verifies that. So why do we care about this type of setup? And one of the answers is that it gives us lower fees because we can compress a lot of things using ZK proofs and there are other reasons why for example we don't have to include every single transaction. If some transactions happen between the times that we verify ZK proofs then those transactions can actually become completely absent from the kind of the state change that gets pushed into the main net so we can get up to actually over 100 x reduction in fees as compared to Ethereum. And the other thing that is important is we can get this without sacrificing security. So in the ideal scenario the ZK roll up is just as secure as Ethereum itself because it inherits security from Ethereum.
01:50:08.786 - 01:50:56.674, Speaker A: There is no way for operators to submit invalid transactions or submit a state transition in the ZK proof for something that is not really valid. So if they didn't have a transaction to verify, they couldn't make it up. And the other exciting part is that roll ups can be very high throughput. And the reason for this is that unlike, let's say in a decentralized ledger of the way Ethereum is. The operators can be fairly heavy and they can have very certificate hardware to process a lot of transactions and we only need to really to have one operator that is honest for. The roll up to work. And even if the single operator, for whatever reason, decides not to be honest, the users can still go directly to the on chain contract to reclaim their funds.
01:50:56.674 - 01:51:43.566, Speaker A: So we of course want to have the roll up to be decentralized. But this kind of setup allows us to have much more flexibility and experiment with different consensus models, execution models and structures of the ledger and achieve this high throughput lower fees while still maintaining the same level of security. And that flexibility portion is something that is really, really exciting to me personally. Okay, so this is a very high level overview. I'm sure most of the people here already knew what a z roll up is, but just to kind of subcontext okay, so let's talk about the next term Stark based. So before we talk about what a Stark based means, we need to talk about proofs of computational integrity. And we talk about this as ZK tag.
01:51:43.566 - 01:52:26.546, Speaker A: But actually the thing that we really care about is this proving that the computation was done correctly. ZK, in many ways is incidental to a roll up, although it is a useful property if we want to achieve privacy as well. So in terms of proofs of computational integrity, usually there are two parties. There is approver and the verifier. And Approver wants to say that they ran some kind of computation and they have gotten some kind of result. In the context of a roll up, the computation is verifying all the transactions that they want to put into a batch, and they want the prover to accept that they've computed the result correctly without the verifier having to rerun those computations themselves. And obviously, we want to be in a setting where we don't want to trust the prover on their word.
01:52:26.546 - 01:53:10.074, Speaker A: So the Verifier cannot just accept the proverbs result. But what we do is the prover can send this zero knowledge proof to the Verifier. And the Verifier, by examining the proof, can be convinced that the prover has run the computation and received the claimed result without having to do this computation themselves. And it's very important in our context that the proofs are small and fast to verify. So overall, in terms of proofs of computational integrity, there are two large families, Starks and Snarks. And Starks stand for scalable, transparent arguments of knowledge. And they were developed by brilliant people at Starkware.
01:53:10.074 - 01:53:50.794, Speaker A: And there are a lot of different Starks developed by many different teams. And they stand for succinct, non interactive arguments of knowledge. But rather than saying like, there are two different families, there's actually a lot of overlap between them. So a non interactive Stark is actually also a Snark, and scalable transparent Snark is also a Stark. So in our context, we're actually working with non interactive Starks, which means also that our system is a Snark as well. Right? There are a few advantages and reasons why I think Starks are a very good choice for a roll up type system. And I'm going to go through them in the next slide.
01:53:50.794 - 01:54:31.740, Speaker A: So Stark advantages, as I mentioned or comes from the name Starks, are transparent and scalable. So it means there is no trusted setup. We don't need to worry about toxic waste or trusted setup being compromised and potentially somebody one of the operators or provers generating fake proofs. So that's a very good fundamental property to have. The other advantage of Starks is that they use very lean cryptography. And what I mean by this is that they rely only on collision resistant Hash functions and that makes them post quantum secure. And in many ways it's a future proof technology.
01:54:31.740 - 01:55:19.500, Speaker A: They're very flexible. And here I'm getting a little bit too technical, but we had the talk from Polygon Zero where they talked about selection of a field and the same applies to Starks as well. You can choose a specific field which is very performant and allows you to generate proofs faster. You can also dynamically kind of trade off between prover time and proof size. So you can increase prover time while reducing proof size and play with different security levels without having to modify anything kind of in the proving system. You can do it, you don't need to choose a different elliptic curve, you don't need to redo your circuits. You can kind of dynamically trade off these properties and tailor it to a specific use case.
01:55:19.500 - 01:56:17.946, Speaker A: And then in terms of performance, this is, I think, one of the big advantages of STARx as well. First, the verification is very light in terms of like real world performance. Most of the proofs that for any practical computation are just single digit milliseconds to verify. And the description of the computation is succinct as well, because there is no preprocessing that needs to happen. And then on the prover side, the proof generation is also very fast. So a few benchmarks on the current implementation of Mitnvm is that right now on a single CPU core you can execute about ten k instructions per second, which is 10 Hz. If you think back to Daniel's talk a little bit earlier, where he mentioned that it took like multiple seconds for a single instruction to be executed on tiny rand, this is already many orders of magnetic improvement.
01:56:17.946 - 01:56:54.460, Speaker A: But it gets better because we can paralyze very easily. And even with the current implementation, we can get up to 400 Hz if we spread the proof generation across multiple CPUs. And we believe that with some additional work, we can improve this significantly in the future. And then if we go to GPUs and FPGAs in the future where there is hardware acceleration, this is going to be in a megahertz range and probably very fast. For all practical purposes, Starks are not perfect. There are some disadvantages to them. And the biggest one is the proof size.
01:56:54.460 - 01:57:41.670, Speaker A: Usually proofs are in dozens of kilobytes, so just a few benchmarks for midnvm. In its current form, if you execute 1000 cycles, the proof size is about 35 cycles. It's around 80. If we need to grow more, it grows, but it grows logarithmically. So it's not going to get too big, it's probably not going to exceed 150 or 200 most for pretty much any practical computation. This kind of like the proof sizes lead to relatively large gas cost when we try to verify proofs on ethereum. So between three and 5 million gas for verifying a Stark proof, probably closer to 5 million on ethereum.
01:57:41.670 - 01:58:20.358, Speaker A: But once EIP 4480 comes through, we should see that gas cost drop to hopefully under a million. But this will need to be verified. But again, because the actual proof is very fast to verify it's, just the proofs are large. Once the cold data costs get reduced, the cost of verifying Stark proof will get reduced significantly as well. The other thing is recursion and it is possible to do recursive Starks. They haven't been demonstrated yet, but especially in a context VM, this becomes like having a virtual machine. This becomes a real possibility to build recursive Starks proof as well.
01:58:20.358 - 01:59:09.990, Speaker A: And this is something that we will investigate on our end too. Now let's talk about general purpose. So if we think about ZK rollup specifically, there are two ways to think about them. So there is a specialized type of ZK roll up which handles a specific use case like payments and exchanges, NFTs and so forth. And then there are general purpose ZK roll ups which allow you to write arbitrary smart contracts. And this is the more exciting type of a roll up which developers can build their own logics smart contracts that we can think about and have the full power of Ethereum type environment within a roll up. And building a general purpose ZK roll up requires to have or at least you need to have a zero knowledge virtual machine.
01:59:09.990 - 02:00:09.670, Speaker A: So in the context of polygonmiden, well, let me talk about first about what the zero knowledge virtual machine is. So zero knowledge virtual machine is you can think about as a virtual machine that takes some initial state and set of programs, executes them and gives you some final state, but it also gives you a proof that it has executed everything correctly. And this is the crucial part where you can execute many different programs on this VM. They don't have to be all the same program, they could be different programs and you'll get a single proof that says all the programs have been executed correctly. And one other distinction is that you can provide this witness data. For example, in blockchain context, this could be signatures that once the VM has verified them, you don't really need to include them into your proof that goes onto ethereum. So you can basically discard the sweetness data or it gets discarded and that allows you to significantly compress things that need to go onto the main chain.
02:00:09.670 - 02:01:01.110, Speaker A: In the context of polygonmiden we have MIDN VM, which as I mentioned is based off of Tstaf VM. And let me give you a few highlights about the VM itself and where we're going with it. So one of the main focuses that we want to have is to have the VM be developer friendly and by that I mean we want it to be as close as a typical virtual machine that you might have encountered outside of zero knowledge or cryptographic context, so that you don't really need to learn anything about cryptography. So for example, it's going to be a simple stack based machine. This is what polygonmiden is right now. It is going to support natively 32 bit integer arithmetic so no need to think about field elements or understand finite fields or anything of that sort. You can just work with 32 bit integers.
02:01:01.110 - 02:02:11.790, Speaker A: It will have read, write memory, so very easy to kind of learn how to use it. It will have native exception handling which is very important in the context of smart contracts where one contract can call another contract and the other contract can fail. So we want to make sure that even if there is a failure somewhere we can still prove that the original smart contract that called the field contract executed correctly. And we are planning to build a very extensive standard library which will support all kinds of goodies that developers for smart contracts and DeFi might be interested in. The other important thing that I want to mention is that we want to make multi language. Solidity will be the first class citizen in this VM, but we also want to make sure we can support other languages such as for example Move or other languages that people would want to compile into Mitvm assembly. And one of the other design goals, and it is kind of related to multi language support is that we want to make the VM safe and actually safer than EVM itself.
02:02:11.790 - 02:03:17.998, Speaker A: So we will not allow dynamic code targets and you still can compile Solidity into this and still will work fine. But underneath the VM will be safer than the EVM itself which will also making bugs potentially much much more difficult or putting bugs into your smart contract and then taking some hints from Rust programming language, all operations in the VM are safe by default. And there is also privacy focus, kind of like we want to build in some privacy features into the VM but this is not the focus right now. But we do want to make sure that the groundwork is there where we want to support both data and functional privacy in the future so that the VM can execute kind of privacy preserving smart contracts. And just to kind of show this diagram of how we think about multi language so solidity we can be compiled into intermediate representation right now. And then we'll have this yule to MIDN transpiler, which will transpile U into MIDN assembly language. And then the MIDN assembly language is the native language of MIDN.
02:03:17.998 - 02:03:48.414, Speaker A: And then other languages will be able to compile into MIDN assembly as well. So I talked a lot about where we want to go. Now let me give you a little bit of a roadmap of how we will get there. And right now, basically last month we released the version 0.1 of Mitenvm. It already has kind of stack manipulation, arithmetic operations and basic control flow implemented. In a few months from now we will be releasing the version 0.2
02:03:48.414 - 02:04:45.598, Speaker A: which will have memory native support for 32 bit integers. We'll have procedures and we'll keep building on top of it over the next year to add such things as storage and support for things that are important for EDM compatibility. And our plan is within a little over a year to get to a point where you can compile solidity into mitan assembly and then in parallel toward the kind of late Q Two of next year, we will start building the actual roll up around the VM as well, starting with initial couple of alphas. And then hopefully by Q One of 2023, we'll have data in production to experiment. This is a fairly legressive timeline but we'll do our best to make it happen and then to wrap up. Just wanted to talk a little bit about kind of some of the values that we have with Polygonmiden. The first one is we do want this to be a community driven and fully open source project.
02:04:45.598 - 02:05:40.770, Speaker A: So the source code for even the early version of Midnvn is fully available right now. We do want to collaborate with anybody who is interested in building on Polygonmiden or even just using VM for whatever purposes they want to have. So please check out the repo and feel free to start using it and let us know if there are any features that are missing. So we really want to make it so that we built it with the inputs from the community. We will be releasing the initial draft of MIDN assembly probably within a week or so and feedback on that will be very helpful. And the other set of kind of dialysis we do want to build a fully decentralized and censorship resistant roll up. So not to have a single operator but to have kind of a network that is fully in alignment with kind of web3 Ethos.
02:05:40.770 - 02:06:24.990, Speaker A: And here is the link to the repo. So check it out and let us know if you would like to use it in some way even before it's ready. That would be very helpful. With that I'll open up to questions if there are any. Thank you so much Bob, and this was an incredible overview on all these improvements that you've kind of made with the help of your team. We don't have any questions specifically to what you've talked about, it's just been comments from like this is incredible and some of these things are just amazing that people are enjoying. If something comes up, I will happily share them on the chat here to you in the zoom, but no specific questions about definitely.
02:06:24.990 - 02:06:53.350, Speaker A: And if somebody thinks of something else later on, feel free to drop me a line on Twitter or anywhere else. Perfect. Well, thank you so much. And with that, we are ready for our next talk. And for this, I'd like to invite Jordy on stage. And Jordy is going to be talking about Polygon Hermes and especially introduction to the Polynomial Identity language. So, without further ado, let's welcome Jordy.
02:06:53.350 - 02:07:48.202, Speaker A: Hello everybody. Thank you very much. Let me share my presentation right now. First of all, before starting, I just want to welcome to the Polygon Zero people. It's a luxury to have them on board and very excited to work with already. As Daniel and Brandon already said, we have been already working with them in the last weeks and has been very productive. And I also want to thank you to to the Polygon founders for setting up this collaborative environment in the technical things.
02:07:48.202 - 02:08:31.254, Speaker A: I think that we can go much farther together, helping one each other as a team. So it's going to be very incredible. And I'm sure that we are going to scale ethereum at the end is the goal that we have altogether. So with that said, I'm going to present today the pill, just to understand where the pill fits in the project. Well, okay. The most critical thing for roll up is generating this proof. Generating this proof is quite hard.
02:08:31.254 - 02:09:22.190, Speaker A: But the idea of this proof is we have a node state, we have many transactions. In the case of Hermes, we have normal Ethereum transactions. We want to be EVM compatible by code, but by code we don't want to compile or using the compiler. We just want to implement the actual bytecode. So we have normal Ethereum transactions and we want to create a new state route. Okay, so building this circuit is this deterministic program or this circuit is the challenge. So to build this circuit, what we do is we have mainly the approach that we are following right now is we are building a Stark and then in order to reduce the gas cost to 300K or even lower is just to verify this Stark with a Plunk or growth 16 proof.
02:09:22.190 - 02:10:07.906, Speaker A: So this is the main approach and the fact of using Starks, this is a little bit game changer between traditional R one CS systems like Plonk or Gloss 16 and more polynomial base. This is comparing with electronics. This is just working with normal ants and ors normal Gates. And you just put them, a lot of them. Or when you introduce the clock, when you introduce a clock in the circuit, you can reduce the semelectronics and run many cycles. So here is where the polynomial stakes very important. When you read the word polynomial, just understand it as an array of values.
02:10:07.906 - 02:10:31.360, Speaker A: At the end of polynomial is a set of values. You can represent a line by two numbers. Well, you can represent a big polynomial with many numbers. So you just have the values. And this works well in this concept of clock of a step. So you have a state machine and every time in the state machine you have a new value. And this is represented by a polynomial in each step, one value and so on.
02:10:31.360 - 02:11:46.498, Speaker A: The idea of defining this proverb is that we need to commit to a set of polynomials and then defining a set of relationships, identities between these polynomials in order to build these state machines that work together. And here is where the polynomial identity language is language that we created for creating these special relationships between polynomials and building these state machines with this relationship, with this set of relationship, with this compiler. This will be used for generating the Starks, for generating the prover of the Stark and then the verifier of the Stark. Okay? So to start, I think the best way to understand how this polynomial identity language is just do an example of how this works. Okay, let's start with what would be the hello world of this language. In this case is we are defining a very simple state machine that actually computes 32 bit numbers from two numbers of 16 bit numbers. So it's a state machine that has two steps, so it's just even steps and OD steps.
02:11:46.498 - 02:12:35.478, Speaker A: In the even step we just get a number and put it in a register. And in the second step we just shift the register and add the second number and then we start over again. So we can generate as many 32 bit numbers as we want. So we have one register, in this case one state variable, this case is Oat and we have one input that's a value that we can put at any state. We can put the value that you want, you can put each value and every two steps we are generating one new 32 bit numbers. So hope we would be write that in pill, okay? This would be as simple as that. We have a constant polynomial, it's a constant polynomial that's so the first is, the first number is one, the second 010-1010.
02:12:35.478 - 02:13:24.942, Speaker A: It's like the instruction, what it's doing? We have the frame, the input of the system, the number that we freely can set to any number that we want and we have the register itself, the register out. Okay? And then we start defining the constraints to these polynomials. The first constraint is that we are forcing that free in is a 16 bits numbers. Okay? In this case we are doing it's included in another constant pole number that we have defined. It here on top that includes all the numbers between zero and FFF. So all the 16 bit numbers, okay? So with this we warranty that the free in can be any number but just between zero and zero xFF. And then we define the constraint for this state machine.
02:13:24.942 - 02:14:01.220, Speaker A: Actually, we're saying that the next step must be if set is one, if it's the first step, then this is going to be one. The second part, this is going to be zero. So if it's one, we are just putting the input to the register. Okay, if a set is zero, so it's the second step, then this is going to be zero, but this second one is going to be one. And then what we are doing is we are shifting the out and adding the free in. So every two steps we are generating a number that we know for sure that's going to be maximum 32 bits number. And we define it this way.
02:14:01.220 - 02:14:31.286, Speaker A: Let's move to a more complex state machine. This case we are going to define state machines that generates groups of five numbers of 32 bytes. Numbers that fulfills this relationship. A times B plus C must be D to two, to the 32 plus E. Okay, so we are dividing this most significant bit and less significant bit. We are doing the arithmetic operation. In this case, it's a multiplication and a division.
02:14:31.286 - 02:15:11.218, Speaker A: Okay, so how we do that? Okay, we have a stain machine with five registers, ABCD and E. And the idea is that in the five first steps we are just latching. We are putting the input, we are setting these registers, we are setting A, we are setting B, we are setting C, we are setting D and we're setting E. And once we have all the registers set, then we force the condition. When we have to force the condition. Well, we have this next polynomial, we call it latch. That's actually when you want to force that, this condition fulfills.
02:15:11.218 - 02:15:55.038, Speaker A: So we have a state machine where you just load five numbers and then you are forcing that these five numbers fulfills this condition. So how would we write that? Well, we will write this. We have this constant polynomial, set A, set B, set C and set D and set D we have also the latch polynomial and we have also the free in it's the input, and then we have the five registers. And here we start adding the constraints. The first constraints is we say free in must be a 32 bits number. Here is the first thing that it's important. We are using the results of the other state machine, the bytes four o.
02:15:55.038 - 02:16:56.026, Speaker A: So this free in must be a number that must be included in the bytes. That means that this number must be a 32 bit numbers. Because we have the conditions that we generated in the last state machine. Then we calculate the next state machine. In case it's A prime, b prime, C prime and D prime and this is if set A is one, then we just set the free into A, and if it's zero, we just keep the last value okay, so this is the conditions for the next ABC and D, okay? And finally, when latch is one we must fulfill that Molsom is zero and Molsom is like an intermediate polynomial that we say that A times B plus C minus D to ten to the 30 plus C and this must be zero in order to fulfill this operation when latch is one. So here we just defined it a state machine where we can do arithmetic operations. In this case it's this addition plus here you can do an addition just for example setting B to zero or you can do a subtraction by setting A and B.
02:16:56.026 - 02:17:27.118, Speaker A: We'll see in the next example how we use this arithmetic circuit. So let's now build something more complex. Okay? Let's build a computer. Let's build a state machine that actually execute instructions. Okay, so here we have five registers, we also have a 6th register. We call it the program counter actually is the line that I'm executing. This is another register that this is in this state machine.
02:17:27.118 - 02:18:10.260, Speaker A: We have a bunch of polynomials that are in somehow. They are the instruction that I want to execute. Okay, we'll see these polynomials, how this works, but this is the instruction. We have also a frame and with this we have a state machine to go from A to A. Prime is the next state of A and we are evolving, we are executing we can put any instruction that's defined by all these bits that compose the instruction and we are executing this. Okay, so let's see different instructions how this would work. Well, first the definition would be just we define all these polynomials the same way that we saw before.
02:18:10.260 - 02:18:45.710, Speaker A: We define here a set of for these instructions must fulfill some initial conditions. For example, we want that in a be binary. So we say that in a times in a minus one must be zero. Here, the only condition for this to happen is that in A is either zero or one. We want free in to be also a 32 bits numbers. We want also the constant to be also 32 bits numbers. But in this case we are at an offset just so that const can be also positive and negative 32 bit numbers but signet positive big numbers.
02:18:45.710 - 02:19:22.098, Speaker A: Okay? And we have an address that's just 16 bit numbers. It's the byte to the first one that we had. Okay, so let's start with instructions that move from A to B or from B to A or something like that. Okay? For this there is part of the instruction that it's in A, in B, in C, in D, in A. Here these are selectors of which register we want to we have this intermediary register, we call it up. Okay? So the data is we will load from eight B-C-D and E and we load to op. Okay, this is the first line of the polynomial.
02:19:22.098 - 02:19:42.320, Speaker A: Language. So if in A is one, then we load A. If in B is one, then we load B. Okay? We add them together. We can also set a const. We can also put a constant value so the instruction can have a constant value. For example, if we want to put seven in register A, then const would be value seven.
02:19:42.320 - 02:20:23.126, Speaker A: And we also can select the free in and we set we have these set values to ABC and D. So, for example, if I want to move from B to C, then in B would be one, the other scenes would be zero, the cons would be zero, and set C would be one. And the set A, set B, set D and set D would be zero. Okay? So in this case this will fulfill this relationship. If I want to put a seven in D, then const would be seven all in A and D in C inde and in free would be zero. The set D would be one and the other sets would be zero. And this is how we define in pill these instructions.
02:20:23.126 - 02:21:39.070, Speaker A: So the instructions for moving, okay, we can also do conditional jumps, jumps and conditional jumps. In this case we have a circuit, I'm not going to enter in detail how this works, but we have a circuit that determines if the operator is zero or not. And if it's zero and the instruction is a jump, then instead of loading the program counter to the program counter plus one, we just load a new address. So we do a conditional jump in here just by defining, by defining which condition the next program counter must fit. Okay, so we have conditional jumps and we want to do also arithmetic operations. We can do want to do multiplications additions and so on and how we do that, okay? And this is probably the most important slide of my presentation because this is the trick and the main thing that we are doing here. Okay, so here is we are connecting the last state machine, the state machine that do operations with the main state machine.
02:21:39.070 - 02:22:26.770, Speaker A: When the main state machine we have the instruction arithmetic. When we force we have this arithmetic instruction, then we are saying that the values in ABCD and the operator must be included in the last state machine. Arithmetic A, arithmetic B, arithmetic C. So somehow what we are doing is we are connecting in the main state machine. We are assuming that summary Matting function is okay. So we can set up freely, we can put the values in the registers and we are saying okay, these conditions must fulfill and we assume that this is okay. But this is because this is delegated, the verification of this is delegated to the other estate machine.
02:22:26.770 - 02:22:56.950, Speaker A: And this concept of connecting different estate machines with the Plockup again, thank you to the Aztec people with the Plockup idea. This is a great thing. This allows us to connect these state machines and this is what allows us to do engineering. We can generate like many state machines that are doing different things and connect them all together. Okay, of course, now we have a processor. So we have a processor. Then we need to define an assembly.
02:22:56.950 - 02:23:37.738, Speaker A: This is a little bit how we would work the assembly. We define this from going from one register to other and we have the arithmetic or the Jumps operations. So this is an assembly. But there is something that's missing here and it's how we warranty that we are executing the right program. We want to have like a Rom. We want to have a program and we want to force that we're executing a program because if we see here these are free inputs, you can put any instruction that you want, this is okay, it will fulfill these relationships. But how can I create a Rom and execute this program? Okay, let's move forward.
02:23:37.738 - 02:24:18.274, Speaker A: And from this assembly we create this table. So we encode these instructions with bits we have in A and B and C and D and we create what we call it a Rom. A Rom at the end is another polynomial where the first instruction, the second instruction, the third instruction and we encode all these polynomials somehow. This is a venue polynomials. We have the constant, the constant value which is wider the address and we also embed the line with the line of the program. This is the first line or the line zero, line one, line two, line three, line four, line five. Okay, so we have a set of values and with this we have a polynomial.
02:24:18.274 - 02:24:50.590, Speaker A: Okay? We have a polynomial, it's the Rom. Okay? And we want to guarantee that we are executing this Rom. And how we do that again, with a single block up, we are taking all the polynomials that compose the instruction, the address, the Set A, Set B's, Jumps arithmetic, all the instruction altogether. We pack the same way that we did it in the Rom instead of having the line. In this case we have the program counter. The program counter indicates which line should be executed and then we just force in. The last is that in strace is included in the Rom.
02:24:50.590 - 02:25:39.358, Speaker A: With this condition we guarantee that we are executing actually a Rom. Okay? So with this we can have a Rom that executed. So we have a processor with a Rom that's executing and all that defined with polynomial identities with this language. Okay? Of course, the EVM for building the EVM, the state machine is similar to this but with much more complex. We have of course, instead of working with 32 bits operations, we are having 256 operations. We need to deal with the gas, we need to deal with the max mem, we need to deal with the stacks, we need to deal with the calls. So there are some extras because we are tailor making this processor in order to execute of codes Ethereum opcodes.
02:25:39.358 - 02:26:12.614, Speaker A: So this is why we are adding these fine tuning things that will help us to implement the opcodes. But they are the same. Of course will be many other state machines like Kecax Signature Verifications Comparators. There is a lot of state machines. But with this we will create the full Ethereum virtual machine. So summarize is we have this pill language. This is like the hardware layer.
02:26:12.614 - 02:26:51.638, Speaker A: We are defining these state machines that work together. With this we define the state machine. We have an assembly and with the assembly we create a Rom which actually has a specific program. And it's a program that actually implements Ethereum. It's a program that process as an input process many transactions and calculates the new state actually is a single process that executes all these. Okay, we have an executor that actually is the runtime, the thing that takes up the transactions and actually creates these polynomials. And then with this we generate the Star.
02:26:51.638 - 02:27:12.010, Speaker A: This is better represented in this Nest slide. Okay, so we have the executor. The executor generates a proof. Of course. Here we have an assembly, we have a Rom. We compile that to an assembly and we plug it to the executor. The executor generates a okay, with the pill language we compose the Stark.
02:27:12.010 - 02:27:49.980, Speaker A: And with the pill language through Circum and all the Stark we generate a circuit to validate the Stark. And then at the end is verified in solidity. So the process is very much this of course the prover is not the only thing that we have to implement. The proverb is just a piece of running of Uzikbn. There is also a note with all the transaction pools and so on and of course all the smart contracts. Layer one, layer two here if you want to see how we are going to do all the transfers. Layer one to layer two.
02:27:49.980 - 02:28:06.682, Speaker A: I did a presentation in Liscon so you can find it in YouTube. You can find there. And of course we also have this coordinator selector. We have a new protocol. We now improve efficiency. Probably we'll have some opportunity to explain how it works. But this also what gives the centralization.
02:28:06.682 - 02:28:31.142, Speaker A: But all this piece is what we are building somehow in Hermes and the planning for Hermes. Of course, this is not full commitment. This is a project we are doing is quite challenging. But this is the schedule that we are managing internally. We are very excited right now. We are running very fast. We have an incredible team right now that's very motivating for doing this.
02:28:31.142 - 02:29:18.770, Speaker A: I think we have at least in my eyes, we have the best team in the world for creating that. I'm sure that we are going to get it. We are very excited and yeah, looking forward and hope to scale Ethereum very soon. I don't know if there is any questions, but that's all on my side. Thanks so much. Jordy there's a couple of common recurring themes in the questions, which is people want to know how they can learn this on their own pace, because there's a lot that happened in this talk and people are curious if they can find whether it's a copy of your presentation to other links to understanding where you've talked about PIL. I will make it copy.
02:29:18.770 - 02:29:51.450, Speaker A: And everything we are doing is open source. So sometimes we are not publishing right now because we are changing very much and it's things like that. But we will open very soon as the pieces get more consolidated. And, yeah, you can check out Polygon Hermes repo and there is a lot of information there. Awesome. I think with that, we are ready to move on to our panel, which is also the last discussion for today. I'll just quickly introduce all of our panelists.
02:29:51.450 - 02:30:19.370, Speaker A: This is going to be a discussion around everything that's happened today. We're just going to talk about all the new themes and all the new products. And for this panel, I'd like to invite Vitalik, Buterin Mihalo Paul, Bobin, Brandon, Jordy and a lot of you already here. And then this whole panel will be moderated by Anna Rose from the Zero Knowledge podcast. So without further ado, let's welcome Anna and team to join us on this panel. And I'll let Anna take it from hello.
02:30:19.440 - 02:30:24.060, Speaker C: Hello. Hi.
02:30:24.450 - 02:30:29.440, Speaker A: Ask everybody to turn on video. And here we go. Hello, everyone again.
02:30:29.810 - 02:31:02.040, Speaker C: Hi. So is everyone up? Let's just give them a second to put their videos on. Hilo, italic and I think Paul's here too. Paul, hi. And I think we're just waiting for Brandon. Brandon. Let's see if he can make it as well.
02:31:02.040 - 02:31:31.370, Speaker C: We'll just give it one more minute. There we go. Do we have everyone? I think we're here. Okay, very cool. So I guess we should kick off. I feel like you've already all been introduced and actually a little bit I want to share a little bit what the plan is for this panel. So we have a pretty big panel, and we have myself and Vitalik, who just joined.
02:31:31.370 - 02:32:09.630, Speaker C: So you have heard from a lot of the other panelists before. You've heard kind of deep dives into what they're working on. But the way I want to structure this is I kind of want to start with talking a little bit about a recent blog post that Vitalik wrote called Endgame. Then I want to bring in all the other speakers to talk about the different ZK solutions they're building to potentially compare and contrast them or see how they're collaborating. I want to talk a bit about ZK community, the tools that are missing, and if we have time, I want to talk about the ZK future. So let's kick off with this. I want to say, like, high level deep dive into Endgame the blog post that you recently wrote, Vitalik.
02:32:09.630 - 02:32:34.758, Speaker C: So in it you outlined multiple paths that would give us a trustless, censorship resistant Ethereum, but it would be scaled. And the first sort of block was very much a roadmap. And to me that was like a roadmap without roll ups. There's four design possibilities, four design changes that would help. So do you want to just explain to us a little bit what that roadmap looks like?
02:32:34.924 - 02:33:37.334, Speaker A: Sure. So that part of the post was basically actually trying to answer the question of if you take a big block blockchain, so something even much bigger than Ethereum, right. Like one of these chains that tries to have very high TBS and completely sacrifice decentralization and have very few nodes. Right. So going much further on that dimension than Ethereum was, if you start from there, how could you turn that into something that, say, I would feel comfortable building an ecosystem around and basically the challenge with those blockchains as they are today. Right. I think I outlined it a little bit when I wrote the post on, I think, the Limits to Blockchain Scalability about half a year ago, right, where I kind of wrote that little short story of what happens if 70% of the validators actually do collude and they try to force through some change that everyone else hates.
02:33:37.334 - 02:34:16.498, Speaker A: And if you don't have fully validating nodes, then they're going to be able to actually force that change through and users are not going to be able to do anything about it until it's too late. So the idea that I have is basically that you just add in the elements of decentralized validation. Right. And decentralized validation is something that we do know how to do. For example, we know how to put the state into a merkel tree. We know how to put state roots after every small bundle of transactions. We know how to make a fraud proof.
02:34:16.498 - 02:35:32.382, Speaker A: We know how to make it so that if the block has even one mistake in it, then you can make a fraud proof that covers just that one mistake and then broadcast that fraud proof. And then the rest of the network, even if they don't have anywhere close to the computational capacity you need to actually run a full node, they would still be able to verify this fraud proof and they would know that, hey, this sub blockchain is trying to push through something that breaks the rules. So that was one change. And then the alternative to fraud proofs was to instead use zero knowledge proofs CK snarks and just verify the correctness of the block directly. And then also if you want an extra layer of protection, you can add committees and to verify data is available, then you can add in data availability sampling, which is itself a big psychological rabbit hole. But the goal of it is to basically have this kind of collaborative, decentralized way of checking that the data of the block actually is published, right? And so if after publishing that block, the nodes disappear, then there's still the data floating around somewhere so that the rest of the network can keep building on top of it. And if it does have fraud, so the rest of the network can fraud through it.
02:35:32.382 - 02:36:02.810, Speaker A: So my argument is that if you start with the centralized chains of today and you just kind of add this extra armor on top, then you could end up with something that's pretty decent, right? You solve the fraud, you solve data availability issues. You add some extra channels for transactions to get in that have to be included so they can't censor. And the result is something where block production is centralized, but all of this kind of extra protocol armor that you're adding prevents them from actually doing anything really terrible.
02:36:04.910 - 02:36:28.900, Speaker C: And there in that first model, that roadmap, you also said, oh, I'm getting an unstable message. Just let me know if you can hear me. Okay, I can. You actually made a distinction between the block validation and the block production. Can you kind of go a little deeper without block production as a separate thing?
02:36:29.670 - 02:37:13.898, Speaker A: Sure. So I think original Bitcoin philosophy basically, like said, block production equals block validation, right? Because to produce a block, you take a bunch of transactions. You run the process of validating or executing or whatever those transactions, and then you make a block and you publish the block. And then in order for someone else to verify the block, they have to run basically the same code, right? They have to check the signatures in the same way that you check the signatures, they have to update the state in the same way that you updated the state. They have to check the hashes. In the same way that you checks the hashes, they basically repeat what you did. Right? And so there's this kind of one to one correspondence.
02:37:13.898 - 02:38:45.190, Speaker A: Every unit of effort that you used in producing a block is also a unit of effort that every other node on the network has to individually take in order to validate it. And this is one of these big bottlenecks to scalability, right? Now, with some of these newer technologies, we can actually start to decouple production from validation, actually, even if you don't take into account like, fraud groups and DK Starks and any of the fancy stuff. One other technology that lets you do this is stateless clients. So this is the idea that whoever builds the block, they have to hold the state. So they have to hold everyone's accounts, hold everyone's balances, hold everyone's public keys, and they then add merkle proofs, right? So they add merkel proofs, like these merkel branches proving all of the individual accounts that were actually touched by the transaction. And then they just include those merkle groups as part of the transaction. And someone verifying a block does not have to have the state, right? You do not need to have more than a few megabytes of Ram basically in order to verify one of these stateless blocks, because everything you need to verify the block is part of the block, right? And the block just has this extra information telling you like, hey, Bob did have 75 coins and Joe did have 115 coins and now Joe is going to have 90 coins and Bob is going to have 100 coins.
02:38:45.190 - 02:40:24.654, Speaker A: And here's the merkel proofs that prove that the numbers are all correct and that they match up with the root, right? And so if you do that, then you're already like you're making this separation, right? You're basically saying, well, in fear of producing a block requires having a big hard drive. Verifying a block does not require having a big hard drive because you just verify these the merkel proofs instead, right? And then ZK snarks, they go way further, right? Just with a ZK snark you're making production even more expensive to some extent, because you're saying if you produce a block, then you also have to do all of this extra polynomial magic with all of its overhead on top. But then once you do that, and once you create and you add the proof, verifying the proof is really easy, right? So the amount of work needed to create the block goes up, but the amount of work needed to verify the block goes way, way down. And so this already starts to bring us closer to this world, right, where you have these more centralized block producers, but with these more decentralized forms of verification, you can prevent them from doing as much, actually mean things. Because if they try pushing an invalid block through, then guess what? They're not going to be able to make a valid proof I see in a disappeared. But I guess just to kind of continue through my post on my own a bit. So that's kind of the principle that enables all of this, right? I think I might even say that's the principle that enables kind of blockchain scalability theory in general.
02:40:24.654 - 02:41:21.642, Speaker A: To me, blockchain scalability theory is all about removing this one to one correspondence and making the verification easier than block production. So then I start talking about roll ups. And roll ups are this really fascinating layer two scaling strategy where basically you do even more separation of basically block production and block verification, right? Because what a roll up does is it says, well, so you have the ethereum network and the ethereum network has its layer one state. Alice can have 75 ETH, Bob can have 100 ETH. And all of these, the Summer smart contract can have a piece of code. All of these things are just kept track of by all the ethereum nodes. And then with the roll up we say, well, we have this one smart contract and we can call it like this gateway contract and that smart contract stores in its storage a merkel route.
02:41:21.642 - 02:41:48.578, Speaker A: And that merkel route is, from the point of view of Ethereum, just a 32 byte hash, right? Like, whatever. It's a 32 byte hash. Like Ethereum handles 32 byte hashes all the time. But from the point of view of its own protocol, it's the merkel root of this whole other universe that Ethereum is not even aware of. And inside of this universe, you can have lots of people doing their own things. Like, people can have different coins. You can have even more smart contracts.
02:41:48.578 - 02:42:31.182, Speaker A: And the theory is basically that to update this merkel route, you don't actually have to directly tell Ethereum what every single one of the transactions are. You just provide a proof. If you provide a proof, then you are doing the proofing off chain and you're providing a little bit of data. And that little bit of data goes on chain. And that's just like a highly compressed record that just says Alice plus 25, bob minus. Looks like you just muted. You got muted.
02:42:31.182 - 02:43:21.758, Speaker A: You accidentally muted yourself. I got a question for you if you want to since we're waiting for Anna to come back, I have a question. I saw a couple of excellent tweets the other day talking about this kind of end state. And one of them that I thought was particularly thoughtful was, in this future, is Ethereum really for blockchain? And end users are going to migrate to the layer twos and the future of Ethereum is blockchain to blockchain connectivity. And our MultiChain future is actually all the Ethereum ecosystem. Wondering your sort of thoughts about sort of what is the future of the end user in that ecosystem? That's exactly what I'm getting at. Right? Because what you have with these roll ups is you have these extra universes that get created where the Ethereum protocol by itself is not aware of them.
02:43:21.758 - 02:44:02.578, Speaker A: All it does is just verifies a few proofs. But then the roll ups, they do have all these universes. And doing things inside of these roll up universes is much, much cheaper than doing them on Ethereum because of how the entire Ethereum ecosystem doesn't have to verify all these transactions. Just the people in those roll ups have to verify those transactions. And so what that basically means is that you have much lower scalability or sorry, you have much higher scalability and you also have much more creativity for people to do different things. You can have a roll up that runs Blossom. You can have a roll up that runs a bitcoin like UTXO model.
02:44:02.578 - 02:45:03.726, Speaker A: You can have a roll up that runs like a version of the EVM, but parallelized. And you can get a lot of these really cool benefits of having Azm multi chain ecosystem except with Ethereum acting as this common base layer that basically provides what we call shared security, right? So instead of having 100 blockchains, those hundred blockchains all talk to each other. And then if you have lots of these very interconnected applications, if even one of those blockchains get 51% attacks, then that might risk the entire network. You just say, well, no, all of these hundred things are roll ups. They all level on Ethereum and they all derive security from the Ethereum based layer, the Ethereum data availability layer and all of these things. And so in order to make any single one of them break, you basically have to make Ethereum break. Right? This kind of shared route of trust and shared route of bridging.
02:45:03.726 - 02:45:48.402, Speaker A: It allows for cross roll of communication to be really efficient. It allows for shared security. You have all these nice properties. But at the same time, what this means is that the long term future of Ethereum, either one roll up wins, in which case that one roll up has to learn how to be really scalable. And we actually end up in a world that looks really similar to the world where you start with a not scalable base layer or sorry, you start with a scalable but centralized base layer and then you add this kind of trust armor where instead here you start off with Ethereum, which is just a trust armor. And then you add this extra roll up thing that provides a scalability. Right? So if one roll up wins, then we're in that world and then if many roll ups win, then we're in the kind of interchange world but with a shared security.
02:45:48.402 - 02:45:57.910, Speaker A: So I thought that's really interesting, right? Because there's a lot of different paths to doing things in the blockchain lands, but a lot of them actually end up leading to very similar places.
02:45:58.970 - 02:46:18.922, Speaker C: Very cool. I'm back. I apologize. I dropped off at some point. But thank you so much for continuing there. And actually this is a great point to open the conversation up a little bit on this topic of multi chain. I mean, here we have with us a number of teams that are going to be building very different types of roll ups.
02:46:18.922 - 02:46:38.706, Speaker C: But first, before we do that, I wanted to actually ask Mihailo Polygon has invested in ZK and I'm very curious. Like a lot of the teams have presented very roll up focused concepts. Do you feel like it's primarily driven by the scaling need right now or do you think that there could actually be new possibilities that are opened up?
02:46:38.728 - 02:46:42.098, Speaker A: By this you mean by the ZK.
02:46:42.194 - 02:46:51.650, Speaker C: Roll ups in general, it's like investing into ZK for scaling. Do you think that's purely for scaling or do you think it opens up new possibilities?
02:46:51.810 - 02:47:28.482, Speaker A: Definitely, yeah. Fantastic question. Yeah. Thanks so much, Anna. We believe ZK based solutions have much wider basically area of application than scaling. Specifically, it's only that scaling is now still a pressing need in the Ethereum ecosystem. So we have started like basically vitalik has started something amazing and millions of people want to join this new paradigm, this new idea of global internet of value and web free and we really have this pressing need to welcome these people and allow them to experiment with ethereum basically.
02:47:28.482 - 02:48:07.034, Speaker A: So once ideally I foresee maybe in one to two years we will have some solutions in place that will be scalable, much more scalable than those that we have today and that will have security properties of ethereum and that pressing need to scale will be kind of relieved, kind of relieved. Then I believe we will see other interesting and amazing applications of Zke technology primarily on the side of privacy related to identity for example and many many really other interesting applications. That is at least my opinion or my bet for the future.
02:48:07.172 - 02:48:40.134, Speaker C: Very cool. I want to ask a question to Jordy and Brendan actually. So the two of you have proposed uniquely the Zkevms and I've been very curious about kind of how those are similar or different. I know you're working together in a lot of ways but could we explore that a little bit, a Zkevm maybe also just define what that actually means and how those two solutions are potentially a little different. Go ahead Jordy.
02:48:40.182 - 02:50:11.266, Speaker A: Yeah, I can yeah, I think there are two projects that they started in different point and probably with different goals and with different environments and we were independent, none of us were at quiet for polygon when we started the project and we come from different places. Well, I think we have good things and things that maybe other teams are better and they have good things and maybe things that we can go better. And the good thing of being in this case together in the same umbrella of polygon is that we have this collaboration relationship and right now I understand much better what they are doing and they probably understand much better what we are doing and we can collaborate and help on each other. Actually I would like this to be even opener, not just to the polygon projects, just to wider projects because we are solving, it's a humanity challenge. It's something that we need to solve if we want to blockchains to scale, we need to somehow work together and for us there is no body that holds like the other knowledge for building this. As much knowledge we can join together in order to scale this, we will do it faster and better. And this is a little bit the spirit.
02:50:11.266 - 02:50:35.038, Speaker A: Of course collaborating teams is not easy, but we are just starting. It's just the beginning. But the feeling is that we are sharing the same goals. We come from different places, we have different knowledge, but we are sharing the same goals and I'm sure that we will work every time closer and better.
02:50:35.204 - 02:50:55.720, Speaker C: Nice. Brendan, do you have any thoughts on that? I mean, I think Jordy, you just shared sort of the. Similarities. And I mean, the use of Fry like techniques in both are a similarity there that I noticed from the presentation that Brendan did earlier. But Brendan. Yeah. Maybe do you want to share a little bit about the way you're thinking about it?
02:50:56.330 - 02:52:04.698, Speaker B: Yeah, sure. I think, just to echo what Jordy said, it's been really great working with Hermes and with Maiden, and I feel like we've sort of created the foundation for something really special moving forward. I think that the way that we see think I don't want to speak for Jordy, but I think what they're working on is super cool, sort of simulating the EVM kind of opcode for Opcode or close to that in A Snark is like an amazing goal for us. Our ambitions might be a little bit more modest. Our goal is we'd like to be able to take existing solidity code and transpile it into what we call ZK bytecode, maybe swapping out some things that are really expensive in A Snark, like Sha Three for more arithmetic friendly hash functions. And yeah, just trying to build something that's very performant and can scale. So that's how I sort of characterize both.
02:52:04.698 - 02:52:10.960, Speaker B: It's just great to be able to work within Polygon and to kind of explore the whole design space and.
02:52:13.090 - 02:52:37.560, Speaker C: To I want to bring in Bobbin a little bit on the conversation now because you talk about the collaboration and then you're using Fry, which originally came from Starks. Bobbin, you're building a Stark based roll up. Do you want to tell us a little bit about, I don't know, maybe we can talk a little bit about the connection or what you're getting from some of this dialogue. Oh, you're muted, by the way.
02:52:40.410 - 02:53:30.294, Speaker A: Sorry about that. Yeah. So I think in terms of collaboration, there is a lot of I think there are some similarities with both projects that kind of we think about in many different technical areas. One example that I kind of talked about in my presentation is that because, for example, Starks are very flexible and Fry allows you to be flexible in terms of choosing the finite field you're in. Maiden is actually using the same finite field as Polygon zero is using. So we have abilities to kind of like, jointly work on different things that might be useful in that specific field and it will be used in both projects. And that's in terms of optimizations and in terms of developing Arithmetizations for specific problems that either they come up with and I can use or I come up with and they can use.
02:53:30.294 - 02:54:20.178, Speaker A: And that works very well. And there are other decision points in terms of how do you overall, I think there are three big buckets of choices when you kind of think about these solutions. And the first bucket is what proving system you use and what Arithmetization you use. And then the other bucket is kind of what is the design of your virtual machine. For example, which field are you going to choose? What is going to be your instruction set? Is it going to be very close to the EVM or as close as possible or is it going to be slightly different and then you're going to transpile. And the third design choice that you need to make is what your network architecture is going to look like. And there are different with SERMAS, we might look more into their network architecture of how they're thinking about and try to have some of the learnings from there to apply to MIDN.
02:54:20.178 - 02:54:32.780, Speaker A: But in a proving system aspect, we might be more close to what Polygon Zero is doing and so we might take some learnings from there. So there's a lot of different areas to kind of collaborate and cross pollinate knowledge across.
02:54:33.630 - 02:54:47.278, Speaker C: I feel like your conversations are probably incredibly interesting. I hope you document some of this for others because just thinking and actually yeah. What are the plans for how to share information within the and also outside of it?
02:54:47.444 - 02:55:36.654, Speaker A: I think one of the things I want to say, the conversations that we have is probably a highlight of my week. I have to wake up a little bit earlier than I usually would like to. But it is a highlight of my week when we just basically chat for a few hours about the things that we've done over the last week and what we are planning to do and what problems everybody is facing and what solutions we have found. So it's really cool. I can only second what Bobbin said and it's really one of the things of course we are really proud that we have all these amazing people and these amazing projects now under the Polygon umbrella. But what we are really proud of and what is really exciting is to see actually these teams working together. It's like very clear spirit of collaboration, not even mentioning or even thought about one team being competitor to each other or something like that.
02:55:36.654 - 02:56:15.134, Speaker A: And we really are aware that this is like early stage of innovation and it's really amazing and inspiring to watch these teams basically on these calls, discussing, exchanging knowledge updating each other. It's like really amazing and exactly as Jordy said, we would really like to be. We don't want to be a closed source ZK powerhouse or something that is not shared with the public. We want to include all other relevant projects, individuals. We are still discussing and thinking about ways how we can make this more open to the general community and to everyone basically that are interested in scaling ethereum in this way.
02:56:15.252 - 02:56:36.980, Speaker C: Cool, well, I like to hear that. Paul, I want to ask you about your solution. So when I heard about Nightfall, this idea of mixing optimistic rollups with ZK rollups, tell me a little bit about when that kind of came to fruition. Why did you decide to actually add the optimistic to this.
02:56:37.990 - 02:57:22.734, Speaker A: We've been working on privacy for what, five, six years now? We actually showed the first version of nightfall in 2018 at DevCon in prague, and we've been cranking out that we cut the gas fees, we implemented, simplified all of the data sources. We ended up with batching. And you're just in an endless race with how you kind of drive down the gas fees and the cost structures that make it affordable. So if you talk to enterprises, there's a couple of things that they really care about. Number one, they really want to have privacy. Enterprises will not do anything if I'm going to move my inventory or buy stuff from another company. That's sensitive business information, how many widgets I'm buying, who I'm selling them to, how much I'm paying for them, that is incredibly sensitive.
02:57:22.734 - 02:57:57.774, Speaker A: They will not do anything without privacy. That's like job one. And then the other issue is, and this comes up all the time with enterprises, and I'm worried about the gas fee. Are the gas fees going to be unreliable? Is it going to be too high? Right. They understand that they want some predictability in our cost structure. So back in the beginning of this year, we were looking at sort of where to go next, and we realized optimistic roll ups give us this excellent combination of kind of speed, cost, and privacy. And so nightfall three became an optimistic roll up.
02:57:57.774 - 02:58:05.040, Speaker A: And then working with polygon, we're able to turn it into polygon nightfall and add things like instant withdrawal that we showed earlier today.
02:58:05.570 - 02:58:27.558, Speaker C: Very cool. And that focus on privacy, I actually kind of, like, we mentioned this sort of, like, maybe with the scaling comes privacy. But to the other three teams here, bob and jordy, and, like, how close on the roadmap is privacy for each of, like, is that built in, or is that kind of coming later?
02:58:27.724 - 02:58:53.040, Speaker A: I can try and think. Oh, go ahead. Okay. I was just going to say very quickly the level of cooperation that you were talking about inside of polygon across those teams that has also been applied to the relationship with EY. And the level of cooperation is astonishing. So many brilliant people like brendan and mahilo is coordinating a lot of that. So I will say we're not going to silo the privacy knowledge in one place.
02:58:54.530 - 02:59:02.082, Speaker C: I wouldn't imagine that. It sounds like that would be very off brand. Cool. Okay, bob and yeah, you were about to say something about that.
02:59:02.136 - 02:59:42.000, Speaker A: I do want to say that even though the scaling is the primary goal, but I do think for me personally, privacy is also almost as important as scaling, although we need to prioritize scaling right now. But I do think when I think about the design of the virtual machine, it's like, how can we support privacy? Design of the network itself, the role of how can we support privacy in the future. And I do make sometimes very conscious design choices, which may not be necessarily ideal for supporting scaling, although it's not going to hurt it too much, but they will make supporting privacy or privacy preserving smart contracts much easier in the future. So it is very much front and center for me.
02:59:42.370 - 02:59:44.234, Speaker C: Cool. Jordy? Brendan?
02:59:44.282 - 03:00:15.414, Speaker A: Yeah, of course it's part of the roadmap. Of course, it's not like the first priority that we have right now. As Bowen says, I think we need to solve right now. The first is to scale, but immediately. Next is privacy. And this is, of course, important. And, yeah, maybe Brandon can add some value here, but I think that recursive Snarks is a key piece for privacy at some point.
03:00:15.414 - 03:00:18.138, Speaker A: So maybe, Brandon, you can add some points there.
03:00:18.304 - 03:01:06.966, Speaker B: Yeah, I think, just like you said, I think for us, obviously, we've been focused on scaling, but one of the benefits of having really fast provers and sort of focusing on commodity hardware for provers is that for applications that require proofs to be generated on the client side, we can do that very quickly. And so our prover performance sort of makes transaction creation feel like instantaneous for a private transaction. Whereas before, if you were creating a proof for a more complicated or general smart contract, it could take like 30 or 40 seconds with previous primitives. And so that's something that we're excited about. But like Bob and Jordy said, we're sort of focused on scaling for short term.
03:01:07.078 - 03:01:21.200, Speaker C: Cool. And Paul, I have one last question on your project, which is, I know the project ZK Operu, which is also a ZK roll up, optimistic roll up combined. How do you see yourselves as different from that or similar?
03:01:21.650 - 03:02:07.402, Speaker A: So we looked across, when we started out some of our work and thinking about optimistic roll ups, we looked across all the layer, two of which we sort of counted up. Twelve, four of them are focused on privacy. Zacopru is next to ours, in terms of Snark usage, one of the most efficient, I think, where we see kind of a really important difference beyond the integration. The work with Polygon is our license is a Creative Commons public domain license, and there's a very conscious choice behind that. There's no GPL, there's no anything. Right? So there's no complications, there's no friction. You don't have to sit there and go through the license terms and think, is there a gotcha or a surprise here? It is a pure unrestricted contribution to the ethereum ecosystem, and there's no surprises.
03:02:07.402 - 03:02:26.420, Speaker A: There's no monetization model in there. I realize I'm not going to make myself CEO of the year with this speech, but we wanted to drive adoption and making contribution to the community, and this is where we kind of made our stand. And in working with Polygon, we found people who are like, yeah, that's a good idea, and that made a big.
03:02:27.990 - 03:02:52.380, Speaker C: Like, the next topic I want to touch on are the tools that are missing and I feel like this kind of open source ethos, let's think of it in that context, so very usable, very open source. What are the tools that maybe the polygon teams or polygon affiliated teams or maybe outside teams should be thinking about building in the ZK space? I know that's pretty general but I'd love to hear from all of you what you're thinking about this.
03:02:53.230 - 03:03:40.906, Speaker A: Sure, yeah. Thanks Anna, for the question. And I think of course the question is very important and there's several ways or several things that we're doing in that regard. So first of all, all the teams are pursuing this Ethereum compatibility in one way or another either implementing the EVM itself or introducing compilers that support solidity, et cetera. And that is already we are doing a huge service to ourselves. First of all, we are benefiting from everything that has been created in the Ethereum ecosystem. So all the toolings from wallets to block explorers to all those very important toolings and we are very much aware building polygon and polygon POS chain example and plasma before that, we are very aware of this big difference.
03:03:40.906 - 03:04:42.606, Speaker A: Once you build a solution that is not Ethereum compatible or EVM compatible like we did with Plasma initially and once you have an EVM compatible solution, it's like a huge difference. So you instantly are in a much better position because you just simply can inherit, I mean simply relatively easily you can inherit all the existing toolings of Ethereum. That's number one. Number two, we are fortunate that we have now multiple ZK based efforts or ZK roll up specifically within the polygon ecosystem and we are already starting an internal effort to kind of introduce some standards because there are a lot of components and tools that will be shared between these solutions. So the core, I guess engines or virtual machines of these solutions are quite different, I would say. But the other components are relatively similar and there's a lot of overlap there. And moreover, there's overlap with the existing POS chain, there's overlap with the stack of our polygon SDK, another solution that we have.
03:04:42.606 - 03:05:38.642, Speaker A: And we are currently establishing these standards. And we believe this will significantly speed up the development and make the solution, all the solutions at the same time, more bulletproof, in a sense that all of them will use the components, so there will be the same components, so they will be much more battle tested and I guess, secure. The third thing that we intend to do is that as I said earlier on the panel, we have this 1 billion which is pretty significant commitment to develop these types of solutions. We will be offering extensively grants and publishing constantly missing components or tools that are required for the ecosystem in general, not only for polygon. And these are at least three things that immediately come to mind when it comes to building or introducing all the missing components. So, yeah, I'll leave it to others.
03:05:38.776 - 03:06:21.360, Speaker C: Yeah, that question of which tools this has actually come up. Recently we're doing a gitcoin side round, just FYI, all focused on ZK tech. And one of the thoughts that we had around that was what does the community actually need in terms of ZK and how is that communicated? Like, I'd love to find ways, maybe together with the polygon team to gather some of those tools and share them to the larger community so that we can find those teams to potentially build them. I kind of want to throw to Vitalik again, about like, from your perspective, maybe it's not exactly tools, but what pieces do you feel need to be implemented for some of those futures you outlined earlier in this?
03:06:23.570 - 03:07:17.022, Speaker A: What kinds of tools needs to be implemented? Let's see. First of all, the ethereum layer one needs to be improved. Obviously, we need to actually finish up with proof of stake. We need to execute on the sharding roadmap and actually add more data space so that roll ups have someplace where they can put data on. About a week ago, Ikea released to this post. It was the data sharding roadmap where Ikea talked about how to split it up into different stages, where just adding more, reducing the gas cost of call data would be the first step. And then you would add a shard or a couple of shards to just add a bit of data space and everyone would still download it, but it would at least get the sharding machinery in place and kind of test it a bit.
03:07:17.022 - 03:07:42.358, Speaker A: And then that just gets expanded over time. And then we add data availability sampling over time so the whole thing stays like client friendly. And eventually we'll just have this several megabytes, a second worth of data space that roll ups could just freely use. And that by itself is already enough to just give a huge amount of scalability for roll ups. But in order to get there, that stuff has to actually be built, right?
03:07:42.444 - 03:07:42.742, Speaker C: Yeah.
03:07:42.796 - 03:08:31.400, Speaker A: So that's one piece, and I think Zkevm implementations that even talks about them a lot, but also just very important and very general purpose infrastructure, I think. One other thing also that I think doesn't get talked about with Zkvm implementations is that they have two purposes. One of them is that they enable fully EVM compatible ZK roll ups. And the other is that they enable better light clients of the ethereum chain. Right. Because for the ethereum chain itself, we would love to snark it, we would love to make it so that you don't need as much of a heavy node in order to be able to process and verify things that are on the ethereum chain. So if that itself gets us narcs, then that's also something that's going to be amazing.
03:08:32.830 - 03:08:35.020, Speaker C: Sure, go ahead.
03:08:36.190 - 03:09:01.300, Speaker A: Number three, cross roll up Bridging infrastructure just if we're going to have a Layer Two world in general that can't be avoided. I've written some posts on how I think some cross roll up Bridging can be done in a fully decentralized way. I know there's a whole bunch of projects that are working on a bunch of different versions of ways to do that. So really excited to see that space mature more.
03:09:01.670 - 03:09:42.320, Speaker C: Very cool and then experience stuff. Yeah, okay, fine, keep going. If you have another one, I really am done. Well, you just mentioned actually Bridging and I want to understand as we imagine more and more roll ups coming out, all having these different forms. Mihalo, you talked about standards and maybe it's too early at this point, but are you all thinking about the bridges between your different potential roll ups and is this something that you already see on the horizon? Is this a problem you're going to get to later? I'm just curious what the thinking is around that.
03:09:42.930 - 03:10:35.102, Speaker A: Yeah, great question and I just might use the word standard. The word standards is we're being very cautious with that word at Polygon at this point. Like we don't want to impose any sort of constraints. We believe this is a phase really of high intensity innovation and experimentation within the Ethereum Community in general, including Polygon of course, and hence why we do not want to impose any final global architecture or ways or standards how these applications or projects will communicate among themselves. We really want things to happen organically. We see a lot of collaboration between these teams and let's see what happens organically so we might see some of these solutions converge into one. Maybe they will not continue to exist separately again.
03:10:35.102 - 03:11:27.422, Speaker A: What is really important for us is that they are all working together towards the same basically goal. So we are being really careful not to impose any standard standards. When I said standards I meant like helpful, basically reusing components in a helpful way. But that being said, moving forward in the future, like Vitalik as he mentioned, he had a very interesting idea how to bridge these rollups in a trustless manner and we are willing to explore, we just want to explore and facilitate as much innovation as possible. We believe Polygon is a very good position now in terms of resources, rich network effects and everything. And all of that was or pretty much all was enabled by Ethereum basically. So we owe Ethereum Community and Ethereum Layer One a lot.
03:11:27.422 - 03:11:32.450, Speaker A: So I think this is now the perfect time for us to start giving back actually.
03:11:32.600 - 03:12:23.620, Speaker C: Cool. I want to kind of imagine I think we're at the stage where we can start sort of also predicting or not predicting, imagining the future of ZK, what could be coming down the line. So we've talked about the private roll ups often in the case often it's like private transaction within the roll up. But when you start offering something like private computation, I'm kind of curious, what are things living in a non private and a private actually, is there any envisionment on how these would interact and I don't know, vitalik? If you were thinking about what a privacy it's a roll up in Cosmos, you call it a privacy zone. But what kind of thing what does a fully private roll up with potentially private computation actually mean?
03:12:25.030 - 03:13:20.854, Speaker A: Yeah, I think privacy preserving, smart contracts, infrastructure is a really big and exciting field all on its own. I think one thing is that there is no magic bullet, right? And that's more true for privacy than it is for scaling, right? Because for scaling you can't say that there is a magic bullet. You just try really hard and have a bunch of amazing and really smart people make Ckvm implementations and then you just copy what you have. But with privacy, the problem is that you have to start thinking much more explicitly about, well, who actually gets to have each piece of data. Because in order to make an update with a piece of data, you need to have a piece of data. And so you can't have literal black boxes where nobody sees anything. And you have to have data.
03:13:20.854 - 03:14:20.440, Speaker A: That data has encryption keys and some people see each of these encryption keys. And so you have to think explicitly about who are the owners of things in a privacy preserving system that you don't have to think about in just an EVM system. And so you are going to have to work hard on programming languages in a way that you don't if all you're doing is just copying solidity. You have to actually think hard about what applications are possible. Do applications even need to be redesigned? Like, is privacy preserving uniswap something that's even possible? Who would even have the encryption keys to the app? Or do you have to design it in a totally different way? So it's this big field and I think it is going to take quite a bit of learning. I expect we're going to start with the simple stuff like just being able to move coins around and then people are going to build more and more and we'll see what happens.
03:14:22.330 - 03:14:43.758, Speaker C: That idea of the private AMMS and this is a topic I've covered a couple of times on the show, I wonder if the roll up builders are the roll up builders working closely with the AMM builders on that front? Is there a strong connection between those two communities? Paul, I don't know if you have any thoughts on so I don't know.
03:14:43.764 - 03:15:21.738, Speaker A: If there's a strong connection, but there's a couple of very specific things that we are working on that I think are relevant. And you could make a private AMM. One is we're building something called Starlight. And the idea behind Starlight is what you can do is you can mark up the logic in a solidity smart contract. And we're doing this now in cooperation with Polygon. You can mark up the logic in the solidity smart contract and it will compile it into a zero knowledge circuit. Now, right now we can take a standard ERC 20 contract, we put it into Starlight and what we get out is the original version of Nightfall basically, right? So we can do it for a simple ERC 20.
03:15:21.738 - 03:16:02.630, Speaker A: What we want to be able to do is to basically throw any arbitrary business logic on it and then generate a zero knowledge circuit coming out of that. So you can have logic that runs on chain. And the reason you want logic to run on chain with privacy and not just do it off chain and post approve is a lot of business processes between companies. There are multiple participants. If you have a network level business problem, you can't have an individual participant successfully solve that for all the other network participants behind their firewall in their ERP system. Right now, they can see too much confidential information from all the others. So this is something that's super important.
03:16:02.630 - 03:16:23.610, Speaker A: And then the other thing is we're working on something called Midnight. The goal is to make all the transactions look the same. Right now you can still look at oh, he made a deposit, she made a withdrawal. We want to basically really make things even darker and make it harder to do network traffic analysis on who's doing business with whom.
03:16:23.770 - 03:16:43.560, Speaker C: Got it? Cool. Georgie, I have a quick question for you. I don't know if have you spoken with any of the AMM builders, any of the DeFi folks? I'm just so curious. If there isn't yet a crew doing that, we should start bringing you together.
03:16:44.490 - 03:17:37.846, Speaker A: I've been talking with some of them, but not in the roll up context. I've been talking just for auditing smart contracts and formal verifications and other stuff that are very interesting and they are managing, but not yet, but at the end they are a smart contract. So of course, if you go to privacy, this is like another story, but at the end actually it's like an example of smart contract that can be run in Zigadm. From this point of view, it's not the only smart contract because we are planning to run any smart contract on there. But of course, if it's not the first, it's the second smart contract kind that you are looking at that's important. A lot of the traffic right now in Ethereum is happening in those smart contracts so far.
03:17:37.948 - 03:18:15.250, Speaker C: I know we don't have too much time left, but I want to ask a few more things. So far we've talked really about zero knowledge for scaling, zero knowledge for privacy. But I know that there's some moonshot use cases for ZK and this is maybe not on exactly what you're working on, but since I have six ZK experts here, why don't we, if you're up for it, just sort of share some ideas or some things you've heard in the ZK space that could be really exciting coming down the line. That maybe even go past those two use cases. This is to anyone. And I hope there are some ideas.
03:18:16.630 - 03:18:53.898, Speaker A: I think they do end up having to do with scalability and privacy. But there are applications that take advantage of the privacy in very different ways. So voting might be one example. A lot of voting in these existing Dows just ends up happening completely in the clear. And I've written many times about why that style ends up leading to bribes. It ends up leading to just all kinds of collusion and some kind of fairly nasty things. And having more secret ballot elections is something that could really improve Dow governance.
03:18:53.898 - 03:19:57.540, Speaker A: So like that and the work that's happening with Macy and all of those projects I think is something really interesting. Another one is zero knowledge proofs for anti Dos or anti spam purposes. So the idea of that you can prove that you have some token and without actually proving which particular one of those people you are. I think that's something that can be used to protect decentralized messengers like status for example against Dos attacks. If the token that you're zero knowledge proving is a proof of humanity token then you can have things like UBIs and things like civil resistance in a way that's friendly to people who don't have lots of money without having to actually create on chain mapping between transactions and people's literal faces. So I think that's something that could be huge.
03:19:59.430 - 03:20:31.200, Speaker C: Yeah, I know. What you just made me think of is an example that comes more from the financial which is like whitelisting using ZKPs but actually that idea could be used also just for accessing groups or knowing that certain tokens are in somewhere without knowing anything more about that account or that user. There's some really cool identity groups group. Yeah, I don't know. This is an exciting space.
03:20:34.290 - 03:21:30.050, Speaker A: As Vitalik said. I think more or less all of these use cases go back to either scaling or privacy in one way or another, basically. But there's really so many interesting or really necessary applications. Like one recent example was this Constitution Dow where people amazingly pulled together a capital and almost bought the US constitution. And that's just one example where you really need privacy because if that pooling of capital was private then the opposing side or other bidders wouldn't be able to know how much Constitution Dow is actually willing to bid them. They wouldn't be able to kind of somewhat trivially outbid them because you just simply know the amount of capital that the bidder has at disposal. So it's just one out of hundreds of very interesting applications of ZKPs when it comes to scaling and privacy, private voting is really important.
03:21:30.050 - 03:21:51.000, Speaker A: And then the other one. I think it's going to be super important is proof of regulatory compliance without having to disclose all of your personal, private information. Right. And I think that this is going to be a super interesting one for regulators. What's going to happen the first time a company submits a mathematical proof instead of all the documentation? I want to be in the room for that.
03:21:52.030 - 03:22:29.906, Speaker C: Yeah, there's going to be a little bit of educating to get there more on the regulatory side. But what about games? I mean ZK games. There's one obviously great example with Dark Forest but I've also recently seen a lot more folks kind of come up with game ideas and yeah, I'm wondering if you've seen anything in that direction as well that might be exciting. Anyone? We can just leave that there. Games and ZK, it's a thing, check it out. I don't know haven't seen anything but.
03:22:29.928 - 03:23:04.240, Speaker A: I think once building ZKP focused applications and if it's easier to do with virtual machines or with compilers or whatever once it becomes very accessible there is going to be a lot of use cases within gaming where you want to prove something to the other person, whether it is for exchange purposes, as it is already happening using blockchains. But maybe even outside of blockchains there could be very interesting use cases for using ZKPs. But I think we do need to get to a point where a regular developer can just pick up and start programming something that has ZKP functionality built in.
03:23:04.610 - 03:23:27.058, Speaker C: Very cool in general actually. Do you see the Polygon ZK fund? This pool, this is meant I'm assuming for a lot of tools like scaling solutions, libraries but do you think that those applications that live on top but really deeply used ZK would also potentially fall under that category?
03:23:27.234 - 03:24:30.234, Speaker A: Absolutely. Also funding important research activities and everything that is directly or indirectly related to building or adoption of ZK based scaling solutions and privacy focused solutions. Everything falls under the scope I guess, of this fund. Definitely for us it was really a huge commitment. As I said in the beginning of the event, we are fortunate enough that our treasury is very strong now. But even for us this was a huge, huge commitment from our side and it just basically maybe manifests our belief how important this technology really is for the whole ecosystem, for the whole industry. And that's why we decided to commit this significant amount and really help again everything that is indirectly or directly related to either research development or adoption of this technology.
03:24:30.432 - 03:24:31.322, Speaker C: That's awesome.
03:24:31.456 - 03:25:07.720, Speaker A: And I want to mention, just to piggyback on Michele was said is that actually we are trying to do a couple of fundamental research type of things where we try to figure out engage researchers even outside of polygon to figure out how we can, for example, speed up hash functions even more. Once we have specific constraints of we're going to use this field, can we do something much more kind of performant in that specific field rather than trying to build an arithmeticization friendly hash function in general? So there are a couple of projects that we're trying to do just even in the foundational research type of area.
03:25:08.650 - 03:25:13.670, Speaker C: Fantastic. I think we may be at time, Kartik.
03:25:14.670 - 03:25:19.130, Speaker A: If there's any closing questions, we can go with that, but we're at time.
03:25:19.280 - 03:25:30.640, Speaker C: Okay, well, yeah, if there's anything that anyone here wants to mention before we sign off, anyone? PK is exciting. Join us. Maybe.
03:25:33.730 - 03:26:00.934, Speaker A: That I previously said that I'm really personally as a co founder of Polygon, I'm incredibly humbled and proud and grateful that we have all these great people now under the Polygon umbrella and such a huge support from the Ethereum community, including Vitalik. This is like very huge honor and again, a responsibility, but we are fully committed and again grateful for all the support and we are really just starting expect a lot of great things from our side.
03:26:00.972 - 03:26:13.100, Speaker C: Very nice. Cool. All right, well, I guess that wraps us up then. Good luck, everyone, and we can't wait to see what happens in the Polygon ZK world.
03:26:14.030 - 03:26:32.706, Speaker A: Thanks a lot, everyone. Thanks, Anna. Thanks everyone. Thank you everyone. And thank you so much, Anna, for Moderating. And I think we are ready for a close. And one thing I just want to say is I want to just thank everybody for tuning in and watching this whole thing, because what we're talking about, what we're doing now, is what's going to be the standard a few years from now.
03:26:32.706 - 03:27:08.030, Speaker A: And that to me, is the most exciting piece. Like, you're seeing a sneak peek at what the future looks like in different pieces. And it's just the same quote around the future is already here, just not evenly distributed, and we're just seeing a glimpse of what's to come. So thanks everybody for sharing everything that's happening in this entire ecosystem. It's not even just Polygon, but altogether Ethereum and zero knowledge and Snarks and starks and can't wait to see what comes out of all this that benefits you. Sorry. And huge thanks to you, Karthik, and your whole team for facilitating this.
03:27:08.030 - 03:27:35.022, Speaker A: Thank you so much. We're incredibly helpful. We were honored to have had the opportunity. So thank you so much. And with that, congrats on everybody who stayed till the end and tuned in to see what's happening in this entire ecosystem. This wraps up zika day for us. And for those of you who are wondering, and I know that I'm going to get this question the most in the next minute, we will be sending the POAP tokens to everybody via email directly.
03:27:35.022 - 03:27:55.320, Speaker A: So don't worry, you'll get that email directly into your inbox if you signed in. And stay tuned for more things to come from the global site, we'll be very soon sharing what 2022 looks like for all of us and hope all of you enjoy the weekend and see you all in the next year. Take care, everybody.
