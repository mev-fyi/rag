00:00:03.690 - 00:00:08.640, Speaker A: Drop off audio here, Kartik, but I will be monitoring. Great.
00:04:12.620 - 00:04:55.160, Speaker B: Welcome, good morning, good afternoon and good evening everybody. My name is Kartik and I'm excited to welcome all of you here today on our future of Ethereum Summit for Ethanline. So for those of you who don't know, this is a month long event we're doing for the hackathon and the summit side. And this is our fourth summit for the week. And I am super excited to welcome all of you. I see a handful of people that have joined us on our live page and I see a lot of chatter on our chat box. So it is my pleasure to say hi to all of you from all these time zones in all these different countries.
00:04:55.160 - 00:06:12.384, Speaker B: So for those of you who are watching this, I'm sure you're enjoying this thing on our live page, which is on Live Ethonline.org. I know we've all been doing a lot of our events this year remotely and joining a lot of these talks online. So for this event, what we wanted to do was give everybody a sort of a more collaborative experience and we decided to make this page so that we can share the same excitement with all of you at the same time. So we decided to structure this thing around being sort of a world computer where we get to share a global state and we have really kind of fun things planned for you where we can make the scene adapt to the talk that's happening. So you get to do anything from change themes to listen to our radio while we're waiting for the next talk, to check out the schedule of what's up and coming for the day. And then I also lastly want to encourage all of you to sign into the chat and explore sort of the conversation with everybody else there. And this is how you get to even ask us questions and ask the speakers any questions while they're presenting and we can relate them to the speakers themselves.
00:06:12.384 - 00:07:01.516, Speaker B: So do kind of sign in and say hi to everybody who's also listening in and we will kind of make this a lot more interactive. So with that, I want to quickly talk about sort of how we structured this month altogether. ETH Online was scheduled to run from October 2 to the 30th. And we structured about this is we've made the hackathon a big piece of this month where the hackathon happened from the second to the 22nd, which was yesterday, and we also structured a summit every Friday. So this is our fourth summit for the month. And the thing that really excites me is that for this month, we had over 700 hackers join in to participate and hack on different projects. And we have over 2600 people who have attended and signed up for our summits.
00:07:01.516 - 00:08:19.240, Speaker B: The hackers themselves span from over 65 different countries and 19 different time zones and when we kind of applauded everybody from where they were coming and participating from, it just sort of blew us away that we have representation from so many diverse places and cultures and countries. And also all six of the continents that are kind of participating are here, not only just today, but also have been with us throughout this whole month. And on top of all of that, the hackathon ended yesterday. So we had 700 people work on projects of their own choosing for the past three weeks, and they submitted a lot of their projects for kind of their final version of the hackathon judging. And I am extremely proud to announce that we've had 175 projects come out of this hackathon, and we'll be spending the next three days going over a lot of these project demos and judging for all of our hackers. And then next Friday, for our last summit of this month, we get to showcase some of our favorites to all of us, along with all of the talks that we have structured and prepared for the day. So we have two more Summits to go, including today.
00:08:19.240 - 00:09:35.632, Speaker B: And today, what you're doing is watching the future of Ethereum Summit, and we're going to kind of COVID everything about what's happening on the development lifecycle of the Ethereum project, whether that's e one and e two and everything around it. And then next week, as I kind of mentioned, we have our summit on the impact of the Ethereum community and in this ecosystem and also our hackathon finale, where we showcase a lot of our projects. So do sign up for our next Friday summit and tune in for not just seeing the hackathon talks, but also some of our summit talks that we have prepared, and especially the one I'm personally excited for, and that is a fireside chat between Vitalik Buterin and Tyler Cowan. And they'll be kind of just talking about all things economics, crypto and tech. So all that said, I want to kind of quickly jump into how we have structured all of our feature of Ethereum Summit. And before I kind of do that, I want to recognize that what we've been doing differently for all of our Summits is that kind of really making this a lot more collaborative for all of us. We recognize that Ethereum is a global community, so it shouldn't just be me here talking to all of you.
00:09:35.632 - 00:10:04.812, Speaker B: What we want to do is make this a lot more collaborative and inviting and involve a lot more people. And today I have the pleasure of welcoming two special guests who will be joining us as our co host and co emcees. We have first Franzi from the Solidity team and Emily from the Ecosystem Support team of the Ethereum Foundation. They'll be our MCs for today, along with me and Franzi and Emily. I want to welcome both of you to say hi and thanks for joining us.
00:10:04.946 - 00:10:29.030, Speaker C: Thanks kartik really excited to be here. Thank you so much for inviting me. Like you said, I work for the Ecosystem Support Program at the Ethereum Foundation. It's the program formerly known as Grants. So if you want to find out a little bit more what kind of support you can receive from the EF, you can go to our website, ESP ethereum foundation to see what we're up to and get in touch. Thanks.
00:10:29.480 - 00:10:33.380, Speaker B: Thanks for joining us from Denver and hello Frenzy from Berlin.
00:10:34.120 - 00:11:25.940, Speaker C: Hi everybody, thank you so much for having me. I'm really excited about this. Yeah, and as Kartik already mentioned, I'm Francie, I'm with the Solidity team. And in the team I'm mainly responsible for project management, community and communication stuff and also the ecosystem. And yeah, talking about the ecosystem, there's one project that is currently taking up most of my time, which is the Underhanded Solidity contest. So in case you guys didn't yet get enough hacking done at the ETH online hackathon, you can hack again or you can hack on at the Underhanded Solidity contest which is still open until the 31 October. And if you want to find out more, you can just visit Underhanded Soliditylang.org.
00:11:25.940 - 00:11:37.930, Speaker C: It's super exciting, so I recommend all of you to check it out. And yeah, additionally to my work on Solidity, I also work on the DevCon team and I'm happy to be here.
00:11:38.620 - 00:11:43.530, Speaker B: We're both excited to have both of you connect with us.
00:11:46.880 - 00:12:22.200, Speaker C: Yeah. So without further ado, let's have a look at what today has in store before we jump into the program. We will quickly run you through the whole agenda of the day. First up, we have today Zena who will give us an insights into the Light client sync. After that, Guillaume will present Catalyst, an east one engine for east two. And after that we will have Sam and Anska that will share a developer's guide to account abstraction with us. And then up next we have Alex, who's going to talk to us about the future of Solidity.
00:12:22.200 - 00:12:34.990, Speaker C: And then we have Paul who's giving a talk titled EVM Three eight four, doing Fast Crypto and EVM. And last we have Tim, who's going to be giving us an overview of EIP 1559.
00:12:35.920 - 00:12:57.410, Speaker B: And then we'll have John, who's going to be talking about the baseline protocol and giving us an overview of what they've been working on. Following with Alexey who's going to be talking about the future of Ethereum One, and kind of how do we continue doing permissionless development on E One. And then we'll have Aditya talk about how do we set up secret shared Validators for ETH Two.
00:12:59.000 - 00:13:32.300, Speaker C: And then to wrap the day up, we will have Superfys with an intro to ETH Two and Staking for beginners. And last but not least, Ben, who will talk about East Two sweet tooth roll ups and data availability. So let's jump right into the first talk of today. I'm happy to introduce Zena, who is with the Evasn team, and his talk will be about how to improve user experience for welcome, Zena.
00:13:37.110 - 00:14:11.130, Speaker A: Oh, it was on mute. Hello, everyone. This is Zena. I don't know what time zone you're in, but it's an evening here in Berlin and you got to give me a minute to share my slides and we'll jump right into it. I'm sorry, this is just going to take a few seconds.
00:14:12.350 - 00:14:14.060, Speaker C: No worries. Take your time.
00:14:16.910 - 00:14:44.846, Speaker A: Yeah, Google is asking for capture. Okay, can you guys see the slides?
00:14:45.038 - 00:14:46.606, Speaker C: That looks wonderful.
00:14:46.798 - 00:15:58.200, Speaker A: Okay, perfect. So, yeah, this evening I wanted to talk to you guys about Light clients, and this is a topic that I care about because my work on the protocol started with the JavaScript implementation of the EVM, and I really like it to become a Light client in the browser one day. And so I decided to work on this topic, and I feel like this is a topic that's not being discussed as much recently, even though it has good synergies with stateless Ethereum and the recent discussions around client diversity. With Light clients, we can have clients that don't have as much implementation complexity. They don't need to implement all the old hard fork logics. They have a simpler networking stack. And yeah, it's good to be able to have nodes running in the browser or on the mobile without relying on third party services.
00:15:58.200 - 00:17:22.478, Speaker A: So lightplants face multiple challenges. Although we have les that's running, one of the main challenges that they face is that in order to have a Light client, you need a Light server, and the servers need to respond to requests from the clients. And currently, this is an interactive process, so my mobile has to send a request to the server saying, hey, give me this data. And this causes these servers to become easily overloaded. And therefore, although most of them are currently altruistic, we need complex incentivization schemes for them to work in scale. So kind of the ideal scenario that I was imagining, and this was first expressed by others like Turbogeth Team, is to have another subnetwork with mostly Life clients, but some full nodes that just broadcast data so that the Light clients don't have to request data. But let's say a new block comes in, the full nodes broadcast the block itself and the stateless block witness, as well as a short proof that shows the total difficulty of that chain.
00:17:22.478 - 00:18:28.938, Speaker A: And this will be the focus of our talk. So we'll talk more about that later on. And this allows the Light clients to be able to follow the tip of the chain in a few seconds, and the amount of data that Light clients will have to request from the servers would be minimized. So, as I said, our focus here is determining the canonical chain. With that, I mean, let's say we have a couple of computing forks and we want the Light plan to be able to safely detect which one is the correct one, the one that everybody is using and currently the way that this is done. So the most safe but also most demanding way of doing this is to simply download all of the headers from Genesis until the last block and verify all of them and this would be just verifying the header, not executing the body itself. This is what the full nodes are doing.
00:18:28.938 - 00:19:42.206, Speaker A: But for life plans you don't really need to do this especially when you consider that the total size of the block headers is somewhere between five and 8GB. So if you want to have a client on the mobile or a browser. This is already a non starter, and the current Les Protocol works around this by kind of hard coding some checkpoints in the client in the form of CHTs. And then you don't need to really verify the block headers from the beginning, but from the last checkpoint. But we want to see if we can kind of avoid this, having this hard coded checkpoints. All right, so our design space is I broke it into a simple spectrum, and these four items that you see are kind of what we will be discussing during this talk. We have full verification that is very not complex at all in implementation wise, but it requires the most bandwidth of all.
00:19:42.206 - 00:20:47.910, Speaker A: And then we have a hypothetical zero knowledge protocol. We have flight plan, and we have EIP 29 35 that is being discussed for Berlin or the hard torque after that. So let's jump right into ZKP. This is a hypothetical approach. It's not being really suggested as being implemented in ETH one, mainly because zero knowledge protocols are really complicated and we want to avoid bringing them into consensus. But the approach would be that each block would come with a short zero knowledge proof that proves that there is an unbroken chain of blocks from Genesis to this last block and that the block headers of all of these blocks are valid according to the consensus rules. And it also tells us the total difficulty of the chain and we use this total difficulty to be able to choose between forks.
00:20:47.910 - 00:22:02.410, Speaker A: What I want you to take away from this approach is the ideal properties that we are looking for and these are ideally short proofs. So for ZK Snarks this is definitely less than 1 importantly non interactive proofs so you don't need to communicate with the client, with the generating or the miner to verify that. So somebody like a miner can produce this proof and just send it to everybody and everybody can verify it on their own. And, of course, security is important, and zero knowledge proofs have very high probability, good security. And now to the flight plan. And this was a paper published in 2019, I think, by Benedict Boons, and it offers some good features. It offers short and non interactive proofs of around 500 megabytes, depending on some configurations.
00:22:02.410 - 00:22:53.310, Speaker A: I think this was 500 KB was in their paper for 7 million blocks. It's secure with really high probability. It was recently activated on Zcash and it's being built by default in some other blockchains, I think on Grin and Beam as well. And it can be configurated to tolerate more or less dishonest mining power. But of course, Fly client protocol needs some changes to be made in consensus. And it boils down to adding a new field to the block header. And this new field is the root of a tree that includes the block hash of all the blocks since Genesis.
00:22:53.310 - 00:23:35.754, Speaker A: And the tree in question is not a normal Merkel tree. It's a customized Merkel mountain range. Let's call it the difficulty MMR. Just as a refresher, this is how a normal Merkel mountain range looks like. And the color of the leaves don't have a semantic meaning that's just because here it's already autumn, so the leaves have changed their colors. And this is an append only data structure. And here we have 15 leaves inserted from left to right.
00:23:35.754 - 00:24:24.186, Speaker A: So you can see kind of that it turns into multiple binary trees next to each other. And it's very efficient. Insertions are very efficient in this data structure. And you get very short proofs when the leaf that you're interested in is very recent. So the older blocks would have longer proofs, but the more recent ones would have shorter proofs, which is perfect for our use case. And now in Flyclin they've changed this MMR to include some additional metadata. And the more important ones are regarding difficulty and the time it took to produce that block and so on.
00:24:24.186 - 00:25:03.718, Speaker A: And here you see like a really simple example of a difficulty MMR with a simplified difficulty MMR with three leaves. And you can see that the leaves have the block hash itself. But also, let me actually turn on the pointer. Yeah. So you can see that this leaf has a difficulty of five and this one has a difficulty of seven. And when you go up, then you add the difficulty to get a difficulty of twelve. So this way you add the roots of each of these trees, you have the total difficulty accumulated in all of those blocks, which is a great property.
00:25:03.718 - 00:26:00.886, Speaker A: And this is used extensively in the protocol. Now, how do you use this tree to actually sync a Light client? Let's say we have two competing forks. We have an honest miner who's advertising a total difficulty of 700 and we have a shady looking guy who's advertising a higher total difficulty of 1000. And the Light client doesn't know which one is the canonical chain. So what it does is first sorts them by total difficulty. So the one that has 1000, it will check it first, just for first. And the gist of the protocol is just to do random sampling of the blocks.
00:26:00.886 - 00:26:54.890, Speaker A: So you don't verify all of them. You check some of them randomly and you can see that, okay, this chain has forked off at this point here and we are checking some of them. Like if you see the ones that have a green circle around them, let's say we are checking those randomly and in fact we've checked three and one of them is an invalid block. So we caught you, Mr. Shady. Okay, so the core of Lifeland is in the sampling strategy. As you can imagine, just doing a uniform sampling won't be really efficient or sufficient.
00:26:54.890 - 00:27:54.282, Speaker A: The goal of the sampling strategy is to maximize the chance of catching an invalid block regardless of the advertiser's strategy. Like whatever strategy they're using. We want to maximize the chance of finding if there's an invalid block in there. And the way it turns out is that it's like a probability distribution where we sample more from the end of the chain, like we sample a few from the beginning and as you go towards the end, we sample more and more. This is just because an attacker could fork later on to use the work that was accumulated by honest miners. And an important thing to note is that we are not sampling in the block number space. So we're not saying let's say we have a chain of 10 million blocks.
00:27:54.282 - 00:28:48.074, Speaker A: We're not saying give me the block 5 million. Rather we're saying give me the block with half of the total work. And we can do this because in the Merkel Mountain Range we're storing that extra metadata that tells us the total difficulty of each subtree. The other point to consider is that each sampling step is independent. You don't have to first get one block and then depending on whether it's valid or not, check another one. They're all independent. So what we can do is use the fiat shamir heuristic to make this process non interactive.
00:28:48.074 - 00:30:21.180, Speaker A: And that's great because then the miner can just, when there is a new block, just produce short proof of all these using some public source of randomness. For example, the parent blocks hash. It uses this parent Blocks hash to get a random seed and get the list of blocks that have to be included in the proof and send that to the light plants along with the necessary Merkel proofs in the Merkel Man Range tree. Next we have EIP 29 35 and this is an EIP by Vitalik and it's a much simpler change compared to Fly plan in terms of consensus complexity. And the difference is that we're not this time storing the Merkelman range in the block header. Rather we are using the existing Ethereum Try to store these block hashes. So, as you probably know, let's say this is a block and the block has a state root field which is the root of the account try.
00:30:21.180 - 00:31:02.810, Speaker A: So what we do is we hard code an address we say this specific address is special, like a system contract. Let's call it the history contract. And this has storage slots like normal contract. And in the storage slots of this account, we store the hashes of the blocks. And the blocks since the hard fork, not since Genesis. This is an important distinction. So we are reusing a lot of the data structures that are already part of consensus, namely the Merco Patricia Tree.
00:31:02.810 - 00:31:33.694, Speaker A: So the motivations of this EIP were stated as light client sync what we are discussing now. But it's not limited to this. Neither this EIP nor flight plant. They both are good for other purposes as well. Another one is, for example, like layer two state providing networks. Let's say clients like Get start pruning history. So as a new client, you don't have access to the older blocks.
00:31:33.694 - 00:32:29.666, Speaker A: And when there is a commitment to the block hashes in the state, then what you can do is ask a third party to give you that block along with the proof so that you can be sure that this block is indeed the third block in the chain, let's say. And this layer two solution can be incentivized as well. You can also use these for stateless witnesses because you have the block hash upcode. So block hashes of previous blocks need to be included in the witness. What we want to address here is if we can build a light plan sync protocol on top of this EIP. And we'll consider two approaches, two variants. One is very similar to flight plans and it's a random sampling approach.
00:32:29.666 - 00:33:32.780, Speaker A: The other one is a superblock approach and superblock, I'm borrowing the term from Nupo Power, another light planting paper. So first, let's see what advantage does this EIP bring? Let's say we don't have the EIP 29, 35, let's say right now. Why can't we do random sampling? The reason is that the like client doesn't know when it's asking for a random block. Let's say block 7 million. It doesn't know that these blocks that he's asking for are actually chained together. The attacker could just give some random blocks that are not even part of a single chain, but committing to the whole chain in the state. It makes it harder for attackers to do something like this.
00:33:32.780 - 00:34:56.498, Speaker A: But there are also some challenges in doing random sampling on top of this EIP. As you saw earlier in Fly clients we query in the difficulty space. So we are asking what is the block that has, let's say, half of the total work? And this is important because otherwise an attacker could create a long chain with many low difficulty blocks, but valid, like from a proof of work point of view and hide a few high difficulty but invalid blocks in the middle. And when you're randomly sampling, it's really hard to find them if you're sampling in the block number space. Again, because we're not using the MMR and we are stuck with the Merko Patricia tree, we can't do sub range checks. So we can't say, let's say when we are verifying an old block, we can't really check that this old block is actually the tip of a chain, that is a prefix of the bigger chain, like the canonical chain. Again with a merkel patricia tree.
00:34:56.498 - 00:36:20.842, Speaker A: This is a really inefficient thing to do. So we are kind of stuck with doing uniform sampling and in order to, if you want to have good security with uniform sampling, you need to sample a lot. So this means doing this approach either entails higher bandwidth requirements or it has lower security guarantees. So as an alternative to random sampling, vitalik proposed an alternative which here I'm calling superblock based sync. And the intuition here is that as you might know, the blocks in ethereum, they have a kind of a difficulty target and a proof of work solution is valid only if it's exceeding this target. And normally most of the blocks, as you can see here, I have, so this red line here, let's say that's the target over time and these blocks, they barely pass that target. But sometimes, by chance, some blocks have a much higher target and there is a relation.
00:36:20.842 - 00:37:32.070, Speaker A: So blocks that have 1000 times more difficulty than the target are 1000 times more rare. So the idea is to use these really lucky blocks as a way of compressing the chain and he Vital estimates the proof sizes. So let's say the idea would be just send all of the lucky blocks that have a thousand times more difficulty than the targets as a way of showing that this chain is canonical. Because an attacker with lower mining power cannot generate so many superblocks. Proof sizes of including all of these, vitalik estimates them to be around 25 megabytes. And this includes all the superblocks, but also a few thousand blocks at the tip of the chain. And we might need to double this estimate because we also need to send the parent headers to be able to verify each block.
00:37:32.070 - 00:38:42.030, Speaker A: So, regarding security, Nepopau was advertised for constant difficulty chains. If the difficulty of a chain is constant over time, then an adversary with less than 50% mining power has a really low chance of being able to produce more superblocks. But in ethereum, difficulty can be adjusted, it can be changed over time by 5% over time each block maximum. So a sketch for an attack could be that the attacker tries to bring down the difficulty so that they can produce more superblocks. But this can be easily avoided by modifying the fork choice rule at the tip of the chain, as I mentioned before. So these blocks that have 1000 times more difficulty, they are kind of rare. So it could be that the last few thousand blocks don't have any of these superblocks.
00:38:42.030 - 00:39:30.970, Speaker A: So we have two options. Either send all of the blocks after the last Superblock as part of the proof. Another one is to send Superblocks of lower degrees or lower levels, let's say blocks that have 500 times the difficulty or 100 times, and so on. So in this talk, we saw all the spectrum from full verification to zero knowledge proofs. But I think in the middle, between full verification and ZKPs, we can find some good middle ground. Fly client has good features. It has multiple use cases, but it has somewhat higher consensus complexity.
00:39:30.970 - 00:40:24.960, Speaker A: EIP 29, 35 has lower complexity. It might need higher bandwidth. The algorithms that we discuss haven't been formally analyzed, so this is something that has to be done, but it might suffice to replace CHTs. And in future, what we plan to do is come up with more detailed algorithm for the Superblock approach and prototype both the Superblock approach and the flight planned approach and compare them quantitatively for more exact comparison. So that was it. There is also a text format for the same presentation. You can find the link on this page.
00:40:24.960 - 00:40:36.880, Speaker A: And otherwise, if you have questions, I'll be available on the chat or on Twitter. Feel free to reach out and I'll be happy to take any questions.
00:40:38.450 - 00:41:02.570, Speaker C: Amazing. Thank you so much, Zena. I think we have time to take one more question from the chat, and I believe there was also some more discussion going on, which you can then maybe take offline after the talk. But one question that we want to ask is from TRX 3114. What are the implications of this architecture in terms of privacy for the users of the Light client?
00:41:06.110 - 00:41:50.250, Speaker A: I don't believe there should be a negative effect on privacy for any of these approaches. In fact, it might be better because we're trying to reduce this request response mechanism. So when you request something, when you request a block, that's an information being leaked to the full node, but when you make it broadcast, only then the full node doesn't know who's getting this message, this proof, and what they're doing with it. So I believe it might be better for privacy. But yeah, I'll be happy to think more about the privacy consequences.
00:41:51.870 - 00:42:07.070, Speaker C: Okay, amazing. Yeah, I think there was some discussion going on in the chat also, so maybe you want to have a look. And then we will wrap this up here. Thanks so much for joining us. And thank you, Francine. Talk of the evening. Bye, Zina.
00:42:07.070 - 00:42:39.370, Speaker C: For all of you who didn't visit the website yet, we have a chat on Live. Easonline.org where you can join and where you can get the full experience, the full digital conference experience. Next up, I'm happy to introduce Guillaume. Guillaume is part of the Go Ethereum team at the EF and also is focusing on scaling. And he will introduce Catalyst, or give us a talk about Catalyst and ETH One engine for ETH Two. Welcome, Guillaume.
00:42:40.430 - 00:42:49.740, Speaker A: Hello. Thank you. Wait, I need to share. My screen. You went a bit fast. Here we go. Can you guys see it?
00:42:50.110 - 00:42:50.918, Speaker C: Yes.
00:42:51.104 - 00:43:34.662, Speaker A: Awesome. Yes. So indeed, I've been working for the guest team and they asked me to look into how to reuse this code that has been developed over the last five years, maturing and getting more secure, how to use it into this new adventure of East Two, which is going to be a lot of new features and therefore more risky. So I looked into building something that has some software that I call Catalyst. It's a reference to mass effect. Oops, that's a bit early. And yeah, I actually don't quite know Mass Effect that much.
00:43:34.662 - 00:44:43.482, Speaker A: But since Geth were the bad guys in Mass Effect, I picked that name as the way to accelerate the East Two transition. So, yeah, I just mentioned briefly the word ETH One engine. So what is that compared to ETH One client? Danny Ryan two weeks ago, I think made a very nice presentation where he explained that East Two is more focusing on he's splitting blockchain into several layers. And he says East Two is more focused on the consensus part, the data part, and not so much the execution part is left, let's say, to the execution engines. So the end result is that you have a lot of data and E Two really doesn't know what this data is about. So it needs some kind of helper that knows how to process that data. And this is the role of Catalyst or of any other ETH One engine.
00:44:43.482 - 00:45:42.062, Speaker A: So what it does is it keeps a local view of the chain and it also keeps the transaction pool. And yet the ETH One Shard client, which is actually an E Two component, is connected to the E Two network and communicates with the ETH One engine via RPC. So there's still, at the moment, at least in Catalyst, a connection to the ETH One network, which is used to update the transaction pool. So transactions still come from the old well, the old network. And yes, let's see how that enrolls. So, for example, in the case of a block proposal, you have the East To client determines it has to create a new block. It determines that from its connection to the network.
00:45:42.062 - 00:46:59.690, Speaker A: And so what it will do is contact the ETH One engine to say, hey, propose a block. And what the ETH One engine will do, it will look at the transaction it's got in its pool, it will select some and use them to produce a block. And it will add it where the Shard client asked it to add it. So it can actually have several forks because presumably because of the way the consensus engine works at the East Two level, you can have several concurrent fork and forks, and the East Two client will have to pick which fork. But until it has decided, it needs to be able to handle several forks concurrently. So once it decided, once it created that block, it will return it to the Shard client and the Shard client will keep that until it's presumably it will not keep it, but it could just decide to try several chains and ultimately it will return one block to the Ethereum network. So it will propose one block.
00:46:59.690 - 00:47:51.978, Speaker A: So what happens on the other side? On the other side of the coin, it's the Validator. The Validator receives the block from the network and it tells the ETH One engine, okay, you know what this is about. Tell me if this block is valid. So the East One engine will because it receives the block and it knows what the parent block of that new block is, so it tries to insert it in the chain. And if it successfully manages this, it will remove all the transactions that are in this pool and that are already taken by this block. And then it will return like the Go signal, say, okay, this block is valid. And ETH One Shard client can also collect those responses and decide which one it wants to vote on.
00:47:51.978 - 00:48:50.138, Speaker A: So ultimately, the ETH One Shard client of the Validator will send a message sorry, a vote on the East Two network. Yeah. So basically that's roughly what Catalyst is. It's some hack of geth at the moment with extra RPC calls and some block propagation has been deactivated. We built a demo with Michael Kellin from Consensus. There's a simulated ETH One Shard client that runs and communicates with Catalyst. And we created several instances of this process so we can see the Shard client telling Catalyst to create a block.
00:48:50.138 - 00:49:17.254, Speaker A: Catalyst producing this block. It's being sent over to the other machines. The other machines validating this block. It made us very happy. So check it out. There's a YouTube video somewhere that Mikhail did, and very fun, I would say. Why aren't we shipping this already? It's because we start using like the chains that we use.
00:49:17.254 - 00:50:00.482, Speaker A: The ETH One chains that we use are created for the purpose of this demo. There's no historical data. And so the big question to answer next is how do we transfer from the proof of work chain to the proof of stake chain? And that's a big question to answer because many things can go wrong. Well, first, there can be a bug. No one is perfect. And especially new code is particularly fragile in this respect. It would be great to be able to if a problem is detected, it would be nice to have a chain to fall back to because this is a billion dollar ecosystem.
00:50:00.482 - 00:50:41.810, Speaker A: So we want to make sure we don't just flip a switch and break everything. But the biggest risk, apart from that are the miners. Miners. They invest electricity, they invest in hardware. They need to get some money back. And the problem is the expense of electricity is good as long as they can expect the next block. But if no more blocks are coming, or if they cannot expect to be the next person minting the block as the deadline with the merge, what's called the merge approaches.
00:50:41.810 - 00:52:00.822, Speaker A: You can think those people are not going to be as interested at spending energy on this block and they can go and have a lot of strategies, but the problem is, if they exit en masse, it's going to have some impacts on the network. So there will be a slowdown because the difficulty takes some time to adjust. And of course, if a lot of miners disappear, what will happen is that you can have a situation when 51% of the network is controlled by one sorry, 51% of the hash power is controlled by one entity, maybe a click. But there's a risk of collusion. And we know the Validators don't really want to they're not really interested in becoming proof of I mean, early feedback from Validators indicate they're not that interested in becoming early feedback from the miners indicates they're not really interested in becoming Validators. It's a different kind of business. So they might try several things to prevent the merge.
00:52:00.822 - 00:52:32.386, Speaker A: For example, doing reorgs, attempting to do last minute reorgs. We are about to switch and then we realize the block we were building on is the wrong one. I will explain this in more details later. They can try double spends, they can try a lot of things. And they can also try to put a short on the price of ether and then exit. And so the block time starts expanding. So people realize, oh, the Validators are leaving.
00:52:32.386 - 00:53:32.370, Speaker A: So they dump the price of ether and they can rake in, buy everything and come back and be bigger, like increase their stakes in the new world. So I'm going to cover a couple ideas that were floated around how to process or proceed with the transition. The first one was to use the so this one is pretty self contained. It's how to use the finality. So, as you might know, East Two has a finality. So after a while, there's one branch or one block that is declared as final. All the other branches that do not include this block are discarded because the beacon chain on one hand can indicate which one is the proper shard block.
00:53:32.370 - 00:54:23.414, Speaker A: That is correct. And because before the merge, the Validators of the shard will be monitoring the East One chain, you can use this to introduce finality to the proof of work chain. So I realize I need to explain a bit what this diagram represents. So squares are blocks. So the parent is on the left and the child is on the right. And the reason why the arrow goes from the child to the parent is a hash reference to the parent to symbolize proof of work. I have used the pickaxe and unfortunately I couldn't find anything to represent stake properly.
00:54:23.414 - 00:55:11.674, Speaker A: So I use this terrible pun. I'm sorry, but yeah, that's the best I could come up with. So the idea is that after a while the beacon chain here will indicate that this block here, which is in the ETH one shard or shard zero has been finalized. And because the validators of this chain keep watching the proof of work chain, they can also get this information and discard everything that came before. So yeah, that's an approach that has a lot of advantages. One of them is that like I said, it's self contained. So even if Ethereum 2.0
00:55:11.674 - 00:55:44.466, Speaker A: stopped there, there would already be a benefit here. And the benefit is that you can actually delete a lot of data that is no longer useful. This comes at a price, of course. Full sync is unlikely to happen. You will need to start always, you will need to find a trusted source that can give you what was the last finalized bug. Excuse me, and sorry, they say bug the last finalized block. And then you have to synchronize from there.
00:55:44.466 - 00:56:08.234, Speaker A: So this is an idea, for example, that works great in conjunction with Regenesis or even Light clients. But yeah, that's also something that has some inconvenience, one of them being the data download time if you start downloading. So for example, in Geth right now, you have the Fast sync, the fast.
00:56:08.272 - 00:56:08.970, Speaker C: Sync.
00:56:11.950 - 00:56:56.422, Speaker A: Fast forwards to a given block and then downloads the data for that block. And the problem is that Geth doesn't keep the data for that many historical blocks, the state for that many historical blocks. So it starts downloading the state. But at some point you get beyond those 256 blocks for which Geth keeps the state and then you don't have any more data to download. So you have to reach like to fast forward to a new block and start the process again and hope that this time you can synchronize faster than the head advances. So you would have maybe the same problem with this approach. But on the plus side, it's very easy to implement.
00:56:56.422 - 00:58:02.974, Speaker A: All you need to do is in the reorg code is to say, well, if that goes beyond the last finalized block, don't reorg. So yeah, there's this idea that's of course a bit of a bummer, but I think it's pretty interesting to consider it. Nonetheless, it's simply to never do any transition. You just introduce the shard mechanism in the current chain, the shard communication mechanisms in the current chain, and then you decide the shard zero is always going to be the main, like a proof of work shard. And then you start the other shards. And the advantage of this is, well, that you can actually leave it to the community to sort of vote whether they really want shards or not. Because if all those accounts start migrating to other shards then, well, first, you know, shard actually brings value, which we expect them to.
00:58:02.974 - 00:59:16.790, Speaker A: But it's nice to have a confirmation. And the other thing is, if a lot of big contracts move away, then it's much easier to experiment and tinker and try to get the proof of work shard to transfer at a later time, because if there's a big mistake, presumably the damage is much more limited. So it's got some interesting aspects to it. And of course, like I said, you can try to roll out any other method of conversion later, but at least you let people escape before you start playing with the ship. And yeah, the other advantage is that if the migration fails, you can always go back to the main shard, like the boat is still afloat. There was a project that I wanted to mention, it's called the Eastwinx 64 that precisely took interest into shard communication. And yeah, that would be a great benefit to this approach.
00:59:16.790 - 01:00:05.190, Speaker A: This one is kind of crazy, but at the same time, for the sake of the argument, I'm sharing it. So the idea is to basically interweave, like, proof of work blocks with proof of state blocks and try to increase the number of intermediate blocks over time. This is an approach that is extremely complex, but still interesting to play with. And so the idea is, like, you space out the amount of time between two proof of work blocks. The biggest problem. So the advantage first is that if it fails, you can reduce the number of block and go back. So it's pretty extensible.
01:00:05.190 - 01:01:08.010, Speaker A: The question is we don't really know when miners are going to decide, oh, those two blocks are too far away, let's just drop that. And the other problem is indeed, like, if you keep the current main chain approach without the finality, at least you will find yourself adding way more blocks and block headers block bodies are taking a lot of space. So there's a growth issue that we expect will be saved by will be fixed when we switch to East Two. But in the short term, that would make things worse. Then there's the idea of the AirDrop. So it's actually independent of any method. The idea is that you keep the chain running and you bribe the miners to keep mining the chain even though it's going to disappear.
01:01:08.010 - 01:02:05.054, Speaker A: And the way you bribe them is, since the Shard Zero is still monitoring the proof of work chain, they can perfectly attribute an extra reward when they see that this miner has produced a block like we hoped they would do. And this block has been confirmed, like has had six confirmations, a reward is given to them on the new chain. So even if the chain dies, they will get the money they should have gotten if the chain was still running. Yeah, so it's pretty safe as a technique. It's not complex. And as a user, you can try to send a transaction on both chains at the same time. I don't think there will be any problem.
01:02:05.054 - 01:02:35.586, Speaker A: At least I don't have vision. I think the same transaction should work on both. The biggest problem is really with legal. The business of a miner is to a mining pool. They put work and they get money for that work. So that's perfectly fine. But when you move to a staking model, what you do is that it's like an investment.
01:02:35.586 - 01:03:22.870, Speaker A: You put some money in a bank and you get some investment back, like more money back. And that is something that is heavily regulated, that is something that requires a lot of KYC. So there's a bit of a risk to merge both of them. We discussed that. We don't think there will really be a problem, but there's a difference between what happens for real, sorry, what the lawyers say and what the mining pools think. So there's a bit of a need to clarify and to make sure they don't panic and still accept that scheme. And there's the last method which the one I call Yolo.
01:03:22.870 - 01:04:44.530, Speaker A: And the idea is simple, you just fork and you hope for the best. So that was, let's say, the sarcastic way of presenting it, but in reality it has good chances of working. It's really not that bad. And the idea that is still being it's pretty fresh. But the idea is that at a given block the validators the validators keep following the proof of work chain and they are validating the shard number zero. So when you reach the predetermined block, what they do is if everybody agrees what the block number N was, they will simply register this on the blockchain sorry, on the shard and therefore bootstrap the shard with activate or how do you call it, launch the execution engine. And because they do this so basically they ignore everything happening in the proof of work chain anymore.
01:04:44.530 - 01:05:53.954, Speaker A: They launch it and they keep running the execution engine. One thing they can also do is to basically also send that information. So if the proof of work chain has a client that is able to understand a bit of what's going on in East Two, you could actually share the information that the execution engine on the East One shard has been bootstrapped. And if so, those miners can keep producing empty blocks so they get that signal. They know they only produce empty blocks so that there's no competition between the chains. And yes, afterward we use the same technique as before, which is the AirDrop or the payment. One thing I forgot to say about the payment is that it could actually be done by instead of being done through the protocol, it could also be done by the Ethereum Foundation, for example, which I think offers more flexibility because we know some people I mean, it happened with Ethereum Classic.
01:05:53.954 - 01:07:06.574, Speaker A: So the proof of work chain could become some kind of original recipe ethereum and those people might want to be paid in original recipe ether. So the human method, even though it looks a bit more centralized, I mean, it is centralized in a way it would allow people who really want to keep mining this chain to be paid into the currency they care about. All right, and I'm going to finish with the next steps. So what remains to be implemented? The finality I think that's the lowest hanging fruit I started on that. So, yeah, that's the next delivery. Then there's the idea that I alluded to in a previous slide. Let the East Two client understand a bit more what is going on in the East Two sorry, the East One client understand a bit more what is going on in the East Two world and maybe get this information even before the switch, before the merge that could be used.
01:07:06.574 - 01:07:54.960, Speaker A: For example, Vitalik has made a few proposals where you could actually try to read the Beacon State and do several operations even before the whole Phase One and Phase Two have been launched. Do a bit of the same thing that shards do, but through a contract on the East One chain. So it would offer already an avenue for people to try or experiment with the East Two UX before they actually switch. And yes, so the last thing that remains to be done is to make a regular release, have a standard release process. That's all for me.
01:07:57.250 - 01:08:19.800, Speaker C: Wow. Thank you. That was very insightful. Guillaume, a quick reminder to everybody, if you want to ask the speakers. So in that case, Guillaume, a question, please do put it in the chat so that I can relay it to him. The chat is on live easonline.org. And I also have a question for you already.
01:08:19.800 - 01:08:28.418, Speaker C: What has the process been like to communicate with miners and gotch their sentiment around the project and around the different options?
01:08:28.604 - 01:08:59.490, Speaker A: Right, very good question. We are still looking for miners, so if you know any miner that would like to answer those questions, please contact us. But so far I've been to the Discord Channel of the Go Ethereum discord Channel, tried to find a few people, but I'm always happy to talk to more of them because clearly I only talk to a few, so I'm more than happy to expand my horizon.
01:09:00.470 - 01:09:09.990, Speaker C: Okay, then another question. Will there be any sort of a testnet that the public can join?
01:09:10.890 - 01:09:44.900, Speaker A: There will be at some point, the idea we haven't really discussed that yet, but from what I see, you need a fork of gurley. I think it would make sense to make a fork of gurley and try to connect it to whatever east to testnet is running Phase One. Or we could just use the simulation like we did in the previous prototype and use a fork of girly. So, yeah, there will be a public testnet. I think it's important.
01:09:45.430 - 01:09:53.934, Speaker C: Okay, cool. And then lastly, is there any way for outsiders to get involved in catalyst PRS?
01:09:53.982 - 01:10:19.580, Speaker A: Yeah, I haven't created that's a good question. I haven't really thought of that because so far Catalyst was a bit of a prototype. But yeah, you can contact me. There's plenty of work to do. I give my branch, you can always open PRS against my branch. Or you can contact me and I'll find you some work to do, no problem.
01:10:20.110 - 01:10:22.938, Speaker C: Amazing. Cool. Thanks so much, Guillaume.
01:10:23.114 - 01:10:23.854, Speaker A: Thank you.
01:10:23.972 - 01:10:55.340, Speaker C: And with that, we are moving on to our next talk of the day, which is going to be presented by Sam and Anskar, who will share an overview of account abstraction, a proposed change that would allow depth to pay fees for users. Oh, I see we're a little bit early. I hope Sam and Anskar are there already. Sam Anskar welcome. In case you're.
01:10:57.440 - 01:11:24.170, Speaker A: So sorry, audio cut off for a second. Okay, I don't yet see a shared screen. Here we go. Okay, so we are here to let me also share my video. Hi everyone. So we are here today to talk a little bit about account obstruction. And the first thing we want to do is just briefly to talk about what even is account obstruction in general.
01:11:24.170 - 01:12:17.000, Speaker A: As you're all probably aware of, the whole idea of Ethereum was to move from bitcoin where you don't have programmability especially you have always fixed effects, right? A transaction can always only send value from A to B or from one UTXO to another UTXO for bitcoin. Whereas in Ethereum you have contracts and what contracts give you is full programmability. However, on ethereum today, that is only true for the effects of a transaction. So the actual validity on ethereum when is a transaction valid and can be included on chain, is always statically analyzed. That is only like a signature check and ECDSA signature and then a non check. Whereas on bitcoin you do, funnily enough, funnily enough, have like a limited programmability with the bitcoin scripting language. It's not turing complete and everything, but at least there's something.
01:12:17.000 - 01:13:05.930, Speaker A: So you can have native multi sticks on bitcoin, you can't have them on ethereum. And now Ethereum with account abstraction basically just is aiming at fixing exactly this. So you just add programmable validity on top of ethereum as you have it today and basically why do we want to talk about it? Well, we recently published an EIP exactly on account obstruction. And then this is also like a good point to briefly talk about the we. So who are we, sam and I? We are here on the call today and to present to you this EP. And then we created the EP together with the rest of the Quilt team, that is a research team within consensus and together with vitalik and our focus. Quilt has been mainly on ethereum two in the last year or two.
01:13:05.930 - 01:13:40.820, Speaker A: And we actually came from the world amateur. Maybe you heard the words of execution environments, right? That was one of the big things we looked into, but it was more and more clear that most likely in It two. Initially we will have, especially with Phase 1.5 where we just bring the it one as it is today, into the E two context. First, before we do all these more advanced things. And so we looked into how can we bring the best of these more advanced E two features already to Ethereum today? And this is what led us to account abstraction. But very importantly, don't panic.
01:13:40.820 - 01:14:05.580, Speaker A: We really try to make sure that this is like a very simple overview. We don't go too much into the technical details. So let's start maybe with a simple motivating example. Say you're a user on Ethereum and there's a contract. Maybe it's like a multi SIG, maybe it's a smart contract wallet, right? Something where you want to send it's. Basically your contract. You want to send from that contract a transaction to a different target contract or account or whatever.
01:14:05.580 - 01:14:56.236, Speaker A: What you have to do today is you have to own an account, an extra account on the Ethereum Mainet that has ETH in it. Then you have to send out a transaction to the contract and then the contract has to send like another transaction to the target. If you don't own a contract, an account, what you can do instead, you can use a relayer, but then the relayer needs to have an account and then a relayer needs to send a transaction and then the contract has to pay the relayer and that just adds even more overhead, right? Basically the idea behind account obstruction is where we want to get to is this simple picture, right? You have a user, the user can just immediately initiate a transaction, add a contract and then the contract pays for the transaction. The contract does whatever you want the contract to do, right? And this is the goal. And so the question is, how do we get there and what does it look like? Yeah.
01:14:56.258 - 01:15:27.210, Speaker D: So this is what Solidity could look like, not what it will look like. And this is based off of an early prototype that Quilt put together of an earlier version of the EIP. So the first thing you're going to notice is the new account keyword before contract. And that signifies that this is an account abstraction contract. The next thing you'll note is that the signatures are embedded in the Call data, which you can see in the transfer function signature. This is a two of two multi SIG. So you end up getting two different signature arguments in the Call data.
01:15:27.210 - 01:16:03.712, Speaker D: We've skipped over it in this slide, but those signatures would be checked in the Perform validation step, which is followed by PayGas. PayGas is a new opcode we're going to get into later. But essentially it determines how much the contract is willing to pay for the transaction. Once Pay Gas is finished, you move on to regular transaction execution. We have a series of examples that we think are really good at representing account abstraction and what it brings. These are not the only uses for account abstraction, they're just illustrative examples. So I want to first talk about the smart contract wallet, which I'm sure most of you are familiar with.
01:16:03.712 - 01:16:55.296, Speaker D: But for those who aren't, it's a wallet which is implemented on chain that provides additional features like social recovery, batch transactions and other cool things you can't do with an EOA. Nosisafe and Argent are two existing examples, so it's clearly possible to implement it. But, and this is a big but they still require an EOA to pay for gas. With AA, smart contract wallets can pay for their own gas and you don't need the EOA anymore. The second use case we'd like to look at are Mixers, which anonymize transactions by batching deposits and then allowing different accounts to then claim those deposits. Tornado Cash is a prominent example today, but the problem with mixtures as they exist today is that you need ETH in your account to claim the funds. So you need external relayers.
01:16:55.296 - 01:17:01.860, Speaker D: Again, with AA, the withdrawn funds, the anonymized funds can pay for the transaction and you don't need the external relayers.
01:17:02.840 - 01:18:19.820, Speaker A: A third example is what we call like in app tokens. And so you can imagine that you have a DAP and the developer of the DAP wants users to be able to just interact with the DAP directly without having to worry about the Ethereum chain and Ether and basically jump through all these hoops, right? And so the idea is that you could just have users pay for interactions with the blockchain directly in DAP tokens. So that could be if it's a game that could be in game currency, or if it's like a different DAP, there could be whatever kind of token, you just can directly pay the contract and then the contract just sponsors your transaction. And so you still get all the benefits from directly interacting with the blockchain. You don't have to go through some indirection, but you don't need to learn about all these to basically ever care about mainnet. And for things like onboarding, while you could obviously do it with your own account and with Ether, you can always also just use a signed message from the web creator. They could basically just give you a signed onboarding statement and then you can just go directly to chain again and execute that and be onboarded in the contract.
01:18:19.820 - 01:18:45.304, Speaker A: And then the tokens, obviously there can be real economic value tied to it. It can also be simply used as a limiting mechanism. This is like a very extensive use case, which we think could have many different applications. And also with AI, obviously it's all just native. And then the last use case, which we call DeFi Arbitrage. This is somewhat also to highlight like I would say, the limitations of account abstraction. But.
01:18:45.304 - 01:20:32.630, Speaker A: The idea let's start with that is obviously I think you all know what DeFi is, right? And so oftentimes there are just simple arbitrage bots that look at or try to find opportunities to have simple profitable arbitrage traits all within one transaction. And so what you'll commonly see on Mainet today is like a pattern where these bots send like a transaction to one of their own deployed contracts. Then the contract checks for opportunities for arbitrage and if they don't find any they just exit immediately. But the issue is that obviously that is inefficient because then you have a lot of these dummy transactions that just exit immediately. They just go to the contract check for opportunities, don't find any exit, but they still obviously take up all the block space, right? And so the idea is with AA what you would hope to do is you could also just tie the validity of these transactions to the existence of the arbitrage opportunities so they cannot ever make it to chain unless they are also willing to actually do something right? And so that would be an advantage. But and this is important and we'll talk about it later a little bit more for many of the more advanced DeFi use cases obviously your validity depends on probably on the state of multiple contracts and then it becomes very challenging to make these use cases work. So this is on the most advanced side of things and so one question that we often get before we then obviously next thing will be we talk about the actual technicalities of the AP but one question that we always get is how does account abstraction relate to all these other proposals? There's been quite a few other proposals in recent times that also want to solve some of the usability issues around Ethereum and specifically these two are somewhat related and so we want to briefly mention them.
01:20:32.630 - 01:21:39.226, Speaker A: So the first one is ERP 20 711. This is sponsored expiring and batch transactions which is a really interesting ERP it is mostly orthogonal to AA and so obviously AA itself decides if you want to pay for a transaction whereas sponsored transactions decide like who pays for that, right? It is important though to note that this specific EIP as it is proposed today is not compatible in the sense you could still have both on chain but AA contracts could not be sponsors then under this EIP. And so we were working on an alternative, specifically Sam is working on an alternative EIP as a pre compile that also does the same thing. It basically authenticates a message and then sets the message sender but it would be compatible. And then last thing obviously this EIP had multiple individual proposals and so it is important to note that batch transactions, that is definitely awesome for existing accounts today, but it is to note that AA contracts as they are fully programmable, they are basically natively able to do batch transactions. You can just send them a bunch of call data and they can then do whatever processing they want and send multiple subtransactions. That's fine.
01:21:39.226 - 01:22:37.902, Speaker A: So you get batch transactions natively with account abstraction. And then the other EIP, I want to just briefly mention that's 28 three, that's rich transactions. That's also really interesting in case you haven't heard of it, do look into it, but it only brings very limited execution capabilities to EOAS and most importantly, it does not give you programmable validity what AA is all about, right? So what it does is basically it lets you execute some code against your count account, which is interesting, but it's mainly useful for batch transactions. And again, you get those for free with account obstruction. So let's now dive into the actual technicalities of how does the EAP work. And so it's important to note that account obstruction has been around forever is a strong word, obviously in the blockchain space. So there's one very early EIP ERP 86 that was authored by Vitalik and that's from February 2017 already.
01:22:37.902 - 01:23:01.826, Speaker A: And that was called abstraction of transaction origin and signature. And that's basically where this already started. And the idea obviously can be traced back even further. The challenge so far has been that implementing a counterproduction is a rather extensive change to the protocol. And it's hard to reason about the exact safety implications. Right. There are all these different actors and you could have safeties at multiple levels.
01:23:01.826 - 01:23:56.630, Speaker A: And so this is the key issue that has to be addressed for counterproduction to actually make it to mainnet. And so our approach was to split the changes of the ERP into two parts. The first one is the protocol consensus changes, right? This is what every node in the network has to support to be able to continue to process blocks after a potential fault. And we would really wanted to keep those minimal. So you can very clearly see that those are safe, that there's no problem with them. And then the other side is for A, and I'll talk about that in a second, is that there are some issues around just networking and propagation of transactions. And so we focus on very restrictive, easy to reason about mempool rules in the beginning and then the idea is to then iteratively change those rules to add additional features and we'll go through them now in a second.
01:23:56.630 - 01:24:20.174, Speaker A: So first let's have a quick look at the protocol changes. So there are two main ones. The first one is the introduction of a new transaction type. People often ask with AA there is no such thing as an AA contract. So there are still only two kind of accounts. They are externally owned accounts, often called EOAS and contracts. This doesn't change.
01:24:20.174 - 01:24:52.246, Speaker A: What we basically do is we add new capabilities to the existing contract type. What we do is we introduce a new transaction type. What even is a transaction type. Until now, there's only one type of transaction in Ethereum, but there's already another EIP called 20 718 and that's typed transactions. So that introduces the option to have multiple different forms of transactions that will also be used. For example, with Yap one five F nine. If you know about this one, this is basically the mechanism that this uses.
01:24:52.246 - 01:25:28.806, Speaker A: And also account obstruction would use that and introduce a new transaction type for account obstruction specifically, and just to very briefly compare the two, like current transaction on Ethereum, I just added a little list. Basically, it's an encoding of the following fields. Basically, you have nons. Obviously you have gas price, gas limit, the recipient of the transaction, the value, the call data, and then three pieces of the signature of the transaction with AA actually, because a lot of that is now the responsibility of the contract. So the transaction itself becomes very simple. You only have nons target, which is the AA contract and data. That's all right.
01:25:28.806 - 01:26:11.410, Speaker A: The rest is then up to the contract. Specifically, the contract sets gas price and gas limit. Interesting to note for compatibility, because again, we are just using the existing contract framework. So AA transactions will technically come from a special entry point address. The only purpose of that is just to initiate a transactions. And then the rest like signature verification, whatever you want to do, you just do that with the call data that the transaction provides. And also interesting to note, obviously because you don't have these more extensive checks upfront, the base cost of the A transaction will be slightly lower than existing transactions.
01:26:11.410 - 01:26:34.486, Speaker A: So now, the second protocol change is two new opcodes. And the first one is a simple one. It's just a non opcode. Until now, you can't actually like during execution, you can't actually access the nons of a transaction. But that is required because you have to be able to sign over all fields of a transaction. And so yeah, that's more technical change. The other one is more interesting, pay Gas.
01:26:34.486 - 01:27:32.430, Speaker A: Sam already introduced that, right? Pay Gas basically is the new way that a contract can say let me pay for this transaction. Obviously, you can only call that if you are within an AA transaction. So if it's actually a transaction that expects to be paid for by a contract, and then the contract gives it just a gas limit and a gas price, and then it works just as normal transactions work today. Pay Gas also acts as a checkpoint, right? Because any changes, for example, the payment itself, but then also if you want to update your internal whatever nons or something, all of these, they're basically set in stone. As soon as you call PayGas, everything after can only revert back to PayGas. And then basically, this is like a kind of obvious one. An AA transaction is only valid if it has been paid for, right? So the contract that you call into at some point has to pay Pegasus, otherwise the transaction is not valid and can't be included.
01:27:32.430 - 01:28:27.050, Speaker A: The last one is just a quick note. The Pegasus Opcode is specified to be versions so we can have future versions, for example. So we can also specify ERP one who have flying compatible gas prices. So now to the mempo rules. The first one, I hope you came away with the impression that the actual protocol changes are very succinct and easy to reason about. Now the mempo rules and the question is why do we even need them? For normal transactions you always have very simple validity behavior, right? You check the signature once and if it's valid, then it can only ever be invalid if another transaction from the same account reduces the balance or increases the non or something, right? And that means you can always just reliably chain multiple transactions together and still be certain that they will all be compatible and you can just execute them one after the other. For AA transactions.
01:28:27.050 - 01:29:18.894, Speaker A: This is way more complex potentially if you don't do anything about it. And the reason is that if a contract accessed any stage before it made the decision, if it wants to pay, right? And this state changes, then you just don't know what the validity of the transaction is. It might be that even with the new state the contract still wants to pay, but it might be that the new state will lead the contract to no longer be able to be willing to pay for the transaction. So anytime any state touched during execution changes, a pending transaction could potentially become invalid. And that is a problem for miners because miners now when they want to assemble a new block, they just don't know reliably that this is a pool of valid transactions, just pick one after the other. But basically they would have to revalidate and then discard. And this is an issue.
01:29:18.894 - 01:30:00.966, Speaker A: And as well, nodes in general are vulnerable to so called mempool wipes, where a mempool is just like your list of pending transactions. And it can like with AA, potentially. What could happen is that a new block comes in that changes some state, and all of a sudden, your whole mempool that only consisted of AA transactions is invalid. All of a sudden, all at once, right? And you really don't want that because that introduces quite a few inefficiencies. And thus obviously these are only problems if you do it like in an ETH way, if you don't have any protections. But the lesson here is, like miners and nodes need a protection mechanism against unpredictable transactions, they can change their validity whenever. And there are two restrictions that we introduced.
01:30:00.966 - 01:30:30.806, Speaker A: The first one is an Opcode restriction. It's a full slide, but I'll just glance over it because the idea is pretty simple. The idea is just you don't want AA transactions to access any external state, right? The idea is they should only rely like before they decide to pay. After they can do whatever as normal transactions will. But before they decide on payment, they are only supposed to look at their own internal state of the contract, not look at the outside world. And you just do that by having like a list of blocked opcodes. I won't go through all of them.
01:30:30.806 - 01:31:21.462, Speaker A: But it's like the obvious ones. You're not allowed to look at the balance of other accounts, not at their code, not at state of other whatever, right? You're just not allowed to look at external state. And this is not enforced by the protocol, right? You could still have a transaction that violates these rules, like in a block, that's fine. Where it is enforced is at the individual nodes. If you're running a node and it has a Mempool where it stores pending transactions, if the transaction violates this rule and accesses excellent state, you just don't keep the transaction around because it's unpredictable, it could change validity at any point, you just don't keep it around. And that way, you know for certain, like my transactions can only be invalidated if their own state changes. And this is the first part of the two part solution that is very important here.
01:31:21.462 - 01:31:51.518, Speaker A: And later on we'll talk about how you can then loosen these restrictions safely. But let's first talk about the second part. Second part is what we call a bytecode prefix. So contract code, obviously that's just like a bunch of bytecode and that gets executed when the contract is called. And the idea is so what do we want to achieve here? We want to achieve that. We already achieved that you can only look at your own state for validity. But now what we want to do is we want to prevent other transactions from changing your state.
01:31:51.518 - 01:32:39.006, Speaker A: So it's basically only you can only rely on your state and only you can change that state. That is the goal. And so how do we prevent other transactions from touching the state? Well, guns were very simple, right? The prefix, the beginning of the contract code just has to have this simple prefix which basically just you just ensure that the transaction came directly from this entry point, which again, this is like this special address from where all AA transactions come. And if it doesn't, you're only accepting like ETH deposits. So you just log a little receipt message, I mean, and then you just immediately return. And that way every contract that has this prefix, you know for sure that its state can only change via AA transactions. It cannot be modified from the outside.
01:32:39.006 - 01:33:26.750, Speaker A: And this is really important because now if you just don't keep transactions to contracts that don't have this prefix and you also follow these restricted opcodes, then your Mempool is basically safe, right? Any AA transactions you have in your Mempool, they are safe. They can never misbehave they will always remain valid. If you are Miner, you can always just take one after the other to include it in your block. All of these rules that you need, all the invariants that you had with normal transactions, now also hold with AA transactions. And again extensibility you can add multiple known safe prefixes. And this is important because now what we want to talk about after we talk about these restrictive rules, now we want to talk about how can we relax them, what kind of additional features can we bring to AA safely.
01:33:27.730 - 01:34:12.138, Speaker D: So the EIP 29 38 that we've described so far is useful mostly for single tenant applications, applications that are used at most by one person. These extensions expand that to allow for more complex prefixes, more users using the apps. They take it from a very basic to a very full featured account abstraction implementation. So the first extension we want to talk about is delegate calls from AA. So this is say you have a library like Safe int or Safemath and you want to call that from an AA contract. You do that with delegate call and this is safe to do, but it's only safe if the call target exists. So we introduce a new opcode set indestructible, which obviously disables self destruct.
01:34:12.138 - 01:34:57.290, Speaker D: So this gives you libraries and upgradable smart contract wallets. Another really useful feature that we pretty much need is static calls into AA. So this gives you read only access to an AA contract. Now, this would always be safe because a read only call can't invalidate transactions, but we can't currently differentiate between static contexts and read write contexts. So we introduce new opcode is static, which remains true, returns true if the context is static. So this lets you read things out like exchange rates or smart contract wallet balances from AA contracts. This is probably the second most useful extension to AA, which is read write calls into an AA contract.
01:34:57.290 - 01:35:50.060, Speaker D: So the mempool wipe that Anskar described earlier is probably the biggest problem with read write calls into AA. A single transaction would be able to invalidate tons of AA transactions and wipe nodes mempools we want to prevent that or at least make it safer. And the way you do that is by preserving the amount of gas an attacker would have to spend to wipe the ratio of the amount of work that would have to be revalidated to the amount of gas spent on chain. And the way we do that is by introducing a commit gas opcode. This establishes a lower bound on the amount of gas the AA contract will consume and it'll be called by the AA contract when it's invoked by a non AA transaction. So this enables deposits ERC, two, two, three tokens and surprisingly allows an AA contract to sponsor another AA contract. And we'll get into that a little bit more later.
01:35:50.060 - 01:36:43.386, Speaker D: This is probably the most important extension to 29 38, which is multiple pending transactions. So this lets you go from a single tenant application like a smart contract wallet to a multitenant application like a mixer or an exchange. This lets an AA contract propagate more than one pending transaction. In normal operation the nonce is chosen by the person who creates the transaction. With this extension the miner chooses the nonce for the transaction immediately before EVM execution. That lets the AA contract choose its own replay protection and basically it can ignore the protocol nons. The downside to this is that every time an AA contract state changes, all of the pending transactions for the contract have to be revalidated.
01:36:43.386 - 01:37:15.442, Speaker D: This revalidation effort is bounded though on a per transaction gas limit and a per contract cumulative gas limit. And again, this enables very efficient multitenant applications. So there's an interesting property about validation. Some parts of validation are pure, meaning that they can never be invalidated. Normal ethereum transaction signatures are a great example of this. So are ZK proofs. Once these pure validation portions are shown to be valid, they can never be invalidated.
01:37:15.442 - 01:38:05.986, Speaker D: If we can cache the results of these validation steps, we can enable significantly higher validation limits since you don't have to repeat the validation every time. The way we enable this is by adding a new prefix where the AA contract calls into itself and that creates a cache point where the EVM can intercept this call and cache the result of the pure validation. Now, to make this a little bit more efficient or elegant, we can introduce two new opcodes pure call and Is Pure, which detect perhaps obviously if the call is pure or not. Or we can just rely on mempool rules. We don't need a consensus change for this. And what this gives us is a lot more gas for ZK proofs. Building on top of that, we can actually increase gas limits even further with this dynamic validation gas limit.
01:38:05.986 - 01:38:54.070, Speaker D: So well behaved contracts can commit to a minimum gas spend and if they do that we can increase the amount of validation gas limit. The limit can be increased safely because we maintain the same ratio of on chain gas to revalidation work if an attacker were to try to wipe mem pools. And this gives us even more gas for ZK proofs and even more importantly, more pending transactions per contract. We've also been working on a sponsored transactions EIP that is complementary to AA. And sponsored transactions is where a sponsor pays the gas for another user. Unfortunately, EIP 2711 is incompatible with AA. So we're just introducing a pre compile that sets message sender based on an ECDSA signature.
01:38:54.070 - 01:38:58.360, Speaker D: This lets you pay for gas and tokens, which is pretty exciting I think.
01:38:59.450 - 01:39:21.310, Speaker A: So now we have like a summary slide. Unfortunately we are running out of time so I think we should just skip it and directly go to the next one. But if you're interested vitalik recently gave a really great talk about account abstraction at a meetup and there's a YouTube link at the end of our slides where you can go and listen to him. I definitely recommend that and he talks about that extensively.
01:39:23.010 - 01:39:55.260, Speaker D: So just a quick summary of where we are right now in the EIP. So EIP 29 38 is a draft in the EIP's repository. I recommend checking it out. There's a link at the end. We've had some light discussion on the all core devs meetings. We want to move it to Considered for Inclusion soon, where it would join the pool of pending features that can be added to hard forks. We, as in Quilt are aiming to get this into the hard fork after Berlin, which is the next hard fork, but we're really not sure if this will ever arrive on Mainnet, but we're really hopeful it will and we hope people find it useful enough to include it.
01:39:55.790 - 01:41:05.540, Speaker A: And just as a last tie into the very first day of these talks, if you remember, Vitalik talked extensively about the roll up vision, the roll up centric vision for e two, where basically e two becomes more and more of like a layer two management layer. And most of the scalability actually comes from execution on these layer two solutions. And so basically one question that we think is relevant not only for account abstraction but for features in general is like, do you want to keep extending the base chain with all of these additional features or is it a better fit to just bring them to the rollups directly? And we are very open to the possibility that AA will not ever make it to Ethereum, Mainet because it's just not a priority. Although at the same point, there are some synergies that you would also get regarding layer two management with account obstruction. And so we are still confident it will provide added value. But this is definitely like a conversation that I think will be more and more important going forward. So we wanted to mention it, but with that, yeah, we want to hear from you.
01:41:06.950 - 01:41:27.560, Speaker D: Does account abstraction make your DAP use case easier? Could it? If it does, what extensions do you need the most? Is there anything else you could use, and do you think this is something that you would love to have in Mainnet or do you hate it? Either way, please contact us on the Ethernet discord links here and also at the end.
01:41:28.270 - 01:41:28.682, Speaker A: Yeah.
01:41:28.736 - 01:41:33.900, Speaker D: So thank you very much and I think we have a few minutes for questions, but I'm not sure.
01:41:34.750 - 01:42:03.380, Speaker C: Wow, that was a very extensive overview. Thanks so much. We do have some questions and there was also a lot of discussion already going on during your talk in the chat, so also feel free to check that out later. But I want to relay at least one or two questions to you guys now. The first one being around backwards compatibility, which parts of account abstraction are backwards compatible and which ones are not?
01:42:04.730 - 01:42:44.980, Speaker A: So I think what we tried is to basically have the changes be as minimal as possible and that includes having backwards compatibility. And so basically the idea is just existing contracts today because this new opcode, this Pegasus Opcode, is not yet in existence. And so existing contracts just don't use it. And so your existing contract will not be able to ever upgrade to an AA contract. But what you can do is existing libraries, as we said, can still be used. And so there would definitely have be the need for a one time migration to AA contracts if you want to use them, but there's nothing that would break or anything.
01:42:46.550 - 01:42:56.630, Speaker C: Okay. And then another question. Would ecosystem infrastructure need to be updated or integrate some special considerations to benefit from account abstraction?
01:42:57.610 - 01:43:31.520, Speaker D: I would say yes. Wallets are definitely going to have to change to support the new transaction type. You're going to have to introduce like Block Explorers are going to have to handle the new transaction type, and they're also going to have to handle the UI of not exactly having a two and having a target. Instead, transaction hash uniqueness is another big one. So during EIP 29 38, we do maintain transaction hash uniqueness all the way through. So that one's a very minimal change. But in some of the extensions, transaction hash uniqueness changes a little bit and that'll require some work in the infrastructure as well.
01:43:32.630 - 01:43:39.090, Speaker C: Okay, and then last but not least, what is the difference between this and the gas station network?
01:43:43.990 - 01:43:45.010, Speaker A: Go ahead, sir.
01:43:45.160 - 01:44:01.370, Speaker D: I was going to say, so this is very complementary to the gas station network. Instead of having external nodes relaying these transactions, you could have a gas station network contract on chain that handles some of the work for paying for these relay transactions.
01:44:03.070 - 01:44:40.994, Speaker C: Okay, amazing. Yeah, please do check out the chat because there's been a lot of discussion during the presentation, but now due to time, I will not relay all of the remaining questions and move over to our next speaker. Thanks so much, Sam. And Anska next up will be Alex. So my teammate from Solidity, Alex, is actually co leading the Solidity team and he's also leading the eWASM team, but this talk is going to focus on Solidity. Alex is going to present the future of Solidity, which will entail a feature preview for the upcoming 0.8 release.
01:44:40.994 - 01:44:44.920, Speaker C: But I guess also a bit more than that. Hi Alex. Welcome.
01:44:49.150 - 01:44:58.890, Speaker A: Heather. I'm just trying to share, is it shared?
01:44:59.310 - 01:45:00.940, Speaker C: Yes, we can see it.
01:45:11.890 - 01:45:16.580, Speaker A: Just a bit confused with Zoom. Can you also hear me?
01:45:17.030 - 01:45:17.780, Speaker C: Yes.
01:45:19.910 - 01:45:54.240, Speaker A: Okay, now I can hear you as well. Thank you, Francis. Okay, let's get through this. So what I'm talking about today is a couple of different parts regarding Solidity. Initially the goal was just to talk about eight, but if time allows we'll see. I'm going to go a bit further than 0.8, but before that before that, I just wanted to highlight a couple of things.
01:45:54.240 - 01:46:55.310, Speaker A: We have launched this week a language portal which can be found on Soliditylang.org and it has all the relevant links to everything, including the blog, all the different forums, the releases and also some background detail. And I think all of the links I have in the slides can be found through the portal. The portal is still in development and it's just the first version and we hope to make it much more packed with features and content. But that's just going to take time. The second thing I wanted to mention is this underhanded Solidity coding contest, which is a fun little challenge to see if you can hide functionality and the source code. Now, regarding the agenda of today, here's another link which also can be found on the portal.
01:46:55.310 - 01:47:45.906, Speaker A: It goes through the past five years of Solidity hadling, which has changed with a couple of interesting tidbits, but we're not going to talk about that today. I'm going to have three parts, but depends on how we get on with time first. The short term part is I just wanted to make clear how the release process works in the team. I know we have mentioned this many times already, but it never hurts to mention it once more. So we try to make really frequent releases, two, three weeks. I know sometimes we haven't really gotten to do that and maybe it took like six weeks and in case of bugs, we do make quicker releases. But I think we are fairly consistent at two, three releases releases every two three weeks.
01:47:45.906 - 01:48:23.406, Speaker A: And we also try to make breaking releases twice a year. We haven't really done this so far, but maybe this year going to be the first year we actually achieved it. So the last breaking release was zero seven in July and we hope to have one 0.8 by end of this year. And for 0.8 I've collected four interesting features which seem to be becoming part of 0.8. I think some parts may change slightly, but I think mostly what I have in these slides going to be what's included in eight.
01:48:23.406 - 01:49:11.420, Speaker A: Regarding those, there might be some other smaller or different changes I haven't listed, but this just isn't time to go through everything. So the first interesting one, I'm sure everybody is waiting for this forever by now. Let's start with a simple example here. So what happens in this function? If he passed in minus one and one, the function would return one because it wraps around and why doesn't it just revert? We really shouldn't wrap around, one could say, and there's a very simple reason to that because Solidity is just exposing what the EVM is doing. We are not trying to add extra code because that costs gas and people don't really like to spend extra gas. If. They don't need to.
01:49:11.420 - 01:49:41.670, Speaker A: But in this particular case it seems that actually people are willing to spend gas because they don't really want this to happen. And that is signaled by the safe math libraries. Or the safe math library. Well, actually probably more of them, which in practice everybody uses. So the good news is that you may not need to use it anymore in eight. So in 0.8 we are introducing what's called checked arithmetic by default.
01:49:41.670 - 01:50:33.640, Speaker A: So this piece of code here would cause a revert in the previous example. And this is exactly what SafeMap is doing. But what if you actually want the overflow the wrapping around behavior? We actually got you covered because we have this extra feature here called unchecked. And any piece of code which is wrapped in the unchecked block won't have the overflow or underflow checking generated. So those will behave as they were in 0.7. I haven't really gone through everything here. There are various different edge cases, so please check the documentation because it's much more than these two slides could cover.
01:50:33.640 - 01:51:32.380, Speaker A: But there's another interesting piece here regarding type convergence. Actually a few weeks ago I tweeted about what do you think is address minus one valid? And initially it was a poll. So initially the response to that was what I hoped I'm going to get is that it's not valid. But then people actually checked the documentation or remix and turned out, yes, it is actually code which compiles. So the poll ended up saying it's valid code. The reason we found this, because it's not about finding, but the reason it came up as a discussion is because we were heavily working on the Smt checker functionality and we wanted to properly implement different type conversions. And actually all these little details on this slide and the next slide is the outcome of that review process.
01:51:32.380 - 01:52:23.830, Speaker A: And we found that this is rather confusing. So I collected a couple of different strange conversion scenarios here. Unfortunately we are not really interactive, but I gave you 2 seconds to think what each of these going to result in. Okay, so this is what actually happens. I mean, some of them you could understand. It could be set to be intuitive, but I find it rather confusing that we have so many different rules and in many cases we restrict everything and then we have this type conversion mess. Luckily we won't have this type conversion mess anymore.
01:52:23.830 - 01:53:28.940, Speaker A: The really interesting part down here is the bottom tree cases. So depending on just basically this very last line here, the outcome would depend on the implementation. And by implementation I mean whether first the sign conversion is taking place on this line or whether the size conversion is taking place first, the outcomes would be different. This is of course not a problem in solidarity because the order precedence hasn't changed, neither it did in the new code generator compared to the old one. So nobody has really seen this as an issue, but as we are having more compilers for Solidity and more people looking into Solidity, if this is not well specified then this could become a problem. But there just doesn't seem to be any reason to support this functionality. So we actually just removed it.
01:53:28.940 - 01:54:23.210, Speaker A: So all of these weird cases here, with the exception of this valid address, all of these are becoming invalid. So basically the literal has to be the literal is this here. So it has to be less or equal to the bitwidth of the type and the signs must match the type. And by this I mean that if it's an unsigned type it's not possible to assign a sign number to it. It's only possible to assign it to sign numbers we do have. One exception is address zero because that's really useful and it's used very frequently. Another I think extremely important piece is panic codes and you will see that this is actually connected quite a bit to the stuff previously discussed.
01:54:23.210 - 01:55:42.440, Speaker A: So currently there are two different ways where code can abort and the one case is where the code is going to have an invalid upcode and that means that all gas is going to be consumed. In the second case is where we have a revert upcode and of course this only applies after Constantinople or Byzantium. Anyway, this has been introduced a couple of years ago but it hasn't been there initially. So basically there are two ways to abort execution with failure. One case is with the invalid of code when all gas is consumed and the second case is the revert of code where the remaining gas is kept. There's another main difference between these two is that with revert it is possible to send a message to the caller and that message could explain why did we abort. Now this is what we have today before eight, in many cases invalid opcode would be used internally and this is changing now just a bit more explanation what reverts are.
01:55:42.440 - 01:56:46.230, Speaker A: So this case, like with the require statement, which I'm sure everybody is aware of, that of course uses the revert of code. And actually what happens here is that we have an encoding for this message. We're not just putting this message into the revert message, rather we encapsulate it and we encapsulate it with the abi encoding and practically it just looks like a function call. So the error function which has a string and then we just encode the message. So one could say that this expression here, this statement, the required statement would equal to this one. So it's just a weird way to encode the same thing. So we have these error messages and now we're introducing a second case, the panic messages.
01:56:46.230 - 01:57:36.700, Speaker A: And a panic message is basically any kind of internal error is emitted as a panic message. So the main benefit we get here is that we are not going to consume all the gas. And second, we are able to deduce from the outside. Why did the execution stopped? I didn't list here all the different panic codes and I think it's still possible to argue whether the code should be different because the release hadn't really come out yet. But we do have, I think I'm missing three or four more because there just wasn't enough space on the slide. It's however, documented in the breaking branch. So it's possible to still discuss these error codes, but this is roughly what they mean.
01:57:36.700 - 01:58:07.570, Speaker A: There is a specific error code for manually calling a Cert. There is a specific error code for under or overflow. Obviously that's outside of the unchecked blocks. There is a specific one for division or modulo by zero. There's specific one for out of bounds access and type conversion. There's also for trying to allocate too much memory, et cetera. There are a bunch of different error codes.
01:58:07.570 - 01:58:57.118, Speaker A: I actually invite all of you guys to check the documentation and maybe give some feedback because it would be bad to hard code these. And then next release people wanted to change them. And the fourth big change I think is the API encoder V Two. What changes related to it? Now, maybe I should just give a first explanation what the hell Abi encoder V two is. I'm sure many of you do know what it is, but maybe some others don't. So it has been introduced in 2017, so quite a while ago. And the Abi encoder is this piece of code which generates the code to decode and encode all the inputs and outputs from a contract.
01:58:57.118 - 01:59:50.290, Speaker A: So when somebody is sending data to contract, it's encoded in this Abi data structure and that needs to be decoded and any response has to be encoded. And the compiler has two implementations of this code generator. The one is I guess Abi encoder V One and the other one is Abi encoder V two. The main difference from the old one is that V Two generates Yule code and the V one just generates EVM bytecode. So it's kind of hard to work with. So the Yule code is much better maintainable and it also supports more types. And probably that's the reason some of you have used it because it does support various features from Structs which are not supported by the old API encoder.
01:59:50.290 - 02:01:14.910, Speaker A: But some of you may have also noticed that this V Two is a bit more strict than the old one and in some cases it may consume more gas. And this gas question may have been more relevant, I guess, early on when this was introduced, but especially last year we have worked by last year, I mean 2019, and earlier this year we have worked quite a bit on the optimization capabilities for you. So I think actually the gas costs are not all that much off compared to the old one, but it is way more strict. And because this was a second encoder, it had to be enabled by choice. So we had this experimental Pragma for AB encoder v two, and for a long while this has to be added in the source code, in every single source code you want to use it. And if a given source code doesn't have it and you mix these together, that is also working. And then for a long while, whenever anyone specified this experimental Pragma, the metadata also had an experimental flag to signal that this may not be production ready software.
02:01:14.910 - 02:02:18.740, Speaker A: However, we have removed this experimental flag from the metadata last year with the six release, but we still kept the experimental Pragma. And now with Eight we are changing this. It is going to be enabled by default, but for those who still want to use the old one, which I don't see any reason to, but if you still want to use the old one, you can do that. We are introducing a new Pragma here called AB encoder and it has two options v one and v two. So if you want to keep using the old version, you have to introduce this extra line. You can also explicitly use this new one, but it is enabled by default and to make life a tiny bit easier, probably we are going to keep supporting the experimental Pragma, but it's going to emit a warning. Now that is all about the zero eight I wanted to highlight, but there's of course a lot more.
02:02:18.740 - 02:03:02.846, Speaker A: We have a project in GitHub I haven't linked here, but I guess I can show it through the Q and A if you have time. And that project we use to rank and sort different proposals and discuss them and that project is just basically working. Of these labels. What I'm trying to show here is that we have two important labels we work with the language design and the breaking change. And you can see that there are 126 open language design issues. So that's quite a bit of changes. And that means of course, these four changes I mentioned are not the only ones which we are discussing.
02:03:02.846 - 02:03:58.100, Speaker A: Obviously it's not possible to discuss all the 126 changes every week, but we do pick from these issues. And here I picked actually four different issues I wanted to highlight, which are important, at least in my opinion, in the medium term. So whatever I said regarding 0.8 is probably what we agreed on as a team. But this part of the talk is more like an opinion piece, it's not like a final decision from the team and I'm not trying to make it look like so. But I think these four issues I'm going to briefly talk about are really interesting and important for the future. I'm sure you have heard about this solidity to Yule code generator for a long while because we have been saying this for years that yeah, we're going to have this Yule code generator done.
02:03:58.100 - 02:04:42.530, Speaker A: And yeah, actually we are quite close to that. So we have a project board, a separate project board, not the one I mentioned, just for this solidity to Yule code generator. And this project board is becoming rather empty. So it would have like five different columns from Icebox to in progress to in review to done. And we only have a handful of issues in the Ice box and over 50 issues in the done, but unfortunately it's still not 100% complete. We have two major things to finish. One is support for libraries and some cases of copying between memory and storage.
02:04:42.530 - 02:06:01.210, Speaker A: However, we at the same time trying to maybe consider moving off libraries. So I'm not sure what's going to happen with this here. The main goal of having this sole to Yule conversion is to have better maintainability, and also because the Abi encoder v two generates Yule, it provides a much better integration. So one could imagine that if we are using this sold to Yule and Abi encoder v two, then we have the entire contract generated in Yule and we can apply optimizations on Yule itself, then we can translate that Yule code to EVM and then we can also apply optimizations on the EVM bytecode. Compared to that today it's a bit different because if you compile directly to EVM in the compiler and then use the API encoder v two, what that means is one needs to compile the API encoder v two output to EVM and then consider the EVM code. So by moving to this intermediate step, we're going to have much better possibilities for optimization. And it also going to finally give some solution to the stack 2D pair.
02:06:01.210 - 02:06:53.322, Speaker A: There is actually a piece of code implemented called the memory escalator, which can move variables from the stack into memory and back and forth. And that is implemented on Yule. So hopefully that is also going to be fixed by this. I don't really give any deadline when this is going to happen, but yeah, we're really hoping to get this down fairly quickly. So this stall to your project, this feature can already compile a bunch of contracts. It can compile the uniswap factory as well, I believe, and it can also compile the e two deposit contract and it can almost compile diagnosis safe and the multisig wallet. There's only just one feature missing in each another, I think.
02:06:53.322 - 02:07:38.170, Speaker A: Rather interesting functionality we are discussing. And here's the link for the discussion is enums not with metadata. Yeah, enums with data or algebraic data types. So here's a small example. As you can see, we are using the enum syntax and it is possible to have various oh, why, why is the okay, yeah, there should be a semicolon, there a comma there. Anyway, this is the current syntax. This is not implemented, this is only under discussion.
02:07:38.170 - 02:08:43.758, Speaker A: I think what I wanted to actually highlight here. So you should rather just go to the issue because it's a lot to discuss. But this is a simple example and what can be I guess seen in the next slide is that we don't have a syntax for matching yet. So basically one would need to have a long if else chain to evaluate all the different types and the current conversation is really just focusing on how to encode this in the abi and the storage. And I guess once we have that done then we could talk about the matching but anybody is invited to come up with any suggestions and especially I would be interested to see if anybody is really interested in this functionality and how would you use it. Another really highly debated topic is the templates. I think this issue was created in 2016, maybe even 2015.
02:08:43.758 - 02:09:46.450, Speaker A: So it exists for a long time and it has been discussed for a long while. On the issue. Actually there are a couple of links to different GitHub, gifs or Gists which have a variety of examples and I'm not going to go into too deep into that, but here's a simple example from how to do typecasting. It's kind of like a weird example but it shows the syntax. I mean it's nothing really strange or unfamiliar, it's kind of similar to C plus plus and Rust. What we are more, I guess busy with regarding templates is writing more examples to find potential problems and by doing so we also figured that we would need a lot of different introspection helpers for types and this is somewhat related. Standard library is something we also this is like the fourth topic we also have heavily discussed.
02:09:46.450 - 02:10:57.674, Speaker A: I want to make sure everybody understands we're not trying to compete with Open Zeppelin. So it's not a standard library for ERCs and higher level protocols and functionality, it's rather a standard library for moving more features from outside of the compiler code base and to also maybe decouple the EVM specific behavior a bit more from the compiler. I gave here two different examples. So one thing what could happen with having a standard library is removing the need for global functions we have. Now this here, it doesn't really say anything more where this standard library lives. It just means that by default in the namespace there wouldn't be an EC recover as it is today, but rather it has to be explicitly imported. And one could imagine that maybe this standard library would be still part of the compiler and all this means is just cleaning up the namespace, which is I think already a useful step to make.
02:10:57.674 - 02:12:34.366, Speaker A: But we could also implement some of these helpers in solidity itself. This is an actual implementation for EC recover, even if it doesn't look nice. But it is possible to move functionality out of the compiler and I think this is going to be some of the really interesting steps to do. And some of these examples I gave prior, like the templates and algebraic data types, these actually play quite well into the standard library discussion. And now we got to the very end is what the long term going to be like for Solidity? So this just has a bunch of different questions and I feel I have some answers to them, but I don't think we have enough time for that. But one question is will we have different targets or different dialects of the language? Will we support roll ups, different roll up systems in the compiler? Will we support some kind of sharding or will we support eWASM? Just a brief answer is what I discussed about this Sol Yule stage is that Yule is an intermediate language which not only can be converted into EVM, but it can be converted into other bytecodes as well. So internally in the compiler we have a piece which converts it into eWASM or just WebAssembly, but we also had a discussion with the OVM team and it would be possible to translate it into OVM bytecode.
02:12:34.366 - 02:13:03.250, Speaker A: However, that hasn't been implemented. And the importance of dialects or targets would be to have the capability to disable features in the source code which would be specific to a different specific target. Yeah, I'm not sure if we have time maybe for one question, but I think I filled out the 30 minutes, so yeah, that's what's I guess next for Solidity.
02:13:04.710 - 02:13:32.780, Speaker C: Great. Thank you, Alex. Yeah, we are perfectly on time, so we don't have that much time left for questions, but I want to ask one question before we wrap it up. So basically all of this was more or less about the future language design of Solidity. Are you currently happy about the setup and process, how it's working? Or if you could make a wish, who would you wish to have participate in the language design more?
02:13:35.390 - 02:14:15.674, Speaker A: Everyone. Yeah, I think definitely the participants we have today is mostly auditors and some people who have been around in Ethereum quite a long time, and maybe those people who have been around since 2016 when the community was tiny, they don't really feel anxious to reach out to projects because they feel like they're on the same level. And I would encourage everyone that we are not like some weirdos hidden somewhere. We are just like you. So we'd like to talk to you whether you start your first hackathon or whether you have deployed your DeFi project, we want feedback from all of you.
02:14:15.712 - 02:14:48.290, Speaker C: Guys that are nice closing words. Thanks so much, Alex. And with this talk, I am also saying goodbye. Not only Alex is saying goodbye, but I'm also saying goodbye. It's time for me to say goodbye and hand over to our next co host of Today, which is going to be Emily from the Ecosystem Support Program. Thanks so much for having me, everyone, and I wish you a very fun and nice second half of the summit. Thank you.
02:14:48.290 - 02:15:26.080, Speaker C: Thank you for the introduction and thank you, Alex. All right, taking over for the second half of our presentation, I just want to remind everyone, if you want to drop in comments, questions for our speakers, please head to Live Ethonline.org. We have a chat going there if you want to get involved. Our next speaker I would like to introduce is Paul Dorzanski. He is having some slight technical difficulties, so he won't have camera on. But Paul will be giving a talk titled EVM three, four, eight, doing Fast, Crypto and EVM. So Paul, take it away when you're ready.
02:15:29.010 - 02:15:48.370, Speaker A: I hope you can hear me. I'm in the process of sharing slides right now. I hope you can see the slides. Can someone confirm that you can hear me and see the slides?
02:15:48.450 - 02:15:50.150, Speaker C: You are all good to go, Paul.
02:15:50.490 - 02:16:22.946, Speaker A: Thank you. So today the topic is EVM 384 for this talk. It's a project and the title is also Doing Fast Crypto in EVM. I should explain the title, what I mean by crypto. So there are three sort of interesting things. The word fast, the word crypto and know doing in EVM. What I mean by crypto is not all crypto, just a certain class of crypto that's in demand right now.
02:16:22.946 - 02:17:05.790, Speaker A: Don't worry, this talk isn't crypto heavy. By fast I mean pretty fast, but not speed record fast, but still pretty fast. Fast enough for our purposes. And by saying in EVM, I mean user deployed, user written in EVM. So that's the topic. A lot of work went into this from Alex, Casey, I'm Paul and Jared and the rest of the eWASM team was very helpful as well. So the goals, this is an engineering project, so we should have some sort of goals and scope.
02:17:05.790 - 02:18:04.002, Speaker A: So I have a goal slide. We want user deployed crypto. Like I said, we want EVM to be generic enough to support any crypto for the purposes of this talk. And this isn't very generic, but there's a certain class of crypto that we're targeting for this talk in this project and that's any crypto with a bottleneck of modular 384 bit or less arithmetic. And for example, BLS twelve 381 operations are very popular now for F two especially, and other projects as well. But not only, there are other things and we want our project, our EVM 384 project to support any crypto, any crypto. This is a strong statement with this bottleneck, not some specific use case, any, it's generic, so I'm putting emphasis on the genericity and we want it to be fast.
02:18:04.002 - 02:18:44.586, Speaker A: What I mean by fast, I said not speed record fast, but very fast. So approaching speed records, there are optimizations that we want users to deploy it. We're in this EVM role, so we accept it. There is some tension between how generic we are and how fast we are. So we want it to still be generic, but we want it to be very fast, more than fast enough for use cases and the generic. And then there's a whole interaction of the speed and why if we're generic, then users can optimize for speed. And as optimizations come along, people can implement the new optimizations and the new crypto systems.
02:18:44.586 - 02:19:46.754, Speaker A: So there's some interplay with this as well. So who's to say today it might be a little slower, but as the optimizations come in, it will improve with time. And the whole idea is we want to have a renaissance of permissionless crypto innovation. And we know with what we have, with permissionless innovation, with things like DeFi and earlier CryptoKitties, things like this. And if we allow users to deploy it themselves to design the whole crypto system themselves, then we're hoping that there will be a whole renaissance and a whole blossoming of this whole ecosystem just based on adding this sort of EVM 384 project. So as engineers, there are trends that we have to be aware of, we want to design, looking forward to the future, we don't want to be obsolete. The big trends in cryptography hardware and blockchain, this is very loose, this is a very high level.
02:19:46.754 - 02:20:18.940, Speaker A: I'm not being very specific about anything and maybe some of these things are controversial. But one interesting thing I want to emphasize on this is that cryptography, hardware and blockchain all interact with each other. They drive each. So I'll explain that as I go along. The first bullet point, fast crypto is designed with hardware in mind. There are examples, I'll give an example. The Blake hash function was based on some cryptography, some stream cipher that was designed with hardware in mind.
02:20:18.940 - 02:20:45.826, Speaker A: The author of it said I designed this with hardware in mind. And that's why it was fast, because they knew that they knew how the hardware works. They knew if they design it like this, it will be fast and if it's fast, it's used slow. Crypto is unused. Fast crypto is good for everybody because people use it. And the most popular crypto in the world, the most used, I mean crypto in the world is the fast crypto. And of course there's some mathematical beauty to cryptography.
02:20:45.826 - 02:21:26.530, Speaker A: We can do this, this is possible. But it's so slow that nobody uses it. Maybe they will in the future when computers get faster. Another trend, the second bullet point is there are breakthroughs in zero knowledge. Snarks and Starks and others to point out a few, but we are in some sort of renaissance as well in zero knowledge and cryptography in general. And so there are interactions with these breakthroughs and with hardware as well in the third bullet point that is making these breakthroughs possible because we can support hardware improvements, allow more expensive crypto. 30 years ago it would be too slow.
02:21:26.530 - 02:22:10.240, Speaker A: Ten years ago even it would be too slow. But now it's usable because hardware is fast enough and looking forward. As engineers, we don't want to be obsolete, so we want it to be reasonably generic. And another one is there's a growing need for cryptography on chain. ZK roll ups is one example, but there are others. So these are all trends and the ethereum client complexity is growing. So this might be controversial to say diversity is shrinking, and some people have gone as far as to say stop, freeze all proposals until we fix the diversity problem.
02:22:10.240 - 02:22:53.546, Speaker A: They say that the reason diversity is shrinking is because client complexity is increasing and it's getting more and more difficult to implement the clients. So there are interactions with crypto and the demands and there's a tension between the demand of new crypto and ethereum stability. The diversity is important for the stability and the robustness of the system. If one client goes down, it should be okay because there are other clients. But now there's a major tension and we have to be forward looking as well. New crypto systems, it seems like they're coming out every year or two. There are breakthroughs and we anticipate more in hardware.
02:22:53.546 - 02:23:42.560, Speaker A: There are coprocessors FPGAs quantum computers. Recently in the news, Was AMD buying Is in discussions to buy Z Links, which is an FPGA producer, as you know, AMD, intel already owns Altera, which is another FPGA producer. And maybe they want to have and this is already possible. This is know in computing we see GPUs as co processors for high performance computing or for machine learning. So there's a trend in this direction and that we're aware of. And of course quantum computers we have to mention for cryptography they're very relevant, but they're not here yet, at least at any useful calculations. And also WASM plays a story as well.
02:23:42.560 - 02:24:40.266, Speaker A: It gives us nice guarantees and we'll talk about WASM soon. So at this point I want to give a history as well. We want to be forward looking with trends, but we also want to be backwards looking with history. There are lessons learned along the way and we wrote a nice history in our first EVM 384 post in our first EVM three D Four update, which is linked on this slide. But I just want to mention the high points and there are more details, there are more stories to be told that are very interesting, but I just want to touch on a few things that are especially relevant. Some high points, some historical high points, and some lessons learned. 2015 early Pre Compiles for EC Recover Chat Two Six Ripe MD ID so this was the mechanism the pre compile was a mechanism for doing crypto that users can call on chain with their contracts.
02:24:40.266 - 02:25:06.406, Speaker A: And you can talk about how they first came out in 2014 in a blog post by Vitalik and how EVM came about as well. We talk about that and something interesting happened in 2016. Sha one pre compile. Was proposed. And then someone said, well, do we need it? Is it possible to implement it in pure EVM? By in pure EVM I mean user deployed. Just EVM. Opcodes contract.
02:25:06.406 - 02:25:29.258, Speaker A: And in the contract code it does this Shawan calculation. It implements the Shawan function, in other words. And it turns out, surprisingly, yes, we can do it in pure EVM. So this is a success story. We don't need precompiles for everything. We can do some things in pure EVM. It's not speed record, but it's fast.
02:25:29.258 - 02:26:13.806, Speaker A: It's reasonably fast. It's more than fast enough for our purposes. As the years went on, we added more pre compiles ECAD EC mall EC pairing modi XP. These each have stories behind them, which you can read about. Modi XP has an interesting story that started with a proposal for RSA that got broken into five pre compiles for Primitives for RSA, and then it ended up that only one was especially needed, which is Mad EXP. The Ecidc Moec pairing is an interesting story about how this pairing, this BN 128 curve operations were needed and there was demand for it. And in 2019 something interesting happened.
02:26:13.806 - 02:26:42.002, Speaker A: Remember in 2017 EC Mall was needed because it's too slow to do on EVM. But that was challenged in 2019 with Wire strudel. It implemented fast EC mall in pure EVM. By fast I mean it competed. We're loose. I'm not saying what fast is by gas cost. There's two questions is gas and speed.
02:26:42.002 - 02:27:15.214, Speaker A: But Wireshooter was I mean by fast, but Wireshooter was optimized for gas use and it was just about equal to the ECMO precompile. This is shocking. How can this be? We needed the precompile because it's not possible to do an EVM. But no, that's wrong. We could do it in EVM, pure EVM, and there's reasons for it, if you understand the calculation that EC Mall does. But it's sort of an interesting story. And also in 2019, EV one EVM one came out, which is a fast EV one EVM implementation.
02:27:15.214 - 02:27:40.230, Speaker A: Pavo showed us that we can do EVM fast, not quite as fast as WASM, but he closed the gap by a lot. And the gap is small now. Smaller. Much smaller. So this is some historical high points and some lessons learned. Pre compiles are controversial. There's a lot of pushback, there's a lot of discussion, there's a lot of discussions back and forth with client developers.
02:27:40.230 - 02:28:39.326, Speaker A: And there are some other pre compiles as well that had more controversy. And one fun question to ask if you're an engineer, do we even need pre compiles? Can we deprecate precompiles? I know it's a sort of aggressive and maybe dramatic question to ask, but maybe it's fun to think about what would a world look like where we can deprecate them or where we can have user deployed? Perhaps that's what we might replace it with. So that brings us to and along the way, the WASM was one of the options in the hopes and I'll talk about WASM benchmarking. Now I don't have the URL GitHub.com eWASM benchmarking and I'll explain the chart as I'm explaining the text. So the chart really quickly. All the way on the right is BLS twelve pairing check.
02:28:39.326 - 02:29:11.750, Speaker A: So it's evaluating a pairing equation in just an interpreter. You can see how the dependent access is time in seconds. I don't know if people can see if the resolution is too small, but it looks like it's half a second in the interpreter. All the way on the right you can see it's 469 milliseconds. And all the way on the left is the rust native, 5.1 milliseconds. So you can see there's a huge like 100 x slowdown for interpreters.
02:29:11.750 - 02:29:54.626, Speaker A: So interpreters are too slow, no surprise. But then we discovered that all the way on the right, but look, just left of that, I'm sorry, the third and fourth. So just left of this interpreter are compilers, and those are fast, but they're still six x slowdowns from the native. So you can't be lazy. Compilers won't solve all of the problems for you. So we have to dig deeper and understand what the problem is. We profiled the BL twelve 381, and this, by the way, I should say it Was from WASM Curves.
02:29:54.626 - 02:30:38.654, Speaker A: That's the project that generates the code that we were benchmarking in WASM. I should say WASM Why are we talking about WASM right now? It's relevant to EVM because it sort of motivates EVM because we have tools and we have implementations in WASM and we can use WASM as an approximation to EVM. And it's important to the whole story, I think. So we profiled this Bos twelve pairing check, and we found that the bottleneck is in something called Montgomery modular multiplication. It turns out this is very popular to use this Montgomery modular multiplication in many areas of crypto. A lot of crypto uses it. There are other ways to do modular multiplication.
02:30:38.654 - 02:31:04.506, Speaker A: In certain cases there are optimizations, but this is a very popular one, maybe the most popular one. And so we did an experiment and the second one we call interpreter with big int native function. So not all the way on the left, not the rest native, but the interpreter with big NUMS. We did it in 9.8 milliseconds versus about five. So it's a two x slowdown. We can do better, we think.
02:31:04.506 - 02:31:27.278, Speaker A: But already we see that all the way to the right. It doesn't make sense. Compilers? I don't know, not quite. But if we do some extra work, we can get performance that might approach native. And there's more to this story. These numbers can be improved for the native as well as for the interpreter with BigAnt. But this tells the story about EVM.
02:31:27.278 - 02:32:14.818, Speaker A: One natural question is can we do the same thing in EVM? So that's where EVM 384 came from. One more comment about WASM. WASM is sort of perpetually, almost done. There are features that we're waiting for and the MVP is already w three C put out the MVP. But it's sort of very basic and there are very important features that are not finished yet. And so WASM I don't think we should deploy it yet because we're waiting for for example, interface types and reference types and some other proposals to be finalized that will affect the design of eWASM if it is chosen. But anyway, let's solve the problem.
02:32:14.818 - 02:32:57.600, Speaker A: Maybe we can solve this problem based on our learnings from WASM with EVM. So let's do a similar experiment with EVM with these three opcodes. We call them Admod 384, submod 384, mole mod 384 and we believe that these will what we call cover all bottlenecks for BLS twelve 381 operations. We hope this isn't confirmed yet, but we're getting closer and closer. We'll talk about it in a moment. And we want to be generic as well. So it doesn't make sense for us to do domain specific, curve specific, modulus specific optimizations because then it's not usable by other people.
02:32:57.600 - 02:33:40.526, Speaker A: We are towards this sort of blossoming renaissance of crypt, of user deployed crypto. We want it to be generic for other BLS twelve 381, but also 377, maybe Starks, which are heavy on a lot of crypto, is heavy on polynomial evaluation to verify Stark. And a polynomial evaluation is just these operations. It's precisely these operations. We're also talking about other ones, some sort of algebraic hashing, things like this. So we don't think that these are very limited opcodes, these are very generic opcodes in fact. And the EVM 384 might also be a template for EVM 378.
02:33:40.526 - 02:34:32.138, Speaker A: The talk is almost done. I'm going to go through some progress that we've made, but all the progress updates are listed in this magician, so you can click through yourself to get more details. I'm just going to be very high level. Is it feasible? There's a chart on the left is the same rust native and then this middle one is the same WASM one that's like two x slowdown. And so our first iterations you can see it's another two x slowdown from WASM with big NUMS, with big int host functions and it's not fast enough yet. We did a synthetic pairing algorithm. We talk about it, I don't have time to explain it, but it's sort of an approximation, it's an estimate of a pairing runtime because we do a similar amount of arithmetic.
02:34:32.138 - 02:35:06.406, Speaker A: But there is potential. The problem is there's awkwardness with the stack, with the EVM, there's just some things that are awkward. So we iterated and we kept iterating. And you can see our iterations. The independent access are the various versions, the dependent access is the milliseconds and you can see the version v one through v seven at a certain point. These are all different interfaces, what we call interfaces. So that means how the stack has to look how the environment has to look when we call this EVM Opcode, these EVM Opcodes.
02:35:06.406 - 02:35:27.802, Speaker A: And it turns out that we can do some things to make it faster and faster. So we went from much slower than two x of WASM or slower. We tried these different interfaces along the way. We tried different languages. We started with Yule, and then we went to Huff. Huff. You remember Wire Strudel from earlier? The EC mull breakthrough.
02:35:27.802 - 02:35:47.846, Speaker A: That was shocking. They used Huff, which is in EVM, you write sort of in this EVM assembly, but with macros. So it's not as unforgiving. But you're very close. You're actually using the EVM. You're writing EVM opcodes. And that gave another speed up.
02:35:47.846 - 02:36:10.400, Speaker A: You can see from V One to V two, that was like close to two X. Then from V Four to V five, we had another improvement with Huff. We had another, like, two X. And all of a sudden, all the way on the right is WASM, and now we're faster than WASM. This is shocking. We were hoping that to match WASM with Big Int, but it turns out that we can beat it with a carefully chosen interface. And we settled on Interface V seven.
02:36:10.400 - 02:36:36.950, Speaker A: This is all based on this synthetic. It's not the full pairing. We're using this sort of synthetic with something called an adjustment factor. But we want to start implementing. So that's what we did, the full pairing. A big chunk of the pairing is the Miller Loop. You can think maybe half of the whole pairing algorithm runtime is in this thing called the Miller Loop.
02:36:36.950 - 02:37:14.862, Speaker A: It gets a little bit more complicated, but just at a high level. It's a huge chunk of the pairing. We implemented the Miller Loop from this library that I believe holds the speed record, now called BLST Blast. But we simplified their mall FP two. They do some handwritten assembly machine code optimizations, which would be expensive using EVM 384. So we used a more generic simplified version of it, of this Mall FP Two function, and our numbers agree. So we are correct with BLST.
02:37:14.862 - 02:37:52.234, Speaker A: The numbers agree. Our EVM 384 Miller Loop and the BLST Miller Loop, they agree on the results after we change this mole FP Two to be the same one. So we're very close. Correctness is still an open question, but we believe it seems to be that we're on the right track. And notice that our V Seven, we're still using the V Seven interface for EVM 384, but we have even better numbers than before. So all the way on the right is our WASM with Big Int. Just left of that is our old synthetic pairing.
02:37:52.234 - 02:38:06.318, Speaker A: And we went from 5.5 milliseconds now with our Miller Loop 4.7. So we're getting better and better, and we're approaching native. Now. There's a whole story to be told about native. We're doing naive things. We're using some Azure VM that's mid grade.
02:38:06.318 - 02:38:52.942, Speaker A: There are algorithms there are optimizations that we're leaving out, but it seems that we can get faster and faster. And of course, maybe all these times can be halved once we do some tricks. And there are some other tricks that we can't do that you can do in native. But EVM 384 isn't generic, but we want EVM 384. So there's a tension with remaining generic, and I think we're okay if we're desperate, we can do these modulus specific or optimizations, not even modulus specific, but just based on what type of modulus it is. We can do some optimizations, but we're not doing them yet, and I don't think we need them. I think we're reasonably fast, and we're fast enough, I think.
02:38:52.942 - 02:39:20.154, Speaker A: So where are we right now? Finishing BIOS. Twelve 381 pairing. To justify, we want to say that we want to make the statement we cover with EVM 384, we cover all bottlenecks or BS. Twelve 381 pairing and we're pretty fast. We want to make that statement. That statement is close to being made, but not made yet. EVM three D, four EIP is being written right now.
02:39:20.154 - 02:39:53.640, Speaker A: We are soliciting implementers. The pairing is one algorithm, but there are other algorithms to be implemented, and we are soliciting implementers advice. We are in discussions with teams and with the BOST people, for example, and with other people. We are asking if they need this kind of thing. But we're also having an open call. We're soliciting feedback, ideas and implementers especially. It would be great to see this used.
02:39:53.640 - 02:40:05.640, Speaker A: This EIP has a lot of support already, and that's where the talk will end. I will open for questions. I don't have the text window in front of me.
02:40:07.470 - 02:40:26.750, Speaker C: All good, Paul. Thank you so much. Excellent talk. I can give you some questions that have come up on the live chat. So, first one I have for you is, let's see, how difficult is it to write the low level crypto operations in EBM using the EBM? Three, eight, four, opcode?
02:40:27.250 - 02:40:59.750, Speaker A: How difficult? Oh, goodness. It depends on who you ask, of course. So they're asking me it depends on your background in cryptography. It depends on your background in EVM. There are growing pains, of course. Once you overcome the growing pains, it gets easier, and it depends on the tools you're using. I think Huff is a great tool, but writing the code in Huff is tedious because the V seven, I didn't go into details about this interface, but we use something called packing, offset packing.
02:40:59.750 - 02:41:37.458, Speaker A: And if something changes with the offset, we have to go in and manually change it. So I have a generator written in Python. It's modeled after the JavaScript generator for the WASM Curves code that we used earlier in the talk. And this generator has been helpful because I just implement, for example, some small parts of the pairing, and then I can reuse them. So I'm generating the code in Python. The huff code and then the Huff code is compiled with the Huff compiler. So it's multiple steps, very awkward and a lot of growing pains, a lot of struggling to debug it's.
02:41:37.458 - 02:41:52.780, Speaker A: My least favorite thing, it's the bane of my existence, is going opcode by opcode and finding some bug or something. So it's very painful at first, but as you go along, like with most things, it gets easier. That's it.
02:41:53.310 - 02:42:03.950, Speaker C: Understood. All right, I have another question, if you have the time, Paul. What are the blockers to saying this on Mainnet and are there any security concerns?
02:42:05.890 - 02:42:20.878, Speaker A: What are the blockers, the finish of the pairing? It seems like we need to have a concrete evidence. We need to have a know. Here it is. You can't take this away. It works, it's fast. You can't take that away from us. So it seems like that's a blocker.
02:42:20.878 - 02:42:58.346, Speaker A: We want to say that it covers everything. And then security, yes, there are security concerns. Obviously, with a consensus system as big as Ethereum, of course there are huge security concerns, but there is demand. And I want to answer this question, there are security concerns with the alternative. So I think this is a path of least resistance in terms of security. The operations we want to implement are maybe 30 lines of code, whereas if the alternatives, if we want to do alternatives, perhaps pre compiles, it's much more than that. It could be thousands of lines.
02:42:58.346 - 02:43:10.340, Speaker A: I think it is. So I think that this is the most secure thing we can do if we are to do something like this. But yes, of course. That's it.
02:43:11.510 - 02:43:35.020, Speaker C: Thank you, Paul. Didn't mean to cut you off there. Thank you, Paul. I think I'm going to take this opportunity to introduce our next speaker. Our next speaker is Tim Biko. He is the product owner of Consensus's Besu client. He is doing a lot of the coordination around R and D with EIP 1559.
02:43:35.020 - 02:43:48.320, Speaker C: Let's see. So EIP 1559 is a proposed change to Ethereum Spee model with implications for UX and monetary policy. So today Tim is just going to give us an overview. So welcome, Tim.
02:43:51.110 - 02:44:33.822, Speaker A: Everybody. Thanks for having me and thanks for the intro. So, yeah, like Emily was saying, I'm going to give an overview of EIP 1559. I work with Consensus and our team has been championing this E for the past couple of months. So we've had a good view into kind of what it is, where it's at and what's needed for it to get deployed. So first in this talk, I'll just give a super quick background on what EIPS actually are and how they work. Then I'll give a very high level overview of 1559 and we'll spend most of the talk kind of digging deeper into the various parts of the EEPs to try to understand how they work.
02:44:33.822 - 02:45:24.654, Speaker A: And we'll have plenty of time for questions at the end. So what are EEPs so EEPs, as probably a lot of attendees know, stands for Ethereum Improvement Proposals. And in short, they're the mechanism by which we can make changes or propose changes to ethereum. And this was basically copied from the bitcoin improvement proposals process which some modifications and the bitcoin improvement proposals were modified off the Python improvement proposals. And it's really just a set of guidelines that anybody can use to submit a proposal for how we would make a change on ethereum. The high level guidelines about how to go about it are all listed in the first EIP, which is EIP one. And then how you go from having a proposal for a change to ethereum to get adopted depends on which type of EEP it is.
02:45:24.654 - 02:46:34.818, Speaker A: And for our purposes, there's really just one distinction that matters, whether it's a core EIP or not. So a core EIP means that we're changing the consensus of the network and that requires every client on the network to upgrade their node and to process the upgrade at the same time, which is what we call a network upgrade or a hard fork. Therefore these EIPS need to be kind of adopted exactly at the same time across the whole network and there's no opting in or out except from forking away but for everything else. So every other type of EEP, there's no real requirement for people to adopt them at a fixed point in time, which gives us much more flexibility. And one good example of that is, for example, ERC 20, which is the popular token standard, is just an EEP that exists to specify the standard and people are free to use it or to stop using it. But there's really no timeline associated with when projects or if projects have to comply to that. So looking at just the different categories of EEPs, like we mentioned, we have core EEPs which are part of this first big kind of group which we call standards.
02:46:34.818 - 02:47:05.626, Speaker A: So there there's four subcategories. The core ones which we mentioned 1559, is a core EIP, so it will require a network upgrade. But along with those we have other types. So networking EIPS affect things like dev P to P swarm the Light client protocol. So basically all the ways in which clients on the network talk with each other. There's interface EIPS which as the name states, deal with the various APIs on Ethereum. So things like the JSON RPC API, the contract abis and whatnot.
02:47:05.626 - 02:47:50.510, Speaker A: And finally there's this last category called ERC which stands for Ethereum Request for Comments, which is about application level standards. So these things are not part of the protocol itself, but they're things that applications on ethereum can use. And the two better known one are ERC 20 and ERC 721 which create fungible and non fungible tokens. And then outside of the big category of standard EEPs, there's two other kind of smaller categories. The first is meta EEPs. We typically use those for network upgrades. So for example, if there's a new upgrade, we'll list all of the individual EEPs that are going into the upgrade in a single meta EEP as well with the upgrade block and whatnot.
02:47:50.510 - 02:48:27.938, Speaker A: And finally, there's informational EEPs which weren't used much until recently. Danny Ryan and Vitalik used this to publish an EEP for the E Two phase zero spec. And so this gives us some background on what EEPs are in general. Now, let's look at 1559 more specifically. So the EEP was put forward by Vitalik in 2019 and really has kind of three high level goals. So first is to make the transaction fees on the network more predictable. So right now, transaction fees on the network are set using a first price auction.
02:48:27.938 - 02:49:13.320, Speaker A: So that means you have to look at what everybody else is paying, make a bet as to what you think the right price is going to be, and then you'll pay whatever that bet you made is. And that leads to overpayment, because if the maximum in a block was 100 GWe and you thought it was going to be 120, you could have just spent 101 and you still would have been higher than everybody else. And we see that a lot on the network today. The other thing that 1559 wants to help with is to reduce the inclusion delays for transactions. So today when you send a transaction, blocks are mostly full on the network. So you get put in the transaction pool, you wait for a while and then you get included. So 1559 hopes that in most cases, you're just able to get your transaction right into the next block and not wait in the transaction pool as long.
02:49:13.320 - 02:49:56.254, Speaker A: And then the third kind of goal of 1559 is to create a positive feedback loop between the network usage on Ethereum and the eat supply. So today there's really no kind of way for more usage to translate into a higher or lower EAP supply and to have any impact there. So the EAP is going to try and address kind of those three things and at a very high level. The way it does it is this. And this what we'll spend most of the talk getting into. But we make the transaction fee more predictable by basically just saying what's the minimum fee a transaction has to pay to be included in a block and putting that right into the block header. So we call this minimum fee the base fee.
02:49:56.254 - 02:50:39.140, Speaker A: And this way you can look at any block on the chain and know this is the minimum that a transaction would have had to pay to be executed in that block. And it makes very easy to see if you can be in or out. And then the way we reduce the inclusion delays for transactions is we double the block size on the network. So when there's going to be the network upgrade, the block size will effectively double from say twelve and a half million, where it is today, to 25 million. And then we just aim to keep it half empty most of the time. So even though the risk capacity for 25 million we try to keep our blocks around twelve and a half. But that means that for the marginal transaction that you want to include there's always kind of half the block of space that you can put it in.
02:50:39.140 - 02:51:32.862, Speaker A: And the way we do those two things is basically how we also create this feedback loop between the network usage and the eat supply. So we set the minimum fee, again, this base fee as a function of how much gas was used in the previous block. So if there's more than 50% of the block that's used, we'll keep raising the base fee. Kind of like surge pricing just to make it clear that more people want to use the network and the minimum you need to pay to use it goes up. But then, because miners could potentially game that, we decide to burn the base fee. And this results in where when we have more demand on the network, the base fee goes up and then we burn more and more of Ether as a function of that for a same number of transactions. So, sorry, that was not super clear, but basically every transaction base fee is burnt.
02:51:32.862 - 02:52:15.698, Speaker A: And so if the base fee keeps going up, then for transactions which pay a higher base fee, there's just a bigger and bigger total amount of Ether that's burnt over time. Okay, let's dig into this in a bit more detail. So there's kind of three big components that we touched on. So the variable size blocks, there's the base fee and there's two new parameters under 1559 called the fee cap and the tip. And finally, there's this fee burning, which we just touched on, that we'll dig into more detail. So the variable size blocks, like we said, under 1559, we want the blocks to be roughly half full. If they're more than half full, we start raising the base fee and if they're less, we start lowering it.
02:52:15.698 - 02:52:59.658, Speaker A: And the reason we can do this is because we raise the base fee and lower it very quickly. That means that even in a worst case scenario where there's a ton of demand, we'll only have blocks be completely full or twice the size of current main net blocks for a short period of time. So the base fee can increase by twelve and a half percent maximum between each blocks. That means that in roughly 20 blocks, or five minutes gas prices are up ten X. In ten minutes they're up 100 X and in 15 minutes they're up 1000 X. And to give you an idea, gas prices in the entire history of Ethereum have ranged mostly between 1000 X. So I think the highest we've ever seen was recently with 1000 GWe and the minimum we have is one GUI.
02:52:59.658 - 02:53:49.680, Speaker A: So that means that in 15 minutes we get as much kind of variability on the base fee as we've seen in all of Ethereum's history. And then once the base fee rises high enough, people will stop being willing to pay that price. So the blocks will stop being full and the base fee will drop again until we've reached this equilibrium where blocks are 50% or more full. And so the way users deal with this in their transactions is by specifying how much they're willing to pay at a maximum for their transaction to be executed. And this parameter is called the fee cap. So the fee cap basically says, this is the absolute maximum I'm willing to pay. And this way, if the base fee goes up or down, you have some wiggle room there.
02:53:49.680 - 02:54:46.350, Speaker A: And because the base fee gets burnt, like we've mentioned and we still want miners to process transactions to hold the state and whatnot, users can also specify an optional minor tip. And this is the part of the transaction fee that would go directly to the Miner. So when you process a transaction, basically you want to check that its fee cap is larger than both the base fee and the tip. If that's the case, you simply process the transaction and whatever is left over after the base fee and the tip have been deducted from the fee cap you refund directly to the user. So there's not really any harm in selling a high fee cap if you don't care about paying a very specific base fee. Yeah, and again, the tip goes directly to the Miner and there's actually a couple of uses for that. So the first is to make sure the miners process the transactions versus mining empty blocks.
02:54:46.350 - 02:55:50.070, Speaker A: But it's also a way for users who absolutely need their transactions to be included ASAP to signal that. So either when you have transactions, say, like DeFi arbitrage trades, where there's a very high opportunity cost of your transaction not going in, you might want to specify a very high tip for that. Or if there's a ton of congestion on the network and we're in one of these rare periods where blocks are actually full up to 200% because there's high demand, then you might want to specify a very high tip to signal that you want your transaction to be included as soon as possible. And so if we take a step back, what we're really doing with 1559 is we're adding this base fee field to the block header on the transaction side. We just remove the gas price and replace that with the fee cap, which is our maximum amount that we're willing to pay, and the tip, which is what we want to set to the minor. And again, the fee cap from the fee cap, you deduct the base fee and the tip you pay that base fee is burned tip. Goes to the miner and whatever's left over goes right back to the user.
02:55:50.070 - 02:57:08.958, Speaker A: And what's very nice about this is we can actually keep supporting legacy transactions under 1559 without any changes if we set the fee cap to be equal to the gas price and the tip to be equal to the difference between the fee cap and the vase fee. And this means that if for whatever reason you're using a service that can't upgrade to 1559 transactions you don't want to or for some reason you don't have access to it, you can still get your transaction included on the network. We don't have to have like a transition period or eventually deprecate those transactions. But the only downside is you don't get that fee cap refund that native 1559 transactions would get. But this is still a pretty big win because it means we can really simplify how we end up deploying this on the network and we can offer long term support of legacy transactions. So the last big chunk of 1559 is the fee burning and it's worth thinking about why we actually burn the fee. So like we mentioned earlier, this base fee we really want to be a function of the usage on the network, right? Like, we want it to be kind of ultra surge pricing when there's a ton of people that want to use it because it makes it clear that you can't get your transaction included for less than that but then you want it to be lowered as soon as that demand goes away.
02:57:08.958 - 02:57:57.710, Speaker A: And if miners receive that base fee they would want it to be as high as possible. So one way you can make them indifferent to whether the base fee is high or low is simply by burning it and removing it from the equation. And to be clear, this means that the miners still get the tip and the block reward which historically has been the bulk of the revenue. So recently with DeFi we've seen fees grow a ton sometimes being much higher than block rewards. But this has been kind of an anomaly. And like we mentioned, a lot of these transactions with very high fees might still want to put a large portion of that into their tip because of the opportunity cost of not being included. So the kind of income reduction to miners might be less significant than it appears at a first glance.
02:57:57.710 - 02:59:05.560, Speaker A: And again here when we burn the base fee we're basically creating a positive feedback loop between how many people are using ethereum and the ether supply because as the demand to use ethereum block space increases, the base fee will go up like we've seen kind of gas prices go up over ethereum's history. And that means that every single transaction burns more and more ether and it's a sort of redistribution to all of the ether holders by decreasing the supply. And as one kind of final point, I wanted to address some misconceptions that we hear a lot around 1559 and take the opportunity to kind of address those a bit more broadly. The first big one is the idea that 1559 will bring gas prices much lower. And this is only very partially true. So like we mentioned at the very beginning, it makes gas price estimation much easier. So it leads to less overpayment because you can know exactly what's the base fee of a previous block and based on the gas utilization, what the base fee of the next block will be.
02:59:05.560 - 02:59:53.894, Speaker A: So prices will be kind of slightly lower at the margin. But the kind of average gas price of Ethereum is always a function of supply and demand, and 1559 doesn't do much to address the supply side of block space. So if we want to radically decrease gas prices, what we need is a massive increase in block space on Ethereum. And this is where things like layer two, like roll ups or just like E two, can help a lot. But 1559 really won't make a major difference here. And the second kind of misconception is that 1559 will make Ethereum or ETH the asset deflationary. And this is only true in the case where the base fee is consistently burning more ether than the block rewards provide.
02:59:53.894 - 03:00:34.770, Speaker A: And until recently, this really wasn't the case. Block rewards were by far the biggest source of revenue for miners than transaction fees. And again, it's worth mentioning that the tip doesn't contribute to supply reduction. So if the types of transactions we see on the network are high tip with low base fees so there's not that much average demand, but there is some very kind of high paying demand. A small amount of that. Then that won't lead to as much burn than if we just see a ton of kind of demand on average, with very low tips. So it's a bit more complex than just saying, because we burned the base fee, we'll make ETH deflationary.
03:00:34.770 - 03:01:30.420, Speaker A: And there's one more thing I wanted to share in this talk. So we learned this over the past 24 hours, and it's great, but we've been doing a lot of work on 1559. And now the ethereum foundation has awarded both consensus and our 1559 multisig, which we've used to pay all of the different teams working on this a grant to get all of the client implementations done, to have proper project management around the EIP and to do extensive testing and bug bounties. So this is good. In the past couple of months, I know a lot of people in the community were concerned about whether there were appropriate resources behind this and whether there was appropriate funding. So now, with both the funding that we got off Gitcoin a couple months ago and the EF, we should be good to get this through the finish line.
03:01:33.110 - 03:01:33.570, Speaker E: Again.
03:01:33.640 - 03:01:44.280, Speaker A: Happy MLG and this is all I had. Happy to take some questions now. And if you want the slides with the links, you can get them from this QR code.
03:01:44.890 - 03:01:56.330, Speaker C: Awesome. Thank you so much, Tim. A lot of great content there. Congratulations on the grant. That's really exciting news. Yeah. So I have a couple of questions from the chat.
03:01:56.330 - 03:02:11.920, Speaker C: The first one is, so what needs to be done for 1559 to make it on main net? We can research forever and just continue iterating forever, but where is the line where we say, let's drip the next hard fork? What still needs to be done to reach that?
03:02:12.310 - 03:02:43.178, Speaker A: Okay, that's a great question. If I can share my screen again. So a lot of people have asked me this question and I've put together what I call a main net readiness checklist, which is tracking all the things we need to do to get 1559 live on main net or 1559 to a point where it can be considered for mainnet. There's a couple of things. So obviously we need client implementations. This is all a work in progress. We have three teams right now geth Basu Nethermind working on it.
03:02:43.178 - 03:03:19.080, Speaker A: They all have work in progress implementations. Open, Ethereum and Turbogeth said they're kind of happy to implement once we're closer to having a final spec. So I think we're mostly covered there. There's a couple of issues that we needed to address in the spec. We've already solved a couple of them when I've mentioned about supporting legacy transactions in the talk. And now the biggest ones we see are, first of all, just denial of service risks on mainnet are kind of one of the biggest, I guess, blockers for a lot of EEPs. So account abstractions, which was a previous talk, has some concerns with that as well.
03:03:19.080 - 03:04:09.346, Speaker A: I won't go into too much detail on this, but this is when people complain that the state size is growing too quickly on main net and we can't manage the state 1559 twice as big blocks makes that twice as worse. So we need to wait for solutions to that to just be available. Most of the client teams are working on it. Turbogast is whole kind of premise is dealing with that more efficiently. Guest has a snapshot sync that they're working on and we also have EIPS in the work to try and reprice some of the opcodes to address that. Then we have several other small issues with the EEP, but I don't suspect any of them is too big of a deal. So I think once we have our implementation kind of done, which we expect to get roughly by the end of the year, we should be ready for the next hard fork.
03:04:09.346 - 03:04:28.266, Speaker A: So it probably won't be Berlin, but possibly the one after. And especially if we can get these issues around the denial of service risk and state management issues solved on Mainnet. But if you want to see the full list, you can just go on the Ethereum PM repo under fee market. There's the checklist there.
03:04:28.448 - 03:04:47.170, Speaker C: Awesome. Thank you so much, Tim. So I have another question. It's kind of a minor question, kind of digging in semantics a little bit, but it appears that your goal is to make a more stable fee. And you discussed positive feedback, and the question here is, wouldn't you want to be using negative feedback?
03:04:48.550 - 03:04:58.920, Speaker A: So, yes, as more people use the platform, there is like a negative pressure applied to the supply. Right. So there's less and less ether in circulation. Yeah.
03:04:59.530 - 03:05:10.230, Speaker C: Okay, cool. Got it. I also have another question. Someone just threw up here. Have you talked with Tim Rogarden?
03:05:10.910 - 03:05:43.278, Speaker A: Yes. So Tim Roughgarden is a researcher, computer science game theory researcher. He's being paid by, I believe they're called the Decentralization Foundation to do an economic analysis of 1559. So it's not done yet? It's in progress. We've been in touch with him and I suspect in the next months or so we'll see the outcome of that. But the goal is to just have a kind of formal analysis on the economic side of what actually 1559 does. How does it compare to the current fee market on ethereum?
03:05:43.454 - 03:05:51.160, Speaker C: Cool. Another question, just if people want to keep up with 1559 or any of the other work you're doing, what's the best way for them to follow?
03:05:51.530 - 03:05:54.566, Speaker A: I love that. It was my last slide. I will share again.
03:05:54.668 - 03:05:55.574, Speaker C: Please do.
03:05:55.692 - 03:06:17.520, Speaker A: Yeah. So I listed four of them up here again. You can get them from the QR code, which is in the live stream now, but the Eevee itself is probably the first place you should have a look. It keeps the spec up to date. Then we have implementers calls every month or so. They're kind of like Core Devs calls, but just focused on 1559. We have recordings and transcripts which are linked there.
03:06:17.520 - 03:06:42.120, Speaker A: I write kind of blog length updates every couple of weeks. So if you don't want to read a whole hour call transcript, that kind of aggregates the different things that have happened on 1559 over the past few weeks. And finally, Eat Magicians has a thread also with 1559 where a lot of the spec and whatnot gets discussed and debated, if you're more interested in really the nitty gritty of how it's going.
03:06:42.490 - 03:06:52.922, Speaker C: Got it. Cool. That all sounds really good. Tim, again, thanks for joining us today, giving us that overview. Really appreciate you making time. Great.
03:06:52.976 - 03:06:53.674, Speaker A: Thank you.
03:06:53.792 - 03:12:41.680, Speaker C: Thanks. Just a reminder to everybody watching, if you want to get involved, you have questions, head over to our chat live ethonline.org. I'm going to go ahead and take a quick break, five minute break, and then we'll be back with the rest of our speakers. Hello. Thank you guys, for hanging on through that quick break. I hope you guys enjoyed the lo fi radio. I know I did.
03:12:41.680 - 03:12:58.440, Speaker C: I'd like to take this opportunity to introduce our next speaker. John Walpert will be speaking today. He's the founder of Baseline. He'll be sharing an overview of Baseline protocol which enables enterprise use of mainnet ethereum. John, welcome. Take it away.
03:13:02.090 - 03:13:06.540, Speaker F: Okay. Hi everybody. Can you hear and see me? Oh, you can't see me yet.
03:13:06.910 - 03:13:08.554, Speaker C: You are all good. John.
03:13:08.752 - 03:13:39.140, Speaker F: Hey, it's great to see everybody. It's been quite a year for the Baseline Protocol team. I'm going to tell you about that and set it in some context. Would love to have some questions. No slides today, I'll just be talking to you. Baseline protocol is not I'm going to start with what I always say to everybody. Baseline Protocol is not a coin, a token, a scheme, a platform product.
03:13:39.140 - 03:15:25.560, Speaker F: It is simply a technique that is under standards development, that is under the Oasis Body for Standards Development, which is a venerable standards organization going back 40, 50 years, I guess behind SGML and AMQP and MQTT and all sorts of standards. The Baseline Protocol is a standard for using a public main net like ethereum as a common frame of reference for integrating and effectively enforcing the consistency between records held in different traditional systems of record like SAP, Microsoft Dynamics or Mongo or anything like that. And the problem that solves is, right now, 30% of any large companies, even the largest of companies like a Boeing 30%, as many as 30% of the companies that work with them, suppliers and others don't do enough volume. Or aren't big enough or have systems that warrant setting up a siloed dedicated integration hub or bus in order to maintain consistency between Company A's system of record and company B's. So what's involved in that distributed Systems 101 is the need to have a common frame of reference. Either you're going to primary one system to the other and most security officers don't love that option. It also gives a lot of control to the primary company.
03:15:25.560 - 03:16:21.030, Speaker F: Or you can set up an integration bus and have this common frame of reference between you. But now we have silos upon silos upon silos. And what does an integration bus do? Well, one of the things it does, the simplest thing is effectively ordering and hash management. That is, I need a way for my record in my database to be verifiably identical to your record in your database. Say my purchase order is I'm going to send you a purchase order, I'm going to message you that by a peer to peer messenger. And now we need a way of breaking the two generals problem of knowing that we've completed the circuit without continuously pinging and hacking. And then we need to be able to say, yes, that proof over there is proof not of existence, but of consistency.
03:16:21.030 - 03:17:17.560, Speaker F: We are certain to have this and neither of us can say that we didn't get the memo or that we fat fingered the entry or something like that. That proves that we at one point in time were on the same page without having to resort to what you would see in a traditional or what we've been doing in Enterprise blockchain. For some of you may know, I helped build this thing called hyperledger fabric. And then I kind of saw the light and said, public blockchain is kind of where I want to go. And the reason is, I think blockchains, all kinds of blockchains are more or less terrible databases. And that was sort of the thought we had when we started doing Enterprise database or Enterprise blockchain was, hey, let's have a single source of truth. Let's put our data collectively on a shared system.
03:17:17.560 - 03:18:13.050, Speaker F: And when you think about that for more than ten minutes, it should have been obvious to us five years ago, okay, now I have my internal sensitive data on a shared threat surface that is now bigger, has a higher risk of exposing that information for every company that has a note. So even a private blockchain is kind of all blockchains are sort of digital nudist colonies. A private blockchain is just a digital nudist colony on a private beach. And that's not a terribly good pattern. Blockchains aren't good at compartmentalization, and companies need compartmentalization. They need ackles, and they need to be able to say, I'm going to share this record with you and not with you, even though we're all in the same supply chain. But then you need to know that the output of what we're doing here has integrity, and that you can do your step later on without worrying about shenanigans.
03:18:13.050 - 03:19:12.778, Speaker F: Baselining is a way to do that. You say, okay, we've got this record in common. We also have shared using zero knowledge circuits. We're able to say that proof is also effectively a key and a key value lookup, and it can enforce workflow integrity and say, this purchase order follows from this master service agreement, and it had to have followed the rules of the Master Service agreement, or it could not have deposited its proof on the main net. If all of this sounds incredibly boring, it is. We like to say that with baseline, boring is the new exciting, after five years of Enterprise blockchain hyperbole part of, I have to admit, to perpetrating some of it along the way. Now we have something where a CISO, a security officer, a CIO can say, oh, that makes sense, that's not scary, and that's not an inappropriate use of blockchain.
03:19:12.778 - 03:19:53.306, Speaker F: I'm still using my SAP. And now we don't have to sell against an SAP. We can say, more blockchain, more baselining means more SAP means more main net. And you'll hear me always talk about the main net. I think we're defining the job of the main net, and Ethereum is my vote for the most likely candidate to perform the job. A main net is a common, always on state machine that resists tampering and resists any group from locking you out of valid transactions or tampering with history. That's what a main net must do.
03:19:53.306 - 03:20:39.580, Speaker F: And Ethereum does a pretty good job of that. And if all you're asking it to do is manage proofs and use those proofs in a clever way that allows you to do ordering and workflows, even ETH one has enough scale to handle a lot of that, and ETH two will get us all the way there. So that's the nutshell of block of baseline started between Microsoft Ernst Young and Consensus back in June of last year and launched officially as an open standard in Oasis. You can go to baseline protocol.org and get involved. It's an openly governed, open source body. Contribution is the only power.
03:20:39.580 - 03:21:17.000, Speaker F: You can be a, you know, you can be a sponsor and you're not going to buy yourself any votes. The only thing that gets you a vote for anything is is contribution. And so please get involved. We have now 700. Last week I had to say or a few weeks ago I had to say 600. Now it's 700 active participants. And it just keeps growing company after company after company, big companies, small companies, and it's a really great opportunity for a small company to jump in, make a mark and become pretty well known pretty quick.
03:21:17.000 - 03:21:21.560, Speaker F: And that's been happening a lot. Any questions?
03:21:24.650 - 03:21:33.770, Speaker C: Sorry for all that. I was muted for a second. Hey, John, I just have to comment. Your video setup is very impressive.
03:21:34.590 - 03:21:37.718, Speaker F: Everybody's got to have a hobby. I got a little spendy during COVID.
03:21:37.814 - 03:21:59.860, Speaker C: I mean, it's beautiful. Let's see, let me check the chat real quick, see if we have any comments coming in. Still waiting on that for a second. Let's see if you guys have any questions or comments for John. Please head up. Live ethonline.org. All right, question for you.
03:21:59.860 - 03:22:08.630, Speaker C: What can non enterprise devs do with this? Meaning, like, if you don't work at a big, like, how does this serve you? How does this help you?
03:22:08.780 - 03:23:13.018, Speaker F: Okay, well, in fact, what's most exciting to even the big companies right now is being able to have this common, always on integration bus called the pain net called Ethereum in this case, and small, tiny companies. So imagine like, say you're a radish farmer and this is actually a story and you get a purchase agreement from your wholesaler big company, they're using SAP or some monstrous system, right? Old school. And you don't even know what that is. But you have an iPhone, you can mash a button. A lot of people are saying now if you can't at least do an electronic signature, maybe I shouldn't be working with you. The days of needing to do paper are very quickly ending, but you don't really know about anything about it. You've got your iPhone, you get this DocuSign, you mash the button and then what if it right after that, it said your wholesaler will give you points or benefits.
03:23:13.018 - 03:23:40.130, Speaker F: Spiffs if you click here and for 995 a month, you have a system of record that is baselined. So that now that wholesaler is a lot more confident that you have some kind of record keeping that they can be confident in. Just that alone can make a big difference to even a radish farmer. And that, by the way, free idea. Go ahead and build that and let me know if you do, because I'll promote the heck out of it.
03:23:40.200 - 03:23:54.630, Speaker C: Awesome. You hear that, everybody? That was a free idea. I got another question for you, John, coming in from the chat. In your experience, what's the biggest hesitation enterprise users have when thinking about baseline and thinking about ethereum?
03:23:57.610 - 03:23:59.842, Speaker F: Say that again, I didn't press the first mark.
03:23:59.916 - 03:24:09.002, Speaker C: It's what's your biggest hesitation? Enterprise users, when thinking about baseline, what are their hesitations? Sorry, I can't think of a synonym for that.
03:24:09.136 - 03:24:52.570, Speaker F: So I used to work for this innovation company 20 years ago, and there was a great line in there, it was called but the butt never, always bring two butts to an argument or a discussion. And what that means is scientists, when you try to do brainstorming and you say, well, you can't just do a brainstorming and say, no killer phrases, all the ideas are good. No scientists know that's not true. I used to work at IBM Research and a lot of scientists, and so what we used to say is, you can always say, but that won't work. But then you have to say, but it would if even if your second butt is crazy. Like one time this is true. Sorry, I was working on something and we were doing that, and somebody said, but that won't work.
03:24:52.570 - 03:25:20.130, Speaker F: But it would if gravity was different. And one of us went, Wait a minute. And obviously we didn't change gravity, but it led to a pretty good idea. Do that five times, you'll have an important idea. Like most often changed my life, that way of thinking. And in a way, that was the process we followed with to get to baseline. We said, well, here are ten reasons.
03:25:20.130 - 03:25:48.470, Speaker F: If you go to my Twitter feed, at the top of it you'll see an article called Common Sense Statement on the Main net for Business. I think something like that. And that came out of an article that we wrote with the EEA Main Net Working Group I wrote called Ten Reasons Why I Won't Use the Public Blockchain for Business. And then, of course, the subtitle and how we can make it. So I will.
03:25:48.640 - 03:25:49.598, Speaker C: Got it.
03:25:49.764 - 03:26:17.874, Speaker F: And the ten reasons are finality is an issue if you're using it as a database. You've got scale, finality custodianship, GDPR issues, privacy issues, confidentiality issues, which are different. Confidentiality is about the business logic. Privacy is about the data. Both things matter, right? If I can decompile that smart contract and it has identifiable attributes that I could use to figure out who you are, and what you're doing and with whom. It's not okay for business. Sorry.
03:26:17.874 - 03:26:59.154, Speaker F: I don't think it's okay for tiny businesses. I mean, if you're dealing with grandma's pension and you're a three person company, you've got the same security issues as any giant company. We don't need a standard for enterprise ethereum versus everything else ethereum. We need a standard for Ethereum. And it needs to work for all of us. The smallest companies or the smallest teams, the most innovative or disruptive teams out there, sooner or later, if they're successful, are going to be the ones that everybody says, oh, they're the bad guys. I still remember when Google was two or three people and don't be evil was a thing, right? There's a life cycle to all this.
03:26:59.154 - 03:27:34.634, Speaker F: So they say, build security in from the beginning, don't tack it on later. So all of these reasons why you wouldn't use the blockchain particularly, all boil down to thinking of blockchains as databases. Don't put data on blockchains. Don't put the primary business data on blockchains, right? 80% or more of that is sensitive in some way or another. You don't want your competitors or other people knowing about it all the time. Even when people say they want to be transparent. Now, you don't really want transparency.
03:27:34.634 - 03:28:15.530, Speaker F: You don't want everybody knowing where all the food is in a supply chain. That's a good way of bad guys finding out ways of doing bad things. You want transparency between counterparties when that's appropriate, and you want to have some control over that. But we don't want to keep on building all these silos. So if we can use the Main net so that's when we said but if we were using mainnet effectively as an ordering and hash management tool and a consistency machine where we're saying where it can say you're consistent. I don't know what about. And the thing I'm using to tell you that you're consistent looks like noise and sounds like a metronome to anybody.
03:28:15.530 - 03:29:19.440, Speaker F: Any casual observer of the main net, then that's okay. But it would work if we so in a way, from a business perspective, and I'm not saying this is the only use of Ethereum blockchain. I'm saying that it's kind of like I wrote an article recently about how the Bose noise canceling the history of noise canceling headsets relate to the future of the Internet. And at the end, the point is there's lots of other uses of compute silicon lithography than just noise canceling headsets. But it was a nice, satisfying specific thing you could do the math on and know at what point transistors would be able to cancel noise out of the air in 1000th of a second. You could do the math on that. You say, Well, Moore's Law is going to give me I'm going to be able to do that in 1992, actually, 1998, and I'll be able to do it on batteries in 2006.
03:29:19.440 - 03:29:43.554, Speaker F: What is that for blockchain. There's lots of other uses of blockchain. In fact, my use with the baseline and by the way, I'm not the founder, I'm just a co founder and I'm the chair of the TSC. And I should say, please get involved. We're having a big summit, go to baseline protocol.org, get on the slack. It's mostly slack, but we do have telegram and other things integrated.
03:29:43.554 - 03:30:05.438, Speaker F: But a lot of the work gets done on slack. There's a lot of people in there, very friendly crowd, very kind and welcoming and engaging maintainers, good engineers, good people to know. One of the teams just got a big deal with Coca Cola, Coke one North America, and because another team just happened to show up, they got in on the deal.
03:30:05.604 - 03:30:06.030, Speaker C: Nice.
03:30:06.100 - 03:30:24.546, Speaker F: So there's a lot of friendliness here, come on in. And we're planning a big summit for the twelveTH and 13 November that will result in some challenges that we were going to do a big Gitcoin hackathon for two weeks in December on with nicely funded bounties.
03:30:24.658 - 03:30:46.010, Speaker C: All right, very cool. I hope everybody caught that. I have another question for you coming from the Chat. So what do you think will be the first major use case of baseline and kind of along the same lines, what business types are you most excited to see? Experiment with baseline?
03:30:48.030 - 03:31:42.366, Speaker F: What business types? Well, I mean, things like purchasing and if you think about what do businesses do to with each other, right. And by the way, if you're not cross business, if you're not multiparty or multi entity, again, you don't really need blockchain. If you have a boss, if you have a single admin and a CEO maintaining things, you don't need blockchain for anything. If you need a shared system that different departments can say they have ownership of, use Git, there's a lot of things you can use. But when you get between companies, that's when blockchain becomes an important integration hub, potentially. And so in those environments, what do we do between companies? We buy stuff from each other and we move stuff between each other. And we have to manage agreements this before that.
03:31:42.366 - 03:32:04.218, Speaker F: What are the gives and gets? What are the quid pro quo? And we need to enshrine those in agreements, some of which not all, but some of which can be codified into business logic that can be executed by a computer. Not everything. Sometimes the rule is don't be stupid and you can't fix that with computers.
03:32:04.254 - 03:32:04.454, Speaker A: Right?
03:32:04.492 - 03:32:12.680, Speaker F: But sometimes you can say, well, this must happen, and I can check that on some kind of system before some other thing happens.
03:32:13.930 - 03:32:26.810, Speaker C: So I've got another question following up to that. So compared to like three years ago, what are the biggest things the enterprise blockchain space has learned? Some hard lessons, some good lessons.
03:32:28.270 - 03:32:38.282, Speaker F: Yeah, well, the biggest one is, and many of my colleagues are still there, I'm kind of preaching to the crowd. Now, don't think of blockchain as databases.
03:32:38.426 - 03:32:39.118, Speaker C: Yes.
03:32:39.284 - 03:33:07.994, Speaker F: If you're not prepared to take your data to Burning Man, don't put it on a blockchain. I know it sounds a little risky, but I think it's an apt analogy. Honestly. It's just a compartmentalization problem becomes really fraught. You can try. I mean, that's what half the good work that we did in the early days of know guy like Gary Singh, who's really good architect, engineer and coder. Gary Singh ought to get the touring awards sometimes, I think.
03:33:07.994 - 03:33:56.394, Speaker F: And he came up with this great architecture for a semblance of compartmentalization so that you could control who knows what, but still have it on a blockchain. But it makes for a complex just factorial nightmare of complexity. And complexity is the enemy of security. So I say if you're trying to maintain surveillance resistance, a well managed, potentially air gapped, traditional database, I mean a database pretty neat invention in 1970. That was the exciting thing. I grew up relational databases were kind of the thing I worked on in the early ninety s. And they're still good, so we should use them.
03:33:56.394 - 03:33:59.062, Speaker F: Not everything needs to be a blockchain.
03:33:59.206 - 03:34:02.010, Speaker C: Right? Totally.
03:34:03.310 - 03:34:08.230, Speaker F: Think of blockchain more as middleware than as a database is. I think what I'm saying.
03:34:08.320 - 03:34:23.700, Speaker C: Yeah, that's a good way to put it. Let's see one more question we got here. So for companies that are onboarding to baseline, are there any interoperability issues between companies you've seen thus far?
03:34:24.790 - 03:35:17.970, Speaker F: Well, I think this comes along at a good time because the API economy, which was the last big wave I was involved in and before know the object oriented wave. But I mean, API economy in particular, you have companies now like MuleSoft and others and Del Boomi that do translation schema management. So the real tricky thing is how do you verify the identicalness of a record that's on Mongo and SQL? Right? So you have to manage that. But there's quite a lot of maturity around that already, and so we can just jack into it and use that. So it's baby steps. It's always baby steps. So you could have imagined us doing this ten years ago, 15 years ago, but now it's easy enough that we can do it in production.
03:35:19.030 - 03:35:23.750, Speaker C: Cool. Go ahead. No, don't let me stop you.
03:35:23.900 - 03:35:25.320, Speaker F: No, that was it.
03:35:26.090 - 03:35:43.258, Speaker C: Okay, cool. Thank you so much for chatting with us. I really enjoyed this. If anybody wants to follow your writing or follow just like you on the Internet at large, what's the best way to do that?
03:35:43.424 - 03:36:23.414, Speaker F: At Jay Walpert on Twitter is one, and then the other is I'm the co vice chair of the main net working group at EEA with Tosh Deans and Hudson Jameson's in there. And really, we're in the age of convergence. This is a time where all of us need to come together. There's a lot of seriously cool developers that work in a lot of different kinds of companies. I remember in the mid 90s, late 90s, there's like 2500 Ibmrs that got behind Java just because we wanted to. Right. And nobody told us we could.
03:36:23.414 - 03:36:55.890, Speaker F: We want that for ethereum. We had to run Java One in 97 in Javit Center because there were 30,000 developers that showed up, if I remember the numbers right. Where's our 30,000? Developer blockchain conference. Right. Well, it's because we're not coming together yet, but we need to do that. Companies, you cannot like, companies you cannot like enterprises. And I might agree with you in a lot of cases, but at the end of the day, we're all just large groups of mostly humans.
03:36:55.890 - 03:37:03.620, Speaker F: And the developers in those companies can be a great ally and we need to reach out to them.
03:37:03.930 - 03:37:06.040, Speaker C: Awesome. Heard loud and clear.
03:37:08.170 - 03:37:15.814, Speaker F: Twitter is probably the easiest way to find me. And again, baselineproteocol.org. Come and join us, please.
03:37:15.932 - 03:37:29.920, Speaker C: Perfect. Thank you so much. This was great. I'm going to go ahead and switch over. Give us a little five minute break. We're going to listen to some Lo Fi radio and I'll be back on the chat soon. All right, thanks guys.
03:37:29.920 - 03:43:03.350, Speaker C: All right. Hello again, everyone. I'm back. Hope you enjoyed the radio. I'm here to introduce our next speaker, Alexe Akanov. I think I did that right. Please correct me, Alexe.
03:43:03.350 - 03:43:19.460, Speaker C: He's the founder and lead of the Turbogas Client. He leads research critical for the future of Ethereum. And today he's going to talk to us a little bit about how modular development will enable more people to be involved in the ETH One core development. So without further ado, please take it away.
03:43:22.150 - 03:43:36.200, Speaker E: Hello, Emily. Thank you for introduction. Yes. So let's get straight into this. So I prepared a little presentation. It will be shorter than the last time, but I need to just click a couple of buttons. 1 second.
03:43:36.200 - 03:43:54.460, Speaker E: Sorry, I am bit disoriented. Yeah, that's here. Okay. So what I'm going to be talking about here is that.
03:43:57.490 - 03:43:58.480, Speaker A: Let'S see.
03:44:01.490 - 03:44:02.670, Speaker E: Just 1 second.
03:44:02.820 - 03:44:03.326, Speaker A: Okay.
03:44:03.428 - 03:45:13.160, Speaker E: So I've started watching Ethereum core development somewhere maybe in beginning of 2018. And it took me a while to really understand what's going on. And I remember we had some kind of gathering or summit in Stanford in the beginning of 2019 and you kind of walking around the campus. Stanford campus was a great place. And somebody just asked me the question without any kind of without any sort of irony or sarcasm, so how does a particular change get into Ethereum? How does it work? And then I stopped and thought about it and of course the prepared answer would be like, oh, there's this process, there's AIPS, and you need to do this and that, you need to go through the stages. But actually I tried to describe what happens in reality, so not just as a process tells us, but what actually happens. And it sort of started led me onto this path of thinking about it a bit more.
03:45:13.160 - 03:46:23.546, Speaker E: And I kept coming back to the same realization that we are seriously bottlenecked on the core developers. And it was obvious to pretty much everybody that we have a very smaller number of people that are involved into the most critical part of this process. And there doesn't matter how many people will be throwing in the AIPS and kind of coming up with the interesting kind of ideas, but eventually all this deluge of stuff needs to get through a very small number of people. And the reason for that is because these people are what you would call the owners of the code. And I think it's understandable that if you own this code, not in the sense that it belongs to you, but in the sense that you are responsible and accountable, well, you feel that you're responsible for the quality. So if somebody gives you the change and you didn't verify it yourself, you are basically feeling that, well, that might break. I need at least to test it myself.
03:46:23.546 - 03:47:37.122, Speaker E: So I can tell you, for example, that in our project I do quite a lot of testing. You might not expect that, but I actually spend a lot of my time testing what other people have written because I feel that I have an ownership and I need to make sure that any serious problems don't just pass through. And I also understand that other people might not completely get all the nuances and I need to basically do all the testing. And so that applies to other code developers as well, so who take their job seriously and they do need to look at everything and they need to try to understand everything. And then sometimes, or oftentimes that causes some frustration from the people who propose things. I was trying to figure out how are we going to solve this? There must be a way to widen this bottleneck, to get more people in, but without sacrificing the quality. And back in 2019, so there was this idea about the working groups and things like this and yeah, wrote the blog post about it, but then after a while I realized it didn't quite work.
03:47:37.122 - 03:48:57.510, Speaker E: And the learnings from there was that the working groups that were created back then, they didn't own any code. So they could basically do research and they could prepare some changes, this and that, but because they didn't own the code, all these changes still had to go through the developers who did own the code. And they have to apply their own rigorous checking, testing process and things before changes go in. And then another realization is that most of the ethereum core developers is not implementing EIPS is something very different. Again, it's a lot of testing, it's a lot of people do fuzzing, it's a lot of figuring out the performance improvements, which most of them don't have nothing to do with theips so that's another thing I realized. So that view that the core developers are the people who implement theips is actually not adequate and there's a lot of other nuances scroll into 2020. And some of you might have seen, we had about three o core dev calls which I put on the slide.
03:48:57.510 - 03:49:45.740, Speaker E: Like when they happen, you can go back and watch them or listen to them. And so we spent these three calls where we decided we're not going to talk about EIPS, but we're going to be talking about other issues. And three main issues that I put highlight here is the burnout of the core developers, which I think is the consequence of these sort of responsibilities that I talked about in the first slide. And the pressure that they feel on themselves. And the pressure also combined with the pressure coming from other people who want something to be changed. But that kind of double pressure from one from within and one from the outside. And not everybody can deal with that.
03:49:45.740 - 03:50:36.602, Speaker E: So then we talked about diversity of different implementations and we also talked about the barrier to entry. So this is the secondary issues. We recognize that it's very difficult for anybody new to come in and just, let's say, produce a new implementation. So I kept thinking about those things. I mean, we did discover quite an interesting thought and then I went ahead and I tried to find the solution. So my solution that I already proposed on these calls, which some people were skeptical about, but some people were supportive, is essentially introducing modularity. And it's actually something that I and my team can do, can action rather than simply propose and try to push for.
03:50:36.602 - 03:51:50.206, Speaker E: So we can actually do that. Before we talk about modularity, let's just quickly go into the so on these calls. One of the theme was that we do need a diversity of implementations for this reason that if we have only so if we have basically the dominant implementation that there might be some bugs happening, and that bug essentially becomes of the rules of the protocol, which, from my point of view, sometimes it's okay. It's permissible, because that's already actually been done a couple of times. But if we look at the different implementations, disagreeing about the state, what people call consensus failures, this is presented as the scariest thing, one of the scariest thing that can happen on the ethereum network, consensus failure. So if we look, I mean, that is my own observation because I do look to the past, but I didn't take the statistical analysis of this. It does happen quite rarely, but it does happen both on main net on a testnet.
03:51:50.206 - 03:52:50.214, Speaker E: And so my observation is that most consensus failures are actually happen in one specific place of the ethereum implementation. And that's what I put here in the diagram. So what I call the interblock state. So it's not actually EVM itself most of the time. EVM isn't generating consensus failure, it's sort of a layer around it which deals with the things like caching, the things that retrieved from the database, it's the refund logic, it's the self destruction logic and things like this. So they are not strictly, I mean, depending what you think EVM is. But for example, if you look at EVMC, which is the very popular interface for written and C, for example, EVMC is if you look at my diagram you would see that EVMC is actually interface between that EVM block and that like a pink block of the interblock set.
03:52:50.214 - 03:53:27.554, Speaker E: So this is the EVMC which is this boundary. And then within this boundary, for example, we're going to be talking about EVM one which is one of the implementation. It implements this but it does not implement that pink thing which needs to be wrapped around it. And I pose it that this is where most of the consensus failures happen. Not in here, not in the peer to peer things, not in Americanization, not in the state reader or anything. It's actually mostly there and that would be important for my next slide. So what we already did so as I said, we started to take action on this.
03:53:27.554 - 03:54:10.218, Speaker E: I mean, we've been doing this for quite a long time. So what we actually did since May, we have started. So we have Turbogyth, of course and that is the derivative of Go ethereum and we've replaced quite a lot of things in Go ethereum but I think we still have a virtual machines pretty much unchanged which is inherited from Go ethereum. We have changed the interblock state quite a lot. So it's very different. So that's why I'm saying TG here. But we also already produced the alternative implementation of these piece where the most consensus problems happen and this is completely clean room implementation.
03:54:10.218 - 03:55:14.040, Speaker E: So there is no code lifting from anywhere. We did not reimplement the EVM. We just took the EVM one which has been written by other people and currently it works actually it works better than this one. So currently the written benchmark was that we could run this bit through all the Ethereum blocks in about 36 hours and we can run this through the same blocks in 21 and a half hours, which is pretty good improvement. But what I was going to say in this diagram is that if you look at those things they have completely different implementations of the part which generates most consensus failures. So for this kind of purpose these could be deemed as a different implementations and they would provide the diversity, let's say, if you run Turbogeth in these two different configuration. So, by the way, we did not finish the integration of the Soakworm thing into turbulet but I think we will finish this quite soon.
03:55:14.040 - 03:56:19.542, Speaker E: So next thing we did is that we started on the path of the modularization and taking out components and that was one of the first things we did, we did it for the purpose of performance. Because we noticed that the RPC requests, they do tend to be quite CPU intensive. I think it's mostly because of the transformations of data between formats and stuff like this. So what we did, we separated them into different processes and we have created the sort of very simple interface, very low level interface between the RPC daemon and the node. And that also has an interesting consequence that there could be multiple RPC demons, for example, implementing different variations of the JSON RPC standards. They could reside on the same machine or different machines. So currently it works with the TCP IP here, but actually we realized we can also make it work with a shared memory.
03:56:19.542 - 03:56:55.074, Speaker E: So if you can put them on the same machine and you need the really high performance, you can do that. It doesn't quite work yet, we just need to fix it. But it could work in principle. But this is where it all kind of started. So this is, by the way, for the communication between the protocols, our current standard. We just basically do it for all the components. If we want to separate them, we use gRPC, which is basically Http two based sort of framework, which is based on protobuff.
03:56:55.074 - 03:57:43.878, Speaker E: And you can generate bindings for pretty much any language in existence. So for example, this is like a protocol definition for our RPC demon. As you can see that what it does is basically it's like a database. You can open a transaction and then inside the transaction you can open a course or to a table and then you just do basically extract, go first, go to the first record, you can seek something in the database and you can retrieve it. And it doesn't have all the semantics of prefetch and everything like this, but the point is that the interface is very universal and simple. So you can implement pretty much any RPC request using that because what you are given is the remote database access. And so that basically was the first thing.
03:57:43.878 - 03:59:00.986, Speaker E: And then that gave us idea that we can actually then next thing, whenever we had this issues where we wanted to do some variations, we said we can split these things as a component. So the work currently ongoing on the Consensus Engine split and this is actually quite an important one. So currently, although people say that the Consensus Engine is pluggable, but actually it's not quite pluggable because it basically lives inside the code of the interlementation and anything that lives inside the code of implementation has to be verified, as I mentioned before, by the code owner. It has to be checked and everything. But if imagine instead you had the Consensus Engines implemented as a component with a well defined interface, then the people who developed the Consensus Engines could own that code. And so that could be already a start of separation of the code ownership. And so let's say if that was a working group that worked on the consensus engines, that working group not only just proposed something, but they could actually own the code, they can do their own releases as long as there is a compatibility through the interface.
03:59:00.986 - 04:00:07.620, Speaker E: And then it's one of the examples where the core developers essentially give up the responsibility and the power they need to come at the same time so that they don't own this code anymore, but they own their implementation of the interface. So the switch to different consensus engine can happen already with the participation of a different group. And so currently we're still figuring out the first version of the interface because in this interface we're trying to support three things already, the already the existing ones that we know about. We trying to support Et hash which is quite simple. So in Et hash you would have, let's say, these things that are marked green. The core wants to verify a certain header, it just sends it to the consensus and there's a bit of chatter going on between them, but in the end it gets the result whether it's verified or not. So the consensus can ask for additional information, for example, for some parent blocks, for parent headers and stuff like this.
04:00:07.620 - 04:00:56.770, Speaker E: But then another use case is when the core wants to do fork choice. And again the fork choice rule is the job of Consensus Engine. So it will ask out of these set of headers which one is the best. And the consensus engine, we say this, it will say this one and again it can ask for some additional information. For example, if these things are actually on different forks, it will ask for the predecessors of these headers to arrive at the common ancestor and then it will be able to figure out, for example, what's the highest difficulty or whatever consensus algorithm says. And then for the ceiling it's like it's for the miners or for validators, there's another request. So we're looking at the Et hash, we're looking at the click could be implemented with this protocol as well.
04:00:56.770 - 04:01:51.170, Speaker E: And we're also looking at aura, but Aura is the most complicated one, but we actually are probably going to try to implement anyway. So next practical example is the P to P sentry. This is also work in progress and it's a bit further ahead than Consensus engine. We actually have initial implementations running and one thing to notice here is that we already have two implementations of sentry. One which is we basically separated the code, the peer to peer code that existed in Tubergyast, which was inherited from Go Ethereum. And so literally like today I was testing it as a sentry and then we have another sentry written in Rust which is basically completely clean room implementation and they implement the very similar protocol. And so I'm not going to go into details, but this is the protocol we're currently working around.
04:01:51.170 - 04:02:52.326, Speaker E: So what we're trying to do is that we're trying to sort of make these two sentries are essentially pluggable. Or even another interesting thing you can do is that you can actually have multiple sentries connected to the same node. And with all of the other this actually opens up some more interesting flexible options because you can have two different centuries of different implementations connected just in case one of them has a bug, then you still actually connect it through another one. So here basically the idea is that the diversity could come in many forms. It doesn't have to be the entire reimplementation of the client, but it could be a reimplementation of a specific bit. Or you could actually have multiple implementation in one installation, like with an example of centuries. And the last bit that I'm going to show is the transaction pool split, which we haven't started yet, but we are going to start it very soon with some help from other people.
04:02:52.326 - 04:03:39.746, Speaker E: And some of you might have watched today's talk about account obstruction. So for example, the work on account obstruction requires a lot of changes inside the transaction pool logic and how I propose we're going to do it. So first of all, we're going to lift it out of the core component. And as you can see here, it could be done with the help of P to P sentry. So essentially a transaction pool becomes this kind of component suspended on the two interfaces. So on one hand, it's interfacing with the Peter to P sentry for the transaction traffic. And on the other hand, it does require some access to the state, but it can use the same interface that Rpcdemon is using for the state because it can basically query the database.
04:03:39.746 - 04:04:28.598, Speaker E: And what is interesting here is that I predict that there will be multiple different implementations of transaction pool. Some for the purposes of the counterbstruction experiments, some of the purposes of some sort of mev experiments and so forth. We also have a project to try to implement it in a different language. I think we're going to try to do it in Python, for example. How cool is that? You can implement transaction pool in Python connected to our sentry, which is written in Ruston to the Trooper guest node, which is written in Go. And it's all going to work. So yeah, as a conclusion to underscore the things I just said, that we need to think about implementation diversity in, I would say more diverse way.
04:04:28.598 - 04:05:17.240, Speaker E: So it could actually come in different forms. It could be not just basically just reproduce the whole thing in a different way, but maybe reproduce the parts in a different way and also kind of recombine them. Another thing is that working group can and should own the code and that would make them, because they are the owners, they will be responsible for code quality, and this is how we're going to widen the bottleneck. And another thought is that for everybody, innovation always happens elsewhere. We are not the smartest people on the planet, right? There will always be people who are smarter than us that will come and help us to do things better. Yeah, that's it.
04:05:21.930 - 04:05:38.350, Speaker C: Awesome. Thank you so much, Alexe, for sharing that with us. If you've got time to hang out for a little bit, I do have a couple questions that came in from the chat. The first question is, how big is the performance overhead for decoupling via JSON RPC?
04:05:39.170 - 04:06:37.918, Speaker E: We actually haven't measured that, to be honest. We did have a project to measure this some time ago, but it was long time ago and we only saw the overhead on the things that were really intense kind of chatter with the database. I think we were testing something like Get storage range at, and it was some considerable overhead. However, if we look at the current usage of RPC requests in Fiora, for example, they had a blog post recently that one of the most popular requests is East Call, which is essentially executing transactions onto the state, which is current state or something. And so this is where I expect a lot of benefits from taking that out of the node, because you will run EVM in the RPC daemon, not in a node. And it will just once in a while go into node to ask for a state. So, yeah, the answer is I don't know.
04:06:37.918 - 04:06:51.220, Speaker E: But as I said as well, we also introducing the mode where you can do shared memory with the node. And actually, I think in this case, the performance might not be even different.
04:06:52.170 - 04:07:08.090, Speaker C: Awesome. Got a couple more questions. Okay, next one. I'm going to mispronounce this. I know it. Is it possible to use C plus plus silkworm EVMone already? This particular user would love to add it to their Fuzzing efforts.
04:07:08.910 - 04:07:34.610, Speaker E: Yes. So the silkworm is open source, and it's got Apache two license, so you can search for it. I think there are also links from the Turbo Get repository. Definitely open source Apache license. It's just out of the oven. So it's not like we haven't did a lot of testing with it. But if they want to experiment, be my guest.
04:07:35.190 - 04:07:35.940, Speaker A: Cool.
04:07:36.870 - 04:07:51.510, Speaker C: All right, let me check on the chat. Got another one. Okay. Will this modularization stay in Turbogas because there is more freedom to design, or have other clients expressed an interest in adopting this approach?
04:07:55.310 - 04:08:57.610, Speaker E: Yes and no. Because I think at the moment, this is very new, kind of new direction, and some people are still skeptical about this. And my sort of way to go about it is that we will essentially just do it. We will not try to sort of try to basically do consensus on this before we're actually just going to prototype and we will see if this brings some benefits. And if it does, I'm sure other people will join and people will recognize that if somebody will try to come in and reimplement Troopergat, for example, or any other client, it will be much easier for them to start with the one component and then get to another component and another one. So the job that I was doing for the last almost three years would be much easier if I just had some small part to start with rather than just looking at this whole thing. I'm sure there will be a lot of cooperation.
04:08:58.110 - 04:09:23.250, Speaker C: Got it. All right, I'm going to hang on just for one half a minute to see if we have any more questions, because the chat does seem to be pretty active. Let me see. Okay, I think that's it. Thank you, Alexei, for joining us, giving us an awesome talk. I'm sure I'll see you around. Okay, bye bye.
04:09:23.250 - 04:10:11.406, Speaker C: All right, before I go ahead and introduce our next speaker, I want to take a couple of minutes and plug the Ecosystem Support Program for the Ethereum Foundation. Since I'm about to exit as an MC. The Ecosystem Support Program is what used to be known as the Grants Program. We changed our name to be a little bit more in line with the whole range of support that we give. So if you want to find out more about the types of support, or maybe if you just want to chat with our team, see what's going on, you can hit us up at ESP. Ethereum foundation. Yeah, and we also have a Twitter.
04:10:11.406 - 04:10:34.394, Speaker C: You can follow us at ESP. No, sorry. EF underscore ESP. Okay, cool. We've got a couple more minutes for our next speaker. Let me just check on him really quick. All right, Aditya, if you're ready, I'm going to go ahead and enter you.
04:10:34.394 - 04:11:05.990, Speaker C: Our next speaker is Aditya. He is a researcher at the Ethereum Foundation. He specializes in consensus research and spends a lot of his time focused on ETH Two. He's going to give us a talk today about secret shared validator infrastructure for ETH two. It's about achieving resilience against no failures and securing against key theft as an ETH two staker. So without further ado, I'm going to go over, hand it over to Aditya.
04:11:12.410 - 04:11:19.980, Speaker B: Hey, there. Thanks for the introduction, Emily. Let me share my screen real quick.
04:11:24.110 - 04:11:24.826, Speaker A: All right.
04:11:24.928 - 04:11:34.240, Speaker B: Hello, everyone. I guess we have two more minutes until we start, so yeah, we just wait till then.
04:11:45.270 - 04:11:49.026, Speaker C: Aditya, you are fully good to go if you just want to log into it.
04:11:49.048 - 04:11:49.906, Speaker A: Yeah, sure.
04:11:50.088 - 04:12:14.634, Speaker B: Hello, everyone. I'm Aditya. I work on consensus things at the EF, mostly focused on Casper and ETH Two. Today I'm going to be sharing a presentation on secret, shared, Validator infrastructure. So let's dive right into it. Staking on e two. Ethereum Two is a proof of stake network, which means that Validators put up a security deposit in order.
04:12:14.634 - 04:12:55.740, Speaker B: To enter as a validator. And the deposit is insurance so that they perform their duties correctly. And the duties consist of producing blocks and attestations at the time specified by the ETH Two protocol. And the main reason for the existence of the security deposit is having strong disincentives for validators doing bad things. So a couple of bad things exist. So inaction on the part of the validator leads to penalties, and malicious actions on the part of the validator lead to slashings. Both of these are reductions in the security deposit that the validator put up.
04:12:55.740 - 04:13:43.330, Speaker B: So more specifically, penalties result from offline behavior such as failing to produce a block or an attestation when required. And slashings result from misbehavior, which is provable malicious actions such as making two conflicting blocks or conflicting attestations in the same slot or violating some of the Casper FFG consensus rules. So I'm sure a lot of our listeners here are very interested in becoming ETH Two stakers. And there's a couple of risks involved with them. Some of them are very serious. The two most serious risks are key theft and node failure. So key theft is when the validator key that you're using on the E two network is stolen.
04:13:43.330 - 04:14:38.860, Speaker B: And there are a couple of keys that are involved in the process. The Staking key with which you sign blocks and messages, and the withdrawal key which you use to withdraw your security deposit at the end of your term. And the other major failure is node failure, where your E two node that is running your validator just goes offline or it produces some unexpected behavior. And of course, our goal with resilient validator infrastructure is preventing against both of these types of failures and theft. So how do we go about that exactly? Let's look at key theft first. So obviously key theft, as I said before, is when your validator key has been stolen. So maybe the machine that you're using to run your validator, it obviously contains your entire private key.
04:14:38.860 - 04:15:29.642, Speaker B: If your machine is compromised, then your key can be stolen. If you're not running your machine yourself, you're running it in the cloud. If your cloud architecture is compromised, that can happen easily if you're using a Staking provider that holds your keys. That can lead to situations like this. But there are standard ways to prevent against key theft. The most usual way to do this is using threshold signatures. What this means is you take the existing singular private key, which is your entire private key, and you break it into multiple smaller parts and you break it up such that signatures from any of the smaller parts can be combined into producing a signature for the complete key.
04:15:29.642 - 04:16:19.738, Speaker B: So, as you can see in the figures at the bottom, the key N is split into three parts n one, N two and n three, and signatures on the same data from n one, N two and n three. Can be combined to produce a signature as if it has come from the entire private key. N so if we split our keys in such a way and kind of provide it's sort of like Horcruxes in Harry Potter. Where you just put one each in different places and then you're secured against key theft. Unless someone goes ahead and attacks all of them at once. The second failure was node failure. And there's two major subparts in this.
04:16:19.738 - 04:17:46.046, Speaker B: The first one is crash faults, which is your node going offline. And this can be caused by a number of factors, most of which are out of our control, such as power outages, network outages, hardware failures or software crashes. And the usual way to prevent against this is using redundancy. So instead of running a single e two validator so for a single e two validator, instead of running just one instance of the e two client, we run multiple instances, multiple redundant instances, so that if some of them fail you still have the others as a backup. The other kind of fault is byzantine faults where your node isn't exactly going offline, but it is producing unexpected behavior which can cause slashable events. So this can be caused by software bugs in the ethereum client, it can be caused by network attacks where an attacker has taken control of the network around your node and the attacker is sending messages in such a way that you are influenced to produce bad messages. And we can prevent against this by having a consensus instance running among the multiple redundant e two validators that you are running, rather the multiple redundant e two nodes that you are running for the same validator.
04:17:46.046 - 04:18:49.162, Speaker B: This is so that no node is making unilateral decisions. Only if a majority of the nodes sign off on something, only then do all of the nodes produce a certain message or a block. So the most resilient ETH two staking architecture is going to be a combination of all of the things that we discussed. So namely threshold signatures, redundancy among nodes and consensus among these redundant node instances. So what does this mean exactly? It means that we are going to run multiple each two nodes for the same validator. Each of these redundant nodes is only going to have custody of a part of the private key, but not the entire private key for the each two validator. And the third one is that there's a consensus instance running among all of these redundant nodes and nodes only sign messages if the consensus running among these nodes instructs them to do so.
04:18:49.162 - 04:20:06.766, Speaker B: Essentially the last point here makes these redundant instances replicated so that all of them always have the same state and transition in the same way. So if this is the current e two architecture where we have a single validator V and the entire private key is existing at this validator instance, what I'm suggesting, or rather what the resilient architecture would be is something like this, where we split up the key in multiple places. So here we have split it up four ways and we put the parts of this key in redundant e two nodes. So v one through v four are the redundant nodes and we have a consensus instance running among these nodes so that none of them is unilaterally taking decisions, right? But in practice it's a bit more complicated than that. And in order to understand that, we should discuss a little bit more about the ETH two client architecture. So an e two node right now consists of two major parts. The first one is the beacon node and this is the part that takes care of peer to peer networking, chain tracking, folk choice management, et cetera.
04:20:06.766 - 04:20:39.450, Speaker B: This is the part that is directly exposed to the network. So the beacon node is responsible for gossiping messages, verifying that the messages it has received is valid and so on. And the beacon node can be run by users who are not stakers. So just like Geth today, you don't have to be a miner to run Geth just like that. You don't have to be a staker to run the beacon node, you can just run it in order to get information about the beacon chain. So that's the beacon node. The second part is the validator client.
04:20:39.450 - 04:21:32.762, Speaker B: This is a rather lighter piece of software. The main responsibilities of the validator client is handling the validator private keys and signing blocks and attestations when it is the correct time. And the validator client is only connected to the Staker's beacon node in order to get information about the network that is unsigned attestations and blocks that, the validator client will then sign and publish to the network. So the validator client is in no way connected to the ETH two network directly. So this is what the current architecture looks like. We have a beacon node and a validator client connected to it and the validator client has the entire key. So a resilient architecture would be splitting up the validator key into multiple pieces.
04:21:32.762 - 04:22:30.574, Speaker B: So key one through key four and also assigning them to redundant validator client instances. So again, v. One two through v. Four are the validator client instances. And all of these are put into a consensus group so that if there is a bug at, say, v three, that causes it to sign something, bad that basically won't go through because all of the other validator client instances have to come to consensus about what to sign before any of the clients actually sign anything. So this results in a replication of these redundant instances, which is a really important part. So this is better than the previous architecture, but this is still not the best we can achieve because obviously there is a single point of failure, which is the beacon node itself.
04:22:30.574 - 04:23:22.770, Speaker B: So the beacon node is how all of these validator clients are getting information about the E two network. They have no way of connecting to the E two network otherwise. So if the beacon node fails, the validator clients will not know what's going on on the E two network. They have no information about the chain, they don't know how to produce blocks without a beacon node. So even though this is better than the current architecture, it's not the best we can achieve. So in practice, the best architecture that gives us the highest resilience looks something like this. So we have multiple beacon nodes, we have multiple validator clients, and we have some secret shared validator middleware that is running in between interfacing the beacon nodes to the validator clients.
04:23:22.770 - 04:24:36.114, Speaker B: So this middleware consists of these SSV clients, these secret shared validator instances which are responsible for basically managing the timing, managing instantiating the consensus instances for each block or each slot and so on. And anything that goes from the secret shared validator middleware to the validator client should be put through consensus instance so that all of these validator clients are in the same replicated state. They sign off on exactly the same things. There's no situation where some of them sign on one fork and the others sign on another fork and then we have a deadlock that should not happen and hence this consensus filter basically. So there's a few more parts that are required to make this work. The most notable is this signature combination component. So each of these validator clients, since they only have a part of the key, they produce threshold signatures.
04:24:36.114 - 04:25:35.580, Speaker B: And all of these threshold signatures have to be combined in order to make something that is tangible to the ETH Two network. And this entire architecture is going to be a single ETH Two validator. And specifically for these numbers right here, where we have three out of four signature combination and we have four redundant instances, we can tolerate one failure, so one node can go completely offline and we can still have all the good properties about our network, about our validator. So there are a few design choices that were made on the way. The most notable is the consensus algorithm itself. The requirements for this were that it has optimal resilience, that is, it tolerates the maximum number of faults that are possible, which is one third. We want this because in order to achieve the same fault tolerance, we want to run the minimum number of nodes possible and reduce our staking costs that way.
04:25:35.580 - 04:26:40.420, Speaker B: The second requirement is fast leader change. So the consensus algorithms have a leader which proposes what the consensus value should be. And if the leader fails, we have to change the leader in order to achieve consensus. And we want to do this in a really fast, daily responsive way because the ETH two network expects validators to produce messages at a certain time. And obviously we'll run the consensus algorithm at some time before the expected time to produce blocks or Attestations. But if the leader fails, we want to be able to conduct the failover and have the new leader propose a block or an Attestation as soon as possible so that we can still produce a block or an Attestation before our expected time. And the almost perfect candidate for this is is TANBUL BFT which has all the good properties that we want and you can find more information about that on this link in this paper.
04:26:40.420 - 04:27:27.262, Speaker B: So, some additional information. There is a proof of concept made by Dankrad and Alon and you can find the proof of concept on this GitHub repo. We had a validator running using this POC on Medasha testnet for a while. I don't think it's active right now, but you can go ahead and explore that if you're interested. Some more additional information. The key people involved in the secret shared validator effort are from the Ethereum Foundation Tankrad and myself, from Consensus, Mara and Colin and from Blocks IO Alon who's been really helpful with the proof of concept. Thank you.
04:27:27.262 - 04:27:37.650, Speaker B: That was all from my side and my contact information is on your screen. Hit me up if you're interested in secret shared validators, have any questions or want to get involved.
04:27:39.430 - 04:28:02.860, Speaker C: All right, thank you so much. Ditya, if you don't mind sticking around, we do have a couple of questions that came up in the chat and also a reminder to everyone. Once again, if you have questions for Aditya or for any of our speakers, go to Live Ethonline.org for starters. Great talk. I really enjoyed the Harry Potter reference. There's like not enough of those in the Ethereum world.
04:28:02.860 - 04:28:16.906, Speaker C: But the first question we have know this sounds like a really good step forward for validation security. Are there any trade offs to it of additional bandwidth? More complexity for node operators?
04:28:17.098 - 04:29:13.780, Speaker B: Right? So I think the most important one is the complexity in operating this node. The E two clients are already a complex piece of software and even technically competent users have issues running E Two clients. So running this kind of a complicated setup across multiple machines is going to be a challenge. And unless there is tooling to make this easier, I definitely don't recommend normal users to do this. But this is a giant leap forward in terms of resilience at the validator level. I hope that the major staking providers or exchanges who are doing staking use this kind of an architecture so that they don't experience some of the issues that we have seen in the test nets. But this is definitely a complex piece of engineering and it'll take time to refine this.
04:29:14.310 - 04:29:26.282, Speaker C: Cool. Next question. So what are other improvements that are left before this can be used at scale for mainnet validators? Like besides launching Mainet?
04:29:26.446 - 04:29:46.650, Speaker B: Right, so I think all of the research problems are taken care of. Basically what I described redundancy and replication those are the two main pillars on which this entire architecture is built. The research is all done. It's basically time to implement this for a production ready setup.
04:29:47.310 - 04:29:59.520, Speaker C: So I guess my final question really is how can people get involved? I know you sort of touched that on your slides, but if you want to reiterate, that would be great. And more importantly, what are some of the things that you would like help with?
04:29:59.970 - 04:30:49.230, Speaker B: Right, so as I said before, there's a few key people involved in this effort. The contact information is on the screen right now. You can reach out to Dankrad or myself through our emails, and we'll certainly be able to figure out the best way for people to help us, depending on their skills. Right now, we are looking for people to maybe get involved in implementing a production ready software in order to run this. That would be a great help for us. Staking providers are encouraged to maybe contribute resources or time into making this because this will definitely be helpful for them. But obviously anyone else who's interested in making this work should hit us up.
04:30:49.380 - 04:31:05.590, Speaker C: Yeah, that's good to hear. Actually, we have another question that came up in the chat, if you're down. Okay. The question is, do you expect this to be used more so by individuals or entities for heightened security, or do you think it'll be more used to enable trustless pools?
04:31:06.250 - 04:31:58.086, Speaker B: So solely, this is not enough for trustless pools. There is an entire set of other things that are required for trustless pools. This addresses some of the issues in implementing trustless pools, but those issues are, I would say, not fundamental. That said, I do expect some solutions to appear using this. I know Alon from Blocks IO is working to convert this into a trustless staking setup. As far as individual validators go, I think the main trade off is that in order to achieve the fault tolerance associated with one node failure, now instead of running one node, you have to run four nodes. So four is the minimum number that you have to run.
04:31:58.086 - 04:32:22.430, Speaker B: Just running two is not going to get any fault tolerance. So because of the heightened costs of this staking setup, I don't think single validators or individuals are really the target users here. Maybe whales who are staking on this or staking providers who have huge stakes and a lot to lose. It makes sense for them to bear these costs.
04:32:23.890 - 04:32:35.060, Speaker C: All right, makes sense. It looks like that's it for questions. Thank you so much for joining us. Great talk. It was good seeing you. I feel like I've really missed seeing people in person these days.
04:32:35.510 - 04:32:36.370, Speaker B: Yep, for sure.
04:32:36.440 - 04:32:36.674, Speaker A: Yeah.
04:32:36.712 - 04:32:37.730, Speaker C: Thanks for joining.
04:32:38.230 - 04:32:40.770, Speaker B: All right, thanks, everyone. Bye bye.
04:32:41.350 - 04:32:55.590, Speaker C: All right, that was the last speaker that I was going to introduce. I really enjoyed my time being MC. I'm going to go ahead and introduce our next MC, Josh. He's going to take over. Josh, welcome to the chat.
04:32:56.090 - 04:33:07.626, Speaker A: Hey, Emily, thank you so much. Thank you, Emily, for being an amazing MC for these several hours. We really appreciate the help. You did a great job. See you at a conference in person someday soon.
04:33:07.728 - 04:33:09.238, Speaker C: I know, right? See you soon.
04:33:09.344 - 04:43:18.814, Speaker A: Bye bye. All right, everyone, we've got a few minutes before the next talk, so we are going to take a short break. But just so you know, in about ten minutes we're going to have Superfiz come up talk about a nontechnical introduction to staking on E Two. So if you are interested in staking, but you're not super in the weeds on the specification or how this all works, this talk is going to be your guide to the core things you need to know. So hang out, enjoy the lo fi radio in the online space, and we'll be back in about ten minutes. All right, everyone. Hello.
04:43:18.814 - 04:43:50.250, Speaker A: Welcome back to ETH. Online, our future Ethereum summit. Very pleased to introduce the next speaker, superfizz, who's a major part of the staking community that's growing around Ethereum Two, is going to give a talk about intro to E Two and Staking for Beginners. So if you're interested in staking, you've heard about that, you don't know a lot about exactly what it means or what to do about it. This is the talk for you, Superfiz. I'm going to hand it over to you now. Thanks, Josh.
04:43:50.250 - 04:44:41.290, Speaker A: Hey there. I'm Superfizz or Hank in the real world, and today I'm going to be providing an overview of Ethereum Two and Staking for Beginners. My talk is directed at normal people who are just curious about Ethereum Two and Staking. As Superfiz, I help to organize the ETH Staker community, and you can find us@reddit.com r ethstaker or in our discord at Invite GG Eatstaker. Our goal is to promote solo staking for the wider Ethereum community and to support all forms of staking like pooled staking, staking as a service, and even custodial staking. We're average users who have a deep interest in Ethereum and we think that one of the best ways we can contribute to the success of Ethereum is by staking Ether and earning a reward for doing that.
04:44:41.290 - 04:45:27.316, Speaker A: And that's really what we do. We reach out to the community, we welcome people in and we help them train to be ready to stake. Every talk needs a takeaway, and the takeaway from my talk ought to be that you're welcome in the ETH Staker community, whether that's on Reddit, on our discord, Twitter or wherever we turn. Up next, we actively welcome people of all ability levels from any background anywhere in the world. I'd like to give you a heads up that I am offering a POAP token to people who listen to this talk live. I'll give the instructions for claiming that POAP at the end of the talk, you have to wait. So the first question is what is Ethereum Two? I believe some people think that Ethereum Two is some kind of clone of ethereum and that no one wants to hold a clone of ethereum.
04:45:27.316 - 04:46:00.404, Speaker A: They want the real thing. Ethereum Two is ethereum. Ethereum One plus Ethereum Two make up the whole ethereum ecosystem. Ethereum One and Ethereum Two represent different layers of the ethereum stack and they're both the same platform. The transition from Ethereum One to Ethereum Two will likely be completely transparent to end users, that is, you won't have to change your workflow just experience the benefit from the changes to make that really, really clear. The changes are under the hood, not in the user interface. There's no such thing as ETH one.
04:46:00.404 - 04:46:21.370, Speaker A: Ether and beacon chain. Ether. Beacon chain ether is ether. For the rest of the talk, I'll refer to Ethereum One and Ethereum Two. Chains. But remember that all ether is ether. So what are the changes that make Ethereum Two so different from Ethereum One? There are two changes that will help anyone have a better understanding of how Ethereum Two is different and better than Ethereum One.
04:46:21.370 - 04:47:13.420, Speaker A: The first change is the switch from proof of work to proof of stake. When ethereum launched in 2015 it was based on proof of work mining which means you take either a processor or a graphics card or later what's called an ASIC miner and you solve a complex math problem over and over. When you're the first person to show a result that's sufficiently complex you're rewarded two ether along with whatever transaction fees were collected in that block. Solving these complex problems seals blocks that contain transactions and that block is added to the blockchain. That's essentially how the network is secured. The problem with proof of work is that it's very resource intensive. It's basically an arms race to have the most powerful miner and these miners use a lot of electricity and as technology improves they quickly become obsolete and need to be replaced with more efficient miners.
04:47:13.420 - 04:48:01.012, Speaker A: Ethereum Two solves that problem with a system called proof of stake which allows users to post a bond or a stake of 32 ether. That is a promise to the network that they will behave honestly or face penalties against their stake. Your staked ether allows you to perform the duties of a validator like proposing blocks attesting to the validity of new blocks and identifying dishonest validators on the network. This stake is basically a credibility deposit that says I'm telling the truth. So if you do a great job, you'll get a small reward and if you're dishonest, you'll get a penalty. If you just decide to give up and go offline without exiting you can lose up to half of your staked ether and be ejected from the pool of validators. But you really shouldn't worry.
04:48:01.012 - 04:48:35.428, Speaker A: In order to have a severe penalty you would have to do something on purpose. We'll talk about an exception to that later. But don't worry, you're not going to accidentally lose your either. So the first big difference between Ethereum One and Ethereum Two is the switch from proof of work to proof of stake. The second big change in Ethereum Two is the creation of 64 Ethereum chains called shards. One of the reasons we need to improve Ethereum is that the network has become very popular and is basically out of space. To address that problem, we're creating 64 Ethereum chains and linking them together.
04:48:35.428 - 04:49:11.008, Speaker A: By doing that, we have substantial capacity growth. When these chains are deployed, probably sometime after 2021, they'll be linked together by a special chain called the Beacon Chain that coordinates progress of the 64 shards. This beacon chain is really what we're talking about today. It's the backbone for the improved Ethereum network. These two features the switch to proof of stake and the switch to sharding are massive improvements from Ethereum One to Ethereum Two. So that's the big change. The next question is I want to be a staker, but 32 Ether is a lot of money.
04:49:11.008 - 04:49:51.340, Speaker A: It's more than I can afford. Well, the reason you have to put up a bond of 32 Ether is that you're accepting a lot of responsibility. And the only way the network can guarantee that you're taking that responsibility seriously is to have something to hold over your head to say, if you want to earn money, you have to do the job correctly. If you don't do the job correctly, we're going to take some of your money away. It turns out this incentive really encourages people to do the right thing. A lot of thought was given to the correct number of Ether for a stake. If the bond is too low, there could be millions of validators on the Ethereum Two network causing congestion that would make it difficult for the network to communicate.
04:49:51.340 - 04:50:21.512, Speaker A: But with a bond that's too high, it becomes prohibitive for solo stakers to participate. Right now, as the beacon chain is preparing to launch, one validator or 32 Ether costs about $10,000. On one hand, that's a lot of money for anyone. On the other hand, it's still reasonable for someone who is deeply committed to participating in Ethereum, too. With the stake of 32 Ether, the network can support every ether being staked and still run, stably and securely. So let's say you don't have 32 Ether. That's okay.
04:50:21.512 - 04:50:47.296, Speaker A: I feel like you shouldn't even consider staking unless you can stake happily with half of your Ether. If you have 32 Ether, as your friend, I'm going to encourage you to just hold it. Don't stake with it. Just hold those 32 ether. If you have 64 Ether, you might be in a good position to run one stake with 32 Ether. If you don't have 32 Ether, it's a good idea to just wait. Wait until the network has been through a trial by fire and just continue to accumulate or do whatever is right.
04:50:47.296 - 04:51:38.492, Speaker A: For your financial situation. Right now, staking on ethereum is in its infancy. For the last several months, we've been running testnets and client implementers have been preparing to onboard stakers onto the network and things look great. But for the first five months of Phase Zero, the bugs and kinks will be getting worked out and your staking node will likely require a lot of attention. If you don't have the time or the commitment to monitor your staking client every day, it's best if you stake on a testnet and wait until the main network is stable. A checkpoint is planned for five months after the beacon chain launch and this would be a great time for someone who is interested in running a Validator without having to monitor it daily. If you don't have 32 ether, or if you don't have enough either that it's a good idea for you to stake.
04:51:38.492 - 04:52:16.904, Speaker A: There will be alternatives available in the future. I don't personally endorse any of these alternatives and that's because as a community, estaker believes that solo staking is the most secure way to stake. But a few months after launch you're going to see things like staking pools where you can hold less than 32 ether and pool with other people to stake and get a share of their reward. That looks really appealing, and I think it's a good idea. I'm actually going to try it myself a little bit. But it comes with the added risk of dealing with third parties who may be using vulnerable contracts or centralized servers. Or maybe they're giving you a reward as a token.
04:52:16.904 - 04:53:04.604, Speaker A: That's something other than ether. In the worst case scenario, someone could take your ether and convince you that they're staking it on your behalf, but they've taken it and disappeared. If they're really crafty, you may not even know this has actually happened until transactions are enabled over a year later. These tradeoffs mean that you could end up losing value that you wouldn't have lost if you either did solo staking or if you just held your ether without staking. I don't want to tell anyone not to use staking pools or other services, but I do want to tell you that you should be very careful when you get involved with these services. In the longer term, we're going to see exchanges who will be involved with staking ether. You'll be able to make deposits and earn a small return off of those that comes with a trade off.
04:53:04.604 - 04:53:31.472, Speaker A: You're going to earn a small return from depositing your ether with them and they're going to stake it. From a decentralization perspective, this is a concern for the network. We don't know how those exchanges are staking. We don't know what systems they're using and they may be centralizing large numbers of Validators. It's actually okay because if they mishandle your stake and lose it, the network will recover. The consequence will be that you as an individual could lose. Your stake.
04:53:31.472 - 04:54:33.400, Speaker A: Even with these trusted exchanges, I would say it's better to solo stake than to trust them with your funds. So let's say that you're interested in Staking but you're not really sure if it's right for you. I would absolutely encourage anyone who's interested in Staking to spend at least a month staking on a testnet so that you can see what it's like to keep up an e Staking server and so you can see what the reward system looks like even though those rewards won't be realistic on the testnet. To begin the process of staking on the Beacon Chain of Ethereum Two, you're going to send 32 Ether to a contract on the Ethereum One network. In exchange, and in exchange for sending 32 Ether, you're going to get a digital receipt that you'll claim using a validator client that tells the Beacon Chain network, hi, I'm a validator and I'm ready to do work as soon as I'm inducted on the Beacon Chain. From that point you'll wait anywhere from a few hours to a few weeks to be inducted as a Validator. Depending on the length of the validator queue.
04:54:33.400 - 04:55:07.200, Speaker A: As soon as you're inducted as a Validator, your staking client will begin doing the job of validating for you. Every six minutes you'll get a payout. You'll get a small reward for doing your job. If you go offline, you'll lose about as much as you would have gained if you had been online. You do not have to have perfect uptime to be an Ethereum staker. But the better your uptime, the higher your potential profit will be. Real rewards will be measured in several decimal places of ether every six minutes and you can see these in real time on your validator client logs.
04:55:07.200 - 04:55:45.810, Speaker A: You're probably wondering what rewards look like on Ethereum Two. Will it be worth the effort? I have to tell you that the purpose of Ethereum Two is not to make validators rich. The rewards on Ethereum Two are designed to be the minimum reward that will encourage people to continue running validator clients. This reward is variable based on the number of ether that are deposited. For example, if 1 million ether are deposited, the annual rate of reward will be 18%. And so what's going to happen then is that people are going to say, oh wow, it's really valuable to stake on Ethereum. So they'll add deposits and it's going to push the deposits up to 3 million.
04:55:45.810 - 04:56:49.556, Speaker A: Then, because it's a sliding scale, the rewards will be 10% and people will continue to be excited and send deposits in. The magic happens when there are between 10 million and 30 million Ether staked and the rewards will be between three and 5%. People will seriously question whether it's worth having their Ether locked up in phase zero when they're only earning 3% interest. The good news is that when transactions are enabled people will be able to add or remove staking deposits more easily and the value of Staking will easily find its own equilibrium. If you're staking in Phase Zero, you're not going to get that deposit back until after Ethereum One is integrated as a shard onto Ethereum Two, and that's probably going to be more than a year down the road. This isn't a deterrent to staking in Phase Zero, but it's a caution that you should carefully consider how you will fare with your ether locked up in the network for an unspecified period of time, most commonly cited as about two years. So you've heard enough.
04:56:49.556 - 04:57:33.516, Speaker A: You know that Ethereum Two is a major upgrade to the Ethereum One network. You know that the validator deposits are 32 ether, and you know that if you send a deposit to become a validator during the first phase of Ethereum Two, those funds will be locked up for around two years and you're still committed to doing it. What's next? Let's talk about hardware. There are a wide range of hardware configurations that will work and there are also some best practices for choosing hardware. I'm going to describe a few common hardware scenarios for home stakers. The most common scenario is to run an ethereum Staking node on some kind of desktop computer at home. Regardless of whether you build it yourself or buy a prebuilt desktop.
04:57:33.516 - 04:58:17.312, Speaker A: You want a processor that has a pass mark score above 6000 on CPU benchmark net. These processors are powerful enough to continue seeking the beacon chain when the network is under stress and they'll continue to support the network in later phases. You'll want a minimum of 16 gigs of Ram, though. I like 32 gigs of Ram, especially if you plan to run any kind of node monitoring software on this system and you'll need at least 1 SSD storage. It's important that you use some form of SSD storage and not an old rotational or platter drive for Staking. Rotational drives are only useful for long term storage now and they just can't keep up with the data requirements of Staking in 2020 prices. You can buy this kind of desktop setup for about $600.
04:58:17.312 - 04:59:01.080, Speaker A: But remember that in the computer hardware market you pretty much get what you pay for. So how about intel nux or mini PCs? I like these and they're a fun little thing that you can put in an out of the way space to do the job. You can buy them pre configured so that you know the hardware is compatible. I think this is a good option. Just make sure you buy a device that is powerful enough to handle the load and you should be set to go. To be honest, for the money value and space requirements, I would seriously consider staking on an 8th generation or higher Intel NUC that was outfitted with 16 or 32 gigs of Ram and a one or two terabyte SSD drive. That brings me to laptops.
04:59:01.080 - 04:59:50.140, Speaker A: There's nothing inherently wrong with staking on a laptop, and I'm sure lots of people will be just fine doing it. But if you're buying a machine just for Staking, I probably wouldn't choose a laptop. You're paying a premium price for a screen you don't need, and a small form factor that reduces heat dissipation. On top of that, laptops simply aren't designed to run twenty four seven. A lot of people are fascinated by trying to stake with Raspberry PiS, and I'm going to suggest that's just not a great idea on a perfectly running network. In phase zero, the first phase of Ethereum Two, a Raspberry Pi Four will do fine. But if anything goes wrong with the network, if there's any difficulty with finalization, or if there's any reason for the network to be congested with traffic, a Raspberry Pi will quickly become overwhelmed and not be able to do the job of a validator.
04:59:50.140 - 05:00:36.392, Speaker A: For comparison, the Raspberry Pi Four has an estimated passmark score around 900, and I'm suggesting a CPU that has a pass mark score above 6001. Other option to consider is Staking on used server hardware. You could buy used servers online for a pretty fair price, and they have a lot of redundancy built in, which increases your odds of being online continuously. And they were designed to run 24/7 forever. That makes them a great choice for Staking, and you can get everything you need for less than $1,000. This is, in fact, the route I chose when I began getting involved with Staking, but it's one that I'd rethink now because I have a large, noisy machine to keep somewhere in my home. I also suggest that you run hardware locally.
05:00:36.392 - 05:01:22.036, Speaker A: If you choose to run a cloud service provider, things are going to work really well. You can probably expect to spend about 20 or $30 a month for that cloud server. The risk is that it's a loss of decentralization. There are likely to be a lot of people running their Ethereum Two nodes in cloud services, and I know they have fantastic uptime. But if by some chance those cloud services go offline, your node will be offline and it will be out of your control. If a large portion of the network goes offline at one time, like AWS goes down and 35% of Ethereum Two nodes are on AWS, the network will go into an inactivity leak, which means that all offline validators will lose Ether until finality is reached again. And online validators will maintain a constant balance.
05:01:22.036 - 05:02:15.790, Speaker A: If they do the validating job perfectly, finality is reached again when more than 66% of the validators are back online. And validating, while one in strong incentive is for stakers to remain online, it's even more important for them to remain online when many others are offline. By this time, you've thought about your hardware and you're ready to start building a stable software stack. A survey of the Ethereum community conducted by my friend Nolan found that 75% of stakers plan to run Linux, 15% plan to run Windows and 10% are virtualizing or running OSX. My strong preference is to stake using Ubuntu Linux 24, the long term support version of Ubuntu. I've had great success with it, and I feel like that platform is more or less accepted as a community standard. 75% of stakers will run a local server while 25% will stake on a cloud based service.
05:02:15.790 - 05:02:55.620, Speaker A: Beyond that, you're going to need data from an Ethereum One client, like Guess and an Ethereum Two client. These are the pieces of software that have been in development for the past two years, and you're most likely to recognize the names Prism, Lighthouse, Teku and Nimbus. But first, let's talk about the need for Ethereum One data. Data from the Ethereum One chain is required for running a proof of stake validator. The beacon chain needs to know about deposits that are submitted for inclusion into the proof of stake network. And the only reliable way to get this is from the Ethereum One chain. The first way to do this is to connect to a service like Infura.
05:02:55.620 - 05:03:45.080, Speaker A: These are services that provide a data feed about the Ethereum One chain so that rather than running your own service, you can connect to them and get the data you need. I don't recommend using a feed because if it goes down it will stop your entire staking operation and you won't be able to do anything about it. If that provider goes down and you decide that you want to switch to hosting your own node, it will take you several days to sync the chain and all of that time you'll be slowly leaking ether from your stake. My suggestion is to run an Ethereum One service like Geth. Some people feel that this is an overkill because you'll probably need 500 gigs of space for Geth in the long term and 48 gigs of memory just to keep Geth running. Happily. But the benefit is that it contributes to the decentralization of the network and it means that if those service providers go down, you'll still be validating.
05:03:45.080 - 05:04:26.772, Speaker A: One of the key concepts of Ethereum Two staking is to be robust and resilient against single points of failure like relying on third party services. One of the things that I encourage people to do when it comes to staking is not to look like anyone else. If everyone runs the same setup, any flaw in that setup is going to ripple through the network and affect everyone, potentially leading to inactivity leaks where greater than one third of the nodes are offline. You don't want to be in that group of people that goes offline. And the way to prevent that is to keep your system updated and secure and to make sure your Internet connection is stable. So let's say you're using a local ISP. That could be great.
05:04:26.772 - 05:05:17.380, Speaker A: You might go offline once a month for an hour, but it's much better to go offline once a month for an hour than to be offline at the same time that many of the validators on the network are offline. If half of the validators go offline, inactivity leakage will kick in. But if you're the only one offline, it'll be a tiny penalty. The next big factor is choosing a client. There are four clients that will likely be launch ready based on what we know right now, and those are Lighthouse, Prism, TechU, and Nimbus. Each of these validator clients are written in different programming languages, and they've all passed tests that show that they know how to interact correctly with the Ethereum Two protocol. Ethereum Two Network communications really are a protocol now because there's a standard way of communicating between different clients.
05:05:17.380 - 05:06:09.152, Speaker A: One of the strengths of Ethereum Two design is this protocol style development, where different clients learn to speak the same language to reduce the chances of miscommunication. If there were only one client, it could communicate any way it liked. There wouldn't be any need for protocol style communication or standardization. It would also be weaker and open to network attacks. When you choose one of these clients, the thing you want to do is choose one that's not used by everyone else. I encourage people to use the second or third most popular client because choosing the most popular client brings the biggest risk of inactivity leaking. If something goes wrong with that client, the validators using that client will leak below 16 ether, and those validators will be pushed off the network unless they can get a software fix or they can switch to a different validator client.
05:06:09.152 - 05:06:45.170, Speaker A: And that really does work. After you've selected, your client set up and you set up your system the way you want, it's time to head over to the Ethereum Two Launchpad for Mainnet. You'll find this at Launchpad Ethereum.org, and for testnets, you'll put the testnet name in front of that address like Medasha Launchpad Ethereum.org. The best advice I can give to anyone is to spend a full month staking on the testnet before even considering staking 32 ether on the mainnet. So it really makes sense to start at Medasha Launchpad Ethereum.org. When you get there, you'll go through a bunch of really valuable information.
05:06:45.170 - 05:07:35.280, Speaker A: It's worth your time to spend plenty of time checking out every bit of information there. After you complete the information portion, you'll have the opportunity to tell the launchpad how many validators you want to run, and you'll download a deposit script that will help you generate a 24 word seed phrase and corresponding keys for all of your validators. Next, you'll use MetaMask to upload a list of your validator keys and submit 32 ether deposits to the Ethereum One network. While I find it to be a pretty easy system, I would really push everyone to play with it for a long time on testnet using girly ether until they're completely comfortable with a flow of events before sending real ether. And that's it. From that point, you'll import your Validator keys into your client and make sure everything is synced up. Then it's just a matter of waiting until your Validator is inducted.
05:07:35.280 - 05:08:33.620, Speaker A: The easiest way to check the status of your Validator is to enter the Ethereum One address that you sent the deposit from into the search field at Beaconchain Beacon Cha In, and it will tell you the public keys for any Validators that you submitted deposits for. As I wrap things up, there are a few things I'd like to share with everyone about best practice. The first best practice is don't host your Validator in two places. This means don't run your Validator on two computers at your house or at one computer at your house and one on the cloud at the same time. Because running those two Validators at the same time will cause you to send conflicting data to the network and it will produce a slashable offense. Basically, what this means is that your Validator has a chance to be randomly selected to provide an attestation every 12 seconds. If you somehow manage to have two machines with the same Validator keys, it's inevitable that they will submit different data at some point, and this is considered a malicious offense by the network.
05:08:33.620 - 05:09:08.610, Speaker A: The easy way to prevent this is to only run one staking node only keep your keys on one computer. The second thing I want everyone to remember is that you should not stake too much. As excited as I am about Ethereum Two, I also recognize that there's risk involved. There are risks that may not be obvious at first. One of those risks is that the price of ether could go very high while your ether is locked up for a long time and you can't sell them because they're locked up. If you think there's any chance that you're going to need the ether that you're staking, don't stake them. Just hold on to them.
05:09:08.610 - 05:09:40.628, Speaker A: The third thing is your seed phrase. When you go through the launch pad process as a staker, you're going to get a 24 word seed phrase. That seed phrase will be used in the future to generate your withdrawal key. If you lose those 24 words, you won't be able to withdraw your funds. You'll be able to continue Validating with your Validator keys, but you'll never be able to withdraw the funds and they will simply be lost to the network. Don't lose your 24 word seed phrase. Finally, be careful where you send your ether.
05:09:40.628 - 05:10:24.664, Speaker A: As Ethereum Two emerges, there will be a lot of exciting places to send your ether to gain rewards. There will be services and exchanges promising to stake for you. There will be people bragging about how good they are at running a stake in node. And there will be people who post fake addresses for you to send ether to for staking you have to be extremely vigilant with your ether and keep in mind that this transition is a prime opportunity for dishonest people to pick up some low hanging fruit. It's been really great sharing Ethereum Two with you. I know it's going to be really exciting, and I can tell you that the Eastaker community has done a lot of preparation to help everyone feel comfortable getting involved with Staking. You can find us@reddit.com
05:10:24.664 - 05:10:58.230, Speaker A: r ethstaker or on our Discord insta GG ethstaker. We also have a couple of programs that you might be interested in. One of them is called the Ethereum Studymaster. It's a program where you can read articles every week for ten weeks and take a quiz on those articles. After ten weeks, you can earn a proof of attendance protocol, a POAP token for demonstrating your knowledge of ethereum too. Now that's just a neat way to show that you have invested time and are knowledgeable about ethereum. It kind of gives you a little incentive to learn more about how things work.
05:10:58.230 - 05:11:46.480, Speaker A: You can learn more about the Study Master program@etheriftymaster.com. Estaker has also developed a really cool collaboration style program where you can signal your interest by completing a form, then get added to a small group with four other people who you can develop a long term staking relationship with in a private Discord chat. It's just a way to connect with a small group of people in the community rather than being overwhelmed with so many people at one time. In addition to that, we have a great group of Estaker educators on our Discord Channel. Those are a group of about ten people who are ready to help you anytime you come into our general room on the Estake or Discord and ask a question. So we want you to feel comfortable. We know that Staking isn't right for everyone.
05:11:46.480 - 05:12:27.680, Speaker A: I'll be the first to admit that you should think long and hard before you commit to snaking. But if and when you decide to do that, or if you even have questions about it, we definitely want to be here to support you, and we will always do our best to lead you in the right direction. Thanks for your time, and good luck with Staking. I'd like to thank Michael Geesen from Unvetica for providing support for this presentation for 30 minutes after this live call. If you check out Unvetica.com Ethonline, you can claim a POAP token that's a proof of participation token by entering the Secret Word ETH staker in Lowercase. You can also find all of the resources from this talk on that site.
05:12:27.680 - 05:13:08.696, Speaker A: Thank you very much and take care. Wow. Thanks, Hank. That was a great talk. We do have one question from the chat if you just have a minute to get into it. So Alex Wyckoff asks, I'd love to hear your suggestions on routers off the shelf brands tend to bog down with a lot of sustained peering connections are y'all using Unified Dream Machines or something else? So I love that you said the Unified Dream Machine, the Ubiquity line, is a favorite among our community. And so it's not that I want to promote them, but I'm using a Ubiquiti er four right now and it's working well.
05:13:08.696 - 05:13:51.236, Speaker A: The dream machine line is very popular. The upper tier of consumer hardware that the Ubiquity line provides is probably what we're looking at. Okay. And we've actually got one more quick question. Great. Can you stake using offline keys? Um, I don't know the current status of that, and I think that it would be better to better to get an answer for that offline. I don't want to shoot in the dark and aim wrong.
05:13:51.236 - 05:14:09.784, Speaker A: Great. Well, the viewer who asked that question, if you want to jump into our chat after you're done here, feel free to jump in, answer some questions. And, of course, there's the East Staker. Discord the links of what you shared earlier that someone can definitely find this answer to. All right, thank you so much for joining. Thank you, supervisors. Thank you, Michael.
05:14:09.784 - 05:14:48.088, Speaker A: And we'll talk soon. Thank you so much. All right, so we're now moving into our last talk of the day. We have Ben Jones from Optimism talking about roll ups and especially using roll ups with the data availability that ETH Two will soon offer us, this is the most Creative Title Award won today. So, Ben, I'll let you take it from here and explain this really important topic to us. All right? Thanks, Josh. Hang on, let me go ahead and share my hello.
05:14:48.088 - 05:15:10.140, Speaker A: Hello, everyone. Let's see here. Wonderful. All right, we're in. Hey, guys. Thank you, Josh, for that wonderful intro. I will take that title of Most Creative Talk Title, but I actually have to give it to the rest of my team because we ran an experiment where I gave them control over my talk title.
05:15:10.140 - 05:15:45.992, Speaker A: So that's what we were left with anyway. Cool. So, hey, everyone. My name is Ben Jones. That is my real name, not an Internet pseudonym, I promise you. And I work with a team called Optimism, and we're building something called Optimistic Ethereum, and it's basically a scaling solution that makes ethereum super extra awesome because it's already amazingly awesome. And today we're going to talk about, as Josh said, sort of roll ups and ETH Two and how they fit into the sort of ethereum scaling landscape and ecosystem.
05:15:45.992 - 05:16:35.770, Speaker A: And also I'm going to talk a little bit about the synergies there, really in particular because I think that that's very important to cover. Cool. So I guess I came at this talk with the title of the thing in mind, which is The Future of Ethereum. But I do think that before we talk about the future, we need to understand a little bit about the past. So I'm going to share a little bit of very brief points of context before I talk about the future. And the other thing that I want to say is that the reason that this is not a recording and I'm sitting here in front of you guys talking live in a way where I'm going to almost surely say something embarrassing and slip up, is that you all can ask questions. So I'm going to try to keep this quick because I'm much more interested in responding to people that are tuned in around the world than me just doing something that could be recorded anyway.
05:16:35.770 - 05:16:59.516, Speaker A: So bring the questions on. Get them ready. I want good questions, but we'll go over some slides first. Okay, so let's talk about the first little bit of context. And this is Ethereum scaling as a general concept. So I've talked a lot about these concepts. I've actually done an east global talk in the past where I talked about what I called the Cambrian Explosion of layer twos.
05:16:59.516 - 05:17:39.980, Speaker A: And I think, really, it's even accurate to say that there's a Cambrian Explosion of blockchain scaling generally, and certainly ETH two and sharding. While it may not technically fit into the category of layer two, because it is a layer one, I think it's worth talking about. And that's the important thing that I want to drive home about what's been going on in Ethereum is that the number and designs and space of scaling this chain is just booming. It's absolutely exploding. Right. Cambrian Explosion. One thing about the Explosion terminology, actually, that I don't really like in some sense, is that it sounds like very violent and things are moving away from each other in this sort of explosion.
05:17:39.980 - 05:18:21.950, Speaker A: But remember that the Cambrian Explosion resulted in a very diverse ecosystem where everything worked together. And that's what I'm going to talk about today, is how some of these things work together. And in particular, I'm going to talk about e two sharding and roll ups, and really particularly e two phase numero one. Numero uno. E two phase one. Okay? So the other crazy thing that I wanted to say is that the other alternative to Explosion that I was thinking about for Cambrian Explosion, because that has a negative context, is implosion. Right? Because somehow what I want to communicate is that there's a lot of energy going on in scaling right now, and it's really important that it all comes together.
05:18:21.950 - 05:19:08.780, Speaker A: And so I Googled implosion, and it turns out that the first Google Image result is like a high level diagram on creating a nuclear weapon, which I just thought was ridiculous. And I figured if I'm on the list, I'll put you all on the list, too. So, very interesting, and maybe we'll even get back to that diagram. Okay? So that's the first sort of point that I wanted to make, is that things are popping off right now. Super awesome. The second little bit of history I want to share, or at least catch people up to speed on is what is going on with ETH two and Sharding. And in particular, what I really want to do is work in this joke that I've been trying to crack for a very long time, unsuccessfully, which is to say that ETH Two phase one is like a hotel during COVID because it has a lot of availability.
05:19:08.780 - 05:19:46.472, Speaker A: Okay? I've been trying to figure out how to get that idea into a good joke form. I tried to do a visual pun. It's okay. I don't know, but I hope you can see that I put way too much effort into it with some photoshop. Cool. Okay, so what does data availability actually mean? This is something that the term data availability right, I think is thrown around a lot on the Twitter sphere and the sort of crypto community, and I think it's worth diving into a little bit more about what that means. But the approach that I don't want to take is diving into what it means by sharing things like this.
05:19:46.472 - 05:20:22.560, Speaker A: Right? So this is like this crazy e two sort of spec, and I think it's too much into the weeds. So what I'm going to share is my quick TLDR, high level vision, simple description of what we can think of ETH Two phase one as what does it mean that there's a bunch of data being made available. Okay. So I'm going to do that by recommending a great website that some of you may have visited called Downforyonestme.com. Okay, so this is a very useful website. You can just go to it if a website isn't working, and this website will tell you whether it's working to them. Right? So if you go to Google or in this case, I tried to go to Rollupblockdata.com
05:20:22.560 - 05:21:19.156, Speaker A: and down for everyone, or just me, also tried to go there and check whether or not the site was up. And of course that was just a made up URL, but of course it's, like, relevant to what I'm going to be talking about, but it was down. So the very simple way of imagining what E 2.0 is, is we take that functionality of a website or another party checking for you whether or not they can download some data, right? Download this web page and we decentralize it. And there's a lot of stuff about random sampling and proof of stake and all that sort of super important stuff in that spec and in how it technically works. But at the end of the day, the way to think about e Two Phase one is that we select a decentralized pool of people, and they all start downloading the block data from each other and making sure that everyone else can access it because what they want to do is make a censorship resistant chain. This is one of the awesome properties of blockchains, and we really want to preserve it, but we want to scale it.
05:21:19.156 - 05:21:57.584, Speaker A: And so this is like. Sort of a funny high level intuition for how we do that. Okay, cool. And then the last context that I will share is just a little bit about roll ups. And in particular I'm going to talk about optimistic roll ups because it's the thing that I am spending all my time on and the basic intuition for why this thing that's a roll up, which I'll talk about later is called optimistic is. I think this slide is like the best summary that I've seen is you don't go to court to cash a check. You go to court if the check bounces, right? So this is a very obvious statement, but if we think of a blockchain as a court, it can give us a lot of intuition for what an optimistic scaling solution does.
05:21:57.584 - 05:22:46.720, Speaker A: Okay, so we can go into a little more technically what that means, right? It's relatively simple. So you have up here your L one, which is ethereum, of course, in parentheses. And this will be E two in this sort of synergistic future I'm going to be talking about. And of course we have an L two down below. And this is what we call the roll up, the optimistic roll up. And the basic idea is that you start off with some state, say state zero over here on the left, and then you have a process that picks a transaction and along with that transaction, someone proposes what the result of that transaction is, right? So fairly simple. If we started off with state zero and then TX zero sent Alice some money, then state one would have Alice with that much more money, right? And then we could do the same thing for TX one and that would produce state two and so on and so forth.
05:22:46.720 - 05:23:52.704, Speaker A: Okay? So the important thing about an optimistic scheme is that it is expensive to calculate what the result of TX zero is on state zero, especially in a decentralized consensus with this crazy gas limit that we have in ethereum and crazy spiking gas prices. So what we do instead is we simply post that data, but we do not do any computation on it. Instead, what we do is we basically say we're going to make this notarized we're going to cement this as this new proposal and if we ever get to a point in time in which there's a fraudulent proposal, right? So transaction two here, let's say, shouldn't result in state three, but someone proposed it. Only then do we go ahead and prove fraud, right? We say, hey, look, I'm going to prove it to you. We didn't want to at first because we wanted scalability, but I have no choice now. Let's execute this transaction and show you what really happens because it's not S three, right? And that's a fraud proof. And this allows you to basically, in the happy case, know that the chain progresses for cheap.
05:23:52.704 - 05:24:24.784, Speaker A: But in the sad case, you can prove that it was misbehaving or some party was misbehaving. Cool. Okay. So normally, if I had, like, a chat in front of me, I'd be like, all right, does that make sense? I'm just going to have to believe that it makes amazing sense to you all. But if it doesn't, now's the time to write down your questions and ask more. Okay? And then the last thing that I'll talk about is just in practical terms, one of the most important things to us about this system is that execute l Two TX can be execute EVM TX. So this took a lot of work for us to figure out.
05:24:24.784 - 05:25:17.680, Speaker A: And we have a Deep Dive article called OBM Deep Dive, if you're interested in checking it out. So we call the version of the EVM that can work in this execute OVM TX. We call it the Optimistic virtual Machine instead of the Ethereum virtual Machine. And we're very, very excited that we can have something that is so close to Ethereum but works on L2. So worth even giving the classic mind blow GIF, which I do pronounce it. Okay, so I guess the last thing that I'll say is, as I was putting together these slides, my mind kept coming back to this nuclear Bob diagram, and I actually realized that it's kind of, at least graphically speaking, a good analogy for the synergy that I'm talking about. Because if I go back a few slides, right, this rolled up right, this notarized, as we call it here, is exactly the same thing that E Two Phase One gives us.
05:25:17.680 - 05:26:01.532, Speaker A: If I keep going backwards here, because obviously the most important property of keeping this system secure is that if TX One or S Two, you could not download them, then you could never pass them into this function. You could never make the transaction that proves fraud because you can't give the inputs. So it's very, very important for this optimistic roll up system to work that the data is made available. And that is exactly what E two, Phase One, does. It makes sure that it's not down. It's up for everyone, including me, right? Okay. So if we were to imagine this diagram as sort of what we're talking about here, what we can imagine is on the outside, we have these roll ups, and they are able to basically surface disputes inward.
05:26:01.532 - 05:27:07.460, Speaker A: And then in the inner part, instead of a supercritical mass, a subcritical mass of plutonium 239, we have a supercritical mass of ETH two. Validators you guys just saw the talk on how to spin up a staker. Go do that, get us to that super critical mass, and we will explode with so much availability. And then, of course, at the end, what happens is not nuclear fallout, but it's sort of a nuclear ethereum adoption explosion, right? If we can get transactions down cheap and we can make them fast, and we can make them easy to do, then we can really drive Ethereum adoption, which is super, super awesome because right now things are very expensive and it's hard for people to be able to use it. Okay? So that is sort of the high level description that I would use to describe what e two phase one is what roll ups are and how they relate to each other and why that's so critical that both of them come hand in hand. Wonderful. And the next thing I'm going to talk about is just a little bit about the system that we're building and sort of our philosophy and how we're approaching building optimistic Ethereum.
05:27:07.460 - 05:27:36.880, Speaker A: Cool. So one of the big important pieces of philosophy that I think we are taking is Pragmatism. So very important. Scaling is an issue today, right? Gas prices are going through the roof on the Ethereum main chain today and it's extremely important that we solve this issue. It's not a hypothetical, it's not a research problem that we can sink years into. We have many awesome research things coming together but we need to find and use solutions today. So it's extremely important that we do that.
05:27:36.880 - 05:28:33.360, Speaker A: And the other philosophy related to Pragmatism that I think is relevant is don't reinvent the wheel. So on one hand, it's a great thing that the EVM can be used for developers because who wants to rewrite their application to deploy it to a more scalable version. But what's equally as important is that we can reuse all the infrastructure that Ethereum uses. So down here in the bottom I have a little diagram which you may not a little clip art which you may or may not be able to see, but that is a diff. And the point is that it is a git diff that is very, very small. So not only do we provide EVM just like developers know and love from an interface level, we have actual implementations of Geth and of the EVM that are very similar. It does not require changing an entire architecture to be able to get the OVM to work and to scale the EVM with optimistic rollups.
05:28:33.360 - 05:29:03.332, Speaker A: Cool. So that's one of our philosophies. Let me see if I can click there. Another very important philosophy for us is collaboration. So we are a public benefit corp and we think it's very important that we have a charter that promotes access to scalable and cheap financial infrastructure. More concretely, the two things that we've been making sure that we do that's very, very important one is develop out in the open. So if you go to our code base, every single commit, everything that you look at there is public and there's no staging.
05:29:03.332 - 05:29:37.444, Speaker A: You can just go and see the weird things that we're trying and maybe sometimes failing at. And the other thing that I think is super important for us to do is to lean very heavily into ETH. One point X so I think we had a talk from Alexey from Turbogast earlier today. There's a very awesome, very large community of people working to make Ethereum core infrastructure better. And the great thing about having a tiny diff is all of the things that people do to make Ethereum better, we can use to make Ethereum better. And all of the things that we do to make our system better, ethereum can use to be made better. So we're super excited about that.
05:29:37.444 - 05:30:11.184, Speaker A: And the other thing is that it's very hard to get Ethereum forks because there's a very high coordination cost. You have to get a core dev call going. You've got to have some people angry on Twitter and discussions there, right? But layer twos, because they're smart, contracts have permissionless, innovation, a much lower switching cost. And so we're very, very excited about being able to do things like stateless Ethereum. So I found a cool stateless Ethereum diagram on the Internet, very exciting. And also things like parallelizing Ethereum, which is something that's very, very important. Cool.
05:30:11.184 - 05:30:46.360, Speaker A: And then the last thing that I'll say is as far as our philosophy goes is that iteration for us is extremely important. So it's very important that we don't build in a vacuum and we take a lot of care to make sure that that doesn't happen. We've launched two test nets, and every time that we've done that, getting real users using a system has taught us so much about what we need to build and what people desire. So we learn from the community and we work for the community. So if you want to teach us things, you should reach out. And if you want to work with us, you should also do that. Speaking.
05:30:46.360 - 05:31:02.576, Speaker A: Oh man, I did that transition and I forgot this was the next slide. But I guess I can't pretend it's intentional. Now we'll pretend it's intentional. Yes. As I was saying, you can find us on the Internet unsurprisingly. We got a Twitter here. We got our website, which has a link to our discord in it.
05:31:02.576 - 05:31:19.190, Speaker A: Go check that out. And like I said, all of our code is open source and ready to be torn apart and viewed by anyone watching. So go check that out. Okay. How are we doing on time? Great. Okay, I wanted to leave time for questions and I think we've succeeded. So that's my talk.
05:31:19.190 - 05:31:45.420, Speaker A: Awesome. Thanks, Ben. Yeah, we do have some questions, so let's jump right into it. How long do you estimate the existing Ethereum chain data will get us? Assuming full roll ups until we hit capacity and need additional layer one data provided by these two. Hey, Josh, I can't hear back. You're back. Sorry about that.
05:31:45.420 - 05:32:31.752, Speaker A: Did you hear my question at all or I'll just say it over again? I'm going to be honest. Up until that point I heard nothing. Ask it again. All right. How long do you estimate the existing ethereum chain data will get us assuming full roll ups until we hit capacity and need additional layer one data provided by E two, right? So the E Two data availability gets us like orders of magnitude better, right? And this is sort of the point of Sharding has been from the beginning, by the way. E Two phase Two is all about doing the same thing that it does for availability, which is just like your ability to sort of timestamp things into the Ethereum chain makes a very provable, very accessible, very downloadable for everyone way to put some data. But of course, the other thing that Ethereum does is give you a trustless way to do computation.
05:32:31.752 - 05:33:05.144, Speaker A: This is your smart contracts. So both of these things are massively improved by E Two in some phases. But the easier phase, thankfully, and also the most important phase for roll ups is that data availability. And so ETH Two has been on a long track knowing that we need to scale orders of magnitude up today. That said, we can get a lot of transactions out of ETH One in the short term. And this is like the important Pragmatic thing. It's why we're not building a system that uses ETH Two availability first.
05:33:05.144 - 05:33:50.976, Speaker A: We're building a system that uses ETH One availability. And so you can get ten to 100 x scalability. The scalability for a roll up is interesting in that it is a little bit dependent on what you're scaling. So, for example, if you are scaling transactions that are very large, but then only a very small bit of computation is done on them, it's not as useful in roll up because you still have to pay your transaction fee to make sure that big data is downloadable for everyone. And all that you save on is the tiny little computation. Whereas you could have a very small transaction that is not a lot of data to post, but then it does a lot of things, a lot of addition and subtraction multiplication, storage, storage loading, storage setting, creating other contracts. Then that's going to give you a lot more scaling.
05:33:50.976 - 05:34:50.248, Speaker A: But we can throw around ten to 100 x 3000 TPS. There's lots of numbers you can throw around. The truth is, it's really a little application dependent, okay? And kind of a follow up to that, just understanding how these things fit into each other in sequence. Once we have data availability from phase One, from ETH Two progress, will there be any reason to use roll ups that use like, ETH One data availability? Or at that point, is it just going to make sense that really all optimistic roll ups are going to be using E Two data availability? Right? So I think it depends on what you mean by makes sense. So on one hand, the point of E Two phase One is going to be making available data way cheaper. And so it definitely makes sense if you think that the roll up, even a roll up centric ETH One is getting congested to move that to E Two because it's going to be cheaper. So there's no question there.
05:34:50.248 - 05:35:40.424, Speaker A: And really what it comes down to, again, I'll go back to pragmatism. The answer to that question will really just be what is the development changes that we need to make? How do we get available data into the computation layer where we do disputes? But as soon as we can do that, it makes a lot of sense. Got it. Okay, moving on. Are there any really major open problems that need to be solved before this is possible? Or is it really just a matter of like, look, optistic roll ups need to be on mainnet for a while, and of course we have to deploy data shards, but once we have those things, are there major unsolved problems or we should be able to implement at that point. So I love research and so I would be very sad if the answer to that question was no, everything is solved. And so that's definitely not the case.
05:35:40.424 - 05:36:28.024, Speaker A: What I can say is that I and other members of my team spent a lot of time working on plasma, which was a scaling technology before that, and doing research. And one of our motivations for its time to act on roll ups was that it was a very pragmatic, very implementable thing as it stands today. And so I think that that's very true and very important. Certainly there are tons of things beyond that that are going to be important, in my opinion. Some of the biggest are basically what I said before, which is all of the ETH One point X stuff. So state Bloat is a huge problem on ETH One, and we can do crazy awesome things that ETH One point X. People have been trying to figure out how to do and actually deploy them in a practical manner on roll ups today.
05:36:28.024 - 05:37:01.268, Speaker A: And so I think that's very exciting. Those research problems are underway, but that's some examples of future important research. Okay, and we got a couple of questions specifically about optimism as well. So let's jump in there. What are the advantages of optimism's roll up solution as opposed to arbitram's optimistic roll up solution? Spicy question. So, I mean, the first thing that I'd say is everyone takes their own approach. And so the answer to this is going to depend on the system that you're building.
05:37:01.268 - 05:37:48.828, Speaker A: Absolutely. The things that I will highlight are honestly things that I've already highlighted about the OVM, which is that we are reusing everything from the EVM humanly possible. And so our view is that taking that approach to the roadmap is going to have huge payoffs as the ecosystem evolves. I think that's going to be really important. Yeah, I can tell you, having spent a lot of time working on doing things in the EVM, I've become so grateful anytime that we can do something that has already been done with the EVM because it is so much easier than reinventing the wheel. So that would be the main thing that I would say. I mean, kind of building on that.
05:37:48.828 - 05:38:44.260, Speaker A: What, if any, extended capabilities do you want to add to the OBM that's beyond the baseline of what EVM gives us today? Yeah, I'll hit you again. Josh with the ETH. One point X. But one of the coolest things that we've done in this release is built out something called account abstraction. So this is a concept that you can go and read tons of ETH research posts on all over the place. But basically what it comes down to is ditching the notion of ethereum wallets as being something that is enshrined and ethereum transaction formats as being something that is enshrined and instead just making everything be smart contract wallets. Right? And so this sounds sort of obvious and smart contract wallets are especially because of meta transactions are really booming at the moment, but the ability to do this natively in the protocol is super powerful.
05:38:44.260 - 05:39:43.940, Speaker A: So that's one thing that concretely we have now. The other things, like I said, I'll just keep harping on ETH one point X is Stateless clients are going to be huge and especially when we have a lot of data available, statelessness is super awesome. Another one that I'll lay out is parallelism. So right now the EVM is sort of this single threaded beast, but we're going to be able to basically split that out into something that is asymmetrically relevant or asymmetrically biased towards the people that are defending the chain. And so we think that that's going to be something that's super exciting and it's going to basically get us a higher gas limit without having to sacrifice in other areas. Got it. And can kind of continuing on this thread a bit, actually, I should say, if anyone watching is interested in learning about account abstraction, we had a great talk earlier in the day from Sam and Angstar.
05:39:43.940 - 05:40:25.510, Speaker A: So check that out. It's going to be on our Twitter, it's going to be on our YouTube. You can go check it out. But continuing on this thread, Ben. So we presume that Officen will roll it a version one, but still have some things on the roadmap, subsequent versions, et cetera. What does upgrading from occasion V One to V Two look like in practice? That is a very great question and that's something that we've been thinking a lot, a lot about as we gear up for a main net release. So the first thing that I'll say is that we want to make sure that people have optionality and there's sort of a trade off sometimes between giving users optionality and giving them a horrible experience.
05:40:25.510 - 05:41:15.830, Speaker A: But one thing that we've been very strongly opinionated about is making sure that the actions that you take in rollup are as generalized as sort of the EVM was if you compare it to Bitcoin. And so what this means is that there's not really actually any sort of function in the OVM that is like deposit or withdrawal. Instead there is what you could consider a message passing layer. And so instead what happens is you have a contract that holds deposited money on L One and a contract on L2 that makes deposited funds appear if they are held on L One. And these two contracts have an authenticated way to communicate with each other. So anytime you lock up some money here, it mints it down here. And if you want to withdraw it'll, burn it down here and it'll release the funds to you up here.
05:41:15.830 - 05:42:15.370, Speaker A: And so this sort of arbitrary message passing construction gives you a lot of flexibility in how you want to design an upgrade mechanism. The thing that we are a big fan of is basically opt in or opt out mechanisms where you have a period of time in which the state is going to be forked to have basically a better improved VM or better improved contracts. And users will have a period of time where in that time they will be able to say, I want to use this upgrade. And they'll basically be able to convert their tokens into the same amount of about to be upgraded tokens and they'll go there. And so that's one approach. Another approach is using that mechanism in the opposite way and saying by default, there will be some governance mechanism that will promote an upgrade. And if you don't like that upgrade, then you can do the exact same thing to opt out and you will be left behind in the old chain, which will still continue running.
05:42:15.370 - 05:42:52.272, Speaker A: So yeah, that's how we think about it, is basically like give people flexibility and when you do upgrades, make it a new chain. Don't destroy the old chain and make a new one. Just make a new one and give people an easy way to migrate over. Great. Okay, if you still got time, we got two, I think what should be relatively short questions and then we'll wrap this is right from the chat just very recently. Is the fraud proof executing the entire transaction or sampling certain steps of it? That's a great question. So it honestly depends on how you define transaction.
05:42:52.272 - 05:43:24.060, Speaker A: I think the way that I would assume you're defining it, the answer is it does the whole thing. So there are a pretty large design space actually of ways to sort of distribute interactivity into these fraud proofs. But again, our focus is pragmatism. And the pragmatic way to do it is you just run the transaction. So that's what we've gone with because this is like the fraud proofs are the sad path, not the happy path. And so we have something that works. We want to get it to people because we really need to scale ethereum.
05:43:24.060 - 05:44:08.430, Speaker A: Okay, next, what techniques do you use to keep fraud proofs attractable size rather than having to replay massive blocks or full transactions entirely on main chain. So that is a great question. So there's two things that I'll say. So the first thing is that this is yet another example of us being so thankful that we're able to reuse the EVM because something that the EVM has that's very important is gas metering. And because the EVM has it, the OVM has it. And so you can bound transaction size and therefore fraud proof size if you're playing the whole transaction by having a gas limit on a per transaction basis. So that's one part of the answer.
05:44:08.430 - 05:44:47.480, Speaker A: The other part of the answer is that we have a bonding system whereby anyone who might be able to take an action that you would have to prove fraudulent to secure your funds requires that you post a bond. You basically take some ETH and you lock it up and you say, hey, I promise to tell the truth. If I don't tell the truth and fraud is proven, this money will be slashed. And so not only does the money get slashed, but some of it is also used to pay back for the gas that people spent doing the fraud proof. And so this is the other sort of MVP detractor from fraud proofs being big and expensive on L. One is make sure people get reimbursed. Got it.
05:44:47.480 - 05:45:04.524, Speaker A: Okay, final question. Ben, it seemed like explosions were on your mind earlier. It appears that you're joining us from a bunker right now. Tell us a little more about your environment. Tell you a little more. Oh, great. Well, I'll DOX myself a little say that I'm in the wonderful city of New York.
05:45:04.524 - 05:45:32.080, Speaker A: What you are looking at above me is not a bunker, actually. It is a shelving unit that I took a hacksaw to and cut some slots in and put some of the shelving things across between these two shelves. So there's another shelf there and my bed is actually up above me right now. Okay, we had lots of guesses coming in the chat. I hope that someone won some money on that. Okay. Thank you, Ben.
05:45:32.080 - 05:45:50.190, Speaker A: Really appreciate it. I think there are more questions in the chat. If you've got a minute to jump in and chat with people, I'm sure that they'd appreciate it. And of course you can find Ben on Twitter, I'm sure, and maybe he'll answer there as well. Oh, yeah, at Ben underscore chain, it's another horrible pun. That's my Twitter handle. I totally didn't put that, but appreciate it.
05:45:50.190 - 05:46:23.412, Speaker A: Thanks Ben. Okay, thank you everybody. It's been an amazing day. Really high quality, high signal content, and thank you for joining us. Now, of course, this is the moment where we hype the next part of Ethanline, which is excitingly and maybe sadly also the last part. So it's been an amazing month of summit after summit every Friday of October. And next Friday on the 30th is our final summit.
05:46:23.412 - 05:47:20.104, Speaker A: This is going to be a combination of the finale and closing ceremonies for the Hackathon and the kind of summit conference style content you've been seeing all day today and the previous Fridays. So at the beginning of the day, we're going to see the finalists from the Hackathon. As Kartik mentioned, at the beginning of the day, we've had more than 170 project submissions, which is incredible. Judging is going to be happening over the next couple of days. And next Friday we'll have some finalists for you who will demo their hacks before some judges talk about what they built and get some great questions. And then after that, we're going to have an afternoon of great summit content all about Ethereum's impact. So this is the day where we kind of look at the big picture and talk about some enterprise use case cases, talk about the impact on developers, talk about the kinds of ways Ethereum is being used around the world to solve real problems for people today.
05:47:20.104 - 05:47:27.210, Speaker A: So please join us for the last day. It's going to be great and I hope you have a great weekend and see you then. Thank you.
