00:00:00.410 - 00:00:01.440, Speaker A: To the cloud.
00:00:13.750 - 00:00:14.980, Speaker B: Okay, we're good.
00:00:17.110 - 00:00:54.240, Speaker C: All right, I'll redo that intro. Welcome, everybody, to this retrieval markets symposium. Set of talks and demos, all sorts of things we've got in store. This is part of Hackafs, which is the Hackathon. I know we have some people joining from outside the Hackathon for this session, but like I said, a couple hours ahead of some presentations from people who are working directly on these parts of Filecoin, so it's going to be a lot of interesting content coming up. Juan, do you want to give any overview before we start?
00:00:55.330 - 00:01:04.580, Speaker B: No, I'll just jump right in and maybe talk a little bit about the I'll include that. Sure. Yeah.
00:01:05.830 - 00:01:11.510, Speaker C: I believe you were the first up to speak, so go ahead, share your screen and take it away.
00:01:11.660 - 00:01:12.438, Speaker B: All right, awesome.
00:01:12.524 - 00:01:13.320, Speaker D: Thank you.
00:01:30.750 - 00:02:26.058, Speaker B: Can you see that screen? Yes. Perfect. All right, great. Hey, everyone. I'm going to be talking about kind of the retrieval market in general, and the whole goal for this set of sessions today is to talk about all of the parts that can combine to make the retrial market. And the goal is to talk about it both at a very high level, kind of give a motivation for it and kind of the vision for it, and also talk about in detail how some of the components that are getting built out now can be pieced together to actually make the real market work. And then we'll hear from a few groups that are working on versions of this, and we'll kind of potentially see some demos of the work that's ongoing.
00:02:26.058 - 00:03:24.180, Speaker B: And then I also had some open time for any Hackaths teams that are working on virtual market oriented things to kind of talk about what they're up to and talk about what they're building. So definitely feel free to add yourself here. I got some requests to also go over since drafting this agenda, I got some requests to go over gossip sub in general, just so that everybody here has a good understanding of how it's supposed to work and so on. And so if that's of interest, definitely note it on the chat for Zoom. And if so, then I'll try to squeeze in kind of like a five to ten minute overview of gossip sub somewhere here on the agenda. Great. So I'm going to be talking about kind of the retrieval market at a very high level and kind of talk about the vision for it so that we kind of set the stage for how that's meant to work and the different kind of performance characteristics that we might expect from a network like this.
00:03:24.180 - 00:04:34.006, Speaker B: And then towards the end of the talk and kind of discuss different potential trajectories for how to implement different there's a lot of ways of building this kind of thing, and so I'll maybe describe some of the different paths that we've been discussing. Great. So I want to kind of ground all of this in thinking about the internet and the cloud in general. So at the end of the day, the internet is just like this very large collection of computers and wires all over the planet. And the structure for this is this kind of like grapevine where a lot of machines, a lot of devices are all over the edges and they connect in a hierarchy and there's kind of a lot of routing kind of close to the center. So this is kind of like a zoomed in view of the grapevine and it's another conception of it another model for the representation. But you can see here each of these links, the farther away the links are going, the longer the latency between those links.
00:04:34.006 - 00:05:54.306, Speaker B: And one interesting fact about the internet is that it's not just kind of the physical topology of the network, you also have overlays that have maybe these logical links but the underlying distance between those is actually much larger or much longer. So you can think of a lot of the peer to peer networks or VPNs and so on as building kind of these overlay networks that appear logically close but in reality very far away in latency. And geographical distribution really matters because when you're trying to retrieve data, the speed of light ends up affecting you not just in kind of the time to retrieve the data because that's not that bad, but it's rather in the kind of round trip time. So whenever you have to coordinate with another side and you have to talk across the planet to do so, then those kind of like 200 millisecond round trips start adding up very much. This is also the case when you have to talk to a lot of nodes that you don't yet know because any kind of cryptographic channel has to be established. And when you do that then you have a lot more round trips per interaction. So here's another view at the kind of internet grapevine you can see sort of like the top tiers of the internet and kind of the interconnectivity there that you can think of those as being very large organizations distributed all over the world and having really fast links between them.
00:05:54.306 - 00:06:24.106, Speaker B: And then as you go down to kind of the home edge environments, the number of organizations grows tremendously but those are much smaller and the kind of the links between them get worse. I have a random question. Can you hear a cat meow in the background? No. Okay, good. Zoom audio is working well. Cool. So then this is yet another conception of the same thing.
00:06:24.106 - 00:08:19.930, Speaker B: It's kind of like a simpler version where you can think of these kind of subnetworks that are put into different tiers of the Internet and different kinds of networks are connected. And you can think of when you start thinking about how to distribute content through networks like this, you can start thinking about how to place certain machines or certain devices in different spots to improve the resurrection times for content. So if you're trying to serve a lot of files or a lot of files that are very big, all the experience gets better the more you can kind of replicate and cache a lot of that content close to where it's going to be used. And so kind of going back to this diagram for a moment, if you have some kind of consumer oriented set of content, so imagine things like movies or any kind of video stream and so on, where the content? Is large and you have potentially tons of viewers all over the world, then it becomes extremely useful to start putting caches of content as close as you can to the users. Usually you can't get it into the user's homes, but you can maybe get very close and you can start putting it in kind of those gray networks very close to the user, but not quite such that you have to kind of route across the world. By the way, all of this gets more complicated when you start considering the kind of emerging geopolitical landscape that is adding all kinds of restrictions all over the internet where these links are no longer kind of easy to move across. Now there's all kinds of delays and blocks appearing across links, kind of thinking about what kind of content can move to various know.
00:08:19.930 - 00:09:29.046, Speaker B: A good example to think about is if you're a group like Netflix, you have some data center somewhere and your application lives somewhere in the cloud and you have a bunch of consumers kind of in the very edge and in kind of what's known as sort of the last mile or kind of beyond the last mile. And in reality what you want to do is you want to get as close as you possibly can to them. And I'm not sure if Netflix is still doing this, but back in the day they used to deploy these kind of racks of hard drives preloaded with most of the content that that region was going to watch directly in the ISP. So they would grab a machine kind of like this loaded with a bunch of hard drives pre filled with the majority of the content that's going to be retrieved by some area and kind of ship that box to that ISP and then preload most of the content right there. This is similar to the strategy that CDNS take in general. So CDNS tend to do this, but for a lot of customers I believe Netflix had to do this for themselves because they had kind of a specific setup and it was just most cost effective for them to do it by themselves. But now a lot of other content delivery networks end up doing the same thing.
00:09:29.046 - 00:10:18.870, Speaker B: Just they ship these boxes to ISPs and then they decide what content to move to these areas and so on. You can think also about the cloud. This is kind of a diagram from Google Cloud's set of systems. They have kind of some set of core data centers at the kind of center of it. And the links between those data centers are though they seem close here logically in reality those are very high bandwidth but also high latency links because they're going across the planet. But then kind of from the data center out, you get these kind of edge point suppressants and then edge caching and other kinds of nodes and you'll see kind of a three tier or two tier architecture depending on the cloud. Some groups differentiate between pops and the actual edge cache, other groups don't.
00:10:18.870 - 00:11:18.246, Speaker B: This is, I think, our diagrams from CloudFront, which is Amazon's CDN thing. And you can sort of see Amazon has some cloud server somewhere or some data center somewhere and kind of like all the content you put in s three or similar kind of storage goes in that data center. And then when you actually want to deliver content to the user and you expect some high distribution of traffic, you kind of want to cache as much of that content as close as you can to the user. And so there's all these kind of cloud front edge locations in a bunch of places to kind of significantly reduce the time. And again, one of the big reasons why this matters a lot is it's not just about the individual round trip time. It's really about kind of the sequential round trips. That's what really kills you because the difference between waiting 50 milliseconds and 100 milliseconds, it's kind of significant.
00:11:18.246 - 00:12:06.326, Speaker B: You'll probably detect it as a human. But the real problem is when they'll start adding up as you fetch one web page, that web page, after you've fetched it has a bunch of links to a bunch of other things. You then have to retrieve those things. You retrieve those things, they themselves. Now you have to get other stuff. And so when you're dealing with a dynamic application that has a bunch of links, especially to a bunch of different servers, where you have to set up a new TLS connection and so on, that's what's really killing you. And so ideally you want to kind of bundle up all of that content, serve it from the same place and ideally kind of as close to the user as yeah, here are a couple of maps from the data center regions from Amazon and Google Cloud and that's kind of like, again, data centers, not the edge caches.
00:12:06.326 - 00:13:17.614, Speaker B: And these are kind of their CDMS and also cloud players. Here you can see kind of how they try to distribute as many nodes as possible as close as they can to the users. Cool. So this is kind of the internet that cloud as we have today, we have this kind of like useful explainer that kind of goes through kind of how we envision this working out in filecoin. The whole goal is to build this decentralized storage marketplace where any party can add storage hardware and software and then contribute the resources wherever they are on the network to make the whole thing much more efficient, to make the entire content storage and content distribution picture more efficient. That means that you will have parties that are specializing, say on long term durability of content. And at that point you want to set up a whole bunch of facilities and machines in kind of a data center quality kind of setup and where you can benefit from economies of scale and you can benefit from having a lot of storage arrays all put together.
00:13:17.614 - 00:14:10.418, Speaker B: But again, that's kind of like a data center. And we'll also have a whole bunch of folks much closer to the end users and end clients of content that are embedded all over the world in specific cities and so on. And those machines will tend to be a lot smaller so those will be kind of in the few terabytes to 100 terabytes range and those will be perfect for this kind of retrieval market kind of distribution. So they won't have the kind of upkeep or economies of scale that say a large storage miner might have. But those are perfect for this kind of virtual market CDN kind of use case. And so normally when a client is going to source something on Popcorn, they kind of make this deal with storage miners and there's kind of this whole contract flow. A client hires a miner or a specific set of miners to store some data.
00:14:10.418 - 00:15:13.160, Speaker B: And then those miners store them in this kind of long term storage mechanism with a proof of replication. And you get this verifiable storage where you get this kind of auditable trace that the data is being stored for a long term term, and so on. Plus you get this very useful kind of subsidy that comes from the block reward which can make the storage of that in the long term much cheaper. But again, this is kind of tuned for these larger scales, these larger scales of storage that are kind of in a more data center oriented capacity. Now, when it comes to retrieval, that's a different part of the equation when you really want to push out the content as close as you can to the users. Sometimes you know that ahead of time, sometimes you know what content you should be pushing where other times you don't and you have to respond to the demand of content. But the whole goal is to create a structure where the content that is being stored by storage miners can then kind of flow out to the edges either in anticipation of demand or as demand kicks up.
00:15:13.160 - 00:16:19.386, Speaker B: And so one of the ideas on this is that ideally you can have a network where you can detect that something becomes hot over time and then decide to move it to different locations and so on. And instead of making this a kind of centralized coordination problem where a single party has to maintain observation operations all over the place and has to detect all of this going on, you turn this into a market problem. Where agents that are in specific regions that happen to be kind of in between where the demand and the supply of the content is can make the decision to grab that content as well and then rehost it somewhere closer to where it's being retrieved. So the example here is imagine a piece of content that's somewhere stored, say in the US. And suddenly becomes really popular in Europe. There's a lot of requests going through, some rotuum minor or the network notices this, decides, hey, it would be great to kind of re host this content somewhere closer to these other nodes for now. And then ideally the content can cascade this way.
00:16:19.386 - 00:17:17.840, Speaker B: It's worth noting that peer to peer protocols in general tend to do this by default and in kind of this very optimistic. But the way that tends to pragmatically work, where if you imagine kind of a BitTorrent or IPFS network and parties start retrieving this content as the content flows out and it starts getting distributed just because of how the latencies work out, parties will tend to as the number of nodes requesting this goes up, parties will tend to retrieve it from whoever is closest and will tend to reshare it. And so on. But the goal of the platform trial market is to take that idea and kind of supercharge it, to make it be incentivized and so on. Great. So that's kind of like the long term vision. By the way, I'm going to pause here and maybe take some questions if there are any before I kind of dive into more specifics of how this is getting implemented or something like that.
00:17:17.840 - 00:18:28.146, Speaker B: Not seeing the cool. So going back to diagram from Google cloud, the Fatcoin version of that is that storage miners are kind of the data center type of operation and maybe up to the ISP. So the way that maybe the circles are sliced in the Falcon case is larger. So you can envision storage miners actually being parties very close to or directly within kind of the same ISP data center because you can have a relatively like a much longer rack. You can think of hundreds of terabytes to a few petabytes stored somewhere in an ISP and that could be a great storage miner. So that's a lot closer to the user than maybe some of these large scale data centers that the cloud gives you. So the storage miners can expand closer to the user, but there still won't be maybe all the way in the very edge or all the way where a lot of the demand is.
00:18:28.146 - 00:19:30.578, Speaker B: And so that's where retrieval miners kind of kick in and so you can think of retrieval miners as following kind of the CDN case or going all the way to homes. And one important detail here is that in the topology of the internet as it is now, there are certain networks. Think of say large organization networks. So either companies or universities or large groups that have and maintained networks. With a lot of computers and a lot of traffic is going to those machines where having a retrieval minor node within that network can just suddenly start serving content for that entire network. And it would be really nice and useful to enable parties within those networks to be able to set this kind of thing up. And so CDNS today don't really think for the most part don't tend to do that unless those networks reach a certain size because the overhead and complexity of kind of creating some sort of agreement between that organization and the CDN is kind of way too costly.
00:19:30.578 - 00:21:05.574, Speaker B: But if this can become a single party's choice a single party can choose to do that and the software and hardware just take care of the problem then you can get a much more efficient distribution mechanism. But that's kind of like the there's a lot of work that we to do be before we can really enable that kind of thing. There's another part of this which is there are a lot of networks in the world where the last mile is actually way more expensive than in kind of the highly connected cities. There's a lot of parts of the world where some town or some small city is connected over kind of like a wireless backhaul or is connected over, in some cases satellite or other kinds of connectivity where suddenly those links become extremely expensive. And having something in kind of like the ISP before that last mile doesn't really cut it. And today most content delivery networks in a lot of cases don't end up delivering content all the way to those places because for example it might not be that profitable for the CDN in general and the CDN's priority listed somewhere else. But ideally, with FalcoIn, what you would like to enable is you enable somebody there in that network, in that city or village or whatever, to decide, unilaterally, to set up a machine and then start kind of being able to cache a lot of the content that is being requested from that area by turning it into a market problem that anybody is able to enter a permissionless market problem, then you can enable that kind of action to happen.
00:21:05.574 - 00:22:11.578, Speaker B: But in order for that to work you need a lot of the kind of observability of what's being requested and so on to be kind of aggregated by the network. Great. I think we talked about this at a very high level but kind of looking at the concrete details of storage miners and retail miners we can look at these kinds of diagrams where I've shown a previous version of the diagram that wasn't updated to reflect the SCR NSE distinction but I've added that here. Now we can think of storage miners as occupying a very large swath of the kind of retrieval latency standpoint depending on where the latest proof of replication land and so today we're shipping with Ser which has a pretty bad retrieval latency but it's kind of the much more secure and stable proof that we have. We were hoping to get an SC out before Mainnet launched. That's not going to happen right now. NSE targeted for November, December, and so that slice isn't there.
00:22:11.578 - 00:23:36.822, Speaker B: But we do have kind of a kind of hacky patch to this, which is, hey, storage miners keep an extra copy that's unsealed, then storage miners can actually participate in pretty fast retrieval kind of in the subsecond sort of range, single second kind of range. But that includes kind of doubling the storage cost. And so it's a trade off where some clients and some storage use cases will really benefit from that kind of fast retrieval. And so that's kind of worked into the deals now where when you make a storage deal with storage minor, you can request an extra copy being stored and miners can sort of do the accounting and price that differently because of that extra storage cost. But then this really kind of patches this problem where one of the copies is within a proof of replication and you get the very hard verifiable trace of proofs that that content is being stored and then you have this extra copy for fast retrieval. Now unfortunately it can't be hard guarantees about that copy really being there, but if clients kind of try to requesting it relatively frequently, they can build the distribution of whether or not that copy is there. And it's really mostly a pragmatic solution to this kind of question around hey, ideally there should only be one copy and it should be coming out of the proof of application and the proof of application should be fast enough.
00:23:36.822 - 00:25:05.570, Speaker B: But that's sort of why the storage miners kind of span that entire region. Ideally over time we want kind of the Storage Miners as proofs, improve and operations improve and hardware gets better and so on to kind of start moving over to these kind of faster ranges and it is totally possible for Storage Miners to get into the subsecond range and start kind of delivering content in the hundreds of milliseconds. We can get fairly close to that with some of the proof constructions that we already know about but it takes a while to analyze them in full, build them, refine them, plug them into the protocol and so on so it'll take some time for research managers to kind of get there. In the meantime, that's where retrieval miners can come in retrieval miners can occupy the whole area of a retrieval latency where for some distribution of content, where retrieval really matters, then retrieval miners can come in and serve that part of the equation. Something I forgot to kind of mention earlier is in general, the distribution of content on the Internet is one where the vast majority of content, the vast majority of all of the data stored is accessed once or never. And very few content, by volume, very little content, occupies most of the content retrieval requests. And so because of that distribution, you can think of storing a lot of the long term storage doesn't need to be stored in a way where you kind of have to store it close to the user.
00:25:05.570 - 00:25:42.960, Speaker B: It's only a small fraction of the content that actually needs to be georeplicated and cached everywhere. And so that's why this problem kind of decouples into these two different parts of it that are really nice. By the way, any questions on this part? I know that this is like an interesting way to look at it and maybe a lot of folks have had questions about kind of the proofs and so on. If you have questions, definitely enter them on Zoom chat. I see a couple. So how is the location information gathered to communicate to the ecosystem? Good question. So right now we don't have good mechanisms for it.
00:25:42.960 - 00:26:45.330, Speaker B: There's a bunch of different ways of doing this and kind of IPFS has one version of it. This is really where there's a lot of open design space for what's going to be the right model. There's a lot of stock standard in the screen systems that we could think of employing and deploying here, but it really depends on how applications want to use them and whether or not they fit the model. And so right now there's no kind of like chosen way of saying this is exactly how we're going to do it. We rather expect that as a community, we're going to experiment with a few different ways of doing this before one is naturally clearly the better one. Another question can storage and retool miners publish exclusive deals? An example if only specific predetermined users of a storage minor service would get a cheaper price, so the general public would be charged a certain amount and that set of friends or other known parties would get a better deal. Yeah, totally.
00:26:45.330 - 00:27:35.700, Speaker B: That's a great question. So right now, I think the tooling on Lotus only allows listing one price and then allows creating. I think there were discussions around creating a discount structure where you could kind of offer some kind of standard discount for automatic deals. But at the end of the day, this is just about generating a deal data structure that's signed by both parties. So there's a lot of operational freedom there where miners could be listing all kinds of different prices and make their own decisions about what price they give to whom. And we anticipate, especially for any kind of large scale data. So when you start looking at many terabytes and petabytes of data, all kinds of other things are going to get into the mix like how you get the data to them in the first place and so on.
00:27:35.700 - 00:29:03.470, Speaker B: So we anticipate that once you hit many terabytes of content in a single deal, that's actually just going to move to email very quickly and clients and miners are going to interact over some kind of conversation and then decide on a price and then they're going to craft a deal based on that price. Many folks have also suggested kind of a market quote mechanism systems where you can kind of as a client you can go and kind of describe the data and describe where you are and describe how you can ship it and then kind of request a quote from a bunch of different miners. Minus can kind of think about this and kind of send it back. And I think that all of that kind of kind of stuff is totally viable here. At the end of the day, this is a marketplace and that'll just require kind of software tooling built around the core of the market, right? So the market brings the miners and the clients together, gives them all kind of identities and then you can think of tooling like that being built on top. There's other kinds of things that matter. There like different kind of storage features like say, being able to certify other kinds of things like say, HIPAA compliance or some specific matching, some kind of industry standard structure for how a storage facility is kept and so on.
00:29:03.470 - 00:29:46.078, Speaker B: And so those kinds of guarantees can be advertised and then kind of kept separately. And you can imagine kind of clients saying, oh well, I have data that needs kind of HIPAA compliant storage miners. And so then would select to a subset of the miners what this really needs is just some website that can do this kind of integration of the offers and so on. I know of a few in development that are doing this, but this is also kind of like an open space where folks could build something in the near future. All right, I don't see any more questions at the moment, so I'm going to proceed. Cool. So a kind of view at the storage market.
00:29:46.078 - 00:30:25.290, Speaker B: The goal is to kind of commoditize digital storage as much as we can and kind of following what I was saying before. This is kind of a set of pictures from a lot of folks who have been deploying storage minor facilities. And you can see that they're very much tuned like data centers and are in data centers where they have large racks and so on. And there's a lot of storage here that can be put to work. But again, this is not as close to the users. A lot of organizations. Are getting involved.
00:30:25.290 - 00:31:38.530, Speaker B: There's kind of a questions around sizing. In the last testnet that we did, I think we got to like 29 or 30 petabytes and so that's kind of the scale there and that's without incentives. So we're about to start the space race very soon and that will show us a much better picture as to how much storage is out there and what we might expect during the main net. At the end of the day, the block reward is going to drive optimization and is going to drive a lot of storage to appear. What the block reward does is it gives us the ability to build a lot of capacity without having to match it with demand at the same time so we can build a lot of capacity and the capacity is being rewarded and then over time we can then put that capacity to work for clients. So now for retrieval though, it follows a very different set of characteristics. As a storage market, most of retrieval has to happen entirely off chain because we want this to be really fast and so that means state or payment channel networks.
00:31:38.530 - 00:33:10.990, Speaker B: We have kind of a straightforward payment channel system within filecoin. There's also a lot of thinking that has gone into state channels in general and there's a lot of really good designs out there. Over time we'll probably see a lot of that kind of stuff migrate into filecoin but we also can make use of a lot of different kind of statement channel networks that exist already separately. So you can imagine kind of state channels working over Ethereum or some other network. And then we also want kind of the distribution of kind of requests and the gathering of information about what to put, where to again be kind of this layer two fully kind of off chain setup where you can think of that indexing or even kind of the requests flowing through from specific clients and so on, to be happening over very low, latency, but potentially very large network. And so when you can think of topologies for retrieval, there's a lot of different ways of doing this, but something kind of many different ways of doing this might will tend to kind of gravitate to this kind of topology where you can think of there being kind of a network of hubs where a hub is really kind of constrained geographically based on kind of what makes sense to do, given the distribution of clients and miners in a specific area. So you can think of those hubs as matching the internet topology.
00:33:10.990 - 00:34:23.240, Speaker B: This might be a very course version. You can think of this subdividing over time as kind of demand increases, right? And so this is one potential strategy for doing this is to kind of construct these hubs where there's one or a few parties, kind of doing a lot of the coordination work of figuring out what residual miners and clients want in a given area and kind of facilitating that exchange of information. And then if something is not in that area, then kind of the forwarding the request to other hubs and the structure. There may or may not need to be this logical separation between, say, a retrieval minor and a hub. It really depends on a lot of the details of the protocols, right? So payment channels tend to work well with these kinds of hub things. The retrieval will also tend to work well with this kind of thing as well. And so, ideally, what we would like to end up with, though, is kind of this almost voronoi style subdivision of the world where as the number of kind of clients and ritual miners grows in various different areas, you get this kind of like subdivision over time.
00:34:23.240 - 00:35:16.406, Speaker B: And you can think of keeping track of that at kind of a network level and over time kind of subdividing more and more and more and more depending on the demand. And this matches sort of what the cloud CDNS are doing in terms of their kind of planning and so on and where they place all the nodes. But ideally you would get to do this just by kind of market action. Ideally this kind of subdivision could be a unilateral move by one party that says, you know, this hub is getting too big. We think we can do much better by just splitting off and forming our own hub closer and then kind of grading that distribution there and again, this is like one potential way to do it. You could not really see these hubs and then kind of treat the entire network as just one single mesh and so on. But that might be and a lot of algorithms could work that way.
00:35:16.406 - 00:36:45.586, Speaker B: There's a lot of R and D that's been done to build these kinds of protocols but that may or may not be easier. Right? I think the hub kind of structure tends to be easier to kind of model and observe and tune whereas kind of treating it as one single fabric might be a lot more elegant and a lot more kind of from a systems perspective easier to describe. But it might in practice turn out to be turned to be harder and more unwieldy. Really not clear though this could really go either way. So I wanted to also touch on some of the components that are kind of already built out and I sort of see the retail market right now as a lot of the heavy lifting of the components of a lot of the components is there. They can be put together into kind of a kind of version of retrieval that can work to some extent and we'll actually, I think, hear from a few folks that have already done this that have put together some of these components. But what's sort of missing is the components that might be missing are kind of beyond this, which is how do you do the kind of aggregation of information going through the network? What are the incentive structures between retrial miners and clients and so on, or between retrieval miners themselves as to whether or not they cooperate or compete and so on.
00:36:45.586 - 00:37:27.506, Speaker B: And so we sort of see the development of the retrial market as having many kind of versions over time as this refines. And so we'll hear about kind of like retrieval v zero and V one early on. Lotus is now, I think, in the V one place and we'll kind of hear about that a little bit later. We'll also kind of look at some of the payment channels that are there now on Popcorn that you can use for this retrieval. Cool. So I think maybe I'll jump into IPFS CIDs. I'll give a brief overview of how that works just because it's kind of like important set of basics to have in mind.
00:37:27.506 - 00:38:31.988, Speaker B: And then I'll turn it over to Hannah to talk about the Lotus parts kind of retrieval on how it works. And then I might do the kind of gossip sub description right after that. Since I see that folks want that cool. I'm going to move to a different presentation. Can folks do that? Great. So if you've used IPFS and you've added content to IPFS in the past, you probably have used IPFS add that kind of takes a directory and adds it all and you get this kind of output that shows a bunch of CIDs. Those CIDs are the kind of hash link to that content and that includes kind of all the links of the stuff inside.
00:38:31.988 - 00:39:35.108, Speaker B: If this is new to you, there's a whole bunch of talks out there that kind of go in depth into this. I'll just kind of talk about it in a general sense. Now then, the way that those CIDs work is that what you end up getting is this contiguous file gets chunked into a bunch of different pieces and each of those chunks is hashed. And then that kind of file that is a collection of chunks gets its own hash and so on. And so over time you can think of all these pieces being constructing a whole tree, large tree that represents a file, one version of it. You can think of entire directories and file systems built the same way, or moving away from the file system notion. You can go into whole sets of data structures like textile threads, which are threads of a bunch of information and kind of like a database kind of view into a lot of the various little data structures in an application being added over time and versioned in kind of a large graph.
00:39:35.108 - 00:40:51.120, Speaker B: So all of those things get addressed by these CIDs. You get the benefit of kind of the duplication all over the place because whenever some things have the same CID, then you get all this duplication that happens thanks to kind of the content model. And so now the way that IPFS works for kind of retrieving the content, sorry, it's spinning wheel is that these nodes maintain a collection of content and then that collection of content is able to be distributed to other parties. And that happens over with a protocol called Bitswap, normally in IPFS. So let's see if I can find if that description is here, maybe yeah, there we is. Bitswap is kind of a protocol where different parties advertise the hashes that they want, the CIDs that they want. And if there's kind of like a peer to peer match, then an exchange will occur where parties will send the content over to each other.
00:40:51.120 - 00:41:21.784, Speaker B: And this is what's in IPFS today. And so some of the you can think of different parties maintaining these want lists. They advertise their want list to each other. They kind of establish that there's some set of content that one can send to the other and so on, and the content blocks themselves are moved over. And then after that, then kind of the want is already satisfied. So then that is no longer there. That's kind of a very high level view of Bitswap.
00:41:21.784 - 00:42:07.480, Speaker B: There's a lot more details into it, into how do you maintain these long want lists. As the amount of content you have increases, that gets a lot more complex. But one very straightforward problem with this version is that Bitswap, the kind of version of Bitswap that exists now, only does it at kind of one layer, right? So when you're requesting one, you're requesting one piece of content at a time. So if you're going over a large graph where you have to retrieve one piece of content, look at the content of that, and then kind of move your way down, you don't get the ability to kind of make that a single request. You're going to suffer from kind of these round trips and you're not going to saturate the download link. And that's what a graph sync is for. So graph sync is another protocol.
00:42:07.480 - 00:42:56.616, Speaker B: We'll talk about that in a bit. But kind of what that is about is using IP selectors, sorry, ipod selectors. Let's see, this is here. So the idea there is that when you have these data structures in these graphs, because you understand kind of the graph structure, you can use a path expression to describe the part of the graph that you're interested in. And it could be the whole graph, but it could be just a subset of the graph and you're able to express that in a request. And so in your retrieval request, you can say, hey, I'm interested in this part of the graph, and then only get the retrieval for that part. And now Graphsync is going to make its way back into Goips at some point.
00:42:56.616 - 00:43:43.880, Speaker B: And JS IPFS. It's already in Lotus now. So you can look at the Falcon specs as well, and you can kind of see how the kind of retrial or practicing works in just an area. Here, let me find it. That kind of goes into kind of how the requests are meant to work. So you can reason through the entire flow of how a graph Singer request works and also how a graph Singer requests within how that works within Lotus. And the way that we built this out is that it includes the capability for thinking about the capability of thinking about different authentication tokens being passed around.
00:43:43.880 - 00:44:49.666, Speaker B: And those authentication tokens could be payments in a payment channel, or it could be some other authentication token. But the idea there is that there's, like, a straightforward way to plug in different ways of thinking about how to authenticate that request to kind of enable a much faster retrieval flow. So Graphsync is, I think, the way that I would recommend kind of building out retrieval in FalcoIn, where you can reason about the graphs, the subset of the graph that you want to get and that it is already as part of the libraries there. Now it just means pairing that with, say, gossip sub to then create some kind of larger mesh of where we can advertise the content. So I'll pause it now and hand it over to Hannah. If Hannah's here, I guess maybe she's not here. 1 second.
00:44:49.666 - 00:44:55.210, Speaker B: Let me ping Hannah and see if she's around activity.
00:46:00.540 - 00:46:06.760, Speaker C: Would it make sense to just skip over to the next chat while we're waiting for Hannah to join?
00:46:07.980 - 00:46:23.980, Speaker B: Yeah, maybe. The only constraint is that probably the other ones would make a lot more sense if kind of after Gotcha, maybe, I think thought that the times in the doc were in PSD as opposed to East Gotcha.
00:46:24.960 - 00:46:26.930, Speaker C: All right, we can hang out for a few minutes.
00:46:27.380 - 00:46:34.630, Speaker B: Yeah. If there are any questions so far on what I presented. Definitely. Yeah. Why don't we do Q A for a bit and then give some time?
00:46:37.960 - 00:46:39.380, Speaker C: Yeah, go ahead, Jay.
00:46:40.200 - 00:46:45.190, Speaker E: Juan, would you elaborate on the term state channel?
00:46:45.720 - 00:47:34.920, Speaker B: I'm not quite sure what you mean by that. Yeah, so a state channel is kind of like a more generalized construction on top of a payment channel or kind of beyond a payment channel, which allows the ability to reason about a lot of different kind of state being able to be updated off chain. So it's some construction for defining and declaring regions of state that are able to change in an off chain way without requiring committing to the blockchain in one go. And highly recommend. There's an awesome project called Fake Channels you should check out.
00:47:47.470 - 00:47:48.902, Speaker C: Hey there, Santa.
00:47:49.046 - 00:48:10.050, Speaker D: Yes, hi. I'm very sorry to everyone. I realize I'm supposed to be presenting right now no worries. In the middle of trying to cut a release for Filecoin, and somehow it didn't get on my calendar, and I got the time zones. So I'm really sorry to ask.
00:48:10.120 - 00:48:11.780, Speaker C: No worries. You're here.
00:48:12.390 - 00:48:20.242, Speaker D: This presentation is going to be a little bit on the route side since I'm mostly improving. It cool.
00:48:20.296 - 00:49:19.560, Speaker B: That's good. And if you want to meet, I can give you a very quick summary of kind of what I presented through. So I kind of gave a very large overview of just payment channels in general. We walked through kind of how the Internet and the cloud works today just in kind of how the grapevine works and so on and how clouds work and how CDNS deploy that all the way to the edges. Then talked about how as we get then kind of like the structure of kind of retrieval and storage miners and the distinctions there, storage miners being kind of large scale operations and then kind of retrieval miners meant to be much closer to the edges. I gave a bit of an overview on Ipfscids and bitswap and graphsync a little bit but feel free to not in enough detail to be super useful. Mostly just kind of described how bitswap works and kind of how graph sync uses electros instead.
00:49:19.560 - 00:49:40.366, Speaker B: And then kind of described, hey, the kind of like retrieval V zero and V one are now part of the markets module inside of Lotus. And then kind of all of that code is there to be used by other folks building retrieval miners. And that's kind of where I left it. I haven't given an overview of gossip, so I'm going to be doing that later for sure.
00:49:40.468 - 00:49:49.120, Speaker D: Yeah, sorry, I already have a question Mpol and a sign message invoking the click method in the payment.
00:49:49.490 - 00:49:50.866, Speaker E: Sorry, we were doing a Q and.
00:49:50.888 - 00:49:51.460, Speaker F: A.
00:49:53.190 - 00:49:59.640, Speaker E: I don't know. No, I think you should do your presentation. We were doing a Q and A right before you joined and I'd been okay, got it.
00:50:00.730 - 00:51:37.140, Speaker D: Yeah. My thought is I would sort of do a little bit of an in depth on how the current software works or to the extent that I can kind of COVID it conceptually, I don't have a slide presentation, so I may probably do a little bit of talking and do a little bit of maybe some showing of code, maybe a little bit of looking at the filecoin spec, which is a bit out of date, but still gives you a decent overview. And then yeah, I think that's probably how this will proceed. Why don't I just go ahead and try to do my best overview of the current software and the different components, how they work and how you might use them in a future retrieval only minor. So the current software, I guess I would call retrieval V one, V zero is largely no longer in use. That was an earlier we originally when we designed it, we were kind of trying to design an incremental model for implementing this and we built this V zero version and then over time built a better version we call V one and we've replaced and removed V zero. And it's actually going to be leaving the file coins back at some point in the near future.
00:51:37.140 - 00:52:05.760, Speaker D: So we'll just call it retrieval v one, and I'll kind of go over how it works. So the basic concept of retrieval is that and this sort of probably goes to stuff that juan has already talked about, is that retrieval is a largely off chain transaction. The negotiation of a retrieval deal.
00:52:07.570 - 00:52:07.934, Speaker A: The.
00:52:07.972 - 00:53:22.998, Speaker D: Sending of data, and much of the payment for a retrieval deal occurs off chain. The on chain component is this concept of the payment channels, but the payment channels are specifically set up as a mechanism by which you can they're essentially an on chain mechanism for doing a bunch of stuff off chain and then submitting it to the chain later. That would be like my super very basic what I understand the payment channels to do, as someone who's largely learned them. As I've written, retrieval the payment channels are a mechanism by which you essentially set up a you put a bunch of funds in kind of an escrow, and then you make a bunch of transactions which you can submit to the chain later. You don't have to submit them in the middle of making those transactions. And that allows you to do sort of all the elements of retrieval without stopping a bunch of times to put stuff on the chain, which obviously carries a kind of time cost to it because everything is largely off chain. The essential mechanism of building, of doing trust for retrieval v one, is this thing I would call incremental trust.
00:53:22.998 - 00:54:32.694, Speaker D: Essentially, retrieval is done. The basic method is we agree to an overall transfer and we agree to parameters about that transfer. And then the provider sends me a little bit of data. And then they say, I'm not going to send you any more data until you send me the payment that we previously agreed to, the amount of for this portion of the deal, and then I send you that. And then they send any a little more and you essentially have this back and forth all the way to the end of the overall data you're transferring. There's a little bit of uncertainty around the last piece. This is always a problem in this sort of scenario of how do you ensure trust? My hope is that a lot of these amounts are going to be pretty small, so the net value of absconding with the last bit of data without paying is not super high, and there's other mechanisms in there to prevent a lot of fraud.
00:54:32.694 - 00:55:44.590, Speaker D: But in any case, that's the basic way you do trust and then the way it actually works in terms of the components that are involved. At this point, what I would say is like three protocols, but they're all sitting on top of each other, so that effectively there is really only one underlying protocol that most retrieval is happening on. But stacked on top of that are two other things. We have payment channels are on chain component, and then we have the retrieval protocol, which is like how we're negotiating this deal and how essentially I'm reading your request for payment and I'm checking whether they match up with the parameters we agreed to in the deal. And then I'm sending you a payment and you're checking out whether that payment matches what you're expecting for the deal. That sort of conversation. So that's the retrieval protocol, and that is sitting on top of the data transfer protocol.
00:55:44.590 - 00:56:39.010, Speaker D: So the data transfer protocol is essentially effectively like an abstract protocol for moving data from one person to another without a specific clear understanding of what the underlying transport mechanism is. Now, this is like a very wide abstraction. The idea of an abstract way of transferring data could mean anything from our standard stuff like Bitswap and Graphsync to theoretically like sending hard drives through the mail. Right? So that's a very wide abstraction. We're still trying to figure out if we can really maintain it. But the data transfer protocol abstracts away the underlying transport mechanism and piggybacks upon that. So the underlying transport mechanism of retrieval is Graphsync.
00:56:39.010 - 00:57:27.950, Speaker D: So Graphsync is the actual network protocol via which data goes back and forth across the wire. Well, actually, I guess underneath that is Lib P to P, which goes to TCP, whatever. But in any case, graph sync is essentially the Lib P to P protocol that things are communicating on. There actually is some mild use of a separate mostly for the initial negotiation of a data transfer Lib P to P protocol. But there is actually no Lib P to P protocol for the deal itself anymore in retrieval. It is all over data transfer. And me, let me rewind.
00:57:27.950 - 00:57:30.274, Speaker D: Okay, so how does all that work?
00:57:30.312 - 00:57:30.466, Speaker A: Right?
00:57:30.488 - 00:58:51.006, Speaker D: Because that sounds very like while you're piggybacking upon piggybacking. So Graphsync and I don't know if Bitswap has this functionality as much, but Graphsync has some very specific mechanisms for embedding other information in a request that can be essentially an auxiliary or what I'd call a side protocol. So when I send a graph sync request, I can embed in that request essentially a packet that will get decoded on the other end, assuming I know how to recognize that packet. And then it can be read and processed by essentially you can essentially register hooks with Graphsync that know how to process specific extensions to the protocol. And so in this case, there is an extension sent over Graphsync that is for it's actually the data transfer protocol extension. And then that gets unpacked by a hook that data transfer, the data transfer module is registered and then that becomes a data transfer message. And inside of that data transfer message is a retrieval message that the data transfer module actually retrieval is registered with data transfer, and it knows how to unpack that message and get the retrieval message out in any case.
00:58:51.006 - 00:59:56.820, Speaker D: So it's a lot of funkiness. But it's cool because essentially the upside of all this is that right now and this is more for the future right now, we do all of this with my software for retrieval, which uses payment channels. There is probably some future version of this that could use state channels. And the nice thing is, because it's all like, each protocol supports putting stuff on top of it, you could write your own retrieval protocol to use state channels and then essentially just run it on data transfer the same way that the existing retrieval software does. Anyway, sorry, I realize it's sort of like, oh, look at this. This is cool. But I'm only bringing it up because realizing that we could replace this with state channels and not change the underlying thing, it finally made doing all that feel worth it, which was largely what retrieval v one was is making all these layers work on top of each other.
00:59:56.820 - 01:00:12.950, Speaker D: In any case, I don't know if any of this if this is all sounding like none of this makes a whole lot of sense, I'm giving, like, a super high level overview, and it's probably totally going to take questions later. Yes, juan, you have a question?
01:00:13.100 - 01:00:21.130, Speaker B: Yeah, it might be useful to just even as you're describing it, if you're kind of just browsing around the code bases. Like, if you just going to the.
01:00:21.280 - 01:00:28.400, Speaker D: Yeah, let me show you guys some of that. I think I should be able to share my screen.
01:00:30.610 - 01:00:31.530, Speaker B: Oh, my goodness.
01:00:31.610 - 01:01:26.174, Speaker D: What am I going to hmm. Hold on 1 second. I think I'm going to use chrome and look at GitHub because that's just easier because I can keep it all in one window that way. Is everyone able to see my currently just show so actually, I just want to talk a little bit about I'm going to show you guys grasping first, just so you all know how that thing works. And it is like I would say, for transferring IPLD graphs, I don't know if all the retrievals we're going to be doing are IPLD graphs, but for transferring IPLD graphs, I would say if you're doing a point to point transmission, it's probably the best mechanism you have. Right. And the way that graph sync works, I think juan probably explained this.
01:01:26.174 - 01:02:17.022, Speaker D: So when I'm transferring an IPLD graph, I have a couple of requirements about how I want to transfer it. I have a notion of a root of that graph. I know a CID that identifies the root of that graph. And I want to be able to transfer the graph, but I want to be able to do it in such a way that I know I'm getting the data that I originally I want to be able to verify that the data I'm getting back is the graph that I expected. So Bitswap's way of dealing with that is to know your graph is made up of a series of blocks with links to each other. And Bitswap's way of dealing with that is to essentially request one block, get it, and then look at it, verify that it matches the CID, and then request the next block. Right.
01:02:17.022 - 01:03:29.622, Speaker D: The downside of that is that you're essentially doing a whole bunch of round trips because I got to do the top layer and then the next layer and the next layer and the next layer. And so particularly when you have graphs that are not wide but very deep, there's a lot of internet round trips in the mix. The way graph sync works is you start with the root and you start with this selector, which is essentially an expression of what kind of graph below the route you want to get. In most cases, a lot of the time, what you're trying to get is the whole thing, and there's a selector for the whole thing. So in any case, what that means is I essentially send a request to you, the other party, and I say I want to get the root and I want you to also send me everything that you get when you apply this selector to the graph below me. So the remote party starts with the root block and then runs the whole selector and gets the whole Dag and sends it back to you. Now, since you haven't done this series of round trips, when you get all that data, you actually still need to verify it.
01:03:29.622 - 01:03:59.170, Speaker D: So the way graphsync works is it runs the selector locally with the data that's coming back in to essentially verify that it matches the selector. There's a bunch of tricky things in the mix with that. If, like, for example, the other party is missing part of the graph that you asked for, you need a way to deal with that. You also may want to deal with the fact that you already have some of your own data. Yeah, sorry, hold on 1 second. Let me zoom in. Is that readable? I can do more.
01:03:59.170 - 01:04:24.694, Speaker D: How about now? Somebody. Trent Vaness. Yes. Yeah. Okay, cool. So let me just show you I'm just looking at the top level file and I'm showing you essentially this thing called Graph Exchange, which is the overall interface for graphsync. And it's actually gotten a little bit longer than it initially was.
01:04:24.694 - 01:05:20.460, Speaker D: Initially, this just had like a request method. But now we have all of these hooks and these may actually move out of the core interface at some point because it seems like not everyone who implements graphsync is going to implement all of these things. When you initialize an instance of graphsync, your main method is this request method. You take the person you're requesting from your root and your selector, and then you. Can see you can pass in a series of extensions, and this is where you would pass in essentially the encoded data for a higher level packet that you would want to pass in with the request. It's going to return, essentially a channel where the progress comes in, a channel for errors. Actually, we've thought about changing that because obviously double channels can be a little bit funky, but fortunately, neither one is blocked on the other.
01:05:20.460 - 01:06:12.374, Speaker D: Forget I'm getting too in the weeds here. In any case. So that's the method. And then you can see you have all of these hooks, which essentially you can actually hook into almost every part of the request. This stuff right here, this persistence option, allows you to change the source block, your locally stored data you're using for requests and responses. This is super useful in filecoin because we're always wanting to put our blocks in different buckets and stuff. You can respond to incoming requests, outgoing requests, and each hook has a structure to it where it largely gets just as an example, like an incoming request, it will get the peer, the entire request, and various things it can do with that request.
01:06:12.374 - 01:06:42.694, Speaker D: That's his hook actions. And you can see if you look at the hook actions, you can oh, my God, where is it? You can respond with extensions. You can change the persistence options. You can terminate it. You can actually pause it right as you start. Another thing that Graphsync now does is it does a whole bunch of pausing and unpauseing options. So I can take any request that is in progress and stop it and resume it.
01:06:42.694 - 01:07:48.410, Speaker D: I can do this on either side. So if I am responding to a request, I can pause it and essentially tell the other party like, I am not going to proceed until I'm not going to proceed until you well, I can tell them that it's paused, and then I can send an extension back that tells them essentially, here's why I'm paused. Here's what you can do to make me unpause. If that's the responder, the requester can actually pause and unpause. It's quite a weird underlying operation because the way it's implemented is we actually cancel requests and then create new ones where we tell the other party not to send all the first bits of data in any case. So Graphsync is sort of like for point to point transmissions. It's a very controllable modifiable protocol where you can transfer large bits of data and have a lot of control over how that operates.
01:07:48.410 - 01:08:08.900, Speaker D: You may or may not work with this directly. If you're doing anything retrieval related, you most likely will want to work with data transfer. Anyone mind if I go on to data transfer? Or if you all have any more questions before I go on to it?
01:08:10.950 - 01:08:11.458, Speaker B: Good.
01:08:11.544 - 01:08:41.974, Speaker D: Okay, I'm going to go on to data transfer unless anyone objects. Cool. So sorry. Graphsync is in IPFS. Data transfer for now is in filecoin, so FYI. Okay. And data transfer as it relates to graphsync.
01:08:41.974 - 01:09:49.060, Speaker D: For right now, there's a notion that we will support other transport mechanisms in the future. But for now, data transfer is effectively what I would say it is like a nice abstraction on top of graphsync that will put things in much more semantic terms for your use case for the use case of retrieval and probably is just what you want to use. The one other thing that data transfer supports, which Graphsync does not support, is push requests. Push requests, meaning rather than I request you to send me some data, I actually contact you and say, I want to send you this data. That's probably not likely to come up as a use case in retrieval, it's more of a storage use case, because in the storage case, we actually are like, we want to send you this data so you can store it. But in any case, that is an additional feature of data transfer that's not in Graphsync, and it's just done by adding a few extra steps to the sort of like, negotiation. Okay, so let me just look at data transfer.
01:09:49.060 - 01:10:57.800, Speaker D: Oh, sorry, one other thing I didn't cover in Graphsync that's a security thing that's probably relevant is that the one thing about Graphsync is you have to be careful about which requests you're willing to serve. Because we're not sending back just a single block. We're actually processing an entire graph query that carries like a potentially large CPU cost and or disk cost on the side of the person. So the way graphync works is it takes certain types of limited selectors. It will serve to anyone, though, in filecoin that's actually turned off completely. So the filecoin instance of Graphsink will not serve any request it does not recognize. And then for other requests where you want to have a more unbounded selector, you use one of these hooks to essentially, it's almost like looking at a cookie in a web request where you look at it and you're like, oh, I recognize this, and so therefore I'm going to serve this large selector to this person.
01:10:57.800 - 01:11:28.414, Speaker D: Okay, sorry, that's a sidebar. It becomes relevant in data transfer. So in data transfer, essentially you start requests by opening a push or pull data channel. It takes largely the same parameters as Graphsync, though you don't actually have to, though the thing you encode is very specific. So you have your context, your other peer, you have your root, and you have your selector.
01:11:28.462 - 01:11:28.626, Speaker A: Right.
01:11:28.648 - 01:12:09.440, Speaker D: So that's all very familiar. The other thing that data transfer has you put in is this thing called a voucher. And a voucher is an encoding of some kind of data that indicate that the other side will be able to decode and know something about. Right, and know how to process. As an example, when you're doing retrieval, in retrieval, everything is done over this. So the Voucher is actually when you open the way retrieval works, it starts by opening a poll data channel. Poll means I want the other party to send me data.
01:12:09.440 - 01:12:43.530, Speaker D: And the Voucher is actually an encoded retrieval deal proposal. So you take the retrieval deal proposal, you put it into this thing called a Voucher, the Voucher, and you send it along with this data transfer request. And Vouchers, from the standpoint of data transfer, they're very simple data structures. I wonder if GitHub will actually let me oh, wow. I love that they oh, no, they're not quite there. I thought, like, GitHub would allow me to just jump right over to the definition of that type. It's going to be over here.
01:12:43.530 - 01:13:11.314, Speaker D: Sorry. GitHub keeps getting more sophisticated, but they're not quite there. Right. So a Voucher is a really simple type. We don't know any from the standpoint of the data transfer module, we really know nothing about it other than it has this thing called the Type, which returns a type Identifier, which is a string. This is only relevant because we need to be able to take and it is encodable, meaning it can be serialized to bytes. And there's some code in here to deal with this.
01:13:11.314 - 01:13:32.620, Speaker D: But the way it works at an actual over the wire level is you take this string and you write it into the message, and then you take the Voucher and you put it in bytes and you put that in the message. And then on the other side sorry, hold on. I realize there's a lot of steps here.
01:13:34.750 - 01:13:35.434, Speaker A: Where are we?
01:13:35.472 - 01:14:26.842, Speaker D: Manager sorry. On the other side, you essentially register types of Vouchers that you know how to process when they are coming in, in the case of retrieval. And we actually have currently two different Voucher types registered for our stuff. Right. We register a storage Voucher type and a retrieval Voucher type. And essentially, you just have to pass it an instance of the Voucher using a little bit of a reflect weirdness internally, but it's not particularly not so worried about the reflect usage because it's basically fairly not used a lot. It's not very speed dependent anyway.
01:14:26.842 - 01:15:28.030, Speaker D: I'm going to just hand wave that. And then when you register a Voucher type, you have to register this thing called a Request Validator. What the request validator is going to do is it's going to take an incoming request, it's going to inspect the voucher, and because it knows how to deal with that voucher type, it's going to look at that voucher and it's going to say, based on what's in here, this is or isn't a valid request for data transfer. In the case of retrieval, the retrieval provider is going to take that Voucher, which is a proposal. They're going to inspect the parameters of that proposal. They're going to say, this does or does not meet my requirements for serving this retrieval deal, assuming it does, they're going to say, go ahead with this data transfer request. They can also when they do that, there's a couple of things they can do.
01:15:28.030 - 01:15:53.650, Speaker D: I'm going to actually show you what the validator looks like. The validator has two different methods. They're just for push and pull. And this is relevant because like, for example, in the case of the retrieval validator, it actually never validates push requests because retrieval doesn't accept push requests. It's all about requesting data and having you send it to me. So anyway, for the poll, it returns two values. It returns an error.
01:15:53.650 - 01:16:36.770, Speaker D: If there is an error and there's a special value here which you can do, that is a valid request, but you can return essentially error pause, which means it's a good request, I want to serve it, but I also want you to start it paused. In any case that's relevant for retrieval for various reasons, you may find it useful. It also can return this thing called a voucher result. And a voucher result is basically the exact same thing as a voucher. But going the other way. It's basically just encoding any additional data about why I accepted or didn't accept your request and sending it back to the client. This is actually something that probably wasn't in the original design but turned out to be super useful for communicating back and forth over retrieval.
01:16:36.770 - 01:17:53.870, Speaker D: Okay, so that's how you negotiate a data transfer. And then in the case of retrieval, we also need the ability to essentially so in the case of retrieval, I mentioned this earlier, the way it works is we send a little data and then we send a little data, and then we ask for payment and we pause it while we ask for payment. And we don't unpause it until they send us payment in the form of a voucher, which is not actual file coin payment until it goes on chain. But that all happens after the retrieval deal. So the way that actually is implemented in data transfer is this concept of a revalidator. And essentially what this does is a revalidator, when it's registered, will get a chance as data is being sent, to essentially pause it and say or you can actually pause it or technically terminate it, though we don't use that functionality. But you can pause it or terminate it and you can send additional information to the client in the form of a voucher result as to why you decided to pause the request.
01:17:53.870 - 01:18:56.180, Speaker D: And then the client has to look at that and they need to then construct a new voucher to revalidate the request. In this case, it is a retrieval. It's essentially it's called a deal payment, which is a thin wrapper around a payment voucher. And then they actually send it back to you by calling this function send voucher. And then when you get that voucher, this method on the revalidator called revalidate gets called and they look at that and they can use that and assuming it's valid, they can unpause the request. So you can see you have this sort of like abstract the data transfer is this abstract layer for negotiating a transfer that doesn't speak about the actual terms upon which you're negotiating. You just have a mechanism for essentially for doing all the different parts of negotiation, including the initial negotiation and if you want a series of renegotiations along the way.
01:18:56.180 - 01:19:33.770, Speaker D: So that is how all of that works. Data transfers also have pausing and resuming. We don't currently have it set up. We have a bunch of the machinery in place to cause it. So if you shut down your node and restart it, a data transfer would automatically resume. A lot of the machinery is in place but we have not yet actually enabled that because there's a few other things that we need to do that I hope will probably ship before main net though we've got a lot of feature requests. We got to get other feature requests.
01:19:33.770 - 01:20:05.940, Speaker D: So that is essentially how the data transfer layer works. I don't know how much more time I have. I have a little bit of I think I have till 1120. Is that right? Yes. Though I maybe want to maybe let me just briefly pull up retrieval. So just to say that these building blocks are here, whether or not you use the retrieval itself. Let me pull up market.
01:20:09.510 - 01:20:11.020, Speaker B: Let it.
01:20:22.030 - 01:21:07.622, Speaker D: Actually bring up the client. Yeah, this is just I'm just pulling up the retrieval client interface. This is sort of the top level interface for using the existing retrieval software. So all the things that use data transfer goes into this method which is retrieve. And retrieve is essentially the main method for running a deal. However, before you make a deal you need to identify a retrieval provider and you probably want to query them for what the deal parameters they are asking for are. And that is relevant in order that you can propose a deal they're likely to accept.
01:21:07.622 - 01:22:16.546, Speaker D: And right now for the most part, if you meet the parameters that are in what they respond to the query, the high ODS are that they're going to accept it. This find providers is essentially your discovery of other retrieval peers. I want to be 100% clear that method is very limited in the current version. Later you're going to hear about a gossip protocol for maybe finding things a little more efficiently. This thing will only return things you personally have made storage deals for. So it's not super useful in its current form and that's to some extent okay because we are working on some mechanisms for improving it with you cannot currently find things to retrieve with the chain only unless you personally know the payload CID of what you want to retrieve and the so called piece site CID. There are two different I'm not going to go into payloads and pieces just yet.
01:22:16.546 - 01:22:50.182, Speaker D: But that is a relevant thing to understand. So query again. It essentially takes the payload, the peer you're talking to, this payload, and parameters. Parameters also can specify this other thing called the PCID. Actually, I think I need to rewind and talk briefly about payloads and pieces, which is a bummer because it's a complicated concept. It'll take a second. So in Filecoin, PCID refers to the actually, let me just start from the beginning.
01:22:50.182 - 01:23:44.190, Speaker D: So let's say I have a file, like a system file, like catpix GIF, right? And I want to store it on Filecoin. If I'm starting from a system file as opposed to existing IPLD data, the first thing I'm going to do is convert that into an IPLD data structure. We call this structure the payload I'm going to use. In the case of a system file, I'm going to use a system called Unixfs, which is built into IPFS, which will take the system file and convert it to an IPLD data structure that can live in IPFS and is a data structure we can put into Filecoin. However, Filecoin does not when miners actually store data, they do not actually store it as IPLD data. They store it as these things called pieces. And pieces are essentially flat bytes.
01:23:44.190 - 01:24:24.902, Speaker D: They're essentially a certain amount of bytes that we can build a merkel tree around that indicates something about what the underlying bytes are. So the way that happens, I start with a payload and the root of that payload. The Identifier for that payload is called the payload CID. And then I need to serialize it to these bytes on the other end. And there's a reason we can't just take the original cat picks GIF and serialize it. There's a lot of reasons. But in any case, the way I'm going to serialize it is I am going to build what's called a car file, which is a serialization of an IPLD graph.
01:24:24.902 - 01:24:57.950, Speaker D: And then I do a bunch of other stuff. I pad it and then I calculate this merkel root based off of the underlying data. I'm just going to do a little bit of hand waving and that is going to give me a thing called the PCID. The PCID is a unique identifier for data that is stored in Filecoin. It is sometimes referred to as compi. But for the purposes of discussion, we have a PCID. So the reason this is all relevant, what is on chain is the PCID.
01:24:57.950 - 01:25:33.200, Speaker D: Currently, the payload CID is not on chain. There is probably no plan to require it on chain. There is a plan potentially to put it in a sort of like notes field in the pieces on chain. And the reason for that is so that we can potentially index payload CIDs that exist in the world. And this is important for what you guys are doing, especially if you're building any kind of retrieval index. So that is a thing that will eventually probably appear in the software. We do not currently store it in the notes field, but that is a thing we may do.
01:25:33.200 - 01:26:10.170, Speaker D: Okay, sorry. The reason that is relevant why do you need payload CIDs in order to serve a retrieval? Payloads retrievals are sent again over Graphsync. Graphsync is an IPLV data format and in order for retrieval to work, we need to be able to verify the data we are getting incrementally. We do not currently have a format for doing that with pieces alone because again, pieces are flat bytes. We don't have an easy way to work with them directly as IPLD data structures. So we need to transfer payloads. It's also helpful because payloads are much smaller, but that's sort of a sidebar.
01:26:10.170 - 01:27:02.778, Speaker D: So when I make a deal for a retrieval, I have to know the payload CID. It is also useful to know the piece CID because the payload CID could be a payload CID is not necessarily indicative of the underlying piece, which may be where the actual content of the piece CID may indicate something about where it's stored, how it's stored, like what stealing method it's stored with. It matters for various reasons. In terms of reliability, it is better if you know both and make a retrieval deal with both. You can determine the PCID from the chain, but you cannot associate it. Right now we don't have an easy way to associate the two. However, if you've made the deal, we will have both on record.
01:27:02.778 - 01:27:48.950, Speaker D: In any case, it's a sidebar and a digression that probably won't seem super relevant until you get super into writing this software and it may come up in any case, when you do this query params, you do have to pass this halo CID. We can't do a retrieval without one, but you in the query params probably also want to pass the PCID, which is part of the query params but is technically optional. We will serve a retrieval without the PCID, but it will be a slightly slower potential process and it is a little bit more error prone. Long story in any case. So here is the retrieve command again. It takes payload CID parameters. And this is again like the query param struct.
01:27:48.950 - 01:27:56.666, Speaker D: The structure for these is in the types file, but they have both the oh my God.
01:27:56.688 - 01:27:56.986, Speaker A: I see.
01:27:57.008 - 01:28:24.290, Speaker D: I'm overtime. The parameters have both like this. Some information about the piece. It also has all the payment parameters. That includes how much I want to pay per byte for this deal and how often we've agreed that we are going to request incremental payment. So that is a thing that both parties can agree on. The provider can also optionally set a price to unseal.
01:28:24.290 - 01:28:54.800, Speaker D: And that is because unsealing if they have to unseal is an expensive process. It's a CPU. Expensive process. So that's a thing that they can set if they want on pieces that they don't have unsealed data for. You're also going to pass the addresses. This miner is obviously the other person you're talking to. We now take this thing called a multi store.
01:28:54.800 - 01:29:43.006, Speaker D: Retrieval can ultimately write to traditional IPFS block store or they can write to this thing called a multi store, which is basically a way of isolating the data that you're retrieving. So you put it in a place where you know the only thing in that block store is the result of this retrieval. I'm not going to get too deep into that. Again, these are the basic functions. The way you know what's going on with your deal is you subscribe to events. This will send you basically a torrent of information about what is going on with your deal. And the simplest thing you can do if you just want to know when it's done or that it fails, is you can wait for a finish event or an error event.
01:29:43.006 - 01:29:58.980, Speaker D: And that's how the code in Lotus works with it. I'm going to skip these. I think I'm over time, since I've gone all over the map over time, I'm going to just take questions.
01:29:59.430 - 01:30:17.674, Speaker B: Yeah, I think it's a bit fine to take a bit longer depending on questions. My sense is that folks need precisely this kind of in depth view into the code and the interface and so on. And I know that some other folks later in the program won't need a lot of time. So I think it's fine for you to take questions and maybe direct based.
01:30:17.712 - 01:31:17.838, Speaker D: On that for sure. Yeah, I'm going to go ahead and just stop and take questions because I think if I keep talking on my own, I'm going to lose my train of thought a lot more because I'm running out of mental energy for it. So questions about any of the things that we have talked about, let me rewind. I'm going to just look at what we've got. Hannah you mean adding a gossip protocol to Lotus? That I believe is the goal eventually in terms of well, actually Juan was not in this meeting, so he may overrule. But for Mainnet, we may not get the gossip protocol in at the point we release mainnet now, it's not a chain breaking anything, so we can always release an update to Lotus with this in without doing anything. It's not a hard upgrade to add it.
01:31:17.838 - 01:31:54.200, Speaker D: We are trying to nail down the core feature set. So that part is that's kind of our priority of making sure it's all stable and people can make deals on our network that we're going to launch into the world. So we'll see where that lands. But yes, there is a hope that we will get it into Lotus eventually. I think that we also could I don't know if we're going to talk about this, but there's a notion that I have in my head of a retrieval minor software package that it might maybe only go in there.
01:31:58.270 - 01:33:09.810, Speaker B: I think there will be a lot of tools that work externally with Lotus or around Lotus or separately, completely independent, that might use gossip sub or might use other things to do all this retrieval. So there will definitely be a lot of retrieval minor software that does not run Lotus. It might take pieces of Lotus, it might take pieces of the libraries like the Gofill markets components and will be built independent processes. And then in terms of gossip, I believe right now the blocks themselves and the messages are moving through gossip sub protocols. And so the Lotus does have the implementation embedded, but I'm not sure that it is exposed the same way that, say, on Guips, you can easily call one command and you can construct arbitrary gossip channels in Lotus. I believe that that facility is not there because given that the gossip sub mesh is being used for the critical function of moving around blocks, don't want people arbitrarily adding a bunch of other channels and passing them through. But it might be relevant to just allow you to do that if we can isolate the meshes.
01:33:09.810 - 01:33:34.170, Speaker B: Cool. Any other questions for Hannah?
01:33:36.990 - 01:34:17.510, Speaker E: Yeah, I had one question from earlier. Hannah, if you just send a message using Mpool push to Lotus node to collect a payment channel, like, you just invoke that it's like method number four, I think. Yeah. Will the whole specs actors thing handle all of the sending whatever fill needs to be sent, either as a refund or as the final payment? Does that just happen automatically? Or what I'm trying to ask is, are messages that come in from Mpool push treated differently? Somehow.
01:34:20.110 - 01:34:57.990, Speaker D: I believe that they're not treated differently. You should know that there's now a direct API method for calling collect in Lotus next. I don't know. I'm trying to think about this generally, if there's an API method for doing the thing, probably better to just call that, which I realized collect didn't used to exist. There's now two methods. One is settle and the other is collect. But the actual transfer of funds will happen in spec actors as a function.
01:34:57.990 - 01:35:45.860, Speaker D: I mean, in assuming you've called settle, you've passed the settling height. It should just run. I've never tried it directly, but in theory, I guess the note, if it exposes Mpool push over the know, you can definitely send anything to the chain that way. The one reason with payment channels, if you're using Lotus and you're not implementing your own off chain payment channel management, you may want to use Lotus's payment channel methods because Lotus has a bunch of software for managing payment channels off chain. That software kind of sucks right now, but we are in the middle of making it a lot better.
01:35:47.610 - 01:35:48.470, Speaker B: Okay. Yeah.
01:35:48.540 - 01:36:02.422, Speaker E: Our particular project can't use the Lotus payment channels APIs because we retain the user's private signing keys on the user's machine and there's no Lotus there. I'll talk about it more later. But thank you. That's helpful.
01:36:02.486 - 01:36:18.880, Speaker D: Yeah, that's helpful. Yeah. That's actually a good point, though. That might be an interesting thing that we should talk about internally, because when you call collect now on the API, it's going to look for a payment channel, and market off chain is.
01:36:22.770 - 01:36:23.520, Speaker B: Actually.
01:36:24.210 - 01:36:51.420, Speaker D: A good use case that we should think about in terms of how we write those APIs. I think it would be fine to just defer to Mpool push. We just want to interact with the actors. Yeah. Other questions. All of that information was like, so clear and simple and obvious. No, it totally is not.
01:36:58.360 - 01:36:59.110, Speaker B: Anything.
01:37:00.760 - 01:37:04.148, Speaker D: How's the parts cool together, all those things.
01:37:04.314 - 01:37:32.530, Speaker B: Yeah. My guess is that people will kind of refer back to the video and then some more questions will emerge. And then if folks have questions working on retrieval, definitely just ask them in the retrieval channel. And Hannah's, there other folks are there, so we can definitely kind of answer them there. And especially as people kind of start using things like graphsync and the data transfer module and so on, I'm sure that there'll probably be a whole bunch of questions here and then for sure, yeah.
01:37:33.700 - 01:37:48.710, Speaker D: I didn't get into this, but there's an abstraction in data transfer to try to handle the possibility of other transport mechanisms. If anybody ends up trying to do that, they want to implement their own transport mechanism. Obviously we can talk about that.
01:37:50.680 - 01:38:08.496, Speaker B: Yeah. My impression is that there's so much awesome stuff in that module, it probably deserves its own whole talk just about that because there's a lot of different interesting ways of using it. And I think that whole thing eventually will make its way back into Goipfs.
01:38:08.628 - 01:38:09.790, Speaker D: Yeah, for sure.
01:38:10.560 - 01:38:11.310, Speaker C: Cool.
01:38:12.080 - 01:38:15.150, Speaker D: Yeah. Definitely in the future we can talk about.
01:38:15.760 - 01:38:42.710, Speaker B: All right, thank you. Thank you very much, Hannah. Let's check back on the schedule. So I just kind of adjusted some of the timing. What I'm going to do is I'm going to give kind of like a lightning fast overview of what Gossip Sub is just to kind of set baseline for folks. Because the next two talks after that are going to be describing how to do retrieval market stuff on top of Gossip Sub. And then that way there are those kinds of basics there.
01:38:42.710 - 01:39:22.210, Speaker B: And then I think then the talk after that from David and Elizabeth at ChainSafe, I think, won't require that all the original time. And we also have kind of open time at the end that can get squeezed a bit. Even with inserting this kind of ten minutes on Gossip sub, we shouldn't be too kind of off track in terms of timing. If folks talking later had a very strict kind of start times and end times, definitely just let me know on the Zoom chat and I'll adjust on the background to make. Sure that we follow that timing. Great. So I will share my screen.
01:39:28.120 - 01:39:31.510, Speaker C: And we do have a break slotted for after this talk.
01:39:44.860 - 01:40:31.544, Speaker B: All right, so this is going to be pretty lightning fast because I'm going to refer just back to a lot of the details will kind of become obvious through the next talks and because there's already a lot of really good material out there. So this is mostly just kind of an intro to what's there. So we've talked a lot about a thing called gossip sub. And what that is, is a specific implementation of a publish and subscribe protocol. And it's one of two in the whole Lipidop library and protocol, and it happens to be used already in filecoin and ethereum two and I think polka dot and others for moving around. Moving around information. So maybe give some pointers to roughly kind of like what the pub sub problem is.
01:40:31.544 - 01:41:17.800, Speaker B: I'll talk a bit about the cost of implementation and kind of what it's tuning for and then I'll talk about kind of how you might do a straw implementation version of how you might do retrieval requests over PubSub and then also how you might use that for indexing information. So there's a docs website for lipidop that has a really phenomenal kind of description of PubSub and how that works. I'll be using most of that for the talk. I just want to mention a couple other resources. First, there's a spec for Gossip sub on GitHub and you can go check it out. And this has a whole bunch of details. Gossip sub moved to 1.1
01:41:17.800 - 01:42:18.490, Speaker B: because we had Gossip 1.0 and then we found and fixed a whole bunch of really interesting security issues. Positive in general is an extremely difficult kind of protocol to make, robust in kind of a world with very sophisticated attackers and so on. There's been decades of work in this and most published ascribed protocols live within a fully trusted regime where there is no kind of adversarial parties inside. And so the whole process of trying to take pulse of protocols and use them outside in kind of the modern peer to peer and modern kind of blockchain environment where you have a lot of parties that might be kind of attacking those protocols, it's pretty new. And so there's all kinds of very interesting security questions, security questions there. And so we've done a bunch of interesting work here and I'll refer to it and I'll show a little bit of a snapshot of what that kind of work is.
01:42:18.490 - 01:42:49.044, Speaker B: But it kind of tunes for different things than, say, latency, which is why Gossip sub might be a good thing in the short term, but in the long term it might want to be replaced by a different pub sub protocol. There are a bunch of talks already on pub sub. I'll plug three different ones. One from Raul called Demystifying gossip sub. This was given at Defcon five. There's a link there, I'll post the links in Chat afterwards. I highly recommend just go watch the whole thing.
01:42:49.044 - 01:44:17.810, Speaker B: It'll give you a very kind of in depth understanding of how the protocol itself works, how topics work and so on. Then after that there's a talk from David talking about the hardening extensions that we added to gossip sub to make it good for Falcon and Ethereum two, this is specifically dealing with how you think about using a pub sub protocol for block propagation. And so block propagation is a very specific problem within blockchains, where you want some very hard guarantees around the kind of propagation time of the block across the whole network. And if parties in the middle can start suppressing the blocks or things like that, then those kinds of attacks can be used to break the consensus or to at least attack the consensus. And then there's another talk from Janice, which is kind of in depth in any detail about kind of this report that the team prepared around kind of all of the different extensions going into gossip sub. I think this one, it's not recorded online, we have the slides and I can happy to make this available, but I think this one will end up getting recorded and put online later on. So yeah, I'll use just the website, the Liptopi website, because that's just a very good description and you can refer back to it.
01:44:17.810 - 01:45:50.320, Speaker B: So if you go to the website, the Lipidop website, you can click on Docs, you land on this site and I think within concepts there is publishescribe and here this kind of goes in depth into the whole thing. So the basic idea is you have some kind of network of parties and you want to allow parties to subscribe to a specific topic, and then you want to allow some parties to publish messages and you want to make sure that all subscribers to that topic are indeed receiving that message. So you can think of that as there's some topic and you want the parties that are sending that message, or the publishers that kind of propagate a message out, you want the message to reach all of the other parties, subscribed quickly with certain kind of guarantees and so on. And the whole universe of Pub sub protocols trade off those guarantees and trade off performance and so on to achieve certain kinds of networks. This problem is pretty easy in the small scale, so when you're dealing with tens to hundreds of nodes, that's not a big deal. When you want to scale this to millions of nodes, or in networks that have high churn or that have security problems and so on, this becomes a very difficult and very interesting problem. And this is part of the peer to P, because a lot of applications in general use bubso as a primitive.
01:45:50.320 - 01:46:52.080, Speaker B: And when you start building peer to peer applications, and especially things in things like IPFS and things like blockchains. You end up needing this as a primitive in a bunch of different parts of the stack, but when you need it, different parts of the system might want to use different protocols, different underlying protocols, because the guarantee is required different. And so one of the goals of WPP is to kind of create this kind of really nice set of abstractions that allow application builders to swap out the underlying implementations to tune for the guarantee that their system requires. And so as an example, right now Lipidop has two such Pub sub implementations. One is called flood sub and one is gossip sub. Flood sub is just a very simple protocol that just floods the messages everywhere and that means it's very chatty. So it's kind of like think of it as a very trivial straw implementation of a Pub sub where all the parties that are subscribed to a single topic just propagated to everybody else, propagate messages to everybody else.
01:46:52.080 - 01:47:58.820, Speaker B: And that might be kind of fine in the smaller scales, but certainly as you grow to millions that the amount of kind of bandwidth overhead for using that would be pretty bad. However, it has good guarantees in terms of making sure that parties do get the message. Now, Gossip Sub, which is kind of the main implementation that I'm describing now, that I'm going to be describing now, is an implementation that kind of tunes for certain kinds of wants to reduce the bandwidth overhead, but wants to do so without kind of losing certain guarantees around message delivery and so on. And so there's a bunch of different kind of design goals when building these kinds of protocols from reliability, speed, efficiency and so on. And this whole kind of description talks about just the puzle problem in general. But at the end of the day you, you kind of have some large mesh of nodes that is all connected and within there you, you have some subset of topics, right? So different. Different nodes are going to be interested in different topics.
01:47:58.820 - 01:49:17.200, Speaker B: And there's a whole bunch of parts of the design space that approach this problem differently. Where, for example, some Pub sub protocols will make sure that routing only happens within parties that care about that topic. Or some other protocols might recognize that some parties might be interested in topics but are not directly in the same topic but are not directly connected to each other. So actually being able to route those messages through becomes pretty useful. And so, yeah, this whole page goes in detail as to how the messages kind of flow and get forwarded along, and then this kind of builds up to how gossip sub works in terms of gossip sub uses an abstraction of sending some control messages that signal what messages you have or haven't seen, because those messages are a lot smaller. And so that allows Gossip sub to achieve much better kind of bandwidth guarantees and so on, without kind of losing a lot of the kind of quick guarantees on the speed of propagation of the whole message. And then of course, you have other kind of properties like being able to choose the fan out of the network.
01:49:17.200 - 01:49:45.130, Speaker B: This lets you tune how quickly in a large network the message might propagate through to other nodes and that trades off again, speed of propagation versus the bandwidth waste. Cool. Let me see if I can.
01:49:55.360 - 01:49:55.724, Speaker D: So.
01:49:55.762 - 01:51:41.760, Speaker B: One of the sets of interesting problems in Filecoin, and this applies for ethereum too, and so on, is that when you're using this kind of protocol for moving around blocks us. Any kind of adversary that can slow down or just completely stop the propagation of messages can really affect the consensus of the network. So it becomes very important to kind of have protect against certain kinds of attacks. And so there's a report that got put together recently, you can look for it in archive. This kind of goes in depth into all of the different kinds of problems that you might worry about in these kinds of networks for this domain of problems around message propagation, it goes into kind of how gossip sub builds, the bash goes into kind of how Gossip roughly works, how its scoring function works. So Gossip hub uses a scoring function for deciding which connections to use for propagating the message versus just control or which connections to actually disconnect from. This also goes in depth into a whole kind of validation using a whole lot of testing on this tool called testground that allows us to simulate pretty large networks and kind of expect certain kind of propagation delays and so on.
01:51:41.760 - 01:52:40.480, Speaker B: And so this is kind of gossip sub compared to the propagation times of the bitcoin gossip structure and Ethereum One's gossip structure. And so these kinds of results are extremely useful for both Falcon and Ethereum Two. And so, yeah, you can dive in in depth into this and it kind of goes into different kinds of attacks. The good news here is gossip performs better than the other two and so it's a really good choice for Falcon and Ethereum too, and it kind of has good properties around delivering the message across the entire network fairly quickly, like within single seconds and so on. In this case, the problem is around block propagation or propagation of messages. So transactions in a blockchain. Now we can use the exact same kind of mesh for propagating retrieval requests.
01:52:40.480 - 01:53:50.168, Speaker B: But now that seems kind of crazy because why would you want to send out retrieval requests to the entire network? You probably don't want to do this, but now there's a way of using a popular mesh to do this. And so that's kind of where kind of the idea for doing this with retrieval market came from. So trying to do retrieval, there's kind of like a straw implementation version which you might say, hey, you're going to have a single topic for advertising for the entire network. And this is probably fine while the network is small, right? So even thousands of nodes trying to do this, it'll probably okay, it'll start degrading once you start hitting tens to hundreds of thousands and so on. But you can do something pretty straightforward where, hey, clients create some kind of retrieval request and they signal the CID that they want and they're interested in, and they send it along in the pop sub mesh. And so it kind of propagates to neighbors. Now, when retrieval miners receive this, they look at the request and they can decide whether or not to propagate it through.
01:53:50.168 - 01:55:07.664, Speaker B: And so they can, first of all, look at the request, see if the CID is content that the retail minor has, and if so, contact that client and then send them the contents and kind of suppress the forwarding of the message. Otherwise, if the retail miner does not have that content, they forward the message through to the network. And so what you end up with is a structure where this, again, super naive, right? But if this kind of request is going through, as the request is going out, it'll hit a set of virtual miners that have the content, and the message will stop propagating at that point. Now, that's like in theory, in practice, that'll likely work with if the message finds its way through a bunch of clients, then you might actually get propagation to the entire network, and that's not what you want. So potentially a different strong implementation would use one topic for retrieval miners to propagate messages to each other, and one topic for clients asking things from retrieval miners. But clients not participating in, clients not forwarding anything. So clients are only requesting and not forwarding stuff and only retrieval miners forward to each other.
01:55:07.664 - 01:56:06.688, Speaker B: And then that way you get a better property where you won't accidentally get this request to propagate to the entire network. You might also employ different kind of strategies like, say, hey, what if you create topics per region? And then that way you're bounding where your message is headed. So, for example, you can define a set of regions and you can define them based on kind of latency bounds in the network. And then clients have some strategy of saying, first try the closest region, and if the content is not there, then you ask kind of like you go a region larger, right, and it has this nice recursive property where as regions get large and dense, you can subdivide them, continue to subdivide them. And this kind of relates back to what I was talking about earlier around kind of having this kind of Voronoi style subdivision of the world. Over time as the network grows. This definitely is a little bit clunky because you have this question of, okay, well, you ask the closest region.
01:56:06.688 - 01:56:53.680, Speaker B: You maybe expect the diameter of that network to be maybe within 1 second. Then you wait a second. If you don't get it, you ask the next region, and so on. And you kind of have to scale up the times there and so on. It's definitely like chatty, it's wasting bandwidth and so on, but it might be a way of using the existing primitives already there to get this kind of scaling to work. Definitely not the most elegant, definitely not the best way to do it. But now, probably a much better way of doing this would be to actually hack into the guts of gossip, sub itself, and then start thinking about forming the mesh and changing the scoring function to take into account latency if we have certain guarantees about how the mesh itself is formed.
01:56:53.680 - 01:58:01.464, Speaker B: Such that you have a very high likelihood that all of the parties you're connected to are actually very close to you, which is not what you want for, say, security properties. But you might want this for retrieval. Then as you're propagating messages out, you can send messages with a time to live where as the message is going out, you kind of are decrementing times to lives and you get a very nice property that is similar. To kind of multicast over large networks and so on, where you can send out packets, these kind of requests over this mesh and those requests are not going to actually expand to the whole network. They're only going to go one, two or three layers deep depending on the choice. That way you can only hit the retool miners that are fairly close to you and you get that guarantee out of the or something close to a guarantee there out of kind of the latency oriented oriented mesh that would definitely require a lot more work. It's diving into the details of gossip sub and tuning the parameters and so on.
01:58:01.464 - 01:59:15.420, Speaker B: You might end up with less reliable guarantees around the likelihood of this mesh staying connected across the world and so on, because you don't get these kind of like long, long pathways to other parties elsewhere in the network. Cool. So one other part of this is that if you have this mesh that is through which you are sending all these requests, then you can also use that to index information, right? So a party sitting on this mesh can see all the requests flowing through and then can aggregate that information and start building kind of an index real time of what's being requested in those corners of the network. So if that's happening, then you can imagine having a number of indexer nodes that are kind of spread out across the mesh, collecting information of what's being requested, aggregating it and sending it to each other. So then parties can then reason about what's becoming hot in certain regions or kind of what distribution or requests look like over time. And so on now. This, of course, is terrible for any kind of reader privacy, right? This would be kind of building these access logs of a ton of stuff that is being requested.
01:59:15.420 - 01:59:51.550, Speaker B: And so there's some questions there around the trade offs between being hyper efficient in delivering content versus kind of privacy and so exploring how to do this well, of how to do indexing the right way such that it doesn't create this problem. Index that kind of breaks reader privacy would be really useful. Right now, we're kind of in a mode of just getting the retrieval to work really fast. Thinking about kind of preserving reader privacy and so on is kind of like a larger scale, longer term thing.
01:59:52.240 - 01:59:52.990, Speaker C: Cool.
01:59:54.160 - 02:00:24.310, Speaker B: If there are some questions and concepts, hope I can cover them. Otherwise we can move on to the next thing. It's cool. Don't see questions. So this is the current schedule. So I think based on timing, let me adjust it. So we ended up 45.
02:00:24.310 - 02:00:44.830, Speaker B: So there's a question now of immediately jumping into the break, which kind of following the right schedule or having the chainsaft talk. Curious what folks want. David and others, if you are kind of tight on time, we can just jump straight in or we can go into the break.
02:00:46.240 - 02:00:48.060, Speaker G: Yeah, we're certainly very flexible.
02:00:52.500 - 02:01:05.270, Speaker B: Mike and Robert, who are the next presenters. Are you fine with going into the break and then kind of just moving? I'll kind of adjust the schedule a little bit to reflect it, or are you tight on time?
02:01:06.200 - 02:01:08.550, Speaker E: I don't have any constraints, anything's fine.
02:01:09.480 - 02:01:12.310, Speaker A: Yeah, I think going into a break now makes a lot of sense.
02:01:13.000 - 02:01:44.910, Speaker B: Great. Perfect. So let's go into the break as schedule here, and then I'll move the stock to go right before we'll start with that, and I'll adjust the times. Thank you very much. See you in ten minutes. Bye. Perfect for Zoom to have elevator music.
02:01:46.720 - 02:01:47.790, Speaker C: If only.
02:01:50.720 - 02:01:52.150, Speaker B: You it.
02:12:32.270 - 02:12:52.950, Speaker C: All right. Welcome back, everybody, from a short break. Next up on the schedule, we have a few people from ChainSafe we're going to be presenting. Let me just double check. I get the names right. David, I think you're one of them, right?
02:12:53.100 - 02:12:53.800, Speaker B: Yeah.
02:12:55.930 - 02:12:57.110, Speaker G: And Elizabeth.
02:12:57.870 - 02:12:59.900, Speaker C: David and Elizabeth. Awesome.
02:13:00.830 - 02:13:01.580, Speaker B: Cool.
02:13:03.470 - 02:13:45.538, Speaker G: Let me share my screen here. Hopefully you guys can all see that. So, yeah. Thanks so much, Juan, for inviting us here today to talk about this. We have been working on some kind of preliminary stuff for enabling secondary retrieval markets, and so we're certainly very excited to be able to share that with y'all and hopefully continue the discussion as to what this is really going to evolve into. So I'm David. I'm also joined today by Elizabeth.
02:13:45.538 - 02:15:01.550, Speaker G: We're developers at ChainSafe, and we've been working on this project for the last few weeks. So presently when we look at filecoin, this is really what we see is that clients are interacting with the miners on the network to retrieve and store data. And so what we really want to do is try to change this model a bit to allow other parties to participate in this and kind of create this secondary market layer that incentivizes them to do so that well, there's a number of benefits to that. For one, we can reduce the reliance on storage miners and so that they can put their bandwidth to better use for storage operations. Specifically, it also allows participation from providers with lower storage capabilities, probably possibly higher bandwidth, lower latency. This will also improve availability of data on the network. As you have more participants able to serve it up, you should be able to do so in a more optimized manner.
02:15:01.550 - 02:16:19.530, Speaker G: By doing it in this kind of secondary layer, we're avoiding adding additional complexity to the core protocol, which is definitely very nice. Try to do this off chain as much as possible so that could be used for more consensus critical things as well. This kind of creates an opportunity for data to exist and be exchanged that may not actually be stored on filecoin. So there's certainly potential here to kind of serve something that's popular, but maybe nobody is actually paying to have stored at the present moment. So looking at some of the kind of base requirements to really make something like this possible, one of the big things is content addressing, which allows us to verify that what we're requesting is what we're getting and do so in a secure manner. It also provides global addressing, which is super useful when you have these large distributed networks and everybody needs to understand really what is being requested as well as data exchange protocol. And so specifically allowing the exchange of data in a way that facilitates payment and protects both parties from malicious actions.
02:16:19.530 - 02:16:48.178, Speaker G: And so those are two things that we have right now. Some of the more open questions is a discovery mechanism for the clients. So ultimately we want to be able to determine the best retrieval provider for a given request as well, for those retrieval providers, we want to give them insights as to what they should store and whether or not that's going to.
02:16:48.184 - 02:16:49.570, Speaker B: Be profitable for them.
02:16:49.720 - 02:17:40.370, Speaker G: And definitely optimizing for that can take many different forms. So we've been working on this repo in particular. Definitely please everyone, check it out. We would certainly love to continue the conversation with you if you're curious, playing around with it or if you have feature requests or ideas, definitely please reach out to us. Some of the goals specifically for this project. So the big one is really allowing this additional discovery mechanism that allows these kind of secondary retrieval providers to respond to requests when clients have them. We also want to make sure that we can inform those parties providing the data as to what could be profitable.
02:17:40.370 - 02:19:04.426, Speaker G: Certainly this is a huge variable and there's certainly many ways to try to tackle this, but just providing insight, I think, is definitely one of the core requirements. And then a big one here is doing this such that miners can be exclusively retrieval miners. So to not depend on any of the kind of pieces that go into the storage market. And I guess another big one here is certainly to do this all with everything that we have and to kind of try to streamline this so that we can start to explore what this will look like further down the road and really give an opportunity to the community to experiment and play around. So, to give you a high level insight into what we've really built here, a client in one of these kind of like, provider networks will be able to submit a query to their peers. Those peers will then use the Gossip sub layer to actually gossip that request and ensure that it reaches other providers. And then providers who are able to serve up that data can respond directly to the client with their pricing parameters.
02:19:04.426 - 02:20:18.280, Speaker G: And the client is kind of free to choose which of those it wants to continue with. And it can do so using the existing data exchange layer that is provided by the Gofill Markets implementation presently. So to look a little more into the fine details so the two kind of components here are the client and provider. The client is ultimately submitting a message to the Gossip subnetwork which contains the parameters for what it's looking for, as well as how to respond to it. It then accepts response from providers and then through some mechanism that can be acted upon using the existing data exchange protocol on the provider side, it's really just listening to requests. We've defined an interface here which is for the retrieval provider store, which is really just the mechanism that determines whether or not it's able to store that file. There's certainly a number of things that could back this.
02:20:18.280 - 02:21:27.130, Speaker G: For example, you could take an IPFS node perhaps and put it behind this and then make that data available for retrieval through more incentivized mechanisms. And then at the bottom here we have the query response. So this is very similar to what Hannah mentioned in terms of the retrieval parameters in Gofill Markets. And this basically just states what the offer from the provider is in terms of pricing. Definitely one of the most interesting questions here for us is kind of how you optimize for profitability or availability for a provider to be listening to requests and just kind of blindly fetching. Things that seem popular may not end up being economically viable because they need to then consider the cost of retrieval for themselves to first make that data available. There's definitely also other methods that we could look at for kind of how you populate those data stores.
02:21:27.130 - 02:22:39.906, Speaker G: For example, if you have something that you know is incredibly popular, like, let's say, the top 1000 Wikipedia pages. It may make sense to just kind of provide that without any further consideration for what the economic incentives may be. One of the big challenges here is definitely that the parameters today may look very different from the parameters tomorrow. And certainly these networks are very open and free and we need to constantly be considering what the kind of economic incentives at play know. Something may become popular in Canada tomorrow that was popular in the UK yesterday and kind of like how we adjust for these changes. And a big one, certainly when we're looking at things like Latency, is that the locality of a particular provider can affect how desirable it is. Furthermore sorry, I just lost my train.
02:22:39.928 - 02:22:40.900, Speaker B: Of thought there.
02:22:43.350 - 02:23:29.060, Speaker G: For Latency. This is certainly a huge factor. You want to be connecting to the peer that can provide retrieval as fast as possible. But it's also important to remember that the perspective of any one provider on the network is somewhat unique. If you observe from any given point in the network, you may have a very different perspective. And so it's important to be careful when generalizing. So to try to kind of facilitate some early exploration and ideation as to what this could really be in the future, we've implemented two fairly simple measures to try to get a sense of what the provider's perspective is.
02:23:29.060 - 02:24:33.080, Speaker G: The first of those is a very simple cache that just provides the end most frequent request using an LFU Eviction algorithm. And so this is just kind of a very naive perspective from the provider's point of view of what is popular, or at least has been throughout lifetime. And then we've also kind of exposed a subscription mechanism so that this can certainly be built on top of by adding kind of like another layer here, we can analyze the requests as they come into the provider and perhaps try to determine at what threshold. We then attempt to retrieve that file from the network and make it available. And this is certainly where a lot of questions still kind of remain. And there's certainly many different avenues that we could attempt. And it's definitely going to be real exciting to see what the community kind of contributes here.
02:24:33.080 - 02:24:59.360, Speaker G: So that's basically everything we've been working on, definitely. Please reach out to myself or Elizabeth if you have questions or if you're interested in getting involved, if you want to make use of it. We would definitely love to chat. I'm going to pass it over to Elizabeth, who's going to do a brief demo of kind of where we're at right now, and then I guess we can bring it back for some questions.
02:25:03.250 - 02:25:28.662, Speaker H: Cool. Hey, guys. Yeah, so I'm just going to do a quick little demo of secondary markets. All right? Can you guys see my screen? Cool. Okay. Yeah. So what I'm going to show so far is so we have a provider integrated into.
02:25:28.662 - 02:26:09.314, Speaker H: Lotus. This is a kind of preliminary integration. So what happens is that when you start up the storage miner, the secondary provider also starts up and as well it's able to access the files that are in the Lotus clients as well. So it uses that to check for whether it can provide the files or yeah, and then I'm just going to show the client doing a query to that and receiving the response. So yeah, I'm just going to start up Lotus now. So this is just a local DevNet based on just like the instructions in the docs. This is Lotus.
02:26:09.314 - 02:26:52.340, Speaker H: This is the Lotus storage minor. Yeah. Okay, so here we have this log for started the secondary provider. And we have some addresses that it's listening at. So I'm just going to keep those there. And then I'm going to go into the secondary client, make the client and okay, so I'm going to just paste one of these addresses here. And then I also need to import a file into Lotus as well so that the provider has something.
02:26:52.340 - 02:27:50.520, Speaker H: So lotus client import. I'm just going to import some text file. This is my CID. I'm just going to tell the client to connect to the provider and then also query for this specific CID querying for payload. Okay, cool. So what happened was it was querying for this CID and then got streamed from the peer and then it response for the CID that it asked for. So yeah, you can also see it got a response and then it has the parameters that it can then decide whether or not to proceed with this retrieval or not.
02:27:50.520 - 02:28:26.802, Speaker H: That's where we're at so far with everything in the future, the integration is probably going to change. This isn't what it's going to look like in the end. In the end we want it to be more independent from the storage liner. Yeah, right now it's like reliant on the file store that's in Lotus. That's basically all cool. Any questions? It cool. Okay.
02:28:26.936 - 02:28:28.322, Speaker B: This is awesome. Thank you.
02:28:28.456 - 02:28:29.540, Speaker H: Cool. Yeah.
02:28:30.790 - 02:28:32.500, Speaker D: I didn't realize you guys had already.
02:28:37.050 - 02:28:52.380, Speaker G: Yeah, it's definitely very preliminary, but it's cool to see. And yeah, we'll definitely have to try to decide if it makes more sense to continue development as kind of a separate entity or to release that move.
02:28:57.790 - 02:29:20.580, Speaker B: One quick question, I wonder if you can do you have it such that and I don't know if this is demo now or not, but just being able to see kind of say like a set of five or ten different miners where only three of them have the content and then have that same flow working.
02:29:21.750 - 02:29:37.480, Speaker H: I could do that with just the provider running on its own. Probably not in Lotus because I haven't set up multiple Lotus nodes locally. So yeah, I could show that if you would like. I guess we still have time. Cool.
02:29:38.250 - 02:29:44.460, Speaker B: Totally up to you. By the way, don't need to put you on the top spot of creating a new demo that you haven't played around with.
02:29:44.990 - 02:29:58.414, Speaker H: Yeah, I'm happy to do it, but yeah, I don't think I've tried this, but I think it'll work. If we have time, I can do this. Cool.
02:29:58.532 - 02:29:58.766, Speaker B: Yeah.
02:29:58.788 - 02:30:20.260, Speaker H: I'll just go ahead then, see what happens. Okay. We also have like a provider that stands alone as well. So let me just get rid of these for now. Okay.
02:30:24.070 - 02:30:25.490, Speaker A: Make this bigger.
02:30:28.070 - 02:30:48.300, Speaker H: I guess I can do I'll do a few. All right.
02:30:54.390 - 02:30:55.140, Speaker B: It.
02:31:04.470 - 02:31:47.220, Speaker H: All right. So for the provider that we have, that's standalone. So right now it just reads what files it has from just like a JSON. It doesn't really what's it called? It doesn't have an actual block store or anything behind it. Provider sample data. Okay, so there's one CID in here so far. So I guess I'll leave it in two of them and then just delete it from the others.
02:32:09.870 - 02:32:10.234, Speaker D: Okay.
02:32:10.272 - 02:32:13.210, Speaker H: I didn't copy that before. I did that. Whoopsies.
02:32:15.630 - 02:32:19.462, Speaker B: I think you might be on the left bottom screen. I think a bunch of CIDs.
02:32:19.606 - 02:33:10.666, Speaker H: Yeah, I have some CIDs but I just need to make a file that has CIDs. Okay, so sample data with CIDs. Okay, cool. All right, so yeah, I guess I'll just take this one from before. All right. Okay, so trigle provider it and then whoops. Okay.
02:33:10.666 - 02:34:00.060, Speaker H: Data. All right, so there's one. I'm going to just connect them together just to make sure that they all know about each other. It'll probably make this go nicer. Okay, so I'll do these two then with no data. Cool. It says what data it has here.
02:34:00.060 - 02:34:40.940, Speaker H: Cool. Okay. Providers are set up. Cool. Okay, so now clients. So I guess I'll just pick any of these because they should all be connected now. All right, let's see what happens.
02:34:40.940 - 02:34:55.486, Speaker H: Okay. Query. Okay. Nice. Yeah. So it got a response from this guy. What is this? QT at the end? Yeah.
02:34:55.486 - 02:35:09.870, Speaker H: So this one and then also 5D. So yeah, this one. So yeah, that was what should happen, which I said. Yeah, it's working. Yay.
02:35:10.210 - 02:35:11.434, Speaker B: That's fantastic.
02:35:11.562 - 02:35:18.840, Speaker H: Yeah. Cool. Yeah. Is there anything else that you'd like to see?
02:35:20.090 - 02:35:22.198, Speaker B: That was magical. This is really cool.
02:35:22.364 - 02:35:24.040, Speaker H: Yay, I'm glad you like it.
02:35:25.050 - 02:35:37.994, Speaker E: I have a question about the cast. First off, really great work guys come to life. So that's awesome. I just wanted to ask about maybe you could just explain it again.
02:35:38.032 - 02:35:38.278, Speaker B: David.
02:35:38.294 - 02:36:01.460, Speaker E: I think I just didn't get the first time. Are you saying that guess sorry, it's not kids would be retained by that minor? Or do you mean that other miners would go out and fetch content based on what SIDS they've seen recently and then retain them like that?
02:36:01.910 - 02:36:19.670, Speaker G: Yeah, I mean right now it solely exists as kind of like an observation, but yeah, certainly it could be naive in that you are constantly storing what is most frequently requested, but yeah, that's not necessarily implied by the present implementation.
02:36:20.350 - 02:36:24.140, Speaker E: Okay. Got it. Okay, thank you.
02:36:32.320 - 02:36:33.390, Speaker G: Anyone else?
02:36:35.620 - 02:36:40.130, Speaker B: This is awesome. Thank you so much. This is super cool. Yeah.
02:36:42.180 - 02:36:42.930, Speaker H: Thanks.
02:36:49.020 - 02:36:58.890, Speaker B: Cool. So since I'm hearing no more questions, and I think we're up for the next slot, mike, you want to take it away?
02:37:01.600 - 02:37:38.260, Speaker E: Yes. 1 second. Let me share my screen and do all the usual preliminaries. Okay, so hold on. I got to hide the zoom controls before I lose control of them. Okay, so guy and I have been working on a project that we're going to present. I'm going to just describe a little bit about it, and then he's going to do a demo.
02:37:38.260 - 02:38:21.190, Speaker E: Then I'm going to do a demo, and then we can take questions, or we can talk about anything. But the basic idea is it's very similar to what you just saw from ChainSafe. It's to build a peer to peer network of retrieval clients. So the same as what they're doing. However, we want ours to run entirely in the browser, and I'll talk a little bit more about why that is. So the browser retrieval client is this green thing on the left. So the idea, again, it's a peer to peer network of these browsers that are offering and providing retrieval services.
02:38:21.190 - 02:39:08.032, Speaker E: They talk to the filecoin storage network, and they talk to something called sudo oracle. So let me just explain about the connection of the filecoin storage network briefly. When we do everything in the browser, there's some limitations or opportunities, depending on how you want to look at it. First off, we don't want to have any lotus node require we don't want to require the user to run a lotus node either on her local machine or on a cloudbox or whatever. I don't know. I'm sure most of the people on this call have played with other blockchains. I mean, it's always the case that to run a full node, it's like you've got to first wait for a week for it to sync to the chain, and there's usually a lot of set up around that.
02:39:08.032 - 02:39:54.972, Speaker E: And we want something where people can just come in and just go just start either doing retrievals if it's an end user and they would just want to fetch some content from a competitive marketplace where prices are being driven down because of the competition between different retrieval providers or if it's a retrieval miner who's on the other side of that market. Someone who wants to provide retrieval services in order to generate income. And so we want people to just be able to come in and go, we want the retrieval miners, especially. They just leave a tab open or just leave an extension running in the browser. There isn't any other ongoing work they have to do. That would be sort of the ideal scenario. And we have this notion of customizable strategies.
02:39:54.972 - 02:40:38.930, Speaker E: So different retrieval miners. This is for miners, different retrieval miners might have different strategies about how they're going to most successfully generate income from providing retrieval. We're trying to avoid dependency on a user run Lotus. So we have this notion of a public lotus, cloud Lotus as it's called here. And the Cloud Lotus can do some basic services for you. It can't do a whole lot because it doesn't have your private wallet keys, right? So those are retained in the browser. That means.
02:40:38.930 - 02:41:43.030, Speaker E: Rather, the browser application logic has to create all of the messages to the blockchain, sign them in the browser, and then it just submits those to the cloud Lotus for publication on the so Hannah talked earlier. I wasn't going to spend a lot of time on Sid Oracle, but I just want to tell you what it is, because this diagram is sort of encompassing everything we're building to try to support retrieval. Hannah talked earlier about payload Sid and PSID, and there's sort of this discoverability problem where if you weren't the original party who stored the content, you don't have an easy way to find out what the payload Sid is. So there's a PR. Now, I don't know if it's merged yet, but I think it will be soon. That adds the payload Sid to the original storage deal proposal, which also contains the piece Sid, and it contains a reference to the storage Miner. And so what Sid Oracle does is it's just really simple program.
02:41:43.030 - 02:42:17.360, Speaker E: It just walks through the blockchain and then just keeps up with it. And it's just reading every message on the blockchain. And when it sees a storage deal proposal, it saves that tuple of payload Sid PSID, storage Miner. It just saves it to a SQL database and then serves it up on a public HTP API. So, okay, with that introduction, I'm going to turn over to guy who's going to demo our browser extension and let me stop sharing.
02:42:20.820 - 02:42:25.410, Speaker F: Hey, guys, let me start sharing my screen then.
02:42:27.780 - 02:42:28.850, Speaker C: See here.
02:42:32.280 - 02:42:34.740, Speaker F: Okay. Can you see my browser?
02:42:35.800 - 02:42:36.550, Speaker E: Yes.
02:42:37.000 - 02:43:23.156, Speaker F: Cool. So this is what the extension looks like right now. We have a few tabs right here, but this is the home screen. And what happens is as soon as the browser starts, it connects to the peer to peer network. And actually you can see here, my own extension has three peers connected to it. So the first thing we can do right now is just upload a new file to these known seeds that I have. So if I upload my own file, it shows up here.
02:43:23.156 - 02:44:05.200, Speaker F: Now I have this seed that I know and I can provide to others. And it's just really a local file transfer. We use Unix FS to actually split the files in chunks and store them in the browser's index DB. So it all stored in blocks. And another thing I can do is just query for someone else's CIDs. And this is one that Mike has on his own computer. He's running the same extension and I can query for this CID.
02:44:05.200 - 02:44:57.540, Speaker F: The download process starts and soon I have the same CID here and I can now start providing the same file to others. Or since it is stored in chunks, I can choose to download it to my own computer. I'm not sure if you can see the dialog here, but it's a download dialog. I can save the file to my computer. I can open it and do whatever I want for that. We have, like, a couple options. We can configure the hand of IP, our Lotus settings wallets, my private key and stuff, payments interval and increase for the whole protocol.
02:44:57.540 - 02:45:06.500, Speaker F: I'll pass back to Mike and he'll explain what point we are on and what's missing from this extension.
02:45:08.440 - 02:45:29.212, Speaker E: Awesome. Thank you. Okay. All right, I'm back. Can you see my slides? Sorry, I can't see anyone.
02:45:29.346 - 02:45:29.836, Speaker B: Yes.
02:45:29.938 - 02:46:04.708, Speaker E: Okay, thank you. Yeah, that was the unmute delay, I guess. Yeah. Okay, so here's basically where we are in terms of progress on this. I think we've made a lot of progress and then I think we've struggled a little bit to find a way to get messages, signed messages, like correctly formatted sign messages with correctly serialized parameter strings actually publish onto the chain. But the first part of this is looking pretty good. This is algebra.
02:46:04.708 - 02:46:56.792, Speaker E: But he's gotten all of these parts working, and now we're just trying to figure out how to create these on chain objects and invoke methods on the actors to manipulate or respond to our objects we create and stuff like that. There is a piece of Voucher handling logic, like making sure that the next Voucher that comes in is higher than the previous ones and stuff. So there's some Voucher handling logic that will be purely in JavaScript that we haven't done yet. And then there's the retrieval from storage miners, but it's really that third from the last one, the on chain stuff that we're kind of stuck on right now. And so zooming in on that. Our progress is okay, but not great. A little bit.
02:46:56.792 - 02:47:14.990, Speaker E: Lotus is kind of refactoring some stuff with payment channels. So it's been a little bit hard to kind of validate some of these things. But creating a payment channel seems to work. There's some not 100% of the time. So we're still investigating that. Settling and collecting are in flight. Like, I've implemented them.
02:47:14.990 - 02:47:58.204, Speaker E: We're using a Rust library which compiles to WASM, so I implemented those, but I don't actually have a way to test them yet because I'm waiting for some refactors in Lotus to occur so I can't check those off yet. And then the rest of these things are the other on chain events and they're not complete yet. Okay, so I was just going to talk about some of the other components that we've built. We'll take questions at the end. This won't go on for too long. But I was just going to quickly talk about a couple of the other components that we've built. So one of them I mentioned the Sid I'm sorry, I sometimes call it Sid index or other.
02:47:58.204 - 02:49:06.240, Speaker E: It's Sid Oracle is its correct name. So there's a typo in the first bullet. But for Sid Oracle, I create a Lotus client Rust Library. It's basically a library that can walk the blockchain from height zero up to the present height and then keep going or whatever you instructed to do. And it's a dependency injection thing, so you give it logic for what it should do on each block, on each message within a block, and it's sort of whatever you tell it to do, it'll do. It only implements a small subset of the Lotus API, just because this is all that was needed for the pseudo tool. Yeah, it does some higher level decoding, but it can't decode every possible param string because there's so many, there's probably like hundreds of them and they weren't needed for pseudoacle.
02:49:06.240 - 02:49:26.168, Speaker E: And yeah, that's its repo. I just jump over here and do a quick demo of this. So this is one of the example programs that's in the repo. So let me move these controls. You can't see that. Okay. But this one is called Print Everything.
02:49:26.168 - 02:49:57.856, Speaker E: And what it does is it just, iterates over the blockchain. So this is a free function exposed by the library. Iterates over the blockchain. And then I've provided three callbacks here for what to do when it reaches a new tip set height. It just prints out what height it's at when it encounters a block within that tip set height, because there can be multiple tipSet blocks. That's the tip set part of it. It'll, again, just print out the block.
02:49:57.856 - 02:50:29.772, Speaker E: And then when it finds a new message, it'll just print out the message structure with dashed lines between it. So let's take a look at that. That was the wrong example. Sorry. So it's, iterating through blocks, hasn't found a message. Okay, so now it's found a message. This one only go, it stops at five blocks for this purpose of this demo here's in a message.
02:50:29.772 - 02:51:19.644, Speaker E: It founded between the dashed lines and you can see it's just dumping the fields. It's not decoding the param string. So there's some things that still need to be decoded that aren't a lot of things, actually. And then this particular one, it was able to find a receipt. Sometimes it finds messages that haven't been processed yet, so they don't have a receipt yet. This one did, but again, it doesn't decode all receipts, only this small subset that were needed for the Sid Oracle tool. So I just want to show sorry, let me just yeah, I just want to show some of the repos that we're working in.
02:51:19.644 - 02:52:18.400, Speaker E: We would love contributors like we would welcome contributors who want to get involved in any of these projects. This is the one I just showed you, the library for the Lotus API client. And this one doesn't have a status indicator, but it's done in terms of what's needed for Sidoracle. Sit oracle is over here. There's a little explanatory stuff here, but basically I wasn't going to demo this one, but it's intended to just run on a server and just look for those Psy payload Sid and minor identity mappings, and then just put them into a SQL database. It's using like SQLite right now. I think to deploy this, we'd put it on something more scalable like Amazon RDS, but that's the idea.
02:52:18.400 - 02:52:52.444, Speaker E: And then there would be a simple node JS, like a five line node JS app. That just all it does is have one single endpoint and it's just like a query for some Sid and it will return you if it can find it in its database, it'll return you that three part tuple of everything that it's mapped with. We get a little fancier SIDS expire over time because storage deals don't have infinite duration. So there's some enhancements that could be made around that, but that's where the tool is.
02:52:52.482 - 02:52:53.070, Speaker B: Now.
02:52:54.960 - 02:53:40.232, Speaker E: I'll show you. So the browser extension, we covered this in the slide, so I won't cover it again. There's that diagram again. But yeah, this is the repo for that one if you want to check that out. So this is how we're sending messages to the chain, or generating and signing messages to be sent to the chain. The cloud Lotus is the way, is the means by which we send them, but in order to generate them and not have the user's private keys ever leave his machine. We are using a library from Zondax, which I made a fork of to add payment channel functionality to.
02:53:40.232 - 02:54:19.716, Speaker E: And I think I showed this before. So we don't have much of the functionality actually working, but and this shows a little bit of the flow. This is the flow that Lotus uses. Our thinking has been to try to just emulate that. And yeah, again, if you're particularly good with serialization and deserialization of rust structures, especially if you're good at it into Seabore, yeah, we would love to have your help. So feel free to contact us. This is just a demo program that works out our extensions to the filecoin signing library.
02:54:19.716 - 02:54:43.070, Speaker E: So not much there. Covered that and covered the Sedorfil. So I think that was everything that I had planned to cover. So let me stop sharing. And are there any questions or any thoughts this inspires in people?
02:54:46.150 - 02:54:47.960, Speaker B: That's really cool, really awesome.
02:54:49.050 - 02:54:50.040, Speaker E: Thank you.
02:54:51.850 - 02:55:39.750, Speaker B: Maybe one question would be how do you envision the store? So kind of like flagging the CADS kind of by hand will get cumbersome in that. If you want to plug in, if you want to run just this extension in the background, you could do it a couple of ways. One is it could look at a local IPFS node and kind of redistribute everything that's in there and be willing to kind of serve out those requests there. Or it could kind of have its own independent cache or it could kind of have a strategy that is willing to grab content that is willing to kind of offer some space and grab some content that other parties tell it is good. I'm just curious how you've been thinking about that and how that might evolve.
02:55:40.090 - 02:56:37.634, Speaker E: Yeah, so the local IPFS thing, that's an interesting idea. I actually hadn't thought about that one yet, so I need to process that one a little bit. But the way I'd kind of been thinking about it was with the customizable strategies you would have a miner who knows that okay at a certain frequency. If I go to this URL, there'll be an update to some piece of software that's stored on filecoin and I can get the CID from there and then I'll just retrieval mine that. Because I know this is a popular piece of software, many people will download it, but I think it's a little bit of a handweight. The answer the whole Sid Discoverability thing is kind of I don't have a good answer for it. I don't know if anyone does because they are kind of like it's a really important piece of information but there is an easy way to discover them and even if you could just get a list of all of them or something, you don't know what they are.
02:56:37.634 - 02:56:57.520, Speaker E: There's context behind each one and obviously you don't want to retrieval mine like somebody's my summerphotos TGZ or whatever because there's only ever going to be one retrieval for that. So yeah, that's kind of the extent of my thinking so far. I'd be curious if you had other thoughts or anybody else does.
02:57:07.770 - 02:58:00.840, Speaker B: Yeah, I think this indexing piece is a big part of the puzzle and how to collect that information. Well, there's a bunch of different guarantees that you might have and especially when you're dealing with petabytes of stuff, there's different levels of granularity that you might want to include in these indices. So it's a whole content routing problem but with a different tuning than the normal IPFS problem. There's like a better different set of questions where in IPFS it's about in most IPS notes it's mostly, hey, I have this content or I want this content, I want to figure out who has it and get it. In this version it's, hey, I want to know what content is interesting and valuable to redistribute from here, from where I am, from where I sit on the network. And that's a substantially different question.
02:58:02.810 - 02:58:03.174, Speaker A: Some.
02:58:03.212 - 02:58:06.760, Speaker B: Indexing strategies will tune for.
02:58:07.230 - 02:58:55.080, Speaker E: Yeah, and Hannah's raising a good point in the chat. The kind of opposite problem of Sid Discoverability is like how do you ensure privacy on mean my thinking on that I basically agree with what Hannah's saying. If every sid is published on the blockchain, then you can't have any privacy for the existence of data. You do have some privacy in terms of no one can easily know that you are the person who stored that sid or the person retrieving it. It's not like the IPS are logged or anything. But in terms of privacy of the data itself, yeah, I think encryption is the only solution to that.
02:58:56.410 - 02:59:38.054, Speaker B: Yeah. And that definitely has all kinds of other properties. And you can take large segments of data that a lot of parties are storing and then store even within that store private data within some subsegments of that. So there are ways to kind of obfuscate storage and access and so on with some security guarantees, but you get kind of these blow ups and storage amounts and so on. The reader privacy, though, so there's different components. One is writer privacy. So when you're writing a thing to the chain, or you're writing a thing to Popcorn in general, knowing who the publisher is, that kind of made that deal with a miner and so on, that's pseudonymous right now.
02:59:38.054 - 03:00:38.898, Speaker B: So you could work on hiding where parties sit. But there is kind of an account associated with that, so that is sydonymous and you can know who the client, the surname of the client, and the accounts associated with it and so on. And maybe through that build same kind of chain analysis kind of stuff that people do on blockchains with money accounts and whatnot. But then a separate part of it is even kind of saying, hey, publisher privacy, even kind of independent of that model. There's a whole question around reader privacy of like, okay, well, there's this content, and then people are trying to what content people are retrieving and where and from whom and so on. That's a whole bunch of other information that doing this efficiently over a peer to peer distributed network is likely going to leak a lot of that information unless special care is put into the design of the system to preserve the privacy. More so than, say in normal CDNS where you're interacting, maybe your CDN provider knows who you are, but that's it in kind of a peer to peer network.
03:00:38.898 - 03:01:06.340, Speaker B: If you're distributing all these messages everywhere, then a lot more parties are able to see it. There's all kind of deeper questions there though. You can get reader privacy by obfuscating the channels itself, where again, you have maybe pseudonymous. You can know the pseudonym of the party that is retrieving, but you actually can't map that to a real IP address. It could be behind kind of layers of something like doors or other things like.
03:01:11.750 - 03:01:55.860, Speaker E: You know, those are all good points. And Hannah also raises the point that we're adding the payload CID to this general purpose label field on the deal proposal. But future versions of Lotus can make that optional or a config file setting, or whatever. So that would give you some greater sort of anonymity. Obviously, as Juan points out, what you're uploading is always associated with your wallet, public key. So, like, anybody who has the ability to subpoena coinbase or whatever for all of its records, I mean, clearly that can't be obscured or whatever.
03:01:59.670 - 03:02:07.880, Speaker B: Great. Thank you. I think we should move on to the next talk. But thank you very much. Mike, thank you for the demo Ghee. That was really cool.
03:02:08.490 - 03:02:09.640, Speaker E: Thank you, guys.
03:02:11.690 - 03:02:26.660, Speaker B: All right, Robert, we can't hear you, I think, or at least can't.
03:02:30.050 - 03:02:52.200, Speaker A: I had two levels of muting going on. Sorry about that, guys. Hey, thank you. Thank you, everybody who's sticking through this. This has been super interesting for me as well, being somebody who's working on a project related to this now. Really cool to see the projects other people are doing. Okay, let me go get started.
03:02:52.200 - 03:03:35.700, Speaker A: Does everybody see the title slide? Looking good. All right, great. So I'm going to be talking about filecoin retrieval market objectives. It has some overlap. I think Juan gave some good thoughts. We've been talking with him as part of our project, but he's clearly the master of thinking about what are objectives in the overall system we're putting our efforts in. So why am I giving this talk? There's a team of us, about seven of us in Consensus's R and D groups that are currently looking at the filecoin retrieval market design.
03:03:35.700 - 03:04:15.614, Speaker A: There's sort of two phases to what we're doing that I'd like to talk about today so people get a feeling for what we're doing. There'll be results that we're kind of putting out there. All the work is going to be open source, open protocol published and so forth. I know sometimes that might be a question with Consensus because Consensus does do some revenue stuff, but this is all open projects with support by filecoin as well. I should mention that. So there's two aspects we're doing. One part is looking at the crypto economic design and there's sort of a step back I really appreciate.
03:04:15.614 - 03:05:19.430, Speaker A: And I think it's absolutely important to be looking at what the other projects are doing who have been presenting today, which is looking at actually building some code bases, putting together components and so forth. As I go through the talk. You'll sort of see, though, that I think we're looking at it from a complementary perspective that ideally just sort of does some cutting the brush out of enabling later work of plugging components together to kind of come together into a retrieval market that more likely organically grows up and works the way that we want it to. So there's an aspect of the crypto economic design which is talking about the objectives of the overall market. I have a couple of slides on that and then there's some work. After looking at objectives when you're doing crypto economic protocols of thinking about incentivized mechanisms, how you would. Want to reward participants, how you would want to penalize them.
03:05:19.430 - 03:05:55.506, Speaker A: Things like slashing in like E two, if you're familiar from proof of stake are examples where people actually put up funds. Filecoin has that great use in proof of space time where storage miners have to actually put staking and they get punished. And so they're kind of forced to not only give a crypto economic assurance that what's going to happen to them. So as a user, you kind of know what it's worth for your storage to stay around, but you know that actors are being incentivized in the right way. And then later in the project we're going to work on protocol components. We're only a couple of weeks in. So let me give you an overview of where we are.
03:05:55.506 - 03:07:00.082, Speaker A: Now, the meta objective, of course, of this work is to do something that's very helpful for Filecoin storage. And the idea here is that if the retrieval market enables a wide palette of different latencies, different throughputs different bandwidths, the more that it has, the more useful Filecoin storage is. And having that all be kind of a one stop shop that you can get the persistent storage of Filecoin yet on the other hand, be using it for websites or IoT applications where you need to have four, five, six millisecond round trip times to some computation. And storage is a really powerful thing and I'm very excited and interested in seeing Falcon being able know, cover that entire gamut of approaches. So I'm grabbing two slides here that Juan has showed in the past. This one's from a while back. I think the pinning summit was when I first saw Juan show this one and very useful, I think, for a lot of people to get a feeling.
03:07:00.082 - 03:07:43.406, Speaker A: A lot of people in the cloud space are kind of familiar with Amazon. Obviously they're the 800 pound gorilla. And what they've shown us, right, is that users are okay with there being different tiers of storage. Explicitly. Amazon has kind of put things together into there being relatively inexpensive glacier storage which is more like tape backup in terms of the latency time and the cost. And then having S Three storage, which is very general purpose, you can use it for EC two instances that are inside of AWS regional data centers. You can actually use it for serving data to your end clients.
03:07:43.406 - 03:08:34.030, Speaker A: However, the way that S Three storage is organized in buckets, it sort of pushes all of the storage in one bucket into one region. So S Three is not ideally the way that you would build a CDN. And so Amazon then offers a version of storage cloud front which basically amplifies the Sdn caching data around the world on demand based on where it's being used. And while there is some additional cost for CloudFront, it is kind of folded in, so it's not really that much of an extra cost. In fact, if it wasn't there, you'd have to pay more. But Amazon does this because they know that if they didn't have it, s Three wouldn't be useful. So it kind of goes back to that previous slide point where if you want storage to be well used, you need to cover a lot more use cases.
03:08:34.030 - 03:09:18.980, Speaker A: So in this picture, Juan kind of showed storage and retrieval mining going a certain distance and pinning services inside of retrieval miners. You can think of that in general as like caching. The pinning service means you're actually telling people what to store inside of retrieval miners. But in general, the retrieval miners would be expected to have caching in them. That allows them to not always have to go back to the storage miners, which are going to have a certain kind of I ops. And throughputs retrieval miners might actually want to serve out of SSDs, for instance, which storage miners are not going to do because storage miners are looking at $20 per terabyte for a hard drive cost, whereas SSDs cost more, about $120. So it's about a six x difference in cost.
03:09:18.980 - 03:10:23.382, Speaker A: But if you're a retrieval miner from an I ops perspective, SSDs are amazing, right? You can not get ten or 100 times better. You get thousands of times better I ops performance out of SSDs. But then through this hack FS, Juan showed a diagram here which I think, and I think is really true here, which is that storage miners can come down a lot in latency. Some of it has to do with do they keep unsealed copies or does sealing get, you know, really storage miners shouldn't know. Pushed in the category where they're just operating like Amazon Glacier, storage miners do have hard disk drives. They have a tremendous amount of read bandwidth available on their spindles, and so they can really cover, for the most part, the S Three use cases. S Three is actually regional where storage miners even have a certain element of CDN to them that if you're doing proof of replication, you can actually populate your storage in multiple places, which already starts making it CDN like.
03:10:23.382 - 03:11:34.106, Speaker A: And retrieval miners. Likewise, as long as the retrieval miners have capabilities like Juan was talking about today, which I think are really interesting to look at, of how we incentivize things that are less DHT like and more index like so that we cut down the number of hops to something that while it's not going to be one in any realistic scale, is kind of order two, order three rather than order login of the number of providers for how many hops you have to do until you are being served the data you look for. So just for the comparison, I think I kind of said it verbally there, but these slides are, I'm sure, going to be attached to the talk later. So I just thought I'd include it here. That the filecoin in comparison with the examples on the right side, amazon Cloud Front as well as Cloudflare's CDN, which has a totally different, interestingly economic model but achieves very similar performance levels of CloudFront. Those s three and Glacier make one storage stack. Filecoin is very interesting that I think that one with the circle is primary retrieval.
03:11:34.106 - 03:12:17.366, Speaker A: So direct retrieval from storage miners. Actually I should have updated this table that already is going like halfway into S Three, maybe even covering a lot of it. And secondary retrieval is then overlapping and going up that. But it's a simpler stack and that's not too surprising because when you think about decentralization, sometimes you end up getting that that decentralization can kind of adapt and handle amorphously more targets. I made a comment here about data rights, but I'm going to skip that for this talk. So performance levels of the kinds of things we're looking for, it was in that previous chart. But I'm just making some notes here in terms of retrieval conditions.
03:12:17.366 - 03:12:58.566, Speaker A: So CloudFront. There's nothing magical. Amazon doesn't actually have license to faster electrons or faster photons. They operate with the normal rules of physics. So if you're going to get these extremely good Cloud Front latencies, it's because Amazon has actually populated some point of service type spots at the Edge. Now there's been a lot of work in the Edge Cloud where you actually do put storage and the server capability like Lambda Edge services inside of 5G cell phone tower. There's a number of startups doing that and I'd really like to see Filecoin and IPFS type decentralized storage in those places.
03:12:58.566 - 03:13:29.460, Speaker A: I think it's possible. S Three, of course, is regional. So if you're in the US, you have east, you have Ohio, you have west regions. All of those are going to be getting you latencies across the US. It's under 60 milliseconds. So you're going to be able to hit reasonable latencies around the world. Of course it's going to be more and so that's where if you're serving from distant places, then you're going to want your S Three to be fronted by Cloud Front.
03:13:29.460 - 03:14:25.460, Speaker A: Just giving those numbers I think is helpful. Here's a drawing which is this is a drawing I put together. So blame me for how crappy it is in terms of any graphic designer would say this is terrible, but it captures, I think, a way that I think we see the participants who are engaged here. The left side is not directly secondary retrieval, but I use this diagram when I'm explaining to kind of other people how retrieval fits in with the storage miners. So the storage miners on the left side, they have their stack from the users to storage miners. I even threw in because there's some people interested in offloading ceiling because that's actually pretty heavy duty computation and could very easily be a separate market. Juan has talked about that in the past and ultimately everything's put on Filecoin and the storage miners themselves are a direct retrieval channel.
03:14:25.460 - 03:15:27.442, Speaker A: I don't actually in this diagram show them realistically as being the people who would serve the end user clients because it doesn't seem realistic or efficient to have mobile applications and web browsers talking to storage miners directly. It's just very efficient to have people in the middle. Putting in those people as intermediaries is not bad or evil. I know in blockchain people dislike intermediaries, but decentralized intermediaries who don't have the ability to control a monopoly can be very good. They can make some money, they can actually make a network very powerful, but they can still be permissionless, they can still invite lots of new participants to come in if they go away. So that's, I think the vision for secondary retrieval providers and what I show there, I show sort of kind of different networks that are coming together. There's a portion of the network on the left side that is going to be a lot more tied together with the storage miners.
03:15:27.442 - 03:16:37.034, Speaker A: In fact, I think some of the projects or some of the easy kind of ways to get started on retrieval markets is in fact for the storage miners themselves to also participate on the retrieval market side. So they can actually stand up SSD clusters at low, latency, close to their hard drives and very cost effective wise, have extremely high capability and make extra money off of their storage, which is always a nice thing. Additionally, just to the right of it, I show an area where these would be sort of like caching only retrieval providers, so they would not necessarily have any special arrangement with storage miners. They would actually have to pay funding in order to do the direct retrieval from storage miners. So they'll have a little different economics picture. And then on the bottom side, I'm sort of showing p to p torrents and that portion of the retrieval market I even show kind of dotted out to include potentially clients. You can envision that if you have torrent like mechanisms.
03:16:37.034 - 03:17:38.942, Speaker A: They're not great, obviously, at doing what filecoin does of having persistent store, but they are great at amplifying storage and they amplify it with the number of participants who are coming in. So I think as an objective it's entirely valid, especially at the low, latency edges, that people who are sitting in the edges, who are retrieving, who are looking to both pay for their continued retrieval and make some extra bucks, should be incentivized and find a place in the market where they can bring in their resources and do those functions. On the right side, there's one point that I wanted to make, which is that all of the payments in here, it's reframing. The thing about it, paying somebody to get a file doesn't mean the simple case that you pay for them and that's the end of the transaction. Payments can actually have larger loops. And what I'm drawing here in terms of third party paying providers, on the top right is a very applicable one. It's used all over the place in Web Two, and there's good reason to expect that to be an objective for Filecoin's secondary retrieval provider.
03:17:38.942 - 03:19:12.254, Speaker A: And it's one where Filecoin allows third party providers to sort know pay filecoin and to also indicate token types, accounting tokens if you want to call them, where the accounting tokens are kind of redeemed as authenticated users are coming in and using the services. So third party providers can actually provide those tokens to users if they want to have direct tracking. If not, there can be other mechanisms that are still basically allowing the filecoin providers to kind of get paid upfront and then redeem it based on usage and similar to other CDNS or so forth. If you were being charged on a gigabyte per second basis, which you aren't with Cloudflare, but you are with CloudFront from Amazon, ostensibly the CDN provider has the responsibility to make sure that they're taking care of Dos and Duplications so that your competitors aren't just using up your CDN dollars. Similar to like AdWords, there's a lot of anti corruption mechanisms where you have to detect if there's real ad click throughs versus fake and so forth. So there's going to be a lot of interesting economics on the right side that the protocol is going to want to be able to support in longer term. Okay, so moving along, one thing that I just think is useful it's a little bit of a refresher, but I gave this talk at Stanford a little while ago, and it's kind of a well received talk.
03:19:12.254 - 03:20:24.338, Speaker A: And I think it's just useful because sometimes we kind of talk about attackers as kind of evil, and we get a little bit it sometimes is useful to think about how the protocol works versus the algorithm works. And one of the principal points of protocol design is know one, it's good to sort of do sooner than later and it's good to do thoughtfully. So there's sort of on the left side in the protocol design, know people in this know we're doing some work. I know Juan and team are all over protocol labs, have done a lot of work. You see the result of it with filecoin storage incentivization protocols which are tremendous and you kind of put them out for public dissemination and review. And it's really important to do this because the issue is that although you try really hard with your protocol to anticipate attacks, you don't really know how well they work until you get a lot of very smart people to look at attacking them. And so on the right side, there's this kind of arms race between protocol designers and algorithm attackers.
03:20:24.338 - 03:21:11.718, Speaker A: They go ahead and hopefully in the altruistic earlier phases, you're not actually staking money on things, or if you are, it's small amounts where you're just testing a market. They attack it. You learn about that. That's part of the public dissemination review, and then you do some editing. And in this, it's useful to define protocol versus algorithm. It's only kind of, I think, really been brought out by blockchain how different these two things are, because before blockchain, we had protocols for IP networking, but other than Dos, you didn't make money by taking down the internet. So you could grief and cause other things, but you weren't actually becoming rich doing it, whereas now that's not the case.
03:21:11.718 - 03:22:03.030, Speaker A: And so when we design protocols, we're designing a set of rules like what's valid and then what can be incentivized and what can be slashed. Both incentivization and slashing have to be really clearly defined because they have to be attributable actions. It can't just be that somebody reported somebody else did something, because that person could be an attacker as well. So ideally, you figure out mechanisms where there's crypto economic proof of what somebody did or didn't do. And the algorithm is a totally different mindset. In the algorithm, you're already taking the set of rules as fixed, and you're figuring out how to work within those rules to maximize for some outcome, typically to maximize for your own performance or for your own rewards. A really useful example that I find in this of looking at protocol versus algorithms, actually, John Adler was suggesting this one, so I'll give him credit here.
03:22:03.030 - 03:22:58.870, Speaker A: It's a is looking at if you're trying to actually have a protocol and then people who are participating the protocol around sorting. And we sort of all know there's different algorithms, and something like Quicksort does much better than naive sorting, having order n log n versus n squared. You can see protocols that have much better performance, like convolution in time domain versus doing FFT again has n log n versus n squared properties. And other examples of algorithms are that when you're thinking about algorithms, you can sort of think about statistical failures. They have certain properties, but you don't expect them to correlate beyond those. Whereas in protocols there's more malicious failures. And looking at things like the sorting.
03:22:58.870 - 03:24:14.266, Speaker A: The known algorithms, or three examples of algorithms are quicksort merge sort, and more recently with Python, tim sort. Tim Sort being the remarkable one, where it sort of looks at the data and then picks the algorithm so that it's able to actually get order n sorting in many common cases, and the average is still n log n, and even the worst is n log n. So it's really tremendous. But the main point on the bottom, I just want to say here is that the protocol recommends the use of something like Tim Sort, but the protocol can't force participants to actually do things closer to the best case. If you want your protocol to actually deliver the best performance, you couple it with making participants want to behave in the right way and wanting to behave in the right way is to get incentives that cause participants to try to give in input data that operates more towards order. N in tim sort or for some reason you have to use quicksort for some other reason at least avoids people from giving the worst case every single time for a quick sort algorithm. And so in ethereum, which is the space where a lot of us in consensus R and D are in.
03:24:14.266 - 03:25:31.080, Speaker A: We've been working for years now on the progression from proof of work to proof of stake. Ethereum two. And it's almost like this title really captures. It can feel frustrating at a time because doing protocol work and incentivization is not about finishing, right? It's just about making it harder and harder and making it more worthless where attackers have to try super hard and at some point it just is not worth it for them to do. The challenging aspect is that while the protocol designer, it's just sort of very easy that we think about things in the Halo Altruistic case and we consider the kind of rational player, the poker player in the middle, the malicious actor is there and in some sense they're not really different than the rational actor. We just got to kind of recognize that a rational actor who's optimizing themselves and harms the algorithm during it we can call them malicious, but they're just following the rules. And so it's kind of the responsibility of the people making the incentives in the protocol to get it as closed up as possible.
03:25:31.080 - 03:26:33.402, Speaker A: So along with that, I think the other view this is a couple of slides you can go to take a look if you haven't vitalik had a really good article about meaning of decentralization and it's important. These sort of definitions here is like architectural is really about the physical network, the computers, the failure domains. Political is about the governance. You can have something architectural texturally decentralized but still governed and logical decentralization is really about you can see this last one here of like when you cut the system in half. Did the two systems work as independent units? Probably with a lot lower performance, but do they actually still work? And then it's logical in that case. And this is another diagram from that same post by Vitalika a while back the net combined filecoin secondary retrieval market. The final objective is that it's decentralized in that bottom right corner.
03:26:33.402 - 03:27:21.978, Speaker A: So it's architecturally, politically, logically decentralized. But the important aspect is to get high performance and to really make it interesting to a lot of participants. The idea is that their individual participants can be in any of the other seven boxes as long as they eventually are in the net combined three way decentralized market. The protocol is sort of incentivized a good overall system. So that's kind of the mindset we're doing in our project. One other view here is from sort of the Scrum view of like think about your users first. What are participants bringing to a retrieval market? Right, they do bring some desire potentially to make filecoin work well and to make this market work well.
03:27:21.978 - 03:28:00.466, Speaker A: But if we want it to scale, we need to look at them more of like maybe they're disinterested parties and they have resources and that's what they're bringing. They don't necessarily bring a zest for filecoin, they're just people who want to participate, make money and help out eventually. We want to get to that point. So small scale participants, people like us who have drives at home, have machines around, maybe even mobile phones as we're walking around. If we have relatively unlimited bandwidth, we could be using it every month. There's no reason not to. So we have idle storage, we have idle bandwidth, we have idle compute.
03:28:00.466 - 03:28:29.374, Speaker A: All of those can be put into the market. We're not going to economically be as efficient as medium and large scale providers, but we have something really good as small scale participants. Our hardware and our bandwidth and our compute are probably already paid for. So we don't have to have our ROI pay for the capex. It just needs to be covering the opex and the administration. So we need those loads to be as low as possible. Medium and large scale providers, they do have bespoke potentially hardware that's just dedicated for this purpose.
03:28:29.374 - 03:29:45.590, Speaker A: But there's lots of idle resources here too. And so ideally as we're building filecoin retrieval markets, we should be looking at making it incentivizing enough. There are people of long term contracts in Amazon or they have their own hosted equipment on equinox and it's useful for them to be able to monetize it as well and worthwhile for them. I know of a couple of projects that are looking at that explicitly design goals a little more, talking about some of the objectives from this view. So one of them that I think is a really important one here is that there's an efficiency to how resources are being used. So some of the kind of naive ways to incentivize the market where a participant is getting paid directly, provider is getting paid directly, and they own that money. The second it comes to them aren't necessarily the best, because it can put providers into an adversarial relationship where they don't want to share query information with each other or they don't want to actually agree who's going to cache data, even though they both happen to be able to serve it with the same I ops, the same latency, the same bandwidth.
03:29:45.590 - 03:30:37.626, Speaker A: So allowing providers to kind of coordinate so that they're able to achieve the largest working set possible given the resources they have and the best load balancing and the best indexing is valuable. So it's important. The protocol covers encouraging the providers to be incentivizing, the providers to actually be collaborative in that sense. And the other very important design goal is that even though protocols are complex, if you look at proof of work in ETH one versus ETH two, proof of stake, proof of stake is incredibly complex. And similarly, some portions of actually being a file coin storage miner are going to be very complex. But joining and participating in the market and doing well in that market should not be complex. It should of course, be as hard as possible for the attacker to attack it.
03:30:37.626 - 03:31:26.474, Speaker A: But people who are doing the right thing should find it to be absolutely trivial. So that's kind of an introduction. We're going to be posting the information as we go in the filecoin retrieval, know we've gotten direction that the right place for us to be putting some of our intermediate stuff as we're going along is in filecoin project research. There may be some other kind of discussion boards as well. I'm not sure if that Starch will do that. And then there should end up being some components or things that will be in shipyard as well and really just welcome people if they have thoughts or if they're interested in reviewing when they see it, or collaborating even more. If it's an interesting topic, we're going to be happy to do that with everyone.
03:31:26.474 - 03:31:27.660, Speaker A: Thank you.
03:31:39.860 - 03:31:41.750, Speaker B: That was really awesome. Thank you.
03:31:43.480 - 03:31:45.430, Speaker A: Oh, it's my pleasure. Thank you.
03:31:51.610 - 03:33:09.840, Speaker B: Maybe on the latter point that you have right now. There are a lot of folks that are super interested in mining filecoin, but they have a lot much. They don't have as sophisticated of a setup as Falcon Storage miners demand, simply because all of the proofs and all the work and so on is like a just larger thing, smaller scale. We've always seen kind of smaller scale miners being very tuned for retrieval markets and so some of that scale could be like in the thousands or tens of thousands very quickly. But there is kind of like a gap between interest and kind of what's ready. Curious how you see the problem, maybe from your perspective on even just reflecting on what we've heard from today and maybe also a question for other folks on the call, how we see kind of the tooling that is there now. How far away do we think that is from meeting that demand from a bunch of folks who would like to mine Falcon, would like to be retrieval miners but aren't quite ready yet, or what do we see as the blockers? What can we do between now and maintain or soon after to kind of land that.
03:33:24.810 - 03:34:34.534, Speaker A: I think that was open to other people as well? I'll sort of make commenter as well. I think what we've seen in all of the different spaces you guys are doing it with, the filecoin storage market, is that ahead of actually having real money staked in the game, it's very valuable for us to start. Getting things that are still kind of altruistic and incentivized but starting to look more like testnets, where we kind of have the equivalent of we have our Drips and we have participants take funny money and we start testing to see are we able to get the comparable latencies and bandwidth. And I ops. If you look I'm very minded actually a lot of us on our particular project are very minded around the enterprise. We come from the enterprise infrastructure space earlier in our career and we come to blockchain with that kind of mindset as well. And we're interested in making this really be a replacement because if you look at it as the scope and the size of decentralized storage is really massive.
03:34:34.534 - 03:36:11.020, Speaker A: But one of the crown jewels that a lot of web two companies and web two cloud providers like to kind of point out is they're like, they like to poo poo decentralization by saying that, oh well, it's decentralized and that's nice, but decentralization is disorganized and decentralization won't get to these performance objectives and so forth. So we're going to want to do testnets where before we as soon as possible where we're able to test and see if people are operating with this kind of funny money token and they follow the protocol as if it was real money. And we bring in the caching providers that we're able to kind of get to data lake type performances, in some cases Amazon kind of touts and brags about you put prefixes and buckets together. Each of them can do 3000 I ops, each of them can do ten gigabits per second. And so if you want to get to 100 terabit per second type data analytics type applications and machine learning applications, just follow this design and then your system is doing it. So I think I'm interested in that, in pushing that left edge of the performance a lot farther ahead of what would be a filecoin retrieval incentivized launch for actual funding. But what I really would like to see and I'm interested ChainSafe and mike and so, I mean it's definitely clear that even for the early filecoin storage getting as much as possible, even altruistic people won't be attacking the system necessarily right away.
03:36:11.020 - 03:36:29.920, Speaker A: Yeah, it'll be really valuable and just exciting to see people using filecoin with a lot more performance, I think, than they would have been getting out of, well, maybe not right away, but hopefully soon getting out of IPF storage as well.
03:36:36.560 - 03:37:47.456, Speaker B: One thought about and totally agree, totally agree with the approach and pushing the performance edge and I think if we kind of methodically go towards that we can even soon enough end up with something pretty remarkable there. I think a lot of interesting things start appearing for the kind of applications that you mentioned around machine learning and so on. The work that Lightpeer is doing, for example, amassing GPU clouds and so on, coupling a GPU cloud close to falcon storage or something like that, that could work really well. Where you can very quickly get a highly scalable machine learning cloud right away. But I think there's probably a whole bunch of work in optimizing the flows to get it to work in a straightforward way, but it'd be great to kind of work towards that. One interesting data point is there's probably around I think the current installs on the companion interface is like in the tens of thousands. I think MetaMask has millions of users and so on.
03:37:47.456 - 03:38:23.630, Speaker B: Right. So one very straightforward thing here might be like thinking about IPOs companion as a target of saying, okay, well, what it would look like to start earning money as a retrieval miner directly from the browser. Right. This might also be something that Brave might be interested in as well because they're already kind of in the crypto space. So Mike, to your point about having an extension and using that as a deployment path, there might be like a straightforward thing there with Brave users where suddenly just dedicating so much of your storage space to the Brave browser suddenly starts being a significant revenue stream for you.
03:38:26.320 - 03:39:32.196, Speaker A: Yeah, and there are some components in here that we'll look to see if we want to plug in on our team, the State Channel team. If people know in the state channel space there's a team MagMo. They currently have joined in with Consensus R and D, but with Ethereum they really have done tremendous work on building out state channels to have kind of channels within channels that allow them to kind of be l two squared type scaling versus simple payment channels that people think of that always exit and enter from the primary chain. And they did a POC earlier this year of implementing an incentivized State Channel system on top of WebTorrent. They called it Web three torrent. It may not be exactly right in the sense that there's already graph sync and bitswap in here, but it's an interesting potential POC that we could do sooner than later to look at that kind of scaling side. I really do also want to point out when you made that good point, Juan, about the cloud providers looking at this.
03:39:32.196 - 03:40:41.572, Speaker A: When I showed the diagram of the different types of decentralization that Vitalik kind of drew out in that diagram, interesting point there is right now there's a big friction sticking factor of clouds is the storage. And it's a really important one. They all do the kind of Roach motel style where you can check in but you can't check out. So putting data into any of these clouds is essentially free. Then you pay some storage costs, but you don't think about that upfront. But then when you want to take the data out of there, serve it out to the network and other internet or so forth, it's extremely expensive and in fact it's disproportionately expensive in the sense if you look at cloudflare, they're able to actually make their business model work without charging per gigabyte fees on data movement. And it's because the cost of actual data movement on the Internet is so low and it sort of exposes that the cloud provider's costs are not there to make money, but it's a little more to discourage storage once it's on their network, to be easily connected to other storage and so forth.
03:40:41.572 - 03:41:44.476, Speaker A: So what's cool about IPFS is if we can drive and get some of these machine learning cases that are using common data sets, genome data sets, some of these massive data sets that are in the public interest, that are accessible there. It points to a really good model in the future, which it's almost like how Apple defeated the cell phone market by producing the first ever cell phone that wasn't owned by a manufacturer. Right, or by a network, I should say. They forced the iPhone even though it was exclusive to Apple 15 years back, even though they forced it was exclusive to at and T for like three years. At and T did not own the software stack for it. And that really is what I mean from a business standpoint, spurred the actual smartphone revolution because now we had a software platform that was separate and very quickly Google bought Android and jumped into that and we had that really good quick acceleration of that market. I feel like IPFS getting cloud providers to recognize that they have to host IPF storage because that's where their applications are going to be.
03:41:44.476 - 03:42:14.490, Speaker A: Doing data processing and other stuff on will probably force them to a different model where they just have to be in an IPFS retrieval and even storage market and then all of the cloud providers will be competing with each other to have the best IPFS storage inside of their clouds. So it's a really nice side effect of this that this is going to end know, causing a lot more effective decentralization to happen across the providers that they just won't do economically on their own.
03:42:16.700 - 03:42:36.430, Speaker B: Yeah, that's an awesome route to end with. Thank you very much, Robert. I think we have one more talk from Thomas. Yeah, thank you very much, Robert, that was awesome. Great discussion, Thomas, take it away and then after your talk, I'll close briefly and that'll be that for today.
03:42:37.060 - 03:43:06.148, Speaker I: Yeah, thanks. Really excited by all these talks. This was really amazing. Yeah, so I'm on the team at Hackface, and I don't really have a demo yet. It's been kind of broken lately. But I'm just going to talk about a little bit about what we're building. And so basically the use case that we went for is how do we kind of bootstrap a retrieval miner and onboard them on the network.
03:43:06.148 - 03:44:03.592, Speaker I: And I'm kind of the primary user. I have a machine here and I'm hoping I don't have that much power, but I'm hoping that I can join the network. So I was looking at how do I serve this data on the retrieval market. And so basically what we have kind of built currently is we're kind of creating this kind of we call it a deal Faucet Node. And the idea is as a new retrieval Miner on the network, you can subscribe to it and it starts just sending you deals just like that. And the idea is that the deal Faucet Node can then subscribe to those deals and follow what's going on with them and kind of measure things. And so the idea is that the deal fault set node is we want it to run a statistical model and just kind of start.
03:44:03.592 - 03:45:36.040, Speaker I: The idea is right now what we're doing is just a quick way for us to switch models and try different strategies and see what works and what doesn't. Right now the deal Faucet Node has a hard coded list of CIDs and we're just trying to see how that works out for a new Miner to just subscribe to it and start new deals and send the data about those deals. And then ideally the deal faucet Node would start kind of recording more and more data about those deals and have more information. So right now we're looking at really basic parameters like just the location of the node and the clients and then the content size, just kind of looking at if we can have any start weighting CIDs and just sending the CIDs that we think has the highest probability of being retrieved from that provider. And so ideally, any type of Miner who's already been on the network for a while could kind of run this deal Faucet Node and just be able to just kind of send out those deals to anyone who subscribes and then in exchange get the data about it. So it'd be kind of like this exchange of things of data and maybe a storage Miner could run that since they already have those CIDs and they can be incentivized. So we're just trying to make this work and it's been a little hard just like getting to know exactly everything and how to make it happen.
03:45:36.040 - 03:46:00.464, Speaker I: But I think we're pretty close and we're also building a UI for retrieval Miner to exactly see what's going on. So they kind of have an idea of understanding what's going on because I think there's a big barrier of understanding and learning about the whole process. So we're trying to make it easy for someone who just has no idea how it works, has a machine and just wants to onboard and gets a few CIDs and is like oh, I'm a binder now.
03:46:00.502 - 03:46:01.360, Speaker B: This is how it works.
03:46:01.430 - 03:46:08.560, Speaker I: So this is kind of what we're hoping to get sorted for the deadline.
03:46:11.860 - 03:46:43.710, Speaker B: Yeah, that's really cool. How do you envision kind of the statistical models evolving over time? Do you think those will be kind of like this deal faucet will be kind of like a single independent operation and they'll run their own model locally? Or are you kind of envisioning a world where there's information aggregated by different parties and then they then share that information or share parts of the model? There's all kinds of hard market incentive problems there. It's probably easier to treat them as independent. But I'm just curious how you've been thinking about the problem.
03:46:44.480 - 03:47:22.440, Speaker I: Yeah, I mean, right now we're definitely just trying to make our use case work really minimally. But ideally, yes, we want to be able to kind of share statistical models across nodes and hopefully make it possible for anyone to kind of just run their own faucet if they want to. And I think it would be an ecosystem of actually sharing CIDs for I think the more you can share, the more places you can just publicly go and subscribe to nodes and get data for onboarding. I think the better it will be instead of just kind of closing things off and trying to be really private.
03:47:26.440 - 03:47:44.156, Speaker B: Yeah. Awesome. Cool. I don't know if you had anything else or if other folks had any other questions. Great. Sounds good. So I'll just close that.
03:47:44.156 - 03:48:01.952, Speaker B: Thank you so much, Thomas. And yeah, good luck with the hack. That's really cool. I'm looking forward to it. Yeah. So thanks everybody for being part of today's day and sessions. We looked at a lot of really interesting parts of the retail market as it's kind of coming together.
03:48:01.952 - 03:49:14.008, Speaker B: And also looking ahead, I think there's a lot of awesome pieces that are coming together now and I think we're still kind of within a chunk of time between now and Mainet. I think that there is a very large opportunity for creating some kind of UI oriented thing that can there is kind of like a hard problem there around what should party store. But I think kind of like some straightforward implementations of it. Now, even if they're not perfect, could actually meet a lot of the demand from a lot of folks out there that want to be able to mine filecoin and want to be participating in the network, but don't have the time or capital to go and build out a much larger set of machines. So I think, yeah, there is a pretty interesting open opportunity there. I think one side of things that we didn't get a perspective on today was actually hearing from storage miners. I wonder they might have a whole different perspective on this where they might have some setups already now and they might already be thinking about how to plug into retrieval networks and so on.
03:49:14.008 - 03:49:54.588, Speaker B: So maybe that's a good to do for me to maybe potentially organize something there where we can hear from storage miners as to how they're thinking about their storage facilities now and how they're envisioning kind of indexing all. Of the data that they're storing and how they're thinking of kind of plugging in with the whole retrieval network and maybe kind of hearing from that, we can collect part of that into kind of the design of these systems. Thank you very much. Thanks to all of the folks that spoke and demoed. Huge. Thank you for also trying out things on the fly. It was really great session.
03:49:54.588 - 03:50:01.290, Speaker B: And thanks to Trent Andrew for running the whole so thanks so much.
03:50:02.700 - 03:50:10.760, Speaker C: We'll see everybody in the session in a few minutes if you're coming to watch or give feedback to demos. So see you. Thank you.
03:50:10.830 - 03:50:11.190, Speaker E: Thanks, everyone.
