00:00:00.250 - 00:00:11.150, Speaker A: Zena, who is with the Ewasn team, and his talk will be about how to improve user experience for Light clients. Welcome, Zena.
00:00:16.000 - 00:00:50.070, Speaker B: Oh, it was on mute. Hello, everyone. This is Zena. I don't know what time zone you're in, but it's an evening here in Berlin and you got to give me a minute to share my slides and we'll jump right into it. I'm sorry, this is just going to take a few seconds.
00:00:51.210 - 00:00:53.000, Speaker A: No worries. Take your time.
00:00:55.790 - 00:01:23.776, Speaker B: Yeah, Google is asking for capture. Can you guys see the slides?
00:01:23.968 - 00:01:25.556, Speaker A: That looks wonderful.
00:01:25.748 - 00:02:37.070, Speaker B: Okay, perfect. So, yeah, this evening I wanted to talk to you guys about Light clients, and this is a topic that I care about because my work on the protocol started with the JavaScript implementation of the EVM, and I really like it to become a Light client in the browser one day. And so I decided to work on this topic, and I feel like this is a topic that's not being discussed as much recently, even though it has good synergies with Stateless Ethereum and the recent discussions around client diversity. With Light clients, we can have clients that don't have as much implementation complexity. They don't need to implement all the old hard fork logics. They have a simpler networking stack. And yeah, it's good to be able to have nodes running in the browser or on the mobile without relying on third party services.
00:02:37.070 - 00:04:01.348, Speaker B: So Light clients face multiple challenges. Although we have les that's running, one of the main challenges that they face is that in order to have a Light client, you need a Light server, and the servers need to respond to requests from the clients. And currently, this is an interactive process, so my mobile has to send a request to the server saying, hey, give me this data. And this causes these servers to become easily overloaded. And therefore, although most of them are currently altruistic, we need complex incentivization schemes for them to work in scale. So kind of the ideal scenario that I was imagining, and this was first expressed by others like Turbogeth Team, is to have another subnetwork with mostly Light clients, but some full nodes that just broadcast data so that the Light clients don't have to request data. But let's say a new block comes in, the full nodes broadcast the block itself and stateless block witness, as well as a short proof that shows the total difficulty of that chain.
00:04:01.348 - 00:05:07.820, Speaker B: And this will be the focus of our talk. So we'll talk more about that later on. And this allows the Light clients to be able to follow the tip of the chain in a few seconds, and the amount of data that Light clients would have to request from the servers would be minimized. So, as I said, our focus here is determining the canonical chain. With that, I mean, let's say we have a couple of computing forks and we want the Light plan to be able to safely detect which one is the correct one, the one that everybody is using and currently the way that this is done. So the most safe but also most demanding way of doing this is to simply download all of the headers from Genesis until the last block and verify all of them and this would be just verifying the header, not executing the body itself. This is what the full nodes are doing.
00:05:07.820 - 00:06:26.036, Speaker B: But for Light plans you don't really need to do this especially when you consider that the total size of the block headers is somewhere between five and 8GB. So if you want to have a client on the mobile or a browser, this is already a nonstar. And the current Les protocol works around this by kind of hard coding some checkpoints in the client in the form of CHTs. And then you don't need to really verify the blockheaders from the beginning, but from the last checkpoint. But we want to see if we can kind of avoid this having this hard coded checkpoints. All right, so our design space is I broke it into a simple spectrum and these four items that you see are kind of what we will be discussing during this talk. We have full verification that is very not complex at all in implementation wise but it requires the most bandwidth of all and then we have a hypothetical zero knowledge protocol.
00:06:26.036 - 00:07:26.840, Speaker B: We have Flight client and we have EIP 29, 35 that is being discussed for Berlin or the hard torque after that. So let's jump right into ZKP. This is a hypothetical approach. It's not being really suggested as being implemented in ETH one mainly because zero knowledge protocols are really complicated and we want to avoid bringing them into consensus. But the approach would be that each block would come with a short zero knowledge proof that proves that there is an unbroken chain of blocks from Genesis to this last block and that the block headers of all of these blocks are valid according to the consensus rule. And it also tells us the total difficulty of the chain. And we use this total difficulty to be able to choose between forks.
00:07:26.840 - 00:08:46.420, Speaker B: What I want you to take away from this approach is the ideal properties that we are looking for and these are ideally short proofs. So for ZK Snarks, this is definitely less than one kilobytes and importantly non interactive proof. So you don't need to communicate with the client, with the generating or the miner to verify that. So somebody like a miner can produce this proof and just send it to everybody and everybody can verify it on their own and of course security is important and zero knowledge proofs have very high probability good security. And now to the flight plan and this was a paper published in 2019 I think by Benedict Boons and it offers some good features. It offers short and non interactive proofs of around 500 megabytes, depending on some configurations. I think this was 500 KB was in their paper for 7 million blocks.
00:08:46.420 - 00:09:36.592, Speaker B: It's secure with really high probability. It was recently activated on Zcash and it's being built by default in some other blockchains, I think on Grin and Beam as well. And it can be configurated to tolerate more or less dishonest mining power. But of course, Flight client protocol needs some changes to be made in consensus. And it boils down to adding a new field to the block header. And this new field is the root of a tree that includes the block hash of all the blocks since Genesis. And the tree in question is not a normal Merkel tree.
00:09:36.592 - 00:10:14.656, Speaker B: It's a customized Merkel mountain range. Let's call it the difficulty MMR. Just as a refresher, this is how a normal Merkel mountain range looks like. And the color of the leaves don't have a semantic meaning. That's just because here it's already autumn, so the leaves have changed their colors. And this is an append only data structure. And here we have 15 leaves inserted from left to right.
00:10:14.656 - 00:11:03.136, Speaker B: So you can see kind of that it turns into multiple binary trees next to each other. And it's very efficient. Insertions are very efficient in this data structure. And you get very short proofs when the leaf that you're interested is in is very recent. So the older blocks would have longer proofs, but the more recent ones would have shorter proofs, which is perfect for our use case. And now in Flyclin, they've changed this MMR to include some additional metadata. And the more important ones are regarding difficulty and the time it took to produce that block and so on.
00:11:03.136 - 00:11:42.636, Speaker B: And here you see like a really simple example of a difficulty MMR with a simplified difficulty MMR with three leaves. And you can see that the leaves have the block hash itself. But also, let me actually turn on the pointer. Yeah. So you can see that this leaf has a difficulty of five and this one has a difficulty of seven. And when you go up, then you add the difficulty to get a difficulty of twelve. So this way you add the root of each of these trees, you have the total difficulty accumulated in all of those blocks, which is a great property.
00:11:42.636 - 00:12:39.780, Speaker B: And this is used extensively in the protocol. Now, how do you use this tree to actually sync a Light client? Let's say we have two competing forks. We have an honest miner who's advertising a total difficulty of 700, and we have a shady looking guy who's advertising a higher total difficulty of 1000. And the Lightline doesn't know which one is the canonical chain. So what it does is first sorts them by total difficulty. So the one that has 1000, it will check it first, this fork first. And the gist of the protocol is just to do random sampling of the blocks.
00:12:39.780 - 00:13:33.840, Speaker B: So you don't verify all of them. You check some of them randomly and you can see that, okay, this chain has forked off at this point here and we're checking some of them. Like if you see the ones that have a green circle around them, let's say we are checking those randomly and in fact we've checked three and one of them is an invalid block. So we caught you, Mr. Shady. Okay, so the core of whitesand is in the sampling strategy. As you can imagine, just doing a uniform sampling won't be really efficient or sufficient.
00:13:33.840 - 00:14:37.444, Speaker B: The goal of the sampling strategy is to maximize the chance of catching an invalid block regardless of the advertiser's strategy, like whatever strategy they're using. We want to maximize the chance of finding if there's an invalid block in there. And the way it turns out is that it's like a probability distribution where we sample more from the end of the chain, like we sample a few from the beginning and as you go towards the end, we sample more and more. This is just because an attacker could fork later on to use the work that was accumulated by Arnis miners. And an important thing to note is that we are not sampling in the block number space. So we're not saying let's say we have a chain of 10 million blocks. We're not saying give me the block 5 million.
00:14:37.444 - 00:15:42.804, Speaker B: Rather we're saying give me the block with half of the total work. And we can do this because in the Merkel Mountain range we are storing that extra metadata that tells us the total difficulty of each subtree. The other point to consider is that each sampling step is independent. You don't have to first get one block and then depending on whether it's valid or not, check another one. They're all independent. So what we can do is use the fiat shamir heuristic to make this process non interactive. And that's great because then the miner can just, when there is a new block, just produce short proof of all these using some public source of randomness.
00:15:42.804 - 00:17:00.130, Speaker B: For example, the parent blocks hash. It uses this parent blocks hash to get a random seed and get the list of blocks that have to be included in the proof and send that to the like plants along with the necessary Merkel proofs in the Merkel Man range tree. Next we have EIP 29 35 and this is an EIP by Vitalik. And it's a much simpler change compared to Fly plan in terms of consensus complexity. And the difference is that we're not this time storing the Merkel Mon range in the block header. Rather we are using the existing Ethereum Try to store these block hashes. So, as you probably know, let's say this is a block and the block has a state root field which is the root of the account try.
00:17:00.130 - 00:17:52.450, Speaker B: So what we do is we hard code an address we say this specific address is like a system contract, let's call it the history contract. And this has storage slots like normal contract. And in the storage slots of this account we store the hashes of the blocks and the blocks since the hard fork, not since Genesis. This is an important distinction. So we are reusing a lot of the data structures that are already part of consensus, namely the Merco Patricia Tree. So the motivations of this EIP were stated as Light client sync what we are discussing now. But it's not limited to this.
00:17:52.450 - 00:18:44.864, Speaker B: Neither this EIP nor Flight client, they both are good for other purposes as well. Another one is, for example, like layer two state providing networks. Let's say clients like Get start pruning history. So as a new client, you don't have access to the older blocks. And when there is a commitment to the block hashes in the state, then what you can do is ask a third party to give you that block along with a proof so that you can be sure that this block is indeed the third block in the chain, let's say. And this layer two solution can be incentivized as well. You can also use these for stateless witnesses because you have the block hash upcode.
00:18:44.864 - 00:19:44.176, Speaker B: So block hashes of previous blocks need to be included in the witness. What we want to address here is if we can build a light plan sync protocol on top of this EIP. And we'll consider two approaches, two variants. One is very similar to Fly plants and it's a random sampling approach. The other one is a superblock approach and superblock, I'm borrowing the term from Nepal Power, another light client, Singh Paper. So first, let's see what advantage does this EIP bring? Let's say we don't have the EIP 29, 35, let's say right now. Why can't we do random sampling? The reason is that the Life client doesn't know when it's asking for a random block.
00:19:44.176 - 00:21:00.360, Speaker B: Let's say block 7 million. It doesn't know that these blocks that he's asking for are actually chained together. The attacker could just give some random blocks that are not even part of a single chain, but committing to the whole chain in the state. It makes it harder for attackers to do something like this. But there are also some challenges in doing random sampling on top of this EIP. As you saw earlier in Fly clients we query in the difficulty space. So we are asking what is the block that has, let's say, half of the total work? And this is important because otherwise an attacker could create a long chain with many low difficulty blocks but valid, like from a proof of work point of view and hide a few high difficulty but invalid blocks in the middle.
00:21:00.360 - 00:22:04.180, Speaker B: And when you're randomly sampling, it's really hard to find them if you're sampling in the block number space. Again, because we're not using the MMR and we are stuck with the Merco Patricia tree, we can't do sub range checks. So we can't say, let's say when we are verifying an old block, we can't really check that this old block is actually the tip of a chain, that is a prefix of the bigger chain, like the canonical chain. Again with a merkel patricia tree. This is a really inefficient thing to do. So we are kind of stuck with doing uniform sampling and in order to, if you want to have good security with uniform sampling, you need to sample a lot. So this means doing this approach either entails higher bandwidth requirements or it has lower security guarantees.
00:22:04.180 - 00:23:38.684, Speaker B: So as an alternative to random sampling, vitalik proposed an alternative which here I'm calling super block based sync. And the intuition here is so as you might know, the blocks in ethereum, they have a kind of a difficulty target and a proof of work solution is valid only if it's exceeding this target. And normally most of the blocks, as you can see here, so this red line here, let's say that's the target over time and these blocks, they barely pass that target. But sometimes, by chance, some blocks have a much higher target and there is a relation. So blocks that have 1000 times more difficulty than the target are 1000 times more rare. The idea is to use these really lucky blocks as a way of compressing the chain and he vital estimates the proof sizes. So let's say the idea would be just send all of the lucky blocks that have 1000 times more difficulty than the targets as a way of showing that this chain is canonical.
00:23:38.684 - 00:24:43.172, Speaker B: Because an attacker with lower mining power cannot generate so many superblocks. Proof sizes of including all of these, vitalik estimates them to be around 25 megabytes. And this includes all the superblocks, but also a few thousand blocks at the tip of the chain. And we might need to double this estimate because we also need to send the parent headers to be able to verify each block. So, regarding security, Nepopau was advertised for constant difficulty chains. If the difficulty of a chain is constant over time, then an adversary with less than 50% mining power has a really low chance of being able to produce more superblocks. But in ethereum, difficulty can be adjusted, it can be changed over time by 5%.
00:24:43.172 - 00:25:28.260, Speaker B: Over time each block maximum. So sketch for an attack could be that the attacker tries to bring down the difficulty so that they can produce more superblocks. But this can be easily avoided by modifying the fork choice rule at the tip of the chain, as I mentioned before. So these blocks that have 1000 times more difficulty, they are kind of rare. So it could be that the last few thousand blocks don't have any of these superblocks. So we have two options. Either send all of the blocks after the last superblock as part of the proof.
00:25:28.260 - 00:26:16.020, Speaker B: Another one is to send Superblocks of lower degrees or lower levels, let's say lucky blocks that have 500 times the difficulty or 100 times, and so on. So in this talk, we saw all the spectrum from full verification to zero knowledge proofs. But I think in the middle, between full verification and ZKPs, we can find some good middle ground. Fly client has good features. It has multiple use cases, but it has somewhat higher consensus complexity. EIP 29, 35 has lower complexity. It might need higher bandwidth.
00:26:16.020 - 00:27:15.750, Speaker B: The algorithms that we discuss haven't been formally analyzed, so this is something that has to be done, but it might suffice to replace CHTs. And in future, what we plan to do is come up with more detailed algorithm for the Superblock approach and prototype both the Superblock approach and the flight plan approach and compare them quantitatively for more exact comparison. So that was it. There is also a text format for the same presentation. You can find the link on this page. And otherwise, if you have questions, I'll be available on the chat or on Twitter. Feel free to reach out and I'll be happy to take any questions.
00:27:17.320 - 00:27:41.440, Speaker A: Amazing. Thank you so much, Zena. I think we have time to take one more question from the chat, and I believe there was also some more discussion going on, which you can then maybe take offline after the talk. But one question that we want to ask is from TRX 3114. What are the implications of this architecture in terms of privacy for the users of the Light client?
00:27:45.060 - 00:28:29.160, Speaker B: I don't believe there should be a negative effect on privacy for any of these approaches. In fact, it might be better because we're trying to reduce this request response mechanism. So when you request something, when you request a block, that's an information being leaked to the full node, but when you make it broadcast, only then the full node doesn't know who is getting this message, this proof, and what they're doing with it. So I believe it might be better for privacy. But, yeah, I'll be happy to think more about the privacy consequences.
00:28:30.740 - 00:28:43.020, Speaker A: Okay, amazing. Yes, I think there was some discussion going on in the chat also, so maybe you want to have a look, and then we will wrap this up here. Thanks so much for joining us. And thank you, Francis.
