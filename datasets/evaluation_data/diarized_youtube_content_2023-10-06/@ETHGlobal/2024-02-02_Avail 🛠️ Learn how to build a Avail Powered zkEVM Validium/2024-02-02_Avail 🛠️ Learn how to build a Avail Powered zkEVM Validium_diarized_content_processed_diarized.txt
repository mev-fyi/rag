00:00:00.170 - 00:00:49.366, Speaker A: Hi everyone. This will be a short walkthrough of our avail validium repositories. So of course the first thing that we want to look at is the avail module itself. Just note that since the underlying polygon CkbM node is written in go, of course our implementation or other implementation of that has to be in go as well. So we have this particular file called Avail co on the underlying layer we use something called GSRPC, which is basically go based client for working with substrate. And then we have forked that as well. And so we have our own custom fork of Avail GSRPC that we use to do everything from sending data to querying data, to set dispatching data routes and so on and so forth.
00:00:49.366 - 00:01:27.622, Speaker A: So let's delve directly into the init function. We basically have this file called avail config. So if we go here and this config go, what you basically expect is a config file that conforms to this particular struct. So you have json in your json you can write seed and then you can put the seed of the account that we should use to dispatch the extrinsics from the API URL. Basically the RPC URL. The app id is the one that you will be creating through create application key. The destination domain will be 1000.
00:01:27.622 - 00:02:10.758, Speaker A: For ethereum, the destination address will be the address of the dabridge router. And then the timeout will be whatever the timeout you want it to be. We just use it to have sort of a fixed timeline for finalization of your particular extrinsics. Okay, so let's go back to avail co in here. What we basically do is we just store all the variables that we'll kind of need to perform repeated queries. So for example, we get the latest metadata, we create the API, we get the config and so on and so forth. And the reason we do that is so that with every other API call, for example in post data, we have a couple of other calls as well.
00:02:10.758 - 00:02:53.314, Speaker A: We don't really want to incur the cost of this again and again. And then that's pretty much all that we do. So we just set the variables, or you can think of them as almost global variables here. And so kind of what we're really interested in is this post data function here, right? So what we do is we construct an extrinsic of the da palette called submit data. And then we just send the TX data and that's pretty much it. I'll show you as well where we kind of call this submit data in a bit. But yeah, basically what happens is we create this particular extrinsic and we create the signature options.
00:02:53.314 - 00:03:42.434, Speaker A: We sign it with our key ring pair and that's pretty much it. And then what we have is like this, a loop that runs for that timeout period that we specified earlier. And so for example, if it is in the block, we just emit that as part of the log. If it is finalized, that's when you sort of break out and this is what kind of timeout is keeping track of. So we just emit a log here, then we hash it. And this transaction hash will become important because we submit it to the ZKavm contract on the l one layer, whatever that is, to sort of prove that the data that we are saying that we're sequencing is actually being posted to avail. So with here what we do is we try to find the extrinsic index so we can keep track of that while 70 the proof as well.
00:03:42.434 - 00:04:33.650, Speaker A: We get the header in which we have the block number and this block number is what you use to kind of fetch the data route from the contract later on. And so we also have this function called dispatch data root, which is doing similar to how post data is sending data in the extrinsic. With dispatch data route we can give it a header. So you will see, we give it the header, we query the header here and we send it the entire header. And then on the other hand, on the l one da bridge order contract, it is able to send that entire data route. So it'll basically map a block number to a particular data route. So again we make a new extrinsic, we get the next nons, we again craft the extrinsic signature options and then finally we submit and wait, similar to how we waited for the previous extrinsic.
00:04:33.650 - 00:05:31.030, Speaker A: And finally we have get data. So this is required for our synchronizer. For example, let's say you're running a new valadium node, right? And then you kind of want to sync your node. That entire process is not that straightforward to the valadium because the data does not live on l one. So what we do is we have this get data function here and then based on the data that we already indexed from that particular l one contract, we will be calling. As you can see, we kind of get the block from a full node with the block hash and then we search for the particular extrinsic where our data is stored, basically. So what we'll then do is that the unsync validium node can then sync using the transaction data that are posted to avail, which then allows you to kind of build trustless sequencer sort of models that would not be possible otherwise.
00:05:31.030 - 00:06:10.264, Speaker A: And then this is just a helper function that kind of gets the next nons from substrate. It's pretty trivial. What we are more interested in is basically the finalizer logic. So in the sequencer module we have something called finalizer Co. And this is where we have sort of readjusted to use avail. So for example, if I were to show you, let's go to close batch. So we kind of get raw transactions when we encode it.
00:06:10.264 - 00:06:57.912, Speaker A: It's basically an RLP encoding of the transactions in a particular batch. And a batch is basically a collection of transaction data across multiple blocks or anything, to be honest. So basically if the block is non empty, what we do is we retry avail three times and then we attempt to post the data and then we kind of hash it. We store everything related to that particular batch so that we can submit it later and then we close the batch. So similar to how you would have the normal function ongoing, we just sort of intercept the closed batch flow and we send over the data to avail. And that is pretty much what we do here. And then of course we dispatch the data route as well.
00:06:57.912 - 00:07:43.428, Speaker A: So we can access the data later on. Or basically we can attest to that data on l one. So that is what we have here. Maybe I can show you the sequence and logic here. The logic is sort of complicated, but the thing that you need to understand is now in this sequence sender, what do we do is we sort of combine multiple batches and then we start sending them as a batch as possible. As the number we can send, the maximum number we can send in a single transaction. So let's say we have batch X-Y-Z and we include x and y and then the transaction is full.
00:07:43.428 - 00:08:40.090, Speaker A: What we do is we just send XY with the corresponding sequencing data on l one. And that's basically what is happening here. So as you can see, we have also sent the DA block number, the DA proof, the DA width and the DA index while we submit a sequence. And that is because of the data attestation function that we have on the l one contracts, or basically what we call your roll up contracts on l one, which does a data attestation verification. So basically if you have no l two data, we don't really care if you have any l two data and then the data route has not been dispatched yet. Then we sort of just wait until the data route gets dispatched, because in the optimistic pattern, it can take some time between 20 to 30 minutes to kind of do the entire flow. And then if we observe the data route is already available on l one, then we just append it to the sequence and then we send it.
00:08:40.090 - 00:09:05.536, Speaker A: And that's pretty much it. On this side of things, we can kind of look into seeing synchronizer a bit as well. So let's do get data here. That's sort of what we are kind of interested in. Let's say the DA block number for this was zero. So this is relevant in the case like, let's say you are roll up migrating to a validium. Then of course your previous sequences will not have any DA block number.
00:09:05.536 - 00:10:02.156, Speaker A: So technically what we do is we give it the DA block number, we give it the DA index, and we are able to get the raw transactions from avail and we then re execute the block as if we have the data already. And that's sort of the trustless sequencer model that I was talking about earlier, which function like this enables through the synchronizer module. And then, similar to here, it's basically when we observe the event on l one, we immediately fetch data from avail and then we ensure, just for sanity, that the data that we posted is actually correct. So it's sort of like an extra layer of ensuring data availability. So that's pretty much it on the node front, I'll do a short walk through through the contracts. So similar to how polygons EKM have polygons EkVM as well. We have our version of the polygon ZKVM.
00:10:02.156 - 00:11:00.790, Speaker A: So to say, we can see that we have inherited this DA router verification contract. That's sort of the extra library that we have added. So for example, if I had to show you, let's go to derouter verification. So we basically just have a router that we set and then this is just some simple merkel root verification and that's pretty much it. So going back to polygon ZkevM, if you go to sequence batches, that's sort of the important function that we have where we sign up, submit the batch of l two data, and then you submit the DA data. And then with the DA data, we're able to check for data station. So with the block number, the proof, the width, the index and the batch hash that, if you recall, we submit from the sequence sender, we can indeed prove that some data that we're kind of claiming to submit here has been also posted to avail.
00:11:00.790 - 00:11:47.408, Speaker A: And then basically the rest of the flow sort of remains similar. We have, for example, first batch data as well. We do some comparison checks here, and this is what's important here. This is what's the current related input hash, where we kind of take the batch hash of what we have submitted, and then based on whatever is previously stored in the contract, we kind of create a new accumulated input hash. And this is what the prover is actually proving. It's proving the change from the previous accumulated input hash to the next input hash. So since we proved data attestation at the time of sequencing, we don't really need to post it again at the time of proving.
00:11:47.408 - 00:12:44.404, Speaker A: And this kind of helps you save on prover resources, because then the proverb doesn't waste its computational resources on basically unattested data or useless data in some form. And that's sort of kind of all the crux of the changes that you have made. As you can see, it's pretty straightforward to implement. We also plan to kind of change this over to use our Vector X bridge, as well as also use the CDK release that will happen in the late February. So this version might kind of change drastically, but from an implementation standpoint, it will kind of remain fairly similar, because the code that we have today is folks from Polygon CKVM, as will be CDK. So the scope for changes will be quite low in that regard. But still, it will be like a decent level change.
00:12:44.404 - 00:13:08.570, Speaker A: So you should be technically aware we'll kind of publish the documentation for the new bridge in the coming days, so you'll also have more clear ideas on what we're doing. And once we have that, we'll also re implement the validium with CDK, so to say, and that's the one that will be officially supported going forward. But if you wanted to build a validium today, this is the solution that you have, and feel free to use it. Yeah, that's pretty much it. Thanks everyone.
