00:00:00.170 - 00:00:15.920, Speaker A: Joining, we have Carson from Textile, who's going to be giving a workshop titled Buckets of Data Easy APIs for off chain data storage on IPFS and filecoin. Thanks, Carson. Let you take it away.
00:00:16.450 - 00:00:35.574, Speaker B: Hi, everybody. Thanks for joining. Hopefully a couple people can keep rolling in. I'm going to start sharing my screen and just jump right in. Cool. All right. Yeah.
00:00:35.574 - 00:01:02.400, Speaker B: So thanks for joining us. Super, super glad to be participating in another ETH global event. These things are always super well organized, so it makes it super easy to contribute. Anyway, I'm Carson from the Textile team. I think we're a sponsor of ETH online this year. So very excited. And that means I get to kind of force you to listen to me for a little while, which is great.
00:01:02.400 - 00:01:53.310, Speaker B: So we did hack a fest just recently, and that was a ton of fun. We got a lot of really great hackers. We met a bunch of mentors and other people who are exploring. And so we're really happy to be contributing to this event. Again, it's super easy to kind of get involved, and we learn probably just about as much as any of the hackers learn about Textile stuff along the way. But anyway, I'm carson, I'm going to talk a little bit about what textile is, a little bit about some recent additions to our tech stack and a little bit about onboarding hackers and developers to actually start storing some. Off chain data in some kind of new and exciting ways that I think you're going to find pretty interesting and quite nice and scalable.
00:01:53.310 - 00:02:47.170, Speaker B: So first things first, I get to talk a little bit about Textile. We're a sponsor, so like I said, you got to listen to me for a little while. But I think in the end, I'm going to cover a couple of things that can be pretty valuable for everybody here, especially if you're starting a hack or something like that, if you stick with me to the end there. So Textile is first and foremost a company driven by a mission. And that mission revolves around the idea of a better web, a data driven web, under the control of the people who are producing that data, namely users. And I've actually spoken about this topic a bunch of times at other ETH events, so I'm not going to kind of hammer you on the head with the user data story, but there are a couple of videos that I can link at the end to some talks we've given about this. And the idea there is really just putting the control of where and what data goes where in the back, in the hands of the users.
00:02:47.170 - 00:03:52.050, Speaker B: But also, Textile is a company that builds things. We build tools for developers to make it easier to leverage a lot of the ideals and the technologies that underlie the distributed Web or DWeb or Web Three or whatever you want to call it. So in a lot of cases, this is technologies like IPFS, like IPLD, Lib, P to P, and more recently, Filecoin. And what we do in practice is we build APIs and products that we hope feel almost as familiar as existing Web Two technologies, but that leverage all these peer to peer web Three technologies under the hood. So, for example, we build and provide a document style database that syncs its updates to IPFS, and we provide Blob or File like storage API that feels like a mix between AWS and Git. And on top of that, you can sync it to decentralized storage networks like Filecoin. And then along the way, we wrap all of this stuff up in a tidy bow.
00:03:52.050 - 00:04:31.220, Speaker B: And we call that the Hub or the Textile Hub. And that's basically like an easy to use, always on DWeb infrastructure for developers. So we have databases, we have buckets, we have offline messaging. We even have some user management capabilities. And we're always adding features along the way, a lot of them spinning out of these types of hackathons where users say like, oh geez, or developers say, geez, it would be really great if I didn't have to manage this piece of my stack if I could just pull something from the Hub. And so at the end of the day, we get a lot out of these events and we end up building features and tools that leverage kind of the best workflows from Web Two and Web Three. Or at least that's the idea.
00:04:31.220 - 00:05:15.246, Speaker B: So today I'm going to talk a little bit about Buckets, and I'm going to talk a little bit about the Hub, which is a sort of primary place to access that stuff. And then I'm going to briefly mention the Powergate. And that one is just like packed full of a lot of things. And we've actually been giving other team members have been giving talks this week about what the Powergate is. So I'll refer you to those talks and slides and videos at the end of this. And then all of these things really end up leveraging our peer to peer database, which we call Threaddb. So we're actually going to kind of touch all these main four product points of textiles or of textile, and we're going to kind of touch them all at the end of the session.
00:05:15.246 - 00:05:59.040, Speaker B: But I'll try to make it so that we're keeping things pretty light and simple. So in addition to textile being a company that builds things and we charge people for using infrastructure and things, we're also a research lab. And actually we started our life more as a research lab than anything, and we started with something we called Textile Photos. Some of you may or may not be familiar with that. Some people ended up calling it a, like, DWeb Instagram or something like that. But what Textile Photos really was to us was an experiment to see if we could build an engaging consumer app. In this case, it was a photo sharing app on Pure Web Three technology.
00:05:59.040 - 00:06:27.106, Speaker B: And that was a pretty cool mission. And what we learned was basically, no, can't really do that all that well. The technology wasn't really quite there yet. The networks that we operate on aren't really designed for it. Certainly mobile phones aren't really set up for that. And the user experience kind of ends up having to suffer just a little bit too much to get all of that Pure Web Three ideals in there. So the experiment that we did sort of failed.
00:06:27.106 - 00:07:02.594, Speaker B: It was an experiment. So experiments can't really fail if you learn something. But it did take us a good while to get to that failure, to get to those lessons. The good news is we learned a whole lot. And probably most importantly, we spun out a whole bunch of developer tools and products out of that experiment and built powergate and threads and buckets and the hub. And so we started building and releasing the tools that we kind of wish we had had when we started building this photos app experiment. So things like offline data persistence, we needed that.
00:07:02.594 - 00:07:30.250, Speaker B: Now we have it. Data sharing, we needed that. Now we have it. Encryption that's built into threaddb. And all of these things are now sort of products that other apps can bootstrap to create new apps faster. And this kind of leads me to a really important lesson that we learned as Textile the research team. What we learned was we had to build a bunch of this lower level stuff before we were even able to get to the point of failure, before we could even learn.
00:07:30.250 - 00:08:23.470, Speaker B: Like, can you build a decentralized photo sharing app easily? The answer is no. But it took us a while to get there. So since then, a key metric for us, and I think a lot of other folks in this space now, is something that our CEO Andrew calls time to failure. And I don't know if this is a common term or not. Maybe it is, but the idea here is essentially, we need to make it we, as tool developers need to make it as easy and as fast as possible for someone to enter the ethereum space or enter the DWeb community, the Web Three community test out a new idea or concept that they have and then fail. Because what we have today, the web we have today, it didn't just start with a bunch of successful companies. It started with like a ton of failures, one after another after another, until something kind of worked, and then something clicked, and companies and businesses were built off of that.
00:08:23.470 - 00:09:30.930, Speaker B: And I think a big piece of our philosophy right now and the stage we're at right now is like that in Web Three. We need to give developers as much time as possible to try things and fail and to reduce that time of failure. So I'm going to talk a bit more about time of failure today as we kind of go through. But I want to leave a bunch of time at the end to get you to the point where for hackers and people who want to play around with things that they can start to fail pretty quick as well. Now, one of the other reasons I really like going to these ETH events is because the Ethereum community is actually already pretty good at this, to be honest, to building projects, tools and starting up companies, building things on the various layers of the overall stack. So there's a lot of folks building developer tooling and then there's a lot of folks building and failing and then succeeding on top of those tooling. So that's another reason why these hackathons are great, why I'm happy to be participating here this year because it helps us figure out what we need to build to decrease that time to fail.
00:09:30.930 - 00:10:49.050, Speaker B: And then it also is pretty exciting to see when people get to the time to fail and realize that they didn't fail, that they actually built something kind of cool. Anyway, the other reason I want to talk today specifically about some of the technologies like Buckets is because a big thing with any sort of decentralized app is data storage and where we are going to put your app assets, where are you going to store user data or whatever it is that you happen to be storing. You can't put it on chain, it's going to be too big. Sometimes it's dynamic, it's definitely way too expensive in terms of gas prices and things. But at the same time, if you have to spin up your own IPFS node, a filecoin node, an Ethereum node, build out your back end, then build out your front end and then maybe give up and use some storage system and all that stuff from scratch every time, we basically already lost you. No one's going to build. If that developer experience is too slow, it ends up leading to shortcuts and frustration and then we end up with a ton of compromises and some significant percentage of people building decentralized apps, they just settle on storing data on some centralized server or in the browser or something like that.
00:10:49.050 - 00:11:42.654, Speaker B: And I'm way too lazy to go out and figure out what all they are. So let's just pretend it's like 37.645%. Very accurate number. And that's a total bummer because we're stuck kind of with the same problems of the current Web, where your users are locked into a particular data walled garden sort of setup. So there's no way for users to access their own data from your app and you end up just kind of sticking to tired practices that we're already seeing. And in a lot of cases this works and that's why people, especially at hackathons will just settle on something that works because why wouldn't you? And so to illustrate this point a little bit further, here's a scenario that we literally see a bunch when users or people, developers come to the textile stack afterwards. And there's a lot of variations on this, but it goes something like this.
00:11:42.654 - 00:12:05.410, Speaker B: They get excited, they got a short amount of time, they're going to work on a hack and so they think, okay, this web three thing is super exciting. I'm going to build the decentralized app. It's going to be pure peer to peer. It'll run on mobile, it's going to run in the browser. We'll have a desktop app. It's going to be awesome. I'm not going to store any user data because decentralized storage somehow, and even the app itself is going to be purely decentralized so no one can censor it.
00:12:05.410 - 00:12:20.600, Speaker B: It's amazing. Okay, awesome. So I've got this. Hello, world. App running on localhost 3000. It all checks out, everything's great. Okay, I just need to add like 100 more dependencies to my bundle here and then I'll have to add a pinning service.
00:12:20.600 - 00:12:57.638, Speaker B: Okay, well, it's not going to run on mobile. That's okay because at least the browser app runs, but not between two peers that are on different networks. Okay? So I'll need to add some sort and when if they're offline, I'll need an inboxing feature. And then maybe if I can just cache some of the data in a database, I could decentralize that later. And then by the time they're sharing CAD photos between their Hello World app, the hackathon or the weekend experiment is over and they haven't even tried out their awesome idea yet. And so we didn't even get them to the time to failure. So that's pretty much what the hub is about.
00:12:57.638 - 00:14:10.250, Speaker B: It's a direct manifestation of our effort to reduce time to failure. It's basically cloud tooling for developers that are designed around user controlled data, trustless data management, and a bunch of cool new models for decentralized data access. So actually like function based write validation and read validation and a bunch of really neat things like that. So at the most basic level, what we're really talking about is like a remote IPFS and threaddb and buckets peers that developers can use to test things out, build stuff, manage teams, deploy apps, even connect up storage to the filecoin network without having to run all that infrastructure themselves, just with basic web two style API keys. Right now all these remote services are free for developers. And that's a big part of our mission is just we want to onboard people to these ideas, get them using it, realize how easy it is to do. And then like any Web Two platform, you start with the hosted stuff, you get a feel for, it works great, you like the user experience that you derive from it, and then you can start building out your infrastructure yourself as you need to optimize for specific web three use cases.
00:14:10.250 - 00:14:46.354, Speaker B: And so we're really hoping people just kind of start playing around with it. And the marketing spiel is like, it's a remote IPFS and textile file calling nodes that will radically change the way you build apps. But actually, I kind of hope it doesn't radically change the way you build your app. That's kind of the point. I hope that you can just build a React or View app the way that you want to build a React or View app and just leverage some of these peer to peer and web three technologies under the hood. I mean, our whole team is a bunch of developers. Some of our best friends are developers.
00:14:46.354 - 00:15:36.614, Speaker B: And our philosophy is we're not going to get a bunch of people building and using web three apps until developers start building arguably better experiences on web three technology. And we're pretty confident that that's the case that it's doable. But we can't do that if our technology ends up getting in our way. And off chain storage is a huge piece of that sort of puzzle. So our team builds tools, other teams build tools on top of those and so on and so forth until we're all guaranteed to win. And it's guaranteed. So for the EF community today, I can't really do a show of hands or anything, so I'm going to ask a question and then not actually wait for the response, which is, how many folks are familiar with File? Actually, before I do that, I'm going to pause and see if there are any questions with what I'm talking about.
00:15:36.614 - 00:16:02.850, Speaker B: So far, it's all pretty markety stuff, so if there's no technical questions, that's okay. I see something in the chat. All okay. Excellent. Silence is deafening. Okay, so I'm going to assume people have a passing familiarity with Filecoin. Yeah.
00:16:02.850 - 00:16:48.574, Speaker B: Okay, cool. Andrew, my own coworker says, shows the magic. Anyway. Okay, so for those of you who are a little less familiar with Filecoin, basically it is a new blockchain and it's a network that provides incentivized decentralized storage. So if you're familiar with IPFS, a lot of the same concepts, it's also a protocol add project. A lot of the same concepts translate over, but Filecoin has an incentive structure that's baked into the protocol itself to incentivize storage of real, hopefully real data. I mean, honestly, I'd be pretty surprised if you haven't at least heard something about Filecoin in the last couple of months, especially because their main net is going live next month and they already have something like petabytes of data on some of their testnets.
00:16:48.574 - 00:17:39.822, Speaker B: And it's shaping up to be pretty exciting way to store data. If you aren't familiar with Filecoin, I'm not going to spend a ton of time talking about it here today, but I'll give you a quick little rundown on why it would be useful and important to consider if you're building an app that needs to store off chain data. So the idea is, okay, you've got some data that you want to store off chain for your decentralized app. Maybe it's your app assets themselves. So it's your built app and you want to be able to access that over peer to peer networks. Maybe it's your actual user data. Maybe it's a cold archive of all those cat photos that the internet seems to be so obsessed about or something more like Noble, like just archiving all of Xkcd.
00:17:39.822 - 00:18:38.486, Speaker B: But whatever the case may be, the idea behind Filecoin is that in theory, you should be able to pay Phil the Filecoin coin to get a filecoin miner to store your data while at the same time proving that they are in fact storing your data. And this proof mechanism is actually how they mine Filecoin. But obviously, like Ethereum, running a full node is kind of a big deal. Not everybody is going to be running a full node on their laptops. And even proposing deals with miners at the outset is a pretty serious bit of work. It's not going to happen from like a browser app or something like that. So you're at the situation where you're like filecoin is this sort of potentially very exciting decentralized storage network where apps and DApps can actually push data to a decentralized cloud, but you can't really take advantage of it directly inside of your app.
00:18:38.486 - 00:19:07.630, Speaker B: So how do we do that? You can use Powergate, the thing I said I wasn't really going to talk about. So this is something textile built. I'm not going to talk about it tons because there's way better people on my team to talk about it with. But it's essentially we abstract away a lot of the complexity of creating and negotiating deals with filecoin miners. It packs up a lot of complexity into single tool. A lot of teams are already starting to use it. I think it's currently the recommended way to push data to the Filecoin network.
00:19:07.630 - 00:19:59.822, Speaker B: And yeah, like I said, there's not a lot of time to talk about it. But I will direct you to some workshops and videos at the end that my colleague Andrew has been giving this week to folks in the Filecoin, Apollo and Slingshot programs. Maybe Andrew can even post some links, I don't know in the chat, but we'll try to make those available on our websites and docs soon. In the meantime, you can definitely check out the docs for Powergate online to learn more yourself. And then I will sort of allude to some of this when we do a little practical stuff towards the end here. But I did want to highlight that we also run Powergate on the textile hub. So again, if you want to explore some of these tools, you can create a Hub account, you can actually push user data or files to your Hub account and then you can leverage Filecoin there.
00:19:59.822 - 00:20:58.740, Speaker B: You can kind of test it out, test the waters, see what it's like. In particular, the Buckets API, which I'm going to show you later, pairs really nicely with archiving data on Filecoin. And this is available on the Hub already. You can push data to Filecoin's testnet, and that's a pretty awesome to be. We'll show a demo of that at the end here. The whole idea here is that you want to leverage these decentralized storage protocols, but we're not going to all be running these full nodes in a browser or something like that anytime soon. So you might be thinking, are we just trading kind of one centralized system like AWS for just another one textiles Hub? But the thing to keep in mind is this is a pretty radically different perspective on how data storage happens because you don't need textile to retrieve things from Filecoin or even IPFS later on.
00:20:58.740 - 00:21:38.990, Speaker B: Essentially you have a system where you push data to the network in one place, say that's Textile's Hub or some other powergate instance, whatever, and then you can retrieve it from any number of other places. And so every single textile project, which are all open source, by the way, we recommend you to go in there and fiddle around and break things. They're all multi protocol. And so what that means is there is always more than one way to retrieve your data. You can do direct peer to peer over IPFS or IPNs. You can get it from a textile or other gateway. So like Infura has a gateway, cloudflare has a gateway protocol, labs has gateways.
00:21:38.990 - 00:22:35.074, Speaker B: You can do it directly over Lib P to P's pub sub mechanism. And Ethereum Two is using Lib PDP as well, so that stack is already familiar to some people. Or you can just do good old Http web pages. And even in the case of Buckets and other tools, if you flush it to Filecoin, then you have a whole nother decentralized network from which you can pull and extract and potentially query data. So any system that supports any of these protocols, including Filecoin, can get that data back out for you. So you've got this sort of one way in, many ways out multi protocol sort of setup. And the reason this is nice is like ultimately if you're using an app, that's a centralized piece of the puzzle because you're using that app, but once you've got that data in there, you want to be able to access it from potentially other applications that could leverage that.
00:22:35.074 - 00:23:11.294, Speaker B: And so you get potentially truly interoperable data. And again, this is something I've talked about before, the folks at Ceramic have been talking about it, lots of folks in the ETH space are interested in this. And I think Ethereum and a lot of the decentralized apps that we're seeing coming out now, this is a chance to kind of get that interoperability picture right. Anyway, the cool thing there is, yes, interoperability, but also if you don't like the service that's being provided by a particular cloud provider, here's the filecoin deal for your data. Take it and retrieve it from somewhere else. No problem. Spin up your own node if you feel like getting crazy.
00:23:11.294 - 00:23:59.754, Speaker B: These are all things that potentially could be leveraged. So I'm going to pause for some more questions here in a second, and then I want to get us into some actual practical workshop, E type stuff. So I'll switch slides here, and then I'll check these chats here. Okay, so first one there is are APIs available for Python? Great question. So all of textile hub's APIs are gRPC APIs. They are also all available via gRPC Web APIs. We provide JavaScript clients that interact with the gRPC Web APIs.
00:23:59.754 - 00:25:11.320, Speaker B: We provide go clients that interact with the gRPC APIs. You could similarly have a Python client that would interact directly with the gRPC APIs. We don't publish those necessarily ourselves, although there is one for the Powergate APIs, so it's called pygate, I believe, and that is a client that you can use to directly communicate with the Powergate remote. But like I said, if you know how to do gRPC calls in Python, then you can write a client for that. And we are looking into publishing more of the gRPC definitions for different languages and things like that. And then Joseph says, got sounds. Did you want to ask that verbally or just type it out verbal? Can we do that? I hear something.
00:25:11.320 - 00:25:12.870, Speaker B: Yes.
00:25:13.020 - 00:25:43.594, Speaker C: Hey, we've spoken a few times on your Slack. The question I'm having right now is, so you're saying that it's accessible. One of the real world things that we've kind of encountered when we've done some testing with this stuff is we're using a document store style thing, kind of like Firebase and the threadsdb is basically a database wrapper around IPFS.
00:25:43.642 - 00:25:44.240, Speaker B: Right.
00:25:44.690 - 00:26:12.570, Speaker C: So correct me if I'm wrong, which it sounds like I'm wrong from what you're saying is that if we have permission sets for a document that we have uploaded through textile, my assumption was that we would need to use textile on all the clients in order to get the right permission set wrapped around those documents, to be able to read them or set them or whatever. Is that incorrect?
00:26:13.230 - 00:26:39.742, Speaker B: Well, you need to use so if you're using threaddb, you need to use the threaddb clients to correctly interpret those permissions and things like that. But you don't need to use the Hub per se to enforce those permissions. You just need a Textile client, which could be running locally, or you could run your own remote one or however you want to set that up.
00:26:39.876 - 00:26:40.358, Speaker C: Gotcha.
00:26:40.394 - 00:26:41.266, Speaker B: Gotcha. Okay.
00:26:41.368 - 00:26:43.026, Speaker C: That makes a lot of sense. Thank you.
00:26:43.128 - 00:27:43.670, Speaker B: Cool. And then Samuel says, how performant are your databases? My experience with IPFS has been that it can be really slow. Yeah, I mean, it's the best database in the world, and it's perfect. But no, seriously, we haven't done a ton of performance evaluations. We do do some ad hoc comparisons, but if you're trying to find something to do like sub millisecond throughput or something like that, then it's not going to be fast enough for that and it's going to be pretty tough to find like a database that's syncing peer to peer that's going to go be able to handle that kind of volume. But thousands of updates per minute. It's really great for things like web apps that are creating user data or documents that are being updated through state changes, things like that.
00:27:43.670 - 00:28:38.070, Speaker B: I probably wouldn't want to capture every keystroke. Not sure that that's going to scale that well. But now we're currently developing a JavaScript implementation that is basically offline first. So you'll be able to do very quick local writes and actually local queries that will be very fast. But then we'll only flush data to a remote over regular time intervals. That might address some of the faster writes that are required for higher throughput data. Is there any caching mechanisms from the Hub? Like a CDN? Yeah, so like for faster IPFS file retrieval and stuff like that, we do enable some of that, like CDN type caching.
00:28:38.070 - 00:29:17.536, Speaker B: Especially useful for immutable data because you know that the content isn't going to change. So you can kind of be aggressive with caching and things. I'm not sure what the latest on that is, so I would probably defer to someone else on my team. Or if you hit us up in the slack, we can give you the details on that. Yeah, cool. Any other questions before I get you getting your hands a little bit dirty? We're perfectly on time, so that's great. Okay, cool.
00:29:17.536 - 00:30:10.160, Speaker B: All right, so if anybody wants to follow along, great. I'm actually going to do this live, so hopefully everything goes smoothly, but if not, it's probably someone else's fault. So let's just jump into it. That's what will happen if this doesn't go smoothly. All right, so what we're going to do is I'm going to get you to download a couple of things to get going. So we're going to start by installing the Textile Hub command line client. All right, so this basically allows you to interact with the Hub, the remote Hub, to create an account and do a bunch of interactions with the remote Hub.
00:30:10.160 - 00:30:58.028, Speaker B: So if you go to our GitHub releases page, it's just Textileio, and then the repo is Textile and you can go to the releases and there's a link in the slides which you don't have, but I can also paste the link into the chat here if you want to go and get that. And you'll just go ahead and grab the latest release. Aaron from my team just released this yesterday and so I'm running Mac. So I'm going to grab this latest Hub underscore version two 10 release here. So there are a couple. Of other tools. The Hub is for interacting with the remote Hub and things like Buck and Buckd are for interacting with a local bucket stamen.
00:30:58.028 - 00:32:01.342, Speaker B: But we're going to use the Hub just so that we don't have to set up any things on our own system. So if you click on that and download it, it should hopefully download pretty quickly. Depends how things are going. And I'm just going to actually I'll just CD into downloads and I'm not going to just list off everything that's in there. And I'm looking for something called Hub underscore V 20, blah blah blah. And I actually already have downloaded it. Okay.
00:32:01.342 - 00:32:37.840, Speaker B: And inside you should see four things. A license to README the actual binary and an install script. If you're on a Unix type system, you should just be able to call install and then it'll probably do something like say something like that and then you'll have to give it some permissions. My password is one two, three password and then it'll move Hub into somewhere on your pass. That's actually useful. I was kidding by the way. My password is not one two, three password.
00:32:37.840 - 00:33:27.810, Speaker B: And so if you check now you should have Hub installed and you can do something like Hub Help and it will try and run it. And if you're on a Mac and this is an unsigned developer tool so just click Cancel there. You might want to pop up to your system preferences, go down to security and Privacy. There we go. And it'll say something like Hub was blocked from use because it's not from an identified developer. Yes, these steps only happen on Mac and I'm just going to click Allow anyway because my computer is trying to protect me from myself. So now when I run it, I can just click Open and I get a nice looking command line tool.
00:33:27.810 - 00:34:14.860, Speaker B: So if you're running on Linux, probably won't say anything like that. If you're running the Windows install, you may have to reference the binary directly, but you shouldn't have to worry about that. As Andrew has pointed out in the chat, I'm actually following along with some installation instructions that are in our docs so you can check those out. I'll also show you here. We're following these docs here and the main one to start with is just docs textile Iohubaccounts. That's where we're going to start. And there's some install instructions in there on how to install things and it warns you about all this stuff.
00:34:14.860 - 00:35:21.320, Speaker B: So if you forget about it and come back later, that's no problem. So once we have that installed, I'm going to scroll down to this account setup step and what you're going to do is you're going to initialize an account on the Hub. The Hub command line uses essentially a magic link sign in setup and you only ever have to do it once you verify via email and you're good to go. So in that situation. You just call Hub in It and you get to pick a username. You get to pick an email and then you'd hit enter and it'll keep going and it'll send you an email and you can validate it. I've already created one, so I'm going to go Hub login, I'm going to go over here and check my email and here we go.
00:35:21.320 - 00:36:10.294, Speaker B: And so when I do that, it says success and it should show you a little authenticated page when you open up the link in your email. So hopefully everybody is having some success there while you wait for the emails. Andrew has posted in the chat a great little link for ways to use some of this and some of these tools in your hacks and projects. And that's available on our blog. He just pushed out the ten things you can do with the Textile Hub. So there's some food for thought there. Anyway, once we've done that, I'm going to just CD into an examples folder which is just empty and I have nothing in there and we're going to get playing around.
00:36:10.294 - 00:36:56.280, Speaker B: So hub in. It logged in. We've created an account. There's lots and lots of things you can look at in the documentation here, including how to create orgs so that you can organize your teams around particular buckets and projects, so you can create organizations. We won't do that here today. You can invite other users to organizations, you can delete accounts, et cetera, et cetera. You can also create API keys so that you can use some of our JavaScript and Go clients for interacting with remote hub to do things like allocating data storage on behalf of your users, so that you can actually leverage the hub for storing user data and things like that.
00:36:56.280 - 00:37:39.330, Speaker B: You can explore a bunch of different ways of using bucket storage. And that's what I want to show you today. All of the APIs and all of the tools that we're covering on the command line here right now have an associated client API as well. So like anything you can do here, you can pretty much do with our JavaScript client. Anything you can do here, you can do with our Go client. In almost all cases, the Go client is the reference implementation, the JavaScript client quickly follows and then there have a few community contributed other language clients like the Python pygate tool. So I want to show you a couple of command line tools just to create a bucket and add some data to it and look at some of the cool things that you can do with that.
00:37:39.330 - 00:38:37.350, Speaker B: As a developer, I'm just going to use my developer account that I've created by calling Hub in It. But you can also use these same tools for users. So if you create an app, you can actually scope access to a given bucket or the amount of storage, things like that, you can scope that to a given user based on a public key. So PKI infrastructure so that you can actually allocate storage or number of threads or whatever, any of these Hub tools on behalf of your users. Those types of uses are really great for when you're building hacks and apps. Anyway, so we're going to talk about some bucket storage here. So a bucket is basically like a mapping of a folder to remote storage, very similar to AWS style Blob storage or file storage.
00:38:37.350 - 00:39:38.044, Speaker B: So what we're going to do is we're going to initialize a new bucket in this folder, and I'm actually just going to start from scratch, show you that there's nothing in this folder, okay? And then what you can do is we can initialize a bucket, and what it'll essentially do is create a seed and a brand new empty bucket that we can start pushing data to. So I'm going to just call Hub, Hub Buck in it. I'm going to give it a name ETH online. And I'm going to not encrypt the bucket contents in this case because I want the data to be available in plain text later. But you could also specify to encrypt it. And then you're effectively creating like a private or encrypted bucket. And that's also an option and it's pretty cool.
00:39:38.044 - 00:40:09.760, Speaker B: So we'll enter that. And I've actually already got an existing underlying database or thread that my bucket was created in. So it's a good idea to just stick with the default. So I'm going to do that. So I'm going to select that thread. I've initialized a bucket, and then I get back a little bit of output here and it says your bucket links. So this is kind of a demonstration of the multi protocol support that I was talking about earlier.
00:40:09.760 - 00:40:47.404, Speaker B: So in this case, buckets have a direct thread link. So this is basically like how you actually can share information between threaddb clients. So these are like if you had clients running on different machines, they could actually directly communicate about this bucket over the threads protocol. That's an open source protocol that Textile developed. We have a white paper about it. And you can communicate peer to peer and exchange data that way. You can also just access it via that URL directly on a public textile daemon or Hub, and you can access the data directly.
00:40:47.404 - 00:41:32.480, Speaker B: You also get an IPNs link. So the really cool thing here is every single bucket is also an IPNs resolvable address. So this will actually resolve directly to some data on IPFS. It defaults to accessing it over the Textile Hub, and I'll show you in a second what we can do with that. But you can similarly access it over other gateways. So like IPFS or Cloudflare or whoever. And then we also have a different URL called the Bucket website, which actually allows you to reference bucket storage directly on Textile's Hub.
00:41:32.480 - 00:42:29.840, Speaker B: But right now we have a bucket with nothing in it, right? So we want to add some data to it. What you can do is you can publish or push files to a bucket, similarly to how you might add something to say, a git repository or something like that. So what I'm going to do is I am going to create a new file and I'm going to just do the classic yes, thanks Andrew. Andrew's got a couple of links for our cross protocol support there and everybody should just give a little round of applause for Andrew because the documentation for this stuff is, I think, really fantastic and it makes giving these sessions so much easier. So big thanks to Andrew there. He also happens to be in charge of my, you know, it's great if you just give him a couple of claps. Anyway, we're going to just add Hello World to an index HTML file.
00:42:29.840 - 00:43:20.172, Speaker B: And so now I've got a file in there called index HTML. And if know cap the content out of that, it's just an HTML file with the words hello, world in it. Okay? So now what I want to do is if you're following along here, we're just going to call Hub Buck or Bucket Push and it's going to say, okay, there's one new file you want to push one change. And I say yes, and it was only twelve bytes or something like that. It pushed it. And what it spits back at me is an IPFS hash of the content, which is pretty cool. It's even cooler because again, with the multi protocol access, I can access that data directly.
00:43:20.172 - 00:44:15.956, Speaker B: Now, I guarantee you there is a whack ton of index HTML files with hello world content out there. So it'll be really easy to resolve this CID from just about anywhere. So I'm cheating a little bit, I would say. But just to show you how this kind of works, here is an IPNs link, which I'm going to copy. I'm going to grab my browser, open up a new tab, copy paste that in there, and I get back the raw IPFS sort of style information. And if I click on index HTML, there we go, there's my content as hello world. But if I go here and I do say IPFS IO IPNs, blah blah blah, and I do that, I similarly access, I similarly access it on there.
00:44:15.956 - 00:44:58.044, Speaker B: So now I'm outside of the textile ecosystem, but still access to the same data and things like that, very handy. And so you can explore some of these other links if you bucket push something. And that's all very exciting, but so far we're still using IPFS to push stuff. And just a reminder, all of these APIs are available over our JavaScript and other clients. But things can get a little bit more interesting if we jump back to our documentation. And there's a lot of information here about what you can do with buckets. They do like local diffing and efficient syncing.
00:44:58.044 - 00:45:39.196, Speaker B: So if I make changes to a structured folder or something like that, it'll intelligently. Only push the changed files and do a bunch of sort of git like things, which makes it awesome for things like deploying apps where you have build assets, your react app or your create react app and you call NPM Run Build and you get all these build outputs. You can push that to a bucket. And then when you make updates, it's only going to push the updates. You can do encrypted ones and you can read all about how we do that. You can talk about how you can actually pull data into a new bucket. You can seed a bucket from existing IPFS data.
00:45:39.196 - 00:45:55.604, Speaker B: All sorts of really handy stuff. It's all right there. Here's an example of the rendering. It on the website. But what I wanted to jump down to is doing filecoin archiving. So if you jump down to filecoin archiving, there's a little bit about what this is all about. You can check the video.
00:45:55.604 - 00:46:39.984, Speaker B: There's a blog post about this and of course a big old warning here that says, be careful what you put on here. This is all very experimental stuff, but you know what, we're living on the bleeding edge. We may get cut as we move along, so let's just do it. So a big thing I was talking about earlier is reducing the time to failure. So far we've already tried out IPFS, IPNs and a bunch of other things with pretty minimal time to figure out if this is even going to work. Now I want to try and push some stuff to filecoin and I want that time to failure to be just as quick. So we've made a lot of effort to try and make it as easy as possible to test the waters here a little bit.
00:46:39.984 - 00:47:36.292, Speaker B: So what we're going to do is we're going to try archiving this bucket. And so this is probably not normally a thing you'd want to do. You don't want to just push like individual files to filecoin. That's just a waste of filecoin mining resources. Ideally, you want to push like a whole archive to cold storage, so maybe like a bunch of app data, or if you need long term storage for your entire app assets or something like that, if you're implementing a game and you want those assets available for a long time, things like that. But let's throw caution to the wind and try this out. So Hubbuck Archive and if you do help, there's a bunch of other things you can do, but we're just going to do hubbuck Archive gives us a little warning saying this is pushing to the test net, not live filecoin because it has not gone live yet.
00:47:36.292 - 00:48:14.092, Speaker B: So this data is going to disappear at any time. But I'm going to just press that and success. Archive queued successfully queued being the operative word, we're archiving something on filecoin. This is not a quick process, this could take hours to process. So this is an async operation. So what you're going to want to do is you're probably going to want to check the Archive status. Now you can just do this, copy pasta this Hubbuck Archive status and it will get back saying, look, it's currently executing, grab a coffee and be patient.
00:48:14.092 - 00:48:54.904, Speaker B: And that's great advice because it's going to take a while. But if for the impatient you can always just sit there and watch. So you can do I'm just going to type it at Hub, hub buck Archive Status Watch and it will sit there. It still tells you to wait patiently. And the way things are working right now is there's only a few miners that are accepting deals on testnet right now in the first place. So the archive is currently executing. We've pushed the new configuration, the configuration is saved successfully.
00:48:54.904 - 00:49:48.960, Speaker B: So your deal is sort of set up and now we're just waiting for the deal to actually get processed by the powergate to find miners to make the deal on behalf of your app and things like that. And if you let this sit here, eventually it'll time out because who wants to be connected to the Hub for a long time and you can check back in later and hopefully the deal will go through. The kind of workflow that this is designed to support is a sort of push and wait kind of thing, just archive and then you can come back in and check if it's been archived later. Again, you don't want to archive a bunch of hello worlds, but the idea here is look at how easy that was when you have real app assets that you want to preserve on filecoin. So I'm going to kill that. And that's not stopping the deal obviously from going through. All that is doing is just stopping to watch for updates.
00:49:48.960 - 00:50:57.270, Speaker B: If we had a ton of time you'd get something that looked a little bit more like this and this is the output that we would get. So Buck Archive status w there and it goes through and eventually you'll get the job, it'll be executed and then you'll save data to hot storage which is IPFS, you'll save data to cold storage which is filecoin. And there's lots more things to learn about here and I highly recommend you check out our docs, ping us on our Slack channel or some of the channels available during the hackathon or the ETH online sessions to learn more about all this stuff and I'm going to leave it at there. We've got what like ten minutes left, so I'll stick around for some questions or discussion. If you got any more questions, I'm happy to take them and I'll just leave this kind of floating here and I'll check the chat. So if anybody's got any questions you want to post in the chat. Please do.
00:50:57.270 - 00:51:02.036, Speaker B: And you can raise your hand if you want to ask live there's one. Here's one.
00:51:02.058 - 00:51:03.844, Speaker A: Joseph yeah, cool.
00:51:03.962 - 00:52:08.376, Speaker B: Is it recommended to create one bucket for an entire app and then create folders inside of that structure, or to make a new bucket per user? Or is this a structural hierarchy that doesn't really matter? Great question. We're still experimenting with a lot of different structures, so we don't necessarily have one that's recommended per se. One thing we're finding is like anything, it's probably not a great idea to have a huge single flat layer of large files in a single bucket because whenever you need to do something in there, it's going to take forever to query it. We've spent a lot of time on the buckets APIs and on the hub to optimize query times and all that stuff. But I think a hierarchical structure is great when you're talking about a tree based structure like a set of folders. There's a lot of optimizations you get from doing that. Kind of like from know when you're dealing with sort of query trees and things like that.
00:52:08.376 - 00:52:38.020, Speaker B: Yeah, so Andrew's got a great answer to that. You can create one bucket for your app and then create new buckets for each of your users if they also need to store user data. And yeah, that's perfect. Know your app, you'd have all of your assets in there. Your users may have file storage that they want and that sort of thing. Great question. So there's a couple of different security considerations that you should keep in mind.
00:52:38.020 - 00:53:53.128, Speaker B: Buckets does have fairly fine grained access control rules so that you can actually do things like allocate access to a specific file in a specific folder, and you can do all sorts of fairly fine grained access. You also have to keep in mind, like if it's an.org bucket, then you're going to have members of your organization will have access to it. So there are a lot of nuances to how you might want to set that up. So the structure should reflect your security model wherever possible so that you're not allocating a bunch of different access control rules for a bunch of different files all in the same folder. But yeah, again, that's a good question for a slack thread, probably, where we can actually discuss optimizing structure and things like that. Jeremy, what kind of uptime availability can users expect on average? Right now, we're doing pretty frequent deployments, but it's all backed by pretty solid cloud based best practices.
00:53:53.128 - 00:54:36.410, Speaker B: We do announce ahead of time if there's going to be any. Don't I don't have any numbers on uptimer availability right now. Andrew might have a guesstimate or even a better number than that. The answer is I don't have those numbers. Yeah, I mean, it's there like 50% of the time. Jeremy no, I mean it's there as much as possible. And we do have people who lose sleep at night if it's not so that's a pretty decent guarantee.
00:54:36.410 - 00:55:34.220, Speaker B: The access can be granted to smart contracts too. Interesting, the access can be granted to public keys. We're spending a lot of time right now trying to think through ways to allow other identity types. So we are working a bit with the Ceramic crew to support DIDs and things like that. So ideally you'd be able to use like an ethereum public key. But right now the best thing like the our favorite thing is ed 25519 public private keys in a public key as an identity. And there are a lot of ways to sort of derive a random one and associate it with an ethereum account if that's what you want to could in theory that's a great question.
00:55:34.220 - 00:56:42.020, Speaker B: I don't have a yes, you can answer to that, but I will try to figure that out. So if you want to ping us on our slack, that would be awesome. Assets stored in buckets speed of download is below 1 second for small that's a question. And then for small files like metadata, is it comparable to CDN available assets? I'm not sure. For the Immutable files we do take advantage of some cloudflare stuff in the background so you should get pretty decent performance on those types of assets. But some teams heavily optimize for CDN availability and stuff like that. It's not going to currently compete with that, but that can be done, I would say.
00:56:42.020 - 00:57:24.762, Speaker B: Yeah, cool. What sort of indexing and query can we do on data stored on buckets and their metadata? This is a great question. So buckets themselves are built on top of threaddb. So the structure of the buckets are actually updated dynamically as a document store. So that means you can query it like a document. So you can do things like find the buckets whose name is this or who were created on this date. You can look at the bucket schema to see what fields are automatically created.
00:57:24.762 - 00:58:19.170, Speaker B: We don't really expose the APIs to do those queries very easily through the buckets API. But you can do it through the thread DB API because basically every bucket is also a database. The really cool thing here, and just like alluding to you mentioned their metadata, the really cool thing here is because buckets exist in a thread, basically they are collections in a database. You can also add other collections to that same database. So you can actually mix buckets and additional metadata in the same database as separate collections. And so you can query one collection that stores maybe comments about a file and then you've got the file within a bucket and you can do some really interesting efficiencies. You gain some efficiencies there because now suddenly anyone who has access to the bucket also by definition has access to the comments and there's a lot of ways you can structure that.
00:58:19.170 - 00:58:55.526, Speaker B: We're really kind of yes. Not as well as I'd like you to be able to. So you can nest collections in that you can create collections and reference the Identifiers for the instances of those collections inside of other collections. But you don't get what I think you're alluding to, which is like highly nested dynamic. So if I update this sub collection, the parent collection will reflect that? It's not quite like that, but you can nest them in that you can reference. Yeah. And then yeah.
00:58:55.526 - 00:59:21.494, Speaker B: Just one thing to point out. Andrew said, regarding uptime, we are also hiring. So if there's anyone who wants to help us add a few more decimals to that uptime percentage, we are hiring and you should come and help us do that. And look at that. I have, like, I don't know, a few seconds left on my time, so that's pretty much perfect. So I'm going to stop sharing my screen.
00:59:21.692 - 00:59:24.040, Speaker A: Thank you so much, Carson. That was amazing.
00:59:25.370 - 00:59:28.840, Speaker B: Thanks for all the questions, everybody. That was great.
00:59:29.690 - 00:59:37.910, Speaker A: Great. So, a reminder to everyone to stake your spot to hack in the hackathon and integrate with textile buckets.
00:59:38.570 - 00:59:41.046, Speaker B: Do it. Join us.
00:59:41.068 - 00:59:41.854, Speaker A: Thanks, guys.
00:59:42.012 - 00:59:43.166, Speaker B: Have a great one, everybody.
00:59:43.268 - 00:59:43.770, Speaker A: Cheers.
00:59:43.850 - 00:59:45.930, Speaker B: Thank you. Bye.
