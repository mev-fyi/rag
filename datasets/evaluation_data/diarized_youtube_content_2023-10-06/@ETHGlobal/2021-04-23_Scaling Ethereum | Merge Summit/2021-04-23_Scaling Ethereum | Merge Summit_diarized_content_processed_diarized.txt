00:08:23.270 - 00:09:02.440, Speaker A: Good morning, good afternoon, and good evening, everybody. How is everybody doing? My name is Kartik. I'm one of the co founders of ETH Global, and I'm super excited to welcome all of you to the Merch Summit. We have a lot of you rolling in. We have a few hundred of us on live, and what I want to do is quickly just welcome everybody here and give a quick introduction to how we're going to be watching the rest of the talks today. We are all looking at this thing from Eatglobal TV. This is our interface to really have a shared experience with how we're going to play the Summit and watch and interact with everybody out here on this global community.
00:09:02.440 - 00:09:59.750, Speaker A: This stage is designed to map really to what's happening on stage and anything you type in live in this chat will be communicated to our speakers. So if you have any questions, if you have any comments, you can type them on this chat and we'll be able to look at that directly from the chat and relay them to our speakers. On that same note, I encourage all of you to also sign in and log in and say hi to us on chat so we can not only talk to you and relay your comments, but also give out NFTs for everybody who is participating today at the Summit. So before we quickly jump in, I want to say that this event is brought to you by ETH Global. And for those of you who are not familiar with ETH Global, ETH Global organization with a very simple mission. Our goal is to onboard thousands of developers into the Web Three ecosystem. And we're doing that in the form of running hackathons and Summits.
00:09:59.750 - 00:10:45.480, Speaker A: This month is dedicated to scaling ethereum. This event, Scaling Ethereum, is a month long event, which is a function of a month long hackathon, a three week long hackathon, and three different Summits. We have over 600 developers working on cool projects related to scalability. On top of that, we have 600 other attendees who are looking at learning what's happening in the world of scalability and Ethereum. And they're all our Summit attendees. The developers participating in the hackathon are coming in from 58 different countries and spanning 18 different time zones. We have a massive diverse crowd working on really what's possible with L2 and L One, and how do we bring everything else in this ecosystem to a really fast and gas less world.
00:10:45.480 - 00:11:29.286, Speaker A: This is just a crowd of who is participating at this event this month. And I'm super excited to be welcoming all of you and thankful for everybody joining in from all these different time zones watching all this live. On top of all that, for the hackathon, we're giving away over $200,000 in prices. There's a lot of momentum here on really bringing everything up to L2. The Summit is designed to be split into three different summits. We have a scaling summit, which happened last Friday about a week ago, where we talked about everything that's happening on L2 sides, whether that's roll ups, ZK roll ups, or optimistic roll ups, or side chains or app specific chains. We covered all of that last week.
00:11:29.286 - 00:12:26.882, Speaker A: Today is dedicated on the merge. We're going to be talking about everything that's happening with E Two and what's going to be needed to bring everything from E One to going fully over to your previous stake. And next Friday, we have our zero knowledge summit, where we'll be talking about how zero knowledge applications and these primitives will enable a lot more new applications and a new level of programmability into the ecosystem. So I want to quickly talk about what we're going to see today on the schedule before we jump into our very first talk. So the merge summit is again designed to give you an overview of what's happening on the merge from ETH One to ETH Two. And everything that we're going to cover will be related to this theme. So we're going to first kick off with our talk on how everything we've done in the past makes it possible for us to really make this transition smoothless and seamless from Metropolis to Serenity.
00:12:26.882 - 00:13:37.242, Speaker A: And there'll be a talk given by Xiaoi. Then we're going to have Lakshman talk about just the state of the general conversation and status of what's happening with the merge. Then we have Perlambda giving us an introduction on Randomism and how that testnet will make it possible to really see everything that we need to do before we fully go to production. And then we're going to have a panel with Danny, Ryan, Trent, Vaness, and Octob who are going to talk about just the evolution of the consensus for the ethereum ecosystem from the beginning to where we're headed now. And then we'll have Jim talk about how do we now manage client diversity with Vouch given that they have so many clients trying to keep an independent language version running and really make everything more decentralized. Then we'll have Superfiz talk about Staking and just in general, how Staking and ETH Two genesis has kind of gone from over the last six months. Then we'll have Mara talk about secret shared Validators where she'll be going over how do we actually design a resilient validator infrastructure to allow everybody to become a privileged take Validator or join one and offer that at scale.
00:13:37.242 - 00:14:33.994, Speaker A: And then we'll have Terrence talk about how Prismatic and Prism is designing everything to be interoperable with other ETH One clients and allowing everybody to smoothly make the transition from one to two. Then we'll have Tomas and Alex go over mev and how the merge will kind of affect this landscape, and we'll have kind of perspectives from Flashbots and Nethermind being shared. And then, lastly, we'll conclude the day with Vitalik giving a talk about what actually happens after the merge when we're fully on a proof of stake system what are the considerations for the Ethereum network and where do we go from there? So that's a lot of stuff to throw in. We'll have a lot of time to kind of talk about this thing. If you have any questions for the talks, please type them out and we'll be able to relay them to our speakers. And with that, I want to welcome our very first talk of the day. The talk is titled features now from Metropolis of Serenity.
00:14:33.994 - 00:14:46.020, Speaker A: And this is going to be a talk given by Xiaoi. She's already here. I'll have her turn her video on and really kick us off with the very first talk. And thank you so much for kicking us off today.
00:14:50.730 - 00:15:03.882, Speaker B: Thanks, Patek. Let me share my screen. Could you see the screen?
00:15:04.016 - 00:15:05.020, Speaker A: Great, perfect.
00:15:05.710 - 00:15:43.954, Speaker B: Okay, cool. Thank you everyone. So my topic is about the future is now and from the Metropolis to Serenity. I'm Xiao, researcher at Eastern Foundation. You can find me by these handles. And here we are. So first these pictures are taken after the East London.
00:15:43.954 - 00:17:10.482, Speaker B: Last year some of our team visited the British Museum thanks to global yay and the right picture was taken by Didrick, aka the Protolambda. It was an ethereum nerd. It was me taking a picture of the Byzantine gold coins. So these Byzantian coins were minted in Constantinople over a thousand years and just a couple years ago iseran used Byzantine and Constantinople as the release cones. So when I visited the museum by that time I was wondering if each line of a code on the Eastern history today will be read and be used by people after 1000 years. So yeah, that's the magic of the history and I think we can actually learn a lot from the past and to build the better future. So here we are.
00:17:10.482 - 00:18:44.458, Speaker B: These are Ethereum releases and say it's the faces. Everything was landed in 2015. In July the profile work chain landed. So we started from the frontier with very small community supporters. Then the next release was the Homestead which provides some stabilized upgrades that just as an announcement how to tell the people that our server is ready to vote future upgrades and then since 2017 to now, we are in the age of the Metropolis. So the ecosystem growing bigger and bigger. We have established the government systems like the EIP process and by design review and implemented this AIPS.
00:18:44.458 - 00:20:20.960, Speaker B: We have a lot of the eva enhancement and bug reward adjustments in the past few years and Metropolis is still continuous growing and at the same time the next stage, the Serenity stage is already happening right now the Serenity contains the proper stake and scaling solutions. So what is Serenity? So I'd say we sometimes call it like Serenity 2.0 but generally the Serenity ish and the timeline was like the things happened before the mouthpaper in I think it was 2016 by Talik. And by that time there were just a couple of researchers in the early years. And maybe the founders of Ethereum have started to do the proper stake study and research. And then we have the hybrid consensus stage. I will talk explain that later.
00:20:20.960 - 00:21:39.686, Speaker B: And now we are at the beacon chain research and the merge research and development. These stages might be impellerable in some way, because each topic can be done by a small group of people and then growing, growing to be implemented in the manage. So the Moss paper, I think maybe the most external client developers has read this paper. So back to 2019, the basic shape of Ethereum 2.0 has already been established in this paper lab we have like six main goals. First of course we want to remove the proof of work to have a more efficient proof of stake consensus. And then things like fast block time and economic finality.
00:21:39.686 - 00:22:35.500, Speaker B: These properties can go with the proof of stake automatically. And then we want to scale the Ethereum. In this paper the main design is the shard chain design. So the shark chain is like you have multiple side chains, but the side chain is not the shy chain. Listen to the mention. The mention also has to listen to the shard chain from time to time. So by this communication bound mention to the side chain and then we can implement the crowdshot communication via the core mentioned.
00:22:35.500 - 00:24:16.910, Speaker B: And then the last one is the censorship resistance. It's like if we want to if we want to remove the proper work, then we will need to found the leader of the blockchain, of the certain block every time. The risk is that if a rich attacker have enough staking power, then could it censorship the blockchain easily. So we want to reduce that possibility as we can. So back to the early stage, what lessons we learned from the Casper proof of stake? The list is the Casper FFG paper. So in very long time ago that there are already some main properties that today in the beacon chain we used, for example, we have the validators to make deposits on the current chain. So that if the validator doing something bad badly lender sorry, let me see.
00:24:16.910 - 00:25:16.224, Speaker B: So if the data do something bad, then they will take the economic flash. So if they did something good, then they will get incentive rewards from the protocol. Also the proof of stake protocol provides the settlement finality. Or we said the economic finality. And also we proved that the Casper FG can guarantee the accountable safety and the if you are interested in learning castor, you can go read this paper back to today. It's still very useful. Let's see.
00:25:16.224 - 00:26:42.780, Speaker B: And also the lesson we learned about sharding. So Sharding is also not novel even in a few years ago. But there are some bottleneck that make it difficult to be useful today. For example, the computational ability and networking capacity if you have one blockchain and how could you scale to like 664 or thousands of chains. And then also it has the same issue as the Casper after proof of stake system because you have to select some of the validators to prove that to validate the certain block is correct. And then these lessons we learned and then to make it step by step. The first practical implementation was the hybrid consensus FFG.
00:26:42.780 - 00:28:06.180, Speaker B: So this is a basic outline of the version of design. You can see that there are two different contracts. One is the caster contract and another is the Sharding manager contract. So in this design, there were two different contracts. And each contract we accept, validator will make deposit on each contract. And then if each contract controls the consensus in the contract layer and then if validator want to make vote to prove that they have attest some certain history, then they have to make EVN transaction to the contract. So yeah, that was the decide back to and except that for example, Casper FFG will change the focus rule and the image contract will of course change the architecture largely.
00:28:06.180 - 00:29:35.250, Speaker B: So what we learned from that stage is that we learned how to do some blockchain, how to provide the blockchain randomness. So as I said, the censorship resistance is important. So it's important to make the attacker to be very difficult to manipulate the randomness result. So the most popular and practical approach including the Rendell and BDS, which is the future Ethereum might use. And the second one is by studying Sharding, then we know that the data availability is really important and also to decide the future Shardings like statelessness, the stateless client and the account abstraction, the account abstraction contract. These different new features are likely to be used in the later event or client decide. That's what the east one point X is working on.
00:29:35.250 - 00:30:32.630, Speaker B: And then the Casper FFG research. So the FFG research back to them already formed the slashing condition rules and the procedure to handle the inactivity leak penalty. So these things are still being used today. But finally the hybrid Casper is doable, but the Sharded hybrid Casper is really hard. So back to them. We realized that we should have started with a new form of profile station. So let's open the beacon chain age.
00:30:32.630 - 00:31:42.440, Speaker B: So, here we are. This is the overview of the picture of the beacon chain design. We will edit the beacon chain which is full pop up stake chain to provide finality randomness and the core of the Sha chains to anchor on. So we already have like four production available to buy when? On the day one we launched the beacon chain last year. So, yeah, shout out to the pi devs now to today. We already have over 3.9 million Ethers are in Staking and over like 122,000 validators are running their beacon node and validator clients.
00:31:42.440 - 00:33:17.956, Speaker B: So learning from the beacon churn R and D we are still learning the first is validator client and we can transparent is good because it's more flexible and more scalable. Also, if you separate the data client which handled validating with the keys and the beacon node handled network p to P network, then we can make the key management more secure. And also the staking user experience is really important. It includes how newbie staker learn to manage the keys and to make deposit and to upgrade the software. So I'd like to thank to the staker community they really helped a lot. And the last thing is, I think the devs generally likes the incrementalism patches in development. So example like we will have the first Beacon Chain.
00:33:17.956 - 00:35:18.210, Speaker B: Hub fork called Air this year which will increase some features of the Light clients and in the future IMD they are already in process but in pellaroli. So each patches in the different code base we can merge it. It's not necessary to be one should be before another or another should be after another then the merge finally it's merge summit. So yeah the merge is about merging the current east one magnet and the east two magnet to be one ethernet. So it's the previous picture but the differences is that we add the proof of work block into the beacon block but we don't have to include the whole block because the mining will be disabled by then so it's only the extrusion payload will be included in the beacon block. So after the merge the beacon chain will become an executable beacon chain and to learn more the talk after list will show you the full picture of the merge by lectureman after me. And if you want to learn more about the post merge ethereum Talik will talk about hotcounts after the merge today.
00:35:18.210 - 00:37:13.860, Speaker B: But what about you want to participate in now? Then if you want to communicate and found some interesting issues to hack we're welcome to go to the ind discord server. Thanks to Proto for making this fabulous website URL and all. The east one is one point. X. East two developer discussions are unlat, so it's now the new development hub now for Eastern community and also we have the East Research forum so we can go to the East Research ch and found the East Two transition category that's the merge related Topics. But if you want to learn other topics, other scaling topics, there are also many layer two scaling solutions like row ops on that and also you can start with the good first issue today to find a project that you're interesting in and then read contribute guide on that and pick one tackle and issue. Maybe try to use the good first issue label and also just want to let you know that don't be scared to ask on that less channels you see like GitHub or Discord or maybe GitHub.
00:37:13.860 - 00:38:16.090, Speaker B: So I think the open source software community are usually really glad to see the contributors are coming. So want to say is that the historic history is not very long yet. It's good because we can took the lessons from the past, the previous work, built the previous work by the early researchers, the authors, they helped build our present and from the Metropolis to Serenity, we are building the future right now. Thank you for your time. Think I talked too long.
00:38:22.540 - 00:39:02.200, Speaker A: Thank you so much. Shaoi. This was a really good overview of how we got from all of our initial attempts to look at any from charting to Casper to FFG to the Gadgets and now we're at the Merch. So I think you did a really awesome talk job at just setting the context into what's been tried before. And that really sets us up for the rest of the day because we're going to have a lot more details on how everything that you talked about has affected the state of the Merge and just what happened with you too. With that, I want to thank you for giving this very first talk for today and also for staying up super late to do this from your time zone.
00:39:02.860 - 00:39:14.600, Speaker B: Thank you. Thank you. So it's an overview, so hope you will enjoy the letter presentations.
00:39:14.940 - 00:39:29.970, Speaker A: Thank you. I think just to kind of make sure that I address one single comment, we have a question from Gabriel who is asking what was the link to the discord server? I believe it's just ETH. Two WTF and discord. E two WTF. Is that correct?
00:39:31.380 - 00:39:36.370, Speaker B: Let me go to that page again and.
00:39:39.060 - 00:39:45.300, Speaker A: I will share the kibril. We'll type that up on the chat as well and we'll coordinate.
00:39:45.800 - 00:39:47.508, Speaker B: Yeah, definitely.
00:39:47.674 - 00:40:09.916, Speaker A: We'll just write that answer in the chat. So thank you so much. And with that, we are ready to move on to our next talk. So the next talk is titled State of the Merge. And this is going to be a talk by Luxman who is a researcher at the Ethereum Foundation. And he's going to be presenting the current state of what the merge is and where it is. Luxembourg's been working on this thing internally for quite some time.
00:40:09.916 - 00:40:29.460, Speaker A: And I know we've all talked about the merge as a big event, but exactly what's happening there and what is planned is kind of the goal for us to uncover today. So with that, please welcome Lakshman. And whenever you're ready, you can share your screen and turn off video. Turn on video.
00:40:29.530 - 00:40:44.168, Speaker C: Hey, is my video everything is great, working great. Let me share my screen. How's that? Are we up?
00:40:44.334 - 00:40:45.704, Speaker A: We're up. Perfect.
00:40:45.902 - 00:41:11.840, Speaker C: Excellent. All right. Hi everybody. My name is Lakshman. I work on a lot of things at the Ethereum Foundation, primarily focused on kind of moving the protocol forward. I think what that's kind of fallen under recently is we're calling this like East Two and the merge. Basically this big shift to proof of stake consensus.
00:41:11.840 - 00:41:46.204, Speaker C: And I kind of wear many hats. I'm involved in some R and D stuff, I'm involved in some coordination, communication stuff, education stuff. I'm always down to talk about this stuff. So on the last slide, my handles will be shared as well. But I'm always available to chat if you're interested in learning more, interested in getting involved, anything like that. All right, so this talk is going to be very specifically about the merge. I'm not going to go into too many technical details, actually.
00:41:46.204 - 00:42:34.190, Speaker C: I'm more going to kind of motivate. What is this thing? Why are we trying to do this? What do we need to do to get there? And where are we now? What's remaining? And finally, talk a little bit about where you can get involved, where you can help, where you can kind of learn more. This talk, there's a lot going on in Ethereum protocol development, a lot of really cool ideas. Xiaoi gave some excellent kind of historical context and some vision about the future. And there's Sharding. There's a whole bunch of cool stuff coming for state size management, like Berkele trees. Epic based data is on the way.
00:42:34.190 - 00:43:28.476, Speaker C: So we're going to have some cool new gas fee market kind of structure, mev extraction, like moving Starks into layer one. There's all kinds of cool stuff out there. This is not going to be about any of that. This is going to be very focused on this kind of singular task in front of us, which is upgrading from what we know as Ethereum today to Ethereum with the proof of stake consensus. And later in the day, Vitalik's actually going to talk about what comes after merge. So if you're interested in all this stuff, which is super cool as well, definitely stick around for that. So where are we going? What is the goal here? Sometimes it's valuable to just step back and remember what this is about because there's a lot of jargon, there's a lot of changing kind of language around what we're trying to do.
00:43:28.476 - 00:44:11.260, Speaker C: And it's valuable to just kind of think about what is the end goal. So this is a near term image. Ethereum 20 years from now may look different, but this is sort of what we're looking at and what we're hoping to get to. Don't quote me on this within the next year, I think. Last week there were a bunch of interesting talks about layer two side chains, various mechanisms for scaling that don't exist in layer one. And it's really cool that all that's happening in parallel with these things that are happening in layer one. Allah proof of stake.
00:44:11.260 - 00:45:32.224, Speaker C: And so what we're working towards, hopefully, is something where these two things work in concert. We have this very fast data layer that operates on proof of stake, environmentally friendly, has economic finality, has these great properties that proof of work doesn't have. And that data layer serves this interesting execution context ordering context for transactions in a bunch of different who knows how many different L two s and maybe they interact with each other. But this is sort of like the rough mental model that we're working towards. This is from a Tweet vitalik made at some point last year that I think while maybe the language is different, the concepts hold up and also don't worry too much about the actual numbers here but this is what we're working towards. When roll ups get there, if roll ups get there we're going to have pretty excellent gas savings in TPS. But if we can combine both of these worlds we go even further.
00:45:32.224 - 00:46:54.290, Speaker C: So that's what we're working towards. So how do we get there? So what is this merge thing on the Ethereum one layer side or sorry, on the layer one side? What needs to be done to actually get there? I sort of think of three different pieces to this. One is sort of like the actual operational hot swap of one subcomponent of the system out for something else which we'll talk about. Consensus hotswap one is sort of the actual client. It's not enough to kind of theoretically just have proof of stake and Ethereum. We need like an actual client implementation that does all the things we want it to do, that does all the things we want from what we call ETH one today and all the stuff we want from the beacon chain. And finally we want to kind of really trust that the new home for consensus is robust, is secure and so some of this is just sort of like a lot of testing but some of this is also kind of more visibility around what's going on over there.
00:46:54.290 - 00:48:32.700, Speaker C: So first of all, I realize there's a lot of shifting kind of perspective on what this looks like. But I've started thinking about Ethereum as sort of analogous to a traditional operating system and that there's different kind of layers in terms of what it is. Just as in an operating system you have kernel space which is kind of like the vital functions of the operating system and then you have user space which is things like higher level stuff. You might think of Ethereum as having kind of this consensus layer which is the core sort of like how do we even grow a blockchain? And then there's a lot of stuff that exists above that which maybe we all the execution which is like how do transactions get into the blockchain? What determines that a transaction is legitimate? I e the EVM. So if we use this model kind of like as our base this is kind of one useful way to think about what the merges. We are keeping the execution layer the same, the Ethereum we all know and love today. And of course layer two is above this somewhere and we are upgrading the consensus layer to this new super fancy fast consensus layer with nice properties.
00:48:32.700 - 00:49:42.550, Speaker C: So that's kind of like the hotswap, just like an image of it. And I'll get back to some of the details of what needs to be done there a little bit later. So just separately and to be more concrete, we need a client. And some of you may have seen this image before. This is, I think, taken from like an older research post kind of originally proposing the idea for what this client looks like. But to get across the finish line today, we probably don't want to rebuild all of the great things that exist in ETH One clients around this execution layer that I was talking about here. Instead, maybe we can and this is the current plan, we can define a clear RPC interface between ETH One and E Two clients and have an ETH Two client driving an ETH One client as our new combined client that will kind of like have all the things from both worlds that we want.
00:49:42.550 - 00:50:45.960, Speaker C: So to do this, there's some very detailed edge Casey spec work, which is a lot of what kind of you kind of merge R D has been focused on recently. We need to make sure we define that RPC such that it can work for every single ETH One client interacting with every single E Two client. Once you start having N squared number of possibilities, you want an interface that captures everything. You don't want to have to deal with those edge cases individually. So that's kind of technically, but more kind of pragmatically. We literally need these people to start the people developing these clients to start working together. So a lot of what's been going on behind the scenes has been sort of like the beginning of collaborations between folks on what previously were separate client teams.
00:50:45.960 - 00:52:01.292, Speaker C: Who knows what this will look like in the future, but there's a lot of cross communication that needs to happen. And finally, as I mentioned, this is a very edge case oriented thing. We want to make sure that this is very thoroughly tested, both kind of like unit tests and integration tests on the specs and clients themselves. But also we want to run devnets and testnets and see that things don't break when you start putting them under load. A lot of testing that still needs to be done. The third item for how do we get there? Was kind of like a proof of stake chain, like a consensus layer that we can kind of trust to be the new home for this booming execution layer that has all this cool stuff going on right now. And so separate from the merge itself, we want the beacon chain, you know, the proof of stake chain to have properties that make it resistant to capture, resistant to kind of like bugs and failures.
00:52:01.292 - 00:52:30.776, Speaker C: So we want things to be very thoroughly tested. We. Want audits. We don't want a chain that is mostly run by a few parties. We want a lot of hobbyists. We want to know that kind of like that part of the chain can fork and kind of social and technical processes around forks work. So we want to have experienced a hard fork in that chain.
00:52:30.776 - 00:53:28.476, Speaker C: And so all of this stuff is stuff that's being worked on. That's like a high level three areas that are kind of being worked on in parallel. It's valuable to yeah, I think I'll now talk a little bit about where we are today, which also kind of motivates what work is needed to get to the finish line. So I really like this image that's kind of like becoming sort of the norm for thinking about what the merge is. One of the reasons I like it because it sort of really clearly motivates like three separate places where work is happening. Work needs to happen on the top line, the proof of work line. Work needs to happen on the bottom line, kind of like the proof of stake line.
00:53:28.476 - 00:54:13.896, Speaker C: And work needs to happen right at this intersection. We need to figure out what needs to happen for this upgrade to kind of happen properly. And so I think in terms of thinking about where we are now, it's valuable to look at. Where are we here? Where are we in the proof of work land? Where are we in the proof of stake land and where are we with regards to merge? So I'll start with merge. So Xiaoi kind of like hinted at this and I'm not going to go into too much technical detail here. I want to leave some room for questions. But also all of this can be found on the east to Spec GitHub, which actually is linked on the next slide.
00:54:13.896 - 00:55:17.250, Speaker C: But if you go read the spec sorry, if you go read the spec, the way that the kind of basic kind of like the data structures are changing is actually quite simple. We're just sticking in the block that exists in the beacon chain. We're sticking in the information necessary to kind of process transactions. So if you look at this execution payload and if you've interfaced much with Ethereum via like Ethers or Web Three JS, you'll probably kind of recognize this because this is basically what's necessary to validate the transactions in a block. So literally just sticking that straight in to the beacon block. It's quite straightforward. There's still some testing that needs to be done, but you can go to this link here and actually get an idea of how this all works.
00:55:17.250 - 00:56:13.520, Speaker C: It's hard to be convincing when I'm not going through all the details, but it is pretty straightforward to understand if you have a basic understanding of how these two systems work that's kind of on the merge side. There is a release cut for the merge spec itself. There's some additional kind of unit testing that needs to be done. But we're basically there on the spec side. Next is implementation. Protolamdo is going to give a talk after me on Ryanism the merge dev testnet. We're very close to starting to test the merge client in production.
00:56:13.520 - 00:57:21.850, Speaker C: So on the proof of stake side, I think I mentioned earlier that we want to kind of know that we can hard fork and the first hard forks spec release has already been cut and we are at the stage of implementing it in all of the clients, which is really exciting. So we're pretty close to full confidence on this front. And if you want to learn more about what's in this hard fork, check this link out kind of side note, but one of the really cool things here, I think, is that Light client committees, the incentive to publish block headers for Light clients will be baked into the beacon chain itself. And this hard fork adds that functionality. Again, slight side note, but if you're excited about Light clients, this is kind of fun to dig into. And finally, on the proof of work side, as we all know, the Berlin hard work just happened. A bunch of important gas cost changes.
00:57:21.850 - 00:58:24.764, Speaker C: Sometime this summer we'll see London should happen and then after that, hopefully I think the merge is the next hard fork for the proof of work side. All right, so finally I'll talk a little bit about I think I've sort of set some parallel places where work is being done. I'll talk a little bit about where you can kind of get involved with this stuff today. So I think there's broadly these three areas. One is kind of like testing of the actual thing, like testing the merge itself, like writing unit tests and stuff like that. One is actually running the client and seeing what happens. 1 second, Frodo is going to talk about that after this.
00:58:24.764 - 00:59:23.490, Speaker C: But there's going to be a test net that you can participate in over the course of this hackathon. And actually really exciting. I think if you think that the upgrade proof of stake is like an interesting place, interesting period event in Ethereum history, I think participating in this testnet is a pretty historic thing to be a part of. And then finally just being involved with the Beacon Chain, contribute to Staking, like run a validator get involved, see what the process is like, see what doesn't work, and generally contribute to a healthy staging ecosystem. So on the spec test side, this is it's, I think. I think you you kind of just need to go read the spec and like understand. And I looked at earlier, where was it? It was here.
00:59:23.490 - 01:00:07.484, Speaker C: It's actually pretty like you can see that there's sort of these three links for the changes. It's not that long, it's pretty straightforward to understand. But I think what we need is people to read the spec and just think about edge cases. Think about, all right, this could happen right at the point of merge. So very high level. It's like there's a difficulty at which merge will trigger or at which beacon chain validators can start, including that ETH one payload in their blocks. And there's probably edge cases, there's probably things that we haven't thought of.
01:00:07.484 - 01:01:34.372, Speaker C: So go just think of those things and add GitHub issues for things you think are missing. I think one area in particular, just to flag something, one area that could always use more love is like that exact point at which the fork should happen. It should be simple, but as anyone who's worked on software knows, there's always weird edge cases you haven't thought about and just be great to get more eyes on that. I'm also happy to help Direct, so my contact info will be at the end of this and feel free to reach out next. And I don't want to talk too much about this because Proto is giving a whole talk on this, but there's a testnet ongoing and it's a pretty cool opportunity to be a part of something quite historic, which is the testing of the upgrade to proof of stake that we've been talking the community has been talking about since the beginning. So, yeah, go participate, go run, pick two clients that may not be, pick an ETH one client and e two client that aren't being tested by someone else and see what happens when you try to stick them together. And then finally, as I mentioned earlier, contribute to a healthy staking ecosystem.
01:01:34.372 - 01:02:58.164, Speaker C: So I pulled this from Etherscan earlier today, and it's kind of just to point out that there's a reasonable fraction of beacon chain deposits that are exchanges. And it would be great if the miscellaneous category here were a little bit larger as well as like the staking pool category, perhaps. So yeah, just go run validators, participate in the decentralized staking pools, help build tools that make it easier for other people to run validators. I think this is something that is very interested in from like a grants perspective. If you go try to run a validator and you start running into some headaches, we want it to be as easy as like one click if possible, someday. So if there are things that are standing between you and that you have ideas to stuff that is very important for a healthy, safe and ecosystem and stuff that the Ethereum Foundation is very interested in, and that's it. I think I blazed through my talk relative to the time allotted me.
01:02:58.164 - 01:03:00.950, Speaker C: So we have a bunch of time for questions.
01:03:02.600 - 01:03:40.640, Speaker A: Thanks, Luxman. And that was a really helpful overview of everything that's happening in this ecosystem. We do have a couple of questions, and I'll relay them as we kind of get this. And for those of you listening, we got a bunch of time for questions. If you see this thing after 32nd delay, type your questions and then we'll relay them back to Luxeman. So the first question is, you've talked about the need for a unified client that kind of bridges E one and E two transactions. So the question is, what does that actually look like in practice? Would that be something that behaves more like missed from the early days or something else and kind of just going deeper into that would be super helpful.
01:03:42.660 - 01:04:26.512, Speaker C: Yeah. So I can kind of talk through what the kind of control flow of this client is. So it's a Beacon Chain client, which does the sort of, like, growing of the blockchain that uses RPC to an ETH One client to figure out what transactions to add to a block and what transactions are valid. So you can think of it basically as a Beacon Chain client with this additional EVM attachment on it. It'll probably feel and operate very much like a Beacon Chain client today. But there will just kind of be this other thing it talks to to figure out, because the transaction pool management is a whole thing. Right.
01:04:26.512 - 01:04:57.800, Speaker C: There's a lot of hard software problems that go into that. The EVM is a hard thing. State size management is a hard thing. These are things that just didn't make sense to rewrite. So all of those pieces will be kind of you can think of it literally as a remote function call. All those pieces will be accessible to the beacon. So the Beacon Chain client will be kind of building and growing the blockchain and calling out to that when it needs to figure out, okay, what transactions should exist in this block, are these transactions valid according to the EVM, et cetera, et cetera.
01:05:00.300 - 01:05:19.104, Speaker A: Thanks for clarifying that. And I think there's another question that popped up from, I think, the slide that had the struct they were looking at. And the question is, why are transactions opaque but the payload isn't? And this may not just be fully in context, but I'm hoping that the.
01:05:19.142 - 01:05:23.600, Speaker C: Question is opaque but the payload isn't.
01:05:24.920 - 01:05:25.670, Speaker A: Yes.
01:05:29.400 - 01:05:43.720, Speaker C: I actually don't know the exact detail of why that has to name opaque transaction. Okay. Proto says that he will answer in his presentation. Yeah, that's better.
01:05:43.790 - 01:05:44.890, Speaker A: We'll have that question.
01:05:46.060 - 01:05:48.570, Speaker C: Proto will know better than I will know.
01:05:49.500 - 01:06:00.190, Speaker A: That's awesome. And I think kind of the one last question, then we'll end up to kind of put you on the spot. What's a realistic timeline for us to.
01:06:02.340 - 01:06:36.024, Speaker C: Well, first of all, I'm not the person who is the best person to ask. And also, I think that I am also hyper aware of the sort of the dangers of mispredicting the future with regards to timelines. But let's say within the next year from today, I don't know. I'm not going to say, like, when within that year, but within the next year soon.
01:06:36.142 - 01:06:58.364, Speaker A: TM right. Well, thanks so much again. I think if you have more questions, we'll lay them to you directly. Or if I see you are hanging out on the TV page as well. So hopefully you can answer those questions directly. So, thanks again. And with that, we are ready to move on to our next talk and also get that opaque transaction questions answered.
01:06:58.364 - 01:07:22.516, Speaker A: So, for the next talk, I'm super excited to welcome Protolanda, who's going to talk about rednessm. We've been hearing about this thing a lot and you've seen blog posts and tweets and everything else about it, but we want to make this really special and do a special talk around what's happening with the E Two testnet. And without further ado, please welcome Proto to tell us about the testnet.
01:07:22.708 - 01:07:36.270, Speaker D: Hello everyone. Right, I'll share my screen. One moment check.
01:07:36.720 - 01:07:38.190, Speaker A: You're all good to go.
01:07:38.720 - 01:08:15.332, Speaker D: Thank you. I'll make this full screen for you. There we go. So, thank you so much to Eve Global for hosting this day. We'll have a lot of talks about merge and I'm going to present about Raonism. So, Rayonism, is this kind of project over the whole Hecton? It's not going to be kind of submission. It's rather like this kind of thing where we with developers, client teams, write this hackathon file and we are building this merge testnet.
01:08:15.332 - 01:08:53.030, Speaker D: We are prototyping Sharding. So let me introduce myself. I'm Protolamta Or Dieteric, researcher at the Ethereum Foundation and a research things about Ethereum Two with Phase Zero, various kind of testnet deployments tooling, this kind of stuff. And then more recently, Rainism. So, if you're just joining in, we had just two talks, one from Xiaoi and one from Luxeman. If you missed them, you can watch them back on YouTube. Highly recommend.
01:08:53.030 - 01:09:45.700, Speaker D: The first talk went through the history, the second talk went through the status quo of the merge. And what I will be talking about is the next month or so with Prototyping development of this merge effort. And just to recap, in case you missed these talks. So what do we have? We have the ethereum one side and the ethereum two side. And you can think of them as the execution engine and the consensus layer or consensus engine. And they have their similarities, but the differences here are key. And the main thing why we keep them separated for now is that we can improve the transition.
01:09:45.700 - 01:10:20.288, Speaker D: If we refactor everything into one node, the merge hard fork is not going to be pretty. And at the same time, there are a lot of components that are just doing exactly what you want them to do. And breaking them during the merge is really not what we'd like to do. So instead, what we are doing is we keep a slim down version of Dev P to P. This is the networking layer of Ethereum One. Then we have the execution state. The storage you're familiar with will keep all the state around.
01:10:20.288 - 01:10:52.088, Speaker D: No changes, no application worries. Then we have the EVM, of course, the transaction pool will stay on this kind of execution engine. This one is marked for debate. We have this new gossip sub in Ethereum Two that serves a lot of consensus messages already. So there is definitely opportunity to improve this. And then the application RPC will stay the same. And on the Ethereum Two side, we introduced this new networking stack.
01:10:52.088 - 01:11:48.732, Speaker D: So this was a lot of work. But within the Ethereum Two ecosystem, we have the peer to peer layer in many different languages. Also, thanks to collaboration with Protocol Labs and other projects that use Lip ETP, we have this Beacon state which you could kind of compare to the execution state, but it's focused at consensus and it's this new structured binary form. So a binary tree to simplify all the light client work, which I'll talk about more later. We have new fork choice, of course, and we have the Fedator API. So Fedators are this new kind of thing, this separate kind of node that sign all the messages, all the consensus messages, and make the fork choice of this. So let's recap the roadmap for a little bit.
01:11:48.732 - 01:12:35.390, Speaker D: So what I'm not going to talk about is the full history or the full future. What I am going to touch on, though, is that there was this one tweet last year, I think around April, and then there was this revisit of the same roadmap in December when we launched. And I think it's like time to just revisit it again and see where we are. This is this full roadmap. There's a lot of research projects. There is Ethereum One, there are these separate Ethereum Three, or basically these kind of ideas where you could improve ethereum in a lot of different new ways, experimental ways. But what I'm going to do is to zoom in on the Ethereum Two part.
01:12:35.390 - 01:13:12.568, Speaker D: So let's have a look. So this is where we were at last December, right? We had just launched and then we were looking at the merge at Sharding specification. We were thinking of implementing light client support in the next hard fork. And so this is kind of where we are. If you take the day today, we have a complete merge spec. We have light client spec for the altar hard fork. Of course, these specs, they're not quite final, but are there you can work with them.
01:13:12.568 - 01:13:58.880, Speaker D: And there's this new Sharding spec which we'll build a prototype for in Rainism. And then this kind of ethereum bond, ethereum Two merge implementation work starts with Rainism where we try and prototype the post merge world. I'll talk about testnets later in this talk. And so the hard floors will look like altar merge and then charting. So, quick summary of altar for those that don't follow the spec as closely. It's this idea of a little stargazer or a light client that follows the state of consensus and then they can take the state. The state is represented with a binary tree.
01:13:58.880 - 01:14:55.800, Speaker D: And so when the root of the binary tree is signed off by a signed committee, the Light client can trust it and can follow down the path to just the data they are interested in without downloading all of it. So we had this pre release a little while ago called Stargazer. There are a few more pre releases and if you're interested in Light clients, definitely have a look at Lodestar and maybe reach out in the discord. I'll share some links after the call, after the talk. Then we have the hard fork two, the merge. This is what it's all about. So what you kind of think about is this execution engine, this rocket ship that we call Ethereum One, that we try and give this space habitat where it can scale, where we can enjoy proof of stake.
01:14:55.800 - 01:15:37.172, Speaker D: And in this kind of picture, really nice illustration from the Ethereum side is you can see these new building blocks being added to the habitat we're working on. Sharding and I'll talk about this later in the presentation. So what is this merge problem about? So there are eight plus client teams working on the merge. It's like this huge coordination effort. I think there are more. Like the thing is, with client teams, you have researchers, you have experimental clients, more and more implementations. I think you should be interested too.
01:15:37.172 - 01:16:40.600, Speaker D: Like if you're looking to get involved, there are various new clients that are up and running and that can use more developer help. Then we have a whole lot of new protocol specifications that carefully navigate this balance between the current Ethereum One chain and the new protostake system chain, or the beacon chain as we call it. And then we have as we merge the two with a few new challenges to support syncing the chain. So maybe some of you remember this. This is way back last year, one of the few last hackathons, it was in Eve Denver. Yes, we all miss them. What we kind of want to recreate and especially hope to get from this month long hackathon with Eve Global is that we kind of get this vibe, this motivation to work between a lot of teams.
01:16:40.600 - 01:17:40.124, Speaker D: This is really just a multi client project. It's communication interrupt first. We do want to see all these clients interoperate and this hackathon is really just to get and start moving. Like without this ID of an Ethereum Two beacon node chatting to an Ethereum monot, there wouldn't be a merge. We need this kind of prototype where we can see that we can steer consensus and where we can move forward with the beacon chain and basically pass through execution to Ethereum One and have this kind of model where we have both the best of both worlds, right? We have the EVM, which you are so familiar with, all the existing state. And then proof of stake. And then later on in Hecton, we kind of split this off.
01:17:40.124 - 01:18:12.680, Speaker D: I'll talk about this later in the presentation. We have merge transition research and then we have also Sharding research. So this is more to get into later. We have these kind of two specifications. Luxeman kind of covered the first one. We have Demerge, but then we also have this Rayness and merge spec. So what do you think of the merge spec? You say? How do you reason about the differences here? The first one is specific to Ethereum Two.
01:18:12.680 - 01:19:02.276, Speaker D: This is to the beacon chain. These are like type changes in the consensus state. These kind of things state transition on the consensus level. But what we are looking at with realism is this additional specification which covers an initial RPC. So this is this API between the Ethereum One node and the Ethereum Two node and all the little details involved in making it a coherent thing and getting this testnet to run. And special thanks to other researchers here, michael, Danny, Guillaume, everyone, really, this was like a huge effort to get to this point where you can simplify and where you can say, this is the minimal thing we really need. It's not this huge, complicated effort.
01:19:02.276 - 01:19:39.540, Speaker D: It's going to be a smooth sailing process to upgrade. So Raynards seek to break the barriers between the artist and the public. What does this mean? Well, for us as hackers during the hackathon, we'll want to build testnets. We want to build this kind of merge, this post merge world where we can see that the chain is stable, where you can test transactions on proof of stank. So this is what we are going to do. Let me try and put this into context. We have the Ethereum One chains of today.
01:19:39.540 - 01:20:13.888, Speaker D: We have the Ethereum Two chains of today and they're kind of separate, right? They're the same currency, but there are different chains with Rayonism. Right now what we are looking at is the post merge. This is where they are already merged. They start from Genesis, from the start of the testnet. They start in a combined manner. So what you have is the Ethereum One payload embedded in the Ethereum Two block. So we have EVM and proof of stake right away.
01:20:13.888 - 01:20:52.584, Speaker D: We can focus on sync, we can focus on validation, work on transactions and all the new infrastructure to make this usable. And then post tranism after the hackathon, what we are looking at is these new testnet upgrades. We kind of can combine any theorem one testnet and any Ethereum Two testnet and merge them. And you can do the same with mainnet. When you combine mainnet with Mainet, you get Mainet squared. Anyway. So what we have right now is this tutorial to set up a local test network.
01:20:52.584 - 01:21:51.432, Speaker D: So you everyone here on the presentation on the Hecton can already run this for themselves. And clients have been actively contributing instructions on how to run their clients in the merge setting both Ethereum One and Ethereum Two. And what we're looking at next week is to spawn this one day developer network where we test interrupt, we work through the initial issues and then we move forward. We iterate, we say we'll try one week DevNet. And what you kind of see here is the Prism Node, the locks where you see both attestation work of validators as well as transaction and application level data. And so it's really just combining the two and making it work. And then moving forward, we have the first like after the developer networks, we'll move to a test network.
01:21:51.432 - 01:22:56.124, Speaker D: This is more public thing and you can think of testing sync. When we run a longer chain we can try and ensure that it's not just the Beacon node providing blocks to the Ethereum node but we also have the Ethereum One node ability to sync the application state. So we refactor the existing sync and also we have these transactions. This is completely new to the beacon chain. And this kind of latches onto the question from the previous call with Luxeman what are these OPEC transactions? So the beacon block contains an execution payload and this contains the block data that we know today from Ethereum. And this contains a list of transactions. The thing is, Consensus does not know or have to know anything about the transaction itself.
01:22:56.124 - 01:23:49.460, Speaker D: It just needs to know if it's valid or not. So what happens is that it passes through the transaction list to the execution engine. The execution engine updates the state, the application state or the execution state. And then it returns this check and says, well, if it's feathered or not. And the idea of making them OPEC, but not making the payload as a whole OPEC is to be able to evolve in the future, to be able to miracle proof individual parts of the block in an elegant manner where you can separate all these little header fields. But within the transaction, that's very much up to the transaction format. And we just have this new EIP to define new transaction types.
01:23:49.460 - 01:24:48.884, Speaker D: This will stay part of the application layer or I should say the Exclusion engine and application layer makes use of these types of different transactions. Okay, so let me give you a quick example of one thing that breaks with the merge if you just try and naively combine the two clients. So previously we had proof of work, now proof of stack. And the fork choice differs in one fundamental way is that we move from total difficulty, which is this block by block thing, to weight of attestations. And the thing about attestations is that there are separate messages and that the weight can change. So if you add a block with a lot of attestations but then suddenly validators change their mind. There are these other Validators that have not voted yet.
01:24:48.884 - 01:25:41.480, Speaker D: They can vote for a different block. And so without adding a new block to the chain you can move to a different fork and with difficulty you didn't have the same problem. So if your mon clients will have to implement support for this new type of RPC message where it's the beacon node that says what the new hat is and then the Ethereum node forces a reorg manages their database to point to the head of the chain the current state of all the data in Ethereum. So what comes after? So after that we don't have a plan. What we do need to do is to iterate. So once the post merge is stable, we just want to break it again. Basically sync, stake, break, repeat.
01:25:41.480 - 01:26:26.740, Speaker D: It's that simple. What we have Ethereum is like a huge amount, basically one of the biggest decentralized exchanges, all the DApps. There's a whole like a super large ecosystem and we should protect it and we should test it before we move ahead towards the merge. And so with these testnets and then after the hackathon as well, it's not going to be a hackathon forever. We move towards production with iteration and you can help. We have a lot of different testnets and with each of the testnets we need to deploy Tuting. Think of block explorers, fork monitors, statistics, Faucets, RPC.
01:26:26.740 - 01:27:20.600, Speaker D: And then on the Ethereum Two side we have slashers and validators. People need to be able to deposit. There's lots of tooling and you can help build this out. So what do we have? We have the merge transition afterwards and then the Sharding prototype. With the merge transition you have to think about how do we get into this new type of chain where we have proof of stake and we have the execution layer. And then with the Sharding prototype is like how do we scale it, how do we make it better? And I think right now we're all focused and working together on making the post merge table. And then going forward we'll have some Ethereum Two researchers focus on the transition and a lot of the Ethereum One teams.
01:27:20.600 - 01:27:59.520, Speaker D: And then separately this is more of an Ethereum two thing. We'll have people looking into Sharding prototypes. So independent of the merge transition work, we'll work in parallel to tray a first Sharding test nut. We have a specification, we are building prototypes and then the next step is to build dev nuts to write Shard nodes, these kind of user focused nodes that provide all the data and then we can think about roll up testing. So what is this Sharding prototype about? I'm not going to get into the merge transition. This is something for after the hackathon. It should be taken very seriously.
01:27:59.520 - 01:28:51.220, Speaker D: But if the Sharning prototype will focus on getting to learn about this additional scaling, like how do we add a huge data availability layer to provide this layer, to provide the layer twos with the data. They need. So what is this data? So you can think of it like a data availability boost. It's not just data. We're not FalcoIn, we're not IPFS. What we care about is that the data is the same domain of trust as the rest of the chain. So what we have here is proof of custody to ensure validators hold onto the data and data availability sampling so that unavailable data does not get into the chain.
01:28:51.220 - 01:29:33.760, Speaker D: And then layer two execution. We're not quite at a point yet where we have layer one execution on charts. We had a lot of research into this, but if you think long about this, it makes a lot more sense to do this with rollups and cost chart communication kind of moves to layer two. And layer two cannot be a lot more creative and doesn't have the same constraints. It's really just the data that's the data availability, that is the bottleneck and that we can solve. So how about the sharding spec? We have the Ethereum two specs repository. There's this base functionality, there is the custody game and there is data availability sampling.
01:29:33.760 - 01:30:46.756, Speaker D: These are three separate parts and we kind of can just build the first debase, the essential part, and this is enough to get to a point of testing the other parts. These are additions and I'll look into these after Rayonism. So what are these essentials? Well, we recently had this nice post from Vitalik on Eve research about staggering of shared data. But what you should really focus on is that if you have chain with these Blue exclusion payloads and these little green headers of shard blobs, then we can register shared data, ensure that it's available in the future with the new upgrades. And we include shard blobs that move into the chain every slot and 64 at a time. I'm only drawing three to keep it minimal and simplified, but we're looking at 64 shards. So let's give you a walkthrough to the simplified sharding specification.
01:30:46.756 - 01:31:38.890, Speaker D: I won't get into any code on this presentation. There's a whole specs repository and if you're eager to learn more about the stuff, just reach out and then you can help build the new sharding prototype. So what is this shard block about? Well, it's really just a blob of data. The data can be extended, it's a list of bos points. And with this extension you can do fancy things like data availability sampling with the data itself. The extended data doesn't have to be included in the block, it's just repetition essentially a little bit more sophisticated. With the repetition of the data, we can summarize that data in a summary and then the header just includes the summary of the body.
01:31:38.890 - 01:32:33.352, Speaker D: This header is what is registered into the chain, which can also summarize it even more into our reference. And then, so whenever there's a double proposal on a chart, we can efficiently slash them without repeating any of the data. And so what I'm kind of omitting here is the signature scheme. So all the signatures for these kind of things are the same, thanks to the way the mercurialization works. What you should just really think about is how data moves along the network, how it's registered on chain and then with enough votes, it's confirmed by committees, the committees that we have in place today with phase zero. And so it's really just introduced these new types and few network changes. And we have the essential Sharding functionality in a prototype.
01:32:33.352 - 01:32:50.560, Speaker D: So that's exciting. Okay, maybe I liked maybe a little bit of code. So these are the kind of definitions we have in the specification. I won't get into any processing though. If you think this is readable, it really is. It's just python. You can get started with this.
01:32:50.560 - 01:33:41.392, Speaker D: If you'd like to contribute to Sharding, try it, read the specification, it's minimal. And then we can discuss building prototypes during Hecton. So what's the prototype about? We do proposals, we do Attestations and we do header inclusion. But what we leave for the future is this kind of data extension, data sampling and custody over all this data. The base layer of the Sharding prototype is enough to provide this API where roll ups and other layer twos can test against. They can get started building something before we even have a running test network. We could try this API because the Shard data is really just low level, low level.
01:33:41.392 - 01:34:22.910, Speaker D: It's just a key value star with a lot of security. And then we have this new KCG proof to register the Shard data to commit to it. And if we can add this into the EVM one EVM, then we can efficiently prove any type of Shard data during an EVM transaction. And then things like optimistic roll ups and whatnot can utilize the Shard data much better. So that's about my talk. If you have any questions, there is this panel as well. Just after this talk, you don't have to do time zone off.
01:34:22.910 - 01:35:10.510, Speaker D: Right after we'll have the Q A, we kind of get into the evolution of the consensus. You can ask anything technical things we have Daniel Ryan or community things we have Aftab. And then we have this break and a lot more talks. And then at the end of the merge summit, we close with this talk from Vitalik where we look into the future after the merge and how we introduce these kind of new things to improve solidify the merged beacon chain. And things like Sharding get a lot further than this base prototype. Thank you. Any questions?
01:35:11.520 - 01:35:42.680, Speaker A: Awesome. Thank you so much, Pearl. That was a really helpful intro and just a super detailed talk that covers a lot of things that all of us had questions about. We did see a lot of questions coming in and I'll start wheeling them in order. And also it's just awesome to see this being built during the hackathon. Just really good to see this all happening in real time. So the first question, which might be a quick one, is just does the Ray NISM testnet have a transition total difficulty like the main net will, or is it designed separately?
01:35:43.340 - 01:35:57.384, Speaker D: Right, so let me go back. I covered this maybe too quickly, but it's there. So this ID about passwords the screen.
01:35:57.422 - 01:36:01.070, Speaker A: Is not being shared right now. So you may want to one more time.
01:36:01.680 - 01:36:50.152, Speaker D: Okay, never mind. I'll try and explain anyway without the slide. So the post merge is about embedding the execution payloads into the beacon chain. And we can do this starting from Genesis. So we don't have the complexity of the transition just yet. We just make sure that what we fork into is a lot more stable before we focus on that one moment of the transition. And then once we have a stable start, the current phase zero chain and the current mainnet and a stable destination, which is the post merge chain which we develop a realism and then improve, we can really get into this transition from one to the other, which should just take one block.
01:36:50.152 - 01:37:08.148, Speaker D: And then there's this proposal with total difficulty and a few others to make the transition happen. I'm not going to get into that with Rainism. It's outside of scope. It's a lot more work. It's an active research. There are proposals. We can make it happen.
01:37:08.148 - 01:37:13.590, Speaker D: But I think that is not something you'd want to do in the hackathon just yet.
01:37:13.960 - 01:37:24.010, Speaker A: Got you. Awesome. The next one is just kind of from where you're at right now. What is the status of all the different clients? Which ones are ready to kind of follow the spec?
01:37:27.180 - 01:38:17.960, Speaker D: So we have this tutorial repository where we merge the instructions to run any client that participates. Right now we always had Deku, which is one of the first clients doing merge research along with Catalyst which is this new upgrade of Go Ethereum to facilitate the merge. So that's two clients. But then more recently we had a lot more input from both Ethereum Two and Ethereum teams with the Rainism project. So we have lighthouse, we have prism. And more recently we'll also get Nimbus on board on the Ethereum Two side. And then with Ethereum One, we have Nethermind ready for the first testnet.
01:38:17.960 - 01:38:34.690, Speaker D: We have Basu looking at joining one of the early testnets as well and then maybe open Ethereum in the future. I think we shouldn't push too much at the same time. This is a huge coordination effort, but there are definitely a lot of clients on board already.
01:38:36.260 - 01:39:03.572, Speaker A: Awesome. Yeah, there was another one around. Just where can we follow the mergenet spec and tutorial. And I think somebody already pasted the link on the chat. So I'll just kind of say that out of just completeness. There are a handful of questions that are happening, but I feel like there's a pretty active conversation going on the chat. So instead of directly asking and taking a snippet out of it, I feel like it'll be better if you were able to join the live chat and just kind of continue with the conversation.
01:39:03.572 - 01:39:45.510, Speaker A: But this has been extremely awesome and thank you so much telling us about everything that's happening with Rainism. And with that, we are ready to move on to our next talk. So the next talk is actually a panel and I'm super excited to be welcoming Danny Ryan, Trent Menps, and Aftab Hussain, aka DC Investor, who will be doing a panel on just the evolution of Ethereum consensus. Trent is going to be moderating this chat and going over how the Ethereum ecosystem has evolved from the beginning and sort of where the discussion is headed. And without further ado, I'd like to welcome Trent, Danny and everybody.
01:39:49.240 - 01:40:09.630, Speaker C: All right, so I'll just start off with a quick intro. Thank you, everybody who's behind the scenes. Really quick, just a shout out to the ETH Global team. Maybe some of you know I was formerly on this team, so it's an honor to be on the other side of this and I appreciate the people that are doing the hard work behind the scenes. So shout out.
01:40:10.160 - 01:40:10.476, Speaker E: Yeah.
01:40:10.498 - 01:40:36.550, Speaker C: And then I'll do a quick intro for Danny. He is the E two coordinator for everything. Proof of stake. He works with a lot of the client teams and DC Investor. Aftab is kind of a well known community member, heavily involved in NFT spinner on the space for quite a while. If you guys want to add anything, feel free. If not, you covered it.
01:40:36.550 - 01:41:22.532, Speaker C: Yeah, we can just jump right into it. So let's talk about the evolution of Ethereum Consensus. And what does that actually mean? I guess I'll start with an anecdote. So I'm working on a book with all of the East Two researchers now and soliciting submissions from all of these different researchers, and we're actually pretty close to releasing it. And one of the things I asked for is what was your start date with E Two research or when did you start building on it? And everybody gave pretty reasonable answers. 2018. I got interested at this point doing this and then Vitalik's response was like, January 2014 or something crazy.
01:41:22.532 - 01:42:06.684, Speaker C: So it really underscores the reality that this is and continues to be a long term project. Right. The Ethereum virtual machine and the Ethereum community are not static objects. They're always under this push and pull between different needs, different expectations from the people that are building it, the people that are part of the community. And I think it goes without saying that this is incredibly important. The community and the things that the community produces wouldn't be the way that they are, wouldn't experience the same success without that iterative process. I don't think it's going to take another six years from now or seven years, know, see out the full vision.
01:42:06.684 - 01:42:48.944, Speaker C: But Danny, why don't you start with saying a little bit about why do we keep working on these things even though they can take years, they change all the time. What would you say is the why behind ethereum generally and e two from your perspective? Right. And first I'm going to talk a little bit about the evolution of ethereum consensus. There's kind of many meanings in there and I think purposely so when we talk about consensus, we're talking about very frequently consensus mechanisms, this decentralized network, the actual technology behind it. But really it's also a coordination. The whole thing is a social and coordination and tool. And so ethereum consensus isn't just that piece of technology in the center.
01:42:48.944 - 01:43:55.140, Speaker C: It's also like the ecosystem, it's also an ideology, it's also the applications and how that affects the real world. And so hopefully we can get into a lot of that. And the why, I think everyone has their own why. I think that there's kind of this deep intuition with the people at the core of this community and rippling outward that decentralized technology, truly decentralized technology, will change the world for the better and will be very exciting as we do it. And so that's kind of the why is we think that we have something good. We think that through the iteration, refinement of this thing that is good, this decentralized thing, that we can make a positive impact in the world. And another component of that why the continued evolution, why not just static, we have a network today and that goes into is this decentralized thing that we have sufficient to make that type of impact that we want.
01:43:55.140 - 01:44:58.344, Speaker C: And Vitalik often writes about this, calls it like functional escape velocity. Does the current network have enough functionality, enough capacity to serve the demand for this new decentralized technology? And I think if you look at fees, if you look at just capacity and who has access to the system today, I think that the answer is no. I think that's the driving light is like one, we think decentralized technology is going to have a massive impact on the world. And two, what we have today, it's not going to satisfy that demand. And so we are constantly working on evolving this thing to get to a place that it is truly decentralized and can satisfy the global demand that we think will then have an incredible impact on the world. Yeah, definitely. To echo that, I think for a lot of people it's easy to see the potential in something.
01:44:58.344 - 01:45:50.008, Speaker C: But it's clear that we're very far from recognizing that entire breadth of potential. There's still so much to do. And like it was a few years ago, when transitions or changes happens in the roadmap, there's always going to be some things that you have to accommodate and make space for. The roadmap is never a static thing. And I guess so from a community perspective, maybe DC, you can talk about this a little bit too, but what role would you say the community has in know the researchers, they're working on hard technical problems. They are spending months researching the trade offs between different technical processes. And then at the end of it there's sort of this boiled down distilled answer.
01:45:50.008 - 01:46:45.390, Speaker C: But it's not really an answer, it's more like a pointer into the next direction. And so what would you say the role of the community has been in receiving these pointers or this information over the years as roadmaps develop? Danny, why don't you start and then we can go to DC. Yeah, I mean, the community has, I think, quite a large role in that, in that the community is the thing that is Ethereum. Again, ethereum is like a piece of technology but it's really like a social coordination tool. And it's a social coordination tool for the people that are building on it and using it now almost every day. And thus the community's needs and demands and wants certainly have to be like a component of the guiding light of the technological evolution. And so here's an example.
01:46:45.390 - 01:47:48.864, Speaker C: The community needs scaling now. The Ethereum community needs scaling now. And thus complementary techniques to l one. All these roll ups in l Two are in high demand and are evolving and coming out now. And so that demand and that pressure that's been from the community is actually likely going to have a significant impact on the layer one roadmap in that. Okay, if we go all in on these roll up technologies that allow for scale with the amount of layer one data, then how do we at l One support that to enhance the community, enhance that kind of demand and drive for community and that's give them more l one data. And so thus that kind of like the interplay between l one technology and l Two technology and community demands and community adoption is kind of like coming together to find a sufficient solution that is decentralized and satisfies ultimately the demands of the community.
01:47:48.864 - 01:47:53.168, Speaker C: But yeah, aftab you might have a different kind of like a technical answer to a community question.
01:47:53.254 - 01:47:54.352, Speaker D: I'd love to hear what you have to say.
01:47:54.406 - 01:49:02.584, Speaker F: Yeah, I think from my perspective so first of all, we have to view public blockchains like Ethereum fundamentally as social technologies. I think the biggest mistake we could ever make is to view it solely as this technology effort because really Ethereum is all about connecting people economically and in other ways. And the reality is taking a working blockchain like Ethereum, which is already securing hundreds of billions of dollars and it could even reach into the trillions over the next year realistically. And just swapping out the consensus mechanism is pretty difficult. And so I just want to take a moment actually to reflect on kind of what's been accomplished so far by the research and implementation teams and really just thank them and congratulate them, even seeding this bonded, validator network which is really what ETH Two exists as now with phase Zero is such a huge accomplishment. I mean, people have dedicated billions of dollars of their own assets and capital to the hope and to the expectation that this thing called E Two is realized. And I think from an impact on a community perspective there's a few ways in which I think about it.
01:49:02.584 - 01:49:51.768, Speaker F: So first, proof of stake is something that has clearly been anticipated for years and really since I've been involved with Ethereum since 2016, this is something that I've been looking forward to and obviously it's going to pave the way for a lot of longer term upgrades related to scaling. It will eventually include sharding. It's going to stack really well with the layer two roll ups which are kind of emerging right now. There are also other reasons why we want to think about this and I think one of them is clearly like the environmental issues which have really started to gain more popular traction at this point. And in particular, I think a lot of participants in the NFT space, which I am fairly involved with, they're bringing these concerns to the forefront. Now, I will say that a lot of those criticisms are not totally fair. They don't necessarily understand exactly all of the ins and outs of how the technology works.
01:49:51.768 - 01:50:35.876, Speaker F: And I don't necessarily think that proof of stake is a waste because the energy is being used for something. But if we can basically offer similar or even better security without expending that energy, then we should do that. I think also too, from a community perspective, a lot of us really view this as a better alignment of incentives for the long term operation of Ethereum as a network. And I think that comes down to just the fact that stakers are going to have to stake their ETH. They're putting it at stake, the base asset of the network. And so in a very real way they're invested in its success versus miners who on the margin are paid for their work and they don't necessarily have that long term alignment with the network. That's not to say that's true of all miners because a lot of miners are also community members.
01:50:35.876 - 01:51:16.850, Speaker F: But that's just an incentive shift. And I think finally for people who are in the community, and I think the community is anyone who uses Ethereum or holds Ether, it does reinforce ETH as this economic asset and I think in order for Staking to be successful it has to be recognized as an economic asset. And so in that way it's a nice companion to functionality like EIP 1559 and it's desirable by ETH holders. I think another underrated point is the fact that Ethereum has DFI and all of these other applications and uses for ETH built on top of it, going beyond that gas use actually is going to long term make for a more secure proof of stake mechanism because the base asset is useful in so many different.
01:51:19.480 - 01:52:25.032, Speaker C: Said. Yeah, I can echo some of that. So there's a lot of upgrades coming. Phase zero, the beacon chain really laid the foundation for that by putting the base proof of stake consensus mechanism in place, having these consensus participants validators, having tons of them and having kind of randomness generation and ability to architect them in the system to create a more sophisticated system as a whole. But the next one, obviously, which we've been talking about all morning with the talks prior shout out to Shaoi Lakshman and proto killing it. But I think it's important to highlight although the merge, what it does is it kind of keeps Ethereum uninterrupted and puts it in a new home. And this new proof of stake consensus, the question might be why? What's the point other than we landed on one with what DC said in that clearly it's a crypto economic consensus mechanism that doesn't rely on burning a bunch of energy, which I think is a major win.
01:52:25.032 - 01:53:06.112, Speaker C: But there's some other cool stuff that comes along with this that I think aren't talked about that much. So I'm going to just go through them quickly. It's really like a combination of a security upgrade, an economic upgrade, and a UX upgrade, and that there's better security properties in that when you move from proof of work to proof of stake, the crypto economic asset securing. The protocol is actually in the protocol, and so you can get a better security margin on the same amount of capital, kind of backing the protocol. So that's great. I mean, Ethereum's as said, securing hundreds of billions of dollars of economic activity and we need better security. It's also cheaper for security because you're not paying for depreciation.
01:53:06.112 - 01:54:05.672, Speaker C: And this hardware upgrade cycle and the energy cost of that, it's lower Issuance, like I just said, which is kind of like it's a security upgrade because if you paid the same amount of Issuance, you'd have more security or you can pay marginally more, less insurance, but have a similar higher security, which is also potentially enhances economic properties. You can debate that all day. Here's one that people don't talk about often, which is you get more regular block intervals. So for one, the block time is always 12 seconds instead of 15. So that's a nice marginal UX game, but you don't have this stochastic process and so you actually have much more of like a heartbeat and a tick on a proof of stake network. So the quality of service then is marginally better, which is nice finality, which is nice for UX. On the normal case, things don't get reorganized, not a big deal.
01:54:05.672 - 01:54:43.350, Speaker C: But in terms of major financial transactions and dealing with financial institutions and dealing with exchanges and things, you could have much stronger guarantees about reorgs. And so even if there was some sort of crypto economic attack, there would be limitations on depths of reorgs and that kind of stuff. So that's pretty important in the finance world and not burning tons of fossil fuels, which is a nice little icing on the cake. Definitely a cake that we're all looking forward to. Soon. Trademark soon TN, actually, soon. Actually, soon.
01:54:43.350 - 01:55:19.808, Speaker C: We spent a long time working on the beacon chain, working on the core of this consensus mechanism so that it is robust, and not only robust, but extensible and slotting in the function for the merge. Turns out it's like a few lines of code, actually. You're like, we have this consensus mechanism. It comes to consensus on itself. You can slot in additional things for it to come to consensus on. And so the application layer, the execution of ethereum, is actually pretty easy to pop in there. But yeah, soon.
01:55:19.808 - 01:56:14.336, Speaker C: TM soon. Yeah, I guess maybe we can jump back a few questions. When I was talking about the relationship between the research process and the community. And I think some people may have this perception that there's a hard line between the research or the researchers and then the users of that technology that's produced out of the research, when in reality, it's a much more permeable relationship or a permeable barrier, maybe. Danny, you can talk about what sort of insights you've gotten from the community over the years. For example, people participating on testnets, just regular community members making PRS to clients. What has that been like? Yeah, I mean, first of all, you kind of mentioned earlier in that although a minor has particular incentives, they also often are a community member or maybe an investor or this or that.
01:56:14.336 - 01:56:51.180, Speaker C: And so our roles are often very complicated and multidimensional. And I'll say for myself, I am a researcher, an engineer, but I also am a staker, and I also am a user. I was watching ETH Price last night and I was like, maybe I should unwind some of this DeFi stuff. So we all have a bunch of different perspectives. But yes, I think some of us are deeper in that stack than others. But the conversation is first and foremost, everything's incredibly open. The ETH R and D discord is like this incredible place where people are talking about all things layer.
01:56:51.180 - 01:57:38.348, Speaker C: Know, I'm scanning through ETH finance every once in a while. We're all hanging out on Twitter, and when you get into GitHub, all the issues around the EIPS and discussions are incredibly open. And so there's information first and foremost, there's information available to the, quote, community, the people that are not, say, a core dev or researchers or whatever. But there's also like, increasingly there's been an open dialogue, I think, the entire journey. But the community has also grown quite a bit. And so there is an open conversation and dialogue. But there's increasingly and a nod to Trent, there's increasingly some efforts to make that even better, to make sure we don't get into a place where an EIP is about to launch and the community is like, but no one's going to use it.
01:57:38.348 - 01:58:40.800, Speaker C: How did we miss that? And so not only are these doors and communication channels open, but I think there's some active and positive growth on increasing those communication channels. Yeah, I think one of the really awesome things to see over the past year was the growth of the East Staker community and their participation and just general support of all the testnets leading up to the Beacon Chain launch. And now there's a really solid community of consistent participants that are giving feedback to client teams participating on current test nets, things like that. And I don't think there would be an equivalent. The research teams wouldn't be able to muster up the same type of thing on their own. It's needed to have this type of grassroots community that are poking around things, breaking things. It's a crucial part of the process.
01:58:40.800 - 01:58:49.350, Speaker C: There were multiple times leading into the launch of last year when I was like, thank God these guys exist and this community exists because.
01:58:51.160 - 01:58:51.716, Speaker D: We would have.
01:58:51.738 - 01:59:23.470, Speaker C: Been able to do this much. But with them showing up and filling in that gap and just kind of like taking ownership of this domain, we were able to do this much and shout out to the open source ecosystem and just ethereum in general. If you're new to this or you haven't figured out that all the doors are open and you can contribute anywhere, all the doors are open and you can contribute to anything and anywhere, I ask you if it feels like something's missing, it's probably because it is and you can be the solution to that.
01:59:24.480 - 02:00:22.944, Speaker F: Yeah, and I always think it's funny when people view ethereum development as like this ivory tower where all these decisions are being made as like a dictatorship or something because it's completely the opposite. And I think most of the people who are watching this probably understand, but the devs can't just implement whatever upgrade they want. They have to create social consensus, certainly among node operators, but also among users who ultimately give value to the chain. And I think the ethereum development process kind of promotes that in a lot of really great ways. And I think a lot of that comes back to the fact that we have this multi client system where not any single team can necessarily dictate the future of what ethereum is. And yeah, that multi client process is hugely inefficient and slow sometimes, but if you didn't do it that way, you could end up with a result that's truly suboptimal. But I think overall ethereum development and certainly more recently on the E two side, really has brought in the community.
02:00:22.944 - 02:00:49.880, Speaker F: And so it wasn't that long ago when. Justin and Danny asked me to kind of participate in some of these E Two merge discussions as kind of a community advisor participant. And it's really interesting to see the energy and the excitement of that process, especially now, as the ETH One and ETH Two devs are kind of coming together as one group. And so, yeah, I'm really excited about kind of how we move forward with this effort.
02:00:51.120 - 02:00:54.590, Speaker C: I want to comment on the Ivory Tower thing in that.
02:00:57.760 - 02:00:58.860, Speaker E: When we say.
02:00:59.010 - 02:01:38.010, Speaker C: The community, sometimes it's like, what does the community want? It's hard to say. You can call bullshit on that. Sometimes I don't know what the community wants. I talk to a bunch of people and I kind of have a feel for what's going on. But there are certainly things where the community would trump the developers, like tenfold if tomorrow three teams were like, we're not doing 1559, we don't like it, there'd be no way. 1559 would not go to main net. Still this year there'd be a revolt, there'd be a new teams, and the community, actually, I would bet money on it, would figure out how to get 1559 to main net.
02:01:38.010 - 02:02:22.730, Speaker C: Yeah, definitely. It speaks to, like Aftab mentioned, this multi client culture is really important, not know for engaging different languages or people with different skill sets. It's also important socially because there's no way, it's much, much harder to bottleneck the community to prevent them from doing something that they see is really important. If you have multiple implementations, there's some freedom there to pursue different outcomes in some possible future. Ethereum researchers went rogue and decided to do like a proof of authority chain or something. We'd be like, no, we're not doing that. We'll find some other people.
02:02:22.730 - 02:03:44.256, Speaker C: But yeah, in related to that, can you perhaps talk about what this future execution and consensus client relationship will look like? Because as many people are familiar, there's ETH One clients gap. I'm not going to list them all, but there's a bunch and there's also a bunch of ETH Two clients. And what is actually going to happen at the merge? How are these teams going to work together? What's the separation of concerns there? Yeah, so we started writing about this a little over a year ago now. There's a couple of ETH research posts and us like, exploring, okay, the beacon chain is coming, but what does this actually look like with respect to ETH One, the current Ethereum main net? What does it look like with respect to the beacon chain as it launches and the software and the different teams? And so we have these ETH One clients, which hopefully I won't even have to call them that eventually. What are they? What is an ETH One client? It's actually this thin consensus mechanism called proof of work that we have not touched at all since the launch of Ethereum. And then everything else that they work on is really like this execution layer. It's really like state and the EVM and transaction management and all the things that the end user really interacts with.
02:03:44.256 - 02:04:39.952, Speaker C: Like the Proof of work provides the service, it provides the cradle for all these things. And then the complexity of an ETH One client and all the work that you see in optimizations of state management and all that crazy stuff is in this execution layer. And so what is the beacon chain? What are these E Two clients been doing for years now? And it's creating a highly sophisticated proof of stake consensus mechanism. And what are these teams good at? They're good at managing tons of validators. They're good at managing rewards and balances. They're good at managing tons of these messages, cryptographic messages coming together and aggregating them and managing and protecting against certain attack scenarios on the core proof of stake consensus. And so then what is the merge? The merge is take this ETH One client, get rid of that proof of work module that we haven't touched for years and is like very wasteful in energy consumption and replace it with the beacon chain.
02:04:39.952 - 02:05:31.270, Speaker C: Essentially the beacon chain is this proof of stake module and we're plugging it together with these execution, this highly sophisticated execution layer that we've been building for years. And so it's the proof of stake beacon chain module. Then it's driving similar how the proof of work module was driving the execution layer. The proof of stake module will be driving the execution layer. And so in practice, in what you're seeing in the Ray and as in testnets and the specs right now is take both pieces of software, run them together and instead of the proof of work module controlling the fork choice and telling it where the head is and things like that, instead it's that beacon chain client. You can create a thin communication protocol. I think it's like four messages, which is like add new execution payload, set new head, give me valuable execution payload which is similar to what the proof of work module would be doing.
02:05:31.270 - 02:06:17.712, Speaker C: But it's the beacon chain telling say it's Lighthouse telling geth how to manage the execution layer or it's Prism telling Nethermind how to manage the execution layer. And with this communication protocol, we get optionality too. I can take my favorite E Two client that I've run as a validator I'm very comfortable with and I can plug it into the execution layer that gives me what I'm looking for. Say Nethermind has these great sync capabilities and I'm very excited about that team. And so I can plug that into my Lighthouse beacon node. And so it's cool because we get optionality on these different layers and we also get specialization, team specialization on these different layers. So we have these teams that are really good at proof of state consensus and they can focus, stay laser focused on that.
02:06:17.712 - 02:07:34.590, Speaker C: And we have teams that are really good at the EVM and state management and they can stay laser focused on that. And so this modular separation of concerns and layers is really exciting. And it actually complements. If you followed Turbogath, they have Silkworm project, which is to not only have a modular consensus module or execution module, but to also take other components like the p to p and take the mem pool and other things and actually put those into modular components so that we can have further specialization in teams and software. And so it all kind of fits together really nicely and I'm really excited about it. Yeah, I don't want to say it's like a happy accident, but it really is a wonderful outcome that you can have these different it really is a different client philosophy. I think where you have, like you said, separation of concerns, people can focus on where their domain experts maybe a question is do you see any of these teams merging or like if they have a similar code, similar language or will they kind of maintain their editorial independence or two teams from Execution and Consensus? Yeah, we might see some of that.
02:07:34.590 - 02:08:37.232, Speaker C: I don't know if we'll see team mergers but we do see certain companies and teams do have a client on both sides of the aisle. So like, Consensus has the Besu e one and Teku e two client and they could go with this communication protocol which I would argue is a good thing to do because then end users have more optionality and maybe choose to use one, not the other rather than having very tight lock in. But they're built in the same language as the same team. So they could instead take the components of Besu, take the Execution engine and turn it into a library and plug it into like there's definitely some interesting architecture stuff to explore. You could also imagine taking Geth and make it a shared object library and people import it. I don't love that, especially out the gate because then I think you lose some optionality and you begin to entrench these tightly coupled clients. And so I think it might be a loss for some of the client diversity that we'd hope to see, but I think we will see some experimentation there.
02:08:37.232 - 02:09:20.350, Speaker C: Similarly, Nimbus, they have a production ETH two beacon chain client and they have something that it's like R and D e one client. I think they're picking up again so you could see them experiment with a similar too. And if we go the shared object library kind of thing, I hope that we see a good standard so that as a configuration parameter I could be like import guess, but then three days later I'd be like screw it, I actually want to import Nevermind. And from a software perspective it would look and feel like you were interacting with the same library. But today we're definitely going with this kind of like RPC modular communication approach as the first go at it.
02:09:22.240 - 02:10:06.540, Speaker F: And by the way, on the topic of ETH One and ETH Two, I know I'm not the only one on this call who looks forward to eliminating that terminology once we merge. And there's just one ethereum. And I think that's actually been Hindsight is 2020. But I feel like the naming kind of connoted the wrong idea behind ethereum as we evolve into this proof of stake architecture for consensus. And there always has been a vision for one Ethereum, this is just kind of the next part of it. So for me, it's really exciting just to see the teams come together with all of that brain power and great thoughts around how to progress ethereum working together on the same network.
02:10:06.880 - 02:10:29.652, Speaker C: Right. You heard me speak over and over again. I've said false sequentiality. I think teams are beginning to adopt these terms a little bit more and post merge. I think we'll see them a lot. And you have the consensus layer, you have the proof of stake consensus layer and there's beacon chain clients that run that. And then you have the execution layer and you have execution engines and you have what is an ETH One? Client is really that facilitates that.
02:10:29.652 - 02:11:03.650, Speaker C: So I think we'll hopefully see some conformance on that terminology over time. Yeah, definitely. While we were talking about client teams merging with a possibility, I couldn't help but think that this is like celebrity power couples where they merge their names. So bestu in tech, it would be like Beckhu. I'm happy to take this offline and consult to consensus what I think the best name combinations would be, but we'll have to. Yeah. And at home, I'll be like, oh, well, I run Death House.
02:11:03.650 - 02:11:33.544, Speaker C: I run Netherism. You can do a whole bunch of permutations. I don't have anything to follow that up with. But I think we should start thinking of these names because it's going to be important. It's critical. Yeah, I mean, naming is critical. In related to what you said, I think it's one of the like DC said, Hindsight is 2020.
02:11:33.544 - 02:12:03.984, Speaker C: Naming is incredibly hard. And clearly ETH one and ETH two. I think the phrase now is ETH One plus E two is Ethereum. That's the simplest way to put it. And we just need to keep hammering this message of they're on separate tracks today, but very soon we're just going to call it Ethereum and that'll be the easiest way. The reality is, once you say something or put it out in the world, it takes a month or two to permeate out to the edges of the network. Yeah.
02:12:03.984 - 02:13:01.830, Speaker C: And then undoing that is really hard. It also plays into just the general bizarre nature, like decentralized contributor sets. Nobody really controls this entire process. There are people who have authority to say certain things sometimes, but in reality it comes down to it's a very grassroots bottom up process. And so naming something ETH One or ETH Two a year ago, it takes a while to change and it's just kind of part of the process that we signed up for and it'll just take that repetition to adjust people's. Well, we have an attempted new nomenclature right now, but we will see how that develops and if and what changes kind of enter into lexicon. Right? Definitely.
02:13:01.830 - 02:13:35.840, Speaker C: Let's see what else. I think we are close to time, but we may have a little bit of extra time. I think we kind of had a far ranging discussion. I don't know if we need to keep going. This might be a good place to end it unless Danny or Aftab, do you want to jump in with any closing? Sure, go ahead. No, you're good. This is a question for Aftub and Trent.
02:13:35.840 - 02:13:38.530, Speaker C: Will the merge launch this year?
02:13:43.140 - 02:14:13.310, Speaker F: I'll start off and I think that would be incredibly optimistic and that would be incredible. And I think the merge will launch when it's ready. And I think what's exciting now is that there's a lot of energy behind making it a priority, certainly after the fork in July, potentially focusing efforts solely on that after that or shortly thereafter. So I know both sets of teams will do everything they can. So maybe certainly by early 2022, if not by the end of the year.
02:14:14.240 - 02:14:34.560, Speaker C: Yeah, definitely. It would be amazing if it happened this year. I don't know if Danny's putting us through his prediction market experiment. No. People just always ask me, so I felt like I finally had an opportunity to ask somebody else. All right, well, hopefully I don't end up being quoted in an article. Danny deferred his response to Trent.
02:14:34.560 - 02:15:19.970, Speaker C: Yeah, it would be amazing if it happened this know, December last year we had the launch of the Beacon chain and to follow it up a year later with the next massive step forward, it would be the best Christmas present a boy could ask for, but we'll see. Yeah. And it's not an incredibly complicated upgrade, but it's an incredibly important upgrade that we get. Correct. And so I think the long tail is going to be around security and testing. Rayanism is really the first kind of engineering whack at it and at some point in the summer we'll kind of be shifting gears into deep security and testing. I think we'll get a better feel around then.
02:15:19.970 - 02:15:50.170, Speaker C: Yeah, definitely. I mean, it goes without saying that there's not an arbitrary date that we're going to set to hold ourselves to. Security is paramount. Obviously launching this year would be nice to have, but the reality is this is a network that hosts billions of dollars of value and we're not going to push things to jeopardize that. I think that might be it. Kartik, you want to wrap up with anything?
02:15:50.620 - 02:16:39.908, Speaker A: No, I mean, I was just going to say this has been a very awesome casual nice discussion. I feel like this is more of a community gathering and I think I'd like to wrap this up in the same way. Danny, you started. Like, consensus is not on the technical side. This is about social economies sort of moving together in a direction and that requires a lot of moving pieces to kind of come and actually be without using the word consensus, I can just they have to be actually agreed upon and dealt with. So it's been a monumentous effort in doing this thing in the last few years of getting not only just the tech, but the people, the community, whether it's miners or developers or researchers, kind of all in the same place and on the same page to get everything moving forward. And this has been a really awesome perspective on how that's gone from each of your sides.
02:16:39.908 - 02:16:51.710, Speaker A: And hopefully, Trent, we get to see a lot more perspectives in details in the book and Danny and hopefully some of these bets that were just kind of made on come true this year.
02:16:54.340 - 02:17:01.600, Speaker C: I don't think Danny actually said his prediction. Oh, no, I defer to the experts.
02:17:05.060 - 02:17:27.000, Speaker A: Maybe we'll now have the same question asked when we do a DeFi hackathon and that's when we can actually have a market around this question. But with that, I want to just thank all of you again and hopefully we get to answer a few more questions that have popped up on the chat directly on Ecobol TV. So thanks again and we're looking forward to seeing the merge happen on production.
02:17:27.740 - 02:17:30.730, Speaker C: Thanks for having us. Thank you. Thank you.
02:17:31.180 - 02:28:37.986, Speaker A: Thanks everybody. So with that, we are going to be taking a quick ten minute break, so we'll see all of you in about nine to ten minutes and we'll resume with the rest of the schedule. So hopefully you get to enjoy a little bit of break to get food, drinks, stretch a little bit and also enjoy some Lo Fi Beats. So I'll see you all in ten minutes. Welcome back everybody. Hopefully you had some time to relax and enjoy some Lo Fi Beats. We are ready to resume the rest of the day's schedule.
02:28:37.986 - 02:29:10.020, Speaker A: We have a few more talks coming and then we'll be ending the day with the last talk by Vitalik on what comes after the merge. So we're going to do something different right now. We're going to have a special guest co hosting this thing with me and I'd like to welcome Frenzy from the Throne Foundation, who's working on the Solidity team to co host the next few talks with us. And she'll be the one taking over for me and I hope that you enjoy the rest of the day. So frenzy welcome. I'm super excited to have you back.
02:29:11.030 - 02:29:13.620, Speaker B: Hi, yeah, thank you for having me.
02:29:13.990 - 02:29:35.910, Speaker A: And it's awesome to see you here. Join us from a different part of the world. It's always awesome to get really nice members from our community and kind of make this event possible. And we're seeing so much excitement. We have a few hundred people joining us on YouTube and our TV chat, so it just gives me even more joy to have a lot more people excited about the merch.
02:29:37.230 - 02:30:15.798, Speaker B: Absolutely, yeah. Thanks so much, first of all, for having me as a co host for the second time now. I love those events. I think they are really the best way in the current state of the world to stay connected with everybody and to keep on exchanging on the current state of Ethereum and what's in store for the future of Ethereum. And yeah, I've been following the talk so far. I really like them. They were incredibly interesting and insightful and I think without further ado, I will do my job as a co host and introduce the first speaker that is up next.
02:30:15.798 - 02:30:48.930, Speaker B: That's Jim McDonald. Jim is CTO of attestant. That's a company that specializes in building tools that improve the staking experience. And today, Jim is going to talk about Vouch client diversity without the downsides. Vouch is an open source, validator client designed to operate with multiple beacon nodes. And Jim today will talk about why client diversity is a good thing and how Vouch can provide client diversity without the downsides. I am really excited to have you, Jim, and the stage is yours.
02:30:53.670 - 02:31:44.450, Speaker G: Thank you. OK, so I'm going to mainly run to a set of slides that makes life easy for everyone. So hopefully you can see the slide deck. And what we're going to talk today, as Ranzi said, is really looking at some of the information around client diversity, what it is, why it matters, how we can achieve it most easily with the current configuration and where we'd expect to go with it after the merge. So, obviously, the first point is to understand what client diversity actually is. We hear a lot about it, but the question is, what does it really mean? So, from our point of view, client diversity means securing the Ethereum Two beacon chain with multiple implementations of the specification. So Ethereum Two is primarily a specification of which each beacon node carries out an implementation.
02:31:44.450 - 02:32:41.154, Speaker G: Why do we want to do this? Why don't we just have a single implementation and everyone's happy? Well, first off, is it hardens the specification? If there is an assumption made in someone building a beacon node and there's just one beacon node out there, then that's the end of the story. However, if there are two or three beacon nodes out there, then there can be questions about, well, what does this really mean? I made an assumption here. I thought that this corner case wasn't relevant, whatever it might be. As a result of that, you end up with a specification with a single implementation being relatively loose. So this hardens specification up. Everything in a specification has to be very clearly defined it also hardens up implementations. So again, if we have just one or two implementations out there, then they can make assumptions, they can read a certain piece of text in a certain way.
02:32:41.154 - 02:33:24.414, Speaker G: Unfortunately, English, as the language of most specifications is written in is not as precise as it could be and is indeed very ambiguous in places. As a result of that, you can take a single read of the specification and create an implementation. You can create a second read of the specification and create a different implementation. If those implementations have to talk to each other, as Beacon notes, do, that however, then means that they have to finally agree on something. So you cannot leave things open to your own interpretation once there's consensus involved. Because of that, having client diversity hardens each individual implementation. Finally it separates specification and implementation.
02:33:24.414 - 02:34:12.270, Speaker G: Now this is a really important point. The ideal world, you can take a specification and create a full beacon node implementation from it. Equally, you could take a single beacon node implementation and rework backwards to create the full specification from that. The reason why you want that is because it means that each one is well defined. If your specification requires some kind of hidden knowledge inside the implementation, then that means the specification isn't well formed, it requires information from elsewhere. So this means that client diversity helps everyone and everything in the infrastructure and ultimately it gives us a much more stable and stronger base on which we build the protocol. So great if we've decided that client diversity is a good thing, let's go pick a client.
02:34:12.270 - 02:34:40.058, Speaker G: There are lots of clients out there though, so here's five of them. These aren't the only clients out there that carry out the ethereum two spec, but a good example, there are lots of them out there. So if we want to pick a client, how do we do it? What do we choose? So let's have a look at some strategies. One of them is let's go and pick the least popular. Well, if we want to have client diversity, we don't want to pick the most popular or the second most popular. We want the least popular. However, maybe it's least popular for a reason.
02:34:40.058 - 02:35:12.420, Speaker G: Maybe it's not quite there yet. Maybe some of the features aren't fully working. Maybe it has security issues, who knows? The problem is that without any additional information, just picking the least popular to increase client diversity could put you and your validating at risk. So maybe you go for a primary that's a little bit more popular or well known and then you pick a backup. Trouble with that of course, is that you don't use the backup very often. So the backup might be sitting around waiting for the primary to fail. And the primary never fails or fails three years from now.
02:35:12.420 - 02:35:35.820, Speaker G: Because of that, it's a very tricky situation just to say hey, we'll have a primary. No backup and we know the backup will be there. Unless you're continually testing it. That's a lot of work, a lot of effort. Makes life quite tricky. Maybe what you can do is you can measure the beacon chain beacon nodes. You can have a look at each of them, you can measure them according to certain metrics and find out which one you're happiest with.
02:35:35.820 - 02:36:20.326, Speaker G: Problem with that, of course, is choosing the right metrics. You might pick something like CPU footprint, but then again, a beacon node that doesn't carry out any proper work will have a much lower CPU footprint. Even though it doesn't succeed necessarily in creating the best blocks or the best attestations. You might have one as the lowest disk usage or network throughput. You might look for some metric that allows you to compare on chain performance. The problem is that you don't know which metric to use and the metrics will change as well. Over the last three to four months since the beacon chain went live, we have seen multiple revisions of all of the major clients as they go through their own testing internally, their own performance metrics.
02:36:20.326 - 02:37:23.840, Speaker G: They find bugs, they find optimizations, they will change. So even if you do find a perfect measurement and you measure it exactly and you pick a winner, the chances are your information is out of date two weeks later. So although there are lots of strategies for helping you to achieve client diversity, none of them really come without downsides. There are always issues to think about, there are always concerns, and as we've said, it's very, very much a moving target. So what if, what if you could run multiple clients at the same time? You could take the best information from each of them and use them to help secure the network while also gaining superior performance and great reliability. That's the type of thing that would be great because that way you take the best of every client, you work with them well, the rougher edges are smoothed out by other clients, you also get great performance and you get great reliability. So that would be a really nice thing to have.
02:37:23.840 - 02:37:56.982, Speaker G: And that is basically why we attestant Built Vouch. So Vouch is an open source, validator client. We have designed it from the ground up to work against multiple beacon nodes. We don't twin it with any single beacon node. In fact, we most of the time actually have no knowledge of which beacon node we're talking to. We have a heavily beacon agnostic view. What we want it to do is give us the information we need so we can do our work equally.
02:37:56.982 - 02:38:35.158, Speaker G: Having just spent the last few minutes talking about client diversity, the last thing we want to do is make Vouch a new single point of failure. Make it a system that everything else funnels through. Because if we do that, then we're back to where we started, where we have an ultimate client diversity of one. So because we want to avoid that, what we do is we offload as much work as we possibly can to the beacon nodes. We let the beacon nodes individually do all of the things that beacon nodes do. They gather state, they run state transition, they store historical data, they transfer data between beacon nodes. They do all of the things that you should do.
02:38:35.158 - 02:39:17.858, Speaker G: And what we do in Vouch is we look at the information that they give us and we act as a decision maker. So we will look at the information provided to us and we will decide which information is good or which information is best. And then finally we want to be resilient in the face of nodes that are unresponsive, maybe unsynchronized due to network errors or just internal problems or ones that are just slow. The reality is that beacon nodes all have different performance profiles. And one beacon node might be fantastic at generating proposals, but doesn't do very well generating aggregate attestations. One might have a fantastic network stack, but uses an awful lot of CPU. When we ask it to do work.
02:39:17.858 - 02:39:50.350, Speaker G: There's all sorts of different pros and cons with each of them. As mentioned, they're all getting way, way better and they're all stepping up. But the simple reality is they cannot all be best at everything. It's just not the way the world works. So we want to be able to talk to all of them and then be resilient if one of them suddenly stops responding, if we hit a bug or one of them has a problem with its network stack. Or we have a system where suddenly there's been some kind of packet flood attack and as a result one of the nodes goes offline. We need to continue to be resilient in all of these situations.
02:39:50.350 - 02:40:23.446, Speaker G: So to give a more concrete example of what we're talking about, we'll think about the idea of block proposals. So in Ethereum Two, when it becomes time to propose a block, each individual beacon node has its own view of state. It has its own view of all of the attestations that have yet to be included. It does its own work in creating aggregate attestations that will go into the blocks. So this is great for us. What we do is we say, okay, that's brilliant. What we want is we want a block proposal from each of you.
02:40:23.446 - 02:40:49.646, Speaker G: So for every beacon node we're talking to, and again, it can be any number of beacon nodes. We can run with any beacon node that works against the standard Ethereum Two API. We can talk to the beacon nodes in parallel. We can have multiple instances of the same beacon node. So we could have two prism nodes or three prism nodes scattered around the planet. So maybe they're getting different views that way we could have a tech on a lighthouse node wherever we want. We.
02:40:49.646 - 02:41:29.822, Speaker G: Can sort of mix and match as we choose. And again, as mentioned, Vouch doesn't really know or care what client software each beacon node is running physically, where it's located or anything like that. When it comes to propose a block, Vouch will just ask for a proposal from every client of which it knows. It will do it in parallel, it won't wait on one for the next one to come along and it will do this with a timeout. As mentioned, we need to be resilient in the face of a beacon node being unresponsive. Perhaps Vouch is sitting in Europe and a beacon node over in Asia we can't contact because of some kind of network glitch. We're not going to hang around and wait as a result.
02:41:29.822 - 02:42:42.450, Speaker G: So we contact all of the nodes we can, we get it in parallel, we bring the data back to us as each beacon node generates a block and returns it to us. We have to decide which one we want to sign. The way we do that is by carrying out a scoring mechanism. So we'll start to dig into the contents of the block and we'll basically run the internal ethereum two scoring mechanism or similar based on the information that we have, but also very importantly, the information from the wider network. So for example, if one beacon node is running for some reason, say it has a clock issue, we might end up getting a block that's earlier than actually the one that we asked for or one that doesn't have information about the correct parent block because it's had a network issue. There are lots of reasons and as we run this in a live environment we see lots of these reasons where for some issue or another we end up with a block that is non optimal. Maybe it doesn't have that many attestations in it, maybe the attestations it has in it haven't been heavily aggregated, whatever it might be.
02:42:42.450 - 02:43:24.382, Speaker G: The idea is that we will boil all of this down into a simple score. So we will look at all of the attributes of the block we have received, we will score each of them and we will assign that score to that block. As more and more blocks come in, we can compare one against the other and we will end up with after all of the beacon nodes have returned their data, or after we've had timeouts from the ones that aren't going to respond to us, we will have a block proposal with the highest score as far as we have ranked it. Once we've got that, that's great. We're now happy that we've done pretty well. We've got a block we're happy with. We will go through the standard signing process as you would expect, then we submit the block.
02:43:24.382 - 02:44:34.550, Speaker G: Now again, we can just send the block out back to a single instance, but that doesn't necessarily guarantee that it will get the widest reach if you send it to a beacon node that potentially again has network issues or has very few peers, that block may not reach much of the network. So what you want to do is again in parallel and again with a timeout, obviously if a beacon node is unresponsive or similar we give up in the end. But we want to submit that signed beacon block to every client we can reach by getting it to as many beacon nodes as possible. What that means is we end up in a situation where we have the greatest chance of getting that information out there, making it part of the chain. And obviously ultimately what we're trying to do here is secure the chain in the best way possible. And by doing this we are giving our block and the attestations in it the best chance possible to get out there be available. And as a result of that we can ensure that we have the best chances, we say, of that block being included in the network.
02:44:34.550 - 02:45:20.514, Speaker G: Okay, so one of the questions that has been flying around is well, what kind of impact do we think the merge is going to have on what we're doing at the moment? Well first off, the merge will give us more data. We're going to have more data on the beacon chain, we're going to have more data flowing between nodes. That is going to start showing up potential weaknesses in some network environments. It's going to show potential weaknesses in network stacks. So again, we won't expect all beacon nodes to be able to handle the new load as well as each other. There are going to be areas where one node might start to get even overloaded or similar. And it's not just down to the software.
02:45:20.514 - 02:46:21.882, Speaker G: This involves potentially the hardware that our peers are running on. It doesn't really matter how powerful our hardware is. If all of our peers are running on low powered hardware and they're getting overwhelmed that's going to slow down movement of data around the network. So having more data will make the world that little bit harder in terms of ensuring that we can communicate individually with all the rest of the nodes in the network. And because of that obviously again, the more nodes we talk to the better that we have in terms of a chance of getting that information out there. Secondly, subjective data. One of the interesting points about the beacon node is that at any point in time if we take state and then we say what should the next block look like in terms of who's its parent, how many attestations should it have, how should they be aggregated? There is an objective ideal block that could be created because of that.
02:46:21.882 - 02:47:12.250, Speaker G: It makes life frankly relatively easily in terms of block creation and attestation selection. There are a few corners in there but ultimately it's a pretty simple thing to do when we get to including application layer blocks. That changes because now we're including transactions. And the world of transactions is large and wild and there are any number of ways you can slice and dice the information out there. This means that then we have a situation coming down the line where suddenly the subjective selection of one piece of data over another into the application block will start to matter a lot more. This is a whole new area for Ethereum Two clients. Now potentially we hand a lot of this off back to the Ethereum One client to build.
02:47:12.250 - 02:47:59.702, Speaker G: But equally there are other alternatives out there. There are people like Flashbots who are building their own systems. We need to start spending more time looking at this. But the fact that there is going to be subjective data makes things like scoring blocks, makes providing optimal blocks and similar a lot more difficult. So it's something that as it comes down the line, it's really, really important to understand that the merge actually will make this kind of thing a lot more important in terms of being able to look at multiple feeds, multiple options for blocks, and being able to score and select the best one. And then of course, more operational diversity. We're going to have ETH One clients and ETH Two clients working with each other because they're now talking to each other in more than just the trivial fashion that they did with the current beacon chain.
02:47:59.702 - 02:48:32.658, Speaker G: It's going to become a lot more complicated. So we're going to see a big step change in terms of the complexity of the individual beacon nodes and their interactions. So just sort of summarize and look at what this means. Well, first off, and very, very important, client diversity is important. It's not just important to the network, it is important to you. It is important to you as someone who wants to send transactions on the Ethereum network. It is important to you as the validator.
02:48:32.658 - 02:49:13.314, Speaker G: It matters because without client diversity we'll end up with non permanent specification. We'll have specifications that have ill defined areas that are never caught because there's only a small number of implementations and they never find them. As we've said though, client diversity comes at a cost and the cost is your own ability to validate. If you want to run a minority client, fantastic. But there are less resources out there on the net to help you with them. Potentially it's not as secure in terms of some of the features and systems that are running in it. Simple example is some of the clients weren't actually active at the time that Madasha had its rough time.
02:49:13.314 - 02:50:03.730, Speaker G: Incident. As a result of that, there are situations that the less used clients have yet to be in. So there's always a concern that if you start to run these, you're going to end up in a situation where you are somehow disadvantaged. And client diversity is great and network diversity is fantastic, but very few people want to do it if it involves a direct cost to them compared to everyone else around them. So Vouch is one way that we've provided that will allow you to support client diversity. It doesn't give you any sacrifice in performance or reliability and in fact can increase both. There might be a minority client out there that isn't brilliant on the networking, it struggles a bit on CPU, but you know what it's brilliant at? Creating aggregate attestations.
02:50:03.730 - 02:50:31.994, Speaker G: If that's what it does, if that's the best piece of it, that's fantastic. Vouch will basically use that when you bring a block in. It will score it very highly. It will use that. So not every client will be best at everything. Not every client can be best at everything, but you can still run them and get the benefits of each of them without experiencing the downsides. That is it for the talk.
02:50:31.994 - 02:51:02.790, Speaker G: We'll do some Q A in a second. But just wanted to say, as we mentioned, this is an open source project. There is a URL there where you can download Vouch. It's used already in a number of areas, obviously internally inside our own infrastructure, but also elsewhere there are other areas where obviously if you go on there, you can file issues, you can contact us on our discord. There's a lot of areas where you can get the information about Vouch and obviously take a look to see if it's something that you would be interested in running to help provide that client diversity for the network.
02:51:04.970 - 02:51:29.710, Speaker B: Amazing. Thank you so much Jim. That was truly fascinating. So far I've only received a few comments here, two Attestations that you are indeed a rock star and people were asking if Vouch is open source. You already responded to that. Yes, it is open source. And here in the GitHub link you can have a look and probably also contribute.
02:51:29.710 - 02:51:39.662, Speaker B: Are there any other questions for Jim? Let me check. Other than that, I also believe you will be available in the chat, right?
02:51:39.716 - 02:52:34.462, Speaker G: Absolutely. And it's worth saying, obviously, yes, it is an open source project and there's a lot of stuff that can be done and added to it. So we have our own focus on what we think needs to be in there. But we definitely have other people's ideas that are coming in and it's something that and again, I strongly believe that with projects like this, the idea is to give people as much flexibility so they can create their own systems. For example, some people actually use Vouch not to validate, but actually to pull a bunch of additional metrics out so that you can provide comparative information across different beacon nodes in terms of how well they create blocks or Attestations or aggregates or similar. So you can start to do that type of thing too. So yeah, absolutely, it's primarily a validator client, but we have had other people actually manage to play with a few different variants on a theme.
02:52:34.462 - 02:52:39.410, Speaker G: And yes, as always, pull requests are gratefully received.
02:52:40.630 - 02:52:41.550, Speaker B: Yeah, amazing.
02:52:41.640 - 02:52:42.086, Speaker H: Interesting.
02:52:42.188 - 02:52:49.190, Speaker B: So now the questions are flowing in. The first question is with regards to your roadmap, what's next for Vouch?
02:52:50.890 - 02:53:12.698, Speaker G: So there are two streams for Vouch. One obviously is keeping track of the new changes. So we're starting to work on altar. Altar, I'm sure you're all aware, is the first hard fork for Ethereum Two that's meant to be going, we're not sure yet. June, July, August, whenever it may be. So we're starting to build out some additional work there to support that. The biggest piece for us will be sync committees.
02:53:12.698 - 02:53:44.638, Speaker G: So sync committees are a new feature that Validators will have to support. So that's going in there. So obviously we're keeping track of that. And a little bit further out, we're also looking at the changes the merge provide separately. We're always looking at enhancing some of the strategies that Vouch uses. We mentioned about scoring of blocks and attestations and similar. There are always ways in which they can be improved, where the efficiency can be increased, or equally, people can have different inputs on what they find valuable and what they want to score highly.
02:53:44.638 - 02:53:48.540, Speaker G: So absolutely, they're kind of the two rough streams that we're working with.
02:53:49.470 - 02:53:58.190, Speaker B: Okay, cool. Then the next question would be do you see Vouch implementing other social ranking mechanisms like they are used in evolutionary computing?
02:53:59.730 - 02:55:01.860, Speaker G: So one of the things that we played around with in Voucher was actually what we called synthetics. The idea that we don't take any of the information from any of the existing beacon nodes as gospel and we don't take them whole, but we use them all as advisory and then we break them apart and use that information to create our own synthetic proposals or attestations. Now part of that would obviously be trusting the information that's coming to us and there's a lot more of that coming down the line. So yes, up to a point equally, at this moment in time, we let the beacon nodes guide us very heavily in terms of what comes through. So again, we want to be a little bit careful about taking doing too much ourselves. As mentioned, we want to be a relatively small piece in the puzzle that AIDS client diversity, increases reliability and performance without attempting to kind of take over the world. So we have to be a little bit careful about adding things like social ranking and similar into the system.
02:55:03.190 - 02:55:12.290, Speaker B: Okay. And then last but not least, do you see Vouch also for ETH one nodes asked by a guest developer?
02:55:15.430 - 02:56:10.550, Speaker G: Vouch will integrate in some way with ETH one node. So obviously when we get to the point of the merge, if you think about the merge, at some stage an E two validator will have to propose a block and the question then is where that block comes from. If Ethereum Two nodes themselves talk to Ethereum One nodes and generate that, that's fine. Alternatively, and quite possibly, Vouch would talk directly to Ethereum One nodes separately to again gain the best application block it can find. That could also, incidentally apply, as we said, to other areas like Flashbots and similar. So, yes, we certainly want to and I think we will add that, as we say, as part of the merge. But yeah, totally, because the benefits, again, for client diversity and Ethereum One, the application layer are as strong, if not stronger than the benefits in the Beacon node.
02:56:12.090 - 02:56:17.738, Speaker B: Fascinating. All right, thank you so much, Jim, for sharing all these insights with us.
02:56:17.824 - 02:56:18.794, Speaker G: Thank you for having me on.
02:56:18.832 - 02:56:59.430, Speaker B: I guess you will stick around in the chat if guys have more questions. And here we will move on with the next two people in the program that I am very excited to introduce. It's two people. You can only see one on the slide, but it's actually two from the East Staker community that most of you will know. And like Superfys, the self proclaimed Beacon Chain Health Coordinator and Unveitika, his emotional support, human and graphics guru. So welcome the two to the stage and they will present today a layman's look at staking on Ethereum six months after Genesis.
02:57:00.410 - 02:57:02.118, Speaker E: Hey, thanks for having me, guys.
02:57:02.284 - 02:57:03.000, Speaker A: Hi.
02:57:05.870 - 02:57:45.714, Speaker E: So, yeah, I'm so glad to be here today. It's kind of nerve wracking to follow Jim. I've been friends with Jim for a long time. Well, in cryptocurrency years, it's been two weeks, so that's equivalent to 20 years. But Jim is a thoughtful and insightful fellow. So, yeah, I'm glad to follow him and I'm looking forward to sharing what little I know about Staking on East Two a little bit after Genesis. So I'd like to welcome everybody to the ETH Global Scaling Conference.
02:57:45.714 - 02:58:26.910, Speaker E: I'm going to talk about the state of Staking on Ethereum six months after launch. So I'm Superfiz, I'm from the ETH Staker community and that's a loose knit of group of people who share a similar goal to improving staking. You can find me at Reddit@reddit.com R slashestaker and on the Ethstaker discord at Invite Ggstaker. Ethaker also puts a lot of content on YouTube because we find out that's one of the most consumable formats for people. And you can find us at Slash eastaker. All of these sources are available at our main site, eastaker.
02:58:26.990 - 02:58:27.810, Speaker C: CC.
02:58:28.150 - 02:59:33.000, Speaker E: So we really look forward to building a strong community and we would love for you to join us there. The goal of this talk is to help you recognize features of healthy staking options. So when you participate in Staking, you can make choices that benefit the health of the network as well as your own investment. I'm going to begin by saying something that's going to seem a little controversial, that now is the golden age for Solo stakers and people who are interested in pooled Staking or Staking with an exchange should really consider waiting until after the merge is complete to deposit funds. You should know that my suggestions are always based on making careful decisions and not based on making spastic or knee jerk actions. I want you to be successful with Staking for 50 years, and not just for a few weeks. So before we get into the details of what that means, I want to tell you a little bit about the ETH Staker community.
02:59:33.000 - 03:00:53.490, Speaker E: At ETH Staker, we have two real goals to promote overall beacon chain health by advocating for open source trustless and decentralized staking, and to lower the technical bar for anyone that wants to stake on the Ethereum Network by developing tools that can improve access to staking with any of the four Genesis clients. We pursue these goals through our Steakhouse initiative, which is a group of developers that work together to find pinch points in the user experience and develop solutions for those pinches. And so I'm really proud of Colfax and what he's brought to the Steakhouse experience. Right now, they are putting the finishing touches on a single click GUI installer, so people who want to solo Stake can really do that through about three mouse clicks, which is a pretty interesting advance. Estaker Core is currently composed of five members superfizz, Lamboshi, Buddha, Worth, Halter and Invetica. And Unvetica helped me here with the slides, and Gray Wizard, another friend, helped me with organizing all of this content. It's really important to me, as one of the organizers of E Staker, that we're reaching out to as many people as we can to help them feel comfortable with staking.
03:00:53.490 - 03:01:44.930, Speaker E: I would like to say now we want E Staker to be more inclusive. We want to see better gender representation, more nationalities, and more cultures represented in East Staker. And I feel like just by opening the door and saying that we can begin to invite more people in, we're really interested in having your participation, whether you look like us or if you look different. Just come to our discord, ask questions, get involved, because we'd love to have your participation. We strive to be welcoming first and knowledgeable second. Hopefully we'll keep our egos in check as we become a part of the Ethereum Staking network. So now that you know about ETH Staker and how to get involved with the community for extra support, let's get into the main points of today's discussions.
03:01:44.930 - 03:02:23.390, Speaker E: First, I wanted to give an overall about who this talk is targeted to. The people we really want to talk to are the people who are curious about staking on Ethereum but don't know a whole lot about it. In many ways, this is going to be a follow up from the talk I gave in October 2020 about the introduction to Staking on the Ethereum Network. But we're six months beyond that now, and I want to give a new window for people who are interested but don't know where to start. And one of the biggest changes has been the change in the price of ethereum. In my last talk, I focused on solo miners. Today I want to talk about principles for staking with pools and exchanges.
03:02:23.390 - 03:03:06.490, Speaker E: When we covered an introduction to staking six months ago 32 Ether cost about $10,000. We all thought, wow, that's just so astronomical. But now 32 Ether costs over $60,000 and it'll probably be a different picture the next time you listen to that. I don't know whether it will go up or down but it will certainly be different. The reason this matters is really about the accessibility to staking. When solo staking started, it was far more likely for someone to have 32 ether and be ready to set up a staking node. Now, with the price being so different there are many more people that would like to participate in staking on Ethereum but they don't have 32 Ether to solo stake.
03:03:06.490 - 03:03:47.690, Speaker E: We're still very early in the process for stakers with less than 32 Ether. But I want to do my best to help you make good choices. Let's talk about how we got where we are today. Ethereum started in 2015 as a proof of work chain so anyone who wanted Ethereum in 2015 could use a computer graphics card to mine and basically exchange the cost of the graphics card and the cost of electricity to receive some ethercoins. That was a great system for distributing ethercoins. But even during launch Vitalik knew the ethereum network would move toward a new system called proof of stake in the future. Vitalik called this progression serenity.
03:03:47.690 - 03:04:48.586, Speaker E: Proof of Stake is a system where you put up a bond and use your computer to verify the state of the network and receive a small payout for doing that. Proof of Stake has a lot of benefits over proof of work and we're very confident that it will allow Ethereum to scale far into the future in an environmentally friendly way and also to enhance the user experience. The Beacon chain launched on December 2, 2020 almost six months ago. Since then, nearly 4 million Ether have been deposited to the deposit contract representing almost 125,000 in individual validators which have proposed almost a million blocks in over 32,000 epochs. If you were a validator that deposited during the Genesis block you'd have attested swell since then, you would have earned more than 1.3 Ether which in today's terms is around $3,000. The Beacon chain is still separate from the main ethereum network and it doesn't process any transactions or smart contracts at this time.
03:04:48.586 - 03:05:35.370, Speaker E: People who deposit funds for staking cannot withdraw them though it's likely that withdrawals will be possible by early 2022 and maybe even sooner than we expect. Since the launch of the Beacon Chain there have been about 135 slashings and the vast majority of those slashings are attributed to staking pools that were misconfigured or not ready to handle user funds. This is one of the main reasons that I encourage users to wait before participating with a pool. It's important to note that going offline isn't the same as being slashed. The goal of this talk is to teach you to recognize the features of healthy staking. So when you participate, you can make choices that benefit the health of the network as well as your own investment. I believe these two goals are closely linked and I want to help you make the connection.
03:05:35.370 - 03:06:20.170, Speaker E: So if we're moving to proof of stake, how does the ethereum network function today? Ether is the native token of the ethereum network. The current way of producing ether is through a system called proof of work mining. Some people are under the impression that ether will change when the network shifts to proof of stake. But ether will always be the same coin you hold. Now, people who want to earn ether through proof of work mining use either a graphics card in their computer or something called an ASIC, which is an application specific computer. Using a graphics card to mine, you can usually earn about a dollar or two of ether per day, or even more while the price of ether is really high. And then a miner can either add that to the stash or sell it.
03:06:20.170 - 03:07:19.760, Speaker E: Some high end miners will use an ASIC, which is optimized to do one job very well in this case, mining ether in a proof of work network. The downsides of mining with ASIC are that they're very expensive and they have a very short shelf life because they're only good for mining one type of hash. Proof of work mining uses a lot of electricity and generates a lot of heat. The ethereum community recognizes that these are unhealthy byproducts for the environment and we as a community have a moral obligation to correct these for humanity if we can. Proof of work has been a successful system and from a technological perspective it's not flawed. But from a moral and humanistic perspective, it's an arms race where miners must continue to upgrade hardware very frequently causing waste, consuming electricity and producing heat. The good news is that we no longer need this energy intensive mining system because proof of stake allows us to reach our goals more efficiently and in a more environmentally friendly way.
03:07:19.760 - 03:08:14.078, Speaker E: Proof of work obviously takes a lot of energy and as interest in cryptocurrency increases, it's only going to take more resources. Even though chip fabrication improvements make it more efficient, the energy required to be competitive with other miners will continue to increase. On the other hand, proof of stake will consume 110 thousand. I want to say that again 110 thousandth of the energy of proof of work mining and low power use will secure the network for the foreseeable future. As computer hardware improves and becomes more efficient, that power usage will remain stable or even decrease because we'll no longer be in a mining arms race. I'd like to give an example of that and I don't encourage you to go out and spend money with a belief this will work perfectly or for the long term. But the Nimbus client runs very well on a Raspberry Pi computer.
03:08:14.078 - 03:09:20.978, Speaker E: So we're going from people using warehouses of 50,000 GPUs to proof of stake where you could essentially secure the network as efficiently by using a Raspberry Pi computer and a good hard drive which today costs about $200. The important thing to know here is that proof of stake can secure the ethereum chain for about 110 thousandth of the energy that's required for proof of work. And it will actually improve the consistency of network timing, which benefits the end user as a person that wants to get more involved in staking. Or maybe you're already doing solo validating and you want to have a strategy for explaining the options to your friends and colleagues. How does this impact you? As we begin to talk about the ways that you can stake your ether? I want to talk about the need for the ethereum network to increase decentralization. It's important that I say we increase decentralization because if we do things the easiest way possible we'll tend towards centralization. This is when people say to me fizz, I'm just here to make money.
03:09:20.978 - 03:10:02.158, Speaker E: I don't care about all of that network stuff, I don't care about centralization. I'm an investor and I just want to make a good return on my investment. And I understand that. And so let me respond by saying that your investment is ether and for ether to appreciate in value as an investment it needs to be as healthy as it can be. And the way for you to improve the health of your ether is by letting your ether work to improve decentralization of the network. So everyone says I don't care about the platform, I'm just an investor and they invest in ways that centralize the network. Then they're actually undermining the value proposition of their own investment.
03:10:02.158 - 03:10:44.750, Speaker E: It doesn't make sense. So let's get into three types of staking options centralized, distributed and decentralized. By design, the ethereum proof of stake network is decentralized. Anyone who wants to start a validator anywhere in the world can start solo validating and they will essentially be a part of the decentralized network. By contrast, centralization happens when a lot of people send their deposits to an exchange and have the exchange stake on their behalf. The exchange is really doing all of the staking and they're concentrating a lot of ether in one place and that begins to threaten the decentralization of the network. A distributed model is somewhere in the middle.
03:10:44.750 - 03:11:21.420, Speaker E: An example of a distributed model might be staking pools that take ether from you and hold it on your behalf and stake at a smaller scale than exchanges that represent a centralized model. So with a. Distributed model. We're essentially looking at hundreds of smaller pools that hold your ether and stake it for you. It's better than being centralized in an exchange, but it's still not the ultimate goal of decentralized staking. Decentralized staking is the gold standard. Decentralized staking is when you have a lot of people running validators in their homes and the world is covered in a thin layer of staking nodes rather than finding them all in one place.
03:11:21.420 - 03:12:09.944, Speaker E: Responsible investors need to take these concepts into account when determining how to stake either. While solo validating is the gold standard of decentralized staking, there are software solutions that can promote decentralized pool staking. They're coming very soon. But the point of this slide is, as a community we need to demand decentralized staking solutions. And we shouldn't fall into the trap of centralized staking just to make money, because focusing on short term profit can be a recipe for long term loss. At this point, I'd like to introduce the Scalability trilema as a potential staker. You may be most interested in finding out where you can send your funds to make the highest return.
03:12:09.944 - 03:13:00.748, Speaker E: But as a reminder, the goal of this talk is to teach you to recognize the feature of healthy staking so that when you participate you can make choices that benefit the health of the network as well as your own investment. The Scalability Trilemma is a triangle with three points security, decentralization and scalability. These points represent three features of an optimal blockchain and it's actually difficult to balance these three things. We found that sacrificing one can give you a tremendous boost on the other two, but at the cost of a healthy chain. And the proof of stake network on ethereum is the premier blockchain because it balances this trilemma most effectively. Let's dive deeper into one of these points decentralization. In a decentralized network, nodes validate all over the world and they're managed by independent operators.
03:13:00.748 - 03:13:42.472, Speaker E: In a decentralized network, taking any number of these nodes down wouldn't fatally damage the network. The decentralized nodes are interconnected and spread out fairly evenly over a large geographic area. This is important for people with less than 32 ether who want to be involved with a proof of stake network in order to maintain decentralization. People who want to participate by staking shouldn't give all of their funds to the same entity. I'm not even suggesting that we don't want to maintain the current decentralization of ethereum. We want to make the network even stronger by increasing the current decentralization. As someone that wants to participate in staking without running their own node.
03:13:42.472 - 03:14:11.156, Speaker E: And as a community we should be looking for solutions that are trustless, open source and decentralized. We want to ensure that pool or exchange operators don't have control over your coins at any time so they can't disappear with your coins or worst yet, cast votes that harm the network or get captured by a government. We want to make sure their code is open source and that it's reviewed by the community. And we want to make sure that the solutions maintain and promote decentralization of the network rather than centralizing it.
03:14:11.178 - 03:14:11.780, Speaker A: Further.
03:14:12.520 - 03:14:58.612, Speaker E: I acknowledge this is a high standard, but it's one that we as a community need to strive for if we believe that Ethereum will continue to grow and develop into a world changing idea. Okay, so now that we've outlined the features to look for in a staking solution, I want to suggest three general ways that you could run a proof of stake validator. These three methods are solo staking, tooled staking, and pooled staking. Solo staking is when you have 32 Ether and you stake it by yourself using one of the four clients implementations nimbus, Teku, Lighthouse, and Prism. In solo Staking, you run one of these clients on your own hardware. And this is generally what I consider the gold standard of staking. If you can solo stake, that's fantastic, you should do it.
03:14:58.612 - 03:15:34.288, Speaker E: A lot of people are doing that now. Although Solo Staking provides excellent decentralization for the network, it's important to acknowledge that it's not right for everyone. It requires technical ability, hardware, and 32 Ether. After solo Staking, the next option would be Tooled staking. Tooled staking might be something like running an installer that sets up the staking node for you. It might be using a remote signer to help you manage the chain data or using a third party data stream to run the node. And so picking up these third party tools to make staking easier is fine when it's the right solution for you.
03:15:34.288 - 03:16:21.836, Speaker E: It's not the gold standard, but it's very acceptable when the tools are community vetted and known to work. The third option would be pooled staking or exchange based staking. And that generally represents people who have Ether less than 32 Ether or they have more than 32 Ether and they don't want to deal with any of the technical complexities or don't have access to the hardware or network requirements. The option that you pick will depend on your own personal situation. So I'm hoping this information will give you the right background to make a more informed decision which will provide benefit both for you and the Ethereum network as a whole. I want to highlight the four Solo Staking. Clients nimbus, Lighthouse, Teku and Prism.
03:16:21.836 - 03:17:21.650, Speaker E: Each of these staking clients is working very well, and they're all running on the network today. One thing I should encourage anyone to do is promote client decentralization by using the client that you believe is least used in the network. The reason this is important is if all four clients have 25% of the client share and one of the clients has a catastrophic event, then the network will still function just fine as long as 66% of the nodes are still running. But if nodes are running, a client that represents more than 33% of the network and that client has a catastrophic failure, the network can't finalize until that client is corrected. The chances of this happening in the real world are extremely low, but it still makes good sense to plan for the possibility. So the best way we can use this knowledge to keep the network healthy is by using a client that is least used to improve client diversity. Right now that client is nimbus, but they're doing such a great job implementing features that I'm not sure that they'll be the minority client for long.
03:17:21.650 - 03:18:27.450, Speaker E: Tooled staking is another layer of support on top of solo staking, and essentially that means using tools like Avato Dapnode, a single click installer developed by Steakhouse, or implementing a single shared validator. Those tools are layers on top of the original implementation clients that can improve the staking experience. I definitely encourage anyone who can benefit from these tools to check them out. I would say that although they represent an additional layer or potential failure on top of the implementation clients, in many cases they're going to provide additional protections and support that make the value proposition balance out. So if you're planning to do tooled Staking, please do your research and get help from the community to make sure the tools you plan to use support a trustless, open source, and decentralized network. I heartily encourage people to check out Tooled Staking solutions if you have a good reason to believe it will improve your staking experience. The final thing I want to talk about in regard to decentralization are pooled and exchanged options.
03:18:27.450 - 03:19:09.120, Speaker E: These staking options on their own are on their own continuum. In general, pooled staking that is custodial is more risky for you. So if you're giving someone your ether, you need to trust 100% that you're going to get it back from them. If you send your ether to a pool that you don't know, that maybe no one has ever heard of, there's a fair chance you won't get it back at all. If you stake with a pool that has a very strong reputation, then you have a good likelihood of receiving your ether back. People often tell me they are comfortable staking with a wellknown exchange because they're confident that they'll be able to get their coins back. But this logic fails to consider the risk of centralization moving down the continuum.
03:19:09.120 - 03:20:13.720, Speaker E: Next would be a semicastodial pool, which for example, would be where you share keys with different actors in the network to reduce the risk of losing access to your funds. One example of how this work would work would be maybe you keep part of the key, maybe they have another part of the key, and it would take a collaboration of actors in order to risk your funds. Semicstodial pools are better than a fully custodial solution, but the real solution we're looking for would be to have a noncustodial solution where the pool operators have zero access to your staked ether. This is generally something that's managed by a smart contract. This kind of noncustodial offering means that malicious pool operators couldn't take your funds and disappear with them. At this moment, we don't have any noncustodial staking pools but I do believe that's something we'll look forward to seeing in the next few months and hopefully we'll see a lot of healthy competition in this space. As a simple recap for decentralization, the most important thing you can do as a person who wants to stake ether is to ensure that you're staking in a way that promotes network health and network decentralization.
03:20:13.720 - 03:21:18.160, Speaker E: As I said before, just focusing on getting the most monetary reward for your ether isn't the best recipe for success for you or the ethereum network as a whole. The best recipe for success is staking in a way that promotes just decentralization so that in 15 years when you're ready to reap the rewards they're maximized by the health of the network, now is a great time to switch gears and talk about some common misconceptions about staking. The first one is this concept that the switch to proof of stake will lead to a fee reduction for Ethereum transactions. Proof of stake on ethereum will not reduce the fees charged for transactions, but it will provide more consistent block times. Blocks or slots on the Ethereum chain are exactly 12 seconds which is a contrast to the current block time that averages 13 seconds. From the user experience perspective, transactions will happen more quickly and more consistently when the ETH one and E two chains are merged. The second thing I'd like to mention is a sensible misunderstanding about slashing.
03:21:18.160 - 03:22:16.112, Speaker E: Many people are concerned that anytime their validator goes down or anytime their staking node goes down, they'll experience what's called a slashing. A slashing is when you have actually done something to conflict with the network and you're penalized and literally removed from the validating responsibility resulting in the loss of funds. A flashing doesn't happen by accident either. You would have to program your validator client to do something nefarious or more likely, you're running your validator keys in more than one place and that can result in flashing. So far, with 135 slashings on the beacon chain network, over 90% of them have been caused by pools who misconfigured and ran their validator keys in more than one location. The more common penalty is called leakage, and that's simply being offline as a Validator. With leakage, you lose about as much ether as you would have gained had you been online during that time.
03:22:16.112 - 03:22:57.232, Speaker E: Leaking is not a significant penalty. It will happen to everyone at some point and it's really nothing to worry about. So hopefully debunking these myths will help you understand a little more what you would most likely have to go out of your way to get slashed as a validator and that leaking penalties are very small and a normal part of validating which shouldn't scare you. So the reason you came to the talk today was to find out how you can stake effectively and what you should take into consideration when deciding how to stake. Let me recap some of the highlights. Two of the biggest benefits of proof of stake to every user are extreme energy savings, chain security and predictable block times. I said two, and then I gave three.
03:22:57.232 - 03:23:47.724, Speaker E: We also highlighted the need to maintain network decentralization by staking responsibly and make sure whatever staking solution you choose is trustless, decentralized and open source for people with less than 32 Ether. The next steps are a little new for all of us because pooled staking hasn't quite reached a point where I'm comfortable recommending solutions. But we're very close. If you look around now, you'll find a lot of exchanges offering staking products and a lot of pools offering staking. And it's not my place to say those options are wrong. But if you're unable to solo validate, I'd encourage you to hold your funds until staking solutions are available that are open source, decentralized and trustless. These solutions will give you peace of mind knowing that you're supporting network health and it will reduce your likelihood of loss in the future.
03:23:47.724 - 03:24:45.970, Speaker E: I know there's at least one provider near launch, but I'm really looking forward to more providers to enter the space and for those to have had time to mature on the network. It's very important for me to say that when a provider who meets these criteria does launch, you would be very wise to watch cautiously for three to six months after launch before depositing funds. Smart ethereums are here to build a long term stash, not to yolo into the first opportunity they can find. If you're still here, I'd like to encourage you to claim a proof of attendance protocol POAP for this talk. To get that, you can hop onto the ETH Staker Discord or message the POAP bot wherever you can find it. You'll send that bot a private message that says ETH Global all one word it's lowercase. If you don't know how to find the poet bot, you can go to the ETH Staker Discord and lots of people will be there to help you find it.
03:24:45.970 - 03:25:41.168, Speaker E: The staking network is still in its infancy now, and it's the prime time for solo stakers. As a staker with less than 32 Ether or a not technically inclined staker. The best thing that I believe you can do is probably wait three to six months to see what other solutions become available and ensure that your choice is open source, trustless and decentralized. I know many people feel antsy, like they need to do something to earn returns on their ether now, but I want to remind you that holding your ether is often the smartest choice you can make. What you might do during that time is reach out to pool operators, make connections in the community to encourage providers to develop solutions that are open source, trustless, and decentralized. These are solutions that promote the health of the overall network and not just the pool operator. If you're not sure how to get started with a community, Estaker would love to have you.
03:25:41.168 - 03:25:56.330, Speaker E: Please join us on Discord or Reddit@reddit.com R slash Estaker. With that, I hope you have a great rest of your day, and thanks for joining. To learn more about what you should do or keep in mind as you consider staking your ether. Thank you.
03:25:59.100 - 03:26:35.328, Speaker B: Wow. Thanks so much. That was an incredibly comprehensive overview of everything, and I love how you always make this so easy to understand and use language that should be familiar with also people that are new to the ethereum ecosystem. So, yeah, well done. Congrats on making it so easy to understand. We have a couple of questions, but before I ask them also, I love how the last two talks were both kind of like pep talks for client diversity, trustlessness, and decentralized staking. So, yeah, that both hit basically the same note.
03:26:35.504 - 03:26:36.790, Speaker E: I speak my heart.
03:26:39.160 - 03:26:57.192, Speaker B: We do have a couple of questions. Let me go back in the chat and find them. First of all, you mentioned the slashings. Do we have some insights of what was the cause of the latest slashings? This person says it would be nice to have an overview of the slashing causes for each individual slashing.
03:26:57.336 - 03:27:57.836, Speaker E: Yeah, that's great. Actually, I spent a lot of time tracking the early slashings because it was very important to know if it was something that we could coordinate to protect the community from. And like I said, 90% of those slashings were perpetrated by staking pools. And I work really hard to, as much as I can, only say positive things about people or organizations. And so I don't want to identify the pools that were slashed, but there were about three major slashing events. One of them was a large pool that was slashed, and then subsequently a smaller pool was slashed twice or three times in a row. And the lesson that I took away from that is home validators are pretty safe, but pools are trying to get up to speed as quickly as possible, and it can be challenging because they want to provide the highest returns.
03:27:57.836 - 03:28:25.290, Speaker E: And what we found is that they're willing to try a lot of different configurations so they can say they have the highest returns, and while they're testing those, they often make mistakes that lead to being slashed. The good news is, in the past 60 days, I think we've only seen two individual slashings, which sort of gives us a hint that pools have a better grasp on that now.
03:28:28.140 - 03:28:36.350, Speaker B: And then. Secondly, oh, my goodness, people are typing a lot in the chat. The poop word make everybody go crazy.
03:28:38.960 - 03:28:39.720, Speaker E: No worries.
03:28:39.800 - 03:28:52.690, Speaker B: The second question I also remember it by heart, I guess, was around the fact that we see an increasing share of Validators now being or like staking being done via exchanges. How can we prevent this?
03:28:53.620 - 03:29:49.620, Speaker E: I don't think the ultimate goal is to prevent exchanges from staking. Their goal is to make money, and they're going to do that in any way they can. Our role is to educate the community and help educated Ethereums recognize that exchanges are not the best way to stake. But at the end of the day, when it's a grandma and no offense to grandmas, when it's someone who has so little interest, I want them to have that opportunity available. It's just that anyone who we can educate if you can educate your grandmother to find a better staking solution, then by all means do that. But as a final backstop, it's great if some people, if very few people choose to rely on exchanges for staking, I'm happy that they're providing the service. I just hope that educated Ethereums will make better choices.
03:29:52.120 - 03:29:56.630, Speaker B: That makes sense. Thank you so much for doing your part in educating everybody.
03:29:57.240 - 03:29:57.812, Speaker E: Thank you.
03:29:57.866 - 03:30:01.380, Speaker B: We are running a bit late, but thank you so much for joining.
03:30:02.360 - 03:30:03.590, Speaker E: Glad to be here.
03:30:04.040 - 03:30:08.968, Speaker B: Yeah, thanks. Bye. Next up oh, yeah, go ahead.
03:30:09.134 - 03:30:12.820, Speaker E: I just want to say the Poet Bot will be open for 30 more minutes if you want to claim.
03:30:12.900 - 03:30:13.790, Speaker C: Thank you.
03:30:15.920 - 03:30:34.370, Speaker B: Next up, we have Mara from Coinbase who will be chatting about secret chat validators. The next chapter for Validator resiliency on East Two, which is an exciting effort in collaboration with the Ethereum Foundation that just rolled into the testnet. Mara, welcome.
03:30:36.580 - 03:30:43.412, Speaker H: Hello, everyone. Let me know if this works and if you guys can hear me. All right? Awesome.
03:30:43.546 - 03:30:43.940, Speaker C: Cool.
03:30:44.010 - 03:31:33.540, Speaker H: Let's get it kicked off. Incredibly happy that Superfiz actually preceded this conversation because it sets a lot of the context around what we're actually going to be talking about as part of this presentation. So secret shared Validators, as just discussed, is an effort that's kind of been underway for almost a year now to address certain parts of Validator resiliency improvements for East Two. So without further ado, let's just jump in. Awesome. So you guys have probably heard a little bit about this throughout the summit today, but just like a quick recap on what the duties of a Validator are on ETH Two. So validators, in short, are responsible for participating in consensus activities on the network, so producing blocks and attestations.
03:31:33.540 - 03:32:26.112, Speaker H: In order to participate, each Validator has to put up a 32 E security deposit and much serves as, like, a form of collateral to ensure that participants are incentivized to behave honestly and truthfully in the network. For doing that, Validators receive rewards. The current annual reward rate is around 7.8% for fulfilling those duties, but a failure to perform in line with those duties result in punitive measures. So there's two types superficial just kind of ran through them with you, but penalties are incurred when you're offline. So when your validator is requested to attest or propose a block and you're offline, basically you're missing out on rewards that your validator would have earned if you would have been online. The second mechanism is slashing.
03:32:26.112 - 03:33:33.104, Speaker H: And slashing is really there to disincentivize malicious behavior. So in the later phases of ethu, there's going to be different types of punitive measures. But for today and as part of phase zero, the two major ways in which a validator can get slashed is either through double voting or surround voting. And then in later phases of e two, there will be an introduction of other types of punitive measures around validators, not storing available chart data or not making that available. So that's really to prevent and disincentivize the withholding of information and making sure that validators are behaving as they're supposed to. So there are a few possible scenarios in which a slashing event can be incurred and not all of them are necessarily a malicious attack, even though the network doesn't really make that distinction. So a really good example of that, and we talked about it in the previous talk, is there can be misconfigurations that happen at the topology level in which validators are operated.
03:33:33.104 - 03:34:33.512, Speaker H: And in many cases that actually happens because multiple validators are run on identical instances, the identical key for the validators are operated on multiple instances. So to date, there's been a few major slashing events where these types of issues have actually occurred. And I think the community is in agreement that this is definitely something that needs to be avoided but is not uncommon for early rollouts and infrastructure configurations. So that's definitely one of the areas that we've observed and taken learnings from over the last few months. So just to summarize a little bit around, really what validator duties look like on e two. So in order to really provide a resilient public good for the ethereum Mainet, and also to make sure that your validators are actually performant and rewarded for what they're doing, there's really two categories that are important to keep in mind. So the first one is liveliness.
03:34:33.512 - 03:35:15.800, Speaker H: So making sure that you're online don't be offline. The second one is safety. And safety can really be bucketed into two things. So ensuring you're not producing any slashable offenses and making sure that you don't run multiple instances with the exact same validator key. The other one really is like protect your keys. So your validator keys, they need to be safe, make sure that there's no compromise that can occur to avoid malicious attacks or behavior on your validator. So preventing validator failures, this is really where we get into the nitty gritties, like how do you actually make sure that those failure modes are addressed.
03:35:15.800 - 03:36:23.776, Speaker H: So let's talk about the first one. So liveliness failures, they can occur both at your beacon node or the validator client level, so they can result as misconfigurations or issues with your hardware, the software network level, or if you're leveraging a cloud provider, potential cloud provider outages, or general or localized power outages. So really, the main form of mitigation for this type of validator failure mode is redundancy. So making sure that you're running your node across different instances that have diversification across different components. So validator clients, different cloud providers on prem versus on cloud, and making sure that that redundancy is built into your node topology to prevent against single points of failure for your node operations. So the second one is safety failures. So Byzantine faults and Byzantine faults really occur when operating nodes in a distributed network have a conflicting view on what reality looks like.
03:36:23.776 - 03:37:39.940, Speaker H: And this can happen as a result of a software bug or a network attack that happens at a broader level. So this really can be mitigated through fault tolerant consensus mechanisms where the consensus on what is the truth is achieved across a number of different node instances. So really, Byzantine fault tolerance is a feature that is part of ease Two at the network level, but it's not something that's been implemented necessarily at the individual node level to ensure that there's fault tolerance and some type of consensus mechanism at the validator level to make sure that there is no Byzantine failure that relates to that. And then last but not least, safety failures relating to key compromise. So this happens when your key gets compromised, when it's not safely and securely stored, but in many ways also when your key is in its compromised form and susceptible to attack. So mitigation measures for that, including protecting for key compromise through something that is called threshold signatures. So validator keys can actually be split and a threshold signing mechanism can be introduced to ensure that signatures can be combined to produce a complete signature.
03:37:39.940 - 03:38:59.652, Speaker H: So BLS, signatures on e two are additive which make them really friendly to aggregation and there's different ways in which e two validator keys can be split. So either through a distributed key generation mechanism or Shamya secret sharing amongst a group of network operators with a corresponding threshold, and then making sure that different share signatures and that threshold is required to command a validator. So, if you split an initial validator key into four individual shares, setting a threshold and Byzantine fault tolerant threshold at around three validator signatures required to command the validator, then that would be a good failure mode to protect against keep compromise from that perspective. So really, to sum up what optimally resistant ease to infrastructure looks like, really can be comprised of just three components that are key to that. So to protect against various node failure modes that can exist, the three key things are having and leveraging threshold signatures, ensuring that there is redundancy and a consensus layer to coordinate your validator. And that really introduces secret shared validators. So we kind of ran through it.
03:38:59.652 - 03:39:41.740, Speaker H: Secret shared validators are really comprised of all of these key elements. You can think of it as open source middleware for improving validator configurations. Or in a more simple term, it really acts like a large multi SIG for distributed consensus finding duties on the ethereum blockchain for individual validators. So the first step in an SSV setup is to split an existing key. Again, as mentioned, there's two ways to do that. You can use shamir secret sharing for an existing key, or you can jointly generate a secret key amongst different parties, leveraging distributed key generation schemes. So the second part of this is a coordination mechanism.
03:39:41.740 - 03:40:36.684, Speaker H: So SSV requires a coordination mechanism with a consensus algorithm that is used to coordinate the beacon nodes that utilize the special signatures that are set. So the consensus algorithm that is utilized to achieve fault tolerance, as we mentioned earlier in this example and the way that it's been constructed today is leveraging Istanbul BFT. So this is a deterministic leader based consensus algorithm. So it can tolerate up to one third of the nodes failing in the setup. So really key splitting and IBFT. So the consensus layer that coordinate different SSVs are the foundation of every single SSV node. So first nodes decide on what to sign, and then after that sign the data and then reconstruct in line with the threshold signature scheme what is to be broadcasted.
03:40:36.684 - 03:41:39.236, Speaker H: So what you can see here is full redundancy components across your beacon notes, your validator clients, and then a consensus layer that basically coordinates a threshold signing scheme between these different components of the validator instance. So this doesn't only provide superior safety configurations, but it eliminates single points of failures. And it enables a variety of redundancy capabilities at all different types and layers across the topology of your validator. So what can this really be used for? And really that's kind of the brunt of what I want to focus on for this conversation because SSVs are really a middleware that almost serve as a primitive. They enable a lot of use cases and all of those use cases are really intended to drive better infrastructure resiliency for the entire ECU network. So let's talk about three main components. I'm expecting there to be a variety of applications and we're already seeing some of these develop today.
03:41:39.236 - 03:42:42.840, Speaker H: So let's talk about what this means for infrastructure providers. So, as an infrastructure provider, and this is offering infrastructure services to other stakers, one of the main benefits of an SSV configuration is the fact that you can achieve active, active cluster redundancy across all the subcomponents that your clusters comprise. So you can have different validator clients within the same instance talking to each other through the SSV API. You can deploy them across different cloud providers, different regions, and your validator still acts in unison through that consensus mechanism to ensure that you really have minimal service disruptions and really enhance the resiliency to protect against node failure. So there are also interestingly and potential. Dynamic deployments that can be leveraged through this for this particular use case. So for example, you can imagine a world in which different providers are making up an SSV node and offering those services to the market.
03:42:42.840 - 03:43:55.228, Speaker H: You could also imagine a customer having an in house setup and leveraging a provider as part of the SSV configuration to stake their ETH in a distributed voting power type of manner. So the second one is at home validators that operate their own infrastructure. In many cases, at home validators don't necessarily have access to or the technical capabilities to implement this multilevel redundancy into their existing infrastructure and introducing additional security measures to their existing configuration. So SSV middleware for at home validators releases to support the validator in distributing their signing power to dramatically decrease the risk of failures and downtime penalties. And what I would envision and really hope for is that one day this can really just be deployed through something like Dapnode. It's like a package that helps you simplify the setup and the deployment process if you're an at home validator in a way that you can opt into choosing an infrastructure setup that is significantly more secure and resilient. The last one is staking pools.
03:43:55.228 - 03:45:00.964, Speaker H: So today the most prominent decentralized staking pools are operated through single validator architectures. So what that means is usually what happens is there's a pooling of stakers ETH that basically gets bundled up into 32 ETH chunks and then that 32 ETH gets assigned to a Robin Hood type selected validator within the pool. So again, that validator serves as a potential single source of failure. And in many ways there can exist scenarios in which if different validators inside of the pool are operating with similar infrastructure components. So call that all of them using the Prism client or some of them, and the majority of them being deployed on the same cloud provider, then you really run into the risk of introducing more severe risk measures into the decentralized staking pool. So as a result of leveraging this type of configuration, you introduce fault tolerance, which fault tolerance on the pool level. And single validator pool setups doesn't really exist yet.
03:45:00.964 - 03:45:53.440, Speaker H: So that really significantly improves network uptime and security. And we're already seeing Blocksu is one of the partners on this program with us leveraging this implementation to build trustless staking pools for their stakers. So super excited to already see that part coming into fruition and on to the next one. So give everyone a little bit of context on how we got here. So the research group for this effort was actually formed back in July 2020, which is crazy to think. And throughout the process incredible efforts and help has been provided from the EF on the research front. So John, Crad, Aditya and Carl have been doing tremendous work on pushing forward a lot of the components that today have actually turned into a functioning testnet.
03:45:53.440 - 03:47:11.504, Speaker H: So we've been supported by a grant from the EF back in February and really with the core purpose of bringing something to the community that is an open source SSV middleware solution that can be leveraged and implemented across different use cases that it's applicable to. So we've rolled out the SSV testnet on April 20, so actually just a few days ago, and we have a group of twelve experienced node operators, both at home validators client teams and staking providers, currently testing the implementation. We're very likely to roll out a second testnet to a broader group of participants later in the quarter, and this really serves to ensure that we've conducted sufficient testing before getting the initial implementation out for an audit and then making sure that it gets released to the community for further work. So there's always ways to get involved. We'd love for you to get involved. So you can connect on the discord in the official ETH R D channel. If you scroll down, secret Shredder validators has a little thread, you can also scan the QR code and that's where most of the conversation is happening at the moment.
03:47:11.504 - 03:47:32.410, Speaker H: You can also check out the GitHub loads of information on there, including information around the testnet. So if you want to participate in that as it comes up again, we'd love to have you. And if you want to just ping some questions you want to connect, feel free to hit me up on Twitter just under my name and yeah, would love to have you there.
03:47:35.100 - 03:47:52.110, Speaker B: So much for introducing secret chat. Validators. We have lots of questions for you, so I'm just going to go ahead and relay them to you. First up, during Secret sharing, we need full consensus, and during signature generation, IBFP is used.
03:47:53.440 - 03:48:38.430, Speaker H: So IBFP is used as the consensus layer. So this is more to ensure that there is a threshold signature mechanism at the consensus layer that coordinates the different SSB nodes. So for the current testnet implementation, we're leveraging Shamra's secret sharing. We're not leveraging a DKG, although it's currently in research mode. And in the future we're really trying to add more advanced functions that currently some of the teams that have been supported through DF are working on. So as we look into future phases of the rollouts, we have a team called Platon working on multiparty computation for a proof of custody scheme that could be leveraged for this as well.
03:48:41.840 - 03:48:47.200, Speaker B: Are the parameters for the IBFT the same as the threshold for signing?
03:48:47.700 - 03:48:51.536, Speaker H: Sorry, could you repeat that for the.
03:48:51.558 - 03:48:54.960, Speaker B: IBFT the same as the threshold for signing?
03:48:55.540 - 03:49:20.280, Speaker H: Yeah. So basically the way that the recommended threshold signature scheme would be set up is ensuring that you use a three out of four scheme. It can be changed and altered, for example, if you were optimizing for things that were not necessarily designed to be fault tolerant, if you are, for example, an individual validator, but in the reference implementation, that is the recommended specification.
03:49:23.340 - 03:49:33.420, Speaker B: Then the next question for home validators, I guess. Mara, what are the risk factors if we have set up our validator keys on a separate offline throwaway machine?
03:49:34.480 - 03:49:37.500, Speaker H: If you've set up your validator keys on a separate.
03:49:39.840 - 03:49:44.448, Speaker B: Line throwaway machine so.
03:49:44.614 - 03:50:31.920, Speaker H: I can't answer that in terms of as it relates to the comparison of this configuration. If you're an at home validator and you have adequate validator private key protection mechanisms as came up in the last talk, we haven't observed a massive amount of punitive measures on at home validators really oftentimes that we've observed slashings on Mainet has really been a consequence of misconfigurations on validator pools and staking providers. So I would say just in terms of configurations, obviously you would continue being able to operate that, but with this mechanism, you'd be able to operate a node that just has some degree of fault tolerance on being able to coordinate across redundant components.
03:50:34.740 - 03:50:45.700, Speaker B: Then we have a question connected to our talk that we listened to earlier today. Is there a plan to integrate this with Vouch? I'm not sure if you're familiar with Vouch.
03:50:46.280 - 03:50:54.890, Speaker H: I am not. Probably I should be, but whoever's involved, please let me know. Would love to take a look.
03:50:57.420 - 03:51:03.172, Speaker B: We had this talk just three talks ago. It's an open source cross validator.
03:51:03.316 - 03:51:08.812, Speaker H: It's a test in teams Jim is working on, right? Yeah.
03:51:08.866 - 03:51:09.132, Speaker C: Yeah.
03:51:09.186 - 03:51:11.950, Speaker H: I love Jim. Their team's great.
03:51:15.860 - 03:51:42.276, Speaker B: Okay, are there any more questions that I can relate to, Mara? Let's check the chat. Does not look like it. Keep them coming, guys. We have a few more minutes and if there are no further questions, I'd say let's move on to the next speaker. Thank you so much, Mara.
03:51:42.388 - 03:51:45.240, Speaker H: Thank you, everyone. Happy Friday.
03:51:49.020 - 03:52:23.590, Speaker B: Okay, next up, we would have Terrence from the Prism team, but I think we are a bit early and I'm not sure if he's here already. Oh, yeah, I think I can introduce him already. Looks like we are almost good to go. So yeah. Next up we have Terrence from Prismatic Labs who will talk about how Prism plans to interope with other ease one clients. And I think he's here already.
03:52:25.160 - 03:52:27.030, Speaker C: Hi. Yeah. Can you hear me?
03:52:27.560 - 03:52:29.350, Speaker B: Hey. Yes, I can.
03:52:30.360 - 03:53:04.170, Speaker C: Thank you for having me. It's great to be here. So let me share my screen 1 second. Sorry, it.
03:53:12.670 - 03:53:37.678, Speaker B: So, just to bridge the time a bit, we've already had a couple of talks today that were talking about interoperability and client diversity between east one clients and between east two clients. And now I guess this talk will cover the interoperability actually between the east one clients and Prism and east two clients. So yeah, curious to learn more about that. Terrence.
03:53:37.854 - 03:53:41.282, Speaker C: Yeah, I'm super excited to be here. So can you see my screen?
03:53:41.336 - 03:53:42.100, Speaker B: All right.
03:53:44.890 - 03:53:46.280, Speaker C: Are you able to.
03:53:48.250 - 03:53:49.222, Speaker B: Looks good.
03:53:49.356 - 03:53:55.320, Speaker C: And then you can hear me as well, right? Cool. Can I go?
03:53:56.330 - 03:53:58.198, Speaker B: Yep. The stage is yours.
03:53:58.374 - 03:54:28.340, Speaker C: Okay, thank you so much. So, hi everyone. My name is Terrence. I'm from Prismatic Labs. I'm most known for my work on Prism, which Prism is an E Two client. So in this talk, I will go through more on the client implementation side on what our perspective on Renisson, the hackathon, which protolanda is doing a great job hosting, and the upcoming merge. So, let's get started.
03:54:28.340 - 03:55:21.342, Speaker C: So, agenda. So I will touch base on consensus on execution, most importantly, the separation of concerns here. But I will skim through them rather quickly, since the early presenters already did a really good job on covering them. So if you haven't checked out the previous presentations, I highly recommend you to do so. It's recorded on YouTube. And then for the second half, I will cover more client implementation detail. I will dive deeper into the code changes, some of the design decisions that we made, and my goal is to hope to attract more client implementers into this ecosystem.
03:55:21.342 - 03:56:09.986, Speaker C: Whether it's for Prismatic Labs, Sima Prime, Lighthouse, Taco, and the E One stuff, or the E Two stuff, there's so much new, and the more people we can get, the better. So, I read this article by Burnerby a couple of days ago, and there's this quote that caught my eye. It says that Ethereum today is what E Two developers are calling ETH One, the current proof of work chain that we all know and love. Ethereum Tomorrow. Isn't it one nor E two, it's just Ethereum, right? And I highly recommend you to check this piece out. It was a great read. So from this I would try to reframe myself from saying E One or E Two.
03:56:09.986 - 03:56:59.906, Speaker C: Instead, I would be using the proper terminologies, which is beacon. Chen becomes the consensus layer. It is responsible for the agreement of data, right? And beacon chain will be represented by what today is called the Beacon node. The Beacon node client, their Prism, their lighthouse, their Nibus, their teku. And I'm sure most of you guys have had experience with them, whether it's for staking, whether it's for playing around. And then there's this current ethereum chain, whether it's the execution layer. And then this is responsible for interpretation, it's responsible for transaction of the data.
03:56:59.906 - 03:57:50.610, Speaker C: And this is represented by the pre merged proof of work client. And that's like get and that's in mind and so much more. So let's go through some graphic representations, right? So now we have essentially two chains. And two chains, they're going on the same time, and both of them have consensus, right? Then you have your current Ethereum chain, which operate under proof of work consensus. It has its application network, whether gossiping, application block. And then you have your state tree, it manages receipt, account balances and stuff like that. Then on the other end of the universe, you have your beacon chain, which is operate under proof of stake consensus.
03:57:50.610 - 03:58:56.630, Speaker C: And this beacon chain is governed under validators, not the miners. And the chain consists of what we call management data. It's governed by validators, right? You track validator balances, you track the latest smashing. And so that and the network is gossip via consensus objects how much the validator balances us, what validators are voting, whether the validators are performing poorly or nicely. And then you have your beacon chain state, which tracks the balance, the randomness finality and all the fun stuff, right? So there is a subtle link here that is the deposit contract and because of the bitch and chain itself, cannot come to life. It needs some sort of bootstrap mechanism, right? So it's bootstrap. The stakers on the beach chain is bootstrapped using this deposit contract into the current ethereum chain to deposit 32 E.
03:58:56.630 - 03:59:56.522, Speaker C: So there is this various pinion subtotal link here. So here's more graphical representations. And you can kind of think of beacon chain, the consensus layer, as a wrapper on top of the execution layer, which is the so called current ethereum chain. And so under this merge, the execution layer no longer needs proof of work because it can piggyback on the proof of stake consensus that bitch and chain operates in. Right? So it no longer needs the agreement on data. It needs the agreement on transaction ordering, right? So its focus becomes execution only. And that's like EVM that's data management, that's transaction ordering, that's the gossiping of the transaction data.
03:59:56.522 - 04:00:25.698, Speaker C: Right? So noted. You don't need to transaction typical block anymore. You just need to transaction data. And do realize in this model, I also added the data layer as well. And for the sake of time, I won't cover so much into what the data layer transits. But the data layer is more for data hungry applications for robs and stuff like that. And then it's also connected onto the Beijing chain.
04:00:25.698 - 04:01:23.298, Speaker C: So the Beijing chain essentially becomes this spine chain that governs through everything. So now we have two chains, which means that there's essentially two pieces of software. And the hard question is that how do they communicate with each other, how do they interrupt with each other? What's the communication protocol for that? Right? So so far we have came up with minimal viable protocols that we can provide to get them to interrupt. There's four essentially JSON RPC calls. There are assemble block, new block, Seth, finalized block. I can go over them quickly in detail. So assemble block is when the consensus layer asks the execution layer to help me produce a block.
04:01:23.298 - 04:02:08.342, Speaker C: And then where new block is consensus layer, asking execution layer to process and verify this block and set head is essentially the transcendence layer telling the execution layer that, hey, this is a new head of the chain because you don't need to run for choice anymore. I can run fortress myself. And this is a new head, so just follow it. And finalized block is essentially there's this knife finality property on the proof of stake. Now the execution layer could leverage that property. It helps the execution layer to say, hey, this is finalized block and maybe everything before this finalized block that we no longer care. You can do some nice Pruning and stuff.
04:02:08.342 - 04:02:45.860, Speaker C: So very high level definition. We can go more into detail. And these definitions are also covered in the Merge spec and the random spec that proto put out earlier. So take a look at that as well. So I'm going to take a little turn and start dive more into client implementation details. The client I'm working on is Prison, and it's one of the four implementation. Our client is returning Go.
04:02:45.860 - 04:03:33.662, Speaker C: Returning Go. It has this natural interoperability with the current execution client, which I will go more into detail later. And that's our GitHub page. Give us a follow, give us a star, and yeah, we'll be really happy. So there is some design decisions that we made and I will go through them one by one. So we use protobob and we use gRPC. So you may ask, why protobop? Why gRPC? Right? And we get a lot, right? So we're a huge proponent of protobot gRPC because we love the compactness of the format, the low overhead, and then the strong typing, right? Bytes are bytes, integers are integers.
04:03:33.662 - 04:04:28.978, Speaker C: And we love the ability to essentially generate schema via spec. And the spec is actually really useful for both servers and client. The spec could be forward and backward compatible changes that cover them really nicely. And it has better communication, primitive, right? You have HTP by default. You get bi directional streaming, you get concurrent request over a single connection and all that, right? And we also can support Rest. We can support GraphQL using its folks as well, right? And then the performance is probably slightly better even for the smaller payload and stuff like. So the argument against protobot gRPC here is that Rest does have the more simplicity, human readable format and stuff like that.
04:04:28.978 - 04:05:17.330, Speaker C: The stainlessness part is very nice and it has more features to support existing infrastructure such as like caches, reverse proxy and all that stuff and stuff like that. But the end of the day is that we can transcode gRPC to JSON very easily with a gateway. And that's what we're doing today. So we can support both. And this is purely a client implementation detail. Here's some code examples. I will go through some code examples and then I will let you see what it looks like in the protobot gRPC war, right? So this is the beacon block changes under the Merge and proto show that earlier as well.
04:05:17.330 - 04:05:59.226, Speaker C: This is what's defined it's. Python it's pretty readable. It's very readable actually. Right. You get fields and then you get the ordering and you get how many bytes are in the field. And this is what it looks like for protobuff, right? It is readable as well, right? You get the fields, you get the ordering, you get how many byte size on the SSE for the SSE stuff, right? So this was the execution payload. And that's one of the new field is it's essentially what the ETH one block becomes.
04:05:59.226 - 04:06:43.790, Speaker C: You translate the E one block into the execution payload and then the payload gets appended inside the big trend block body and stuff. So it's the last field, it's field nine. And here is the bigtrend state changes. So the big chain state changes has this new field, it's called latest Execution Payload header. And then the header consists of the current data executions block, hash parent, hash Chong base and all that fun stuff. But it's readable in Python and it's also readable under protobuff. And here we define SSD size.
04:06:43.790 - 04:08:27.740, Speaker C: So we know like when we marshall on Marshall, when you has true root, we can get the consistent data behind it. This is what it looks like right? Now, the next question we have to answer is that how do consensus and execution nodes start? Right? So now there's probably two pieces of software, two pieces of software that we have to consider. So you start your execution node, which is your current ETH one node get categ net and mine and more, right? So you launch a command and the execution node has RPC server and I'm just using get as example right now and I use a five, four, five localhost. And then you start your Vin node which essentially has this proof of work node client that listens to F 545 and then you start your validate client. So there's three steps to it and it's not pretty, right? Don't get me wrong, this kind of sucks to have to type this and launch this three times and stuff like that. So a cool hackathon idea that if anyone wants to work on is essentially translate this into a more unique front end stuff. So whether you can launch it with a script or launch it with a UI and stuff like that, and I think the community could really benefit that.
04:08:27.740 - 04:09:44.098, Speaker C: So the how does how do the interactions look like? Right? So you can think of this as two distinct event, right? So there's two things that could happen. Either you are processing a block or you are producing a block, right? So to process a block, it's very simple. You get your typical bacon block from the P, two P gossip and then it goes to the networking service and then it goes to the stage transition service, right? And at the stage transition service, you break that block apart. Then you get this execution payload which consists of application data. And then you pass that into the if one pro for what client service. And then you pass that to the get your SMI node and then they will verify that and then tell you whether, hey, this is valid data. I can perform Valid execution on top, then they will return the status.
04:09:44.098 - 04:10:26.818, Speaker C: Then you can safely confirm that, hey, my Become blockchain is correct. So that's on the processing side, right? And now there's another event. As a Validator, you get to produce block, right? You get to propose block. And that's how you make your most money. So to propose a block, it's the same path, right? Essentially, you determine you're the proposer of a slot. Then you ask your Get node be like, hey, I'm producing, can you give me some data to produce? And then Get will return execution payload. And then you package that into the block.
04:10:26.818 - 04:11:15.910, Speaker C: You pass it to the Validator client. The Validator client then signs up on the data and then you broadcast to the entire network. So there's two paths, but it's pretty similar into this process, how we process and how we propose. The only difference is that we add this additional step that we have to check with the Execution Client. I was actually surprised that the lines of changes so the amount of changes were not so much. And it's pretty nice so far. If you look at our branch, it's probably like 500 lines of changes.
04:11:15.910 - 04:11:55.760, Speaker C: But keep in mind, this is not production level code. We're doing a proof of concept here. So this is predominantly so the number of lines of code may increase later, right? But with that said, I do think the heavy hitting is on the ETH One client side. They're doing more subtraction by addition. So respect and props to the E One client for doing a lot of those work as well, right? So prison probably has 25 package as of today. And then the only changes are in those package. So probably 20% of the package are gaining changed.
04:11:55.760 - 04:13:00.784, Speaker C: So here's just one of the main addition. Right? After you process the block, you check with this application Execution client on the validity of the data. So essentially, you take the block, you look inside the body, you get the Execution payload, and then you basically pass the translate the payload from our protobuff data to JSON data. And then you insert that into the Execution client. And the Execution Client will tell you whether the data is valid. And if it's valid, then you can process the block. Very simple, right? And then on the other side, to produce a block, you essentially ask the Execution Client to give you the latest payload.
04:13:00.784 - 04:13:25.288, Speaker C: And then you return the payload back to the beacon block. And the beacon block gets signed by the Validator and gets broadcast over the network. So again, it's very simple. So here's a few screenshots that we did have the interruption running earlier. So I just covered a few screen. I put up a few screenshots for fun. They're the same screenshots that Protocol posted earlier.
04:13:25.288 - 04:14:02.480, Speaker C: This is just a prison beacon node. Inserting application execution data into the if one node. And this is a view on the if one node, it's producing block and importing new chain segment. And this is the validator client. Validator client are attesting the block and also proposing the block. Right? Now I'm going to go through a few more slides of design decisions. So we use Basil and we get this a lot.
04:14:02.480 - 04:14:41.392, Speaker C: Why Basil? And Prism is a model repo. So instead of using the default Go build system, our team relies on the Basil build system to manage this mono repose structure. Right? We love it because of its reproducible build. It has a sandbox environment and the builds are pinned with proper dependencies such as Go versions. Right. And this would be guaranteed to have the same outcome despite different developers. Users may have different machines.
04:14:41.392 - 04:15:14.892, Speaker C: They did. Right. So some common issues that I've seen, right, that Bars introduced because of the environment is different than being shipped to users. There's just messy dependency management when dealing with a mono repo with multiple programming language, right? And bezos nice. It also has remote caching and then it has advanced dependency analysis and stuff. So we have bezo go ethereum repo. It's a basil friendly copy of Go Ethereum.
04:15:14.892 - 04:16:03.390, Speaker C: It's nice because we do have Go bindings. We take advantage of using Go. We create Go bindings on top of JSON RPC, so we no longer need JSON RPC cards for the RPC codes that we need to do and stuff like that. So check out the report if you're interested. It's very cool. And here's just an example of when we call assemble block, if we want to produce execution block and we can just literally dial the Http client in this case is gas metamide, and then we just call assemble block. That's it, plain and simple.
04:16:03.390 - 04:16:32.810, Speaker C: We also use this SSV library. It's called fast SSD. So a quick rewind. Right. What is SSD? So SSD is a serialization format that's used in e two, essentially conveys two standards. The first standard is encoding and decoding. So how do we decode e two data such as bacon block and patient state? Right.
04:16:32.810 - 04:17:02.144, Speaker C: We essentially want a string of bytes that can be sent over the network or storing in the database. And that string of bytes have to come to consensus with each other. Then the second one is virtualization. So how do we find the hash of a data? And in this case, we don't say hash, we say the virtual root of the data. Right. So we use this ffest SSZ library. It's essentially generated code, so it's very fast.
04:17:02.144 - 04:17:33.016, Speaker C: And if you're working on this SSC related fields, check out that repo. And it is very interesting. And there's a lot more work can be done there as well. So just to recap on the API, we have our own Ethereum API. That's the protobop spec that we implemented. It has more advanced features in terms of just like bi directional streaming. And it's a little bit more powerful.
04:17:33.016 - 04:18:45.644, Speaker C: Then we have the standard E 20 API which we also comply as well. And then the default endpoint on the beacon chain on the beacon node is 3500. So if you're playing around with it, you're building cool hackathon project with it. Definitely use those. And the next one is Matrix, right? And matrix is super useful in terms of debugging and then monitoring. And then we use Prometheus for scraping matrix, their default 80, 80 on the beacon node, 80 81 on the bell there client. And then we use Grafana to visualize basically for visualization and here's a cool one that our community member build, right? And then maybe a cool hackathon idea for this is that someone could combine the matrix for beefing node and also the execution node, combine them to the same place to visualize performance, to visualize transaction related interesting fields and topics and stuff like that.
04:18:45.644 - 04:19:49.412, Speaker C: And just another idea for our true hackathon project then. Yes, I'm personally very excited to hopefully to have a merge testnet very soon. We're working towards that. I think there's some great value in understanding how the merge will work once we have a testnet with the community and then people can interact with DApps and then you can visualize the network performance and stuff like that, I think that will be very useful and very valuable as well and getting towards the end. So to contribute, feel free to go on our discord or check our GitHub open issues. We have a contribution guide as well and not just for Prism, prismatic Labs, right? Please do check out other E two clients. They're not here today, but they are not here presenting today, but they also are doing amazing work and they could use help as well.
04:19:49.412 - 04:20:06.830, Speaker C: So check out Taku, Lighthouse, minbus and Lost Star. There's definitely tons of work there and they are also amazing. And that's for me. Thank you for having me and shout out to ETH Global for hosting such awesome event.
04:20:09.520 - 04:20:41.396, Speaker B: Great, thanks for all the insights. We have a couple of questions that I want to relay to you. So first of all, while your talk was going on, there was a lively discussion already going on in the chat with regards to execution responsibility. And basically the overall question was like, why not implement the ETH One execution functionality into the beacon node software directly? The person who was asking this, he already got a reply via the chat, but he wants to hear your opinion.
04:20:41.428 - 04:21:38.632, Speaker C: On this as well, right? I think there's two sides for this. So I think there's short to midterm and there's long term. I think to me to short term, having the separation of concerns is nice, giving the timing constraint and stuff like that and you also lower the risk, right? You don't want to redevelop EVM, you don't want to redevelop transaction pool, you don't want to redo those. It takes years of engineering research expertise. Of that, right? And then we want to leverage as much as possible. So having a separation of concerns is nice. So now the question is that how can we make the process better, right? Whether it's launching a unified front end to essentially to kind of make people think that we're actually not running multiple process, but we are running multiple process.
04:21:38.632 - 04:21:45.390, Speaker C: So I think that's a really good hackathon each project to think of and I think people should pursue that.
04:21:48.080 - 04:21:55.920, Speaker B: Then another question right here. Will the public testnet also interact with other non prism clients and any time frame for the testnet?
04:21:56.420 - 04:22:30.892, Speaker C: Yeah, so the goal for the testnet is to be multi client ish to start with. So there will be lighthouse. They're doing great work technical, of course, they're the first one that's doing it and inbuzz as well. So there will definitely be multi client talking with multiple institution clients as well, right. So, timeline, I think this week we'll be making a lot more progress. To be honest, we have probably made so much progress the past week than we have for the whole month. So I think we're close.
04:22:30.892 - 04:22:45.950, Speaker C: I think next week and early next week, with the leadership of proto and stuff, we should be able to launch semi Deafnet. And once that goes well, we can go full public.
04:22:48.180 - 04:22:54.130, Speaker B: And then lastly, a rather technical question. Why are you using Basil instead of pure go?
04:22:54.820 - 04:23:23.710, Speaker C: Oh, yeah, I thought I covered that. So, long story short, we use Basil just because of the reproducible build, because of the mono repo ish setup that we have and also because of the remote and then the local caching and stuff like that. And I get that, right bezo there's a little bit of learning curve and stuff like that, but once you get used to it's, not too bad. It is super fun.
04:23:26.240 - 04:23:55.110, Speaker B: I think that's it in terms of question, thank you so much for joining and sharing all those insights with us. And with that, I am also handing back to Kartik, who will moderate the last few sessions of this summit today. Thanks so much for having me and I wish you all a lot of fun with the last two remaining sessions, I believe.
04:23:55.640 - 04:24:29.360, Speaker A: Well, thanks Terrence, and thank you so much, Franzi. It's been so wonderful having you co host this for the last few talks. And I want to thank you for not only just facilitating all the questions, but also being an excellent host. So hope to see you again in the future. And looks like I'll see you on the live chat on Ethcopal TV. So thanks again and hopefully everybody found that talk insightful. I'm seeing a lot of comments from a lot of the attendees on just everything that Terrence said and kind of how everything is on the same page.
04:24:29.360 - 04:24:53.512, Speaker A: So two more talks to go and hopefully we'll have a lot more excitement coming for these two talks. Looks like we have our new speakers on deck. So I'm super excited to welcome our second, last talk of the day. Presenting. This will be Tomas and Alex. Tomas is from Nethermind and Alex is from Flashpots. And they'll be talking about what the world looks like with Mev after the merge happens.
04:24:53.512 - 04:25:00.468, Speaker A: And we'll be presenting other minds in flashpot. So welcome Tim house and Alex. Thank you. Karti.
04:25:00.564 - 04:25:01.210, Speaker D: Hello.
04:25:01.980 - 04:25:03.930, Speaker C: Hey, Kartik, thank you for having us.
04:25:04.300 - 04:25:05.930, Speaker A: Excited to have you all.
04:25:12.660 - 04:25:13.410, Speaker C: It.
04:25:23.460 - 04:25:45.690, Speaker A: Never works as dastard. While Tomas is loading the slides, it would be awesome to kind of get an introduction from you each and maybe then jump into the talk.
04:25:47.580 - 04:26:16.332, Speaker C: Sure. I'll start while Tomas is sharing. So I'm, Alex from Flashbots, have been looking into Flashbots after the merge and Mev after the merge. So, Flashbolts, we focus on Mev, the negative externalities that come from the current extraction techniques related to it, and generally the way Mev is shaping this technology. Yeah, I guess we'll share more in the presentation.
04:26:16.396 - 04:26:52.700, Speaker A: Thomas, hi. Yeah, I've been working on the Nethermind Ethereum client for the last few years and yeah, today we're going to present mev after the merge with Nethermind and flashboards. So, welcome everyone. Hi. And what we'll talk about? So, first of all, we'll talk about the merge itself, so the minimal spec for combining Ethereum One and Ethereum Two nodes. Then we'll talk about the Ryanism, which is this event within the event. It's all the if one, if two core devs getting together to prepare the DevNet of the merge.
04:26:52.700 - 04:27:02.812, Speaker A: Then we'll talk about introduction to Mev, so minor extractable value. And we'll show how. Flashbots, enable mev in ethereum.
04:27:02.876 - 04:27:03.490, Speaker C: Now.
04:27:04.100 - 04:27:57.110, Speaker A: Then we'll ask a question whether Mev is possible after the merge. And then there'll be lots of numbers, alpha leaks, so make sure that you stay till the end. So maybe what is Ryan easement and why we've been involved in all of this. So, between April 16 and May 14, as part of the global scaling hackathon, we see the Ethereum One, Ethereum Two core developers building together to deliver the minimum specification of something that in the future will be simply called Ethereum. And Ethereum One will be the execution engine there. And Ethereum Two, as it is now, will be consensus layer node. And what you see here on the screen is the example of the Taku and Nethermind working together as part of the first work that was done on the first minimal spec.
04:27:57.110 - 04:28:44.784, Speaker A: So the merge delivers something most basic. So it delivers the clients that operate isolated and only connected through a simple RPC interface. And so far we already see implementations of Dotspec in Ethereum Two clients in Lighthouse and in TECO. And soon we should see the other clients following. And for the Ethereum One execution engine, that's for the catalyst, which is GEF for the merge and Netamind. And again the other clients should follow promptly. So both of the clients here operate with their own peer to peer networks so there is no interaction of the networks themselves.
04:28:44.784 - 04:29:26.104, Speaker A: So they discover their peers, they do all the gossiping. However, some of those operations on Ethereum One Node are disabled. So Ethereum One Node doesn't gossip anymore any blocks, but it does listen to transactions and it will do some synchronizations. And all the block gossiping is moved to Ethereum Two clients. And the spec, the minimum spec of this JSON RPC consists of four methods really. The requirement to assemble block and that comes from the Become chain, from the validator to the Ethereum One Node. So the request comes from Ethereum Two, as it is now.
04:29:26.104 - 04:30:10.364, Speaker A: But the actual block construction is on Ethereum One and it's super important for mev, as we'll see in a moment. The new block is more like information from the beacon chain network that the new block was created so the Ethereum One Node can prepare the state for the construction for the block proposals. Set head defines what the beacon chain decided that the current head of the chain is and finalized block allows to do some cleaning and pruning when the block becomes finalized on Ethereum Two on the consensus layer. So that helps. Ethereum one node to keep everything clean. And as you see, it's a 0.1.2 spec version, very early version and we can expect lots of changes.
04:30:10.364 - 04:30:49.956, Speaker A: The net amount implementation is available. It was reasonably big effort, but at the same time not so big that it would be a massive thing to deliver. And it's a good start. And all the teams are now focused on this. So this is already merged in netamind on the main branch. And you can test all of this by going to protolambda's mergenet tutorial repository and you can launch your own local DevNet setup for the Ethereum Two and Ethereum One clients working together. So you can pick GAF, Catalyst or Netamind.
04:30:49.956 - 04:31:42.616, Speaker A: And you can pick also I think Lighthouse or Taco here. And something really fresh from today, from the last all core devs meeting, we see that this timeline for the merge and for the network upgrades on Ethereum One this runs in parallel. So we'll see the London happening in July, then Shanghai, which will be a set of additional EIPS happening in October 2021 and Cancun following somewhere later, probably 2022. And in parallel the merge happens. So that work that already started should be coming in October. So it's pretty soon you can expect the proof of stake transition potentially even October this year. Let's move to the mev explanation.
04:31:42.616 - 04:31:45.756, Speaker A: So I think Alex will take over from here.
04:31:45.858 - 04:32:36.060, Speaker C: Yes, thank you so much. So, because this talk is about mev and E Two and at least mev after the merge, it's important to maybe first understand mev. It's definitely something that has been more prominent in the public debate in the last few months and it's something at flashbox that we've tried to educate the community on or at least share more resources about. So, I mean, at a high level, I've put a definition here, which is that mev is a measure representing the total value that can be extracted permissionlessly from the reordering inclusion or censoring of transactions. So the little image that you have under is on Ethereum, you have different transaction senders. They submit their transactions and before transactions are included in a block, they're part of the mempool. Miners pick transactions from the mempool and order them in a block.
04:32:36.060 - 04:33:19.480, Speaker C: And they have full control over that ordering. So they usually do that ordering by gas price. But in reality, there might be more optimal ordering for them to extract more value out of this set of transactions or to insert their own transactions or censor transactions. Basically, they have full control over the content of this block in terms of the ordering of transactions. So that's why the term minor extractable value was first coined by Phil Dyne in his paper. flashboards 2.0 was to represent this upper bound on the maximum value that can be extracted by the entity that has the most permissionless power over the ordering of a block.
04:33:19.480 - 04:34:31.548, Speaker C: Next slide, please. And so Phil, in a later presentation, Phil Dyne again set out a simple parametric definition of mev that can give you even a better idea of why you can think of it as an upper bound. And so, as is written here, mev is for a miner, is how much ETH a special miner account can extract given a set of user transactions t in a mempool initial state s and a set of contracts t that the transaction state will interact with. And the constraints on that is by inserting their own transactions. So inserting their own transaction in the set s, censoring any element in the set t sorry, censoring any element of T or reordering t in any way that optimizes their ETH account balance. So that's the idea for the upper bound rod. But in reality, there's mev that's extracted on Ethereum already and it is not extracted by this entity that has the most power of transaction ordering.
04:34:31.548 - 04:35:47.364, Speaker C: So the miners can you click actually again? Yeah, there you go. And so we've tried to quantify this at Flashbots and we've released some of our work on this. And there are other teams that have done so as well at Imperial and University and other groups. But so what is essentially happening is today it isn't miners that are extracting value themselves, but it is users that are expressing preference on the ordering of their transactions and are sharing some of the value that they reap from having that preference met with the miners in the form of transaction fees. So again, as I mentioned, we've tried to quantify this to give some number for this and we've put a lot of our results in a live dashboard mev explore that, quantifies what this amount represents not only in dollar terms but also in ETH terms and kind of breaks that down and breaks that activity down further. So I realize this is all still a bit conceptual, right? So I put an example here of a transaction where there's mev and I think I really want to nail this concept of transaction ordering. So for example, this transaction is an arbitrage transaction between different liquidity pools.
04:35:47.364 - 04:36:47.950, Speaker C: So you can see balancer shuc swap and Sushi swap going between each pools. And you can see the sender started with 16 E and ended with 32 E, making there's 16 E in that process. But you can also see the transaction fee in there of twelve E. And so to have their transaction included, they paid an extremely high transaction fee, much higher than the average transaction fee at the time, or an implied gas price of 51,000 guay. And we expect that they did so because they engaged in a priority gas auction, another term that was introduced in Phil Dyne's paper where they iteratively bid up their gas price against other competitors that also wanted to seize the same transaction. And so we have an example in the same block and so in the next slide of someone who seemingly went for the same opportunity but didn't manage to seize it. And you can see they still paid 1.1
04:36:47.950 - 04:38:07.012, Speaker C: Ether in transaction fee and still paid 27,000 GWe. So still twice as less as the winner, but still extremely much higher than the average gas price at the time, which was 94 GWe. And so this shows the power of transaction ordering, right? Because you're going to have a single winner that's going to win the opportunity and then other people going for the same opportunity that are not going to be able to seize it. And this opportunity itself has value associated to it and ultimately a miner, given that they have the most power related to a transaction ordering, could put themselves in front of all these transactions. And so this is why this is a subset or included in the broader set of extractable value from an entity that has the most permissionless power of a transaction ordering. This is what we're working on at Flashbolts in the sense where, as you can see, this extremely high gas prices actually influence the whole Ethereum network because it pushes gas prices up for everyone else and that's problematic. And generally we are working on reducing the extraction costs and this takes many dimensions which is beyond the scope of this talk, but kind of gives you an intuition for what mev is.
04:38:07.012 - 04:38:17.400, Speaker C: And Tomas is now going to talk about what Flash vaults is and also how mev might work after the merge.
04:38:20.400 - 04:39:25.570, Speaker A: So if you think how it was before miners could potentially extract the mev by themselves or maybe through some private arrangements, what flashboards delivers is a more democratized way of doing it, more decentralized way of doing it so everybody can become a searcher. And you have assumption that there's lots and lots of searchers looking for those arbitrage opportunities. You can call them Arbitrage or Sequesters or depending on the strategies that they follow, they may follow all of those, depending on the circumstances. And Flashbots deliver an Mev relay which is sitting there between the searchers and mining pools and provides the searchers with the direct lines to practically at the moment 58% of the hash rate that is available on Ethereum. So you really have a way of ensuring that you can sense the set of transactions and they will be included very soon in one of the blocks. Like there is a big chance that these mining pools were included and.
04:39:28.740 - 04:39:29.104, Speaker G: Just.
04:39:29.142 - 04:40:46.600, Speaker A: One moment, okay, so what are the types of bundles that the searchers discover and send? So they may find a strategy to some kind of liquidations or sandwiching or white hat transactions that can be delivered directly to the miners or toast transactions. All of them always include the payment to the miner. And that payment to the miner does not necessarily go through the transaction fees, but by discovering the coinbase value, you know the address of the miner that you're paying to. So you can make a payment through the smart contract or directly by including the transaction to that miner. And all of those are used by the miners to assign scores to various bundles and pick the one that is providing the miner the biggest profit. So it creates this separate fast line for execution in front of all the other transactions from the Mempool and causes all the searchers to only compete for the T paid to the miner between themselves and not causing the entire network to compete with them. And potentially it has this opportunity of decreasing the guest prices for the entire network.
04:40:46.600 - 04:42:26.264, Speaker A: And all the blocks that will be created by Flashbots participating mining pools will have those bundles in front, in front of all the vanilla transactions from the Mempool and might be Zero One or a bit more bundles if the bundle merging is included. And the big question appears whether it's still possible in Serum Two after the merge. So what changes really based on what we described in the merge minimum spec and how we plan that the Ethereum One will go forward. So, if you think about the current flow where you have a searcher sending bundles to relay, then the relay sending that to the miners or mining pools, and then those blocks being published on the chain, so published to the network and then included on the chain if they win the proof of work. So what happens now is we practically keep Ethereum One Nodes being responsible for transaction sequencing and they can still expose those endpoints to allow searchers to contact them and suggest reordering transactions deliver the private bundles. It's just that the searchers are still sending that to relay and then it's the Validator that receives the information about the bundles and the Validator connects with the Ethereum One Node. And when asking for this block assembly, it can ask it to create that block with specific bundles included, so that bundle delivery between Validator and Ethereum One node happens.
04:42:26.264 - 04:43:23.070, Speaker A: And then again, the Beacon Chain is used for block proposals and attestation. So that block is being propagated with the MEB included. So this really still exists in Ethereum Two. And when you think about this value that exists at the moment on Ethereum One, we see that on average for the blocks that include mev bundles, the miners will make additional zero point 18 if for every block which includes a bundle. And those numbers may change because it's still based mostly on the early versions of the Flashpots. So we can expect that this number will be much bigger, even like anecdotally twice as big. And now I think Alex will continue with more numbers and how these numbers actually operate within the context of Ethereum Two.
04:43:24.320 - 04:44:18.172, Speaker C: Thank you, Thomas. Indeed. So why we've chosen that train of thought in our presentation is because ultimately the interesting bit is, should Mev exist in E two as it exists in ETH One, how will that impact the yields that Validators hope to make because they are the ultimate block proposals after the merge, as Tomas has mentioned. And this is something that people have been wondering about recently, as you can see. On the right, like Cyrus was talking about, something very similar in .3. And on the left, Justin was thinking about the impact on Validator yields, not from Mev specifically, but from transaction fees, which we expect Validators to make on top of the protocol mandated yields for good behavior. And so these are important questions.
04:44:18.172 - 04:45:34.048, Speaker C: And so we thought let's run the numbers. Yes, running the numbers. And as Tomaz mentioned earlier, we have numbers on Mev not only from our quantification that I mentioned earlier, the Dashboard Mev Explorer, but also from Flashbolts, from the activity on Flashbolts itself. And because we have significant amount of the Ethereum network running Flashbolts and also significant amount of traders or searchers on the other side submitting transactions to Flashbolts and actual value going through, we can use this as a proxy or a low bound for the value that validators can make from mev at least from running Mev enabled client like Flashbolts mev GEF. And so the core assumptions we're making in the numbers is, as Thomas mentioned, zero point 80 per flashbolts block as well as the probability of blocks that are flashbaults blocks, which is 52% from our data. Again. And these numbers, I mean, we're kind of going through this model using Pintel's amazing article on Beacon Chain Validator rewards and the modeling that they have done there and the Python code that they shared.
04:45:34.048 - 04:46:24.564, Speaker C: So we've modified their Python code to actually reason about Mev enabled Validator rewards versus the nonmev validator rewards. And there are some limitations to our approach that are mentioned here. First, we're not accounting for transaction fees like was mentioned in Justin's Tweet before Flashbolts data. However, the activity that's on Flashbots is still a few months old. And we expect as adoption grows and also as people become more sophisticated, this number will change. And ultimately, what Tomas mentioned, some of the current infrastructure limits the amount of ETH per flashbox block, and this is set to change very shortly. And so again, take these numbers with a grain of salt and it's more to get an intuition for this and also thanks to the people that have contributed to running these numbers.
04:46:24.564 - 04:47:26.440, Speaker C: So Tarush, Vimulapali and also Terence and Ruhl from Prismatic labs. All right, so now getting into numbers, we first start with the ideal case. And the ideal case here is a case where all validators behave perfectly on E Two or after the merge. By this we mean there's 100% uptime and 100% participation. And this is also the ideal case because we look at a long enough time range where we don't have to necessarily think between the Beacon block proposal reward and the general validator reward. And we can mix both together because on a long enough time range, each validator can expect to as many blocks on average, right? And so, as you can see on this graph, the blue line is the ideal validator yield reward without Mev. And this reward decreases with the number of validators, of course.
04:47:26.440 - 04:48:19.332, Speaker C: And then the orange line is the same reward with Mev. And the blue dotted line that you see is the current number of validators we have, which is about 120,000 validators for an equivalent of about 4 million E staked. And so at the current amount, as is written below, at the current amount of validators, 120,000 mev boosts validator yields by 17.9%. And so that's an implied Apr of 14.5% instead of 8.49. So that's really significant, especially given that when we compare the impact of Mev on minor revenue in the current system, the ETH One system, with Flashbaults, we're looking at something of a 5% to 7% increase in revenue. Whereas here we're talking about something a lot more meaningful.
04:48:19.332 - 04:49:33.584, Speaker C: And that has implications, not only implications that are mentioned by Justin in terms of the security of ethereum, but other implications that we'll mention briefly after another case that we looked at, again, this is all following. Pintel's approach is a perfect validator on an imperfect network. So in that case, we're looking at a validator who behaves perfectly, but the other validators in the network might not behave perfectly. And this is modeled by the participation rate of the overall network. So that's the P that you see here and the way rewards and penalties are set up on E Two or sorry, after the merge is that even if you as a validator behave perfectly, you are influenced by the behavior of other validators. And so Pintel computed by how much your rewards fall if you're a perfect validator and imperfect network and you can see these numbers on the left, and then if you look at these numbers now with Mev, you can see these numbers falling as well. And so the kind of takeaway from this is with mev, a validator yield falls less quickly than without.
04:49:33.584 - 04:50:05.192, Speaker C: It given different levels of participation for the network and given, again, perfect validator. Again, another interesting takeaway from it. And the last kind of case that we modeled, again based on Pintail's own modeling, is an imperfect validator on a perfect network. So we're kind of switching things around. In that case, the validator is not doing the most amazing job, but the whole network is. And this is modeled between these two graphs. And so on the left you have Pintails.
04:50:05.192 - 04:51:01.240, Speaker C: And so what Pintail found is that for a validator to receive a net positive reward, the minimum uptime that's required of them is about 43%. And that's what's charted here in the kind of meeting of all these lines for different number of validators. Now, if we add mev to that, the actual uptime required, so it's slightly different by the number of validators, but it's about 27%. So what's required of validators in terms of uptime has decreased, thanks to mev. Again, whether that's good or bad is something that we need to think about more deeply, but is definitely important. Let me delete that as well. All right, so the numbers, I guess a quick summary is mev boost validator yield significantly.
04:51:01.240 - 04:52:01.308, Speaker C: And so as Justin says in the tweet that I mentioned earlier, more yield means more validators, means more security. So that's probably a good thing. However, given the difference between minor revenue that's enabled by flashball so this 5% increase in revenue versus here something that's much higher intuitively, even though we have to run the numbers even better, it's even more important for mev to be extracted in a democratic fashion. Because, as is mentioned in Phil's post, this is what mev what do stands for. It will create a stronger divide between the validators who extract mev and the ones who don't. And that's very important. Similarly, as we saw in the other numbers, mev decreases the impact of our validators uptime on a validator awards, and mev decreases the lower bound on uptime necessary for validator to make a net positive reward.
04:52:01.308 - 04:52:47.024, Speaker C: The second bullet point is something that we took out. So ignore that for now. And in terms of next steps for all this modeling, we definitely want to model the inflation that's caused by the base fee burn in the IP 1559, as well as the transaction fees validators can expect in a post EIP 1559 world. And we want to include them in those results because the impact of Mev is currently overstated, because fees will also play a part in validator yield. And that's what justin showed. Similarly, we want to chart mev enabled validator yield at different levels of participation and uptime. So imperfect validator and an imperfect network, which is something that Pintail charted and that we want to do more work on.
04:52:47.024 - 04:53:20.292, Speaker C: And I guess we want to release all this work shortly and the code behind these numbers as well, so that other people can play with them. So these are kind of next steps related to that. So this gives maybe a little bit more intuition around the numbers. But there are other aspects to mev and E Two that are important to consider. The first one actually, Tomas, a few. Thank you. The first one is that there are new actors that emerge in E Two.
04:53:20.292 - 04:54:23.996, Speaker C: And this is important to keep in mind when you think about mev and the landscape around it. One of these actors are these kind of metapools, in a way. So if you compare them to Ethereum One, you have miners and mining pools. And this is kind of the dynamic that stops there. Here you have Validators, validator pools and aggregators of validator pools that offer services on top, such as Lido, that offers staking derivatives where they connect to several validator pools that you can see below, right? Staking Facilities chorus surgis One And this is interesting because it will shape the mev landscape via soft power, because Lido will have a lot of staking volume involved and they'll be able to have a say in how mev is extracted and also the profit sharing between validator pools and ultimately validators themselves. So I think it's an important thing to keep in mind. The other important thing to keep in mind when you kind of start building a mental model around me, VNS too, is the new players that emerge, specifically exchanges.
04:54:23.996 - 04:55:00.010, Speaker C: Right? And so if you look at the Ethan deposit addresses, this is taken from Beacon Chain, you can see that Kraken binance are very large chunks of validator power. And this is interesting because exchanges are not solely in the validator business and they also have other regulatory requirements that might influence the type of mev activity they take part in. And this is important to keep in mind. That's pretty much it. So, yeah, looking ahead, Thomas, I'll give it back to you.
04:55:00.380 - 04:55:51.764, Speaker A: Yes. Soon you'll be able to also play with this mev construction within our deployment of Ryanism. So with that netamind packages that you're downloading, I think mid next week, ukash is delivering now for netamind the mev bundles within this Ryanism exercise and coming back to how the environment will look after the mev in Ethereum One, Ethereum Two. So I think that there might be some quite interesting questions. Does the market appear because the big validator pools will be actually able to run their own mev operations and maybe start offering to the pool participants information about what fees they charge for this quality of the mev and what their returns.
04:55:51.812 - 04:55:51.988, Speaker E: Are.
04:55:52.014 - 04:56:48.044, Speaker A: So it's almost like you'd be comparing that like some funds or pension funds, exchange traded funds, whatever. You'll be looking at those numbers is what quality of the financial activity this extraction of value happens on the pool that you join. On the other hand, it's like Ethereum one node operators may start acting as this Mev managers and say that they are providing those quality solutions to maybe smaller validator pools or some independent validators here. That's a good question to ask. It might be less likely because the independent validators may not follow the privacy requirements for the bundles that are included in the Mev blocks. And this second thing may potentially lead to independent solar validators to be disadvantaged because of not being able to capture the Mev. I think it still remains a bit vague of how exactly those markets will look.
04:56:48.044 - 04:56:53.340, Speaker A: So great set of open questions to see how that market evolves.
04:56:55.280 - 04:56:55.800, Speaker C: And a.
04:56:55.810 - 04:57:16.470, Speaker A: Few others that we'll probably won't really discuss as we're running out of time. But we just want to leave them open here and leave them for you to follow, to follow the links, explore the opportunities. I think that's what we've prepared for today, right? There's no other slides even. Are we perfectly time?
04:57:18.600 - 04:57:37.352, Speaker C: We're on time. Yeah. I guess there's just the last slide to mention. There you go. There's definitely an open conversation that we're trying to have. And as I mentioned, these are early numbers and we want to talk more about this. So Flashbots has regular community calls.
04:57:37.352 - 04:57:56.736, Speaker C: We call mev Roast and we're going to have one soon to talk about ETH two and layer two. And we also have a research arm to what we're doing and we definitely want to dive deeper into many of these open questions. And so join us and then on the nether mind front, yeah, make sure.
04:57:56.758 - 04:58:17.240, Speaker A: That you play with the DevNet. Make sure that my core developers feel the chase of the users. So thank you so much everyone. Thank you for invitation to Global. Thank you cardi. Alex, thank you so much. I know we are right on time, so I'll just maybe do one question.
04:58:17.240 - 04:59:18.380, Speaker A: There's a lot of chat on the timelines which I guess I won't go into right now on shipping this. But the one question I want to ask here is it's for both of you? And just the question is right now you have to send your Mev transaction to three pools that are available. With more valid data coming on, do you think it'll be harder to get into the next auditor as a transaction or what does that world look like? I think that the way the Flashbots is organized is the Flashbots deliver those solutions to be able to connect the searchers with the miners. And in the future you'll have Flashbots or other operators delivering the same kind of matching of the validators with the searchers. So this way you won't have to discover those validators that will be having the majority of the network. You'll be able to get to that pools. And also, I think, as Alex was saying, there'll be some of this kind of meta pooling which will also route the bundles to the validators.
04:59:18.380 - 04:59:20.430, Speaker A: Alex, do you want to add anything?
04:59:21.280 - 05:00:10.430, Speaker C: No, that sounds good. I mean, I will say this is something we want to explore more. I think there's like mechanics, there the economics of it that are still things that we have a good idea on and theoretically it makes sense. But as we've seen with what we've been building so far, there's many points where that breaks and we have to see how things change. And so partly what Tomas mentioned, right, like at Ryanism to kind of maybe experiment with that with the dev testnet, could be for us a good way to understand that better. And also, as I mentioned before, it's not only the technical factors, but also maybe softer factors that are going to exert pressure into how this is going to work and how revenue might be split. And so these are all kind of nested questions.
05:00:11.120 - 05:00:26.880, Speaker A: Absolutely. Well, thank you so much for sharing your insights and thanks again for giving this amazing talk. We have a lot of excitement that everybody's thanking you on our chat. So hope to see you there. Thank you so much. Thank you for having us. With that, we are ready for our final talk of the day.
05:00:26.880 - 05:01:11.612, Speaker A: I am super excited to be welcoming our next speaker. And in anticipation of this last talk, we've seen a really big surge in the number of live audience viewers. So to anybody who's joining in, just want to remind everybody that this is a live chat and a live conversation. So if you have any questions for our next speaker, you can ask them out in the chat and we'll relate them to Vitalik after the talk finishes. So if you're tuning in right now, be sure to log in, say hi, and ask any questions and we'll be able to relay those questions directly to our speakers. So that said, I am super excited to welcome our next speaker. The next speaker is Vitalik, Buterin Vitalik is going to be talking about what happens after the merge.
05:01:11.612 - 05:01:18.728, Speaker A: We've been on a long road from e one to e two, but let's talk about what's next. Welcome, Vitalik.
05:01:18.924 - 05:01:21.716, Speaker I: Thank you, Cardik. Are you hearing me?
05:01:21.738 - 05:01:22.356, Speaker H: Okay?
05:01:22.538 - 05:01:23.876, Speaker A: Everything is great.
05:01:24.058 - 05:01:53.580, Speaker I: Okay, great. Then I am going to just turn on my slides and we'll get straight into talking about fun stuff. Okay. Share. Okay, what's the second word happens? Okay. And what's the second word now we've excellent. Okay, thank you very much for doing the check verdict.
05:01:53.580 - 05:02:58.032, Speaker I: All right, so I guess I'll just start talking. So far we've had a lot of excellent presentations about the process of the merge, about all sorts of things kind of having to do with the merge around the merge right now, I wanted to kind of finish off by going a bit into the far future. So where is Ethereum going from a technical perspective after the merge? So this is a kind of quick diagram of the roadmap that we know so far, right? So we have the execution layer. Currently it's a proof of work chain but the proof of work part is going to be removed at some point fairly soon. And then we also have the proof of stake inside the Slayer, which also exists. As of December, the Berlin hard fork already happened. The genesis of the proof of stake side already happened.
05:02:58.032 - 05:03:31.224, Speaker I: So next up we have the Altar hard fork on the proof of stake side and the London Hard fork on the execution side. That's going to introduce our beloved 1559. And then after that there's a possibility of a feature fork, maybe a bit with like one EIP either before the merge or after the merge. The Core devs calls are still discussing this but much smaller than say, 1559, for example. But the next big thing is just the merge itself, right? And the merge is this kind of big topic. Then there is the question of well, what happens after the merge?
05:03:31.272 - 05:03:31.436, Speaker C: Right?
05:03:31.458 - 05:05:18.064, Speaker I: So after the merge, basically the execution layer is no longer an independent chain. The execution layer is a thing that lives inside of the chain and the chain is run by the proof of stake consensus layer. So once that's done and proof of work is behind us, where do we go from an Ethereum consensus layer point of view? From there, the first big thing that we'll need to have is a post merge cleanup fork, right? So basically the idea here is that we've rearranged the roadmap so that the merge is done in this very minimalistic and simple way where you basically just kind of take the existing proof of work chain, strip out the proof of work and then just make it live as a chain inside a chain inside of the proof of stake chain. But it's like a very no frills merge, right? So like for example, the ETH One voting data mechanism continues to exist even though the chain is basically just voting on the chain, which is completely stupid because, well, the chain knows the history of the chain, right? Withdrawals are also not enabled. There's also this kind of awkward mix of RLP and SSD. There's a lot of kind of format issues there's theoretically the ability to read the execution chain but you don't have any opcodes that can actually take advantage of that. So the post merge cleanup fork is this fork that's expected to happen very soon after the merge that just fixes all of those issues, right? So some kind of top priority items that could happen around here.
05:05:18.064 - 05:06:02.200, Speaker I: One of them is to remove the ETH One data voting mechanism. So the ETH One data voting mechanism is how currently the beacon chain is aware of the proof of work chain and it has to be aware of the proof of work chain to process deposits. But we're not going to need this because the executional layer will just be able to read the consensus layer that is living inside or the consensus layer will be able to read the executional layer that's living inside it directly. Right? So that simplifies the protocol a bit and it removes an honest majority assumption, which is good. Adding withdrawals. So withdrawals will not be available immediately after the merge, but they will be available after the cleanup fork. And that's just like a simple mechanism.
05:06:02.200 - 05:06:18.260, Speaker I: Just need to figure out exactly the way to process withdrawals. But not very technically hard. Potentially fully moving. Actually. This should not be the application chain. That's outdated terminology. It's not politically correct.
05:06:18.260 - 05:06:51.720, Speaker I: As of April. Sorry, guys. Fully moved the execution chain to SSZ. So RLP and SSZ are these serialization formats. The existing Ethereum execution chain runs on RLP, the beacon chain runs on SSD. And SSZ is like in my opinion, as the inventor of RLP, RLP is kind of trash and SSZ is better, so it's just more convenient. It has these really nice properties in that you can prove merkel paths going into SSD objects that are inside of SSD objects very easily.
05:06:51.720 - 05:08:21.112, Speaker I: It doesn't require any kind of weird dynamic positioning for most things. So it has these nice properties. And the way that the merge will work is basically that the execution block that is embedded inside of a beacon block is going to have transactions and those transactions will just be blobs, right? And they'll still be RLP. Now you can't change them immediately because the format of the transaction really matters because the thing that is signed in the transaction signature is a serialization of the transaction. And so if you change the serialization, unless you add some awkward backwards compatibility layer that sort of reformats the transaction when you're checking the signature, you just want to have a new transaction format and that's something that needs to be added later. Right? And this would be a good opportunity to kind of clean up transaction formats a bit, add something that supports access lists, supports the chain ID in a very kind of nice and native way, supports EIP 2929 stuff, potentially supports account abstraction related stuff, just supports everything that we know transaction formats need to contain. And eventually all of the previous transaction formats can be retired beacon State access opcodes.
05:08:21.112 - 05:09:17.900, Speaker I: So opcodes to access the state of the beacon chain is also something that's important. So they're basically things like the Randow opcode or potentially remapping an existing opcode to Randao so you can access the Randall value inside of the chain and that can be used by applications for on chain randomness. Also the beacon block root opcode. So a block hash opcode that actually points to a beacon block. And this is nice because it allows the chain to be aware or it allows things in the execution layer and applications to be aware of what's happening in the consensus layer. It also allows them to be aware of history. You can prove history more easily and it's important for when we add sharding because you have to prove that some shard data blob actually got included.
05:09:17.900 - 05:10:53.964, Speaker I: And then the last thing is this kind of somewhat controversial thing of agree that we need to agree that at some point, hopefully clients just stop trying to download the pre merge proof of work chain, right? So at some point I think it's a good idea to break the invariance that the base ethereum protocol is responsible for trying to or for giving you the entire history all the way back to Genesis. And the first simple step to that is to just at some point agree that we stop kind of providing the proof of work chain before the merge as part of the protocol. And if you want that stuff, then you can go to the graph or you can go to some other protocol that someone comes up with and it'll always be possible to access history but it will just kind of stop being the responsibility of the ETH protocol that's mandatory for consensus nodes. Apologize again for calling it the application chain. In my writing, it's called the execution chain now. So that's the first big thing, right? It's not very kind of featureful, not very sexy, but cleanup has to be done and it is sort of the technical debt that has to be paid. Once this accelerated merge is finished, then the reward is that the merge can come half a year earlier and the ecosystem can save potentially many billions of dollars in resources from just moving over to proof of stake more quickly.
05:10:53.964 - 05:11:53.160, Speaker I: So after that sharding. Right. So for those of us who have been around in the ethereum ecosystem a bit longer, you'll probably remember that the original plan was actually to do sharding before the merge, but now, because we're prioritizing the merge, we want to do sharding after the merge. Right? Of course it'll be developed in parallel, but it'll likely actually be implemented and turned on at some point after the merge is done, just so we have a bit of separation and we don't want to do all of the potentially dangerous things at the exact same time. You want to do first one and then the other so that developers can kind of pay attention and focus. So sharding, as we've been talking about for a year, starts off as being just data sharding. So no execution on shards.
05:11:53.160 - 05:12:01.900, Speaker I: Instead the shards are blobs of data. And the reason we care about having blobs of data is so that those blobs of data can be used by roll ups.
05:12:01.980 - 05:12:02.224, Speaker C: Right?
05:12:02.262 - 05:13:14.504, Speaker I: So 64 data shards on each shard targeting, I think an average of I forget if this is an average or the max, but an average of either 256 or 512 kilobytes in a shard block per 12 seconds. And then the security of the shard blocks will start off being committee based, so just a randomly selected committee will vote on it, and if it signs off, it's accepted. But then over time, we'll add higher levels of security with things like proof of custody and through this big thing called data availability sampling. And at some point we also want to or potentially even immediately have staggered shard blocks. So you don't want the block of every shard to come at the same time during a salot. You want, say, shards one to ten to come early in a slot, shards eleven and 20 to come later in a slot and then shards like 60 to 63 to come near the very end of a SWAT and have some way of some incentive mechanism to ensure this actually happens. And so roll ups that are willing to kind of walk between multiple shards will basically be able to just keep on using these new roll up blocks immediately as soon as they come.
05:13:14.504 - 05:14:47.140, Speaker I: And they'll be able to have block times that are much faster than the twelve second kind of tick time of the Beacon Chain itself. Right. So this is kind of currently the most promising potential strategy for basically making the ETH two system and roll ups on top of ETH Two kind of have initial confirmation times that are competitive with more centralized chains, basically without actually incurring the risks of being centralized. Data Availability Sampling so this is this really important security technology for sharding, where basically the goal is to have clients verify data availability guarantees in this probabilistic way. And the goal of this is to allow clients to detect data availability failures or reject shard blocks that are not available, even if the honest majority assumption for committees is broken, right? So this sort of preserves the property that we're used to with Ethereum as it is today, and with bitcoin as it is today, that basically says even if there is a 51% attacker, they can revert the chain, but they can't force you to accept invalid or unavailable junk. And in a shorted system we want to maintain this invariant, but obviously without requiring every client to personally download and check everything. And state availability sampling is this lovely and wonderful technology for actually doing that.
05:14:47.140 - 05:15:40.570, Speaker I: So every block gets redundantly encoded with polynomial commitments. And the idea is that if you have any 50% of the block, so the block, let's say, gets split into 1000 pieces. If you have any 500 of those pieces, you can recover those other 500, right, because it's a kind of degree 500 polynomial. So if you evaluate it at some points, you can extract the polynomial, then you evaluate it at all the other points. If he wants to know more of the fancy math like this has been written about in I think even the post on Das on Notes Ethereum.org that actually contains this picture. And then once you have this mechanism where you only need half a block to be available for the entire block to be available, you can randomly sample to check that enough of the block probably actually is there.
05:15:40.570 - 05:17:20.900, Speaker I: So a lot of work has gone into this, a lot of thinking has already gone into some kind of starting some prototypes, coming up with optimized algorithms for polynomial commitments, thinking through the Shard or the peer to peer network design. But it is still lower priority than just the merge and Sharding because we do recognize the kind of urgent need for security or sorry, for scalability and the need to just get users, get some kind of scalability in the hands of users first and then kind of push up the robustness of the system second. Now the system still is fairly robust even without data availability sampling, right? It just relies on an honest majority assumption and data availability sampling basically removes the honest majority assumption, at least for Sharded data and then more security improvements, right? So these are things that Justin really loves. Single secret leader election. So basically ensuring that the proposers of upcoming Beacon blocks and possibly Shard blocks are not publicly visible anymore and that mitigates dos issues and mitigates collusion risks and basically kind of replicates many of the benefits of kind of the way proof of work works, where block publication is more anonymous in a proof of stake context VDFS verifiable delay functions. So these once again, Justin has talked about these a lot. Basically verifiable very secure randomness for choosing committees.
05:17:20.900 - 05:18:36.156, Speaker I: So committees become more difficult to attack, which in practice means that the security of the system goes up. Or you can rely on the committee before VDFS if say more than 70% of nodes are honest. But with a VDF that requirement might go down to, I don't know, somewhere around like 57%, maybe 60%. And then proof of custody is basically just a further mechanic that forces nodes to actually download, keep and validate block data and it's basically a kind of anti centralization measure. So those are some kind of short term consensus layer focused things. Well, I shouldn't say short term because this is after the merge, but some medium term consensus layer focused things that I expect that will be the major focus after the merge. And these are all ideas that we've been working on for quite some time, right? So for a lot of them there's basically a complete kind of proto spec that's just been sitting around and at some point he just needs to turn the proto spec into a false spec and then turn it into an implementation execution layer improvements.
05:18:36.156 - 05:19:21.880, Speaker I: So the execution layer does have some needs to continue evolving. So one is address extension. So this is a small thing, but it's probably important for the long term. Basically increase the length of addresses from 20 bytes to 32 bytes and this is an important security improvement. 20 bytes is in the long term not safe for collision resistance and this gives 26 bytes for collision resistance, which actually is safe. Add some version numbers for future compatibility and it's needed for state expiry proposals, which is the next thing here. So basically statelessness and state expiry, this is a really important thing for the ethereum execution layer post merge.
05:19:21.880 - 05:20:36.448, Speaker I: So right now the main thing that is making further increases to the gas limit not safe, is actually not even things to do with execution. It is state size, right? The state size is growing by something like 30GB a year potentially. And then with the recent gas limit increase it'll probably go up to about 35 and then if there's more gas limit increases, it'll go up even more. And there's this just permanent ever growing state. And this is really inconvenient because it means that just syncing to the network for the first time is something that would just permanently become more and more inconvenient. It also means that the hard disk space requirements of a full node just keep going up and up. And it also actually kind of interacts with the Dos issues because the larger the state size is and the more amount of data you need to have on database, the more time each individual database read will take and so the more vulnerable the chain becomes to denial of service attacks.
05:20:36.448 - 05:21:31.544, Speaker I: So state size kind of management is this really important issue and I've talked about and really the ethereum community in general has talked about two big categories of strategy to address this. One of them is statelessness and the other is state expiry. So the idea behind statelessness is to basically say well, we're going to create two classes of nodes. One class of nodes continues to require the state and the other class of nodes instead does not have to store any state at all. And in order to verify blocks, they have a witness that basically provides the portion of the state accessed by a particular transaction plus a cryptographic proof that shows that the values that are provided are actually correct. Right. It's also been called stateless clients and vertical trees, which we've been talking about a lot, and code virtualization, which we've been also talking about a lot.
05:21:31.544 - 05:22:42.796, Speaker I: It's going to become code virtualization, but these are all really important to make the witness sizes actually small enough for this to be viable. EIP 2929, which happens in Berlin, also contributed to making statelessness viable because it established a nice low bound on the number of state accesses that can happen inside a block. It also made my uniswap transactions about 1% cheaper, which is interesting. So if you have a DAP, there's a chance that it got 1% cheaper and there's a chance it got about it got 1% more expensive, but I think a lot of things did, especially more expensive things did end up getting slightly cheaper, which is interesting. But basically it was a reworking of gas costs that just ensured that there's this bound on the number of storage reads, which means that there's a bound on the witness size. And so there isn't a risk that there's going to be blocks where just the size of a witness is too big to propagate to everyone in time. There is one thing that is still required to kind of finish that task.
05:22:42.796 - 05:22:56.270, Speaker I: One more gas cost change, which is at the same time as covert verticalization. We will need to add a gas rule that charges sub amounts of gas for every chunk of code that gets accessed and.
