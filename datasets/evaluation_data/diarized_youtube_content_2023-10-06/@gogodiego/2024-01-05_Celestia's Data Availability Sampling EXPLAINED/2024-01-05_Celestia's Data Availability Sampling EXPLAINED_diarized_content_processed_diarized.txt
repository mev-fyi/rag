00:00:00.170 - 00:00:02.234, Speaker A: This video is sponsored by Celestia Labs.
00:00:02.362 - 00:00:10.346, Speaker B: What is data availability sampling? When roll ups inherit the security of Celestia, they're paying to post their blockchain data on it. But how do roll ups and users.
00:00:10.378 - 00:00:12.222, Speaker A: Make sure that their data is actually there?
00:00:12.276 - 00:00:22.254, Speaker B: It's easy for smaller blockchains since it's not hard to fully download and verify blocks that are tiny. But what happens when these blocks get bigger? At the beginning, this is cheap to check, but as blockchains scale and have.
00:00:22.292 - 00:00:25.910, Speaker A: More expensive hardware requirements, normal people will get priced out.
00:00:25.980 - 00:00:29.986, Speaker B: So how will you know your data is available in a modular feature with millions of roll ups?
00:00:30.098 - 00:00:34.918, Speaker A: This is the data availability problem and how data availability sampling can solve this.
00:00:35.004 - 00:01:02.898, Speaker B: Celestio's data availability layer uses Aerisher encoding technology that spreads and divides this data and allows users to randomly select and download small portions. This gives users with cheap consumer grade hardware like your phone, high confidence in data availability. And if light clients detect a data withholding attack, they can band together to reconstruct a block. The power of Celestia modular blockchains and data availability sampling is this positive feedback loop. So the more light clients, the larger the blocks, the more transactions, and the.
00:01:02.904 - 00:01:44.030, Speaker A: More possibilities to build whatever. I'm so excited to talk about data availability sampling, but first, special thanks to our sponsor Celestia for allowing me to go modular. This video is jam packed and isn't afraid to get technical. We're going to be talking about blockchain scaling the DA problem, the concept of probabilistic sampling techniques, then a technical deep dive, talking about the core of how celestial works, explaining the technology that no other videos are willing to get into, and then talk about the math, the probabilities, and the magic of DaS data availability sampling. Like clients, we have a lot to learn.
00:01:44.100 - 00:01:45.390, Speaker C: So let's get to it.
00:01:45.540 - 00:02:28.458, Speaker B: When scaling blockchains, you have to factor in throughput relative to decentralization. So we can do things to increase throughput on our chain, like increasing block sizes and TPS, but that requires more storage and computation. We can increase speeds and finality, but that requires faster Internet and smaller validator sets. Bitcoin raised block sizes from 1 megabytes, doubling its output but now requiring twice the hardware. While 200 megabytes is manageable on a $200 laptop, continual block increases are impractical. Imagine if bitcoin kept on doing this, reaching 1gb block sizes capable of holding thousands of transactions. Although this boosts metrics like TPS speeds and data throughput, it doesn't scale it properly you can't verify any of this data and would have to trust someone else.
00:02:28.458 - 00:03:17.302, Speaker B: And this is the data availability problem. The purpose of blockchains isn't data throughput, but the ability to participate and verify yourself. Scaling them requires designing under the constraint of increasing data throughputs while keeping verification low in simple terms, creating large blocks that hold lots of transactions while making sure that all data is there in the first place. This describes the data availability problem and makes it easy for anyone to check for themselves. This is what Celestia does, and with these design constraints came the idea of data availability sampling. The research paper fraud and data availability proofs by coauthors Mustafa al Basam, Alberto Samnio, and Vitalik Batarin on page two describes a probabilistic sampling technique. It states 1% of block data needs to be downloaded in order to check that the entire block data is available with 99% probability.
00:03:17.446 - 00:03:32.906, Speaker A: Celestia uses probabilistic sampling techniques on two dimensional erasure encoded data. That was a lot to unpack. So first we're going to learn about what are probabilistic sampling techniques, Reed Sullivan's algorithm and 2d erasure encoding, and then the math and probabilities behind data availability sampling.
00:03:32.938 - 00:03:33.982, Speaker C: So let's get started.
00:03:34.116 - 00:03:37.966, Speaker A: Probabilistic sampling techniques Celestia's data availability sampling.
00:03:37.998 - 00:03:39.426, Speaker B: Reminds me on how my teacher used.
00:03:39.448 - 00:03:40.290, Speaker A: To grade my work.
00:03:40.360 - 00:04:06.070, Speaker B: We had these big, thick packets of work, and under the constraints of one poorly paid teacher with six classes each, with 30 students, each with 15 pages, and ten multiple choice questions each, she had to grade 27,000 individual multiple choice questions, and her solution was a probabilistic sampling technique, sampling, meaning she didn't check every question, only a small subset, and probabilistic, meaning she sampled them randomly and found the average.
00:04:06.150 - 00:04:07.850, Speaker A: So she would randomly pick ten questions.
00:04:07.920 - 00:04:11.754, Speaker B: From her packet, and the average was her test grade. So I could have technically not done.
00:04:11.792 - 00:04:13.226, Speaker A: Some part of the paper, but she.
00:04:13.248 - 00:04:14.898, Speaker B: Might randomly pick the ones I didn't.
00:04:14.934 - 00:04:20.158, Speaker A: Do and I would fail her class. So the stakes were high and I had no choice but to actually do the work.
00:04:20.244 - 00:04:34.226, Speaker B: She had successfully scaled her grading from 27,000 individual multiple choice questions down to only 2880. That's an 89% decrease, all by using probabilistic sampling techniques. And Celestia sort of does the same.
00:04:34.248 - 00:04:36.778, Speaker C: Thing, like clients like your phone randomly.
00:04:36.814 - 00:04:52.214, Speaker A: Sample small bits of information from Celestia's blocks. But the way these blocks are prepared are through read selen encoding. And Celestia uses a unique two dimensional erasure encoding scheme erasure encoding protects information by adding extra at the end for redundancy and recovery.
00:04:52.262 - 00:04:53.754, Speaker B: It's used by cds if they get.
00:04:53.792 - 00:05:09.630, Speaker A: Scratched, or satellites if they lose information across long distances. In either scenario, you can use parts of information to recover the full message. And Celestia uses erasure encoding for data availability sampling. So our rollup data gets deposited into celestia. Let's say these four are our data.
00:05:09.700 - 00:05:44.470, Speaker B: Using erasure encoding, it gives them order and adds backup information to the end, called parity. Using Reed Sullivan's algorithm, it uses polynomials that sort of look like this. The more data then, the more longer this equation is. Now, this algorithm is complicated and out of scope, but essentially these polynomials combine the information from the original data into these new parity data. If we lose data one two four, and parity two, we can use lorange interpolation and, using existing data, draw a polynomial to solve for the missing data. And if someone tries to withhold any amount of data, they would have to hide over the amount of parity. So four out of eight to be unrecoverable.
00:05:44.470 - 00:06:19.010, Speaker B: This property of erasure encoding is also used for data availability sampling, but Celestia doesn't use the traditional format. Instead of rows, they use grids. In a 2d erasure encoding scheme, if a lie client finds a gap in a long erasure encoded row of data, the context and ordering can help it fix it. But an attacker can incorrectly encode the parity at the end, compromising the 50% recoverability factor. So it's vital for lie clients to detect this and fix it. However, the conventional method of reencoding evolves local recomputation of all this data. So this is linear with block sizes, and larger blocks demand more computation.
00:06:19.010 - 00:06:57.234, Speaker B: Celestia, on the other hand, uses a 2d erasure encoding scheme, avoiding this like client scaling issue. The 2d erasure encoding scheme simplifies verification and reencoding into smaller rows and columns. So first the data gets erasure encoded vertically into rows and then horizontally into columns. An attacker must withhold over a quarter of a block to make it unrecoverable. If there's a partial data withholding light, clients can detect this and start block reconstruction with just one honest full node, first recovering this column, then using that data to recover the rest of the rows. Now this block is fully reconstructed and available using this 2d erasure encoding scheme. A cool way to think about this.
00:06:57.272 - 00:06:58.946, Speaker A: Is if blockchain data were to be.
00:06:58.968 - 00:07:47.790, Speaker B: Put into a large sudoku puzzle, it could be divided into 4096 squares and it's easy to randomly sample squares from each quadrant, and if you discover a small gap in a large sudoku puzzle, light clients can just reencode it and solve a small row of the puzzle. Now let's go through the math and probabilities of data availability for light clients. In page six of the lazy ledger white paper, it talks about the probabilistic validity rule using 2d read Solomon coding at a quarter rate. It only requires 15 samples or zero 4% of a downloaded block to have a 99% guarantee in data is available. And if you wanted more assurance, you could always sample more, meaning scaling properties of data availability sampling are lightweight and scalable for any flexible range. The magic of Celestia and data availability.
00:07:47.870 - 00:07:54.062, Speaker A: Sampling is that it gives you high confidence that your data is safe and secure without needing to run an expensive full node.
00:07:54.126 - 00:07:55.906, Speaker B: By design, the requirements to run a.
00:07:55.928 - 00:08:27.902, Speaker A: Light node are extremely lightweight. All you need is 2gb of ram, a single core cpu, 50 gigabyte SSD, and 56 kilobit Internet speeds. And with these requirements being so ridiculously low, people get very creative in what they can run on it. You have the obvious you can run it on a cheap pc or laptop or a popular one, which is your phone, but you can also run it on a Kindle and a Nintendo switch, a Tesla on an airplane or DJ equipment. And with requirements being so low, there are lots more possibilities. If you want to run one of these lite clients, go to celestia.org slash Runano to learn more.
00:08:27.902 - 00:08:47.758, Speaker A: The cool part about running a light client is one, you help secure the network by checking for data withholding attacks. Two is that you feel confident that your blockchain data is safe, secure and available. And then three, the coolest feature, it helps scale Celestia. Celestia has a super unique scaling flywheel where the more light clients means the more eyes that are monitoring and securing the network.
00:08:47.854 - 00:08:49.414, Speaker B: This allows the DA layer for more.
00:08:49.452 - 00:09:01.542, Speaker A: Data throughput by having larger blocks, all while keeping it decentralized. Because there's light clients all around the world that are constantly and randomly sampling this data. This is super cool, and Celestia are the pioneers of this technology.
00:09:01.676 - 00:09:04.166, Speaker B: There's ongoing ideas and research developments on.
00:09:04.188 - 00:09:08.274, Speaker A: How you can improve this technology. Imagine a private mixnet for anonymous sampling.
00:09:08.322 - 00:09:10.746, Speaker B: Or increasing speeds by having multi block.
00:09:10.778 - 00:09:13.086, Speaker A: Sampling, or increasing adoption by having light.
00:09:13.108 - 00:09:15.082, Speaker B: Clients in every single wallet and browser.
00:09:15.146 - 00:09:20.780, Speaker A: I could talk about Celestia all day, but to sum it up, the future is bright and data is available for all.
