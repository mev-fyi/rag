00:00:00.640 - 00:00:16.730, Speaker A: Is that the throughput will not be a so called problem if there is adoption. So because of the light client infrastructure that we talked about, so the more light clients that is there, we can easily scale it up, but we need the adoption to come.
00:00:19.870 - 00:00:52.700, Speaker B: Well, thank you for joining me today, Prabhu. Really excited to have you on the podcast. I missed spoke on my podcast with Xerox incorrectly stating that Polygon is ultimately a data availability committee, is not, or is a data availability committee, but it is not. So I appreciate you correcting me and really looking forward to having this conversation. I think, as I said earlier, this is the best way to learn, and I think ultimately, having the longer form conversations are the best place to do it as well.
00:00:53.370 - 00:01:17.194, Speaker A: Yeah, thank you for having me here, JJ. Excited. And yeah, I think it's okay. And there are very. There are many wrong conceptions about what avail is. And it's. And sometimes people say things differently because it's a new space, it's evolving.
00:01:17.194 - 00:01:25.644, Speaker A: We are also changing things quite rapidly, and this is throughout the ecosystem. So things like this, I think it's completely okay.
00:01:25.812 - 00:01:47.130, Speaker B: And ultimately, there's quite a few ways to kind of crack the nut, so to speak, and scale these different architectures. So I'm looking to get into that as well. Maybe just briefly, do a quick intro on yourself and then how you got into the blockchain world and we'll jump right into it and talk about scaling.
00:01:47.990 - 00:03:31.802, Speaker A: Yeah, I had more of a conventional introduction, so I was pursuing my PhD in 2016 when I came to know about blockchains. And before that also I was like reading about cryptography and it already interested me, so I was doing that. And while doing blockchain, just reading about it, and it was quite fascinating about the power that it has, the amount of, you know, the trustlessness that it brings to the table, the openness that it brings to kind of having anyone participate in this protocol and just have more auditable, like, verifiable interactions among non, you know, non aligned participants, so to say. And I found it very fascinating because this is one of the fields which had not only computer science, but also things like game theory and inside even computer science, you could have attacked it from the network level, from operating systems, from compiler design, any of these are all applicable, and you get to be able to work with them in a really. In a way where you can actually impact real people. So, yeah. So while doing my PhD midway, I realized that although I'm able to write some papers, mostly about data marketplaces and how you can do like proof of retrievability on data and things like that.
00:03:31.802 - 00:03:52.870, Speaker A: I realized that this is good, but it is not making too much of an impact and hence I wanted to switch to the industry. So in 2020, I joined Polygon and started working on avail, among many other things within polygon. And that's how I came into this.
00:03:53.410 - 00:03:58.150, Speaker B: Amazing. So you've been involved with the Polygon team fairly early on?
00:03:59.050 - 00:04:25.930, Speaker A: Yeah, yeah, it was a complete gyride there. We had like the pos chain running when I joined. And since then there was a lot of like research went into proof of stake into how we can scale, and Polygon was always about scalability and how we can scale ethereum and things like that. So it was already helpful.
00:04:26.790 - 00:05:10.580, Speaker B: Amazing. Well, I would love to maybe just dive into scaling. I think Avail has wrote some really great blog posts and I highly recommend people to actually read them. I'll link them in the show notes, but the article is titled Avail's ability to scale and where we stand today. And I think one thing that you guys touch upon that is just being able to increase throughput. And I talk about this quite a bit, if anything, large throughput. Maxi, can you talk and maybe explain in layman terms why throughput is so important and what are kind of the two large, like, key bottlenecks to actually scaling throughput?
00:05:12.080 - 00:06:19.764, Speaker A: Yeah, I think it's good to kind of, you know, define throughput in some sense, because mainly because it is misunderstood sometimes, and also because ovale is a different type of blockchain. So I don't want to compare apples with oranges when I say something. So essentially, in terms of avail, the throughput is the amount of data that we can accept and keep available inside our network. And that's how we think of throughput. What this actually means is that we are only solving a specific issue, which is data availability. In traditional blockchains, what you do is you have a monolithic stack, which means that you do not only availability, but also perform computation over it. Hence, in order to increase the throughput, you have to essentially either create bigger blocks or create blocks more often and propagate blocks more often and have more data sent across, things like that.
00:06:19.764 - 00:07:10.180, Speaker A: And not only is that the bottleneck, but the bottleneck is also that whoever is participating in your network needs to get the transactions and then execute them. And only after re executing them can they accept it. And this is for the full nodes. And then the lite clients, they have to completely rely on the full node that they are talking with in order to know whether either the data is available or whether the computation was done correctly. So traditional chains have this bottleneck. And what we are trying to do is we are trying to make the entire ecosystem more modular. So we are trying to create this base layer which only keeps like, takes care of data availability, so that execution engines on top can do the execution scaling.
00:07:10.180 - 00:07:13.180, Speaker A: And I can talk more about execution scaling.
00:07:13.480 - 00:08:03.210, Speaker B: I think we could definitely go down the modular versus monolithic rabbit hole, but maybe sticking on the topic of throughput, I think the two big things that you mentioned was ultimately the block size and kind of the velocity of blocks. So how frequently those blocks ultimately come about. And I truly, I think that is kind of a misunderstood topic. Maybe not in the technical world, but maybe for all the normies that are watching the podcast and trying to learn, really the amount of data that you can propagate is massive, really the true bottleneck that we all have to crack, just because if you can increase more data, ultimately that is more space for more transactions and you can build more interesting applications. So I think you put it beautifully.
00:08:04.270 - 00:08:48.936, Speaker A: Yeah, yeah. And that's why there is the scaling challenge. Let's talk a bit about not only the challenge, but also the solution. Right. The idea with computation scaling is hard, but there are newer primitives that are right now there, which we can talk about. But talking about data scaling, it's easier because you can do sampling over data. And once you do sampling, because sampling is a very powerful technique with which you don't need to care about how much data is there, you're sampling numbers remain same, irrespective of the amount of data, which is an extremely powerful primitive.
00:08:48.936 - 00:09:23.410, Speaker A: And that makes you not only like the light clients, not to worry about how much data is being passed throughout the chain, but also at the same time, the network is, so to say, elastic, in the sense that if you have more of these lite clients doing the same amount of work, you can have actually more data kept available throughout the network. So I think that's why data scaling is extremely powerful, because it not only forms a base layer, but it also scales along with utility.
00:09:23.750 - 00:09:53.350, Speaker B: Yeah, maybe before diving deeper into light clients and full nodes, one misconception that I had, and I again just wasn't following it as closely as I should, was that the veil project ultimately decided to break away from Polygon. Could you speak just to a little bit of that decision, and why the team ultimately wanted to pursue their own layer one architecture to build kind of a fast data availability layer.
00:09:54.130 - 00:10:47.618, Speaker A: Yeah. So before I go to that, let me just share some history so that the context becomes very clear. So before I joined Polygon, when it was Matic Network, they started, my co founder Anurag and the others, other co founders of Polygon, they started with building a plasma solution. And at that point they wanted to again scale Ethereum, and for that they wanted to have plasma. But if people are aware with plasma, then one of the main bottlenecks was again, data availability and the mass exit problem that follows if you cannot keep the data available and things like that. So that's when they pivoted to the POS chain. But the idea of actually being the data availability, being the bottleneck to scaling remained within.
00:10:47.618 - 00:11:36.700, Speaker A: And that's why even before I joined, they dabbled with the idea about how we can make a truly scalable data availability layer. And so after I joined it, we kept on working there along with other growing team working within polygon. And ultimately we wanted to, you know, create a critically neutral data availability layer so that not only the polygon l two s, because polygon has around three layer, two projects within them, the polygon zero Biden and the ZKVM project. Right. So we didn't want to only cater to those solutions, but we wanted to be credibly neutral so that we can cater to any execution scaling platform which wants to scale.
00:11:37.760 - 00:11:44.380, Speaker B: That makes sense. That was recently announced in March, correct?
00:11:45.200 - 00:11:45.888, Speaker A: Yes.
00:11:46.024 - 00:12:45.836, Speaker B: Okay, cool. Now, it's hard to keep up with everything in this space, and so there's always new announcements and new things and so definitely appreciate you adding more clarity there. Maybe jumping back to. So we've kind of established that in all blockchains to really scale the true bottleneck and the end game is increasing the amount of throughput. You can kind of do that in two ways by increasing the block size or increasing the velocity of the blocks and how quickly they come. And then outside of that, the big thing that we all kind of know and love with blockchains is that censorship, resistance, being actually decentralized. So before we kind of jump further into some of the technical nuance, could you just talk about full nodes and talk about like clients and their respective kind of duties and roles in how they play in blockchains before diving deeper into like block sizes and else, should.
00:12:45.868 - 00:12:49.440, Speaker A: I, should I cover it in a general sense or in availability?
00:12:49.780 - 00:12:58.740, Speaker B: I would just say in your timer and ology how you view them. Full nodes, why they're important. Like clients, why they're important. And we can kind of go from there.
00:12:59.440 - 00:13:42.572, Speaker A: Yeah. So in a general, typical blockchain what happens is there are validator nodes, there are full nodes and then there are light clients. So validator nodes are just special type of full nodes who participate in the consensus. So in a typical proof of stake blockchain, they have staked. So they have a skin in the game and they have the right and the authority to produce blocks. And these blocks gets appended to the chain and hence creating the like continuing the blockchain. And what the full nodes do in a typical blockchain is that they download these blocks and then they verify the correctness.
00:13:42.572 - 00:14:42.470, Speaker A: So not only that the validators or the validator set or the super majority of the validators, they not only agree to creating a particular block, but the full nodes also take that block and verify the correctness of the block. What the lite clients typically do is that they rely on a full node to give them the correct information and how they do. Typically a light client only downloads block headers. So a block be divided into header which contains the metadata and the body which contains transactional data and other things. So the lite clients typically only download the header. They verify that the header has gone through the consensus and then they query the full node and verify the information that they receive against the header. That's typical lite client.
00:14:42.470 - 00:15:41.910, Speaker A: What they do now, if you think about something like Ethereum, what Ethereum lite clients can do is like, they can say query state, like what is my account balance? What is my account balance? And if they ask the full node, they will get back a response which they can check against, say the state route. Right. And this makes it verifiable, but it doesn't make it extremely trustless because you are still trusting the validator set to have done the state transitions correctly, to have computed the state tree correctly, and so on and so forth. So there is an inherent trust that the lite clients need to have on the validators in this kind of a light client model. And that's where data availability designs along with data sampling is extremely powerful.
00:15:42.070 - 00:16:18.440, Speaker B: Yeah, and that's where I was going to try to steer the conversation next with data availability sampling and light clients. I think this was really a true kind of unlock for the industry. And ultimately what it truly provides is as you kind of increase the hardware cost for full nodes, you can still get trust minimization via light clients. And so it kind of ultimately can offset some of those larger hardware requirements. Could you speak to that kind of explain data availability, sampling and lite clients roles again in that process?
00:16:19.420 - 00:17:28.992, Speaker A: Yeah. So what it means is that this validator set, which typically used to create blocks, will still create blocks here. These blocks right now, block creation process means taking some transactions, executing those transactions, putting them inside a block, making sure that the block is correctly formed, and all valid transactions are there inside the block, and reaching consensus within the validator nodes, then sending it to the full nodes and the rest that I followed in a data availability focused chain like avail. What we do is we do not verify transactional correctness. So what do I mean by that is, we check well formed mess, of course, but we do not check whether the transaction execution is done correctly. We treat all transactions as data blobs, so to say. So they are just data to us and the validator nodes, they take these data blobs and put them inside a particular block, and which is getting propagated right now.
00:17:28.992 - 00:18:44.330, Speaker A: What the light clients do here is that the light clients perform availability samplings. And again, similar to the previous construction that we discussed, they check that the sampled data is correct. And how they do it is where the erasure coding and the polynomial commitments, they all come into the picture. So we create, while creating the block, we have enough redundancy in this, inside the data, so that even to hide a very small amount of data, you need to hide a lot, which means that it is easily detectable now. So the power of sampling coupled with the redundancy in the data is what makes sampling so powerful, so that it can catch if something is hidden by the block producer. And what the case polynomial commitments do is that it creates a small commitment or a set of commitments inside the header, so the sample that you receive, you can verify against the polynomial commitment, or so to say, you get an opening which you verify against the commitment.
00:18:45.400 - 00:19:14.540, Speaker B: So maybe if I could re articulate. Ultimately, the more light clients that you have on the network sampling these full nodes, the lower the probability that the full nodes are withholding data or being malicious. So that maybe if you can't participate in buying a full node, but you have many light clients on the networks, that allows you to have similar guarantees, because there's so many light clients checking the full nodes and making sure they're not doing anything nefarious.
00:19:15.340 - 00:19:16.160, Speaker A: Yep.
00:19:16.460 - 00:19:41.040, Speaker B: Awesome. Cool. No, I appreciate all that context. I try to build ground base and then we go from there. So in the article, it also mentioned that I think avail at the time was doing about 22nd block times with two megabyte blocks. Where does the chain stand today and how has it progressed? Because I think these were written in 2022.
00:19:41.910 - 00:20:33.022, Speaker A: Yeah. So we haven't, you know, scaled up, as we mentioned in that particular article, it's again, easy to scale these numbers up, but it doesn't make sense to have bigger blocks just for the sake of bigger blocks. Right. So what we have right now is the blocks can be extremely small if there is no data. It can be like even a few bytes, like few hundred bytes. But if the blocks are filled with data, they can go up to two mb of raw data, which means around four mb of erasure coded data. So that's where we stand today, as we have mentioned that we have tested internally for up to around 120 mb blocks.
00:20:33.022 - 00:21:15.600, Speaker A: And we could have pushed further because. Because just to mention briefly, is that this technology that we are using, the commitment generations and the openings, they're pretty same for most ZK proofs and so on. Some ZK systems use fry, some use cases and so on. So we could easily make the blocks bigger, but we also need light clients and the number of lite clients to actually complement that, because otherwise we are creating bigger clocks which may or may not be available because there are not enough.
00:21:16.180 - 00:21:21.160, Speaker B: That makes sense. So is the block time today still around 20 seconds?
00:21:21.660 - 00:22:04.516, Speaker A: Yes. And one of the things to clarify here is that the block time for a DLL is not the same as that of a monolithic chain. That is because there are many orientations in which this execution engines can use the DA layer. So although the data availability guarantees come, like, the blocks are produced every 20 seconds, but that doesn't stop anyone from having soft confirmations from your roll up sequencer in a much, much faster manner.
00:22:04.668 - 00:22:12.080, Speaker B: That makes sense. Interesting. So the block size is a little bit dynamic today, or is it hard coded?
00:22:13.220 - 00:22:21.240, Speaker A: So it's hard coded up to two mb, but that is also changeable very easily through a runtime upgrade.
00:22:21.550 - 00:22:51.056, Speaker B: Gotcha. Okay. Very interesting. Yeah, no, I love kind of understanding all the different nuances. Ultimately, I do want to get into maybe different virtual machines on the compute side, but kind of continuing down this path. One thing that you also mentioned on, like, the throughput side was just propagation delay. And in the article that you guys wrote, and as you mentioned, you were sending 120 megabyte blocks every 20 seconds.
00:22:51.056 - 00:23:00.900, Speaker B: Could you share a little bit more about, like, when you scale up the actual block size to increase throughput? What were some of the results that you found in the research that you did?
00:23:02.200 - 00:23:56.378, Speaker A: So the idea is that there are a few different ways in which we can build the availability blockchain. Right. And this is not exactly related to the data availability sampling of the light clients. So just the core blockchain. There are a few different ways in which we could have proceeded. Right now what we do is we want to create a blockchain which can actually support the thousand validator sets that we envision, right? So we are, we are, we are, we are trying to make this, the node requirements of the validators as small as possible. And although we have very, very light requirements for the light clients, but essentially for the validators also, we want to keep the entry barrier pretty low.
00:23:56.378 - 00:25:03.992, Speaker A: So if we try to push the amount of data that gets pushed inside a block, then the validators need to work somewhat harder. We wanted to keep enough, you know, breathing space, and that's why we had a very relaxed block time, very small block data throughput. Right. So to say, having said that, why we wanted to keep it extremely relaxed is because this is just an optimization, right? We want to make the groundwork extremely stable, extremely resilient, because that's what is going to be the core of our chain. And we have been working on the note and the lifeline for the last two years and trying to make it extremely robust, because we are pretty sure that scaling it up once we have the infrastructure in place will not be extremely problematic. But it's important to do it in a right manner from the ground up.
00:25:04.176 - 00:25:36.860, Speaker B: Makes a lot of sense, I think. I mean, even doing it correctly is extremely important. But I think the other thing is just getting the demand outside of ethereum, I'm not sure any other blockchain per se has really needed the demand or really hit throughput capacities. And so we still need, even though we're investing a lot in the infrastructure side, we need engineers, application engineers, to build really cool things, to take advantage of all the throughput that you and the team are building.
00:25:37.800 - 00:26:16.490, Speaker A: Yeah, absolutely. Agreed. And that is the main objective, right? That why are we building this base layer? Because we want to enable the modular stack. And what is it that is so, you know, appealing about this entire modular stack is that you can actually bring newer applications, and that is, you know, that is a culmination of many things. It's not just that data availability was not the only factor. If you see right now, the optimistic design of building a chain is extremely efficient. There are production rate chains right now.
00:26:16.490 - 00:27:38.734, Speaker A: The challenge window periods are getting lower and lower. There are pretty well understood crypto economics behind it, and so on and so forth in the ZK space. We have all witnessed incredible performance improvements and so on. And then there are ways to now easily spin up your own execution engine using an existing vm. Like you can even use something like say RISC zero today to write an existing rust or c program and then have a, you know, of a proof of correct execution that was like a few years back, completely impossible, right? So those things are coming up, and then you see the advent of, you know, SDKs like the sovereign SDK and so on, which are, which are making the best use of it, and having the developers, like giving the developers the platform to, to take this incredible technology and build a chain of their own. We want to play our role and complement that stack so that the developers can build new applications which will actually bring adoption to our incredible technology that we have.
00:27:38.862 - 00:28:04.480, Speaker B: Definitely, maybe a little bit down the line, we can talk about the modular stack versus the monolithic. I think that would be an interesting conversation. But for those that are not as familiar with like the full node side of things and like the validators that you have on the network today, you said the end goal is approximately 1000, or kind of a good kind of goal to shoot for. Where does the network stand today in terms of full nodes?
00:28:05.420 - 00:29:15.610, Speaker A: So right now what we have is a very small testnet. We have around 15 validators, or 20 validators in the testnet. But very soon we are going to launch the next phase of the testnet where we want to increase the increase in numbers to a very high degree. Why this was important is because we have been iterating extremely fast on the base layer, and if you have a very big validator set in the initial days, pushing changes becomes harder. And although we use the incredible runtime upgrades from the substrate ecosystem, where the coordination becomes much, much easier than existing, unlike other platforms. But still, we wanted to make iterate faster, we wanted to have smaller sets, we wanted to have partners who can give us extremely valuable feedback. And that's why the initial variation, and it still currently is extremely small, but we hope to increase it pretty soon.
00:29:15.950 - 00:30:00.660, Speaker B: Awesome. Very cool. In terms of, I think maybe a little bit of a comparison to avail and maybe what they're doing to either Celestia or Eigen da. I think now there's multiple different kind of people trying to tackle the throughput problem. And I think going forward it's going to be up to the engineers and people like yourself to kind of attract the engineers to build on your specific data availability layer. What would you kind of say to engineers listening to this podcast about maybe like comparing and contrasting avail to Polygon or to Celestia and then also Eigen layer?
00:30:01.810 - 00:30:43.050, Speaker A: Yeah, I would just first, start with saying that. Just come to the modular ecosystem, just take any modular pace layer, take any modular SDK, take any of the execution environment and just build something new. So that would be the first thing to say. And now about comparisons. There are trade offs. Like many systems, there will always be trade offs about what you can and cannot do and what you have to select. Like with Ovil, what we have done is that we have spent a lot of our time to make the lite clients pretty resilient.
00:30:43.050 - 00:32:15.510, Speaker A: They have their own peer to peer network where they keep the data available. So that even if the super majority nodes suddenly try to make the data like gone or something like that, the light client, peer to peer can still hold your data and things like that. In terms of the value reset, what we have is we have a babe grandpa based chain. What this means is that we have a hybrid ledger, so to say, where block production or liveness can be assured even though parts of the network is down and finality can follow. And we have seen the benefits of that very recently in the Ethereum network, for example, where finality was delayed but block production continued, which is a very, very important thing when things go wrong. And not only that, what we have is that if you are building a ZK based chain, then you want to rely on validity proofs, right? And what we have is that we secure our data availability network also with validity proof, so you don't have to wait for fraud proofs to arrive or not arrive before you can determine that your data is available or not. So that's why we think that the availability is going to be also chosen.
00:32:15.510 - 00:32:40.290, Speaker A: But as I mentioned at the beginning, we welcome all these users, all the players in the field that we have, all the developers that wants to enable a new application. And in the end, I think people will see some kind of choosing multiple DL layers, multiple settlement layers, and so on and so forth. So we will see that option there.
00:32:41.160 - 00:33:22.510, Speaker B: I think that was the very kind answer. Maybe if you could pull out a little of some knives and talk about maybe the more small nuances between the chain. Because I think at the end of the day, it is hard for engineers to actually understand this. And unless they spend a large amount of time, as I've even articulated on past podcasts, where I would really love the industry to get to is the application engineers know longer have to focus on the infrastructure. They can just kind of deploy that. And I think today there's so many different options, it's hard for them to kind of make an educated kind of guess so. Specifically, I think the throughput, as we've talked about, is a mass bottleneck.
00:33:22.510 - 00:33:34.070, Speaker B: Each of those, yourself and celestia and Eigen DLA are kind of taking unique approaches. Do you feel like there any in particular that you could highlight?
00:33:35.000 - 00:34:47.274, Speaker A: Yeah, so one thing I would like to say is that, so if you are building, as I was mentioning that if you are building a ZK based application versus a fraud proof secured application. So I am talking about the execution engine. Now, if you are building optimistic style execution, then you have to wait for short proofs to arrive or nothing. You have to wait for the challenge period before you know that the computation was done correctly. And in that model, if you have a fraud proof secured data availability layer as well, then it probably, you know, makes sense for you to use a fraud proof secured DA. But if you're building a ZK validity proof secured execution engine, then you wouldn't want to have a proof of correct computation in your hand, but have to wait for a challenge period because you haven't got data availability guarantee yet. Right? So in that sense, it would make sense for you to use something like Oval.
00:34:47.274 - 00:35:50.638, Speaker A: In terms of capacity, I think it's still very early and we haven't hit the, hit the ceiling in either of any of these networks. So I would not say to pick one over the other because of throughput. And I think we have mentioned in this one also and in other places as well, is that the throughput will not be a so called problem if there is adoption. So because of the light client infrastructure that we talked about. So the more light clients that is there, we can easily scale it up, but we need the adoption to come in terms of readiness. You know, I think the developers will any day choose something which is available today. So they will come and use some of our testnets and like try it for themselves, because in the end, I think they will see the benefits once they start using.
00:35:50.638 - 00:36:00.370, Speaker A: And these benefits will also depend upon not only the DLA, but also in the orientation in which their execution is bootstrapped.
00:36:01.230 - 00:36:59.600, Speaker B: Interesting, very interesting. Maybe shifting a little bit to maybe monolithic versus modular. There are, I mean, Aval, Celestia, Eigen, Da, I think are all really kind of pioneers in pushing forward the modular stack and allowing the separation of execution environments from data availability and consensus. And then there's a opposite side of the spectrum that Solana is taking, that Swe is taking. And I would say aptos say where? I mean, I think obviously you're a pretty big fan of the modular stack maybe if I could get you to articulate the pros of maybe a monolithic stack and then the cons, and ultimately from that, why you've decided to that over the long term, the modular stack is probably the correct one for users and engineers.
00:37:01.460 - 00:38:15.998, Speaker A: Yeah. So there are many ways to answer the question, right? So one of the ways is what I probably were having this conversation on Twitter, right? And the way to think about it is, if you are building a chain from ground up, there is a lot that a developer needs to do. The developer, like the blockchain developer needs to think about bootstrapping security. That is a huge, huge bottleneck. And we have seen the pains of that. Although there were very good frameworks to build your own monolithic chain, and even after that, even suppose you have a good validator set, you have enough security, just, you know, just think of the improvements that we have made over the past few years since the blockchain started, right? The idea was that previously you had to have crypto economics to bind the node operators together. Like all the participants of this network have to be crypto economically aligned to do the right thing.
00:38:15.998 - 00:39:51.254, Speaker A: Which means that you need the, you have notions like super majority has to be correct, like how much is, how much of stake is securing, how much hash rate is there and things like that. Right? But if you think about the change that we have seen now, the correctness of execution is almost mathematically secured. They are cryptographically ensured using ZK proofs. And what this means is suddenly from the model of super majority being honest, we are going towards either honest minority in terms of optimistic change, or you are relying on cryptographic proofs for correctness of execution. Now, if you think about that, the correctness of execution is being secured, and your data availability, which is, again, we have said that it's scalable, is now secured, then they both can make very powerful partnership. And then you do not need to run your own chain to secure your own chain, to be able to run a trustless infrastructure, to run a decentralized trustless infrastructure, depending on what your requirements are. And also the other thing is there is a lot of trade offs and people are sometimes with the monolithic stat, people have been given all the powers together along with their responsibilities.
00:39:51.254 - 00:40:54.388, Speaker A: So even if I do not want decentralization, suppose I just want a way to do it trustlessly. If I do not want that in a modular stack, you can run a centralized sequencer, you can maybe have force transaction inclusion to keep it censorship resistant at some degree. And then you can create an application which can have a particular user base. But what we have been doing with monolithic stack is we are like the chain developers are the ones who have done their trade off analysis, and then the application developers cannot do the trade off analysis on their own. They're having to inherit all the properties of the chain itself. Now that might be great because you might not want to, as you might have pushed, is that you might not want the developer to actually do that. That's ok.
00:40:54.388 - 00:42:08.778, Speaker A: And that's where we think that the SDKs will also play a role. It's not going to be that the developer has to select everything, but at the same time it will be easier for an average developer to build some application on this stack. Now, going towards the pros of the monolithic system, if you can tune your entire architecture right from the network, to having the best crypto economics, to having the best virtual machine, to having the best state design, the best structure, to cryptographic guarantees, to the signature schemes, if you can actually tune the entire stack perfectly, then you can of course create a better chain. But can an average developer actually do that? Maybe. No, but maybe some people are good enough to actually create very good chains. In the end, all we need is good blockchain applications. And we think that monolithic ones have not been able to garner too much at this point.
00:42:08.778 - 00:42:13.010, Speaker A: And modular design will open up new avenues.
00:42:13.990 - 00:43:11.650, Speaker B: Yeah, I think somewhere down the line there is a misconception that modular blockchains are inherently, I think, more decentralized than monolithic chains. And I think it's incorrect, just in the sense that now that we have light clients, you have similar trust properties across, whether it's a monolithic or modular architecture. But maybe if I could reframe just some of your key points. The biggest thing was for application engineers, if they wanted to spin up their own custom environments, change the virtual machine, change some of the specific parameters around how decentralized they want it to be. That's easier to do in a modular stack versus a monolithic stack. And it just ultimately gives the developers more degree of freedom to kind of ideate and iterate on their product stack, correct?
00:43:12.190 - 00:43:15.330, Speaker A: Yes, that's correct. If I may add.
00:43:16.070 - 00:43:17.930, Speaker B: Sorry, go for it.
00:43:18.430 - 00:44:17.380, Speaker A: So, yeah, I just wanted to add that even now in the monolithic world, now if you have two applications which want to communicate within themselves, if they are inside the same chain, then great. Otherwise you have to cross trust zones. And we have seen that these cross trust zone bridges are extremely fragile. And that also leads to, if you are within this application, you get synchronous composability and so on, and maybe atomicity as well. But if you go out of this trust zone, then you have to cross trust borders. And if you are using a monolithic, if you are using a modular stack, and if you have the same data availability layer, then the roll ups on top are within the same so called security zone and then they can interoperate in a much more trustless manner.
00:44:18.000 - 00:45:08.290, Speaker B: Yeah, no, I think that's a good point as well. The trust assumptions are very important in these blockchains. Oftentimes we don't think about them until it's a little bit too late. But the modular stack definitely has the benefit of being able to have different virtual machines. And because they all settle to the same data availability layer, have the same trust assumptions and modular or monolithic blockchains, it's all just in one thing, but makes sense in terms of virtual machine design. Where do you think personally this space is headed? Do you think EBM kind of remains the dominant architecture? Do you think paralyzed virtual machines come about? I'm curious your point of view there.
00:45:09.550 - 00:46:30.180, Speaker A: No, I think in general we will see similar trends that we have seen in general operating systems and such. So this is going to probably evolve in a similar manner, is my bet. So what I mean by that is EVM has shown the way about what you can build the power of blockchain applications and so on. Bitcoin has done its job about how you can transact in a decentralized manner without any centralized authority and things like that. And what we will see is as, and when people want to build richer applications, they would want richer primitives. Now, one of the, one of the, on the other hand, you also need tooling, right? So although I can today build an extremely performant virtual machine with all the nicest of properties, if I do not have the tooling and the ecosystem around it, it's very hard to convince a developer to come there and build something. You cannot expect a developer to come and write SM level code just because your virtual machine is extremely performant.
00:46:30.180 - 00:46:58.560, Speaker A: That's not how things are going to work. So it has to go hand in hand. That is requirement for newer types of virtual machines, parallelization even, you know, things like asynchronous communication across the different applications and things like that. And once we have the infrastructure there, the tooling there, the new applications will then come interesting.
00:46:59.420 - 00:47:07.680, Speaker B: In your world, do you envision like many different variants of virtual machines, or do you think ultimately the industry will coalesce on a couple.
00:47:10.220 - 00:47:58.760, Speaker A: In my opinion, there will be not a couple, but a handful of, in the end architectures, so to say. And then we would see a lot of instantiations of those of the same architectures, of the same virtual machines. So what we think will happen is we will see the good ones, they will able to gain adoption, they will have the tooling and the ability for developers to actually use them. And once that is there new roll ups, the application specific chains will actually come up and utilize them. So you will have a lot of instantiations of maybe the handful of virtual machines which stand out.
00:47:59.260 - 00:48:31.630, Speaker B: That makes sense. Think I agree with that as well. I don't think there's going to be hundreds of thousands by any means. I think it's probably going to be a handful that win. And people do like small permutations on them, if any. But it's super fascinating in terms of, and I assuming avail is ultimately kind of agnostic. It's not only limited to the Ethereum virtual machine, it can accept any different type of virtual machines.
00:48:32.310 - 00:49:29.490, Speaker A: Yeah, so in awl we accept, as we mentioned, like it accepts data blobs. It doesn't care what those data blobs are. Right? It can be EVM style transactions, it can be just a custom transaction structure that someone wants to send. It doesn't care about, you know, what kind of signature scheme do they use, what kind of transaction compression have they used and things like that. We only care about the data blobs and the sizes of it. So yeah, so we are pretty agnostic to virtual machines, although we think that there are a few players which are making it pretty easy to have developers work with them and few nice properties to build something, something novel. And we think that they will be playing a major role in garnering more adoption.
00:49:29.790 - 00:49:31.010, Speaker B: Can you name those?
00:49:31.590 - 00:50:25.550, Speaker A: Yeah, so I think even within like we have RisC Zero, which as I mentioned makes it extremely easy to take it. We have Neil foundation has an LLVM compiler that they, anyone can use. We have Polygon Midin, which is an extremely novel type of model that it follows the actor based model and I think that is extremely scalable. It can be pretty good once it has a high level programming language to work on with. Of course we have EVM with all the good properties that we all know of. That is Solana VM, which again some of our partners are working on. So yeah, so I think all of them are pretty good.
00:50:26.770 - 00:51:12.290, Speaker B: Awesome. No, I'm very interested to hopefully see the throughput uptick once these different virtual machines come about. And I think the biggest thing maybe if kind of wrapping it all back to the user is hopefully with more throughput, cheaper costs and that cheaper cost, ultimately allowing engineers to build more interesting applications that were frankly just almost impossible to build on. Ethereum today because of the throughput is so low and the fees are so high. So I am uniquely excited about what you're building because I want to see engineers take advantage of the more throughput. I think it will spur large innovation of more interesting applications.
00:51:13.310 - 00:52:09.110, Speaker A: Yeah. And because of the large design space that this enables where, you know, different roll ups and different ways you can design your, you can have a sequencer, you can have many sequencers, you can not have a sequencer, you can have a shared sequencer, you can have a single prover, you can have a proven network, you can have like, all these different orientations make this space extremely exciting, where, you know, there are different trade offs. Some are about costs, some are about finalities, some are about, you know, the amount of trustlessness, the decentralization and things like that. So as I was mentioning, there are many levers to play with, and that is what would enable developers to make choices which make sense for them, which will make users to choose applications which make sense for them.
00:52:09.890 - 00:52:34.610, Speaker B: Yeah, 100% in terms of. So we talked going maybe back to the beginning of the conversation about the blocks and ultimately increasing velocity. We talked about avail and increasing the block size. So increasing the amount of data over time. Do you also expect to decrease block time? And if so, what are you guys ultimately trying to target?
00:52:35.670 - 00:54:00.986, Speaker A: So at this point, like you need the base layer block frequency to have, you know, subjective finality, so to say, because you want your batches to be final inside the base layer. And that becomes very apparent when you're using a base sequencing kind of a model where you do not want to have a sequencer, you want to use avail as the sequencer, and then maybe faster blocks will enable, you know, faster confirmation times to the users and things like that. But I think even if we try to push it to say something like 5 seconds, it will not increase that utility by that much. It will give them faster confirmations. But not all rollups are going to use this. Most roll ups would want to offer soft confirmation to their users to give better ux. And you can see this today, right? You can see this today where, for example, I can take names and say, like Arbitrum, they started with an inbox contract on Ethereum, using Ethereum for sequencing as well as data availability and then computing the assertions of chain and things like that.
00:54:00.986 - 00:54:34.200, Speaker A: But if you see now, you can still use the inbox contract for post inclusion, but you have an off chain sequencer which improves the UX like by a margin. Right. So which means that as a user, if you send a transaction you get immediate confirmation and that is good enough, that is good enough for most users. And that is the kind of orientations which we think will be getting pushed over and over again throughout the ecosystem when more and more adoption takes place.
00:54:34.360 - 00:54:59.586, Speaker B: Makes sense. Amazing. I've really enjoyed the conversation, enjoyed the nuance. I love talking with engineers and people that understand the intricacies of these different blockchains. It's hard and I mean you've done your PhD to understand these. So I appreciate the long form content in terms of maybe, but I didn't.
00:54:59.658 - 00:55:01.826, Speaker A: Finish it, I left it midway.
00:55:01.978 - 00:55:20.260, Speaker B: Oh, even better. That gives you more clout. But in terms of what you're maybe looking forward to, you said that avail is still on Testnet. Could you share any information of how testnet is progressing and ultimately when people can expect to use it in prod?
00:55:20.920 - 00:56:14.460, Speaker A: Yeah, so the testnet is going pretty well, as I was mentioning, we are going to launch the next phase of the Testnet, we are going to expand the Testnet, we are going to launch new features and so on along with the next phase. So that is progressing well. We are trying out different things, we are trying out more nuances more and more. We are trying to improve the performance to the security, to kind of take the optimizations to another level. And we are trying to reach mainnets sometime late Q three or Q four this year. So yeah, that's when people can use it in production. But if someone wants to use it today and just give us some feedback about how they like it, they dislike it.
00:56:14.460 - 00:56:21.516, Speaker A: What are the integration points, how they would want to see avail and integrate with it? Happy to check.
00:56:21.668 - 00:56:48.260, Speaker B: Amazing. Sooner than I guess, than people probably are expecting. Q three or Q four is not too far away in terms of what you're looking forward to in the industry, outside of avail directly, what things get you the most excited? What are you looking forward to either on the scaling front, either on building applications, the user adoption, what excites you about the industry?
00:56:49.480 - 00:58:59.836, Speaker A: So I think in general, I know I've been talking about modular and so on, but in general what we want to do as a principle is we want to scale trust in the sense that if you see existing systems outside of blockchain, that is one of the main key pain points and that is why blockchain became so interesting to us, is how you can have, you do not trust but verify things like that. You do not have to rely on a particular group or a centralized authority to be able to censor you, to be able to, you know, to be able to, you know, shut down the network and things like that. So in general, what, what we think is in when we see newer developments which allow people to take an existing primitive, I wouldn't say an existing application, but every application has a few core primitives, right? So, for example, if we want to think of, you know, sending transactions, that's essentially a ledger. And if you can have a ledger as a trustless primitive possible inside this architecture, and then you can have something like, say, an escrow and then an auction and things like that. These are primitives which are used very frequently and very adeptly because things have evolved over a lot of time in the traditional web space. And if we can take these primitives and enable them in a trustless manner inside our ecosystem, then developers are going to use them and build on top of them. So whenever someone comes with a potential to build something new, to have something which is not possible in the existing design, that really excites us.
00:59:00.028 - 00:59:20.120, Speaker B: That's amazing. Yeah. The things that are being built that are not possible, but possible kind of, on these new primitives, I think I fully agree, are the exciting things that will ultimately push forward the industry, get us many more users, and hopefully the adoption that we've all built this infrastructure to support.
00:59:21.690 - 00:59:22.710, Speaker A: Absolutely.
00:59:23.010 - 00:59:55.050, Speaker B: In terms of maybe just wrapping up the podcast, I kind of do spicy questions, and then maybe I'll open the mic to you and see if you have any questions for me. Maybe. One spicy question that I asked in the past was, what blockchains, I think, do you think or ecosystems do things uniquely well that you maybe admire and kind of are copying bits and pieces of? And what architectures or designs do you think are just not going to be around long term.
00:59:58.510 - 01:00:14.062, Speaker A: It'S very hard to bet against, but at the same time, I can maybe say what we got inspired with, for example, the original work by Mustafa and Vitalik Leger and so on. They pushed us.
01:00:14.086 - 01:00:14.850, Speaker B: Straightforward.
01:00:15.800 - 01:01:24.822, Speaker A: Yeah, yeah, absolutely. Push the space forward. We, like we started with them, they actually showed the way about how things can be done. We, of course, took some different trade offs and every design has its own trade offs, right? So there is nothing right and wrong there, but we took something else. But that inspired us a lot in general because we had been in the ethereum ecosystem, we have seen the, you know, the pain points and the good things that can be done within the ethereum ecosystem. So that is for there to stay, right? So it's not like any one of these ecosystems are going to say that from tomorrow we do not longer present. And that is one of the beauty of blockchains, right? So if you think of, and I keep giving these examples, is that if you think of a hard fork, also even within a particular chain, if there is a major, major hard fork, then people have the ability to keep running a separate fork of the chain if they find utility in it.
01:01:24.822 - 01:02:14.010, Speaker A: So maybe some utility might drop because applications keep moving from one chain to the other, from one ecosystem to the other and things like that. So some of the utility might fade and newer utilities might come in that it might be more suited for some particular use cases, but not much suited because there are other chains or other ecosystems which make it serve it better. But at the same time, as long as there is a single application and a single utility which is powerful enough, chains will run, and those ecosystems are going to survive and run. And that is because of the resiliency of what we are building. That's the resilience of blockchain systems in general.
01:02:14.550 - 01:02:18.850, Speaker B: I do agree. Last question. Any questions for me?
01:02:19.950 - 01:02:35.730, Speaker A: No, I just want to, I just want to understand, what's your take on the entire modular thesis? And in general, are you excited about what the power that it brings to the table?
01:02:37.200 - 01:03:59.282, Speaker B: I mean, as I said in the Xerox podcast, I just really want users. And I think when I kind of peer behind the hood, I was just sad about how few users there are. And I think to me now, with light clients, there was some kind of, and I would say rightfully so, like the monolithic design, ultimately what they've done is have larger hardware costs, but with light clients, you can minimize that. As we talked about earlier in the podcast, to me those just seem like a simple dumb design, and I almost liked the dumbness of it. But as you said, you really have to get it right, and it has to be perfect, otherwise it's going to suck. And so where I admire the modularity stack is the fact that if they, the monolithic chains, have not chosen the correct architectures, maybe there's not enough data, the virtual machine is not optimized enough, maybe or 100% the case, then if that is true, the modular stack would be able to iterate much faster, find better solutions in the long term. I personally still lean towards the monolithic stack just because of the dumbness and simplicity of it from the user experience and the developers.
01:03:59.282 - 01:04:09.250, Speaker B: But I appreciate the design space that you're pursuing and think a lot of new innovations can come out of it and push the space forward.
01:04:11.710 - 01:04:19.886, Speaker A: Yeah, we hope to catch up after maybe a few months and then revisit 100%.
01:04:19.958 - 01:04:29.290, Speaker B: Well, again, really appreciate your time. Thank you for correcting me and really happy that we had the conversation. I think people are really going to look forward to it.
01:04:29.950 - 01:04:32.490, Speaker A: Thank you for having me. It was a great experience.
01:04:32.830 - 01:04:33.230, Speaker B: Thank you.
