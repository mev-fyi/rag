00:00:01.800 - 00:00:38.470, Speaker A: Well, today we have a very special episode. Today I'm excited to bring back Sriram of Eigen Layer. Ultimately, Shiram and I did a podcast in the past with Kevin talking about different consensus algorithms, but this episode is going to be definitely different. I think Shiram brings a very deep and nuanced point of view to blockchains, and he's putting his talents and his team's talents towards helping Ethereum scale. And so, yeah, just thank you so much for coming back, Shirom, I'm happy to have you again.
00:00:38.770 - 00:00:46.138, Speaker B: Thank you so much for bringing me back again. Super excited. Really enjoyed the last conversation. So looking forward to it.
00:00:46.314 - 00:01:25.870, Speaker A: Awesome. Cool. Well, I think one of the things that I would really like to start with is just because of your expertise and background and computer science and all the research that you've done is really just start from the building blocks of blockchains more holistically. Some of the limitations that you see today, some of the limitations that will persist indefinitely and kind of your point of view, and maybe how the market will mature over time, maybe ultimately that will kind of lead us to what you and your team are building.
00:01:26.460 - 00:02:40.800, Speaker B: Absolutely. The core value proposition of blockchain is decentralized trust. So this is the essential differentiator. So you take any blockchain application, remove decentralized trust out of it, then you wouldn't call it a blockchain application at that point. So if the core value proposition of blockchain is decentralized trust, the way we see it is decentralized trust has been usually bundled with specific technologies till now. So if you look at, like, the core blockchain stack, you have some kind of a trust network, whether it be a proof of work, proof of stake, proof of space, some mechanism by which, you know, some Sibyl resistance and consensus mechanism, right? Sibyl resistance makes sure that people cannot create multiple duplicate identities, or each duplicate identity needs to have some kind of like a resource constraint, so you can't create them infinitely. And so I would say that is the trust network, right? Like the people who actually underwrite the trust of the blockchain, on top of which you have a consensus protocol.
00:02:40.800 - 00:03:34.792, Speaker B: The consensus protocol mediates how these different nodes kind of come to agreement on a common view of the world, whether it is, oh, this is the correct ledger, these transactions are valid or not, whatever the set of actions is that that particular blockchain application is supposed to be doing. On top of the consensus protocol, there may be some necessary amount of a scaling layer, which may include things like sharding you know, how things transactions are separated between the groups and so on. And on top of this, you have the virtual machine layer, right? Like you have, you know, bitcoin uses this bitcoin script, which is a simple programming interface. Or, you know, Ethereum has the Ethereum virtual machine, Solana sea level and so on. So Cosmos SDK. So each blockchain has like a potentially different virtual machine layer on top. And on top of all of this, it's applications.
00:03:34.792 - 00:05:03.258, Speaker B: So this is the kind of like a full picture of the blockchain stack. So cross network, consensus scaling virtual machine applications. And if you think through this, right, I started this with, what's the core value? Propositional blockchain is decentralized trust, which is the kind of like the base layer, the lowest level, which then like trust flows from that particular group into the consensus protocol, into the scaling layer, into the virtual machine, and into the DAX, right? So that's the kind of flow of trust. And so what this means is, if you had a new idea for how to build a new consensus protocol, if you had a new idea for how to build a better virtual machine, if you had an idea for how to do a different scalability layer, the one option that we have today is you have to create a whole new trust network, which then supplies trust to that particular consensus protocol, that particular scalability layer, that particular virtual machine. So one thing I've left out in this stack is a whole bunch of other middleware which are also required to make these applications work. These could be things like oracles, these could be things like bridges, things like event driven actions, like you want to trigger an action if such and such an event happens, call keepers. There's a whole bunch of authentication layers and a whole bunch of other things that are needed to actually make blockchains work.
00:05:03.258 - 00:05:54.922, Speaker B: Each of them also have their own trust networks, because to run an oracle like Chainlink, you need to have a chain link, decentralized network. To run a bridge, you need a decentralized relay of network, like in axelar or wormhole, and so on. So each of these middlewares, not only you have this set of the stack, which is the trust network, consensus, scaling, execution, and applications, you also have like an adjacent stack, which is applications also depend on these middlewares. And so this is kind of like a rough breakdown of the modules, of what we see in the blockchain universe today. And they all have to interoperate in order to create these applications which rely on decentralized trust. And because decentralized trust is a fundamental ingredient. Each of these new middleware also have to have their own decentralized trust.
00:05:54.922 - 00:06:29.884, Speaker B: So this is the kind of picture that I'm painting of, like how the blockchain modules look today. And the one major phase transition in blockchain comes from the era of bitcoin to the era of ethereum. So what do I mean by that? Bitcoin bundled the decentralized trust very, very tightly with what application was running on top. So in modern terms, we would say bitcoin was the first application specific blockchain. Of course it was the first blockchain.
00:06:29.932 - 00:06:30.800, Speaker A: So it's also.
00:06:31.620 - 00:07:11.170, Speaker B: But it was an application specific blockchain. And there were a bunch of application specific blockchains that were being built around the time of bitcoin, like namecoin and color coin and meta coin and whatever, a whole bunch of others. And each of them was hyper focused on one particular application. But the problem was when you focus on one particular application, you also need to create a trust network around that one application. And this is a very difficult task in general, because who is going to underwrite decentralized trust? To me, decentralized trust appears like a unicorn. It's a rare object. It's not to be found easily.
00:07:11.170 - 00:07:51.300, Speaker B: Trust is already hard. If you want to trust me, you should know me, you should know my antecedents and so on. But decentralized trust is a whole other piece. It's an emergent property of a community. And so decentralized trust is even harder to find. So how are each of these applications going to go and find appropriate decentralized trust? I'm just situating the picture from like 2011, 2012, where this was kind of the dominant model. And then I think what really Ethereum did is Ethereum modularized trust to some extent.
00:07:51.300 - 00:08:39.710, Speaker B: Why? Because now all that did was just made a general purpose programming platform. But by making it a general purpose programming platform, anybody could come and build applications on top. So the value flow that I envision is that applications built on top of Ethereum borrowed Ethereum trust and pay for it. I mean, they don't pay for it themselves, their users pay for it, but really it's the value flow that goes to the application. So applications sitting on a blockchain are basically borrowing trust from the blockchain and paying for it in return. And that's the general purpose smart contract programming blockchain model. They are general purpose programming language on top of which anybody can come and build applications.
00:08:39.710 - 00:08:48.046, Speaker B: And these applications themselves do not need to be trusted. I think this is a really huge transformation.
00:08:48.238 - 00:09:27.540, Speaker A: I totally agree, maybe. And I think this is a beautiful kind of setup to the entire conversation. In your beginning explanation, you kind of explained that, like, trust was kind of the core thing that flowed up. Could you break apartheid or just dive a little bit deeper into what that trust actually underlying that is, is it the node operators? Because you also mentioned the consensus and execution environment and then the applications built on top of all that. But could you go a little bit more specific into the actual trust of, say, the ethereum network, for example?
00:09:28.440 - 00:10:09.920, Speaker B: Absolutely. So depending on the underlying paradigm, I think we have different ways in which trust is modulated. And I'll give three examples. Proof of work, proof of stake, proof of space. I think these are three completely different paradigms. So proof of work, trust, right? Like, why is bitcoin trusted? Why wouldn't you get your bitcoin double spent, right? And if you go back to it, so that's what, when an end user trusts bitcoin, what they mean is, hey, I'm holding bitcoin, which means in the ledger, my bitcoin will be there unless I actually spend it. So that's the inherent trust that a user has in the platform.
00:10:09.920 - 00:11:35.814, Speaker B: So you can ask, how is this trust actually emerging? Why do people trust it? And what actually makes sure that that happens is in the bitcoin pro work world, they are saying that it's going to be very difficult for somebody to control 51% of the mining power and create a new chain which forks away your original blocks. And if that is the case, then 51% of the power is controlled by normal honest parties or rational parties, maybe then your transaction will not get forked. So if you put a lens into the bitcoin trust model, really it is due to the illiquidity of hash power markets. Imagine in a world where like, hash power is free to rent, right? Like I'll just go and rent more than 51% of bitcoin hash power. And, you know, you can ask, what is the cost of renting it, right? Like cost of renting 51% of hash power. And I can then go and basically create another chain which removes your transaction out. And it's really fundamentally, bitcoin is secure because of the illiquidity of the hash power markets of bitcoin hash, and also because the sunk cost required in actually buying bitcoin mining equipment, right? Like somebody has to go and buy the mining equipment.
00:11:35.814 - 00:12:20.330, Speaker B: And these are special purpose asics. So it is expensive to buy and acquire this hash power. Nobody is running, you know, a cloud service where you can rent more, more bitcoin mining power than 51% of bitcoin. So all of this together basically creates trust in the bitcoin network. And you could imagine like a kind of like a crazy world where like one guy comes and builds the zig and then he has 51% of the power and then he actually attacked bitcoin. But the point is, if you do all these things and if the community forks you out just by saying, you know, we know that that's a bad chain, you, huge amount of sunk cost and you have very little reward. So it makes these attacks basically untenable.
00:12:20.330 - 00:12:59.552, Speaker B: So this is, I would say the bitcoin trust model. Now you can switch to what is the proof of stake trust model. So the proof of stake trust model is actually maybe the right intermediate one is proof of space. So the best example of this is chia. Bram cohan, who's the inventor of BitTorrent, started a new project called Chia like several years back. And the idea was to use memory as the gating factor to actually participate in the network. And this is similar to proof of work, right? Because it is in proof of work.
00:12:59.552 - 00:13:46.536, Speaker B: The gating factor is computational power. In proof of space. The gating factor is like memory, SSD memory. And so you have this proof of space network, and basically if you have more than 51% of space, you can control the chain. And so that's similar to proof of work type model. The one benefit of proof of space is unlike proof of work, which is energy intensive, which means proof of work naturally gets centralized around places where energy is less expensive. There's no such natural aggregation law or centralization tendency towards proof of space.
00:13:46.536 - 00:14:36.130, Speaker B: So actually, at least as of today, that hasn't yet been discovered. And pro space Chia network is a pretty decentralized network today. So that's basically same type of model. But instead of work, space is the getting factor. Actually, if you dive into the pro, the Chia protocol, it is actually, you'd say it's a space and time, because there is a notion of time which is built in due to, due to this construct called the verifiable delay function, where you keep hashing things forward, and then that gives you like an arrow of time. And so that particular component is somewhat similar to proof of history in Solana. But basically this combination of space and time is what gates participation into the Chia protocol.
00:14:36.130 - 00:14:58.078, Speaker B: So the trust is actually coming from that. Now switch into proof of stake, like the thing that actually underwrites trust. There is especially in proof of stake protocols, which have slashing. So you can divide the proof of stake protocols into two camps. One which do slashing. Slashing can. What is slashing? Slashing is just programmatic negative incentive.
00:14:58.078 - 00:16:02.060, Speaker B: Right? Programmatic negative incentive, which means if you actually put your stake up and you're executing something and you do not comply by the rules of the protocol, then you will lose your stake. And this is a very, very powerful incentive model because you have both carrots and sticks in bitcoin, you only have carrots. You can give people rewards for behaving correctly. But if somebody behaves incorrectly, I cannot go and find out their mining protocol, definitely cannot go and find out their mining equipment and then burn them. That's not a possibility, but that's really. So one way of thinking about it is blockchain and crypto is purely in the digital world, but work is happening in the physical world, and to actually, the digital does not directly control the physical. So it's not possible for the protocol to burn your equipment, but it's possible for the protocol to burn your stake because the stake is purely digital.
00:16:02.060 - 00:17:13.980, Speaker B: And so you can think of rule of stake as like you have to put up some stake, and if you do not behave correctly, your stake will be burnt. There are a bunch of protocols like Algorand and Avalanche and Cardano and others which do not do slashing, or at least do not have slashing as a fundamental component that basically it's just an entry control, saying that you should have some amount of stake to have one vote. But we are just going to trust you with the positive incentives in the protocol, rather than enforcing explicit negative incentives. So these are different kinds of trust models, and each of them have some trade offs. And I think among this, in my understanding, the stake trust model is the sharpest for safety, because if you violate safety, you're guaranteed to lose whole bunch of money. And in particularly in a pseudonymous world, I don't think that we can rely purely on positive incentives. There needs to be positive and negative incentives in order to guide behavior into the narrow direction of what the protocol wants.
00:17:13.980 - 00:17:38.209, Speaker B: So that's kind of like a quick tour on the different trust models. And these trust models then flow into the consensus protocols, the scalability layer, the virtual machine, because the same set of nodes does all these things. And for example, slashing people can get slashed because they don't do the consensus correctly, or because they don't do the scaling correctly, or because they don't do the virtual machine execution correctly.
00:17:38.949 - 00:18:15.870, Speaker A: Makes sense. Now it's super detailed breakdown. I really do appreciate it. So maybe we walked through that trust layer. The other things that you talked about were the virtual machines and the execution environments. I want to briefly touch upon those and then jump into what you're building at eigen layer and some of the solutions that you've come up for these different things out. Kind of talking about the trust layer and moving forward to the execution and consensus.
00:18:15.870 - 00:18:33.170, Speaker A: Out of consensus and execution. Do you believe one to be more burdensome or a more difficult challenge? And the scaling architecture of kind of bringing blockchains to the option? I would love to kind of just get your thoughts on both.
00:18:33.590 - 00:19:19.890, Speaker B: Absolutely. I think this is really fascinating that you put the question like this, because as we start building a more modular world, we need to understand what the criticality of each of these modules is and where the problems are in order to actually scale them up. Okay, so let's zoom out a little bit and see what guarantees we are actually expecting of a blockchain. So, the first one is, I would say, reorg resistance. So, Reorg resistance basically means, you know, if my transaction is included, it doesn't get reorganized away. So something that's in the ledger remains in the ledger. Right?
00:19:19.920 - 00:19:20.470, Speaker A: Right.
00:19:20.630 - 00:20:00.284, Speaker B: In a theoretical sense, this we call safety. So, Reorg resistance particularly means the chain that existed at time a perpetuates for, like, time B and time C in the future. So that's Reorg resistance. Then you also have the counterpart of that, which is censorship resistance. Censorship resistance means any honest party who wants to include an entry into the ledger or emit an event that needs to get absorbed into the ledger gets absorbed within some resellable time. So that's a censorship resistance guarantee. Beyond this, you also have other guarantees, like validity.
00:20:00.284 - 00:20:37.182, Speaker B: You also want to make sure that the state claimed in the chain. Not only the transactions get included and remain, but the state claimed in the chain is actually correct. If you execute that set of transactions, you get that particular state. And why is this important? This is important for things like, you know, if you're using a bridge and there is a kind of claim made by one chain, and there is a certain state or a state route that basically says that this is the state. And if it's not correct, then basically, you could steal the money, even if the transactions have not been reorganized. So, Reorg resistance. Censorship resistance, validity.
00:20:37.286 - 00:20:37.934, Speaker A: Right.
00:20:38.102 - 00:21:40.100, Speaker B: I'm going to add one more to this, which is data availability, which is the idea that, you know, what you may be including in the ledger may or may not include all the data, but you want to make sure that all the data is somehow transparent and public. For example, you may think of the chain as just comprised of headers, but then there's also block body, and the body needs to be published. And so we have reorg resistance, censorship resistance, validity and data availability. And the interesting thing is that some of these are more difficult to obtain than the others. Some properties are more difficult than the others. Okay, let me take the property that I think does not really need a blockchain, validity. You have a bunch of transactions and you want to claim that if you execute this bunch of transactions, this is the new state.
00:21:40.100 - 00:22:05.910, Speaker B: And this is actually. So the differentiating property of this kind of like claim validity is. It is solid. What do I mean by solid? Solid means that it is non fluctuating across time and space. If you do it and I do it, I execute a bunch of like, EVM transactions. What the output is, you execute a bunch of EVM transactions, what the output is are identical. So it is consistent across space.
00:22:05.910 - 00:22:42.766, Speaker B: It's also consistent across time. If we have a consistent version of like, we are running EVM version one or version 3.3 or whatever, and like, if I execute it today or like 100 years later, it's the same thing. So it is a very solid property. So what you can do is you can create many other mechanisms by which this solid property can be satisfied. You can create zero knowledge proofs or basically succinct validity proofs. I take all of these transactions and then I run the execution and I compress it and send it to you.
00:22:42.766 - 00:23:24.690, Speaker B: So that's one kind of like a mechanism. Another mechanism could be an economic mechanism where I say that if you make a claim that this is the state, you have to put some money behind it. And if your state turns out to be wrong, I'm going to take away your money. And it's possible to do both of these things because the claim that we are dealing with is solid. It doesn't not temporarily fluctuate. Okay, now we go to a property where clearly this is not true. So we talked about validity and validity ties in the model that I was explaining most closely into the execution environment.
00:23:24.690 - 00:23:58.886, Speaker B: You have a bunch of execution claims and is this correctly executed or not? And validity is one of that. And really what I'm arguing is validity does not require decentralized trust. It's actually a pretty interesting claim just by itself. Validity does not require decentralized trust. You can have cryptographic trust for validity or economic trust for validity because it's a solid object. And so I think of validity as one of those things which you really don't need a blockchain to solve. You need some other thing.
00:23:58.886 - 00:24:35.784, Speaker B: It is technology. It is things like zero knowledge proofs, things like interactive proving systems, crypto economic trust. But it's not really decentralized trust. Neither is validity. And then you go and take the one which is most contentious is censorship resistance. Why is censorship resistance most contentious? So, censorship resistance is basically the claim that all honest transactions were eventually included. And this claim is neither spatially observable nor temporarily is fluctuating in space and time.
00:24:35.784 - 00:25:18.384, Speaker B: The set of transactions I see in a distributed network may be very different from the set of transactions you see in a distributed network. Spatially different. And if tomorrow somebody else comes and says, hey, Sriram sends it a transactional. Logan sends her a transaction, it's not post facto observable or provable that you did it or you didn't do it. And the simplest example I give is me and my wife cannot really agree on what happened yesterday. We have completely different versions of who took care of the kid or didn't take care of the kidde. And so the idea that censorship resistance is a very liquid fact.
00:25:18.384 - 00:25:54.088, Speaker B: It's spatially and temporarily fluctuating. It depends on the vantage point. It's very important. In fact, to me, one of the most important functions of a blockchain is freezing this liquid transactions are coming in kind of like a liquid. And what the blockchain is doing is kind of freezing them into something solid that remains for perpetuity. So that transition resistance is an example where we need decentralized trust. So let me just zoom out a bit and see why I'm saying you need decentralized trust.
00:25:54.088 - 00:26:32.368, Speaker B: So how do we decide what happened yesterday? Right. And in natural course of events, we nominally resort to either, there should be something solid. Like, oh, there was DNA evidence. There is, you know, this forensic evidence that actually, like, freezes things in time. Or there is eyewitness accounts. Right? Like, these are really the only thing. And we can think of blockchain and decentralized trust as just, like, a mechanism to create an eyewitness account by just having enough eyewitnesses bad testimony to that event, so that you need decentralized trust.
00:26:32.368 - 00:26:49.350, Speaker B: So censorship essence is an example where decentralized trust is really required. Validity is an example where decentralized trust is not really required. And so there are others which are kind of in the middle, and I'll come to that. But any comments on these two freeing of these two?
00:26:49.730 - 00:27:03.590, Speaker A: No. So when you're speaking to censorship resistance, it's the fact that the transaction is included in the block and then finalized kind of indefinitely, and you know it's not going to be reverted.
00:27:04.220 - 00:27:05.320, Speaker B: That is correct.
00:27:05.780 - 00:28:14.880, Speaker A: The other definition, so this is kind of like post kind of block being submitted, and then it's ultimately not getting reverted. The other definition, the other hard part about, I'd say crypto more broadly, it's just like the different terms being used in different ways. And the other definition that I've personally heard for censorship resistance is kind of the ability for a certain number of nodes to ultimately start to censor transactions even prior to that block or your transaction being included into the block. So the super minority ultimately being able to control one third plus one of the stake, if that happens, then they can start messing with consensus, and that is kind of their point of view or definition. On censorship resistance, could you speak to maybe the different points of view and kind of how you arrived at the definition and maybe the mismatch between people's different point of view?
00:28:16.540 - 00:28:19.316, Speaker B: Actually, I think both are pointing to the same thing.
00:28:19.388 - 00:28:22.372, Speaker A: Okay, perfect. Make it even easier.
00:28:22.556 - 00:29:31.070, Speaker B: Maybe I'll just elaborate on that. The definition that I was using is that a transaction that is entered into the network, not into the block, but entered into the network, gets by an honest party, eventually gets included into the ledger. The thing about whether it's one third honest or we need two thirds honest, or one have honest or whatever, is basically just the process by which that is happening. Right? Like transactions enter into the network. And if you had, let's say, if you needed a two third quorum signature to form a block, and if you know more than one third don't want to include the transaction, then you will never form a two third quorum which includes that particular transaction. So I think one is just a more mechanistic description and another is more just an operational description. The operational description is basically if I'm a client and I send a transaction and, you know, the transactions valid according to the current state of the blockchain, then my transaction should eventually get it, or eventually get it within some period of time.
00:29:31.070 - 00:29:35.850, Speaker B: And so I think both are, we're on the same page on definition.
00:29:36.010 - 00:29:36.830, Speaker A: Okay.
00:29:39.010 - 00:30:31.560, Speaker B: I want to kind of separate two types of censorship. And I think maybe that is where your kind of latent questions come in from. One is, you know, I'm so the way that most of these blockchains work is they're all leader based. They elect a leader, somebody who's actually creating a block, and the others attest to that block. And the way I think censorship is panning out in the wild is basically, you can think of a node which will not include the transaction on its own volition, but it will attest to blocks where other people have included the transaction. This is one kind of censoring. So censorship for inclusion, but not censorship for attestation, and another kind is censorship for inclusion and attestation.
00:30:31.560 - 00:31:36.432, Speaker B: I don't include a transaction myself, but I've also never built on a block which has that transaction. Okay? And I think to actually have censorship, we need a majority or more than one third to do censorship for inclusion and censorship for attestation. Because if you're only doing censorship for inclusion but not censorship for attestation, then as long as there is a small fraction, 5% of notes, which can include transactions, the other people will attest to it. And when they are testing to it, actually, then the transaction gets included. So right now, for example, in ethereum, there are some transactions which are censored for inclusion, but no transaction is censored for attestation. So everybody's attesting to everything else, and this is led to, you know, I would say this is the most important properties when we talk about censorship. Resistance is there's no censorship for attestation, and there's at least a small fraction of which will include these transactions.
00:31:36.432 - 00:32:22.470, Speaker B: But I want to address a more fundamental point here, which is, I think, the idea that blockchains have kind of input .2 different things. One is, you can think of a blockchain as just, like, freezing information flow, information flowing through the network, and I'm just freezing it and creating these, like, blocks. And then another one is that, you know, blockchains are enabling, like, you know, settlement of transactions or whatever, you know, settlement of value. And it's a. It's a different aspect of what a blockchain is doing. And I think the first one is much more fundamental than the second one.
00:32:22.470 - 00:33:07.590, Speaker B: So, as far as you and I are concerned, there is an entry in the ledger and an entry that says that bitcoin has been moved from one person to another person. So that's an entry in the ledger. And the important property of the bitcoin blockchain is to freeze these entries. You, for example, may not like that person and that particular bitcoin transaction and say that I don't attribute value to it, and you may never transact with that particular bitcoin. It's your freedom to do so. But that is fundamentally different from just bitcoin as a ledger, attesting to that transaction happened and just putting it on the chain. I think we should take a similar view to what blockchains actually are doing.
00:33:07.590 - 00:33:49.480, Speaker B: That's why I don't like the word even validators. Their fundamental rule is not validation. The fundamental role, like I said, validation does not require a blockchain. Validation can be done by, like, you know, zero knowledge proof, by computational integrity, either by using trusted execution environments, SGX, whatever, or using crypto economic arguments. Validation does not require a blockchain. What really requires a blockchain is things like censorship resistance, which frees transactions. And so I think we have to do a better job of like, both, you know, inside the community coming to this agreement that fundamentally what blockchain is doing is adding testimony to the flow of information.
00:33:49.480 - 00:34:52.500, Speaker B: And so I like terms like consensus nodes or even witness notes better than validator notes, because, you know, when somebody looks at it from the outside and says, oh, this guy is validating that that transaction is correct, it's not making a subjective opinion of to whether somebody has to transact with somebody else or not. They're just attesting to a certain information flow, and it's up for the other people to imbue value in it or not. Right? Like we've seen time and again, like, things have value, and then the value evaporates, because value is objective. But I think having a powerful source of objective history, which is what blockchain is really providing, is a digital hardened, digital history, is just in like the interest of the commons, in the interest of all of us, to have it be return and permanent, as opposed to, and as complete as possible. Censorship resistance makes it as complete as possible. And I think we should do a better job at communicating this both internally and outside. So the real value of blockchain services.
00:34:52.960 - 00:35:24.990, Speaker A: I agree, there's a lot of terminology. Often it's very hard to follow, and then often people use them interchangeably, interchangeably. And so it's very hard to follow along. But I appreciate definitely all the clarifications on your end. I want to switch the conversation slightly to talk more about what you're building and what the team is building at iegantlayer. And so maybe just start off. I know you have two unique products that your team is building.
00:35:24.990 - 00:35:35.006, Speaker A: Start with perhaps an overview, and then diving deep into each one of the products and ultimately the services that you're providing there.
00:35:35.198 - 00:36:46.702, Speaker B: Absolutely. So I'll start a little bit with, like, why we got into building this over the last five years I've been at the University of Washington, Seattle, where I run the UW blockchain research lab. And we've done a lot of work on consensus protocols, scalability, what, you know, other middlewares, how do you build game theoretically correct systems? Maybe it's things like data availability, but also other things. One thing we found is every time you have a new innovation, which is deeper than a smart contract, you have to go and build your own decentralized trust network. Going back to where we started this conversation, every new innovation which cannot be fit into a smart contract needs a new trust network, which is rather insane, if you ask me, because I see one of the real value propositions of blockchain being the separation of who brings trust and who brings innovation. The blockchain brings trust, and the application developer, for example, just brings innovation. They don't need to be trusted.
00:36:46.702 - 00:37:31.470, Speaker B: The blockchain needs to be trusted. It's a huge, like, acceleration to the rate of innovation. I know, for example, you are in this because you want to see technology grow as rapidly as possible. And that's the same kind of interest that I have. And one of the things we see with one of the things that enable this on blockchains is the fact that application developers don't need to be trusted. Blockchain needs to be trusted. That fact and fact, unfortunately, flips when you are a creator of a new consensus protocol, a new middleware, a new virtual machine, because now not only you have to be trusted, it's actually you have to create decentralized trust.
00:37:31.470 - 00:38:19.844, Speaker B: To create decentralized trust is like much, much higher bar than to you being trusted. And so it actually flips the equation completely. If you had to create a new innovation at a consensus layer or virtual machine or a middleware, the barrier to entry is enormous. And I was very frustrated with this over the period of time that I was actually a professor and trying to create new techniques. Not only I was seeing our techniques didn't make it in, I was seeing this, all these other really amazing ideas that people just badly write a paper or maybe just write a blog post, and then there's no way for that to get converted. They even write programs and working systems. They can't be deployed with the same ease.
00:38:19.844 - 00:39:10.870, Speaker B: Forget the same is even much more difficulty than building applications. I think this is the dominant reason why infrastructure in crypto and blockchains have been very, very slow, is because each new infrastructure innovation requires them to build a new trust network. I think this is completely untenable. So one thing I've been obsessed with is how do we leverage existing large trust networks and build new innovations on top without us having to create decentralized trust, which, like I alluded to, is a very difficult community and social project, as different from engineering of these distributed systems, which is a completely technical problem. So, yeah, go on.
00:39:11.170 - 00:39:17.130, Speaker A: No, a great setup, I would say. Just keep diving deeper.
00:39:17.290 - 00:40:20.472, Speaker B: Yeah. Okay, so you have this decentralized trust as being too tightly bundled with technology. And you can ask, can we create marketplaces of decentralized trust, flexible decentralized trust, where you can just sell decentralized trust to anybody who's interested in buying it? So can we create, can we leverage these existing cross networks and build any innovation that we want? So this has been one of our driving questions, and it led us into like a variety of different projects, which you know and talk about later, but the one that I'm kind of devoting all our energy to is actually building Eigen layer. What is Eigen layer? Eigen, in German for your own Eigen layer is basically your own layer. You can build anything you want on top of this trust network. Which trust network? We chose Ethereum because of two reasons. So we looked at what are the largest trust networks.
00:40:20.472 - 00:41:26.580, Speaker B: It's basically bitcoin and ethereum. And it turns out that proof of stake trust is just more programmable because of the negative incentives that you can attach to it. And also when you have a proof of stake network, which has general purpose programmability like the EVM does, then you can start building very, very general trust models. So what do I mean by this? What we want to do is we want to take the Ethereum trust network, the proof of stake nodes, and the inherent trust in it, and offer it to anybody who wants to build a new service on top of this trust network. The simplest version of it would be actually the same set of nodes are also running my other software. That would be the simplest version of it. The same set of nodes that are running Ethereum are also running my new Oracle or data availability or authentication service, or my new chain, whatever that new object is, you basically have the same set of end nodes to do it.
00:41:26.580 - 00:42:57.590, Speaker B: But that is actually not sufficient. Why? You can have the same set of end nodes, but their incentives on being honest in Ethereum is very different from the incentives on being honest in your other network. So not with the basic premise can we get the same nodes to do this other thing. Yeah, you can potentially get the same notes to do the other thing, but that does not mean you're getting the same trust model because the economic incentives associating with the first network on Ethereum could be very different from the economic incentives on the second network, whatever other thing you're building on top. So we can then say, going back to some of our earlier discussion, what is the root of trust of Ethereum? Root of trust of ethereum comes from staking the fact that you put on some capital and you're using it as a gating barrier, as a barrier to entry into the system. But also you're using it as a negative incentive because if you don't behave correctly, you're liable to lose your stake. And so the first one we already absorbed because it's the same set of nodes being doing this other task, also the second one, which is the slashing trust, also if you can absorb, which means you will get slashed as an Ethereum staker, not only if you behave improperly on the Ethereum protocol, but also on your other protocol that you're running on top of this network, the Eigen layer network, then essentially you have transferred trust from Ethereum to this other network.
00:42:57.590 - 00:44:21.848, Speaker B: So that's the core idea of Eigen layer is this principle we call restaking. You stick your eat into the Ethereum core protocol, into the core smart contracts, and you can supply that trust by like opting into additional slashing conditions voluntarily, right? It's not imposed on everybody who's running Ethereum, but any subset of the Ethereum stakers are actually interested in running the Eigen layer protocol can opt in by saying yes in a smart contract, and that smart contract then forces them or imposes on them additional slashing conditions. But why would they do all of this? Is because they're getting additional positive incentives, which is, oh, because I'm validating this other network, or oracle, or data availability or authentication or keepers. I'm going to get additional fee for people who want to use that service and I'm accruing that fee and but I'm also taking on this additional slashing risk that, okay, actually if I may misbehave on that, I'll actually lose my stake. So that's the high level of like why we started building this project. Because every new distributed systems innovation needs a new trust network. And if we could leverage a homogeneous existing trust network and then build new innovations on top, our prediction is the rate of permissionless innovation will go up massively.
00:44:21.848 - 00:44:33.030, Speaker B: Because now if you want to build a new oracle, a new bridge, a new chain, a new consensus protocol, a new virtual machine, you just innovate on top of this network, as opposed to requiring a whole new network.
00:44:33.850 - 00:45:38.480, Speaker A: Perfect. If I could try to reiterate it in layman terms. Ultimately, Eigen layer decided to choose Ethereum, just because today Ethereum has the largest floor of security with highest amounts staked for Eigen layer. And what you do ultimately is allow individuals to deposit into Ethereum's proof of stake protocol, but opt in to additional staking. That is taking on slightly more risk, but you earn slightly more yield per se by taking this risk. And it allows engineers and developers to kind of have more open permissioned functionality that would not be possible otherwise. And by tapping into that floor of security, you are able to bootstrap this additional network and features with all kind of this security that Ethereum brings to the table.
00:45:38.820 - 00:46:30.058, Speaker B: That's absolutely right. I would say there are really two distinct dimensions to this security. One is the economic security, which is clearly measurable in dollars. But there's also decentralization, which is how many different nodes are running your protocol and so on. And we can kind of borrow both of them from the Ethereum protocol. And yeah, different protocols or different modules need these different features kind of differently. For example, already, if you wanted to run a chain, if you misbehave on this chain, on top of running on top of a ligand layer, if you misbehave on top of this chain, then you may lose your Ethereum stakes.
00:46:30.058 - 00:46:58.190, Speaker B: So it relies on economic security, but censorship resistance on this new chain requires decentralization because inclusion or otherwise of a transaction is not slashable in the same objective way that validity errors are slashable. So that brings two different kinds of trust, and you can borrow either the decentralization trust or you can, and you're borrowing the economic trust inherent in the net.
00:46:58.570 - 00:47:22.876, Speaker A: Perfect. Interesting slight, not tangent, but the event that ethereum somehow lose dominant market share and did not have the floor of security that it has today, is Eigen layer kind of flexible enough to be like a middleware solution for other additional blockchains?
00:47:23.068 - 00:47:34.436, Speaker B: Absolutely. I mean, the core concept of Eigen layer is it can be built on top of any blockchain. Proof of stake blockchain with general purpose programmability. So those are the two conditions that you need.
00:47:34.508 - 00:47:34.860, Speaker A: Gotcha.
00:47:34.900 - 00:47:44.916, Speaker B: Proof of stake, general waste programmability. I think proof of stake with slashing is the right particular fit because it is built on this slashing protocol.
00:47:45.108 - 00:47:45.836, Speaker A: Makes sense.
00:47:45.948 - 00:48:49.106, Speaker B: That's the set of technical conditions we need to actually build this one. In fact, we recently had a paper called trust boost, and this is a very interesting idea which says that if there are n chains and each of them have some amount of security, let's say there are n chains, each of them have $1 billion of security. Can you create a meta chain which has n billion dollars of security without actually changing the protocol on each of these end chains? And it turns out it's possible, actually what you have to do. And we actually built it. We actually did. This is an academic project, not in the agile project, but the idea is that if you have n chains, each of them have separate economic security, you can kind of tie them all together and create a meta chain which is the sum of the economic security of all of these, by running a consensus protocol on top of smart contracts. The smart contracts themselves run the consensus protocol across these states.
00:48:49.106 - 00:50:23.090, Speaker B: So there are all these complex architectures that are possible, but the reason we choose Ethereum is, like I said, the two dimensions of trust that we optimized for decentralization, trust and the economic trust. And we see that that's, you know, that's there on, in the Ethereum protocol. There's also something soft, which I think is not measurable, but I think it's equally important is the adherence of the community to certain means and principles, and which, which are beyond any individual or organization. So there is a kind of emergent behavior which comes out of, oh yeah, you know, we want to focus on decentralization, or we want to focus on, you know, verifiability and things like that. These are means that permeate into the ecosystem deep enough that if there is a change which is against these core principles, whoever pushes it, they won't get accepted. And I think the bottom up, like social structures that have been built around these means, are quite powerful. And so these are the reasons we gravitated towards Ethereum, but the technology itself is absolutely transferable to any proof of stake chain which has with slashing and has general programmability, because then you can write slashing permissions.
00:50:24.390 - 00:50:50.338, Speaker A: Very cool, very interesting. Ultimately, you and the team have kind of used your own product in the sense and building the eigen layer, data availability. Could you talk about what you're building there? And again, kind of why you decided to build it? And then from the user perspective, some of the benefits of that as well.
00:50:50.514 - 00:52:04.000, Speaker B: Absolutely. So now that you have this idea that you can take the Ethereum Trust network and build any service you want on top of it, one can ask the question, what are the important services that one should build on top of? Of course, there's one possibility is to go and build whole new chains. And one reason that I think things like that will happen, but one reason that we don't necessarily optimize for that, is because we want to see how to keep the validators in the network as light as possible. Because, you know, one of the things that can happen is now we have Eigen layer, which means like, validators can opt into all these other services. And if you run like thousand heavy chains and services on top, then, you know, the core value proposition we started with, which is, oh, you know, this is decentralized, and so on, will may go away. So we have to be careful on how much we overload on top of that network. The one way we think about it is the modular blockchain stack.
00:52:04.000 - 00:53:10.142, Speaker B: And so if you, you can just zoom out and ask, what are the core like constraints, resource constraints that nodes are operating with? And I would say that there are four fundamental resources that nodes are operating with. Number one is computation, right? Like there's certain computational power that you have. Number two is ram or memory, right? Like you have limited amount of ram and memory, and so you're limited by that. The third one is network, right? So you have a finite amount of network bandwidth, and you don't want to go beyond that. And then the fourth dimension is like historical storage. Like, how much do you want to store an archive? And the nice thing we were discussing earlier is about the relative role of these different layers. And I was saying that validity, checking whether some set of state is valid or not is actually doesn't need a blockchain, doesn't need decentralization.
00:53:10.142 - 00:53:55.992, Speaker B: This can be cryptographically proven, or it can be proven via interactive games and so on. And so once you have that, that computation or validity does not have to rely on decentralization, what happens is compute and memory no longer become dominant resources. In the decentralized network. You can have a centralized node which does ZK, proofs of every transaction that ever went through. And so fundamentally, there is no necessity for each node to also execute the entire computation and store the memory. So compute and memory do not become, are externalized out of the network and do not have to be the biting resources. So then you can ask, what are.
00:53:55.992 - 00:54:24.320, Speaker B: So then there are really two remaining resources. Compute and memory have been outsourced. So what remains is networking and historical storage. Turns out historical storage is also not a biting resource. The reason is, once you have committed to a ledger, even if one node stores the data, you can prove that that data is correct relative to the ledger. So it's not a very complex trust model. It's just a one node storing data trust model.
00:54:24.320 - 00:55:30.972, Speaker B: And if you assume there are at least ten reasonable parties storing it, at least one will have it at the end of the day. So the historical story is also non dominant. So, turns out, fundamentally at least our view of blockchain, and I think this is largely the Ethereum view too, is that the entire system is bottlenecked by networking. The entire system is bottlenecked by networking. And so, and that's the dominant resource, which constraints like, you know, scaling of blockchains. If that's the case, then you can ask, why are things bottlenecked by networking? Because even if you had, like, something which is attesting to the computational integrity, you still need something which is widely published and available. The data that you know is being used to process these cryptographic computations or crypto economic games, needs to be published widely for other people to access it, continue building on top of it.
00:55:30.972 - 00:57:37.054, Speaker B: And so, because publishing of data into the network is a requisite aspect, so it dominates the scalability for these systems. Okay, so given that a chain can be kind of broken up into these modules where like, execution is kind of off outsourced, and publication of data is in source and historical availability is done at a separate layer. So the dominant thing is publishing of data is what limits throughput and scaling of these systems. So, okay, so that brings us to how we think about building the first set of like lightweight services on top of eigenveyor, is if we can somehow have n nodes in the network, and you can publish data to these end nodes in a way that the system works even as long as the majority is honest, but every node doesn't have to download all the data, then you're actually bringing genuine scaling. So another way of saying it is you can ask if networking is the dominant constraint, you can kind of phrase the entire problem in throughput in bytes per second. You can ask, what is the system throughput in bytes per second? And what is the node requirement in bytes per second? Right? Like how many mbps nodes do you need? And how many mbps can you kind of conduct through the system? So the ratio of these two things, the system throughput in bytes per second divided by the node requirement in bytes per second, is what I call scaling, system throughput divided by node. And if you look at it just from this lens for forgetting all the other bottlenecks, computational memory and everything else, and historic storage, then you come up with like a very simple kind of characterization of what a blockchain is you have the system throughput in bytes per second and a node throughput in bytes per second.
00:57:37.054 - 00:58:14.736, Speaker B: And the ratio of these two is what we would call as scaling. And if you look at most blockchains today, this ratio, I would say pretty much all blockchains, this ratio is less than one. It looks much closer to like one by 20 or one by 40. Okay, just giving some examples. Ethereum, suppose you use the Ethereum blockchain not for doing any computation, just publish data blindly. So block size is one megabytes, and block size is like max block size per like 12 seconds. So one megabytes by 12 seconds is 83 kb/second so this is the data bandwidth of ethereum.
00:58:14.736 - 00:58:29.840, Speaker B: And what is the node requirement to run ethereum nodes, it's like two megabytes per second. So system throughput, 83 kb/second node through with two megabytes per second. So scaling ratio is whatever this ratio of these two things is.
00:58:31.140 - 00:58:39.840, Speaker A: So you don't take in costs of nodes, it's just the node total throughput divided by what it's actually doing.
00:58:40.180 - 00:59:12.172, Speaker B: So the cost ratio is another kind of like a yemenite fundamental performance metric. And we can cover that. But right now I'm just doing throughput by node, like no balance requirements. And to me, this is kind of like the fundamental scaling ratio. And when we say that a blockchain is scalable, I look at this number and say, is this like much bigger than one? Then I would say it's scalable. If it's much lesser than one, then it's not scalable. And I want this to be much bigger than one.
00:59:12.172 - 01:00:20.358, Speaker B: While not losing security. I should be able to attack a majority of the nodes, or less than a majority of the nodes, and still be able to get the system to work. So this is why some of the earlier concepts like sharding don't fully work, because there what you do is you're sending like each transaction or computation or data to a small group of nodes and saying that that group randomly sorted and that group certifies it. The problem is, what if after the group is sorted into that, holding that particular data, you corrupt them, right? Like people, when they want to corrupt, you know, anything, they corrupt it after they know who is responsible for it, right? Like, you know, they're not going to corrupt everybody in the universe and then figure out so, and then hope that, you know, they're going to get the right group of corrupt people. But you will basically say, oh yeah, these five nodes are holding my data and then I can just corrupt those guys and then my data can be corrupted after that. So the problem with sharding like that is that essentially you do not get majority resilience. You're losing out on resilience because of scaling.
01:00:20.358 - 01:01:08.136, Speaker B: I don't like systems like that. You can, you get majority honest in terms of, like, security, while you do not have, you know, you still have a lot of scaling. So this ratio that I was talking about is much greater. Okay, and what's the ideal this ratio can be? Right? You'd say, oh, what is the system throughput by node bandwidth? If all nodes contributed equally, you'd say that this ratio should be n, right? Because I'm selling a fully, almost contribute fully, which means each node is storing a distinct unit of data or whatever. Then basically you can say that this should scale like n. And so that's the gold standard. The gold standard should scale like n.
01:01:08.136 - 01:01:46.620, Speaker B: But then, you know, you say, oh, but we want this to be secure, even if, like a majority of the nodes are honest. So then you'd say, yeah, the gold standard is n by two because half the nodes may be malicious, so you don't want them in your calculations. So just aggregate, like the bandwidth of all the good nodes. So that's n over two. Okay? And then the theoretical question becomes, can you get this ratio to become as close to no two as possible? And it turns out that's entirely possible to get it close to n over two, whereas existing systems, like I said, are far less than one. This is true for ethereum, it's true for Solana, it's true for avalanche. It's true for everything.
01:01:46.620 - 01:02:29.916, Speaker B: Every system built today, the node bandwidth requirement is much higher than the system throughput because there is overheads due to peer to peer networks. You have to download the data from so many nodes, you have to upload it back, you know, and there is what I call the heat of consensus penalty, which is when you send the data through the network, the data has to propagate much before inter block time. Otherwise, you know, you get forking and all these things. So you have to have slack. And that slack is like a ten x. And so there's like a series of penalties that basically massively reduce the system throughput relative to node bandwidth. And forget the fact that these systems are fully replicated, right? Because the system's fully replicated, there's no way it will be better than one.
01:02:29.916 - 01:03:03.718, Speaker B: This ratio cannot be better. The system throughput cannot be better than a node bandwidth, but it's much lower than a node bandwidth because of all these overheads. So this is where we are today. So the gap between where these systems are and what the theory says that systems can be is like 10,000 to, like, you know, even higher. Four, five, six orders of magnitude. It's pretty insane. And, you know, just coming from wireless, okay, just to give a little bit of personal color on it, my PhD was in peer to peer wireless systems.
01:03:03.718 - 01:03:40.140, Speaker B: And in wireless, if you improve, like, 4g or 5g performance by 5%, you'll be the king. Everybody lose your protocol if you improve it by 5%. And here we are looking at systems which have, like a 10,000 or 100,000 x in performance left to be obtained. The gap between the theoretical limits and practical systems. Massive. So just to give a sense of these numbers, right, like I said, 80 kb/second is like validity. Ethereum, like, theoretically has 400,000 nodes.
01:03:40.140 - 01:03:46.820, Speaker B: Theoretically. But it's important. This, this number is, this number is very important because the.
01:03:46.860 - 01:03:53.220, Speaker A: Not all of those are full nodes, though. Sorry, not all those are of the 400,000 heavy.
01:03:53.300 - 01:04:44.810, Speaker B: Most of them are run by the same node. And really, in practice, maybe there is ten to 20k nodes, but that's not what I'm getting to. So the system design is constrained. So why are we constraining? In ethereum, the philosophy is that a home validator in Venezuela should be able to participate, and that sets the parameter at like, we need to keep the bandwidth requirement to one, two megabytes per second. Understandable, but we have no reason to say that if 10,000 nodes are sibling and they're all running in a data data center, maybe it is absolutely fair to require them to have 10,000 times one megabytes per second. It's absolutely fat. So, in fact, if you just go by the core ethic of the ethereum design, you would say that actually, the theoretical limit is there are 400,000 validated nodes.
01:04:44.810 - 01:05:26.120, Speaker B: Many of them are Sibyl and basically run on the same data center. Let's not worry about that right now. But the core design consideration is that a single home validator in Venezuela should be able to participate at one or two megabytes per second. So if I create a horizontally scale system which says that if you put in 32 e, you need to have one megabytes per second connection. If you have, like, 32 eth times 10,000, you need to have one megabytes per second times 10,000. That's just a linearly scaling constraint. Then essentially what you can do is you can start so you can ask like what is the total kind of bandwidth available? The theoretical limit of the system is one megabytes per second, times 400,000.
01:05:26.120 - 01:05:37.320, Speaker B: It's like 0.2, you know, by two is 0.2 terabytes per second. Absolutely insane scale is possible without compromising on the node requirements.
01:05:37.780 - 01:05:49.050, Speaker A: And you get that terabyte plus of data availability because, note, every single node is not downloading all the data, it's just downloading a partial part of the data.
01:05:49.180 - 01:06:34.776, Speaker B: Enabling this is twofold. One is erasure codes, where you break down, you take large portions of data, break it down into small chunks, and distribute these chunks in the right mathematical architecture so that any half of the nodes can kind of then reconstruct the data. But also due to basic validity proofs like KCG polynomial commitments, which essentially ensured that the data was encoded correctly because nobody is downloading all the data. If suppose the algorithm said, give Logan x one, x two, give Sriram X two, x three, and so on. But actually I did not do this correctly. I messed up and distributed things that confuse everybody. So that's, that would be a problem.
01:06:34.776 - 01:07:17.162, Speaker B: But the nice thing is that is a purely mathematical claim that I've distributed, that I've encoded the data correctly. And you can do a kind of like a Zk or a validity proof for it. And, you know, there's a nice way to integrate the validity proof along with the erasure code. And so that's what we are building is basically systems that integrate validity proofs with the erasure codes. You can also take the other view and say that I don't need validity proofs, I can do fraud proofs on erasure coding. So that's, you know, some other architectures build on top of it, but we can do either of the two. But we feel like the validity proofs are more direct.
01:07:17.162 - 01:07:54.880, Speaker B: And actually, it seems like the simplest use of validity proofs is because of how the structure of the two things, erasure codes and these validity proves, are both based on polynomials and they align very nicely. So that's how we get the scaling is by making sure that nodes are dividing the data into small chunks. Each node only gets a small chunk of the data and they also get a proof that that data was encoded correctly. And so using these two things, you can actually kind of scale the system.
01:07:57.020 - 01:08:03.480, Speaker A: It's very interesting. How long does it take to create the validity proof?
01:08:04.110 - 01:08:40.706, Speaker B: Yeah, so the validity proofs today are, you know, if you had a 32 core system, maybe you can crank out like 30 megabytes per second. Valerie proofs. So that's, that's where it is right now. But all the acceleration that is going into ZK proof, the hard part of it is actually accelerating a certain FFT calculation. And you know, people are building FPGA's and asics for this purpose, they will just like naturally translate to this particular problem. In fact, they're just made for this problem.
01:08:40.898 - 01:08:47.282, Speaker A: And that 30 megabytes per second, is that just like a batch transaction or.
01:08:47.306 - 01:08:52.234, Speaker B: Is that like a whole blob of data? And then. Yeah, and you can crunch it in like 1 second.
01:08:52.282 - 01:08:52.990, Speaker A: Gotcha.
01:08:53.970 - 01:09:51.260, Speaker B: The nice thing about our architecture, so this is like encoding rate constraints, constraint on one node, really like a 32 foot, but you can do this. So the architecture, the engineering architecture supplements this massively, because each roll up, imagine like there are hundreds of rollers, or thousands of rollers, each roll up has like a 32 megabyte data they need to write every second. So each roll up does their own like ZK proof, and each of them can be done in parallel, and they all send this data to the network and so on. So the engineering architecture permits of parallelization not only in multicore, but also completely across different cross boundaries, in different roll ups. Different sequences can be doing these things completely in parallel. And one of the reasons we are able to do it where other architectures don't necessarily allow you to do this, is because we separate the core attribute of data availability from consensus. So consensus gives you a total ordering of everything that happens.
01:09:51.260 - 01:10:49.286, Speaker B: And that's actually a heavy task, whereas data availability is a light task you don't need. So for example, the way our system works is each node which wants to send data to the network creates this encoding, open separate TCP connections to each of these like end nodes, and sends them their appropriate chunks. They get their signatures, sign off on the hash or commitment of this data, and immediately you aggregate all of them. You just suppose like one, this commitment is aggregate, signed by all this quorum, and either like you take an aggregate signature or a ZK proof that you've gotten these signatures and put them on Ethereum. That's the architecture. What this does is it allows you to have like hundreds of different nodes doing this in parallel, because it's very difficult to get relative ordering among these hundreds of nodes on the external network. And we don't do it all.
01:10:49.286 - 01:11:10.850, Speaker B: The ordering is done back on Ethereum in slow timescale. And so by just having the engineering architecture done correctly, it allows for massive parallelization of the, of the system. Both inside a given computer, like I was talking about 32 cores or FPGA's or asics, but also across trust zones, because they can just be done independently.
01:11:12.710 - 01:11:55.214, Speaker A: Yeah, it is fascinating. I'm super excited ultimately for the industry to go from like single threaded in many instances to multicore and parallelization. I'm surprised it's kind of taken this long as it has. But I am very excited for this transition to happen. The one kind of caveat with not this model specifically, but more just like roll ups and like ZKPs is the proving time and just latency kind of in between those. Do you see latency being an issue, or is there a huge problem for.
01:11:55.302 - 01:12:58.936, Speaker B: Like the cryptography provers? So my thesis on this is in the short term there is going to be, for all the most scalable applications, things like optimistic roll ups are going to dominate. And in the long term you probably have ZKP's catch up to the most valuable applications. But still, I think the highest throughput applications will not necessarily be running on CKP's. So that's our kind of core thesis on that, especially for complex computation, complex general purpose computation. You know, imagine you want to run like general purpose machine learning. You know, our thesis for what needs to be on a blockchain is much larger than what people are thinking of today. And in that world, you would want, firstly, very specific domain specific virtual machines for each application, like MongoDB should not be running on the same virtual machine that a game engine runs on.
01:12:58.936 - 01:14:21.026, Speaker B: It should not be the same virtual machine that, like machine learning runs on. There will be custom virtual machines for each of these specific tasks. And the optimistic robots, which are basically just economic systems, will be very powerful, and they can express a much wider choice, you know, execution environments in the short to medium term than ZK systems. But I think the high value per bit applications will go through the ZK systems even in the beginning, because they just do much sharper cryptography guarantees as opposed to economic guarantees. That's the thesis for how that would pan out. But I think between the two, I don't see like any major like application not coverable between optimistic and Zk type roll ups, because any other environment which is really nice and well parallelized actually fits very nicely as a virtual machine environment, as a roll up, in our view. In fact, the more parallelizable it is, it naturally fits in with the optimistic roll up paradigm, because you are specifying, for example, each transaction, which set of state is the touching and so on.
01:14:21.026 - 01:14:36.830, Speaker B: And that actually lends very well, for things like fraud proofs, because, you know, in a fraud proof, you have to specify which state it touches. So to us, the idea that like you have these very highly parallelizable virtual machines fits beautifully with the optimistic roll up.
01:14:38.050 - 01:14:50.614, Speaker A: And for my understanding, how do. So I think Eigen layer, the data availability you've mentioned, it was 15 megabytes per second that you were able to achieve.
01:14:50.662 - 01:15:18.640, Speaker B: That's what we're building right now. But we have a roadmap to push it all the way to the theoretical limits to terabytes. Yeah, I mean, if there is a gap to be squeezed. So the thing that is going to constrain us is actually whether there are applications that want to consume this throughput and pay for it, is what is going to determine how far we are pushing the scalability. But I actually don't see theoretically limitation.
01:15:19.220 - 01:15:28.840, Speaker A: And by consuming that throughput, you just mean the hardware to be able to store data and do the parallel processing.
01:15:30.260 - 01:16:20.310, Speaker B: Is that we do not require the hard disks to be very heavy for the system. Why am I saying this? So again, I think this is a place where the trade offs that Eigen layer is willing to take is very different from what Ethereum is willing to take. And one of the reasons we are kind of complimentary on this dimension, if you write some. So one of the views of the Ethereum core devs and like the Ethereum Core protocol, is even data which is returned into the Ethereum data availability layer. There should be somebody who stores it for perpetuity. So that's the kind of, it's not an algorithmic guarantee, it's a kind of social guarantee that like there should be enough people storing this data for perpetuity. And it's very, very different from our view.
01:16:20.310 - 01:17:24.500, Speaker B: It's not to say that we are saying that for every data item it should be like this, but we think there's a huge amount of like applications which will want to drink massive throughput, but absolutely don't care about storing data for perpetually. They only need the current state to be stored for perpetually. Not all historic like state and transactions to be stored for perpetually. So fundamentally, like the kind of volume of storage that we are talking about is they only need to store this for a couple of weeks. So, and if each node is consuming one megabytes per second in bandwidth and they're storing it for like two weeks, is basically just one megabytes per second, multiplied by two weeks, is basically the storage which I think comes out to like maybe one terabytes so as long as you have 1 tb storage per node and the node also has one megabytes per second, both of which are extraordinarily reasonable assumptions, you can actually scale this out. Basically you can do extreme horizontal scaling. You don't need vertical scaling, that's our thesis.
01:17:24.500 - 01:17:28.904, Speaker B: You don't need each node to become beefy. You can scale by like having many.
01:17:28.952 - 01:17:57.248, Speaker A: Many nodes, which are like many, many nodes, each doing a small subset of the overall work. Interesting. No, it's super fascinating. The one I would say, question mark that I still have in my mind is. So ultimately, I think Washington, we mentioned Ethereum today is doing 80 some kilobytes per second. I think with sharding it's going to do 1.3 megabytes per second.
01:17:57.248 - 01:18:17.100, Speaker A: And then with eigen layer, ultimately you're going to start with around like 15 megabytes per second and then ultimately continue to scale that. Eventually. In my point of view, to kind of inherit Ethereum security, you still have to settle back down to Ethereum at some point. And correct me if I'm wrong, how? No.
01:18:17.600 - 01:18:49.930, Speaker B: So the way our architecture works is on Ethereum. So suppose you're writing a 1gb file into like our eigen DA or data. Then on Ethereum all that goes is like a hash or a commitment of the data and an aggregate single aggregate signature that the data was published. So the load was like 200 bytes as opposed to like 1gb. So your Ethereum, in some sense, you know, in the roll up world already, this is the view. In the roll up world, Ethereum is just an adjudication layer.
01:18:50.050 - 01:18:50.626, Speaker A: Yeah.
01:18:50.738 - 01:18:56.830, Speaker B: Not an execution layer, it's an adjudication layer. And that's really what we are pushing it further into.
01:18:57.210 - 01:19:13.590, Speaker A: So theoretically, do you ever envision kind of eigen layer taking 100% of Ethereum's block space, of using that like 1.3 megabytes just as you posting back down even headers to Ethereum?
01:19:14.090 - 01:19:35.840, Speaker B: It's a great question. I don't want to push it that far. And who knows what will happen in the future. But one thing we think a lot about, because you mentioned block space, and a lot of people are trying to think through block space, I feel like it's. There's no such thing called cloud space.
01:19:37.020 - 01:19:40.884, Speaker A: You just add more. Sorry, you just add more.
01:19:41.012 - 01:20:12.358, Speaker B: You just add. That's exactly what I think is going to happen to block space. There's no such thing called block space. And pricing for like scarcity of block space is just going to go completely away because the theoretical limits tell you that as you get more notes, you can just get more performance and more security. So if it were true that as you get more nodes, you need to get either more performance or more security, then there's a kind of fundamental, like riff raff between the two. It's not the case. Limits don't tell you that.
01:20:12.358 - 01:21:17.808, Speaker B: The theoretical limits tell you that as you get more nodes, you just get more security and more like scalability, especially in this modular architecture, where the thing that I have to like asterisk heavily on this is, it's true for data availability, it's not true for other things. We don't know how to do computation like this. For computation, the only thing we know is either to do the economic game, which is optimistic roll ups, or we have to do the ZK proofs, or use some kind of computational integrity device like trusted execution environment, or SGX. But for data availability, just scaling data bandwidth, we know this most amazing thing, which just scales both like and in fact, this underpin the entire architecture of Ethereum. Why Ethereum pivoted to the modular roadmap is precisely this. You know, Vitalik phrased it like this, we know how to do scalable data availability, but we don't know how to do scalable computation. So Ethereum's modular roadmap is do scalable data availability, but outsource, like computation.
01:21:17.808 - 01:21:38.880, Speaker B: And because computation, you know how to do cryptographic integrity. And so you just like do the right thing at the right layer and like the whole system scalable. So that's the same kind of like thing. We're just taking it to the natural endgame, just like figuring out how to get rid of a lot of engineering limitations in the middle.
01:21:39.580 - 01:22:25.260, Speaker A: So how would you kind of compare and contrast kind of this point of view to, I think now the commoditization of block space is kind of what newer blockchains kind of ultimately are trying to say that there is no commodity of block space that you can add bandwidth, you can add, compute essentially indefinitely and continue to scale that as the network scales. I would say with like Aptos and Solana have kind of all taken this approach. How would you of, in your words, compare and contrast what those networks are doing compared to this approach?
01:22:26.040 - 01:23:26.588, Speaker B: I think what all three of them have focused on heavily is vertical scale bloody. By adding more power to each node, you can start scaling. And that's not trivial. There is huge amount of engineering architecture in making sure that as you get more power in a given node, the system starts scaling. But it is different from horizontal scalability, which is as you just add more nodes not inside the same trust zone. So the say sui approach is in a given trust zone, like I'm running a validator, I can add more and more nodes inside of my portion, my trust zone and then I can scale the system and if everybody else does the same. But the horizontal scaling is I think, much more compliant with the basic like ethos of blockchain which is keeping node requirements small across trust zones you can scale up.
01:23:26.588 - 01:24:26.020, Speaker B: If you can scale across trust zones, why scale it inside a trust zone? It's not necessary actually. So, but the best ideas from those architectures which can then be taken into the modular architecture where they become like very powerful parallel executed virtual machines or parallel proven ZK machines and so on. And we are very bullish on all these new virtual machines. One of the things I'm working, one of the projects I'm working with is this project called Zkmove, which tries to build move as a CK roll up on the ethereum. And we're working closely with them, very excited about things like eclipse building these, like, you know, sea level virtual machine in other environments and so on. So I mean if, you know, people ask, you know, you eat maxi or this or that and so on. And I'm a kind of open innovation, Maxi.
01:24:26.020 - 01:24:52.760, Speaker B: And I think an open innovation is continuous. Open innovation is contingent on having a large decentralized trust network. And I'm just looking at where is the plausible place where we can go from what it is today to actually have more decentralized networks but also more scalable. If you get both, then why make a trade off?
01:24:55.620 - 01:25:12.132, Speaker A: And to reiterate, kind of your version of trust layer is kind of the economic floor of security combined with the number of full nodes kind of securing that network, I think that's the right.
01:25:12.156 - 01:26:16.334, Speaker B: Way of thinking about it. And one thing I'm fascinated with a possibility on eigen layer is imagine there are applications. So like I was saying, some modules and applications rely more on economic security, and other modules and applications rely more on decentralization because if it's not provable and slashable, you cannot rely on economic trust. It doesn't mean anything. So what could very well happen is one of the problems that people talk about a lot in the Ethereum world is, hey, yeah, you have all these tens of thousands of dollars, but you know, majority of the stake is concentrated with 30, 40 whatever nodes, or if you count Lido as one thing, which I think is not a fair way of counting things, say there are three nodes or four nodes which basically take up like all of Ethereum, irrespective of how you count these things. There is still like all these thousands of nodes. You know, for example, rocket pool has thousand 500 nodes, and there are so many similar number of like home stakers and so on.
01:26:16.334 - 01:26:56.470, Speaker B: They don't add up to a lot of stake, but they add up to a huge amount of decentralization. But the problem is in the core Ethereum protocol, they are not paying specially for more decentralization. Nobody is saying that, oh, you know, you're a home staker, you're going to get like 2%. Additionally, even though there's some value to it because it's not measurable, is, since anything that's not quantifiable objectively in the protocol cannot be paid for it. So here is the beautiful thing that can happen on Eigen layer is suppose you're building a service, it's a secure multi party computation. You're distributing some kind of secrets across end nodes. And these things are not slashable.
01:26:56.470 - 01:27:34.282, Speaker B: You need decentralization, trust for it. And you may come in and say, yeah, actually I'm not going to weight all these nodes equally. I'm going to weight the decentralized nodes much more in my system. My middleware that I'm building on Eigenveil. And what you may say is, oh yeah, I know, the rocket pool quorum, maybe this like home staker quorum. I'm actually going to pay most of my fees to them and I'm going to give them like one vote each in my system. So there is a subjective expression of value that a middleware can do that a core layer like Ethereum cannot do, right? Like Ethereum cannot say, hey, home staker, you're better, or that guy is better.
01:27:34.282 - 01:28:03.416, Speaker B: This is not something that Ethereum wants to do or wants to get involved. Like Solana foundation is for example, doing some of it, like saying, yeah, actually I want it to be more geographically diverse. I want it to be like nodes, to be in different data centers. There's a subjective expression of preference. I think in Eigen layer we see middlewares as actually expressing these preferences. Concretely. They could come in and say, I'm only allowing home validators, these 3000 addresses to participate.
01:28:03.416 - 01:28:32.720, Speaker B: And then what happens is the home validators then start earning differentially. Additionally, just by the fact that they are decentralized. So you are paying for their real utility. And actually this can make the network more decentralized. This is something I am actually quite fascinated about as a possibility because subjective expression of preferences, because it is a free market, decentralized trust is bought and sold in this free market. If decentralization has a value, it will be paid for. Unfortunately, if it doesn't have any value, it won't be paid for.
01:28:32.720 - 01:28:35.100, Speaker B: And maybe it's the right way that things should be.
01:28:37.840 - 01:29:42.880, Speaker A: It is super interesting. And being able to express your own preferences, be able to stake through Ethereum, restake to Eigen layer and express those opinions directly, I think is quite beautiful. We've been talking almost for an hour and a half. I'll try to wrap it up with my spicy questions. And I think my spicy questions to you now is, I would say if you had to argue for kind of these newer blockchains coming online and why they would be more successful than the not Eigen layer approach, but the technologies that already exist today, why would that be the case? And then vice versa. If Eigen layer and Ethereum and some of these technologies that existed prior ultimately continue to supplant these newer technologies coming online, why would that be the case as well?
01:29:43.740 - 01:30:51.924, Speaker B: Yeah, I think the way I think about it is it's the surface area of innovation, which is what these new blockchains coming on, coming aboard have actually started bringing on a new surface area of innovation which was simply not possible to be expressed in the old world. Right? Like, you know, you have a better idea for how to run a consensus, how to run a virtual machine, how to tie these things together in some tight architecture. All of these things could not be done. And so that, I think is ultimately the bull case for these newer chains is basically they're bringing on much more sophisticated technology to bring into the same, as well as bringing like a plethora of like high power talent, right? Like at the end of the day, you know, talent. Like, there is this famous code. People ask, what is the best asset class? And I think talent is clearly the best class. And so there is a kind of inflow of talent because of these new chains.
01:30:51.924 - 01:31:59.090, Speaker B: And I think that is the bull case for them. Okay, if I were to flip and answer, what is the bull case for Ethereum or in particular, taking our own narrow viewpoint, the bull case for that is it. It is not, it does not require you to be backward compatible. It does not require things that are built on eigen layer to be compatible with anything you can, anything you can imagine, you can build. Like you imagine a new sui, a new aptos, you can build it on top of icon, you imagine a new celestia, you can build it on top of anything you can imagine you can build. And I think that ultimately, combined with the existing decentralized trust, social convergence, all of this gives this existing protocol more power of more surface area of open innovation. But I think where both of these.
01:31:59.090 - 01:32:57.330, Speaker B: I think one place where I would really like to see the new blockchains experiment and have not been experimenting enough is take more drastically different, like position on things like proof of stake. You know, like world kind is trying to say, oh, you know, one person, one vote, the way they went about it, and it's clearly antagonized some people and so on. But I think at the end of the day, we need many, many more such like radically departure approaches to take a view that is kind of fundamentally different from where ethereum has gone. You know, you can build an eyeliner and say that, like, okay, I can use this for other innovations, but, you know, if you had a system where one person had one identity, that may just be like fundamentally superior in a very basic way. So I think there is a scope of open innovation that has not been explored yet deeply by these other systems, and I think I hope they'll start doing it.
01:32:58.870 - 01:33:26.830, Speaker A: Wonderful, beautiful answers. I will wrap it up there. We're about a little bit over an hour and a half, but I really appreciate your time. Shiram, every time I talk, definitely learn something new and hope to keep having you on the podcast as much as possible. But no, I really wish you and your team the most success and definitely continue to follow along with the journey. So thank you again.
01:33:28.610 - 01:33:31.202, Speaker B: Chatting with you and look forward to future options.
01:33:31.386 - 01:33:32.714, Speaker A: Awesome. Thank you.
01:33:32.842 - 01:33:33.210, Speaker B: Thank you.
