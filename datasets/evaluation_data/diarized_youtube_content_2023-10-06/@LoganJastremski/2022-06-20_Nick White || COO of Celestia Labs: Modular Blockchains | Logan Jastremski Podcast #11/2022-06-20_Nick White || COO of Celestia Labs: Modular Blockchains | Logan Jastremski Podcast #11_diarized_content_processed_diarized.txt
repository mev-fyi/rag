00:00:00.320 - 00:00:01.366, Speaker A: Welcome. Thank you.
00:00:01.478 - 00:00:02.886, Speaker B: Yeah. Good to be here. Thanks.
00:00:02.998 - 00:00:34.638, Speaker A: Yeah, no, thank you. The podcast and Twitter game is super funny. I think overall, a lot of the blockchain, not debates, but talking points, are super hard to narrow down on Twitter. It's just like, quote, tweeting or retweeting or just going back and forth. I really appreciate you coming on the podcast and willing to chat, Nick. So thank you. I'm excited for the conversation.
00:00:34.734 - 00:00:48.022, Speaker B: Yeah, me too. I mean, recently there was a really long Twitter thread where we were debating tons of stuff like settlement layers, and I think one of the reply trees got to 260 or something.
00:00:48.126 - 00:00:48.486, Speaker A: Really?
00:00:48.558 - 00:00:57.750, Speaker B: Yeah. And, I mean, it's just, I think sometimes you end up just talking past each other on Twitter because, I mean, there's only so much you can fit into 280 characters, for sure.
00:00:57.830 - 00:01:27.676, Speaker A: No, it's super hard on Twitter. The more long form you have, I think, is the better, because a lot of these, especially in blockchain and crypto, the devil's in the details. And so it's very hard to get into the details within 280 characters. So being able to talk about it long form and get into the nitty gritty and even just explain, like, I mean, your technical depth and then break apart the smaller components, I think is vital.
00:01:27.788 - 00:01:47.732, Speaker B: Yeah. And also, I think for a lot of people that jump into crypto, you know, they don't start from first principles. And so I think that's also a really big thing. And even for me there, like, it's taken me a long time to actually realize what the true first principles were because I think there's a lot of sort of misconceptions out there.
00:01:47.796 - 00:01:48.676, Speaker A: I fully agree.
00:01:48.828 - 00:01:50.464, Speaker B: And so, yeah, hopefully we can.
00:01:50.572 - 00:01:59.660, Speaker A: No, I think that's a great jumping off point. If I'm a maxi of anything, I'm trying to be more of a good vibes. Maxie. And then the first principle is maxi.
00:02:02.040 - 00:02:03.000, Speaker B: You can go wrong.
00:02:03.080 - 00:02:17.472, Speaker A: Exactly. So maybe to kind of kick off the conversation, how would you describe celestia in your own words? And then maybe from there, talk a little bit more about the modularity of it.
00:02:17.576 - 00:03:26.682, Speaker B: Absolutely. So celestia is the first modular blockchain network. So its core insight is that, again, if you go back to first principles of blockchains, you realize that every chain since bitcoin has followed one specific design pattern, which we call monolithic blockchains. And thats where the protocol does every single function that is necessary to power decentralized applications under one roof, within one protocol, using one set of nodes or validators. But a modular blockchain takes the insight that actually you can break those functions into separate layers and each layer can be its own protocol, like its own blockchain, practically. But then those blockchains are stacked back on top of each other and recomposed to produce the same functionality, but actually with dramatically better results. It's more scalable, it's more decentralized, you have more flexibility, and you have something new called sovereignty, which we can also talk about.
00:03:26.682 - 00:03:41.550, Speaker B: But the core thing is that modularity is sort of like it's a new paradigm that we're trying to bring to the world so that web3 can actually reach its full potential.
00:03:42.010 - 00:04:00.190, Speaker A: No, I think that's excellent. And for the people that don't know, kind of like the individual parts, maybe start with articulating those and then how they are separated in their individual components versus say a monolithic blockchain.
00:04:00.810 - 00:05:02.382, Speaker B: So it's tricky, because a lot of people, when they learn about blockchains, they just assume that the miners or the validators do everything. And so they don't even think about some of these layers as separate components. But the components are first consensus, then data availability, which is also very misunderstood or not commonly known property. So we have to talk about that a little bit more. Settlement and then execution. So consensus is the layer of the stack that is just responsible for ordering data. So if you were trying to build a sort of distributed state machine replication thing, the first thing you need to know is, well, what is the ground truth? What do we agree has happened and in what order? That's all that consensus does is basically just says, this is the order and we all agree on it.
00:05:02.382 - 00:06:13.796, Speaker B: No matter what view, no matter what node you are in the network, we all are seeing the same thing then is data availability. Data availability is this concept that in addition to having an ordering over data, we need to make sure that that data was published to the network so it's available, anyone who wanted to could download it. And this is a super nuanced and confusing thing, so we can go deeper into that. And it's also a lot of people say datability is not a problem that blockchains face, and that is true insofar as if you actually save that for later. But basically it becomes a problem when you want to scale blockchains without sacrificing decentralization. So the next layer is settlement, which is actually kind of an optional layer, but essentially settlement layer is a place for execution, which would be roll ups to post updates and resolve conflicts and verify proofs. Then of course execution is essentially where applications live.
00:06:13.796 - 00:06:41.160, Speaker B: So it's where you'd have smart contracts, it's where you'd have users transacting with each other, deFi, nfts, daos, and all the different use cases that people envision. And they're all basically stacked vertically in that order, although it's not like they're all stacked on top of each other. They all are different components that can be recombined how you want. So the developer has the flexibility to choose among those components.
00:06:41.660 - 00:06:57.990, Speaker A: The main difference between the monolithic blockchain and slesia is that slesia has the ability to ultimately break apart those where. Monolithic, you cannot break those apart. They're just kind of set standards within the individual monolithic blockchain.
00:06:58.370 - 00:08:02.146, Speaker B: Yeah. So the monoliths basically do all of that within one protocol, and they also set those standards themselves so that you can't. For example, like if I'm building on Ethereum, the protocol, the people who design the Ethereum protocol have made choices for me that I, as a developer using this protocol, can't escape from. So one big one, probably the one that's most important is that they have chosen a certain version of the EVM that is going to be the canonical virtual machine that everyone runs their programs on. But there's advantages and disadvantages to any given sort of like programming environment. And so people might want to actually build on something else, like Solana has sea level or some more like wasm based thing. Like there's a hugely, I think, underexplored area of different execution environments that can be run on blockchains.
00:08:02.146 - 00:08:11.500, Speaker B: And so that's one of the first things that the modular paradigm breaks wide open, is like, you're actually not locked into any execution environment.
00:08:11.960 - 00:08:41.548, Speaker A: No, I agree. I think it's wonderful to be able to explore all the different execution environments, and I hope we can go more into depth on that later. But I think maybe just sticking with the separation of these different topics for the moment, maybe diving deeper into the data availability, because you said that is probably the most confusing part within the entire stack.
00:08:41.684 - 00:09:56.096, Speaker B: Yeah. Data availability, as I said, is trying to solve the problem of how do I know that the data behind a given block in the chain is available, meaning that it was actually published to the network. In order to understand what this means, we have to go deeper into how blockchains work. So usually when you come to consensus when a new block is published, it's just a block header with some metadata and then a state route and maybe like a transaction route or whatever. And so the point is that block header doesn't actually contain all the data of the transactions. But in a normal blockchain, every full node and every validating node downloads a block header, and all the transaction data associated with it re executes all those transactions, and then make sure that it lines up with the data that was committed to in the block header, if that makes sense. So every full node in a monolithic chain downloads all the data, re executes every transaction, and then says, okay, check, this is okay.
00:09:56.096 - 00:10:20.356, Speaker B: Now that is a fundamentally the scalability of any protocol that follows that is fundamentally limited because each node, if you want to, let's say, now we want to increase the number of transactions that we're going to process in a given block. Now you increase two things. One is the node has to have more bandwidth to download more data.
00:10:20.428 - 00:10:24.560, Speaker A: Yep. So in this scenario, you're increasing the block size.
00:10:24.700 - 00:11:24.892, Speaker B: You're increasing the block size. Why is that not a viable way to scale? Because if you increase the block size, the number of transactions in each block, essentially, then you need to have more bandwidth to download more transactions. Then you also need more compute to basically be able to re execute all those transactions and verify the block. The whole goal of modular is to break out of this scaling trilemma, essentially, and it's something that I think essentially was not really solved until this new paradigm came about. The point is, the first step is rather than re executing every transaction, we're going to leverage rollups, fraud and validity proofs to verify that the execution of the transactions is correct without having to actually do the computation ourselves. That's the first step. Now you have a block.
00:11:24.892 - 00:12:31.834, Speaker B: Now what you can do is you can have a quote lazy ledger, which was the original white paper behind Celestia, where rather than now all the nodes in the network don't have to execute anything, but they do still have to download all the data. Now if I do increase the block size, I'm still increasing the node requirements, meaning that I will have to have a bigger and bigger computer to actually stay up with the network. So what data availability solves is that now that we've gotten rid of execution, we want to make it so that you don't have to download the full block to verify that all the block data is there. Because basically I forgot to mention, for roll up execution to be secure, you need to know, even if you can verify that there was no fraud or that the state transition was valid, you need to know that the data behind that was released. Otherwise the whole security model breaks down. And this is actually, I mean, it's true of any blockchain. If even in a monolithic chain, if a block producer proposes a block, he sends out the block header, he says, hey, please sign this.
00:12:31.834 - 00:13:22.710, Speaker B: But didn't actually release the transactions. No one's going to sign the block because they're all like, well, dude, I, we don't know what the hell is in here, so how can we say that it's valid or not? So in a trivial way, that's how data availability is solved, in monolithic chains. But the point is we want to break free from that. What data availability sampling is, is basically a way of making it so that end users or nodes on the network, rather than having to download the full block, they just download a handful of chunks, random chunks. And because of this special encoding scheme, you can have a statistical certainty that the full block is available. So you could, let's say, download 1% of the entire block data, but by downloading 1%, having 99.99% certainty that the full block is available.
00:13:23.170 - 00:14:06.950, Speaker A: Gotcha. Lot to unpack. So maybe let's kind of, I think from the data availability stuff, I think it would be kind of interesting to kind of keep on like the node topic. So one thing I'm super curious about with the data availability, the data availability sampling for Celestia, are you doing that with full nodes and light clients? Or maybe let's start with like, what are full nodes and then what are light clients, and then kind of articulate what Celestialia is doing for both.
00:14:07.330 - 00:14:58.320, Speaker B: This is amazing. And I feel like maybe we should, I should have covered this before going into all this stuff about data availability and whatnot. Because one of the core motivations, like the modular paradigm for blockchains, comes from a very deep place. And that place is that essentially it comes down to the question of what blockchains are for, what is special about them. And at the end of the day, blockchains are tools for social coordination. They are, this means of having a trust minimized environment where people can transact and build applications without having to worry about third parties. So it's like a peer to peer network.
00:14:58.320 - 00:15:41.648, Speaker B: It's like a peer to peer verifiable computing environment. So it's like this fortress where whatever happens in that blockchain, you know, it's not breaking the rules. You sign up to the rules by participating in the network, and you want to make sure that those rules are upheld. I, and the only way to do that in a trust minimized way is to actually verify the chain directly yourself. If I'm not verifying the chain myself, then I am delegating my trust to someone else who's going to say, oh, yeah, trust me, I checked wink, and I'm telling you, no one's cheating. Everything's cool. But then again, you're falling away from what the original purpose of a blockchain is.
00:15:41.648 - 00:15:59.684, Speaker B: So how does that relate to full nodes versus lite clients? So in any blockchain, there's two classes of nodes. And I'm not talking about, by the way, I also feel like the term node is overloaded because there's miners and then there's validators.
00:15:59.772 - 00:16:03.020, Speaker A: I mean, feel free to separate them as much as you want.
00:16:03.100 - 00:16:51.470, Speaker B: Yeah, yeah. So there's like, miners and validators are what I would call block producers. So they're the ones who build blocks and basically run consensus to agree on which is the next block that gets added to the chain. And they are nodes. They are nodes in the network, but they are non validating nodes. Those are just people like you and me who are running a node on their computer at home. Why would you do that? It's because you want to either interface with the network, sending transactions, and basically be able to interact, or also, probably most importantly, in my mind, to actually validate the chain yourself and make sure that everything is going well, because you can't.
00:16:51.470 - 00:17:54.322, Speaker B: If you don't verify the chain yourself, then these block producers, the miners or validators can break the rules without you knowing. So, like, basically every time when you're using a blockchain, there's, you've, you've signed up for certain rules. Like on bitcoin, it's like, okay, well, the rules are no one can print money and you can only spend coins that, you know, you have the private key to spend. Now, if the miners, if no one's checking what the miners are doing and looking over their shoulder and saying, like, okay, they're not breaking any of these rules that we all signed up to, then in theory, they could print money to themselves. They just like, mint a utxo out of nowhere and be like, yo, I got a billion bitcoin. You know what I'm saying? And so we're trying to. The point of having a fully validating node is that you can ensure that the miners are not misbehaving, because if they do misbehave, it's catastrophic.
00:17:54.322 - 00:18:44.026, Speaker B: Like, they, again, they break the rules of the network and they can do literally anything they want. So having a fully validating node basically means that even though you're not participating in building the blocks or coming to consensus on the blocks, you're downloading everything and you're verifying it yourself. If anything goes wrong, you're going to notice it and you're going to be like, well, I'm not going to get cheated by these guys. They're trying to pay me with money that they minted out of thin air. Like I know that that's not real, so I'm not going to get screwed over or like, I see you're going to catch any misdoing, essentially. So there's a fully validating nodes which again verify everything directly themselves. And then there's this thing called a light client, and essentially a lite client is a node.
00:18:44.026 - 00:19:19.622, Speaker B: Basically there was this kind of like acceptance in the early days that not everyone is going to have enough computing power to run a full node. It could be expensive. It also might be hard technically to even set one up. It could take forever to sync one like on ethereum instead of everyone. It was not realistic for everyone to run a full node. So as a compromise, there's this concept of a lite client where instead of validating the chain directly yourself, you just download the block headers and make sure that that is the accurate block header. And then you assume that that header is valid.
00:19:19.622 - 00:20:10.290, Speaker B: Basically you make an assumption that the block producers are a majority honest, or some people say majority correct. But again, the danger with a like client is that if there is a dishonest majority, all bets are off. Like you literally, you lose every guarantee about that chain. Like they could, as I said, print money, they could send your balance to zero, they could break everything. And so that's the key distinction. And so the core motivation of celestia and modular blockchains is that we want to make it so that everyone can fully verify the chain directly themselves. Everyone should be a first class citizen of the network and that's the future that we're designing for.
00:20:10.830 - 00:20:26.440, Speaker A: Awesome. And how so in Celestia specifically, how are you kind of breaking down the components of what nodes are doing consensus and then what are doing the data availability sampling?
00:20:27.100 - 00:20:51.998, Speaker B: So celestia is within this modular stack. It does consensus and data availability together in one protocol. Essentially when a new block is mined on the celestia network, it's added to the canonical chain. So it's like consensus. The validators sign it. So you have this finality guarantee that the ordering will not change. That's consensus.
00:20:51.998 - 00:21:45.664, Speaker B: And then on top of that, you have an ability as a node. Obviously the validators are supposed to make sure that the block is, they're not supposed to sign a block that's not available. But as I said, if they're malicious, they could sign the block. A block is not available. The beauty of data availability sampling is that a light client equivalent amount of resources, we call it a light node, can sample that block and it could be like your smartphone, and it could verify for itself that the data is available. So your phone essentially becomes a full node. Of course we call it a light node because we're trying to emphasize the fact that it requires very few resources, like, you know, running a, typically to get those kind of guarantees on a normal blockchain, you have to run like a very beefy machine.
00:21:45.664 - 00:22:12.990, Speaker B: Like if it's a Solana, it's like a very expensive, you know, you know, cloud computer or on, on Ethereum. It's probably like a pretty advanced, like at home. I don't even think you can run it on your laptop anymore, but. So that's what we're trying to, we're trying to go for, in fact, it's, it's possible. So like several of our community members, Mustafa, have run Celestia data ability sampling light nodes on their mobile phone. Fortunately, you need an Android, so I don't have an Android yet.
00:22:13.730 - 00:22:45.508, Speaker A: IOS only. Yeah, nice. And so just to make sure for everybody else, the clear part, and correct me if I'm wrong, the biggest advantage, or Celestia's key innovation was the separation of these individual stacks, but then also the ability for the like clients to act similar to full nodes to prove that the data was available on blockchain.
00:22:45.604 - 00:23:36.510, Speaker B: Yeah, and the other really interesting thing, there's like so many different benefits to this separation of consensus, data availability and execution. One of the cool things about these light nodes is that by sampling the network, they are actually contributing to the scalability of the network too. That's where we can talk more about this Bittorrent scalability property of a data availability layer like Celestia. But essentially it's like this collective effort where the more people are out there sampling data, then the bigger the block size can grow. That's the beauty of Celestia, is it doesn't have this finite block size. It can actually grow as more users and more nodes join the network.
00:23:36.890 - 00:23:43.026, Speaker A: Are those nodes, just for me, are those the light clients that are doing the data sampling?
00:23:43.178 - 00:23:48.050, Speaker B: Yeah, it includes the light clients up until any node, essentially that is so.
00:23:48.090 - 00:23:50.282, Speaker A: Light clients and full nodes both do the data.
00:23:50.426 - 00:24:24.048, Speaker B: And also the other cool thing is that there's no, any amount of sampling that you can do is beneficial. So if you wanted to, you could download the entire block, or you could just download half of the block, or one 10th of the block or 1% of the block. It's sort of in the same way that in bittorrent you contribute as much storage to the network as you have available capacity for. That's what you can do on Celestia. And then the more that people are sampling, then collectively the bigger the block can get.
00:24:24.144 - 00:24:42.232, Speaker A: Interesting. And is that from that data availability sampling standpoint, is that because there is more collective throughput, or is it just that those clients and full nodes are providing more security to the network, allowing for the block size to be bigger?
00:24:42.336 - 00:25:32.586, Speaker B: So it has to do with basically the actual scheme for data availability sampling, which is that essentially one of the important things is being able to, let's say the blockchain, sorry, the block, he allows everyone to sample it, or he allows you to sample it, but then he then takes it away and stops letting you sample more. You need to be able to reconstruct the block. And so if you have enough nodes who all sampled random chunks, they'll be able to then gossip around the chunks that they each have and then be able to reconstruct the block on their own. So you need to at least have a minimum number of nodes downloading a minimum amount of data to ensure against that attack, essentially.
00:25:32.658 - 00:26:03.916, Speaker A: Gotcha. That makes sense. So if you have 100 nodes and they each download 1% of the block, they can theoretically put that block back together in its entirety and make sure that that block is legitimate. Cool. Awesome. Maybe to transition slightly on, I mean, and we kind of touched upon it. Going into it on a high level is like the scalability and maybe just kick off from how celestia kind of plans to scale.
00:26:03.916 - 00:26:11.072, Speaker A: And then maybe after touch, after touching upon that, speaking a little bit more to the throughput.
00:26:11.216 - 00:27:09.780, Speaker B: Yeah, this is a really great transition. It's often mistaken, and I also made this mistake early on when I was getting into blockchain, where absolute throughput of a given protocol is equated to scalability. Right. But that's a misconception. And in fact, they're very different things because again, so throughput is just basically, what is the maximum number of transactions that this protocol can handle, essentially. But that doesn't, if you just maximize throughput without taking into consideration will anyone actually be able to verify these blocks or actually keep up with the protocol, then you're sacrificing decentralization. Again, this is where the fully validating node thing comes in.
00:27:09.780 - 00:28:02.664, Speaker B: What scalability really is, is throughput divided by the cost that it takes to verify that throughput, if that makes sense. And you can see this in a lot of different places. Let's take Ethereum, low throughput, let's say 15 transactions per second. But then the cost to verify that is, you just need a laptop. Now, if you think about Solana, yes, sure, they can process a lot more transactions, but they've also increased the cost. So in reality, is it more scalable? Slightly, because they actually did make some cool optimizations in the execution and parallelization and whatnot. But they also, a big part of their scalability boost was just straight up increasing the cost to run a node.
00:28:02.664 - 00:28:17.568, Speaker B: And so it's really important to make that distinction because people will just be like, oh, well, my protocol can do 7 million transactions per second, and it's like, yeah, but no one's going to be able to actually verify that. So it's no longer really a blockchain.
00:28:17.664 - 00:28:26.494, Speaker A: I like that definition. The amount of throughput divided by the amount or the cost that it takes to verify that throughput.
00:28:26.622 - 00:28:27.246, Speaker B: Exactly.
00:28:27.318 - 00:28:27.926, Speaker A: I like that.
00:28:27.998 - 00:29:19.266, Speaker B: Yeah. So real improvements in scalability don't come from increasing the node requirements. It comes from clever things like optimizing the execution environment, innovations like data availability sampling and roll ups and things like that. Yeah. Again, if you think about what modular blockchains do, not just the data availability, the data availability sampling makes it so it's very, very cheap to verify the consensus and data chain like Celestia. But also rollups make it extremely cheap to verify execution, because all you need to verify the execution of a roll up is just a fraud proof or validity proof. Again, that's something that a smarteende, you don't need very much computational power to be able to verify those things.
00:29:19.266 - 00:29:41.682, Speaker B: So again, a smartphone could both do the data availability sampling and verify fraud or validity proofs. And so modular blockchains don't just increase the throughput, they also actually dramatically decrease the denominator, the cost to verify gotcha.
00:29:41.826 - 00:29:56.380, Speaker A: And for like the phone example, is there certain requirements of like bandwidth or compute that that would be required to like be a contributing node to the network?
00:29:57.800 - 00:30:35.372, Speaker B: The contributing, being a contributing node to the network is, again, as much as however much you can contribute, like how much you can sample and whatnot, at least on the data layer, that's how much you can contribute. But there will be sort of like, there is a function of how big the block is, um, to how much bandwidth you'll realistically need to be able to sample everything. Um, in terms of compute, I think it's going to be to verify fraud and validity proofs. I think it's going to be, well, fraud, fraud proofs could actually be more complicated. Depends on the roll up. But most validity proofs are pretty easy to verify, is my understanding.
00:30:35.436 - 00:30:58.004, Speaker A: Okay, maybe we've touched upon fraud and validity proofs a couple times, and I'm sure we will as we continue this conversation. So maybe just in like your words, like articulate what a fraud proof is and what also is the validity proof and how those will help the blockchain overall.
00:30:58.132 - 00:31:40.692, Speaker B: Absolutely. So a lot of times I think of if you really want a simple mental model for understanding how modular blockchains work, it's really the combination of data availability sampling and fraud validity proofs. The data availability sampling piece is again how you verify that the data is available. And also implicit in this note is knowing that the transaction ordering is there. So it's also following the consensus of the chain. But then that's just one half of the picture. Right now I have ordered and available data, but what I need to do is actually execute over that data.
00:31:40.692 - 00:32:39.870, Speaker B: I need to put those transactions into my state transition function, my execution model, and basically update the state. I send a transaction, I'm trying to pay you $5 and then it will take dollar five from my account, put it in your account. And so that's basically execution. Now, as I said before, in a monolithic chain, every node has to re execute every transaction. And clearly that doesn't scale because as more transactions are processed at the same amount of time, you need to do more computation to keep up. So the roll ups, the whole design, the whole idea behind rollups is that essentially it's a way of knowing that the execution is valid without having to re execute every transaction yourself. So how does it work? There are two different approaches.
00:32:39.870 - 00:33:41.032, Speaker B: The fraud proof, which is optimistic rollups, and validity proofs, which are zero knowledge rollups. In the optimistic roll up case, as it sounds, you optimistically accept when you see a new sort of block header for that roll up, you assume that it's valid. You're like this. I trust that this sequencer is, this is the node that runs the roll up, is following the rules. But how would I catch them if they did something wrong? That's where fraud proofs come in, is you assume that there's at least one honest full node who's watching the network and re executing everything so there is one guy that's actually doing that work. He's downloading all the data behind that roll up and he's re executing every transaction. And then if he sees, hey, this dude, he posted this new block that basically doesn't add up.
00:33:41.032 - 00:34:21.399, Speaker B: Like he didn't follow the rules. Then he has a way of basically proving to that that happened. And he can send it around to all the nodes in the network and they can verify it themselves and be like, oh, okay, thank God this guy like caught this guy trying to cheat. I'm not going to listen to that block. So that's the optimistic side. And then the zero knowledge roll up side is a lot more straightforward. It's basically that the sequencer, the person operating the roll up, when they execute those transactions, they also put them into a zero knowledge circuitous that proves it comes out with a succinct proof that this update that he just made is valid and you can just verify it directly yourself.
00:34:21.399 - 00:34:27.503, Speaker B: So they don't even need someone watching them, they just need that proof. They need to generate that proof.
00:34:27.631 - 00:34:46.300, Speaker A: So with fraud proofs and validity proofs, they're more, I mean, in another term, just an optimistic roll up or a ZK roll up being posted back down to celestia, the base layer.
00:34:47.160 - 00:35:36.362, Speaker B: So this is another confusing thing. So there's a lot of different options and it depends on the design of the roll up. But I think the question you're getting to is, well, where do these fraud or validity proofs go? Who's in charge of either verifying them or circulating them? And there's a ton of different, there's a huge design space for how you might set this up, but essentially there's a few options. One is you could post it down to Celestia. However, Celestia does not have any execution on chain, so it can't verify a fraud proof or a validity proof in protocol, but you can post it there to make sure that it's available for anyone who wants to see it. So there's that option, the next option. And I should also make a distinction.
00:35:36.362 - 00:36:00.118, Speaker B: So in Ethereum, Ethereum is a monolithic chain, as it is right now, and it actually has an execution layer built in. So rollups that are built on top of Ethereum today often will rely on Ethereum's sort of the EVM to post their fraud or validity proofs and have it be verified on behalf of users.
00:36:00.254 - 00:36:05.730, Speaker A: Yes. So like the roll up chain will post that data back down to the layer one.
00:36:08.310 - 00:36:30.300, Speaker B: Essentially. One way to think about this is rather than having, I could just verify, I have a computer on my phone. Everyone's got a computer these days. They could send me the fraud proof and I could verify it on my own machine or I could have Ethereum's computer verify it for me and I can just trust Ethereum that it's valid.
00:36:30.380 - 00:36:35.364, Speaker A: Gotcha. So there's this distinction on where that is being posted or verified at the end state.
00:36:35.452 - 00:37:26.860, Speaker B: Yeah, so it's either, so when we say that someone in themselves who's running a node is verifying it, we say that it's being verified client side. Like I'm running a client of this roll up and it's verifying that proof. Or if we say that the, the fraud or validity proof is being verified in an execution layer like Ethereum, we call that as basically being like a settlement layer. Ethereum in that instance is being used as a settlement layer. Now I want to make it clear though that Celestia, you can build roll ups like execution and settlement layers on top of Celestia. So if you want to build a roll up using this paradigm, there will be lots of options for you on Celestia because a sentiment layer is just again, another execution environment.
00:37:27.720 - 00:38:06.722, Speaker A: Gotcha. Interesting. So if you don't use like, I mean, so you have to use the base layer of celestia, which is the consensus and data availability, and then you have a fraud or validity proof or optimistic or Zk rollup on top of that. If you don't use a settlement layer in between the data availability and whatever roll up of your choice, are you settling back that to slassia or are you just settling within that individual chain and just using slesia as the data availability and consensus layer?
00:38:06.866 - 00:39:14.400, Speaker B: Yeah. So in that case, where there isn't a settlement layer where you're hosting these proofs to be verified, you would basically just distribute them in the peer to peer network of your roll up. So the fraud or validity proof would just get gossiped around and your client would download it and verify it directly. And that's important because it draws a distinction between this idea of a sovereign roll up versus I guess a non sovereign roll up. And all that really is, is this roll up using, is it rely on some other roll up or another execution environment to basically, for its security? Or is it more independent like an l one like sovereign, like its own blockchain. And so in the case where you're not using a settlement layer, I, we like to call those sovereign roll ups because they're basically, though they are using Celestia for data availability and consensus. They are, for all intents and purposes, an independent blockchain.
00:39:16.660 - 00:39:30.860, Speaker A: In the independent blockchain standpoint is that you're distributing or gossiping that data to all the other nodes that are running that specific virtual machine.
00:39:31.480 - 00:39:55.462, Speaker B: Yeah. So, well, in any. So roll ups are basically like blockchains themselves, and any blockchain network has a peer to peer network, and so those nodes are connected in some sort of topology. And when a message gets gossiped, basically, it propagates to every node in the network, essentially.
00:39:55.656 - 00:40:16.330, Speaker A: And my question here is, if you have, say, a EVM optimistic roll up on top of Celestia, when you're propagating that data, is it just to the other nodes that are also running that optimistic EVM roll up? Are those nodes within that specific execution environment?
00:40:16.450 - 00:40:39.736, Speaker B: Yeah, exactly. So each roll up that, let's say, is built on Celestia is kind of its own network, but there's an overlap with Celestia insofar as they post their data and they read data from the celestia chain, maybe. Yeah, and they're also. All of those roll up nodes are also running clients of Celestia, so they're sampling and doing it.
00:40:39.808 - 00:40:57.046, Speaker A: So maybe what I think would be super interesting for everybody else listening is walk us through from a user point of view. I submit a transaction to Celestia and then just the entire stack, because then I think people, mentally, will be able to maybe follow a little bit through better.
00:40:57.198 - 00:41:23.232, Speaker B: Yeah, I feel like we should bust out the whiteboard for this, but. So, let's start with, I'm a user of a roll up, and I want to. Let's say I'm just going to pay you $5 on this roll up. I sign my. I generate my transaction. I sign it, I send it to the network, the roll up network. It gets propagated around, and everyone sees it.
00:41:23.232 - 00:42:16.740, Speaker B: And then whoever the sequencer is, he takes that transaction, and then he fits it into a roll up block, and he executes it. Right? And he updates the state of the roll up, saying he deducted $5 from my account. Now, your account has five more dollars in it. And my transaction, with a bunch of others, other payments, or whatever people are doing, gets turned into a block in that roll up. Then the sequencer would say he would call up Celestia, and basically, hey, I have this block of data that I want to publish on Celestia to make it ordered and available. I want you to come to consensus on this, and I want you guys to ensure that this data was released, he would sign basically what we call a pay for data transaction, where he would say, here's the data. Here's how big it is.
00:42:16.740 - 00:43:01.730, Speaker B: Here's how much I'm willing to pay the Celestia network to include it in a block. Then that block, which, again, has a bunch of transactions in it, gets propagated to the celestia network. And then the celestia validators and all nodes all gossip it around, and they all see it. And then whoever is the leader and the block producer for that round of consensus, and Celestia would slap that block in that roll up block inside of a celestial block, along with a bunch of other blocks of other roll up, other roll up blocks. It gets confirmed. Then all these guys, once it's confirmed, all these guys that are in the. We're now back to the roll up network.
00:43:01.730 - 00:43:23.288, Speaker B: They would sample that block, and they'd be like, oh, okay, this block was available. It also makes sure that the block is actually included in the celestial block. And then all of a sudden, they're like, okay, now, I can trust that this is valid, if that makes sense.
00:43:23.344 - 00:43:32.522, Speaker A: That makes sense. So, is the transaction final? Final once it's actually been disseminated across the celestial base now network, yeah.
00:43:32.546 - 00:43:37.042, Speaker B: Well, once it's included in block, that's when you can consider it final.
00:43:37.186 - 00:44:17.788, Speaker A: Okay, cool. Very interesting. Maybe, I mean, just touching on, like, the virtual machine or execution environment, I think could be interesting. I'm personally super interested in the non evms roll ups or execution environments, as we kind of talked about initially. I think there's a lot of interesting design space to explore there. So maybe talk about Celestia's vision or future of how rollups will be included. And I think you already did, but maybe some interesting designs within the execution or virtual machine space.
00:44:17.924 - 00:44:42.706, Speaker B: True. Yeah. Great. So I first want to frame this as going back in time a little bit and thinking about the evolution of blockchains over time. We started out with bitcoin, and bitcoin was essentially the simplest decentralized application you can think of. There's only a handful of rules. Like we talked about.
00:44:42.706 - 00:45:40.690, Speaker B: It's like you can't print money out of thin air, and you can only spend money that you have the right to spend, and all of a sudden, that's enough, it turns out, to have a decentralized money and payments system. The problem is that as people started realizing, they're like, wow, this whole blockchain thing means we can build other kinds of decentralized applications. But the thing was, in that time, in order to try to build a different decentralized application, you had to build an entirely new blockchain. So things like namecoin or Mastercoin or all these different projects, every time they wanted, or like Dogecoin, let's say even every time that you wanted to build a new application with different properties to bitcoin, you had to start a whole new network. And that's inefficient for a lot of different reasons. You got to get people to mine it, you got to coordinate everything. You have to make it valuable.
00:45:40.690 - 00:46:29.210, Speaker B: It's a huge amount of overhead to launch a network. Then ethereum came along and they said, okay, we see this as a problem. So rather than having everyone launch a new blockchain every time they want to build a new decentralized application, we're going to launch a general purpose blockchain where actually you can write any application and just deploy it as a smart contract. And you get security, you get shared security for free, basically because it's running on top of Ethereum's network. And then on top of that too, one of the best things is you get composability. So, like, all these applications can talk to each other. So that was a huge step forward in terms of being able to explore the design space for decentralized applications because you brought the cost to launch a new decentralized application down dramatically.
00:46:29.210 - 00:47:17.230, Speaker B: Now, taking a step forward, if we think about like, well, the drawback of that model is basically now everyone's stuck on one virtual machine, one blockchain, and you run into all these scalability problems. Now, sort of the next step is modular blockchains. So rather than, and also, I should start before going there now, like, as each sort of blockchain, as like Ethereum reached capacity and reached its limitations, like people didn't like the EVM or what have you. They wanted to experiment with different kinds of models. People had to build new blockchains. So you got back to square one where you're like, okay, I want to build Solana, I want to try a different execution environment that has parallelization. I want to experiment with more application specific blockchains, like cosmos.
00:47:17.230 - 00:48:16.250, Speaker B: Again, you had to spin up new chains, new consensus networks. Each time we're back to that inefficient paradigm where it's a huge startup cost. Now in modular blockchains, essentially, we've had made it so that you can actually spin up a new execution environment, a new execution chain, without spinning up a new consensus network. So again, our goal is basically to make it as cheap and easy to deploy a new blockchain, a new execution layer, as it is to deploy a smart contract on Ethereum. And I think by doing that, it's going to reduce the cost and therefore increase the experimentation of different execution environments. So that didn't answer your question, but I thought that was a very important thing because it shows this arc of development of blockchain over time and how modular blockchain fits in and is going to enable a new wave of innovation in execution itself.
00:48:17.070 - 00:48:28.776, Speaker A: So jumping off that, what are you personally excited in the execution environment or virtual machine space or what have you kind of seen starting to develop?
00:48:28.928 - 00:49:37.960, Speaker B: Yeah, there's a huge design space, so there's so many different things to play around with, and it's not just. Well, so I'll start with one, which I think is kind of a no brainer, is just parallelizing execution. So clearly, if, let's say I have 100 transactions, and I want to compute them as quickly as I can to generate or verify them, if I had to do it in serial one after the other, then obviously there's only so many that I can. It's going to take me a certain minimum amount of time. However, if I can split those transactions, the transactions that aren't dependent on each other, that don't impact each other into separate sort of like work streams, I can verify way, way, way more transactions in a shorter amount of time, essentially, or even with less, like, computing power in some sense. Like, you can tap into the fact that you can have multiple cores and like, basically, that's a huge one. And Solana actually, I think, did a really good job of that.
00:49:37.960 - 00:50:42.840, Speaker B: But there's other chains, something that I think is very similar, but has other innovations on that same topic of parallelization as fuel. So I really encourage people, they're curious to check out fuel. It's funny, Ethereum introduced this idea of account based blockchains, and bitcoin was Utxo based, and fuel actually. And it was kind of long thought that you couldn't do smart contracts in a UtxO based model, but fuel has actually figured out a way to do smart contracts in the Utxo based model. And Utxos fundamentally are extremely parallelizable because basically they're all like, all the state is sort of self contained in little containers, basically. So you know ahead of time what their, each transaction is going to interact with a set of Utxos. As long as those Utxos are not overlapping, you can basically verify separate transactions in parallel, yeah.
00:50:43.300 - 00:51:32.710, Speaker A: I'm super excited for both fuel and in general, the parallel processing. I think, as you mentioned, it should be a semi obvious step for additional throughput. I'm super excited just for the ability for people to create custom virtual machines, our execution environments, and play around with those different parameters and learn and see what works and what doesn't work. So super excited about all that. That would be very cool. Within those, I mean, maybe continuing on the execution and virtual machine side, how do you kind of see ZkSdev versus optimistic combined with the execution environment?
00:51:34.050 - 00:52:42.514, Speaker B: That's a really, really good question. And where things get very complicated. And frankly, what's possible is changing every month, practically. So the way to think about the difference between optimistic roll ups and zero knowledge roll ups is that in an optimistic roll up, the challenge is that you need to make the execution fraud provable, meaning that has to be easy for someone with a small amount of resources to be able to verify when something goes wrong. And that becomes challenging, because basically one given transaction can, in order to prove that that transaction is fraudulent, it could take a lot, it could touch a lot of state in the chain and could basically end up being very costly to verify the fraud proof. So you need to build execution models that solve that problem, or you need to build a method of doing, of generating fraud proof that solves that problem. There's been a huge, huge amounts of progress there to things like canon, that optimism is developing.
00:52:42.514 - 00:53:42.680, Speaker B: And generally this idea of interactive verification games that arbitrum sort of, I think, pioneered are some really good examples of solutions to that. And I guess we could go way more, those topics go extremely deep, but there's also single round fraud proofs, which are a different model. That's the main. But the cool thing about optimistic rollups is that you can do more generalized computation on them, because there's, there's just like, you don't have, it's very much just like normal blockchains, except you're trying to make sure that you can do fraud proof. Prod proofs. Zero knowledge roll ups are amazing, because the benefit is that, you know, if, as long as you can generate a proof to prove that it's valid, then everything else is fine. The issue is that, um, making generalized, sort of like computation fraud provable.
00:53:42.680 - 00:54:44.648, Speaker B: I. Sorry, not fraud provable, but it fit into like a ZK circuit is very challenging, and still, I think, kind of an open research question, although again, there's been like massive like, innovations in that, even in the last like six months. So the problem with ZK is that basically you end up sometimes limiting the kind of computation you can do or making it so that you have to write a whole new computer language. This is what starkware did with Cairo is they just started over and basically said the optimistic roll ups are emulating the evms, some of them, so that developers don't have to change anything. Then starkware is like, well, we're just going to build an execution thing that is designed to be stark provable. But then the downside is it's a whole new programming model and developers have to learn a whole new thing. But then the benefit is that it's natively designed for that.
00:54:44.648 - 00:54:56.220, Speaker B: So those are the trade offs, I would say. There's also the proving time thing and the cost of proving zero knowledge roll ups, which can be expensive or can be really slow.
00:54:57.290 - 00:55:47.672, Speaker A: Makes sense. No, I'm super excited for it all. In my head I try to think of all blockchains in general, I think are relatively complex. If you didn't study computer science or distributed computing, they're hard to follow along. But how I try to think of them, L2s in general, is data compression back down to use the block space as efficiently as possible so you can propagate, I mean say the same amount of data, but because you're using the block space more efficiently, you can propagate more transactions within a single block. And I'm super excited about whether it's optimistic or zk rollups or even just like the different virtual machines, like trying all the different avenues of being able to explore increasing blockchain throughput overall.
00:55:47.786 - 00:56:35.282, Speaker B: Yeah, I think I like that. I like that mental model. I would say rather than data compression, it's more like compute compression, because at the end of the day you still have to publish the transaction data to some chain, and there's limits to amount that you can compress that part. But what you're really doing, the real magic sauce of rollups, is the fact that you don't have to re execute all these transactions. So like, you know, that's literally what a zero knowledge proof is, compressed computing. It's basically like, hey look, this is the program that I ran and this is what I got. And rather than you having to run that program yourself and verify that this is like the correct output, just verify this proof and fraud proofs basically achieve the same result, but in a different way.
00:56:35.386 - 00:57:15.760, Speaker A: Yeah, no, I'm super excited about it all. Maybe going down that like decentralized or compress compute. I'm kind of interested on Celestia's kind of like fee model, like kind of breaking that apart by, how do you guys like an Ethereum per se? You're paying for gas or to be included in a block. Solana eventually is moving towards paying for a specific, specific contract. How in, like, Celestia's design, what are the fees? Is it block model? Is it state model? I'd love to learn more.
00:57:16.380 - 00:58:33.660, Speaker B: Good question. So Celestia is different from Ethereum, or most monolithic chains, where in Ethereum, you pay for gas, which is basically units of computational steps in the Ethereum virtual machine. And the more computation you want to run, the more you have to pay, essentially, in Celestia, because there's no computation, it's literally, you just pay in proportion to the amount of data that you want to put on chain. So literally, the fee will be basically a price per byte of data is the way that it will work. Now, the interesting thing, and also Celestia plans to, eventually, we probably won't ship with this, but eventually we will implement a fee model that mimics EIP 1559, because we think that's a really elegant fee model. Now, the interesting thing about Celestia, as opposed to monolithic blockchains when it comes to fees, is that you will not have this situation where once the block gets saturated, the fees spike. Well, you will you.
00:58:33.660 - 00:59:23.304, Speaker B: In theory, yes. The associate block could become full for a period of time, and the fees could start to climb in any monolithic blockchain, because there's a finite blocks of supply. There's a finite supply. As that supply gets used up and becomes more and more valuable or constrained, the price just has to go up, because that's the only way of basically allocating that resource. It's just supply and demand that results in this negative network effect, where even though, yes, more users and more applications on a given chain makes it more useful to more people, you end up also making it more expensive. That's why I think monolithic chains always run into this adoption problem, where once they get to a certain scale, it's like they can't really add more users because it's just everyone gets priced out.
00:59:23.392 - 00:59:26.056, Speaker A: Unless they increase the parameters or unless.
00:59:26.088 - 01:00:24.580, Speaker B: They increase the block size. But then if they increase the block size, they're centralized in the network and they're compromising the whole point. They're there now in Celestia, the interesting thing is the block size is not fixed. So if it ever gets full, it can be increased, and it can continually be increased so long as there's enough people in the network sampling, so we won't have this negative feedback loop of like, oh, there's too many people using the chain, so everyone's getting priced out. It has to go somewhere else instead. It's like, you know, people use this metaphor for blockchains as like, cities, and it's like, oh, well, you know, you know, Manhattan is super expensive because you can't make more land. Right? But the beautiful thing about celestia is actually, it doesn't really, it kind of breaks that analogy because essentially it's like, maybe it's more like the frontier, the US, where the colonists were kind of over here, and then every time they needed more land, they just expanded.
01:00:24.580 - 01:00:32.260, Speaker B: So it's sort of a similar kind of thing where you can just expand and generate new land for people to build on as needed.
01:00:32.340 - 01:00:45.090, Speaker A: That makes sense in this scenario that more users are coming on, more nodes are coming online. Are you specifically increasing the block size? Are you increasing like the bandwidth in between nodes or both?
01:00:46.070 - 01:00:53.970, Speaker B: Mostly increasing the block size. So I think what you're getting to is do, as you increase the block size, does it increase the node requirements at all?
01:00:54.390 - 01:01:03.614, Speaker A: Does it increase, like, do you have to up the bandwidth for the node requirements to propagate more data between the nodes because the blocks are bigger?
01:01:03.782 - 01:01:46.208, Speaker B: Not necessarily. I mean, yes and no. So I don't think propagation will be, is the bottleneck in increasing the block size? The main thing is that the end node, that end user, that you want to be able to verify the chain, as you increase the block size, the amount of data that he has to download does slightly increase. But it's basically a square. It grows at the square root of the size of the block. So it's actually like really, really, it's sub linear, essentially. So there is a slight bandwidth increase, but it's very, very minor, especially, and especially as the block gets bigger and bigger, that increase with each nominal block size increase gets less and less.
01:01:46.264 - 01:01:46.592, Speaker A: Gotcha.
01:01:46.616 - 01:01:47.112, Speaker B: Makes sense.
01:01:47.216 - 01:02:14.132, Speaker A: Very cool. Awesome. We've covered a lot, I think maybe just to recap overview, kind of started with the modular versus monolithic and kind of like the key differentiators in your point of view. Those are the consensus layer, the execution environment, data availability. Was it the three, or did you have another consensus?
01:02:14.196 - 01:02:16.916, Speaker B: Data availability, settlement and execution.
01:02:16.988 - 01:02:32.720, Speaker A: Settlement and execution. I'm curious, out of those four, what do you think? Like, maybe not for, I mean, celestia, but like, blockchains overall will be like the biggest bottleneck of like, scaling blockchains.
01:02:33.180 - 01:03:42.060, Speaker B: That's a really good question. Well, at the end of the day, so any execution layer is kind of, they could keep expanding their block size, um, but they would eventually maybe reach a point where, you know that like in theory, they're not really increasing the node requirements for someone to verify the chain. But the, the other assumptions, like there's one other honest full node become more and more limited, right? Because like the box set is so big that there's so few people who can afford to run an honest full node that they might not do it. So they, they might not want to continuously just increase the block size forever. But what they could do though is it's basically free to just spin up another execution environment in parallel. And then those two roll ups can basically have a bridge between them. But there might be more, I guess, activity on one given roll up and more people will want to execute there.
01:03:42.060 - 01:04:19.302, Speaker B: Execution within a given roll up could be sort of a scarce thing. But ultimately, if you're getting priced out of a certain execution layer, you can always migrate to another one. And ultimately, no matter what, every rollup has to post back down to the data availability layer. So in some ways the bottleneck for the overall throughput of the system will always be the data availability layer, like how much capacity that it has. But again, there's a lot of nuance. It doesn't fit into the standard model.
01:04:19.406 - 01:05:16.684, Speaker A: If you will, for sure. Now I was just curious to what you think it will be because I'm super interested in it as well. Awesome. Yeah. So covered kind of the monolithic, I mean, tech stack and also the modular, and the benefits of modular going into the difference of blockchain scalability, scalability versus throughput. Also getting into the execution environments, the nodes requirements from full nodes and light nodes, and then also the execution environments, roll ups, ZK roll ups, optimistic roll ups, like the fee structure, one thing, and maybe like on a couple more things, we also touched upon the data availability and data availability sampling, maybe just like a couple of things to wrap it up. I'm super.
01:05:16.684 - 01:05:32.940, Speaker A: As the blockchain kind of grows from state being posted to the data availability layer, where ultimately does that state get stored and how is it stored in celestia.
01:05:33.680 - 01:06:27.390, Speaker B: So one of the biggest problems, as people might be aware, for scalability of blockchains is not consensus, which is a massive misconception as actually state bloat or state growth. And it's a very nuanced problem. But essentially that is that like as more and more people use a given chain, the chain only accumulates state, meaning there's only more and more accounts or smart contracts that a node who's verifying that chain has to know about. And the issue is that as that state gets bigger, it becomes slower and slower to basically compute over that state. It's like a much more deep technical conversation. And frankly, I don't understand all the nuances of it. Only really smart core people on Geth and whatever, really totally get it.
01:06:27.390 - 01:07:22.670, Speaker B: But essentially. So that's one of the major bottlenecks of any given blockchain, is that so? For example, that's why Avalanche can spin up basically a fork of EVM and run it a lot faster in the near term because they're starting from zero state versus Ethereum can't run it faster because they already have a massive state. But eventually, Avalanche will catch up and basically run into the same scalability bottleneck. Now, Celestia thinks about this in a very different way, as a layer one, which is that basically, we want to have as minimal state on chain as possible. And one of the first things you do to minimize that is basically by excluding all execution. So there's no smart contracts, there's nothing that you can do on chain. There's literally just account balances.
01:07:22.670 - 01:07:50.000, Speaker B: And that's basically it. Oh, and staking. You have to know who's staked how much. And that's it. Essentially, Celestia has been designed to push all the state and state growth and everything up to the execution layers. So the roll ups are the ones that deal with that problem. And Celestia, essentially, no matter how big it gets, will always have a very limited size of state because there is very little.
01:07:50.000 - 01:07:54.780, Speaker B: It's just literally a ledger of who owns how many tokens.
01:07:55.160 - 01:08:17.290, Speaker A: Gotcha. Just to re articulate, because the majority of the computational and state executions are happening on the roll up. And I. You're just kind of posting the data or the proof, whether it's an optimistic or a zk, roll it back down to the data availability layer. The total amount of data is just much less in general.
01:08:17.710 - 01:08:54.364, Speaker B: So it's not about the. So I think this is a really important thing. So, state is different from sort of the transaction data in a given blockchain. So, like, each blockchain has sort of a state tree that basically is in every block, and it commits to basically what all the state is in that chain. But that state is not posted on chain. The only thing that's posted on chain is the transactions. So, in Celestia, you can think of it as like the warehouse where all the transactions get posted.
01:08:54.364 - 01:09:18.222, Speaker B: But the state itself is held on the nodes that are running the roll ups themselves. So it's not about making the amount of transaction data less, it's more. That state is like the nodes that run Celestia, because they don't validate anything, they're not responsible for keeping that state. Each roll up is, if you will.
01:09:18.286 - 01:09:18.846, Speaker A: That makes sense.
01:09:18.918 - 01:09:50.642, Speaker B: But this also kind of. Maybe we should talk about this topic, because it's also a common confusing thing, is the difference between. Yeah, like, storage and data availability. A lot of people think that, oh, well, Celestia is just like a. It's just a database. Like, why can't I use arweave or filecoin and post my roll up data there? It makes sense because the term data availability sounds a lot like data storage. Essentially, it's like, oh, well, it's available because I can store it on Arweave.
01:09:50.642 - 01:10:24.686, Speaker B: But they're very different. Like, data availability is not about storing your data. In fact, there aren't any guarantees. Celestial doesn't make any guarantees about the fact that your data is going to be around indefinitely. That's not in the same way that Ethereum. Just because you post data on Ethereum doesn't mean that that data is going to exist forever, right? Like the Ethereum protocol, that makes no guarantees that, like, the transaction history will be around 30 years from now, right? That's an orthogonal problem to any layer one blockchain. So celestial doesn't solve that problem.
01:10:24.686 - 01:10:54.662, Speaker B: What it solves is, again, that problem of I need a way to prove to other people that data that I posted is available without them having to download it. And in Arweave and in Filecoin and these sort of like data storage networks, that's not the problem they solve. You can post data to Arweave or Filecoin or whatever, but in order to actually make sure that that data is there, I still have to go down and download all that data. So it hasn't made that problem any more efficient, if that makes sense.
01:10:54.726 - 01:11:21.912, Speaker A: I see no good distinction. Awesome. And then maybe to wrap it up, and then we can go into more spicy questions. What are your kind of thoughts on security as a whole? Like shared security and non shared security, and say, the pros and cons of those different.
01:11:22.016 - 01:12:44.906, Speaker B: So glad that you asked this, because this is actually one of the biggest benefits of a modular blockchain design is that in addition to the fact that we make it easier to spin up your own chain, the chains that are built on Celestia, that share Celestia as this common consensus and data availability layer, they get something very special, which is called shared security. And essentially what that is, is the security models of those roll ups basically are overlapping in a way that makes it so they can interoperate in a trust minimized way. On top of that, they're able to pool their security. If you think about it, if I spin up a new chain, let's say it's a cosmos zone or a subnet on Avalanche, I have to actually get a bunch of stake onto that chain to secure it. My security is always basically bottlenecked by how much stake people are putting at risk. And then if you think about that broader ecosystem, let's say we have 100 different cosmo zones, and they each have their own independent stake. Their stakes are all fragmented.
01:12:44.906 - 01:13:36.262, Speaker B: So their security is all fragmented. And the problem that there's two problems. One is imagine if they could all pool their stake together into one big security layer, then they would all be as secure as basically the sum of all of their security budgets. So that you get that pooling factor, which is awesome network effect. And then on top of that, when those chains security is fragmented, when they want to talk to each other, this gets back to one of the fundamental things in blockchain is blockchains. When they want to interoperate, they have to basically have some way of verifying what's happening on the other chain. Typically, and like the best in class right now that we have, like, models like IBC, they all.
01:13:36.262 - 01:14:42.802, Speaker B: And this is like ignoring completely most bridges, which are just basically committee based bridges, like multisigs. IBC is the sort of the best in class for now. And it basically says, I'm gonna assume that any block that I receive from this other chain that is signed by the majority of validators is valid, but I'm not actually going to re execute any of the transactions, because to do that is way too expensive to do on chain. So what you're basically doing is any bridge now is between these two separate chains, is assuming that the other validator set is a major, is majority honest, and as we talked about earlier, that's essentially a lite client in that model. Now, if that chain becomes malicious, they can steal funds from my chain. They could steal any funds that are bridged over, essentially, and do all kinds of bad stuff. Now, when you expand that into hundreds of thousands of chains, the likelihood that any one chain is going to be malicious or be insecure essentially gets very, very high.
01:14:42.802 - 01:15:37.048, Speaker B: In a shared security model, not only you get this pooled thing, but each one of these chains in the same way that rather than running light clients which are insecure, you can run a full node, but with very light resources, these chains can actually run essentially light nodes of each other on chain. They're able to interoperate without making the honest majority assumption that bridge becomes ultra secure wherever the other chain basically can't lie or trick the other chain into believing something that's not true. So they can't steal stuff. And all of a sudden you have, it's sort of like a HTTPs, like you have like this secure, sort of like, like a true Internet of blockchains, but the connections between them are actually secure rather than like sort of this insecure, hackable, you know, sort of. Yeah. System that we currently have.
01:15:37.224 - 01:16:34.800, Speaker A: That makes sense. Yeah, I definitely makes sense, I think. I mean, I'll try to reticulate it again. Like on a high level, in your opinion, the non shared security is just ultimately a little bit more. I mean, it doesn't have, because whether it's a cosmosone or a subnet, they can be as minimal or maximum amount of state in each subnet or cosmos zone, a wide variety. But in celestia of security guarantees. But in celestia, just because Celestia is that shared base layer of the data availability, and you can use the light clients with the majority or all execution environments posting back down to Celestia, that shared security is ultimately what makes it stronger than these other models, in your opinion.
01:16:34.890 - 01:17:02.690, Speaker B: Yeah. So there's two sides. One is the amount of stake everyone gets to aggregate their stake so that the overall network is more secure in terms of the consensus. Like you get stronger settlement assurances with each block. Then on top of that, you have the ability for these chains to, rather than making an honest majority assumption on the other chain, when they want to connect with each other and talk to each other, they don't have to make an honest majority assumption at all.
01:17:03.190 - 01:17:05.046, Speaker A: And that would be like the bridges.
01:17:05.118 - 01:17:47.752, Speaker B: Per se, the honest majority, essentially, yeah. Any bridge, unless they share a common data availability layer, there's an impossibility result which basically says, unless you share a data availability layer, there's no way to interoperate without making an honest majority assumption on the other chain, essentially. So it solves that problem. And that's massive. I mean, if we want, like imagine, think, think about this way. Like, it's like imagine if on Ethereum rather than when I launch a new decentralized app. Like, let's say I launched my own lending protocol instead of it, like, instead of being able to trust it out of the gate because it's built on Ethereum.
01:17:47.752 - 01:18:38.888, Speaker B: Imagine if it were just separate. And then like, not only do I have to trust that, like, okay, this, you know, this application is like, this smart contract code is good, but the validators are also secure. So when I deposit my money in there, it's not like, oh, I can't just read the contract and be like, oh, yeah, I can tell I'm not going to get hacked. It's like, I also have to trust that whoever's running this app is not going to screw me over. We take that for granted because we've been building on these monolithic ethereum chains. But when we try to move into a multi chain world, which we inevitably will like, we will start to run into that exact same problem that I'm describing where it's like, oh, yeah, it looks like a great chain, everything looks good, but all of a sudden I have to trust the validators there. And that's just not a very scalable or secure way to go about things.
01:18:38.944 - 01:18:48.146, Speaker A: Because those validators don't have a lot of stake behind them. So their properties or a level of like, risk is higher.
01:18:48.258 - 01:19:33.268, Speaker B: Well, yeah, so that's a good point, which is that I also want to make sure, make it clear that we don't believe in, like, you know, there's only one chain to rule them all. We still think there's going to be. So there's going to be what we call clusters, which is essentially there will be blockchains that are built on a shared data availability layer that can talk to each other in a trust minimized way, but not, but there's still going to be independent chains that talk to each other. There's still going to be bitcoin and ethereum and a bunch of different cosmos zones and whatever. But the thing is that inherently that system is limited to a much smaller set of blockchains because those blockchains need to have a ton of economic security for those bridges to be secure.
01:19:33.324 - 01:19:43.838, Speaker A: For sure. For sure. It makes sense. I'm just trying to re articulate it correctly. Cool. I don't know. We've touched upon quite a bit, I think.
01:19:43.974 - 01:21:19.230, Speaker B: One topic that I want to also mention, before I forget about it, is this idea of defining what modular blockchains are, because there is some confusion around what is it that makes a blockchain modular or not modular, and there's especially this conflation of a modular blockchain and then modular software that is used to build a blockchain. So, like one of the precursors to Celestia's idea of separating consensus and execution was Cosmos. Basically, the people who built Cosmos had a similar insight in that you can separate consensus and execution, but they didn't know how to do that at the protocol level. Instead, they did that at the software level. So they built software. They built these two major components, tendermint, which is just like a consensus protocol, an implementation of a consensus protocol and then an interface, and the Cosmos SDK, which is basically these different modules that you can use to build, like, your execution layer. And it's really beautiful because by separating these two things, it makes it so that I can take tendermint and then I can write a new Cosmos SDK app and just deploy it and plug it into tendermint as this utility.
01:21:19.230 - 01:22:04.206, Speaker B: That is what this modular software thing is. I think Avalanche is pursuing a similar model where they have the avalanche consensus, and they have all these different execution things that you can plug in on top of it. However, when I launch a cosmos zone or an avalanche subnet, that chain is still monolithic because it's still doing consensus, data availability, settlement, and execution all in one chain, in one protocol. And same. So same with subnets. Even though they're using modular software to build the node software, the nodes are still monolithic. The protocol is still monolithic.
01:22:04.206 - 01:22:52.112, Speaker B: A really good rule of thumb that people can use to identify the difference between a monolithic chain or a modular chain is that a modular chain on its own is basically useless. Modular chains are only useful when they are combined with another modular chain. So celestia is a modular chain because all it does is consensus and data availability, and that, on its own, is totally useless. You can't do anything on Celestia. It's like, oh, I'm just going to post data all day. Well, who cares? There's no utility to that. Where it becomes, where you get utility is you get a roll up to plug into it and run execution.
01:22:52.112 - 01:23:08.440, Speaker B: So all of a sudden, with the combination of celestia, a modular consensus and data layer, with a modular execution layer, all of a sudden you get something that's useful. Therefore, celestia is a modular blockchain. Nice.
01:23:08.560 - 01:23:22.592, Speaker A: I like it. No, thank you. I'm super excited for everybody to listen to this conversation. Is there anything else? I maybe have, like, a couple of spicy questions I can just throw in at the end.
01:23:22.656 - 01:23:24.136, Speaker B: Let's do it. Let's do it.
01:23:24.328 - 01:23:29.960, Speaker A: Other things, just while you're going on a high level. If not, we can go into spicy questions.
01:23:30.000 - 01:23:34.336, Speaker B: No, I think we've covered a lot, frankly. This has been great.
01:23:34.408 - 01:24:09.464, Speaker A: Awesome spicy questions. My favorite recent spicy questions is just like surrounding the other chains and just like a couple pros and cons. So maybe Eth pros and cons, in your view, maybe if you could remove the modularity component, because I think those could be applied for, in your opinion, on the vast majority of blockchains except for Celestia. So if you remove the modular critique out of ETH, what would you say is a pro con on Avax procon and Solana Procon?
01:24:09.592 - 01:25:19.354, Speaker B: Hmm, good question. I think that Ethereum, I mean, the real benefit is frankly the network effect. I don't know that the EVM is necessarily the best execution environment or obviously Ethereum is still running on proof of work, but they will transition to proof of stake. But the network effect around the EVM with auditors and tooling and developer support is super important. And also just the network effect of all the applications and users and value on Ethereum is really, I think the main thing, and what's beautiful is that I like Ethereum because they're aware of this new modular paradigm and that is basically their entire roadmap going forward, is basically to launch data availability sampling via dank sharding and they have a bunch of roll ups already being built. I really like Ethereum, obviously because they align with the modular thing. So sorry to bring the modular thing back into it, but it's hard to ignore that because to me that's the only long term viable way to scale blockchains.
01:25:19.354 - 01:25:51.704, Speaker B: And if people are interested, read Vitalik's posts on the roll up centric roadmap for Ethereum or also his post on the end game. And I think, yeah, I think those are like really good way to understand from their perspective what that means. Solana, I like, because they, as I said, I mean they optimize for something just totally different. And I think they like philosophically they just disagree, but they don't. There's a really great debate actually between Mustafa and Anatoly.
01:25:51.752 - 01:25:53.072, Speaker A: I listen to that. I like that.
01:25:53.096 - 01:25:55.968, Speaker B: It's really, really good. Is at the modular summit, which we.
01:25:55.984 - 01:25:59.664, Speaker A: Did in Amsterdam, it needs more views. There's only like a couple hundred views on it.
01:25:59.832 - 01:26:48.364, Speaker B: I think it got a lot more views because we only released the newer. We had to cut up the video and then repost it. But I agree it still needs more views. But one of the things that came out of that was we disagree with Solana's design because as I said, they are ok with increasing the node requirements and making it so that fewer and fewer people can validate the chain directly. And then that makes it so that if someone captures a validator set, they're able to basically misbehave. So everyone that uses Solana is basically inherent, implicitly making an honest majority assumption on the validators, I think. And actually, from recent Twitter conversations, it seems like anatoly is actually trying to think about new ways of sort of like curbing that and making that less of a problem.
01:26:48.364 - 01:27:45.670, Speaker B: So it's really cool because I like that the Solana team is very open minded and pragmatic. And also, as I mentioned before, they've done a lot of really cool stuff around block propagation, the whole idea of shreds. And they use erasure coding in their block propagation. They do stuff like parallelization of the virtual machine execution. And I thought it was really cool how they're doing this pricing of transactions based on, rather than just having this big bucket where everyone who's trying to use the chain has to pay the same price, it's priced on units of what state are you trying to touch? So if everyone's all trying to mint an NFT at the same time, it's like, okay, you guys are isolated and you guys are going to pay a lot, but the people who want to do their normal stuff don't get screwed. So I don't know. I think there's a lot of really good innovation coming out of Solana.
01:27:45.670 - 01:27:54.698, Speaker B: And on avalanche, I really like the fact that they're pursuing the subnet thing because, I mean, I'm a big fan of cosmos already.
01:27:54.834 - 01:27:56.698, Speaker A: We should throw cosmos in at the end.
01:27:56.834 - 01:28:42.410, Speaker B: True. Yeah. The thing that I like about avalanche is kind of the same thing I like about Cosmos, which is that they're making it super easy for people to build their own blockchains, which I think is a really good, I think just really healthy for the ecosystem. And for now is probably the best way to scale until we get modular chains. But I think, and I like cosmos because cosmos probably more, like significantly more because I think that they have really done a good job with tendermint. It's an extremely robust consensus protocol that is powering now so many different chains, even chains that, like, you don't think of as cosmos chains. They're basically using the insights of tendermint or some flavor of tendermint.
01:28:42.410 - 01:29:39.524, Speaker B: Like, tendermint is, to me, really like the state of the art when it comes to, like, proof of state consensus. And cosmos. IBC is also state of the art when it comes to interoperability. Like, they just, the team behind Cosmos was so prescient and like, ahead of their time, like, way too ahead of their time, Frank, frankly. And they're still not recognized, I think, for the kind of work that they did. My only gripe with avalanche is essentially that the narrative that the consensus is the bottleneck for scalability is, I think, disingenuous because there's a lot of other problems like data availability or the state boat thing and all that stuff. And also the fact that avalanche consensus has some, I think, security problems around the fact that because it's actually, I mean, it's beautiful.
01:29:39.524 - 01:30:33.178, Speaker B: It's pretty crazy that you can have a network where no matter how many validators on the network in constant time, they can all basically agree on the block. That's seriously pretty magical. It gets over this issue. Of all the overhead of voting in something like tendermint, you're really limited on the number of validators you can have. Avalanche has a really cool benefit that way. But the problem though that arises is that because it's just a probabilistic voting polling rather than voting, you don't end up having what we call accountable safety. So, like, in a normal proof of stake, one of the benefits of proof of stake over proof of work is that in proof of stake, if someone is being malicious, then you'll be able to know who is.
01:30:33.178 - 01:31:26.064, Speaker B: Like, if they're double signing blocks or trying to mess around with stuff, you can see who did that because they signed a transaction and you're like, and everyone has a global view of who signed what. So then, like, when you see someone, okay, they forked the chain, but they tried to make two conflicting blocks, you're like, okay, well, I know exactly this stake double signed. And so I know to slash them. Or like, even if they're a malicious, like malicious majority, even if we have to restart the chain, at least I know who is bad and I know who to cut out. And an avalanche, unfortunately, as of now, maybe they'll make changes, but I don't know how you can make changes while still preserving this property. You can't actually know what the malicious stake is. And to me that's really problematic.
01:31:26.064 - 01:31:50.460, Speaker B: And that's why they don't have slashing, because I don't think you can't implement slashing if you can't know who's acting maliciously. So that's, I think, an issue. And of course, again, it can still work, but you're giving up one of the main benefits and one of the biggest, most important, I think, security features of a proof of stake blockchain.
01:31:52.680 - 01:32:02.780, Speaker A: Maybe briefly Dukon of Cosmos and then touch upon do a con of cosmos and then one follow up question.
01:32:03.290 - 01:33:03.550, Speaker B: Okay. I think the only con cosmos is essentially that it's still expensive to launch a new chain because you have to bootstrap a new validator set and proof of stake network and all of that. And the lack of shared security, even though IBC is a state of the art right now, it's the best that you can do, I think, between two independent blockchains. If you really want to get to this scale of thousands or even millions of chains in the future, far in the future, this trust assumption that you have to make for interoperability with cosmos as it is, is not, I think, very good. Again, Celestia, you can launch a chain without bootstrapping a new network and you can have shared security. And I think those two things are really going to be like theyre going to turbocharge the cosmos vision of an Internet of blockchains.
01:33:04.970 - 01:33:14.578, Speaker A: I cant think of the last question it escaped me. But overall, thank you so much, Nick. Ive really enjoyed the conversation, and I know a lot of other people will, too, so thank you.
01:33:14.674 - 01:33:16.130, Speaker B: Yeah, my pleasure. Thank you, Logan.
01:33:16.170 - 01:33:16.530, Speaker A: Appreciate it.
