00:00:00.320 - 00:00:22.534, Speaker A: On Akania, anyone can place the limit order, and if an order matches, it also, like, settles. During that transaction, the assets are routed between counterparties. It's done. There's no. There's no waiting for settlement execution. Like in traditional finance, there are so many terms that describe, like, just a subset of what we do in one runtime call. Like, there's settlement, there's execution, there's clearing, there's matching, there's like, there's so many different terms.
00:00:22.534 - 00:00:40.230, Speaker A: But in Akani, it's just like, I have this, I have this asset, you have this asset, let's trade. And that's all that happens. It's the way you, like, you would intuitively think it should. That's the way it happens on Acania. And we don't have to deal with any of this, like, fragmentation or any of this, you know, this cumbersome data manipulation I mentioned earlier.
00:00:44.370 - 00:00:55.790, Speaker B: Well, Alex, thank you so much for joining me today. I appreciate you upgrading your home studio to make your sound a little bit better too. Really appreciate that. And the listeners do as well.
00:00:56.090 - 00:01:12.670, Speaker A: Yeah, thanks, Logan, for having me. It was fun. When I first got the mic set up, I was testing out the delayed feedback and then hearing myself in my ears was pretty wild for the first time. I think I might risk it without that because I was talking so slow and figuring out how to hear myself in my ears.
00:01:12.710 - 00:01:45.728, Speaker B: Yeah, well, your setup may be a little bit more than mine. Mine's still a little bit echoey, so I'm trying to figure that out. But I appreciate you coming on, Alex. Really excited to dive into what you're building at Ecunya and what you're ultimately building at Aptos. I think, in general, what frictionless capital is really excited about is really what these high throughput blockchains enable. I think it's really a new paradigm and people have not really understood how large this new paradigm shift is going to be. So really excited to dive deeper into that with you, too.
00:01:45.728 - 00:01:48.060, Speaker B: Why aconia and why aptos?
00:01:48.600 - 00:02:45.648, Speaker A: Yeah, totally. I'm glad, too that we can definitely hit on the higher level topics of what makes the high performance of aptos work and why it's so important, and then obviously on a more granular level, why we make use of that in the Akani protocol, which is built from the ground up to leverage the memory model, the execution model, the optimistic concurrency of AP DOS. I put out a little message on Twitter the other day about this acronym I called and ramping, which has to do with parallel memory allocation, and I was thinking over the weekend about how that accurately captures pretty much all of the reasons why we're building on aptos and getting more into the details of why we want to allocate non deterministic memory in parallel and what that means as far as DeFi applications and more generally for any crypto application, I think is a really important topic that touches on this question of like, what parallel blockchains are and high performance blockchains are gonna bring, and how that changes the.
00:02:45.664 - 00:02:56.940, Speaker B: Way that we think about scaling 100%. Maybe before diving into the nitty gritty of the tech, if you could just do a quick intro and background on yourself and ultimately how you ended up in this position.
00:02:57.440 - 00:03:55.790, Speaker A: Yeah, so I'm Alex. I'm the co founder of Akania Labs, author of the Akanea V four protocol, which is a fully on chain order book for the Aptos blockchain. My co founder Kirsten and I had been building on Aptos since I believe it was April 2022, and we attended the first pre mainnet inaugural hackathon in May of 2022 in Palo Alto, where I presented on a rudimentary forum of the order book that eventually would become the Akani protocol. One thing led to another, we just kept on saying yes, ended up in an accelerator program worked out at Aptos Labs HQ and Palo Alto for about six months, met investors, did our seed raise, and eventually helped to launch the Akani protocol on Aptos Mainnet, which is live right now and has been since November of 2020, November 28 of 2023. And the protocol adds some important technical improvements over some of the limitations we've seen in the past. And I'm glad to get into later in this podcast, but that's my background.
00:03:56.450 - 00:04:30.140, Speaker B: I appreciate that. Thank you. I guess maybe jumping into Aptos specifically, to me, it kind of reminds me of the early days of Solana. Wherever I particularly a lot of VC's were like, there's nothing interesting going on with Solana. It's all copy projects of Ethereum, the builders are not good, it's a VC chain, et cetera, et cetera. What would you say to kind of the skeptics about the Aptos ecosystem that really kind of got you uniquely excited?
00:04:30.560 - 00:05:37.578, Speaker A: I'm glad you brought up the early days of the Solana ecosystem, because my exposure there is what really pulled me into crypto and led me to view these systems in the way that I do and to be able to use them the way that Akania does. When Solana came out, it was in some sense the first chain that brought parallelism to the fore. And they did it in a certain way that Aptos has improved on for the sake of developer experience. But Solana really told us in a big way what it means if you can actually execute multiple thousands of transactions at the same time. When you bring up essentially speed playing defi history on Solana, it does become qualitatively different when you can do it at a fraction of a penny and multiple thousands of times per second. It's a step change in the kind of functionality that is enabled from just one amm pool versus five amm pools, or a router that can hit all of them at the same time at a fraction of a penny with an aggregator, because then you get more price efficiency, you get better price discovery across the board. And I think that Solana was, and my heart goes out to all the devs who worked on Solana in the early days.
00:05:37.578 - 00:06:15.772, Speaker A: And I think it was really inspiring because to me it showed me that this is a new kind of cloud compute in some sense that we're working with. And Apdos has improved in it in many ways. I think the main thing comes down to the execution. In the memory model, Aptos takes what Solana did with pessimistic concurrency and uses an optimistic form. Their execution engine is called block STM. And at a high level, this means that developers do not have to declare upfront any of the areas of the global state machine they might touch. So if someone is executing a smart contract that modifies memory in one region, that region can be dynamically computed at runtime.
00:06:15.772 - 00:06:52.430, Speaker A: And in our case, because we've built an order book, an order book is just a giant list of addresses. And so if you have to declare upfront every potential call path in that order book, it changes the way that you design the protocol, introduces design constraints, and of course I'm just talking about the one protocol that I'm working on, which is an order book. But this applies across the board for things like NFT events, for other DeFi applications. And as far as Aptos goes, the performance that they've demonstrated on their preview nets and on Testnet and with some of the low testing on mainnet I think shows that it really is a high throughput chain and just makes me even all that more confident to be building there.
00:06:53.130 - 00:07:40.690, Speaker B: Yeah, it's a little comical to me. The VC's that say there's really no difference between say a dollar transaction versus $0.10 versus like one 1000th of a penny. To me, the types of applications that are ultimately enabled and that one not only buy kind of cheaper transactions, but as you mentioned, higher throughput doing more transactions in parallel, it really unlocks a unique application set. To me, analogies are always some degree wrong, but I like the analogy of going from dial up to broadband and just that framing of you can now build more interesting applications because you're no longer limited to the sandbox that you once were.
00:07:41.190 - 00:08:18.658, Speaker A: Yeah. So for the first part about that, when you talked about the difference in orders of magnitude of transaction size, okay, sure. If you have some high floor NFT on ETH and you're a whale, you don't care. So people are going to continue trading their high floor nfts on ETh L1 until the sun goes down. But it does change things dramatically when you can place a limit order on an order book at a fraction of a set, because market makers who are used to placing and canceling thousands of orders each day, they end up running into cumulative costs. On Akania, the gas costs are per month to make markets around a couple hundred bucks. We're obviously working to get that down.
00:08:18.658 - 00:09:09.962, Speaker A: But that means that the average gas cost of placing a limit order comes down to, I think the last time we did the calculations, it was two hundredths of a cent or something. And so you can place more limit orders at less expense, at a lower expense. And this ultimately just results in a tighter spread, which is a better product in some sense, for the retail trader who wants to just get the best price they possibly can. And for all these other applications too, cost and speed are often closely coupled. Yes, Ethereum is more expensive, but it's also much slower. And twelve to 13 transactions per second on an l, one doesn't enable the kind of atomic composability that chains like Aptos unlock. And also because it's so expensive and so slow, you lose security assurances like the ability to liquidate toxic debt, for example.
00:09:09.962 - 00:10:12.872, Speaker A: But if you can reevaluate those positions, rebalance a borrow lend pool, it opens up whole new avenues of opportunity for bringing not just old DeFi protocols that maybe have been battle tested on ETH to a new chain, but also bringing real world traditional finance applications on chain. And in that, I would include an order book, because we've historically only seen this thing in, you know, for example, in the Nasdaq, like a data center in New Jersey. And they give these people, I can't recall if it's the Nasdaq, NYSE, or one of these other, like, national security exchanges, but they give people the same Ethernet cable and they're like, you have to use this cable because otherwise that guy is going to have like five nanoseconds less latency and he's going to be able to spoof all your orders. And so they have to level the playing field. And this is an artifact of having everything co located in one place. And on chain, we have this unique property of the data dissemination. Basically, the speed of light ends up being that kind of triple bottom line for trading.
00:10:12.872 - 00:10:35.510, Speaker A: And so you get this leveling effect where someone who buys $10 million more valuable real estate in some server rack building in New York doesn't have this unique advantage that they used to have. So we can bring those applications on chain and we can also level them in a way that that changes some of the latency and informational adversarial dynamics.
00:10:36.250 - 00:11:17.242, Speaker B: Great explanation, maybe a lot to part. Let's maybe start with the differences between the optimistic parallel execution from a technical standpoint to the more strict access lists that Solana and Swe use. And then I would love to dive in afterwards into not the limitations, but everybody likes to call out, oh, central limit order books on chain are never going to have as low latency as these co located things, but the pros and cons of that. But maybe first start with kind of the optimistic parallel execution, why you're generally excited about it, versus the strict access list that Swe and Solana use.
00:11:17.426 - 00:12:02.108, Speaker A: Yeah. So on a theoretical level, there are these two major flavors of parallelism. The concurrency models, which have been pretty well established in traditional database theory, are optimistic or pessimistic. And the idea is that in pessimistic, you assume the worst. You assume that two transactions are going to touch the same areas of memory, and then you only allow one to start before the other one, and one has to execute completely before the other one. And also for you to know which one starts, you also have to require from these transactions upfront, declarations, headers, transaction tags, labels, whatever you want to call it. But a transaction needs to say, like, here's my payload, here's the function I want to call, here's the arguments I want to call it with.
00:12:02.108 - 00:12:58.698, Speaker A: And also, here's all the data I want to touch. So it leads to more complex transactions. Solana, for example, I know, has this mantra of eating glass, which I think does have the unique property of filtering out developers who maybe aren't really going to fight for it. So I think that there's actually, that could be a feature, not a bug, but there was the anchor laying, because essentially it was so complicated to just program Ross along transactions, and people are spending so much time serializing all the data headers that it became cumbersome. I digress. But I'll contrast again that approach of you have all this data and you have to declare from what you might touch with optimistic concurrency, which is what Aptos uses, some people call it dynamic parallelism versus static parallelism, but on the optimistic side, you assume upfront the transactions won't collide, and then if they do, you just replay one of them. In most cases you don't lead to a replay in the block STM model.
00:12:58.698 - 00:13:30.236, Speaker A: In Aptos it's called a reincarnation, and this is all tracked very efficiently using a multi version hash map. So Aptos has this executor that runs over all the transactions. It sort of simulates them. It checks, if any, had colliding write sets, and then it commits them. And this is a way that it can dynamically evaluate memory access at runtime. This becomes extremely important for an order book because the memory access is in some sense the entire product. An order book is a list of addresses of people who want to place and cancel orders.
00:13:30.236 - 00:14:28.568, Speaker A: And if you're executing in an on chain environment like ours, which is peer to peer, which executes atomically at the time an order comes in and matches like that during that transaction, the assets need to end up at the counterparty. So data needs to be modified in one person's account, at the order book, at the sender's account, and you, and you don't know what the accounts those are going to be until you've actually stepped through the order book and evaluated it. Optimistic concurrency allows for this assessment of memory modifications at runtime, which vastly simplifies the process. And then there are also some other technical limitations that are imposed by the pessimistic model that optimistic models additionally benefit from, besides just the cumbersome nature of the headers. For example, on a pessimistic model you also have to declare any transactions that may modify state. You don't know if it will, but if it could potentially modify this account or that account, you have to issue a write lock versus an optimistic concurrency. A replay, a write lock only happens when there's contended state.
00:14:28.568 - 00:15:18.882, Speaker A: So there's more redundancy in a pessimistic system, and because it assumes the worst, it assumes even worse than actually ends up happening a lot of the times, so that you end up having less throughput because it locks it. And then one result of this is that there's got to be limitations too. How much memory can you lock at a time? Can I pass in a terabyte of addresses that I want to lock? Can I pass in 100 megabytes? There has to be a limit at some point. And one of the main trade offs that this introduces in order books is the trade off between permissionless market making and atomic settlement. And this is a trade off that Akania doesn't have to make. But this is a trade off that does have to get made in a pessimistically concurrent system. Because the order book is again, it's a giant list of addresses.
00:15:18.882 - 00:16:12.286, Speaker A: You can't declare all those addresses upfront, so you have to do one of two things. You either have to restrict who can place a limit order on the order book, because then you know that, okay, my potential list of counterparties is limited to this subset, and therefore I can declare that entire subset in the transaction header. Right? That's one approach, permissioned market making. Or alternatively, you have to fragment the settlement process into serum. Historically, which was on Solana, used this process called a crank, so matching actually was broken over two transactions. One transaction computed the headers of the next transaction, and then the next transaction actually applied those headers and modified the data. So you can apply fancy tricks like you can maybe determine some algorithm for who's allowed to make marketplace limit orders, but at a certain point you have to limit like who's on the book, or you have to fragment the transactions, and Akania does neither of these.
00:16:12.286 - 00:16:40.370, Speaker A: So on Acania, anyone can place the limit order, and if an order matches, it also like settles. During that transaction, the assets are routed between counterparties. It's done. There's no waiting for settlement execution. In traditional finance, there are so many terms that describe just a subset of what we do in one runtime call. There's settlement, there's execution, there's clearing, there's matching, there's so many different terms. But in Akani it's just like, I have this asset, you have this asset, let's trade.
00:16:40.370 - 00:16:53.100, Speaker A: And that's all that happens. It's the way you would have intuitively think it should happen. That's the way it happens on Acania, and we don't have to deal with any of this fragmentation or any of this cumbersome data manipulation I mentioned earlier.
00:16:55.040 - 00:18:15.206, Speaker B: Great explanation. I think in terms of not the counterpoint, but what the SWE and Slanik camps would ultimately say is that by specifically specifying your headers or the access lists, you'll always know whether you have state contention or not. And when you do know you have state contention, you can obviously serialize those transactions and everything can be run. Every other transaction can be run in parallel. And if I'm understanding correctly, the unique advantage to Akania is by having the optimistic parallel execution you may you're pushing that complexity of specifying transactions from the developer to the runtime. The only not down, the only semi drawback is you could possibly rerun the transaction twice if there's serial dependencies. But because you have the optimistic parallel execution and you don't have to specify the headers, you can then enable these transactions to be settled atomically within one block, so to speak.
00:18:15.358 - 00:19:21.032, Speaker A: Yeah, that's exactly right. The issue with just accessing the same state, whether it's pessimistic or optimistic, is that someone has to go first. If there is a $5 arb waiting on some pool to happen, and a bunch of MeV bots are going to try and flip that switch, someone has to go first. So yes, you're either going to have to allow someone in the optimistic queue to get that first one, and then you'll replay the other ones and their logic will probably abort, because this is typically what maybe transactions do. Or in the pessimistic one, you'll throw them out at the address header state. It ends up being the same kind of model. One drawback of the pessimistic one though, is that in the general case where not everyone is contending for some hotspot in state or whatever, you end up unnecessarily locking a lot of state because many call paths, the typical ones in complex applications, are non deterministic based on the state you have access to at the time you submit the transaction.
00:19:21.032 - 00:19:47.870, Speaker A: If you had an indexer that could perfectly index everything, and it could simulate your transaction and you could know what state you were going to touch, then you could reduce redundancy, but only if you knew your transaction was going to be executed the moment you submitted it. But you've got things like mempools, you've got RPC nodes, so you can't necessarily know that the like okay, what if there's a new order on the order book that just came in by the time I submitted it to the mempool that things like that can change.
00:19:48.770 - 00:20:07.350, Speaker B: So do you think it will be like the highlighting the key differences are just slightly lower latency with kind of swe and Solana just because of the strict access list versus atomic composability and everything settling within one trade?
00:20:07.930 - 00:20:47.646, Speaker A: Yeah, I wouldn't necessarily say that access lists are necessarily going to accelerate execution in any sense because it just adds overhead. There's so many different things that have to happen during a transaction. You've got to load certain byte code out of memory. You've got to load the state either from maybe a hard drive or if it's cached, you load it from Ram and you're going to have to access an l two cache at some point. So there's all kinds of extra operations you have to do. And the overhead of access list is just another step. There's necessarily going to be in runtime, some kind of, you're going to touch the state based on a call path.
00:20:47.646 - 00:21:08.070, Speaker A: And in the optimistic model, it's just saving that for a pure commitment later on. So this is going to get evaluated anyways during the execution stage. And putting extra overhead at the submission level is not necessarily going to make this run any faster.
00:21:08.730 - 00:22:20.126, Speaker B: Interesting. I'm very curious to see how these different design paths end up, because I very much would love to see an order book on chain. And I think people underappreciate how order books on chain can fundamentally trade changed, kind of like the paradigm of trading. And to me, and I would love to hear your opinion, I kind of think of these order books as synchronizing these heterogeneous systems that are kind of like the New York Stock Exchange, the London Stock Exchange, Japan Exchange, where they're separate order books and they have different sets of rules. To your point earlier on cable links, they have different cable links, there's different latencies. And what these on chain order books do, even though that they have higher latencies generally, is that they enable kind of this unified trading experience globally, which I think is rather underappreciated today. I would love to learn your thoughts, though, on why you think order books on chain are this unique primitive, like where their pros are, where the cons are.
00:22:20.318 - 00:22:58.322, Speaker A: Okay, I'm glad you talked about that. And changing it from a data silo in New Jersey to a globally distributed system. I think the first point I'd like to make about that is that if some information event that's going to affect markets happens in, say, Singapore, but the market that affects is getting traded in New York, that information already has to propagate from Singapore to New York. Like that's going to go through your Bloomberg terminal or, you know, I don't know, someone's going to like telegram the guy who's like placing orders from his like, broker.
00:22:58.346 - 00:23:01.234, Speaker B: I don't know, there's a lot through fiber.
00:23:01.362 - 00:23:30.486, Speaker A: Yeah. So there's exactly like, the information still has to get there. And the markets are going to be more accessible if the person in Singapore can actually place that order. But again, like, if some. If they bought a trading desk in Singapore, they still have to somehow contact, through their internal network, the desk in New York. And this ultimately is really just going to be access blocked by the large institutions that actually have paid for these spots. Not anyone can trade on the Nasdaq or the NYSE.
00:23:30.486 - 00:24:24.196, Speaker A: I cannot. Even though I'm an american citizen and I can have a Robinhood account and I can submit orders, I'm not actually trading on the Nasdaq. I'm trading against other people on Charles Schwab's internal excel spreadsheet. Then they're packaging up these orders and they're selling it to Citadel or something. So you actually get better price discovery when you allow more participants into the order book, which is what happens when you make it totally permissionless. And in the case of national securities exchanges or commodities or something, like you said, there are these different mechanical implications and limitations of them that play into it. If you trade commodities in the US, like, oh, man, I think you have to, like, prove that you can take delivery, because in the past they've had like, you know, literally tons, like 2000 pounds, like the unit of measure of corn showing up at like, some Wall street office.
00:24:24.196 - 00:24:56.010, Speaker A: And people like, I don't know what to do this. I'm just speculating on the price, right? So. So you have these weird, like, guarantees around it. Same with securities. Like in national securities in the US. Like, there's voting power, there's certain requirements that, like, by it just being in that trading venue, there's something additional that's kind of captured by it, whether it's deliverability of a commodity or voting power in a United States corporation, obviously, I'm showing my United States bias. I'm based in the US and these are the things I'm most familiar with.
00:24:56.010 - 00:26:02.656, Speaker A: But I would add that I think there's a really interesting new use case that is just starting to show its face this year for this commingling of traditional finance and on chain finance. I would say that that is the emergence of non us dollar backed stablecoins, because these assets and us dollar stablecoins in some sense have the deliverability characteristics baked into it. If you hold USDC, you essentially have a promise from circle or TUSDT, you have a promise from tether that you can redeem it for us dollars. And there's ultimately an enforcement chain link that goes back to the United States government that guarantees that this asset can be traded, or if you do not have a circle business account, you can sell to someone who can. So by virtue of it being on chain, there's this guarantee of the deliverability and whatnot that isn't necessarily present in these other use cases. And this year, we've started to hear rumblings of non USD based stable coins. Obviously, some of these primitive earlier versions have been issued, but haven't really taken off.
00:26:02.656 - 00:26:47.324, Speaker A: But if there is now a digital euro or a Singapore dollar, whatever, if that gets on chain, then we now have a proxy for forex markets that actually completely eliminates the fragmentation we have in the traditional markets. And I think makes a much more superior experience on chain because, like, forex by definition, doesn't exist in any one country because you're trading one against another. Like, there's, like, the jurisdictional question is. Is not a well posed question. And so if you can't have it based in some place, you know, like, obviously the. The us government would probably want it to be based inside some kind of us, like, jurisdictional funnel, but you can't necessarily have that if you're trading two currencies against one another. And the most straightforward place is to do it in a decentralized on chain manner.
00:26:47.324 - 00:27:28.790, Speaker A: And forex markets are already, like, 24/7 crypto markets are already 24/7 on chain. The atomic settlement, the deliverability that you have, that you can take this stable coin and you can put it in whatever bank account and you can, like, on an off ramp, I think makes for, you know, perhaps the. The biggest exodus maybe to an on chain venue. Obviously, we're seeing more real world assets take off, but the preferred venue is still. The NYSE is not going anywhere anytime soon. But forex markets are extremely fragmented, and there's a huge opportunity for non USD stables to bring an on chain proxy that demonstrates real world value for a traditional use case that is not, I think, optimized in its current form of existence.
00:27:29.450 - 00:27:35.390, Speaker B: Do you think forex trading is going to be the big first markets that ultimately move on chain?
00:27:36.490 - 00:28:11.410, Speaker A: I think it has a lot of opportunity. I mean, I know that, like, you know, people want to trade stones and. Yeah, okay, like, maybe the, like, game stone is going to end up on chain at some point. I think that the us jurisdictional grip around a lot of these things is ultimately pretty tight. I've seen things like mortgage backed securities. I think I remember hearing at one point was on chain, but a lot of these, I think, are kind of contained in like, silos between tradfi institutions or maybe like internally testing or they want. They want like, guaranteed settlement between institutions, but they don't want to, like, open it to everybody.
00:28:11.410 - 00:28:36.470, Speaker A: I think forex has a real, like, like, strong claim for use case on chain for something that is like a traditional asset. And I think a way that's not necessarily, like, self referential or circular in the way that a lot of crypto trading has been, because it's something from the outside that actually comes in. And like I said, it has the unique deliverability guarantees that you don't necessarily get in other asset classes.
00:28:36.810 - 00:28:50.106, Speaker B: Yeah, no, it is an exciting market. You can see how it could potentially take off fairly well. I know circle has the euro. We just need all the other currencies to be added.
00:28:50.298 - 00:29:25.108, Speaker A: Yeah, yeah. And, like, it's not really getting used in a big way. And I think too, like, you touch on, like, the euro and the US dollar and pricing. And it's funny because, like, even when I think about this is like little. A little bit of a pivot, but even when I think about people talking about, like, bitcoin historically and, like, their bitcoin maxis, and they think you should pay everything in bitcoin, like, they still, in their memory price, all of their fond memories of, like, bitcoin being at this US dollar denominated price. And I'm running an order book, and so I'm always thinking, like, base and quote, base and quote, base and quote, like, sorry, I don't run an order book. I develop protocols that help facilitate decentralized order books.
00:29:25.108 - 00:30:01.214, Speaker A: Right. But, like, I work on protocol software and I'm always thinking, base, quote, base, quote. Like, when is the quote not gonna be USDC? For now it is, but it doesn't have to be. And I think that when we can actually get that, like, paradigm shift where we can start pricing things in terms of other assets, then I think then that's when things have really taken off. Obviously, the US dollar is going to remain the global unit of measure for a long time, but I still think that there's a large amount of public awakening for us to realize that we're swimming in this ocean and actually see it for what it is. Fish doesn't see water. We don't see that.
00:30:01.214 - 00:30:15.910, Speaker A: We quote everything in us dollars, but we can quote it in anything. Akani can quote it in anything. The NYSE actually can't. I believe they are forced to quote everything in us dollars, but you can quote anything in anything on chain and that forex markets are a great use case of that.
00:30:16.210 - 00:30:35.990, Speaker B: Yeah, it's very interesting. So maybe to kind of like put the nail in the coffin on like high throughput blockchains and what they enable. Did you evaluate kind of l two s in that landscape or what were your thoughts ultimately that led you to build on aptos versus others?
00:30:36.490 - 00:30:55.450, Speaker A: Yeah. So in the. I can rewind a little bit to like, the early days of my experiences with aptos. I believe I actually learned about aptos from a Kyle Samani tweet. And then I started reading more about it. And my first intuition, honestly, was, hold on, hold on. Facebook engineers built this.
00:30:55.450 - 00:31:29.370, Speaker A: Okay, this can probably scale. And I think that intuition has like largely proved correct. You know, obviously we've been building on app dos for a really long time. It's like, you know, it's been years that this has been playing out, but it has played out in the way I originally, I think, kind of like was hoping. I was just thinking like, I had heard about the DM Libra project. I knew that there was some kind of kerfuffle about enforcement, not necessarily favoring Facebook, having intervention in questions of what might be considered money supply. So I was kind of aware of it.
00:31:29.370 - 00:32:21.172, Speaker A: And then when I just started looking more into the tech, it was all, it all made perfect sense of how and why to build there. I think from the ground up, the developer experience has really been tailored to like blockchain for solutions. That's like the move programming language, for example, is basically built just for blockchains. It actually takes a decent amount of unlearning of other languages before programming and move, which is how it works. And the consensus model, the black STM execution engine, interleaves with move in a way that just leads to better evaluation of the parallelism. I do know that, speaking of l two, s other projects have started to incorporate block STm, for example, as an execution model. And then also people are looking into move just because of the developer ease.
00:32:21.172 - 00:32:59.150, Speaker A: And I think the performance superiority of these two primitives. As far as building on L two s, I guess there's something that hasn't really gotten through to me yet. Maybe you can change my mind about this, about how an l two is different from an l one, but with a special ethereum bridge. And so if the throughput is going to be high on an l two and you can deposit it back to EtH, then okay, that's great. But if Aptos has an ETH bridge, then it's the same thing. And Aptos is already trying to build something that's super high throughput.
00:33:00.210 - 00:33:03.430, Speaker B: I think that the everything is bridges.
00:33:03.850 - 00:33:37.542, Speaker A: Yeah. And I think that one, I'm concerned about fault tolerance too, and I recognize that there are not as many validators on even Solana or Apdos as there might be on Eth l one. Personally, I don't think that's an issue. I think that you just have to have enough. And what the absolute minimum bound on that is, I'm not sure, but I think these high throughput chains have enough. At least I'm willing to say that, but I think having too many can potentially slow it down because not everyone can meet the validator requirements. You just can't build a race car with junk parts.
00:33:37.542 - 00:34:11.370, Speaker A: And so for the l two, I think, narrative of what is it? It inherits the security of the base layer. I think that means ultimately that if you can bridge it back onto EtH, that eth is really secure and that your assets are safe there. But what happens if the sequencer goes down? That has happened on some of these more prominent ones. And it confuses me why you're not allowed to build a blockchain with only ten nodes, but you can build an l two with one node, and you can get away with it if you just call that one node a sequencer.
00:34:12.230 - 00:34:51.607, Speaker B: I mean, I'm not going to change your mind. I'm very much in the same camp, so you don't have to. I agree with you on a lot of this. To me, l two s are always going to be bounded by the throughput anyways, because if you're settling to the l one, you're going to be using that like data compression or the other blockchain to compress data to put it on the layer one. But you're still going to be bidding against all the other transactions or other l two s on the network. And potentially even that single l two could consume a majority of the block space on the l one. And then if you hit block saturation, you're still going to have high fees.
00:34:51.607 - 00:35:51.040, Speaker B: So I don't know. To me, it never made that much sense. I do understand if you want to have like a new execution environment and then settle that somewhere else. But the single node, to me, what got me kind of sad about it or the SQL sequencer was the lack of data propagation. So the fact that you're not sharing this data to everybody around the world, it's just this kind of single sequencer that's ultimately ordering all the transactions, because then you're going back to that New York or Nasi model where it's a one sequencer and you're just, everyone in the world has to aggregate their transactions no matter where they are. And to me, the interesting thing outside of blockchains and just shared state was that like, synchronization of data. And I don't think people have really dug into how important it is to have a decentralized validator set so that anybody can really have access to that synchronized data.
00:35:51.580 - 00:36:20.136, Speaker A: Yeah. So I think that the. I wouldn't say that I necessarily rule out the l two approach in totality, because what I'm after as far as like a platform to build on is atomic composability and data dissemination. Because, like you were saying, we need to make sure that the state is distributed. We want everyone around the world to be able to place an order to update in order to have access to that same market. And it needs to be composable so that you can change transactions back to back. You can start building derivatives on top, that kind of thing.
00:36:20.136 - 00:36:36.706, Speaker A: But as far as data sharding or isolating state channels, that approach of the single sequencer model, like, you know, the one node blockchain, if I can call it that here, maybe without getting too much heat, like, that does work.
00:36:36.858 - 00:36:39.554, Speaker B: As engineers, you need to call things what they are.
00:36:39.722 - 00:37:32.796, Speaker A: Yeah. So like, that works in the case of, if you don't need atomic composability with the state and the rest of the chain, and you're fine with trusting one actor to not shut down the sequencer, then it's not a problem. And I think this has legitimate applications for things like some game studio runs its own sequencer and you've got an app chain and you want to be able to update all this who owns what in game asset on chain and revoke it. And you only want to commit to the chain to ethel one or whatever once a week, and then you have your snapshot of who owned what at the end of time, and that's good enough. I think that's a perfectly viable use case. I think it's more transparent than the alternative, which is just some database that no one can access because you're, you know, because you like World of Warcraft and Blizzard uses its own server. I don't know, I don't think they're on chain yet.
00:37:32.796 - 00:38:10.620, Speaker A: I'm just gonna, like, throw that off the cuff. I'm pretty sure they're not, but, yeah, like, the guarantees of these things are not necessarily different from the guarantees I would say, like, as far as security goes with traditional databases, but there's more transparency, and I think more transparency is good. So I definitely would advocate for it over a centralized database. But functionally, as far as on chain composability, I don't think it's so much different because for the simple fact of it goes down, who cares if you had any assets in there? If it goes down, you can't get it on ethel one. So you actually haven't inherited the security of the base layer, you've inherited the security of a single node.
00:38:10.960 - 00:38:37.160, Speaker B: Yeah, they have the escape hatch, but if everybody's hitting this escape hatch at the same time, it's not going to be a great user experience and it's going to be super expensive. I just generally think people have underestimated data dissemination and how important that actually is to building at least financial applications. I think it's very under explored.
00:38:38.220 - 00:39:16.232, Speaker A: Yeah. I think that one of the major premises of blockchains is the replicated state machine, so to speak. And consensus algorithms are not necessarily just a way to make sure that everyone agrees on the same data, but it's like a Trojan horse for making sure that everyone has the same data. If someone's participating at and they have the same ledger, they have the same loan book, the same order book as everybody else, and then they can all make decisions in lock stuff in a more efficient manner. That eliminates these bottlenecks of having to call up the trading desk and ask them how many bids are up right now, depending on the market, that still happens.
00:39:16.416 - 00:39:28.060, Speaker B: People pay a lot of money to get access to that data. And I think one of the interesting things about these high throughput blockchains is you can almost democratize that access, which I'm super excited about.
00:39:28.880 - 00:40:22.360, Speaker A: Yeah, I believe that the NYSE and the Nasdaq, or at least the Nasdaq, generates most of its revenue from selling proprietary data, which, yeah, like the matching engine is not actually the most important thing. The most important thing is how many trades went through, what the trends are, because from that you can run your AI ML algorithms, you can maybe try and predict things. Obviously, you can't win spoofing. If you're going to be spoofing people and you want to run your HFT bot, you actually have to mainline in and you have to pay for that ethernet cable or whatever. But there's a lot of value that comes from having that, that after the fact, or in quasi real time, maybe in a subscription model. One of the cool things that on chain training does, like Akania does is opens all this up and makes it so everyone can access it. We have what we call our data service stack.
00:40:22.360 - 00:41:21.020, Speaker A: It's completely open source on Akania that anyone can deploy, whether they run a docker on a terraform config, it sniffs the blockchain and it indexes everything for you locally in a nice cleaned up postgres database with market data, candlesticks, with TVL, with volume, all that. And the source of truth is all on chain, and it's just doing the post processing for you. So there's a lot of downstream analytics paradigms that are cropping up too, with whether it's RPC nodes or analytics platforms that people are using regularly and growing to love, and it's basically inverted. This model of the data is now there's so much of it, and we want it to be abundant and free, and then the real value comes in from how you can synthesize that data and what you can do once you already do have access, assuming that everybody else has access too. So I think it really changes the nature of the game and like you said, in a more democratized way that leads also to new paradigms and technology that we have to develop around it.
00:41:21.760 - 00:41:41.520, Speaker B: So, maybe, speaking more to Ikanya, how has your kind of entrepreneurial journey been since kind of sleeping at a hacker house, to getting funding, to building a product, to seeing that product be live on main net?
00:41:41.860 - 00:42:34.656, Speaker A: Yeah, it has been really exciting. It has been the most work I've done in my life and some of the most satisfying work I've done in my life because of that. As I mentioned earlier, Akania was born out of a hackathon project that I think got noticed by people who understood the potential for not just a Kania, but what it would mean when implemented on a blockchain like Aptos, which is the vision that I had in the beginning. From that first Aptos hackathon back in May of 2022, I formed relationships with people at Apdos Labs, the core team who are still there. We're still committing on each other's poll requests and whatnot, and I think have built this understanding together of what we're really building here together that I think that is amazing. Folks at Aptos Labs helps get us put in touch with the investor network. We have expanded our connections within the industry as well.
00:42:34.656 - 00:43:38.500, Speaker A: Did the fundraise? It was, yeah, $6.5 million seed round led by Dragonfly that we closed in May of last year. And it's been exciting to see so much shared vision. And I think there's like that understanding that's like so niche in crypto, like when you know, you know, and I think even more so when you're in like such a, like, again, like a niche part of the industry. Like we're building an atomically settled, like fully on chain order book and like, okay, yeah, maybe I've like, traded NFT before and maybe I use crypto, but like what? So, like, you know, like people really have to have, have known and I think believe from earlier part, which has been, you know, extremely satisfying and I think motivating to work in that community, especially as we launched the protocol with four front ends who were building on top. So Akania is a fully back end system, and that means that it has developer APIs, everything's open source. It's got move code, it's got Python code for interacting with the chain, but it doesn't have an official front end or ux.
00:43:38.500 - 00:44:48.344, Speaker A: The UX is what you want it to be and you can choose from that, that four partners who have built and they went live with us. At the same time, we have been working on an internal model that we're going to open source so other people can clone it and fork it. But at the end of the day, there is only an official Akania backend, which is exactly the kind of system that we want because ultimately it comes down to the way that we're disseminating data throughout the fabric of the Internet and the way that that allows people to interact in financial situations. That's been really exciting as far as the people that we've been working with across the industry. It's been exciting to be in Aptos, I think not just during the early days, but also during a time when there's been skepticism and when market sentiment has been lower, because that's almost acted as like a forcing function for pushing through the developers who only care about these idiosyncratic topics, of which brand of parallelism matters. And so I found that the mind share there is maybe at times too engineering focused and people could maybe improve their marketing a little bit. But I'd rather have it that way.
00:44:48.344 - 00:45:02.860, Speaker A: I'd rather be geeking out with a bunch of engineers and building the future of finance, especially when we all know what we're here to build together and we know what the promise is. And anyone who's building an Aptos doesn't really need any convincing at this point. So it's extremely refreshing.
00:45:04.120 - 00:45:29.928, Speaker B: What is the biggest bottleneck to ultimately get a Kanye to say, ten x or 100 x, the trading volume, is it getting on chain market makers? Is it getting more users, retail traders, into kind of the Aptos ecosystem? Is it figuring out kind of the toxic order flow? Kind of walk us through where. How can we help Ikanya build a better order book?
00:45:30.104 - 00:46:10.760, Speaker A: Yeah, so there are all like, there are avenues we can take all of those that are applicable. You know, there's more retail interest in altos, obviously, and talking about the tech and proselytizing it, which, like, which we're doing here, right, is exactly like part of that plan. As far as market makers integrating, some are integrating, others are starting to dip their toes. The more experiences that those have, the more data and tooling we provide, the more integrations, the easier that becomes. I think native stablecoins is actually a really big one that I believe is in the works right now. A lot of it is bridged USDC from other chains. And I think that once that becomes canonicalized, that that also leads to a huge boom in activity.
00:46:10.760 - 00:47:12.414, Speaker A: I think generally too, like, from the cycles that I've observed, which is not as many as other veterans in the industry, but when number go up, the tech gets its chance. And I think that's sort of, we're sort of starting to see that happen. I've been working on app dot enough to see sentiment and market activity still have its own little microclimates, and things are shifting, which I'm glad for. Obviously, there's some more noise that comes with that, but there's also more recognition that comes with that. And so I'm just glad to see that all playing out and to see more daily active users, more professional partnerships, more trading volume on Akania, all of that. I'm glad to see you taken off, and I do think that those topics you mentioned before, and my response is that how we can work it, I think it's all coming together, which is really exciting to see because it takes, you know, like, I forget where I saw it was, I forget where I saw this, but it was like, it takes, you know, like a month to write a smart contract. It takes like five months to get it audited.
00:47:12.414 - 00:47:32.810, Speaker A: It takes like five years to explain to the world, like, why it matters, you know? And I think it's the same thing with like, the belief and the bet we took on the ecosystem that, like, I've been kind of saying a lot of the same stuff for a while. Like, guys, this is why optimistic currency matters. And, like, people didn't care as much, now they care more. Like, they'll they'll care more in the future. So it's, like, kind of fun to just be along for the ride, for sure.
00:47:32.970 - 00:47:45.426, Speaker B: How can you share any metrics in terms of how much volume you guys are doing on the backend side, or how many people, generally wallets that are interacting with the chain?
00:47:45.618 - 00:48:01.802, Speaker A: Yeah, so we just got integrated with DeFi llama for our volume endpoint. Just got the TVL PR submitted today. We're approaching. Yeah, yeah. So we're approaching. I think it's between two and 3 million. TVL hovering right now.
00:48:01.802 - 00:49:00.366, Speaker A: Volume, we've done 38 million, I believe, cumulative, and this is only across two markets, APT USDC and wrapped ETH USDC, that has gone through the economy protocol. Right now we're working with a partner who's actually working on integrating perps, which I'm looking forward to because we actually see a lot of higher notional volumes on those than happens on spot trading. As far as unique wallets, I believe it's approaching 50,000 at this point. We run an internal Grafana dashboard to track some of these things, and we're also working with folks across the ecosystem who are starting to spin up their own data integrations. Can't speak to all those names yet, but we are excited to see some of those that are coming down the pipeline. And obviously, I think what's really cool, too, is the permissionless market making aspect. And like I said, anyone can trade on any market, anyone can open a market, but with more native coins in the Aptos ecosystem, more meme coins, more maybe real world assets issued on chain, and market maker integrations.
00:49:00.366 - 00:49:24.220, Speaker A: I think it's exciting to just consider the prospect of more markets taking off, which is what I'm looking forward to, because obviously everybody wants to trade apt on chain, but I think some of the real interesting use cases are going to be when the things that are getting traded on chain are actually derivative instruments that are used in someone else's business logic that has maybe a composable NFT or something in the background that links it all together.
00:49:24.520 - 00:50:21.284, Speaker B: Yeah, that's when the composability nature really becomes interesting, I think. Again, we had that first inklings and defi summer in 2020 on Ethereum, but the chain was so slow and then got so expensive, it kind of became hard to do. And then Solana kind of fell over a couple times, and now there's like a new breed of high throughput blockchains. And so I'm very curious to see how it ultimately comes together. I think one thing that you said to me, that kind of caught my eye, was kind of the two to 3 million in TVL and how much more volume that you've actually done. And I think one thing that is kind of comical to me is kind of the more ethereum maxis of the world saying how important TVL is. But I think, again, these high throughput blockchains in Acanya is even demonstrating that you can do a lot of volume with a small amount of TVL.
00:50:21.284 - 00:50:28.840, Speaker B: And it's not Tvl just for the sake of TvL. It's that usefulness of the TVL that actually matters.
00:50:29.220 - 00:51:21.410, Speaker A: Yeah, so there's that saying that, yeah, you can drown in like a teaspoon of water. You can do all the volume in the world with like a liquidity if it's just trading back and forth, which a lot of on chain trading is trading back and forth. Like you have people on both sides of the trade. And I think one of the unique prospects of order books that honestly makes the data around in the marketing somewhat difficult is the volume to TVL ratios, which are astronomical when you compare them against a Uni V two automated market maker, constant product pool. This is the typical Defi Dexe and Amm. And you can get lower slippage if you have 5 billion locked in it. But then that liquidity could be somewhere else on an order book.
00:51:21.410 - 00:51:55.812, Speaker A: You can basically guarantee as much slip it as an amm with just a fraction of the amount of liquidity. That's what you said. The comical numbers of being like, okay, we don't necessarily need to have 8 million or 8 billion locked in this contract. We need like 100,000 on each side of the book. And that's going to cover like pretty much every retail trade that goes through this thing. So I think that the data reporting is going to have to change somewhat around order books. Or maybe this will be something that Defi intelligentsia just likes to smirk about.
00:51:55.812 - 00:52:08.708, Speaker A: I don't know. But I do think that the TVL volume ratios are, at the end of the day, what demonstrates why an order book in the first place, it's more efficient. You have less liquidity and you have more volume. And so that liquidity can be put.
00:52:08.724 - 00:52:41.352, Speaker B: To use elsewhere 100%. I don't know. We're so early that a lot of people are still just figuring out these things, including going from the more low throughput blockchains to the higher throughput blockchains. And I think I am just extremely excited by this shift because, one, I don't think people realize how massive of a different. It is. And to your point, the types of new applications that can ultimately get built on these high throughput blockchains. It's not a small difference.
00:52:41.352 - 00:53:00.560, Speaker B: It's quite literally a hundred x in most cases, where you can build more novel applications like order books. In terms of kind of wrapping up the podcast, is there any particular things that you feel like we have missed or ultimately did not cover?
00:53:01.940 - 00:53:28.554, Speaker A: Oh, man. Yeah, I prepared a bunch of notes for how we could geek out on here, which we've done. It's been fun. I. Maybe we'll just go through a little bit of those at the end here. When you talked about the high throughput, and we're kind of at the beginning, I think that we just need to focus on the fact that there is a theoretical limit to this, but we're not even close. And it's basically like the speed of light around the world.
00:53:28.554 - 00:53:37.390, Speaker A: And however much egress you can pay for on like a GCP instance, which is like terabytes, maybe even petabytes per second at this point, depending on how much you're willing to pay.
00:53:37.690 - 00:54:15.298, Speaker B: And so we're really running on GCP. It would be a little bit expensive, but their egress costs are a little pricey. But yes, I think that's why I got excited about high throughput blockchains versus the low throughput blockchains, because to your point, there was, we were nowhere even remotely, remotely close to like the limitations of hardware. And you can pump that so much harder. And then, I mean, you can get into like the decentralization debate. But at the end of the day, you can have these highly decentralized systems and have very high performance. And I think the market hopefully is, will start to realize that, yeah, I.
00:54:15.314 - 00:54:53.840, Speaker A: Think that the, you know, the coming decades of the industry are going to be kind of like, oh, man, in a Rubik's cube, you have this idea of, it's called God's algorithm. So, like, you can necessarily solve any Rubik's cube, and I think it's like 23 moves or less, and there's no closed form solution for what is the optimal solution to any configuration. Sometimes you can maybe solve it in 18, but like. But then that's the question, like, how do you do it, you know, so called God's algorithm. And I think we're still, we're just looking for that. Like, it ultimately comes down to software in some point because we already have the tech. Like we've got, we've got enough gpu's, we've got enough fiber optic cable.
00:54:53.840 - 00:55:01.200, Speaker A: Things would maybe go faster if we lived on a smaller planet. Who knows? Maybe we'll shrink the earth so that we can get the speed of light around the world faster.
00:55:01.820 - 00:55:05.452, Speaker B: I don't know, everything to Starlink and use lasers.
00:55:05.596 - 00:55:28.526, Speaker A: Yeah, exactly. We're not even close. And so I think that it puts software on a pedestal now to really do the heavy lifting, which, again, is what blockchains have done from the beginning. They're like this inverted. They're this new kind of computer that inverts the power dynamic where software runs the show instead of hardware. Like, if I smash my phone, the software on my phone dies. But if you smash a node in a blockchain, the blockchain continues to run.
00:55:28.526 - 00:56:13.894, Speaker A: And if you try and do something funny at a node, like, you get kicked out. So the algorithms are taking over, which I think is exciting, just like, as an algorithms guy. And on that note too, the algorithms that I'm used to from before crypto, actually, I was in embedded systems and robotics and aerospace, and before I got into. And when I was in the Palo Alto summit earlier this year, I visited one of my professors from Stanford, from my mechatronics class, and I said, hey, okay, what microcontroller are you using? Are you still using that Tiva that I had, like, programmed C on with interrupt vector and everything? He's like, no, no, we're using the Pic 32 now. It's using sixty four k of memory. And you can't do floating point operations unless you do it in software. Like, there's no hardware floating point operations.
00:56:13.894 - 00:56:30.710, Speaker A: And I was like, you know, I'm programming this new moviem and you got a 64k package size and you've got no floating point ops natively. So, like, we are just at the cusp of implementing, like, a really tiny, cheap microprocessor in distributed fashion. And there's so much more room to go.
00:56:31.050 - 00:56:42.060, Speaker B: I fully agree. I fully agree on that. Maybe a prediction in one year time, where will Akanya be in terms of trading volume on chain?
00:56:42.400 - 00:56:58.780, Speaker A: Oh, a prediction. Okay, more. And then I'll try and be conservative and I'll say between one order of magnitude and two orders of magnitude. Let's go with that.
00:56:59.080 - 00:57:04.648, Speaker B: So between 300,300,000,003 billion, I think.
00:57:04.704 - 00:57:06.054, Speaker A: Yeah, let's go. Let's go with that.
00:57:06.192 - 00:57:15.986, Speaker B: Yeah, it can be definitely more than a billion if, if things take off as we, we think. I think it's a low estimate issue. Yeah.
00:57:16.018 - 00:57:55.050, Speaker A: And one thing too, that's like, that I think is fascinating too, is if like in some sense more transactions is actually, I think, like as an engineer to me, more impressive than more volume. Like it's cool obviously, if people are moving, like if they're trading all these things on chain and larger volumes, but if they're trading more transactions, then the tech is actually superior in some sense. So I would like to see thousands of Akania transactions per second that I think to me would be almost even more satisfying than just the volume. Because at the end of the day, it's about the tech. That's why I'm here, and that's why we've built a connie the way that we have on top of aptos.
00:57:55.390 - 00:58:13.328, Speaker B: What would be needed to enable that? Would it just be higher core machines be spreading out the markets, the individual markets, on different threads? What do you need from a software standpoint to enable 1000 plus transactions per second?
00:58:13.504 - 00:58:54.222, Speaker A: Yeah, right now we're actually working on benchmarking with Abdas Labs, including economy as a canonical transaction. So I'm excited to see what the results are. We're working on tests for what is the max TPS for one market, for multiple markets. How does it change logarithmically if you've got ten markets, perennial versus 100? The entire order book is designed from the ground up so that markets are independent state channels. So there is no actual like state contention in between two markets. There's only on each one is contended. And so we've already seen Aptos has done benchmarking of like tens of thousands of tps, you know, during load testing of simple, simpler coin transfers.
00:58:54.222 - 00:59:13.310, Speaker A: So I'm excited to see the results of this upcoming benchmarking for what it for more complex ones, as far as what software or whatever is needed, I really think that ultimately it just comes down to demand for block space, not necessarily any existing limitations imposed by the design constraints or the hardware that the average validator is running it on.
00:59:14.090 - 00:59:55.378, Speaker B: Makes sense. Cool. Well, it was a fascinating chat, Alex. I think we'll look back at this podcast in a year and kind of laugh at maybe some of our predictions, because I really do think we're really on the precipice of something very interesting happening. These markets moving from more of these heterogeneous systems into more of these integrated systems where everything is kind of unified. You have access to the shared state, and that's really kind of the magic that blockchains originally promised. And I think as more and more people figure that out and that these are better, elegant solutions, we're really in for a treat over the next couple of years.
00:59:55.514 - 01:00:39.700, Speaker A: Oh, yeah, yeah. And first part too, like you talk about how funny it is of like making predictions and looking back, man, there's some book, I think it's called like experts speak, where it's just a collection of quotes from people in the fifties being like, we're gonna have nuclear powered vacuums at the end of this decade. And sometimes they're way ahead, sometimes they're way behind. It's funny too, and I think too, like with onboarding all these new users, like in our case, when I think about what happens to, for someone to get excited about Akanea and for a new user to happen, I'm thinking we just need to excite a couple network engineers at some HFT platforms. We excite the right people and they talk to their boss. They talk to their boss. The more trading that happens on chain, the more screenshots that people post on Twitter of the charts.
01:00:39.700 - 01:01:05.752, Speaker A: We really only need to convince a small number of people who are in these right decision making positions. And I think a lot of these people are already starting to come to the fore, especially in like larger institutional players. So like I said, I think it's exciting and I think it's going to be bizarre also to see who like and when, like this whole Franklin Templeton laser eyes things like was really surprising to me and some of the news cycle stuff is hilarious. So I'm like looking forward to more.
01:01:05.776 - 01:01:08.576, Speaker B: Absurdity slowly than all at once.
01:01:08.728 - 01:01:09.980, Speaker A: Yeah, yeah, exactly.
01:01:10.280 - 01:01:38.344, Speaker B: Cool. Well, thank you again, Alex. It was really, really awesome conversation. I really appreciate your depth, sharing the differences from a technical perspective, but also the product perspective. I think that balance of both is rare and excited to see what you guys continue to build in these decentralized systems because again, I really think it's the early days and what you're building has the chance to really change quite a bit. So thank you.
01:01:38.392 - 01:01:55.550, Speaker A: Yeah, it has been a blast coming on here, nerding out with you. I appreciate all the targeted questions. It got me to think about new things. I'm excited as always to talk about you and other folks in the community about what makes appdaas so special, why I'm excited to be building there, and what we bring to the table in the Akania protocol that we built.
