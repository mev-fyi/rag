00:00:00.280 - 00:00:16.750, Speaker A: Thank you, Avery. Super excited to be here. I think this podcast ultimately was one of the most requested. Everyone's super excited to learn more about Aptos and what you're building here. So really appreciate the time to sit down and chat with you.
00:00:16.910 - 00:00:20.278, Speaker B: I appreciate as well, Logan. I'm excited to share whatever I can with you.
00:00:20.454 - 00:00:49.900, Speaker A: Awesome, awesome. Cool. I think on a high level I try to start off the podcast from a little bit of background on kind of you. I know in your past you were kind of at the Apache Software foundation. Could you kind of talk a little bit about your work there and kind of like the open source nature of it and then maybe what kind of intrigued you on like the blockchain space?
00:00:50.200 - 00:01:26.150, Speaker B: Sure. So, yeah, I guess I've been working open source for a long time. My PhD work was in high performance computing. I worked on a couple pieces of open source software, MPI, the Mpitch implementation at Argonne, which included MPIo, as well as a file system called the parallel virtual file system, which is also open source. In the past and when I was at Yahoo, I worked on a project called Apache Giraffe, which I kind of co founded. And during that time I was leading that project and eventually became vp of the project. And as well as that becoming a member over time from this Apache Software foundation.
00:01:26.150 - 00:01:47.842, Speaker B: During my time in data infrastructure at Facebook, I continue to work in the open source space with Korona, with Spark, with Hive and a whole bunch of other open source projects. I think that's just been something that I've always enjoyed doing. I enjoy working with others in community, I enjoy the ability to put out code there and see what people think about it. And I think blockchain makes perfect sense for this to of course be a part of that journey.
00:01:47.906 - 00:02:05.908, Speaker A: Definitely. That's awesome. No, definitely appreciate all the contributions to the open source community. Yeah. So maybe jumping off that, what kind of intrigues you about the blockchain space and then we can kind of dive down into what more you're building here at Aptos.
00:02:06.004 - 00:02:43.384, Speaker B: Absolutely. So having worked in infrastructure for a long time, and my background is again in high performance computing, data infrastructure, it's always been about scale, about performance, about reliability and deploying systems that work to serve huge amounts of products at places like meta. And when it came to blockchain, I think the interesting thing about it was the ability to change society through technology. That was really intriguing to me. It's not just about making it faster, better and more performant and cost less money, of course, over time, but the ability to impact society for the better is just something that is a really unique opportunity within tech.
00:02:43.472 - 00:03:44.780, Speaker A: Yeah, definitely. Now, I've kind of had a funny journey as well into the crypto industry, but I think the biggest thing that I've kind of been saddened by today is just the lack of users in this space. And I really, really do appreciate what you are building and kind of what the ecosystem is now kind of heading towards is like trying to try different versions of like, actually bringing blockchains to the mainstream. So on that, I would love to learn more about Aptos on a high level and maybe a little bit to back up on that. I think one question that I hear on crypto Twitter a lot is there's so many different layer ones or L2s and all these different architectures. Why do you feel like Aptos is needed today and maybe some of these other different design or architecture choices will not be able to kind of onboard like the hundreds of millions of billions that we ultimately want to get to?
00:03:45.440 - 00:04:23.932, Speaker B: Yeah, it's a great question. So maybe it's worth thinking about, like, where our journey kind of started as a technology platform and is that. I think a lot of the team here came from. We were working on a project within meta Facebook at the time to understand how blockchain would be useful for the many billions of people that are coming to Facebook on a daily basis. And looking at some of these are challenges in that space. I think what we found was generally it's about user experience, and you kind of break them in two different sections. One is the things like how do you interact with wallets? What is your thoughts around key recovery and key safety? How do you make sure you're not signing transactions that are going to do bad things to your account? Essentially, there's that side of things.
00:04:23.932 - 00:04:47.474, Speaker B: This is the system side of things, which is how much is it going to cost because you can support high transactions or not, what is the latency? And that affects user experience as well. And there's some use cases that can be supported by it. And so it combines these two problems that we thought we would build technology for and solve. And that's the starting point of where diem was, and I think where Aptos is taking it to a much broader and a much more generalized problem in.
00:04:47.482 - 00:05:13.410, Speaker A: The space that makes sense so holistically. I try to always approach this from, I used to be at Tesla taking the Tesla first principle type approach to blockchains and these ecosystems. From that first principles kind of standpoint, what do you feel are like the biggest bottlenecks of blockchains today that hinder performance.
00:05:13.790 - 00:05:17.102, Speaker B: Yeah, performance specifically.
00:05:17.166 - 00:05:30.534, Speaker A: Okay. Or like why? Yeah, I would guess like more like general performance or like better, like user experience. Like all the things that, like, we ultimately want to get to with like hundreds of millions of users.
00:05:30.582 - 00:06:12.990, Speaker B: Absolutely. So today, I think a lot of people are reluctant to jump into the space and use it because of the fraud and the hype and stuff that's been associated with it. The fact that a lot of blockchains are unable to be stable these days or might get congested, pay very dynamic fees because of that congestion. I think the difficulty of networks to upgrade over time is very important. This space is super early. If you think about from a technology perspective, all the blockchains in the world combined can't even do a single MySQL server can do today. And you think about a company like Ameta or like Amazon or others have hundreds of thousands, if not millions of SQL servers to support one or two applications.
00:06:12.990 - 00:06:49.660, Speaker B: And that just kind of tells you where we are in the industry. We have a long way to go, and so there'll be lots of new ideas in the l one space, in the l two space, and l three, whatever and beyond. And I think, you know, as an industry, we need to just kind of try these different ideas and see what works over time. And at Aptos, we think that our approach of being focused on great ability actually is the most important thing. The fact that you can start on a journey and that journey can end up in a very different place because of the fact that you can continue to move the network forward with respect to technology and use cases and the primitives that people need to do to enable their use cases that we don't even know about yet.
00:06:49.970 - 00:07:21.330, Speaker A: That totally makes sense. It's been interesting in the past, it was more of an ethereum maxi, but it's been kind of interesting now just watching their progression and how long it's taken them to try to upgrade the network from proof of work to proof of stake, and then also ultimately introducing sharding. So maybe if that is the key component, what are the team doing here at Aptos to enable better upgradability for the protocol over time?
00:07:21.450 - 00:08:11.234, Speaker B: Yeah, it is a great question, and it's kind of two things I think about. So one thing is there's a technology aspect, which is how do you actually do upgrades quickly and reliably? And so one of the things we've done was when we designed the blockchain together with move in the first place, we made a design decision to actually store the on chains, the state of the configuration of the blockchain on chain itself. And by this, what you can do is whenever you change that configuration, everyone sync state, they learn about it, they actually upgrade at the same time. This kind of makes the upgrade process really seamless. So when we deploy this technology in kind of like a more private, dominant setting, we're actually able to upgrade the consensus protocol with no downtime, deploy new frameworks with no downtime. And we think this is a really powerful way for us to continue to evolve that network instantaneously across a set of validators. I think the other aspect is really good testing and for lack of a better.
00:08:11.234 - 00:08:45.392, Speaker B: It's not exciting to talk about necessarily. At Meta, we've done a lot of work to manage hundreds of thousands of millions of machines deploy. Some of our spark deployments were easily across hundreds of thousand machines. The schedule I built was scaled up to more than half a million machines. In those cases, it's really just about good processes in place to make sure that you have good CI CD, you have good chaos testing, different stage rollouts and different kinds of testing at every level of the stack. Again, not super exciting for a lot of people, but I think these kind of engineering practices are really important for being able to roll out code both quickly and reliably.
00:08:45.536 - 00:09:24.600, Speaker A: No, I definitely agree. Yeah, I'm super, super excited just for the space to move forward, there's definitely a lot of stuff that needs to be improved for us to actually get to mainstream adoption. I'm glad you guys are spearheading that. So maybe to jump into Aptos tech specifically, maybe you could just start off with what is Aptos, why are you building it? And kind of a general overview, and then we can go into some of the more technology specific features of it.
00:09:24.680 - 00:10:04.610, Speaker B: Absolutely. So going back again to our journey of how we came with the base core pieces of technology three plus years ago, we had a lot of discussions with our team at Meta about what it would take to build a reliable, smart contracts platform that would scale to billions of people. And with that, we came with two pieces of the puzzle. One was the move language and the other was the blockchain aspect of it. And these were co designed to work in tandem and to work in harmony. The upgrade process, I think, is a good description of how we leverage those two components together to make that process seamless. The blockchain itself, though, has evolved a ton, and I think at Aptos we've taken a pretty big departure from its original design.
00:10:04.610 - 00:10:34.570, Speaker B: I think we've learned a lot from the previous iterations and also I think we have some new ideas which we're really excited about. We think about transaction processing as something that we want to fully utilize the resources of the machine. I think again, my background in high performance computing is like let's make sure we're maximizing every single possible resource, even if it means shipping off computation to network cards or doing RDMA access on network cards. There are things that we would do that are really about fully utilizing all the resources. I think that same thought process of bringing it to blockchains I think has not been fully explored.
00:10:34.690 - 00:10:35.710, Speaker A: I fully agree.
00:10:36.090 - 00:11:30.812, Speaker B: What we try to do is take a pipeline approach, just like your cpu, modern day cpu, you think of a kind of this super scalar architecture where you have pipelines and then you have certain units that can operate in parallel. That's the way we take approach to transaction processing. So we have a phase for transaction dissemination, we have a phase for ordering of transactions, we have a phase for the parallel execution, then also batch storage, and finally what we call ledger certification. Essentially, I think this is pretty unique. I think in the space it allows us to have each of these stages working in parallel and operating independently, which allows again for us to fully utilize resources on a single machine. It also provides us a very unique opportunity to expose different kinds of finality primitives to users, as well as for sharding this in various ways to different kind of machines in the future for scalability. So I think that's something pretty interesting with respect to our approach, with respect to parallel execution.
00:11:30.812 - 00:12:08.860, Speaker B: I think that's something that a couple of networks have done out there, not too many. And I think our approach is very unique here as well. With block STM, you know, other, other frameworks have done things where they kind of force you to declare your dependencies upfront, your read write dependencies, and that's a reasonable way to actually understand the parallelism of the transaction, transaction with respect to others. Our approach is different. It kind of is leveraging the old database technology to think about optimistically executing transactions, where you get to preserve determinism as well as order. But many transactions do not conflict in reality. And in fact we can extract that inherent parallelism and not have to put those restrictions on the programmer.
00:12:08.860 - 00:12:29.100, Speaker B: This does not break atomicity the way that other forms do. With this now a proer can actually do all kinds of complex operations within a transaction, determine winners of auctions or other dynamic information they just don't know about before the computation has been completed and they'll have to break and have all or nothing semantics essentially.
00:12:29.260 - 00:13:02.420, Speaker A: That's awesome. I'm super excited. I would love to, definitely will eventually break down all those, but I totally agree. Exploiting the full potential of modern hardware. I think it's been kind of comical today that most of the virtual machines and execution environments are single threaded and that they don't use very many resources, and it's just kind of limiting the overall throughput. So super excited that you guys are ultimately going. I think the route that has to be required to scale.
00:13:02.420 - 00:13:25.284, Speaker A: Yeah, maybe just touching a little bit more on kind of like the hardware specifically with that. So obviously you're doing the parallel processing. Could you maybe break down your execution environment in virtual machine? I think you started to touch upon it, but I think it'd be awesome to go into.
00:13:25.412 - 00:13:54.110, Speaker B: Yeah, so we use move and the move virtual machine alongside it. And the move virtual machine is single threaded, so we just execute a whole bunch of virtual machines on a single box to get that parallelism. And the idea here is what we're doing is inherently determining at runtime the dependencies, the breed and write dependencies, and looking for conflicts. Whenever we see a conflict, we have to execute those set of transactions that are conflicted, but otherwise we can just proceed in parallel and then make sure those results are resolved at the end.
00:13:55.210 - 00:14:14.896, Speaker A: I know some blockchains, ethereum, for example, is doing global fee markets, but I think one unique thing about the parallel processing is being able to do, or one feature is the possibility to do fees per contract. Or are you guys exploring that, or are you doing a global fee market?
00:14:15.008 - 00:14:48.230, Speaker B: Right. I think for now we are doing a global fee market, but we definitely are interested in the work around the more localized markets with respect to smart contracts or accounts or any kind of other interesting primitives in that space. I think there's a lot of novel work to be done in that space. Just in general, multidimensional resource pricing as well is a very interesting area for us, and I think that's where upgradability is really important. We can start with some basic things for now, and we can actually explore those areas and make sure we deploy things that we think are going to be adding value to customers without just getting too crazy in production, essentially.
00:14:50.610 - 00:14:57.346, Speaker A: Sometimes it can be a little, you can overengineer a lot of things and then ultimately people don't want them.
00:14:57.458 - 00:14:57.826, Speaker B: Exactly.
00:14:57.858 - 00:15:32.006, Speaker A: So it's good, I think, on a high level, to try to keep it as minimalistic as possible. Cool. Yeah, I am generally super excited for the parallel processing. I think it's funny that it's taken this long for the space to move this way, but extremely happy that it is cool. I think in the beginning you mentioned some unique ways that you're doing the data dissemination and the transaction ordering. Let's start with the data dissemination and how that's happening. The Aptos blockchain.
00:15:32.078 - 00:15:49.382, Speaker B: Right. So data dissemination is. So I think starting with data dissemination a little bit, I think people tend to think consensus is slow. I think this is often a misnomer. Consensus is not generally slow. In fact, consensus is very fast. The reason why consensus is thought of to be slow is because people pack all kinds of junk in there with consensus.
00:15:49.382 - 00:16:14.612, Speaker B: They have nothing to do with consensus at all. Consensus is just about agreement. And so in our world, what we're trying to do is move to a world where I. You're agreeing on an ordering of something, right. And that's important. In our case, we're agreeing on an order of these blocks from transaction dissemination, but it's not actually necessarily having to have those transactions with you before you agree in the ordering. I think that's really important.
00:16:14.612 - 00:16:54.610, Speaker B: Only when you finally need to execute, that's the point at which you need those transactions. Once you break it up in these different stages, I think it becomes very obvious that consensus is not a bottleneck at all. And so going back to transaction dissemination, this is the bottleneck, actually. This is the problem where you have every validator talking to other validators to make sure that they're synchronized with the current state of the blockchain. So that being cut into its own phase, I think, is some of the key innovations that our research team has actually developed over time and in conjunction with our engineering team. So there was a piece of work called Darwin Tusk that I think you met earlier, Sasha. So Sasha and Radhi and others had worked on it with a bunch of other folks as well at Novi.
00:16:54.610 - 00:17:46.400, Speaker B: And I think the key insight really was this decoupling was the blocker in terms of unlimiting performance with respect to consensus. And so in this case, we are taking an approach where each validator will basically generate blocks of transactions, and then those blocks will be then sent to other validators in which they will agree to store them. Once you receive a quorum of f plus one, at least validators, you have a sense at least one honest node is stored your, your set of transactions, and then you can kind of propose the metadata for that block within ordering. We'd like to go with the two f one variant. The reason why is because if it's one, then actually you have a signature of f one. You have to search all of them potentially to find at least one from the worst case scenario that actually has that block. Whereas in two f one scenario you kind of expect the value to be more than half actually contain that block.
00:17:46.400 - 00:17:58.350, Speaker B: But this tendency to in practice have very little difference in terms of overhead from networking perspective. And like I said, every valid eventually needs to get those blocks, regardless if you get f one versus two f one.
00:17:58.430 - 00:18:05.930, Speaker A: And I think that's pretty misunderstood as well. I don't think there's a way around the two f plus one ultimately.
00:18:06.630 - 00:18:40.034, Speaker B: So two f one is definitely required for the safety guarantees here. But there are certain characteristics. Again, you can get by in a sense that as long as one odds value is stored, at least you're good. But again, from a practical consideration, that's probably not the best way you want to run a system. And also, the reality is, even if people are running validators as best as they can, they may not be acting malicious ways. There are machine losses, there's power outages, there's failure of SSD's and other things. So look, people may be accidentally malicious is some of the concerns we have.
00:18:40.082 - 00:18:50.008, Speaker A: That makes sense. One interesting component I think you guys are doing is the batching part. What are the batches size before those are submitted?
00:18:50.114 - 00:19:27.236, Speaker B: Yeah, so the batching is something that I think is a tunable parameter. I mean, I think we'll want to make sure we have constraints to not make batches too large, because that causes a host of other issues. But we think it's a trade off between latency kind of, and basically efficiency. Batching is something that's a very common use technique in almost all systems to get very high performance transaction by transaction processing will be horribly inefficient because of the overheads involved with that. And so being able to. One of the things I'll give an example of is when you do a batch of transactions, you get a quorum of signatures on it. Someone has to verify the signatures.
00:19:27.236 - 00:19:47.548, Speaker B: So depending on your batch size, you have proportionate number of signature verifications to do in your network. If your batch size is one, that's a lot of signature verifications, a lot of signatures being generated, a lot of storage with respect to transaction and so on and so forth. So we think of this as an efficiency parameter and we will play with it kind of according to the needs of our network. We haven't decided the exact amount of time to batch.
00:19:47.684 - 00:19:50.880, Speaker A: And is the communication overhead n squared?
00:19:51.300 - 00:19:58.148, Speaker B: The communication overhead of broadcasting transactions to other nodes is going to be. It will be n squared, yes.
00:19:58.204 - 00:20:28.780, Speaker A: Okay. Very cool. No, I'm super excited about it. So we touched upon narwhal, maybe a little bit more on Tusk and that unique property, because I think obviously, your team at Nova Facebook ultimately did a lot of amazing research, and I think some of that, like, people are still trying to wrap their heads around. So I'd love to touch upon that as well.
00:20:28.940 - 00:21:04.356, Speaker B: Absolutely. So I think there's a whole line of this kind of work that started even before. There's like a paper even before that, like, all about dag or something like that, or dag rider and a whole series of dag based works have been studied for a while. And I think, you know, also, to be fair, there's also long history even of DAG based protocols prior to this to also note that point. I think Tusk is an interesting take on things. It's asynchronous in terms of networking complexity. And I think when you think about Bullshark, that's a partially synchronized protocol and a very simplification on top of it, you could think of as the PBFT to BFT of protocols with respect to dags.
00:21:04.356 - 00:21:46.664, Speaker B: And we are excited about both of those things. In practice, there are some trade offs that are very interesting with respect to finality time. So with the partial synchronous protocol, it's generally much lower. I think Bull Shark is, I think, two and a half network round trips compared to the more it is inside of tusk. Of course, an distinguished protocol works better in certain circumstances as well, when you have malicious actors and a lot of network latency issues. But it's about trade offs right here and what's best for your network at the time. The other concern I'll just bring up as a trade off is, with respect to asynchronous protocols, is the notion of garbage collection.
00:21:46.664 - 00:22:20.684, Speaker B: You need to make sure that you can't be attacked by people trying to fill up your storage, either intentionally or intentionally. And it's a huge problem, actually. So that's something that, again, taking something from research into production can be very difficult. I worked in recent, I did a PhD in hyperforce computing. We can spin out a paper in three, six months. It's fine, get some amazing results. But prior to that work, going into a production grade setting where you can trust it for something as important as decentralized assets, we have a very high bar for ourselves before that gets in there.
00:22:20.684 - 00:22:29.004, Speaker B: Makes sense, but we are evaluating it, we are working on it, and I think we'll keep iterating our consensus protocol using our upgradability mechanism in the future.
00:22:29.132 - 00:22:34.020, Speaker A: Today you're not currently using a DAG, but you're exploring it?
00:22:34.100 - 00:23:00.656, Speaker B: We are currently exploring it, yeah. We already have some of the pieces already inside the code base. We've been doing some experiments with it. The other thing I'll share a little bit earlier is that we also don't see this as the bottleneck today. So no matter how fast your consensus protocol is and how fast your transaction dissemination is, there are other factors. Like for instance, how fast can you execute? What are the number of resources on your machine? Because we find that to be the bottleneck. Actually currently it's not about the consensus protocol at all.
00:23:00.656 - 00:23:08.616, Speaker B: It's just about how powerful is your hardware and can you actually support all those I o ops and things like that?
00:23:08.768 - 00:23:30.180, Speaker A: I've always found it funny that people initially have been trying to run blockchains on raspberry PIs and then ultimately laptops. I think ultimately to support hundreds of millions of users. It's going to be very hard to run it on just smaller hardware.
00:23:30.560 - 00:24:07.156, Speaker B: It's going to be tough. We've been trying our best to make it better, though. One of the things we do is state synchronization obviously is something very important for full nodes. One thing we've been doing is exploring, and we have this already implemented. I think our nodes already do this today where you can synchronize the trust from the validators. You know that they've signed off on the execution of transactions and you can just reapply those transaction outputs to your database without having to re execute. So as long as your IO devices can kind of keep up and your network can keep up with it, you can kind of keep pace with the blockchain without having crazy amount of cpu and processing power to support you.
00:24:07.238 - 00:24:22.860, Speaker A: Very interesting, very cool. So that was Narwhal and Tusk. And are you using. You mentioned Bull Shark, but Bull Shark is more dag oriented, correct?
00:24:23.920 - 00:24:29.448, Speaker B: All those are dag oriented protocols, so we're exploring variants of them and I think even something better possibly. Thank.
00:24:29.464 - 00:24:57.126, Speaker A: Excellent, very cool. And then ultimately one thing that you also mentioned was some of the unique things that you're doing on the execution environment within the virtual machine with block STM and not having to explicitly state upfront which kind of contracts that you're going to be interacting with. Could you talk about why that is such a big deal and a little bit more on Block STM.
00:24:57.238 - 00:25:38.702, Speaker B: Yeah, absolutely. So just going back to the approach again, block STM was, and these are interesting things because they're nothing, they're not conflicting in some sense. Our first approach to block SCM was we have this kind of increasing history of transactions by version. So we all have this version history that increases and similar to Ethereum in some sense, but how can you make that work as efficiently as possible? There are some really benefits for having that history. So one example is we timestamp every transaction out there. In other systems where you have complete parallelism, you can't timestamp those things accurately because they're actually, they're updating clocks. They need to get the full parallelism dependencies upfront.
00:25:38.702 - 00:26:40.196, Speaker B: Having that single dependency would cause a block for all those transactions. And so when we were looking at that approach, block STM was a very nice way for us to extract as much parallelism as possible out of the transactions without having to change the program environment one bit. So every transaction that would have worked prior to block STM worked after block STM, and it's just a matter of now, how do you kind of make sure that you have a very efficient engine to extract those dependencies at runtime and then execute them? The other thing you can do, which is neat, is you can reorder transactions to exploit that parallelism. So even though I have a sequence of transactions that hit the same shared resource, you could reorder them in a way with a bunch of other blocks or across the same block to make sure that you can extract as much as you can and get full resource utilization. To be honest, if you're sticking on a single machine today, you got maybe 816 cores or something like that. So there's not that many lanes of parallelism you can extract from your single machine. Now, over time, as we explore sharded infrastructure where maybe a machine's got, you have several machines behind there, you might get more lanes of parallelism that are available to you.
00:26:40.196 - 00:26:47.720, Speaker B: But at least today, as we have very simple operating environments, this is an easy way to extract that parallelism for sure.
00:26:48.780 - 00:27:06.612, Speaker A: I think one of the, another misconception in this space is the importance of the blockchain timestamps. Yes. Could you talk about why that is important? And again, maybe go a little bit deeper into how you're doing that and the problem that it fixes or addresses.
00:27:06.756 - 00:27:48.544, Speaker B: Yeah, we think an accurate clock is just a very valuable primitive on chain, and so it's actually integrated with part of our consensus protocol. Someone proposes a timestamp, it has to be better than the previous timestamp. People have to have their own clocks past a timestamp prior to voting and syncing state and all those good things. And so you kind of have this BFT based supported timestamp that is extremely accurate. You can see our Devnet clock is very, very, very accurate and maybe one of the most accurate clocks in the world. And with that you can use it on chain for all kinds of different contracts. So when you think about oracles bringing data into the system, you can timestamp that data, verify what the data you got from off chain sources, and make sure that there's a good amount of accuracy with respect to those sources.
00:27:48.544 - 00:27:57.454, Speaker B: When you think about smart contracts that want to end at certain periods of time based on real world events, obviously having that accurate clock is a very important thing to support those kind of use cases as well.
00:27:57.552 - 00:28:10.870, Speaker A: Yeah, I totally agree. How does the clock, how do you ensure decentralization within the clock to avoid a single source for the point in time?
00:28:12.250 - 00:28:26.394, Speaker B: We call this a BFT based clock protocol. Since it's part of consensus, it actually follows the same principles. As long as two one nodes are honestly having a good clock, we're good to go. If not, then, well, we don't have the VF two guarantees anymore.
00:28:26.442 - 00:28:37.146, Speaker A: That makes sense. Very cool. Yeah, I think that's widely misunderstood. I think so too, kind of in the industry of like why it is important. Awesome.
00:28:37.258 - 00:28:50.914, Speaker B: Also, sorry, sorry to interrupt you, but with respect to auditing, especially when you think about auditing transactions in the real world, when people are having legal lawsuits or other things like that, to understand when things happened, that clock provides something very accurate for them to depend on.
00:28:51.002 - 00:29:05.122, Speaker A: Yeah, yeah, sorry. No, no, it makes sense. And I think you also touched upon the DM BFT V four slightly, maybe going to that. Yeah, I just want to make sure I cover all kinds of things.
00:29:05.146 - 00:29:05.570, Speaker B: Yeah.
00:29:05.690 - 00:29:08.298, Speaker A: Components that would love to secret sauce.
00:29:08.474 - 00:29:52.198, Speaker B: So going back a little bit further on that process, at the time, I was the tech lead for the consensus side of things on the project, and we did a lot of exploration. It was fun. We looked at probabilistic protocols, we looked at VFT based protocols, we looked at different variants of DAG based protocols even at that time. And we ended up with starting off with something called hot stuff, which we thought was a nice improvement upon kind of PBFT, much being simpler for view change, but reasonably it had a lot of gaps, especially with respect to liveness. How do you implement this kind of active pacemaker problem. And so I think we've started off with one version we've iterated many times. I think this is the thing I was talking about where we upgrade the kind of protocol in practice in a private mainnet multiple times and so on.
00:29:52.198 - 00:30:50.364, Speaker B: That fourth version of the protocol, some of the innovations I think that we worked on are pretty cool that I love to talk about. One is this idea of leader reputation. I think it works really well for decentralized environments where you have participants which may go down at various times or may have lost a network connection due to some worldwide events. This allows us to adjust the rotation not just based on the staking, but also based on performance on chain. Every validator is watching the votes that have happened, the proposers have successfully proposed, and then using that information to bias the proposal towards the nodes that have done really well in the past network and just supporting a decentralized environment really well. The other thing we've done is actually the HaasA protocol was a three step protocol, and we were able to turn it into two, which improved the latency quite a bit. I think it's one of the fastest, if not the fastest, consensus port out there with respect to that latency aspect.
00:30:50.452 - 00:31:46.100, Speaker A: Very cool. Yeah, it's interesting just being able to do all the individual components of the blockchain and then ultimately being able to make small improvements, or in some cases large improvements on each, and combining those and how different the real world performance actually is, even on comparable hardware versus a single threaded virtual machine, or the performance stacks pretty quickly. Very cool. One other thing that I saw in the white paper that I was very interested about is the possibility of the multi chaired ledgers or the multi horizontal scaling with the different ledgers. I know once the mainnet goes live it's not going to be in the first iteration, but I would love to learn the vision for that over time.
00:31:47.300 - 00:32:47.642, Speaker B: I think going back to again what we're trying to do, what cloud computing has done for the Internet is just incredible. What AWS and GCP and Azure and even internal clouds within companies like meta and others just enables this massive scalability of applications and the way we know it today and all the services that are provided for it, we think that blockchain needs to do the same thing for web3. Going back to my analogy of MySQL server, we have a ways to go. The only way we see that going forward, forward, honestly, is through a combination of sharding techniques. One is going to be the idea of vertical sharding where we have each valid getting more powerful, turning to cluster machines potentially over time, or just getting better hardware. But that has limits, I think. I don't know how many operators can run a data center in a decentralized fashion, or is it going to just be ten people around the world operating all the data centers for decentralized infrastructure? If that's something that we want to end up at as an industry, the only way around that then is the notion of horizontal scalability.
00:32:47.642 - 00:33:39.416, Speaker B: Thinking about this again from an analogy of MySQL server sharded MySQL for us, we want to explore both directions. We will do so in parallel, which is how we do all the things. But the horizontal solution, I think is really interesting to think about. A world where you have a single language move which understands the nature of shards, wallets understand the nature of shards. It's an integrated experience. We call this the homogenous sharding platform where developers have a chance and users have a chance to decide how they want to shard their data. It's not up to the system to do it based on the use cases, based on their needs, they will find a way to do that, integrate into wallets perfectly, and then having one single language and one infrastructure for controlling it all still, and then depending on what single token is the approach we like to take with that.
00:33:39.416 - 00:33:46.240, Speaker B: But these are all still very early things, and like I said, we'll rely on upgradability to kind of bring these innovations forward as time progresses.
00:33:47.180 - 00:34:17.974, Speaker A: Upgradability is a key theme. Was nice. And then I think maybe last point on the overall technical archer, and if I miss anything, we'll definitely come back or can circle on that. The white paper also mentioned the ability for light clients. Yes, could you touch upon why introduce the light clients versus just having the full nodes and ultimately the benefits of them there?
00:34:18.062 - 00:35:30.126, Speaker B: It's a great question. We go back and forth on this one a little bit internally, I think like clients provide an amazing opportunity for someone to verify the state super cheap from their mobile device or from underpowered raspberry PI's. And I think in general, going back to our earlier point about having more security in the space, having a user experience where you have end to end security, end to end verification that your account balance is what you think it is, it hasn't rolled back, it hasn't rolled forward, it hasn't done anything strange to you. It's not relying on a TL's Cert only to verify that this data is correct, but something more important, like this proof and then again, talking about time, this proof, providing a timestamp to know that this is current as of 10 seconds ago is something that's really important. So we think like clients are pretty needed thing in our industry or things like it where people can verify the state of their data without having to recompute all the transactions in the history or just trusting again, the goodness of a server to provide the right data for them. And even those, no providers may accidentally provide incorrect data or out of date data. Just they might be trying their best.
00:35:30.126 - 00:36:07.378, Speaker B: But it's just that now that you have this end to end verification that I checked my balance and I know today that it's $20 and I know that as of when it was $20 and I know the history of when that happened. A like client is a pretty valuable thing now. It comes at a cost. The cost is you need to kind of generate these miracle trees which are going to verify state and then also have the validator sign off on that state. That's our fifth phase of the execution, which is we call it ledger certification. We do that though today's support line clients and we've taken a hybrid approach where we don't generate all the time, we generate it periodically. This is to actually allow us to take advantage of those batching operations I mentioned earlier.
00:36:07.378 - 00:36:35.468, Speaker B: If you generate a tree every time you have an operation, that's a very expensive process. If you batch up those operations and then generate the tree in bulk, you can actually remove a lot of duplicate confrontation and then provide for a more efficient like, client experience. And so that's the approach we've taken for now. This is a tunable thing. This is something that is controlled by transaction itself, like a state, like a proof generation transaction. And we can play with that over time or even change the proof structure over time. So today it's a Merkle tree, tomorrow maybe it's a vertical tree, maybe tomorrow, another day.
00:36:35.468 - 00:36:41.500, Speaker B: It's incremental hashing function. We can try all kinds of different things that work best for the customer.
00:36:41.580 - 00:37:10.260, Speaker A: That makes sense. No, it's super exciting. I'm always curious. Yeah, I think like clients are not only internal debate, but also a very popular discussion topic amongst the industry and kind of their different roles. So awesome to kind of hear your general thoughts on it. On the overview of Aptos. Did I miss any of the key core components that you think make it special or that we haven't addressed yet?
00:37:11.240 - 00:37:12.688, Speaker B: I think we talked to most of them, yeah.
00:37:12.744 - 00:37:45.540, Speaker A: Okay. Okay. Very cool. So kind of like I know all that was very technical and in the nitty gritty of the details. But how does that translate to the real world performance? I think ultimately the end state that I want to get to is the ability where people don't know the web. Two analogy about IP addresses, or the backend, or even Google or AWS or what infrastructure they're using. They're just using it and it works and it's easy.
00:37:45.540 - 00:37:54.702, Speaker A: What kind of real world performances does all these innovations ultimately unlock from the user perspective? On aptos?
00:37:54.886 - 00:38:44.282, Speaker B: It's a great question. So what we think is that if we do our job really well at the infrastructure layer, this will support the scalability of lots of new applications coming forward. I think just today with the again, single SQL server approach that the combined industry has, how many Internet applications could you run from a single share in MySQL instance? It's not a whole lot. And I think when we get to that scale, where infrastructure becomes more available and prices go down and scale increases, we will see that massive adoption. But I think it's not just, again, going back. It's not just the systems problems. We need super low finality times, we need really high throughput, but we also need to solve those user experience challenges.
00:38:44.282 - 00:39:54.994, Speaker B: For me, one of the things that we talk about a lot is key recovery. Key recovery is great. A lot of wallets are doing it today at the wallet level, but I think at the L1 level, it adds a lot of value. If you have key recovery schemes built in, some of the issues we saw with friends in slope would have been avoided in the past because maybe you import those into a new wallet, you rotate your key is the smart thing to do, and then you're not subject to any flaws that happened in the past with respect to that key being divulged to other parties and then thinking about even things around. Again, why wouldn't I not recommend my mother to use this yet? She's probably gonna lose her key. And how does it work in a world where you can have contracts that support things like semi custodial access where you control your key as long as you keep interacting with your account, and then maybe you don't interact for a period of time and a custodian has the right to rotate your key, and this is all happening on chain. So you know when this happened, you know when it's time stamped, you know exactly like if there's some laws to be filed, who you're going to file the lawsuit against, and it's very easy for these things to be transparent and available for all wallets that interact with the Optos blockchain.
00:39:54.994 - 00:40:03.242, Speaker B: It's not something that's just built for this special wallet. And then someone's got to reproduce that whole line of work that's very intensive. They get it for free, out of the box.
00:40:03.386 - 00:40:22.940, Speaker A: Yeah, I love that. I think ultimately, outside of the core infrastructure, the wallet and that private key management is definitely the most important thing. All the. It's. It's definitely a tricky problem. So have you guys started to play around with different functionality to enable that, or is it.
00:40:22.980 - 00:40:52.732, Speaker B: We are. We are trying to play with it, I think so. Our cryptographer is actually enthusiastically shoving BLS signatures into accounts as well right now as we speak. I think multisig BLS is something that a lot of folks are excited about. We have multisig anyway for accounts. We also support signal signature EdDSA 5519 and we'll keep adding more primitives over time through upgradability, as you would guess. The other aspect I think is beyond the blockchain is to add some of the protections.
00:40:52.732 - 00:41:24.288, Speaker B: I think you've seen websites today, like in Chrome. If you go to a maliciously known website, Chrome will warn you that this is a place you probably shouldn't go. I think that same idea where you have wallets that understand before you sign a transaction, you can do a simulation and that's something we support out of the box with aptos and infrastructure around it. Understand? Are you interacting with contracts that are dangerous? Warning, you probably don't want to do something with this player. Those things I think will also really improve the user experience a lot and we want to make those happen as.
00:41:24.304 - 00:41:36.922, Speaker A: Quickly as possible, for sure. I definitely agree. Are there any explicit numbers on the throughput or megabytes and expected finality that you're looking at?
00:41:37.056 - 00:41:53.574, Speaker B: Yeah. So for finality times, I think you can check our Devnet today. It's extremely. Well, the devnet's only four machines. People propound it pretty hard, but it's the finale. I think we do something like 15 blocks a second, which is really fast.
00:41:53.662 - 00:41:56.130, Speaker A: And again, how many are the blocks?
00:41:56.870 - 00:42:40.100, Speaker B: These are very small blocks going back to our clock. Again, we try to generate a clock as fast as we a new clock timestamp as quickly as we can. And also there's some other operations that happen in the metadata transaction. This just happens very quickly. But we also can support in our aits we've seen sub second finale times pretty easily. It really depends again on the network infrastructure we have from a hot stuff and ApTos BFT perspective, it just takes two round trips before you hit finality. And going back to finality, I think, you know, it's a metric that's really hard to measure consistently across networks because they're so different and everyone picks a different metric.
00:42:40.100 - 00:43:06.076, Speaker B: One thing that we get from our pipeline design, which is really cool, is that you can actually understand how your transaction is progressing through the network. You have this first phase of transaction dissemination, you have a two one sign off on that. You have a pretty high likelihood it's going to be included in a block fairly soon, at least more than a certain number of nodes agreed to store it persistently. And then in the ordering phase, once it's been ordered, you know that as long as your transaction is valid, it is guaranteed to be executed in that order.
00:43:06.188 - 00:43:06.756, Speaker A: Interesting.
00:43:06.828 - 00:43:36.522, Speaker B: So even before it's executed, you know it's going to be included. And then you have the execution phase, and then you have the where, you know, the execution result itself. You could actually execute the transaction yourself if you knew it ahead of the validators. And then you have dispatch write, and then you kind of have this ledger certification stage. And so in the certification stage you kind of have not only the knowledge of the transaction being included, but also what the results are and signed off by the validators. But it really depends. So you can actually think about finality as like a series of finality times.
00:43:36.522 - 00:44:22.840, Speaker B: And depending on what you're looking for from a user perspective, to get makes sense from a throughput perspective, I think this is an interesting area. Again, from a systems perspective, whether I write 100 bytes in a single operation or I write 101 byte operations, they accomplish the same thing. I really get a little concerned when you think about our benchmarks in this space right now, which is TPS. TPS, it means so many different. How can you measure two completely different transactions in two different systems is very difficult. Going back to the parallel execution topic we had earlier, if you have to split your transactions up, you may have a very high TPS number, but you're not measuring the right things. If you're just thinking about what's the TPS here, what we think is the right benchmark over time is to understand use cases.
00:44:22.840 - 00:44:56.168, Speaker B: To say my objective is to mint a million nfts, or my objective is to have oracles bring data into the chain this quickly. And then what is my time to finality for those use cases? Not the number of transactions it took me to do it, but the finality of start to finish. And I think that hopefully we would love to actually run some of those benchmarks over time across some of the networks and see where we are. But those are the kind of benchmarks we think that are much more indicative of throughput and latency than what we have today.
00:44:56.264 - 00:45:23.270, Speaker A: I agree, I do agree. It's definitely up there with Lite clients in terms of difficulty to agree on within the industry. But I do like the way you're approaching it. One thing that you mentioned that caught my eye was how often do you refresh the timestamps? You said you're doing it fairly frequently, and there was 13 blocks in one thing.
00:45:23.430 - 00:45:24.062, Speaker B: Yeah.
00:45:24.206 - 00:45:25.902, Speaker A: Sub 100 milliseconds.
00:45:26.046 - 00:46:00.510, Speaker B: Yeah. Again, this depends on your network speed. In the current version of the Aptos BFT protocol, every block you can do a commit, essentially, and it takes two blocks to do, from a latency perspective, to do that commit. It just depends on how fast your network is. If you have a really fast ten millisecond latency between nodes, then you can get it in basically 20 milliseconds for a commit. But in a decentralized environment, in our aits, we see probably subsecond, but it's not going to be 15 blocks a second. We don't have those latencies across the node providers that we've seen so far.
00:46:00.890 - 00:46:30.966, Speaker A: Okay, that makes sense. And then one thing that we haven't touched upon that I know everybody also is extremely excited about is move, the programming language and all the exciting stuff that comes with that. And I think it's kind of a refreshing take from solidity and what's happening there. So could you talk more about move the programming language, why it needs to exist, and kind of the unique functionality it brings?
00:46:31.098 - 00:47:06.084, Speaker B: Yeah. So going back again to our history of move and the blockchain at that time, there was a lot of decision making at Novi to think about what would be the best user experience. And I think, looking around, we thought we could improve on that experience by building something from the ground up, from a smart contracts platform standpoint. And that was how Mova was developed. It's a fairly new language. It's something that hasn't been as stress tested in production, but the idea was to start small and iterate on top of it and then build out new functionality for it. It's safe, it's traumatic, it's hermetic, it has all these great properties.
00:47:06.084 - 00:47:40.528, Speaker B: It has the move prover, which can actually verify aspects of your contract, as long as you determine them upfront in a formal way. And what we've seen is that one of the things we're concerned, of course, with move is that how fast would be adopted. There's a huge inertia around existing ecosystems out there. And so that was one of the reasons why we put out our devnet in March of this year, to evaluate that and understand how people felt about it. We were very pleasantly surprised to see that people loved it. And I was a little shocked, honestly, because we know the tooling is not where we'd like it to be. We know that the language itself is still in its infancy.
00:47:40.528 - 00:48:04.666, Speaker B: It's still developing a lot. And like I said, there are a lot of primitives that are missing. There's a lot of things that we would love to add over time. But after we launched our Devnet, we immediately saw some transactions having the network. We saw people writing code, asking questions. In our discord, we saw tens of thousands of nodes connecting to our devnet, which is really fascinating. Interesting was a really unexpected, but surprisingly good thing.
00:48:04.666 - 00:49:03.138, Speaker B: And then we continue to have a hackathon in May where we brought in folks from various companies like meta and Snap and others, and also a lot of web3 developers that had worked on other networks in the past and said, hey, if you have 36 hours, what can you do with move? We were impressed to see people developing amms, NFTs, Dexs, and demoing those all within like 36 hours. They didn't sleep a lot and they drank a lot of coke and pizza, but it was just incredible and mind blowing. And there were a lot of questions, of course, and our team was on hand to answer as many questions as we could. But it was an exciting night, or 36 hours. And at that point we felt very comfortable. That move was something that people were excited about as a way to safely program smart contract development, even if it had some limitations in terms of expressiveness. And it doesn't have dynamic dispatch on purpose to make sure it's easy to verify and some other things, but people seem to really love it.
00:49:03.194 - 00:49:33.182, Speaker A: Yeah, I've heard good things. Yeah, a lot of people have said good things about it and excited to kind of see it developed. Yeah, maybe talk just like staying on this topic ultimately, and I think you kind of touched upon it on a high level, but the unique properties that move specifically allows and why it's kind of a safer programming language instead of something that could be more expressive.
00:49:33.326 - 00:50:21.544, Speaker B: Right. So I think without things like dynamic dispatches, it's easier to formally verify aspects of language. That's, I think, one of the key things about it. The other thing is it has scarcity built into the language itself so it understands resources and the constraints on resources you can put in things like where you can't drop money on the floor, you can't duplicate money, you don't treat it like an integer and it's actually got certain semantics you just cannot break about it. I think those kind of things just generally trying to make sure we have less chances for programmers to shoot themselves in the foot. And we think that when you work on decentralized assets and the safety of those assets, that these new paradigms, while they may be a little bit restraining and constrictive, they provide that safety that is necessary for reaching billions of people.
00:50:21.672 - 00:50:37.340, Speaker A: Yeah, that totally makes sense. Very cool. No, I'm excited for everything that you guys are building here and all the unique innovations that you're bringing to the table and ultimately excited for Mainnet. Do you guys have a date yet?
00:50:38.160 - 00:50:39.660, Speaker B: I think officially we say autumn.
00:50:39.760 - 00:50:40.440, Speaker A: Autumn.
00:50:41.020 - 00:50:56.444, Speaker B: Autumn, yes. We're trying to stay a little vague here, but I think we're at the end of our ait three. Well, ait three is ongoing right now. That's incentivized testing. At our third one, we're stress testing the heck out of everything, and we're excited about the participants that are joining us. And then shortly after that, we expect Mainnet to launch.
00:50:56.532 - 00:51:26.730, Speaker A: Awesome. Very cool. What do you feel like is going to be like some of the biggest bottlenecks for that onboarding? Do you think it's going to be like the culture or the developer onboarding? Do you think it's going to be continuing the parsing part, the individual technology pieces, and ironing out those? What do you think is going to be the hard part for maybe those initial 10 million users?
00:51:27.870 - 00:52:08.826, Speaker B: Yeah, great question. I think right now our users are, of course, the end users who would be leveraging the decentralized applications that people build on top of us. A lot of our users we work with today, though, are developers. And so we've been very fortunate to have a really strong developer community come together in a very short period of time. I think there are more than 150 projects publicly building on top of Aptos today, and there are many more that are kind of still in stealth mode or looking for people to team up with. It's super exciting to be a part of a community that's growing and this fast. And, you know, we're trying to support everyone as much as we can.
00:52:08.826 - 00:52:48.742, Speaker B: Some of our teams are actually in the office with us, and it's really exciting to have them share their experiences in building apps on top of apptos and also using move and what we see as improvements and opportunities in the future. So I don't know. I mean, we feel like we need to continue to focus on developers. We need to continue to focus on these safer wallet experiences that I talked about. And I think we are making good headway in that direction. We've been sharing all the things we can do with wallets and our blockchain with these teams that are respectively working in those areas. And just keep pushing, just keep pushing and iterating.
00:52:48.846 - 00:53:13.360, Speaker A: Yeah, I love it. Very cool. What would your message be if you could talk with the developers or the Aptos community that would love to get out and either you want people to learn more about or just on the community aspect that you're doing here.
00:53:14.140 - 00:53:49.760, Speaker B: Yeah. Going back to our name, Aptos means the people. We're very much a society focused network. We invite everyone to come and build and use, participate, and we're very community driven. We hold move Mondays every Monday is almost without fail, a couple exceptions when there's holidays and such. And we also have a very active discord group, more than almost 100k people in it now, which is really fun as well. We have forums that we can get deeper interactions on and we have lots of folks coming by the office all the time, which we love.
00:53:49.760 - 00:54:02.652, Speaker B: So definitely come and get involved. We are very friendly people. We love to chat about anything, anything related to web3 or even beyond web3. And that's, I think, the kind of community we're looking to build.
00:54:02.756 - 00:54:12.636, Speaker A: Awesome. I love that. I try to, in my kind of like wrapping up the podcast, I've kind of. I like doing spicy questions.
00:54:12.668 - 00:54:13.560, Speaker B: Oh, okay.
00:54:14.460 - 00:54:45.080, Speaker A: I don't know if I have any particular. I did a podcast yesterday with gentlemen on the Celestia team. They're doing modular blockchains. I'm curious. I asked him, maybe just, I'll reuse the question. Outside of Aptos and what you're building, what blockchain do you think is doing things like uniquely well, and then what is a blockchain that you think is doing things like uniquely bad?
00:54:50.970 - 00:55:02.510, Speaker B: I think it's a tough question. I guess that's an answer. In general. I love to see the industry moving forward, and so people are trying different ideas in different spaces. I am excited about all the ideas. There's a lot of l one's for a reason. There's a lot of l two s for a reason.
00:55:02.510 - 00:55:27.812, Speaker B: We need to continue exploring the space as an industry. I think that's really important. And we are one of many participants there. But I think all these explorations are ultimately going to help us to find the best practices and the right way to build the infrastructure for billions going forward. So things going badly may turn out to be good in the future. Things going good maybe turned to be bad. Or hopefully this combination of knowledge will help us to get the infrastructure where we want to be in the years to come.
00:55:27.916 - 00:56:02.576, Speaker A: Yeah, I totally agree. I've talked to a handful of people now and they always ask me what they're building. I think everything will work. It's just to what degree and what things you're optimizing for. Do you want scale? Is it focusing on decentralization? They all kind of have their own unique trade offs. And I think the hard part and one of the reasons why I wanted to start the podcast was to break apart those individual components and allow people to tell their stories and what they're trying to optimize for, because I do appreciate that everybody's trying different things, and ultimately it does help the industry move forward.
00:56:02.768 - 00:56:03.592, Speaker B: Definitely.
00:56:03.736 - 00:56:20.500, Speaker A: Awesome. Cool. Well, I hope we touched upon everything that you're looking to get out of this. Was there anything that you miss or that I miss, or maybe any closing thoughts?
00:56:22.880 - 00:57:22.334, Speaker B: I think the closing thought I might have is just that it's going to take time for us to get to the point at which we need to be. As I mentioned before, cloud computing is something that enabled Internet services. We know today people just have some patience for us to do the same thing with web3 infrastructure. And that one thing, again from my experiences, is that building distributed applications is hard enough. Building distributed applications that are also parallelizable in nature, which I've done in HPC world, is even more difficult. And then yet doing the same thing with worrying about a decentralized environment and all the interesting interactions that happen there with permissionless development and uninhibited interaction is yet in order of magnitude harder. And so for people to wait for that technology to get there, like, you know, we're working as hard as we can, everyone's working as hard as they can to make it get there, but it's going to take time, and we should all just work kind of hard to accelerate that as quickly as possible.
00:57:22.462 - 00:57:37.598, Speaker A: I love it. And again, I really appreciate your time. A lot of people are extremely excited and I am very thankful for you to have sat down with me. I'm excited for people to listen to this one. It's very good.
00:57:37.734 - 00:57:38.606, Speaker B: Thanks a lot, Logan.
00:57:38.678 - 00:57:39.470, Speaker A: Awesome, thank you.
