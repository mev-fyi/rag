00:00:00.120 - 00:00:12.030, Speaker A: It's been a while. Did a podcast with you, one of the first, I think, technical podcasts that you did, and then did a podcast with Jeff as well. But how have things been since, like, we chatted?
00:00:12.110 - 00:00:21.598, Speaker B: Things have honestly been pretty insane in good ways and in bad ways. So, I mean, yeah, first of all, thanks for having me on as, I guess, a repeat guest.
00:00:21.694 - 00:00:22.582, Speaker A: Yeah, thank you for coming.
00:00:22.646 - 00:00:44.554, Speaker B: Last time we chatted. Yeah, man. I mean, this was like, probably one of the best technical pods that I've had a chance to come on. So it's been just fantastic. And, like, whenever we've had conversations with, like, third parties, I often just share the conversation we had. Like, I think it's been over a year ago now, so it's still pretty, pretty timely. And, yeah, man, I mean, since we last chatted, I would say there's been a couple of big things that have happened.
00:00:44.554 - 00:00:56.310, Speaker B: The first big thing is, Steve, you went live, so say it's currently on Mainnet. This happened August of 2023. It's may right now. So how many months is that? Nine months if I did the math.
00:00:56.690 - 00:00:57.162, Speaker A: Yeah.
00:00:57.226 - 00:00:57.474, Speaker B: Yeah.
00:00:57.522 - 00:01:08.086, Speaker A: I mean, it's no easy feat to actually ship code to produce it, run live and see people poke holes in it and see a lot of learning lessons. Learn about.
00:01:08.238 - 00:01:44.540, Speaker B: Oh, yeah, I mean, tons of learning lessons. I think in our case, it's been pretty fortunate that, like, everything has been just very stable on Mainnet. But leading up to Mainnet, there was, like, definitely the Pareto principle at play where, like, everything, there were so many just last minute issues that came up in like, the couple of months leading up to it that it was just incredibly, incredibly hectic. Yeah, we went live on Mainnet in August, and then November of last year, we put out the Save e two proposal. So save two is the first paralyzed EvM. And that really was able to strike a chord, I think, with the community. So that's just been one of the biggest things we've been head on recently.
00:01:45.400 - 00:02:11.070, Speaker A: I guess maybe for the listeners, why did the SaI team or what did you learn from nine months, eight months in production that was like, hey, we need to not change ships, so to speak, but grow the piece and start to look at what we can do in the ethereum space and help scale with, like, the ethereum ecosystem with save e two.
00:02:12.610 - 00:02:53.982, Speaker B: Yeah. So just as kind of a refresher for any listeners that might not be familiar, save one is the fastest blockchain in existence. It's had a sustained around 400 millisecond finality since August. And I would say when we went live on Mainnet, from the technical side, everything was fantastic. Chain was super stable, chain was extremely fast. And then we kind of started asking developers like, okay, what would be there? Like, what could be done from the technical side to help improve the developer experience. And there was kind of just one consistent message that we kept hearing, which is, we want the EVM if we want to build on, say.
00:02:53.982 - 00:03:25.850, Speaker B: And that's honestly the biggest thing that we build conviction up around in the past, like, year or so, which is the idea that the EVM is here to stay. Nearly every single crypto native engineer right now, they are EVM developers. If you look at the distribution of developers right now, it's basically VM has almost all the developer activity sea level. So Solana's virtual machine comes second. Solana. I mean, I think it's around like ten to 15%. So it's not like a tiny portion, but it's also not a huge portion of developer activity that's happening on Solana.
00:03:25.850 - 00:03:59.276, Speaker B: Then essentially every other vm has close to zero. Move has pretty close to zero. Cosmosm does not have that much activity happening right now either. Same thing with fuel. We tried to understand why this is the case, because it's not just that most activity happens on the EVM, but most EVM developers are very resistant to the idea of going to a new execution environment. I think part of it is technical. If you're already familiar with the intricacies of the EVM, then you're scared to go to a new execution environment, because any bug that is there in your code can just result in your entire project getting drained.
00:03:59.276 - 00:04:29.992, Speaker B: Like, literally, one bug can just kill your entire project. So that's a very scary proposition. But I think it's beyond just technical limitations. I think fundamentally, the EVM is more of an ecosystem. And what this means is it's not just the tech stack, but it's also the tooling the mind, share the community. And that makes it really difficult to convince people to convert and go to a new execution environment. So that was the inspiration for why we realized, okay, we need to be supporting the EVM.
00:04:29.992 - 00:04:36.968, Speaker B: And then that kind of led us down the path of thinking to ourselves, what is missing from the EVM? And that ultimately led to save e two.
00:04:37.064 - 00:05:53.430, Speaker A: Yeah, I think one of the things that I've been most impressed by you and the team is just how quickly you ask for feedback, take feedback from the real world, and then actually go and implement something. And I think I kind of, as you're explaining it with, say, v one to save you two is really a testament to the team that you have built and just being able to iterate quickly. And I think you and I have kind of known for a long time, even with sev one, that parallelization is really going to be a requirement for scaling blockchains to mass adoption. I think historically they were kind of nice to have while the industry wasn't really requiring scale. But as we actually go from low millions to hundreds of millions and then eventually billions, parallel processing and high throughput are both going to be requirements. So can you maybe talk a little bit about, say, v two in itself and what you guys were doing to essentially enhance the Ethereum virtual machine to allow the current engineers that already have the developer chops within the Ethereum ecosystem to expand the functionality of what they can do?
00:05:53.940 - 00:06:22.410, Speaker B: Yeah, of course. One thing I wanted to touch on real quick, like you mentioned how fast we're moving, and I definitely do agree with that. It's been interesting to just see the pace at which, say, moves compared to, honestly the rest of the crypto ecosystem at large. I think there's a couple of reasons for that. The first is just the philosophy that we have internally. We were very much a team that grew up in the bear market. We were not a sexy project by anyone standards when we got started.
00:06:22.410 - 00:07:13.574, Speaker B: A couple of young founders that don't have a huge Persona in crypto already, it's pretty difficult to get vc interests around that. It's pretty difficult to get the cold falling that you see with perhaps some other projects that we're able to get started with. We've had a very strong underdog mentality from the get go. We've basically had to fight for everything that we've been able to get so far. But I think this creates a, like, when you have this underdog mentality, you do tend to do things very differently than if you're a project that just has like infinite resources. You tend to be laser focused on what moves the needle, and as soon as you identify what that thing is, you just focus on like making that, like just devoting all your resources to that and like really optimizing that as much as you can. So in our case, like, building as quickly as we can is something that is just, I would say, second nature for the team now.
00:07:13.574 - 00:08:17.636, Speaker B: And that is one of the biggest differentiators between a team like save versus several other teams that might move a lot more slowly with regards to seib two. So, yeah, I mean, we basically realized we need to support the EVM. And then we started thinking to ourselves, what is that single focus point that is missing from the EVM? What is the single biggest limitation for the Ethereum virtual machine right now? And the simple answer to that is throughput, which, I mean, you were talking about how we haven't really needed to have a significant skill for anything that's been happening in the past, because there just weren't that many users in crypto. As soon as you start having more people that want to make use of this global state machine, that leads to limitations. And if you look at Ethereum l one, this kind of manifests itself in different ways. When you have low throughput, aka low transactions that can be processed every second, this leads to higher gas fees, and this also leads to a more restrictive design space for developers with regards to higher gas fees. In the past six months, there have been times when it's been like more than 100 GUI per unit of gas on Ethereum.
00:08:17.636 - 00:08:55.222, Speaker B: And the end outcome of that is it's just ridiculously expensive to do anything on chain. If you need to spend like a do a swap on your swap, you're pricing out essentially the entire human population. So it's just completely unusable for most people. And even I think beyond that, it severely restricts the design space the developers can have to work with. Because if you're a developer and you're kind of constrained to this like 50 transactions per second type of environment, you don't really have much wiggle room in terms of how you're able to build your applications. There's a very kind of rigid way that you need to design your apps. And this leads to things such as an example would be automated market makers.
00:08:55.222 - 00:09:45.470, Speaker B: Amms don't exist in traditional finance because they're not as capital efficient, but they have basically been forced into existence in crypto because they're one of the things that allow you to trade with this restricted design space. Launching, basically, we thought to ourselves, throughput is the biggest limitation. How do we solve that? And that's where this idea of parallelization came in. If you look at the Ethereum virtual machine as it stands right now, it is single threaded. For any listeners that might not be as familiar, a single threaded vm basically means that if you have 100 transactions, you'll start, and you start executing them or start processing them, they'll get processed one after the other. And that's incredibly inefficient. And it doesn't take advantage of the modern hardware that you have to be able to actually process these transactions more efficiently.
00:09:45.470 - 00:10:41.956, Speaker B: If you're listening to this on a laptop, if you're listening to this on a phone, your device will have multiple cores and it's capable of handling multiple work streams at the same time. And if you basically start paralyzing transactions and you're able to take advantage, you're basically able to make software level optimizations to take advantage of the hardware that you already have to be able to get better performance. And that's exactly what, say, b two does. It paralyzes the EVM. And perhaps one quick mental model for the listeners here would be it basically lets you build something like Solana that has super high performance, except it builds it with the EVM as a first class citizen. The reason that that's incredibly exciting is because the EVM is where essentially all crypto native activity is happening right now. So if you're able to give the EVM the type of performance that you see with chains like Solana, you're just going to have a massive opportunity in terms of the new developers you're able to onboard, and in terms of the types of users you can support.
00:10:42.068 - 00:12:01.996, Speaker A: Yeah, it seems fairly obvious that with kind of the arc of history for the last 1015 years, parallel processing was going to become best practices. I think Solana was the first vm to implement parallel processing. And it always seemed kind of like a natural evolution that the Ethereum virtual machine, being the first smart contract platform, would have parallel processing for one reason or another. It's just taken the team, the Ethereum team, quite some time to do. And then, as you mentioned, it's not only the parallel processing that increases throughput, but also just being able to propagate a lot more data. And I think, again, to me, what was always interesting about, say, v one was being able to focus on the parallelization, the high throughput, the unique trading experience that you guys unlocked. And now bringing that higher throughput and parallelization to the EVM seems like a natural next step, not only for crypto, but just the entire EVM landscape, which does, to your point, have the most engineers, it has the most applications, and you should be able to, in large part, drag and drop your GitHub repo into, say, with v two.
00:12:02.108 - 00:12:58.076, Speaker B: Yeah, I strongly agree with that. It makes sense why Ethereum got started the way it did without really focusing on performance. Because this idea of a global state machine where people can write smart contracts, that was completely novel at the time. And it was definitely not like, it was not as high priority to support scale as it was to just build something that worked in the first place. So it makes sense that Ethereum got started by just focusing on functionality and then scaling was going to be a secondary issue that they were fortunate to have to deal with. In Ethereum's case, it's really difficult for them to support things like parallelization at the base layer with the philosophy that they have around, um, the type of hardware requirement that they want their nodes to have. Because essentially one of the biggest, like as soon as you start supporting greater throughput, as soon as there's like more transactions that end up being processed per unit of time, what you start seeing is more state that gets created.
00:12:58.076 - 00:13:37.054, Speaker B: Um, so for any listeners that might not be aware, state is essentially the data that needs to persist on any full node to be able to process any incoming transactions and to be able to generate state routes. There's, I mean, several different parts of state, but like it's a rough mental model. You can think of it as being just account balances. Like I have Tensei, Logan has 20 say so just like balances for the number of tokens each account has. And then also smart contract state. This uniswap pool has this many tokens, and if you have more transactions that are coming in, there's more state that gets generated. And then this date needs to persist on all these full nodes.
00:13:37.054 - 00:14:25.500, Speaker B: And if you don't persist all this state, you're not able to generate a state route for after you're done processing the next block, and then you're not really able to keep up with the chain. So if you have the kind of philosophy that ethereum has with its full nodes, or you want to have extremely minimal full nodes, you just can't support something like parallelization because it becomes extremely difficult to be able to store all this data. So yeah, I mean, with regards to state, this concept of state bloat is something that needs to be addressed if you want to be able to support parallelization. State load essentially manifests itself in two primary ways. The first way is with storage. If state starts ballooning and then you have let's say ten terabytes of data that needs to be stored. It's really difficult to do that on consumer grade hardware.
00:14:25.500 - 00:14:45.320, Speaker B: Then you start need to increase the hardware requirements. The second issue is around state sync. So let's say you're going to start running a new full node. You need to import all the data that was previously there in order to do that. That could take a long period of time. If it's ten terabytes of data that you need to import. That could take a while.
00:14:45.320 - 00:15:09.122, Speaker B: Let's say that you're running a validator, your machine goes down, you need to quickly start running a new machine. In that case, it could be just a security hazard for the network if it becomes too difficult to start syncing state. So those are a lot of the things you need to deal with. And I can talk about how we're addressing that from saes side as well. But there's a lot of things you need to start dealing with to be able to adequately improve performance.
00:15:09.266 - 00:15:38.692, Speaker A: So maybe to kind of parse it apart a little bit just for the listeners and how Saiya is approaching the parallelization. If we can maybe touch upon high level on the virtual machine, and we can even get into the nitty gritty of the runtime of opcodes, optimistic parallel execution, maybe focusing on the VM first, and then talk about how SEi is constructing the read and write access to state from the database side.
00:15:38.756 - 00:16:09.370, Speaker B: Yeah, let me just share my screen. So we have some nice slides from a presentation we gave a month ago. So, yeah, I mean, at a high level, I think this was kind of tying into one of the questions you asked before as well. What is save e two? There's four primary parts of save e two, all of which are shown in this diagram over here. The first part is the smart contracting part. So with save one, we only support cause and logs and smart contracts. With save two, we have interoperable EVM and cosmosm.
00:16:09.370 - 00:16:44.640, Speaker B: The way that this is built is that under the hood we're making use of get is the most stable implementation of the EVM that exists. What is processing, I think, around 80 85% of blocks on Ethereum mainnet right now. So it's just extremely battle tested and stable. So we're making use of that to process any incoming transactions. The way that this is built in the chain level is that it's implemented as a first class citizen. So it's essentially identical to cosmwasm in terms of how it's implemented. So if any transaction comes in, it's basically interacting with EVM as kind of a native part of the chain.
00:16:44.640 - 00:17:15.760, Speaker B: This is different than other implementations you would see. One example would be like neon on Solana, that's implemented as a smart contract on top of Solana. So there is downsides that come to that because it's not a first class citizen. Another approach would be something like Aurora on near where it's a separate shard and it's not interoperable, like not synchronously composable with the rest of the chain. So there's downsides that come with that as well. In the case of, say, it's like literally a first class citizen that's built into the chain itself. The second thing that we have is parallelization.
00:17:15.760 - 00:17:28.320, Speaker B: We make use of optimistic parallelization, which I can touch on briefly. We've also created a much more performant state EB. So a state storage layer, and I can touch on that as well.
00:17:29.620 - 00:17:52.240, Speaker A: So for someone that's maybe not super technical, kind of looking at these slides, what is the biggest breakthrough from v one to v two, outside of just the VM component switching or allowing engineers to essentially get higher access to throughput?
00:17:52.320 - 00:18:27.396, Speaker B: Yeah. So the two biggest improvements here, the two biggest sources, especially of performance improvements, come from optimistic parallelization and from SADB, which is the state storage piece. In terms of the smart contracting layer, supporting the EVM, like making use of get under the hood makes it so that it's fully backwards compatible. Anything that you have working on Ethereum, zero one, you can just take that and then deploy it to say, and it'll just work. So I think that part is exciting around the smart contracting layer, but from a pure performance standpoint, it's the parallelization in the state storage that help unlock that 100 x improvement in throughput compared to Ethereum.
00:18:27.428 - 00:18:27.780, Speaker A: All one.
00:18:27.820 - 00:19:03.740, Speaker B: So this is a diagram that the team made around parallelization. But at a high level, there's two ways that you can paralyze transactions. The first approach is where you have developers define dependencies. A canonical example over here would be Solana, where whenever a transaction is passed in, you need to specify what accounts are being touched by that transaction. The other approach is optimistic privation, which is where the chain is able to figure out dependencies. There's pros and cons to both approaches. We think long term optimistic parallelization is going to become just the standard.
00:19:03.740 - 00:19:42.550, Speaker B: I think there's a couple of reasons for that. The first is that with optimistic parallelization, where the changes figures out dependencies, it's better from a user experience standpoint, and it's better from a developer experience standpoint, because developers don't need to really do anything at all to get parallelization working. It's a lot simpler for them. And at the end of the day, if you think about like a blockchain as like a SaaS company, the customers that you are selling to are developers. And if you make their life easier, they will be more likely to build on you and have just a better kind of satisfaction rate. I think that's one part of it. The second part is around just backwards compatibility.
00:19:42.550 - 00:20:10.520, Speaker B: If you're going to be supporting the EVM, then you want it to be backwards compatible so that everything, all the innovation that has been done on Ethereum one can be deployed over to some kind of new EVM. And if you're not backwards compatible, if you need to update the smart contracts or change the way that transactions are passed along, that's going to make it much more difficult for engineers to deploy. So that's why I do think optimistic fertilization is going to start becoming the default in many cases moving forward.
00:20:11.380 - 00:20:47.610, Speaker A: It's interesting. I think Aptos community has said similar things. I know Monad is doing optimistic parallel execution. I think Solana and Swe kind of stand alone and they're kind of strict access list by making engineers state dependencies upfront, which to your point is very onerous on the engineers and kind of frustrating. But how does that kind of trade offs work from strictly a technical side in your mind?
00:20:48.260 - 00:21:23.610, Speaker B: Yeah. So in terms of the. I guess maybe I can cover how optimistic parallelization works first, and then, like the trade offs that come with that. So the way that optimistic parallelization works is you don't know what state each transaction is touching. And as a result of that, you basically try to run everything once, which is this first line of execution over here. Then from there, you see what state is being touched by each of these transactions. So if a transaction is not touching state that any other transactions is touching, then that's non conflicting.
00:21:23.610 - 00:21:59.158, Speaker B: That transaction is non conflicting. So in that case, you can just go ahead and commit it. If it is, let's say that there's like five transactions that are all touching the same state. Maybe that's some pool on uniswap because they're all trying to trade against that pool. In that case, the first transaction can be committed, but then the remaining four, they would need to be re executed. So because of that, if you need to re execute transactions a lot, then that will be, in theory, worse for performance. The two things to note with this, though, is that because you've already pulled essentially all this data that needs to be touched into memory.
00:21:59.158 - 00:22:52.300, Speaker B: When you're executing, like when you're executing these transactions the first time, there's a lot less overhead tied to just accessing memory. So that makes it much more performant. The second thing to note is that in the absolute worst case, let's say there's n transactions, you'd have to rerun things n minus one time but in practice, if you look at Ethereum L1, it's somewhere around 80% of transactions are non conflicting. So in practice you end up seeing much better performance on the realistic type of workload. I think in the block SCM paper that Aptos had put out, in the absolute worst case, I think it was a 30% worst performance compared to sequential processing. If there is a malicious attacker, you just get unlucky where all the transactions are touching the same state. Then you will have worse performance than the access list based approach that you were describing.
00:22:52.300 - 00:22:59.900, Speaker B: But in the normal scenario, it's going to be simpler for developers and you'll be getting just substantially improved performance as well.
00:23:00.850 - 00:23:10.630, Speaker A: So long term prediction, optimistic parallel execution wins outside of the strict access list because of the developer friendliness.
00:23:11.210 - 00:23:56.440, Speaker B: Exactly. Because I think that in the long term infrastructure, people that are creating infrastructure, their goal is to onboard as many developers as they can. The simpler it is for developers, the more likely developers will be building in an ecosystem. So I do think like in our case, like we did have the access based approach for V one, and that does add more overhead for developers. So my prediction is that it's just going to be the default for gains that are supporting parallelization to start making or to start moving towards the optimistic direction, especially if they're supporting the EVM. If there's a different runtime, I think it might be a different kind of design decision for them. But if you want backwards compatibility with l one smart contracts, I think optimistic parallelization is the way to go.
00:23:56.520 - 00:24:18.024, Speaker A: Yeah, that makes sense. How do you feel like kind of the strict access list versus the optimistic parallelization ultimately determine fees? Because I think that's one thing that other communities have highlighted potentially can be cumbersome or challenging.
00:24:18.112 - 00:25:12.338, Speaker B: Yeah, so if you have the strict access list based approach, then having something like localized markets on Solana, I think that becomes a much simpler thing to do. The way that like localized fee markets work is that you see what state every single transaction is touching. And then as the block producer, you can schedule transactions in a way to optimize the number of transactions that are being processed by. Like, let me see if I can explain the Solana approach. Well, you can correct me here if you have anything to add on, but like as a block producer you have like a local cache and usually like, let's say there's like 100 transactions that are coming in, then from there you're able to schedule them so that if there's like 25 transactions that are each touching like one piece of state and you can have four separate work streams happening simultaneously. Where each work stream is processing one of these different transactions. I believe in Solana's case, it like, it's set up so that at the minimum, you end up having, like, four different work streams.
00:25:12.338 - 00:25:28.428, Speaker B: That are being processed at any given time. So that encourages, like, better performance. Whereas if you have some kind of naive implementation where you don't have localized free markets, all the transactions that are coming in that you try to process might be touching the same state. You're not able to paralyze it as efficiently.
00:25:28.524 - 00:25:31.160, Speaker A: Yeah, that's my understanding as well.
00:25:31.580 - 00:26:03.968, Speaker B: Yeah. So I think in Solana's case, it becomes much easier to have localized fee markets. With that being said, I don't think that even with optimistic parallelization, it still is possible to have localized fee markets. I think it's more difficult to do that. It's not going to be as simple, but the fundamental problem is you have these transactions coming in. How do you figure out what state each of these transactions is touching? I think it's still an open research problem. There's a couple of ideas that I've been brainstorming around for what that could look like.
00:26:03.968 - 00:26:23.992, Speaker B: One approach here is just making use of block builders. Then block builders would just define as the block builders are composing the blocks. They'd see that all these transactions are like, this is a city transaction is touching. And then that can be passed along. You could just offload it to blockbuilders. Another approach would be having some type of inference. Where each transaction starts to, like.
00:26:23.992 - 00:26:46.182, Speaker B: You basically just infer what state each transaction is touching. Based off previous transactions that you've seen. This could be done in kind of a naive way. Where you basically have some kind of, like, logic at the chain level. Where, like, you see what the previous 100 transactions were. And then based off of that, you're able to, like, kind of like, guess what state is going to be touched. Or you could do this in a much more sophisticated or complex way.
00:26:46.182 - 00:26:54.090, Speaker B: Where you have some kind of model that is locally there on each node. And then from there, you're able to process it and figure out the dependencies.
00:26:54.390 - 00:27:20.400, Speaker A: I think that's a super cool idea to infer that. Then over time, as you understand what contracts are hot, so to speak, you're able to have a better and better understanding of, hey, this should most likely be able to be completely paralyzed. Or this is able to just be kind of skip this step. Based off, like, the transactions historically, it's a very interesting idea.
00:27:20.900 - 00:28:01.098, Speaker B: I definitely agree with that. And I also think one thing that is underappreciated here is that there isn't that much novelty and stuff that happens on chain. If you look at the type of stuff, if you look at the past 100,000 blocks on Ethereum, most of them are going to be touching the same sets of smart contracts. It's the same apps that that's where most activity happens. So I think inferring the state that a transaction is touching is actually going to be a much easier thing to do than people might expect, because it's a very limited type of activity that ends up happening most of the time. So I do think, I mean, in safe cases, this is something we're going to be exploring. But I do think this idea of like print based generation of dependencies that will become prominent in the future.
00:28:01.098 - 00:28:14.176, Speaker B: And with that, it's much easier to support something like localized fee markets, because if you're able to pretty accurately guess what state each transaction is touching, then in the default case, you're going to be able to block. Producers can schedule transactions in a much better way.
00:28:14.248 - 00:28:49.978, Speaker A: Yeah, it's definitely going to be an interesting problem, and one that I'm excited about, but I think more broadly, that AI crypto intersection keeps rearing its head and lots of different ways to make those synergies happen, but maybe shifting slightly part of it, I think what you guys are doing is specifically around the execution and being high throughput. And the other half of that's kind of the more back end infrastructure with SADB. Can you talk about what you have done from technology perspective with SADB and the unique innovations there?
00:28:50.074 - 00:29:15.128, Speaker B: Let me just share my screen again. Beautiful. Okay, so as I was saying before, there's two things you need to account for. The first is state storage, and the second is state sync. And with SADB, we're able to address both of these. And there's two different things that we've done as part of stateb. The first is a memory mapped IVL tree, and the second is async writes to disk.
00:29:15.128 - 00:30:00.266, Speaker B: So this, I guess the part on the right is around the memory mapped IVL tree. What essentially that does is there's this concept of an IABL tree, which is what we use to basically have the merkle tree that is then used to generate this dip root after transaction execution on each block. So what we were previously doing with JB one is there was essentially just one tree that was stored on disk. That's extremely inefficient. What we ended up doing is we took this one tree we split it up into three separate files that are stored on the file system. And there's different parts of the tree that are there with each file. So one of these files is for key values, another one is for leaf nodes, another one is for the intermediate nodes.
00:30:00.266 - 00:30:39.700, Speaker B: And by doing this, you don't need to store as much metadata. Previously, if you had the entire tree being stored on disk, then every single node would need to have some key value pair that you're storing. Every single node would need to have some left child, some right child, and some other metadata as well. If you move towards having three separate files, then you don't need to store all that metadata. Like, if you're an intermediate node, you no longer need to store this key value pair because that's not really as relevant anymore. If you're a child node, if you're like at the bottom of the tree, then you don't have any left or right pointers, like any left or right children. So in that case, you don't need to store that data either.
00:30:39.700 - 00:31:07.280, Speaker B: So when you move towards having a memory mapped IVL tree, you're actually able to get much less data, much less metadata that needs to be stored. And what we observed was actually a 60% reduction in the overall state size. 60% is huge. Like, this means you're able to. Yeah, no, it's a huge win. And it makes it much easier to have lower hardware requirements to be able to run the same type of. To be able to get just better performance.
00:31:07.280 - 00:31:58.348, Speaker B: The other thing that's really nice about this is when you have less date that needs to be stored, it improves dating times as well because if you're trying to run a new full node, there's less data that you need to import. And then setting up the tree after you import the data ends up being faster as well because you can paralyze instantiating that memory map to IVL tree. So when there's like a 60% reduction in state size, you can paralyze across these three files. When you're writing into your file system, um, you're able to get a twelve x improvement in sits in times as well. So that was huge. Um, the other thing that we also did, uh, just for pure performance, um, is async writes to disk. Uh, the way that that works is, um, rather than essentially the, the way that things would work for us before is after transaction.
00:31:58.348 - 00:32:50.466, Speaker B: Like, transaction execution will only finish after the entire state tree, uh, was written to disk. What we end up doing instead is we generate a state route in memory and then afterwards, that's when transaction execution finishes, when we generate a state route in memory, and then we go ahead and write to a write ahead log all the updates that need to be written to disk. So this is like the change set that is there in the diagram. So this gets written to write a headlog, which is similar concept to what you see in traditional web two databases, like postgres for example. As soon as something is there in the right ahead log and you have disaster recovery, even if that node goes down, then it's very easy to recover and you can just reference what's there on the right head log. And then from there you can go ahead and write it to disk asynchronously. By doing this, you don't need to write to disk.
00:32:50.466 - 00:32:55.910, Speaker B: So you get a 287 x improvement in commit time. It ends up being a huge performance win as well.
00:32:56.420 - 00:32:59.452, Speaker A: 287 X is a big number, Jay.
00:32:59.636 - 00:33:16.760, Speaker B: It's a very big number. So it's a graph kind of like this, where if there's low throughput, there's a smaller improvement in commit times because there's less data that needs to be written to disk. But as soon as you start hitting like that 50 00, 10,000 GPS type of scenarios, it ends up being just a huge improvement in the commit time.
00:33:17.500 - 00:33:45.640, Speaker A: So I appreciate the super technical explanation for all the engineers that watch the podcast. For those that are not technical, how would you explain this from the product point of view? What does this actually do for the everyday user that wants to use save e two and figure out how it can actually make their applications better so that they can create better user experiences?
00:33:46.060 - 00:34:20.636, Speaker B: Yeah, so I would say there's two parts of this. One is for people that are running infrastructure. It just makes it easier for them to run infrastructure on the network. The second is from the pure end user performance standpoint, you're able to observe much, much faster execution times over here. And this helps contribute to the performance that we've seen, like the 100 x improvement in throughput. As soon as you have commit times improving, as soon as you have parallelization, which helped improve performance as well. This will result in primarily two things from the, I guess, end user standpoint.
00:34:20.636 - 00:34:59.732, Speaker B: The first is cheaper gas fees. If you can go from supporting like 50 tps to 5000 tps, the gas fees inherently will go down. Assuming it's the same type of activity that's going to be happening. What we anticipate to have happening on, say, at the start is gas fees that'll be in the range of like around one cent I think there's trade offs to being too high. I think there's also trade offs to being too low. So at least from a normal user standpoint, we think that in this range of transaction is pretty reasonable and we have a philosophical design or decision or discussion around that as well. But yeah, so that's one of the things that will be beneficial there.
00:34:59.732 - 00:35:11.048, Speaker B: The second is developers now have a bigger design space which lets them build new types of applications which will then result in end users just having more stuff to be doing on chain, which is I think a double win.
00:35:11.104 - 00:36:27.082, Speaker A: Personally I'm the most excited about really as you mentioned, the larger sandbox, because I think historically it's been frustrating that VC's in my mind, fund so many infrastructure projects. But it's not really the fault of the infrastructure projects because when you're constrained till twelve transactions per second, or even less than 50 transactions per second, you need better infrastructure to enable application engineers to build more novel types of applications. And it sounds like this is doing so. And I think on the topic of maybe new VC's continuing to fund different projects now there is kind of movement labs that is trying to do parallelization from an l two standpoint on top of ethereum to help scale. You have slesia as like a high throughput data availability layer to increase throughput that l two s can utilize. You have a click that again is making use of the parallelization and then Monad which is integrating both the high throughput and parallelization. This broad landscape, how do you view, say within that or unique feature sets compared to these other blockchain architectures?
00:36:27.186 - 00:37:08.500, Speaker B: Yeah, I think most of the projects you mentioned there, they're not really competitive in any way, would say because we're focused on a completely different type of developer audience. We have conviction in the idea that the EVM is here to stay and that the EVM is like, that's what needs to be improved. If you're trying to support something that's an alt vm, l two, for example, movement. We think it's an entirely different type of. If you think of blockchains as SaaS companies, it's an entirely different type of customer that's being targeted. In Ruchi, for movement's case, his target audience is going to be web two developers. He's going to have to convince people from Facebook, leave their jobs, start writing move code and building on movement.
00:37:08.500 - 00:37:44.100, Speaker B: In the case of the EVM, there's already a massive amount of developers that are building with the EVM. So you empower those developers to support entirely new types of applications that benefit from this higher performance. I don't think it's really competitive with the projects you mentioned over there. I'm also happy to have a discussion around integrated versus modular, since you mentioned Celestia. But in general, we think that you need to improve performance, but you also specifically need to improve performance for the EVM. Otherwise, you're not really going to be catering towards crypto native engineers at all.
00:37:44.180 - 00:38:33.130, Speaker A: Yeah, I've always. To your point, it's simplifying the user experience for engineers to build novel applications, and they don't have to worry about infrastructure. And I personally have always just biased towards kind of the integrated change for the simplicity of the fact that when you have a bunch of l two s, not that necessarily they're bad. It's just that you increase the fragmentation, you increase the liquidity, kind of spread across all these different venues. And then it's kind of like engineers, when given too many choices, they don't know which choice to make. And by having kind of everything integrated together, I very much appreciate the simplicity or the dumbness of the architecture so that you can focus on just one thing, which is building the best user applications.
00:38:33.710 - 00:39:18.610, Speaker B: Yeah, I very strongly agree there. I mean, I think long term, our goal is to scale the EVM, and we think that there's different ways of scaling the EVM. We think that roll ups make sense L2s and having modular approaches, they make sense for certain types of use cases. But if your end goal is performance, you're not going to be able to get any better performance than with an integrated chain. And the reason for that is if you have, let's say three separate layers, let's say a separate execution layer, a separate settlement layer, and a separate DA layer that adds there's overhead tied to just performance overhead where you need to communicate between all these different layers. By definition, it's going to be faster to just integrate all three layers into one chain. You're going to be able to get better performance because there's no more communication complexity.
00:39:18.610 - 00:39:52.472, Speaker B: From the simplicity standpoint, I also think that it ends up being better for a couple of things. One of them is just around engineering complexity. If you have three separate layers, any issues that happen on any of these layers, let's say there's a bug in one of these layers that's going to affect the end user experience. So that explodes. It just makes it much bigger. The number of scenarios you need to account for in terms of bugs would be one example. Even things like reorgs if the DA layer has a reord, then everything that's built on top ends up having that impacts everything built on top of that as well.
00:39:52.472 - 00:40:06.432, Speaker B: So I think it just balloons the complexity around that. And then, as you were mentioning, there's also fragmentation of liquidity when you have a thousand different roll ups. I definitely do think that if your goal is performance, it just ends up being better to have one single chain where everything is built into the chain itself.
00:40:06.536 - 00:41:01.970, Speaker A: Yeah, it's removing complexity to focus on applications. And I think at this point where we are, I mean, Ethereum shipped in what, 2015? I believe we're almost ten years into Ethereum's roadmap. And I think the complexity has only gotten larger. And I appreciate teams like say, ultimately trying to take that complexity in house, say, hey, don't worry about it, we'll still give you the Ethereum virtual machine that you kind of know and love, but we'll expand upon that with giving you the tools to only focus on the application, because we'll take in the performance side, increasing throughput, increasing parallelization, build the best user application that can onboard 10 million daily active, monthly active users, and get us to the next leg that we need to, to grow the entire crypto pie.
00:41:02.510 - 00:41:30.898, Speaker B: Yeah, I definitely agree there. And the complexity point on Ethereum, like, I think that is just very, very timely. This entire kind of, like a lot of the innovation we've seen happening in the past couple of years, like molecular chains, there's just so much complexity tied to that with like for example, proposer builder separation with the mev landscape on Ethereum, even with separate DA from settlement and execution. The only reason that's happening is because Ethereum is not scalable. If Ethereum had been scalable, things would have been much, much simpler and I.
00:41:30.914 - 00:41:33.070, Speaker A: Think I would have stayed an eth maxi.
00:41:34.650 - 00:41:54.250, Speaker B: Exactly. Yeah. And I think that in long term we will start to see more and more of that activity start to happen in like simpler environment. Simplicity, I think is extremely important for infrastructure. The more complex any system becomes, the more blast scope there is for error and the less accessible it becomes for developers as well, to build on top of that ecosystem.
00:41:54.330 - 00:42:40.104, Speaker A: Yeah, fully agree there. And that's why I've always been a fan of people trying different things outside of the ethereum ecosystem to help push the industry forward, because I think we've been stuck for a little bit, maybe to push you slightly. I think the main, not competitors, so to speak, but main people, when thinking about parallelization of the EVM today, most people think about you or the Save Foundation's, say, engineering team focusing on parallelization of the EVM and then kind of also Monad being both integrated chains. I know Monad is in test net today. They're mostly closed source. If not, I don't think they've actually released any lines of code. So completely code closed source.
00:42:40.104 - 00:42:50.360, Speaker A: Is there anything that you would highlight differences between, say, and Monad? Or is it just too early to tell? Because a lot of it's kind of there.
00:42:50.900 - 00:43:37.698, Speaker B: Yeah, I mean, I think that they've done the kind of community that they've been able to build, the kind of excitement they've been able to build, like they've done a fantastic job of building from their side. In terms of technical differences, I think it's pretty difficult to comment until more things have been released out in the open. From a peer user experience standpoint, I anticipate it to be the same. It's going to be a fast, backwards compatible UVM. So all the same tooling will be usable, it'll be similar performance metrics you're going to see. In terms of how I think this is going to evolve, I think that for any vertical where there is a ton of excitement, there typically ends up being a couple of category leaders. For example, we saw this with arbitrum versus optimism anticipated to be pretty similar with like say in Monad.
00:43:37.698 - 00:43:56.074, Speaker B: And I think the end result of that is both teams can win. I think that as long as there's like, if it is a vertical where there will be a lot of developer activity happening and a lot of mine share, that's like kind of going in that, in that general area, I think there can be multiple teams that end up doing a good job there.
00:43:56.162 - 00:44:30.466, Speaker A: I think your pr team will be super happy with that answer. You killed it. But no, it totally makes sense. Maybe once you guys both, once Monad launch, once, say v two launches, any alpha there, we'd love to hear it, but we can have back you and keone on and talk about kind of the parallelization in the future. But I would totally echo the sentiments. I think broadly speaking, we're still at the 1 yd line for the entire crypto industry. Even though it feels like we've done a lot or market values are high.
00:44:30.466 - 00:45:14.178, Speaker A: It's still very, very early in terms of user adoption and I think that's why I got most excited and why I wanted to do frictionless just because it was so early. And that user adoption can come with these newer architectures that enable paralyzation and high throughput maybe shifting though slightly again, but we spoke about briefly, amms were really born out of the limitations of the low throughput blockchains. Order books are uniquely enabled because of high throughput and parallelization. Are there other types of applications in that category that you would love to see built on save e two when the EVM goes from low throughput to high throughput.
00:45:14.274 - 00:46:01.132, Speaker B: Yeah, so I think that having this like global shared, like global shared state machine, probably one of the biggest things that allows for is verification to be happening across a lot of different actors. So what I anticipate we'll start to see more of is use cases where having this verification happening on chain, use cases tied to that being enabled. Now, a couple of examples I'll mention around that. Outside of order books, one example would be social networks. I think a lot of crypto based social networks recently, in the past, they've only had a small amount of activity happening on chain, and the rest of it has been happening off chain, and they might try to decentralize that. But most of it is not something that is very easily verifiable on chain. We envision a world where everything will start to move on chain, because there's fundamentally no reason not to have everything be verifiable if it's easy and cheap to do.
00:46:01.132 - 00:46:36.304, Speaker B: So I think social networks where every single like comment kind of thing that happens, we anticipate that to start moving on chain. Another example here would be games. Most crypto games right now, it's like token movements that happen on chain, everything else that happens off chain. So we anticipate that there will be essentially all of that activity starting to move on chain as well. So I think it's still pretty early in terms of the applications that are starting to build with this bigger design space. But I do think over the next year, year and a half, we'll start seeing more and more experimentation happening around that on chain. On say specifically.
00:46:36.392 - 00:47:18.456, Speaker A: Yeah, there's just a huge sandbox. And whether it's gaming, to your point, more interesting applications with defi social, they're really all untapped areas. Just because historically it's been so limited in what you can do when you have twelve transactions per second. I'm just bullish, man. I'm very, very excited for what is to come, because you can build much more interesting things. And I think by not handicapping engineers with very low requirements, you can build much, much more novel applications. Any alpha you can drop or hints that you can give to the community on Wednesday, v two will actually go.
00:47:18.488 - 00:47:44.322, Speaker B: Live when save e two. Yeah. So obviously what is put out is h one of 2024 is when b two will at least the governance proposal will go out in terms of progress we've made from sale app side towards that. We're code complete right now. So the only new code that is going in are any small bug fixes or any kind of things tied to. I mean, there's like no new functionality that is being added. So we're code complete.
00:47:44.322 - 00:47:59.952, Speaker B: We've wrapped up two audits right now. So we've wrapped up the Ottersec and the Zaloc audits. So we're essentially good from that side as well. I think there's a few last moving parts that need to be accounted for, but it is getting very close to us putting out the governance proposal.
00:48:00.016 - 00:48:14.570, Speaker A: I'm looking forward to it. I think that will honestly be a huge moment. Is it going to be one to one compatible with current EVM or is it going to be, will you have to make slight adjustments in the code of.
00:48:14.640 - 00:48:55.510, Speaker B: Yeah, so it should be like we've had several projects that have built in other ecosystems coming over to say, in their case they've needed to make zero changes at all. The only difference between say and the EVM yellow paper is around the NPT, the Merkel Patricia tri versus in our case, the IVL tree. So if you're building anything that requires making use of an NPT, you need to update that code to have dependencies on an IVL tree instead. Um, but essentially no one actually has dependencies around that, unless you somehow need to look at some kind of state proof. And in that case, it's, uh, not actually that difficult to update it from like reading from an MPT to reading from an IBL tree. So kind of for both. Answer to your question.
00:48:55.510 - 00:49:01.446, Speaker B: Um, essentially for most teams there are serial changes and if you're doing anything type to state approved. So it'll be a small number of changes you need to make.
00:49:01.518 - 00:49:02.570, Speaker A: Very minimal.
00:49:03.230 - 00:49:04.438, Speaker B: Exactly. Yeah.
00:49:04.614 - 00:49:38.950, Speaker A: Perfect. No, it's. I think I. It is a very interesting design space because Ethereum has been so limited. It's kind of comical to me that even today when you add up all the l two s and the transactions per second that they're doing, it's still less than a solana of the world. And there's no reason why you can't have other high throughput blockchains that do parallel processing that have the EVM enabled that or can be competitive with that. And I think that landscape is really untapped and there's a lot of really cool things that you can do there.
00:49:39.370 - 00:50:09.252, Speaker B: Yeah. So, I mean, I've been tweeting about this, I think, for the past six months, how I think 2024 is a year for ally change, and I think we're starting to see that just play out in real time. You were mentioning like movement before. I think there's going to be several projects that are coming out this year whose sole goal is to help improve performance on both. I mean, the EVM and also like all vms as well. But I do think this idea of having like 10,000 tps be commoditized, like we will start to see that play out by the end of this year. So from user standpoint, I think that's going to be fantastic because that just makes crypto much more accessible to everyone.
00:50:09.252 - 00:50:22.300, Speaker B: And then we can start focusing on, as you were mentioning before, the recent new applications haven't been getting built is because it's such a rigid design space to be building them on chain. I think that'll help unlock the new wave of applications that we'll see probably around the 2025 timeframe.
00:50:22.460 - 00:51:23.440, Speaker A: I would love nothing more than user applications at scale. It would make me super excited. As we wrap up, I feel like there's a bunch of nit picky things that we could potentially get into with like decentralization or governance work, even like clients. I think we may or may not have talked about those in the past. I don't know if we can hash those out, but I think more importantly, talking about like the VM, talking about like the reads and writes, just the general performance updates are probably the biggest thing along with when it will potentially come to Mainnet. Is there anything like you really want the user to come away with or the engineer, someone that's watching the podcast? To take away from this podcast and a conversation. I know we went nitty gritty on the technical side, talked about some of the product use cases, but any like things that we would want to double or triple click on?
00:51:24.060 - 00:52:00.180, Speaker B: Yeah, I mean, just I guess a few sentences of a TL doctor from my side. We're currently working on SAB two, which is essentially the best of both Ethereum and Solana, where it's like taking Solana and building in the EVM as it first classes in over there, you're able to get the high performance that you see with chains like Solana while also having all the tooling mind share developer activity that comes with the EVM. Yeah, I mean, savvy two is 100 x improvement over the kind of performance that you see with existing chains. And it's going to be coming out. The golf proposal will be coming out of, I mean, very soon, from the time of this recording.
00:52:00.480 - 00:52:28.428, Speaker A: Almost leaked it, but we'll leave it there. Jay, thank you so much for coming on the podcast, as always. I always enjoy our engineering discussions. There's some of my favorites, so thank you again. We'll leave it on 100 x improvement over existing AVM implementations and ultimately the design space that that opens up, because I think, really, it's truly needed, and I'm looking forward to that future. So thank you again.
00:52:28.624 - 00:52:30.320, Speaker B: Yeah, thanks for having me on, man.
00:52:30.700 - 00:52:31.140, Speaker A: Thank you.
