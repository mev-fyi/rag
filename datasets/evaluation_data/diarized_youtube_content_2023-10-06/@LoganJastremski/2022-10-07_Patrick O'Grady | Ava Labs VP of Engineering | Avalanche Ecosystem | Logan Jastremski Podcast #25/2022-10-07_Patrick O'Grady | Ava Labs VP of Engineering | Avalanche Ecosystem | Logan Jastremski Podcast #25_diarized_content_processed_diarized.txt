00:00:00.160 - 00:00:54.660, Speaker A: Super excited to be doing this. Patrick, thank you so much for having me. Yeah, excited to be doing the first live stream and super excited to be getting into the nitty gritty details of Avalanche. Ultimately, I mean, I got really interested in Avalanche through my good friend Laura, and ultimately she inspired me to go to the Avalanche conference and met a bunch of awesome people and eventually got put in touch with Kevin and through that, kind of snowballed all my interests, had a really awesome conversation with Kevin and kind of breaking down avalanche from first principles. But I would love to kind of start this podcast and conversation with you. Just a brief background on how you got into crypto. I know you were at Coinbase for a little bit, but then kind of what ultimately, what got you excited about crypto and then what got you excited about Avalanche?
00:00:55.910 - 00:01:22.410, Speaker B: Yeah, sure. I'll give a brief introduction here. So I actually grew up in Wisconsin, so pretty far away from the tech scene in general. Growing up there, I actually didn't know anything about crypto at all in the midwest. And then when I came out, I went to Stanford. In 2014 is when I first got involved with it. So at that time, that fall, there was actually a.
00:01:22.410 - 00:02:06.942, Speaker B: A class taught by, at that time, Balaji, which was pretty funny. I think it was called CS 251 P. So it was kind of this side class, just about bitcoin, and everyone took it because you got a free 21 miner. So if you've ever seen one of these, I'm trying to figure out how to make it fit. It's like a raspberry PI with an ASIC on top of it. So I've had this on my desk now for the past eight years or something, just as a reminder of where we've come from, but so they have this huge shipment of stuff there. So I did that class, learned a lot about what was going on in bitcoin at that time, and I was just totally enamored with it.
00:02:06.942 - 00:02:45.250, Speaker B: I think the joke I always tell is at that time at Stanford, there were three things you could do. It was either AI rockets or crypto. And I tried my hand at AI. It didn't really go very well, and crypto and distributed systems always made sense to me, and so I just stuck with that. So, you know, I learned about it there. I think my faith wavered after the first crash in the 2015, 2016 a bit, but in 2017 went in full on really getting involved with it. And after Stanford went to Coinbase, I took a leave of absence to play around different crypto projects at that time.
00:02:45.250 - 00:03:31.540, Speaker B: So I actually took a two week break from school, went to Coinbase, went there, worked there for about a year and nine months or so on a project on the crypto engineering team called Rosetta, which Coinbase uses now to add assets to Coinbase. That was a lot of fun through that process. Got to meet a bunch of amazing crypto teams at that time. So added a bunch of assets to Coinbase in the process. And then through that, really just started to deepen my opinion on what projects I really wanted to work on. And at that time is conveniently when Kevin reached out to me on Twitter asking if I had heard of avalanche. And the rest is kind of history, but I left Coinbase in the beginning of 2021, took a one day break, and then went to avalabs.
00:03:31.540 - 00:03:51.160, Speaker B: So I left Coinbase on a Wednesday and started at Avalabs on a Friday. And then, yeah, have been here. Been here ever since. Beautiful, beautiful. The brief history. But now when I go home, people that unheard of crypto at that time were all like, should I buy? Should I do whatever? Should I. My rule is never to give anyone advice to buy crypto.
00:03:51.160 - 00:03:58.308, Speaker B: If they're asking me if they should think about crypto, it means that they should go into crypto. It takes more conviction than that.
00:03:58.364 - 00:04:25.460, Speaker A: Yeah, I made that mistake in the 2017 bull market, and I did not repeat that this time. So definitely some learning lessons each cycle, but awesome. No, I appreciate the background. Well, let's just dive into it. You're currently the vp of engineering at Avalabs. In your words, how would you kind of describe avalanche and how does it make it difference from other kind of layer ones in the space?
00:04:26.120 - 00:05:50.350, Speaker B: Yeah, I mean, I think l one s right now, I think I certainly am not a history major, but one of the more competitive landscapes I think I've ever seen, at least in recent, recent history, for sure. And it's interesting where you have a space where the users or the typical people that use it are pretty much wholly unaware and or uninterested in the underlying mechanics. And then every single project has war chests in the realm of, like, hundreds of millions of dollars to compete, uh, directly. Um, but, uh, you know, at Coinbase, uh, before I made the move to avalanche, I got a chance to really dig deep into a number of, you know, of the common projects at that time and some of the ideas that were coming out, I think, around then, there's a lot of hype around telegrams like Tom project, if you remember that, with, like, the whole, like, 3d blockchain stuff and everything. But, um, avalanche to me, uh, when I left and really wanted to do it, I had a few, uh, really high conviction, uh, opinions that matter to me when joining a new project. One of them, first thing was just speed to finality, I think so much time and effort and, like, UX has been put into handling you, like, the finality that takes on the order of minutes or even like, on the order of tens of seconds to make it feel like actual web experience for people. And Avalanche was the first project I saw where finality was really as fast as it felt like on any other web service I'd ever use.
00:05:50.350 - 00:06:45.628, Speaker B: And so for me, finality was the first thing that really brought me to avalanche. But typically, other projects that sought the same sort of finality made compromises elsewhere, either by increasing or decreasing the validator count in some meaningful way. So, like, okay, yeah, you can achieve crazy finality with five validators, right? Or by wrap or like, kneecapping the virtual machine they were using so that it was not composable, not in some way. Interesting for a lot of different applications. And Avalanche had three different virtual machines that were all running at that finality rate that had very different properties. And so for me, it was a question of finality, distributed participation and consensus. Like, how many participants are there, how many people that can actually produce blocks, how many people can generally participate in consensus? And then basically an expressive virtual machine framework, whether or not that was the actual virtual machine, or on subnets, you can create custom virtual machines.
00:06:45.628 - 00:07:11.980, Speaker B: Both of those check the boxes for me. So when we talk about avalanche, it's both the consensus as well as the platform. And so I had clear interest in both of those. Most people that come to us, I think, to work at Avalanche Labs, see the consensus and want to dig deeper and work on something that they have found very interesting just because of how different its properties are from a traditional PBFT approach, for sure.
00:07:12.360 - 00:07:40.388, Speaker A: I think one of the things that always caught my attention about avalanche was the ability for the subnets to have the custom virtual machines. I always thought that was fascinating. Let's break apart some of that you mentioned fast finality. Ultimately, what kind of enables that on avalanche? And why is it pretty fast compared to other traditional layer one blockchains in the space?
00:07:40.564 - 00:08:44.110, Speaker B: Yeah, so, I mean, I think first off, I'll talk about really how it works just to kind of give people a sense of what the consensus mechanism is in general. And then I'll go into some of the ways that people may consider doing different parts of it separately. So first off, avalanche consensus is a repeated subsampling protocol. There's this awesome website if you just search, I think it's like avalanche simulation by Ted, which is the most visually captivating thing. One of the most things I just go back to always and I can just sit there forever and just keep replaying it. But the idea is instead of having a more deterministic consensus protocol where you have a participant that proposes a block and then all the participants, or some super majority of the participants sign some payload, return those signatures to the proposer, and that block is somehow distributed to the rest of the network. Instead, how it works is participants will just query each other, or what we call poll each other, asking what their preferences are with data that they've seen.
00:08:44.110 - 00:09:41.514, Speaker B: So very simply, let's say that I have a group of 20 people and I say, hey, what do you guys think of the stuff you've seen? And then they return back what's called a chit or their vote of what, like their current preferences. And so the node then, you know, that's doing this poll or this process says, okay, do I have what we call Alpha over K votes within that poll? And if so, that's considered a successful poll. So like, okay, in this case, maybe 15 out of 20 people agree that a certain transitive dependency exists. Like great, that's a successful poll. And then I'll do that again and again and again until I have so many successive polls in a row for the same value. And once that happens, it achieves finality. Now the typical parameters for that are like 15 successive polls in a row of success sampled of everyone in the network.
00:09:41.514 - 00:10:15.672, Speaker B: So the big difference is that on a traditional consensus, or like let me back up an avalanche, it's all about subsampling. So you don't ask everybody what their preference is. It turns out that if you just ask a small portion of the network and everyone does that again and again and again, you actually have the whole magic of avalanche that leads to this emergent property of really quick basically alignment. And then the nodes based on what they're hearing and the preferences in the network will start to shift their preferences to what the majority is.
00:10:15.856 - 00:10:30.480, Speaker A: If I could just repeat it real quick. Ultimately, because the, I mean, you're not querying all nodes, you're just doing the small sub sampling. They're able to achieve consensus faster, ultimately kind of having quicker finality for transactions.
00:10:30.600 - 00:11:12.690, Speaker B: More holistically, strictly speaking, like the network complexity is less because you have to deal with less people. But there's a few properties that come out of that? That I'll explain, I think, why it's faster in general. One, we issue those polls in parallel so you don't have to like issue one. Wait, issue one, wait. We actually issue like four at once and then process them when they come back in order. So first of all, you have this like kind of parallel consensus process going on with just network layer stuff. Secondly, we perform authentication with each other in the initial handshake, so that when you're actually voting for something, you don't actually have to sign anything, you just respond on a mutually authenticated TL's connection.
00:11:12.690 - 00:11:55.452, Speaker B: So there's no overhead of different cryptographic properties that require you to sign it. And then lastly there's no hotspots. So in other systems where there's a proposer, that proposer is in charge of collecting all the votes and then putting them on the block and then sending it out. Additionally during that they typically have to send the entire block for you to sign, and then you have to send back that signature in avalanche instead. How it works is someone just proposes a block and then you start voting on it on your own. There is no central node that collects all the signatures or collects all the votes and then tells everyone what happened. Instead, everyone just determines for their self with their own parameters.
00:11:55.452 - 00:12:03.748, Speaker B: If they choose what is valid. There's not necessarily the step where they have to send all this data to everyone, they just do it on their.
00:12:03.764 - 00:12:20.170, Speaker A: Own whenever they get that makes sense. And so from the user perspective, ultimately, that was from the more technical standpoint, from the user perspective, ultimately, what is the user experience derived from all these technical innovations?
00:12:20.630 - 00:13:06.462, Speaker B: Yes, I mean, for most people, what that means is you send a transaction on most people interact with the c chain, and then it's confirmed within the end to end finalities in the realm of 2 seconds. So transaction gets included in the mempool. Mempools gossip to some validator, validator includes it in the block and then finalized. So that whole process, on average, we have a lot of telemetry and metrics of this takes about 2 seconds. So once the block actually proposed, like a validator actually proposes a block, it's finalized within 750 milliseconds or so. But the end to end latency requires kind of a pre step. And so that's, I'm sure that we can spend a lot of time talking about future optimizations of different things.
00:13:06.462 - 00:13:19.486, Speaker B: And I think the mempool is one area that a lot of other projects have gotten a lot of interesting results from in recent recent months and something that we're definitely going to dive deeper into over the next few months as well.
00:13:19.678 - 00:13:42.870, Speaker A: Awesome. No, I think that's a great walkthrough, ultimately. So with the subsampling, you're only reaching out to a couple nodes. How long does that kind of finality, it is final, as you mentioned, in a couple seconds. How long does the subsampling or finality take to propagate to all nodes?
00:13:43.330 - 00:14:03.356, Speaker B: Well, so that's the thing is there is no actual propagation of the finality. So every node on their own runs the consensus protocol and determines finale when they choose. So now this is a very interesting thing that we don't really talk a lot about, but when you run avalanche go, you determine, you can specify what your consensus parameters are.
00:14:03.428 - 00:14:04.120, Speaker A: I see.
00:14:04.420 - 00:14:56.106, Speaker B: And then you alone determine that. So for different partners, for example, let's say there's an exchange that's like, hey, I'll tolerate up to 5% byzantine node failures, and I just want to, against every possible heuristic optimized for safety. Like, I want to be past 33% safe. I just want to sit there and only wait for like, a hyper super clear preference in the network. Maybe I'll say instead of doing like 15 out of 20 in a poll and 15 consecutive, I'll do 19 out of 20 with 100 consecutive. And so you'll finalize slower because you're performing many more polls, but you can kind of configure your own safety parameters, which is super weird and interesting and totally different. It's kind of like the way with bitcoin, you determine your finality, like your confirmation depth of like how long you'll wait for a deposit.
00:14:56.106 - 00:15:13.436, Speaker B: In the same way, you can kind of have this configurable safety when you're running avalanche nodes. Now, what that means typically is like, these are just standard messages between each other. Like, there's no overhead, right? So, like, a typical round trip between two validators takes about 150 milliseconds.
00:15:13.538 - 00:15:14.392, Speaker A: That's quick.
00:15:14.576 - 00:15:37.368, Speaker B: So. And that's just network. That's not us, right. That's just standard networking packages in 2022, right. It's pretty fast. And so you just do a simple round trip, and then you process, you know, polls in parallel, because if you're sampling a huge portion of the network, typically your polls don't really conflict at all. Not that that really matters, but you can just sample more people and then typically get to finality faster.
00:15:37.368 - 00:15:38.176, Speaker B: So.
00:15:38.368 - 00:15:53.120, Speaker A: And for most networks, they're not doing the subsampling and the network communication overhead is n squared. What is Avalanche's network communication overhead with subsampling?
00:15:54.460 - 00:16:20.552, Speaker B: The theoretical assignment is logarithmic, which is why we say that the validator set can grow much larger. Now, the reason for that is, let's say that you have a traditional consensus system. You have a block. That block is then you create it. You then distribute that block to everybody in the network, all validators for their signature, and then they send that block back to you. Then that rotates.
00:16:20.696 - 00:16:39.086, Speaker A: Gotcha. The key difference there is they're propagating it all to all, which makes the n squared overhead in avalanche with the subsampling is ultimately able to reach out to fewer nodes in doing so, able to come to consensus faster and have the faster finality.
00:16:39.248 - 00:17:21.772, Speaker B: Yeah. So, like in a, let's just say in one simple example, right? So it's a two part thing where there's no bottleneck on a single node that's kind of like running this process, and there's no cryptographic stuff that must be returned for the block to be shared. The second part is. Yeah, like, let's just say in a simple case, right, you do 20 polls and you query 20 people in each of them. That's like 400 back and forth, right? Yeah, we have, you know, 13, 1400 validators. So, like, even in the case where you're doing these repeated subsampling, you know, in the best case, or like, even in the average case, you're touching a small fraction of the entire population, which is just a super interesting property in general.
00:17:21.956 - 00:17:54.216, Speaker A: Yeah. No, I do think it's from being able to go from n squared to logarithmic is a big accomplishment ultimately. I think so. There's multiple chains, as you mentioned on avalanche. And I think this is often kind of a big misconception as well. A lot of people just fully focus on the c chain. Could you talk about the different or the overall architectures and then kind of dive into some of subnets as well?
00:17:54.368 - 00:18:43.534, Speaker B: Yeah, yeah. So I'll give a little primer here. The first thing you should note is, first of all, great, that's a really nice property to allow that many validators. But there's a reason for that outside of just the decentralization of the system, which feeds into the subnet story, which is if you have all these validators that are part of the primary network and you want them all to run their own validators on their own subnets that are still part of the primary network, you need to be able to support a lot of validators. Otherwise, the subnet story that's connected to the primary network won't work very well. So there is an immediate capability that we have because of the way that avalanche works that enables much more interesting kind of subnet architectures in terms of number of participants and the way that they can all fit together. But I'll get into that a little bit later, I think.
00:18:43.534 - 00:19:15.730, Speaker B: So, yeah. The avalanche is broken into what we'll call almost like two tiers. There's the primary network, which is we call the primary subnet or really what all validators participate in. Most people, this is what they touch. If you're validating on avalanche, you're pretty much just validating the primary network. If you're using avalanche, you're probably just using the C chain, which is one of three chains on the primary network. Now people are like, well, why is it designed that way, first of all? And then on there, there's a P chain that's like the staking chain.
00:19:15.730 - 00:19:45.278, Speaker B: So that's not embedded into the C chain, for example. And then the P chain determines how subnets work, like the membership in them and rewards on them. And so the P chain then, you know, governs all these subnets that are loosely connected. But to be on a subnet, you have to be a validator of the primary network, right? So people are like, there's a lot of complexity, especially when you're starting off. Right. So let me back up, and I have to, I have to say a preface this by some of these decisions. I think, you know, I didn't come up with these ideas.
00:19:45.278 - 00:20:18.980, Speaker B: So, like, I joined a little bit after, after mainnet launch. So I have to say, like, a lot of the ideas and some of the brilliance of what we're trying to do here is not my doing, but I'll do my best to recant it. So on the primary network, when a new blockchain launches, it's typically a really slow uptake because you have a whole new virtual machine, a whole new framework. You have to build new explorers. You have to build new wallets. You have to work with all the integration partners. At the time Avalanche launched, obviously there was huge interest in the EVM and what you could do with it.
00:20:18.980 - 00:21:37.410, Speaker B: When Avalanche was created, there was a desire to get what was needed in there for the subnet story, which was the p chain, but then also to provide an easier virtual machine to interact with or more popular virtual machine to interact with with smart contracts in a place that didn't require people to create subnets from day one. Because, like, you know, if you already only have like 200 validators on Avalanche. It's a lot to ask to like start to like, really slip things out. And so for an initial growth story and to get people's attention, you know, the EVM was added initially. Now the unintended consequence of that, I think, was that everyone started calling avalanche like an ETH clone, geth clone, whatever have you had? And don't get me wrong, we're super indebted to the work of the Ethereum foundation and the Geth team and all the stuff they've done, just like a lot of other projects that support the EVM are. But people, I think at that time failed to understand that it's OVM running in a subnet, primary subnet, on top of an entirely different consensus, entirely different storage, entirely different everything underneath. And so over the last, really since that time, we've been working on improving performance and then shoring up all the systems for the launch, more of the subnet story.
00:21:37.410 - 00:22:22.682, Speaker B: But people then also ask, feel free to cut me off at any of this. But the other part, I think people tend to ask, then the immediate follow up is cool, I get it. You have this primary network that has all the validators, and I get, you have these subnets. People can do their own custom virtual machines with their own tokens, with their own everything. But why do they need to validate the primary network? Because it's like Cosmos doesn't need to do that. Zones are all independent and they connect to each other. And to that, I would say with the current feature set on the network, I can totally understand why people would say that the subnets don't benefit in any way from knowing, at least right now, about the membership of other subnets, what's happening on the primary network.
00:22:22.682 - 00:22:39.388, Speaker B: They just care about themselves. So people are like, well, maybe I could fork avalanche, go rip out everything, just run the subnet module and then be on my merry way. And I think that'll all make sense when the cross subnet messaging protocol.
00:22:39.484 - 00:23:29.502, Speaker A: I'm super excited to talk about the cross subnet stuff, maybe on the EtH stuff initially. So the main C chain is a instance of the ethereum virtual machine, the EVM ultimately you have and the team have done lots of improvements to speed that up. But the main, I would say that is, as you mentioned, how you guys bootstrap the network. But ultimately going forward, the real kind of not innovations, but one of the key benefits of avalanche is being able to create your own subnets and end those subnets. Or how would you describe a subnet. I asked Kevin to describe it, but I think it'd be awesome for you to have it describe it in your words. And maybe the pros and cons of an individual subnet as well.
00:23:29.696 - 00:24:02.320, Speaker B: Yes, I mean, subnets in a sense are just some subset of avalanche primary network validators. That's it. That subnet has the validator set is controlled by the P chain. And the reason for that is any node that joins the network then knows who's participating and who they should reach out to if they want to discuss. But subnet itself, in the proper term, is just the validators that are participating in some group. And then on a subnet you can instantiate any number of chains. A chain is just an instance of some virtual machine.
00:24:02.320 - 00:24:44.792, Speaker B: For example, is why I come back to the primary network. The primary network is just a subnet with three chains from three different virtual machines. Most popular subnets, like DFK, for example, are a subnet with their validators on it, with one chain which is running an instance of a virtual machine. We call the subnet EDM, which is basically a forked version of core ethnic, which derives a lot of stuff from Go Ethereum. And so you could, for example, add multiple evms to a single subnet if you wanted to. But the hierarchy is very much like you have a group of validators, that's your subnet. The subnet then can have multiple chains.
00:24:44.792 - 00:25:20.760, Speaker B: Those chains are just instances of some virtual machine or binary, and that is communicated with avalanche go. And so it just handles all of the management of all these virtual machines running concurrently. In a sense, it's almost like a little mini container runner or almost like a web3 version of AWS in a sense, where you have a subnet that runs all these random virtual machines that are user generated. All avalanche go provides is a consensus plugin. The interface that a VM gets is like, hey, here's this blob of bytes. Parse it as a block. Tell me if it's valid.
00:25:20.760 - 00:25:40.212, Speaker B: Okay, you think it's valid, now verify it. So execute what's going on and make sure the state transitions are correct. And then the consensus says, like, accept or reject. And then, you know, outside of that, you can do whatever you want, right? Like you want to define the blights however you want. Good to go, you know, but I don't want to get too ahead of ourselves in terms of just subnets and.
00:25:40.236 - 00:25:58.020, Speaker A: Going back to like the user application standpoint of this, what ultimately the subnets allow is a lot more customization because you can customize the virtual machine, and then you can have your own individual subnet if you would like. So you can tailor that to whatever needs you desire.
00:25:58.060 - 00:26:36.916, Speaker B: Yeah, I mean, I think a lot of people talk about this. We're like, you know, there's a lot of different approaches now to trying to achieve some form of scalability while balancing that with the ability to customize different parts of the experience. Right. So if you do want to provide all your own validators and maybe you want to provide your own security, which some people, you know, have a lot of interest, doing a subnet's a really good option because you can determine the native fee, you can determine the state transition rules. You know, it's really, if you can compile it into a binary, technically it'll work. So, like, that's your constraint. And I think some other cases, there's much more strictness on terms of what you can actually do and, like, what things you have to have.
00:26:36.916 - 00:27:34.710, Speaker B: And our whole purpose with Avalanche go, or really the goal is, what's the lowest level interface we could provide for people to build or, like, run binaries for some point that borrow some form of networking and consensus. And one thing that we're really excited about providing is a more opinionated SDK that sits on top of that. So that when someone's thinking about, okay, I want to create my own chain, it may be like, cool, cool, cool. Yeah, I can do it from scratch, but if I don't want to, is there some great form of tooling I can use to build on top of? And I think the Cosmos team has done a great job with Cosmos SDK rapid tendermint. I think we would look to do something similar with, like, you know, an included database and included, you know, account management, included, you know, signature verification, because we've rewritten that almost every time that we've done our own virtual machine. And so it'd be great. I understand the creator's question where they're like, yeah, like, do you have a thing that keeps track of balances? Like, well, kind of.
00:27:35.970 - 00:28:08.270, Speaker A: Awesome. Cool. Maybe just to wrap this kind of, like, overarching conversation of avalanche up, and then we can get into some of the new things that you've just released. In your words. I think there's still two main kind of focuses of scaling today. One is more the modular architecture that, say, avalanche and cosmos are trying to pursue. And then the other one is still the monolithic architecture.
00:28:08.270 - 00:28:21.080, Speaker A: Could you compare and contrast those in your words and why avalanche has specifically chosen to go the modular route and why it thinks it's advantageous.
00:28:21.540 - 00:28:50.684, Speaker B: Yeah, I mean, in my opinion, I think it all comes back to a single shared state. I think, generally speaking, if it was possible to have one massive state for every possible computation, anything anyone was doing, that'd be ideal. Imagine infinite composability and everything right there. But we know that that's been a bottleneck so far, managing a single state that large and updating it accordingly by having a canonical state across one large virtual machine.
00:28:50.772 - 00:28:53.120, Speaker A: And what would be the bottleneck in that instance?
00:28:53.940 - 00:29:58.740, Speaker B: So most times, people that have that large of a state are trying to authenticate it or not authenticate it, but as much as hash it, or have a merkle tree or some sort of tree or some sort of hash that represents its current state, maintaining that data structure on top of an ever growing state ends up being a really, really complex thing to do. And people are doing a lot of cool stuff to improve that and, you know, maybe do different approaches to it. But, you know, I think Peter Schlage from the get team had a fantastic presentation he did at Devconnect this year, where he talked about the inherent bottleneck just in trees in general, with how. How they're, you know, how they work with databases and how they work with blockchains in specific. So, to paraphrase him, think of it this way. You have a tree, let's say that some level of depth that represents the state on your project, even with the best stuff, like a path based storage merkle tree or anything. If you want to modify, let's say, two accounts, and you have a huge tree, and each trey is like, each tree node is persisted to disk.
00:29:58.740 - 00:30:40.704, Speaker B: So let's say that they're just on two separate sides of the tree. Let me see if I can get my. And they're on two different sides, right? Let's say that it's seven layers deep, and you modify one, and then you modify an account on the other side. You'll have to rewrite 123-4567 nodes on one side. If you're hashing the nodes all the way up, if you're not doing path based, but just traditional, this is what people traditionally have done, where you hash each trayee, and then if the tray node changes, you rewrite it. So seven, and then seven again. So that's like more or less like 14 reads and 14 writes to do one simple transfer.
00:30:40.704 - 00:31:39.670, Speaker B: Right? So the overhead gets kind of crazy as you start to manage these ever, ever enlarger trees. So, like, is it possible to have a huge state. Yes, but to authenticate it in an interesting way so that other people can state sync up to it is a much more difficult challenge that I think people have really glossed over, in a sense. So a lot of new work has been going to like, okay, if we're doing path based storage, can we store things more efficiently for this and stuff? But at the end of the day, if you're generating a root hash of all the state that's going on on a block by block basis, which is what most projects do, some have started to do periodic batching to reduce this overhead, and that's what people tend to do. But you have a lot of overhead of just maintaining this structure. So, like, a breakthrough in this sort of space would be huge for every. But generally speaking, a large composable state has a premium.
00:31:39.670 - 00:32:36.352, Speaker B: It's more expensive. And so if you have many parallel states. So, like, in the case of, like, you know, horizontal scaling with subnets, you have tons of tiny little spaces rather than one massive space. And because of that, you can kind of remove some of that overhead, depending on what's going on, and you can kind of tune things accordingly. So the way I think about it is, yes, in an ideal world, would it be great if we just had one monolithic chain and monolithic state that everyone could access at any time? 100%, it would be fantastic. But if you want to minimize the burden for individual validators to participate and then make sure that they only maintain the minimum amount of state necessary, and then within that, each subnet or chain or whatever, like, that only has to, like, kind of hash in for any activity. Only the stuff it cares about, you can start to save, in general, across the entire board on the system.
00:32:36.352 - 00:33:19.814, Speaker B: So the thing you give up, though, is the composability, right? Like, okay, now if I want to interact between two subnets, you just made that way harder than the way it was if you just had one single state. So I think the real question in the next couple of years of blockchains will be, does that play out? So I think Aptos sui Solana are pursuing the whole view of having a single state, and then you have the polka dot the cosmos avalanche that are pursuing this kind of heterogeneous state story. So it'll be really interesting to see what ends up being the approach that captures most of the mind share.
00:33:19.942 - 00:33:33.030, Speaker A: No, I'm fascinated as well. The state definitely does get very large on those change, and then it is a lot of historical data as well. So I appreciate that's a whole nother aspect of it.
00:33:33.070 - 00:33:38.650, Speaker B: Right. It's like, okay, you can keep the state required to validate, but then the archival thing, you know.
00:33:40.990 - 00:34:00.648, Speaker A: It does get. It gets pretty difficult. Cool. But I appreciate you breaking that down and comparing them. Uh, let's talk about some of the new stuff avalanche has recently launched. Uh, you have recently kind of written. The avalanche team has posted a new medium post kind of outlining a lot of the changes that you guys just published to the network.
00:34:00.648 - 00:34:05.480, Speaker A: Could you walk us through what those are and, uh, the results of them?
00:34:05.640 - 00:34:13.832, Speaker B: Yeah. So, um, I think, uh, for the most part, this has been, uh, something that we've been looking forward to, uh, probably my entire time I've been here. So it's.
00:34:13.936 - 00:34:15.032, Speaker A: It feels good to ship.
00:34:15.096 - 00:34:23.140, Speaker B: It's. It's so like, I think before I hop on this podcast, I think five minutes beforehand, I clicked the button to ship the main net release code. So nice.
00:34:23.180 - 00:34:23.880, Speaker A: Congrats.
00:34:24.660 - 00:35:04.790, Speaker B: I want to get that done before this so I could focus just on this. But, you know, I won't mince words and say, I think this is the biggest release from a functionality perspective that we've ever put out. Congrats and why I say that is very simple. So when the network originally went out, subnets were out in a simple form, more or less a proof of authority form. And a lot of people are like, oh, you know, subnets are cool and interesting stuff, but they're controlled by a single individual. And that individual determines who can be a validator on that subnet. So, for example, a lot of the popular subnets have some form of a control key or let's say just an administrator of the subnet that determines who can become a validator.
00:35:04.790 - 00:36:02.766, Speaker B: Is this the final vision we had for all subnets? Of course not. People are like, wow, subnets, cool stuff, but just. Just fall short of the vision. And to us, it's like, God, it really just pushes us further to get something out. So the vision, obviously, is that anybody at any time can launch their own full network, really as a subnet on avalanche. And to do that, you need to have proof of stake validation. So in the most recent release, we released something called Elastic Subnets, which adds permissionless validation and rewards to subnets, which means that if I wanted to today, or at least in two weeks on Mainnet, but today on Testnet, I could create a token, create a subnet and then require that people have to stake that token on that subnet to become a validator.
00:36:02.766 - 00:36:26.718, Speaker B: This is all run by avalanche. Go. So you as a virtual this is some of the stuff we do, right? Like, if you're running a virtual machine, you don't even know that this is happening. It's not even a part of what's going on. And then you get rewarded. If you're up for some percentage of time on that subnet, all of this is handled externally to the virtual machine. So when you're launching a virtual machine, you can do literally anything.
00:36:26.718 - 00:37:17.760, Speaker B: And then as long as you're up, you can get rewarded. So let's say that you wanted to generate like a blockless virtual machine, or like a p two p virtual machine that just did, like, periodic gossip, and you wanted to keep track of people that were online and then reward them for that. The system will now can provide that capability. So for us, it's like a whole quantum leap in where avalanche can go now, now that it's totally open. So in a sense, for people that maybe aren't familiar with some of our terminology, it's the equivalent of, let's say that you, previously you were the one that controlled all the tokens, and then you staked all the people that were going to participate, and that was how it worked. Now you can distribute the staking token or the weight, and then people can validate as they see fit on the subnets. Very cool.
00:37:17.760 - 00:38:02.490, Speaker B: Because it's been such a large thing and we've spent so much time on it, we haven't had enough time to prepare all the cool tutorials and documentation. I can assure you that's coming very soon. But we're very excited to see this out the door. For the astute reader, you'll find that the transaction that we added to enable this includes an argument for a BLS public key. And that should maybe play our hand a bit in terms of what we are seeking to do. But from a protocol perspective, we are now unblocked from the cross subnet story a little bit more.
00:38:02.650 - 00:38:28.944, Speaker A: Nice. Congrats. Now that's a big accomplishment. It always feels good once you ship something, get it out the door. So congrats again. And so the main could you maybe just re articulate in one or two sentences from the user perspective or the node operator perspective again, why this is such a big accomplishment on avalanche, you.
00:38:28.952 - 00:38:33.286, Speaker B: Can now stake on multiple chains at once and get rewarded for that.
00:38:33.368 - 00:38:35.018, Speaker A: So multiple different subnets?
00:38:35.154 - 00:39:22.394, Speaker B: Yeah. So let's say that you're an astute set, like an astute validator, and you're like, hey, I want to generate, I think, let's say I'll give you a very simple example, I'm a validator. I tend to think I'm a good validator and I think that I could use my experience validating to help other subnets get off the ground and then be rewarded for that. Well in the past I would have to go to that subnet and say like, hey, I'd like to validate. Do you trust me? Will you add me as a validator? Which is like the whole point of crypto is this is not necessary. So that's how it used to work. Now I can just go buy up or do whatever I want with some of the token related to that subnet and then just stake it just like I would stake on any other network and I can now participate in that subnet.
00:39:22.394 - 00:39:58.488, Speaker B: That's huge. There's no way that we go from us double digit validations on a particular subnet into the hundreds or thousands without actually providing the true mistake functionality. So we expect in the next few months we'll see subnets rapidly decentralized and have much larger validator sets and everything. So yeah we're, you know, I think we're very, we pay attention to what people say. Right. And I think that generally a lot of the critiques of subnets thus far has not been their speed or their performance or anything like that, but it's been, you know, cool, cool, cool. But there's only like ten people validating that.
00:39:58.488 - 00:40:27.660, Speaker B: Yeah, like does that bother you? And the answer is yeah, of course it does. Everyone would love to see more but the mechanism for adding that I don't think has been there. But now it is nice. Also I think as we add more functionality the clear benefit for users is you can now stake or delegate on subnets which can make subnets way more interesting for the people creating them. And then secondly your funds and everything you do on that subnet will be basically protected by much more, many more validators.
00:40:28.360 - 00:41:19.984, Speaker A: Awesome, very cool, awesome. Maybe moving the conversation a little bit forward. One thing when I was speaking with Kevin was talking more about the virtual machines and exploring the design trade offs and different kind of world of what are performant virtual machines. It's kind of been interesting now just even like this last year seeing like Ethereum kind of be and it still is like the most dominant virtual machines but people continuing to explore like more parallel processing virtual machines. Could you talk about avalanche and its ability? I mean we spoke upon it initially but to create custom virtual machines. But how you guys are kind of thinking about the virtual machine space and avalanche.
00:41:20.112 - 00:42:13.320, Speaker B: Yeah. So for me I think that there's a lot of aspects of the custom virtual machine story, whether it's technical or actually like, more organizationally and kind of resiliency. So I think that the one thing that I think is super underappreciated about the way custom virtual machines work in general and being able to deploy them on subnets is it's much easier for us to iterate on all sorts of interesting virtual machine designs without actually, like, impacting a production network. So, for example, if I had a monolithic chain and I wanted to totally change the way something worked, sure, I could do all my testing and do everything. At some point you have to launch it, right? So you put on a Devnet, you put on a testnet, right? That takes a lot of effort to do that and make sure it doesn't break, right? Because if you try something and then like, oh shit, doesn't work, like people lose their shit. They're like, it's not stable. Everything you guys do is a joke.
00:42:13.320 - 00:42:56.180, Speaker B: You have to be really careful with subnets. You can take the exact same virtual machine framework that you may run on the main net or on your subnet, spin up a subnet, try it out, get people to use it, do whatever, and then if it doesn't work, you just forget about it like it never happened and start over. At least from the inspiration of like, what you can do with virtual machines and subnets. It's huge for developer productivity. Now, people may not realize, because we haven't really talked about it publicly, but we will in the future. Some of the stuff we're doing on virtual machines. Initially, most people have seen that the common virtual machines on avalanche are just EVM variants in some way.
00:42:56.180 - 00:43:59.168, Speaker B: That's, again, not really our goal or end goal as much as it's just where people are today. And we found that with our current size and everything like that, it was much more efficient for us to just support an existing virtual machine project rather than to build it entirely from scratch, which is extremely complicated and expensive to do, right? But now that we've gotten further along in the story of what we're doing, we have turned our attention back to the virtual machine side. We now have a team of quite a few people that are working on rust specific stuff in the next two weeks or so. Don't hold me to that. We plan to release the Rust SDK for building virtual machines for people that have been like, oh, I want to play around with subnets, but I'm a big rust fan. You'll now be able to build your virtual machine entirely in rust and then run that on top of avalanche. Because of the way virtual machines work, like I mentioned, with different binaries and things, it's totally separate.
00:43:59.168 - 00:44:51.456, Speaker B: If you can compile it, you can probably run it as a set. But the interface that virtual machines bring about is very low level, like I mentioned. So compared to like I think other virtual machine frameworks which have very strict ideas of like how you should interact with the state tree where your virtual machine may be called during the block production process, ours is much lower level. So instead you get, basically the engine will say you parse this block, verify, accept or reject, but then you also have full control of the subnets networking library. So you can send arbitrary messages between other validators on your subnet for anything. So like, oh, you want to gossip things in a certain way, like send blob bytes. Yeah, so we haven't really talked about it much, but this is exactly so how we build different things.
00:44:51.456 - 00:45:36.820, Speaker B: So like we built statesync on top of this arbitrary blob transport layer exposed in the virtual machine interface. So I think for people that want to build things, we just don't have great documentation or SDKs yet that wrap a lot of this functionality. But if you're trying to build a virtual machine and you want to go like as low as possible, I firmly believe there is like no better place to build than on top of our virtual machine interface because it just lets you do pretty much anything. The gas isn't metered, there's no compute metered. So you don't have to like format things in a certain way. You don't have to even use the database provided by Avalanche go. You could call out to a SQL database, you could put it in s three buckets.
00:45:36.820 - 00:45:40.672, Speaker B: It's totally in your control. All it has to do is just run.
00:45:40.816 - 00:46:01.330, Speaker A: Yeah. So ultimately you can control the networking messaging because of the customization of the virtual machine, and you can design the virtual machine however you want from scratch. And ultimately because you can pretty much have infinite amount of subnets, you can be able to iterate faster with much many different designs.
00:46:01.830 - 00:46:09.278, Speaker B: I think I've built around 200 subnets and about four of them exist still to give you some concept of what I'm talking about.
00:46:09.374 - 00:46:23.506, Speaker A: Yeah, very cool. Ultimately, do you think the industry will kind of end up in more parallel paralyzed virtual machines or do you think they'll be more single threaded? Just to get your thoughts.
00:46:23.598 - 00:46:54.332, Speaker B: So we're working on some stuff internally. Obviously, I would imagine most people are around parallel execution because we don't have to. When we start building on our virtual machine interface from scratch, there's no constraint. You can just start from scratch. An idea you can explore is let's say that you wanted a hyper transfer optimized virtual machine. In that case, you know all the keys that will ever be touched during block execution without any extra work. Like from two.
00:46:54.476 - 00:46:55.180, Speaker A: Yeah.
00:46:55.340 - 00:47:28.134, Speaker B: All right, you got two database keys and now you have 1000 transactions. During the parsing phase, you can completely separate all the execution out into different compete groups without any overhead and then start prefetching the state. Now if you have smart contracting, it's obviously dramatically more complicated. You have to understand what keys you're going to touch. Like do a bunch of pre fetching work. And I think a lot of projects have spent a lot of time trying to figure out the best way to do that. Even the EVM added access lists which will let you specify the keys that you're going to touch during execution.
00:47:28.134 - 00:48:06.662, Speaker B: You get a small discount if you do that because it will help with stateless execution and everything like that in the future. But I think for me, parallel for a lot of use cases makes a whole hell of a lot of sense. I think it's not a hot take by any means. I think that with Avalanche and the generic way the VM library works is you're empowered to build parallel execution environments for your specific use case, which, if it's not super generic, maybe much simpler than a super generic smart contracting engine.
00:48:06.766 - 00:48:17.386, Speaker A: That makes sense. Cool. I've always been fascinated by Avalanche's ability to do the custom virtual machines. And I did not know that you could also do the custom networking.
00:48:17.538 - 00:48:28.562, Speaker B: Yeah, the interface for that. Because I've talked to a lot of people that have said the same thing. Like, wait, you can do what? So the way it works is you'd literally just say app send and you say the node id.
00:48:28.666 - 00:48:29.338, Speaker A: Interesting.
00:48:29.434 - 00:48:59.984, Speaker B: And that's the entire interface. And it just goes. So you don't have to worry about, when I say networking, I'm not saying, oh, you have to implement this fancy send bytes to this person or gossip bytes to all validators. Those are your options. Then you can have a clear request response and the avalanche networking interface will handle all the request ids for you. Failures, timeouts, all of that's managed for you. You just have this super simple, what looks like a function interface to say, ask this person for something.
00:48:59.984 - 00:49:18.372, Speaker B: Great, I got something. It's a really awesome primitive for building really cool p two p applications. I call it almost like the p two, p factory, but it's really not. There's a lot of branding. I think we can do better to make people aware of what you can do with it, and I think that will come in the next few months.
00:49:18.556 - 00:50:15.320, Speaker A: I've definitely always been extremely impressed by avalanche's, the entire team technical capability. I think sometimes it is hard for the general population to be able to follow along with how does the tech ultimately transfer to the user applications or even the engineering and some of the simplicities that you guys ultimately do. But super excited again, about all the stuff that you guys are doing, maybe to keep going. How would you. I think a lot of people today are like, the cosmos verse just happened, but how would you kind of compare, again some of the stuff that avalanche is doing to cosmos or even a L2 or a parachain? Because I think other people also have a hard time understanding the differences and some of the nuances between each of those.
00:50:16.180 - 00:51:33.146, Speaker B: Yeah, I mean, I think by far and away that's probably the common question we get asked. I think that in the space now, there's such a small number of projects that are really, you know, legitimate and doing really great stuff that the especially, like, sought after ones get a meeting with all the top folks. They like, hear their pitch, right? And understand, like, okay, you know, you hear the pitch of, like, mahbed, you hear the pitch of like, l two, you hear the pitch of the Cosmo zone, you hear a pitch of alloy subnet, and then you're left to somehow pick what makes sense. And I think for different projects, there are certain components of it that are really useful. Like, for example, if you don't want to run your own nodes and don't provide your own security and don't want your own virtual machine taking something that's much more polished off the shelf or that provides just a standard execution experience you're used to can be super nice, right? Like if you just want to do what you do on eth but cheaper, you know, l two s are great for that. I don't disagree whatsoever. But if you want to build, you know, much deeper applications and use your own token and provide your own security and, you know, run your own nodes that are optimized for a use case, you know, you have to go deeper and you have to take a deeper stab at it.
00:51:33.146 - 00:52:01.790, Speaker B: So, you know, subnets in a sense. I think the thing I'll add here is that the way that at a high level, the cross subnet messaging story will work. And the part why I need to tell this is it makes sense to actually compare them when you have at least some understanding of where we're going or what's coming as a part of this. But simply put, all virtual machines in avalanche know the validator sets of all other virtual machines.
00:52:02.810 - 00:52:03.226, Speaker A: Okay?
00:52:03.258 - 00:52:48.788, Speaker B: Very simple, okay? No overhead. It's just provided in the block execution context by avalanche. Go. All that data is stored on the p chain, so everyone knows it. For every block execution on every subnet, straight up. And so if every subnet validator has a key associated with them, you can trivially verify any message you get from another subnet with no, what I'll call like housekeeping overhead or bookmarking overhead or anything like that. So let's just say very simply, right, like you're on Subnet A, and the validators in subnet a use their aggregatable signature to sign something and then pass it along to another subnet.
00:52:48.788 - 00:52:54.120, Speaker B: That subnet can, without anything from the other subnet, verify that that message is authentic.
00:52:55.460 - 00:52:56.036, Speaker A: Okay?
00:52:56.108 - 00:53:30.584, Speaker B: No one else does that right now. Because you have this root of trust that everyone already is aware of and then uses for these interactions, you can end up having a high throughput messaging protocol between subnets that's entirely generic. We don't have a standard message format. We don't have any of that. You can just produce arbitrary signatures and then use them anywhere within subnets you want to interact with. Subnet, blah blah blah, that has a totally different virtual machine. If the subnet a knows what it wants, they can send it.
00:53:30.584 - 00:54:08.680, Speaker B: We would expect different protocols to implement different messaging standards probably as a result. But our vision is more the lower level. We enable that and then different virtual machines that find it interesting can use it. So to back up why we're comparing this is that if you have this multi zone or heterogeneous state world, the thing you need to replace composability, if you're not going to provide that as a single state, it better be really easy and fast and quick to interact between the different state zones. And so subnets, when this is rolled out will provide that.
00:54:08.800 - 00:54:09.520, Speaker A: Awesome.
00:54:09.680 - 00:54:57.240, Speaker B: And so people come to us and say, well yeah, there is no subnet interop story. There is no whatever it's like, no, there is, there always has been like subnets as separate zones that don't talk to each other, that just are randomly connected to the, this to primary network doesn't make a whole lot of sense. It's not very coherent if you think that's where we've ended up and that's what we're going with. No, we have much deeper aspirations for what we're doing. And so subnets, how they compare in that form with cross subnet messaging to other people is if you're trying to move messages all over the place, how they compare, for example to parachains, where parachains all the messaging actually flows through the relay chain. Like that's its whole point. Outside of rotating validators around on different parachains.
00:54:57.240 - 00:55:28.566, Speaker B: There's a lot of overhead then because message passing has to go through there with cosmo zones, you have to pass, the relayers have to keep headers in sync on every end of a connection because they serve as like lines of each other. So you have this overhead of passing headers back and forth all the time and then messages on top of that with subnets. And the interop story that that is there and will come soon is that there's no overhead. You can just take a message and then verify it just as fast as you can verify an aggregate BLS signature.
00:55:28.718 - 00:55:38.330, Speaker A: Nice. Without that overhead that is uniquely enabled by avalanche, what are the properties from the user standpoint?
00:55:39.070 - 00:55:50.570, Speaker B: For the user it's like you're on subnet, whatever, and you want to go somewhere else. You can do that in a matter of single digit seconds. So it should be in the order of two, 3 seconds to move between subnets.
00:55:50.650 - 00:55:54.106, Speaker A: So it'd be the same type of finality as like a single subnet.
00:55:54.178 - 00:56:05.910, Speaker B: Well, so the way it would take is you wait for finality on one subnet and then you include that in another. So let's say that if the end to end finality on a subnet is 5 seconds, it should take 10 seconds to move between subnets.
00:56:06.770 - 00:56:07.750, Speaker A: Interesting.
00:56:08.050 - 00:56:08.986, Speaker B: Fully finalized.
00:56:09.058 - 00:56:22.034, Speaker A: Yeah. And so in Avalanche's kind of point of view and all of this is just enable that functionality and let third parties be able to come in and create that like subnet to subnet communication. Or is Avalanche going to build that in house?
00:56:22.122 - 00:57:08.740, Speaker B: We are going to provide like a, you know, a useful example of actually doing it. Like it'd be one thing for us to just be like, the keys are there, man. Like good luck, it works just like, yeah, we're going to go further than that. But we expect people to have alternate implementations that use the same primitives. I mean the way we look at engineering at some point is almost like two levels, right? I, there's primitive platform engineering and then on top of that you have, what are virtual machines that you can end up building that use all those primitives? When we develop anything, our main goal is primitives, as many as possible, as powerful as possible, and then leave it up to virtual machines to use those in a way that they find interesting. Because the whole point is, you know how to build your virtual machine better than we do. We'll provide you tools, but the whole point is that we want to enable you to do things that you can't do elsewhere.
00:57:08.740 - 00:57:29.806, Speaker B: When we're thinking about virtual machines, when we're thinking about comparing to other people, we say, what do they provide? And typically they have some really full featured SDK. But if you don't use that SDK, it's not possible with us. We're working on an SDK you can use that has these higher level primitives that's more opinionated, but you by no means don't need to use it.
00:57:29.958 - 00:58:02.860, Speaker A: Yeah, okay, very cool. Awesome. We've touched upon a lot the general overview of Avalanche, kind of the new things that you've launched recently, the differences between like Cosmos parachains, l two s, even the different trade offs, subnet communication and even I think we touched upon scaling. Is there anything that we missed out on, the technical aspects outside that you want to touch upon?
00:58:03.770 - 00:58:51.398, Speaker B: Yeah, I mean, I think the only other thing I would touch on is, I think another question we get is, okay, cool, you have these subnets, but then what happens when subnets hit their capacity? They're like, oh, you know, like cool, cool. You just move the problem to another place. But it's the same problem. Right. And the thing I want to touch on that a bit, which is our goal with subnets on a performance level basis, is not to take inefficient virtual machines and just allow more throughput by having more inefficient virtual machines, which you could do. That would be a cop out. Like, you know, if I had a virtual machine that just did transfers, but it would did like super dump stuff and you know, it was only going like two tps, right? Like, and I just made a million of them.
00:58:51.398 - 00:59:40.430, Speaker B: Yeah, I could get 2 million tps. But like is that really what you want? Probably not. So instead our goal more so is okay, let's optimize the shit out of everything and then have n number of those also working. So we're already, that's our main goal. But then within that, within a subnet you have this multi chain primitive, right? So you can have multiple chains within a single, single subnet. The really interesting thing you can do is because each chain has a separate state, you can actually run each chain on a separate host, with separate storage, with separate networking. And so you can actually create avalanche go as a cluster rather than a single binary.
00:59:40.430 - 01:00:20.206, Speaker B: So you can scale out not just the consensus process of a single chain, but then once you're at a chain's capacity, you can scale out horizontally that way so that you have each chain so that's running on its own host with its own networking. And so then you only need to synchronize within the subnet for movements between those chains within the subnet, which, because all validators are there, you don't actually need to do like a cross subnet transfer or anything. It's just sharing state between them. Like you have a single thing, but like a simple key value store between them that they just read out to once in a while. So what that enables is like, let's say that you're some crazy gaming company. You could have a chain for a region close to you. You could have a chain for, like, Asia.
01:00:20.206 - 01:01:18.792, Speaker B: You could chain for, you know, whatever. And then for only certain cases, you could move between those. But then every time you want to launch a new region or launch a new game, you could create another instance of a chain and then put that on an entirely new host or entirely new state store. So that's something that we'll look into, like kind of towards the beginning of next year. But, you know, I think a lot of the stuff that we're doing will make a lot more sense to people in a span of like two to three months. I think we've been doing a lot of stuff behind the scenes the last year, year and a half or so that, you know, I think we're transitioning a bit to be a little bit more public about some of the development that we're planning to do so that people are more aware of what we're actually up to behind the scenes because we have some, you know, to the credit of everyone on the team, some brilliant people. And I think a lot of the work, it's kind of a weird thing when all your work you've been doing for like a year is not public or no one even knows it exists, right? And then, you know, you see all the stuff people talking about, and so, you know, hold us to that.
01:01:18.792 - 01:01:34.968, Speaker B: You know, we are very excited to work with the community and share more of what we're working on so people have a better understanding of, you know, we're very interested in the stuff a lot of people are interested in. I think that our engineering efforts, you know, when they come out in the next few months or so will clearly indicate that.
01:01:35.064 - 01:02:10.876, Speaker A: Awesome. Well, again, super excited kind of for your recent launch and then and ultimately for the stuff that you're continuing to enable. How, I would ask, like, I think a lot of people are interested in the avalanche ecosystem. I'm super excited. I think you guys just announced that you guys are going to do a hacker house in the Bay Area. Definitely really excited for that. Outside of like coming to like this hacker house and future hacker houses, how can people get more involved or opportunities for collaboration with Avalanche ecosystem system?
01:02:11.068 - 01:03:01.992, Speaker B: Yeah, that's a great question. I think we have been working on building out a whole new team to help facilitate, I think, a better connection with the community. But in the next few months, you can look forward to us kind of opening up a proposal program of sorts for people to better communicate and discuss different changes that could be made or different things that could be done. Right now, the best way to get involved is to hop in discord. We have a super active discord of people suggesting different things like whether the virtual machine research or whatever. If you're a project looking to build something and plan to, you know, we always create different, like slack connect channels between Ava labs and, you know, whatever you're, you're working on to try and support you as much as we can. And then, you know, we have our own like call to action things, which is we would love to create a virtual machine dedicated just to ZK proving.
01:03:01.992 - 01:03:45.556, Speaker B: And like, it'd be super cool to work with people on that. You know, we'd love to see virtual machine SDKs and other languages. I think as soon as we shipdez Rust SDK, which is totally defined on its own, it'll be much easier to copy that and create a C wrapper or I don't dabble in some of the other stuff, but JavaScript, Python, whatever you want. For us, the best way to get in contact is, yeah, do that. And I forgot to plug the Avalanche creates project, so thank you for reminding me on that one. But yeah, in Berkeley towards the end of October, we'll have a, an event. This is a new series we're doing where we're traveling around the world and instead of providing like a really quick hackathon sort of thing, like come in, win some prizes, disappear, never see you again.
01:03:45.556 - 01:04:13.840, Speaker B: The idea is to kind of create a rotating kind of incubator project closer to like a Y combinator than like a hackathon thing. So we'll bring, you know, different vc's in the area. We'll bring different support from avalabs and the community to help you get your ideas off the ground, let alone just like kind of technical stuff. Because turns out, as much as I may not like to think so, a lot of doing things in technology requires all sorts of other aspects of legal business.
01:04:14.220 - 01:04:17.092, Speaker A: Lots of things behind the scenes. I've been learning that as well too.
01:04:17.196 - 01:04:33.972, Speaker B: Yeah. So for me, or someone that's much more engineering minded, it should be a great experience for people to come and learn a little bit more about what you should think about or mingle with like minded folks that maybe aren't as technically gifted but want to support your project in other ways.
01:04:34.116 - 01:04:55.372, Speaker A: Perfect. Awesome. I did just check the chat. It is live, which is cool. I don't know if that was going to work or not. A couple of questions from the chat and then maybe we can wrap it up. It says, can you say a word about BSL signatures for those who aren't gigabrains?
01:04:55.516 - 01:05:29.520, Speaker B: Yeah. First of all, we had nothing to do with their development or their production, productionizing in the last few years. So huge shout out to the researchers, I think some amazing cryptographers. I always joke around about crypto being just like the most absurd, one of the most absurd things humans have ever. The stuff you can do with it is just absolutely insane. I for one, I am not smart enough to understand how to develop new cryptography, but I feel like I can appreciate just something really, really cool. BLS is that.
01:05:29.520 - 01:05:58.030, Speaker B: And so BLS is a project or really not a project. Jesus. A curve of cryptography that lets you do aggregated signature verification. So very simply, let's say Logan and I both have, you know, a public private key pair of BLS. I forget the name of the BLS stands for the people that came up with this. So you'll have to look at that. There's two BLS's, actually, so don't get confused.
01:05:58.030 - 01:06:06.870, Speaker B: But let's just say we both have a key pair and we want to tell. We want to show to someone, you know, external to us that we both sign something.
01:06:08.130 - 01:06:08.874, Speaker A: Okay.
01:06:09.002 - 01:06:58.494, Speaker B: BLS has a primitive where we can basically, let's say I have my signature, you can send me your public key and your signature, and I can aggregate them, slam them together, and then it looks like another just key pair, just a standard public key with signature. So you may be like, well, okay, I guess that's kind of cool. You can combine stuff. Well, what you can do with it, one is like prove joint ownership of something. Like none of us can produce a signature of this joint public key without combining both of our signatures. Secondly, it's fully asynchronous. So, like, if we're in a huge network of people and we're trying to produce a single aggregate signature, all the rest of the network has to know is you get the signature bytes, which is like 96 bytes, and it can represent arbitrary participants having participated.
01:06:58.494 - 01:07:24.654, Speaker B: So eth two does this at scale with a lot of what they do. So huge credit to them over the last few years for helping to productionize a lot of the libraries that people use for this filecoin also use. It's very popular now in different crypto projects, but it's a really cool way to just aggregate signatures and then show or authenticate that people sign things in a certain way. So super useful for group proof of participation.
01:07:24.782 - 01:07:32.810, Speaker A: Nice. And then one other follow up question. Does Avalanche plan to support move in the future?
01:07:34.630 - 01:08:14.670, Speaker B: Very good question. So this goes into what I was saying with the lower level and high level concerns. A prerequisite to that is most of move virtual machine is in rust. So if we didn't have a rust SDK, it wouldn't really be possible to do that. So our first step is just the Rust SDK, and then we are very interested in seeing how people adapt it. I think, generally speaking, maintaining a connection to another virtual machine that you didn't really write takes a lot of resources and it's very error prone. And so what we'd like to see is to take the rust virtual machine.
01:08:14.670 - 01:08:25.770, Speaker B: We have the Rust SDK. We have built and work with people that actually are developing move and want to take it to different places to actually launch it as a subnet without us having to maintain the virtual machine.
01:08:25.850 - 01:08:26.602, Speaker A: That makes sense.
01:08:26.746 - 01:08:58.623, Speaker B: So are we interested in it? Yeah, definitely. And if you're interested in like, kind of trying to create your own move based chain, we'd love to talk to you. And what I found out, actually, for people that aren't familiar, is people talk about moo virtual machines just like they talk about the EVM. Like, you know, you can take your contract from EVM whatever, and put somewhere else most of the time. Time, that's not the case for Mov. Every dialect of it has a very different feel, like a, you know, a different feel. So you can't take a contract, for example, from, you know, diem, apply it to aptos.
01:08:58.623 - 01:09:27.992, Speaker B: There's different, you know, frameworks and things going on with different object and how they, like, kind of give you access to different primitives. So it'll be interesting to see if there's also, like, a. Like a fork of things, because most people would have no idea. You think move, and you're like, all right, yes. Whenever move contract turns out it's not that way at all, and it's very different. So I'm curious to see how that plays out. But we think our expertise is much more in the lower level stuff and giving that to that ability to different people.
01:09:27.992 - 01:09:37.300, Speaker B: But we're obviously watching it closely. So if we do see a huge spike in transaction, we may try harder to get other people to work with us on it.
01:09:38.400 - 01:09:45.040, Speaker A: So that's the virtual machine side. On the programming language side, that's another conversation.
01:09:45.740 - 01:10:21.690, Speaker B: Yeah. I mean, I think some people have considered, like, injecting move language into other virtual machines, to which we'd be like, sounds complicated. Like, I feel like in that effort, you may get the worst of both worlds, rather than, like, the best of both kind of combined. So we'd be much more interested, I think, in helping people, like, having a dedicated virtual machine. If you wanted to, like, let's say, extend the subnet EVM to support move, we'd also love to, you know, check that out. But generally speaking, the move language in general, we would think, would be best housed by a dedicated virtual machine for.
01:10:22.150 - 01:10:48.720, Speaker A: Cool. Awesome. Well, definitely appreciate your time. Maybe we can wrap it up. I tried to do spicy takes. I didn't really come up with any spicy questions beforehand, so maybe I'll just open the floor. Do you have any spicy takes that you would say are contentious in the space or, like, an unpopular belief that you hold near and dear to your heart that others do not hold?
01:10:50.700 - 01:11:16.890, Speaker B: Yeah, I think there's two interesting ones, maybe, that I'll share. So I generally like to think I'm a pretty reasonable person. I try not to take part in a lot of the crypto, Twitter back and forth and stuff like that because I think I. I think it's very charged. I think you have a lot of people that are interested in certain things for, you know, any number of bag bias. Yeah, I think. I mean, for whatever reason, I mean, I think that, you know, different.
01:11:16.890 - 01:12:00.574, Speaker B: Different ecosystems have different people that, you know, will periodically share really hot takes about, like, other ecosystems in general and stuff like that. So I try very. Try very hard to never really, like, I don't think it's a good exercise in many cases, to talk specifically about other projects because there's usually a lot of context and things that, you know, I may not be aware of and stuff. So I do. I do very very hard to try away from that. I do think about abstractions though, and, you know, how to use them. And one thing that's kind of an interesting to me is like lately a lot, a lot of interest has gone into the l two story and the l two story in a, you know, for a lot of people have a lot of stuff to say here, so I'm not going to get too crazy about it.
01:12:00.574 - 01:12:18.834, Speaker B: But generally speaking, I agree in general that in a perfect world it is the most efficient way to process huge amounts of transactions. Right. Like if you can have only one node processing everything and then everyone else just like, you know, can kind of follow along or verify it and it's provable on a different chain, like that seems really cool.
01:12:18.922 - 01:12:19.590, Speaker A: Yeah.
01:12:19.970 - 01:13:08.750, Speaker B: However, I think there are two things about l two that I think people don't really talk about much. And I think that will be become very pertinent in the next, you know, six to twelve months. One, two regulators that I think maybe don't have familiarity with the nuances here from the outside looking in, you got one node running on one computer that has a shit ton of state to it. Now, I think as soon as regulators get wise, I don't think they're going to care about the arguments that like, it's decentralized because you can fraud proof it out. I think that there will be serious legal challenge to people that are operating different l two batching engines. And I think it's one of the things where like with decentralization, it only matters when it matters. And for l two s it hasn't really mattered yet.
01:13:08.750 - 01:13:31.790, Speaker B: Likewise, I think people talk a lot about l two s being totally the future. We're already there. Let's get going. If you check out l two beat, for example, and it talks about how far different l two s are long. A lot of them don't have a lot of the features that you would hope that an l two that you're putting billions and millions of dollars on would have. You can't exit. There's no batcher recovery.
01:13:31.790 - 01:13:56.930, Speaker B: And so I understand that it's a super important part of the story and everything like that. But I think it's interesting, the narrative in the space really saying that. As far as I understand or what I see, it's like l two s are the thing. They're here. Everything else you're using in l two s, only without really talking about some of the nuance of some of the risks that you're taking. Everything has risks. I'm not saying there's other projects that you should pick instead or not.
01:13:56.930 - 01:14:53.660, Speaker B: But it's just interesting that it's still a very realistic concern that people haven't gone into. Maintaining l two s with the fraud proof overhead is really complicated because you have to make everything that executes on that l two executable on another chain and everything must be that way and executed by another chain. Otherwise you can't prove the fraud proofs. Initially people thought like okay, we'll build these super targeted things and like will fork a ton of stuff to make it work. I think the general approach lately has been to not do that anymore and instead try and find a generic compiler for other chains, which I think is probably a better direction. But your trusted computing base to run, that can sometimes be huge. Keeping that up to date and making sure it works as you make every single change to the upstream.
01:14:53.660 - 01:15:33.206, Speaker B: Like l two, that's a lot of, I think, complexity to worry about and think about as people go. And so I don't think we've had any cases yet because there hasn't been really any active fraud proof I've ever seen. But it will be interesting to see what if your fraud proof prover fails and you don't know that until it's actually on the chain. Right. So I just think that a lot of solutions that a lot of people are pursuing have a lot of complexity and people tend to talk mostly about the potential upside of it rather than just, I think, kind of presenting everything. And I'm not one to say that subnets don't have different trade offs as well. Like I'm not here saying like, you know, l two s.
01:15:33.206 - 01:15:52.992, Speaker B: Like, you know, forget about, like, forget about them. Like subnets are the way to go. Like subnets have different properties too. Like they're very tightly connected to avalanche. You know, right now most are controlled by an administrator. So, you know, I'm not saying that like, you know, strictly better or strictly worse. I just think it's, there's interesting trade offs that people will have to make for l two s to be successful.
01:15:52.992 - 01:15:55.992, Speaker B: And I'm not sure that a lot of people are talking about those trade offs.
01:15:56.096 - 01:16:17.674, Speaker A: I fully agree. I think it's funny. Yeah, the optimal design is like the single sequencer. But then to your point, if there's only a single one, there's some drawbacks. And then ultimately if you start adding multiple sequencers, then you start having to do all the same things that you would with kind of a normal blockchain with like the data propagation and you.
01:16:17.682 - 01:16:42.830, Speaker B: End up like recreating a blockchain again. I mean, it just reminds me of some other kind of like state channel stuff that people tried to do for a while. It's like, oh yeah, we don't need the blockchain, actually. We can just do everything in state channels. And then you realize that you're just building another blockchain on top of blockchain state channels. And so, like, as you get into the multi sequencer world, I think you kind of get into very interesting territory. But I.
01:16:42.830 - 01:17:32.218, Speaker B: With multi sequencer in general, I think there's a large looming legal regulatory battle with l two s and I'm very curious to see how it plays out. I hope for the hope innovation wins. But in recent weeks it's certainly come under question whether or not that will always be the case. I think the last thing I'll leave on this topic with is subnets actually provide a lot of the stuff that you would need to build an l two. Like a lot of the consensus stuff for you. So you could just have any number of sequencers contribute and then still produce a single state route that you could then checkpoint to an l one if you wanted. So we haven't seen anyone build a subnet as an l two yet on Avalanche, but we think that that will happen.
01:17:32.218 - 01:18:15.280, Speaker B: You know, maybe not next three months, but something that people will certainly look into in the next twelve months if they, they don't want to rely on their own security of the subnet or they want to have like a recovery pattern where like maybe they do trust their own security, but in the case where shit hits the fan, maybe they want to have a checkpoint to recover to in the case that they're not maybe doing strictly fraud proving. So I think there's a lot of interesting collaborations here and there. I think we had the first l two launch on Avalanche two weeks ago or so. So that was always a funny narrative. One subnets, l two s. Like, you can have l two s. Like, yeah, it's just a chain, right? Like whatever people want to do.
01:18:15.620 - 01:18:27.800, Speaker A: Definitely. Awesome. I really appreciate your time, Patrick. Thank you again. I think a lot of people will enjoy watching and listening to this conversation. Any final parting words before we hop off?
01:18:28.780 - 01:18:37.290, Speaker B: No, I don't think so. I think. I think we covered pretty much everything I wanted to kind of chat about. Thank you for hosting me. I. I always love doing. This is my, my first podcast.
01:18:37.750 - 01:18:39.030, Speaker A: Thank you for being the first.
01:18:39.150 - 01:18:56.502, Speaker B: Yeah, well, not only the live thing, but like my first podcast I've actually ever done. So we'll see how that went. But thank you for the recommendation for the podcasting microphone. I forgot my first. My first podcast. But, you know, I think. I think it's a really.
01:18:56.502 - 01:19:29.610, Speaker B: I know with the Bear mark and everything like that sometimes takes wind out of people's sales, but I do have to say that the parting thought I had is I've never been and more excited to be in crypto and be part of what everyone's doing. I think that the things, you know, not just us, but just across the space people are doing are so inspiring. It's great to be part of it, and, you know, I think that the cumulative work of all of us, you know, we'll make massive changes in the world around us, and so it's just really exciting to see that really come to truth before our eyes. It's just super inspiring variants.
01:19:30.350 - 01:19:37.550, Speaker A: I fully agree. I think that's a perfect place to end it on. Thank you again, Patrick. It was a pleasure. Awesome. We'll end it there.
