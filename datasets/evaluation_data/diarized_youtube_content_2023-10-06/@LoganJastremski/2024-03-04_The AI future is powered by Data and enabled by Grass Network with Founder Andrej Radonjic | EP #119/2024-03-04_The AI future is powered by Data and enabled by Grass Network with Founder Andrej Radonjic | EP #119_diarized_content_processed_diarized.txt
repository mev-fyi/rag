00:00:00.240 - 00:00:25.970, Speaker A: I guess as a broad stroke, we see grass as the data layer for AI. And what I mean by that is, when you're looking at the AI stack, you have two ingredients at the end of the day that are ever present. You've got compute and you've got data. And we feel that, especially in the decentralized world, in the field of crypto, data is severely underrepresented.
00:00:30.100 - 00:01:09.320, Speaker B: Well, today, I'm very excited to have on the founder of the grasp protocol. Really excited to have you on. We first spoke a couple months ago, and you were telling me how many people were downloaded the plugin for the grass network, and I was shocked by some of the numbers that you have ultimately been able to achieve. But now I feel like you guys are just now entering into the mainstream and really excited to dive deep into the nitty gritty of the grass protocol, what you guys are doing there and what got you excited about it. So thank you so much for coming on the podcast and really look forward to this conversation.
00:01:09.480 - 00:01:12.740, Speaker A: Yeah, thanks, Logan. And thank you for having me here as well.
00:01:13.400 - 00:01:31.210, Speaker B: Of course. Well, maybe for those that don't know you or are not as familiar with the grasp protocol, could you just do a brief introduction about yourself and how you kind of fell into this crypto world that we all find ourselves in?
00:01:31.870 - 00:02:08.180, Speaker A: Definitely. It was kind of a funny story, I guess. My background is in computational physics, like nuclear engineering, in industry, mostly in finance, honestly. And my experience with crypto is one that was kind of one of missed opportunities in the early days. Back in 2014, I participated in a doge faucet. I've told the story a few times, but essentially I accumulated a ton of doge, which at the time, I think was worth, like a few dollars or something. And during the last bull run, I just saw doge on the news, and that kind of got me back into crypto.
00:02:08.180 - 00:03:13.050, Speaker A: And there's this whole adventure of trying to get that wallet back because it was suddenly worth a bunch of money. And it was secured by my university email that I had at the time, but I had since graduated for years and couldn't get it back. And I was, like, calling the administration, trying to re register, and they were telling me, well, you've got to come back and enter a program. We'll give you an email. And I was thinking, okay, how much does tuition cost, and how much will I get out of this? Like, what's the expected value? And that was a whole little adventure. But as I was going through that, I started reading a bit about DeFi and I was honestly pretty fascinated by the way that a lot of individuals just sort of created these permissionless, composable systems that mimic the rest of finance, which is this really huge ossified industry and the pace at which it was changing. As I dove into that rabbit hole, I started seeing that a lot of these concepts that are being developed in DeFi could be extrapolated to other industries as well and used to disrupt them.
00:03:13.050 - 00:04:05.080, Speaker A: A few worlds collided. I met my co founders who have backgrounds in low latency systems and real time AI. And at the time I was actually running a small startup on the side while I was in graduate school. It was mostly based on web scraping scaling solutions. And as we started talking about web scraping and data and crypto and all three of us were into defi in that moment in time, we thought, hold on a second, there's a huge opportunity here because the way the web scraping industry exists at the time or existed at the time was extremely unethical and I'm more than happy to dive into that in a bit. That was the initial problem we set out to solve. But as we began building it as an entire new industry developed before our eyes with Chao GPT, we started realizing there's a much, much bigger opportunity here.
00:04:06.740 - 00:05:10.970, Speaker B: That's a crazy story with Dogecoin, I feel like it's a familiarly, a familiar story that unfortunately heard too often where someone has some amount of crypto, they forget about it and then crypto does its thing and then a couple years later they're like, oh shit, I need to find that USB, the drive that I threw away. Yeah, but yeah, very funny story. And I mean, kind of an amazing journey. I always find it interesting how kind of people's path into crypto is so circuitous, but interesting enough. Like crypto I kind of imagine as this black hole that's kind of eating a lot of kind of the traditional categories you mentioned. Ultimately quite a few things. Maybe if we could start off with what you envision kind of the grass network protocol becoming or like what is it today and where in that future state of the world, what value would you like it to provide to the world?
00:05:11.310 - 00:05:53.100, Speaker A: Yeah, that's a great question. So I guess as a broad stroke, we see grass as the data layer for AI. What I mean by that is when you're looking at the AI stack, you have two ingredients at the end of the day that are ever present. You've got compute and you've got data. And we feel that especially in the decentralized world, in the field of crypto data is severely underrepresented in its current state. What grass really is. It's a network where anyone can install a node on a device that they already own, free of cost.
00:05:53.100 - 00:06:44.586, Speaker A: As you mentioned earlier, it's as simple as just adding a chrome plugin if you're on desktop or if you're on saga, we're on over two thirds of the saga phones. Just because it's as simple as downloading an app and you start earning points. What it's really doing is effectively scraping the public web. And when I say the public web, I mean just websites that anyone can access. And you might ask, okay, why do you need a network of hundreds of thousands of devices on residential networks to do this? And then you start going into the rabbit hole and kind of the history of web scraping and web data extraction. So as I mentioned earlier, we actually set out with this vision of building a web scraping scaling solution. The idea there was that scrapers need residential networks, because if you're a flight company or like a travel agency, you need to know the price of every hotel in every country from the perspective of every ip address.
00:06:44.586 - 00:08:04.034, Speaker A: If you try doing this from a data center, you start getting honey potted, you start being given false prices that you can't execute on. And if that's the only data you have, it's actually literally impossible for you to operate in that industry. We were looking at this entire category and realizing, hold on, so how are they getting access to millions of residential networks? And we quickly realized it was through SDKs that they were sneaking into like free VPN's, free games, screensavers on smart tvs. There's actually a story I read about a garage door opener that was a botnet used by Fortune 500 companies. That was hilarious, but at the same time, a little bit freaky because a lot of people at the end of the day, were hosting some pretty serious infrastructure for massive corporations to just essentially abuse them and take advantage of their network resources. We thought, okay, all these people are getting robbed. What if we just cut out that middleman, the middleware provider that's sneaking these SDKs into all these apps, and made a system where companies that need access to this sort of data from a global perspective, from a residential point of view, can actually go in and start compensating users directly? We felt crypto rails, in the first place, are an amazing way to do this for two reasons.
00:08:04.034 - 00:09:00.194, Speaker A: One, you cut out any sort of race to zero because you eliminate any sort of margin. When you build a decentralized system. And two, just in terms of payment rails, how else can you go and compensate or incentivize a network that's present in 180 different countries around the world? If you try doing that in tradfi, you're going to have headaches. So anyways, as we were building this, I remember we had a lot of conversations in the early days between ourselves as co founders and also with a lot of external parties. And we were saying right now, all this pricing data for e commerce and for travel and stuff like that, it's very valuable. But one day there's going to be a huge industry around big data and machine learning, and a lot more data is going to become valuable and this market is going to explode. A lot of people thought, okay, yeah, maybe in ten years or something.
00:09:00.194 - 00:09:50.340, Speaker A: And then two months later, chat GPT came out and the game changed. And we saw this thing where new technological innovations, they tend to echo the same value extraction patterns from the past. So previously, ecommerce was the epitome of this sort of PvP behavior where everyone's just trying to scrape each other's data, like everyone's gatekeeping, everyone's honey potting or rate limiting blocking. And when ChatGpt came out, the game changed overnight. Like a few years ago, it was a very small subset of Internet data that was valuable, and today, literally all of it is valuable. And the Internet has become like not only the world's greatest database, but the world's most valuable resource. And all of us combined are actually what comprises the Internet.
00:09:50.340 - 00:10:55.996, Speaker A: And it makes a lot of sense for the data layer of AI to be owned by ordinary people and to be run by ordinary people. So yeah, in its current state, what grass is doing is it's scraping the entire web, extracting a ton of raw HTML from websites, preprocessing that into JSONL files or whatever, applying whatever necessary transformations you have in order to adjust that into an AI or ML model. We have a few other things that we want to add to this because, for instance, all those preprocessing steps, you have to customize them depending on what you're scraping. If you're scraping a table on a website versus if you're scraping a blog or a forum, the data is going to look very different. When you want to go and transform that data so you can feed it into an AI model, you have to write a different script every single time. If you think about how many different schemas and how many different websites exist on the Internet, it's like, okay, this could take a long time. But the beauty of AI is that it's able to be recursive.
00:10:55.996 - 00:12:07.010, Speaker A: And what I mean by that is we have this amazing technology nowadays that can transform language data into literally anything, if you think of HTML language. So we thought, okay, what if we train an LLM that's actually able to just go and structure all this web data into an ingestible format? Not only does that, you know, from a business perspective, save you a lot of money because HTML is very dense. Like, it costs a lot of money to store these things, but in terms of workflow, like, all of a sudden you're automating the entire data workflow from extracting the data to cleaning it and processing it. And we thought, all right, so we can train this LLM and we can stick that directly into the grass nodes. So not only is your node extracting data from the public web, but it is also driving insights from it. And that's sort of this value accrual mechanism that we feel is one of the biggest benefits of creating an integrated data layer instead of just focusing on a bunch of different, a bunch of different modular pieces or whatever. Um, and yeah, beyond that, one of the key tenants for us is the whole point of blockchain is to make it accessible to, like an average person.
00:12:07.010 - 00:13:23.998, Speaker A: Like to make an industry accessible to the average person. Like you look at Defi, for instance, um, like, if you think about something as simple as LP, like, sure, that has its own inherent problems, you're constantly getting arbitrage and whatever. There's some, like, uh, like a lot of things need to be figured out, right? But the idea behind it is very, very powerful, because essentially by creating an LP, you're becoming the exchange, and there's no other way in the world to actually take it or to actually earn ownership in a system that is actually quite profitable and has a very, very high barrier to entry. So we felt, okay, if we're going to build a data layer for all of AI to be built on top of, we want this data layer to be accessible so the average person doesn't need technical skills to go and build a miner or a validator or something, or to even just go and write their own machine learning model. I mean, they can if they want, but to participate in this network, that's unnecessary. And two, there's no upfront capital constraint, there's no barrier to entry in terms of not having enough money. If you have a device that's connected to the Internet, and I'd say more or less most of the population has that at this moment of time, globally, then you're good to go.
00:13:23.998 - 00:14:34.200, Speaker A: So, yeah, anyway, that's essentially what we're doing now, given all the participation in the network is completely passive right now. We actually started seeing a need for more active participation. What I mean by that is, when you scrape a ton of data from the Internet, when you go and do everything you need to turn it, that data into an AI model, or I guess train an AI model to go and summarize that data or draw conclusions from it, you get these outputs, and if you want to feed those outputs back into the model, you can only automate, like, 25% of that. The other three quarters of the work is actually human labor. And one of the things that makes Chad GPT so good and so our evolutionary is actually the fact that they have millions of hours of human labor that went into annotating data sets. And that's anything from writing, from aligning, from aligning outputs, saying things like, hey, I don't think a chatbot should be telling people to do these things, or just clicking on images and saying, this is an elbow, and this is a knee, and this hand doesn't have enough fingers. This is all human.
00:14:34.200 - 00:15:20.180, Speaker A: These are all human inputs. It's still data, and it's extremely valuable. And we realized, okay, we have this funnel of something that's approaching a million users at the moment, is growing very quickly. A lot of these people have joined the network because they share the similar vision of, hey, I have these resources, I would like to monetize them. And we want to give people the ability to also monetize another resource, which is time. And that's why we've added some people might have noticed in the dashboard, it says, data labeling coming soon. We're actually working on adding a new tab in the dashboard where people can actually go in and start doing these annotation jobs and things like that to actually help recursively train AI models.
00:15:21.840 - 00:17:04.374, Speaker B: I feel like we can spend the rest of the podcast parsing apart all this, but maybe starting with ultimately your point around data and how valuable data really is. And I think obviously, with chat, GPT, everybody wants to experiment with creating either new neural network models, large language models, really the Gambit, and more recently, that bottleneck has been compute, but that compute still does not matter, even if you have unlimited access to GPU's unless you have that data. And now it's been interesting just in my background being at Tesla. I think Tesla had really a unique advantage by having all the cars, whether you buy kind of the full self driving suite or not automatically have the cameras that collect that data and is used to kind of retrain that model for eventually what will become Tesla's full self driving feature suite. And that data set that they've collected by automatically including all the hardware necessary to collect that data has been a massive moat. And I think now, historically it's been even more competitive here. Even recently where I think it was like the Wall Street Journal meta or not meta twitter kind of closing some of their API access and refusing to allow scrapers to get access to that data because they actually know how valuable that data is for these large machine learning models.
00:17:04.374 - 00:17:35.569, Speaker B: And so I think it's a fascinating point that you really called out that, yes, we are in a kind of a GPU shortage, but there's also a shortage of making sure that you have the correct data. And I think that the industry, to your point, has not really focused on this as much because the immediate need, at least what people perceive, is the hardware side, but that software and data side is as equally, if not more important than just the hardware itself.
00:17:36.029 - 00:18:29.884, Speaker A: Yeah, absolutely. I don't know if you've read the Chinchilla paper two or three years ago, actually, some of the earlier research into, yeah, so it was actually pretty fascinating. Some of the earlier research into LLMs proved that most of them LLMs that are, that have come out recently, they're actually not compute optimal. So they, they trained a ridiculous number of models and evaluated their performance, and they came to a pretty fascinating conclusion. And that was that when you scale the number of parameters in the model, you need to proportionally scale the number of tokens that the model's trained on. And like, intuitively, this makes a lot of sense, but the fact that it was one to one was pretty fascinating. So what that means is if you double the number of parameters in the model, you literally need to double the amount of training data.
00:18:29.884 - 00:19:27.904, Speaker A: And what a lot of people, I guess, don't really think about is the fact that, okay, when you're increasing the amount of compute, the reason you need to do that is because you're adding parameters to your model. At the end of the day, you're doing proportionally more operations as you add parameters to an llmdeh. But what a lot of people are just not really spending a lot of time thinking about is the fact that we literally don't have enough data to optimally train models beyond a certain size, and that becomes a real problem. And to your point, about a lot of companies suddenly realizing, hold on all of our data is valuable. We need to start hoarding it. And that was one of the things that we were saying early on with, hey, all these e commerce patterns, they're going to show up in the rest of every industry that's on the Internet. Just because e commerce is so directly monetizable.
00:19:27.904 - 00:20:21.100, Speaker A: When you have a data advantage, you can in real time go and undercut a competitor or in real time go on snipe ticket or whatever it is that you're doing. The conversion cycle is a bit slower and it's taking a lot of companies a bit more time to realize how valuable their public web data is. But they 100% are the medium. CEO did an interview not too long ago, I think it was last year, where he actually suggested poisoning datasets. I thought that was fascinating because when you have an AI crawler that's going and reading a bunch of blogs, the thing is you have no idea whether or not the data that you're pulling from that website is actually the data that you want or just the data that they're giving you. It's very easy to figure out that you're getting blocked. If you're getting rate limited, you're getting a 403 error or something like that.
00:20:21.100 - 00:21:01.108, Speaker A: But the concept of just putting a bunch of garbage on websites and diverting crawlers is really fascinating. And that's actually something that's existed in other industries for many, many years. And now anyone that has any type of language data or any type of image data is starting to do the same thing. And that's one of the problems that we actually aim to solve is the data poisoning problem. It happens at two levels. So you've got data poisoning on the web server side where a company is saying, you know what, we don't want you to go and crawl our website. We don't want you to scrape our data because we think it's valuable.
00:21:01.108 - 00:21:45.096, Speaker A: We want you to pay for it, but we don't know how valuable it is. So we're not going to actually offer for you to pay for it because we don't know if we're going to get ripped off or not. So instead we're just going to go and divert your traffic or just give you really crappy language data that we've generated with a really bad LLM and then we can go and poison your model. And that happens on the web server side, but then you've got something that's a bit more malicious, which I like to think of as the ad tech cycle. So you've got the e commerce cycle, which is ok, you're going to change everything happening at the web server side. And then ad tech is a very interesting one, because everything that happened in search engines is going to happen with LLMs and chatbots. This isn't even some sort of theory or prediction.
00:21:45.096 - 00:22:26.790, Speaker A: I just think it's common sense. The incentive has never been so high for someone to give a chatbot powered by an LLM the same opinion that they have. So whether that's a political opinion or a brand opinion. Now, when you have a massive open source data set, let's think about common crawl, for example, with petabytes of data. If someone goes in and retroactively adds 1000 sentences that say Sephora is the best place to buy makeup, how do we catch that? It's very difficult. If someone goes and alters reviews that are being used to train an LLM, it's very hard to know. You can look at the versions of files.
00:22:26.790 - 00:23:31.808, Speaker A: At the end of the day, do you really want to use that as a metric? That's so easy to spoof. So then you have this other problem with data poisoning, retroactively so at the scraping side, and then also once the data set is structured and cleaned. And one of the things that we actually want to implement once we're fully launched and on chain is a proof of request mechanism. So as data is getting scraped from the Internet in real time, every individual node is submitting a proof of this request to a smart contract. That smart contract was delegated some tokens from a centralized or decentralized sequence or whatever it is. And then as a reward for that proof of request, it goes and earns its compensation or its incentive or whatever. What you end up doing with that, using ZK TL's to do this is you create a clear link between the scrape data and the website at the time that it was scraped.
00:23:31.808 - 00:24:08.610, Speaker A: So you remove any suspicion of whether or not someone might have tampered with this dataset. And this is something that not a lot of people are thinking about right now, but it's going to become increasingly valuable. If you think about the way your search engine operates. It's kind of a black box, but you rely on it for the wealth of your information. 90% of the information that hits your brain is coming from a search engine. Someone asks you, hey, did you hear about this thing that happened in the world? You're going to go and google that thing, you're not going to go to the New York Times website and search for it there a lot of people don't really think about this too much. LLMs are going to be the medium for this in the future.
00:24:08.610 - 00:24:33.820, Speaker A: We've already seen how ad tech has become pervasive across search engine, that it's going to do the same thing to LLMs, and we want to make sure that if they're going to do that, they're doing it fairly. And the only way to do that is to create these blocks and to create these proofs, because otherwise you're never going to know whether or not what you're receiving is unbiased and uncensored information.
00:24:35.480 - 00:25:21.448, Speaker B: I think that's a great explanation. Maybe before going a little bit more down, like the nerdy side of things and that rabbit hole of how you guys are doing it, I really want to double click on why this data is so valuable and why, and I think you've touched upon it a couple times with like scraping competitors websites, being able to undercut them, training just large language models from a neural network standpoint. Can you go just a little bit more in depth on why this data has now become much more valuable than, say, even a couple years ago, and why people are being a little bit more protective of that data set?
00:25:21.624 - 00:26:05.230, Speaker A: Definitely. So at the end of the day, without data, your model is nothing, right? You can have, and languages are probably a good analogy for this. You can have petabytes of data in English. If you don't have anything in French, your model will never know how to speak French. And that's kind of a silly example, but I think it very clearly demonstrates the fact that if you want your model to know how to do something, you have to train it somewhere. And if you think about the way the industry is progressing right now, we've got, of course, this huge supply bottleneck when it comes to compute. But there are a few very large centralized players that kind of control the development of this industry right now.
00:26:05.230 - 00:26:47.370, Speaker A: And for them, money isn't really a problem. And the compute bottleneck is really just a money bottleneck. At the end of the day, if you have enough wealth, you can gather enough compute to do what you need to do. When it comes to data, it's a bit of a different story, because we're building a lot of models to emulate human behavior. And in order to do that, we need data that was produced by humans. And the data produced by humans that we specifically care about is the data that's on the Internet. We see that as one of the most obvious ways to go and gather very large amounts of this data.
00:26:47.370 - 00:27:47.820, Speaker A: It's one of those things where if you don't have, for example, Twitter going and rate limiting their API, for instance, Twitter is amazing for trading a chatbot because it's all conversational data. And it's no surprise that when you go and access Twitter while you're not logged in, you don't see any of the replies. It's because the replies to posts are actually what's valuable. And that's one of those things where we don't even touch Twitter for that reason, because we focus on public web data. But Reddit's another one, too, and Reddit's pretty fascinating because, and something a lot of people don't realize is most of the original chat GPT was actually just trained on Reddit. And the reason it's so good is because, a, it's conversational, but b, they have this upvote system and most of it is in question and answer format. So people are posting questions or concerns or whatever their thoughts are, and a ton of people are responding to it.
00:27:47.820 - 00:28:27.920, Speaker A: And it's a chatbot's job to learn and understand how to respond to something. What you effectively have is millions of people, a, responding to things, and b, rating those responses with an upvote system. What they're really doing is just training a very, very large AI. Reddit took down a bunch of their APIs last year for literally this reason, because they weren't paid a cent for any of this data. It was just publicly available on the Internet. And they thought, oh, my goodness, we should have gatekept this, we should have charged for this, because an entire industry was born out of what we've created and we're not getting anything for it. And it's just one of those things.
00:28:28.620 - 00:29:22.990, Speaker B: In college, I was doing a research project where we created a very small neural network that scraped our cryptocurrency and based off, we ran sentiment analysis based off the words and ran it through a neural network that would give us our prediction of whether crypto prices were going to go up or down based off sentiment. It was kind of a cute thing, but it was interesting just how we could scrape Reddit at the time. And to your point, we used the upvotes and downvotes to help that neural network really synthesize. Hey, are people bullish or bearish and try to make that into a trading algorithm? It sucked, but it did not work. But it was interesting that we could even try that, make it at least some version of a real world product.
00:29:23.770 - 00:29:57.890, Speaker A: Yeah, absolutely. And one of the interesting things about data too, when you're comparing it to compute. Because at the end of the day, your AI is two things, compute and data. Everything else is just layered on top of that stack. When you're looking at compute, it's actually pretty homogenous. Sure, you can have specialized compute for certain operations, but at the end of the day, the main metric that you actually care about is probably floating point operations per second or per some unit of time. And you can kind of scale this horizontally quite easily.
00:29:57.890 - 00:30:50.572, Speaker A: But when you're looking at data, it's not as homogeneous because you need specific facts. And if you're trying to create, like, these semantic trees to get models to actually understand something or emulate knowledge, you need those specific facts readily available. And if you don't have them, it's very difficult to guarantee that your model will know them. Whereas if you have enough compute, you can always guarantee based, if your bottle is trained properly, that it's going to converge. But if you don't have the right data, you'll never have that guarantee. And that's why it's also very difficult for a lot of companies these days to put a price on data. Like I mentioned, the medium, like CEO's interview earlier, one other thing he had mentioned was like, hey, we want to compensate our authors, but we don't even know what to charge for this because certain data is more valuable than other data.
00:30:50.572 - 00:31:23.630, Speaker A: But how much more valuable and for how long will that be the case? So, yeah, it is pretty fascinating. It's one of those things where it's a huge bottleneck, and maybe that's why it isn't being talked about as much, is because it's very difficult for people to understand how it scales because you need a very large diversity of data in order to build a great model. Whereas when you're collecting a ton of compute, you don't really, you just get a bunch of the same GPU's, right? It's not quite the same problem that you're trying to solve. It is a fascinating one, though.
00:31:24.610 - 00:32:11.848, Speaker B: Yeah. And I think a lot of great examples are there. I think Reddit, Twitter or X. I mean, it's interesting how these platforms are very collaborative in the beginning, and they're like, oh, we have something very valuable over time and kind of become a little bit closed source. And I think ultimately that's at least the promise of web3, is to make it a little bit more democratized and giving people more fair access, that you can't be rugged by the underlying platform. But I would love to kind of shift a little bit. So we've gone down pretty deep into why this data is valuable and we've touched upon too how you guys are starting to scrape with this public data that you are scraping, not private data.
00:32:11.848 - 00:32:41.868, Speaker B: I would love for you to go a little bit more in depth on why you need these residential IP addresses to scrape this public data and why it's not really possible. And I think you touched upon it initially with either being rate limited or honey pots. But what is the secret sauce that these residential IP addresses unlocked that allow you to get access to this data where traditionally you would not have access to that?
00:32:42.004 - 00:33:43.080, Speaker A: Yeah, that's a great question. And it's kind of one of the biggest differentiating factors of our network as opposed to other web scraping solutions is the fact that all of it natively lives on real devices, on real people's networks if you think of it. I guess the way I like to think of it is from the perspective of the person that's hosting the website. You've got this web server with this website and the way it really works is your computer is going and asking a question to the website and the website based on who's asking is going to give a different response. So if it sees for example an AWS server running some crazy Linux distribution with ridiculous hardware specs it's going to think hmm, is this a human or is this a machine? And it's probably going to say okay, this is a machine. Whereas if they see a MacBook on some residential IP address in Germany it'll probably say okay this is a human. So we're going to give it the output that we want to show a human.
00:33:43.080 - 00:33:54.988, Speaker A: Now the output that will usually show a machine is 403 forbidden or access denied or something like that. And then what it'll show a human is what it actually wants to show.
00:33:55.164 - 00:34:14.604, Speaker B: Now that's actually interesting because I have a MacBook Pro, but I also have a custom gaming computer with a little bit higher hardware respects. And I've been seeing those 403 forbidden errors and it makes a lot of sense. They may just be too beefy of a computer and that they're like no normal person would actually run this. You're forbidden.
00:34:14.692 - 00:34:40.229, Speaker A: Yeah, that's exactly what it is. You've got your device fingerprint and your IP address. A lot of people don't realize this, but every time you visit a website you're sending that in your HTTP header. Your browser is saying hey I'm chrome, I live on this machine, we're in this network. And because the number of IP addresses in the world is actually limited. We're talking about IPv four here, and that's the dominant one. It's here to stay.
00:34:40.229 - 00:35:29.060, Speaker A: I'll touch on this very briefly, because a lot of people don't realize this, but it's pretty fascinating. We ran out of IP addresses more than ten years ago and the price went from zero to $50 to buy a single IP address because this secondary market got created and then a lot of different data centers needed ips, or different ISP's needed them and started trading them and stuff like that. But because you have this limited supply of IP addresses, there are actually databases out there that just go and label them and soft KYC every single one. So you can actually tell very easily. And this is how we can actually tell as well. When you're running the extension, if you try turning on a VPN, your network score goes to zero. It says you have a VPN turned on, turn it off if you want to keep earning points.
00:35:29.060 - 00:36:51.606, Speaker A: And then you can actually identify whether something is like a commercial IP address, whether it's a data center, who the ISP is, and that ISP can either be AWS or it can be at and taideh. When you have a distributed network of IP addresses and device fingerprints that is global and spans millions of computers on a wide range of networks, you can actually get a complete and unbiased view of the Internet, because all these individual web servers are going to spit out different things based on where you are and who you are. So, you know, sometimes, and one of those things I mentioned earlier, like data poisoning medium, is not going to poison data. If you're some residential user just browsing their blog, that would just suck for them. They're going to poison it if they see the aforementioned crazy spec'd out Linux distribution on an AWS network. If you have a wide variety of asns and a wide variety of geographies, you can actually not only get around rate limits, but capture all kinds of different views of the Internet. So whether it's like numeric data, such as what are the prices of things in various regions, or just websites that automatically get translated to other languages when you visit them, there are a lot of things that you're able to do.
00:36:51.606 - 00:37:22.300, Speaker A: And having that residential device on a real by paying for your Internet plan and by hosting a piece of hardware on that network are creating a lot of value. And at the moment you're not seeing any of that value, even though there are companies out there that are monetizing it. And one of our core missions is to actually just bring that power back into the hands of people and say, listen, you're the one that's hosting this infrastructure. You're the one that should be compensated for it. That's basically it in a nutshell. Really.
00:37:24.120 - 00:38:00.074, Speaker B: Yeah, it's pretty fascinating as you walk through it, a lot of the pieces, it really makes sense in terms of some of the limitations today and how you guys are going about it. One piece of FUD that I hear about the grass protocol is that the network is scraping that personal user data, and that's just incorrect. Can you talk a little bit more about what data is actually being used from the grass network once that plugin is turned on and you start earning points?
00:38:00.242 - 00:38:37.954, Speaker A: Absolutely. Yeah. Happy to dive into this. So when you install the grass plugin, actually, one thing a lot of people don't really realize about plugins is that you can check the source code, you can right click it, you can inspect it, you can check its network activity and see exactly what it's doing on your chrome browser. If you were to go and do that, you'll see that grass has nothing to do with any of your personal browsing data or any of your personal data or your sessions or anything like that. And beyond that, if you go to the Google Chrome extension store, you click on the privacy practices tab. I think that's what it's called.
00:38:37.954 - 00:39:25.168, Speaker A: You can actually see that the only thing that's being collected is your location. We need to know the location because we need to route web requests. The only things that the grass extension is actually collecting from a user perspective is the device fingerprint, which I mentioned earlier is a very important thing when you're doing web scraping and to the IP address, everything else is completely anonymous. And you can kind of think of it as, we're not like, this has nothing to do with personal browsing sessions or private data. Think of it as, you know, hivemapper, right? Like people driving cars around the world, mapping streets. You can think of it as a digital version of that. Wherever you're setting up a node that's going and mapping the streets of the Internet, the public Internet.
00:39:25.168 - 00:39:58.178, Speaker A: So I'm talking things like forums, things like review websites, things like job listings, things like Wikipedia, Reddit, stuff like that. And when you have this, while you're running this drone and while it's going and collecting all this data from the Internet, you're being incentivized with points at the moment, but none of this data actually belongs to you and none of it is really specific to you. It's just your view of the public web that's being collected.
00:39:58.274 - 00:40:41.560, Speaker B: So, yeah, I appreciate you clarifying that because I think all computers have IP addresses and you're sharing that ip address really regardless. And what VPN's ultimately do is kind of spoof your ip address. So it seems like you're in some other location that allows you to watch Netflix in Germany or ultimately, yeah, just change your location. But this is not data that's generally tied to your personal computer or your like, address, your browsing history or anything like that. And so I really appreciate you clarifying that.
00:40:41.860 - 00:40:42.840, Speaker A: Yeah, thanks.
00:40:43.740 - 00:41:26.368, Speaker B: In terms of one thing that I mentioned earlier that I think when the first time we spoke that was very impressive, is just how many people have actually downloaded the grass plugin and have started contributing to this network. Can you talk a little bit more about any of the figures of how many people have approximately downloaded the plugin for an individual basis? Are they communicating to the network? Kilobytes? Megabytes? Are they doing gigabytes of data? Can you share a little bit more about, if you do download the plugin and start contributing to the network, how much data is actually going to be utilized at an individual basis as well?
00:41:26.424 - 00:41:57.920, Speaker A: Yeah, great question. On an individual basis, it's actually quite low at the very max network capacity. You're probably looking at something like 30gb in a month. And it really depends on your geography and where demand is for certain types of web data. But yeah, in terms of the user growth, it's been really fascinating to see. I think one of the big things that drove that has been the fact that it's very easy to join the network. All you really need is a referral link.
00:41:57.920 - 00:43:06.790, Speaker A: And the fact that the points system heavily incentivizes referrals has caused a lot of just very natural people just naturally going and saying, okay, I have a very large following, I'm going to go ahead and share this referral link because it's going to benefit me with more of these points. And people have been just growing the network very organically and it's been really exciting to watch. And yeah, I think at the time that we're recording this, we're coming up on 700,000 people on individual IP addresses, on individual devices that have installed the chrome extension. And we're on over two thirds of the saga phones. And one of the things that we've got in our back pocket at the moment is the Android release. So it's been approved by the Play store. And right now for us, it's actually mostly about making sure our systems are scalable enough so that when we release the Android, once we release the mobile app and literally millions of people joining that work because we know it's going to be around that number, we're ready for it and things don't start breaking and things don't start going down.
00:43:06.790 - 00:43:18.422, Speaker A: So right now it's mostly more of a scaling problem that we're solving ahead of launching the Android phone just because we know the network's probably going to ten x now as far as like.
00:43:18.606 - 00:43:20.382, Speaker B: A good problem to have as a.
00:43:20.446 - 00:43:57.536, Speaker A: Startup, I think so, yeah, yeah, yeah. Not the worst. A bit stressful, but not the worst. Now one of the pretty exciting things actually is like in our early tests and we did this about a month ago, we decided okay, let's go and scrape a bunch of articles and test the network's capacity for web scraping. And using at the time 0.1% of our network, we were capable of scraping around 15 million articles in a day. And not just scraping them but structuring them into data that's actually ingestible by an LLM.
00:43:57.536 - 00:44:05.020, Speaker A: So that was really exciting. And we'll be conducting further tests and probably releasing user statistics around that.
00:44:05.960 - 00:44:49.890, Speaker B: Amazing. Very impressive in terms of maybe jumping back to the gigabytes on the high end, potentially using up to 30gb a month. Do you ever feel like if this product hits scale in terms of say 100 million people contributing their IP addresses or 1 billion people contributing their IP addresses, that Internet service providers would flag this traffic and potentially say, hey, like we agreed to allow you to use the Internet, but please don't use it for this capacity. And then they frown upon this type of activity or start to implement some different rate limits such that you have less network traffic over time.
00:44:50.230 - 00:45:31.308, Speaker A: Yeah, it's a good question. So most ISP's don't actually care about what you're really doing with your bandwidth, but they care about it whether you're reselling throughput. Now those are very different things where, you know, bandwidth throughput is more of like a rate of change, right? It's how, it's like you've got like flow in a pipe, like how much can you push through in a given second? And usually, you know, in developed countries that can be like 300 megabits per second on a good, pretty good line. Then in data centers you'll see like a terabyte or ten terabytes. I've been seeing 100 terabyte per second connections like it recently. Data centers throughput is actually where ISP's make their money now. Bandwidth.
00:45:31.308 - 00:45:43.916, Speaker A: And the amount of bandwidth that you're actually monetizing with a tool like grass is pretty negligible in the eyes of an ISP. And that being said, this is already.
00:45:43.948 - 00:45:55.894, Speaker B: Happening is that just because it's spread out over a month time, it's not like you're propagating a lot of data in a spike, generally a slow trickle throughout the month.
00:45:56.022 - 00:47:07.342, Speaker A: Yeah, exactly. You can think of the, I mean if you think about the active web scraping, really, it's kind of like someone opening up a chrome tab and just browsing through a bunch of different websites. If you think about how much bandwidth that actually uses, it's more or less impossible for any ISP to actually flag that because the behavior is literally just reading websites from the Internet and you already have so many other automated processes running on things that you've downloaded that are accessing the Internet programmatically, like whether it's APIs that websites are calling or apps you've downloaded and things like that. It's almost impossible to just flag that from an ISP's perspective without ruining the user experience and just literally blocking websites for you to go into browse. Now the other thing too is this is already happening in most people's networks. Most of us have downloaded something like if you own a Roku TV, you're probably contributing to one of these networks, you're just not being compensated for it. And you know, you go and you downloading a screen saver and then that screensaver is sharing like 20 to 30gb of bandwidth per month for web scraping purposes to train AI models and you not being compensated for it.
00:47:07.366 - 00:47:12.570, Speaker B: This is a low key show of we should all ditch our Roku and go buy an Apple tv.
00:47:15.450 - 00:47:51.750, Speaker A: I won't give tv advice, but honestly, just one of those things that's pervasive. It's going to happen no matter what. There are actually some ISP's that are selling bandwidth out from real people's routers. That's a completely separate issue. I probably won't dive into that one. But at the end of the day, most of us already have devices that are engaging in this type of activity. If ISP's are going to block something, they're much more likely to block that than a network that's actually just automating a bunch of web scraping and compensating you for it instead of just doing it out from under your nose.
00:47:52.570 - 00:48:49.210, Speaker B: Interesting. Well, we've covered a lot, so ultimately touched upon the differences or kind of the being in the current AI boom. You have the hardware limitations that are there from just a compute capacity. But if you have more money, theoretically you could get your hands hold up more of compute. But there's that additional problem of, look, it's not only compute, you really need the data and a diverse data set to train these neural networks on. And that data is actually very important as people start to realize that data is more and more valuable. And by using the grass network, what you're ultimately allowing is these home residential IP addresses that are generally not flagged.
00:48:49.210 - 00:50:00.848, Speaker B: They don't get rate limited, they're not traditionally honey pot, or the data is not poisoned in some form or fashion because people think you're a bot traffic. It allows you to scrape this very valuable data from websites that generally would be much harder or if not impossible, given the amount of compute resources needed to scrape this at scale. And the grass network, by having 700,000 approximate users, is able to tap into these residential ip addresses and build a very strong collective, robust data set that is helpful for these AI models to train very useful things. And I think maybe the last thing that I kind of want to touch upon that I don't think we've been able to dive into as we're closing out the podcast is what you're doing in structuring this data, because you've mentioned this a couple times where now you have this extremely valuable data, but data in and of itself has to be structured to be digestible. Can you talk about what you're doing to make that data digestible as we close out the podcast?
00:50:01.024 - 00:50:33.458, Speaker A: Yeah, yeah, absolutely. So in essence, the problem really boils down to two processes. The first one is taking your raw web response for a web server, which is just a bunch of garbled HTML. You take that and then you transform that into a JSON file or something similar. Then what you do beyond that is really up to the model. You might choose to go and do some embeddings, or you might want to go and tag that or whatever it is. We leave that to the AI platforms to figure out.
00:50:33.458 - 00:51:17.600, Speaker A: This is more of just a base layer of data. The idea is to just make sure that no one has to go through the process. If you try using common crawl, for instance, it is such a headache because it's just a bunch of garbled links in HTML. We figured if we're going to build a decentralized version of common crawl, we'd like to do it isn't the case. So that is mostly just a ton of custom scripts. Honestly, it's a bunch of python scripts that are just going and cleaning up this HTML. And at the moment, we're exploring, trading our own LLM to go and automate this because having a separate script for each of these things is extremely time consuming and just very difficult to implement if we're going to put more workload on the nodes.
00:51:17.600 - 00:52:12.894, Speaker A: But having a very lightweight but specialized model sort of changes the game. And then the second step, actually, when it comes to preprocessing data, is labeling, and that's adding this human feedback to help push this reinforcement learning loop. One example I like to really enjoy, actually, is if you're training an image model, you can go and scrape a ton of images of faces. For example, you want this image model to produce faces with certain emotions or whatever features. You can scrape a bunch of faces from the Internet. Eventually you run out of faces on the Internet because either a, people stop, people aren't taking photos of each other or of themselves at a fast enough pace, or b, most of the data on the Internet is just AI generated. It's very difficult to differentiate whether it's an actual image or AI generated.
00:52:12.894 - 00:53:20.866, Speaker A: And you don't want to mess with your data set. You say, okay, there's another way of training this, and that's recursion. So you go and generate a million faces using your initial model, and you give it these prompts, and it goes and generates faces. Then you say, okay, now we need a human to sort through this, or we need many humans that are distributed globally with different, you know, cultural biases and different, like opinions and different, like, you know, whatever, just like different styles of life to go and look at these faces and say, okay, I interpret this face as this emotion or this class of beauty or whatever it is, then having that human element is actually so, so valuable. And for us, it's very obvious to just add this to the platform. So that's something that we're pretty excited about launching soon, is it's almost like a mini game that you can go and play where you're being incentivized to go and just classify data that was generated by an AI in order to give it back to the AI and tell it, okay, here's this augmented synthetic data set, and it's been augmented by a human's opinion. Now go do it better.
00:53:20.866 - 00:53:23.270, Speaker A: Go ahead and do it better. And just continuing that loop.
00:53:24.850 - 00:54:16.210, Speaker B: So, yeah, it's very hard. I mean, in general, well, it's not difficult for humans, but I mean, I think the holy grail of AI is like self labeling, self training data, where you just give it more data and it's able to parse apart all these things. But until we kind of have this general purpose AI, you're still going to need humans in the loop. And even at Tesla, I mean, they had this massive data set of the cars. They had different diverse training sets with the cars being driven in very unique places, but you still need a human to go in and say, hey, this is drivable space. This is not drivable space. And really make sure that not only is the data diverse, but you have people going in and making sure that labeling is correct so that it can be applied correctly when put into these large networks.
00:54:16.750 - 00:55:13.306, Speaker A: Absolutely. And, yeah, yeah, I mean, it's also one of those things, too, where it's unclear how far we are from self labeling and self improving models. I think, actually, I had a friend that was really into self driving cars a few years ago, and he told me something very interesting, which I guess, like, you literally worked in that space or in that field. So this is probably might be redundant telling you, but I remember hearing that self driving cars are a very difficult problem to solve, because in most applications, having, like, one or 2% error is fine. But if you have a self driving car that is like 99% good, that 1% could kill people. So it's. And regardless of whether that's still better than a human, you don't really, you want to minimize that risk wherever possible.
00:55:13.306 - 00:55:52.554, Speaker A: And the problem with minimizing it is like, you have this logarithmic function of when you add more resources to try to train a model. Like, the improvements become much, much, much smaller as you move along that curve. And I actually think that we haven't started really seeing these loss functions converge yet for AI. But as we do, we're going to start seeing exponentially more human input necessary to train the next iteration of AI models and exponentially more raw data. And that might be a big part of why we haven't really, really seen that flywheel take off yet.
00:55:52.642 - 00:56:24.070, Speaker B: But, yeah, yeah, it is interesting. I mean, if one out of 100 times a car wrecks, I don't think you would get in that car. You definitely need 99 and mini nines of reliability. And before you say, okay, sure, I'll trust this thing, but it is a march of nine s and going to be a long road. So it is interesting. This self labeling data is the holy grail. But until then, we'll have humans in the mix.
00:56:24.070 - 00:57:45.720, Speaker B: And I think to your point, throughout all this, it's, it's not only compute, you need a lot of data, and that data is extremely valuable. It's not only as people realize this data is more and more valuable, you need to be creative in figuring out ways to access that. And historically some of these scraping protocols have been a little nefarious and being kind of cheeky about it in terms of routing or Roku tvs, et cetera, et cetera, that you're not actually getting paid and your resources are being used anyway. And so now what grass network is ultimately enabling is, to your point, throughout all this is removing the middleman such that you're rewarding the direct contributors to the network they're opting in and knowingly getting points for that contribution to the network, which I think is a fascinating flywheel. The fact that it is pretty passive in terms of hey, download the plugin and you're off to the races, I think is even more evident to the fact of how many users you or contributors to the network you have been able to surmass in just a short period of time. It's really fascinating.
00:57:46.640 - 00:57:48.580, Speaker A: Thanks. Thank you.
00:57:50.000 - 00:57:57.260, Speaker B: As we're kind of wrapping up the podcast, is there anything that we missed touching upon or anything that you would just like to double click on?
00:57:58.720 - 00:58:12.814, Speaker A: Honestly, I feel like it's been pretty comprehensive. I think maybe one thing that you could do is drop your grass ref link somewhere in there for sure. Yeah. So users who are in front of the network, what's that?
00:58:12.982 - 00:58:16.486, Speaker B: Yeah, for sure. I can put it in the show notes.
00:58:16.638 - 00:58:17.342, Speaker A: Sounds great.
00:58:17.446 - 00:58:21.170, Speaker B: Yeah, we'll get some more people onboarded.
00:58:21.550 - 00:58:22.370, Speaker A: Sweet.
00:58:24.150 - 00:59:21.630, Speaker B: Awesome. Well, thank you again for podcast. I think you did a phenomenal job of explaining not only the technical aspects and why a network like grass is going to be required for the future to get get access to this valuable data. But from a product sense, why you need this data in the first place. And it's not just compute and these AI networks, it's also the data that data that is cleaned at scale, diverse, all these attributes. I think it's very insightful that you're really thinking about this, and I am excited for what the grass network is going to be doing or has already accomplished up until this point. But I'm very excited to see the next couple years and how large the grass network grows, because this problem is really only at the precipice of where I think we all think the AI movement is going to be in a couple years time.
00:59:21.630 - 00:59:37.372, Speaker B: And you're really well positioned and the grass network are very well positioned to help catapult that movement even further. So thank you again for coming on the podcast. Thank you for diving into what you're building and excited for what is to come.
00:59:37.516 - 00:59:42.670, Speaker A: Yeah, and thanks again for having me here. This was. This is actually a lot of fun, so thanks, Mandy.
