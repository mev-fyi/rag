00:00:00.400 - 00:00:20.702, Speaker A: If it's going to be able to serve the planet and whatever else, then you really need it to be linear in scale to some degree, right? You add more compute, you add more resources, and if you've got double the amount of resources that you had before, then you could potentially scale twice as much as you could before. Whether that's good transactions, bad transactions, whatever it is, right?
00:00:20.766 - 00:01:05.330, Speaker B: The previous sharding attempts that we have seen up until this point with kind of L2s or kind of even how near approaches sharding those type of sharding as reflected to the user, kind of say an optimism shard or a polygon shard are known to the users which make that confusing not only from a user experience, but also as a developer experience and RaDX point of view is to enable sharding to get that further scalability down the line, but ultimately have it integrated such that it's at the software level where users and engineers, it's kind of opaque to them.
00:01:06.030 - 00:01:16.490, Speaker A: To a naive observer, a user or a developer, all they just see is I send something in and then I get something out and I don't have to worry about where I'm sending it to or what it's doing.
00:01:20.110 - 00:01:54.468, Speaker B: Well, today I'm super excited to be joined by Dan, one of the co founders of RadX. Dan, thank you so much for joining me. We were just talking briefly. I feel like I've interviewed quite a few people, technical co founders in the space, but I have not really dug into RaDx on Twitter or X. I'll tweet something technical and the Radx community will ultimately come into my tweets and say, why are you looking at RaDx? So really excited to dive into what you're building and ultimately kind of your journey into web3.
00:01:54.524 - 00:03:29.660, Speaker A: Yeah, cool. Thanks for having me on. So, a bit background, I guess. So, my journey started a long time ago in 2012, when I really started to think about what is bitcoin, what is it doing, what does it mean for the world as it happens? Initially, I was more interested in the technical aspects for me, kind of computer science consensus point of view. Right. How is this doing what it's doing in such a way that it's permissionless, which hadn't really been done before, so that you could have like these kind of non third party permissionless consensus mechanisms and then from there kind of dug into that rabbit hole and I got an appreciation for the economic aspects and how disruptive that could be as well, and just kind of went down the bitcoin rabbit hole for quite some time. But being the kind of technical engineer that I am, it was a case of take it apart and see what's not optimal or what problems is this going to run into if it wants to potentially scale to the planet and stuff and highlighted a bunch of things that was like, okay, this is going to be, it's difficult to scale this thing and you're going to get centralization of mining power and the volatility of the currency might end up being a problem long term if it doesn't stabilize.
00:03:29.660 - 00:04:26.706, Speaker A: And all these are kind of like a short list of stuff. Yeah, this might be a problem. So initially kind of thought, well, maybe I can look at solving some of these from the perspective of bitcoin. But after a while of doing that, kind of became apparent that the foundation of bitcoin is very difficult to scale and difficult to alleviate some of the issues around decentralization and all that kind of stuff. So ultimately thought, well, new file, let's go. Let's start from scratch with the fundamentals of bitcoin and that whole philosophy and ideology and try and build something else like a second generation if you like, that solves some of those issues. That's where my journey started.
00:04:26.706 - 00:05:57.818, Speaker A: And scaling these kind of systems and scaling them so they're responsive, you're not having to wait 60 minutes for a transaction. And what does the scaling strategy look like? What does that model look like? How do you preserve things like liveness and safety and different scalability architectures? And what about usability and developer experience and all these other things? You start to get on that rabbit hole and that rabbit hole goes real deep. Spent a long time just really under the radar, just researching for a long time, just on my own with a small community that kind of gathered around me, interested in what I was doing in the coding cave, as they called it, for a number of years, and then started to make some real progress with scaling solutions and nail them to the wall and say, okay, that's definitely a component, that's definitely a component. This is a component to preserve decentralization. This is a component that preserves this particular thing that we need and then start to scale the project out to bring more people in, engineers and then follow C suite and marketing department and all that kind of stuff. And so over the past couple of years, our scalability solution has been peer reviewed and past its peer review published in a journal. And there's all the kind of interesting stuff around our smart contract language and developer experience, user experience.
00:05:57.818 - 00:06:03.990, Speaker A: It's all kind of complete stack built from the ground or basically amazing.
00:06:04.070 - 00:06:37.900, Speaker B: Definitely appreciate all the background context and that journey. You mentioned some limitations ultimately discovered by bitcoin, but maybe more broadly, what do you feel like are the largest limitations to actually scaling blockchains from either a networking side consensus, the virtual machine, layer storage. Where do you ultimately see the bottlenecks arising and why did you feel like a new kind of layer one ecosystem was necessary to get that off the ground?
00:06:37.980 - 00:07:16.502, Speaker A: Yeah. So from the early days, really my kind of idea of a scalability solution was sharding. So I was investigating sharding solutions back in 2013. I think I was the only person investigating sharding solutions back in 2013. And so it always kind of monolithic systems never scale. Right? You look at Google with all of its data, it's not all stored on a single machine, and it's not replicated across the same data. The same monolithic hunk of data isn't stored on a bunch of replicas in its entirety.
00:07:16.502 - 00:08:35.560, Speaker A: It's split up, it's chopped up. There's an addressing model, estate model that addresses on which machine and the giant data center this piece of data is on and stuff. So, thinking forward looking in terms of cryptocurrency, if you want to be able to serve 8 billion people and all their fridges and toasters and phones and everything else, then it seems like the only, the only real viable option. Right. And the early scaling solutions were, I was thinking about where were kind of primitive in terms of, okay, well, you could have this particular shard for these kind of transactions, and you can have this particular shard for this kind of transaction, which is like subnets, which Avax is doing, right, or there you have side chains and all these other things, but then thinking about that trajectory of sharding, it's like, yeah, but what if these two different transactions need to interact with each other? Then you've got cross shard communication complexity and issues around safety and liveness and rollbacks and all these things. So maybe that approach isn't the right approach. So that was like the nth iteration of, okay, new file, start again.
00:08:35.560 - 00:09:59.920, Speaker A: Lessons learned. There's been about five or six different iterations where pretty much built an entire kind of solution and then understood that, okay, this is a step closer, but then there's still these issues that don't really make it viable in terms of all transactions can interact with all different applications and dapps and you can have all these kind of very exotic transactions that do multiple things, composability and all that kind of stuff. Very hard to do that if you sidechains or sovereign chains and subnets and all this kind of stuff. And even more difficult to do if your scaling solution is like Altoo, you're getting fractured liquidity and fractured application state and all this kind of stuff. And so the kind of, the approach that we finally took was, okay, well, let's just have a kind of a unified scaling model where you have shards and you have state that is kind of distributed around all these different places. But from the point of view of the state model, and from the point of view of the execution engines and from the point of view of the users and the developers, they have no idea that it's sharded, right? So all of the sharding is actually abstracted away, and anything can live anywhere. And the key to being able to do that is actually, it's not really anything to do with your kind of practical sharding model.
00:09:59.920 - 00:10:34.220, Speaker A: It's more about how is the state represented in the network? Right? If there's an application that's storing some variables and storing some data, how is that represented in the network? How is it determined where that is stored? What kind of address space does that look like? Is it a fixed address space and all this kind of stuff? It's more about your state model as opposed to your actual sharding model. And that took quite a while to figure out, but then it was one of those, ah, that's how you do it. And it turns out it seems like it works pretty well.
00:10:36.200 - 00:11:56.820, Speaker B: So maybe just to re articulate for the audience, ultimately, as kind of networks grow, I mean, you need to do more computation or kind of more bandwidth. And your thought process was always, if we hit some level of scale, you'll need essentially machines that are too large to be bounded by one system, essentially requiring for sharding down the line, kind of modeling off what has happened in traditional kind of web two companies. But the previous sharding attempts that we have seen up until this point with kind of L2s or kind of even how near approaches sharding those type of sharding as reflected to the user, say an optimism shard or a polygon shard are known to the users which make that confusing not only from a user experience, but also as a developer experience. Radx's point of view is to enable sharding to get that further scalability down the line, but ultimately have it integrated such that it's at the software level where users and engineers, it's kind of they don't know.
00:11:58.520 - 00:13:51.658, Speaker A: To a naive observer, a user or a developer, all they just see is I send something in and then I get something out and I don't have to worry about where I'm sending it to or what it's doing. I don't have to be thinking about, okay, if my application fails, do I have to deal with rollbacks? Which is something else that we have in our sharding model that isn't anywhere else, which is like atomicity. Like strong atomicity, right? Either all of this stuff completes and executes and terminates correctly, or none of this stuff leaves any residual changes left on the ledger, right? And the reason that that's important is because if you don't have that atomicity, then your failure cases become a large overhead, right? And when you have a large overhead that maybe gets out of control, and an adversary may see that as a potential attack vector, okay, if I can make lots of transactions fail, then I can make the overheads of the network grow so that it doesn't scale as well as it does when everything is operating normally. Those overheads always kind of plateau your potential scale, right? So if you've got failures, or even if even in some scaling systems where scaling models where everything's going fine, the overheads eventually creep, right? The scaling solution isn't linear, it's sub linear. So eventually you hit this plateau. So another thing that very strongly drove our, our scaling model and how we're thinking about it was if it's going to be able to serve the planet and whatever else, then you really need it to be linear in scale to some degree, right? You add more compute, you add more resources. And if you've got double the amount of resources that you had before, then you can potentially scale twice as much as you could before.
00:13:51.658 - 00:14:40.100, Speaker A: Whether that's good transactions, bad transactions, whatever it is, you need to make sure that the overheads are linear with, in comparison to the computer you've got, which is something else that we have, too, which I don't think you see in any other scaling solutions. The overheads always seem to eventually plateau, which is one of the main reasons why we went through so many iterations, is because, okay, this isn't quite linear, right? It's 2% below linear. But when you're pushing billions of transactions across this thing, that 2% matters, or that 3% of overhead that you don't want starts to matter eventually because it accumulates. And then you hit a plateau and you get into diminishing returns, which is what we didn't want.
00:14:40.440 - 00:14:57.236, Speaker B: Yeah. So, to my knowledge, the only other two blockchains, at least today, that are pursuing an intra validator shorting where it is opaque to the users and really handled on the software side, are going to be solana and sweat.
00:14:57.338 - 00:14:57.888, Speaker A: How?
00:14:58.024 - 00:15:30.050, Speaker B: I guess maybe going through the core bottlenecks of blockchain architecture. I think you were talking a little bit about the consensus design and the overhead there. Maybe we'd love to just tap into starting with consensus, how does RadX ultimately approach that? How does it differ from some of the more integrated chains that are all to all or avalanche with probabilistic polling? What unique things has the RadX community done to enable linear scalability?
00:15:30.090 - 00:16:13.606, Speaker A: So there's actually two consensus mechanisms. So you have your traditional consensus within a set of validators, and that can technically be anything. It could be PBFT, it could be hot stuff, it could be proof of work based whatever. And then what sits above that is the second layer of consensus called Cerberus, which handles all the cross shard stuff. And so your local consensus just, okay, we got this transaction and we're going to process it. Do we all agree that we processed it within our particular group and we're happy with the result of what the consensus was? Yes, we are. No, we're not.
00:16:13.606 - 00:17:31.650, Speaker A: Whatever. And then the Cerberus layer on top that handles all of the cross shard consensus, but it does it in such a way where you get these ephemeral validator sets. So if we had to have a network of, say, 100 shards with 100 different validator sets, and there was a transaction that touched three of them, then the serverless consensus piece orchestrates an ephemeral validator set, which includes the validators from those three sets that this transaction touches, and that's completely independent consensus process to anything else they might be doing. So those validators in those three sets, they'll be included in many different ephemeral validator sets, processing all manner of different kinds of transactions. And the reason that you need the local is so that shard can agree on what was processed for safety and for liveness and stuff, and so that they can agree an order on which, okay, between us, we're in all these different validated sets that represent all of these different transactions. And some of these transactions are dependent on each other. Some of them are not the ones that are independent.
00:17:31.650 - 00:18:58.132, Speaker A: We don't really need a total order for them, but the ones that are dependent on each other, like if they are all touching the same smart contract, right, and that smart contract resides within one of those particular validator sets, then those validators have to come to an agreement on what order were we going to process these transactions in. That mutates this piece of state for this smart contract that we as a shard are responsible for, what's the order? We're going to process that. Then those three validator sets, they execute, they create some proofs of execution for correctness and all that kind of stuff. Then those validator sets exchange a compact representation of what they have executed in terms of that transaction, and they attach a vote as well aggregated signature of all the validators on who voted or who rejected. And if you have two thirds majority across all the validator sets that say, yeah, we executed this and all of the execution proofs, all the same hash, then the transaction gets accepted. If one of the validator sets doesn't have a majority, then it gets filled everywhere. Now it's atomic because all the three validator sets that are responsible for some piece of state within that transaction, they're kind of semi synchronized in terms of when they're doing the processing, right.
00:18:58.132 - 00:19:19.132, Speaker A: So it's like a multi phase. So they've multiplaying. The first phase is, are we already. Yeah, we're all ready to execute. Okay, let's swap. Any state that everybody needs for this execution. So I might be responsible for some piece of state, you're responsible for another piece of state, but so we can both execute so that we can check each other's execution output.
00:19:19.132 - 00:19:34.720, Speaker A: Because I don't want to just blindly trust you what you say. I want to execute it too. You need to send me the state inputs that I need. And I need to send you the state inputs that you need. And then when we have them, we can both execute. We can both vote on the execution. Was it good or bad? Then we exchange these proofs.
00:19:34.720 - 00:20:22.658, Speaker A: And you compare my proof to what you executed. I compare your proof to what I executed. And if they match, and we were in agreement that we can apply these state changes, then the next phase of consensus is then a commit. But before that commit happens, nothing's been changed on ledger, right? So if you don't hear from me or I don't hear from you, or your proof doesn't match mine, then I fail. You have to fail because you won't get a commit quorum signature from me to say that I've actually committed. So this transaction just gets aborted. And that gives you the atomicness across commits, across shards, which is very important, because if it's not atomic, if it's probabilistic in some way.
00:20:22.658 - 00:20:44.464, Speaker A: Right. Then there are edge cases where I can convince you that I've committed it, but I actually don't. And I go and do something else and it causes a safety break for me and for you. Or there's a rollback afterwards. It turns out, oh, no, I've got to roll this back. But you don't know that you've got to roll it back. So then I've got to communicate to you that you've got to roll it back too.
00:20:44.464 - 00:20:46.864, Speaker A: And you're like, why? And I've got to give you proof of why.
00:20:46.992 - 00:20:47.560, Speaker B: Yeah.
00:20:47.680 - 00:20:50.460, Speaker A: And then there are those overheads that we were just talking about. Right.
00:20:52.360 - 00:21:19.972, Speaker B: So what I'm hearing ultimately is you can kind of shard linearly in the sense of adding new shards increases more capacity. Each shard has its own consensus algorithm where it can locally kind of come to agreement. And then there's a separate consensus. Separate consensus algorithm across all shards to come kind of to agreement when there is a cross shard transition.
00:21:19.996 - 00:22:01.534, Speaker A: Yeah, but that kind of. That second consensus of Cerberus, that's not across all shards, that's just across shards where there's some state. So if me and you were involved in a transaction that was touching shards a and B, and then somebody else was involved in a transaction that touched shards T and s, we don't need to be in one of these service instances because they're independent. Right. They don't touch, they have no overlap. They have no dependency on each other. So those Cerberus consensus processes are, they just include who needs to be included for the sake of this particular transaction.
00:22:01.534 - 00:22:19.570, Speaker A: If your transaction is extremely complicated and it touches all the shards on the network, then, yeah, occasionally you will get a service consensus instance where everybody in the network is involved, whatever that is, that transaction is doing. But most of the time, you're going to be in terms, just small, small subsets of everybody.
00:22:21.310 - 00:22:41.398, Speaker B: In terms of. Traditionally, when I think of sharding, I think that potentially different shards could have different security properties. One shard has, say, 1000 nodes, another shard has ten nodes. How does RaDX ultimately ensure that each shard has similar security guarantees?
00:22:41.534 - 00:23:22.692, Speaker A: So there's a lot of. So the first place to probably start, actually is every piece of state in the network, whether that's an account balance or some state for a smart contract or whatever, has what's called a state address. And the state address space is fixed to 256 bits. So you've got 256 bits worth of possible state addresses, which is law, as you probably know. And the reason it's so many is obviously collisions, right? Reduce. Reduce the likelihood of collisions. Although in this case, a collision isn't necessarily bad.
00:23:22.692 - 00:23:53.148, Speaker A: It just adds a little bit of overhead. But the chances of getting a collision. Ridiculously small, anyway. But that state address space is fixed. So any piece of state that is created, there's a hash that represents that piece of state, or a hash identifier, at least called a state address. So you look at that hash, and that tells you where in this address space, that hash lives. Right.
00:23:53.148 - 00:24:42.400, Speaker A: Now, let's say we've got a really simple network where we only have two shards. So those two shards, one half of them, one shard represents as responsible for maintaining one half of the state address space, and the other shard is responsible for the other. So, given any state address at any moment in time, so long as I'm somewhat up to sync in terms of the last couple of epochs, I know who right now should be responsible for this piece of state. Which are the validators that are currently assigned to shard one, which are the validators that are currently assigned to shard two. I have a transaction with some state that's mapping to shard two. I know who the validators are that are responsible for that. So I know that I need to send it to at least one of those validators.
00:24:42.400 - 00:25:24.232, Speaker A: Maybe I send it to a couple just to be sure that it gets there and I haven't hit one that's just gone offline or something. Your usual kind of message reliability stuff that you would do now, if you scale it up and you say, okay, well, now there's ten shards because the network has grown. Then this state address that I now have, okay, it's in one of these possible ten shards because it's dynamically changed, right? There's more demand, so we need more shards for the through port. So everybody's shuffled around, and the number of shards has increased, etc. Etc. But it still maps to the same place in the state address space. So, okay, now, what used to be in shard two, but now it's in shard six, let's say.
00:25:24.232 - 00:25:31.140, Speaker A: And because I'm fairly up to sync, I know exactly the validators that are responsible for shard six, so I can send this transaction to them.
00:25:32.840 - 00:25:36.944, Speaker B: And I'm assuming, though, each shard has its own quorum.
00:25:36.992 - 00:25:39.500, Speaker A: Yeah, each shard has its own quorum. Yeah. Yeah, it does.
00:25:40.080 - 00:25:53.980, Speaker B: So how many, I mean, does, as the network scale, does the quorum size for each shard stay the same? Or is that dynamically adjusted? Could those be different?
00:25:56.340 - 00:26:37.810, Speaker A: There's an element of randomness in terms of what shard you're assigned to. Over a large enough sample set, you get small amounts of variation, but it's essentially random. So you get a fairly uniform distribution of number of validators in each shard group. Obviously you might have, but you will have situations where I. There's a lot of demand to be a validator in the network. So this additional computer is joining, but there's not yet enough demand to warrant doing a shard increase. We're at ten, and everybody's pretty happy at ten.
00:26:37.810 - 00:26:58.950, Speaker A: Nobody's signaling that they're constantly beyond their resource capabilities and stuff. So we're good with ten right now, but double the number of validators come in because know it's profitable for some reason, like fees or emissions or whatever it is. So those validators that join. Excuse me, I've had a bit of a flu over the past week. I'm at the end of my cough.
00:26:58.990 - 00:27:01.690, Speaker B: So that's good that we're no worries.
00:27:03.190 - 00:27:05.750, Speaker A: I've only coughed once, so that's definitely a good start.
00:27:05.870 - 00:27:06.930, Speaker B: You're doing good.
00:27:08.150 - 00:27:30.362, Speaker A: So, yeah, so you'll get these new validators that are coming in, but the network doesn't need to increase the number of shards yet. So let's say that initially there is 100 validators in each one of these. In each one of these shards, but you double. So now on average there's 200 in each one, right. That's fine because the local consensus can take care of that. And there's some baseline that the network will want to kind of maintain. Right.
00:27:30.362 - 00:28:11.480, Speaker A: Whether that's 100, 100, 5200, whatever, whatever the network decides is the baseline that it wants to maintain in these groups. But at some point then if the throughput demand increases to a point where some of these groups are signaling, hey, I'm getting a bit stressed on my resources here. I'm constantly 80% or 90% or something. And then you eventually get a quorum that is saying, yeah, I'm beginning to get overstressed here, I'm at 80% frequently. Then it might double the number of shard groups. And so those additional validators that came in, everybody then gets distributed fairly uniformly across now 20 shard groups instead of ten.
00:28:12.980 - 00:28:16.876, Speaker B: Does the network just decide that dynamically or how does that process?
00:28:17.028 - 00:29:02.600, Speaker A: So there is a protocol that decides when this happens. Now, you could modify it if you were an adversary to try and get the network to do things, but that protocol that signals these network topology changes is also kind of quorum driven. So as long as you're not breaching the usual fundamental bounds of consensus, two thirds majority is honest, then you can use that signaling mechanism as a means to grow or shrink the network as well. If demand has dropped off a lot or the number of validators has dropped in the network significantly, then you can also shrink the number of groups, too, so that you can get the per group validated count higher for security if needs be. So it can go both ways.
00:29:04.460 - 00:30:05.096, Speaker B: So, just to reiterate, to make sure that I'm understanding correctly, and for the audience with RADX, ultimately, I mean, in this example, there's ten shards. Each shard has a hundred nodes, and the throughput is not a limiting factor. So if more nodes want to come onto the network, you would essentially increase the security of each of the shards by going from 100 full nodes to or 100 validators to 200 validators within that ecosystem, or each shard. If throughput does become a concern, how the ECo, how RADX ultimately adjusts, that is, makes new shards with additional validator capacity that was on the network, and that can kind of happen dynamically to increase throughput over time, but that would go back down to slightly less security, or the original security guarantees with 100 full nodes for each.
00:30:05.128 - 00:31:14.152, Speaker A: Yeah, but what's interesting there as well, in terms of the security dynamic of this is interesting, too, right? If we had ten groups and we had 100 in each, and then there's 200 join, and so the security of the network has essentially doubled, even though the throughput hasn't yet. And then the throughput goes up, and so we split into 20, then the per group security is back down to its original. Right. But what's interesting is that almost everything in our network is a cross shard transaction. If you've got ten groups, then 90% plus of any transactions that happen are going to touch two groups from my address to your address, there is a probability that they'll be in the same group. But that's a one in ten probability. When the network then goes to 20 groups, then if me and you were just sending a simple transaction to each other, there's now a 95% chance that all of those kind of swap transactions are cross shard.
00:31:14.152 - 00:32:19.600, Speaker A: Right? So a greater percentage of transactions will be cross shot. So, in the previous configuration, where I sent you a transaction, and it happened to be in the same shard, and so we were taking advantage of that 200 validator security metric because the network topology has changed. There's now an increased chance that I'm going to send you a transaction now, and we aren't in the same shard anymore, right? So that will involve two shards, which we will still then benefit from 200 validators worth of security, because instead of it being in the same shard, and therefore the same validator set is responsible for both mine and your state. That's not the case anymore, because the network has grown. So in a lot of cases, even though the network grows and the per group security metric decreases, a much larger portion of transactions touch more groups. And so, therefore, the number of validators that actually take part in processing that transaction is greater than it otherwise would be, or at least the same as if it was a smaller group count.
00:32:21.140 - 00:32:49.460, Speaker B: So maybe an example of currently, how I understand it. Say my assets were on shard one, which has a hundred or ten validators. If all of those ten validators went offline, how does the security, I mean, as my understanding, wouldn't be able to access that shard if all those validators are offline until they come back on and restart the quorum. Is what you're saying different than that?
00:32:49.960 - 00:33:21.240, Speaker A: Yes, although that is a good question as well. What I'm saying is, let's imagine that me and you are just in the network. Someone is sending a transaction, and both pieces of that transaction you're responsible for. You don't need to talk to me about that transaction. You could just process it locally. You don't need a Cerberus instance with more than just you in there, because you're responsible for both bits of the form and the two. So you just do that.
00:33:21.240 - 00:34:02.698, Speaker A: But then the network topology, the sharding topology changes because there's more demand, right? So now the two pieces of that transaction touch my shard and your shard. Right now, in our shards, we both got ten or 100 validators each. So previously, let's say you had 200 in your group with you. That's now going down to 100, because the number of shards has increased. But that transaction now because the address space that you're responsible for has shrank, then one of those pieces of state aren't in that address space anymore. It's there, but you've shrunk to just be there. And I'm now covering this piece.
00:34:02.698 - 00:34:26.030, Speaker A: So me and you have to be involved in that transaction, which now means that two groups are involved. So you get more. You get more transactional security, let's say, because it's gone from having one group responsible where you might have had 100 or 200 or whatever, whereas now there's two groups which may have 100 each or 200 each. Right. So.
00:34:28.490 - 00:34:33.162, Speaker B: I understand the logic. I guess I'm confused. If just, like, my shard and all.
00:34:33.186 - 00:35:06.268, Speaker A: The validators went, that's a different problem, how would that say liveness and data availability problem? And, like, because if you've all gone offline, then as long as you come back, safety won't have been violated. Right. So if you, if you're all in the same data center for some, you know, by some probability, and that data center lost its power and then it came back up an hour later, nothing could have happened. Right. Because there was no quorum to make things happen. You have no liveness, but everybody will come back and all of their state will be the same. So you won't have a safety issue.
00:35:06.268 - 00:35:25.668, Speaker A: It's a liveness issue. If they go away and never come back, then it's still not necessarily a safety issue because nothing could have happened to change the safety of the data, but they're just not available. You start to then get a bit into the kind of realms of cap as well. It's not available. They're not petitioned. They're just not available. They're consistent.
00:35:25.668 - 00:35:30.200, Speaker A: They're not petitioned because they've all gone, but they're not available.
00:35:33.330 - 00:35:57.570, Speaker B: That makes sense in terms of maybe moving slightly around. When you go from one shard to n plus one shards, you have these cross shard communication, as we've been talking about on a higher level. How does that affect real world latencies from the user standpoint of that cross shard communication.
00:35:57.650 - 00:36:26.976, Speaker A: Right. So youre a. Oh, dear. There's a second one only five minutes after the last. I hope this is not going to get so because all of the groups that are involved with each other are kind of quasi synchronized. Right. And that synchronicity comes from the fact that I can't execute until I've got all of the inputs that I need from somewhere else, and you can't execute until you've got all of the inputs that you need from somewhere else, including from me.
00:36:26.976 - 00:37:03.332, Speaker A: Right. So provided that you have reliable message broadcast and provided that you have a, you know, above two thirds super majority of honest and not faulty actors. Right. So faulty is like crashing or, you know, issues and stuff, then we will all receive all the information that we need approximately within the same interval of time, like a second or second. Right. So all, so the transactions will execute across all of the validators that it needs to within a window of about a second or so. And so that helps when you're looking at.
00:37:03.332 - 00:38:51.020, Speaker A: Okay, so if we've got a very complicated transaction, how much more latent is that transaction to completion than a simple one, right? Because of the synchronicity and because of the mechanics of data broadcast and gossip and stuff, the jump in latency or finality going from one group to two groups, and then the jump going from two groups to three or four groups is much less. It's logarithmic in nature because of this kind of synchronicity and because we're using gossip, and gossip is also logarithmic in terms of its latency. I can reach, on every additional hop I can reach an ever greater number of recipients that receive this data, like gossip networks, as a definition of virality, right? So if you're just doing simple transactions and you only need two groups, it's primarily driven by what is the local consensus, what is the interval of each phase for the local consensus? And so on a sufficiently geo distributed network, you're looking at maybe between 100 validators, you're looking at maybe a second to a second and a half, right? I of this local consensus happening, you've got two of them for a simple transaction, right? So, and they might be out of step a little bit. They might not be completely in sync because those local consensus processes are independent. So if you're, if you're committing a proposal every second, then those two processes can be out of sync by a second, right. I could just have committed mine as you're just starting the phase of that was related to the one that I've just finished. So you can have a worst case latency there of about 2 seconds.
00:38:51.020 - 00:39:28.624, Speaker A: Then you've got the crash yard stuff that happens, which is minimal in terms of latency, right? Because that stuff happens based on the phase completion locally. So if I've just finished my phase and you're just starting yours, I don't care about that. I'm still going to send you what you need, you'll receive. Oh, I'm not quite ready for it yet. I'll just cache it for a second while I finish this phase. Right now I'm ready for it now I execute, and then you've got another round, which is the commit round, because I exchange stuff when I'm finished. Then you do a commit round, and again there can be perhaps a latency worth of consensus phase.
00:39:28.624 - 00:39:55.940, Speaker A: And because we're out of phase maybe another second or so. So worst case, 4 seconds. If you're very synchronized in terms of the phases, then you can be down at let's say 2 seconds for two groups. So 2 seconds finality to 4 seconds finality for a two shard group transaction, and that's complete. It's final atomic. There's no probability that is strictly safe. If you add another group in, then you have the same kind of dynamics.
00:39:55.940 - 00:40:37.090, Speaker A: But because the phase interactions that we were talking about and the additional communication and stuff, it doesn't actually add much more because there's only an upper bound of worst case that we can be out of phase by. And the more groups you have, the less that epsilon of error becomes. And so you can have, say ten groups. And the finality for all ten is only say seven and a half seconds or 8 seconds. So even though you've got five times more complex transaction, the finality is only say three times as long. And then you add more groups and more groups, and it just, it just logarithmically flattens out.
00:40:38.270 - 00:41:44.768, Speaker B: I see. So maybe if I could just pause here, because we've been nerding out for quite some time on the consensus design and some of the more minute details of how routex works, but maybe to just re articulate it from the beginning, just to summarize it for the audience, is really RADX was created because of ultimately these systems will hit their limitations. You need to do some form of adding additional resources to the network. RaDX is ultimately doing sharding where you can add additional shards to the network to increase overall network capacity. Each of those shards can communicate across kind of the ecosystem. There's two. Two consensus algorithms, one more locally, and one that ultimately can communicate for the relevant shards for your transactions as you do the cross shard communication.
00:41:44.768 - 00:41:57.806, Speaker B: There is some latency because there are physics in the world, but that is really upper bounded by logarithmic in terms of how much additional latency there is.
00:41:57.838 - 00:42:03.166, Speaker A: Going to be in that for a transaction of a particular complexity. Yeah, correct.
00:42:03.278 - 00:42:35.834, Speaker B: Okay, cool. Well, maybe, I know we've been nerding out that for a little bit, maybe moving on slightly. The conversation in the blockchain world today has really seemed to be centered around kind of single threaded virtual machines, particularly around the EVM, and now more so to parallelizable virtual machines to be able to enable additional throughput. Can you talk a little bit about the RaDX virtual machine and how you guys are kind of thinking about that problem space and kind of where you fall on the spectrum?
00:42:36.002 - 00:43:51.102, Speaker A: Yeah, we've been thinking parallel execution for five years, if not longer. The Cerberus piece of the consensus just parallel execution drops out of that. As we were saying, you have these multiple instances that are doing executions if those transactions aren't related to each other, and then you have a sequencer that determines the order for ones that are. And parallel execution obviously is extremely useful because if you have a particular set of transactions that have some dependencies, then you can just put them into a thread of their own and they can just rattle through. But if you then have a lot of other transactions that aren't codependent on each other, you can maximize the number of cores in the cpu, or you can maximize IO as well, even if you have a really responsive IO system on the machine that it's running on. So parallel execution is, in terms of its utility, a no brainer. You just want to always squeeze out the maximum amount of utility for any box of resources that you have, especially if you're doing validation and you're mining or you're getting stake rewards or that kind of stuff.
00:43:51.102 - 00:45:04.562, Speaker A: You want to, you want to maximize the hardware you have so you can make it as profitable as possible. But at the same time, there are a lot of complexities around parallel execution. It depends very much on the EVM or the VM that you're using plays a big part on one, is it possible even, two, how efficient is it to do it? And a lot of that efficiency obviously comes around to lock contentions and stuff. If you do end up having some transactions where, especially non concrete transactions, where some dependency wasn't known at execution time, but then the execution of that particular thing reveals that, oh, there's a dependency now, right? And that dependency means I've got to wait for that thing over there to finish because I can't make this parallel anymore. And just those kind of like edge cases which even if they're rare, they can cause a bit of havoc, right? So suddenly you get a few of them and suddenly your execution throughput goes from thousands a second to hundreds a second, right? And again, a nice sweet spot for an adversary if you want to come and play havoc with your network. Oh, if I do this, I can slow the execution from 1000 to 100. Okay, cool.
00:45:04.562 - 00:46:11.660, Speaker A: Let's just pile the network for these non concrete transactions or whatever the issue might be. So, finding efficient ways to make sure that you can run things in parallel and they stay parallel, especially as transactions get more complicated, you know, with swaps and token swaps here and touching this component over there and doing this thing, that becomes ever challenging. So it's an interesting area of research and it's an interesting area that we focused on a lot. And it's one of those things I think, where there will, there's always going to be room for improvement. There's always optimizations, there's always improvements to the efficiency and also the security as well of the execution. And then you've got your kind of your baseline execution security as well around. Okay, what actually is the virtual machine? How does that behave? Is it Turing complete? Is it object oriented? Is it asset oriented? What are the constraints that it puts into play? Some of those constraints may prevent certain parallel executions.
00:46:11.660 - 00:46:25.900, Speaker A: In some cases just, it just becomes a plethora of, oh, okay, right. What is the sweet spot here for what we want to be able to do versus how secure do we want it, how fast do we want it, etcetera.
00:46:26.800 - 00:46:39.112, Speaker B: So is today RadX, the virtual machine, single threaded and then kind of down the line exploring different ways to paralyze the VM locally within?
00:46:39.256 - 00:46:44.580, Speaker A: Yeah. So at the moment our network is single sharded essentially. Right?
00:46:46.360 - 00:46:47.976, Speaker B: So sequential processing.
00:46:48.088 - 00:47:52.932, Speaker A: Yeah, we kind of did things a little bit different to the way that everybody else seems. So we wanted to be mega scalable, right? So we spent ten years researching how to be scalable, and then when we got the answer for that, we said, okay, right now let's just put out a single sharded network. Because what's actually important before scale is developer experience, user experience, security. Right? How good is the wallet experience? How difficult is it for exploits to happen on this network so that users don't want drained every day and all that kind of stuff? There's no point having scalability if younger users. So even though we spent a lot of time on scale first, and then the focus on the devx, it's actually the devx, the user experience stuff that's gone out first, and then the scalability piece follows later on. So at the moment we don't have any need for parallel execution because we don't have both the user base and the network. And the single shard aspect of what is Babylon has an upper bound through port of maybe 100 tps at the moment, and we know any of that anyway, so we don't even need it.
00:47:52.996 - 00:48:03.924, Speaker B: Yeah, I mean, I fully appreciate that. Kind of in my past life doing product and engineering, you shouldn't optimize things that don't exist yet.
00:48:04.012 - 00:48:19.460, Speaker A: Right. It's like, yeah, I always think of it as like, okay, yeah, we've built this rocket that can to Mars, but sorry guys, we've kind of skipped on the life support a little bit and stuff. Okay. It's like, but it'll be fine. Don't worry. It's like, come on, you got to look after that stuff first.
00:48:20.800 - 00:49:18.248, Speaker B: So maybe I kind of shifting from the virtual machine slightly. One thing that continuously is highlighted to me when talking with a lot of researchers and builders and kind of a layer one, L2 ecosystem, is this idea of data availability. Whether, if I'm talking with L2, they have to post that data back to the layer one, or ultimately, if you're a high throughput layer one, you still bottlenecked by how much data you can really propagate through the system. What is kind of the RaDx goal or thinking around how to scale the data availability? I know we've talked in length about adding additional shards. Is it essentially just as we spoke about, if each shard, say, has 1 mb capacity and there's ten shards, then you have ten megabytes of data availability, and that can be scaled?
00:49:18.304 - 00:50:19.450, Speaker A: Basically, yeah. You just get this inherent piece of data availability, not the complete picture, but the bulk of it, just from being able to do that, that linear scale that just automatically gives you 80% of what you need. Then all you've really got to think about is, okay, so what about in certain edge cases, like you were saying earlier, what happens if that shard disappears? Then how do you retrieve the data for that? Well, as we were discussing earlier, a large portion of transactions, because of the way the state addresses work in the state, the state address space, a huge majority, like 90 95% plus of transactions will be cross shard. Right. So you'll actually have two copies of that transaction and what it did and what the state transactions were in the network, you'll have one on one of the shards and one on the other. So to lose that transaction in its entirety, you've got to at least lose two shards. Right.
00:50:19.450 - 00:50:55.754, Speaker A: And the probability of that happening obviously reduces significantly. But then there's another factor as well. In the serverless piece, where every transaction also includes, in that ephemeral Cerberus consensus instance, some random validators from around the network that aren't in that shot. It's just, okay, you're randomly selected to just witness that this transaction happens. Then every transaction ends up in three or four or more shots. And it's very cheap for those witnesses because they're not actually doing consensus. They're just waiting for a commit event to happen.
00:50:55.754 - 00:51:18.440, Speaker A: And, okay, I'll copy all those quorum certificates, and I'll just keep them here for a period of time. And so you actually end up with many copies of that transaction across the network. It's all randomized because of the witnesses. So in order for your shard to be completely lost, a lot of stuff's got to go wrong, right? And if it's going that wrong, you've probably got bigger issues to worry about at that moment in time.
00:51:19.820 - 00:51:31.364, Speaker B: Are there any issues with kind of scaling a single shard to, say, 1gb or ten gigabyte in terms of data.
00:51:31.412 - 00:51:53.020, Speaker A: Availability and ingestion and stuff like that? I mean, yeah, just like it's the same with, with anything in this kind of system, right? You can go as fast as your slowest majority. So if the slowest majority can do 1, then no matter how fast the machines are, they're going to be doing 1 as well.
00:51:53.440 - 00:52:02.020, Speaker B: That's true, I guess. Does the network have any minimum specs or recommendations on upload download?
00:52:03.320 - 00:52:41.180, Speaker A: When I'm doing my testing at the moment, which I'm not sure if you've seen any of the stuff I've been doing recently, which is quite interesting, I'm doing all my testing against a baseline spec of four core eight gigabyte with a SATA SSD, and they are sharing a 1gb pipe between eight of them. So they've got quite a lot of bandwidth. But that bandwidth is, if I was to max the bandwidth, the satire would get Max bottlenecked before the bandwidth did. Right.
00:52:43.680 - 00:52:53.288, Speaker B: And this maybe leads me to my next question on how you do storage. You can ultimately paralyze storage or state access with reads and writes.
00:52:53.344 - 00:53:38.760, Speaker A: Yeah, I'm not doing any of that stuff. It's just like, okay, what is the lowest spec commodity hardware that I just want to baseline on? And like a four core, eight gig SATA SSD is like commodity from five years ago, right? And recently on a network of those spec machines, I'm actually going to do another one of these tests. Hopefully today, if I get the time, sharded 16 shards was peaking out at 10,000 swaps a second. So this is, this is swaps. This isn't like just your regular TPS that everybody goes, oh yeah, we can do a million of them. Well, of course they're not doing anything right. These are like pool swaps.
00:53:38.760 - 00:54:28.260, Speaker A: So there's a pool contract, multiple pool contracts, and we're doing, you know, I want swapping token a, token b, and it's a liquidity pool and everything, 10,000 of those a second across the network. And I'm hoping to hit about 15 to 20,000 after some optimizations and stuff we've been working on. So. And that's all powered by the baseline four core eight gig specs. But we've done some testing in the past with the community and one guy joined with a raspberry PI, and he was also able to take part and stay pace the network for a good portion of time as well until we got beyond about 200 transactions a second in the particular group he was in. He was able to participate quite happily with the raspberry PI, of all things.
00:54:30.120 - 00:54:43.520, Speaker B: Very cool. So I mean, was the really the impetus for sharding in your mind really to keep those resource requirements kind of arbitrary low so that more validators could participate in?
00:54:43.600 - 00:55:02.690, Speaker A: Yeah, that's the motivation. Yeah. So that there's no friction to entry. Right. Because if you've got friction to entry, then in my opinion that increases centralization. There are arguments that say that it increases decentralization, but I don't believe that those arguments really carry any strength. Right.
00:55:02.690 - 00:55:26.012, Speaker A: If the barrier to entry is very low, then decentralization should be a byproduct of that. Right. Especially if there is a some incentives. And the incentives then can be lower too. Oh, I've just got this raspberry PI that's cost me $30 and I can join this network and I can participate. I'll get some rewards, leave it running in the corner for weeks. Yeah, brilliant.
00:55:26.012 - 00:55:43.610, Speaker A: I'll do that, no problem. So, yeah, keeping the, keeping the spec as low as possible should hopefully promote decentralization and larger, or at least larger numbers of validators per group makes a.
00:55:43.610 - 00:56:45.344, Speaker B: Lot of sense in terms of the current narrative. I feel like obviously ETH has really been their predominant smart contract platform. Now I feel like the horizon is kind of expanding to the newer blockchains, high throughput blockchains that really Solana pioneered, but also swe aptos, say Monad. And I haven't as much, just from my opinion, have had heard RaDX kind of in that conversation. As a vc and kind of many other VC's watching this podcast, what would your kind of, what would you want them to take away from listening to this podcast on why they should either, either investors, engineers, those builders, what is your message to them? To really look at the RaDX ecosystem and say, hey, we are actually a serious competitor, you should take a look at what we're doing?
00:56:45.392 - 00:57:29.010, Speaker A: Yeah. I would say that we've spent a long time doing things right from our perspective. A lot of thinking, a lot of time and effort thinking about what is the right way to do this. And so I think it, if something is done in the right way, then from a user and developer point of view, it's a nice surprise. And you will see that if you look around the writers community as well. There's a lot of developers that really love the script or language, which is an extension of rust and the whole developer experience. There's so much positivity around how easy it is to just pick up and build and build safely as well.
00:57:29.010 - 00:58:26.012, Speaker A: The chances of issue are hugely mitigated and smart contracts. We've only had script alive on our platform just over three months now, even though we had a lot of betas. We had three betas, I think, and an alpha when we were building this thing. But we involved the developers a lot in the process of. Okay, what do you want a smart contract platform to look like? Surveyed a lot of developers. What's a big bugbear that you have with the EVM or any other kind of smart contract platforms that you're using? So we had got a lot of feedback and we took the time to build that feedback and build solutions for that feedback into the platform as well. Obviously while we're doing that, we don't really want to go and hype it too much because we're still building, but now that it's out in the wild, we had a good run of initial TVL as well.
00:58:26.012 - 00:59:08.360, Speaker A: I think our TVL is currently around 25 million after three months, which isn't too bad. And we just really go after the organic growth. We don't want a mob, a flash mob to turn up and then they all disappear and it's like, where did everybody go? We want to build slowly an organic, strong developer community. Mobs are great for bull markets, but when the bear market comes, they've all gone. Obviously you want to. Obviously this is a long term vision. I've been in this game for ten years already, over ten years now, and so slow and steady wins the race in my opinion.
00:59:10.660 - 00:59:50.720, Speaker B: Makes sense. Yeah. I'm fascinated just by how these different ecosystems work, obviously. I really love this conversation, learning the inner works of RaDX, how these different systems come together, how they make the different kind of trade offs from either a hardware perspective, a software perspective, how developers have to learn how to adapt to that ecosystem and also to lead the users. And I think for me it's just been a fascinating journey to kind of compare and contrast these different ecosystems, to see what may or may not work long term. So it's been a long journey.
00:59:50.760 - 01:00:25.566, Speaker A: Yeah. I think that the biggest thing for me over this journey has been no compromise. So obviously you have to accept some trade offs somewhere and you can't strive for perfection because it doesn't exist. But just no compromise. Don't compromise where you don't have to. If intuition is saying that can be done better, then have a look at it and try and do it better. And then you end up with, in my opinion, the, the strongest tech stack, especially for developers and for overall user experience.
01:00:25.566 - 01:00:35.210, Speaker A: And there's still quite a lot of on the milestone list to come as well over the next few months. And it'll just get more refined, easier to build on, easier for users to use.
01:00:37.950 - 01:01:13.160, Speaker B: Yeah. And I think to that, if I maybe had to summarize my thoughts on RADX is ultimately you get linear scaling, which ultimately is the holy grail of all these ecosystem, is to achieve linear scaling as you increase more resources to the network. But Radek uniquely allows with sharding these lower resource nodes to participate in the network such that really any hardware, so to speak, can be added to that network quorum and continue to increase capacity over time.
01:01:13.240 - 01:01:30.700, Speaker A: Yeah, I mean, you know, eventually, who knows, everybody might be running a radix node in their smart tv, right? I mean, imagine, imagine that from a Nakamoto coefficient of decentralization, everyone's tv is running a small radix instance. I mean, that'd be great.
01:01:31.160 - 01:01:53.870, Speaker B: Well, Dan, we've been nerding out for about an hour. We've gone through how RaDX does like their consensus design, how you guys ultimately do touch on virtual machines, how you guys do data propagation and ultimately storage. Is there anything that you feel like we didn't touch upon as we're kind of wrapping up?
01:01:56.170 - 01:02:17.990, Speaker A: No, I mean, I think we've covered all the main points, right? It's like consensus scaling, developer experience, user experience, and those things really are supposed to kind of four pillars of what we are focusing on anyway is, you know, those things. So, yeah, I think we've, I think we've done pretty good for a first introduction.
01:02:19.690 - 01:02:29.026, Speaker B: Likewise. Well, we'll wrap it up there and really appreciate you coming on the podcast. I appreciate you nerding out with me, and it was a lot of fun.
01:02:29.098 - 01:02:30.510, Speaker A: Hopefully we'll do another one soon.
01:02:31.810 - 01:02:33.210, Speaker B: Of course. Thank you again.
