00:00:00.240 - 00:00:14.438, Speaker A: Thank you so much, Sam. Extremely excited to chat. I've been looking forward to it for a long time and learn more about Swe and what you've been doing here at Missin Labs and all the tech and secret sauce that makes it work.
00:00:14.614 - 00:00:18.382, Speaker B: Logan, thanks for having me. I'm excited to meet you and excited to be on and tell you all about it.
00:00:18.526 - 00:00:56.640, Speaker A: Awesome. Cool. I think maybe I just got done with an awesome chat with some of the other founders and kind of did a high level overview of the general Sui and Missin and the overall ecosystem. But I would love to just to dive into the tech. So maybe from, I think we both kind of share the philosophy of breaking things down from first principles. So starting or jumping off that, what do you feel like are some of the biggest bottlenecks of just like blockchains in general today?
00:00:57.180 - 00:01:22.314, Speaker B: Yeah, I think that's a great question and a great starting point. So really we look at there being three different places where there are bottlenecks. So I'm going to describe the classical blockchain architecture, which sort of is sort of painting other stuff out there with a very broad brush. Describe some blockchains but not others. But I think it's sort of the classical design. I think more or less will give a good overview of where the bottlenecks are and where Swe is doing things differently. So the first one is, of course, consensus.
00:01:22.314 - 00:01:46.532, Speaker B: So you have transactions that are coming in. Sometimes they're related to each other, sometimes they're not. But in the classical architecture, you take all these transactions, you put them through full byzantine fault tolerant consensus, and you establish a total ordering on those transactions. So that's thing number one. We know a lot of workloads don't actually require full consensus, but, you know, we order them anyway. And this makes sense. Like this is how the classic state replication, like state machine replication paradigm works.
00:01:46.532 - 00:02:00.004, Speaker B: So of course people do it this way. But, you know, it's not necessarily the most, it's the most straightforward architecture. It's the easiest to reason about the safety of, but it's not necessarily the fastest. Okay, then the transactions are done being ordered. Now you execute them. Usually this is also done sequentially. Again, doesn't have to be.
00:02:00.004 - 00:02:32.996, Speaker B: Some folks do parallel execution, either optimistic or other forms, but like classically, this is what you do. And definitely the result you need to have is that the execution result needs to be equivalent to the sequential ordering. Even if you optimize by achieving and doing, doing a parallel, when you can tell that it won't make a difference. And then the third thing is there's a blockchain. You don't just want to execute transactions, you want people to be able to read the results and read them in a trusted way. So usually you take the result of executing transactions. And in a classic account based blockchain, it's maybe a merkle tree where accounts are at the leaves.
00:02:32.996 - 00:03:02.824, Speaker B: You apply these rights to the leaves and then you bubble them up and compute a new Merkle state route. And this will bottleneck, because as the state gets bigger, there are more and more leaves, there are more intermediate nodes, and you want this merkle tree to be on a single machine usually, or it's not going to be fast. And so this too is a bottleneck. So I think these are the three areas where in the conventional blockchain architecture, where you really have points of contention, you have operations where you need to look at the whole global state, or you have everything in memory, or you need to have an operation that's just fundamentally sequential.
00:03:02.992 - 00:03:18.396, Speaker A: Awesome. Let's just keep rolling from there and break those three apart. Starting with consensus. How is Swe doing consensus differently from the traditional blockchain model?
00:03:18.568 - 00:03:56.964, Speaker B: So with SWE, we take a step back and say, okay, fundamentally transactions are doing computation. And different kinds of computations have different characteristics. Some computations need ordering. Like if you and I are both hitting a Dex or participating in an auction, then we need to know whether your bid came first or mine, or who gets what price on the trade. But there are other kinds of computations that are commutative. If I'm transferring money to you, and then someone else is transferring money to someone else, there's no reason that those transactions need to be ordered in sweep. We have a way to recognize whether a transaction is going to commute with all other transactions that are sent by it, that don't touch the same objects as that transaction.
00:03:56.964 - 00:04:45.448, Speaker B: Taking a step back and to talk about how that's possible. SWE's data model is different from other blockchains in that it's not account based, it's object based. The structure of the global storage is there's a map from object ids to objects, and every object has ownership metadata embedded in it. An object can say I'm owned by this address, or I can say I'm shared. And if it's owned by an address only a transaction signed by the public key associated with that, the private key associated with that address can actually use that object in a transaction. So if we look at a, and then when you have a transaction it says, I'm going to use this object id and that object id, and I'm using it in the shared mode or the single owner mode. And then there's authentication logic that checks whether this, this signer actually has permission to use this object and then it goes off and executes the transaction.
00:04:45.448 - 00:05:20.234, Speaker B: And so what this lets us do is when we look at a transaction, we can see is it only using single owner objects? And if it is, that means it actually doesn't need consensus. Because the only one who can send other transactions is the person who signed this transaction. And then this transaction is going to commute with any transactions that don't touch that same set of objects. And so we can skip the ordering step. And we use this systems primitive, called byzantine consistent broadcast, where we basically just check that this other person isn't equivocating or trying to send another transaction that's doing different things with the same set of objects. And then we can execute and commit that transaction much faster than going through full consensus.
00:05:20.322 - 00:05:23.466, Speaker A: And does that still have to go through the two thirds plus one vote.
00:05:23.498 - 00:05:51.434, Speaker B: Or it still has to have two thirds plus one votes, but it doesn't have to be ordered. So the way it works operationally is I have my transaction, I send it, I sign it, I send it to two f one validators. They sign it. I get back their results on the signed transaction. When I send it to them and they sign it, they have an internal lock database where they say, I've seen this transaction from Logan with this hash. And then they set that lock. And if they see another transaction that's trying to touch the same objects, they're going to say, hey, I'm not going to sign that.
00:05:51.434 - 00:06:10.772, Speaker B: I've already seen this transaction from Logan that's trying to touch his cryptokitty or whatever. Then the user gets back these signatures. It aggregates them into an artifact we call a certificate. This is two f one signatures, or two f one by stake signatures on the transaction. And then it resubmits the certificate. And that's the point at which execution actually happens and you get the results.
00:06:10.956 - 00:06:32.846, Speaker A: Gotcha. Is there in. The idea is that by not ordering, you reduce the latency by not having to order that specific transactions. Because it's not contentious for, uh, something that, um, it's not a contentious piece of state, you can just execute it.
00:06:32.958 - 00:06:54.426, Speaker B: That's exactly right for Bizdean fault tolerant consensus. Like in the worst case, you're going to have, you're going to have a cost that's basically quadratic in the number of validators. But here everything's linear. You can broadcast to all the validators in parallel. They can get back to you the they sign and get back to you in parallel. Then you just transmit the certificate, they receive those and process them in parallel. So you don't have this quadratic step.
00:06:54.426 - 00:07:03.610, Speaker B: And it's basically as long as you don't run out of sockets, it's constant, and then it's a constant cost because you can just send all validators at the same time.
00:07:03.690 - 00:07:08.710, Speaker A: And how are you able to do all that parallelly? Is it the object model?
00:07:09.250 - 00:07:29.006, Speaker B: Yeah, so it's the object model. So there's two things that are parallel from the user perspective. It's that you can submit to all the valid. It's that you can submit all the validators at once because, right, you have multiple sockets on your machine. And then for the execution, because, you know, these things fundamentally commute. If the validator is sitting on a pile of 50 single owner transactions, they don't have to worry about ordering them. They say, hey, I know all these commute, I can just throw them to separate cores.
00:07:29.006 - 00:07:30.478, Speaker B: I can do whatever parallelism algorithm I want.
00:07:30.494 - 00:07:45.974, Speaker A: It just makes work very cool. I think that's super unique, the consensus part and not having to do the ordering on everything. Yeah, it's very clever.
00:07:46.142 - 00:08:27.790, Speaker B: Yeah. So this comes from my co founder George and some other colleagues at Facebook who worked on the system called Fastpay, wherever they were looking at. What if we just wanted to do sort of payments only? And we want to do this in the most scalable way possible, and we want it to be horizontally scalable. We throw more machines at the problem, we get more throughput, and we're going to very much overfit the payments because they're a special kind of transaction we can make commutative. So what we did with SWIA is we took the model of that work, which had this sort of byzantine, consistent broadcast structure of send out the transactions, get the signatures, collect their certificates, and then we figured out how to generalize it to any kind of move code that commutes instead of just payments. So this idea, credit for this idea goes to George, Alberto, and Mattu, who are all the stuff at Facebook.
00:08:28.090 - 00:08:48.162, Speaker A: That's awesome. Maybe we can come back to the consensus. I do ultimately want to touch upon the DAG versus the more traditional blockchain architecture. Maybe we can just go into that now and then we'll hit the execution environment.
00:08:48.226 - 00:09:16.694, Speaker B: X. Yeah, I talked about the consensus. When we skip consensus, I do want to talk about improvements in the actual consensus algorithm, too. And then that'll inevitably us into the dag bit, too, because I think the execution is where the dag comes in. So then in consensus itself, we use this narwhal tusk thing. And then, so the inside of that, I'm not a consensus expert, so I'm going to do my best to channel the key parts, but you should definitely talk to George or Alberto or one of our other pros. So, in consensus, there are basically two things that validators are doing.
00:09:16.694 - 00:09:56.420, Speaker B: One is there are two problems they're solving. One is a data availability problem of getting everyone on the same page of what are the transactions that we're trying to agree on the order of. And then the other one is actually ordering them. And in conventional consensus algorithms, these things go together like you're doing this sort of n squared thing and you're solving both, you're both disseminating the transactions, the first problem and doing the ordering at the same time. But the insight of normal tusk is that, well, these are separate problems, and you can do them separately. You can do the dissemination, which can be done, which is cheap and can be done in parallel. And then you can get everyone on the same page of, okay, what transaction are you going to be agreeing on? And then you do the ordering, not on the transactions themselves, but on sort of fixed size references into a table of transactions that, you know, everyone has.
00:09:56.420 - 00:10:25.246, Speaker B: And so this is how normal test does it. It has this phase of disseminating the transactions, getting everyone to agree on them. And then it does order, and then it does the ordering, the very expensive part, the ordering just on these fixed size references. And then, so it splits this big n squared problem into a smaller o of n problem, and then the n squared problem that's there fundamentally. So that ends up performing a lot better than conventional consensus algorithms. So there too, the way we like to think of it is we skip consensus when we. So we don't need it.
00:10:25.246 - 00:10:30.690, Speaker B: And then when you need ordering, you use normal test, which is going to be better than conventional consensus algorithms in a number of ways.
00:10:31.310 - 00:10:37.210, Speaker A: So if you're skipping the traditional ordering, it's linear or constant.
00:10:37.990 - 00:11:00.142, Speaker B: So it's sort of like an engineering thing. It's linear in the sense that you have to submit your transaction to two f one validators by stake. So that cost is going to go up. But because that can be done in parallel, it's essentially constant. In the fastpay paper, you look at how the latency changes when you add more validators and it doesn't change. And of course, throughput always goes down when you add more validators, but then you add more machines within the validator, it can go back up.
00:11:00.246 - 00:11:27.912, Speaker A: Interesting. And then maybe can we break apart narwhal and tusk a little bit more? Because I do think they're relatively new and I think people have a lot of questions. So maybe start like the data dissemination and decoupling from the ordering mechanism. I mean, I think you kind of just explained it, but maybe just going a little bit further on that.
00:11:28.016 - 00:12:17.772, Speaker B: Yeah, so, I mean, the reason that we say narwhal and Tusk is that narwhal is the dissemination algorithm, and that part is sort of the same word I review. And then Tusk is a consensus algorithm that sits on top of it. But you don't necessarily need to use tusks with narwal. You can also use hot stuff, and they do that on the paper or in sweet. We're actually using bull shark, which is a successor of Tusk. Basically, there are many different ways you can decide on the ordering, and it just sort of depends on these trade offs that you think about in conventional consensus algorithms with, do I care more about having good average case behavior versus good worst case behavior? Do I have primitives like a common coin to rely on to choose the leader? Or am I okay with having a deterministic leader election algorithm and the sort of DDoS attack vectors that come with that? It's a framework that narwhal is always there, but then what consensus algorithm you put on top of it, it's parametric in that, and you can make many different choices. And we're choosing Bull Shark for now.
00:12:17.836 - 00:12:50.572, Speaker A: Gotcha. Maybe to back up a little bit, I was at the Stanford conference yesterday. Yeah, yesterday feels like forever ago. But they were talking about how important transaction ordering really is and kind of going through the different processes of what that affects taking that step back. Why is transaction ordering so important? And then how maybe going in after that, how like, Bull shark is ultimately a very effective way to do so.
00:12:50.756 - 00:13:26.520, Speaker B: Yeah, so, I mean, I think it's important for a number of different reasons. I mean, first is that just like many use cases require, like we talked before about the Dex and the auction or other things like, you know, many, many important kinds of computations, especially like financial stuff that goes on blockchains. Like you're going to need ordering. And then the other thing is that who gets to decide the ordering gives, depending on how the design works. And almost all designs we know of gives the chooser a lot of power to extract MEv, to do things that are unfair. And also to ordering also gives you the idea, potentially, it gives you the ability to censor transactions or delay them or do this other thing. So you have a lot of power in that.
00:13:26.520 - 00:13:55.546, Speaker B: We're concerned mostly with trying to. We're concerned mostly with the efficiency points. I think we're not trying to break new ground in terms of some of the mev prevention you see elsewhere by ordering encrypted transactions and then decrypting them after the fact. Although this is the sort of thing we may consider in the future. They're trying to get off the ground with solving the three pull problem, and then we'll see about these things later. But I think this is why ordering is important, because it's the most core part of one of the most core things that a blockchain does.
00:13:55.618 - 00:13:58.390, Speaker A: Yeah. And then bull shark.
00:13:58.690 - 00:14:27.788, Speaker B: Yeah, bull shark. So with Tusk, it works well. The leader election tusk works well when you have this common coin, which is like a trusted source of randomization. This is a tricky cryptographic problem that Costa sue, you'll talk to next, and some other folks are having, or working on solving and baking into sweat. But we don't want to gate safe leader election on that. We want to be able to do something else in the meantime. Again, I'm not a consensus expert, so I'm going to butcher the explanation of what goes well here.
00:14:27.788 - 00:14:44.798, Speaker B: But my understanding is that Bull Shark has a safer and more performant leader election scheme than Tusda's without requiring this common coin component. And so it may be something where we switch in the future. Once we have that primitive work with, or we stick with Bull Shark, this is just what we're doing for now.
00:14:44.894 - 00:14:59.686, Speaker A: Interesting. Very cool. And maybe kind of jumping off all this, how does sue and the dag kind of operate versus a more traditional blockchain architecture? And what benefits does the dag bring?
00:14:59.838 - 00:15:33.582, Speaker B: Yeah, so one of the benefits, when we talk about a dag, we're talking about, you can have a linear order on transactions, which is what you conventionally do. Or if transactions explicitly specify their dependencies as they do in sweat, you don't actually ever need to linearize anything. You have a partial order transactions, which is equivalent to a dag. Transactions have edges between them if there's a dependency. But then transactions can also be incomparable in their ordering. So this lets you do a couple of different things. So one is to choose different consensus, like fast skipping consensus or going to full consensus as we talked about.
00:15:33.582 - 00:16:05.682, Speaker B: And then of course in execution this matters a lot too, because if you know what transactions can potentially conflict, that helps you a lot with scheduling. If you see transactions are touching the same object, you throw them on the same thread or to the same worker, and otherwise you can separate them. So it definitely comes into play there. But there are lots of other areas as well. In a wallet, for example, this may seem like it's going off base, but it's interesting. As a user, you get a request from an app to sign, and you don't really know what that request is asking. Usually it's some opaque sequence of bytes.
00:16:05.682 - 00:16:55.160, Speaker B: Maybe you trust the source, it came from Uniswap, it came from my discord. But the best you can do is either look at the bytes, or maybe you try to simulate the transaction, but that requires fetching state when you explicitly specify your dependencies. And then also you have these mutability permissions that move has, which maybe we can get into later. You can interpret these as transaction permissions, like iOS or Android permissions, where this transaction is asking for permission to read your balance and transfer your board, ape and write some other thing that you have. And this is something that even an ordinary end user can understand. It's very privileged to ask to transfer something, but it's less privileged to ask to write it, and less privileged still to ask to read it. When you have the DAG structure or the dependency structure explicit in the transaction format, that lets you have safer wallets by encoding these transaction permissions.
00:16:55.160 - 00:17:53.868, Speaker B: One other area where it makes a difference is a problem with high throughput chains is that, or at least high throughput chains in the style of say, Swe and Solana, is that if you want to scale up your throughput by adding more machine resources, it's going to become more expensive to operate a validator, and then if you want full nodes to keep up, it's also going to become more expensive to upgrade a full node. You have to be careful here that you're not creating an oligarchy of it costs millions of dollars to run these things, and so the only people who can afford to audit the state of the system are people can also afford to pay millions of dollars. But if you have this dag, if you have a Dag structure and you have explicit input, and you have explicit dependencies between your transactions, then you can do this thing that's like a full node, but called a sparse node, where instead of tracking all the security, yeah, you have some set of addresses or some set of objects that you care about. And basically you track and re execute those transactions and track that state, but not everything. And so your wallet could be a sparse node for all the coins you have balances in.
00:17:53.884 - 00:17:57.520, Speaker A: And your nft you can run on that type of light hardware.
00:17:57.830 - 00:18:18.970, Speaker B: Yeah, because you're just trying. Running a transaction is super, super cheap once you have the inputs, and then you can query the inputs and run stuff. It just gets expensive when you're running the traffic of the entire financial world. But your wallet already has to store all of the balances, all of the balances you own and all the nfts you have. So it's just running a couple milliseconds of computation. You could do this on a phone.
00:18:19.790 - 00:18:21.810, Speaker A: And are there bandwidth requirements for that?
00:18:22.990 - 00:18:44.306, Speaker B: It's proportional to however many transactions are touching the state that you care about. So if you're a wallet, like you're sending these transactions transactions, and then, so it's just about when someone sends you, say, a new coin, you might want to replay some of the transactions in the history of that coin to see where it's been and made sure it looks okay. Or if you're a game server, you might want to make sure the validators are executing the logic of your game correctly. All of this kind of stuff.
00:18:44.418 - 00:18:52.550, Speaker A: Interesting. That's super cool. And that sparse node is uniquely enabled by.
00:18:54.850 - 00:19:09.850, Speaker B: The dagan, by the combination of the dag and then explicit transaction dependencies, because you need to be able to look at a transaction and see, is this relevant to what I care about without executing it? Because if you have to execute it well, now you execute everything. And then also to be able to identify the dependency structure.
00:19:10.350 - 00:19:19.410, Speaker A: All dependencies in SwE are explicitly stated upfront to help the parallelization or the overall throughput of the network.
00:19:19.910 - 00:20:06.480, Speaker B: Yeah, to help, I mean, basically stated up front for all the benefits we mentioned, it helps with that, helps with deciding which consensus path to go through, it helps with parallel execution, it helps with safer wallets. They sort of all go together and all is sort of a tricky thing. This is definitely where you hear that, and it sounds like, oh, this is going to be, this is going to be very annoying from the developer experience perspective, I have to say, I have to give the runtime hints or something. And so we're very careful to set this up in a way that you specify the minimum amount of things you'd have to convey the intention of your transaction. You specify the name of the smart contract you're calling and the id of the NFT you're transferring but nothing else. And the runtime is able to infer the other stuff that you're going to use instead of asking you to specify that.
00:20:06.940 - 00:21:11.680, Speaker A: Neat. Very cool. I maybe touched upon a lot on the consensus maybe, and we've started, I would say to touch upon the execution environment, but maybe go dive deeper into that. I think going forward it's been interesting just watching the progression of blockchains starting as single threaded and those virtual machines. I'm not personally super optimistic on those. I think it's very hard to fight physics, and I think if you have parallel processing, ultimately you can take advantage of modern compute and all the benefits that come with that. So I would love to kind of learn more about like the architecture design of your parallel execution environment and how that is kind of taking advantage of the more modern compute.
00:21:12.230 - 00:21:45.070, Speaker B: Yeah, absolutely. We touched on a little bit. But basically the thing is when you have static dependency information, that just helps you with scheduling a lot because you can basically define it. You see upfront what things conflict and you can set up workers. Workers can be threads, workers can be other machines, and then you send the transactions off to the appropriate place, and then they can be executed in parallel, and then you take the results and apply them. So that's all pretty easy when it's on the same machine and when you have one disk. But for us, one of the big challenges that we think about is, okay, if you really want to boost throughput, you can't just add more cores to infinity.
00:21:45.070 - 00:21:58.850, Speaker B: Those have costs. But how do we actually deal with workers that are multiple machines and set it up in a way that the transaction, like the data is sharded, the computation is sharded, but we're minimizing cross shard reads and writes.
00:21:59.950 - 00:22:04.130, Speaker A: So are you sharding in the execution environment?
00:22:04.590 - 00:22:46.810, Speaker B: So we plan to, and it's in the sort of theoretical setup we plan to launch with a single node architecture. Just because the amount of throughput we can squeeze out of a single box is still quite good, we think more than enough for what we're going to have at launch. We think in the long term the demand for block space, especially as the adoption of crypto improves, is only going to increase. And so you have to have an architecture that doesn't just get bound to okay, this works really well in a single box that has a lot of cores. But now when my disk fills up or I need more cores, I can't push any more throughput, or I need to add sharding the protocol level we really want a validator to be say, hey, I add one more box in my hosting environment and now I just get more throughput and then the marginal cost of doing that is low and then I can just keep doing that.
00:22:46.890 - 00:22:51.950, Speaker A: Would that be application specific or just more network wide?
00:22:53.290 - 00:23:32.490, Speaker B: More network wide. We really want this to be, we call this intra validator sharding. It's an implementation detail of how a validator is set up and not, that's opaque to other users and to other validators. We really think that's important for developer experience, comparing to putting sharding at the protocol level, where now you have both developers and users worrying about things like cross shard transactions, seeing the price difference, seeing the latency difference already blockchain programming is very, very challenging. And we think the experience where you have this illusion of all the assets in the world are in one place and they're just a function call away from you and you have an atomic lock on all of them when you're executing a transaction. That's what smart contracts are about. That's what we know is valuable.
00:23:32.490 - 00:23:49.600, Speaker B: These other things may be valuable too, but we really want to preserve that and we think we have the architecture do in the web two way. I have a sharded store and I have some transaction that needs to write to a bunch of different machines. This is a hard problem, but it's also a problem you can hire folks from Amazon and Facebook and Google to solve.
00:23:50.100 - 00:24:00.200, Speaker A: So how do you solve that problem with the latency and the cross shard communication with entra shard?
00:24:00.620 - 00:24:41.842, Speaker B: Yeah, that's one of the challenges here where if it's on a single box, there's no latency for communications for other machines in the validator cluster. Soon as you have multiple machines in the cluster than there is. So you have to be very smart about knowing which transactions are likely to touch the same data and try to set it up so that in the common case it's all on the same machine and so you can send a transaction there and it acts the same as if you had a single node architecture. There's always going to be these edge cases where someone knows your algorithm and they're trying to trick you or you just set things up wrong. But also because it's not the protocol, you can provision it dynamically too. If you're seeing, oh, I'm really getting a lot of cross shard rights here. Maybe I need to move this object, or maybe I need to set things up differently or reorganize my ring.
00:24:41.842 - 00:24:46.550, Speaker B: These are all hard engineering problems, but at least the familiar ones for sure.
00:24:46.890 - 00:24:55.310, Speaker A: Super interesting. Awesome. Then the last thing that you talked about, I believe, was the storage front.
00:24:56.210 - 00:25:35.722, Speaker B: Yeah, I guess I talked about the mercle tree and actually applying the effects of what you want is you have a notion of transaction finality, and then you want users to be able to read that in a trustless way as quickly as possible. The traditional way is you build this merkle tree and everyone has the root, and then the user has the roots. They can ask for a proof and get the state of their particular account. This is very convenient, but computing this thing on the critical path for execution is slow. We think it really doesn't scale well as you go beyond a single machine and as you get more and more state. Our principle there is simple. It's just we want to provide authenticated reads without needing to recompute the entire global state route.
00:25:35.722 - 00:26:12.000, Speaker B: And then we want to do any bookkeeping, like checkpointing this set of transactions in the ledger or building a merkle tree off the critical path for execution. Do it asynchronously, the stuff becomes available. You can use it for auditing, but a user also shouldn't need to wait for that to go and spend their money or to take their coffee away or whatever. The way that works is because of this certificate architecture that we described earlier. When you execute a transaction, what you get back from a validator is what we call transaction effects. As you'd expect, this says these objects came into the transaction. Now, here's what they look like at the end, and here are the objects that were created and deleted by the transaction.
00:26:12.000 - 00:26:55.676, Speaker B: This thing also has two of one signatures on it, and it carries the digest of the objects. If someone wants to know, hey, what did this transaction do? What's my balance afterward? Then they can get that right after the transaction is executed without having to wait for this Merkle tree computation or the checkpointing. Those only matter for someone who's trying to look at the entire state at once. For a user that just cares about their transaction and their set of transactions, they get back this transaction effects artifact and can do authenticator reads immediately. This trick Solana has done off the critical path. I think Aptos has also switched to doing it this way. I think most of the new systems are doing it, but in the conventional architecture, you do have this Merkle tree computation on the critical path, and that's a big bottleneck.
00:26:55.748 - 00:27:43.260, Speaker A: Yeah, very cool. No, I think, yeah, it's hard to, it's been just super interesting to me as I said, watching the virtual machines continue to progress to parallel processing and multicore for that additional throughput. And then I've been super fascinated just watching the consensus algorithms develop as well, and how that they've continued to kind of morph over time. How do you feel? Like, out of like the execution environment consensus, or even just like data throughput or storage out of those, do you feel like one is more so potential bottleneck than the others? Or are they kind of like equally weighted in your mind?
00:27:43.680 - 00:28:36.222, Speaker B: So I think storage is the one where that everyone is going to have to worry the most about in the future. I think high throughput systems haven't been around long enough and also, like, had the kind of volume that's testing them to really, like, build up very large amounts of storage. But I believe once you do, and especially once you get way beyond the single machine barrier, then all of these engineering problems we described before are going to come up. Either you've set up your system in a way that it's easy to have a distributed store and hide it inside your validator, or you're going to have to figure out how to do this at the protocol level, and people are going to start to really notice from a programming perspective. But I think that's really going to be the challenge. If you actually have a system that's pushing hundreds of thousands of transactions per day and a lot of that is new object creation, then you're going to end up with many terabytes of data or more quite quickly. And then especially the impact on full nodes, I think, is the thing where you'll feel at first, validators have the incentives to add more resources and be able to store that stuff.
00:28:36.222 - 00:28:55.010, Speaker B: But then we'll see a system that starts off decentralized in terms of having a lot of full nodes, and then as the disks get full and the throughput goes up, that gets less. So. So we're very concerned about having the sparse node architecture to prevent that out of the gate. But I think that the storage is going to be, or if you ask me to predict that's going to be the big problem for these high throughput chains.
00:28:55.330 - 00:29:15.882, Speaker A: The storage, especially if you want to, like, start from Genesis, is going to be a lot. Especially if you're doing like, I don't know, gigabytes per second of throughput. It's a lot of data. And so, I don't know. I think Solana, like, if at a gigabyte they were predicting somewhat, it may be incorrect, but, like, four petabytes a year.
00:29:15.986 - 00:29:16.298, Speaker B: Yeah.
00:29:16.354 - 00:29:55.486, Speaker A: Which is just like the number starts to get very large, and then if you're starting to sync with that data from the beginning, you have to have pretty fast bandwidth to download all that data. Um, but no, very interesting. Um, yeah, so we kind of touched upon full nodes, or I don't know if we touched too much on full nodes. I'm curious, like kind of learn more about those. Like, what do you feel like is going to be the general architecture for those? Are they going to be like mostly cpu? Are they going to have high throughput from like the bandwidth standpoint and then maybe like talk about like more some of the node requirements and the sparse nodes as well?
00:29:55.638 - 00:30:31.260, Speaker B: Yeah, absolutely. So I don't think there's anything too unconventional about our full nodes. They operate on this checkpoint artifact that I described, where periodically off the critical path for execution. The validators have a procedure for saying, here are all the transactions that have been included since the last checkpoint. You gather signatures on that, and then you share that publicly. So it can go into archival nodes or other entities that are trying to audit the system or keep the state of things. And then a full node can grab a checkpoint and it can download all the transactions, re execute them, replicate the state, serve queries, help other full nodes get set up in all of these sorts of things.
00:30:31.260 - 00:31:10.680, Speaker B: And then they can start from genesis. And then we'll probably have a sort of intermediate checkpoint kind of procedure where other folks do like snapsync, these other things where you start from a trusted state and you speed up from there instead of going all the way back if you want to. And so we anticipate that'll work in the short term when the throughput isn't too high and the disks aren't too full. And then of course, if throughput goes up and the cost of running a validator becomes higher, the cost of running a full node will also become higher. That's just the fundamental. There's not so much that you can do about that, especially if your validators are being very smart about executing things in parallel. If they're always sequential, maybe the full nodes can be smarter about discovering good schedules and executing in parallel.
00:31:10.680 - 00:31:40.076, Speaker B: But if you're using all those smarts in the validator, the full node can't do better. Yeah, I think that's the full node story where I think those will always exist. I think an exchange will always run a full node, for example. But more and more as the costs start to go up, you'll run a sparse node that tracks the state that's relevant to your application or your addresses or your assets, then the cost of that will always be proportional to the amount of state you're touching or storing, which is just going to be a lot smaller than all the traffic running through the network.
00:31:40.228 - 00:31:49.340, Speaker A: Could you share any more specific on core count or what throughput of bandwidth on the full nodes or sparse nodes?
00:31:49.880 - 00:32:19.044, Speaker B: So for sparse nodes, we don't know yet because we haven't implemented them. Like we're launching with a single node architecture and just full nodes and the sparse nodes and more of a theoretical design thing that's on our post. Main roadmap for full nodes I can't remember what our current recommended architecture is. We have a blog post where we say this is what you need for a full node running in a testnet. I'm not remembering because I'm hesitant to claim that those will be the final numbers. We're going to go through many different ways of testnet. I think we've got some starting requirements.
00:32:19.044 - 00:32:37.692, Speaker B: See how those hold up and they may go up or may go down, but I think it's nothing crazy. Like sort of like eight cores, like 32 gigs ram, that kind of range. Nothing unreasonable or no crazy special hardware like ASICs or GPU's or anything like that.
00:32:37.716 - 00:32:52.500, Speaker A: Do you think ultimately just because the amount of users onboarding into the ecosystem and because hardware has to increase, ultimately support the increased number of users that will end up in FPGA's or ASIcs?
00:32:53.400 - 00:33:46.570, Speaker B: Yeah, that's a good question. I think it really depends on how specialized and how regular different parts of the SWE validator and SWE client software are right now, other than hash functions and signatures, I definitely could see the ones we're using are already very highly optimized, but you could see further improvements coming there. Or maybe there are other things we do that are very idiomatic, where it makes sense, where you can write the code differently and offload it to a GPU or I'm trying to think of something that move does that could be hardware accelerated. These things will definitely happen. If there are things that are bottlenecks and continue to be bottlenecks, but I don't immediately have any that I would predict this is something is going to clearly benefit from better hardware, from throwing to a different kind of a component that makes sense.
00:33:47.230 - 00:34:18.954, Speaker A: Awesome. Maybe we can transition to move. I think a lot of people are super curious about it. What is move? Is it a blockchain? Is it a programming language? I feel like a lot of people confuse. Is move a virtual machine? Programming language, its own blockchain. A lot of people that I talk to just have no idea. So maybe in your words, kind of explain what move is and kind of what the goal is of move.
00:34:19.042 - 00:34:23.970, Speaker B: Yeah, it's not a blockchain. There is no token. We get our first words, the move token thing in the.
00:34:24.010 - 00:34:27.510, Speaker A: This is an alpha leak. It's not a blockchain. I'm shocked.
00:34:28.330 - 00:35:00.830, Speaker B: It is a programming language. It is a virtual machine. It is, I think, the core thing. It is a bytecode format and a bytecode programming language with an associated source language. But really, when people talk about move or get excited about move, or like, about using move on different platforms, I think it's really about this bytecode language and what it is. I mean, sort of like backing up into why it was created, like, you know, at Facebook in 2018, I was one of the founding engineers in this Libra project. And they, the prompts in this project was, you know, we're pulling folks from all over the company for the secret global scale blockchain payment network.
00:35:00.830 - 00:35:49.810, Speaker B: And they're, we're getting experts in the areas we think are relevant, like distributed systems, like cryptography, like payments. And then they pulled at me as the programming languages expert, and they're like, there's this smart contracts thing. Take a look at it. Figure out why they're so insecure and why there's so many hacks. Figure out should we use the existing languages and build program verifiers and better tooling for them? Should we build a new source language that compiles to the EVM, but it's better, should we use a conventional virtual machine like WAsM or the JVM? Or should we come up with our own thing, like, sort of figure out what the, like, first principles approach? Like, look at this, like, figure out what this model, like what these things are and what the ideal way to do it is. And so we took a very careful look, and it's like, okay, these are these very unconventional kinds of programs. Like, you're not going to write an operating system in a smart contract language.
00:35:49.810 - 00:36:12.662, Speaker B: You're not going to write a compiler. You're not going to write a machine learning here. People kind of do now, but you're probably not going to write a machine learning classifier. What these things do is, is they define shapes of assets. They describe policies for being able to transfer assets and reiterate them, and then they do access control checks. That's basically all they do. It's very sort of domain specific in that way.
00:36:12.662 - 00:36:43.552, Speaker B: And these are things that they're simple tasks, but they're not really well supported in conventional languages because there's no notion of scarcity. When you write x equals y, that copies y. You don't want that if y is a coin or an NFT, but you really do want that in a smart contract language. And then in a conventional language, there are all these things that you have that you probably don't want in your smart contract language, like interfaces. Interfaces are great. This is how you write extensible code. But then this is also a way for an attacker to take something that you've written and make assumptions about what it's doing, and for them to inject their own code.
00:36:43.552 - 00:37:22.554, Speaker B: And that's how you get things like reentrancy vulnerabilities with move. What we did is we said, okay, let's take these core concepts that really matter, the concepts of scarcity, the concept of assets, and then let's design a language that just has those and then has no dynamic behavior or things that are hard to reason about. And let's have that as the foundation for a smart contract language. And so that's what move is. It's a bytecode language that has this built in notion of assets, of scarcity. And I mean like user defined assets, where it's like you write an arbitrary struct with whatever fields you want, and then you say this thing can be copyable or not, and this thing can be sort of dropped on the floor without being explicitly destructed or not. This thing's allowed to be stored in the global blockchain ledger.
00:37:22.554 - 00:37:54.296, Speaker B: This thing's sort of temporary and can only live for the transaction that defines it. It then it's a byte code language that does all these things. And then there's a source code language that's paired to it. There's a compiler that compiles to the bytecode language, and then there's this component called a bytecode verifier that basically checks the same things as a source language compiler does. And we need this because we don't want to run the compiler on chain. It's a big expensive piece of software, and we can't let somebody who just decided to write bytecode manually get around all of our nice guarantees of memory safety and asset safety and all this other stuff. So does what the JVM and Clr does.
00:37:54.296 - 00:38:15.340, Speaker B: There's this bytecode verifier component, so that's what move is. And the other thing about it is that when I said we're going to put in these core concepts and nothing else. I really mean nothing else. There's not accounts baked in, there's not cryptography, there's not the native token of this chain. There's not even tokens at all. There's not the token of this chain. We intentionally made it really minimal so that it can be used in a cross platform way.
00:38:15.340 - 00:38:49.330, Speaker B: We didn't want it to be just used for Libra or DM. It's like you should be able to take this thing and then you should be able to embed it in whatever blockchain you want and make your decisions for what your transactions are going to look like, what consensus you're going to use, what cryptography you're going to use. This is an emerging space. People should have all the flexibility. They want to try out those new things. Then this is the way you're going to have a language that actually has a meaningful community. It won't be one new language for blockchain, it'll be you build a new blockchain, you use move, you customize it the way you want, but you use the compiler, you use the prover, you use the VM, you use existing code, and you can migrate your expertise.
00:38:49.330 - 00:38:55.980, Speaker B: This is sort of a long winded answer to what is move. That's how it came to design move, and that's what it's trying to do.
00:38:56.060 - 00:39:40.390, Speaker A: Very interesting. Now, I think that protections, especially for engineers, that. Well, I think for every engineer, but ultimately engineers that are maybe a little bit more hesitant to get into the space, these guarantees are very nice. And being able to at least try to minimize the possible attack service with this programming language is extremely beneficial. And it's cool that you guys open source it and allowed are kind of not specific to the Libra association. So it's very cool.
00:39:40.970 - 00:40:04.278, Speaker B: Yeah, I appreciate that. A programming language is biggest asset is its community. No matter how good it is. If it doesn't have a lot of people writing code, if it doesn't have. Lot of people are excited about it, if it doesn't have a great tool chain, then it's just not going to have any value. And language growth is one of. I don't know, it's like the hardest problem I know, because it's like this combined socio technical problem where at baseline there's never a good reason for creating a new language, no matter how good it is.
00:40:04.278 - 00:40:49.268, Speaker B: The network effects of the existing ones are going to overwhelm it. And so if someone says they're going to create a new one, you should yell at them and say they're crazy and tell them not to do it. And so you got to give Facebook a lot of credit for both, like being willing to invest in the time and the people and the money it took to create this thing and then also to be willing to open source and to give us the leeway to really make it useful elsewhere. But it's because they do open source really well and they understand languages and had big successes like react, where it's like build something, like put something out there that gets developers excited. And so we had the benefit of that with move and for the protections. I mean, I totally agree where I think people look at some of these big hacks and they're like, oh, these smart contract developers today, they're just too careless and this will somehow get solved in the future. But I really don't think so.
00:40:49.268 - 00:41:12.364, Speaker B: The smart contract developers I know are really security minded, are really careful, deeply understand the platforms they're programming for, and really worried about these things. And this stuff still happens. It's just human fallibility. I don't think it's unacceptable carelessness on their part. And when you broaden the space and bring in more programmers, it's not going to get better, it's going to get worse. It's like these people don't know as much. I think smart contract safety is really an existential threat for the growth of crypto.
00:41:12.364 - 00:41:25.324, Speaker B: And I think if we can't expect new developers to be better, the only thing we can really do is bring in better languages, better tooling, bake in more correctness at the base level, because people are what they are and they're going to make mistakes. So, yeah, I really feel that part.
00:41:25.372 - 00:41:56.360, Speaker A: No, I fully agree. I think. No, I definitely agree. I think a lot of people today that may be watching this podcast are like solidity or EVM base or even rust. How would you kind of compare and contrast like solidity or programming in the EVM to building a application in rust versus move and moves programming language and the pros and cons.
00:41:56.740 - 00:42:44.008, Speaker B: Yeah, let me tackle this separately because I think the answers are somewhat different with solidity in the EVM. Solidity and the EVM are sort of, and the EVM specifically are the very first effort at figuring out what a smart contract language is and what it does. Like you got to give them a ton of credit for on their first try, come up with something that has really stood the test of time and is managing hundreds of millions or I guess billions of dollars. And there are some security issues. The thing got the job done for sure. So I think the main advantage move has is really in being a second mover and looking at what they did, seeing what kind of programs folks are trying to write in the EVM and seeing what they did that worked well and what doesn't. And so I think the biggest single thing is that if you look at the data model and this gets back to being like, what are smart contracts for? And being all about assets, there's no notion of an asset in the EVM.
00:42:44.008 - 00:43:05.000, Speaker B: The way you represent assets is you have a hash table and the keys are addresses and then the value is a balance or some bytes that represent the address. And this just makes it really hard to do the things you want to do with smart contracts. You want to write code that returns an asset from a function. Well, you actually can't do that. There's no representation of an asset. You want to store an asset in a data structure, you also can't do that. Pass it to a function, also can't do that.
00:43:05.000 - 00:43:36.108, Speaker B: There's no representation. The whole thing is about program with assets, but there's no vocabulary for describing this. And so you can still do this, but it just ends up being a lot of indirection, where instead of transferring a coin by sort of passing it from one function to another, you call a function that decrements here and a function that increments there. And you try to make sure nothing else happens in the meantime. And so I think it's just the wrong level. Well, it's the wrong vocabulary and then the wrong level of abstraction for what folks are trying to do. I think the other thing, and this is related to the assets, is that that in conventional programming abstraction is all about types.
00:43:36.108 - 00:44:10.090, Speaker B: I define some types, I have some associated functionality. And then you pick up my types and you build on them and then someone else builds on them. And then all of a sudden you can say new web server and that just works and no one has to know what goes on underneath. But if you don't have types that can be reused across trust boundaries, and in the EVM you don't. You have types in your local contract, but you only communicate via raw bytes across contract boundaries, then you can't do type based abstraction. Your code in the code ecosystem is just like a bunch of actors who sort of communicate via message passing. But it's very hard to build these towers abstraction that we use to design complex applications in conventional programming.
00:44:10.090 - 00:44:28.792, Speaker B: And with move you can do that because the bytecode verifier, you can write some types, you enforce some invariants on them. It flows into some attacker who wants really badly to steal your money, but they can't. It's set up so that they can't get inside your types and they can't violate your invariance. And so that lets you do the thing that you're used to doing, which is type based abstraction and building large and complex applications.
00:44:28.986 - 00:44:47.588, Speaker A: Nice. Maybe. I think, I mean, obviously we're definitely in the weeds by now. Maybe kind of on a higher level. Kind of explain what the bytecode verifier does and why that's so significant to the average day person.
00:44:47.764 - 00:45:21.208, Speaker B: Yeah, absolutely. So I think it's easier to start with a concrete example for this. Let's say you're defining a module and you're defining your own NFT type, and you give it some fields and you want to have a policy like this has a counterfield, and only you're going to be allowed to increment it. Now you have that type and you pass it into some other code. The question is what stops them from being able to just do whatever they want with it. It's bytes is on their code and they have full dominion over what's in there. You have to make sure that they can't do that.
00:45:21.208 - 00:45:51.298, Speaker B: Just to give, let me back up and change the example a little bit where, let's say the hard one is, I think you've defined this NfT and you really don't want someone to be able to copy it because there's supposed to be 1000 additions. And if you can copy it in your code, then that would be bad. Well, copy is an operation to move. We have integers. And when you say let X equals seven and let Y x, then you should just copy that. And so what the verifier does is that you import the type into your module. And so it'd be Logan's NFT.
00:45:51.298 - 00:46:40.462, Speaker B: And then you write something and then you would. And then when you import the type, you say what abilities it has. And the abilities are things like the ability to copy, the ability to store in the global blockchain state, the ability to drop without using a destructor. The verifier will check. If you copy something, does it actually have the copyability? You get that check there then also when you do the import, there's a linker that says, hey, I'll look at the type of definition, the original module, and if you tell me you've imported it with copy, but it doesn't have, it's also going to slap you on the wrist there. So what it does is it lets the programmer define types, define policies on who can write its fields, who can read them, whether they can do operations like copying, like dropping, like putting the global storage. And then those local invariants in your module become global invariants even as your types flow out of your module and are used by code, including untrusted code.
00:46:40.462 - 00:46:53.160, Speaker B: So it just gives you a very strong bill of rights as a programmer. And then you also know that other people have to respect that. And then, so you have this strong isolation, but you also have a lot of expressivity in terms of being able to share code and share objects.
00:46:53.460 - 00:47:21.448, Speaker A: Nice. I think it's very unique. It's definitely, I mean, it's unique today, but I think going forward it's a necessity. And I'm glad that the Facebook or meta ultimately has allowed the open source and it to be used by everybody. So I'm super curious to see the general adoption over the long term.
00:47:21.624 - 00:47:42.662, Speaker B: Yeah, me too. I mean, we're off to a good start with there's, I think, four different move based chains now. There's Starcoin, which is actually the first one, launched almost two years ago. At this point there's ol, which is this permissionless libre fork. Then there'll be aptos and it'll be swe. Then there's a lot of existing chains that want to incorporate move as sort of a second language or a different layer. And then I think there are new chains too that are very curious about using it.
00:47:42.662 - 00:48:09.070, Speaker B: It's designed to have this, we sort of call it embedded architecture, where you take the VM and then you plug in your transaction format and accounts, and then you just go to town. You can do things in a very different way but still use it also. We'll see how that goes. The JavaScript of web3 is how we pitch it, not in terms of language design, but in terms of if you're going to write code that runs smart contracts on blockchains, then that should be moved. Just like if you're writing code that interacts with the DoM, that's JavaScript. That's what we'd like to see.
00:48:09.470 - 00:48:15.050, Speaker A: And how would you, we touched upon the EVM and solidity. How would you compare it to just more generally rust?
00:48:15.390 - 00:48:50.048, Speaker B: Yeah, so this question is always a little bit puzzling to me when I started at Libreo and was like, what should we use as a smart contract? My first thing is like, yeah, we should use Rust because it has these affine types that look a lot like what moves, resource types are. It's got this nice borrow tracker, it's got a lot of safety built in. It's a language developers love. That was my first thought. But then the problem there is that Rust is not an executable language. Rust is a source language that compiles through LLVM to machine code. You can't deploy machine code directly on the blockchain because then the attacker is going to publish machine code that goes around.
00:48:50.048 - 00:49:28.332, Speaker B: Basically the problem is that all of rust guarantees are checked at the source level. So if you want them, you have to go through the Rust compiler, but you do not want to run the rust compiler on chain. It's going to be too expensive, it won't be deterministic and all these sorts of things. What you want is something that's rust like, but enforces Rust guarantees on an executable representation, and that's basically what move is trying to do. And so if you have something like where people are like, oh, I write smart contracts in Rust on Solana, you're not actually using Rust there. You're using a Solana SDK that's embedded in rust, and then there are lots of rust things that you're not allowed to do. Like if you try to do something not deterministic, hopefully the Rust compiler and the Solana SDK is not going to let you do it and similar.
00:49:28.332 - 00:49:50.662, Speaker B: So I think Rust can be a nice language if you're going to host an SDK for some blockchain language, whether it be Solana or cosmwasm or something. But Rust is not itself a smart contract programming language. It's missing a lot of things that smart contracts need, like gas metering, like accounts, like coins. And then it also has a lot of stuff like concurrency and non determinism that you don't want or can't have in your smart contract language.
00:49:50.766 - 00:50:32.278, Speaker A: Makes sense. Awesome, cool. We've been chatting about 50 minutes, maybe kind of on some of the overall architecture design choices. Why not include a L2 or do sharding or application specific chains? I think as, I mean, the entire industry is kind of moving towards that. I mean, except for the exception of, I think, Solana, Sui and Aptos. Why? Like, what are your kind of your general thoughts on that direction? And then why have you chosen to go in another direction?
00:50:32.454 - 00:51:39.664, Speaker B: So I think it is just swee and Solana that stand alone in this guard. I think in Aptos recent white paper they said that they're going the direction of sharding, and it's more of an e three style design where all, well, no, sorry, it's opinionated design where all the subchains are move. And it's sort of like one paragraph so you don't see all the details, but definitely the things like, yeah, there are going to be shards and it's intervalidator sharding in contrast to intravalidator sharding. As we're describing our model, where you see it at the protocol level, programmers have to program against it, users are aware of it. As you said, the broader trend, although there are many different flavors of this, the reasons why I think we touched on this earlier is just we think this is the best developer experience, we think this is the best user experience and we think it's possible to implement the system in a way that you can provide it. Repeating myself, but I think it's important to say where. I think the value we've seen for blockchain so far is all about taking things that lived in these siloed areas, like your bank and my bank, or even you and me as people who have our own money, and then just knocking down those walls and putting the state in the same place and giving you a programming environment where you can touch these things at the same time and you can interact atomically.
00:51:39.664 - 00:52:14.208, Speaker B: And that you can do this both as a programmer and as a user, where this happens quickly and you don't notice like that it takes more time to send because my bank is on a different chart than your bank and that kind of stuff. So really we think this is the gold standard to aim for as far as that developer experience and user experience. And so that's what we aim for in our design and it's really as simple as that. I think that's, if you think it's impossible to achieve that, then I think yes, you should try to do l two s, you should try to do app chains, you should try to do intervalidator sharding. This stuff is important. All this can work. But I think I view that as a compromise that you go for after you can't get the base layer to scale anymore.
00:52:14.208 - 00:52:22.536, Speaker B: Because I think as much as you can preserve atomic composability, you should try to do it. So I think it really doesn't go any further than that. I guess maybe we're just more optimistic than other folks.
00:52:22.728 - 00:53:15.260, Speaker A: I mean, I fully agree. I think one of my kind of hesitancies about those other design choices is that we're nowhere near the fundamental limitations of how much you can actually push a single architecture design before doing L2s or sharding and all that, and the fact that we're kind of just barely scratching the surface and already exploring those trade offs, I'm not as much of a fan. And so I very much appreciate what you're building here with SWE and Mistin labs. I think ultimately, from the product point of view, people appreciate all those complexities being removed. One from an engineering point of view, but also from a actual user standpoint. And any of those complexities that can be removed and hidden behind the scenes is very important.
00:53:16.240 - 00:53:54.052, Speaker B: Yeah, no, absolutely. And to be clear, there's nothing that stops you from implementing L2s on top of Swe. And maybe there may be reasons to do that, because maybe you're going to have some private state in your L2 and that's not supported in the native layer and sui, so you do that, or maybe there are certain things that end up being too expensive, so you put them in a L2. I fully expect that stuff. You know, I think Anatolia has an interesting point of view on this, too, where it's like, yeah, sure, like, you can have L2s, there will be unslanable, but, like, you know, let's just not bank on that assumption. I think. I think we're very similar there, where it's like, you don't want to keep these things out, but it's just, you know, you, as a protocol designer, want to focus on scaling your protocol, the base layer, and then people are going to build these secondary layers as they need to.
00:53:54.156 - 00:54:09.270, Speaker A: That totally makes sense. Awesome. Hmm. Is there any kind of, like, thoughts or expectations on, like, the raw, like, data throughput between nodes? Like, 1 megabytes?
00:54:09.730 - 00:54:35.450, Speaker B: I think it's just really hard to know that without having an idea of what the validator account is going to be, what the topology is going to be, what the workload is going to be. I really hope we can get to a point where we have some performance bottlenecks that are performance benchmarks that are more interesting than payments and that take those things into account so we can actually have more apples to apples comparisons of networks in the future. But for now, people are just like, what's the TPS going to be? It's like, man, I need to know, like, 20 more things before I answer that question.
00:54:36.870 - 00:54:53.650, Speaker A: Awesome. Maybe, like, what are your kind of, like, thoughts, comments, or like, words of wisdoms that you would like to impart on, like, developers or people that are looking to learn more? I mean, kind of about building in the sui ecosystem?
00:54:53.950 - 00:55:59.292, Speaker B: Yeah, I think I would take a look and just try to understand the things that are different about Sui, like try to understand these different and the fast path where you skip consensus and then the ordinary consensus. Just look at what's different with suite and what's move and look what's enabled there that doesn't happen elsewhere. I think we get a lot of folks coming in and saying something like, where is ERC 721? And then it's like, well, yes, you can do this, but you also get a lot of that stuff built in because every object has a built in unique id. Every object has a built in ability to transfer. So don't ask where the standards are, ask what you want to create from an NFT perspective and then add custom fields, do those sorts of things. So I think it's just back off the limitations and really think what is interesting from a use case perspective for me in terms of the product I'm building, what do I want to create and then figure out where can we help with that? I think this is very generic advice, but I think we're so used to thinking in the way one platform's designed to, the limitations and advantages of that. And Swe, as we've been talking about, is very different in a number of ways.
00:55:59.292 - 00:56:17.428, Speaker B: So I think taking the broad perspective, and we're trying to help with that too. So folks don't have to become a swe consensus expert or move expert to see that, but to show you here are some interesting things you can do, or the way you do the same thing differently, but then add on this feature or that kind of stuff just so people can digest that. But I think just taking the blue sky view is the biggest piece of advice I would give.
00:56:17.524 - 00:56:39.990, Speaker A: Awesome. And you touched upon one thing that I don't think we, or I brought up initially, maybe going a little bit more into just like the object oriented model of SWE, because I think they're unique in that architecture design and kind of what that enables. I think we've kind of touched upon it broadly across the board, but maybe just go slightly deeper into that.
00:56:40.070 - 00:57:23.500, Speaker B: Yeah, totally, because I think this is something you'll definitely run into from either a programmer perspective or end user perspective. Wherever in Swe there are addresses, as there are elsewhere, but there aren't accounts where we talk about being object centric, where the global storage is organized as a map from object id to objects, and then every object has this global unique id, and then it has this ownership metadata at work and says the address that owns it or that it's shared or something else. And then the other thing that's in an object is basically aRb. To remove data. An object will have a type. It'll say this is a coin or this is an NFT, this is a Dex. And then it has the fields of whatever you typed, when you typed struct s, and then Xu 64 bbool, whatever else went in there, that'll go into the object.
00:57:23.500 - 00:57:50.552, Speaker B: When you write your code, you say, here's an entry point function, here's the object types that's going to take as input. And then you just write the logic that says, oh, I'm going to update this one, I'm going to transfer this one to some address I got as the input and maybe I'll create another one and give it to somewhere else. So it ends up having this very sort of a tactile flavor where it's just like I'm in this room and I've got all these objects and I run around and do things with them. And then once all the objects disappear, my transaction's over. And that's sort of how it works.
00:57:50.656 - 00:58:13.808, Speaker A: Okay, very cool. I think it's a unique thing that you guys are doing, and I'm excited to see how ultimately developers end up using it and also just kind of all the unique properties that it enables. I think we touched upon composability as well, but it definitely helps with that, correct?
00:58:13.984 - 00:59:14.912, Speaker B: Yeah, I think so. We think a lot about don't just have a standard that rigidly defines how you do things. Have various building blocks that can be composed to put different pieces of functionality together in a way that doesn't dictate a choice for one thing or the other, instead of having one NFT standard that says here's how royalties are going to work, and here's how listing on a marketplace is going to work, and here's how the unique id works, and here's how an image URL works or something. Instead, maybe you have something like here, here's something called royalty of t and someone can define their own royalty policy and then stick that inside the NFT or not if they don't want royalties. And maybe they and the unique ids that are built into the objects, you don't have to put that on the standard, it's just there by default. And then for something like listing, maybe different marketplaces would do this in a different way. So they're sort of utility modules with generics that different folks can use to instantiate that in one way or another, but not something that folks have to agree on in a top down fashion in general, or.
00:59:14.912 - 00:59:42.062, Speaker B: One of the ways I think about composability is that if you can coordinate via code, just use a type that does something you want and pick that up and not have to argue or write in a forum or have social consensus on it, then that's always a win. That's easier to coordinate on, easier to iterate on and innovate on, than needing the entire community to agree on one way to do something before you can even move forward. So we think a lot about that. When we think about standards or the expressivity of move or how to shape things in swe.
00:59:42.166 - 00:59:57.490, Speaker A: That's awesome. Well, I think we've kind of gone through all my high level questions and the granularity. Is there anything that you particularly want to touch upon or things that I missed out on?
00:59:57.790 - 01:00:04.090, Speaker B: I think you've asked a great set of questions. Logan, you really kicked all the right tires, and it's been a fun conversation for me, so I appreciate it.
01:00:04.570 - 01:00:14.530, Speaker A: Thank you so much, Sam. I have been really looking forward to this conversation and kind of peeling back all the layers and getting into the nitty gritty. So thank you very much. I really appreciate your time.
01:00:14.610 - 01:00:15.890, Speaker B: It was my pleasure. Thank you. Awesome.
