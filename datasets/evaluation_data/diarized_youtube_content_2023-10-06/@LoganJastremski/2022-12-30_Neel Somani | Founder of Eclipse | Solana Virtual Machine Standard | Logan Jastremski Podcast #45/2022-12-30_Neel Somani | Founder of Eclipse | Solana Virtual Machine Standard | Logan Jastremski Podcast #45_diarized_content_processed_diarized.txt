00:00:01.200 - 00:00:12.970, Speaker A: Awesome. Well, thank you so much for coming on the podcast, Neil. Really appreciate the opportunity to chat with you. We've been going back and forth on a couple dates, and I'm finally excited to make this happen. So thank you once again.
00:00:13.350 - 00:00:15.410, Speaker B: Yeah, thanks, Logan. Thanks for having me.
00:00:16.030 - 00:00:39.960, Speaker A: I think what you're building is super unique. But before we dive deeper into that, let's start a little bit about your background. I think you. We were talking earlier before the podcast, and you said you just got into crypto this past year, so I'd love to hear more about your background and what led you to crypto and ultimately what made you take the jump.
00:00:40.700 - 00:01:08.168, Speaker B: Yeah, definitely. So I've always been interested in crypto. I bought my first bitcoin, I think, in 2017 or something. I started my career in software engineering. I was over at Airbnb, worked at Citadel for the last couple of years. I was a quant in the commodities group, so that's doing power and gas research, trying to figure out what's the right price for electricity. And at that time, I was friends with a few folks from college and from other places who had gone into crypto professionally, whether as venture capitalists or some other role.
00:01:08.168 - 00:01:58.992, Speaker B: And they were talking about some innovations that were happening in, like, stable coins and defi, which I thought was really interesting given my background in traditional finance. I just started riffing on that, and one of them convinced me, you know what? If you just, like, pick an idea and start working on it casually, you can probably do it on the side while you're working a little bit. So I was just playing around. I wasn't actually seriously pursuing anything. I definitely didn't think I'd be starting a company or anything, but I was just reading white papers. And ultimately, I got so interested in one idea that I quit my job in March, and I ended up pursuing it, which is an EVM on Terra, which is kind of like neon EvM that exists on Solana, the same idea for Terra, which is tricky because neon leverages things like parallelism in the Solana VM. And there's just a lot of advantages to using neon, whereas for this, I was building, like, a generic EVM implementation for Cosmos.
00:01:58.992 - 00:02:59.270, Speaker B: So all these cosmos chains can now run Ethereum code. And the thought at that time was that there'd be so many l one s given cosmos is spinning all these up, and there would be some need for them to have EVM compatibility, given we saw, like, what that could do for the binance ecosystem, how it benefited Aurora on Nier. So that's what I was building then ultimately Terra de pegged scrapped that project, spent some time going back to the drawing board, thinking about what the future of crypto infrastructure would look like. Chatting with Solana core folks over in Chicago. Got really into the Solana ecosystem, and then the celestia team reached out and they kind of roll up pilled me for the vast majority of use cases, convincing me that there are a lot of use cases that are better suited to roll ups rather than being on the l one. And that's how I started building eclipse, which is basically taking the Solana virtual machine and letting you customize it in a bunch of different ways and spin up your own application specific roll up rather than an application specific l one. And then you could use Solana under the hood for data availability, aka like security or any other l one.
00:03:01.170 - 00:03:41.072, Speaker A: We were jumping right into it. I love it. Perfect. Yeah, I've always found these different blockchains super interesting. After my time at Tesla, I became frustrated while using ethereum, mostly just because the high gas fees. I was really interested in the roadmap l two s sharding. But once defi summer started and I had to start paying more for gas, and then once the NFT bull market started, gas fees just took off and I personally paid over $1,000 for a uniswap once.
00:03:41.072 - 00:04:07.024, Speaker A: And after this I was like, Mandy, I really need to start looking at different ecosystems and figuring out how this all works, because it's too crazy. So what was your kind of, you said you ultimately started with terra and the EVM. Why was that kind of your first jump into the ecosystem? Was it just trying to get your feet wet and learn about crypto, or you found something particularly interesting about the EVM and Terra, so terra in particular.
00:04:07.152 - 00:05:02.424, Speaker B: I was really interested in the stablecoin. Just given that, I still feel like there's a huge need for some kind of actually decentralized stablecoin, since USDC circle will just freeze your funds if they feel like you did some kind of wrongdoing. It's really a fairly centralized part of our ecosystem, and yet it's at the core of crypto. Especially when people on ramp onto crypto, they often start with USDC as their stable coin of choice. So I felt like there was some sort of need for something that no one could censor, such as ustinous, of course, whether UST was the right invocation or the right manifestation of that idea, used to say. But writing at the problem statement made a lot of sense to me and I was just interested in all the kinds of interesting financial derivatives you could construct using UST as a primitive. Once you have a decentralized stable now, you could have currency swaps, all these currency related derivatives you could build in loans in a much more natural way.
00:05:02.424 - 00:05:44.430, Speaker B: There's all sorts of things that I was interested in building out eventually, but the first step was basically creating use cases for UST, given that there's nothing really to peg it. So as a result, you need to have real demand for UST in order for it to maintain its peg. So that's why I wanted to do the EVM. I wrote this blog post called the Future of Tera Defi, where I was basically sort of reiterating the points that a lot of people were making that presented the risk of UsT de pegging, which maybe some folks didn't recognize at the time. But the solution that I was proposing was let's create an EVM so that we can suck up all this UsT into there, and then we can bring all the Ethereum applications over. We can start using this and putting this UST to work.
00:05:46.610 - 00:05:54.082, Speaker A: Yeah, makes sense. The tarot ecosystem, a lot has happened in 2020.
00:05:54.106 - 00:06:00.430, Speaker B: It's still growing, apparently. Yeah, we have. Chris Amani and Joe Kwan are still doing things.
00:06:01.320 - 00:06:30.742, Speaker A: Yeah, we'll see what happens there. How? I mean, so joining the space in March of 2022, I know you've been active prior, but kind of going full time. What has your just general thoughts been on this kind of first bull market and seeing the ups and downs and implosion of Luna, implosion of FTX? How has it been personally, as a founder and someone that has been raising money and building a team and a product?
00:06:30.936 - 00:07:34.090, Speaker B: Yeah, I think that the bull market rewarded reflexive products like it rewarded Ponzi nomics, basically, because if you're a regular, sustainably growing company, your growth's not going to look that cool compared to someone who's just using unsustainable incentives and building their ecosystem in this sort of like hype away that won't, that wouldn't necessarily last without those, that level of emissions. So as a result, I feel like there are a lot of great projects that couldn't be back during the bull market, but now in the bear market, they're looking relatively good because they've actually held their ground. So I think that was one major difference that I felt over the last several months. Another one is that I also feel the kinds of things that people are interested in were typically like nfts, for example. Now we might be seeing things that are giving more utility to nfts, so more infrastructure type like adaptations of consumer ideas. And I think nfts will still be great. I think there's still a lot of really good use cases, but maybe going back to the drawing board a little bit and re envisioning what should the role of nfts be in the crypto ecosystem.
00:07:34.090 - 00:08:07.890, Speaker B: So that was another one. I think some of the narratives failed to play out during the bull market and because maybe the tech just wasn't ready, such as gaming and crypto, I think there were a couple of really great projects that were very close, like Axie infinity. I mean, it basically pioneered this mass popularized play to earn. I think ultimately, in retrospect, play to earn was not the right mechanic. But those kinds of experiments I think are really valuable because we know what not to do and we gain some real insight about what is a sustainable yield model and what should we be building moving forward.
00:08:10.070 - 00:08:40.416, Speaker A: Yeah, it totally makes sense. At the end of the podcast, I want to come back to what things you're excited about, most particularly kind of building in the bear market, and we can definitely touch upon some of those topics, but I would love to just jump into eclipse and what you're building. Can we start with just like a more holistic overview and then kind of share a little bit of the vision and what the end goal is for eclipse?
00:08:40.608 - 00:09:15.374, Speaker B: Yeah, totally. So I'll take a step back and just give some context for the audience in case they're not familiar with rollups. Typically there's a layer one blockchain, and every time you spin up a layer one blockchain, you have to provide security somehow. So if it's a proof of stake chain, you need stake on that chain. If it's proof of work, you need nodes to run this network. And in general, it's difficult to spin up a new layer one blockchain because you need to redo this process, you need to bootstrap a validator set, you need to ensure the reliability of this network. And really we all want to use some base layer, such as Solana or Ethereum, which we know is going to keep working.
00:09:15.374 - 00:09:48.498, Speaker B: They've already done the hard work, they're constantly improving it. They have these base level optimizations they're always working on to increase the throughput. So we won't want to rely on that. But we also want to customize certain things about the execution layer, such as let's say you actually want a mempool, you want to do that in order to recapture MeV for your protocol through some sort of auction, like an MEV auction. Or maybe you want to run your own private l two, you're like a group of hedge funds who wants to trade against each other. None of you trust each other. So you all want to be able to run this permission network together.
00:09:48.498 - 00:10:24.156, Speaker B: But you also want the transparency and the rules enforced by some layer one. So these are like the types of applications that are interesting that it's at least cumbersome to do on an l one, if not just impossible. So these are the kinds of applications that we want to double down on for roll ups. And there are also just some cases where maybe you just want your own chain for whatever reason, you want to adjust how gas is paid. You want to just not charge gas at all for some operations, or you want to build in your own syscalls. So you want to maybe your hardware like a tip in project token incentivized physical infrastructure network. And you have some specific use case that you want the nodes to actually do.
00:10:24.156 - 00:10:50.810, Speaker B: Some regular contract invoked every few seconds or something like that. So anyway, those are the types of use cases that we're interested in. And the solution is that you spin up a node or you spin up a network, which is insecure by default. It doesn't provide its own security. But then you're able to, quote unquote, borrow the security from some other layer one through some mechanism. And the two options are optimistic rollups or zero knowledge roll ups. So that's rollups in general.
00:10:50.810 - 00:11:15.082, Speaker B: We're building a modular roll up where you can swap out some of these pieces. So let's say you want to switch the chain that you're using as a layer one. You could swap that out, or you want to switch your execution layer. So we're using the Solana virtual machine, which there's the neon project for. Eventually there'll be a move bytecode letter. So if you want to switch what bytecode you're running, we're able to support that moving forward. And we're working with the neon team on supporting EVM for the Solana virtual machine as well.
00:11:15.082 - 00:11:41.830, Speaker B: So, like, those are the types of customizations that you'd be able to do with this kind of system, as opposed to a traditional roll up like optimism, where it's not really solving like the issue with roll. It's not really providing, at least at this point, it's not providing the major benefits of rollups, which is that on optimism, you're all on the same chain. Still, it's all using the EVM. It's all using Ethereum. Under the HUD, you don't get to customize anything, and that's where rollups really shine, and that's what we want to push people toward doing.
00:11:43.170 - 00:12:18.020, Speaker A: That makes sense. Definitely pushing people towards more customization. You mentioned a couple things in there, and I want to parse them all apart. I do want to talk about zks versus optimistic roll ups on l two s, but I think maybe before we jump into that, a good place to start would be on the Solana virtual machine. And that is what eclipse is kind of using for its virtual machine. Can you talk about why you chose Solana and the Solana virtual machine for eclipse?
00:12:18.320 - 00:12:41.416, Speaker B: Yeah, and to put it shortly, it's just the best virtual machine right now. It's definitely the most. It's like a swiss army knife in the sense that it will support many byte codes moving forward. It's highly parallelized. These gpu's for signature verification. The simplest alternative would be like an EVM, for example, the Ethereum virtual machine, which is single threaded. It's very slow, and there's also a lot of EVM rollups that already exist.
00:12:41.416 - 00:13:16.770, Speaker B: What's cool about rollups is that the execution layer for the roll up or the type of code that it's able to run, doesn't necessarily need to be the same as the base layer. And that's what people maybe don't realize about optimism or arbitrum. There's no reason, like, no particular reason why it used the EVM for technical reasons. If you look at how arbitrum is implemented, there's a smart contract on Ethereum which is able to execute EVM bytecode in the same way that geth does. So it's like an interpreter for EVM byte code that's run using the EVM. It's like a virtual machine in a virtual machine. So it's not like they're leveraging the base virtual machine of Ethereum.
00:13:16.770 - 00:13:54.130, Speaker B: I mean, to some degree you do, because maybe someone invokes a system call, like a syscall or a native program. I don't remember what it's called in Ethereum, but you invoke some sort of special operation. Then of course, if you're on Ethereum, then you get that one to one parity. Whereas if we were to implement the Solana virtual machine as a smart contract on Ethereum, like if we tried to do settlement on Ethereum and there's some elliptic curve that was invoked, that's probably not going to exist on Ethereum. So it'd be intractably difficult. It'd be very expensive from a gas perspective to re execute that one elliptic curve indication. So that's one reason why the bytecode of the base layer matters.
00:13:54.130 - 00:14:31.010, Speaker B: But in general, it's really two separate things that you can be thinking about. So, yeah, that's basically why we chose this lawn, a virtual machine, which is that it's highly parallelized, it's very fast, supports different byte codes, and they're constantly improving it. Now we have this best in class fee market. There's people who are building tooling such as seahorse laying, which lets you write python programs run. It'll doesn't even compile it to Solana bytecode, but it actually converts it to a fully typed rust program. And then you can compile that. Just amazing projects like that, which we want to continue supporting, and we want to borrow those innovations and allow people to leverage that on eclipse, too.
00:14:32.510 - 00:15:14.890, Speaker A: Yeah, once I kind of went down the rabbit hole of like, how do blockchains work? How do they scale? It became very clear to me that any virtual machines, modern virtual machines, and like these newer layer ones or L2s going forward should be paralyzed. And any virtual machine that is just single threaded is kind of going to get eclipsed by these newer blockchains or these newer virtual machines. Can you talk particularly, like, can you explain to the audience again, like, why, like, what is the big bottleneck? Not bottleneck, what is the big unlock from parallelization to a single threaded virtual machine like Ethereum?
00:15:15.510 - 00:15:56.718, Speaker B: Yeah. So let's say I want to send some money to Logan, and Logan wants to send some money to, I don't know, my parents or something. They have like 100 transactions that need to be run on Ethereum. There's no other way to do it other than by running these transactions one by one. And these are probably just not just sending money around, but it's running full programs. So I want to invoke uniswap, or I want to do some complicated amm thing, and then Logan wants to, I don't know, do some yield farm or some sophisticated NFT trade, who's to say? So you have to do all this stuff. And ultimately everyone has all this contention they have to bid higher gas in order to get their transaction into this single thread of transactions.
00:15:56.718 - 00:16:32.820, Speaker B: Whereas for Solana, because you have all this additional information upfront, we specify exactly whose accounts we're going to read to or write to. So I'm going to say my transaction involves Neil's account and Logan's account and Logan's. Like, I depend on my account and Neil's parents and everyone specifies that information. Then the Solana virtual machine is able to start executing. If you have ten cores, you could execute 100 transactions, or possibly more, you could execute tons of transactions in parallel. And as a result, you can, because there's no overlap. If I'm sending money to Logan, then Logan won't be able to send money to my parents in parallel.
00:16:32.820 - 00:16:40.580, Speaker B: But maybe there's some other two people that are sending money to each other. Now, both of those transactions can be made at the same time. And that's the big unlock.
00:16:41.760 - 00:17:04.524, Speaker A: I think that's a great explanation. And the other thing that you touched upon was localized fee markets. And I think also Solana, I think, is the leader in this front as well, versus global fee markets and really unlocking new possibilities here. So could you talk about, and your words explain, like, localized fee markets and kind of compare it to global fee markets?
00:17:04.652 - 00:17:38.040, Speaker B: Yeah. So for that single thread, right now, all of those transactions are all competing with each other, and they all have to bid the most in order to get their transaction in. But really, what if there's like an NFT drop and that's responsible for a thousand out of. Out of 1000 in one of those transactions? Really only those guys should be paying the price, rather than this last person who is just minding their own business and doing a uniswap trade or something. So the local fee market allows you to separate these two concepts as the people who are responsible for causing the price spike, who will also be paying the price, which is more economically fair.
00:17:39.340 - 00:18:05.894, Speaker A: Yeah, it's brilliant. There's a couple of things that Solana has done where I'm like, why? Why was this not the de facto standard going forward? I guess ultimately, it took us a little bit of time to see how these blockchains were going to be used and how these fees incurred. But I think now the design is getting pretty clever, even though it's relatively simplistic.
00:18:06.062 - 00:18:46.298, Speaker B: Yeah, that point on just seeing how people use it, because sometimes I've heard this critique of crypto, like, why don't we see cases where crypto is being pushed to its limits? And we actually do. The female is a great example of that. Where NFT drops happen, we didn't anticipate there'd be such a huge strain on the network. And then, boom, the architecture was adapted to enable it in a more economically efficient way or on Ethereum. Even the concept of roll ups, the way that they're trying to use Ethereum retroactively. It wasn't really built for rollups. They have to store block data in a very unusual way on the l one, trying to squeeze it into the call data and stuff like that.
00:18:46.298 - 00:19:23.770, Speaker B: And that now Ethereum's roadmap has adapted to better accommodate these roll ups. Things like that, I think are really organic, and it makes crypto or blockchain infrastructure start to look more like a biological organism adapting to its environment. Especially given nfts aren't really a huge thing on Ethereum. I mean, maybe it's probably roughly equal to Solana, but we saw Solana ate up the NFT market share so quickly, and that's why that's like a prime concern for Solana, whereas Ethereum's like, you know, like, maybe that's not what we're best at. People shouldn't be using Ethereum to be trading like a million nfts because they're going to be paying all this gas for no reason. It's really just not built for that.
00:19:24.790 - 00:20:13.336, Speaker A: Yeah, yeah, it was. Nfts were funny last, last couple years. I think in retrospect, the high gas fees on Ethereum were bullish for the NFTs there just because it almost set a universal floor price when you have really high gas. I mean, you're not going to buy a cheap NFT that's only worth $10 when you have to pay dollar 500 for gas fees. It's definitely been interesting, but yeah, the virtual machine stuff, localized fees and parallel processing have been a massive unlock. The other thing that you talked about with eclipse was kind of an l two s. More broadly was the difference between optimistic roll ups and Zk rollups.
00:20:13.336 - 00:20:29.944, Speaker A: I believe eclipse, at least initially, is going to start with an optimistic roll up. But could you compare and contrast again optimistics versus ZK, and then go into why you're choosing optimistic rollups and how you think about ZK technology more broadly?
00:20:30.112 - 00:21:44.444, Speaker B: Yeah. So an optimistic role, just to reiterate, is run a bunch of stuff off chain, take that block of data which contains all the transactions that were ran, store it on an l one, and then also store some hash that represents the results after you finish running everything, which is kind of tricky for the Solana VM because that hash, it's not as quote unquote useful as the hash in Ethereum, because in Ethereum it represents the root of some merkle tree, whereas in Solana that doesn't quite exist in the same way, but that's like a change that we have to do under the hood to make eclipse work. But yeah, anyway, so you store that hash of the result, and the l one by default does not verify that that hash is correct. But anyone can watch. And when you put that block of data on the l one, you also put some kind of bounty, put like a million dollars or something, and anyone can re execute all those transactions themselves. And if they see that that resulting hash is not correct, then they can say, hey, someone needs to rerun this and figure out what's the truth here. And then the system will rerun the transactions, figure out who's lying, and the correct person gets slashed and whoever the challenger maybe gets rewarded, or they themselves get slashed if they call it for a false fraud proof.
00:21:44.444 - 00:22:23.120, Speaker B: So that's an optimistic role. That's like the simplest form of it. And then a zero knowledge proof is you run a program and you can almost produce like a hash of the execution trace, like a way of verifying that this program was actually executed correctly, that every instruction that was run was actually done properly, and that all the stores were done correctly, all the loads. And then after that, you put that cryptographic proof, that evidence on the l one, and it's very efficient to verify that evidence. So it's like a form of verifiable compute. Right now it's very expensive to generate that proof. It's very cheap to verify it, but over time, there's more activity.
00:22:23.120 - 00:22:24.140, Speaker B: Sorry, go ahead.
00:22:25.680 - 00:22:37.976, Speaker A: I was just going to say, could you talk about Eclipse's point of view on choosing the optimistic roll up and your thoughts on the zero knowledge? I know you mentioned that they're expensive to compute at the moment.
00:22:38.048 - 00:23:11.028, Speaker B: Yeah. So the one thing about rollups is that the network effects are far more brutal than they were for l one SDE, because the more activity you had on an l one, the fees got higher, such as like what we saw on Ethereum. Whereas for a roll up, the more activity you have, you're just posting this block of data periodically to the l one so that cost per transaction gets amortized so it ends up cheaper for everyone. So the time to go to market matters a lot. And you really want people deploying as soon as possible, especially if you have people knocking on the door trying to use eclipse. It's like we don't want to be delaying this by years or something. They're going to forget about us.
00:23:11.028 - 00:23:42.330, Speaker B: Maybe they'll decide on some other architecture. So we want to launch as soon as possible with like a working optimistic roll up, not some silly side chain or something. Because then people like, there's no reason to be spinning up a side chain of Solana that's doing consensus and being like, hey guys, deploy to us, that's just a worst Solana. There's no customization, there's no reason why they'd be doing that. Whereas like if we have the optimistic fraud proof, then it's something that's demonstrably different. So yeah, that's our reason for starting optimistic. We are building a zero knowledge vm, we're leveraging risk five.
00:23:42.330 - 00:24:11.030, Speaker B: We're doing some, making some changes under the hood. We actually have a maybe I shouldn't share, I think it's probably safe to share, but we have a proof of concept which we're going to be posting soon. But the thing is that proof takes a while to generate, like we mentioned. So yeah, it's more just like a proof of concept, nothing more than that. As the proving technology gets better and as people get better at leveraging gpu's to optimize different parts of the proving system, then it might be feasible.
00:24:13.370 - 00:24:27.840, Speaker A: Awesome. That's some big news. Congrats. Do you plan on launching the initial testnet? And then if all being well, when do you plan to actually go live with Eclipse?
00:24:28.010 - 00:25:03.372, Speaker B: So we've started giving private Testnet access to a handful of projects which we're building custom infrastructures for. We're trying to show them what eclipse is capable of. We'll have more of a public testnet over the next couple of months. So, Q one, I don't know when this podcast will be posted, but let's say Q one of 2023 and then the main net launch will be Q two of 2023 and then we'll actually decentralize the system. The Testnet launch won't have fraud proofs enabled. So the Eclipse team will run all of the nodes or some of the projects that we're working with. They might run the node for their roll up themselves and then in that case they would capture the sequencer fees.
00:25:03.372 - 00:25:20.240, Speaker B: But it's Testnet, so there's no real economic value. We have like a faucet, so you just claim tokens. But yeah, down the line. Basically once we have the optimistic settlement ready, then we can decentralize and anyone can run nodes, anyone can join the settlement layer and yeah, we'll go from there.
00:25:21.060 - 00:25:53.760, Speaker A: Could you talk about just on the topic of nodes, with Solana's virtual machine being paralyzable, it benefits from having additional cores. Could you talk about what kind of node requirements look like for eclipse? I know right now you're on Testnet and it's just going to be the team running it. But eventually on Mainnet, everybody will have the opportunity to kind of jump in. What is your vision there? Do you want like one sequencer? Do you want 101,000?
00:25:55.060 - 00:26:47.206, Speaker B: Yeah, I feel decentralizing the sequencer is a little, it's substantially less important than decentralizing an l one, given that the only real concern is that the sequencer will censor your transaction, they can't run anything incorrectly, assuming that fraud proofs work, because you can always just call for a fraud proof as a verifier. But I think it's definitely good to anyone who has skin in the game who depends on this roll up for their transactions. Maybe you're like a serious hedge fund who's running tons of volume. You should probably be running a sequencer. So I'd say the correct answer is somewhere between two and somewhere less than, definitely less than 100. There's no need to have 100 sequencers in a roll up, because unless all the hundred are run by the same person or something, at which point then the censorship concern is relevant again. So that's the requirement is going to be basically the same as the Solana execution layer.
00:26:47.206 - 00:27:15.210, Speaker B: So we'd want you to have as many cores as you want, given that's just going to increase the throughput of your roll up. And I actually have it bookmarked in my bookmarks tab, the validator requirements. But it ends up if you run it on Google Cloud, which maybe you shouldn't do, you should probably get your own hardware if you're serious about this. On Google Cloud, this costs somewhere around a grand a month. And then Google will also give you like $100,000 of credits if you want to run an eclipse node using that. Yeah, they'll subsidize that.
00:27:16.750 - 00:27:50.570, Speaker A: Cool. Yeah, I was always fascinated. I'm very interested in different hardwares and what is required to run the network. I'm personally a fan of bigger nodes because as we were talking about with parallelization, the more cores you add, the more throughput you get. And I just want the industry to scale. So I'm kind of a big block, big nodemaxy, at least at the moment. I mean, you still have to have the properties of censorship resistance and a large number of full nodes, at least at the base layer.
00:27:50.570 - 00:28:24.844, Speaker A: But having more throughput I think is very important. So maybe jumping, jumping ship a little bit. Talking about data availability, you said you and I have actually discussed this a couple of times offline. But I would love to. You said you ultimately got red pilled by Celestia. I would love to hear that story. And then how you are also evaluating other data availability layers, or maybe before that, talk about data availability, why they're important for l two s.
00:28:24.844 - 00:28:27.660, Speaker A: And then, yeah, jump into the other things.
00:28:27.780 - 00:28:54.122, Speaker B: Yeah. So just to take a step back about the lifecycle of a transaction, again, you submit a transaction to some execution layer. They execute it, they produce a block of data. Now that block of transactions must be stored somewhere, and then you have to. So that establishes the order of the transactions and what order they were run in. And then there needs to be some process to determine that those transactions or those state transitions were done correctly. So that's settlement.
00:28:54.122 - 00:29:26.840, Speaker B: So those are the pieces. There's execution, there's data availability, consensus, and there's settlement. So data availability is that second part where you store the block of data. And then as a, let's say I'm a user of the blockchain, I want to verify that that block of data was actually stored. So one way for me to do that is just download the full block and then I can just verify the Merkel structure of that block. It's an Order and Operation, obviously. Download it, verify all the branches add up correctly, and then that verifies the integrity of the block and verify it links together like the block header works on Ethereum.
00:29:26.840 - 00:30:00.162, Speaker B: A like client doesn't usually do that. It's just looking at the block headers and making sure they all link together. And if you want to verify that a Transaction was actually executed, you can give a Merkle proof. So you give some branch, you say like here's Neil and Logan's transaction, then here's the parents and so forth. Just keep hashing it together. So now you've reduced it from order n to log n to verify a transaction's inclusion. So these are the types of things that lets you do, but data availability lets you verify in a sub linear amount of time that that block was actually stored correctly and that all these branches are valid.
00:30:00.162 - 00:30:34.088, Speaker B: So that's, there's two different types. You can do an optimistic Data availability proof. There are zero knowledge, data availability proofs. So last year it does optimistic things like Polygon avail or zero knowledge. There's Eigen Da, which I think is more like a data availability committee, which is more similar to like let's say you. So that, so this is like an alternative where you're not actually giving a proof that a block was stored correctly, but instead you have some sort of like a bot. Like, let's say you're a validator that's running this network and you're supposed to store blocks of data.
00:30:34.088 - 00:31:02.050, Speaker B: How do you verify that all these validators are actually storing it and not just like being lazy and just ignoring them? So in general, it's like you give them a block, they sign, they're like, hey, that block was valid. They just keep doing that. But you basically just kind of randomly assign these bombs. And if you sign something with a bomb in it, then you lose a ton of money. And that's how they enforce that. People are actually checking these blocks, they're actually verifying that they're correct. So that's the idea behind a DAC.
00:31:02.050 - 00:31:14.080, Speaker B: So yeah, there's a variety of data availability solutions to basically ensure that people are actually storing these blocks of data. People aren't being lazy and ignoring them. And yeah, that's kind of the idea.
00:31:15.380 - 00:32:07.832, Speaker A: And the big, I mean, we're definitely getting into weeds. I think the biggest thing when I talk to people is, or less technical people, is the difference between light clients and full nodes, and the difference that each provides. Full nodes actually providing like actual decentralization in the sense that as long as one independent copy of the state exists, you can recover the ledger where light client, it's a honest minority assumption where you have to have a lot of light clients essentially agree on the state of things to reconstruct the blockchain if what, in whatever event, it ultimately went down. But I, it's an important distinction that I think people get confused by, and rightfully so. It's rather technical.
00:32:08.016 - 00:32:43.160, Speaker B: Yeah. And if someone's running a live client, they can't really verify. If you just give them one transaction, you're like, here's the result. The lie client alone cannot verify the accuracy of that because nowadays, now with some of these wallets, you can even run like a lie client in your browser. And I guess now you can submit a transaction straight to the ledger, but usually you have to depend on some RPC that's running the lite client on your behalf. Anyway, the big thing is for a lite client, you cannot actually re execute transactions, whereas the full nodes literally re executing everything. They know everything, they've done everything, and they store everything.
00:32:44.580 - 00:33:25.520, Speaker A: Yeah, perfect. The reason why data availability is so important is because the l two s, as you mentioned, take a blob of data or a bundle of transactions that are on eclipse or some L2, whether that's an optimistic roll up or a ZK rollup and settle that back down to the layer one, and you have to store that data on the layer one blockchain, inheriting its security but also gaining additional scalability properties because you're bundling those transactions into one and posting them on the layer one.
00:33:26.100 - 00:33:34.880, Speaker B: Yep, exactly. Yeah. As more roll ups get spun up, they have these blocks of data that need to be stored somewhere. So we need data availability layers to do that. Storage.
00:33:37.340 - 00:34:23.696, Speaker A: On data availability. You initially took the Solano virtual machine and you're using that on eclipse for the eclipse L2. But you and the team are kind of, correct me if I'm wrong, more chain agnostic on how eclipse will be used across these various layer one ecosystems. Does the team kind of have any point of view on which layer one that celeste, not Celestia? Which layer one that eclipse will work the best? Or are you largely just going to take a hands off approach and say, we're building the L2 and we would like to see this deployed on as many layer ones as possible?
00:34:23.848 - 00:34:45.595, Speaker B: Yeah, we're totally hands off. We don't want to have a preference toward any layer one. Just given that, I think it's really up to the developer and that's what we're really trying to sell. It's like, as a developer, you shouldn't have to pick, be biased in any direction. There's people with bags. That's really the issue. Everyone's trying to push you to use their layer one, and it's like, as a developer, you should just be looking at everything, weighing the trade off.
00:34:45.595 - 00:35:29.960, Speaker B: Some of these layer ones will be cheaper than others. Some of them will be more effective at storing data, or maybe some will provide DA proofs and others won't. Some of them will be more decentralized. So you have to figure out where you are in terms of that security, usability, decentralization, or like cost spectrum and pick the right layer one. So right now we've just implemented for the way that we do it for Solana is there's like a CTC or canonical transaction chain contract, and we are able to construct, block, or store blocks of data using that we have obviously implemented for Celestia. And then our plan is to implement for a bunch of these other DA layers, some of whom we took funding from, like polygon Eigen layer, like a handful of other protocols.
00:35:31.180 - 00:36:20.654, Speaker A: Excellent. Yeah. Going back to the core building blocks, I got super down the virtual machine rabbit hole, and then I got super down the data availability. I was always fascinated with this because to me, once you have the parallelization and the larger nodes with high core count, then it's all a data availability problem. How much data can you actually propagate through the chain? And so I've done a lot of research, really trying to take an unbiased as much as I can, even with this podcast. I want it to be unbiased as possible and learn from the builders. But I also want to be able to try to compare and contrast these individual chains on an apples to apples basis.
00:36:20.654 - 00:36:45.904, Speaker A: And the data availability layer, and being able to articulate that core throughput one, it's challenging, and I think there's the upper bound, and then there's kind of the average. And also these things can change depending on node counts or how beefy nodes are. But being able to try to do some comparison, I've had a lot of fun with, because it is a big bottleneck.
00:36:46.072 - 00:37:26.704, Speaker B: Yeah, it's really interesting. And then it's especially tricky because some of these chains aren't really designed to be storing just like blocks of arbitrary data. We have this 4844 type proposal for Solana. We're putting together a SIMD Solana improvement document, which is basically like, it'll provide a separate channel. We'll see how this implementation ends up evolving. But this is the initial suggestion, which is basically that if you're storing a block of data, it's almost like a separate channel, which is separate from the one that receives transactions. Because the transaction, the way that you send transactions, there's strict limits on the size of what you can send, whereas we want something where you can send larger amounts of data and it can really operate independently.
00:37:26.704 - 00:37:56.140, Speaker B: It doesn't have to be even the same nodes that are storing the data as the ones that are executing transactions, because these blocks of data are not composing with anything. They're just blocks of data that need to be stored. So we're submitting a suggestion like that. And some folks in Solana, I pass around a little bit, some of them are like, it seems unlikely that still go through, and other folks are like, this is a really interesting idea, and it's a great use case for these new account types that are being created and Solana. So we'll see how it shakes out, but I think it's just good to get the community's thoughts on it.
00:37:56.840 - 00:38:20.274, Speaker A: Yeah, that's super interesting. So in the Solana block, you're carving out small amount of data specifically for eclipse, but more broadly, L2 blobs to post that data. Post that data to just because it is a different data type than a traditional layer one transaction.
00:38:20.442 - 00:38:39.350, Speaker B: Right. And there are other roll ups too, like layer n, which is going to be using Solana for settlement. There are like a few roll ups that I think are going to be built on Solana, which I've caught. Some went through, through the grapevine, some have been publicly announced. So we're all going to need this anyway, especially if we want to use Solana as the DA layer if we want to provide it as an option.
00:38:41.330 - 00:39:11.140, Speaker A: Yeah, makes sense. On your website you've also mentioned layer threes. I think it's, I'm curious to get your take on layer threes and kind of, I guess after layer threes, do you think, like we'll need layer fours or what? How, how much do we need to compress data on more broadly to like hit scalability?
00:39:11.650 - 00:40:00.268, Speaker B: So, like the layer three for us doesn't really provide any additional, it doesn't provide any additional compression. It's just a matter of separating the sequencers that are being used for one thing versus another. So maybe you want like longer block times for some application you're building, some, you're doing an NFT mint and you want nice long block times. You want to have a Kyc mempool or maybe that's like a modular mempool where some amount of the mempool is reserved for people who have been kyc'dan. Some amount is for, I don't know, just anyone which is likely going to be filled up by bots and maybe want to have some sophisticated architecture for just your NFT drop. But ultimately you have some other set of sequencers that are operating on some faster block time for ordinary transactions. Maybe you're a big project who wants to run their own NFT marketplace, enforce royalties, you want to customize other, you want to pay gas in your native token.
00:40:00.268 - 00:40:07.560, Speaker B: So that's what we were thinking about for layer three is it's really just like app specific customizations, whereas our L2 is just doing the settlement process.
00:40:11.100 - 00:40:27.948, Speaker A: On customization and being able to change some of those parameters. Could you talk about what specifically customization that you do allow with the eclipse L2 or virtual machine or l three s?
00:40:28.044 - 00:41:05.744, Speaker B: Yeah, right now it's highly bespoke. So, meaning that if you want to do anything really fancy, you kind of have to talk to the eclipse team and we'll build in that feature. But the thought is that as we build these bespoke, like doing things that don't scale type implementations, then we'll work it back into the eclipse CLI and eventually it'll just be an option where someone would be like, I want a mempool in my Solana VM, which normally Solana has no mempool, but maybe they want it for whatever reason, probably for like MEV recapture or something. That's something that they could do. Or it's like, I want to charge gas in, I don't know, ethereum for some reason. Or I want to charge in USDC. Now that's an option.
00:41:05.744 - 00:41:32.962, Speaker B: Or I want to use, there's this thing called, like, gravity bridge in Celestia. So you could store a bunch of data on Celestia and reference it in the VM. It's like, oh, I want to support that. Or, I don't want to support that. I want to support that on Polygon avail or something like that. So those are the types of customizations that we want to offer, but we're leaving it kind of open ended in terms of when these big customers come to us and they're like, we want our own bespoke chain. We'll work with them initially and we give them a solutions engineer.
00:41:32.962 - 00:41:47.910, Speaker B: So it really, the way we're doing it right now, doesn't scale because we want to make sure that we have these custom implementations that are really sticky and that people really are in love with and that are solving all their problems, and then we'll work it back into the CLI later.
00:41:49.730 - 00:42:16.300, Speaker A: Makes sense. So, one other thing that you mentioned was Mev. Once you move kind of the transaction ordering and execution to a L2, that L2, and that sequencer ultimately captures the MeV. Can you share some of your thoughts around Mev and Eclipse's thoughts around it?
00:42:16.460 - 00:42:39.824, Speaker B: Yeah, I guess one way to think about it is, so there's good Mev and bad Mev, where bad Mev is. Like, I'm a validator. I see that Logan wants to make a trade. He set his max slippage to like 10% or something. So I front run the trade. I just buy it up, get him so it executes exactly at the worst price that he was okay with, and then sell it all. And I basically just scalped money off your back.
00:42:39.824 - 00:43:00.552, Speaker B: Like, that's not good meV. And then there's like, good Meb, which is like, there's two order bucks and there's like a slight price discrepancy. Now the market is less efficient economically. There's just less total goodwill. So you want that discrepancy to be resolved and someone should orbit away. So that's the kind of meV. There's other more sophisticated good meV's.
00:43:00.552 - 00:43:54.556, Speaker B: But these are at least a simple example to demonstrate how I think about it and that MeV is always on the table. It's up for anyone to capture. The question is just who captures it? Where does the value go to? And then the question to me becomes like, well, who's responsible for it? Is it the user that placed the trade? Is it, let's say it's two order books. Is it the first order book, the second order book? Is it whichever one tends to be out of sync more? Is it like, it becomes a question of like, how should you redistribute that value that's just sitting there? Or is it the validator who executed the transaction the way that it works right now? Are they the ones who should be capturing all the upside? So the view that we want to take is we want people to be able to customize that, like impose their view. We have. Like our suggestion is that it should just accrue back to, let's say your protocol, you run your own app specific roll up that your underlying token should capture, recapture all that MEP.
00:43:54.668 - 00:43:57.160, Speaker A: So it should just like Cheeto.
00:43:58.020 - 00:44:31.980, Speaker B: Yeah, exactly like Cheeto. If Cheeto had blocks, because Jito has no blocks, it's like there's continually sticking in transaction. I guess they have their own like separate RPC, from what I understand, wherever people submit their transactions there and then people can trade against it. But I feel like that probably allows for sandwiching stuff. I haven't looked into it deeply enough. But yeah, I think once you introduce blocks, then you get this really interesting difference, which is that people are basically paying not really for block space, but to be at a specific point in time. They're like, ok, this is the frozen current state of eclipse.
00:44:31.980 - 00:44:54.540, Speaker B: Then they have some time to process. They're like, oh, this is a cool opportunity. I want to pay money to capture that. And it's like, here's the next state of eclipse. Whereas for Solana right now, it's just continuously being updated. So there's not really any chance if there's a great arb opportunity or if there's like a really sophisticated one, you can't really execute on that. So that's why I think longer block times are they can allow for more sophisticated MEB strategies.
00:44:56.120 - 00:45:04.598, Speaker A: Initially, is eclipse going to also launch with 400 millisecond blocks and then have the customization on top of that, kind.
00:45:04.614 - 00:45:08.210, Speaker B: Of staying in line with Solana block time? Yeah.
00:45:08.510 - 00:45:53.024, Speaker A: Okay, very cool. Interesting. So maybe just to recap it all, could you once again in your words kind of explain why you want to move transactions from a layer one to a L2. So we've gone through a lot of the technical stuff we've talked about zero knowledge, optimistic, different virtual machines, but I always like to try to wrap it back to the user and the product stuff. Can you once again explain the benefits of using l two s? Because I do think this is a good place to wrap it on.
00:45:53.152 - 00:46:27.992, Speaker B: Yeah. As a developer building a protocol, the reason you use eclipse is now you get to customize every part of the stack. You could say this is the execution layer I want. This is the base layer that I'm most comfortable using, given where I'm at and my security, I don't know. Reliability, wherever you are on the spectrum in terms of how much you're willing to pay, so you get to swap out all those pieces. You can customize other parts of the architecture in order to recapture value that typically goes to other parties. So as a big application, like a big NFT project, now, you could run the marketplace yourself and enforce those royalties.
00:46:27.992 - 00:46:56.760, Speaker B: If you're a defi protocol, you can introduce a mempool and recapture that Mev. If you're like whatever kind of protocol you are, come to us, Chad. We can tell you how we can adjust the architecture so you can kind of push the value back up into the application, and the l one will still capture some value, but just the value becomes different and the value of the l one becomes. How good are you at acting as a data availability layer for this roll up, rather than what are all the other features that it provides?
00:46:57.500 - 00:47:14.520, Speaker A: Perfect. Let's dive into that last piece a little bit on the data availability layer. I know you said you got Celestia pilled. What are your thoughts on in general, just different data availability layers in the ecosystem?
00:47:15.180 - 00:47:49.154, Speaker B: Yeah, so basically none of them have launched yet. So that's the first thing to preface it with, which is that if you want to launch something right now, that's production. Really the only way to do it is to store it on some existing l one, which doesn't necessarily support data availability proofs. So as far as like, how people should think about what da layer they use in some cases, like if you're Twitter, for example, and you're building. I don't know why I actually hate that example, because I don't think a Twitter should just switch to like, a being on chain. I don't think that makes any sense. But let's say they did, then they're pretty centralized.
00:47:49.154 - 00:48:07.370, Speaker B: It's like they run the smart contracts, they own everything anyway. Might as well just trust Twitter to store your data. Who cares? They get all the benefits of you. Enforce the correct state transitions. If you want to audit it, you can see how the code works. But ultimately Twitter is going to be the one who stores your data because you're trusting them anyway. That might make sense.
00:48:07.370 - 00:48:48.278, Speaker B: And it'd be super cheap because Twitter's marginal cost for storing data, they have massive data centers, doesn't have to be decentralized, whereas if you're like a highly decentralized protocol, you can't compromise that for anything. Then maybe it's store on Ethereum and just pay this massive price. And if you look on optimism, for example, the transactions are still not trivially cheap. It's not so low where I'm like I'm going to just start spamming hundreds of transactions. You're still paying an amount that you can perceive. Whereas Solana, the transaction cost is so low, it's pretty much imperceivable you can spam it and it doesn't matter now with local fee markets. Maybe that's not true anymore, but at least before you could just kind of hit the network pretty hard.
00:48:48.278 - 00:49:08.460, Speaker B: So that's kind of how I'd be thinking about it in terms of what am I willing to pay? How much security do I really need? Am I already making trust assumptions that kind of undermine having a highly decentralized DA layer anyway? And if you're not doing any trust assumptions like that, then it might be worthwhile to pick a highly decentralized layer.
00:49:09.680 - 00:49:51.000, Speaker A: Makes sense. Yeah, I'm super fascinated by this. The other thing that I feel like in general, people have a harder time understanding, which makes sense again, is the decentralization and being able to quantify it. I'm personally of the opinion that decentralization is measurable in the sense that the number of full nodes matter. I think today ethereum is between 6007 thousand. Even after proof of stake, Solana is around 3200. I think these other networks, I think avalanche is like 1200.
00:49:51.000 - 00:50:45.290, Speaker A: But the number of full nodes matters just in the sense that as long as one independent copy does existential, you can recover the ledger. So I think that's important. And then the other thing is the stake distribution. So how many independent parties would need to collude to have one third of the stake weight in these proof of stake networks so they could start censoring transactions if they got over that one third threshold? So it is. I really wish, like ultimately all my research has really just been around, like, how can I kind of like, standardize some of these things and compare and contrast these things and like a non bias, like apples to apples way. And surprisingly, it's still kind of challenging to do, but the number of whole nodes and kind of the Nakamoto coefficient and then data availability layer is kind of like some of the best things that I've been able to come up with or find.
00:50:45.410 - 00:51:08.690, Speaker B: I agree. Some other interesting things that this is probably hard to measure, but what's the geographic distribution of those nodes? Are they all in the same country or are they distributed throughout the world? Yeah, stake distribution is really important. Maybe cost to run a full node could be viewed as like a potential signal for, like, how easy will it be for people to spin off future nodes? Yeah, those things are all really interesting to me, too.
00:51:10.710 - 00:51:50.936, Speaker A: So, yeah, again, coming back into this was like, you've been into crypto, but this was your first full time year, and now you're building a company or a protocol. How do you think about, like, telling new people that want to join your team? How do you, after all that's happened this past year, how do you convince people or even your team members that are already on the team that the world's not ending, the sky's not following, and that crypto will continue to go on?
00:51:51.088 - 00:52:13.192, Speaker B: Yeah, I think we've been lucky that we've hired people who are very ideologically aligned. So it seems like they're totally unfazed. Everyone's very excited about the future. I think the first thing is get the crypto sell. That's the first thing. Some, it's just like crypto in general, especially as you're making a new hire, if they don't understand the value prop of crypto, they're gonna be like, why am I. You can get into all this technical stuff, but it's just, it's kind of lost the point.
00:52:13.192 - 00:52:49.064, Speaker B: Right. You have to first get them to see financial freedom and like, why that's necessary to have the rest of our rights. And what does it mean? What does it meant for people in other countries, such as, like folks in Ukraine or Russia or like canadian truckers? Just like, there's so many examples where, like, this stuff was extremely relevant and it's actually solved real problems for people. And also just like the use case of. I don't know if they've ever made an international payment, but sometimes I just show them, like, this is literally how we choose to do our international payments because it's just so much easier. Like, this is like objectively the solution. So yeah, things like that, getting them to see like this, it's not all noise.
00:52:49.064 - 00:53:26.796, Speaker B: I think there's a lot of distractions in the news over the last couple of years, especially the last year, especially as it gained like retail attention. I think that the public is still fairly confused on many of these use cases. So I think trying to get them to see like, here's something that you could only do in crypto and trying to encourage them to see those real use cases. And we have literally lists I spend like tons of time thinking about. Here are cool consumer ideas where crypto. The first question we always answer is, why does this need to be on chain? And if there's no good reason for that, then don't do it on chain. We're at this time still where if something could be built without crypto, then it should be built without crypto.
00:53:26.796 - 00:53:57.098, Speaker B: So definitely not like a blockchain, maximalist, but aside from that. So that's the first part. And then the second part is to get them to see the value of eclipse. We just help them think about what would an ideal world look like crypto? What's the variety of use cases we want to enable, and what should the end state look like? Should it be a million l one s or should it be, we have a few l one s, and then all this application specific stuff borrows these highly decentralized base layers. And those are the ones that basically.
00:53:57.154 - 00:54:06.764, Speaker A: Went, yeah, I definitely don't think there's going to be hundreds of layer ones. I think there's going to be a select few.
00:54:06.962 - 00:54:07.740, Speaker B: Yeah.
00:54:09.000 - 00:54:40.600, Speaker A: So, no, perfect. So on the application front, you mentioned, like having real world use cases and not building something on a blockchain. Just to say that you're building on a blockchain. What applications? Because ultimately you're kind of building another ecosystem in itself. What applications? And would you like to see built on eclipse? Or what words would you say to engineers watching this to start building on eclipse?
00:54:41.660 - 00:55:38.516, Speaker B: So I'm really interested in games. I think some game mechanics that I think are really interesting is like something similar to pay to watch, where you're just constantly streaming payments to someone in order to watch them or kind of shadow them in a game, which is similar to what people kind of already do with Twitch, maybe in game advertisements where the advertiser streams to the users. All these things related to streaming payments in real time, I think are best suited to crypto just because the settlement for that in traditional finance, you have to spend a one penny transaction through like PayPal every second or something. That's just, you're going to end up paying so much in fees, it makes no sense and the settlement will take forever. Things like that, like the best parts of crypto, such as allowing people to have ownership over things that they made. So create to earn, you create an asset and then people can pay money to maybe create a map in a game, and people pay money like micro payments in order to use that map. And it returns the value to the person who is responsible for creating that content.
00:55:38.516 - 00:56:23.838, Speaker B: I think that's cool because it also solves a problem for the game studio who usually that's a bottleneck. The game studio can only produce so much now. You've decentralized it in the same way that Roblox did, but you actually reward players for doing that work. I think privacy and all the ZK stuff is really interesting. Maybe you pay for some sort of some additional privacy layer on top of whatever you're doing. And then just aside from gaming, I think there's a lot of cool, like consumer apps that can be built decentralized scale AI, people who are doing data labeling and taking these marketplaces where there's middlemen, removing the middleman, or marketplaces where you just think that the price transparency is not very good. Like, I think teleport's awesome on chain Uber, partly because like, as an Uber user, sometimes I like book a ride in like an hour in advance.
00:56:23.838 - 00:56:36.450, Speaker B: I'm like, this is wild. How cheap this, because I know it's going to rain. I want to go long, 1000 ubers and then like in an hour I want to dump it. And like, I just know like arbitrage that. So things like that I think are really cool.
00:56:38.310 - 00:57:25.400, Speaker A: Yeah. Now I'm very much excited as well for real world applications and I think now applications and protocols like eclipse, like these high throughput layer ones, I'm very optimistic that now we have that infrastructure to actually allow us and the virtual machines to allow us true scalability. I'm very excited to work with founders and talk with them and ideate on these different applications because I definitely agree they're what we need to push the space forward and reward the long term builders instead of what's happened the past couple years with unhealthy growth.
00:57:26.340 - 00:58:03.284, Speaker B: It's the best time. Right? And also the kinds of people we're seeing stick around in the Solana ecosystem are the right people. So it's like, I think it's kind of okay to see some of these. I hate to bring up the news and stuff, but just seeing some NFT products leaving, it's like, whatever, that's fine. Because the NFT projects that will take their place will sprout and they actually get the attention that they deserve rather than some project that was essentially maybe. And who's to say if it's even a bad thing, it's kind of good that people are able to move their assets around to other chains, move to ethereum or something they want to tap into that they can move back to Solana in the future. I think all that stuff is actually kind of good.
00:58:03.284 - 00:58:05.484, Speaker B: So yeah, it's an exciting time.
00:58:05.532 - 00:58:44.240, Speaker A: I do agree. It's definitely open market, I think ultimately creates the best competition. We're definitely in a fierce competition at the moment and I have kind of my point of view, but excited to see where things land. But no, it's super exciting. I really am excited for what you're building and excited that you and the team are taking kind of this long term mindset. I'm glad you're using the slana virtual machine. I think it is really in a class of its own and excited to see what people build with it.
00:58:44.700 - 00:58:57.360, Speaker B: Agreed. Yeah. Thanks, Logan. All of our conversations have been very helpful. Especially I feel like we've been talking for a few months now, even as this project first got started. So yeah, your thoughts and feedback have always been very helpful.
00:58:58.580 - 00:59:05.820, Speaker A: Well, I appreciate it and thank you so much for coming on the podcast, Neil. It was a great conversation. I had a lot of fun. Thanks.
