00:00:00.280 - 00:00:44.040, Speaker A: This is a podcast with the co founders of Jensen, Ben and Harry. If you haven't been under a rock, you've seen chat GPT take over the world by storm. And a lot of that is powered by immense amount of compute and a lot of data. Today we touched upon the founders and their thesis behind building Jensen and creating the machine learning compute protocol to power the world. I hope you enjoyed today. I'm super excited to have on Ben and Harry, the co founders of Jensen. I think we all kind of with the prolification of chat GPT-3 chat GPT four.
00:00:44.040 - 00:01:10.300, Speaker A: We're all very interested in AI and also the crypto adjacent AI world. And I am very excited for this conversation, because I don't think there is anybody else better to have this conversation with than Ben and Harry. Before kind of diving into the specifics of Jensen, what you guys are both building, I would love for you to just do a brief background on yourself and how Jensen ultimately came about.
00:01:11.280 - 00:01:44.652, Speaker B: Absolutely. Thanks, Logan. Thanks for having us on. I know it took a while for us to actually get this scheduled in. It was a tricky one, but really pleased to be here, I guess, very, very briefly in terms of background on me originally from the machine learning space. So I started out my career, I guess, in machine learning research, doing a PhD in deep learning, when it was essentially exploding for computer vision. So after Alexnet came out and we realized that, like, convolutional neural networks were kind of incredibly powerful for computer vision specifically.
00:01:44.652 - 00:02:22.576, Speaker B: And then after that, there was this proliferation of different model architectures and things, and everyone was sort of exploring the space. That was when I started doing my research. Essentially what I realized was the design of those neural networks was being done by hand. It was kind of seen as novel research to hand design a new network, but actually that was just a big search problem. And so I focused my research on automating that search problem, so doing evolutionary algorithms for growing the structure of deep neural networks. Long story short, what I realized doing that is machine learning is incredibly computationally expensive. It just requires loads of, at the time, GPU's and still kind of predominantly GPU's.
00:02:22.576 - 00:02:50.076, Speaker B: And as a relatively poor PhD student, I had about four of them that I could run my research on. And the big players were doing similar models and similar kind of explorations with thousands of them, and they'd run these things for weeks at a time. And what stuck with me from that was the realization that I had everything else I needed to do these experiments. I knew how to do them. I had the data. I just didn't have the compute and it stopped me doing these explorations. I realized if I'm in that position, so are tons of other people in the world.
00:02:50.076 - 00:03:18.696, Speaker B: Therefore we're not moving as a species towards a kind of machine learning and AI future as fast as we could be. And I really didn't like that. So that was kind of where I guess the seeds of Jensen started for me. And feeling the problem itself took a bit of a detour into data sovereignty. I co founded a different startup before Jensen ran it for a couple of years. Very ideological, it was focused on individual data privacy, ownership of data, and how we kind of interact with the world. I think all of these things in the future will kind of come together.
00:03:18.696 - 00:03:33.160, Speaker B: But yeah, I had a lot of learnings from that startup. Mostly that people like privacy, but they don't really pay for privacy. So some good startup lessons there. But when we wound down that startup, I joined in Accel in London called entrepreneur first, and that's where I met Harry.
00:03:35.820 - 00:04:46.372, Speaker C: And from my side, my background academically is in econometrics. So that's a combination of basically statistics and economics as applied to different spaces. Originally it was in the kind of financial world, laterally it was in the political world. But then after completing my postgrad, I was, or just on the eve of completing it, I was introduced to a kind of new, new class of applied machine learning model in the lab that I was working in. And I basically at that moment just knew it was what I wanted to spend the rest of my life doing. I thought there was something kind of like a bit like Promethean about it, like being shown this kind of tool which could make these far superior predictions to the current models that we were using, number one, and then number two, the idea that it was all, you know, it was like I understood it, you know, for lack of better words, it made it feel like it, like really powerful. So after the, after the postgrad, went back to London where I led the data research team and applied machine learning startup for about three years.
00:04:46.372 - 00:05:41.920, Speaker C: And it was a narrow application of machine learning to disasters and insurable risk. So as a team, we're focused on basically applying statistical machine learning models to building fires, wildfires, hurricane modeling, damage frequency severity, etcetera, modeling within the built environment. So an enormous data problem where insurers would typically model our buildings fire risk with a handful of data points if it's small, medium business. But we were able to add in hundreds more data points and some of them relatively exhaustive data points. As well, which weren't used. We tested very bizarre data points as well. I remember at one point we were modeling things in Australia, and as a joke, we just took the UFO sightings over Australia and solidly correlated with any fire damage.
00:05:41.920 - 00:06:27.896, Speaker C: That's a bit of a kind of tongue in cheek one, but you get the idea there are lots of different things. We use lots of different models. And really, through my few years there, it got to the point where I came to two conclusions. One, the compute that we are using is very expensive and in many ways a huge bottleneck when we're grid searching between lots of different models and hyperparameters to find the most superior kind of solution to one of these problems. But then secondly, that the data itself is generally like quite high value, particularly these kind of more like nuanced private datasets with respect to the built environment and physical risk. That was enough of a conviction for me to think it's a narrow application right now. But I.
00:06:27.896 - 00:07:11.700, Speaker C: We get access to more compute, or if we could get access to more compute and or more data, we could really explode this space and take these narrow predictions and make them much more general. And lastly, that's been born out with things like GPT, where it's a generalized language model. But back in 2020, in January, that was enough to make me quit my job. And I joined entrepreneur first when I met Ben and really reconnected on that idea immediately. For anyone who doesn't know, entrepreneur first isn't exactly like Y combinator. It's a bit different. So you arrive as a person having quit your job or quit your PhD, with the idea of kind of pairing up with a co founder on the program.
00:07:11.700 - 00:08:05.706, Speaker C: So some people describe it as like Love island meets Shark tank or the bachelor meets shark tank. So there's a bit of a kind of courting process, flirting process that you have to kind of go through to find your candidate. Ben and I meth just before the program launched, which was incidentally the week before the UK lockdown for Covid-19 and consequentially, we spent almost the entirety of the early days of Jensen virtual, and that's continued to today as we're a fully remote company. We connected on three things to conclude. The first was the idea that we wanted to build a very large infrastructure for training machine learning models, which would either give access to more data or more compute laterally. We landed on compute, as that was the biggest problem that we saw in the space. Secondly, we shared, although we didn't know it yet, a lot of the kind of principles which underpinned the crypto movement.
00:08:05.706 - 00:08:41.920, Speaker C: So we bonded on Edward Snowden's autobiography, which had been released quite recently. Before that, we shared many cypherpunk ideals, although it hadn't quite made the transition into crypto for us yet. And finally, we shared a sense of humor, which we've kind of said on multiple podcasts, but basically, is the, at least in our opinion, one of the absolute prerequisites, maybe one of the top set or second top prerequisite for finding a co founder, because if you can't laugh when your bank collapses during a series A or something like that, you're in trouble.
00:08:42.580 - 00:10:02.460, Speaker A: Definitely a very important quality for going through the trenches with somebody. It's not easy being a founder. And I think to each kind of touch upon what each of you kind of clicked into was that compute is hard to get, especially kind of after this chat GPT moment that everybody has kind of more recently experienced, where you have lots of amounts of compute and a lot of data, and kind of, as you kind of ramp both of those things up, you get more interesting results from these neural networks. And so it's kind of been a arms race more recently to really get access to that compute. With GPU selling for way over MSRP value, people really trying to go to whatever links to and get access to these GPU's. And so it's amazing that you two kind of foresaw this problem early on, had the foresight to integrate it with some of the crypto components that allows, really, these networks to scale and put it all together. It's super fascinating, and I think maybe to use that as a starting point.
00:10:02.460 - 00:10:21.890, Speaker A: I think Jensen ultimately was described as the network for machine learning, compute protocol, uh, bringing together, really all the world's compute. Can you maybe use that as a starting point to go further into the vision of what Jensen is and how you guys are trying to accomplish that?
00:10:22.430 - 00:11:01.126, Speaker B: Sure. Yeah. Um, maybe I can kind of say it from my perspective and then Harry can say from his. But I suppose to give, like a very brief sort of history like Jensen came about because we'd felt that compute problem like we knew it was a problem when we were doing the work, and we could see that it was a problem that wasn't being solved by the current progression of the market. Basically, the chip shortages were only getting worse, and the requirement for access to those chips was increasing. And if you picture the graphs of the two things, the lines are diverging, they're not getting closer together. One of the really obvious things we can do, humanity is make more chips, but that's a very difficult thing to do.
00:11:01.126 - 00:11:39.138, Speaker B: The supply chains are very complicated. It takes a long time to build these things. Very few people can actually build them. So then you think, okay, if it's very hard for us to make new chips, what's the next best thing to do? And that is just optimize our usage of the existing chips out in the world. And then you kind of look at the market for essentially converting a capex cost into an opex, which is what all the big cloud providers are doing. They're buying up these chips, and then they're converting those chips into this operating expense that somebody can do on demand. But when they do that, they're able to charge absolutely enormous margins because the work that goes into doing that is quite complicated.
00:11:39.138 - 00:12:40.006, Speaker B: There's a lot of different aspects that you have to cover, and those aspects are like weird costs where you have to set up infrastructure, you have to build a lot of stuff. But then there's a lot of people costs. There's like administration, there's customer services, account managers, there's legal contracts. There's all of this stuff that just gums the whole system up and allows these companies to essentially monopolize that area and then print a lot of money based on it. And what we kind of saw was this opportunity to automate a lot of that process. And in automating it, drive down the costs that come from that and essentially take away those profit margins because it's automated, you don't have those costs are starting building something like this, you can just access it programmatically. So that was the sort of the light bulb, I guess the light bulb moment was a little bit later, but it was that intuition that we can automate all of this process, and we can make it a lot smoother if we can distribute compute over all these kind of latent GPU's and the data centers that are already out there, etcetera, when we kind of dived into, how do you do that? What is the process of automating this? That was when we discovered this sort of like crypto technology, and I'll touch on it now.
00:12:40.006 - 00:13:33.962, Speaker B: We can talk about it way more depth if it's kind of interest as we go on. But basically we realized that when you have an untrusted piece of compute, if you want to connect up everything in the world, you can't trust it all. You're going to have a GPU on your network that you've never interacted with somebody who wants to train a machine learning model is going to send something to that GPU, and they're not going to know if that person is going to do that job correctly. You have to solve that problem if you want to make the network as large as it can possibly be. We realize that the way to solve that is through decentralized and crypto technologies, and that's crypto in the sense of cryptography, but also cryptocurrencies. And that was the light bulb moment for us when we realized, hey, there is now a solution to this that there wasn't before. And that's why at this point in time, you can solve this problem, you can reduce those costs, and you can make something absolutely massive, and then from there, you can extrapolate out and maybe Harry can touch on this.
00:13:33.962 - 00:13:47.882, Speaker B: But the vision beyond that of Jensen is when these environments become automatable. So right now it's loads of people interacting with computer. In the future, we don't think it'll be people. But, yeah, I'll let Harry expand on that a bit more.
00:13:47.906 - 00:13:57.710, Speaker A: And just briefly, Penn, that problem that you're touching upon is that the verification problem that is uniquely solved by the crypto aspect.
00:13:58.130 - 00:14:35.430, Speaker B: Yeah, when I refer to that kind of trust problem, that's it. You've got to be able to verify that this work done by a untrusted device somewhere in the world was actually done correctly, and that is verification of that compute. In the traditional world, that's done by trusting who you're interacting with. Aws, you have a legal contract with them. If they for some reason don't do your task correctly, you have the courts and you have all these things that you could go through, but ultimately you trust them because they're a big centralized entity. When you build a protocol, anyone can connect up a device, you're just interacting with that person. The protocol has no centralized governance or people involved.
00:14:35.430 - 00:15:01.510, Speaker B: It's literally just communication technology. What is your recourse to recovering assets and things if that work isn't done correctly? That recourse to us has to be purely programmatic automatically through essentially smart contracts. And that's what the verification system is. And we think you can't build a network like this without that. You can build a small network, but to make this global and beyond, you have to solve the verification problem perfect.
00:15:02.570 - 00:15:07.270, Speaker A: We'll definitely double click on that. But before maybe passing it over to Harry.
00:15:08.130 - 00:15:56.664, Speaker C: Yeah, I think there's. When it comes to the vision, there's two prior things to think about. First, the first thing is the idea that there's been a few kind of industrial revolutions over the past few centuries. You could say that steam was one of the first. Following on from that, you've got electricity, the electrical grid, and then thirdly and most recently the Internet as the main propellant that sat beneath a lot of the kind of advances for those respective time periods for our kind of generation. Now, the kind of big unlock is machine learning. It's AI, and the biggest kind of primitive underneath that is access to compute.
00:15:56.664 - 00:16:55.744, Speaker C: Because scaling laws dictate that the more compute you typically throw at these models, basically, the bigger you allow them to get or the more data you allow them to train on. They just keep getting better and better and better and better. So it's clear it computes the kind of the kind of core issue. And then the second thing you need to believe is that there will be a point in the kind of future where machine intelligence will reach a level whereby it is autonomous to the point that it can exist in society on its own merits, in the sense that it could create an API, which just the most basic example, let's say a machine thinks, I want to make money. It scrapes some financial data, creates an API, stands up the API, provides the data, gets paid for it, and then uses that kind of money to retrain itself to make better decisions about how to add features to the API. Just something assuming it's objective functions to get rich.
00:16:55.792 - 00:16:57.380, Speaker A: We're getting pretty Sci-Fi now.
00:16:59.160 - 00:18:08.698, Speaker C: Yeah, well, I mean, it's imminent in our opinion. So those are the two kind of priors. One, that machine learning is like v unlock for the kind of next industrial revolution that we're just starting, and secondly, that there's going to exist this kind of at least like autonomous systems we can get into like artificial general intelligence and all these other kind of things. But bare minimum, autonomous systems which exist on neuron volition, our goal is to basically be the electrical grid for that world, to be the substrate on which all the advances are built. So if you have an autonomous system and it's a machine, it doesn't have a passport, it doesn't have a bank account, it doesn't have any of the stuff that we use in the kind of meat space to interact with each other. All it has are bits and silicone, and exists in a world which is then perfectly primed for using crypto as a means of payment and as a store of value, number one. Number two, what right does it have to exist if someone else can turn it on and off? It's not really autonomous, but you can persist in the kind of crypto world.
00:18:08.698 - 00:18:42.638, Speaker C: You can exist on chain. You can exist on the chain. And we think kind of as an extension of that, that we want to make compute kind of for machines as available as oxygen for humans. We want the compute, which is the kind of fuel for, you know, the kind of intelligence that we're building just now to be available as oxygen is for us. And as a result, that's going to maximize the impact that machine learning can have. So the goal, that's the kind of the long term vision. We think that lots of models will exist in this kind of permissionless kind of open space.
00:18:42.638 - 00:19:17.882, Speaker C: And through the kind of open availability of all those models, people will be able to train the models they want, and those models will be reflective of their values. People talk about this idea like we want to deviation models. Models are biased. There are parts in models like generative adversarial networks, which are literally called discriminators. Like, their idea is to, like, you know, make decisions based on inputs. That's all that a model does. You know, any model, people in many other areas of their lives choose certain biases.
00:19:17.882 - 00:19:55.998, Speaker C: They send their children to certain schools, which, you know, align with their values. They watch certain news channels which give them the kind of, you know, reversion of events that they want. You know, they read offers who they think are actually better than other offers to consume art, which they think is better than, you know, other art. Like, they make decisions with bias encoded everywhere. We think that people should be able to build their own models and that those models should be able to exist kind of in perpetuity and serve those people, instead of having one kind of closed source like, you know, Oracle, which is produced by someone in Silicon Valley that no one, no one gets act, you know, they get one worldview. So there's a lot in there, but that's the kind of the broad tapestry of the vision.
00:19:56.174 - 00:20:52.216, Speaker A: No, I absolutely love it. And I I think the wording that really stuck with me was kind of the oxygen for general kind of AI. It's a very kind of powerful imagination or, like imagery. And you can definitely see a world that we're just now tapping into where we're on really a exponential scale of what we're going to be required for compute and training inference. And we're really just touching upon that. And so by organizing that compute, there's a lot of benefits to that, maybe kind of sticking on this topic. And then I do want to get into some of the like, real world physical limitations, whether that's like bandwidth latency, like different compute clusters, or even the verification problem that we touched upon.
00:20:52.216 - 00:21:29.450, Speaker A: What do you think, at least initially, when Jensen is kind of spun up is going to be the main driving use cases for the network? Is it going to be purely on kind of more AI related with like the training of these models or inference? Do you think just by unlocking GPU compute, people will be able to do more generic computing, like rendering jobs? Or is it kind of all the above just building the compute cluster to allow people to access that compute where before they didn't have that.
00:21:30.510 - 00:22:05.106, Speaker B: We're pretty opinionated in the way we think this should be done. So Jensen is designed to be machine learning training compute. That's the idea. We've seen quite a lot of attempts to do this for general purpose compute, where you can see the benefits that decentralization could have for general purpose compute. But when it comes to that verification problem that we mentioned before, it gets really, really hard to solve. I mean, it's a really hard problem to solve for any computation, but if you try and do it for a generic computation, it's really difficult. Essentially, Ethereum does that.
00:22:05.106 - 00:22:38.862, Speaker B: Ethereum does this for general purpose computation. It's quasi Turing complete. But the size of computation that you can do is obviously heavily, heavily bounded. And lots of people have tried to sort of expand that out. And typically when they expand that out, they try and solve the verification problem, and then they fall kind of back to something like a reputation system or like an eBay style feedback system, some kind of like soft verification done via people as a way of saying we've solved this problem. But when you do that, those systems are very, very game able. Like just as eBay feedback used to be gamed and still is gamed, you can sibyl it really easily.
00:22:38.862 - 00:23:20.720, Speaker B: You just give yourself feedback, you do small transactions, etcetera. It's very easy to kind of attack a system like that, and it erodes the value of the system to the point where it doesn't have kind of use case value anymore. So when we looked at that, and when we were looking at solving the problem for machine learning, we realized that. But we strongly believe in the thin protocol hypothesis, where very, very kind of narrow niche use cases are the way forward for building protocols. You do one thing and you do it really, really well, and somebody else will do another thing, and then the kind of space is built up by multiple protocols together, all solving these niche problems and being experts at each of those individual problems. So that's what we do. We only focus on machine learning training.
00:23:20.720 - 00:23:53.866, Speaker B: Only focusing on training is a deliberate choice as well. If you think about the computations involved in the the two main things that you do with machine learning, you're doing training and then you're doing inference. Inference as a use case is ultimately, in our opinion, a latency game. So that's when you're sending your query to chat GPT and you're getting your answer back. The end user in that case cares about how quickly they get their answer back. That's going to be the biggest determining factor for them. If you look at decentralization as a technology, latency isn't necessarily the foremost thing that it solves.
00:23:53.866 - 00:24:19.486, Speaker B: Ultimately, in the future, you could have a decentralized CDN style network, could get models really close to users and things. But that's not a problem to solve right now. It's a kind of problem for down the line. Training is very different. It's a massively asynchronous process where you need access to a core resource, which is quite expensive for a long period of time. Where you train a model for three days, you probably don't really care if it trains for another half day. It doesn't really make a difference to you.
00:24:19.486 - 00:24:56.626, Speaker B: Which is absolutely perfect for decentralization, where you can ship that task off to any one of thousands, millions, billions of different devices in the world and have it be performed out there and just asynchronously returned to you. So that's where we focus. And then in focusing so specifically on machine learning training from a use case perspective, we also have a much easier time solving the verification problem. We can do really specific tricks that you can only do with matrix multiplications. You wouldn't be able to do it with other general purpose computations. But it's nice and efficient for our use case. And we've seen that in a similar space with render network, where they only do rendering, but they do it really efficiently.
00:24:56.626 - 00:25:13.510, Speaker B: And rendering has its own quirks that make it really nice and easy to do. It's embarrassingly parallel, it's really easy to shard up, it's easy to verify so they can have that part. We have machine learning training, other people will have machine learning inference, other people have storage, et cetera. We don't really want to expand out any further. It just doesn't make sense.
00:25:14.290 - 00:25:15.314, Speaker A: Interesting.
00:25:15.482 - 00:26:12.042, Speaker C: I would add to that to referen what Ben said, the two benefits that Jensen network confers to users are price in scale. So because you kind of I guess, you know, disintermediate the cloud oligopolist who sits in the middle and extracts a rent on the hardware. By just going peer to peer, you remove margins which can touch 80% gross margin on some of this cloud. Compute. It's not typically common that, you know, people are consuming a computer that has the kind of, you know, those margins like a lot of people like will strike deals and stuff, but you know, it can be eye watchingly expensive. Similarly, because it's a global network of devices, you aren't basically limited to a singular provider, number one. But number two, you kind of expand the pie of available devices.
00:26:12.042 - 00:27:33.228, Speaker C: So obviously in the constraints of high latency environment whereby you're training models that, that will comfortably take over a day to trade and you can deal with that latency. Suddenly different devices become available to you, right down to things like M two MacBooks. And this brings back in to view this idea of almost like mining bitcoin on your laptop, where you can actually now use devices in a way that previously weren't used to provide some economic value to the world and in a permissionless way and get a return it. So those are the kind of two benefits. And we see those kind of basically come, they kind of map onto different types of users, you know, from researchers at universities who don't have access to high performance compute clusters. And they, you know, they want to, they want to do certain trainings, but they can't afford it or they don't have access to it. You know, it could be a stipend issue, it could be an institution issue all the way to startups, which are building large models and, you know, they're trying to compete in this arms race whereby I kind of the kingmakers like, you know, AWS might invest in a company or Microsoft might invest in a company and they provide, you know, this funding which, you know, for a few smart researchers is essentially kind of, even with VC funding is kind of just, you know, insurmountable.
00:27:33.228 - 00:28:13.946, Speaker C: It's just hard to beat in a raw kind of, you know, firepower sense. But then we can come along and we can say, well, we can actually remove this huge margin that you would have been paying otherwise. And suddenly now, you know, you maybe get up to like five x more, compute like bang for your box. And as a consequence, there are $100 million of funding now, essentially $500 million. And that allows them to fight with the heavyweights. And there's a variety of other kinds of use cases in there as well, including some crypto ones, whereby people want fully transparent and decentralized training flows. We tend to focus a little bit more on the web too, so to speak, use cases purely because that's where the absolute lion's share of trading is happening.
00:28:13.946 - 00:28:16.470, Speaker C: And it's also the worlds that we come from as well.
00:28:18.650 - 00:28:54.886, Speaker B: Sorry, I just wanted to add to that the crypto space, I think, has been through. It's obviously been through multiple waves and things, but it's had a reasonably consistent problem where everyone within the crypto space is very ideologically aligned with crypto. They want to decentralize these things out. They want access to perfectly permissionless access to resources and things by people. The problem is, as a value proposition position for a network, that ideology doesn't kind of capture the minds of everybody outside of crypto. It captures this small group who are really interested, but it can't expand out. We believe in that ideology.
00:28:54.886 - 00:29:33.868, Speaker B: We think it should be available to everyone. We view Jensen as essentially like the open source equivalent of access to compute, where it can't be fully open source because there's a base cost to compute that you have to pay. But if you can drive that base cost down to the absolute fair market value of the resource, then you get the equivalent of open source for that resource. But you can't just have that as the value proposition. The crucial thing for Jensen is the two value propositions Harry mentioned are unlocked by decentralization. And their value proposition is that it doesn't matter if you care about crypto. If you're in the web two world, you're a machine learning engineer, and someone comes along and says the compute that you're accessing is now a fifth of the cost.
00:29:33.868 - 00:30:06.762, Speaker B: You're very interested in that. You don't need to know why it's a fifth of the cost. You just know, hey, this is so much better for me. Think Jensen is one of those use cases, and I think there are other ones, and I think the kind of next wave of crypto is going to be discovering these places where decentralization has a huge value proposition within the web two world, completely functionally separate from the actual decentralization itself, but enabled by it, where we're going to capture out these lots and lots of users, and then as a kind of downstream benefit, we get the decentralization out of it. And that's a really powerful thing.
00:30:06.866 - 00:31:07.530, Speaker A: And I've been hearing that a lot more from builders, I'd say more holistically. I think we're going from the early adopters of crypto to the early innovators that will take us to the vast majority. I think it's really powering or using the crypto rails to unlock a new capability that was impossible instead of just being blockchain for the sake of blockchain. And that unique difference I think is truly the thing that's going to kind of take the entire industry to the masses as we learn how to actually effectively use the blockchain for things that it's good at. But maybe to double click on a couple of each of your points. One, I would say primarily focused on training is interesting in the space. I've talked with a couple other teams that are trying to do either just the general purpose compute, such as cosh, or just focusing on inference.
00:31:07.530 - 00:31:58.570, Speaker A: And I would love to ultimately get into kind of the cost segment as well. And I think maybe this line of questioning will get us there. But in terms of kind of real world modeling and creating these very large systems, typically it is my understanding that you have a large compute cluster of GPU's. Typically those are bound by like Nvlink located in a specific server farm in some part of the world. How I guess by accessing thousands or millions of decentralized GPU's and not having kind of that direction linking in between GPU's. How do you deal with some of those problems, even if they are asynchronous?
00:31:59.070 - 00:33:11.068, Speaker C: Yeah, so it typically, yeah, it typically comes down to the framework that you use to train the model. So I mean like the kind of the kind of, when you get onto brass tacks, like the interlinks are obviously far slower. So you have to do, you have to do more training on a device and you have to basically to some extent manage the bandwidth if you're using, depending on the Internet connection you're using for by, you know, only send parameter updates to some centralized model, some centralized kind of version of the model which is being maintained by the whole network. That's like point one. Some protocols, of example, bittensor, they steer directly into locking it to only one framework. They do decentralized mixtures of experts which came out of the Yandex research labs and that's a good example of this. But it's their whole protocols just customized on that one, or at least currently just customizing that one type of framework.
00:33:11.068 - 00:33:31.880, Speaker C: We want to be framework agnostic, but with the knowledge that if you're building large models which are typically going to be kind of multi GPU, as you said, they typically have these huge clusters that are trained on, then we'll need frameworks which kind of operate over those devices? I don't know, Ben, if you want to touch on some of those frameworks.
00:33:32.380 - 00:33:58.686, Speaker B: Yeah. I think the crucial thing to think about is what Jensen does is change the base cost of certain resources within the system. So you think about the simplest ones. You've got compute as a resource, and you've got communication as a resource. And in a traditional cluster, you've got infiniband and a data center. You've got thousands of GPU's connected up. The kind of resource cost for the person using that is compute is relatively expensive.
00:33:58.686 - 00:34:38.352, Speaker B: Communication is then relatively cheap because of the fast interconnects. So you build a system that uses those to their optimal degree. So you do a lot of communication because communication is cheap in comparison to compute. And you're kind of trying to use compute in the most efficient way. When you look at Jensen, what we're doing once we build the network to its maximum size, you've brought down the cost of compute drastically. So compute is way, way, way cheaper. You can access it at a vastly decreased cost per whatever unit you want to call it, of computer flop, maybe machine learning, you can call it other things, but essentially that base cost has gone down, but the communication cost has gone up because of the interconnect sas lower, like Carrie described.
00:34:38.352 - 00:35:04.924, Speaker B: But when you look at that from a distributed systems perspective, we have a lot of things at our disposable disposal to trade off for communication. And that's all this process is. It's looking at that and saying, okay, we've got a new environment now where the cost of compute has come down, the cost of communication has gone up. Let's do like optimal scheduling over that, and we can use those resources to their best degree again. And that hasn't really been done very much. It's been done a little bit. Like Harry mentioned, the Yandex lab have done some of it.
00:35:04.924 - 00:35:43.500, Speaker B: Stanford have got some great research in it. And ultimately, when we look at that whole space, we think there's, there's kind of two main paths forward. One of them is a short to medium term, and one of them is very long term. The short to medium term is treated as an optimal scheduling problem. So weight the cost of those resources and then send parts of the model to the compute that has fast interconnects where you need it to have fast interconnects, do more computation where it doesn't have fast interconnects, and generally kind of come to an overall optimal point when you're training over this, like heterogeneous latency compute. So you can do that right now. There's some good research that shows doing that over volunteer compute.
00:35:43.500 - 00:36:20.244, Speaker B: Essentially long term, like Harry said, we think there will be completely new models. So something like decentralized mixture of experts, maybe even decentralized mixture of experts, could be the way forward. The crucial thing is, as Harry said, we don't want to make a bet on that. We don't want to say we think this will be the right framework. What we want to say is, whatever happens underneath this, you need compute, and you need to do that trade off of communication and compute to build a network that covers the entire planet and beyond. And the kind of key reason behind that is something we touched on earlier, where the scaling hypothesis still holds for machine learning models. The bigger you make them, the better they are.
00:36:20.244 - 00:36:41.722, Speaker B: The spaces progress down. Let's make bigger data centers, because this is the way we build models right now. Let's add another thousand gpu's, and we'll make a bigger model that's getting more and more expensive. There's fewer places you can actually build a data center that can host enough GPU's. And our answer to that is, okay, stop this arms race. Stop trying to build ridiculously large data centers. Let's just connect everything up and use it then.
00:36:41.722 - 00:36:51.754, Speaker B: Yeah, we're gonna have to use it in a slightly different way. But when we do, instead of adding another thousand GPU's to your model, you've just added millions, which just blows everything away in theory.
00:36:51.922 - 00:37:23.922, Speaker C: Yeah, exactly. It requires like a shift change in the way you think about training models. But it kind of answers the natural conclusion of more like, how do I get more like? Because if you give someone a cluster, it's kind of like. I often think that the GPU's are like bones. And if you look back at models, it's like the fossil record. It's like how big the dinosaur was. How big the model was, was based on the GPU's, like the bones, you know? So when it was like, you know, people were training on the v 100s, now they're on the a, you can kind of see the models getting bigger and bigger and the clusters getting bigger.
00:37:23.922 - 00:37:53.346, Speaker C: But, like, eventually just human needs are kind of unsatisfiable. It's going to get bigger and bigger and bigger, and it's going to have to come out the data center. There's stuff where people are even trying to optimize it within the data center. So Ben spoke about the team at Stanford. There's good stuff out of UCLA, which takes stuff like g pipe and turns into stuff like pipe dream. The bamboo paper, if anyone's interested in reading it, covers that. We're actively researching how to build these large, very large decentralized networks.
00:37:53.346 - 00:38:08.490, Speaker C: And we'll probably have various frameworks available on the Jensen platform which are useful. As Ben said, one of them might be decentralized mixture of experts, but there's definitely multiple other ways to do it. But it just does require a kind of shift in thinking, so to speak.
00:38:09.150 - 00:38:52.820, Speaker B: I'd love to just add to that. We would love to speak to anyone who's building these frameworks, if we can tailor Jensen. Because one of the traps for building a network like ours is you have to make certain decisions when you're building this protocol. And some of those decisions could cut off avenues of research, and we don't want to cut off any avenues. So basically, if anyone has any opinions on how to build a decentralized model over enormous amounts of compute, we're very open to talk, collaborate, run experiments, just so that as we're building Jensen, we enable that use case alongside enabling all the other use cases. And you can think about like competition in our space and like you've mentioned, a few kind of players doing things. Ultimately, we think there's loads of opportunity here.
00:38:52.820 - 00:39:08.436, Speaker B: People doing optimal models for the new infrastructures can sit on top of Jensen. We would absolutely love that to happen. So we're open to collaborating with absolutely anyone who is kind of building these new types of models or new optimal scheduling or anything like that. Please come and talk to us. Basically, yeah.
00:39:08.468 - 00:39:25.832, Speaker C: And you don't need to be a machine learning person. A lot of the best ideas come out of the distributed systems world where these are problems that already exist, but like in different kind of vertical. But they're becoming obviously, given the topic of this discussion, exceptionally important for machine.
00:39:25.856 - 00:40:05.030, Speaker A: Learning and maybe double clicking on bringing the cost down for decentralized GPU compute. Is that really just accessing idle like three thousand eighty s forty eighties that people have in their homes for gaming computers or now even laptops, as you mentioned, MacBook Pros have rather beefy GPU's, ps four s, ps five s. Those type of things that historically have just been local to kind of someone's home now get added to a network that can then be tapped in to start training models.
00:40:05.530 - 00:41:02.188, Speaker C: Yeah, I think before answering that, it kind of like a kind of walkthrough history kind of frames the discussion quite well. Like if you think about the other industrial revolutions, if someone said back at the turn of the 20th century, if we made the electrical grid cheaper, let's just say we reduced the priceless of energy somehow. Or in the late 20th century, if the US had leaned into nuclear power more, and there's more power, there's more scale. Or if Andrew Carnegie and his associates had made the Bessemer steel process earlier and better, things like that. Have you said more of these kind of core resources? And they were cheaper and there was this higher scale of them, the world would be so far ahead, given the compounding returns you would get on those advancements. So we really think that getting the price as low as possible is as cheap as absolutely possible, is the core view here. We do it by three different ways.
00:41:02.188 - 00:41:51.284, Speaker C: The first is you disintermediate people charging rent a. This has happened to a large extent in the web two world with companies, quite vanilla example, like transferwise. So transfer wise, international payments, disintermediated banks to some extent, they kind of took away the kind, or they intermediated banks, depending how you look at it. They basically took away this kind of huge fee structure which was being charged on both sides by banks, and they made it much cheaper to send money internationally. And then in many cases, that's become even cheaper with crypto, where you can send even larger sums internationally. So obviously permissionless as well. When we think about it, that's a good example of someone stepping in, reducing the fees, taking it away from oligopolists.
00:41:51.284 - 00:42:21.356, Speaker C: Of course, now they're the kind of oligopolist, and they can increase the fees. So it's whatever. We're building a system which can't be turned into an evil thing. It's a crypto protocol, it doesn't charge a fee fee. All it has is peer to peer. And of course, the person on the other side of the transaction, someone renting out a GPU, they can choose the machine learning job, they can have a price, they'll accept, et cetera, but it'll be significantly cheaper than that price, plus some enormous gross margin charge on top. So that's kind of .1.2
00:42:21.356 - 00:43:15.706, Speaker C: is accessing. As you kind of said, this sort of long tail of dormant and kind of of underused or like unused compute increases the size of the PI significantly. Obviously, the more you go to that type of stuff, you then come in with even heavier kind of limitations, with the bandwidth issues, et cetera, with the available memory on the machine. If you've got a gaming PC, that can be problematic. But the idea is, by increasing the size of supply, you create an overall depression in the price of computers. And the third and final example is our longer term vision is that instead of there being where's the compute going to be in the world, it'll be highly decentralized. But we think it will kind of go the way of bitcoin mining to some extent, whereby you'll have these kind of, I guess like mines set up which are mining kind of machine learning models.
00:43:15.706 - 00:44:08.826, Speaker C: The word mining is probably wrong here, but like just using it for simplicity. Mining machine learning models, they're sat over a geothermal vent in Iceland somewhere, where they're getting ultra cheap electricity to do it. It's 100% customized for, you know, the structure just for training machine learning models just on the network in a permissionless way. And then these data centers will then encourage like downstream, an industry of people building ultra specialized, you know, hardware just for trading in these protocols, which might look slightly different to the hardware today. So then you kind of get this, it's kind of like the ASIC industry for, you know, antminer and stuff like that. For bitcoin, I will say that like, you know, building machine learning chips is a pretty hard task. And there's various companies in the space, you know, who work in this area, like cerebras and graphcore and stuff, have been really super valiant efforts over the years.
00:44:08.826 - 00:44:16.426, Speaker C: And so this fair point is way easier said than done. But you know, it's definitely long term, our thinking more customized chips.
00:44:16.578 - 00:45:26.752, Speaker B: This is one of the piece to that because Logan, you mentioned like the gaming PCs. And obviously, as Harry said, we expect this progression to be very specialized, similar to how bitcoin mining went, where there'll be a lot of optimization of where machine learning is done over the planet. It be done in nice places where renewable energy is and energy is cheap and things, and then it proxies energy markets in a similar way to bitcoin did. But there's another axis, which is AI for individuals is ramping up incredibly quickly. And what we expect to happen is almost a progression of when we kind of like back in the day when like Microsoft and various other people were saying there'll be a PC in every home, the future in our minds is there'll be a brain in every home. There'll be a essentially kind of AI system that is running that home. And each individual person is going to have personal kind of companion AI models that they interact with on a day to day basis that they ask questions to, that they kind of like learn quicker through all the ways that I think at the moment, relatively technologically literate people do with chat GPT, but much, much wider.
00:45:26.752 - 00:46:02.064, Speaker B: It's going to get integrated much more deeply into what we do. It won't be just a chat interface, it will be inside the UX of the apps that we use. But in order for that to happen, it's going to be driven into the home. And there will be device manufacturers who put a big machine learning capable device in each home, like centered around your kind of home chat device, like a Siri or an echo or something like that, or centered around your tv or your entertainment. But somewhere there's going to be this big device. That device is going to be expensive. To get that device into every home, it's going to cost those people, those homeowners, a lot of money.
00:46:02.064 - 00:46:40.314, Speaker B: Jensen presents a way that we can optimize usage of all of those devices and essentially bring down and subsidize for the homeowner the cost of that device. Because when they're using it, they can be using it, but they could be using millions at the same time. So when their models training, it could train over the local neighborhood. And then when it's not training, it can be being used by the local neighborhood. So it becomes this like smoothing function over all of these devices and essentially makes them affordable to everyday people. It's a little bit like how mobile phone contracts gave us all iPhones. Like back when the iPhone came out, the idea of dropping like a grand on a phone was absolutely insane.
00:46:40.314 - 00:47:02.190, Speaker B: But getting a 30 pound a month contract not quite as insane. So it's that kind of like way of getting it into the hands of people that actually gets it further out. Jensen can do that similarly to phone contracts, but without you having a contract with this centralized entity. Instead, you're just using a peer to peer network to earn money when you're not using your device, and then use other people's devices when you're not earning money.
00:47:02.650 - 00:48:00.516, Speaker A: So just to reiterate, kind of the three top points ultimately kind of dismetate, dis mediating the current monopoly of GPU oligarchs, a little bit of tapping into kind of the home GPU's and the future, the larger compute clusters that are going to be needed to power kind of our everyday devices as AI becomes more prevalent. And then the third is again tapping into larger compute clusters that maybe have the NVlink or idle GPU's or special purpose built for these decentralized compute networks that are long term and in some form or fashion all of these three are progressing simultaneously. And while Jensen is ramping up that the network ultimately will be able to add more compute over time, allowing bigger models to be trained over time.
00:48:00.708 - 00:48:32.814, Speaker B: Exactly. Jensen's agnostic. Jensen is designed to be really low level technology. So we think about it as like a sliver above the electricity itself, where all we want to do is network up all of this, compute for this purpose, and that's it. So as much compute as we possibly can, we talk about it as anything that's machine learning capable should be able to run Jensen and be able to move bits of models around and do that training. We don't decide it has to be these specific devices. We want it just right at that lowest level.
00:48:32.814 - 00:49:10.970, Speaker B: Which is why when it comes down to the kind of the optimal way of using that network, we're a little bit more sanguine about that because we like an ecosystem built on top of it that figures out loads of different ways of using this compute. And we just sit underneath it like a protocol in the web one sense, like TCP IP or HTTP or something, like it's a communication rails. And then on top of that, if somebody thinks, hey, I can build a large language model using decentralized mixture of experts in a super optimal way over that network, please do. Go ahead. Same for a computer vision model, same for an audio, any modality. And we think there'll be a rich ecosystem of this stuff that other people can build. We'll just be there kind of behind the scenes.
00:49:10.970 - 00:49:15.630, Speaker B: Everyone will forget we exist, but we'll be kind of like the rails underneath everything.
00:49:15.970 - 00:49:18.802, Speaker A: Yeah, maybe I'll just go ahead.
00:49:18.826 - 00:50:08.634, Speaker C: Sure. I'd add one more thing, which is just a lot of people, they kind of view the world through, through the current centralized paradigm of like, you know, everyone runs everything in the cloud, but, you know, in the early two thousands and, you know, nineties and stuff, the idea of running stuff in the cloud was like heretical. You know, it was like it was on Prem. And, you know, the success of the cloud giants today and, you know, other providers who are adjacent to them is really, you know, kind of by virtue of them managing to pull all this stuff out, you know, out of the edge and into like a centralized place. But there was huge resistance. So we see this as kind of like the natural kind of evolution again, where it was originally kind of, you know, decentralized and they got centralized, not something decentralized again. There'll probably be a push to make it more centralized.
00:50:08.634 - 00:50:35.120, Speaker C: You know, like it just, it kind of comes and goes, and there's a certain degree of like myopic thinking, I think, from some people in the space when they kind of like, we've heard it all right, you know, we have like a public light paper and, you know, you hear all, you hear all the kind of critiques on it. And it's interesting kind of framing it relative to the kind of web one world where things were very much decentralized and there are a large number of people whose livelihoods actually depended on things not moving to the cloud.
00:50:36.940 - 00:51:34.690, Speaker A: Maybe. Last question, just kind of on the technical architecture. And then I do want to move forward to the verification problem that we've kind of highlighted in the beginning in terms of, I think not an investor standpoint, but some of the biggest critiques when I talk with various people in the industry is that that compute offsets of cheaper compute cannot be, will not get to the point where it's cheap enough to offset bandwidth costs or maybe even those bandwidth costs just cause higher training times. Can you maybe address those concerns of people that either think bandwidth is going to be too expensive versus the compute, or think that training these models in a more decentralized way is just going to be much harder than kind of doing a large server farm?
00:51:35.950 - 00:52:54.420, Speaker B: I think some of this is like collectively, I think when it comes to machine learning, we have a weirdly short memory. And when you actually look back over the progression of machine learning, these centralized clusters of multiple GPU's, all with super fast interconnects, haven't really been around for very long. It's only been a few years as we've progressed on this path, to say this is the best way of doing things, then if you dive back into the history of, and I say history is literally a few years, but why did we go down this path? And then you think, well, we went down this path because the research teams at large organizations thought, hey, we can make the models bigger if we put some GPU's together. So they did, and then they had a load of GPU's that were together. So they thought, what if we build a better model over these GPU's? And then they did that, and then they thought, well, what if we had more GPU's? And then essentially, if you look at the entire space of how we could have progressed in machine learning research, we took one path through that space, and that path was driven by research organizations that had the resources to put together a cluster to initially kind of show, hey, making it bigger makes it better. And then they did research for those clusters. And it really siloed the development of machine learning down that one path.
00:52:54.420 - 00:53:33.264, Speaker B: But there's lots of paths within a research problem where you go down them a little bit and then you pull back a bit and then you go down a different path and go way further. And that's what we think is happening here. We've gone down this path of centralized clusters being effective, and we're starting with slowing and slowing the program just because it's much harder to create a bigger cluster now. Maybe it's time that we just walk back that path a little bit, do a different path, which is let's connect up all the devices and just build over that. And then we explore that. And we're very, very bullish on that path, particularly because of the point of when you're in a, you can kind of see research as like any search problem. You have exploration phases and you have exploitation phases.
00:53:33.264 - 00:54:10.260, Speaker B: Exploration is where essentially in like a big evolutionary kind of search, you would have loads of different like, things zooming all over the place, trying to map the whole space. Once you've found an area that's compelling, you zoom into that area. You need fewer, doing much more deliberate steps. We think machine learning is still in an exploration phase. We should have as many people as possible building as many different weird and wonderful ways of doing this as possible. And the way to do that is to give out access to a resource like this to way more people, rather than continuing down this siloed path where very, very few people can build models for those centralized infrastructures. Which means that we're just not exploring as much as we could.
00:54:10.260 - 00:54:15.392, Speaker B: We're in exploitation when we should be in exploration. So pulling back, going a different path.
00:54:15.496 - 00:54:44.666, Speaker A: Incredibly compelling to us, and ultimately making the whole AI ecosystem more open source in that project. I think it was Peter Thiel that said, ultimately, something to the fact is AI is kind of like centralizing and crypto is decentralizing, but really what you're trying to do here is have the best of both worlds where you can really give access to that decentralized computer to the many instead of the few.
00:54:44.738 - 00:55:43.780, Speaker C: Yeah, there's a core kind of final point here, I'd say, which is maybe a bit like philosophical, but you know, throughout human history, we've been very like most people over, you know, in most kind of recent, like, civilizations have been pretty comfortable with the idea that the biological neurons in your head kind of belong to you and your thoughts belong to you and that, you know, you're, your ability to like you know, reason about things should really be, you know, between your ears. Like it's. It's all you, obviously, there's lots of sensory inputs from all over the place, you know, and lots of people try to bias you and condition to do things, sure. But it's all you inside. We've entered this weird kind of like, kind of slightly transhuman world just now where we're getting very close, literally close, you know, with brain machine interfaces to extending our minds with artificial neurons. In a sense, we already do it. Things like autocomplete on the phone.
00:55:43.780 - 00:56:49.174, Speaker C: It's kind of there, but not directly, not in the, like, blade runner sense, but we will get there really soon. And there's a question everyone has to ask themselves, which is, who do you want to own the artificial neurons which are going to fuse with your biological neurons? Because it's going to happen. And we're going to have brain machine interfaces which process signals from our brain and make decisions based on it. And someone might control the weights in those neurons. Right now, there's various regulators around the world who are pushing for either them to have ultimate say over what can go on those models, which is our absolute nightmare. That's, you know, just the amount of control that would provide them, the amount of, you know, the amount of misuse which is capable there is awful. But then secondly, we have a second shot of, you know, kind of AI oligopolists who also want to control it.
00:56:49.174 - 00:57:39.340, Speaker C: And, you know, we go back to that point about bias right at the beginning, bias, you know, it's up to you to determine your own biases. You know, you don't want someone else to determine them for you. You have to think critically. But, you know, if you can ask chat GPT a question about something which someone in Silicon Valley deems politically incorrect, you're losing your ability to reason. And, okay, whilst that's outside of your head, maybe it doesn't matter so much, but if you're going to fuse it with your mind in some way, in a more direct way, suddenly you really do have the reason to care about it. And right now, the battle for open source is the first battle on a long path to ensuring that work isn't going to go the dystopian way that we painted previously there. And that starts with things like the EU AI act, various things going through Congress just now.
00:57:39.340 - 00:57:56.706, Speaker C: The litany of AI dumarism around the world's going to Armageddon. And all this type of stuff is going to come from allowing open source models, which is just so patently false. Yeah, it's just a philosophical point we're thinking about. Slightly long term point, but worth.
00:57:56.778 - 00:58:29.950, Speaker A: No, it's very beautiful. I think you don't really even have to squint too much to see the things that Elon's doing with neuralink and how AI is going to be more evolved in our daily lives. And you really want either that compute, that's running those neural networks to be decentralized or local. You definitely don't want it to be on someone else's server, where they're kind of controlling, especially in the neural link scenes. Your individual thoughts.
00:58:30.450 - 00:59:04.338, Speaker B: Yeah, I think when we think about, it's like Harry mentioned, that kind of general human progression of just making our minds more effective. And you can trace it all the way back to writing. Writing allowed us to offload memory so you don't have to keep everything in your head anymore. You can write it down. And the sort of conversation we're having right now is, do we write it down on a piece of paper we have, or do we write it down on a piece of paper that's owned by, like, one company in the world, and they go and put that in a big warehouse somewhere and read it, like, whenever they want? Like, that'd be absolutely insane.
00:59:04.514 - 00:59:10.510, Speaker C: And it's the same thing with a pen that sometimes turns off if you're writing about a bad word, or, like.
00:59:11.970 - 00:59:36.314, Speaker B: Exactly. It's crazy when you think about that. And it's like it's this continual progression of, we've made it more and more efficient. So instead of writing, we now have conversation with chat GBT, which is just so much more efficient than is writing something down. And like Harry mentioned, and you mentioned, Logan, in the future, we'll have brain computer interfaces, which are even more efficient. We'll be putting more information in it. We're basically making our own diaries that become an extended memory of our brains.
00:59:36.314 - 01:00:13.640, Speaker B: And then we're having the choice now, like we said, of, do we just hand them to this one person, or do we own them ourselves? And I think, crucially, on the penniless, we're benefiting as humanity in collective progression of this stuff. We wouldn't want everyone to build their own pen and only use that. We would want to sort of collectively design pens together and then be able to use the different pens that other people have designed, because then we're using the best pen. That's, to our mind, why we need open source AI, because then we're all inputting into the pens that we use. If I pick a pen and use it. I know that it could have been audited by anybody in the world. World.
01:00:13.640 - 01:00:37.100, Speaker B: I know an academic somewhere can look at that pen and can say, hey, something really weird going on inside that pen. Maybe you shouldn't use it, rather than it being just this, like, completely foreign object to me that I just write with. And like Harry said, every so often, like, the words don't actually come out and the ink stops and then maybe it writes something that I didn't write and I don't realize. And, yeah, all this kind of weird stuff. We've got this open auditability where somebody can tell me if that's happening and I can pick a different pen.
01:00:38.080 - 01:01:12.428, Speaker A: Yeah, it is. I feel like we could talk for hours and this kind of opened up a new can of worms that I haven't put too much thought into. But you can definitely see that future just in the sense of time. I do want to kind of like, in the final, like five minutes, double click on the verification problem that each of you kind of mentioned and highlighted as the core bottleneck to actually scaling these decentralized compute systems, verifying that the model has run and run correctly. Can you touch upon that?
01:01:12.604 - 01:01:46.220, Speaker C: Sure. So the verification problem kind of comes from two places. Firstly, what are we trying to verify? Basically, you train a machine learning model. The output of a machine learning model being trained is basically a bunch of tensors, which are which basically a bunch of arrays which have, have values. And those values, when used in the network architecture, produce more favorable outcomes than a random set of values. So that's what you want. That's the prize.
01:01:46.220 - 01:02:43.524, Speaker C: And that's what, when we say open sourcing models, typically we talk about open sourcing the weights, better weights. So the idea then is, okay, how do you confirm that the weights that you ended up with are a reasonable conclusion from the training process? So a very, like, trivial way to do that would just be for two people to run the training process where one of them is trusted, and then it says, yeah, okay, this is the same result, you know, whatever. But if one of them is trusted, then you don't need the other person. So immediately, like, the idea of decentralizing, it requires, you don't want to have 100% replication, you don't want to have someone just, or you don't want anyone centralized. Who's can they give in this kind of arbitrary authority over anyone else else? So you solve this by getting around two issues. The first issue is when the model is being trained. Firstly, it's state dependent, which means as the training process proceeds, it requires the previous section of the training process to update.
01:02:43.524 - 01:03:21.160, Speaker C: So models update their parameters cyclically. You need the previous parameters as the starting point to give an x parameters. That makes it quite difficult when compared to stuff like rendering, whereby if you have an image you can imagine, you can just pick and someone renders an image, you could just render a tiny bit of the image yourself, take that tiny bit from their renderer and compare it and say, okay, it's the same. I'm going to trust that the rest of it's the same given that they didn't know which part I was going to audit. And then you can make that game theoretically secure. That quality of the image being kind of split up into those different chunks is called something being embarrassingly parallel. That isn't the case for training machine learning models.
01:03:21.160 - 01:04:20.414, Speaker C: That's first point. Second point is, even if you overcome the embarrassingly parallel process, you still have to actually redo part of the work. And one of the biggest advantages of machine learning in the 2010s was a lot of the hardware acceleration which happened on the GPU's, basically making models train faster by doing essentially like for Nvidia, like Cuda operations faster, basically doing matrix multiplications faster, finding a bunch of ways to do it. That's come at the cost of determinism in many cases. So basically the idea that if you put the same inputs for the same model model on the same device, it might result in different outputs, which in the context of a decentralized network where you're sending around hashes of things, is awful because it means that if there's one bit out of place somewhere, you're screwed because the entire hash is different and you can't compare them. We solve that by fixing, number one, the reproducibility problem. So we rewrite certain kernels on GPU's compilers.
01:04:20.414 - 01:05:25.758, Speaker C: We also work to basically change at various levels, like the framework level and the system level, various parameters on the nodes which are running our software, such that the results are bit wise reproducible both on the same device and between devices. Obviously it's a bit like, as you add in more exotic devices, it becomes a bit harder. But certainly there's a lot of Pareto kind of laws with respect to the devices which are being used. So we fix the reproducibility problem, and then we use a kind of customized checkpointing and replication scanner in addition to a proof scheme, a polynomial interactive proof scheme to basically check the work in a way which is optimal given the bandwidth constraints, such that you can chunk the work up into slightly smaller sections, not totally atomic sections, but smaller sections, and then those sections can then be tested by a third party. And then the kind of result of those two parties doing the computation can then be rolled up on chain and checked by the chain. And with that, you get a game theoretically secure verification process for the Nash equilibrium, as nobody cheats. It's very complicated.
01:05:25.758 - 01:05:38.690, Speaker C: Obviously, it touches on, like, lots of different areas, from distributed systems to cryptography to GPU hardware, kind of. But, yeah, I don't know, Ben, if you would add anything to that.
01:05:40.390 - 01:06:40.100, Speaker B: I'd probably just add one thing, which is when we kind of describe that it is complicated, a lot of the complexity comes down to just having a kind of real world perspective and saying, whatever happens, this has to be efficient. So there are kind of theoretical ways that you can solve this, which in a perfect world would be the kind of ultimate solution to this problem. But in reality, when you actually try and implement them would just be horrendously inefficient over all of these devices. The goal of Jensen is to be kind of pragmatic whilst also actually solving the problem. So that's why we kind of have that combination of the game theoretic aspects with the probabilistic aspects, with the cryptographic aspects, where if you were going for the kind of soundest system, the most secure system, you might go down a pure cryptography route. But you'd have a system that once you actually get it out into the real world, would never actually be usable because it would be ludicrously expensive and it would be ludicrously slow. And over time, what we expect to happen is, as technology progresses, is you can change those trade offs.
01:06:40.100 - 01:07:09.620, Speaker B: So you make it really efficient in the beginning, and it's kind of using the technology as it stands right now. But then as cryptography progresses, maybe we trade off some of the probabilistic aspects for more cryptographic aspects. The system gets more secure, and overall, it gets sounder over time. But it is a progression. It's not something that you build once, and then it's kind of done forever. It's this thing that will grow and expand as we explore the space of cryptographic techniques and encryption and all of these new things that we can do. But ultimately, like I said, for us, it's pragmatism.
01:07:09.620 - 01:07:20.164, Speaker B: It's making something that actually solves a problem in the real world. And then we can build on top of it and we can refine it, and we can make it more efficient and kind of better over time.
01:07:20.332 - 01:08:23.730, Speaker A: My new kind of favorite phrase that I've been saying to builders is ruthless pragmatism because I think that is really what crypto needs for mass adoption. But Ben Harry, I feel like we could talk for hours, but I want to be aware of your time. So really truly appreciate you coming on the podcast. I feel like we touched upon a lot from kind of the open sourceness, even the hardest problem of verification to how these actually networks actually work under the hood, from getting larger compute clusters online to the home compute to dis mediating kind of the middlemen that exist today, through walking through all the technical problems to being able to ultimately build these models and run them locally or either on a decentralized cluster when kind of the neural links of the world ultimately enter our minds. It was a really fascinating conversation. And again, thank you so much for coming on.
01:08:24.070 - 01:08:25.710, Speaker B: Thank you for joining us. Really enjoyed it.
