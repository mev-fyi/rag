00:00:00.920 - 00:00:30.710, Speaker A: Well, welcome, Stephen. Thank you so much for coming on the podcast. I really appreciate it. We had a really awesome spaces that we did recently and talking about some of the upcoming things that you've launched with Avalanche warp messaging and more broadly, I've had a lot of fun just interviewing a lot of your colleagues on the team and what you guys are building. So super excited to have this podcast. Thanks for coming on.
00:00:30.750 - 00:00:42.042, Speaker B: Oh, yeah, thanks for having me. Yeah, no, I watched Patrick's podcast and I really enjoyed that whole vibe and your back and forth. So I hope to have a good time and look forward to chat.
00:00:42.146 - 00:01:05.580, Speaker A: Yeah, yeah. Patrick's awesome. I initially met both of you at the Avalanche conference. I believe it was the first one that you guys hosted in Spain. And I came up to you guys and was just blasting you guys with questions, trying to learn more about how Avalanche worked and the different virtual machines. Really glad we met there. So again, thank you for coming on.
00:01:05.580 - 00:01:26.100, Speaker A: I would love to start the podcast, though, with a little bit of your background. I do find everybody's story unique. You are at Cornell at kind of the birthplace of a lot of innovation that's happened in the crypto world. So I'd love to hear kind of your point of view of kind of the origin stories of avalanche and how you got involved.
00:01:26.180 - 00:02:11.246, Speaker B: Yeah, for sure. So obviously goon is going to be involved in this story for sure. Right. And so I started at Cornell in 2016, focusing on cs, ended up doing the standard track, going through whatever, ended up getting particularly interested into distributed systems. So not necessarily consensus or I anything that specific or even blockchain, but just in general distributed systems. So at the time, I thought that I was going to be really excited about distributed computing and things like that. But what I ended up finding was consensus.
00:02:11.246 - 00:03:51.812, Speaker B: And the thing that really kind of drew me into consensus was how logical the system is. So you can kind of like fully understand the end, the full process and prove its correctness, but it's also extremely difficult to reason about. So it's extremely complicated, but also extremely logical, which is just something that drew me into that. So I first actually met up with Goon when I was taking his operating systems class, and I really liked the operating systems class, but I also just really liked Goon's teaching style. He felt like a very, like, direct, manner of fact kind of guy, which I just enjoyed that it was kind of like a breath of fresh air. And so after completing the operating systems class, I had that in the back of my head, and it turned out that he was actually teaching this class called introduction to cryptocurrencies and smart contracts, which that class, if you actually looked at the syllabus of it, it's a lot more low level than the kind of the title even says goes into a lot of how does proof of work consensus work dives into smart contracts? Does that dives into merkle trees, of course, ethereum, bitcoin, and then also just consensus as a whole and then some cryptography things. And so that was super interesting to me, kind of focusing in now a little bit more on these kind of more complex distributed systems and all the different pieces being interacted with.
00:03:51.812 - 00:04:24.817, Speaker B: And so then I went to do an internship. And right before I went to go on the internship, I asked goon if I could do research with him because obviously I was very interested in the area taking a couple of his classes. And he said, yeah, like, let's start right now. And I was like, oh, sorry, I'm going to do this internship. Let's do it when we get back. And so on the way to the, to drive to, like, where I was going, which was New York City, for the internship, it was an internship with the Google whatever. The avalanche paper first dropped and I was literally reading it.
00:04:24.817 - 00:05:09.076, Speaker B: And he had kind of like hinted that something interesting was going to be released in the space during the class. And I was, like, reading it. I was just so upset at myself because I was like, oh, I could be working on this right now and instead I'm going to do this internship. I did not turn the car around, though, you know, whatever. But anyway, so ended up doing the internship at the, after the end of that, going back into the, what was it, fall semester of 2018 or so, started doing work with Goon on avalanche. And so pretty much my first day, I just went into his office, was like, cool. Like really excited to work on this.
00:05:09.076 - 00:06:02.502, Speaker B: Like, what should I, what do you think I should start on? And he asked me, so do you know go or rust? And I knew neither of those at the time. And so he said, all right, that's fine. So pick one, learn it, and write an avalanche client in that language. And so I was like, all right, sounds interesting to say the least. And so ended up, after doing an extremely insufficient amount of research for sure, on the, the languages, I ended up picking Golang and started work on what is now avalanche go. So that was essentially the, like, avalanche go is my first go lag program and it's pretty much my first foray into really deep diving into the whole crypto space. Yeah.
00:06:02.502 - 00:06:07.450, Speaker B: So that's probably my very intro background story.
00:06:07.790 - 00:06:22.674, Speaker A: Yeah, that's an amazing story. And great place being at the correct place at the correct time. And amazing that you got the opportunity to join Goon's class on cryptocurrencies.
00:06:22.802 - 00:07:08.254, Speaker B: Yeah, it was all very, very lucky for me. I feel like, I feel like the most important part of this process was not necessarily my ability in programming or anything. It was like my refusal to move on. I was definitely in the early days of working with Goon, I went into some meetings where Ted, Goon, Kevin and I were basically there and they were doing this research sync. And I must have been so annoying because I remember very distinctly them trying to explain stuff to me and me just being like, I don't get it. I don't understand. You're saying a ydezenhe implies like this doesn't make sense to me.
00:07:08.254 - 00:08:08.370, Speaker B: I am very confident that I've destroyed a couple meetings just due to not letting them move on. I remember very explicitly one meeting, they were like, well, we can get back to this. And I was like, I don't understand how we're gonna move on if I don't understand this, because this seems pretty important for like. And so, yeah, I ended up spending a lot of time just digging into the avalanche paper and understanding stuff. I worked a lot with Tedinh, who Ted had done a lot of work with actual implementations of avalanche. So Kevin was definitely more on the theory side of snowball consensus proving, convergence, safety, liveness, those things, whereas Ted did a lot of the work on kind of making simulations and doing throughput testing and things like that, and actually implementing the protocol ended up needing to bridge the gap in a couple places. But yeah, it was definitely very lucky for me to be there.
00:08:08.910 - 00:08:26.142, Speaker A: That's awesome. After you started writing the avalanche client in go, what was the timeline? How long did that take in from there? How did it progress towards you joining the team full time afterwards?
00:08:26.206 - 00:09:16.920, Speaker B: That's actually a great question. It progressed really rapidly. So at the time, the consensus paper had been out, but there was really no thought except for things probably in Goon's mind on what the actual platform would end up becoming. This was before avalabs existed. So I'm sure that Goon had thought about starting avalabs, but I don't necessarily know what his thoughts were on that. But yeah, so we spent probably the first month or so just kind of like implementing the bare bones. Like, you have to obviously implement the networking, you have to implement the random message formats.
00:09:16.920 - 00:10:01.546, Speaker B: And then of course the actual consensus protocol. That was probably not as long as you would think to get something just running first pass. But there were a lot of interesting developments along the way. At the beginning, the client was just implement avalanche. Then it was like, all right, now we want to introduce this notion of red nodes and purple nodes, and these guys don't actually know how to communicate with me. These end up turning into subnets. But at the time, we want to add this complexity.
00:10:01.546 - 00:10:41.720, Speaker B: And I'm like, okay, never thought about that redesign. Redesign of the whole code base. Then I remember one time I ended up going into his office, and he's like, hey, Steven, we had a talk today. How hard would it be to add a linear chain to your code? And I was like, I don't know. Find out. So the very first pass of snowman was done in that week of trying to cram stuff together. And looking back, there's a bunch of really short iterations that happened where things were getting proven out.
00:10:41.720 - 00:11:14.090, Speaker B: What we ended up trying to introduce, staking. And that is basically how the p chain was very first created. It's actually really interesting. If you look at the code base now, the code base is very separated between avalanche consensus and snowman consensus, and they're very separated entities. At the time, that was definitely not the case. You had one dag, and the actual whole consensus mechanism was just over the dag itself. They weren't separated at all.
00:11:14.090 - 00:12:14.700, Speaker B: And that created some really interesting challenges. But one of the main reasons that it was kind of nixed and we didn't end up going down that route was because as you started to introduce subnets, it made less and less sense because the idea of maintaining this global dag for voting, but you're only voting over these cuts. And, like, how do you define where these cuts are? And you have to, like, have all this verification to make sure you don't have, like, a spurious edge between two things. Hey, we ended up kind of doing away with that and focusing more on, like, a more modular design. But, yeah, just, I think it probably makes sense to give a little high level before. So, yeah, so the way that. Is it cool if we just dive a little tangent here into consensus?
00:12:15.960 - 00:12:31.296, Speaker A: Yeah, let's do it. Avalanche, I believe, was the first to implement a dag structure on consensys. I know swe now is kind of going that route. At least the first big chain that I know of. Do you know of others?
00:12:31.368 - 00:13:05.400, Speaker B: So I think that the dag based stuff had been thrown around for a little while. But I think that there were always some issues. So I don't want to name too many names, like, for whatever reasons, it just will sound too negative. I don't necessarily know that avalanche was the first bag based protocol, but I think it was the first one that really I felt good about. Let's leave it there.
00:13:09.060 - 00:13:23.284, Speaker A: Yeah. Let's jump into avalanche. Let's start with snowball, because I do think it's very unique. A lot of people have a hard time understanding it. I think it'd be awesome hearing, in your words, how it works.
00:13:23.332 - 00:14:04.452, Speaker B: Yeah. So the key to all of avalanche consensus, as you alluded to, is the snowball consensus. Primitive. Essentially, all snowball is doing is trying to decide on a single bit of information. So either yes or no, red or blue or whatever you want to call it. So the way that the protocol works at its most fundamental is pretty much just basic statistics. So if you imagine doing a census or like, a sampling of a population in general, the whole population isn't sampled.
00:14:04.452 - 00:14:46.150, Speaker B: Right. Like, if you look at, you know, say, election results or whatever, like, you know, I've never personally been asked, you know, by any of these people who I voted for. Right. But, you know, you end up getting these, these polls that come out and say, oh, people are like leaning this way, that way, whatever. Right. And the reason that that works is just from basically simple statistics and probability is if you have a sufficiently large population, if you sample like, say, 10,000 people or 1000 people out of like millions, you're actually going to get a statistically very accurate idea of the entire population. Right.
00:14:46.150 - 00:16:05.582, Speaker B: So that's the core idea behind the subsampling approach is, well, rather than knowing exactly how many people prefer red or prefer blue, what if we got a statistically significant measure to say we are, with high probability, at least 80% prefer red. If you know that you don't necessarily need to continue on with asking more people if the protocol is just trying to figure out if you have 80% or more of redhead. Right? So that's all it's really doing at the end of the day. And the way that it does that is by just sampling a small, constant size number of nodes in the network. Now, in order to actually create a consensus protocol out of it, you have to eventually have everyone decide on the same value, which is why you end up introducing this kind of like, repeated sampling approach wherever I. You end up converting yourself to the color that you think is the majority color. So if I sample people and I see 70% of the network is red, well, then I'm going to turn myself red just by performing that.
00:16:05.582 - 00:16:32.500, Speaker B: If everyone in the network is performing that, you're going to very rapidly see that. Everyone is seeing that the network is red and they all turn red. So that is kind of the core of the slush primitive. So the slush primitive is actually just the first part of the snowball primitive, which is just this interactive kind of protocol of like repeated sampling. Sorry, do you have a question?
00:16:34.920 - 00:17:13.578, Speaker A: So I always, I love trying to get engineers to explain technical things and like non technical manners, and I think you did a great job bringing it back to how. So it does this like statistical substance sampling to try to find out what is the correct kind of state of information without having to ask everybody. By doing this, what is the benefits either from the user or directly to the network that benefits in real world use cases?
00:17:13.674 - 00:17:57.760, Speaker B: Yeah, it's a very good question. This whole side of the protocol is mainly focused around scaling out to have a large number of nodes. So if you look at other consensus protocols, I think that the one that I always name is hot stuff, or maybe PDFT, things like that. So hot stuff is a consensus protocol, which the way that it works is it's like a broadcast base. Rather than subsampling, it communicates with everyone. And essentially the way it works is there's a leader, and that leader will send a message out to every node in the network and collect their responses, perform like maybe some aggregation. If it's using threshold cryptography, it doesn't necessarily have to do that, but anyways.
00:17:57.760 - 00:19:14.164, Speaker B: And then it will send those messages back out to everyone and then move along. And then there's basically a rotation of the leaders. The problem with that is that the leader ends up communicating with everybody, and they have to then perform this additional work of like, you know, maybe collating the messages and things like that, which ends up creating a large imbalance in the network. So all of the other nodes in the network are essentially only sending and receiving like one message. In this example of like a fan out, fan back in, there's multiple iterations, but the leader in this case is going to be dealing with messages from everybody. That means that a single node in the network has to deal with an o of n number of messages for a single decision. And that leader ends up becoming the bottleneck, you could say, well, rather than making everybody in the network do slightly more work, why don't we just have that one leader run a really beefy machine? Well, the problem is that if that leader fails, you have to select a new leader, and now all of a sudden, the new leader has to have that really beefy machine in order to maintain this.
00:19:14.164 - 00:20:07.602, Speaker B: So what ends up happening is everyone in the network has to run a really beefy machine in order to have larger and larger numbers of nodes in the network. This is very different with, compared to something like avalanche, where avalanche, because you're only doing the subsampling and everyone is pretty much created equally. As far as the consensus sampling goes, there's no special node in the network that's doing way more work than any other node at any given time. What that means is that because you're basically doing a constant amount of work per node, regardless of the number of nodes, you can just keep scaling out more and more. That's essentially the key for why you would pick avalanche. Consensus over something else is just that scaling out factor. Yeah.
00:20:07.796 - 00:20:29.622, Speaker A: And that helps being able to ultimately increase the number of full nodes. And by doing so, you can effectively increase decentralization without kind of the performance degradations that other networks have because of this consensus and messaging overhead.
00:20:29.726 - 00:20:33.262, Speaker B: Yeah, exactly. Yeah, like, exactly.
00:20:33.366 - 00:20:34.078, Speaker A: Very interesting.
00:20:34.134 - 00:21:20.290, Speaker B: Yeah, cool. There's a lot of questions. I think there's a lot of weirdness around, like what is actually a validator on some of these networks? And so it gets, it ends up being kind of like a hotly debated topic, even though I don't necessarily understand why. So touching a little bit on that. So in avalanche, we typically consider validators as a full node that is participating in the consensus process and more explicitly being queried during the consensus process. So actually all nodes in the avalanche network will perform queries, but only validators will receive queries. They're basically the civil prevention of the network.
00:21:20.290 - 00:21:42.740, Speaker B: In other protocols, you might have, I don't know, say like Cosmos, you have 100 nodes and you have delegation and whatever hundred highest delegated nodes are part of protocol. Those are the validators, I would say. In Ethereum, they term validators as deposits.
00:21:44.720 - 00:21:51.820, Speaker A: Yeah, that was always super strange to me and intellectually dishonest, which I do not appreciate. About the Ethereum community.
00:21:53.050 - 00:21:58.290, Speaker B: I don't know, like, I don't think that that's necessarily fair. I don't know enough about Eth two.
00:21:58.370 - 00:22:39.526, Speaker A: Maybe it's, maybe it's a little bit harsh, but I think for me, like, I would love to see like a standard framework. And this is really what I've tried to, like, do a lot of my research on, because I wanted to essentially find like an apples to apples comparison. And the number of full nodes to me was like the best representation of like actually being able to quantify decentralization and then the Nakamoto coefficient, because if you can achieve more than one third stake, then you can start kind of messing with consensus. So to me those were measurable. But the light clients like, yes, I guess I get them on like ETH or deposits, but I don't know, I wouldn't count them as full nodes. And I get frustrated when people do. I definitely, I would love to.
00:22:39.526 - 00:22:41.574, Speaker A: If I'm wrong though, I would love to hear your point.
00:22:41.662 - 00:23:18.210, Speaker B: Well, I definitely agree. Like, I think things can be conflated. I don't want to say that it's intellectually dishonest, because I think that they had good reasons on why they did it. Right. My understanding of how ETH two works is you basically have these committees that end up getting selected for the epoch. And I can tell you for sure, it is a lot easier to select committees uniformly than from weighted distributions. And so it makes a lot of sense on why they would just say a 32 ETH ends up being part of the lottery to get selected for a committee.
00:23:18.210 - 00:23:59.586, Speaker B: But I definitely wouldn't consider that something analogous to my word, not yours. Yeah, I definitely wouldn't consider that analogous to a validator. I think if you look up validator statistics or full node statistics on ethereum, it's 7000 full nodes or something run ethereum. As far as a quick google I could find on avalanche, we have 1300 validator nodes and around like 1700 other full nodes connected to an hour. I think bitcoin has the most full nodes out of everyone at like some like tens of thousands, but yeah, pretty cool.
00:23:59.698 - 00:24:08.610, Speaker A: And avalanche also has a very high knockout moto coefficient, which I think is extremely important. So it's impressive as well.
00:24:08.650 - 00:24:42.134, Speaker B: Yeah. Anyways, it's all powered by the subsampling is kind of how we end up scaling out. I don't know if I'm just tying back into the consensus a little bit. Perfect, but yeah, so continuing on a little bit with the snowball algorithm. So we said that that's how the slush is done is by this repeated subsampling. The additional part to that is you have to know when to finalize. And so we actually end up finalizing the consensus decision based off of the same statistical analysis.
00:24:42.134 - 00:25:49.630, Speaker B: So essentially, once you have seen enough of the network, say like 90% or something of the network is all reporting this color, you know that if the process were to continue, everyone will eventually match that color, say red. And so you can just stop and finalize red at that point. So that's how consensus works on the bit level on a single decision. And the kind of natural progression to that is, okay, well, how do you decide, like, multiple things, right? Like a blockchain, you're not deciding just like yes or no, right? So the actual natural progression of that is much more snowman consensus than avalanche. Avalanche is a lot more complicated than we probably should have jumped into the first paper. But snowman is essentially how you would create a linear chain of this. And essentially the way you can do it is deciding this one bit of information, like red or blue, you can use that to determine the bit of the next hash for the sequence of hashes.
00:25:49.630 - 00:26:37.660, Speaker B: If you decide 256 bits, you've decided a full 32 byte hash, which is actually the hash of a block. And so if you just add in the requirement that this is a validated consensus. So essentially any decision that you're willing to vote on has to be provable to be a valid decision. Then essentially what you've done then is you have these blocks, and you're just voting over the bits of this hash sequence, and you're just moving along. So you decide something for this bit, decide something for that bit. You move along, and you end up deciding a full block, and you've moved along the whole chain. That sounds super slow when you say it that way, because you're like, oh, my God.
00:26:37.660 - 00:27:23.956, Speaker B: You're deciding. You're performing consensus, which is expensive for every single bit in the whole series. But an important kind of addition for snowman to work efficiently is pipelining. So you're not voting over just one bit at a time. You're actually voting over a full prefix of bits. What ends up happening is, in the normal case, you're ending up just basically performing the work of a single snowball query for however many blocks you have undecided. And so that ends up making the snowball transition into snowman very efficient, assuming there are no conflicts.
00:27:23.956 - 00:27:48.080, Speaker B: If there are a lot of conflicts, then you have to do a logarithmic number because it's bit wise to decide which block to actually end up accepting. And then this ends up kind of going into snowman plus plus territory with designated block proposers or producers. But, yeah, so that's kind of.
00:27:50.860 - 00:27:51.172, Speaker A: The.
00:27:51.196 - 00:28:10.530, Speaker B: Key of pretty much the whole avalanche network, with the sole exception of the x chain. So almost every chain in the network runs Snoman. Snoman consensus. The only exception to that is actually the x chain, which uses the full Dag based avalanche specification.
00:28:11.990 - 00:29:03.666, Speaker A: Gotcha. How does. So maybe if I could just resummarize. What you have created and the team has created is kind of statistically being able, like with a high degree of confidence, kind of based off statistics. You don't have to pull every single node in the network, and you have a high degree of saying that the network is agreeing upon a certain transition or execution on the network. And what that allows for is a large number of full nodes that has constant work where other nodes are not constant. And so it requires much larger nodes.
00:29:03.666 - 00:29:18.916, Speaker A: So you can have a higher degree of one or a higher probability, I guess, of censorship resistance and a large degree of full nodes in the network. And then with the consent. Sorry, go ahead.
00:29:18.948 - 00:30:22.058, Speaker B: Yeah, no, I wanted to agree on that, but also maybe pull the veil back a little bit. We aren't necessarily doing all this solely to get as many nodes as possible. A key part that is also important to avalanche is the fact that it confirms blocks and transactions extremely quickly with low latencies. And so if you actually look at proof of work, proof of work is pretty much the most efficient thing you could ever imagine. As far as a verifier side, definitely not on the producer side, but on the verifier side, really all you have to do in order to perform consensus is just gossip. These blocks around the network, they have a very similar communication overhead to our sampling, but it's actually even smaller because all they're doing is just pushing the block once. They never have this repeated thing.
00:30:22.058 - 00:31:20.592, Speaker B: They're basically only doing it once. And then they're just verifying hashes and just following whichever one has the most proof of work. Essentially what avalanche is trying to do is take the nice parts of that, which is extremely low overhead on a node, but combine that with something more traditional, which comes out of voting based protocols, which is the low latencies. If you look at bitcoin, ethereum pre merge, the confirmation latencies were on the order of minutes, sometimes even higher. The bitcoin white paper, I think, recommends, is it six block confirmations, which is like an hour? I think an hour, yeah. So I don't think anyone's using that now. I think most exchanges, which is generally the rule of thumb for confirmation requirements, is probably way lower than that.
00:31:20.592 - 00:32:00.962, Speaker B: Probably like two or three confirmations, but even that's like really slow. But avalanche confirms this whole process in generally around like a second, maybe like a second and a half once the node actually starts polling, which means the end to end latency for a transaction that is submitted into avalanche is probably like, I don't know, maybe like 2 seconds or so, which is usable. The whole joke about bitcoin with, like, you order your coffee and then it gets cold as it's waiting for the confirmation to go through. Like, you can definitely order a coffee with the avalanche latencies. So that's really the.
00:32:01.026 - 00:32:01.458, Speaker A: That's important.
00:32:01.514 - 00:32:11.698, Speaker B: Yeah, that's really the whole importance is being able to order your coffee and also having a large number of nodes in the network. So. Yeah, yeah, perfect.
00:32:11.874 - 00:32:47.558, Speaker A: Well, I appreciate you walking us through that. I do think. I mean, it is misunderstood, I'd say more broadly, I think one, like the early days of the Internet, unfortunately, the space is just technical. A lot of these kind of innovations come through these technical breakthroughs. And so I just wanted to talk about really the thing that powers the avalanche ecosystem. So I appreciate you breaking that down, maybe shifting gears a little bit. You and I are big fans of virtual machines and all the different kind of combinations of what they can do.
00:32:47.558 - 00:33:03.426, Speaker A: Can you talk about one kind of avalanche's flexibility in its design to allow subnets to have custom virtual machines and then maybe some virtual machines that you're excited about for this upcoming year?
00:33:03.458 - 00:33:38.480, Speaker B: Yeah, yeah, for sure. So going back to our conversation earlier, where we have like, the purple nodes and the blue nodes and the red nodes. Right. So what we ended up kind of falling down was this route of subnets. And so subnets, you register on what we call p chain, which ends up resulting in you being able to allocate validators to your subnet. And, well, what do they validate? They validate a blockchain of the subnets choosing. So it's really interesting even talking about what is necessarily a blockchain.
00:33:38.480 - 00:34:25.278, Speaker B: So a blockchain in the traditional sense is just essentially a hash chain up the, making sure that you record all the previous states and then you have maybe some state machine that executes over them. So I was, you know, when I was first implementing the client, I was wholly incompetent or I had no confidence in myself to actually be able to write a universally nice virtual machine that everyone would just see and be like, oh, this is perfect. And so, and I think that goon definitely echoed that kind of sentiment of like, yeah, no, like, we're never going to come up with the perfect virtual machine. Right.
00:34:25.374 - 00:34:25.560, Speaker A: I.
00:34:25.590 - 00:35:31.660, Speaker B: And so the idea was to basically allow users to register plugins with the avalanche go binary to define that state machine that executes over these blocks to then produce whatever the blockchain is. So it actually extends a little bit more than that as well. We don't necessarily need to have blocks anymore. For a VM you can define basically just your own networking interface. So you could define a plugin that just latches into avalanche's PDP network and send messages. You don't necessarily have to be performing consensus, but the traditional sense, you typically have a blockchain. And so a VM is just defining what the blocks are so how they're serialized, what is contained inside of them, and also you're defining what the disk state looks like, what is the state of the virtual machine, or say like the current state of the blockchain.
00:35:31.660 - 00:36:27.414, Speaker B: That's pretty much it. Essentially the way that vms work is they all talk over this plugin interface, which actually just uses GrPC. So Google's protobuf based RPC protocol. And yeah, there's a relatively extensive and very flexible interface that avalanche go provides for VM developers. And so that means that these vms can be implemented in pretty much any language that supports, that supports GRPC would be useful, but not necessarily required because you could technically implement it by yourself. But that includes things like Java, Rust, C, you name it. There's I think node js.
00:36:27.414 - 00:36:56.818, Speaker B: You could probably implement a VM in node if you wanted to. So it's an extremely flexible interface and pretty much all you have to do to get started is define things like verify this block, accept this block, reject this block, build a block. Those are, those are pretty much the main things required to get off the ground and running. And then there's a bunch of additional things as you peel back curtain even more that you can play with.
00:36:56.874 - 00:37:13.904, Speaker A: But that's, yeah, the one thing that you touch, well there's a couple of things that you touched upon, but the first one I think that is interesting is being able to customize the networking layer. Have you seen any subnets start to play around with that?
00:37:14.082 - 00:38:09.002, Speaker B: So I think that the, we've definitely seen subnets utilize this interface. The most common thing is to hook up a med pool to it. So that's like the most natural progression, right? Like it helps out with like building blocks and various things like that. The subnet EVM also uses this to support state thinking. So that's actually a large functionality that the semi EVM provides using this. I'm really excited for stubborn to go really off the wall using this. So some of the ideas that we've had are things like implementing a DHT using the PDP network here, rather than implementing this BFT consensus protocol.
00:38:09.002 - 00:38:46.100, Speaker B: What you could use this for is just a much more standard like distributed hash table for managing data doesn't even need to necessarily be. BFT would be really cool. It's just an example of utilizing the avalanche PDP network for additional interesting things. So I think that's something I would be really interested in seeing someone kind of pick up. I don't necessarily know that anyone has picked up something like that yet, but yeah, it's pretty flexible. So. Yeah.
00:38:46.480 - 00:39:43.290, Speaker A: Yeah, that's what I've always been super impressed by the design of avalanche when I met you and the team was, how flexible it ultimately was being able to customize the networking stack. I think I'm excited as well to see different teams kind of play around with this and see what they try to do on the virtual machine side. So the c chain today is a modified version of the EVM, which is a serialized virtual machine. And are you, I guess, for you and the team? I know there's the EVM. And what other virtual machines do you. Or what enhancements, I guess, are you going to make to that, like specific virtual machine? What would you love to see people experiment with on the virtual machine sides on individual subnets?
00:39:43.590 - 00:40:38.310, Speaker B: Yeah, this is a great question. So I'm going to start with the second part first because I think that that's the most exciting just to me to talk about, which is what people should be experimenting with. So the really nice thing about avalanche, right, is you can deploy whatever VM you want and if it sucks, it breaks. It doesn't necessarily do anything. You don't have to have some huge investment necessarily to play around with something and launch it. As far as the subnet EVM goes, or just EVM in general, the most interesting design space for me is around state management. I think there's a really great talk that Peter Schlage gave about how read amplification and write amplification ends up happening through the merkle tree used by the EVM.
00:40:38.310 - 00:41:51.380, Speaker B: A really interesting thing is you don't necessarily have to worry about any of this if you have a fixed state size. The way that Ethereum works is you pay some money for gas to, say, deploy a smart contract or allocate storage, but that storage can pretty much just stay there forever. You don't have to necessarily continue to pay more and more. There's a lot of really interesting ways on how you can implement fixed state size virtual machines, and I think it would be surprisingly easy to implement some of those inside of the subnet EVM to play around with that. So a good example of that may be, say, you have some token and you use that token for gas, but you could also use that token as just a fixed deposit. So say we wouldn't probably do this, but like say like you wanted to say, have one of ox, allocate eight megabytes or something. Or say 1 make the math easier.
00:41:51.380 - 00:42:29.964, Speaker B: If you did that because of AUx is hard capped, you know that there's never going more than 720 million of ox. You have already hard capped max storage state size of the VM to be 720 million megabytes. That's a lot. So you probably want to reduce that a bit. Say allocate less than a megabyte per box or something. But if you said one byte per box, then I think you end up getting a 720 meg capped state size, which is really fascinating. That's probably going to be way too much money in practice.
00:42:29.964 - 00:43:30.312, Speaker B: But that's just an example of a not super complicated way to introduce a fixed state size into a virtual machine. Now there's a lot more interesting ways you can do like state rent and things like that. And maybe you have like an Eip 1559 style which is basically dynamic usage targeting for your state size. So if you go above the target state size, then you increase the rent price and things like that. There's a lot of interesting challenges that comes along with these protocols, but like that is something that if implemented, the performance of that VM could just be absolutely ridiculous. The whole reason why BC chain and Ethereum and things like that, the reason that they're, well, they're okay, there's two reasons. But the first reason why they don't do this is because if they just increase the gas usage, the network wouldn't actually tip over.
00:43:30.312 - 00:44:19.196, Speaker B: You've seen this with things like BSc where they have a much higher gas usage over time. And the problem isn't the current time, it's in the future you're going to have huge disks and you're going to be extremely slow and clunky and you're not really going to be able to move things along. But if you had a fixed storage state size, that's actually not a problem. You know, you've bounded how much work you're going to have to do. You can process significantly more transactions now and not worry about in the future needing to pull it back because it's too slow. There's still the concern around people needing to sync the chain so they have to be able to process faster than new blocks are coming in. You still have to gauge that out.
00:44:19.196 - 00:45:51.242, Speaker B: You can't necessarily run it as hot as humanly possible, but you can run it much hotter or faster than currently people are running it that are doing it responsibly. So that is a very interesting design space that I would love people to play around with. Going into what the EVM is, it's actually probably mainly targeting around the same thing, which is state storage, but not necessarily in a fixed state size way. So there's two different kind of levers to pull, right? And the first lever is, well, let's just cap how much state you use, and then you can go from there. The other lever is saying, well, how efficiently can you manage that state? If you're managing state extremely efficiently, then you can have a higher state cap, which is more usable and still be able to interact with it. I think a lot of the work internally around the EVM right now is focusing on the state side, but there's also some kind of interesting ideas floating around about how you might extend the EVM, keeping it to be compatible, bytecode compatible with Ethereum. So you can still deploy your contracts, but while potentially adding in some new additional features.
00:45:51.242 - 00:45:59.950, Speaker B: So a lot of those new features are still kind of up in the air, but the state side is really pretty concrete and pretty cool.
00:46:02.130 - 00:46:35.400, Speaker A: I love asking people, what do you think is the kind of like biggest bottleneck in blockchains? I've had different answers. Some say it's the virtual machines, some say it's the amount of data that is available, some say it is storage. In your opinion, what do you think? If we truly want to scale blockchains, what is the biggest hurdle to hitting that large amount of daily active users from the technical side?
00:46:35.840 - 00:47:53.200, Speaker B: Okay, yeah. So from my viewpoint, I think that current blockchain networks probably try to do too much, honestly. So if you look at things like geth or like the bitcoin core, these nodes do a million things of which they probably don't need to do that. So an example of that is syncing the full historical state of the chain. Is it actually important for nodes to be able to join this p two p network and sync the historical state of the chain at a whim and support querying the balance at any state of the network at any given point in time in the past decade or however long this thing has been running? Probably not. I would say that feature creep to the max and should probably not be part of most blockchain node implementations. I think it's super important to be able to look at and still audit the state, because that's the whole point of blockchain is you have this auditable ledger going back forever, but it's not really up to everyone in the network to do this.
00:47:53.200 - 00:48:44.644, Speaker B: I think that current blockchain clients should probably delete around 90% of their tech debt and say, you know what, like we're running a client. All this client does is moves blocks along. It keeps adding new blocks to the ledger and fully validates. Right. So from that perspective, I don't necessarily think that clients should keep historical chain state. I think they should pretty much just keep whatever chain state they need in order to be safe and continue the protocol along. And when you say it kind of like that and like, all right, we're just, that's all we're doing on the client side, pretty much all that matters at that point is managing the validation of new state and the lookups into the current state.
00:48:44.644 - 00:49:37.896, Speaker B: Right. So that means that there is the virtual machine actually executing instructions and then there is the state management for the current state. If you look at VMS right now, pretty much the main problem with VMS is that they pretty much ignored the state management side. So I think that there's a lot of VMS coming out now. I think move does this pretty well around separating out different parts so that it's actually easier and more efficient to manage move state, the current state of the network. And then there is a number of data structures that people are working on or that existed and weren't being used for managing the state. So I think that the core bottleneck is generally the state.
00:49:37.896 - 00:50:11.988, Speaker B: But what ends up happening is the VM design can sometimes cause the state to need to be less efficient than is necessarily optimal. So it's really both of those two things, in my opinion, that need to be optimized for. And yeah, so that's kind of our focus is implementing some high performance state engines right now and then plugging that into some high performance vms. So we have a really cool, really cool demo that we're working on, but I don't necessarily know when it's going to come out.
00:50:12.044 - 00:50:15.910, Speaker A: But yeah, you can share the alpha here.
00:50:16.020 - 00:50:59.052, Speaker B: That's about it. That's about all the Alphabet is to share. Shocking. We're working on high performance state management and high performance vms, you know, shocking, right? Like, but yeah, I mean, you know, people throw out like TPS a lot and like, you can get to stupidly high TPS numbers if you bound the state size, and then you perform efficient, like state transit. So, like, if you have a well designed VM in a well designed state model, you can get absolutely ludicrous throughput numbers. Like, I think Solana is probably showing at least some of that.
00:50:59.076 - 00:51:00.600, Speaker A: And what's ludicrous?
00:51:00.900 - 00:51:29.788, Speaker B: Like, what do you want it to be? Like? You know, like, at some level, it ends up being, how big do you want your machines to get? Right. Like, if you want it to be verified on a beefy laptop, you can probably get, you know, at least tens of thousands of TPS for sure. Performing, like, performing, like, say, like, something along the lines of, like, transfers, things like that. I don't want to go down, like, the realm of.
00:51:29.964 - 00:51:30.756, Speaker A: You don't have to.
00:51:30.788 - 00:51:35.120, Speaker B: Yeah, the realm of different TPS games and transfers.
00:51:36.020 - 00:52:04.998, Speaker A: All good. Well, I definitely appreciate us kind of getting in to snowball and deep diving the algorithm. Appreciate your thoughts on the different virtual machines. But I would like to talk about the Avalanche warp messaging just because that's something that you guys launched recently. So please, maybe just in your words, kind of describe it and why it's kind of so important for the avalanche ecosystem.
00:52:05.054 - 00:52:57.746, Speaker B: Yeah. So having all these vms running all their cool designs is a really nice thing to have for the avalanche network. But there's really no point in there being a network if those vms can't communicate with each other. It's always been a goal for the avalanche network to be able to communicate across subnet, awm, kind of, or, like, warp messaging, added that capability in. It's really not too difficult to add such a thing in. Like, we've kind of known how we wanted to do it for a really long time. And really, the crux of it is making sure that you can perform these verifications efficiently.
00:52:57.746 - 00:54:04.280, Speaker B: And by efficiently, it generally means without the primary network or every node in the network performing work, and also for the individual subnets to be able to handle that efficiently. So that's really the thing that was kind of blocking for us to end up really deciding on how we wanted to do this. What we ended up deciding on doing is creating something that is pretty upgradable. Not only is it just some structure, and you say, this is it, and it'll always be this, we wanted to support upgrades and future additions to the protocol and get a reasonable first pass in for efficiency. The way that the warp messaging works is utilizing the P chain. The P chain for the avalanche network is really the heart of the network. It manages all the validator sets of all the subnets and defines all the vms for all the chains on those subnets.
00:54:04.280 - 00:55:19.110, Speaker B: The way that warp messaging works is you may send a transaction on some chain, and the validators will end up attesting to that event, whatever it may be. And then a recipient chain can receive that message, verify that attestation based off of the state of the P chain, and then execute that message on the destination chain. So the way that that attestation is currently formed is based off of BLS multi signatures, or aggregated BLS signatures. That is necessarily, that's how it currently works, but can be upgraded and changed as more efficient or more say like future proof things come out. So I know that there is a talk around like quantum security around BLS, and sure, that's a concern. And so we definitely didn't want to just like marry ourselves to BLS forever, but currently it's definitely suits all of our needs. So that's the core of how warp messaging works, is just this simple, like send a test and then receive.
00:55:19.110 - 00:56:36.260, Speaker B: But how we actually got to that point is a lot more complicated. So it actually goes all the way back to the introduction of Snoman, which is when we actually created this dependency of the chains onto the P chain. So the way that Snoman works is it ends up defining proposer lists for blocks for their chain, their respective chain. And in order to do that, it has this dependency on the validator set of the P chain at that point in time. And so you end up basically tracking the validator set changes and incorporating that into your chain through P chain height references. By doing that, we end up registering not just the validator set of your current chain, which is all still a used it for, but actually the validator sets of all other chains in the avalanche network. By just extending this to basically be shown over the VM interface, you can actually directly reference the height of the Pchain provided in the proposer VM block and reference the validator sets of any subnet on the network.
00:56:36.260 - 00:57:41.474, Speaker B: And that's how that attestation is actually performed. So a lot of the work around this messaging primitive has really been focusing on p chain extensions and support and efficiency. If you look back over all of the prior network upgrades, it might not have been super clear at the time, but this is what we had always been pushing towards, is centering the P chain in this network for message passing. There's a lot more left to do with the message passing though. So the way that warp messaging works is inherently as, it's as efficient as possible because it doesn't touch the primary network. But by doing that, it also inherently introduces some non reliability of the messaging. So one thing that I'm really excited on doing is introducing different flavors of this messaging.
00:57:41.474 - 00:58:32.362, Speaker B: So an example of something really cool that you can do is if you wanted to have this message be delivered reliably guaranteed. Right. Well, you can again use, say, the primary network to use that as a global timestamping service so that the sender chain is guaranteed to know that the receiver chain has actually received that message. That's an extension that I'm looking forward to pushing along, but also just seeing protocols come out as a part of the warp messaging. Warp messaging is very flexible. It's basically just an envelope of attested data from a chain. That data can be arbitrary.
00:58:32.362 - 00:58:58.970, Speaker B: It can kick off turn complete communication, or turn complete execution upon delivery and pretty much anything. And so I'm really excited to see some of the usages of the warp messaging, especially coming around in things like the EVM, because I think that'll probably be the main focal point when that comes out.
00:59:00.920 - 00:59:58.772, Speaker A: Definitely, yeah. Subnet to Subnet communication is massive, and being able to natively do that in the avalanche ecosystem I think is going to be something that's helpful for the entire ecosystem and hopefully loved by users to be able to pretty easily go from Subnet to Subneta. One thing that we've seen, I would say, in last couple years has been different hacks or different bridging communications where they have kind of fallen down. How would, in your words, would you explain to non technical people how the avalanche ecosystem and team is thinking about security from Subnet to subnet more broadly?
00:59:58.876 - 01:01:24.640, Speaker B: Yeah, so I typically break these down into two separate concerns. So the first concern or reason that exploits can happen is caused by just incorrect implementations of the protocol. So it might be that as an example, you might have some smart contract based bridge with just a pure multisig. And that bridge is fine, the multisig isn't corrupted, but there's some exploit in the smart contract that ends up happening where you can drain stuff out. That kind of exploit is pretty hard to just fix in any general messaging protocol, but the hope is that you can make it as clean as possible to implement and as simple to implement that it's hard to get it wrong. So you want basically something that's extremely simple from a UX side and very easy to understand from the dev side, so that you minimize the attack surface there. The other situation where bridges could fail is if the, the actual bridge owner fails.
01:01:24.640 - 01:02:25.460, Speaker B: Generally you have these multisig bridges. If you have say a seven out of nine multisig, well, if seven of those people don't manage their private keys correctly, then they can just arbitrarily take anything out of the bridge. That is definitely something you can just solve immediately using something like AWM. And the reason for that is because using warp messaging, you don't actually have what are traditionally called the relayers of the bridge. Those relayers, there might be relayers in order to actually pass the message, but they aren't actually involved in any of the security of the message itself. There's no one necessarily to attack other than the validators of the subnet themselves, which is just the security of your chain. Those are the two areas.
01:02:25.460 - 01:03:12.120, Speaker B: And so the way that warp messaging is set up, we just cut out the entire concern around that bridge operator failing to do their duty. And then the goal then is to make the actual implementation of the bridge as kind of simple and as easy to use as possible. And so definitely look forward to seeing some really cool, crazy protocols coming out for warp messaging that will make bridges extremely easy to implement. So that's definitely a major focus right now for us to incorporate into the subnet. Evm. Yeah.
01:03:14.740 - 01:03:56.360, Speaker A: Yeah. I think bridges, bridges today kind of have a scary condentation. I think definitely over time that will fade, especially as better implementations get developed. But they are, I would say, even, I would say they're still difficult to understand. One maybe other kind of point of reference here. A lot of people sometimes, well, a good amount of people kind of compare avalanche to the cosmos ecosystem. And the cosmos ecosystem also has IBC that allows bridging between different cosmos zones.
01:03:56.360 - 01:04:07.772, Speaker A: How would you maybe to kind of wrap up avalanche warp messaging, how would you describe, like, the differences or comparisons to the implementation that cosmos has?
01:04:07.876 - 01:05:06.890, Speaker B: Yeah. So IBC is focused on. So the way that IBC works is essentially running a light client of your sister chain or whatever, right. Light clients are generally, well, they're much more efficient to run than full clients, but they're not generally free by any means. So if you actually look for how a light client is implemented in something like Cosmos, you have to validate all the signatures going from block to block or validator set to validator set changes, and you have to basically keep that up to date. So if I am some zone on Cosmos, I have to be keeping up to date with the block headers and verifying all the signatures of all the zones that I am interacting with. That is super general and super flexible, which is nice.
01:05:06.890 - 01:06:28.360, Speaker B: But the problem is that you have this n squared complexity where all of a sudden everyone's verifying headers of everybody else as you're trying to pass messages around. One of the really cool things that I like about the warp messaging is how it uses the p chain as this global validator set service. The assumption is that, well, validator sets, they're probably not going to be changing some huge amount. You're not going to just see rapid changes. Why don't we set up a global ledger that keeps track of everything for everybody? Now, rather than verifying headers of all these different n squared chains, you can just keep track with p chain, keep that up to date on all the validator sets of all of the subnets, and then use that information to then communicate. So I would say that the actual guarantees at the end of the day are probably pretty similar, because if on Cosmos, you're keeping up with the headers, you do know their validator set, and so you can then interact and take that message from them based off of their validator set reliably. But this is just, in my opinion, a more efficient way of performing that for a much larger number of chains.
01:06:29.180 - 01:06:45.462, Speaker A: Yeah, makes sense in the cosmos example, because it is n squared that either affects the number of nodes you can run or does it just make it so the light clients have to be larger hardware?
01:06:45.606 - 01:07:50.840, Speaker B: Yeah. So the n here is actually the number of zones. So, like, if I'm a zone, then I have to be running light clients for all the other chains that I'm accepting messages from. So that means that if there's a click of like five people, then all of a sudden everyone has four connections, which is where you get that, like, n squared interaction from. But, yeah, so actually, an interesting point on that, though, is, and this is maybe diving outside of my area of cosmos, so maybe I shouldn't talk, but I believe that the, like, client verifications are a function of the number of validators on that chain, unless they use threshold crypto, which I don't think they do. And so that means that if you have more and more validators, like, the actual communication can actually start getting a lot more expensive. And of course, that is something that, as a chain, you have to keep up to date as it moves along.
01:07:50.840 - 01:08:02.288, Speaker B: And so that ends up taking away from the actual operations that your chain can do of its own. Interesting kind of operation, because now you're validating all the headers, all the other kind of chains that you're doing.
01:08:02.344 - 01:08:50.406, Speaker A: So, yeah, very interesting. Well, yeah, definitely appreciate the deep dive. I think as people become more, kind of have greater depth of knowledge in these specific areas that today, I would say largely they feel uncomfortable. They'll over time, feel better about moving their assets from different chains. So appreciate the clarity there. We've definitely touched upon a lot kind of in the avalanche ecosystem with snowball, different virtual machines. What you guys are doing in terms of the EVM, also the new Avalanche messaging between different subnets, I guess kind of wrapping up the podcast.
01:08:50.406 - 01:09:09.142, Speaker A: What things? I mean, a lot has happened in 2021, kind of crazy year. A lot has happened in 2022. What are you specifically looking forward to for this upcoming year in 2023, or things that you would just like to see get built more broadly to get user adoption?
01:09:09.286 - 01:10:11.430, Speaker B: Yeah. So when this kind of comes up, I have two things that immediately jump into my mind. So the first is moon math stuff. So there's a lot of really interesting cryptography advancements being made pretty much every day. So I think, like today there was a paper pushed out about like, BLS threshold, or that that could support like, BLS threshold generation and like cubic complexity for a threshold threshold cryptography using BLS, which I find really fascinating for various things, including vrfs and things. I'm really excited about just the general advancement of cryptography, and that's a big reason why we're pushing for the Rust SDK. The Rust SDK, I'm hoping, is going to allow the usage of this more exploratory cryptography to explode, because a lot of the interesting cryptography advancements are happening in the rust ecosystem right now.
01:10:11.430 - 01:11:12.646, Speaker B: So I'm really excited for the Rust SDK to take off and see a lot more of just crazy new vms that I never knew were possible. I think pretty much a week ago or something, some random person threw out that they were working on a zero knowledge VM, that using bulletproofs and Rust SDK, I was like, that's cool. I'm in on that very exciting stuff. That is an area that I'm really excited on. The other area is just general SDK development for more high performance chains. Currently, I would say it's not super easy to deploy a custom VM from scratch unless you are a couple people in the world right now. So the XSVM of the cross subnet VM, in case people didn't know what that actually stood for.
01:11:12.646 - 01:12:52.092, Speaker B: The XS VM took me probably a couple days to implement from scratch, but I would not expect that from just following our online documentation. I think that we have a long way to go on both improving just the docs of like, how to actually build out a VM, but also for developing out a lot more of like composable VM components that people will find really interesting. So I definitely don't want people to be implementing their own mempool or implementing their own merkle tree storage or implementing how block state is passed from parent to child in the VM. These are all things that can be pulled out and modularized and then just reused as components. So I think that really 2023 I'm hoping is going to be the year of pushing out VM development and making it a lot more mainstream with just how usable it is. I'm hoping that the development flows out from the avalanche core stack, which is say something like avalanche go into things like the VM implementations. So I have no idea how happy I would be if someone just randomly said, hey, I took the XS VM and forked it and added in some random VM scripting thing and added in some state that would just be oh cool, let's dig into this.
01:12:52.092 - 01:12:57.920, Speaker B: Let's see how it works. That's my hope for 2023 for sure.
01:13:00.100 - 01:13:25.184, Speaker A: Perfect. More and cooler cryptography and modularizing and better vms. I love it. Perfect. Well, how to, I guess people that want to learn more about the avalanche ecosystem, if they have questions on any of this cool stuff or ultimately want to build new virtual machines, where would you point them to?
01:13:25.232 - 01:14:05.898, Speaker B: Yeah, so I think the two areas that I would definitely point people to is either the discord. So we have. I think if you just go to like chatbox.net work, I think it'll immediately link you to our discord. And that's definitely the first place that I would stop at for for asking any technical questions. If you're just getting started trying to play around with what avalanche is and just kind of learn about it, the docs is probably a good first place to go, but you'll probably end up needing to then go to the discord to ask any really low level, meaty questions. So there's docs, avox.net
01:14:05.898 - 01:14:33.640, Speaker B: work and then also chat avox.net work are probably the two, two major places to go if you're looking to do vm development. I also am trying to pay attention to things like Twitter. Now, I'm definitely not a native Twitterer or Twitter tweeter or whatever, but I try to pay attention to that. So if anyone has any particularly interesting questions, either feel free to dm me there, just post or something at me.
01:14:35.910 - 01:15:02.254, Speaker A: Perfect. Well, thank you again for coming on the podcast. Really was looking forward to it. And always my favorite conversations are with the technical people that can go extremely deep and communicate those into normal speak. So I really appreciate you coming on, sharing all the special sauce that makes avalanche and yeah, look forward to 2023 as well. So thank you.
01:15:02.342 - 01:15:03.070, Speaker B: Thanks very much. Logan.
