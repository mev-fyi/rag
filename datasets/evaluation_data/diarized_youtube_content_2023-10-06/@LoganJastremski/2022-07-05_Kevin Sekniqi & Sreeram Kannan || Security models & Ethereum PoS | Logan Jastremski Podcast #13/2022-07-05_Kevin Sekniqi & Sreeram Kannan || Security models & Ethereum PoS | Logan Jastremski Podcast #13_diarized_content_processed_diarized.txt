00:00:00.240 - 00:00:26.410, Speaker A: Welcome, everybody. We have a wonderful podcast episode today that I'm super excited about. We have Kevin returning from Avax, and we also have Shiram on as well. Before we get too much into the weeds on all the wonderful topics that we're going to discuss, Shiram, could you just do a quick background on yourself and an intro to all the guests?
00:00:26.830 - 00:01:09.748, Speaker B: Yeah. Thanks so much. A pleasure to be on this podcast with Kevin. My background is I'm an associate professor at the University of Washington, Seattle, where I run the UW Blockchain research lab. Before that, I did my PhD back in 2012 at Illinois Urbana Champaign, working on network information theory, where we tried to study peer to peer wireless networks. So my interest in peer to peer networks goes way back. But at that time, it looked like wireless networks were much more infrastructure driven rather than peer to peer driven.
00:01:09.748 - 00:02:11.090, Speaker B: So the interest in that area or potential kind of waned a bit. So I got interested in computational biology, which I started studying human genomics at Berkeley and Stanford before starting here, working on some of that. And four and a half years back, I got reintroduced to or introduced to the idea of blockchains, and that there is a kind of new generation of really cool ideas coming up in peer to peer networks and how that can kind of underpin a new cooperation economy. So I got fascinated by that. One of my favorite authors is Yual Noah Hararie, who wrote the book sapiens, and in it he makes the case that homo sapiens are kind of distinguished from all other species, not necessarily in intelligence, but in our ability to cooperate flexibly in large numbers. Cooperate flexibly in large numbers. And the fundamental impediment to cooperation is trust.
00:02:11.090 - 00:02:50.444, Speaker B: I will cooperate with you if I know how the rewards of such cooperation are going to be distributed among us. And as I was thinking through some of this, it looked like blockchains can play a major role in upgrading our systems of cooperation. So to me, the analogy is, if Internet is the information superhighway, blockchains are the cooperation superhighways. So that attracted me kind of deeply into blockchains, beyond the technical things, which I'm also fascinated about. So over the last four and a half years, I've been working on consensus protocols, scaling peer to peer networks underlying blockchains.
00:02:50.622 - 00:03:02.020, Speaker A: Excellent. No wonderful background, and excited to get into the nitty gritty of everything. And maybe, Kevin, do a slight quick intro as well.
00:03:02.320 - 00:03:28.240, Speaker C: Yes, very, very quick. I don't want to say the same thing as the last time. Yeah. So currently work at Avalabs trying to build avalanche spend a lot of time on business operations type of thing these days. But my background is engineering, it's research. Right. Before working at our labs, I was a graduate student working in distributed systems.
00:03:28.240 - 00:05:01.236, Speaker C: My primary focus was how do we build reliable, scalable distributed systems using new types of constructions and primitives, and how do we build them in production in a really elegant, simple way that just works and is reliable. Before that I was doing more work into cryptography type of stuff. This was during my undergrad, not in the research necessarily on the theory of it too much. It was mostly on the implementation of it, implementation side of some fully homomorphic schemes for working for a library that I worked on, for computing unencrypted data, specifically speaking, statistical computations on encrypted data and then generally speaking, Binna, um, in the crypto space for a very long time, since the very early days when I first discovered about bitcoin, uh, through some Reddit thread very, very early on, uh, and I was fascinated by it and I ran a whole bitcoin note and it was really cool, but didn't really understand it. So, uh, these days, uh, uh, you know, I, I really want to get back into more of the uh, the engineering side of things and back to principles of research. And it's kind of part of why I wanted to do this podcast as well, because, uh, you know, I'd recently seen tree realms, um, tweet thread about appreciation of ETH two, and it was a fantastic thread. And I said this is a great opportunity for us to just talk about all these things in distributed systems.
00:05:01.236 - 00:05:04.680, Speaker C: And so we came up with all these topics and here we are.
00:05:05.020 - 00:06:08.356, Speaker A: Nice. We have the heaviest of hitters on this podcast at the moment, so I'm super excited to dive into all the small details of the blockchain architecture. I think the podcast that Kevin and I lasted on blockchains from first principles really set up the conversation well, kind of wanted to overall explore the different possibilities and nuances of not only ETH, but just blockchain architecture and design overall, and thought this would be a wonderful conversation between the three of us. So maybe just kicking it off on a high level, trying to figure out or articulate kind of like the trust models and blockchains. And there's kind of different trust models per se, but how would each of you articulate how blockchains work on a high level, or. Yeah, from the trust standpoint shrub.
00:06:08.388 - 00:06:09.320, Speaker C: Do you wanna go first?
00:06:09.900 - 00:06:45.108, Speaker B: Sure. Yeah. The standard VFT models basically assume the following. There is one set of honest nodes and another set of byzantine nodes, or adversarial nodes, that can behave arbitrarily. So this is, I would say, the classical byzantine fault tolerant systems model, which has been very prevalent in distributed systems. So the idea in this model is there is one group of nodes who follow the protocol religiously. They will not deviate from the protocol.
00:06:45.108 - 00:07:57.798, Speaker B: And there's another group of nodes who can arbitrarily deviate. So you binarize behavior into really, really good, exactly as per protocol specification, and another who can deviate arbitrarily, you know, do whatever they want. So this is the classical model of distributed systems when you want to study adversarial behavior. So I would say this is one canonical model in which these systems should be studied. But blockchains, being incentive machines, also bring other very interesting models to understand these systems. The second model, I would say, is the incentive model, where what you try to do is you try to understand each node has some utility functions and incentives, and you want to know whether a rational node operating in its self interest, will continue to behave correctly. And in this model, usually what you do is you want to study things like, is the protocol specification a Nash equilibrium? If everybody else is doing the.
00:07:57.798 - 00:08:57.750, Speaker B: Playing the protocol rules as it is, is it in your self interest, as in incentive maximizing node, to actually follow the protocol or to deviate from the protocol? I would place things like the selfish mining attack from Kevin's advisors group into this class. So there's a second class of models underpinning how to study consensus, which is rational nodes, and Nash equilibrium. But I think there is a third class of models, which. Which are also important in studying blockchains. It's kind of like a combination of the adversarial model and the incentive model, where how we think about it is, instead of saying that something is a Nash equilibrium, means that if all the others are following the protocol, you have an incentive to follow the protocol. In the third model, which I'll call the crypto economic model, I mean, even the second one could be called a crypto economic model. But just for.
00:08:57.750 - 00:10:49.840, Speaker B: Just for clarity here, let's call the second one the Nash equilibrium model, and the third one is the crypto economic model. In the crypto economic model, what you want to do is, I want to understand what is the cost of corruption? What is the cost of corruption? You want to say that if something bad happened, suppose safety failure happened, then there is a certain cost incurred by misbehaving notes, and it doesn't matter how they misbehave or what they're doing. But if something bad happens, and bad can be defined as safety failure, aliveness failure, whatever the thing that you're most interested in. Or for example, if you're studying an oracle, you may say that an oracle misreported an input, that's a failure. So when you, in the third model, the crypto economic model, you're asking what is the cost of corruption? And the cost of corruption is basically, if something bad happens and bad defined in whatever way, what is the cost imposed by the people who created, made the bad thing happen? And the reason I think this model is equally important and different from the second Nash equilibrium model is this model says that it is not just in your interest to, in your interest to continue operating the protocol as it is. If everybody else operates the protocol as it is, it's saying if you unilaterally deviate and you had like a majority collusion, which I think is a possibility in some of these proof of stake systems, fundamentally, because stake has potential centralizing tendencies. And so when you have systems like that, if you can somehow bound that the adversary or somebody behaving badly does take a certain cost, then that is something of interest.
00:10:49.840 - 00:11:18.290, Speaker B: And so I would say that's there are three models. The adversary model, the Nash equilibrium model, which is, if everybody else behaves correctly, do I also have an incentive to behave correctly? And third one, the crypto economic model, which is we can collude arbitrarily and do something bad, but we need to incur a cost for doing something bad. And can we guarantee, can the protocol guarantee that you will take a certain cost of corruption? So I would say that's the three models that I've been thinking about in blockchains.
00:11:18.830 - 00:11:21.170, Speaker A: Awesome, Kevin.
00:11:22.150 - 00:12:43.278, Speaker C: I mean, nailed it. There is a few other things to consider when talking about trust models is really about, I guess, the complexity of trying to model them realistically. If you want to design protocols, especially distributed protocols, the more assumptions you make, or the more you try to analyze, rather, I should say, the more you try to analyze the system accurately, the more set of assumptions you're going to make. And sometimes a lot of these assumptions, even if you create a long chain of them, even if one of them is not correct, it seems to invalidate your entire model. It becomes really, really difficult, and it's a challenge oftentimes to get it done accurately the way that I usually myself, at least conceptually speaking, try to approach. Designing these systems is what is the bare minimum set of trust assumptions that we can write on without incurring game theoretic assumptions or cryptocurrency assumptions or so on. And this is actually sometimes a topic of debate where people seem to, on Twitter, for example, people are like, hey, if the system, uh, breaks down, uh, you know, the protocol is not working anymore, we can still rely on, on full nodes and so on.
00:12:43.278 - 00:13:15.138, Speaker C: Well, you're making a lot of assumptions about what happens afterwards. To me, building the distributed system is just simply about what is the base set of things that we can write on. In this particular case, we really are assuming two things. We're assuming that there is a certain set of nodes that behave according to what the protocol is saying to do. And then the network is according to some defined structure, it's asynchronous or synchronous or whatever it is, and there is some delta communication. That's all that you assume. And as long as you assume these things, we can build protocols that are correct.
00:13:15.138 - 00:14:00.440, Speaker C: And a correct protocol is a protocol that is both safe and live, is a protocol that produces blocks consistently and also produces blocks that are all valid. And there is no double spends, there is no corruption and so on. So anything that you add on top of that is great. You can add full nodes, you can add the client side verification, you can add anything you want. But ultimately, your trust model for building a blockchain system relies on purely these two assumptions. And anything else, basically, is not part of the protocol itself. It's just like something secondary exogenous to the system, and it doesn't necessarily help you with building safe and live protocols.
00:14:00.440 - 00:14:57.864, Speaker C: So to me, a trust model for a blockchain is really simple. It's just, what is the set of nodes that you can assume are going to behave correctly, and then you can add incentives to make them behave correctly or reduce the probability they're going to misbehave. These are all things on top, and you have to reason about all these things. And then what is the network communication layer of this protocol? Anything else is almost certainly very exogenous, and it's very hard to reason about. And these are the kinds of things that typically I try to stay away from, but for some whatever reason, I get drawn back into it. It's like, listen, if the protocol does not have these assumptions withheld, you no longer have a correct protocol. And if you don't have a correct protocol, we no longer can recover safely from it because we would have to then rely necessarily on social agreement and things like that that are completely, you know, orthogonal or rather exogenous to the protocol itself.
00:14:57.864 - 00:15:27.100, Speaker C: So, yeah, that's, that's really like the way that I think of it. It's like the simplest possible thing, because anything on top of that is probably too complex to think about. And it's not necessarily. The complexity of it is, is that attempts to try to think about it will almost certainly result in models that are not accurate. And so we will come up with analysis of these models, but they're not even like descriptive of the reality of the situation very accurately. So I try to typically, you know, don't, don't get to engage with these kind of discussions myself.
00:15:27.960 - 00:16:09.514, Speaker B: Okay, so it reminds me of Silvio Michali, who's the Turing award winner behind VRFs and other things, and behind the algorithm blockchain. He said almost exactly the same thing that Kevin said, which is, after thinking about cryptography and game theory for ten years, I've come to the conclusion that there is massive model misspecification. The fact that there is model misspecification means that these are extremely difficult to reason about with. And therefore, Algorand, for example, took a path where, you know, he said specifically, if we can't assume a majority is honest and we can't build a society around it.
00:16:09.642 - 00:16:10.350, Speaker C: Correct.
00:16:10.690 - 00:17:16.456, Speaker B: And I appreciate the thinking that goes behind it. But also, there is something that is very dissatisfactory to me about this. And the reason I am dissatisfied with the majority honest type assumption is we do not have a mechanism to make sure that stake remains distributed. And we can distribute the stake when you start the system or mine or whatever, to initially be distributed. But the fact that stake can re aggregate, you know, just like Twitter stock was distributed sometime back till a noteworthy gentleman made a bid for it. So there is a possibility that, you know, the systems, the stake can get reaggregated. And to me, there is something quite valuable to reason about models which do not assume a majority are honest.
00:17:16.456 - 00:17:36.220, Speaker B: And I think there are absolutely scenarios where we can continue to assume majority are honest. For example, if gating somehow happens via some proof of personhood, some identity system, but somehow relying on stake and majority honest, I would say, you know, there is something dissatisfactory about it.
00:17:37.850 - 00:17:41.490, Speaker C: Go ahead, please, please. No, please. I'll let you finish.
00:17:41.650 - 00:18:46.588, Speaker B: Yeah, no, the idea that. And people try to measure things like how is stake decentralized and stuff like that, but to me, it is not stably decentralized in the sense that will it continue to stay distributed is not clear to me. And this is one of the reasons that if we can build some guarantee that of course it should work in a majority on a setting. So that's to me a minimum assumption. But in addition, if we can add incentives that can ensure that even if the noteworthy gentleman controls then 50% of the blockchain, they will incur a cost for misbehavior is a much stronger statement that can arrive out of a certain crypto economic modeling that 1 may not be able to give in the majority honest model. So I see the point of view. I think it is not a shallow or superficial point of view.
00:18:46.588 - 00:19:04.606, Speaker B: It comes in actually after quite some thought, the point that models are misspecified and therefore incentives are very difficult to get correct. But the problem with stake is somehow the majority honest appears in sufficiently.
00:19:04.638 - 00:19:49.646, Speaker C: So I actually don't disagree with the point about reasoning with more crypto economic models and more complex ones. The problem is that the current way that we reason about them is not satisfactory in the sense of it's not precise enough. We talk about adding all these incentives for trust models, but none of them are actually reasoned in a mathematical model, and that's very unsatisfactory. So I'll give you an example. You know, slashing is a very hot topic. And as I was saying, you know, all these incentives, a, b, c, all of them, obviously you can add as many incentives as you want, you will reduce the probability of something going wrong. And slashing seems to be one of these incentives.
00:19:49.646 - 00:20:41.522, Speaker C: But it's nothing specified as far as like what the trade offs for these things is, right? So at first stab at it, slashing does seem to disincentivize misbehavior. You know, you remove your principal, that seems obviously to be an added bonus on top of the block rewards, which is great, but at which level do we slash? Right? Like let's suppose that we slash the principal at a level that seems scary enough to destroy most people. Let's say like 25%, right? At that level, that seems great. Like if we slash 25% of your principal seems amazing. The problem with that is that there is significant amount of trade offs and consequences. So if you slash that kind of principle, you actually make it much less likely for people to come in and decentralize the network. And actually by that I've heard this many times, people are like, I'm not going to touch this, it's too dangerous.
00:20:41.522 - 00:21:30.012, Speaker C: I don't know how to do this. A lot of people comment on this, on this kind of stuff often. So if 25 is is too high, let's suppose that we do it only like 1% slashing, but then 1%, you know, most people would be okay maybe with 1% of slashing the principal. But does 1% of slashing give you the incentives that you really need? It's not clear, right? So to me slashing is just an added bonus incentive. But it has to be reasoned or there's two ways in order for me to be able to say, okay, slashing absolutely needs to be an approved state protocol. Reason number one is because somebody mathematically modeled this, models this accurately through a satisfactory set of assumptions that I believe to be accurate of the real world, and they really capture it. And then we reason about that.
00:21:30.012 - 00:22:24.428, Speaker C: And then we prove that there is some sort of an optimal marginal utility curve, at which point adding more slashing doesn't really additional security. And there's a level at which we are satisfied with the added bonuses of desensitization and so on. So we have basically a curve of utility of slashing. That's number one, that seems incredibly hard, but hopefully maybe somebody does it and reason and way number two is through long enough in production empirical evidence to suggest that slashing is in fact something that is safe. And it seems to incentivize enough people to come in and it protects us. And there is real world evidence of it functioning properly right now. The only to be fair, we don't have real modeling or satisfactory modeling for the block rewards either.
00:22:24.428 - 00:23:17.360, Speaker C: We only have empirical evidence that bitcoin block rewards and staking block rewards work just well enough. But if somebody provides additional evidence to suggest that slashing is in fact a great method to disincentivize misbehavior and the tradeoffs are reasonable. So the marginal utility curve is actually something that we can reason about, then by all means slashing is a great thing. But as it currently stands, slashing is a very politicized issue. It's not like an issue with concrete evidence to either side. It's just more like do we criminalize drugs or do we decriminalize them? Do we right to bear arms or no right to bear arms? Somebody just needs to sit down and say slashing is good or slashing is bad for the following reasons. And the way that I think about system design is, okay, I have a set of incentives that I know work.
00:23:17.360 - 00:23:49.298, Speaker C: They have been working for many years. I use those. If I add additional incentives, obviously I will add additional protection. But I cannot reason about what the consequences for the protocol, those additional incentives, they could be, they could be disastrous for the decentralization of it. They can make it really, they could actually end up, making it more insecure over the long term and so on. So to me, I welcome personally all crypto economic models and so on. They just have to be satisfactory in the sense of modeling the real world accurately enough.
00:23:49.298 - 00:24:03.900, Speaker C: And if they don't, then it's really just a political issue of do I like this or do I not like this? There is no evidence either which way. So it has to be mathematic or at least evidence based. Everything has to be with some sort of evidence, either theoretically or practically.
00:24:05.360 - 00:25:22.556, Speaker B: Okay, yeah, I agree with the philosophy behind it. But one, you know, the one concern I think I've seen a lot is, hey, if I participate in as a validator, is there a risk that I'll get slashed even though I'm behaving honestly just forgot to update the software, whatever the set of things, I think the way at least I understand Ethereum's incentives is something like this. The slashing penalties are very light when very few nodes misbehave and tighten as close to one third misbehave, basically satisfying both ends of the trade off to some extent. So what you want from these crypto economic models is if safety was failed, then I have burned one third of the stake, for example. But to protect an honest node who's just happened to run like an old version or whatever, if a very small fraction of nodes misbehave, or like double sign or whatever, the bad thing is that is getting slashed, if a very small fraction of them actually misbehave, then their percentage penalty is very, very small. Like instead of slashing their entire stake, you slash like 0.1% of their stake.
00:25:22.556 - 00:26:11.196, Speaker B: But as that. So there's a kind of quadratic curve that as you go more and more towards basically one third of the nodes having double sign, then you start like tightening that curve very sharply, because it is very unlikely that everybody's just made a mistake in configuring their validator software. It could happen. It could happen that there's an upgrade and something happened and 50% of the nodes didn't do. So. Those need to be handled much more carefully when there is slashing. But there is an inherent trade off which has been built into these models, which is at the end of the day, I want my cost of corruption, which is if a lot of people misbehave, I need to slash them, but which is basically a coordinated attack.
00:26:11.196 - 00:27:08.514, Speaker B: But if a small fraction of people misbehave, I'm going to be very light on slashing. So that's the thinking behind it. Whether that is sufficient or not is of course, like all other principles up for debate on the empirical side. Actually, I was also interested to see what is the response of people. Are they going to opt into a system where there is potentially significant slashing? And at least I think what we've seen in the beacon chain has been encouraging. A lot of people do run nodes, and the centralization tendencies of staking are fundamentally much more due to capital accumulation than due to small nodes not being able to run nodes. So that's my high level response.
00:27:08.514 - 00:27:20.626, Speaker B: But I do agree with the philosophy that the sharper things are modeled, the sharper these assumptions are articulated, and the more they're tested in practice, that's when we award them a battle tested model.
00:27:20.658 - 00:28:06.582, Speaker C: Absolutely. Yeah. I mean, all of you can, I can imagine adding all kinds of sophisticated slashing curves, right? Like in any which way, they exponentially increase whatever linearly increases one third. Although of course, it's interesting because like I could counter, I could make the counter argument that like, you know, if there is a large corruption, it's probably accidental. A lot of like a bad, like, client just got pushed and then all of a sudden everybody screwed it up. Realistically, it's more likely that people just screwed up their configuration and get slashed and so on than anything else. We've seen this happen, for example, with avalanche staking and then an actual coordinated attack, an actual coordinate attack might actually be much smaller and so on.
00:28:06.582 - 00:28:59.200, Speaker C: Although in practice and happen. But yeah, I mean, like, to me, incentives are a double edged sword. On the one end, you're adding more incentives for making people behave according to what the protocol we want. Honestly, fairly, you know, fair is a very subjective term, but we're adding all these, all these incentives and we're reducing the probability for misbehavior. On the other hand, we are adding, the fact that we are adding these protocol dynamics means that we have just opened up pandora's box for all kinds of complexes that can happen. So, you know, to me, protocol design, give it for all the bad, like all the crap that we talk about, bitcoin development and so on, the one thing that they do exceptionally well is they do very, very careful upgrades and careful upgrades. You know, I don't necessarily agree with it at all times.
00:28:59.200 - 00:29:25.492, Speaker C: Sometimes it's like too aggressive. It's like, oh my God, like just, it's obvious this is going to work. Just do it. But it's, you know, when we're building these kind of systems, these systems should be very calm. You know, it should be very easy to reason about. I would love to hear to see, like, formal specification of these systems at some point. And so the more of these assumptions we add in our protocol, the more complex the trust assumptions are.
00:29:25.492 - 00:30:09.040, Speaker C: And there is so much happening behind the scenes that we would have to reason about. So I'm personally just a fan of well defined protocol specifications with well defined incentives, well defined everything, and then reasoning about them, and then only then do we actually get to implement these rather than just say, okay, well, it seems like that, feeling wise, you know, slashing is probably a good idea, but I don't know what the consequences of it are going to be or if it even is going to be effective, depending on the parameters you choose. As I said, like 5% slashing, maybe it's not even going to work. Right? Like, 10% slashing is a really determined attacker. Might be like, you know what? Screw it. It's going to make so much more than this one, and it's 5% slashing is not going to cause me much trouble.
00:30:09.120 - 00:30:41.092, Speaker B: So, yeah, just on the percentage point. I think the way to quantify this, the crypto economic cost, is basically saying, suppose something bad happened, which is, in this case, suppose safety failure happened, then how much money is getting burned? So that's the economic security that is being afforded. And given any curve or whatever, we can exactly calculate what it is, and then we reason about whether that is sufficient or not, right?
00:30:41.156 - 00:30:41.564, Speaker C: Absolutely.
00:30:41.612 - 00:31:49.484, Speaker B: So in some sense, the interesting thing is with this curve design is the crypto economic cost is only incurred when the one third attack. But you want to make it smooth so that, like, nobody tries to attack, because what you don't want is, now that I agree with Kevin a bit on this thing, that these incentives have, like, complex ramifications. And so, for example, what you don't want is you added a penalty for, like, 33% of the notes misbehaving. But what if, like, 20% of them, and then let's say there was no penalty for 20% of the notes, then, like, maybe 20% of the notes test the waters, and then, like, one person just, like, comes in, and then they can flip the balance and so on. So these incentives are indeed nuanced, and. But at least there is one quantifiable metric that one can get out of it, which is, what is the cost of a safety attack if somebody. So I think the shift in this model is only the following shift in this model is suppose 100% of the stake is held by the same party.
00:31:49.484 - 00:33:01.056, Speaker B: Can we still guarantee that? Put a lower bond on what cost, economic cost that they're incurring to actually take down the system. And I think this leads me to one, I think very underappreciated property of blockchains, which I think all of us here may appreciate, but maybe some viewers don't. The idea that blockchains are fundamentally powerful because they're open state more than any of the other things that we're actually doing. The fact that the state of these blockchains are open, which means anybody can fork them and create a new system without like skipping a heartbeat, that's really what powers the fundamental security of these blockchains. I think it's very underappreciated from the outside. How powerful? The fact that once you have a forkable system, even if a single centralized intermediary runs the system at any point, we can pick out from there and start a fork, that we can continue on from there. I think this is one of the most powerful incentives that just underpins the core of all blockchains.
00:33:01.056 - 00:33:11.320, Speaker B: I think this is very underappreciated. And I think whether you add slashing or don't add slashing, this exists. And this is quite powerful, super high.
00:33:11.360 - 00:33:59.600, Speaker A: Level, just to try to re articulate this conversation for the vast majority of the people, because we are getting into the granularity of things. But to zoom back out a little bit, how are we actually, the conversation is forming around how do we actually describe or trust full nodes, in a sense, to make sure that these blockchains are acting rationally and that we can trust them either based on how many nodes are in the network per se, on the majority of the trust or one of end nodes is majority cracked versus the economic side of things where we have kind of divulged into slashing.
00:34:01.700 - 00:34:49.876, Speaker C: I would say, I think, generally speaking, in agreement. The difference here is that when implementing new incentives for adding behavior that we want to these full nodes and so on, although actually we have to be careful. Full nodes, I mean like a node running the protocol and validating and producing blocks, not just the node that downloads the blockchain. You have to have well specified description of the protocol, the assumption that you make and so on. So, for example, when Sheran was saying that you can quantify what the cost of the attack is and you can put a lower bound. Yes, but there's a caveat in that it's not quite accurate because you also have to add what the reward for misbehaving is. And there is a reward to misbehaving.
00:34:49.876 - 00:35:10.026, Speaker C: There is like an exogenous reward that you get for doing something bad. You obviously incurred direct costs in the protocol, but you also are getting rewarded somehow elsewhere because somebody's bribing you or somebody. Or you maybe have an incredible neb. Opportunity on chain. And because of that you can make so much money with double spending and so on. And, um. So.
00:35:10.026 - 00:35:24.882, Speaker C: So it's very hard. It's. You know, it's already becoming hard to reason about it. Uh. Even just like on the very basic assumption that, like, oh, we can reason about this by just using the cost to it. That's not enough. You also have to look at the reward for it.
00:35:24.882 - 00:35:48.860, Speaker C: But then the reward is unspecified. Like, how much is it? We don't know. It's an exogenous factor that could be unbounded high. Uh. Or you just don't know. Right? Like, is it coming from chaining? Is it coming from elsewhere? Somebody's paying you off, ban bribing you and so on. So these are the kinds of things that basically, to me and I guess Silvia Mikali as well, we look at this and we're like, oh my God.
00:35:48.860 - 00:36:19.772, Speaker C: How can we even reason about this? You cannot reason about this in a satisfactory way. You can make hand wavy assumptions about them. You can make hand wavy arguments. But it's very hard to reason about them. And the only thing that we really know how to do in these systems is assume that a bunch of nodes are going to act correctly. That's number one. And then number two, give baseline rewards for acting correctly, which is what Nakamoto has been doing for many years.
00:36:19.772 - 00:36:42.778, Speaker C: And we know it seems to work satisfactorily. Well, anything outside of that is. I personally welcome it. I just don't know that it's necessarily well specified, or at least to the standards that I would want it to be specified formally. But otherwise, I think we're generally aligned. That the trust model here should be beyond just the basic assumption of a BFT system. It should be cryptocurrency.
00:36:42.778 - 00:36:54.790, Speaker C: It should be game theoretic. It's just that it's very, very hard to reason about them. And so I'm like, I just don't know how to do it. And Shiram is more like, let me try to do it.
00:36:56.250 - 00:37:34.176, Speaker B: Okay, so I'll say something about the. The cost of corruption. I think you're. You know, Kevin's right in that there is exogenous rewards for behaving incorrectly. And the way we think about it is there is a cost of corruption, which is what you're paying but there is a profit from corruption, from misbehaving, you can make some money. And so, fundamentally, what these economic guarantees state is, as long as your profit from corruption is less than x, whatever the X is, X is basically how much you will get slashed for misbehaving. That's the cost of corruption.
00:37:34.176 - 00:38:02.778, Speaker B: As long as profit from corruption is greater than the cost of corruption. If profit from corruption is greater than the cost of corruption, you are incentivized to act maliciously. If the profit from corruption is less than the cost of corruption, you are not incentivized to act misbehave. And this actually tells you some things. Once you put a number on the cost of corruption, it tells you how you can potentially keep the profit from corruption down. I'll give a concrete example. Okay, so let's say there is a chain.
00:38:02.778 - 00:38:47.214, Speaker B: And, you know, one of the things I saw in some of the tweets is, you know, the thing about, like, light node versus full node, you know, people are discussing, and one issue there is, and I think Kevin pointed out correctly, is that, hey, you know, you guys are talking about validated errors and stuff, but if majority is not honest, you can get, like, reorgs and other stuff. And when you get into things like that, basically, you have no guarantees. And it is true that you have no guarantees. So. But what we can actually do is, what is the bad thing that happens downstream of something like a reorg? Right? We can reason that. And here is something that happens. So, if you had a reorg, what happens is somebody who thought they got the money now don't have the money anymore.
00:38:47.214 - 00:40:16.472, Speaker B: And so one thing you can start doing is, for example, if you're an exchange or the set of all exchanges together, if the total amount of money that you can pull out within, like, a reorg period. Right, within a reorg period, if that is less than the amount slashed. So, basically encouraging, or, like, actually explicitly adding in the things like exit frictions, because that's the thing that you can do, like, profit off, like, just sign a block, go and withdraw of coinbase or FTX or whatever other exchanges, and then now you can profit off of it. So once you know the baseline economic cost incurred for the attack, that gives you, like, a toolkit, actually, for going and making sure that the profit from corruption is actually smaller. But to agree partially with Kevin on this, I would say that the mechanics for actually bounding the profit from corruption have not been well developed. To me, that's actually an opportunity that we can actually come in and start specifying, for example, many bridge, bridge hacks will not be as bad if we have made sure that bridges have some volume frictions, that you cannot withdraw more than XDev per unit time and so on. And these are essentially, I think of them as part crypto economic mistakes in the sense that we have to bond how much volume goes out, because that is really the profit from corruption.
00:40:16.472 - 00:41:05.996, Speaker B: So in some sense there is an opportunity. Once you have like a solid slashing protocol, you have a bound on the cost of corruption, but you need like lot of other things to actually bound the profit from corruption. And by doing both, you can start creating systems where you can reason about them unconditionally. I think that's the word when I'm saying unconditional. It's just to the extent that we do not have to make any assumption that all stake is with one person or not with one person. So I think in my mental model that whenever I go to a crypto economic analysis, I just go to this extreme of 1% controls, 100% stake. What guarantees can we still make about blockchains? And I think the problem with just the majority trust is that's not the case.
00:41:05.996 - 00:41:46.616, Speaker B: And the reason I also mentioned something like the open state, the fact that a blockchain is open state means that the system will continue to live. That's not a problem, because even if there is a single person running the system and they start like doing some bad stuff, somebody else for the system, because it's an open state system, whether there is slashing or no slashing, the system will continue to live as long as enough people care about it. And that is itself like a massive, massive property of blockchains. But all we are doing by looking at the economic analysis is kind of bounding what the transient loss is from going from one system to the continued system on the other side.
00:41:46.768 - 00:42:41.170, Speaker C: Yeah, on that part, fully agreed. I think it's a very ripe area for research. There's a lot of opportunities for, I mean, frankly, if I were to bet, I would probably say that the economic incentives for bitcoin, ethereum, avalanche, whatever of today are probably not going to be the same ones in ten years or 20 years. There's going to be a new set of incentives, even if they're very small and very subtle but well studied ones that we know how to reason about. Because probably in a couple of years we'll have more formalization around a bunch of these incentives, and then we'll know with high degree of confidence that there is something that we can do that is actually very effective. It bounds and it well, defines extremely well with internal protocol costs and exogenous rewards, how the adversary may behave. But yes, as far as right now it's still very unspecified.
00:42:41.170 - 00:43:00.570, Speaker C: But I totally expect even bitcoin that never changes. I don't think that the incentives for bitcoin are going to remain the same in 25 50 years. I think it's going to change significantly and they're going to need other additional mechanisms to incentivize good behavior. So I think it's going to be an interesting time.
00:43:00.650 - 00:43:16.332, Speaker A: Overall, we've largely specified the different trust models and gone into slashing, but now kind of segueing or segueing into ethereum and e two and kind of talking about the protocol design. Kevin, you want to kick it off from there?
00:43:16.476 - 00:44:00.390, Speaker C: Sure. Yeah. So I'll talk at least a little bit high level about it from maybe talking on how it evolved. And then I think the really cool topic of discussion that it's part of why I was really, really interested in talking to Shiram was this dynamic availability problem that ETH two has been wanting to tackle. It is a really interesting problem. It has a lot of assumptions behind it, but that's really where we want to get to next. But yeah, let's talk about proof state design, or really, I should say quorum based protocols, protocols that use voting to make blocks of rather than hash power, which is what bitcoin uses.
00:44:00.390 - 00:44:38.276, Speaker C: So the most classical interpretation or example, if you take any distributed systems course on talking about quorum based systems, is the granddaddy of them all. Pbft came in 2000. PBfT is generally simple protocol. It has some very complex parts to it, but for the most part it's fairly simple. The way that it works is you have a proposer. Proposer is, and you're going to see that it works on a sort of like a vote by vote basis, or rather a proposal by proposal basis. And then there is issues with that.
00:44:38.276 - 00:45:01.650, Speaker C: But the way it works is that we want to make a decision on a block. Let's say we want to just a single block, or we want to make a decision on a single vote. Alice base, Bob, that's just a single decision. The way that it works is you have a network of actors. They vote. Each one has a particular weight. Let's suppose for simplicity that each one of them has the weight of one, so they have equal weights.
00:45:01.650 - 00:46:06.760, Speaker C: And the protocol works in rounds effectively. The way it works is in the same way that you would work. You get pass a bill in the parliament somebody proposes, it collects the votes from everybody else, then it shows to everybody else what everybody else's votes are. And then as long as there is at least two thirst plus one signatures on a particular ballot or a particular block proposal, and so on, then there is a very simple set, theoretic proof, that says that as long as there is no more than f, which is one third, effectively the network B byzantine, there is no other way to create two equivocating quorums that vote for two different ballots, as long as there's enough correct notes. So, proposer, let's suppose we have a network of four nodes, which is the minimum that we need in order to tolerate one single byzantine node. It's three f plus one. The proposer sends a proposal, let's say Alice space Bob, to all the other three nodes, including itself.
00:46:06.760 - 00:47:21.330, Speaker C: Everybody sees this proposal and signs on it, sends back the proposal to the proposer, signed with their vote, saying either yes or no, typically just yes if there is no other conflict. And then as long as there is enough votes, in this particular case, three votes out of the four vote for a particular vote, then the proposer sends it back to everybody else. And as long as a single correct node sees that two thirds plus one of the other nodes have seen this proposal, it can safely terminate, meaning that it can safely record the vote into its disk, into its memory, or into its storage. Because even if the network were to go offline, like one of the nodes were to go offline, any one of the nodes, as long as it's one at most, if it were to recover, then there would be no other way for any other proposal to have been committed, as long as the assumptions are maintained. And, but there is, this is very simple. I don't know, Sriram, if you have, like, more analogies to this to make it very simple, but it's very simple. But there is a very massive problem with it.
00:47:21.330 - 00:48:03.616, Speaker C: Suppose that we now go to, let's say, 1000 nodes. Four nodes is very easy. But let's say you have a thousand nodes of all equal weight and you have a proposal. A proposal generates a block and sends it out to 1000 nodes. Now, each one of those 1000 nodes has to sign this proposal, has to collect, then the proposer has to collect all these 1000 signatures and has to send them back to all the other nodes. All 1000 signatures has to be sent to all 1000 other nodes. And then all those nodes have to sign the fact that they have seen all the 1000 signatures, or rather two, three plus one.
00:48:03.616 - 00:48:45.876, Speaker C: So 667 of them from this network. This is obviously a three step process that has cubic. Well square blowout, and in the view, change it as cubic and so on. So if the network starts getting into the 10 00 nodes, the number of messages grows quadratically. At minimum, in the view change phase, it grows cubic. Some protocols are even larger because they trade off in different steps, but this can be significant. So for a single vote, you're talking about 10,000, let's say 10,000 nodes, which is what bitcoin has right now.
00:48:45.876 - 00:49:12.730, Speaker C: 10,000 squared signatures is an incredible amount of signatures, or messages, incredible amount of messages to be propagated across the network. It's actually extremely bandwidth intensive. The network is just going to be constantly checking signatures, crunching them, sending them back. And so it's a very laborious process. So these kinds of protocols cannot scale in that sense, as far as the nodes go. Now, there is a biggest caveat, or.
00:49:12.770 - 00:49:23.370, Speaker A: Not caveat, but the biggest thing there for the 66% is just, it doesn't scale well as the nodes continue to scale. But with lower nodes, it does scale well, correct?
00:49:23.410 - 00:50:03.368, Speaker C: Yeah. If you have a network of four nodes, ten nodes, it's very fast. It's not like I don't think that there's any difficulties deploying PBFT will be very fast. But as it goes in, the number of nodes becomes really expensive. And by the way, there's a very simple proof to why there is two thirds plus one. It basically comes down to the fact that the adversary can create two partitions of the network and can have these two partitions sign two different transactions. But as long as you have no more than one third and you have two thirds plus one or more correct nodes.
00:50:03.368 - 00:51:01.022, Speaker C: Turns out that there is no two partitions where there is not at least one correct node that is shared between these two partitions. And so it's a very simple set theoretic argument on the safety of it. And as long as you have this, then it's guaranteed that you will never have two thirds plus one signatures. But if you have more than one third amount of byzantine nodes, what ends up happening is the byzantine node could theoretically at least create two separate partitions of the network that both sign for two thirds plus one votes on two completely equivocating transactions. So Alice pays Bob and then Alice pays Charlie. So that's the granddaddy of the mall PBFT. And the fundamental difference between that kind of model and eth two gasper is the fact that you chain the decisions rather than do them over time.
00:51:01.022 - 00:52:06.960, Speaker C: So in PbFT, you send a proposal, you do this, multiple rounds of voting, collect them, and then finalize the block or the transaction and so on. Now, the really interesting thing about this process is that if you have continuous decisions in a block by block basis, where each block proposes the previous block and you have a hash of them all, what you can do is you can actually pipeline these decisions one block at a time. So block one, you have the first round of voting, then block two comes in, which points back to block one. And if you vote for block two and you have two thirds plus one votes, that means now it's an implicit vote on block one, because block two points on block one, and then you do it for block three and block four and so on. And eventually, after three, four rounds, you have all the signatures, and now block one can be finalized and so on. And so that's the, the big sort of like invention behind chain BFT. I think, as far as I know, hot stuff was the very first one that was a chain BFT protocol.
00:52:06.960 - 00:52:59.058, Speaker C: Gaspro came on later, also uses chain BFT. But that's the fundamental trade off. I mean, that's fundamental trade off, a property of how ETH two works, how Gasper works, and with differences that hot stuff and Gasper trade off, trade off, effectively finality for the ability to produce blocks. Now, this is Shiram, if you want to add, come in at any point on how this works and give some intuition. But I'll give you my intuition without going too much detail. It's very simple, actually, though, in hot stuff, you require all of two, three one of votes because you assume that the network is not too big. But if the network is 10,000 nodes, the way ETH two works is that ETH two uses committees, and committees get shuffled for every single block.
00:52:59.058 - 00:53:57.936, Speaker C: But now all these committees, they don't have the two thirds plus one of all the network. So for block, you want to get two thirds plus one of the small committee, and then the next block gets two three postponed off another committee. And eventually, after a very long string of blocks, you will have two thirds plus one of the entire network, in which case you finalize, whereas a hot stuff just says, just get two, three plus one of the entire network right now this block, or we don't make progress. So it stops the liveness guarantee, it's not guaranteed, but it stops producing blocks until it gets a signatures, whereas hot stuff just keeps producing blocks even though it never finalizes until. Sorry. Yes, a gas bird never keeps producing blocks, even though none of them are finalized until all the way when you go to the end and you have all the signatures and then it finalizes. I don't know what the numbers are, but as far as I actually have the paper here.
00:53:57.936 - 00:54:06.082, Speaker C: What is it? It's like 64 blocks per epoch. And then how many seconds is each block .5 seconds.
00:54:06.106 - 00:54:09.026, Speaker B: So it's like six minutes or twelve minutes. Finality.
00:54:09.138 - 00:54:29.446, Speaker C: Yeah, right. It's like 1012 minutes. That's right. For finality, assuming that you have the two thirds plus one for each block. And so that's effectively at least the basics of Gasper. But there's much more that goes to it. There is this dynamic availability stuff and all that kind of stuff, which I really, really want talk about because it's fascinating.
00:54:29.446 - 00:54:35.650, Speaker C: And maybe it's a good time to talk about the ebb and flow model and just kind of teach us there. Shuram?
00:54:36.110 - 00:54:38.622, Speaker B: Yeah. Logan?
00:54:38.766 - 00:54:40.302, Speaker A: No, go for it.
00:54:40.486 - 00:55:43.160, Speaker B: Okay. So I think Kevin gave a really good tutorial on PVFT, which represents to me one class of protocols. Almost all of the BFG protocols are kind of built on this basic idea. You have, like somebody proposed two third committee sign and get finalized, and you have not one step, but two or three steps inside that process. How do we pipeline these steps? How do we arrange them? How do we do view change when somebody is bad? How do you throw them out? There are various differences in the class of VFT protocols among it, but this is the core idea remains the same. But there's another class of protocols which we can, maybe I can give an overview of and then try to explain what that group of protocols trying to do and how that is related or not related to these BFG protocols, and then we can try to understand. So this was rather mysterious to us when we entered into this area.
00:55:43.160 - 00:56:13.720, Speaker B: We're not from the consensus space, like I mentioned, from information theory, and we looked at this. So one of the reasons I got into the space is because we looked at something like bitcoin. It's so simple, like just a chain. Everybody just grow the longest chain and it seems to work. And it's something really. It captivated our interest quite a bit. I think, like most of us here, the main reason we're here is because of that captivation.
00:56:13.720 - 00:56:57.530, Speaker B: So when you look at a longest chain protocol like bitcoin. Right? You can ask. Bitcoin does have this symbol resistance mechanism, which is the proof of work. You make these blocks by solving a certain proof of work puzzle. But you could maybe zoom out a bit and say, what if you created a longest chain protocol call with end nodes, you have the fixed end nodes, or these end nodes are staked. And what happens is that each time, by some random lottery mechanism, one of these nodes is selected to make a block, and that block gets appended, and then you create a longest chain. So this is a very simple, clean protocol.
00:56:57.530 - 00:58:20.300, Speaker B: So you have, in the proof of work model, you can ask the question is, when? When does the longest chain protocol work? In fact, that was one of the first papers we wrote with David Shea, Pramod and others. The paper was called everything is a race and nakamoto always wins. I'll explain the motivation for this title a bit. So, if you go back to like, just in a proof of work model, you know, as we are discussing here, the model that is inherent in all these bisenten fault tolerant models is one group of nodes behaves exactly as per specification, and another group of nodes can deviate arbitrarily. But if we go back to, like, Satoshi's original paper, the bitcoin white paper, what we see is we see that there is this protocol, which is a longest chain protocol, and one attack on the protocol is analyzed in the white paper. And that attack is, what if I grow like a chain in private and then reveal it? If I have more mining power than all the public honest mining power, then I will create a longer chain and take over the entire longest chain. So this is the clean, simple attack that was analyzed by Satoshi and underpins, for example, how you choose the confirmation window.
00:58:20.300 - 00:59:33.492, Speaker B: One question that one might ask is, why are we only talking about this one attack? There may be many, many attacks that one can execute, because all we are seeing is that the adversary controls 50% or whatever, some fraction of the mining power. Maybe they can do more complex attacks, like create two chains in public and let everybody get confused between which is the longest chain, or create some other kind of attacks. So we needed to have, like, a much more formal way of reasoning about what the longest chain achieves in such an adversarial environment. And I think one of the pivotal papers in this area is called the bitcoin backbone protocol, written by Agilewski IIs and others. It's a beautiful modeling of this complex adversary space. And they showed fundamentally that, you know, when at least when the mining rate is much slower compared to the network latency, you actually have this property that as long as honest nodes control more than 51% mining power, you will have safety and liveness. It's a beautiful analysis.
00:59:33.492 - 01:00:44.492, Speaker B: What we did is we took up on that model and started analyzing what are the exact bounds on the mining rate relative to the network latency under which this protocol is secure across all attacks that the adversary can execute. And so that was the analysis. And the reason we call the Weber everything is a race and Nakamoto always wins, is everything is a race, basically, is the attack that Satoshi considered, which is that you mine a private chain and you're racing with the public chain, and you have to outrace the public chain in order to win. And it turns out that in these longest chain protocols, it is actually the dominant error event that if you're secure against this kind of like a private attack, you're also secure against all kinds of other attacks. This is not true. Generally, it turns out, when you switch from the longest chain protocol to other kinds of protocols, for example, the ghost protocol and others, this ceases to be the case. You do not have only the private attack being the worst.
01:00:44.492 - 01:01:57.626, Speaker B: You can have other attacks, like balancing attacks and other attacks, which are safety and liveness determining, rather than the private attack, which is just grow a chain and then take over the public chain. Okay, so this is just a background context on approval work and the longest chain and how that the models that we talked about in the first part, which is the trust models, map to how you show whether these protocols work or not. So, but you can start from there and say, can I construct a proof of stake protocol like this? And there's been work from Cornell and other places, you know, the Snow white sleepy consensus, and then we have our borough's family of protocols. There's been, like, a bunch of work on longest chain proof of stake protocols. What do they do? Roughly? All they do is you can imagine that the staked group of validators are fixed, and all you're doing is randomly selecting one of them to make a block, and they propagate the block, and you then just track the longest chain. So this is the longest chain proof of stake family of protocols. So unlike other kinds of BFT protocols, they are very simple in that description.
01:01:57.626 - 01:02:40.680, Speaker B: Right? Choose a random node, let them make a block, propagate the block, let the next guy make a block randomly, and so on. And there is no separation of roles, which we see in all these BFT protocols. Your proposal is very different from your voting, and you may have multiple types of voting and so on, depending on the class of protocols and the phase of voting, which is, you know, whether you're in the commit phase, prepare phase, whatever. So you have a uniform type of message, which is just make a block. And we found this like aesthetically very interesting. But one can ask now, you know, there are these two families of protocols. There is the bisenten fault tolerant family of protocols and there is the longest chain family of protocols.
01:02:40.680 - 01:03:40.680, Speaker B: How are these two related? Are they related in some fundamental way? Are they actually distinct classes of protocols that achieve very different properties? And it is in this quest that we were trying to like enumerate these various properties. And I think the sharpest categorization of which what one family protocol achieves and the other doesn't is the, it's clear that BFT protocols achieve a finality, right? And finality is basically you make a block and if you get a two, three quorum, that block is confirmed. Confirmed means confirmed, right. And since it's confirmed, you have very low latency. But also you have slash ability, potentially because of the quotum set property that Kevin alluded to, which is if you have enough people signed on it, if you also sign on some other thing, you could get slashed. So that's a very natural, simple slashing. So BFT protocols have this very nice property which is slash ability and finality.
01:03:40.680 - 01:04:24.396, Speaker B: So these two kind of go together. I'm going to refer to them as the same property. Whereas longest chain protocols have this problem that they take a long time to confirm. Even bitcoin, if you wanted to do a really tight analysis, actually takes quite a bit of time to confirm if you have anything like a 40% adversity. So, okay, you're giving up something. Are you getting anything in return for like constructing these long proof stake protocols other than the fact that they're kind of simple or whatever? The main property that you actually gain is dynamic availability. And what is dynamic availability? Dynamic availability is the idea that you have the protocol.
01:04:24.396 - 01:04:53.202, Speaker B: Suppose we have end nodes, okay? And we don't know which of these end nodes are going to show up to actually participate. Maybe. For example, let's say we have a proof of person hood on all the eight or 9 billion people in the world. So you have a proof of personhood, but you do not know which of these 8 billion people are going to run nodes for you. There is no registration phase, nothing. Whoever wants, they can just show up and make a block. Okay, so this is what we call unsized participation.
01:04:53.202 - 01:05:41.810, Speaker B: Unsized participation basically means you may have an upper bound on what the participation range is and who the set of possible participants are. But you do not have any guarantee that like some fraction of them are going to show up. So this we call and a protocol which works. I'll define what works means which works under this setting is a dynamically available protocol. And dynamic availability means like different nodes are available and still the, you know, different fractions of nodes are available and still the protocol continues to march on. So the very nice thing with the longest chain protocols is they are dynamically available. Why? If you make a block and you don't know who the next block producer is, maybe the next block producer didn't show up.
01:05:41.810 - 01:06:18.550, Speaker B: Okay. That block doesn't get produced and people wait for the next slot and next slot you may have a slower production of blocks, but you do not stop. You're not waiting for a two third quorum or anything like that. So the idea is the longest chain protocols work under unsized participation. You do not know what the participation set is and still it works. And the starkest example of this is bitcoin, right? There is no way to know what is the maximum amount of mining power in the world. So you cannot say that I'm going to wait for two thirds of the mining power to show up and then make a vote on top of it.
01:06:18.550 - 01:07:04.824, Speaker B: So proof of work is naturally unsized. So obviously you have to work in a very dynamically available scenario where you have, you don't know what the upper bound on the total mining power is. So you're waiting for whoever is able to contribute the blocks. And you have this variable difficulty adjustment which tries to keep block production at some fix it rate. So that's the main property of dynamic, of the longest chain protocols is they're dynamically available. So just zooming out now, you have these two categories of protocols. On the one side you have finalizing protocols, protocols which like the canonical example Kevin described as PBFT, and the dynamically available protocols, the canonical example just being randomly sorted.
01:07:04.824 - 01:07:49.470, Speaker B: Longest chain. So these are two different protocols. And this class of protocols gets dynamic availability and that class of protocols gets finality. And one can ask, can I construct a protocol which gets you both right? And it turns out, you know, it's just a modern manifestation of the cap theorem that essentially there is a trade off between availability and finality and you cannot have a protocol which is both available and finalizing. And so, and I think this goes back to one thing which I left out earlier, which is, what do you mean by a dynamically available protocol works. It has to make some assumptions and show that it, it's safe and live. And the kind of assumptions that we usually think of are the following.
01:07:49.470 - 01:08:44.665, Speaker B: You need synchrony first to be dynamically available, because if you don't know how many nodes are participating and you don't know when they're going to participate, there's no way to get security. So you need to assume some synchrony bound, which tells you how long it takes for the blocks to reach everybody. So that's one thing that you need to assume. The other thing you need to assume is that the number of online honest nodes is greater than the number of online adversarial notes. So that will be the most dynamically available protocol is basically behaves almost exactly like bitcoin. You have like some fraction of honest nodes, and as long as the honest nodes at that time who are available on online at that time is greater than the number of adversarial nodes at that time, than the system should be safe and live. So that's what we mean by you can actually get dynamically available protocols and then you have the finalizing protocols.
01:08:44.665 - 01:09:27.690, Speaker B: And like I was saying, the cap theorem kind of rules out the existence of a protocol which does both. This was actually formalized, you know, many times, I think one of the early papers by Pasanchi, but also later in a paper by Andy Lewis Spy and Tim Rufford card. And specifically in this way, as the availability finality dilemma and a protocol cannot be both. So that summarizes my discussion of why we have these two class protocols and why you cannot get both. Because basically you cannot have uncertain participation and uncertain network behavior. Then you don't know what the set of nodes you're dealing with. So there is no way to get security in that model.
01:09:28.000 - 01:09:33.540, Speaker C: And did you construct an ebb and flow, like a way to get from one to the other?
01:09:34.120 - 01:10:07.129, Speaker B: Yeah. Okay, so that's a great question. So what happened at that time is, as we were kind of like trying to understand what is going on in this landscape, there were various pieces that we found were missing. On the one side, in the dynamic availability, we found that there is a gap in what dynamic availability exists. Protocols like ouroboros, they had a series of protocols called genesis, provos and others. What set of properties are needed to make these work? And we improved on some of that. These require some other cryptographic constructions.
01:10:07.129 - 01:10:39.080, Speaker B: For example, the verifiable delay function was integrated into one of our protocols. We call this posat. PoSAt is proof of stake with the arrow of time. And we found that this is one of the cool features of something like bitcoin, is there's a natural arrow of time. As you're mining, you can only mine on forward. And this arrow of time basically establishes, brings very strong properties, whereas in a proof of stake, you can go back. And because it's time stamps, you can always go back and try to create blocks in the past.
01:10:39.080 - 01:11:19.030, Speaker B: And one attack is the following. For example, let's say you're running this proof of personhood system that I talked about with unsized participation. You don't know how many people are going to show up. And let's say in the first year, as systems start, maybe only 100,000 people show up. And. But in the next year, there's like 100 million people show up, of which, let's say 80 million are honest, but 20 million are adversarial. But on the first year anniversary, what can happen is this 20 million adversary people can go and start creating a chain from the genocide of the blockchain and say that, hey, I've been around all these time, and I actually created this long chain.
01:11:19.030 - 01:12:14.654, Speaker B: So this is kind of like an attack on these dynamically available systems, and you need things like checkpoints to prevent this. So what we ended up doing is adding things like verifiable delay functions, which give you some kind of like an inherent arrow of time to protect you against these things, thus genuinely getting the same type of properties that something like bitcoin can get in a proof of stake environment. So that was one piece we found was missing in the proof of stake landscape, in the dynamically available landscape. But the other thing we found was Ethereum had this really interesting protocol, the evolution of the Ethereum protocol as follows. So initially they had the proof of work protocol. Even now, only the proof of work is running the main chain. But on top of the proof of work protocol, they were building this finality gadget.
01:12:14.654 - 01:13:15.584, Speaker B: There is a proof of work protocol going on. And then on top of which, there is a BFT group, which is a staked group, which signs over on some messages. And we found this to be like, hey, it seems like you cannot have both. You cannot have both dynamic availability and finality. But somehow, by constructing a longer strain protocol and on top of it having like, a finalization gadget, something interesting was happening. What is happening is that on the. If, for example, it turns out that enough people were not online, right? If enough people were not online, then what could happen is just the longest chain keeps going on because you cannot finalize anything, right? So the mental model here is there's a longest chain protocol, and then there is a BFT, kind of acting on top of it and check pointing and finalizing some blocks, signing with exactly the same type of protocols, the PBFT type protocols that Kevin talked about earlier in the podcast.
01:13:15.584 - 01:14:38.382, Speaker B: So you have a long machine protocol which keeps on running independent of like whether the forum that is there to sign the checkpoint is there or not, but the longest chain keeps going. So it seemed interestingly that they were trying to get both these properties simultaneously, both dynamic availability and finality. But wait a minute, I just said that you cannot get both. So what is going on? So this started like a line of questioning and research that actually, you know, I brought up to my collaborator David Shea at Stanford and promo that Luna at Banach Champaign, and we all started talking about it and thinking about this question, which is, can you have a protocol which kind of gets you both? What do you, what do I mean by kind of gets you both? Can I have a protocol which itself does not make a decision whether it is dynamically available or finalizing, but let the user make the decision whether it is finalizing or available. Imagine a system, you know, you're running a blockchain, or, you know, on which many different applications, like an ethereum or a solana or an avalanche, where like different applications of very different characteristics are running together. There is very hard, hard money running. On top of it.
01:14:38.382 - 01:15:32.484, Speaker B: There is financial applications. There's also things like games and poker and other things for which you need verifiable randomness, and other things running on top of the same system. And the trade offs that you want to take for these systems are potentially different. And if, instead of enshrining one of these assumptions into the core of the protocol, can we have a protocol which is generic and externalize this decision to the users? Some clients can run a different confirmation rule, which is very conservative and only takes finalized procs. Some other clients can run a different rule which continues to operate even when not enough nodes are online. So by. So then we went and looked at the core of the trade off, which is the availability, the finality dilemma.
01:15:32.484 - 01:16:48.576, Speaker B: It was not really a statement about protocols, it is a statement about confirmation rules. So you cannot have a same confirmation rule be finalizing and dynamically available at the same time. But you can have two distinct rules, one of which is finalizing and the other which is dynamically available. So we found this to be a general, interesting property that had not been quite formally thought about and put into a proper mathematical model. And this is what we ended up doing in two papers, one written by David, David Shea Yuki called ebb and flow, snap and chat, which I was not a co author on, but it was kind of like, you know, one of these academic races where you have multiple groups trying to solve exactly the same problem. And David and his students, they ended up solving it before us. And so they have this beautiful construction, which they call the Snap and chat protocol, which is at high level, you can take any longest chain protocol and any BFT protocol and compose them in a way that there are two confirmation rules.
01:16:48.576 - 01:17:34.956, Speaker B: One confirmation rule, which is dynamically available, and another confirmation rule, which is. Which is finalizing. And we also had our own version of it, which had some more interesting properties. Like, one of the properties that you have in almost all the blockchains today is what I call coupled validity. When you make a block, you already know what the state of, if you existed, what is the output state of it is. And this is built into almost all the protocols today, because this enables a variety of other things, like lite lines and bridges and other things to be, like, very deeply integrated. But the snap and chat construction does not have this property, because you have to ex post construct what the ledger is.
01:17:34.956 - 01:18:20.534, Speaker B: There are two ledges, kind of like, going on, and then you stitch them together ex post. And so that means you lose this basic property of blockchains, which is coupled validity. So we had our own construction called the checkpoint at longest chain, which actually arrived at some of these properties. Okay, so that's the high level story of the two different properties that you may want in a blockchain. And instead of internalizing it into the protocol, you externalize it to the clients. And this also, I think Satoshi leads the way for us, because if you look at the confirmation rules that different people can run in bitcoin, they're not enshrined into the protocol. Kevin can say, I'm waiting for 30 blocks, whereas I'm just selling coffee.
01:18:20.534 - 01:19:06.738, Speaker B: I can just wait for one block and sell the coffee. Within that, there's the same spirit that we can bring to the. To the blockchains to actually dynamic availability and finalization. And we found that both the finality gadget that Ethereum had thought of building on top of the provable work longest chain, and the later protocols, which is the current casper, are actually protocols of this type, even though, because of the various engineering details involved, we cannot sharply prove the same type of theorems about them. But they're fundamentally trying to achieve exactly the same set of properties that I just laid out.
01:19:06.914 - 01:19:59.818, Speaker C: So there is another way to say this is, well, it's not quite accurate, because nakamoto is an identity less protocol. There is no identities, you just produce hashes. But another way to say this is effectively, you know, fast, like a protocol that has, in the good case, a fast asynchronous confirmation using two thirds. And then if it doesn't work, then you go to a synchronous confirmation and you wait for the delta bounce, in which case you might have to wait much longer. But it's effectively basically a way to go from an asynchronous two thirds plus one to a, you know, one half plus one type of protocol. Although the mixing of the proof of work with the proof of stake might actually turn out to be very complex for the simple reason, this is one.
01:19:59.834 - 01:20:03.266, Speaker B: Of the reason that the ebb and flow paper was very hard to, like.
01:20:03.338 - 01:20:42.162, Speaker C: Get correct, because one of them has no identities, the other one has identities. And so you have to now reconcile the two, which is incredibly difficult process. But actually, Thunderella was one of the first few. I'll tell you a story after this. But Thunderella was one of the first few papers that also introduced some of this idea. This was Elaine, she's work where basically you have like a fast confirmation track, and then you go into a slower track, which is synchronous. But funny enough, there was some very early work at Cornell that we were working on, which was trying to work with this idea that or isn't the problem, or trying to circumvent the problem.
01:20:42.162 - 01:21:36.156, Speaker C: Like, there's this feeling that, like, people have against proof of stake, that, like, you have to have coins in order for you to produce new coins versus a proof of work. You don't have to have coins in order to produce new coins. You just have hash power and you produce new coins. So how can you mix the two? How can you create a proof of stake protocol that doesn't need coins in order to produce new coins? And what we came up with, but it wasn't published, was a new protocol that basically had a two chain model where the chain model on top just was a proof of work protocol that just produced coins. You just use it for coin production, nothing else. And then it air dropped the coins into the proof of stake protocol, which then you would use to just simply validate the blocks, and you would checkpoint back into the proof of work protocol. So you would have this proof of stake protocol.
01:21:36.156 - 01:21:53.480, Speaker C: It just has a bunch of validators, they have a bunch of stake, but the new coins don't come from their own validation. They just produce. They just collect the fees in the blocks, but the new coins anybody can produce. So I just produced them over there. Checkpoint back here. And we never published this favor because.
01:21:53.520 - 01:22:06.850, Speaker B: You thought it was. That is so fascinating because I've thought about exactly the same problem, which is very different from the ebb and flow reason, which is that proof of work has a kind of token distribution model, which is very different from.
01:22:06.930 - 01:22:07.410, Speaker C: Very different.
01:22:07.450 - 01:22:20.162, Speaker B: That's right. Token distribution model, which is actually whatever. Whereas in a proof of work you can just like let people mine and earn it as, as they deem fit. Is this fascinating that you were thinking about this?
01:22:20.266 - 01:22:45.840, Speaker C: Yeah, because we were like, we kept hearing, oh well, proof stake, you know, you have to have coins and so on. It's a very non open system. Even though it's kind of crazy to say that because you know, like buying hash power is just as gatekeeping as buying tokens. You have to go to some guy that has hash power, buy the equipment. It's kind of a pointless discussion. So ultimately we thought this was stupid, but we were like, ok, well let's entertain this idea for a second.
01:22:46.980 - 01:23:18.224, Speaker B: One really interesting thing is in this use case, there is some interesting thing about it. For example, let's say that most of the tokens were in some countries that banned cryptocurrency staking and therefore they got hold of the keys or something. Then what could happen in a protocol like that is more pro work. Miners from other countries can then start mining mining blocks and then like gaining enough of these blocks to then start producing a quote.
01:23:18.352 - 01:23:18.776, Speaker C: That's right.
01:23:18.808 - 01:23:21.720, Speaker B: So there is an interesting interplay of some of these ideas.
01:23:21.760 - 01:23:45.234, Speaker C: Yeah, yeah. Basically, ultimately you have to. But ultimately it's a, it's effectively a very tough question on when you finalize, is it the finalization at the two thirds plus one or is the finalization at the, you know, one half plus plus one? Which is, you know, it becomes very complex and ultimately people just want to be told, when do I finalize? And I don't know when that answer to that one is. But this was in some sense, you.
01:23:45.242 - 01:23:58.990, Speaker B: Know, you are, the risk determines like what it's exactly like in bitcoin. Right? Like if you're, if you're a copy coffee shop, you can do even zero. And if you're selling a Ferrari, you better wait for like a, you know, half a day or whatever.
01:23:59.070 - 01:24:31.926, Speaker C: Yes, yes. But then there's also all kinds of, all kinds of like crazy stuff that happens here because like then, you know, the proof, the proof stake layer, which is doing the actual block generation at which point does it consider a new staker to be available? Right. Like a new staker to have come into existence because it has new coins, it has to account for the proof of work chain. But at which number of blocks is it finalized? And so suppose that one forks. Now this one has a staker here which was not supposed to be there in the first place. And so all kinds of stuff blows up.
01:24:32.078 - 01:24:48.974, Speaker B: That's really actually like. That explains some of the intricacy of getting the ebb and floating correct. The idea that if you have two distinct chains, they have to be tied together and intermingled correctly and each one has to respect the confirmation of the other one in some intricate manner.
01:24:49.062 - 01:24:49.390, Speaker C: Exactly.
01:24:49.430 - 01:24:59.454, Speaker B: That's why, you know, actually it took us nearly like one year to both of our groups to actually race to kind of find the correct ebb and flow model.
01:24:59.582 - 01:25:35.754, Speaker C: Yeah, it's fascinating. It's a fascinating question. And, but it's been actually, it's also something that, you know, very kind of sneak peek on avalanche v two. We're not going to prove work, but some sneak peek is around increasing dynamically the liveness of the protocol from here all the way to a two, three plus 133 percent. You can obviously go beyond that up to one half. As long as you only use voting. It's fine because ultimately we have a staking set.
01:25:35.754 - 01:26:07.140, Speaker C: We're not using identityless verification, but it's a very straightforward mechanism. If you have identities across the board, you can go from whatever avalanche is right now to one third to one half as long as you keep the same set of validators. That's easy. The second that you start going outside of that and you start saying, I want even stronger guarantees. I don't even want to rely on a known set of validators. I just want any blocks to be produced by anybody, which is what proof of work does. Then it just becomes incredibly complicated.
01:26:07.140 - 01:27:12.866, Speaker C: But I think if there is somebody that writes a paper that specifies very precisely what the transition function from a identity based voting protocol into an identity less protocol safely, then you can actually have any proof of stake protocol ultimately collapse down to the same guarantees as the proof of work protocol in the same sense of censorship, resistance and so on. In the worst case, in the normal average case, day is good, day is sunny. It just confirms fast. It never has to touch the proof of work. The second that something goes wrong, like, let's say a bunch of machinery gets turned off or whatever staking goes offline, then you can actually collapse down and use any available machinery anywhere across the globe that doesn't have any tokens. And now you gain the liveness back. But again, the more liveness we try to gain, there is this trade off that the higher the probability of us losing safety, which is somewhere along this path, somebody will be sacrificed.
01:27:12.866 - 01:27:24.270, Speaker C: Like that's a fundamental, like trade off. Somebody will be sacrificed here, and sacrifice will be basically on a long sequence of chains will get reorged at some point.
01:27:25.410 - 01:28:42.596, Speaker B: That's the sacrifice that is the necessary risk of using an available protocol. So I want to explain a little bit more on what Ethereum did, which is like really interesting. Yeah, I call this the recovery of the safe, the finalizing rule. So what Ethereum does is we talked about slashing a bit earlier in the conversation, and the thing that we were talking about there is slashing for safety, but there's also, what if nobody shows up and does anything, then is there slashing or is there anything that essentially prevents that? And this could be because of some attack, which could be basically a ban, for example, could essentially, like the China ban on mining, for example, stalled, could have stalled bitcoin block production if it were not for the dynamic availability. Okay, so what is Ethereum's proof of stakes idea for solving this problem is. So what happens is in the. So firstly, when, let's say, more than 50% go offline, either the synchronous nor the asynchronous rule can make any blocks at that point, but the longest chain keeps marching on.
01:28:42.596 - 01:29:03.598, Speaker B: So you're making blocks and making blocks in the longest chain. Right. This is one of the reasons the particular specification of Gaspar is a little different from what Kevin pointed out, which is you're sorting these committees and they're voting on these different blogs, but you do not need a two third of the committee to sign in order to go to the next block. It's going on and on, and you're just adding attestation weights.
01:29:03.694 - 01:29:10.422, Speaker C: As long as eventually you collect two thirds plus one of the entire committee, then you can finalize. But you can continue. Then you can finalize.
01:29:10.486 - 01:29:12.890, Speaker B: Exactly. You don't need to finalize, you can just keep.
01:29:13.780 - 01:29:19.880, Speaker A: So there's no guarantee or until it's actually the two thirds vote that the transaction has been finalized.
01:29:20.380 - 01:30:00.766, Speaker B: There is a probabilistic guarantee. Just like in a longest chain protocol, there is a probabilistic guarantee, assuming that the fraction of the honest guys who have shown up is greater than 50% and the network is synchronous. You can actually finalize a block when it's like three deep or 40 per fight, whatever. So there's a confirmation parameter exactly like in bitcoin. So there is probabilistic finality, no deterministic finality. There is probabilistic finality under these assumptions, which is that the fractional honest guys online is greater than the fraction of adversaries online, and the fact that the network is synchronized. So under those you can actually get a sharp mathematical guarantee on.
01:30:00.918 - 01:30:06.038, Speaker C: Actually, let me understand that a little bit. You do not have finalization on the, until you get the two, three, one.
01:30:06.214 - 01:30:09.286, Speaker B: Commit unless you have dynamically available rule.
01:30:09.438 - 01:30:13.350, Speaker C: Okay, I got it. So you're leaving to the user, you're leaving it to the user to decide. Got it.
01:30:13.390 - 01:31:03.112, Speaker B: Okay, we have two rules, right? And we have to specify the rules and the user can choose one rule or the other, and the dynamically available rules, the way it doesn't finalize, but it confirms, right? Because it's a weaker confirmation. The weaker confirmation is if you have like, you know, your block deeply embedded into the longest chain, then you confirm. So that's the standard bitcoin type rule. So you have like these two different rules. One is the dynamically available rule, and which can confirm under these synchrony and correctly available conditions, but does not confirm correctly under an asynchrony and the availability conditions are not met. So what happens? So let's kind of play this attack through this system. So the system has one more feature which I think is very important, which is the slashing, or it's called inactivity leak.
01:31:03.112 - 01:32:15.566, Speaker B: In Ethereum, the inactivity leak is again the idea that if finalization have stalled in the chain, every like 32 blocks or 64 blocks, you're finalizing, like you're getting a finalized, finalized block, but it has not happened for like 128 blocks or whatever, you start activating an inactivity leak. An inactivity leak basically says on this chain, since I have not seen any finality, I'm going to start slashing or like penalizing or leaking. The validators that were actually on did not sign messages in this chain. So you start leaking these guys, and over like a period of weeks, I think it's like three weeks is the time constant. It's like there's like an exponential decay of like how much stake is slashed within a period of weeks. What happens is if all the offline stake would have been slashed in a period of weeks, or at least like a significant majority of the offline stake would have been slashed, and suddenly in this chain you have a majority of stakers online who can now start finalizing a block. I call this the self healing property.
01:32:15.566 - 01:33:37.700, Speaker B: So what has happened is, like, nodes have gone off offline, and the longest chain rule has continued, and keeping the system alive and keeping the system alive enough to account for the fact that I'm going to penalize the offline users, and then now suddenly, from the dead like a phoenix, raises back the finality rules. So it's a very interesting, like, property. But, you know, intuitively, the first thing that occurs to me is, wow, that's interesting. But also that must violate some cap theorem somewhere. And the place it violates the cap theorem is if instead of these nodes actually going offline, if these nodes that just stored their blocks somewhere, and then suddenly, you know, the nodes that went offline wake up and they say, here is my, like, alternative chain. And then you have two conflicting factors of history. And this is where the kind of core underlying assumption of almost all proof of stake systems is used in Ethereum, which is the weak subjectivity, which is the fact that you need users to be online at least periodically, at least in a matter of weeks or months, because if they're not, then people can use the old stake keys and start signing old blocks, which basically don't make any sense, and you cannot slash them because they've already like, bought that Ferrari and settled in the Caribbean.
01:33:37.700 - 01:34:12.846, Speaker B: So that's the design and architecture that I, that was part of the Twitter post that I wrote when we realized that basically the combination of this, like, Airbnb flow type model, along with the inactivity leak leads to a self healing property, and it is optimized for a certain type of adversary, the adversary who can shut off mining, but not take over everybody's keys and start doing like, complex protocol attacks. So that's the. Yeah, that is the adversary for which this system makes sense.
01:34:12.958 - 01:35:09.016, Speaker C: So Elaine's sleepy model was basically the same thing where you only really use like, act online nodes and you make a lot of assumptions, but exactly right. Like, ultimately, there is a very clear trade off. You really cannot have both safety and liveness guaranteed. If you do want the liveness, somebody's getting sacrificed. And as we, in fact, we can probably build, you know, I can probably figure out, or even here, like, we can come up with a very simple protocol where I can guarantee to you that as long as one single staker is online, the network will make progress. The problem with that, of course, is that even if that guy, last guy is correct, you can create a network here that even just one byzantine guy at the very end equivocates, the whole network gets reversed. And so there's a whole bunch of problems with it.
01:35:09.016 - 01:35:11.224, Speaker C: So to me, it's like the way.
01:35:11.272 - 01:35:29.498, Speaker B: This problem is containerized in Ethereum is the weak subjectivity assam assumption. And the assumption is that because, you know, you've taken, like, three weeks to actually implement this inactivity leak. And if people bear online at least once during this period, they will not accept a competing chain.
01:35:29.594 - 01:35:29.874, Speaker C: Right.
01:35:29.922 - 01:35:52.036, Speaker B: And if this assumption is violated, then you get into the problem. So the way the whole thing is containerized is you say, yes, you get the recovery of the finalizing rule if the weak subjectivity has, which is like a, which is not a normal path in the design of PFD. But maybe it makes sense in a practical system to actually think about this.
01:35:52.148 - 01:36:05.612, Speaker C: Absolutely. Well, the obvious artifact of that is the finality is only three weeks away, because you have to assume that in three weeks, that's really when we have, like, done everything and now we are safe.
01:36:05.716 - 01:36:11.726, Speaker B: So basically, you are not going to get the fed up sold for the three weeks. You could still sell your coffees for your three weeks.
01:36:11.758 - 01:36:12.390, Speaker C: Correct.
01:36:12.550 - 01:36:24.230, Speaker B: Assuming that, like, maybe, you know, I'm gonna lose your money. But the most important thing is, after three weeks on the system, you can have the Ferraris be sold without any manual intervention, which I found, like, fascinating.
01:36:24.310 - 01:36:36.144, Speaker C: It is. It is. Yeah. It's. It's definitely a set of things that. That would make. It would be interesting for, like, a lot of protocols, I think we're thinking about this kind of stuff, and we've thought about this kind of stuff for years now.
01:36:36.144 - 01:37:02.280, Speaker C: On the avalanche side, oftentimes, we tend to be very allergic towards anything that delays finality. So we can add as much live as possible. But if it delays finality, we're like, oh, okay, well, that's not going to work for us. It's like, we want instant finality. So that's usually how we go about it. But no, it's really fascinating. So, you know, and did you guys ever implement, by the way, ebb and flow or like, at least like, prototype?
01:37:02.320 - 01:37:09.848, Speaker B: Yeah, yeah, we have both the versions of the protocol all implemented. Yeah. Happy to share. It's on the papers, but I'm happy to share the links.
01:37:09.904 - 01:37:13.752, Speaker C: Amazing. Amazing. Yeah, I'd love to at some point, take a look at the GitHub or whatever. You guys.
01:37:13.776 - 01:37:40.690, Speaker A: You guys have spicy questions. I think outside of Ethereum, like, for you, Shiram, specifically, what would you say? Or like is one pro and con that you think of like Solana and Avaxen and then Kevin, like Eth and Solana, spicy.
01:37:42.670 - 01:37:44.454, Speaker C: Whichever you want to go first. Your.
01:37:44.582 - 01:37:46.290, Speaker B: Yeah, Kevin, you should go first.
01:37:49.190 - 01:38:30.108, Speaker C: So, I mean, I think Solana obviously is practical. They try to go after practical stuff. There is issues with some of the economic designs that I think the early designs of allowing super cheap transactions just obviously not going to work because eventually you're going to have spam, and spam is put in the same type as high value transactions. I think they're changing this now. They're adding per contract hotspot type of stuff, which is something that we're also thinking about. And that stuff, I would say that like all the EVM based chains should be implemented. It's a fantastic thing to do.
01:38:30.108 - 01:39:33.056, Speaker C: So I think it's a, you know, in a practical consideration, it's, it's a chain that, you know, it's trying to work through all the kinks and all that kind of stuff. I will say that I don't fully grasp the safety analysis of Solana just yet. Their, you know, optimistic confirmation stuff is like, you know, I've spoken to anatoly about this. Like it's a, it's, you know, it doesn't really matter unless you get the two three plus one, this is useless. You're going some other territory. So I don't quite fully grasp, or rather I'm not 100% confident in the safety analysis of it, but assuming that it's using common, well understood principles, assuming that it's using the same kind of set theoretic proofs that any PBFC system will use any voting basis and would use, and they have worked through them out, at least on the back of a napkin, and it seems to be reasonable, then those are well established proofs. So as long as they have the quorum intersection property, we are good.
01:39:33.056 - 01:39:38.504, Speaker C: I think it's safe, but I'm not fully sure that they haven't put it that correctly. So that's something.
01:39:38.552 - 01:39:41.128, Speaker A: This is the current implementation or a future implementation.
01:39:41.184 - 01:40:19.170, Speaker C: I'm talking about the current implementation. I'm not exactly sure they have fully, fully gone through all of the important stuff intersection guarantees that we would need. Once you have those, and the proofs are easy, we know them, but without them it's really hard. I don't know that they quite have them, because I try to look at the specification from the docs, to be fair. They don't really have very good docs, so it's partially my fault. So I don't really understand it because they don't have the literature for me, but based on what I read, I didn't quite see a specific part in the same way that you would see for Cosmos. Cosmos is a very clear specification right there.
01:40:19.170 - 01:40:42.230, Speaker C: They have quorum intersection. There is no dispute around it. I didn't see that for Solana. So it's not, I don't have, like full confidence that it is there yet, but what I like about it is very practical. You know, it doesn't try to do a lot of crazy stuff. So, you know, hopefully worship kings and so on. That's the Solana side on the ETH two side.
01:40:42.230 - 01:41:05.982, Speaker C: The ETH two side is interesting. EtH two side is a lot of. I mean, frankly, I would just say. And the Ethereum people, I. Listen, I have said this in the nicest way possible, because I want Ethereum to succeed. Because if ethereum goes down, I am in a world of hurt. Everybody's in the world of hurt.
01:41:05.982 - 01:41:46.620, Speaker C: So I wanted him to succeed, obviously, economically, I'm going to be in the world of hurt. If ethereum goes down, we are going to be in a very deep bear market. So I want the east merge to go successfully. I wanted to work out the issue. Like my honest to God representation as somewhat of an expert in distributive systems is they probably should have just done hot stuff or maybe something like an avalanche pork or something like that, but they don't have to go the avalanche way, do hot stuff. If you really just want to go down the voting based side with, you have all the certificates and everything, just do hot stuff. And it would have been much simpler, much more straightforward.
01:41:46.620 - 01:42:32.512, Speaker C: Obviously, all these additional things they're trying to add are super intellectually stimulating, really, really interesting. They're trying to solve for all the right things. It is a little bit difficult to believe that the implementation will be correct. I'm a little worried that it's a little bit too complex because they're trying to have their cake at 82, and when they usually try to try to do that, it starts getting very complex and some stuff will start breaking and so on. And so that's my general critique. On the one side you have Solana, which is just like, do anything that works. And then on the other side you have ethereum, which is like, make it work in the most crazy way possible, like most adversarial way possible, but it's not even clear that it's going to actually perform like the way you want.
01:42:32.512 - 01:43:08.590, Speaker C: And so these are my critiques and the pros of it. But ultimately, these are both big projects. They both should succeed because failures of both would really be pretty bad for all of us. And they're frankly also not trying to solve for the bad things. So I'm definitely rooting for both of them to be very successful. So, yeah, that's at least my interpretation of it all without being too political, even though anything that I say will be deemed political. But it's sheer fact that I'm a vox Maxi, so anything it could be the sky is blue through you.
01:43:08.590 - 01:43:11.190, Speaker C: We don't want to listen to you.
01:43:13.010 - 01:43:25.430, Speaker A: We'll clip it all and put it to twitter, get some spicy takes, and then Sriram, what are your kind of, like, pros and cons of Solana and then Avax as well?
01:43:26.460 - 01:44:56.328, Speaker B: Yeah, Solana, clearly the pro is high performance and a beautiful programming environment for people to build on. The con is, of course, everybody talks about the stalling problem and the fee market, so I'm going to leave that out. Another one is the more thought needs to be put into the censorship of protocol like Solana, particularly because, you know, it is taking a unique approach in saying that let, you know you need a much higher entry barrier. So it is not safety failures that we should worry about that much. It is really censorship resistance, which is, and, you know, the idea that, oh, we have many block producers is not the same as censorship resistance. Because if 50% of the stake is with one node and you even have many block producers, it doesn't matter whether they get included or not, is absolutely at the mercy of the 50%. So putting more thought into how you achieve censorship resistance in this protocols is the other projects like Ethereum and Avalon launch and so on have basically by making the entry barrier low, you know, you get some natural notion of, you know, decentralization that leads to censorship resistance.
01:44:56.328 - 01:45:58.250, Speaker B: It is not simply a matter of how many independent block producers do you have per second, which is the metric I've seen Solana gravitate towards. It's actually how many block producers, how many independent block producers contribute to percent of the stake? What is the minimum number of block producers needed to contribute to 50% of the stake? Because they can kind of take over the chain in a way that's adversarial. So anyway, I think that's one, I would say, recommendation for what to look at more deeply is the ideas of censorship resistance. And here I think one thing we can look to is things like roll ups. So roll ups absorb sensor resistance from the core chain. And, you know, I think there are many more ideas that are simply not explored in consensus as to how to separate safety and liveness and have a different layer underwriting liveness and a different layer for safety. I think there may be something interesting there.
01:45:58.250 - 01:47:26.602, Speaker B: Of course, you know, I appreciate one thing about some Solana which is totally on Twitter being very honest about either the challenges or Solana is just a dos resistant token or whatever, is very honest stakes, I think is something which is very good. On the Avalon side, I think clearly they had one major, major breakthrough is fast finality and kind of constructed a user experience centered around that. And I think that is clearly paid off. And because, you know, so when people thought about like formally think about consensus protocols, one of the most important things is the time to finality. And, you know, by obsessing about that, I think I definitely avalanche has done a good job on creating an entire developer experience and bringing in use cases that would simply be not tenable. The con side is we discussed crypto economics and slashing. And I do hope that as the understanding of these things emerge and we have better and better understanding how to leverage it both in the core chain and the subnets, is something that could be quite, quite useful on the subnet thing.
01:47:26.602 - 01:47:47.760, Speaker B: You know, we see a lot of discussion just about slashing, but I think there is like a lot of benefits to the subnets. Clearly it maximizes the heterogeneity in the variety of different applications that can be run. But if that can be merged with a strong crypto economic underwriting of the safety edit, that could be really interesting.
01:47:48.340 - 01:47:48.972, Speaker A: Awesome.
01:47:49.076 - 01:48:06.760, Speaker C: The second a paper comes out on slashing being provable, then I've said I am all for it. That's the second that I go for it, not opposed to it. It just needs to be done in a principled way, I think. That's all. At least I request for.
01:48:07.580 - 01:48:25.502, Speaker A: Awesome. Well, thank you again, gentlemen. It's been a pleasure. Hopefully we'll do a part two. In the meantime, we have a lot of things we still have to discuss, but no, I really appreciate the conversation and your time. Excited for people to listen to this one.
01:48:25.686 - 01:48:26.582, Speaker C: Thank you.
01:48:26.726 - 01:48:27.142, Speaker B: Thank you.
01:48:27.166 - 01:48:28.190, Speaker A: Awesome. Thanks, guys.
