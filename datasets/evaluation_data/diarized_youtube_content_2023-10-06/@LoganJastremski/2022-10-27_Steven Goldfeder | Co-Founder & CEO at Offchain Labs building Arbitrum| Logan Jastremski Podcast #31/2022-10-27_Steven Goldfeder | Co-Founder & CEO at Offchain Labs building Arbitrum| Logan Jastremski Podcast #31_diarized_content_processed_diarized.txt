00:00:01.400 - 00:00:30.040, Speaker A: Awesome. Super excited to have you here. Steven, thank you so much for coming on the podcast. I a fan of kind of all the work that you and off chain labs have been doing to scale arbitrum and more holistically, just scaling kind of the blockchain ecosystem. I'm a very big fan of anything that brings more users into the ecosystem. So it's been fun to follow along with you and your team's work.
00:00:30.780 - 00:00:36.556, Speaker B: Awesome. Well, thank you for having me, and it's great to be here. I'm excited to dig deep into a bunch of those things.
00:00:36.748 - 00:01:03.990, Speaker A: Perfect. So I would like to, before we jump into Arbitrum, more specifically, I wanted to kind of start off with Ethereum and kind of arbitram's overarching goal of scaling Ethereum. I think now there's many different choices and many different flavors of L2s on many different chains. Can you talk about you and your team's focus to scale Ethereum specifically?
00:01:04.810 - 00:01:42.588, Speaker B: Absolutely. So it's funny because if you go to the history of arbitrum, actually, arbitrum predates ethereum as a chain in some sense as an academic project, not the modern version of Arbitrum or the current flavor. But if you actually, you just go on YouTube and look for Arbitrum, I used to say back a year or two ago, I say look on YouTube and search for Arbitrum. There was one video, now there's plenty. But if you look on YouTube and look for the oldest video in Arbitrum, you'll find a Princeton course video from the fall 2014 semester. I think the video was uploaded in January of 2015. This was at the stage of the project.
00:01:42.588 - 00:02:08.806, Speaker B: My co founder, Ed Felton, was a longtime professor at Princeton. He started this project as an academic project. It was his idea years ago. I got involved a little bit later as an academic project, but it started off from Ed. And then there was this Princeton seminar where Ed led a project on this. The reason I mentioned this is because the project goes back really to the earliest days of smart contracts. Smart contracts were theorized.
00:02:08.806 - 00:02:57.392, Speaker B: There actually was no ethereum. There was no smart contract platform. There were already academic papers that were saying, hey, smart contracts have some issues with scalability. And one of the early papers was maybe 2013 paper was called the verifiers dilemma, a very early academic treatment of this that pointed out some of the fundamental issues around smart contracts. And Ed basically said, hey, I want to tackle this problem, and I want to think, how would we build a scalable scaling solution? Fast forward. Ethereum launched in 2015 so six months after that video, and then Ed actually went off to the White House and the project lied dormant for about a year and a half or two years. And then when he came back in 2017 is when I got involved and we picked it up from an academic project.
00:02:57.392 - 00:03:33.914, Speaker B: And at that point, and from that point on, the company formed in 2018. But from that point on, it was always exceedingly clear to us that Ethereum was the dominant smart contract platform. Ethereum was where all the interesting activity was. It was the technical leader, but also the community leader, both of which are really important. And it's a very simple equation that we make today in the company. We're growing, but we're still limited resources. And I say if we could take this many developers and I choose between making the arbitrum and Ethereum experience that much better for trying to spread out other chains, it's very, very clear.
00:03:33.914 - 00:03:50.110, Speaker B: We're believers in Ethereum, we're believers in the long term, that long term, Ethereum will be the dominant smart contract platform from a technical perspective, from a community perspective. And there's so much that we can do to make the Ethereum experience better, to scale Ethereum, and we're all in on that.
00:03:50.230 - 00:04:50.120, Speaker A: Nice. No, I definitely appreciate all the context. It has been impressive just to see how far Ethereum has come, and even more wild, in my opinion, just kind of hearing that you guys were kind of ideating on this back at Princeton, even before Ethereum release. So that's super cool, maybe kind of touching and staying on the Ethereum topic, before we jump into some of the specifics on arbitrum and what you're doing. So I think today their roll ups more broadly can kind of use a lot of data and posting back down to layer one. So today's, I think the average block size of Ethereum is around 85 or 90 kb. But ultimately, kind of with some of the upcoming upgrades on Ethereum EIP 4844, you'll kind of get access to more data.
00:04:50.120 - 00:05:11.000, Speaker A: And then ultimately, I think once full sharding is implemented, it will be about 16 megabytes per block. Could you talk about from the L2 standpoint, what each of kind of this upcoming EIP unlocks for you? And then ultimately, once full sharding is unlocked, kind of how that affects you and your team as well?
00:05:11.770 - 00:05:56.646, Speaker B: Yeah, absolutely. So there are a lot of different sharding proposals and as you mentioned earlier, proposals that will hopefully get deployed more quickly, like EIP 4844 and then more general data sharding. So let me give you first the problem from a L2 perspective, and then I'll talk more about the problem is. So what we do is the way L2s work is we scale by taking a lot of the computation off of the blockchain and take a lot of the work off the blockchain. But we leave just enough on the blockchain to make sure that we can stay accountable for everything we do off chain. And therefore we can be challenged on chain. We have to prove on chain of the correctness of what happens off chain, because ultimately that's what we want.
00:05:56.646 - 00:06:23.788, Speaker B: We want the security of Ethereum even for what happens off chain, which means Ethereum needs to actually verify that, hey, even though I didn't do all that work, I believe that it's correct. And the first step of that is having the data availability. The Ethereum needs to know that this is the data. This is the set of transactions. When we talk about data, we talk about the transactions that are sent to Ethereum or to arbitrum. And the question is the way that it basically works, this is arbitrary. This is really any roll up.
00:06:23.788 - 00:06:54.606, Speaker B: Lots of transactions get submitted, the execution is taken off chain, and then you prove back to Ethereum what happened. Rather than doing all the execution on chain, you prove back to Ethereum what happened. Now, roll ups differ in how these proofs work. Are they zero knowledge proofs? Are they fraud proofs? Or in an optimistic case, we can get into all of that, if that's of interest. But the thing that they share is that you're proving back to Ethereum what happened. And there's a deterministic process that there is one correct result, and therefore you can prove about what that result is. And you can argue this is the correct output.
00:06:54.606 - 00:07:32.822, Speaker B: This isn't so the critical starting point to all that is having the data available, because if we don't even agree on what the set of transactions are, we're going to be hopeless to agreeing on what the output is. We're trying to say we executed all these transactions off chain. This is the final result. But if you say, well, the transactions we're talking about are transactions a, B and C, and I say, no, they're d, e and f, what are we even talking about? We can't even possibly try to get to a point of agreeing on the output. So there's two problems, really. There's the data availability problem. How do we just get agreement on what the underlying data is? And then there's the problem, okay, we know what the set of transactions are.
00:07:32.822 - 00:08:15.356, Speaker B: You and I are both looking at the identical set of transactions. But what is the execution of those? How do we make sure that the execution is correct, that these transactions were correctly executed off chain? But the first problem is, so how do we make sure that the data availability is correct? And there were different solutions in the history of Ethereum scaling. So plasma, if it was an old blast from the past here, really tried to take everything off Ethereum, take the data off Ethereum, too. It got into a lot of problems there. Rollups actually said, let's do something much simpler. Let's just put all that data on Ethereum. And they said, even if we put all the underlying transaction data in Ethereum, there's still a lot we can do by taking the execution off chain.
00:08:15.356 - 00:08:50.000, Speaker B: So we're not going to try to get everything off Ethereum, we're going to try to just take a lot off Ethereum. But the data availability problem, which is one of the hardest problems back in the plasma world, we're going to just subvert that problem completely by just putting all that data on chain. So now the problem is, and that's what happened. That's where rollups work. You take a look at arbitrum today. We're about 97% cheaper than ethereum on average. But the majority of the cost that remains actually putting those bits of data on Ethereum, putting that transaction data on Ethereum, and it's two sides of the same coin.
00:08:50.000 - 00:09:40.594, Speaker B: The less that we can put on Ethereum, the more transactions we can fit in arbitrum, because in theory at least, if you can use the same space in Ethereum to squeeze more transactions in, then we're doing better. And it's also the cost of transactions, because, hey, it costs money, costs eth to put that data on Ethereum, and therefore that ETH has to get passed on to users because we want a sustainable ecosystem. And the lower that we can get those costs, the more, the cheaper that we can get. So, okay, so that's sort of the background. And now, so how do we get that data cost cheaper? And there actually is interesting, there are L2 approaches, and there are layer one approaches. We recently shipped arbitrum nitro, and one of the benefits of arbitrum nitro, one of the many benefits was better compression. So actually using less data, that's one way you reduce the cost.
00:09:40.594 - 00:10:29.848, Speaker B: You say, hey, if I can compress better and just use less data, then that's great, I'm using less. So now let's talk about the layer. One approach is saying, hey, even if we use less data. We're still using data no matter how much data we use. Can we reduce the costs of using that data? Right? Can we reduce the costs of using that data? And that's where these various proposals around data sharding come into play. And they're going to make storing data on Ethereum significantly cheaper over time. And interestingly, some of these, like EIP 4844, are, you know, as opposed to some of the further proposals down the line, the earlier ones at the very least, are really targeted at rollups, right? They're saying we are going to make the data that rollups use cheaper, and therefore we're going to really make roll ups cheaper, which is interesting if you think about it from a roadmap perspective.
00:10:29.848 - 00:10:41.128, Speaker B: It shows, like, Ethereum really has this roll up centric roadmap where all this layer one work is going on to reduce the cost of putting data on Ethereum that will primarily benefit roll ups.
00:10:41.224 - 00:10:58.200, Speaker A: Definitely. Maybe to continue pulling on that thread. What specifically, in your words, is Ethereum doing that allows the cheaper transactions on the layer one so that roll ups ultimately are cheaper when posting back that data down from the L2.
00:10:58.900 - 00:11:05.452, Speaker B: Yeah. So it's about effectively not to get too technical, but happy to get as.
00:11:05.476 - 00:11:06.812, Speaker A: Technical as you like.
00:11:06.996 - 00:11:54.930, Speaker B: Okay, then let's get a bit technical fundamentally, and I don't want you. There are many, many different data sharding proposals, and some of them get quite a bit technical. But the basic idea is that there's these, like what they call these data blobs that you put on Ethereum. And part of it's around the persistence of this data, how long it needs to stay. It needs to stay there, right? So have data that only needs to stay for a relatively short period of time. But by optimizing the way that Ethereum stores, stores its data, we can make this class of data blobs available to roll ups in which they. In which there's this cheaper data option to put your data on Ethereum.
00:11:54.930 - 00:12:52.650, Speaker B: Essentially, there is one thing which people saw, even the way we do today, the way we make roll ups work today, there's different types of data on Ethereum. Taking data and just what's called call data or data that Ethereum logs is much cheaper, for example, than having data that's stored in the Ethereum state tree, where you actually have data that's tracked by contracts and stored in the tree. So there are different categories of data. Ethereum, for rollops, you really need the most basic thing. You just need to know that that data is available and it can be used in a challenge, for example, to get consensus on what the underlying data is. So essentially having these sort of cheaper forms of data that you can put on Ethereum and use these advanced schemes for that data is committed and you can sample the data to make sure that data is available over time. Lots of innovations going on there.
00:12:52.650 - 00:13:00.050, Speaker B: And so committing and testing that the data is there is going to ultimately lead to cheaper data availability.
00:13:00.470 - 00:13:03.926, Speaker A: That totally makes sense. Yeah, I've been fascinated kind of.
00:13:04.118 - 00:13:17.110, Speaker B: And also, sorry, just the last point was. And also which the short term aspect also keeps data. Right. So the idea that the data is not going to be always available in this form forever, but available for just enough time for these disputes to happen, essentially.
00:13:17.270 - 00:14:14.296, Speaker A: Definitely. That totally makes sense. Yeah. I personally have been super interested in identifying the different bottlenecks in blockchains and kind of making sure that data is available and that you have enough data on kind of the layer one to scale these different blockchains has been a huge focus of mine. And so it's been cool to kind of watch the Ethereum roadmap over time and kind of these different Ethereum enhancement proposals to see how they're ultimately changing how much data is available. And so EIP 4844, I think it's like either a ten or 20 x improvement and the data that available and then on top of that with full sharding, I think ultimately each blocks will have 16 megabytes of data available, which is super cool to kind of watch the progression of Ethereum and how they're continuing to ship these different product features.
00:14:14.458 - 00:14:55.366, Speaker B: Yeah, absolutely. And in terms of the, like, you know, I don't actually know, to be honest, the exact like I've seen like proposals that say, oh, like this is going to reduce roll up costs by 100 x. To be honest, I'm not fully convinced that that's, you know, that we'll actually see that much. You know, I don't know for sure. I have some questions still what the exact will be, but I think it will be, you know, very, very significant. I think, you know, certainly in order of magnitude, likely a lot more than that, which will be, again, roll ups today, obviously Ethereum fees are depressed, but roll up transaction fees, like your typical arbitrary transaction fee is in the single cents today. And again, it all depends on what you want to do.
00:14:55.366 - 00:15:26.910, Speaker B: But for things like sending eth or swapping assets, usually in the single cents to maybe $0.10 area. And if we get, and again, remember the vast majority. So in that is there again to get a little bit technical. There are layer one costs and L2 costs, right? So layer one cost is this data that you're posting on Ethereum. The L2 costs are metering the usage of what you're doing on L2, similar to how you would on Ethereum today, just like, you know, gas metering based on what you're doing. But the majority is actually layer one cost.
00:15:26.910 - 00:16:17.502, Speaker B: So if we can reduce that by an order of magnitude, or somewhere between one and two orders of magnitude, that's going to be a massive savings for users. And ultimately I think it will, well, position Ethereum and its scaling solutions like arbitrum to be not only the things that Ethereum had today. So technically, clearly the leader, community wise, clearly the leader, but from a cost of scalability perspective, often other solutions have done better in the past. And that's my opinion why these alternative layer ones even sprung up, because of there's practical concerns where it wasn't. I haven't, I can't tell you. I don't think I've ever had a conversation where someone said to me, unless it's someone particularly involved in a project, an application that's sort of neutral, said, hey, I don't want to be in Ethereum. It's always been like, I want to be in Ethereum.
00:16:17.502 - 00:16:34.780, Speaker B: I want to be an arbitrum. I want to be in the Ethereum ecosystem. But this is more appealing. The costs here are more appealing to me. My project needs those. So this will, I think, make Ethereum really dominant and more competitive, even on the cost perspective, which is fundamentally important.
00:16:35.240 - 00:17:17.240, Speaker A: Yeah, I'm super excited just for these upgrades to come about and ultimately all the stuff that you and arbitrum and the team there are working on. So maybe jumping into those specific topics, I know arbitrum now has quite a few kind of different flavors of arbitrum live. Could you give a more like just kind of a holistic overview of arbitrum and even kind of maybe for some non technical people, just how kind of arbitrum also goes from like the, what arborim draws at the l two level to ultimately get it down to back to layer one.
00:17:18.060 - 00:18:05.786, Speaker B: Yeah, so I'll go back to that sort of high level description that I gave before, which is what roll ups do, generally, arbitram or any role of design, is they put all that data on Ethereum and they go ahead and they execute things off of Ethereum. And the question is, how do you get Ethereum to vouch for the correctness of what you do off Ethereum because that's the sort of most important thing here, right? We want the security of Ethereum, so we don't want to just do this stuff off Ethereum and then say, okay, trust me when I say no, ethereum is actually proving Ethereum is actually vouching for the correctness of the whole thing. And that's how we scale. Because rather than doing it all on Ethereum, we can now have our cake and eat it too. We take stuff off of Ethereum, but we still get ethereum security, still get a stamp of approval that says, I know that this is correct. So how do you do that? And in one word, it comes down to proving. You say, I know I did this off chain Ethereum, but here's the result.
00:18:05.786 - 00:19:01.626, Speaker B: Here's a digest of the results when it posts on Ethereum, and here's a proof that it's correct. And the way that arbitrum and optimistic roll ups generally work is this proof is actually not a proof at all. The proof is basically just a statement that says, hey, this is correct. And then a window of time opens where someone can challenge that. And then this is where the real proof happens, where you have what's called a fraud proof, where basically you're able to prove the incorrect statement and say, no, this is incorrect, or this challenge is incorrect, and thereby you're able to get to the correct resolution. So the system is optimistic in the sense that if nobody challenges and the incentives are lined up so that people generally won't challenge, you don't need any proofs at all. But if something happens, or if someone challenges or someone tries to put incorrect data on chain, that will be quickly resolved and it will never have any effect in the system.
00:19:01.626 - 00:19:47.460, Speaker B: And that's basically the way that optimistic roll ups are designed. Arbitrum's particular flavor that we innovated in this context of rollups is interactive fraud proofs. And these allowed us to a be quite cheap and also have the most compatible EVM experience. This is like today everyone's doing this, but like a year or two ago, this was more controversial. This idea of having a roll up that can actually run full EVM and exactly compatible with Ethereum today, it's like that's the baseline, everyone's working towards it, and that's like you need that in order to be a roll up. But actually, I think we were the first ones that were actually even aiming for that two, three years ago. And I think that's the reason, which is why we've had a lot of success, not because anything that necessary.
00:19:47.460 - 00:20:02.720, Speaker B: We've done more than we've made, given people the tools to use the knowledge and the tooling and all that they've already known in arbitrum. And the switching cost is quite minimal. So that is how arbitrum works at a very, very high level. But I'm happy to go as deep as we want.
00:20:03.180 - 00:20:18.930, Speaker A: I think that's great. I would love to parse apart some of the individual things that you're working on, maybe even starting with, like, arbitrum classic going into nitro, and then some of the work that you've been doing with Nova and how it's kind of utilizing anytrust as well.
00:20:19.230 - 00:20:59.700, Speaker B: Yeah, absolutely. So our original classic, and this really dates back to our Princeton days. And, like, if you look at our 2018 academic paper and the first several years of the company, but this was the sort of design. So there was something called the AVM or the arbitram virtual machine. And this was a virtual machine that was really, really optimized for these fraud proofs. So, remember, it turns out that, right, the way that it's designed is someone makes a statement on chain someone else, a window of time opens, someone else can challenge it, and then there's just interactive fraud proof that happens. What is an interactive fraud proof? Basically, imagine that we say, remember, we all agree on the inputs.
00:20:59.700 - 00:21:18.326, Speaker B: That's the data availability. It's not ethereum. So we say, well, these are the set of inputs. These are the set of transactions. We all agree that step zero, this is state of the system. What happens when we execute these transactions? So I go ahead and I say, okay, I executed all these transactions, and I could be anyone. I executed all these transactions off chain.
00:21:18.326 - 00:22:03.320, Speaker B: Here is the result, and this is the resulting state of the machine. And these payments, these withdrawals happen, or these messages were sent from l two to l one, etcetera. And then someone else can say, if that's correct and no one wants to challenge it, that's it. We wait a week for full finality and full confirmation of this, and done and is accepted, and ethereum accepts it, and the withdrawals are processed. But if someone goes ahead and challenges, there's interactive fraud proof that happens. Where? To use a computer science term, there's a binary search where you basically start off something large. Let's say we start off with 100 steps, and I say, okay, what about step 50? You agree with me what happened there? And you either say yes or no.
00:22:03.320 - 00:22:35.622, Speaker B: And if you say yes, I agree, then we say, okay, then we're all agreement over here, let's focus on the second half. And if you disagree, you say, okay, so we know that something went wrong in the first half. And by doing this a few times, a logarithmic number of times, which in practice, we optimize it, and it's four or five times. Generally, to do a large amount of computation, you get down to the smallest possible set of dispute. You wanted to get down to the first virtual machine instruction that we disagree. And we said, okay, here's an ad instruction. This is where we disagreed upon.
00:22:35.622 - 00:22:57.296, Speaker B: And now great. Ethereum only has to execute that add instruction. And by doing that, we'll know, is my result correct there, or is your result correct there? So the really nice thing is we take a big dispute. You can have an hour or two or three of computation here, and we very quickly turn it into a very small dispute. And that's how Ethereum can vouch. Cause Ethereum doesn't have to run this. You know, there's hours of computation.
00:22:57.296 - 00:23:08.920, Speaker B: Ethereum doesn't have the capacity to run it, but it doesn't have to. All that it has to do is referee this dispute process and say, what about halfway? What about halfway through? What about halfway? And there will be down to this one step. And all ethereum needs to do is run this one step.
00:23:09.040 - 00:23:22.806, Speaker A: And was this more in the. Or is this kind of specific to the arbitram virtual machine? Or is this kind of more holistic in how you run all of your kind of processes?
00:23:22.878 - 00:23:48.712, Speaker B: A simplistic version. We really, for example, don't bisect. We actually have 100 cuts or so, each much larger number of cuts each time I remember the exact number. But there's a lot that we do to make it more efficient. But this is the basic principle of the protocol and actually is similar in classic and nitrate. Think about classic and the arbitrary virtual machine in particular. The arbitrum virtual machine was optimized to have really efficient and small proofs here.
00:23:48.712 - 00:24:30.720, Speaker B: So these one step proofs were highly optimized for the arbitrage and virtual machine. And then that's what classic was built on. It was built on the arbitrary virtual machine, and it was highly optimized. But the truth is, as you run a system over time, you think maybe whenever you optimize one thing, you choose not to optimize something else. And in arbitrary virtual machine, we optimize the size of these proofs. But but then we thought maybe, like, if the proofs are a little bit larger, that's okay. Maybe if the proofs are slightly less optimized that's okay, maybe we can optimize, because remember, the proving doesn't really happen, right? The incentives are such that fraud, you know, it's one of those weird things where the fraud proof mechanism needs to be there for security, but if the system is operating correctly, it's never going to really be invoked.
00:24:30.720 - 00:25:04.320, Speaker B: Then maybe you say yourself, well, maybe we're over optimizing this fraud proof mechanism. Maybe we can turn some of those optimizations and optimize instead some of the active, active parts of the system that are running every day. And that's really the shift from the arbitrary virtual machine to nitro. So we moved away from the arbitrage virtual machine and instead we used webassembly as the virtual machine wasm or web web assembly. That was the virtual machine that we're using on chain. Let me step back for 1 second. Even when we were using the AVM, we had an EVM compatibility layer built on the AVM.
00:25:04.320 - 00:25:30.014, Speaker B: So the EVM of course is the ethereum virtual machine, and that's what users want to use. They send their transactions to EVM. That's all the tooling is built around the EVM. So you want the EVM compatibility. So we built the AVM to be as close as possible to the EVM. And then we built a translation layer in the EVM. So you can send in the AVM, you can send EVM code to the AVM and it will basically translate it on the fly to AVM.
00:25:30.014 - 00:25:42.036, Speaker B: So basically users were sending their regular ethereum code and I, our system was doing some translation. And if we ever got to a dispute, we're actually not disputing the EVM code, we're disputing the AVM code.
00:25:42.108 - 00:25:42.660, Speaker A: Interesting.
00:25:42.740 - 00:26:30.944, Speaker B: So I know there's a lot of nuance there with the e and the a, so I hope it's not too hard to keep track of. The idea was we're actually, the on chain architecture was AVM. It was quite close to EVM, but we had a compatibility layer and that was called Arbos, this layer, or small operating system that did that translation. But again, in this system, what was really optimized was the fraud proof mechanism. And then we said, well, can we? And we built our node. The arbitrary node was built from the ground up. It was the new software that we built, new database, everything in the arbitrum node, I mean we used off the shelf, we didn't write our own database software, but basically it was, it was a new node that we built from scratch.
00:26:30.944 - 00:26:58.590, Speaker B: Whereas then we said to ourselves. And this was a move to nitro. Can we reuse components of Ethereum? So here's how nitro basically operates. Again, the AVM was retired, and instead we had wasm webassembly virtual machine. So we were able to prove webassembly on chain. But remember, EVM is what we want users. We want this Ethereum virtual machine.
00:26:58.590 - 00:27:31.916, Speaker B: That's why users send transactions to. So the interesting way we did this was geth or go. Ethereum is the most popular ethereum software, Ethereum execution client. And we compiled Geth to webassembly. So we took this Geth codebase, and Geth code base can execute eVm, but has also its particular state. And basically we prove on the output of Geth itself. So basically we use Geth as our execution client.
00:27:31.916 - 00:28:09.576, Speaker B: So rather than now have our own client, we're able to scrap our own client as well and say we're going to use Geth as our returns client. But wait, Geth is layer one software. How do we go ahead and prove disputes of Geth? Maybe, let's say you say the output of Geth is this, and I say, no, it should be something else, right? Because remember, we agree on the transactions, the data we agree on, we run it through Geth. And now, how do we resolve disputes if my geth output looks different than yours? Well, we compile Geth to webassembly, and we're able to dispute, therefore, of the output of geth. Say, hey, we're executing Geth with these inputs. What is the output that it is, and we're on our wasm on chain machine. We're able to execute Geth.
00:28:09.576 - 00:28:51.688, Speaker B: So why is this? And there are also some other benefits that happen in nitro. Like I mentioned before, we introduced much more advanced compression to minimize the data we used on chain, as well as some other nice advantages. But the big one was moving over to Geth, and let us have several advantages as well. First of all, Geth is a much more mature code base than our, than the arbitram node. Arbitrum node was fantastic and really great work of engineering. But when you have an open source project that has five, six more years of engineering behind it, you're gonna have five or six more years of optimizations and hardening, et cetera, built into it. So that was really exciting to be able to build a build with Geth.
00:28:51.688 - 00:29:28.960, Speaker B: But also, there's another benefit, which is arbitram was always fully EVM compatible. And I said before, I think that's one of our strengths. But under the hood, remember, it wasn't really EVM was a compatibility layer under the hood. It was quite different than, say, what Geth was doing. It wasn't geth, it was running some of those compatible with the API that Geth exposes with the EVM and its public APIs. But things that were like internal APIs or things that relied on guests internals were different. You imagine, for example, the analogy I like to use.
00:29:28.960 - 00:30:09.810, Speaker B: You're building a car, you have a car, and you say, I want to build an electric car. You can build a car that looks like a car and drives like a car, but if you open the hood, it looks absolutely nothing like your other car, as opposed to building an electric car, that you open the hood and looks very, very similar. So that's basically the shift between arbitrum classic and arbitrum nitro. It always had, the car always looked like a car, the car always drove. And the nice thing about us, it always drove exactly the same, right? Users had the gas pedal, had a steering wheel, had everything you'd expect, whereas that's not a given. Some other projects, they made slight changes, so maybe the steering wheel became a square or whatever. I'm taking this analogy too far.
00:30:09.810 - 00:30:56.860, Speaker B: You get my point where we never did any of that, but there was always some tooling, some tooling. Majority of the tooling you just worked out of the box of arbitrary. There was some tooling that required the internals of Geth, the way it was implemented, like tracing software, that really required knowing things about the internals of geth more deep than these surface level APIs that we use. Imagine your steering wheel worked, but your jumper cables for your old car weren't going to work anymore. But now that we moved over to actually running Geth software in L2 as well, all that tooling falls out as well. So today arbitram is always fully EVM compatible. Today it is as compatible or equivalent even as you can expect, or you can hope to be for a L2 platform.
00:30:56.860 - 00:31:22.572, Speaker B: And also, we gain all the benefits of this mature code base with heavily optimized code base, plus more advanced compression that we introduced. All in all, users saw a great benefit here. So transaction costs. When we upgraded to nitro August 31, transaction costs in arbitrum were reduced drastically. And also our node provider friends told us that the cost to run infrastructure in arbitram reduced by 90% as well.
00:31:22.676 - 00:31:41.160, Speaker A: Interesting, lots of benefits. And was the kind of increase in transaction throughput because of the change in the virtual machine, or was that because you were able to do more of the lossless data compression by switching to virtual machine.
00:31:41.700 - 00:32:19.652, Speaker B: So there's actually two things that happened in parallel. So the decrease in cost and layer one usage was because we introduced advanced compression. We introduced it in the context of the nitro stack. But had we taken those engineering resources and put it to the classic stack, we probably could have introduced more advanced compression there too. So that was like something that was part of what we packaged up in nitro. But again, the specifics of what we did were tailored to the nitro stack, but we definitely could have done better compression in the classic stack as well. But the other thing is the L2 usage.
00:32:19.652 - 00:33:01.756, Speaker B: The arbitrum node before, like I mentioned, Geth is a more optimized node. So just executing via Geth versus executing via the EVM to AVM was a much cheaper operation. So for running a node or participating in the network, the cost to run that went down. And your average user, it was always relatively easy to run a node. But to those users that are running a lot of node or want to sync their node quickly, these things matter a lot, because when you sync a node, you have to run through all that historical data. And if you are 90% faster now, that means instead of taking 10 hours, it's going to take 1 hour. And that's a lot better.
00:33:01.908 - 00:33:21.810, Speaker A: Definitely. Perfect. Maybe to stay on some of the hardware standpoint for a little bit, could you talk about the sequencers and this new update, how many sequencers existed, and even maybe some of the hardware requirements as well?
00:33:22.230 - 00:34:03.400, Speaker B: Sure. So, sequencers in arbitrary, there are two worlds, and people often confuse these, I think because other ecosystems use the terminologies a bit differently. In arbitrary, there are two roles. There are various roles, but the two roles I'll talk about are the sequencers and the validators. The validators are the ones that check the protocol, engage in those fraud proofs, will challenge and really make sure the protocol is correct and that the output is correct. And if they see a different validator posting something that's incorrect, they'll go ahead and challenge that and make sure that we use this fraud proof windows to make sure everything happens correctly. That's the validator.
00:34:03.400 - 00:35:08.448, Speaker B: So what is the sequencer? Well, Ethereum transactions take around 15 seconds on average to be processed, right? So Ethereum blocks the Ethereum block time. That means is if you're a user, you'll submit a transaction and you'll see a spinner or whatever, wait, and then you'll see your transaction get confirmed in one block and one thing that we wanted to, and this is something which many roll ups do. The question is, can we do better in a roll up context? The answer is, in some sense, not really, not with full security, because if your transaction is not included in Ethereum, then you're not going to get that full confirmation. And that's true. But can we do like, can we offer some affordance to users that gives them a software confirmation, a confirmation that at least is more instant? Because especially as we move, by the way, to more mainstream users, this becomes very, very important. Gaming and the like. I don't know about you, but if I press search on Google and 200 milliseconds, I don't get a response, I get frustrated and I press refresh.
00:35:08.584 - 00:35:09.448, Speaker A: Exactly.
00:35:09.624 - 00:35:50.000, Speaker B: Your average crypto user, they get this right, they understand this, they're fine, they'll wait that time. But as you move to more mainstream, this is a critical part of the usability, these faster confirmation. So can we do something here? So this is where the notion, and this is not our innovation. Other projects, we're using sequencers, where sequencers come in. And what the sequencer does is basically it batches up transactions and it says it gives you a fast confirmation. So rather than, you know, you sending your transaction in arbitrary today, actually, I should say you have a choice. You can send your transaction to the sequencer or you can actually send it to a specific inbox, contracts, you call it on Ethereum.
00:35:50.000 - 00:36:02.912, Speaker B: So you can send it to the inbox, or you can send it to the sequencer. If you send it to the inbox, it will, it will be included eventually. And that's, I can talk about in a minute, you know, when it will be included. But that's the sort of censorship resistance path.
00:36:03.016 - 00:36:03.440, Speaker A: Okay?
00:36:03.480 - 00:36:31.002, Speaker B: All you need to do is for getting it onto Ethereum, your transaction will be included, but you won't get this fast confirmation. The sequencer is going to give you a fast confirmation. So you're going to send it to the sequencer and boom, you're going to get a response that says, here is your transaction receipt, and we execute your transaction. Here's the result. Now, how does a sequencer do this? Well, basically the sequencer is just giving you a promise and saying, I will post your transaction on chain. I haven't yet, because I couldn't, right? Because there was no block yet. Remember, blocks take 15 seconds.
00:36:31.002 - 00:37:12.696, Speaker B: Sequencer is not creating Ethereum blocks more quickly. It's just saying, I'm giving you a promise, a guarantee that I will post your transaction and a receipt that I will post your transaction in the next block and therefore I'm going to pre tell you what the execution result is because I see these transactions. Now the thing you have to realize then is there can't be two sequencers logically, because remember, these sequencers are giving you the result of your transaction. And if sequencer A is telling me I'm going to result, you're going to introduce your transaction in this order, this is the result. And sequencer B is telling you the same. Well, it's just going to be a race because the ordering of transaction matters, what the result is. So you can't have, so logically, how is Mev? Yeah, exactly.
00:37:12.696 - 00:37:50.284, Speaker B: Mev tells you that these results will be very, very different. So certainly there has to be one sequencer. The sequencer is basically committing to the order that your transaction will be included. And the sequencer can be a very nice person and not lie. But if there's someone else that can front run it and get a transaction first, its promise is going to be broken because it's going to say, I will include your transaction in this order. This will be the result as like, oh no, not true, because I was faster and sorry, what I told you is false. So what that means is, in order to have a sequencer, which is really nice, use Ux affordance to get these fast confirmations, you basically have to give the sequencer the ability to make sure that its transactions get in first.
00:37:50.284 - 00:38:22.232, Speaker B: The sequencer needs to know that it's going to be able to keep its word, right? If it's not malicious and it really wants to keep its word, the sequencer is going to have to know that it will be able to get its transaction. If anyone could just send a transaction before the sequencer. Sequencer can't give you any sort of guarantees. And therefore, and there is a model like this, in fact, you can have a roll up without a sequencer at all. It's just you don't get these fast confirmations. But if you want the fast confirmations, you have to give the sequencer this ability to control the ordering for limited periods of time. You can say the sequencer after some period of time.
00:38:22.232 - 00:38:41.440, Speaker B: If you don't include this transaction, sorry, that's where the slow path comes in. We're going to include these ones in Ethereum. If you want to keep your word sequencer, you'll have enough time to make sure that you can. The sequencer can't just not submit his transactions, but the sequencer needs to be given enough time that it can get his transactions in.
00:38:42.100 - 00:39:05.750, Speaker A: That totally makes sense. Yeah. The L2 space is fascinating and all of the different kind of nuances that make it work. So today with arbitrum, there is a single sequencer that is kind of giving soft confirmations to people using that layer specifically or two.
00:39:06.050 - 00:39:42.500, Speaker B: Yes. And so let me tell you a few things. There is a sequencer giving those confirmations. If you don't want to use a sequencer, you can go on the layer one inbox directly, you can go to Ethereum directly and get your transactions in that way. That's another thing which people should know. Also, there is a call you can make, an API that we surface that tells you, is my transaction committed to layer one already? If you don't want it, you want to send your transaction to the sequencer rather than posting ethereum, but you don't want to rely on the sequencer's confirmation until it's actually committed. Usually, by the way, our sequencer commits quickly, it's every two or three minutes it will happen.
00:39:42.500 - 00:40:23.702, Speaker B: You can say, ok, I'm going to send to the sequencer, but I'm going to wait until it's actually an ethereum, because at that point I know the sequencer can't undo it and the order is really, really, really committed on ethereum. Now the question though is, so everything I said before is you say, well, is there only ever one sequencer? You just said there could only be one sequencer. So logically there can only be one sequencer, but you can distribute the sequencer. You can have many nodes taking part of a consensus protocol that are logically the sequencer. The sequencer needs to have an ordering. So you can imagine this is basically a consensus problem. Now, one thing I want to mention which people I think don't appreciate enough, is the sequencer is extremely minimally trusted today, so the sequencer can't push in a bad transaction.
00:40:23.702 - 00:40:57.962, Speaker B: Why? Remember, we have the validators. If the sequencer tries to commit a bad transaction, the validators are going to swoop right in there and say, no, this result, the sequencer isn't even responsible for posting the result. The validators are. So the sequencer can't do anything malicious there. It can't steal money, it can't change the state of a smart contract. So what can the sequencer do? Is a question, and what are the benefits of decentralizing it? So I'll tell you what the sequencer can do is it can reorder transactions, right? And that is the MEV extracting MEV is what the sequencer can do. So, today, we don't extract MeV.
00:40:57.962 - 00:41:39.802, Speaker B: We serve transactions on a first come, first serve basis. But you can imagine that a malicious sequencer could extract meV, and a user might want a better guarantee, an in protocol guarantee that says, we're not extracting me, rather than Steven just sitting here and saying, don't worry, we're not extracting MeV. So the question is, can you build a consensus algorithm that has some notion of fairness in it and some work that I did at my time at Cornell? So, actually, when I was postdoc at Cornell, the paper that introduced MeV was called flashbots. Sorry, it was called Flash Boys. Flashbots is Phil's company. So Phil was the primary offer on that paper. But I was co author of Phil on that paper where we first studied MeV.
00:41:39.802 - 00:42:15.958, Speaker B: And then a follow up paper that some of us did was on the notion of fair ordering. Like, can you actually build a consensus protocol that orders fairly? Because your typical consensus protocol, your BFT, or even your proof of work protocol, they don't care about ordering at all. They care about agreement. So transactions come in ABCDE, and the block that we all agree on just has transaction DNA. We're fine, right? Everyone's fine. We all agree. But, like, there's question of, can you build a consensus protocol that has fairness built into it? And that's basically what we plan on using for decentralized sequencer.
00:42:15.958 - 00:42:53.286, Speaker B: We're considering, actually some mixed models in which we have fairness, you know, time bounded. So the idea is that it's a fair, you know, we will do first come, first serve. But within small windows, there will be some auction mechanisms, so users can, within very short windows of time. So we're still deciding on exactly how we want to deploy this protocol and what the parameters and the fine tunings of it will be. The idea is to distribute the sequencer, not by having multiple sequencers. There are others, by the way, that have this approach, that saying, here's how you distribute the sequencer. You auction off the right to be the sequencer, and therefore, for this period of time, I'm the sequencer.
00:42:53.286 - 00:43:09.810, Speaker B: This period of time, you're the sequencer, and we auctioned off this. Right. The reason we don't like that is because it encourages maximal extraction of mev. Because you imagine if I'm a person that, and I know by, hey, I can buy the right to be the sequencer. For the next hour, I'll be able to extract a million dollars for it. So what am I going to do. I'm going to bid $999,000.
00:43:09.810 - 00:43:29.842, Speaker B: $999. And then I'm going to extract maximum. And that's what will always happen. Right. The highest bidder will be the one that will extract the most value. We believe in minimizing extraction of mev to the extent possible. So our goal is not to sell the right to extract mev and therefore bring in the most brutal person that will extract all the.
00:43:29.842 - 00:43:51.394, Speaker B: I shouldn't judge you. Bring in the most mev extracting person that will extract the most mev. We say, no, there will be mev. And we should reduce that, but we should democratize it when it's available. But to the extent possible, we should reduce it. So to the extent that we can build a fairness into the protocol, that's what we're interested in doing. But it's important to stress that this is only around mev and reordering.
00:43:51.394 - 00:44:13.686, Speaker B: Again, the sequencer is not trusted to push back transaction. In fact, our sequencer is so minimally trusted that we don't even allow the sequencer to tell us what the layer one gas price is. Remember, the sequencer is charging. Yeah. And people don't. Actually, one of the other projects on Twitter was asking us about this because, like, it wasn't even our threat model. Our threat model is, remember, the sequencer is choosing what to what the sequencer charges, charges users, and it's supposed to charge them enough for their layer among gas.
00:44:13.686 - 00:44:47.616, Speaker B: You can imagine a sequencer that goes rogue and just pretends that the price is really high and start and raise the price. So actually, we have an on chain source we trust basically only on chain data. The sequencer has to rely on the on chain data to determine the price of a. We don't even trust the sequencer to tell us what the price is. So it's a bit of like, it's funny, I think people don't appreciate this, how minimally, I'm not trying to minimize the problem mev, but that is the limited problem of what the sequencer can do. It can't really do any other things or wreak any other havoc on users other than reorder transactions, which is not great. But it's.
00:44:47.616 - 00:44:52.328, Speaker B: I think it's different than what people think, for a large part.
00:44:52.464 - 00:45:25.050, Speaker A: Definitely. And I truly appreciate the depth and kind of the clarity. I think this is kind of ultimately why I wanted to do the podcast is because a lot of this is hard to communicate on Twitter or even in written. Sometimes it's a little bit more of a challenge so I definitely appreciate the depth. Ultimately, if you do end up kind of continuing to pursue the decentralized sequencer model, is there a number that you ultimately would like to get to in the sequencer standpoint?
00:45:25.550 - 00:46:28.342, Speaker B: So the constraint here, the interesting constraint, I don't have a concrete number in mind, but I'll tell you what the thought process and how you get there is. Remember, right now, people like the point of the sequencer are these instant confirmations or variants and confirmations. So you want to hit the sweet spot where it's significantly decentralized and distributed, and you have these guarantees, or these in protocol guarantees that the sequencer is now reordering, but you don't want too many parties because then the latency, if you had a send your transaction and this consensus protocol has to happen with 20,000 parties as an extreme example, that's a big problem because you might as well just put an ethereum because the latency of this protocol now is going to be very, very high. Probably a nice distribution, but maybe tens of people are in that order where it's much more limited than have. So it's feasible to have a fast protocol that has low latency as opposed to adding latency by just having so many participants.
00:46:28.526 - 00:46:54.900, Speaker A: Yeah, that definitely makes sense. Very cool. I'm personally just curious, what are like the requirements to run a sequencer? I guess it doesn't matter too much if there's just one sequencer. But you also mentioned the full nodes that are kind of watching the sequencers. Could you talk about go a little bit more into depth on those and then also just kind of curious on the hardware requirements of those as well?
00:46:55.400 - 00:47:45.860, Speaker B: Yeah, so the hardware requirements are pretty low. They're basically just running a node, and our nodes run at about 7 million gas per second. So it's a little like on the order of seven times more at max resource intensive than running an ethereum node, but pretty in scope for a nice size cloud instance, for example, your average user, it's pretty accessible. If they want to go ahead and do this, that they'll be able to do that, to your point. Yeah. So validators or full nodes, what you probably want to run the sequencer is really this, again, it's running very similar software. It's processing your transaction, giving results, but the validators that are keeping track of the network.
00:47:45.860 - 00:48:32.380, Speaker B: Today we're in what we call Mainnet beta. The protocol is fully built out. You know, the code is code complete. You can go on our GitHub, you can look at it, but we are whitelisting who can actually submit these fraud proofs. We've actually expanded it, you know, a decent amount past the company. We're going to put out some nice announcements on that soon. Nice, but we're not too far away, but still not yet at the point where we're in fully open that anybody can run fraud proofs while we just continue to test and do some upgrades of the protocol and hardened around that.
00:48:32.380 - 00:49:38.726, Speaker B: But today we have again today in terms of what you can still run the software. There is that administrative whitelist, but the software requirements are actually, like I said, just think about running on Ethereum node, which is like roughly 7 million gas per second running a geth node and that are the requirements. And by the way, just in case you didn't ask, but I'll say anyway, in case you're wondering, hey, why are these nodes like running at a higher capacity than Ethereum nodes? Or why is it okay for, one of the reasons why Ethereum keeps its gas low is not because it can't turn it up, it's because it wants a wide set of decentralization and a wide node. Why is it okay for us to have a little more? And the answer is it's a slightly different security property. Remember, ethereum uses, well, now uses proof of stake and it has sort of, it needs a majority, or even like a super majority of nodes to do the right thing. So you really want this really large wide distribution in arbitrary and rollup. All you need is one validator to do the right thing.
00:49:38.726 - 00:50:24.024, Speaker B: So you can imagine you have like a million validators and they're all saying like a state a is the right way one, this one validator, one row validator is like, I don't know what you're talking about, it's actually b. All that validator, if that validator is correct, it's not a voting process. They go to Ethereum and they say, they challenge the ones who say a, they say b, and they will win if they are correct. So the idea is, since you go from requiring this like super majority of nodes to do the right thing to requiring only one, you know, it makes sense to require some less, you know, have a little bit beefier requirements, even if it means there'll be some less participation, because you need so much less out of those that are participating. You don't need a majority or a supermajority to all get it. You need one of them. So that's basically the rationale.
00:50:24.024 - 00:50:40.552, Speaker B: We still want to keep it relatively low. We don't want to jump to 25 million gas per second because we're nervous about. There are more problems that need be solved to allow others to participate before we get there. But it's pretty safe because of this model to expand a bit past Ethereum.
00:50:40.736 - 00:50:50.080, Speaker A: I'm personally of the fan of just cranking up the hardware requirements, especially when you only need one event. Just start cranking that throughput. I think it would be super interesting.
00:50:50.660 - 00:51:08.732, Speaker B: In some sense, that's what we did. So we are running at 7 million gas compared to 1 million compared to 1 million gas right here. So that's a seven x increase in what we're doing. And I think we'll get further. But one of the problems. So here's one thing that doesn't change. So you see there are some chains that crank it up a lot further than that.
00:51:08.732 - 00:51:41.484, Speaker B: And a lot of these chains aren't roll ups. They actually don't have this one requirement. They have this two thirds requirement also, and they just crank it up more. And what you see is de facto centralization where now there are basically either an explicit limited set of validators or an implicit. So maybe anyone can participate, but hey, requires a data center to participate. So it's like, okay, in practice it's pretty centralized and limited. But one of the problems that persists is the data growth, the state growth, or it's called state load.
00:51:41.484 - 00:52:14.706, Speaker B: And we have some really good. We as a community, we as a company have some really good initiatives and thoughts of how do we resolve that. You know, things like, like clients that allow people to participate and validate and get, and get verification and get validation even with more limited resources. And there are lots of other interesting proposals, some which may happen, some may not happen. You may have heard of things like state rent or, you know, these ideas of. Anyway, there's a lot of interesting research out there, but we think that we need some more answers there. And we're working on these as well.
00:52:14.706 - 00:52:21.138, Speaker B: Before we crank up the gas too much more than we are today.
00:52:21.234 - 00:52:53.192, Speaker A: It definitely gets super nuanced. I try to lean in the decentralization camp on the Nakamoto coefficient and the number of whole nodes just to actually quantify some things. But yeah, it's definitely interesting. I'm excited just to watch all the different optimizations and the different chains and see how they kind of progress. And as you mentioned, the state float does get very large ultimately. And so you have to handle a lot of that data throughput to store that in an effective way.
00:52:53.336 - 00:53:19.706, Speaker B: Yeah, it's the type of thing by the way, that it might feel really good today. We're doing a lot, but we put a lot of our thought into sustainability, and these problems get worse and worse and worse over time. Yeah. If you start a new chain and you crank up the requirements, the state will grow fast, but it's still going to be relatively limited in its first year or two. Then it's going to grow and it's going to grow and grow and they're going to have to carry this weight with you. And that's going to be a lot of data. Yeah, we think about, not today.
00:53:19.706 - 00:53:59.372, Speaker B: It might feel really good today if we, if you crank it all the way up, but then you think about, is this sustainable? And that's really what's our guiding factor, our guiding factor in these decisions, is it sustainable in five years? Will this still be in five years, even if we don't make any fundamental changes in the data model, will this still work? That's what we think about. And hopefully we'll make changes, we'll make improvements and we'll be able to increase more than that. But at every point we want to make sure that we're sustainable. And the same thing is with fees. I always say if the company goes away, you want to make sure that the system is sustainable. And that's why we believe in transparency and fees. Transparency, proving costs.
00:53:59.372 - 00:54:14.292, Speaker B: Because, you know, you really want to make, we're selling scalability here, so you want to make sure that it's sustainably scalable. And if scalability has like some, you know, VC funding in the background, that's not a very good. It might work.
00:54:14.396 - 00:54:15.720, Speaker A: Money always runs out.
00:54:16.060 - 00:54:18.240, Speaker B: Exactly. You want to be, you know, sustainable?
00:54:18.700 - 00:54:35.628, Speaker A: Definitely. I fully agree. No, it's super interesting. Do you have any like metrics or kind of on how many full validators there are today? On running the arbitrum full nodes, the.
00:54:35.644 - 00:55:21.740, Speaker B: Answer is not really, because here's another interesting thing. Today we've only whitelisted a relatively small list of others to run validators that can't submit fraud proof. But we know there are lots of others that are running nodes. Even validators like, unlike proof of stake systems where validators are constantly signing things and constantly doing things and participating and need to participate, if they don't participate, they'll get slashed in a roll up. It's actually different where, remember, the only thing a validator needs to do is, well, one validator needs to go and actively say, hey, here's the claim, here's the state, but everyone else, they need to do nothing. Unless something goes wrong. If they agree with the other validator saying they literally never need to do anything.
00:55:21.740 - 00:56:19.396, Speaker B: You could be running a validator, and if I do something wrong, we'll test right away and you'll pipe up right away. But if I never do anything wrong and the incentives are such that I don't, the reason are, is because whenever you say something as a validator, you put down a deposit and you will lose your deposit if you do the wrong thing. So I'm not going to want to go ahead and make a false claim and therefore imagine I'm that active algorithm that just constantly post these updates and I always tell the truth. There could be a thousand other others on the network and you'll never hear from them, which is interesting because what it means is you really don't know who's running nodes. And even if the system, even in a post whitelist phase where validation is completely open, you often won't know. And it's actually a good thing, because if you imagine someone is trying to something in the network, they don't even know where the validators are. They don't even know how many validators there are.
00:56:19.396 - 00:56:28.332, Speaker B: They just know that there are people that will pipe up for the first time ever that will ever hear these people exist. If I do something wrong, which is a pretty nice security property.
00:56:28.436 - 00:56:42.320, Speaker A: Definitely. Yeah, it totally makes sense. Any kind of analytics on how many fraud proofs have been challenged? I'm curious if that happens fairly often or rarely, if ever.
00:56:42.800 - 00:57:10.570, Speaker B: So today, zero in production. That's expected, particularly with. It's expected always because the incentives, particularly with the parties running farproof today, we wouldn't expect that. It's like proof of stake systems where when someone gets slashed, probably 99% of the time, it's because their software is misconfigured. That's really the time that you expect this to go wrong. No one's going ahead and you're going to lose money if you do this. You're going to get slashed.
00:57:10.570 - 00:57:24.802, Speaker B: The game is such that you don't really expect this to ever happen. So I would expect we do it in testing all the time, but in our actual production deployments we have not seen it.
00:57:24.906 - 00:58:04.160, Speaker A: Awesome. Very cool to hear. Well, maybe shifting the conversation a little bit to Nova and Etrus. Actually, let me back up a little bit. We went into a little bit of the technical weeds, but ultimately really the end goal here is kind of scaling ethereum. And I think ultimately, you said seven x. The capacity that ethereum has today ultimately allows engineers and developers in kind of that native EVM format that you guys have built to ultimately kind of drag and drop their applications into kind of a much more highly scalable throughput system where they can build more interesting applications.
00:58:04.160 - 00:58:05.588, Speaker A: Cool.
00:58:05.724 - 00:58:24.396, Speaker B: Absolutely. And by the way, that's seven x, you know, because you mentioned Nova, it's seven x, like per arbitrage one. And seven x for Nova also is running at the same capacity. So together they're doing, like, together their capacity, you know, at capacity, they can do 14 x Ethereum before fees, you know, before they get into congestion.
00:58:24.508 - 00:58:54.210, Speaker A: Very cool. And then Nova, I did want to get into as well with, like, any trust and kind of the external data availability committee. I thought this was kind of very interesting, what you guys are doing here, and even kind of watching one of your previous podcasts, the committee members, many notable names on that committee. Could you kind of just talk about what you're doing there and go into a little bit more depth in how you kind of see that playing out going forward?
00:58:54.790 - 00:59:09.810, Speaker B: Absolutely. So, Nova. So what is Nova? What is any trust? So, first, let me get the terminology clear. So there's the matrix. We have technologies and we have chains. So we have arbitram roll up, which is our optimistic roll up technology. And then we have Arbitrum, any trust, which is a slightly different technology.
00:59:09.810 - 00:59:45.556, Speaker B: Those are just technologies, and then we have chains. Arbitrum one is the blockchain. Many people just call that arbitrum, but arbitram one is the blockchain that runs the arbitrum roll up technology. And Arbitrum, Nova is the blockchain that runs the arbitrum NHS technology. You can imagine that we have. There could be any number of blockchains that do both of these, but we have one public chain for one public roll up, and one public, any trust chain. So what is Nova and this committee, and how does it differentiate itself? So, the way to think about it is it's very much like a roll up, except that whole thing we talked about before, that data, putting that data on Ethereum, it doesn't do that.
00:59:45.556 - 01:00:24.310, Speaker B: Instead, it sends the data to a committee. And where the word any trust comes in, it means you just have to trust any of them. In practice, one or two, depending on the configuration, you have to trust one or two of these committee members to return your data. So, whereas in the roll up world, remember, you post that data on chain, here, in the any trust world, you don't post data on chain. The sequencer sends the data to this committee, and the committee signs a data certificate. And you post those certificates on chain where the committee says, hey, I know the data corresponding to this hash and I will provide it upon request. So rather than posting the data on chain, you post these certificates on chain that say they'll post the data.
01:00:24.310 - 01:00:59.908, Speaker B: Now remember, back to the earlier part of our conversation, the vast majority of the costs in arbitram one or the roll up is the data. Putting the data on chain. And we've done a lot to reduce that by compressing and EIP 4844 and other proposals will make the data cost even lower. But that still remains today the vast majority of our costs. So if instead we send it to a committee, we are therefore reducing the vast majority of our cost. And you mentioned the committee. So that includes, since it's any trust, again, you can add people to it, it's just fine.
01:00:59.908 - 01:01:31.236, Speaker B: But basically you want to have a committee such that most people will feel comfortable said, I don't trust those, I trust them. I trust that person. So we have a nice mix of web two and web3 companies in the committee. So hopefully the idea is that just about any community member can find someone in the community they trust. And so it's us. It's us at off chain labs, it's FTX, it's consensys, it's Reddit, Google Cloud and Lido in the data availability committee. And a quick note as well.
01:01:31.236 - 01:02:10.164, Speaker B: So a nice group of people where the idea is that most people in the ecosystem, or even not in the ecosystem, in the web tool world, can say, I trust that Google cloud will keep my data, or I trust that consensus or Reddit, you name it, will keep my data. Now, the interesting thing is, from there, the protocols are exactly the same. So it has the fraud proofs and the challenges. All that happens on Ethereum. The execution and the challenging happens on Ethereum. Really the only difference is that the data is sent to this committee. One interesting thing though, is you might say what happens if the committee stops doing its job and stops signing these certificates built into the protocol, is that it falls back to roll up.
01:02:10.164 - 01:02:42.938, Speaker B: So if the committee stops doing its job, it falls back to being a roll up and putting data on Ethereum. And actually, I always say, as this is really made for the set of facts today, where there were certain applications that needed a lower cost solution, they wanted one that's higher security than a sidechain or alternative layer one. And there's arbitrage. Many trust came in. It's not quite a roll up, but it's higher security than alternative solutions. And that's why we wanted to provide it for them. And the reason it's lower cost is because it doesn't put the data on Ethereum.
01:02:42.938 - 01:03:13.770, Speaker B: But if EIP 4844 and further data sharding proposed are successful, there's definitely a possible future where this will just become a roll up if we reduce the cost by really 100 x. So maybe it just makes sense to say, all right, let's just put this data on Ethereum again. So we'll see what happens there. But it'll be very interesting to see what happens with the data cost in Ethereum and in the new world, if it still makes sense to have a committee doing the data availability.
01:03:14.150 - 01:03:42.288, Speaker A: It's a super interesting solution. I really do like it. I think with EIP 4844, the target megabyte per block was 1 limit of two megabytes, and then with full sharding it was 60 megabytes. But the data throughput was 1.3 megabytes per second. Do you know with the data committee how much throughput or megabytes per second that you're going to be offering there?
01:03:42.464 - 01:04:33.420, Speaker B: I don't actually know the exact numbers offhand, to be honest. What I do know is I don't know the exact numbers offhand when presented in megabytes per second or those numbers, but I do know is the way we think about it is in terms of gas per second, and, you know, whatever. So I'll tell you what it is. I don't know the number. I know. Here's the calculations we support, like I said, 7 billion gas per second. So if you imagine that you were using all that gas towards just storing, towards just storing, or all the call data associated with the maximum amount of call data that you can associate with 7 million gas is of execution, is that what it's going to be? I don't actually know, though, in terms of translation data size, but that's interesting.
01:04:33.760 - 01:05:14.120, Speaker A: Very cool. Awesome. No, I think that was overarching. A lot of my thoughts or questions on your core products. Some of the things that I also just wanted to. And we touched upon sequencing and I think MeV as well. Any thoughts more on validity proofs? I think that's the different approach to scaling Ethereum more holistically with like the zero knowledge tech is off chain's lab and arbitrum primarily going to stay focused on the optimistic side and the fraud proofs.
01:05:15.220 - 01:05:55.022, Speaker B: Yeah. So here's where our commitment is, and we are fully committed to providing the best technology available to solve the problem of scaling Ethereum without compromising security and doing that in a way that's as compatible as possible with Ethereum and also with the best user experience. And part of that, of course, is the lowest cost validity roll ups, whereas they use $0 proofs. The big problem today for many of them is on those two fronts. One is cost and one is compatibility. The cost part comes from the cost approving. And remember, we're running Geth today, and that's just extremely efficient.
01:05:55.022 - 01:06:22.118, Speaker B: We're basically able to run like almost on the hardware of our machines, and that's how fast we can go, at least from a cost perspective, we're limiting it to 7 million gas per second. But from a cost perspective, it's not very resource intensive. Zero knowledge proofs. They run through circuits, which is this alternative computational model. And basically any hope to be EVM compatible is using a translation layer. And there are various different approaches. Each project has its own approach.
01:06:22.118 - 01:06:45.636, Speaker B: They go through some intermediate language, etcetera. They go through some intermediate like LLVM, or, you know, how they do it, or use a transpiler. Lots of different approaches out there. The facts are that that adds more bloat. And also, you know, it's fundamentally trying to fit something that wasn't made for this computational model. Right. You know, into we want EVM directly via Gethse.
01:06:45.636 - 01:07:11.168, Speaker B: They're trying to run it via circuits. And so the problem is, one, is the cost of proving is typically very high. Yes. You'd be hard pressed for most of these projects to find benchmarks on proving today. Some of them you won't even find a prover code. You know, it's all very opaque. And so I can only comment on what I've seen, and I haven't seen anything that suggests that the costs of proving are really feasible today for many applications.
01:07:11.168 - 01:07:37.012, Speaker B: And also it's compatibility. Right. Models weren't really meant for each other, and therefore certain opcodes are just not possible to do, or certain opcodes are very, very not possible to do efficiently, I should say certain opcodes are very, very limited in that they're just very expensive to do. The things that you can do very fast in EVM, going through circuits have a much higher cost. It's not one to one cost. For example, hashing. Hashing is very fast, one of the fastest operations on a typical hardware.
01:07:37.012 - 01:08:04.222, Speaker B: But these bit wise transformations are actually very expensive in most of these zero knowledge proof systems. And therefore, what you have is a system that's often more costly and less compatible. But there are some teams are making great progress. And there are some interesting directions there. And here's what I can tell you. If we fast forward five years from now, everything I'm telling you today in terms of our technology will be primitive. I'm sure I'll move on to nitro.
01:08:04.222 - 01:08:28.420, Speaker B: Nitro two, nitro three, who knows? And that's true for every project, right? Just because there's so much innovation happening, so many great minds enter into space. So the pace is so fast. And you know, we are. And it's true. If you look back, right, if you look back at the arbitrum of 2018, it looks a lot different than today. We just launched nitro, right? Remember that avm we spent, spent years working on that? Well, that's retired. And you know what? We're happy to retire it because we're happy to have something better.
01:08:28.420 - 01:08:59.161, Speaker B: So we're very, very focused on the solution of providing the best possible scaling solution for ethereum. We're less focused on the tool. Optimistic roles, particularly the flavor that arbitrage developed, in my view, and I think this is non controversial, is the best scaling solution available today. There are many out there that will tell you that we're very, very, very close to something better, but they've been very, very, very close for a long time. And by the way, we've also been very close to slam better. Right. Our, which today is running nitro three months ago or even two months ago, it wasn't running nitro.
01:08:59.161 - 01:09:08.665, Speaker B: And so we're also getting much better over time. That's another fallacy, you'll see, where people will say, let's compare arbitrum today to the theorized validity roll up in three or four or five years.
01:09:08.697 - 01:09:09.049, Speaker A: Exactly.
01:09:09.089 - 01:09:57.920, Speaker B: It's like, no, no, let's compare arbitram today to these projects today, or arbitrum in five years. To these projects in five years. Because the commitment that I'll make to you is we're going to stay, we're going to develop and utilize the best scaling technologies, and in five years from now, we'll look very different. Will it include zero knowledge proofs? Maybe, maybe not. If it does, that's great, and we'll use them. If it doesn't, then we'll continue to optimize in our current architecture or whatever we're doing then. But one thing you'll find interesting is without naming anyone specifically, if you look at us and basically all of our competitors names, just about every single one of them, take a look at the name of their company and the name of their product, they hard code their technology in there, either they hard code the word optimistic, or the hard code the word zk, or the hard code, a type of word, a type of ZK proof in some cases.
01:09:57.920 - 01:10:16.644, Speaker B: We don't do that because, again, we're about the solution. We're not about the tool. And I'm not going to. If ZK technology is ready in a year or two or three, I'm not going to eat my words. I'll be happy to use it. We're not saying that. We're not saying forever, forever, forever.
01:10:16.644 - 01:10:27.140, Speaker B: Nothing will be better. I'm saying I'm sure things will be better. I don't know what they are, but I know what's best today, and we're continuing on that research path, and that's basically our commitment. We will continue to do that for the long term.
01:10:27.920 - 01:11:06.034, Speaker A: I personally love that approach. I mean, ultimately, I just did a podcast with the founder of DYDX, and he had a very similar kind of product focus, like, what can I build today that actually derives value to users? Not something that's hypothetical, that may or may not exist in the future. And I think that is something that's very much needed kind of in the crypto world, because sometimes we get lost in these next thing. But it's very much so. It's being able to actually build things that people want and use and can use today, I think, is highly important. So applaud you and your team for taking that approach.
01:11:06.162 - 01:11:11.362, Speaker B: Thank you. Yes. And, yeah, that is, we will continue to take that approach for the long term.
01:11:11.546 - 01:11:43.810, Speaker A: Perfect. Just a couple more topics and then kind of wrapping it up. I think one thing that we've seen today more, hopefully, holistically in the industry, is a little bit of nerves around bridging and going from, say, like an l one to an l two or a different ethereum ecosystem to another ecosystem. Could you touch upon how arbitrum is kind of trying to secure the l one to l two bridge and if people should have concerns?
01:11:44.990 - 01:12:29.832, Speaker B: Yeah. So one of the nice things about rollups in general on arbitrum, chief among them is that the bridge is a part of the protocol. Right? So the entire security assumptions or security guarantees that the protocol provides, like we say, the security of ethereum and all these words that we say that applies to the bridge itself. And that's actually, you know, there's a view out there. Patrick McCrory of consensus, a friend of mine, he's, he's a put out, I think, a paper that actually classifies rollups as what he calls validating bridges. So there's a few out there that what a roll up is really a type of bridge. And what that means is that the bridge is core to the protocol.
01:12:29.832 - 01:13:40.400, Speaker B: So what does that mean? Does it mean that rollup bridges can't have bugs? Absolutely not. Software bugs can definitely find their way into any piece of software, as can protocol bugs. Of course, what it means is that as opposed to other types of bridges where basically you have network one, network two, they have their security assumptions, then the bridge is just another set of assumptions that doesn't exist in rollups, particularly for the canonical bridge between the layer one and L2. If you want to go ahead and bridge from a roll up to a different network that it's not on top of or to another roll up without going through the base network, then sure you're going to have to bring in one of these non fundamental bridges that have their own assumptions. But the roll up bridge, which is the one that secures the value and brings it from the base layer, from the layer one. So the roll up is part of the protocol and all the guarantees of the protocol apply to the bridge as well, which I think are very strong. Again, that's not to say that there can't be anything wrong with the bridge, it can't be software bugs, but it's to say that from a protocol level, it's a much more simplistic design and it doesn't add its own set of assumptions.
01:13:40.400 - 01:13:48.160, Speaker B: So that to us is exciting. Roll ups are again, in one view just very secure bridges.
01:13:48.740 - 01:14:25.360, Speaker A: Yeah, definitely appreciate the clarity, I think. Again, there's lots of nuance, so definitely appreciate you laying it out. Last thing that kind of wanted to touch upon Washington, I think another popular topic in the Ethereum community at the moment is layer threes. I feel like we're just now starting to get our start in L2s, but ultimately kind of talking about layer threes, what is often labs and arbitrum's kind of point of view on layers threes, kind of with that product focus in mind.
01:14:25.940 - 01:15:01.448, Speaker B: Yeah. So the first thing I always tell people is you have to think long and hard. You really want. So the layer threes are like blockchain that sit on top of roll ups, or there's another layer of roll up that sits on top of a roll up, basically. And often they're motivated from a project that says we want our own dedicated lane. Right. So I don't want just to be on the public roll up, I want a dedicated lane that sort of dedicated this project, depending on the for formulation, it could be that like it's a general chain and others can choose to be in this dedicated lane too, but it's just branded around them.
01:15:01.448 - 01:15:38.380, Speaker B: Or it could be actually that it only supports these contracts, it's more application specific. It's locked down to only support these contracts. I do think layer threes actually have a place, but I think you want to think long and hard about it. Here are some questions. One of the nicest things about blockchains, and I think the things that really allow defi to take off are these synchronous calls. And the idea that you can, like in one app and one call atomically take a flash loan out here, send it through this protocol, and you've seen these crazy charts. And that's pretty valuable for a lot of applications.
01:15:38.380 - 01:16:39.640, Speaker B: Also, things like accessing liquidity in that manner and that synchronous manner are very important. So there are a lot of benefits from being on a public chain. Another one is just infrastructure. Like you want chain link oracles, do you want Etherscan block Explorer? Do you want alchemy or infuria or quick node to run infrastructure? There's a lot of just shared public infrastructure that is available on the public chains, and maybe it will be available on some layer threes, but if you imagine that there are thousands of these, it's not going to be available on most of them. So that's one thing I think people need to think a lot about is what are the benefits? There are benefits for many projects, but there are also costs. There are public services that either won't be available or will be much more difficult to get available, or maybe we'll be able to access them, but the user experience, you'll have to go via the L2 or it'll be a worse user experience. So not to generalize too much, but I think users need to think long and hard about whether or not they want these.
01:16:39.640 - 01:17:12.460, Speaker B: But I do think that for some protocols they, they do make sense. And I think you will see layer threes on top of arbitrum for sure. But I always encourage people to think long and hard about the implications of this, not just think about the good, but also think about the bad. But that's not. And I do think that there are some projects that are probably excellent candidates for them, but it's probably a bit less broad than the amount of projects that you'd think about when you see all the chatter about it on crypto Twitter.
01:17:12.620 - 01:18:11.930, Speaker A: Yeah, no, I definitely agree. Going back to the product standpoint, being able to have that atomic composability, a major fan, as well as trying to keep latency as low as possible. And then, as you said, the infrastructure also becomes a challenge with a lot of these things. Again, I'm very interested to see how it all will work out. I am appreciative of everybody trying these different things and seeing what works, especially after looking at some of the analytics on the user adoption. TVL, I think is high, but I'm very across the Board in these Web three systems, but really trying to get Users and Active Accounts into these ecosystems and getting User Adoption is something that I'm really trying to push hard for. Appreciate everybody's kind of trying these Different Trade offs, but I think sometimes People forget the drawbacks of them as well.
01:18:11.930 - 01:18:14.454, Speaker A: Interesting to get your thoughts.
01:18:14.622 - 01:18:29.670, Speaker B: Yeah, and, you know, I think, you know, getting User Adoption is critical. And by the way, the thing I think about the key to getting User adoption is really tapping into some existing communities. You know, we, when we launch our Original Nova Reddit, they launched their Community Points on our chain on.
01:18:29.710 - 01:18:31.008, Speaker A: Congrats. That's awesome.
01:18:31.134 - 01:18:57.292, Speaker B: Thank you. Yeah. And it's exciting, obviously, to have to work with, like, a big partner like Reddit. But the other reason it's exciting is because, like, how, you know, people, like, there's this meme of, like, the next Billion Users and what do they look like? How do we get them? And it's not by, like, you know, me going on Twitter and, like, somehow reaching a billion people. I'd have to get a few more followers for that to happen. But in all seriousness, it's, in my opinion, it's about tapping into existing communities. Right.
01:18:57.292 - 01:19:34.242, Speaker B: There are large communities that exist and we have to, rather than, you know, try and go after individuals you need, you know, that helps. I'm not saying we shouldn't do that, but rather than thinking we're not going to get to a billion users by getting to a billion individuals, but, you know, if we are able to tap into products like Reddit or, you know, you see companies like Instagram doing things on the blockchain, that's a path. And a lot also now with gaming companies, these are paths to bring mass, mass markets to the blockchain. I think that's how, I think that's a big piece of the puzzle of how we attract mass market users here.
01:19:34.386 - 01:20:21.820, Speaker A: Yeah, I definitely agree. Excited to see the web two companies kind of ultimately dip their toes in the water of Web three. And now that the scalability is getting to the point where they can actually use it. It's very exciting to see what they do kind of with wrapping it up now that there are kind of more multiple different L2 solutions in the space and kind of all kind of eyeing to kind of be the more dominant chain. What would your advice or words of wisdom that you would give to people on building on arbitrum and the products that you're building and even the users to kind of come and explore what you and the team are building at arbitral?
01:20:22.400 - 01:21:06.630, Speaker B: Absolutely. So I think technology is probably the foundation here, which is we have strong technology, we have a very strong team, we have market leading technology, and really the only fully built out code, complete roll up that exists today for general purpose roll up that exists. Community is also a big part. We're fortunate to have a very strong community, I think in large part from appreciation of that technology, but also just a lot of really strong community members, I think is important. Also very strong ecosystem. Arbitrum defi ecosystem today is exploding, and there's just so much to do there. And if you're a Defi project, that's probably where you want to be, because you want, again, back to those synchronous interactions and also liquidity.
01:21:06.630 - 01:21:51.980, Speaker B: You want to be on the platform where you can for Defi, for many users about these tools and these building blocks, and you don't even always know exactly how different users will compose these different tools. But being in an ecosystem where you give those options to users and make your tools available to users, I think is important. It's ecosystem, it's community, it's technology. The other thing I'd encourage people is to really focus on those. And other things are nice, like saying grant programs, for example. Very, very nice as a cherry on top. Make your basic decision based on where you want to be, where you want your community, where the technology feels right, where you're best suited.
01:21:51.980 - 01:22:26.576, Speaker B: And things like additional grant programs are obviously very, very nice. But it's not going to make or break your generally, for a funded project, certainly it's not going to. The other fundamentals need to be in place for success. And I'd encourage users to really, really focus on the fundamentals. And I think those are, at least from what I've seen, the projects that I see succeed. Those are critical to the long term success of projects, really thinking critically and building out their business model based on these fundamentals.
01:22:26.728 - 01:23:02.150, Speaker A: I fully agree. I think it's beautifully put. I try to wrap up the podcast with what I call spicy questions. The thing that I've kind of been asking people recently is what project or ecosystem outside of what arbitrum is building, have you kind of found to be doing some things uniquely well, and then what could be layer one or L2, and then what is like an ecosystem or layer one or two that you think is just kind of taking the wrong approach?
01:23:03.090 - 01:23:52.400, Speaker B: Okay, good question. One ecosystem that I've always had a lot of respect for from a technical perspective, is the zcash community, actually. So Zuko and also like Matt Ian, et cetera, really amazing technologist. And a lot of the innovation and zero knowledge proofs, really my opinion, came to the forefront. They were the first ones doing a lot of these things. And actually, another one, while we're talking about ZK technology that's doing a lot of interesting things there that people don't even realize is also the filecoin ecosystem using ZK in very interesting ways there as well. I've always been a big fan of their technical work very, very much.
01:23:52.400 - 01:24:40.382, Speaker B: Another one, I think people, I think we see a lot of really interesting this idea of layer threes and now coming to others, but the predecessors of a lot of this is what you saw in the cosmos ecosystem. They really organize around their chain. So I think a lot of that wisdom now, people are thinking about this in different ways, but I think they get a lot of the benefit of that innovation around, around a lot of the DAP chain conversation that's happening now in terms of ecosystems that are probably doing the wrong thing. I'm not going to name any in particular, but back to what I said previously. I think focusing on the technology and the community are so, so critically important. I am so bullish on Ethereum. It's half technology, half community.
01:24:40.382 - 01:25:11.092, Speaker B: And there will be others out there that will say, but look, our technical design is much better than the EVM. And maybe they're right, maybe they're wrong. But the point is, I'll tell you this. You look at the list of eips that are non controversial. There are many, many, many of them. No one designing Ethereum today, just about no one would, if they can go and clean slate, would design what they have today. They'd all want to make at least some changes that are somewhat difficult because of existing pressures.
01:25:11.092 - 01:25:31.452, Speaker B: And that's okay. So the fact that Ethereum is not only about its technology, it's also about its community. In my opinion. It has the best technology, and it also has the ability, as it's shown recently, to innovate. And the merger was a really big one. It showed that Ethereum is not stuck in time. Ethereum can incorporate innovation.
01:25:31.452 - 01:26:08.600, Speaker B: And hey, some people may say it's at a slow pace, but it's at a pace that's responsible also and takes effect, its responsibility to its community that exists. And so I think that the focus on the tech and the focus on the community are critically important, and others that come along and focus and say, but this piece of technology is better than that, are sort of just missing the larger picture of what is happening in the Ethereum ecosystem and all the mind share there and all of the technical innovation that has happened and is happening in that community. So, yeah, I know. Not as spicy as you probably would have hoped for.
01:26:09.860 - 01:26:10.960, Speaker A: I got to try.
01:26:11.580 - 01:26:12.640, Speaker B: Absolutely.
01:26:13.660 - 01:26:30.892, Speaker A: No, I really appreciate you kind of walking everybody through this, even the technical nuances of how arbitrum and off chain labs is building things. Really do appreciate it, and I wish you and the team the most success. I really do appreciate it.
01:26:31.036 - 01:26:33.428, Speaker B: Thank you so much for that. And thanks for having me today.
01:26:33.564 - 01:26:36.300, Speaker A: Awesome. Thanks. Awesome.
