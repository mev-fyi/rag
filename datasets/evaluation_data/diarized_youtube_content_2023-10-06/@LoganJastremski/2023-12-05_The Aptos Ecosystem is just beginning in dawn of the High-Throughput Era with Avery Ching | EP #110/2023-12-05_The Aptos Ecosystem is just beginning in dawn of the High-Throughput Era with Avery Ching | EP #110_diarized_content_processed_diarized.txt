00:00:00.280 - 00:00:30.622, Speaker A: For any kind of large customer base to be able to use the blockchain efficiently. High throughput is obviously required because without that you have a very poor user experience where transactions are severely delayed or never completed or fail. There can be very interesting reasons for alt two s to exist. I think traditionally most of the reasons now have been to scale out other stacks that just kind of the base layer doesn't scale that much. The issue, of course with that is the fragmentation of the ecosystem. The user experience is suboptimal. When I've got to manage assets in one ecosystem and then transfer them to another.
00:00:30.622 - 00:01:02.630, Speaker A: I know it's not horrible, but the bridging time takes effort, takes a lot of user cognitive overload. Having that together in one ecosystem is really, really important. And especially with a network like aptos, we don't have this kind of constraints on the bandwidth. The throughput and the latency aspects were, but we tried our peer to peer transfer test and we saw peaks of 30,000 transactions per second, which is definitely the highest verifiable test result across the industry.
00:01:05.850 - 00:01:27.790, Speaker B: Avery, thank you so much for rejoining me. I feel like it's been some time since we last sat down together in Palo Alto and did the podcast in person. A bunch has happened since then, but really excited to do this podcast. I think the Aptos ecosystem has been making tremendous progress. You've been doing a lot of interesting things on the technical side, so I think it's going to be a great chat.
00:01:28.270 - 00:01:31.726, Speaker A: It's my pleasure to be here with you again, Logan. Always a pleasure to chat.
00:01:31.878 - 00:01:53.410, Speaker B: Amazing. Well, let's jump right into it. I think been slightly over a year since we last chatted. Aptos is live now on Mainnet. Before it was not. Maybe just a quick recap of how things have been going on from the ecosystem level. What cool things have you doing from the technical side? Love to dive into it.
00:01:53.950 - 00:02:18.926, Speaker A: Perfect. Thanks for starting off with that point. It's been a very eventful slightly over a year since we last chatted. We launched our network in October of 2022, and for those of you who weren't following at that time, it was maybe the fastest network launch of all time. We formed our core team in February and we went to market in eight months later. We had three incentivized testnets. We had a bunch of other testing.
00:02:18.926 - 00:03:08.144, Speaker A: We had a lot of developers coming live with us on day one. Super excited about the teams that have put in the blood, sweat and twitters to get us to where we are. And at that point I think we've had hundreds of developers building on the platform and testing it out. Our thesis has always been to iterate as we learn more about those developers that have been moving the platform forward. And so just taking a quick side note is that while we had worked on diem for four years before this, it had never been tested at scale with a large number of developers and different kinds of applications. It had been tested in a very small context around money movement and remnants transfers. So being able to kind of see in the wild what people want to do, how people want to use moved, allows us to kind of iterate and evolve that language in many ways that we didn't even think about.
00:03:08.144 - 00:03:50.636, Speaker A: But with this developer community, it's been amazing to push forward new language features like higher order functions, making sure the prover is accessible to the community, to make sure they can reduce their time from ideation to getting to production in a safe manner. And we've also seen massive partnerships, I think from aptos in enterprise space, all the way from companies like Lotte in Korea to those like Microsoft and of course the US. And just seeing this ecosystem come together really, really quickly and also delivering on the technical promises that we kind of made early on. We said we're going to have a very aggressive launch schedule. We're going to do these things. We're going to be the one, those few, if not any, crypto team that actually hits the milestones on time. And then what we promised after that was rapid, rapid upgrades.
00:03:50.636 - 00:04:21.757, Speaker A: And so we've seen in the last year is new cryptography primitives coming in, Quorum store being launched to improve throughput. We've seen massive improvements in the framework upgrades that have come and token V one, token V two standards that responded to developers needs and opportunities in the space. So it's been a very fruitful year. We're not done yet, though. We have a lot of things to do. The other point I want to raise is we started to build some interesting products in the space. And so on our anniversary of our mainnet launch, we launched Graphio.
00:04:21.757 - 00:04:56.732, Speaker A: I don't know if you had a chance to play around graphio, but it was this interesting collaborative art drawing canvas where we wanted people to kind of draw whatever they wanted to to commemorate the one year aptos Mainnet anniversary. And the response was definitely exceeding all our expectations by a lot. We had more than 600,000 wallets drawing simultaneously over a 36 hours period. And it was absolute chaos. If you try to draw something, immediately be overwritten. It was, it was the beauty of the Internet and chaos and life and people all wrapped into one and the outcome. You can go to graphiel art if you want to see it.
00:04:56.732 - 00:05:07.772, Speaker A: It was beautiful in its own WAy. It was really representative of what we thought humanity was. Chaotic, beautiful, and not what YoU would expect for sure from the start.
00:05:07.916 - 00:05:15.170, Speaker B: That's fantastic. Amazing progress in eight months. That is truly impressive. That's quick.
00:05:15.790 - 00:05:34.398, Speaker A: Yeah. Yeah. I think, you know, we have a really amazing team here. Super hard working, focused, and it's not a huge team, but it's definitely a lot of motivated, you know, people that want to see a public utility come into existence that's going to be built for the long term. And I think that mission pulls us all together and keeps us pushing, pushing really hard, 100%.
00:05:34.454 - 00:06:14.370, Speaker B: ANd even in my time at Tesla, those small teams, I feel like we're the true ones that were pushing things forward. Amazing to hear that's happening internally as well. I think. Kind of staying on the topic of upgradeability, I remember that as like a core focus when we first did our initial podcast. Can you maybe dive deeper into some of the things that you've seen or kind of that you mentioned earlier on the upgradability side, maybe that you like, either didn't notice or that you saw that like, you couldn't fit into the early prod network and just needed to get shortly live. How has the network kind of evolved since being in production?
00:06:14.750 - 00:07:23.878, Speaker A: Yeah, so we have AIP process, or aptos kind of improvement proposal process for upgrades that anyone in the community can propose. Things like delegated staking were actually built out by one of the staking operators, which is really neat to see, and anyone can contribute towards that and give feedback and watch that code commits go into production and then see a timeline for it. I think I take a lot of pride in the fact that the network is able to upgrade really, really rapidly from the time of AIP inception until the point at which it goes live, usually in the order of a month, maybe two at the most, depending on the type of upgrade, the complexity behind it. But some key things have come out. Delegated staking, but I'd love to get that done before Mainet, but it came after and that's totally fine. Things like Quorum store, which is a really great improvement on how we think about the first step for data dissemination at scale, where validators can now send blocks of batches of transactions to be kind of preserved and maintained before they're off to ordering, was launched in July, and that launch has been phenomenal. No issues, just seeing massively higher throughput that's available in the network.
00:07:23.878 - 00:08:10.726, Speaker A: And also our crypto libraries, I think, have been amazing upgrades that have come in to support things like ROS 16 bulletproofs, and also things like confidential type tokens where you can send out a token, have a transaction between you and I. Our addresses will be known on the chain, but the amounts transferred will be hidden. And so those kind of things just come as users and developers make their requests or start to submit what they think is important to them. I think the object standard is also very important. So as we saw, people wanted an object type standard in the framework, which was a differentiating feature from others. We did it at the framework level rather than the core system, because we can actually upgrade those things much faster. So we went from object v, one standard to object v two standards.
00:08:10.726 - 00:08:46.866, Speaker A: We understood more of those use cases and more of what people wanted to see, and being a bit more biased and opinionated about this is the way you ought to mint tokens, this is the way you sought to transfer them. This is the standard that we think is going to help the ecosystem get the most growth out of it. And so I think it's been an amazing set of upgrades that have come on the network. We have a lot more coming, I think. So we've started to discuss on chain randomness, which is something I'm really, really excited about. We'll come up with some really cool name for it, project name in the future. I know this has been done in other networks to some degree, but the difference about this in Aptos is that it's going to be stick weighted randomness.
00:08:46.866 - 00:09:24.438, Speaker A: It's going to be something that's available for every single transaction. It's not going to be gameable. And this kind of primitive allows, especially game developers, to produce the truly. When I open up a treasure chest, what is my random odds of achieving certain kind of outcomes? I can't cheat the system, I can't game it. We've seen so many issues with that in nfts that have happened over the years, where people understand the algorithms, they can bias the election towards their results. And this is going to be a primitive that completely changes the game when it comes to any kind of randomness entropy that comes into the system. And so there's a whole host of applications that are excited about building on that.
00:09:24.438 - 00:09:29.582, Speaker A: We're planning an amazing hackathon just specifically around this feature. More to come in that space.
00:09:29.726 - 00:10:05.340, Speaker B: Amazing. And I think you do an exceptional job as an engineer, speaking to the engineers, but maybe for those slightly less technical gifted, what are even perhaps speaking to engineers, what features like feature sets are you most proud of the Aptos community cultivating? And for those that are even less technical gifted, what would your recommendation be to view the Aptos ecosystem versus all the other modular l two s, l three s, l 100s that are in the space?
00:10:05.960 - 00:10:54.514, Speaker A: Yeah, I think that's a good set of questions. Maybe I'll answer the last one first, which is that l two s. There can be very interesting reasons for l two s to exist. I think traditionally, most of the reasons now have been to scale out other stacks that just kind of, the base layer doesn't scale that much. So like Ethereum, and I think it does work. We've seen base come out, we've seen others that have launched recently, and the issue, of course, with that is the fragmentation of the ecosystem. The user experience is suboptimal when I've got to manage assets in one ecosystem and then transfer them to another, I know it's not horrible, but the bridging time takes effort, takes a lot of user cognitive overload, and it's just broken compared to the system we have today in our current experiences.
00:10:54.514 - 00:11:32.930, Speaker A: When I go to online banking, if I had to transfer funds to another bank, wait seven days for those funds to be transferred and then be able to use them, and I also have a lot of risk where that transfer can be hacked or my funds can be lost, and a lot of things can go wrong. I think that's definitely a concern. I think we're having that together in one ecosystem is really, really important. And especially with a network like Aptos, we don't have those constraints on the bandwidth, the throughput, and the latency aspects where everyone build together and just share a part of this decentralized database, but have that composability still maintained.
00:11:34.350 - 00:12:15.900, Speaker B: To me, the magic of web3 crypto was that you're taking these heterogeneous systems, making, like, one unified standard, like, everybody agrees upon that standard. It's interesting to me now seeing like, there's definitely some applications for modularity, but that we're going back to kind of like, these fragmented ecosystems, and then we're trying to, like, put them back together. It seems like we're. I'm glad the experimentation is happening, but it seems. It seems more difficult. It seems like we're going back to kind of the what to, like, silo databases, where, like, I, the web3 promise was being able to touch all these interesting objects within a shared ecosystem.
00:12:16.760 - 00:13:03.150, Speaker A: Yeah, I understand there's a lot of experiments going on with modular blockchains today. It's a different path. I think ideally to get into the same path, which is a very small number of ecosystems, where these ecosystems are really the ones that developers have gathered around and feel like they're supporting their needs and use cases most efficiently. I think the best example of that today we see is in cloud computing where there are four or five major cloud providers that take up 80% or 90% of the traffic and the usage and the revenue. The rest are handling very very small tail use cases. We'll see that over time. That's one path to getting there is to say, well, I don't know what the execution environment to be, I don't know what the programming language should be, but we'll support all of them and then we'll see what shakes out over time.
00:13:03.150 - 00:13:34.456, Speaker A: Our stance has been always the opposite of that. We want to be very opinionated. We want to make sure that we support the use case really really well. The analogy I give to this is like the Linux kernel. It's a monolithic kernel. It is something that Linus decided upfront was really important to him as he making progress in terms of being simple and performant and giving out maybe a little bit of the here's the other things people can build on top of it. There are definitely things like printable modules and all that for some kind of composability, but very different than the microkernel approach.
00:13:34.456 - 00:14:23.132, Speaker A: And I think Linux has done really really well because of these design decisions. It is important to have modularity at the implementation level though. So the ability, I think we've talked in our last podcast about aptos being this pipeline chain of execution. We have data dissemination, you have ordering, you have parallel execution, parallel storage for efficiency reasons and you kind of have the proof generation after that. This natural modularity allows us to move each of those components forward in its own individual standpoint, allow them to test these individual components. But when you have a modular blockchain based approach, you also have to think about how does the interplay of testing work. If I want to test how fuel and move interact together on Celestia, for example, it becomes much more complicated to pull those testing opponents test for every single commit that happens in either those ecosystems.
00:14:23.132 - 00:14:51.222, Speaker A: Whereas being very opinionated about it, saying so far we've heard from all our developers, the way that move is working on Aptos is really, there's no complaints about it. Once people start using it they're like wow, this is amazing. For me I'm able to get a lot of more productivity out of it. I'm able to deploy my code faster. I'd love to have this feature, and we can go ahead and add that feature really, really quickly. This is a space where we can move faster than others. In the EVM space, it's obviously got a lot of traction out there.
00:14:51.222 - 00:15:32.530, Speaker A: But the downside of that is that being able to move the VM forward and able to move the language forward, it takes a lot more consensus building, and it's going to move at a very different pace. Whereas in our ecosystem, once our developers explain some of the things that they're looking for, oh, we really need the move proverbs support to cover this particular aspect. Those changes can be done really rapidly, and we can all iterate together much more quickly. I think the same kind of resource rust as the language grow over the last five years, it's been a very rapid progress because the developers are so passionate about, and they keep pushing the feedback into it. Whether it's going to be adding new functionality or new kind of language features, those things all happen rapidly. We see the same thing happening with move within the Aptos ecosystem.
00:15:32.690 - 00:16:11.212, Speaker B: Amazing. Really amazing. In terms of, when I think of Aptos, I think about what you guys have done with move. Can you talk a little bit around some unique things that you've built with move? I know ultimately, as you mentioned, supporting some of the other programming languages, but I think when I peer into my crystal ball, I see move ultimately becoming a predominant programming language, if not the predominant programming language for web3 assets, because of some of the protections that it allows from some of these early iterations of programming language. Can you talk about that? And specifically the Aptos version of move?
00:16:11.396 - 00:16:58.962, Speaker A: Absolutely. So just quick history move. Of course, it was built at meta in 2018. It was started off there. And I think obviously the principles make a lot of sense today, which is how do you prevent programmers from making those easy mistakes? Programming for smart contracts platform is so different than a generic programming language, which is find or write some bugs, fix those bugs in production, roll forward. Those kind of allowances are much more costly in a smart contracts language platform. So how do we push fewer mistakes into the code? How do we maybe limit the set of things that can be done while still allowing programs to get their work done as efficiently, as quickly as possible? But it never was tested at scale, and that's why we love about bringing the Optos network to life.
00:16:58.962 - 00:17:57.150, Speaker A: We're kind of the longest running move chain out there. Like major move chain since 2022 of October, and we've gotten a lot of feedback since that time. What our developers have told us is that they want to see. They really love the fact that the move prover works for everything in Aptos, and I think we're the only move variant where the moveproofer can cover all the objects being created, the global storage. One thing neat about that is that the framework itself is fully formally specified and also proven by the move prover, which is really neat and gives programmers a lot of confidence in that the libraries they're depending on are highly trustworthy, in addition to auditing, of course. I think the other thing that's really cool is that we've added things like higher order functions. We also support code upgrades in place, backed by either multisigs or other known primitives, but that allows us to iterate and improve things as time goes on very quickly.
00:17:57.150 - 00:19:19.004, Speaker A: The other thing that I think is really exciting is people don't realize this. There's a lot of legacy code move from the VM and from the compiler, while we've made massive improvements to support block STM really really efficiently, and I think others have benefited from this in the ecosystem. We also are looking at move efficiencies going forward in developing the new VM, and it's a big endeavor, it's a big effort, but to push forward and how do we really optimize the VM to work well with the underlying infrastructure? How do we take advantage of all the underlying implementation of pale execution at the blockchain layer from move and co design? This almost like a hardware software co design, which is obviously important for traditional systems. Then from a compiler based approach, as we think about adding new features, as you think about adding the usefulness of interfaces in the system without allowing reentry attacks, which is something move prevents very well. We see those every week happening in the EVM world. Sadly, that has to be solved really, really soon. We're starting to propose new ideas there and find that kind of balance where perimeters can reuse code as easily as possible without having again to deal with the reentricity problem that we've seen elsewhere.
00:19:19.004 - 00:19:56.260, Speaker A: So in summary, just really listening to our developers, listening to what features they need, building out the best developer experience possible. We actually added a gas profiler yesterday. We kind of announced that so you can use flame graphs to kind of see where gas is being used in your programs and optimize accordingly, depending on your optimization metrics. And then the last thing of course is going to be how do we kind of continue to build on this theory of parallelism out there and really drive throughput to the point where you have very low costs that are going to stay low pretty much, because you'll never hit those throughput limits in the system.
00:19:57.040 - 00:20:16.992, Speaker B: Some of my favorite topics are parallel processing and throughput. I would love to dive into each of those. You mentioned a new VM or VM version two. Can you maybe dive slightly deeper into that? What were some of the early learning lessons? Just seeing the network in production and what has been improved on with version two?
00:20:17.096 - 00:21:10.890, Speaker A: Yeah, the early version of the VM is designed for single threaded execution. I think most people that use parallelism for the VM are going to spin up multiple vms and execute those. But there are more optimal ways to do this. Exploring how does a single process VM kind of work in a multithreaded environment that reuses code libraries? And the way we load and cache and store operations can bring massive efficiencies. And so that's one project we're taking on. It's going to be highly complex, highly challenging, but we believe it'll lead to massive, massive wins in the vm space. From a performance aspect, from the compiler side, I think that one is really interesting because unifying the compiler between the move prover as well as the language, which really been separate for some period of time, allows for us to iterate much much more quickly on language features and also making sure that preorder support is there for them on day one.
00:21:10.890 - 00:21:27.170, Speaker A: And so that's a really exciting effort. And I also, I think the compiler right now is in beta. We're going through a lot of testing with it. Obviously we want to take that very very seriously. We don't want any mistakes or bugs that come out of the compiling and we'll definitely encourage the user community and developer community to help test on that with us.
00:21:27.950 - 00:21:49.490, Speaker B: Yeah, the parallel processing is tricky, I guess. Have there been any things that you've seen just from the production environment today that you're like, ah, this is maybe some real, real world randomness that I was not anticipating people using aptos like this, and now we kind of have to make some changes?
00:21:50.470 - 00:22:25.904, Speaker A: I think what we've seen so far for the parallel processing side is that things have worked surprisingly well. We haven't. You know, we always palette processing is very complicated and I think the biggest challenge is that complexity introduces a lot of potential for bugs. And so we thought we'd see some bugs in the parallel processing unit as things rolled out. Even in testing and in production we might have missed something, but we've been pleasantly surprised to see that everything has worked very, very similarly. The first thing people always see when they see a bug is like, oh, it's part of the parallel processing, but it never is the parallel processing. To our experience, which is really interesting.
00:22:25.952 - 00:23:15.500, Speaker B: So far, that's good news to hear. I think. Hopefully, I'm excited for everybody to eventually get to multiple client implementations just because those bugs are tricky. But in terms of you also mentioned expanding upon throughput, I think this is a underappreciated aspect of web3. In my mind, we're historically going from low throughput blockchains, transitioning my analogy from dial up to broadband, and now fiber optics and the blockchain equivalency. Can you expand upon why high throughput is so important for the blockchain ecosystem as a whole and then go specifically into what have you been doing in the Aptos foundation to really make that the best high throughput ecosystem?
00:23:16.080 - 00:24:06.484, Speaker A: Yeah, I'd love to. So first of all, I'd say for any kind of large customer base to be able to use the blockchain efficiently, high throughput's obviously required because without that you have a very poor user experience where transactions are severely delayed or never completed or fail. It just limits the scale of what applications can be built on top of the blockchain if you don't have a high throughput blockchain. The other thing, of course, is that the price starts to become a real issue as the throughput limits are reached, most systems are going to drive prices higher. This demand based pricing makes also the number of applications that are usable in the blockchain just fall to a very, very small number. We believe that high throughput is required for large enterprises, large applications to be developed and built there with high confidence. It's very much the same problem of what happened in cloud a while back.
00:24:06.484 - 00:25:01.800, Speaker A: If you think about cloud in the early days, a lot of large companies like Meta and Facebook that I was at were concerned what if we put a large part of our infrastructure on Amazon or on Google or others? What if they drive the prices on us eventually, over time it's not secure enough. It might not actually support our workloads. They don't have enough machines or these type of machines that we need to support our workloads. And now we're going to be constrained and we're going to have to have a poor user experience. Those very valid concerns, I think those concerns have been largely addressed today for most use cases, except for the most extreme ones, honestly, I think Aptos is there today. We can actually build some of those very, very extreme high throughput use cases on top of aptos. And we'd love to share what we've done in that space with respect to what we call Previewnet right now, if that's okay with you, Logan, for sure.
00:25:01.800 - 00:26:01.768, Speaker A: Okay, so one thing about Aptos that's definitely different than other networks is we take our co development process very seriously from a testing perspective. Every code review has two commits, sorry, two reviewers. Every code review has unit tests, regression tests that run on it before committing. We also have nightly tests that kind of spin up clusters and run a whole bunch of security tests, and byzantine fault tolerance tests, performance based testing. And then when we finally get ready to make a release, we will of course deploy to Devnet, then Testnet, then mainnet. But for features that are kind of more, let's call them a little bit more experimental, we will put them to what's called a previewnet as well. Previewnet is a really cool exercise where we get together with the node operators for Aptos and we mimic what the mainnet looks like in terms of the number of nodes out there, the staking distribution, as well as the node operators and the geographical distribution.
00:26:01.768 - 00:26:45.654, Speaker A: Then we run a series of performance tests as well as drills onto what the results might be. And so we finished our second preview net and oh, I do want to pause here a little bit and talk about benchmarks to some degree. In March we actually released a study on performance we talked about. From our point of view, the industry is lacking a couple of things. One is, what is the framework for repeatable testing? A lot of people put out numbers, but they don't put out the ability for people to really independently run those tests and kind of repeat those results that are going into production. The other thing is that we need to also produce same thing on temporal benchmarks. Those benchmarks, to be reproducible, they need to be kind of well understood.
00:26:45.654 - 00:27:27.304, Speaker A: And over time they need to cover a whole suite of tests ranging from different kinds of use cases you have in the blockchain, money movement, NFT minting, maybe some DeFi applications. Those kind of things need to be put into test suite the same way that you have TPC benchmarks for databases. TPC benchmarks databases allow different kind of databases to compare their performance across different types of workloads. Unfortunately, our industry is still in a very nascent phase where people throw out numbers. There are sometimes theoretical numbers. There are numbers based on unrealistic assumptions these numbers are very hard to reproduce, if impossible. And so we started our first phase of that in, in March of this year, kind of putting out, here's the numbers, here's what we're testing.
00:27:27.304 - 00:27:50.690, Speaker A: We're testing peer to peer transfers of funds. We're testing in an environment of this 100 plus nodes like Mainnet, across three different regions. And here's the exact hardware that we used on GCP to have you replicate that same experiment. And with that, we think that's the first step towards that. Now, preview.net two is taking that to the next level. We of course kept the same setup.
00:27:50.690 - 00:28:20.130, Speaker A: We have the same, you know, a bunch of node operators are participating. We have the same kind of stake distributions that mimic mainnet. And this one we actually added a couple of new tests, which we found to be very interesting. I think by the time this podcast was live, all these numbers will be present and people can dig into them more detail. But what we saw was, was incredible. And we have to go through the results of why. But we tried our peer to peer transfer test and we saw peaks of 30,000 transactions per second, which is definitely the highest kind of verifiable test result across the industry.
00:28:20.130 - 00:29:03.414, Speaker A: Really, really proud of that and the work that the team has done to do that. The other thing that we've done also was to see how sustainable can we make. Throwing up the peak numbers is great for sure, but we also want to see how sustainable is this over a longer period of time. We ran our load tester for a whole 24 hours period, throwing out the maximum throughput we could even. The load tester itself is actually starting to fall behind a little bit of time, because that system needs to be improved a little bit. What we saw was we could actually, we could run more than 2 billion transactions in a single 24 hours period, which was kind of mind blowing. For context, just for context.
00:29:03.414 - 00:29:30.610, Speaker A: Visa net processes about 150 million transactions per day. So we can sustain well over ten 1212 visa nets on Aptos in this preview net setup, which was really, really cool to see. And it was a pretty constant, you'll see the graphs later, but they're pretty constant workload. You'll see them coming along. Everything is great. And we turned off the experiment after 24 hours. But it's cool to see that kind of sustainability.
00:29:30.610 - 00:30:22.630, Speaker A: The other thing that's a really neat result that we saw was we took a look at NFT minting. And NFT minting is really interesting because there's a sequential component of NFT minting when it comes to the sequence numbers, right? So you know, you have the NFT version, one, two, three, to say a million out there. And this typically is a huge problem for parallel processing engines to build it to figure out. And you know, like how do I make sure I don't mint beyond the supply? How do I make sure these are all numbered in order? And so we've developed this technology called aggregators. And so for those of you that are familiar kind of with, you know, Mapreduce, there's a notion of anything kind of certain types of operations are aggregatable. They are things like addition, for example, don't need to be computed upfront. They can be computed later on in the process.
00:30:22.630 - 00:31:07.352, Speaker A: With aggregators we'll have a very good set of detailed posts out there. They allow us to make these sequential operations of itering a counter mostly parallel. By doing that, this allows us to do NFT minting at very, very high throughputs. And so our previous experiment showed kind of around maybe one k per second of minting of nfts, which is still pretty good obviously for sequential engine, but with aggregators we get ten k per second sustained. And so what this means is you can mint basically, I think a million nfts in just 90 seconds. You can mint 5 million nfts in eight minutes. It's a crazy result and we're excited about that different kind of this.
00:31:07.352 - 00:31:41.440, Speaker A: Again, nfts minted with a fixed supply as well as an index number associated with those. So these results I think are pretty amazing. They're just about maybe eight months after the previous results we had for PVNet. And I think also more importantly we start to see these standardization of benchmarks coming out where we'll produce the benchmarks or boost the setup. Anyone can rerun them on the same kind of experiments of hardware that we used. And we'll of course be running more preview nets in the future to kind of keep developing and pushing forward in the benchmarking as well as the open production requirements that come out of it.
00:31:41.600 - 00:32:36.400, Speaker B: I would love nothing more than the industry to agree upon a set of standards, but that is very hard to do. I think frictionless capital has taken a crack at it. I always need to improve how to do that. But very happy that you guys are publishing something that can also be reproduced because I think that's also one of the challenges. I mean one from the engineering side, it's hard to determine if you're a smart contract engineer on where to build your application because it's not super transparent. And then from kind of the investor side, there's always these fabulous claims, about 100 million billion tps that don't come to fruition. And so I am definitely in the corner of either reproducible or some set of standards that the industry can agree upon, because they would really push, I think, the entire space forward.
00:32:37.300 - 00:32:55.480, Speaker A: Yeah, I mean, we are 100% alignment with that. That's why we're trying to do our part to push forward these reproductive benchmarks and make sure that everyone can use them if they want to. They're helping to recreate them for their own systems and produce similar results. And we invite the industry to really join us in this journey towards more transparency around performance.
00:32:55.860 - 00:33:31.900, Speaker B: I appreciate you guys doing that in terms of, I would say, things that can utilize high throughput or kind of transitioning from the infrastructure to itself to applications. One hot area has been AI. Everybody likes talking now the interesting thing has been drop out of crypto, pivot to AI, but now I think we're back to maybe drop out of AI, pivot back to crypto. But I think you have some interesting viewpoints and the intersections of the two industries. I would love to dive deeper a little bit into those.
00:33:32.560 - 00:33:59.578, Speaker A: Absolutely. So just some context maybe. I dropped out of AI to work on crypto meta. I was working on data infrastructure for seven years. So a lot of data infrastructure is actually doing feature generation preparation and also some algorithms. I worked on matrix factorization and some other things related to ML in the past. And so it is really interesting to see these industries kind of collide in a good way.
00:33:59.578 - 00:34:47.954, Speaker A: And I think they've been really, really help each other a lot. So we've also been exploring this with Microsoft, who's really been involved in AI, of course, with their open AI efforts, with what they've been doing in Azure, with Dali and with chat GPT. And so I think there's a couple angles we could talk about this in. So one thing is we've been definitely working on some interesting products alongside them. One is going to be the app test assistant, another one is going to be TBD, but it will be released hopefully in the coming weeks or months, which is going to be consumer facing product, but just more broadly than the space of AI and blockchain. There are so many different things that are interesting about this. One thing is we see the advent of content that comes out there, and now we're not sure what to trust as people.
00:34:47.954 - 00:35:28.370, Speaker A: We see a video of some horrifying footage. Is it real? Is it fake? Or we see a politician's message or something. That's just unbelievable about this politician doing something really silly. And it's again, hard to tell this been doctored. Is this real? And so this is where blockchain can really help. You have the ability to verify, attest to what the content provides. So if, for instance, I have a message and I'm a political candidate, I can go ahead and sign something that says on chain like verifies my id, also shows that this is an attestation that I support this statement.
00:35:28.370 - 00:35:43.368, Speaker A: And someone can see that attestation on a decentralized ledger like Aptos and be able to have high confidence that I am standing behind that message, that this is truly my statement that I want to make.
00:35:43.494 - 00:35:54.956, Speaker B: Similarly, the world has to move in this direction. There's so much content now being generated. I mean, even just the pictures, even some videos now.
00:35:55.068 - 00:35:55.932, Speaker A: Exactly.
00:35:56.116 - 00:36:14.890, Speaker B: It's getting harder and harder and you can definitely see as it continues to ramp how good the technology is going to get. It doesn't seem like there's really any other option than the intersection between blockchains and kind of that private key that you're signing. Hey, I actually verified this is me and I post this message.
00:36:15.390 - 00:36:53.918, Speaker A: Exactly. And I think on the flip side too, it's not that these companies that are doing AI want to be responsible about it. They want to also kind of let people know that if I generate an image with Dolly, that this is a dolly generated image, right? So having that lineage to say like, you know, this was generated by Dolly, you know, don't think this is real or whatever, and kind of specifying the parameters, maybe that was used to generate it or the version of software. All our possibilities and those being stored in the blockchain as is lineage and proof of audibility, I think is really, really important for everything that's going to be generated in the future. And there are billions of pieces of content that are generated a day. That's a great use case for a.
00:36:53.934 - 00:37:21.780, Speaker B: High throughput blockchain, obviously 100%. Any areas. One other area specifically in the investment landscape that has been starting to heat up is this Airbnb for your graphics card per se, where you can rent out a graphics card on one of these decentralized networks. Any thoughts around, like, decentralized computing used on blockchains or this category?
00:37:22.800 - 00:38:16.996, Speaker A: Yeah, I think it's a very interesting idea. At the same time, I do think it's going to be very challenging to disrupt. And the reason why is there's a lot of, there's a lot of kind of natural monopoly type features when it comes to cloud computing, whether it's going to be buying servers in bulk, buying cards in bulk, buying data centers and power in bulk, that make it very hard for decentralized infrastructure competing. It needs to acquire a mass amount of capital to kind of get those economies of scale and fight against these kind of national monopolies, monopoly tendencies. And so I'm very hopeful. Of course, I'm always an optimist, but it's just going to be a big challenge, I think, for us to go down that path and make substantial progress there. Just given that the current parties are well entrenched and they have those economies of scale.
00:38:17.148 - 00:38:39.384, Speaker B: Yeah, it's remarkable how many GPU's now you actually need to do a large neural network training. The inferences, I would say difference, maybe you could potentially do that in a more decentralized fashion. But training, I mean, those GPU's are co located, they're interlinked, all co located, and you still take long times to actually train those models.
00:38:39.552 - 00:39:09.510, Speaker A: Yeah, I think maybe the one thing that those systems could go for, those like the less distributed type training, or the kind of training that doesn't require high network speeds. And so there's always an interesting, you know, going back to when my data had on, there's always this interesting trade off of like speed versus compute power and also data amounts. Like ten x data does not yield ten x better results. There are probably use cases when it comes to training and inference and machine learning that work well on smaller subsets of hardware. And for those actually decentralized infrastructure might make a lot more sense.
00:39:10.450 - 00:39:53.058, Speaker B: I'm fascinated to watch. Hopefully we can see some of those built on top of aptos in terms of getting that technical talent into the space. I think there's obviously a lot of smart contract engineers that have kind of waded into the water over the years. Historically, they have started with like the EVM ecosystem, predominantly because that was the first smart contract platform. I think one thing that I've observed, it's very hard to convince other engineers, whether it's the tribalism or just being comfortable with one smart contract platform to another. It's hard to go from like a different programming language per se, or at least a different vm. That is also a challenge.
00:39:53.058 - 00:40:14.110, Speaker B: And so I think one place that I'm super interested in is just how we bring more adoption to say, like the Silicon Valley of the world, or all the hundreds, millions of engineers that are not in these smart contract ecosystems yet how do we bring those in to expand the pie of talent within the ecosystem.
00:40:14.880 - 00:41:09.530, Speaker A: It's a great question. I think one thing we do is we start earlier. So for instance, I think the foundation is holding a IIT event in Bombay with a series like a lot of students in terms of building out some really cool technology teams and applications on Aptos and getting them familiar with move from the beginning is definitely going to be helpful because once you've seen what move can do, especially the move variant on Aptos, you're probably not going to want to go back to anything else. And that's a big part of our efforts. The other thing is going to be just having the developers tell that story. And we've seen parties like Livepeer come over, we've seen parties like Sushiswa from the EVM space come over to Aptos. And being the first non EVM chain, I think it's been really, really exciting for them and they get a lot of benefit from that pancake as well.
00:41:09.530 - 00:41:23.410, Speaker A: Having those developers tell the story and what they've seen as the differences and the advantages of building on with move and Aptos has been something that we've definitely seen help the developers get much more excited about the platform.
00:41:24.470 - 00:41:50.720, Speaker B: Yeah, it's a slow trickle, and it's funny because in the bull markets, unfortunately or fortunately, number go up is often the bat signal for everybody to come look at crypto again. I'm interested to see this next crop. Hopefully with asset prices continuing to go up more broadly in the crypto ecosystem, that brings in more engineers.
00:41:51.660 - 00:42:09.464, Speaker A: Right. And the other thing of course, is just getting many more of our counterparts in the non web3 space to come up with. Right. So the more and more we make this feel like just a traditional database to them. Right. It's a JDBC connector or whatever. Yes, it's decentralized, obviously.
00:42:09.464 - 00:42:55.880, Speaker A: And yes, it has different characteristics, but from throughput's perspective it looks very similar. It's getting, you have very high throughput, it's giving you very good latency. It costs a tiny amount, but anyway, so does renting your MySQL server from Amazon. The more it starts to feel like that, I think we'll start to see those developers starting to understand like, hey, these are benefits I can't get anywhere else. I can't get randomness, like true randomness in a decentralized fashion. And I need that from my application, or I need this ability to have this immutable ledger type thing just to make sure that what I generate as content can be understood, the lineage of it and whether it's AI content or not. The more that that happens, I think the more we'll see the natural evolution from traditional developers move into more web3 applications.
00:42:57.180 - 00:43:20.550, Speaker B: Fully agree there. I hope more people come into the ecosystem and continue to experiment with it because I think now, I mean, maybe like, do you think ultimately we're at that point where the engineers can kind of like just tap into it, or are there still a couple technical changes that need to have happen to make it seamless?
00:43:21.570 - 00:43:54.748, Speaker A: I think it's getting close. I think it's again, from a throughput perspective with the numbers you have from previewnet, like that can satisfy the needs of, if it can satisfy the needs of ten visas, you know, and a whole bunch of other things and twitters, I think that kind of throughput is fine there. I think from a same latency perspective you're getting subsecond latency. You can get most of those use cases done there. If you need developer features, they're coming out within a month or two at most. Those things are all there. I think we're getting close to that.
00:43:54.748 - 00:44:24.706, Speaker A: We haven't talked about it yet, but we have identity connect in aptos that support social login, making the login experience much easier where you're not having to click through so many links and buttons and do scary things as a user. But we're getting really close, right? So the more and more so it looks like traditional Internet access, Internet applications, where you log into a site like I log into my bank account, I do some interactions, I log into Amazon, I make some purchases. We'll see that adoption and as well.
00:44:24.738 - 00:45:08.440, Speaker B: The developers coming with it in terms of world presence and where you think adoption is heading. I recently went to Stanford blockchain, spoke there and then got the opportunity to go to Asia for the first time. I was in Japan, got to go to Korea and their South Korea blockchain week and then unfortunately didn't make it to Singapore for token 2049, but got to visit that side of the world for the first time. Is that an area of focus for the Aptos ecosystem? Is that a region of the world that you're excited about or where more broadly have you seen adoption come from since Mainnet has launched?
00:45:09.180 - 00:46:29.050, Speaker A: So we've definitely seen worldwide adoption, which is great and definitely in places that we didn't expect, whether it's going to be like Russia, Ukraine, Korea, China, Vietnam and of course the US, and we've seen builders come up from everywhere. What's really interesting though, about Asia and Korea specifically though is that we see large enterprises really taking a hard look at different technology stacks in the web3 space and taking faster moves than we see elsewhere. So partners like Lotte Group are working with us, which is the largest retailer in all of Korea. And we'd love to see the large retailers in the US and other places start to look at blockchains more seriously. And so with that kind of different kind of culturally minded entities and they're willing to move, take, move on the faster end of the spectrum. We love that and we love the fact that they are picking the best technology stack out there in us and excited to see what happens when other entities watch what they can do and then want to replicate their advancements out there. Definitely excited to see Korea be a huge important region for us, Asia in general being important for us, but also companies like Microsoft are starting a partnership with them around building different kind of products is also important.
00:46:29.050 - 00:46:41.878, Speaker A: So we're definitely everywhere, but we definitely see the asian market moving a lot quicker to adopt web3 and blockchain in even the biggest enterprises. And that's really, really encouraging for us in abtus.
00:46:42.014 - 00:47:29.250, Speaker B: It's interesting when you look even at trading volumes, Asia is very active outside a much larger market than the us market is. And I think that trading then leads into experimentation on chain, whether that's nfts or just saying, hey, let me try to click around on different blockchain application. And I think that experimentation phase is really what is continuing to need it, not only on the application side, but users willing to jump through some of the hoops, at least today, while there's slightly higher friction, and that friction gets removed over time, being able to experiment is important not only for developers, but also the users.
00:47:30.470 - 00:48:00.350, Speaker A: Absolutely. I couldn't agree more. And just seeing that there's different parts of the world that are much more amenable to using blockchain, whether it's users or developers, and ideally both. And sometimes it's different. Sometimes you'll see some countries have a great number of developers, but not that many users. And sometimes you'll see the inverse, where it's a lot of users and not that many developers. And we definitely are looking to encourage both audiences into aptos and working very closely with teams on the ground in those areas.
00:48:00.650 - 00:48:33.030, Speaker B: Yeah, it's going to be a fascinating market to watch and definitely want to keep your eye on. I think in terms of when you look forward to the future, obviously you're working on multiple different priorities with the updated runtime or virtual machine. Some of the things with abstracting logins, making that a little bit more simplistic. But if you had to highlight the top technical priorities or things that you wish you could snap your fingers and have in the Aptos ecosystem today, what would those be?
00:48:33.500 - 00:49:01.670, Speaker A: Ooh, this is a good question. Start from the technical side. I think we definitely want to see the aggregators make their way to production. We think that that is a very interesting and unique feature that others will hopefully adopt over time. And the other thing is, we've actually put out a paper called shoal. It is an archive. It's actually also been accepted to a conference, but we'll have, you know, we'll kind of have more details on that later.
00:49:01.670 - 00:49:52.574, Speaker A: Unifying the BFT, kind of traditional BFT leader based election protocol with high throughput, DAG based protocol and maintaining both the high throughput and low latency has been a challenge for a long time. But we, we've definitely found some interesting breakthroughs here and documented a lot of those in shoal. We also have even more breakthroughs that we've not yet documented, but we'll write another paper on those in the coming, in the coming months and those are going to be really even pushing the landscape further. While 30K TPs and Max throughput and on subsegment latency is great, we definitely want to keep driving those even closer again to traditional databases out there where companies now don't have to think twice about. Is this going to support my needs for the future? Absolutely. You will never get to this use case. That's really important.
00:49:52.574 - 00:50:34.914, Speaker A: I think the other thing is just again focusing on things you can't do elsewhere. On chain narrativeness, no web two product supports that source of entropy for randomness. And this is always a concern for gamers and for even other kind of markets where that's really important to drive on. Multi party computation, privacy libraries, cryptography, all things that are really, really important for us to continue to evolve and develop over time as well. As you mentioned, the login experience, the fewer clicks. Social login is a great step forward. How do we then think about management of these social accounts? That's really, really easy for users today, in your bank of America or Wells Fargo, you have a checking account, you have a savings account.
00:50:34.914 - 00:51:03.080, Speaker A: Most people can handle moving money from one place to another. It's got to be that easy for crypto to think about. This is the thing I use on a periodic basis. This is what I'm willing to put at risk to some degree. Here's the protections I put in place for each of these different accounts. This can actually over time become much better than even what my bank of America offers because I can't put limits, I can't control the limits easily on what I want to do. I have to go with what they impose.
00:51:03.080 - 00:51:58.160, Speaker A: And so being able to do it on the blockchain, imagine where you have wallets that have customizable limits, where if I'm playing a game, any transaction under one unit of whatever the gas cost is, is going to be auto signed. But anything above that have to ask for authentication for it. And so making that a very frictionless experience and then allowing the controls to be suggested but ultimately user controlled I think is, I'm saying that something a really advanced industry going forward. So I think all these things like from a systems perspective to a user perspective, and also, as we kind of highlighted, to a language perspective and language tooling perspective. So the things like up to us, gas profiler, new vm, new compiler, new language features, as well as more parallelism being put into the system. We're really excited about what's coming up in 2024. It's going to be pretty amazing.
00:51:59.300 - 00:52:34.650, Speaker B: It's very impressive in terms of since mainnet has actually launched, have you found any particular part of the tech stack with blockchains being harder to scale than others, whether that be in data dissemination, scaling, throughput, the different workloads on the virtual machine, not being as paralyzed as you would like what, like unique insights or even like on the storage aspect as throughput continues to ramp. Any things that you've been not caught off guard but you surprised by.
00:52:36.710 - 00:53:25.630, Speaker A: I think the. Well, I'm not sure they're surprised by the right word, but definitely happy to share some insights here. So one thing that got us that massive win on the throughput side recently was storage sharding. So a lot of systems, including ourselves, store in a single rocksDB instance and then allowing us to go to multiple rocksDB instances and store that state across different storage instances is trickier to manage for sure, but it did lead to really, really great outcomes in terms of throughput. And so I think while we're the only ones who do it today, this is probably something that others will, will do as well in the future. And this is also the first step towards not just storage sharding on say a single machine, but you can imagine instances where you have now large storage devices hooked up to it. You can shard across multiple devices, you can shard across multiple machines.
00:53:25.630 - 00:54:15.704, Speaker A: This is something like a great step in that direction of where our ultimate vision is, which is a validator will become more powerful through more cores or through multiple machines, or through even disconnected pieces of hardware attached to it. Now what's important is to make sure that these machines can be run by a series. In order for decentralization to work, it still needs to be supported by different kind of cloud environments. So that nothing is like either you have to run a supercomputer or it's only one data center that can support this kind of infrastructure. That's not going to work out well for decentralization. So finding that balance of what's the right kind of hardware combination that makes sense for decentralization, that also allows us to get very, very high throughput and low latency in the system. So one thing we actually started to do is run some experiments of even sharding across multiple machines, execution based sharding.
00:54:15.704 - 00:54:52.180, Speaker A: So a validator, again taking the same number of validators in the system, but increasing the compute power of each validator by making each validator like 1234 machines as opposed to one, is one way also of scaling the system. And so we started to run some experiments there and are definitely our initial thoughts was once you go from one machine to two machines, actually slower because that network communication starts at an overhead that's much heavier. But after you hit 2345 machines, you start to see now some gains. And then I wouldn't say it's again surprising, but that's what we've seen and observed in our experiments and we'll have more of those experiments coming out in the future days.
00:54:52.560 - 00:55:58.300, Speaker B: Logan I'm excited for those experiments. I always like the tech papers as like a blockchain nerd myself, I love reading about the either benchmarks that previewnet ultimately put out, really understanding the nuances because I think whether you're an investor, a builder, anybody that is now trying to learn about the blockchain ecosystem, it is harder to do. And as much as possible, I think Aptos has done a good job at starting to do more of that because everybody's trying to find, all right, what's this chain do versus this chain? How is it different from my l two or l three? So as much as you guys can continue to write, and also, I mean one for engineers, I think everybody likes the research papers, the smaller community likes the research papers, but also the human readable kind of translation of what that actually means for everybody else, I think it's important to keep driving forward.
00:55:58.690 - 00:56:46.480, Speaker A: Yeah, we'll do our best to keep writing research papers as well as the kind of higher level papers that give people the gist of what's going on. I think we recognize those are two different audiences, and it's worth doing the dual efforts of writing for those different audiences and always happy to do that. Maybe the one thing I'll say is going back to parallelism. We talked last time a lot about block STM and getting the implicit parallelism from the programmer without having to do anything explicit there. And that's been something that I think we talked also at that time, that we'd see others hopefully doing the same thing in the space. And we're really excited to see that others are adopting the same techniques of block the STM, whether it's going to be polygon or what saes announced recently, or what Monad is doing. I think that it shows in industry that once we do these things, others will also pick up the same ideas and follow in those footsteps.
00:56:46.480 - 00:57:24.834, Speaker A: Definitely like to see that validation market. The other thing we're going to be doing in the coming months or years or whatever as well, is to explore the pessimistic parallelism. So making programs work a little bit harder to kind of specify more about what they're doing in their actions would allow for other opportunities in parallelism to be done as opposed to block STM, optimistic parallelism type techniques. And so we'll be exploring those as well. And so now from a program perspective, that kind of gives you two avenues in which to explore parallelism. One, you write your code by default, it does great, it exploits the parallelism necessary and you're very happy with it. Two, you want to go a little bit deeper and harder into things.
00:57:24.834 - 00:58:10.042, Speaker A: You can go ahead. And a good example would be like you can write your SQL query, it's going to be great. It's going to work well for Spark or for Hive. Now, underneath the covers, though, if you don't like the way that the query optimizer developed that plan, you can go and decide what you want to do. You can take the stages apart, you can rewrite them and to do a sort join merge, or you can go ahead and decide, I want to do a map only merge. In this process, the more control we give to the programmers and the covers to do things that allow them for more finer grain optimization is going to be helpful, potentially. And so that's a very interesting area of exploration for us going forward to kind of going back to a very famous quote, make easy things easy and hard things possible when it comes to parallelism.
00:58:10.042 - 00:58:13.074, Speaker A: So that's, I think, a really exciting direction for us going forward as well.
00:58:13.202 - 00:58:42.088, Speaker B: I love it. Yeah, I'm curious, like when you go down the parallelism rabbit hole, kind of stating those dependencies upfront versus allowing kind of the block STM, it's super nice from the engineering point of view to not be able to state those dependencies, but by giving the extra flexibility, you upfront know which contracts you compare versus not, at least initially in the first pass. Having that flexibility is nice.
00:58:42.264 - 00:59:15.314, Speaker A: It is, it is. And so that will kind of allow permits to kind of pick and choose their, their preference. And also, it's also not clear whether it's even better, by the way, to have programmers specify the up front, because again, I can try to query optimize my SQL query, I can go and try to redo things better than the optimizer did. But sometimes I get it wrong and sometimes I might just be overcomplicating things and not doing what the core optimizer can do for me. I think that choice is really important for making sure we get the most efficient use of the system makes sense.
00:59:15.402 - 00:59:40.790, Speaker B: I give people the option in terms of like hard questions that people have either asked you at the community level or even privately in terms of like the Aptos community or from an engineering perspective, are there any difficult questions that you get repeatedly that come up that you could answer here?
00:59:42.020 - 00:59:47.220, Speaker A: I think we covered a lot of them, to be honest, trying to think if there's any others that we did not.
00:59:47.260 - 01:00:23.390, Speaker B: One general feedback that I hear, and I think Solano went through this pretty much all new l one s outside of the ethereum ecosystem kind of go through is like being labeled alt l one or historically, because now chains can't do ICO and have to comply with regulations that they've taken vc money and now they're a vc chain of. How would you address people that think those incorrect thoughts about the aptos ecosystem and really differentiating aptos itself from other high throughput ecosystems in the market?
01:00:24.450 - 01:01:00.424, Speaker A: That's a great question. I think one of the ways that question could be answered is actually by looking at the tokenomics of the Aptos network. It has one of the lowest investor allocations, I think, of any launch for LNL1, and also, I think for the community bucket, also one of the highest allocation of tokens for the community bucket. So I think that would be one way to think about it. The other thing is, of course from the tokenomics perspective, there's a ten year lockup on most of the tokens out there. And so this is a community that's built for the long run. I think those are some of the things that could hopefully address those concerns.
01:01:00.552 - 01:01:07.380, Speaker B: Perfect. Well, maybe we can wrap it there. Avery, unless there's anything else we didn't cover.
01:01:08.240 - 01:01:12.792, Speaker A: No, I think we went through a lot. And it's always a pleasure to chat, Logan.
01:01:12.896 - 01:01:18.180, Speaker B: Likewise. We got to make this a yearly thing because I thoroughly enjoy them.
01:01:18.680 - 01:01:19.160, Speaker A: Same here.
