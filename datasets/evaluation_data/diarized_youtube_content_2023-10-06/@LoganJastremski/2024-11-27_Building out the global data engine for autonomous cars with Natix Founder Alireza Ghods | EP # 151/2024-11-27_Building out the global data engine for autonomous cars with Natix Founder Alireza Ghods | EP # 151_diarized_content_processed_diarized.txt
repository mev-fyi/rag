00:00:00.640 - 00:00:07.780, Speaker A: Thank you so much for joining me on the podcast. Really looking forward to it. Today, the co founders of Native. Are you guys each co founders actually.
00:00:08.720 - 00:00:18.060, Speaker B: So I am a co founder. Ulrich is a very high end advisor and hands on building our automotive department. Almost like a co founder.
00:00:18.560 - 00:00:48.064, Speaker A: Perfect. Well I appreciate you guys coming on. Really looking forward to this chat. We ultimately got connected where was it? In South Korea and nerded out over some food on what Natex was building and I was impressed by kind of the depth of thought. So looking forward to diving into it today. Perhaps before diving a little bit deeper into Natax, can you each just do a quick background on yourself and how you ultimately got to this point in time?
00:00:48.152 - 00:01:11.092, Speaker B: Sure. So I will start and then I will, I will pass it to ulvik. So co founder, CEO at Natix. Background I studied wireless communication. So I worked on localization algorithms, location enabled services, building maps essentially or high definition maps, local dynamic maps. During my R and D time, during my PhD time I was actually working under the vertical of autonomous driving. That's how I met Ulis.
00:01:11.092 - 00:01:48.918, Speaker B: He, he had his own company, a LIDAR manufacturer, one of the leading lidar manufacturers actually at the time. Ebo he came with this car, this is like back in 2013 he came with this car equipped with, with riders. It was a Volkswagen I think. And on our uni campus it's going fully autonomous and blew my fuses. You know at that time it was like a sci fi movie and I was like okay. This was before we actually started the R and D project together with Ulush's company and my PhD work essentially and you know, fast forward I started native 2020. We started working on computer vision AI on edge.
00:01:48.918 - 00:02:22.502, Speaker B: So on device essentially which is also privacy compliant. We sold it to cities, governments, businesses, different use cases, parking monitoring, crowd monitoring. And then two years later we pivoted into Natix network and the realization was we realized camera is a super sensor commonly available. That means it's, it's cost efficient and when you combine it with computer vision, AI and compute capacity at edge, it's, it's. Its powers are unparalleled. That's why for example, you know Logan, you're, you're ex Tesla. That's why Tesla has been betting on camera as something that is scalable at least today.
00:02:22.502 - 00:02:58.798, Speaker B: And then we have other crazy sensors like Glider. I'd be explained those, but it was, it was a super sensor and we said okay, we want to build the largest camera infrastructure in the world without owning a single camera. The same way that Uber has the, you know, largest taxi network and being familiar with Deepin model, it just was, you know, closest Euclidean distance to do this the fastest and most efficient way. So we created the helium for smart cameras and the first application of that was obviously mapping. And today actually, you know, we're, we're launching a new product which we're even going for autonomous driving. But the flagship product that people know us with is the, is the Drive. An app.
00:02:58.798 - 00:03:48.462, Speaker B: It's a mobile driver, companion app, smartphone app that you just download, put it on the dashboard of your car, you drive around. The AI is collecting what we call dynamic map information, like traffic signs, like fire hydrant traffic lights, if there's a accident and so on. This information is broadcasted outside, you earn reward. It has navigation, dash cam, so a lot of utilities for the user. And we have a new product which I would love to, you know, I'll bring it up later when the time comes. But you know, with that said, I'll pass it to Ulrich and yeah, ULIS has been joining our company for, you know, I would say more than half a year. And this new product that we actually built is, is a child of, you know, both us and Ulis and a collaboration because we didn't have to be very honest with you, the depth and knowledge that he has in the autonomous driving.
00:03:48.462 - 00:03:52.570, Speaker B: So anyway, I leave it to him to also introduce and we can go further.
00:03:53.440 - 00:04:31.989, Speaker C: Yeah, Alec. Actually I started my career in the German army, so I was an officer. If you do that in Germany, you have to study as well. So I studied mechanical engineering, but I was more on the software side. I did my PhD in cooperation with Audi and Volkswagen on collision avoidance of autonomous vehicles project. I learned, okay, lidar is important. So afterwards ibo we decided to work on LIDAR development for automotive applications.
00:04:31.989 - 00:05:36.176, Speaker C: We started that company and as part of this company I also added a Bachelor of Business Administration in Switzerland and an executive Master of Business Administration at University of Maryland in the US Studies which I took in China afterwards, also doing some business in China. Technical point of view, we developed LIDAR sensors. So if you see the Audi vehicles today in the front Grill, there are two black boxes. One is a LiDAR. With the technology which was originally developed by IPO. Later on we also developed so called solid state LIDAR devices. Additionally we also developed all the software which is related to object recognition, environmental recognition, lane markings, whatever, and that was what Ali just referred to.
00:05:36.176 - 00:06:27.012, Speaker C: We also used all this information to develop our own autonomous driving solution. But not to sell it but, but to Test our sensors in exactly the same environment as our customers would do it. Part of that it was also necessary to think about validation. Validation of the sensor system, sensor fusion of all the algorithms. We recognized quite early that validation will be very expensive in terms of time and development costs. I started ago a team which up with software solutions with a reference system LIDAR based reference system. We developed our own HD maps.
00:06:27.012 - 00:07:47.680, Speaker C: We put in these maps all the trajectories of the road users. And so we we developed algorithms to generate automatically ground truth information. Enabled us also to validate complete vehicles with a complex sensor setup in public traffic. Personally believe that today there is still too much time which is used in order to to generate ground truth data to collect all the data. This is exactly the the idea of bringing together data collection the community that Natix has developed processing this data in a way that we can generate scenarios which we could call ground truth scenarios and can be used in any simulation environment in order to have let's say data which comes from public traffic. So it's real data, not generated manually or virtually or whatever. But the testing in the future, from my point of view, we can do environment based on real data from public traffic.
00:07:47.680 - 00:08:00.320, Speaker C: That's exactly it was called the first product. Just fantastic working with Ali and team to make it come true.
00:08:01.840 - 00:09:05.600, Speaker A: Well, appreciate the background context from you both. Definitely lots to dive in throughout the podcast, but maybe passing it back to you Ali. I think the market today, especially if you're probably listening to this podcast, is fairly familiar with crypto, fairly familiar with decentralized physical infrastructure networks. Two will probably the most well known ones today are hivemapper and also helium of some of the first OG products. Really testing the bounds this space. Hivemapper also working on focusing on outsourcing mapping data, but then also taking a kind of AI slash approach as well. Maybe to start off the bat, how would you kind of view the differences between natex aims to build and also what hivemapper is aiming to build and kind of delineating yourselves in the marketplace.
00:09:06.420 - 00:09:43.320, Speaker B: Yeah, so I think originally when we started we were more focused on the dynamic layer of the information or dynamic layers of the maps. Hivemapper was mostly focusing on the static layer of the maps because originally their dashcam didn't even have on device compute. I think the new ones will have. So by the way, as the time goes on and I said this, if you look at the podcast like one or two years away, these differences in map making will fade away. But at the end of the day this was One of the biggest differentiators which we were focusing on dynamic map information, they were focusing on static map information. We had. Computers.
00:09:45.550 - 00:09:54.198, Speaker A: Are not as deep on the mapping terminology. What's the difference between static and dynamic in terms of how it relates to mapping?
00:09:54.294 - 00:10:36.746, Speaker B: Yeah, so, you know, in the, in the map making world, you have the base layer of the map which shows you where the roads are, where the buildings are, where the landmarks are, and so on. And then on top of this you have, you know, sometimes over 2000 layers of dynamic information. Traffic signs is one layer, weather is another layer, lights are another layer. You know, you have inclination, you, you know, all this is additional pieces of information, additional layers of information. And then the, the dynamicness of this information depends on the, what we call information decay rate. Actually we, we were the first people, I think, who coined, you know, dynamic maps. It was part of EU projects.
00:10:36.746 - 00:11:02.536, Speaker B: And then later on, you know, it was also HD maps that came close to that. But they're very focused on, you know, autonomous driving. But basically the more dynamic that you go, the, the, the, the, the faster the information will decay. So the highest dynamic layer is all the traffic participants. Right. Cars, bicycles, pedestrians, because they're moving every second, right. It's, it's, it's very difficult to catch that in real time.
00:11:02.536 - 00:11:34.316, Speaker B: And for example, pothole is something in between. It's there, you know, today. It's also probably there the week later. And traffic signs as well, for example, they do change. There are certain dynamics to them, like speed limit signs because they change because of traffic, sorry, construction on the road and so on. So when you focus on street level imagery, which means you collect images from the road and you have to ship these images to the cloud infrastructure and process them with AI to build those maps, it's more for static layers of the information because there's latency included. Right.
00:11:34.316 - 00:11:51.910, Speaker B: Send this information, process it and so on. And when you can do on device computes and capture, let's say traffic signs in real time and broadcast that, that, you know, literally within seconds to an update the map. This is, you know, where we call dynamic map information collection.
00:11:53.290 - 00:12:22.660, Speaker A: And so kind of pulling on that string a little bit more, you guys kind of took the separate path from dynamic to static to begin with, but I think you kind of diverged even further, particularly around the hardware. Can you talk about that? Because when, in my mind, when I think about the two differences, this to me is also a really big component between the two of you.
00:12:23.600 - 00:13:00.640, Speaker B: Yeah. So by the time that this podcast will come out, you you know, people will know about our newest product. And this is, this is basically something that has been, you know, built together together with Ulis we were always looking into. So the dream was always building a camera infrastructure. And camera it comes in different shapes and forms, right? Smartphone was just the most scalable go to market for us which numbers are approved for that yet. The quality of the camera. So camera is extremely scalable but we can go into other ways of data collection with higher fidelity, with higher data quality.
00:13:00.640 - 00:13:37.954, Speaker B: So the newest product that we have built is also for drivers because we do have experience with that users type of users, it's actually for cars, is for us to tap into 360 camera of the Tesla. So something that neither smartphone can do. So this is like a disagreement between me and you know, Ariel from hivemapper. It's like dash cam versus a smartphone. We can both agree that something that you know, none of these can do is to collect 360 data. And the rich ment of the 360 data and what you can do with it, even in mapping, it's just, it's just different. You know, you can, you can use Gaussian flat and you can build these immersive metaverse like environments.
00:13:37.954 - 00:14:28.120, Speaker B: You can do, you can do a lot of really, really high end stuff but you also don't need to collect this data very frequently. So that's why for example Google cars can actually do this or some larger mapping fleets and more advanced one can do this. But it's very expensive and you don't need it every day so to say to update. And essentially the newest product that we have is a little box. You plug it into your Tesla glove box, you open it, you plug the box in. Instead of the normal, you know, memory stick that is there, you plug our box and we access the 360 video footage of your Tesla. We collect this and we use this pose for high fidelity map making but also for something far more interesting I think than map making which is another big differentiator between Natix and high power today.
00:14:28.120 - 00:15:05.436, Speaker B: The other one is what we call scenario generation. I think Ulrich is, he talked about it very quickly about referencing and validation. But essentially if you are autonomous driving company like BMO or you are a normal car manufacturer like Honda and you want to build an autonomous driving functionality like lane change or automatic right turn. The only way that you can test this functionality before going production is either you built a car and you put it on the roads of Japan, uk, Germany, us and you really test this, which is Extremely expensive. It's not scalable. Right. The other thing is to rely on simulation environments.
00:15:05.436 - 00:15:54.020, Speaker B: So you basically try to replicate the car and the traffic behavior inside these like, you know, game environment, game engines, you know, simulation environments essentially and see how that functionality, how that car with its sensor basically behave with all kinds of, you know, they call it near misses stupid behavior of traffic participants. Right. And today this process is very expensive and it's not, it's not very efficient. That's why, you know, I would say it's not as easy to release safe autonomous driving functionality. So we have us on one hand where they just release it and they care about the legal consequences later. And then we have Europe on the other side of the spectrum where they're extremely careful with releasing something like that onto the road. But either way, you know, you need this information.
00:15:54.020 - 00:16:40.628, Speaker B: And what we do with the 360 imagery that we collect from the data from the, from the cars is we essentially detect objects and their trajectories and we put them into the simulation environment. So now we have these, let's say, replicas of the world with all the traffic participants in it and how they behave. And then we classify them into scenarios, which means, you know, we break them down into right turn, left turn, lane change, you know, and OEMs and autonomous driving companies like let's say Waymo, they can just go and they, they're looking for, let's say, scenarios in New York. Right. Turn scenarios in New York. They search and then they're gonna find, you know, 50 different ones with the metadata description of this. They can download it in their own format, put it into their own simulation environment and test it out.
00:16:40.628 - 00:16:48.824, Speaker B: Right. Very, very efficient. So this is kind of the autonomous driving which goes far beyond mapping with this 360 video data that you're going.
00:16:48.912 - 00:17:44.570, Speaker A: Maybe to just reiterate it back to you and the audience just so that they're following along. You would highlight the main differences between you and hivemapper really being around the hardware devices. You guys originally focusing on cameras, they had a kind of dedicated device that you had to buy and then purchased and put in the car. Really natex focusing on the dynamic kind of mapping data. But what you have realized with this new device is AI really needs a lot of data. And if you can tap into that 360 degree camera, especially that Tesla has kind of pioneered some of their camera suites and their cars that can be really useful to train large scale neural networks and ultimately further us along in terms of generalized full autonomy.
00:17:45.630 - 00:18:41.140, Speaker B: Correct, correct. And just to be clear, our strategy is still, you know, borderless capital coin this term commodity hardware. So if you think about it, you know we are still tapping into an existing device by the way, we looked even into building a360 camera and talk with some partners and the retail price of that 360 device, let's say so four cameras around would have been bill of material, would have been you know, two 3K dollars. And then imagine like we also have cost and distribution and so on. So it wouldn't have been very, very much scalable. And now with you know, few hundred dollars, we're tapping into something that, that worth even from hardware point of view, a few thousand dollars. And it's able to generate data that no smartphone or no dash cam can generate which is 360, not even pictures video data which can be used for, like you said, you know, for, for, for building better ADAs.
00:18:41.220 - 00:19:35.376, Speaker A: Stack and Ulrich, I'm curious what got you kind of excited about this vision. I, I think today in the autonomous landscapes there's two really kind of schools of thought, so to speak. One around vision only approach that kind of Tesla is taking, doing full end to end neural and the other I would kind of put on the other side of the spectrum is add lidar, kind of do the Waymo approach where you can get things down to the centimeter or even higher degrees of accuracy, geofence certain areas and then let the cars go. What kind of about this Natax vision ultimately got you excited to kind of help the team along and felt like this was kind of the right approach. Push back on any of that if you disagree with it.
00:19:35.448 - 00:21:41.430, Speaker C: First of all from an autonomously driving point of view, if you take the Tesla approach or if you take an approach with all the three different technologies, at the end of the day you somehow need to test and to validate that the system does in public traffic what you expect. Personally I'm convinced that three technologies do have advantages. So I, I had my own company with making LIDAR technology so I'm definitely a LIDAR fan, no doubt about it. When it comes to validation it is a different topic because validation requires ground truth data, understand the truth first and then any type of sensor fusion system and you take the ground truth information and by comparing the two results comes from the original setup which is in mass production or will be in mass production of the vehicle, including all the sensor fusion and the other one is the ground cruise information which has to be more accurate, more precise to test the results, to validate the results and to figure out, okay, what are the limits of fusion system which will be in production or is in production. So and in order to do that you at let's say expensive measurement systems, usually LIDAR based gps, imus and all the additional stuff to a few test vehicles. And then you operate these test vehicles, you collect data and then when you have done all this, you don't build up 100 vehicles because it's much too expensive. So you build up as an OEM just a few vehicles and then it takes longer, it takes some months to collect the data.
00:21:41.430 - 00:22:54.700, Speaker C: First point, so data collection time, which is crazy in development, slows down development significantly. Second point is the original approach and this is the common approach today. You take this data and you have a lot of people which label this data. So they go frame by frame for any video image and they label the objects, they label the lane markings and so on. And that again takes a lot of time, it is expensive. And the result is you have ground truth data, which means after starting the first test ride, it takes three to six months or even more time the data that you need to understand or to evaluate your own results of your own produce sensor fusion system, figure out, okay, where are the limits and performance? From my point of view is get rid of that, be faster, we have to reduce the costs. That was the basic idea.
00:22:54.700 - 00:23:40.960, Speaker C: Can we make use of community of Natix data collection? And from my point of view, yes. So that's the first step. And then if you do this, it comes to the second question. Okay, is it possible to take out from doing the ground truth data generation? Yes, absolutely. Because we have done this in the past. There are some other companies in the market which can do this. It's the right time to come up with a new approach, an AI based approach to provide this information.
00:23:40.960 - 00:24:54.978, Speaker C: And if you put this together, start at any time with a vehicle that you want to develop, that, that you want to sell, that you want to equip with a certain ADAS application or level three, level four application. And on the other hand, at the end of the day you can also think about validating vehicle in a virtual environment. Do it today and as we have done it in the past, based on real data from public traffic, not collected anymore in the future by measurement vehicles which are expensive, time consuming and so on done scenarios which are generated in public traffic. And then you have the real environment your vehicle in and do the same tests as before in public traffic. But it's, it's available all the results. You don't need to wait for the results. You know and understand all the environment perfectly.
00:24:54.978 - 00:26:04.632, Speaker C: From the simulated is a real scenario. It is not generated be manually whatever it's taken from public traffic transferred to the virtual environment and then you can do everything with it. You can add rain and so on. You have all the fantastic advantages of a simulation environment available. And so for one single scenario where you collected the data and public traffic, you can generate hundreds or thousands. In order to check your software on all different levels level data level of of interest you can check can I see the objects then can I track the objects, can I classify the objects? Can I correctly fuse the objects between the different sensors make use of this in order to have consistent description about the up to the driving strategies. You can test everything.
00:26:04.632 - 00:26:28.120, Speaker C: And that's fascinating far in order to coming to that point it requires a lot of work of time in development. That's the point when so okay together two worlds are so far not connected. And all the advantages.
00:26:29.740 - 00:27:43.984, Speaker A: It's, it's definitely interesting hearing kind of the different approaches. But to your point, I think regardless of either path that you kind of choose, you still have to determine the the ground truth. One thing that Elon used to say quite a bit when I was at Tesla is close the loop and then iterate quicker on that feedback loop and just make it turn faster. So I'm, I'm curious this kind of overall structure and continuing to iterate on the product side, starting and continuing to kind of push forward the dynamic mapping but also adding and kind of expanding the products suite to say hey, there is this massive market out there around autonomous cars and really what they're limited today is by data and if we can go out and capture some of that real world data, we can get to the ground truth much quicker. How does I guess natex plan to scale this new product suite to enough Teslas where you can collect enough data to actually go out into the market and say hey, we have some really awesome data collections from these different Teslas but now we want to go sell it to the real world.
00:27:44.152 - 00:28:13.426, Speaker B: So this is a go to market kind of thing. And we looked into it actually extensively from from distribution point of view. Something that helps I think is Tesla is one of the few non crypto products that has very strong community because you know, community is everything in crypto, but not necessarily, you know, in non crypto. Like HubSpot doesn't have a community. Right. But they have a good product. There is no like community of people like nerding out Tesla is you know is like that.
00:28:13.426 - 00:29:13.550, Speaker B: So this makes our life a little bit easier to tap into, to tap into these like audiences. But one thing that I have always also said Logan, is you know, utility, utility, utility, aside from Token, which obviously it's, it's a, it's a catalyst is not, it's not the reason that people should buy your product. At least not in my, you know, school of, school of thought. Utility is the most important thing and for us the utility Here is the 360 video that we collect. You can actually store it on your, on your, on, on your first basically personal cloud space and you can access it anywhere, anytime, which is not a functionality that Tesla is providing at the moment. So yes you can record your, your footages if you own a Tesla upon a certain incident happening. But let's say you're a fleet manager and you want to entire the entire, you want to record the entire trip of your entire Tesla fleet in case something can happen at least for a period of like one week or so.
00:29:13.550 - 00:30:00.764, Speaker B: So you know, we are introducing this let's say cloud dash cam functionality with fleet management which is going to be I think one of the big utilities for the driver. Aside from that we have same components from our drive and app like Marketplace which has both. Let's say it also brings non crypto rewards into it. We're thinking of putting something similar to what we did for example for navigation. But let's say in the along along the lines of like EV charging and things like that. So there's a lot of these other utilities that you can, you can have for the Tesla drivers to actually buy your product and use it. Some of them are related to their own data which creates a flywheel effect, you know, in terms of drive and the navigation uses the data that they collect to get better over time.
00:30:00.764 - 00:30:18.180, Speaker B: So that is better one better utility in my opinion. Some of them are not like the simple dashcam functionality is just a simple utility that the user gets. So this is how we are looking at basically distributing this hardware and having a selling point for the end user.
00:30:18.920 - 00:31:24.610, Speaker A: Yeah, I'm very fascinated by this because I do think this is a huge untapped market more broadly I've always thought this data collection on Tesla it seems on the surface at least to that enjoys the kind of technology of autonomous cars and getting into the nitty gritties from afar that Tesla should win the generalized autonomy. But that feedback loop that they've created is very valuable and that data mode is even much higher than most would kind of care to think and so it, to me it's really fascinating being able to tap into all the video and the data that the Tesla is producing to say, hey, this kind of generalized solution that Tesla is taking, we can take that data, iterate on that data, establish some ground truths to help other car manufacturers potentially speed up their autonomous driving lifecycle much quicker.
00:31:25.270 - 00:31:58.228, Speaker B: Yeah, and this is a drastic change. Logan, you know, when we talk with like simulation companies right now, again, let me emphasize that if it wasn't for Uldish, we would, we would have not been able to build this. That's why he's not an advisor. He's like hands on working with us on the project and what we're building. But you know, creating, let's say, those ground truths as those scenarios takes experts right now these days to do it. And depending on the complexity of the scenario that they want to test the vehicle that's functionality, it's from few hours to few days of work, right. And this is like one right turn scenario, right.
00:31:58.228 - 00:32:59.300, Speaker B: Like we were talking like one scenario and they want to test it in, you know, a couple of hundred actually to really make sure that this is efficient. So you can imagine like how much this way of automating these ground truth or scenario generations would save how much time and how much money it would save the, you know, the autonomous driving companies or, or automotive, let's say industry in general. And then on the side of the dream, by the way, there are still like Nvidia is working on this, a lot of other companies are working on synthetic scenario generation. Right. Or ground truth generation in that sense, or scenario is a better word in this case where they use generative AI to create these, you know, traffic models and how the, basically the different traffic participants are behaving and then putting the car and the sensor in and testing all the, all the functionalities essentially. Even if that I think we're, we're, we're, we're a little like still at the R and D stage. But even if that happens, like we're talking with these guys and data is still the input to this.
00:32:59.300 - 00:34:05.420, Speaker B: So the more data that you have from the real world and how the traffic participants are behaving, the better your generative AI can create these synthetic scenarios. And what is interesting here, that's why it's not like things that human or AI can generate based on logic because we have a lot of what we call near misses. Let's say a pedestrian not walking properly on the zebra line, right. They just take a turn or he's drunk or he does something stupid or a bicycle just doesn't behave the way that they should. And these are let's say the more attractive scenarios that Natix can capture from real world data or Natix can feed into these synthetic or gen AI scenario kind of technologies to generate, you know, a much closer and wider range of you know, synthetics. And, and that's why I believe, you know it's, it's just game changing at the end of the day. And Tesla is not the only car by the way that we are going to tap into like at the R and D stage for a couple of other car manufacturer, quite big ones that we know that it's very easily integratable.
00:34:05.420 - 00:34:18.078, Speaker B: Tesla was the, was the market entry point for us or is the market entry point for us. But the same box can eventually be just by, by you know, slight integration it can be connected with other car manufacturers too.
00:34:18.214 - 00:34:23.130, Speaker A: Do any other cars have 360 cameras integrated to them?
00:34:24.070 - 00:35:16.462, Speaker B: Yeah, yeah. I mean today BMWs, Mercedes Benzes, you know a lot of cars actually have 360 camera and you know some of them do have the optionality of us tapping into that. Some of them is more closed source so it's a longer way and you know, for crowdsourcing obviously we're going to start with let's say the ones that are open to tap into, they have a port that we can tap into and yeah, eventually I think there is a world that once we grow and because automotive is the end user, they need it. Let's say, you know, giving an example, Honda needs this to create their ad functionalities and to test it basically inside simulation environment. So you know, it's at their win. If we can also integrate it, if you can also integrate and tap into their you know, data collection. But I think, to be very honest with you, I think that's like a five to seven year projection.
00:35:16.462 - 00:35:24.702, Speaker B: We're first going to start with things that are let's say open already, ports that are open already and we can just you know, build a plug and play hardware.
00:35:24.766 - 00:35:49.612, Speaker A: Rich, I'm, I'm curious since you've been in the field so long in autonomous driving and kind of seen the different progressions, what do you feel are the large bottlenecks kind of today to get us to generalize self driving? Is it more data just establishing these ground truths? Do we need better lidar such as solid state? What's the missing bottleneck?
00:35:49.676 - 00:37:26.998, Speaker C: I think two aspects of software development on all different levels in terms of fusion but ending up in driving strategies and at state of the art vehicles still a combination of different types of sensors. They as an engineer, depending on some parameters like weather conditions, lighting conditions. In this situation you prefer this sensor, in another situation you prefer another sensor. What we need to have any three different types of technologies which convert to the same moments with respect to field of view, with respect to range, with respect to resolution, but with the specific advantages or disadvantages of each technology. If they are all, let's say somehow comparable with the basic parameters, much more easily fuse the data just based on some much easier mathematical models. And this will enable us also from a safety point of view, a better understanding about the environment. That is from my point of view essential.
00:37:26.998 - 00:38:56.274, Speaker C: But I wouldn't say we need to have better lidars or better radars or better video. I think we need to have let's say solution and range into the scenarios that we want to target. And if we talk about autonomous driving, which is the top on our desk in order to solve it. But if we take specific scenarios as mentioned, for example, a right turn range is not most important, but you need to understand how it does the road. You need to understand that a bicycle for example, the red light and comes from the right side and you might probably, it might probably crash into your car. So these require technologies are related to that scenario of range and resolution. If we take companies like Waymo, we mentioned them before, while they go for the challenge and really a big task to solve all the different scenarios with all the patience that you can expect in public traffic, from that point of view, that is definitely the biggest task.
00:38:56.274 - 00:39:28.610, Speaker C: And you can see the sensors that they put on these vehicles today it's more important to get the right data instead of, let's say making sure that every sensor is invisible is part of the overall design. Still, let's say in development, even though these vehicles operate in public traffic, from my point of view also feel safe today in these vehicles because there is a lot of driving experience already available in these companies.
00:39:29.800 - 00:40:03.800, Speaker A: It's interesting kind of you mentioning the Fusion stack because and kind of bias a little bit towards Tesla here just because that's where I spent some of my time. Andre Andrepathy had a good talk about kind of software stack 1.0 versus software stack 2.0 and 2.0 stack was a neural network that was kind of being into the software 1.0 stack and the past. They used to have, I don't know how many cameras they have on their cars now.
00:40:03.800 - 00:41:11.220, Speaker A: I think like seven or eight. Each of those cameras ultimately had their own kind of unique view or the car had seven different inputs and those were treated independently. And the goal over time was to make a neural network that could digest all those different inputs and instead of viewing each as one individual, kind of make all those individual inputs one single input and kind of taking that collapsing the seven different or eight different camera inputs, radar inputs, lidar inputs into one unified fusion stack. And it's just recently that they kind of push both city driving and highway driving to a single neural network stack. So it's a very exciting time in kind of the autonomous car driving space. I think most people, if they got in behind a Tesla or even in a Waymo, they'd be pleasantly surprised by the experience. And it's quickly sneaking up on I would say the general population.
00:41:12.520 - 00:41:46.140, Speaker B: Yeah, I think Ulgis also told me something on our team offside in Lisbon. We were chatting, a recent Tesla accident happened. That's why I think sensor fusion is even more important. Where the sunlight was coming in and it basically blocked the entire camera because the camera and the, the neural system thought that it's just a blockage or a concrete or something in front of the car. It was a heartbreak and everyone died. Polish was telling me like this, this can never happen with the or this can barely happen with a lighter. And that's the point that I think every sensor has its own advantages and disadvantages.
00:41:46.140 - 00:42:21.766, Speaker B: Lidar is still too expensive, although I think Lou Ulrich is advising another company which they're, they're building a super scalable 3D state lighter. And at the end of the day, you know, if you want to build something that can provide High quality service 24,7 across all conditions, across all environments. The key is sensor fusion, you know, and fusing all these different sensors also across the sensor types. Right. Not just through the camera as you said, but lidar and so on. Just lidar is a little bit expensive or it was expensive and Tesla wanted to go live five years ago. So I think that's why they relied mostly on camera.
00:42:21.766 - 00:42:33.770, Speaker B: But if lidar gets as cheap as camera, I don't see a reason and ulush correct me, I don't see a reason for them not to even, even Tesla not to equip, you know, Teslas with, with a few lighters.
00:42:34.270 - 00:43:57.420, Speaker A: I know they definitely with a hundred percent do lidar testing their vehicles to kind of establish the ground truth, infuse that with their neural networks during kind of the training of the neural network models least again going back to Andres of his podcast has said useful for training but not necessary in production vehicles, at least 100% of them. And so again we'll see kind of how that nets out. I don't know. Again, kind of biased towards the vision approach, but looks like at least Waymo has a very good product at the localized level. I'm curious though, on wrapping it back to natex and how this is ultimately going to affect what you guys do and the crypto space and the product itself. And you guys kind of started as this dynamic mapping solution, now you're expanding this to new hardware device that can tap into 360 cameras or just cameras in the vehicle. How would you communicate the product that Natax is trying to build to the broader audience for people that wanted the one minute pitch, the 30 second elevator pitch to try to understand what you guys are doing here.
00:43:57.540 - 00:44:28.184, Speaker B: Somebody with community member just pitched me today something. I mean, I think it still needs massaging, but it was like your Tesla, your cameras, your crypto or your data, your money. Right. And it can go along those lines. But as I said, you know, I think I want to, I have always put an emphasis on the utility for the user and crypto is not utility in my opinion, it's just, it's just a side benefit. So the way that we are, we're kind of pushing it into the market. One of them is the utility that the app directly provides.
00:44:28.184 - 00:45:16.840, Speaker B: And I think the fact that they are contributing in something grander, which is bigger maps or sorry, better maps, higher quality maps as well as a much faster autonomous driving, you know, development essentially as you said, you know, Elon was always saying he closed the loop and then they can just go back. This is exactly what this product is able to do or at least contribute to. And I think a lot of people, especially a lot of Tesla drivers, you know, they drive Tesla because they're just geeks and nerds like me and, and you know, we just, we just love doing that kind of stuff. And it's the same kind of community that, that invests in stuff in Kickstarter. Right. They're also geeks and nerds, but, but a bit, you know, it's a bit of a different game. So it's not really like pre launch kind of product.
00:45:16.840 - 00:46:09.166, Speaker B: When you buy a Tesla, it's actually working. But a lot of them actually want to see that autonomy coming in a bigger scale. So I think this is also a big message that we want to get across. And yeah, that's, that's kind of, you know, when, when we, when we started chatting with Ulis as Well, there was this world of crypto and this world of heat, non crypto. And what we were, what Ullish, I think was, was kind of hooked with was like what he basically realized was crazy was how crypto can, can build the networks and coordinate them super, super efficiently. And he saw basically this, this data gap and what a crypto incentivized data curation network can provide, which has been something that he has been trying to do for 10 years, you know, with, with, with different sensors, with gliders. And I think, yeah, it's, it's just going to drastically increase the speed that autonomous vehicles are going to be built, developed, test and released into production.
00:46:09.358 - 00:47:05.700, Speaker A: I'm excited for this world. I think AI and crypto are probably the most interesting things. Reason why I appreciate, I'd say crypto a little bit more is just because you don't have to spend maybe a hundred billion dollars or a hundred million dollars on compute clusters to go out and do large scale neural network trading. But it's really cool what you can do when you incentivize people with crypto and having these global coordination rails to contribute data. And to your point, Tesla has a lot of very excited fans that are willing to help experiment and definitely willing to try this out. Is there any details that you can share around specifics on price and this will actually be live if people want to actually buy this for the Tesla and start contributing to the Natak's network and ultimately earning rewards.
00:47:07.000 - 00:47:48.210, Speaker B: Yeah. So I think things can change right from the time that we are recording this podcast until, you know, the, until the announcement as well as the delivery. But the current status is we want to make it accessible. So it's going to, and it's going to go in phases, essentially the hardware. So the first phase is going to start a little bit cheaper because we want to, you know, build the network faster. It's going to be, if I'm not wrong, you know, below $300, the basic hardware, which has very large storage and some compute capabilities and then it's going to go higher from there on in multiple phases. And the delivery of the hardware is expected to be end of March, early April.
00:47:48.210 - 00:48:19.658, Speaker B: And actually the hardware is ready so we don't have production issues at all. Hardware is, is, is rather ready much sooner. It's just building the software, we take a little bit of more time and, and you know, we are still a smart small team. So you know, we're not, we're not, you cannot compare us with companies like Waymo that they can build something super, super. I mean, they have the resources that we don't have. So a software component is something that is taking us a little bit time and there is a lot of optimizations that we need to do. Right.
00:48:19.658 - 00:48:53.650, Speaker B: So, you know, collecting this video data has different requirements. It's big in size. So we're doing a lot of like optimization in terms of what we collect, how we collect, how we're transmitting. And this is based on the requirements that you're getting. So we do a lot of requirement engineering, but we get a lot of requirements both from mapping industry as well as the autonomous driving industry. So the good news is, you know, I think almost from day one, we're going to have a couple of very large map makers, you know, on board with us. So this is going to be something that, that, you know, we will announce as well soon.
00:48:53.650 - 00:49:47.810, Speaker B: But it's, you know, in quite good talks, I would say again, because of the type of data that is being collected. And it's the same thing. We were chatting with a lot of these, you know, simulation companies or even autonomous driving companies because they need this data to be able to collect and build from day one something that they need. So we're very fortunate that we have Ulish with, you know, 95% of the requirements because he has tried to build this, by the way, at the time of video, people don't know, but Ulrich had a mapping department, he had an autonomous driving department. So he knows all of these worlds very well because he was building a product for each one of them and then selling it to automotive referencing department, you know, so all of this was even for scenario, basically simulation, sorry, not scenario, simulation environments. He had a, he had a department and a company that was doing that. So we get a lot of requirements already, but, but a lot of it is also, you know, the last 5% fine tuning.
00:49:47.810 - 00:49:49.310, Speaker B: It takes a little bit of.
00:49:51.290 - 00:50:36.030, Speaker A: Oh, it's, it's an exciting field and it looks like an exciting partnership between you guys as well and making this happen. I think crypto incentives combined with doing things in the real world is in these more broadly decentralized physical infrastructure networks are really set the starting line in terms of what they're going to do and how people are incentivized to do things in the real world. I think make really cool products. And so I'm excited about what you guys are doing here for as we're kind of, I guess, wrapping up the podcast. Is there any specifics that we haven't touched on that you want to or kind of dive deeper into any more niche topics as well.
00:50:40.890 - 00:51:21.008, Speaker B: I don't know Logan, what you think but in general I think with Deepens there are different value accrual mechanisms and I think you know, that is also something interesting to talk in our approach as well as the tokenomics design. It's just something I would love to get your. Because I think we did talk about it actually in the, in Korea when we were having, we were having dinner. But you know, that is something just generally I would like to both get your input on it as well as see like because we have a very unique approach and I don't think any other project really did the exact same as us. Right. There was this very generic model that was there which was non deepen and then there was the helium model with its own. But it was the first model that went out.
00:51:21.008 - 00:51:41.712, Speaker B: Right. So obviously it had, was but, but, but still it was revolutionary in terms of the business model. And then you know, we tried to build something that is different than any of these. And yeah, just, just I think that is something maybe we can talk about just getting your input as well on how you see that the tokenomics of design these days.
00:51:41.896 - 00:51:49.500, Speaker A: Anything that you can share on the podcast today or, or things that you can share down the line.
00:51:50.620 - 00:52:27.952, Speaker B: Yeah, so I think in general we have, you know, the value accrual mechanism for us is. And a couple of other projects also do this is buyback and burn and also going towards a staker from the protocol revenue. That's the majority of it with that order. And I think this is quite important in Deepin models. Like I have seen also some models that they don't have as clear or as good of a value accrual. And of course a lot of projects can also serve on you know, speculation and marketing and you know, having the, you know, the top two VCs in terms of AUM on your cap table. And I do see that right.
00:52:27.952 - 00:53:23.620, Speaker B: There's nothing, you know that's, that's also a way to go. But if you're looking at a, at a, at a long term essentially plan five to 10 years and I genuinely believe like, like, like every other industry, a lot of Deepens might fade away. But a lot of them that have good economic models, good unit economics, they are going to survive and they're going to scale big. But if they don't have right value accrual mechanisms aside from speculation, we're not going to see much. And I'm not pointing any fingers but I'm just saying like for us this was One of the most important, yes. As for news like we're going to do our actually second burn because we're live with the token, not for a long time, since July this year. So we did the first burn close our customer, we're doing the second burn and you know, the numbers are going, you know, up quite exponentially and I think this is quite important for, for, for any, any deepen projects that is trying to build something sustainable for the long run.
00:53:24.960 - 00:54:51.910, Speaker A: Helium was definitely a pioneer and not that they got it wrecked by any means, but they enabled to see what the world was possible when you incentivized people with tokens. And now I think it's the broader job of the crypto community to figure out how do we continue to iterate on that, whether it's the buyback and burn, how that US is also having some politics and being a little bit more pro crypto at least on the surface thus far, potentially revenue share, there's a bunch of different revenues that ultimately open up if the US kind of leans into crypto as the market expects them to. And so yeah, definitely happy to on different feedbacks on the token. But I think you guys are very directionally in how you guys are thinking about this. And so it's exciting to see your guys progress, exciting that tapping into cars data, being able to learn from that, continue to train neural networks to hopefully make the world a safer place, get everybody from point A to point B with less accidents, but in the meantime also building the best mapping solution that you can. And people can still do that for my understanding, with their camera alone. And that's kind of part of Natek's unique value prop, right?
00:54:52.610 - 00:55:37.594, Speaker B: Correct, correct. And you know a smartphone is not going to go anywhere because a lot of people ask us now that you're launching the Tesla hardware, would smartphone get less priority or go anywhere? By the way, if you look at very large crowdsourcing map comp, the gurus of crowdsource happening, you can see that they have the entire stack from smartphone to dash Cam to 360 cameras. You know, they have the entire stack and each one of them has its own advantages and disadvantages. And I think there is a little bit of similarity between autonomous driving and this as well. The core is to do fusion because each one of them can do good in certain places and can not perform very well and in certain other aspects. So we're still going to keep the smartphone. We signed our first customer for the smartphone data.
00:55:37.594 - 00:56:36.196, Speaker B: There's no reason for us to kind of believe that Yeah, I think in general. And you started actually your podcast with asking me like, what is your difference with it Mapper? And I think with this new product and the new direction that we're going, which is, you know, basically scenario generation for autonomous driving, this would be one of the biggest differences. We are, to the best of my knowledge, we are one of the first crypto companies or deep projects that is actually doing something around, you know, autonomous driving, something directly helping autonomous driving, you know, to, to develop faster, to develop more efficiently. And if you ask me, I would say this is, you know, mapping. People might, might hate me for saying this, but I think this is a bigger problem and, and both inside and in challenge. And that's actually where I see Deepin generating a multifold of value compared to mapping. Because, you know, crowdsource mapping also is nothing new, right? Us and hivemapper, we're, we're, we're nothing new.
00:56:36.196 - 00:56:54.484, Speaker B: If you want to really look into the best crowdsource mapping project, I would say in the world, you have to check. Grab the Uber of Asia. These guys have the entire stack. They invested a lot of money. They're one of the best, you know, technologies that are doing that. And obviously crypto would, would just make it faster. But in terms of technology, I would tell you they're, they're one of the best crowds for snapping.
00:56:54.484 - 00:57:38.806, Speaker B: But this autonomous driving problem is something very unique. I think it's a new, new baby in deep end, in my opinion. And we might have a couple of other competitors as well, but this is definitely something that you cannot do with smartphone or touchscreen. You just absolutely cannot do it. So, yes, can we have competitors in this sense? Absolutely. But I think our cutting edge here is, I think if they have somebody like Ulvis as well in the game that is as advanced in knowing this world, because believe me, Logan, we were even talking with one of the DS companies in Adas very large company, and they're actually owned by one of the largest chip manufacturers. And these guys also do autonomous driving to, to, to the highest grade.
00:57:38.806 - 00:58:08.430, Speaker B: I would say they're very well known in the world. And when we told them about how we're doing, let's say scenario generation and scenario classification, they, they basically were asking like, how exactly do you do it? And OLE was explaining to them like, this is, this is our approach without saying too much. And you could, you could see that they're basically completely clueless. I mean, what was interesting is that Ulwich actually sponsored like four PhD students. This is like five, six years ago. Just to do the scenario classification side of it, right. So we have the cookbook to do these things.
00:58:08.430 - 00:58:39.582, Speaker B: And that's the defensibility that we have essentially for this product and for this direction, which is, which is very exciting. So, one, I think nobody has had done this yet. And two, I think we have, we have a team that, you know, it's defensible. It's not that easy to just say, okay, I'm just going to, you know, build the same. Because hardware is not that difficult, I can tell you this. Hardware is not, is not the complex part of the story, nor the distribution. What you do with the data is the real core part of the story and the knowledge that you need to actually build those scenarios.
00:58:39.646 - 00:58:59.570, Speaker A: Perfect. Well, perhaps we'll end it there. Gentlemen, I really appreciate each of your time and coming on the podcast, diving into what you guys are doing at Natix and also kind of the cutting edge of autonomous driving. So really appreciate the time today and I'm sure the listeners will thoroughly enjoy the podcast.
