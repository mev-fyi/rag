00:00:00.560 - 00:00:52.850, Speaker A: I am super, super excited to do this podcast, man. Thank you, Anatoly. I think I've learned so much kind of through my blockchain journey from just following you on Twitter and going through the Twitter threads. And I think you approach the space much differently than everybody else. And I think at a time where everybody was kind of going one direction, you went another direction. And I think I would like to just maybe like, start off like, about like, kind of your insights and going this other direction that was like, super contrary to, like, the entire space and why you went this direction and how ultimately you had the conviction and perseverance to continue to go this direction after. I mean, the industry just kind of went the other way.
00:00:53.170 - 00:01:38.634, Speaker B: Yeah, I think it kind of starts probably with my career at Qualcomm. I was there, you know, 2003 ish, till I forget, 2015 or something like that. But that period of time was when we started with these two megabyte arm devices that have flip phones, small screens, two inch screens. And then my last project, I was in the advanced technologies group. Like, one, we were trying to get like, project tango off the ground, which is an augmented reality device on like eight core, four gigs of ram, kind of beast, 64 bit beast. So I saw the. How fast hardware can move.
00:01:38.634 - 00:02:19.254, Speaker B: And being an engineer there, I just kind of was ingrained with me. Okay, next year, this chip is going to be like 50% faster. It's just a given. And it's going to have more cores, more threads, more caches. So the way you're designing software is like, how do I make sure that it's designed to take advantage of that? But it doesn't have these, like, single threaded, single state thing assumptions. And when you've spent a decade doing that, like, as soon as you build a, you find a solution that's solvable with increasing number of cores, you're done. You're like, okay, we're done with this problem and move on to the next one.
00:02:19.254 - 00:02:27.126, Speaker B: Yeah, because if it doesn't work enough, if it's not fast enough today, I'll be fast enough. Two years from now, I'm like, it's the hardware guys problem.
00:02:27.278 - 00:03:24.370, Speaker A: Yeah, no, I think the, I love the design, and I think one of, like, just my pastime at Tesla, like, I think a lot of these blockchains are trying to fight physics in some sense. And I appreciate that you guys kind of unilaterally just kind of lean into the hardware aspect and embrace that and are building the software to ultimately take advantage of that increasing core performance, maybe on kind of just touching on the hardware stuff for a second. One of the biggest misconceptions that I hear quite often is that Moore's law is kind of dying. That the amount of compute on like a. Yeah, maybe just talk about Moore's law and then separate that from like the multicore performance.
00:03:24.830 - 00:04:27.642, Speaker B: Yeah, Moore's law actually go this law, forget the original name, but it goes back to airplanes, that people are seeing efficiency improvements in airplanes because they weren't looking at any single part, not just the engine, but all of the designs were getting slightly better and better. And you kind of saw this constant exponential improvement over period of time. So when you talk about Moore's law, he was talking about number of transistors per dollar built, basically. And that's a very, very kind of high level thing. What does that mean? Does it mean that they all have to be in the same chip or in the same computer, right in the same box, in the same rack, in a data center. So it's a very large number, like kind of large spaced optimization space. But we do see that there's a lot of overheads, like even starting from the number of pins on a package, that keep improving and keep getting faster and bigger.
00:04:27.642 - 00:05:02.680, Speaker B: And these bandwidth increases may not be frequency increases like people got used to in the nineties, where you saw gigahertz jump all the time. But these bandwidth increases in terms of IO throughput, memory throughput, cores that can all be in the same die continuously, just keep making these systems faster and faster. And you see this with the new max, you just notice instantly this machine is faster than the previous one. They charge you a little bit more, but they don't charge you twice as much for them.
00:05:03.810 - 00:05:52.790, Speaker A: No, I think it's a very kind of elegant solution. I think just to add more cores and continuing to scale that way as the amount of transistors on a single chips gets harder to condense. But I think it's a big misconception more broadly, maybe, to kind of step back a little bit. I think also you have a very unique approach to how you think about blockchains more holistically. And I think it would be awesome to maybe start where you see the bottlenecks of blockchains more holistically and how you and Solana are ultimately trying to address some of those higher level bottlenecks.
00:05:52.910 - 00:06:27.986, Speaker B: Yeah, for sure. So even when we started, we had this kind of different idea of what a blockchain is for. We had the silly tagline blockchain and Nasdaq speed, and we weren't focused on settlement. And this is where I think in those early days, people were asked me these questions. I didn't even understand. What do I care about settlement? What we care about is information and execution. Effectively, you can think of Solana as in its ultimate state.
00:06:27.986 - 00:07:41.464, Speaker B: Imagine there's a world where anybody can throw in a computer connected to the Internet, high speed Internet, and they have information that's synchronized at the speed of light around the world for the exact same state as everyone else. And that state we thought was probably the most important use case for that state is to represent something like an order book, like serum, or bids and asks or even offers and nfts, but effectively, price discovery. And what's cool about this idea that this thing is synchronizing at the speed of light around the world is that when a newsworthy event happens in Singapore or whatever, that news still has to travel through speed of light, through fiber, to a Bloomberg terminal in New York before trader looks at it. Solana exists at that ultimate latency. Then that state transition is propagating through Solana. At the same time, that person in Singapore will make the trade right there. So by the time the trader in New York sees that news flash, the price is already reflected on Solana or the New York Stock Exchange.
00:07:41.464 - 00:08:08.404, Speaker B: There's no arbitrage between the two. And if there's no arbitrage, it means it's as fast and as good at price discovery as these very expensive monopolies that exist and charge a lot of money. So this is kind of like what we thought was the most important use case to build for. So given that requirement, you kind of can't build sharding, because as soon as you build sharding, you've actually separate the state and you've created arbitrage. So that was, like, out of the question.
00:08:08.532 - 00:08:43.510, Speaker A: Yeah. And there's a lot of latency, like, in the cross communications between shards. Yeah. I really do like the kind of, I know people rag on it, but maybe we should come up with a new name for monolithic architecture. But I think what you're building is excellent. And maybe to kind of double click on the speed of light through fiber. A lot of blockchains, I think, today, are trying to limit the amount of data propagation that they do or keep those requirements unnecessarily low.
00:08:43.510 - 00:08:56.730, Speaker A: Could you talk about why is it important, I know you mentioned the trading aspect, but why is it kind of a requirement from a technical standpoint of fast data propagation and the fiber optics?
00:08:57.070 - 00:10:14.190, Speaker B: Yeah. So the naive way to propagate the block is you have a leader and they have a connection with every other validator. And if the block is 1 mb, then that leader has to send 1 every node, and that's not scalable. That just doesn't work, because if you have 1gb blocks or you want to do one gigabit worth of data per second, that one leader node can't have 10,000 connections that are all one gigabit each. So you have to have this shared block propagation technique, and that ends up being the fundamental limit of the network. So even if you had infinite number of cores and you don't have enough bandwidth, you basically can't propagate enough transactions, and therefore it doesn't matter how many cores you have. So this is kind of the thing that we've always, that was almost like the first thing we built or like tried to crack was like, how do we propagate the state information efficiently, such that we can scale to any number of nodes? And the technique that we built, turbine, is pretty good, so it can propagate, you can see it on stats from our validators, is that even RPC nodes, everyone in the network receives the block simultaneously around the world.
00:10:14.190 - 00:11:08.294, Speaker B: And this is within the block time is 400 milliseconds. Within 15 milliseconds, everybody has received it, which is really, really cool. Run by randos with RPC nodes, it's 3500 machines. And the way turbine works is it borrows bandwidth from everyone else, very similar to how BitTorrent works, but really latency optimized. So we're able to do this fan out and borrow bandwidth from all the other participants. And we do it in a way where there's no advantage to where the first participants receive the block first or something like that. Every packet or every block is effectively split into little shards we call shreds.
00:11:08.294 - 00:11:15.530, Speaker B: And all those shreds are erasure coded. And then each erasure code and shred takes a random path.
00:11:16.110 - 00:11:26.006, Speaker A: Could you go into the, well, try to just break apart it a little bit more? Could you explain what erasure codes are for the vast majority of people as well?
00:11:26.118 - 00:12:25.640, Speaker B: Yeah, this is, I don't know if you remember in high school where you had to like, look at a curve, right? Like those curves with, you know, with three points or something like that. And imagine you draw, you know, x squared equals Y curve or x cubed, you know, x, you know, those polynomials. I'm sure everyone has had to solve this in high school. Well, a chunk of data, if you think of it as points in a polynomial, you can effectively interpolate, like, the function through any chunk of data. You're not going to do it by hand, because it's too hard for a human, but computers can do that, and then they can extrapolate more points along the same curve. And then, if you remember solving for those equations, any certain, any n number of points, if you're looking at a thing of order three, a polynomial order three, you only need three points. Be looking at one, at, like, you know, some higher order.
00:12:25.640 - 00:12:48.688, Speaker B: You need that many points to solve. So the way erasure codes work is that you extrapolate this curve further, and then you generate a bunch of points. And then when you receive the data, if you have, you know, if, let's say your erasure coding scheme was 32 chunks of data, 32 chunks of erasure codes. If you receive any of the 32 chunks, you can recover the whole data set.
00:12:48.824 - 00:13:03.824, Speaker A: Gotcha. So, it's mainly a way to make sure if any single node kind of acts maliciously, that you're still able to retrieve all the data from the block.
00:13:03.952 - 00:13:35.902, Speaker B: Yeah. So the way that turbine works is each leader sends only one shred, which can be data or erasure codes, to every different node, and then each one of those retransmits server nulls. So it kind of has this fan out effect, and we can't assume that those nodes are going to be malicious or down or something like that. So there's erasure codes that are padded in that transmission. And if at least the way it's set right now is about 50 50, so if at least half of the nodes are participating, the block should recover.
00:13:36.006 - 00:13:53.310, Speaker A: Awesome. I think it's very cool. And maybe to back up a little bit on the talking about proof of our turbine.
00:13:55.690 - 00:14:00.042, Speaker B: Turbine is the coolest thing that we build, but everyone talks about proof of history.
00:14:00.186 - 00:14:16.920, Speaker A: Yeah, no, I think turbine is super unique. I've been trying to study other blockchains as well to see if they have any unique data propagations, and I haven't found any as of yet. I do think just the vast majority of people don't really understand turbine.
00:14:17.620 - 00:14:26.892, Speaker B: And so this, again, comes from, like, if you have the design principle that we have to have no sharding, you kind of have to solve this problem.
00:14:26.956 - 00:14:27.600, Speaker A: Yeah.
00:14:28.220 - 00:14:39.580, Speaker B: All the other solutions, like, well, if you have sharding, then you limit the block, and then it doesn't really matter if you propagated using kind of the slow gossip technique that ethereum and bitcoin use.
00:14:39.660 - 00:15:14.038, Speaker A: Yeah, retrain, my thought. So, in your point of view. And the ultimate bottleneck to scale blockchains is the amount of data that you can propagate in between them, because the more nodes you have in the network, you have to transmit that data or block to all those different nodes to agree upon it for the state execution. And if you cannot propagate that data quickly, that is overall the bottleneck, whether you have a fast execution environment or not.
00:15:14.094 - 00:15:34.464, Speaker B: Yeah, I think the important thing is that, well, why don't you just use sharding? I think you can't solve the same problem with sharding for price discovery that you can with a system like this. You're just simply not going to get to that magic universe where information and state is all globally synchronized.
00:15:34.552 - 00:16:29.786, Speaker A: Yeah, I fully agree. And I think one of my biggest reserves of sharding is we really haven't even tested what a layer one monolithic architecture can do. From the physical limitation standpoint, it seems like we've gone down these alternative paths relatively quickly without seeing how far we can actually push it from the physics standpoint. So my kind of, like Elon and Tesla mindset loved that. Like, you guys are thinking about it from, like, the first principles and physics standpoint. And I love that transitioning maybe slightly, maybe talk a little bit more about proof of history, because I think turbine and proof of history are the biggest misconceptions. And it was funny, at the Stanford blockchain conference that I was at, they were talking about transaction ordering and how it was so important, but.
00:16:29.786 - 00:16:40.350, Speaker A: And they wish that they could figure out a way to do timestamping for blocks. So maybe talk a little bit about, like, what is proof of history, what is it solving and why is it useful?
00:16:40.810 - 00:17:16.418, Speaker B: Yeah, so how this is, again, goes back to my current qualcomm. I wasn't working on wireless protocols, but just being there, you can't absorb them through osmosis. So the way that classic 2G networks work is using this protocol called time division multiple access. And when you have two radio transmitters that transmitter over the same time or the same frequency, you get interference. So information can't pass. So you effectively noise. So how you solve that is you give each one a synchronized clock, and they alternate by time.
00:17:16.418 - 00:18:01.288, Speaker B: And that's literally called time division multiple access. And it works pretty well because you can basically get your frequency band and a bunch of transmitters, and you assign each one a slot, and that clock ticks no matter what happens. And even if one is down, there's no communication between any of the nodes to check. Is this guy down or not, you just transmit whenever you hit your slot. And that's very different from how even sequential, like leader based, leader, schedule based consensus algorithms work. Like tendermint. When you have a leader schedule in tendermint, you have all the nodes have to agree in a block before the next one can transmit.
00:18:01.288 - 00:18:31.220, Speaker B: And that agreement takes time because it requires all of them to communicate and go through their rounds of consensus. So this is kind of like the fundamental annoying problem, why you can't scale tendermint to the same size network as Solana, and why Solana can have 2000 validators all in the same quorum. So the way that we use proof history or what it is, first of all, it's a verifiable delay function. I talked to Dan Bone, so he actually told me you can call it a VDF.
00:18:32.080 - 00:18:33.100, Speaker A: Good, good.
00:18:33.960 - 00:19:17.976, Speaker B: But it's a very, very rudimentary one. It is basically taking a hash function, Shapta 56 specifically, and running it in a loop where the output is the next input. So this sequential recursive operation cannot be parallelized because shata 56 is pre image resistant. So there's no way for you to know what the result is going to be 100,000 iterations from now. And there's no way to optimize that path. That's not proven, but we think that it's impossible to optimize this path in any parallel way, even if you do it sequentially. I think there might be recently a proof that that's true, but if there is, I'm not 100% sure.
00:19:17.976 - 00:20:13.252, Speaker B: But that's definitely no one has been able to demonstrate this ever. Otherwise bitcoin would be much more efficient. So that's really the cool part about this, is that the sequential operation, when you run it and you sample it, just literally you get a checkpoint every, let's say 100,000 cycles, you get a data structure that represents the time has passed somewhere. So without using a clock like you do with TDMA, you can then force every block producer to generate a data structure to a different size before they make their block. So that's a proof that they waited a certain amount of time. And if all of them are doing this, and there's no way they can cheat, because they literally have to run these cycles, you effectively build TDMA. There's no block producer producing at the same time.
00:20:13.252 - 00:21:03.700, Speaker B: And when you don't have two contentious blocks, you don't have forks. Right. And you basically eliminate most of the bottlenecks in consensus. And there's a very subtle other difference. And this goes back to how tower BFT works is that we can make assumptions about how time actually moves through the chain, and we've been able to basically run consensus purely on top of this time series. We don't need to have these nodes to come to agreement at any point in time. They can literally just submit votes, and then you can observe that they came to agreement a little later because there's no way for those votes to expire without the ledger actually moving forward.
00:21:03.860 - 00:21:17.680, Speaker A: So effectively, you're timestamping all transactions. Why do you think other blockchains in the industry are not trying, or not trying to kind of solve this clock problem?
00:21:19.500 - 00:21:54.490, Speaker B: Well, one is, I think most folks were giving us sharding because they're really building like a different protocol. Right? Like, I think it's not really focused on this idea that there's going to be one giant unified price discovery engine. So that's just not their north star. And if you're okay with sharding, then you can skip all these steps. You can have like a subcommittee of 200 that they're the ones that produce the block. They come to agreement on the block, and then they prove that the data is available to the rest of the network. And that's eth two's design.
00:21:55.470 - 00:22:05.710, Speaker A: Yeah, maybe kind of. I would like to talk about more about sea level as well, but maybe.
00:22:05.790 - 00:22:49.816, Speaker B: Kind of on one cool thing that proof history enables, and this goes back to your comment about time stamp like timestamping transactions, is you still have block producers, right? So you send a bunch of transactions to them right now with only one block producer. What you're really only time stamping are blocks, right. You have this separation that a block can only be created when it's scheduled, and that takes real time to get there. But when I send you a bunch of transactions, the block producer can reorder them and do whatever they want with them, and they can potentially pack them at the start of the block or at the end. So the cool thing is that, imagine there's two block producers at the same time.
00:22:49.848 - 00:22:50.568, Speaker A: Multiple leaders.
00:22:50.664 - 00:23:26.704, Speaker B: Yeah, multiple leaders. So now both of them are creating this hash function and they're spinning on it. And if I send the one that is closest to me physically, that transaction is going to end one to the farthest one. The one that's closest to me physically is going to receive it first, all things being equal. And they will literally that. The way that block production works in Solana is that you take the transactions, hash them into the proof of history hash, and you make a note saying, that the hash of the value that I added was x. And that's.
00:23:26.704 - 00:23:56.916, Speaker B: That's your ledger, because that changes the next hash in an unpredictable way. There's no way for anyone else to be able to simulate that and predict it ahead of time by sending this transaction to the closest leader, and it gets hashed first. When both of these ledgers are examined, you know that mine, which arrived at the closest leader first, was actually first in real time compared to everyone else, right? So that's a really kind of different scheme that no one else can really simulate.
00:23:57.028 - 00:24:05.090, Speaker A: When do you think you'll be able to? Or the Solana labs team will ultimately have the multiple liters?
00:24:05.870 - 00:24:45.518, Speaker B: Yeah, there's a couple things that are getting done. So one is improvement in turbine to make it much more reliable. Then we're basically using a vector commitment for the shreds to make it so that anybody that recovers any of the shreds can continue retransmitting. Once that's live, I think the next piece is making the execution separate from block production. So leaders shouldn't be executing blocks. They should just be packing blocks. Once we have that, then it's possible to do this next step.
00:24:45.574 - 00:25:11.760, Speaker A: Gotcha. I'm super excited for the updated turbine, not white paper or medium posts. I've read each of your medium posts that you originally posted multiple times, and every time I read them again, I feel like I learned something new. Cool. They're super helpful, so hopefully we can get the updated turbine docks. I would love to read more about it. No, I think the multiple block producers is super interesting.
00:25:11.760 - 00:25:45.300, Speaker A: And being able to effectively timestamp, that has a lot of benefits as well. Maybe going back to the throughput just real quick. I think one of, I mean, going back to, like, overall throughput and, like, trying to achieve it at, like, speed of light, what is Solana's kind of throughput? What is the throughput it's doing today? And ultimately, kind of where do you kind of foresee that evolving over time to support, like, the hundreds of millions or billions of users?
00:25:45.840 - 00:26:02.046, Speaker B: Yeah. So it depends how you measure it. So if you have, like, nodes in a lab, we can. We've seen tests that go up to, like, 200,000 tps if you give it enough cores, and that saturates, like, Google's, like, 800 megabit link.
00:26:02.118 - 00:26:02.662, Speaker A: Okay.
00:26:02.766 - 00:26:53.176, Speaker B: And that means turbine is taking, like, a chunk of that because you have erasure codes and everything else, but an open Internet with a lot of cores, you start seeing bottlenecks across, like, just different parts and bandwidth included. And on our testnet right now with 3100 validators, and these are the same validators that run mainnet, we just ask users to run an extra validator for the testnet. It's pretty close to what we think Mainnet performance can be. The peak TPS sustained has been over, like 13,000 to 15,000. That's awesome. And that's on, I think these systems are 24 cores. So effectively, when we double the cores, we should be able to, if we're good software engineers, double the bandwidth.
00:26:53.176 - 00:27:09.016, Speaker B: But that's not always the case. Every time you double, there's always, you find, okay, there's lot contention here now, or there's this algorithm that we wrote is single threaded, and it's blocking everything, so we got to rewrite stuff. And that's just kind of a constant, you know, never ending work.
00:27:09.128 - 00:27:09.980, Speaker A: Makes sense.
00:27:10.650 - 00:27:13.190, Speaker B: Job security for.
00:27:16.250 - 00:27:35.370, Speaker A: No, I think, I mean, I appreciate kind of. I know the software side is very complex, but I appreciate the kind of simplicity and adding more cores and increasing the bandwidth to continue to scale. Bandwidth. What law is it that bandwidth, like, doubles every couple years?
00:27:35.410 - 00:28:17.620, Speaker B: Nielsen's law. Yeah, so we already see that this happens over decades, so it's kind of hard to believe that it does, but with the 5G rollout, you're seeing data centers have to go much higher bandwidth, and that's also pushing even home connections to ten gigabit connections. That's a lot of data. It's just ten gigabits is a lot. And that's the target for fired answers. So our target was to demonstrate, can we saturate the network with turbine at, like, about one gigabit? And we can demonstrate that at a Google cloud deployment. That means that the software is good enough for that.
00:28:17.620 - 00:29:02.514, Speaker B: And going to ten gigabits is much, much harder because you need more cores, and there's more contention. And what's important is, like, can we saturate it in a way that we can keep adding more nodes? Because the way Solana is designed is that votes, everything else, is really going through the same pipeline, the same turbine pipeline. That means that votes are transactions that use up the same bandwidth, that eat their contention for resources with applications. And that seems like a limitation, but that's actually really important, because when we increase the bandwidth, we also increase not just transaction bandwidth, but bandwidth for validators, bandwidth for decentralization.
00:29:02.682 - 00:29:39.336, Speaker A: Yeah, I think that's a super unique point. I definitely want to touch upon that again, I kind of want to say just on the core, Solana tech real quick and talk about the additional cores and how you design sea level to take advantage of those cores. I think this is now starting to become more commonly understand, but I think a lot of people are still kind of confused on like execution environments and single threaded virtual machines versus like multi core virtual machines. Could you touch upon like sea level and kind of maybe the background or inspiration behind that?
00:29:39.368 - 00:30:20.548, Speaker B: Yeah, yeah. When we were designing it, I was trying to figure out how do we lay up memory such that we can process transactions on something like a GPU SIMD scalable architecture. The reason for that is because GPU's are just really, really cheap in terms of cores. And the way that they work is a little tricky, is you have one execution thread and then a bunch of lanes and that thread is able to run the exact same instructions over all the memory lanes. So as long as the instructions are the same, it can continue all like 80 lanes at the same time. Right. That's really, really fast and cheap.
00:30:20.548 - 00:31:06.366, Speaker B: Effectively I think 20 x to 100 x cheaper for computation and GPU's than CPU's. So the way that you have to lay out the memory is that you can't have global state variables or anything like that. That's really dependent. So all the memory has to be passed in and relocatable, meaning it should be landed anywhere in memory. So this is basically my last gig at Qualcomm was working on dsps and dealing with memory management and those things. So a lot of the same ideas applied here. So the way we built our runtime is that code is pure elfs.
00:31:06.366 - 00:31:45.092, Speaker B: So these are shared object executable Linux files. They have no global variables, no state of their own. So it's just pure, pure code. And all the memory that a smart contract needs to execute on has to be passed in into an entry point to this executable file. So the developer has to specify ahead of time. I'm going to read these accounts, I'm going to write these accounts and call into this kernel, this bytecode. So when you have a bunch of CRM transactions for a bunch of different markets, each one of those markets would be a different writable account, but the code is exactly the same.
00:31:45.092 - 00:32:22.030, Speaker B: So that means that we can take the serum kernel, potentially transpile it to sphere five, which is like the GPU bytecode, and then take all these markets as different chunks of memory, lay them out on the GPU and zip across all of them. So that was really like from the ground up I was thinking how do we execute as most things in parallel as possible. And I. Alex, I don't know if folks are following development on c level. Alex has been like the bytecode master. He called it Solana bytecode format. So, SBF, that's funny.
00:32:22.030 - 00:32:36.314, Speaker B: He's been running experiments and getting SBF compiled to like spear five and stuff. I don't know when that's gonna ship, but that's basically all the design is there. It's just work to get that stuff out.
00:32:36.402 - 00:32:53.028, Speaker A: Awesome. And the biggest kind of. I mean, ultimately you're just able to paralyze many more transactions versus like a single threaded virtual machine, just allowing much more throughput for the overall blockchain.
00:32:53.204 - 00:33:38.700, Speaker B: Since we know what the, what all the memory reads and writes are ahead of time in actual validation, in that part of execution, you don't have any locks, right? You kind of know there's no contention once it's laid out. So you can basically execute really, really fast. But this was the big gaping hole in how we launched. And what we launched with is that you're pushing the problem up the stack. And now the really hard part becomes packing these transactions and figuring out how to create these blocks. And that's a classic knapsack problem. And it's not always easy to find the right heuristic and the right solution for it off the start.
00:33:39.280 - 00:33:41.260, Speaker A: So how's the block packing going?
00:33:42.400 - 00:34:16.192, Speaker B: Yeah, so going back to like, we made the right mistakes at the right time to learn something really, really important. Otherwise, I think if we borrowed off the shelf solutions, we wouldn't have learned a really important lesson. So what do we do wrong? Is that we made the assumption that all the transactions are going to be something like serum. We have an order. It's effectively a bid or an ask or a cancel. It's like a lookup into a queue. And then you flip some bits, you set the price on some bits.
00:34:16.192 - 00:34:33.369, Speaker B: That lookup is bisect or something like that. It's a pretty simple algorithm, so all those instructions would be exactly the same. They're very, very fast. To execute a serum transaction takes about like 5000 compute units. 7000. The cheapest one is about. Takes about 3000 compute units.
00:34:33.369 - 00:34:45.409, Speaker B: So these are like, this is what we expect it to run. And it does run about 20 million, 15 million serum transactions per day, which is serum alone does more transactions than all the EVM chains combined.
00:34:45.489 - 00:34:48.185, Speaker A: I think that's pretty wild. I don't think people appreciate that.
00:34:48.217 - 00:35:44.978, Speaker B: But just serum on its own is more than all the other blockchains combined. But we thought that the way we can control fees is that if there's too many transactions, that we start seeing capacity limits, that we simply double the price per transaction, blindly, for all transactions. And the way that we measure them is by signatures, because that was the most computationally intensive part in the whole pipeline, like, actually verifying a cryptographic signature. So that was wrong, because even though serum itself never hit capacity limits, you had a very small percentage of transactions that were basically all Mav, maximally extractable value, or MAV. And that happens when. Very classic example is you have a market maker that puts up an order and something changed in the world. Now they have to change that order.
00:35:44.978 - 00:36:17.050, Speaker B: They have to cancel it. So they send a cancel. And now an arbitrage bot sees that something changed in the world, and they want that order, and they send a message to go take that order. So there's a race now, right? And the way that you solve this race is you try to be the fastest, right? And to. Only if there's no way for you to be the fastest. By paying more, you send more transactions. You just flood the network with as many as you can.
00:36:17.050 - 00:36:38.570, Speaker B: So our poor block producers were seeing, at first, it was, like, 400,000 packets per second. Then it was 4 million, and then in some cases, 20 million packets per second. And the funny thing is that the biggest users of this that were sending these bots wasn't for, like, sophisticated high frequency trading. It was for nfts.
00:36:40.430 - 00:36:41.878, Speaker A: You gotta love those jpegs.
00:36:41.934 - 00:37:34.448, Speaker B: Yeah, exactly. So we made. And, like, observing this was like, okay, what do we do? Do we need to build a mempool? Or, like, how do we solve this? I mean, it's obvious that mempools solve this, but they do it in a very slow way and also create a single environment for fiece to fight each other. So the. The eureka moment took a while. So it took, like, solid, like, two months of us just solving the obvious bugs, like, making the transaction processing faster and, like, dealing with memory issues there until I kind of just started narrowing down. Okay, what really is the problem are these specific mint accounts are priced too low, and everybody's trying to get to them, and there's no way for us to encode all the transactions because it is four in a thousand or, you know, 4 million packets.
00:37:34.448 - 00:38:12.406, Speaker B: We can't possibly encode all of those. There's a limit there. And because they're all touching the same state, they're not taking advantage of any parallelism. So we can't, like, we can't even saturate our block. Like, blocks weren't being full, it's just all the rights to a single account would take up all the single threaded resources that that account could provide. So it became obvious to me is that we needed some way to localize the fees only for those accounts and have fees spike for those users, but isolate everyone else. And this is where the design for these localized fee markets came about.
00:38:12.558 - 00:38:32.700, Speaker A: I think it's brilliant, and I think I've seen other blockchains try to do something similar to this now, because once you are able to remove kind of the global fee markets, it makes pricing discovery for interacting with those individual contracts much better. And I think ultimately it creates a better experience for everybody.
00:38:32.780 - 00:39:38.750, Speaker B: And this is the bearish case for sharding. Why sharding can't work, let's go into it is because, like, you have this giant state machine that has these hotspots, and when you split it into two generic state machines, both of them still have hotspots. Like, would you buy the idea that, like, you know, optimism and arbitrum, amazing teams are going to be successful if they can only handle nfts or deFi, right. They have to support both. And the only way to support both is to make sure that when there's an NFT mint that doesn't impact liquidation. So you have to isolate hotspots, you have to find some solution where the fees that are driving up for an NFT mint don't impact the liquidation users and don't impact payments and all this other stuff. And if you have a single mempool, everyone's bidding to be accessed, priority access into the block, that's going to force a fight between all the NFT bidders, all the liquidation bidders, all the payment bidders, they all try to outbid each other.
00:39:38.830 - 00:39:42.406, Speaker A: Is it a mempool thing or is it an execution environment thing?
00:39:42.438 - 00:40:07.206, Speaker B: It's both. If your execution environment can't support isolation, there's no point to try to do this hospital isolation anywhere, right? So it kind of starts at the bottom, but as soon as you build it, right, then what's the point of having more than one l two? Then you only have one l two. And then if you only have one l two and one l one, what's the point of separating them?
00:40:07.318 - 00:40:14.374, Speaker A: Yeah, I think, yeah, l two s are funny, right?
00:40:14.422 - 00:40:16.930, Speaker B: So then you end up with a design like Solana.
00:40:17.470 - 00:40:50.452, Speaker A: I always say, like, when people ask me questions, like, I think all of it will work, it's going to work. It's just, what are they trying to solve for? And then how will it actually scale, and then what is the user experience from that? And I don't think people have kind of put all those pieces together. And so I really do think Solana's architecture, when everybody else was running another direction, you ran the complete opposite way. And I think it's brilliant. Super excited what you guys really are doing. It's.
00:40:50.556 - 00:41:10.556, Speaker B: Appreciate it. Super fun, too, because these are, like, really, really challenging problems. And being alone in the space, alone in that design is like, I don't know, we're discovering more stuff. Yeah, we get to learn our own lessons, which is pretty cool.
00:41:10.748 - 00:41:39.120, Speaker A: It is cool. You guys are definitely leaders in the space. One thing you're also leaders on is kind of the active accounts or unique signers. And I think that's sometimes hard for people to digest. But one thing that I thought is interesting for you, or that you've said in the past, is unique signers is kind of the goal. Could you touch upon why you believe that is the ultimate goal for these systems?
00:41:39.250 - 00:41:51.880, Speaker B: I think these represents, like, the number of self custodied users that are using the chain, and that's kind of. I think if you're not building for that, then you're not serving humanity.
00:41:53.660 - 00:41:54.668, Speaker A: I like that.
00:41:54.804 - 00:42:32.814, Speaker B: Yeah. Which is like, I don't know, maybe there's folks that care. I think there's lots of application specific chains that could be really great on volume and like, defi and like, even nfts and stuff like that. But I think if you try to maximize the number of users that use it, it ends up being, I think, a much more generic and open platform, like any one of these. Some folks talk about application specific chains as a solution to a lot of these problems, and they actually do isolate state. You have NFTs and separate pools from.
00:42:32.902 - 00:42:38.272, Speaker A: Defi just because they're on completely separate blockchains, essentially.
00:42:38.376 - 00:42:58.216, Speaker B: Right. But what that part misses is this magic composability of everyone in the world being super connected with everyone else and having no friction between it. And that's a very, like, hand wavy thing, because it is. It's like the web. The Internet. Like, why would you use the Internet versus your local intranet?
00:42:58.288 - 00:43:42.230, Speaker A: Yeah, yeah. Everybody was super excited about the intranet initially, and then it became very obvious over time that the Internet was with a much cooler thing. It is funny, maybe staying on the hardware topic a little bit longer. I've seen one interview that you've done in the past where you talk about the lean line rate and doing a full loop before more packets arrive. I think this is fascinating. I think this is super, super interesting. I know that's the end state, but could you go more into what is the link line rate? And how is that kind of like the end goal of Solana?
00:43:42.350 - 00:44:27.774, Speaker B: Yeah. So the link line rate is basically when you have Ethernet or fiber optic cable connected to your network, there's just Shannon limits, real physical limits in terms of how many bits that thing can transmit. So can you receive all these bits and pipeline it such that your throughput is equal to the link line rate? So if you're receiving like 100 gigabits worth of bits per second. Yeah. Your execution environment does it at the exact same rate and then transmits, it obviously will add some latency because there's still that execution state, but you can pipeline it. So you're like, as you're receiving, you're sending out exactly the same. So is that achievable? In theory, yeah.
00:44:27.774 - 00:44:40.980, Speaker B: And like, I jump crypto fire dancer project is like an attempt to get us closer. Their target is ten gigabit, which is a much more. It's harder than one gigabit, which was our target.
00:44:41.140 - 00:44:44.228, Speaker A: Do you think like the end state will be 100 gigabits?
00:44:44.324 - 00:45:15.910, Speaker B: I mean, I don't know. Like. So there's this, I think, fallacy that the demand for block space is infinite. I think that's false. I think when you look at total number of Google searches per second, it's about 80,000. And that's like one globally scaled application that everybody uses without Google's pretty big. Yeah, that's like an app that every human uses while thinking.
00:45:15.910 - 00:45:34.310, Speaker B: So I would say that ten x, that is probably reasonable to satisfy nearly everything in the world. Every usage for blockchain, and that's like a million transactions per second. That's ten gigabit.
00:45:35.170 - 00:45:36.130, Speaker A: Interesting.
00:45:36.290 - 00:45:42.590, Speaker B: I mean, with erasure coding and other overhead. Right. That's like, basically fits within the ten gigabit performance.
00:45:42.930 - 00:46:05.406, Speaker A: Okay, that'd be wild. I'm looking forward to that day. And then, so, like, on the hardware side again. So if you're doing ten gigabits a second, what type of hardware? Or what would the nodes kind of look like at that standpoint? Would they be FPGA's, asics? Like, would it be on GPU's?
00:46:05.518 - 00:46:39.018, Speaker B: Actually, we don't know. I think it's very likely that basically rewriting this, like, they get the benefit of looking at how the design and where it ended up at the end of the maze, and they know the start and they can cut through a lot of the technical debt that we accrue trying to build this thing and run it at the same time. So I think it's very possible that modern day 128 core system could handle ten gigabit, maybe dual socket one.
00:46:39.114 - 00:46:41.794, Speaker A: And that's without the need for GPU's or asics.
00:46:41.922 - 00:46:56.032, Speaker B: Yeah, you might, like, maybe GPU's just for signature verification. So, like, things like that might be useful, but, like, those things have, like, AVX, and that's approaching, like, the throughput that you need.
00:46:56.096 - 00:46:57.752, Speaker A: Okay, super interesting.
00:46:57.816 - 00:47:00.160, Speaker B: We'll see. Like, maybe it's 512 cores.
00:47:00.200 - 00:47:00.488, Speaker A: Yeah.
00:47:00.544 - 00:47:23.820, Speaker B: But I'd be really surprised if it's over 512. Like, a really optimized system. Yeah. The cool thing is that, like, the jump folks are, they've built these systems before. They've literally built high frequency systems that have to handle 80 gigabits worth of data, like, easily. Like, that's, like, market data that they all receive. They have to parse all of it and bucket in and filter it.
00:47:23.820 - 00:47:30.760, Speaker B: So ten gigs for them was like, like, yeah, no brainer. This is achievable.
00:47:30.840 - 00:47:33.856, Speaker A: Yeah, very cool.
00:47:33.888 - 00:47:36.632, Speaker B: We'll see. But, like, we'll see. But I'm pretty excited.
00:47:36.736 - 00:48:15.100, Speaker A: I am, too. I wanted to kind of talk about some of, like, the critiques of Solana, because I personally think Solana is by far ahead of the general competition as it stands today. But the biggest kind of critiques that I hear are the downtime, which I think you've kind of touched upon with some of the updates that you're doing, but the other ones being the Nakamoto, or not Nakamoto coefficient, but the general decentralization of Solana. Could you talk upon that and what the Nakamoto coefficient is?
00:48:15.180 - 00:48:49.618, Speaker B: Yeah, totally. So the biggest thing for downtime is actually fire dancer. Having a separate team build a client from scratch means that it's very unlikely that there's a memory leak in both implementations or some other catastrophic failure. So there's always fallback options for validators, and that's huge. That's probably the. The biggest thing anyone can do for reliability and safety for the network is have a second client. In terms of decentralization, I'm a very engineering driven kind of person, so you kind of have to think of it.
00:48:49.618 - 00:49:12.320, Speaker B: What is the physical manifestation of decentralization? How does it actually. We can talk about it, but what does it actually physically do? So, from first principles, you'd look at something like bitcoin. If I destroy every copy of the ledger, all the 12,000 or so copies that are live today, just go to delete them all bitcoin is gone. Right?
00:49:14.380 - 00:49:17.588, Speaker A: And those are only full nodes, not like clients.
00:49:17.684 - 00:49:56.968, Speaker B: Yeah, so, and nobody can prove what bitcoin they own, right. So that means bitcoin ceases to exist. So that is the physical manifestation of decentralization is the number of independent replicas that are up to date. So these bitcoin node that's like a year old may be helpful at that point because you can at least prove that a year ago everyone had this. And we start from there. But if it's a long enough state that's changed, it will destroy the network like the heart of it, I would say. So you need up to date clients and you need a lot of them and they need to be independently owned.
00:49:56.968 - 00:50:26.528, Speaker B: And it doesn't really matter why people are running these things. It doesn't matter if they're a validator that's running a validator business, that's in the top five of the stake nodes. Or this is a monkey Dao that's running it for fun, for their dao. Or it's magic Eaton that runs a bunch of RPC nodes to serve their users. Any one of those copies that survives can say, hey, look, I do have a copy of the ledger. These are all the signatures from everyone. We can prove that that's a legitimate copy.
00:50:26.528 - 00:51:01.420, Speaker B: All the VDF proves everything. And then we can re hypothecate the network and restart it and continue. And there will be downtime in that case, like the network stops and you have to go through the process, but it will survive. So I think Solana is the third most decentralized network from that objective kind of pov one and two, bitcoin and then Ethereum. So Ethereum, despite having 400,000 validators, it's only 5500 in sync boxes.
00:51:01.500 - 00:51:02.372, Speaker A: The full nodes.
00:51:02.476 - 00:51:13.740, Speaker B: Yeah, if you go to node, watch IO, they actually, these are all the full nodes. It's about 5500 of them. If all those get destroyed, does it matter how many validators each one is running?
00:51:13.780 - 00:51:17.452, Speaker A: Just a lot is close to like two thirds at like 33 or 3600.
00:51:17.516 - 00:51:50.360, Speaker B: Yeah, yeah. So I would like to see Solana be the most decentralized and the biggest. So the biggest critique is that this is not the right way to look at decentralization. That's actually like a lot of ethereum. Folks have a different point of view. They think that core value or core important piece of decentralization is the cost to verify how easy it is for an average user to go verify the chain. And this is why ethereum too is really focused on minimizing that cost.
00:51:50.360 - 00:52:25.726, Speaker B: So I applaud that value. But I don't think that's a critical value for actual decentralization because the cost to verify are different from validation. Validation requires real time, high availability system that's running at this high rate. Let's say me as a user, I need to validate. The very, very simple way to reduce cost is I just take more time to validate. I need network. Something happens in the network and everyone's like, hey, I don't know if it's real or not.
00:52:25.726 - 00:53:06.212, Speaker B: As long as there's one honest node that's providing data, everyone else can download it and then take their time on a much, much cheaper system that doesn't have to have this high availability and actually verify and validate the chain. And that's 100 times cheaper. Like, that's just like, it's way cheaper to do that. And if there is a minimum cost to run an Ethereum validator that's satisfactory for decentralization, there is some maximum amount of time that hits that minimum cost to verify. Solana Ledger. So that's it.
00:53:06.376 - 00:53:49.760, Speaker A: Yeah, I do think, I mean, yeah, you can't, I don't know how you're gonna run like 100 million like concurrent users or like have people active on the chain on just like low hardware from like the physical standpoint or like low bandwidth requirements. Again, like, I think it'll all work. It's just like what you're optimizing for and what you want the chain to do. But no, I appreciate you addressing that. I think it's very misunderstood. And then the other thing that I think people also kind of misunderstand, we touched upon it briefly earlier, was the communication overhead of Solana. Could you touch upon that? I think right now, like, the non voting transactions are between 20 and 25% of the network.
00:53:51.740 - 00:54:13.530, Speaker B: Yeah. So no, sharding consensus is brutal in communication overhead. It's n squared. Total bandwidth is required. That means that it's a scary number, n squared. Right. But all that means is that when you have n nodes, each one needs to receive n packets and transmit n packets at least.
00:54:13.530 - 00:54:58.226, Speaker B: And you have n times n. That's where the n squared comes, comes from. And that means that if you have n nodes voting on every block, like Solana does, you have a lot of messages that are votes. And because we build a really, really fast pipeline, we didn't build a pipeline for votes. That's separate from transactions. We literally like, okay, if there's any faster way to process any message in the system. Why wouldn't we use that for transactions, especially the messages that have this n squared requirement? Because when a vote is transmitted, it needs to be propagated to the entire network, because the entire network needs to verify that vote, the two thirds plus.
00:54:58.258 - 00:54:59.674, Speaker A: One to achieve consensus.
00:54:59.762 - 00:55:42.228, Speaker B: Same thing with any state in this non sharded network, any state transition, any serum order, any other transaction, any payment has to propagate to all the nodes for them to verify it. So they're kind of the exact same application, like whether you're sending transactions or whether you're sending votes, the exact same thing needs to happen. You need signature checks, you need to state transition, you need to propagate that message to everyone. This is why there's only one pipeline. And this pipeline is somewhat agnostic. So it can kind of trade bandwidth. If there's too many applications, it doesn't need to use all the, it doesn't need all the votes, maybe at that same time.
00:55:42.228 - 00:56:19.968, Speaker B: So it's flexible in how it can manage its resources. And that was part of the advantage of this design. But when people look at our transaction numbers, people that are in the know, when they look at the Solana explorer and they see it's doing 3000 tps, they know that the chain is healthy. If it's doing like 800 tps, they're like, oh shit, there's congestion, something's wrong. And that's because they know that the total capacity for consensus and applications dropped. And now that the likelihood of them getting a transaction through is low or expensive, if there's priorities, that makes sense.
00:56:20.104 - 00:56:28.020, Speaker A: So the main kind of benefit of treating all the transactions same is reducing latency as, or making latency as small as possible.
00:56:29.840 - 00:57:19.820, Speaker B: It's really just from a software design thing. There's a clear benefit in like when you're, we build a runtime with parallel execution, so there's multiple threads. It's very, very hard to make this deterministic. So the only way we know is deterministic is because every day there's 300 million of events that propagate through it. So having the exact same pipeline that's used for the heaviest loaded application like votes actually means that we have more confidence in that software. Like separating them out would mean that we build now two pipelines, both of them have to parallelize. And like, that means twice the number of bugs, right? Like it's just kind of insane overhead in terms of like just engineering makes sense.
00:57:20.560 - 00:57:26.940, Speaker A: Is there any way to make it less than n squared? Or like, is it not worth the trade off for that?
00:57:27.840 - 00:57:58.572, Speaker B: There is, there's a way to cheat. It's not really cheating, but you're still doing un squared at the end of the day. Worst case is n squared is you can sample for the who should be voting. Like pick a subcommittee and have them sample for it. And intuitively why that's safe is that let's say there's some finite number of partitions, the network is split. It doesn't matter if it's eight or 100. You sample the network, you pick 200 nodes.
00:57:58.572 - 00:58:13.144, Speaker B: It's basically probability of you. Of those 200 nodes having a partition representation that's different from the network is like virtually zero.
00:58:13.232 - 00:58:13.664, Speaker A: Yeah.
00:58:13.752 - 00:58:21.660, Speaker B: Right. Like if there's three partitions, 33, like all 33%, your 200 nodes are all going to be split by 33%.
00:58:22.720 - 00:58:23.336, Speaker A: Okay.
00:58:23.408 - 00:58:49.804, Speaker B: So when they vote, you know, whether there is, whether the network, everybody in the network received a block, and whether they all agree that the block executed deterministically for everyone just from those 200 nodes. So that would reduce kind of the number of votes that the network has to handle, but you're still doing n squared for everything else. So it's an optimization, but like a constant one.
00:58:49.892 - 00:58:56.636, Speaker A: Yeah, just because there's not a way around the, like two thirds plus one to achieve consensus. Like all to all.
00:58:56.668 - 00:59:27.002, Speaker B: Because we still need to propagate the all to all, like state transitions. Everybody has to receive serum like values. Right, like serum state transitions. Because when you're, like, not in the, in the voting sample subcommittee, but you will be in like two minutes, you have to have processed all the blocks to get the verified state. So, like, it's an optimization. I'm actually working on it right now just for fun. But it's not a requirement.
00:59:27.002 - 00:59:37.990, Speaker B: Yeah, it won't fix the end scored problem. So the only way to do that, I think that designs that I've seen that work is like, eth two's dank sharding.
00:59:38.330 - 00:59:56.164, Speaker A: Interesting. Cool. Well, I really appreciate your time. I think we're very close to time. We've touched upon a lot. It's been wonderful to get to chat. I think I try to end the podcasts on like, spicy questions or just like, spicy takes.
00:59:56.164 - 01:00:05.876, Speaker A: I know, kind of. You said charting is not like the optimal path forward just for like, global price discovery.
01:00:05.948 - 01:00:14.772, Speaker B: So. Yeah, so people say that there's ethereum two is not state sharding, but roll ups are logically sharding state.
01:00:14.836 - 01:00:15.332, Speaker A: Yeah.
01:00:15.436 - 01:00:21.716, Speaker B: And they're not doing it in any application specific way. So you're going to end up with the same kind of hotspot problems.
01:00:21.788 - 01:00:38.660, Speaker A: Yeah, I think, yeah. You have to do the compute or data propagation somewhere. You can't not do it. Whether it's on the l one or l two, you're doing it somewhere. It's just what trade offs you make. Maybe we can just end it on that l two still have.
01:00:38.700 - 01:00:40.412, Speaker B: There will be one chain through them all.
01:00:40.476 - 01:00:56.646, Speaker A: Yes. Cool. Well, no, really. Thank you again, Anatolia. I've been looking forward to this for a long, long time. I'm super excited to have done the conversation and happy to have people to listen to this. I think they're really going to enjoy it for sure.
01:00:56.646 - 01:00:57.118, Speaker A: Awesome.
01:00:57.174 - 01:00:57.814, Speaker B: Yeah. Thank you.
01:00:57.862 - 01:00:58.150, Speaker A: Thank you.
