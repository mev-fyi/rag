00:00:00.640 - 00:00:24.846, Speaker A: And they are. They're so magical, in fact, that when I. The door to Diagon Alley, I was a Muggle, and I was looking at where this door reputedly was, and I refused to see it. And he said, there's this thing where you can take a computation, and you've done the computation and you can prove it to someone, but you don't need to give them the whole computation. You can just make this claim, and through this magic, they can be totally convinced that you did it.
00:00:24.878 - 00:00:29.606, Speaker B: You give a little bit of context on what we as an industry have accomplished.
00:00:29.678 - 00:00:38.220, Speaker A: But for blockchain, but for this industry, we would be a lot, lot, lot further back than we are. It has been breathtaking.
00:00:41.640 - 00:01:22.280, Speaker C: Well, today I'm very excited to be doing a podcast with Tom and Eric. Tom has an immense background in the zero knowledge world and rollups. He ultimately was the founder of Aztec Protocol, is ultimately focusing on privacy, and has since stepped back and now started his own fund. So I think it's going to be a very technical conversation and diving into the weeds. But before we kind of get into some of the more technical components, Tom, I would love for you to just do a quick background on yourself and how you got involved into the industry.
00:01:22.940 - 00:02:22.180, Speaker A: Yeah, sure. Thanks for having me on, Logan. So my background was originally at Undergrad and Postgrad. I did maths, quite a lot of algebra, which became kind of relevant later on, not that I expected it to. I then had a glancing blow with music, but basically was in finance for a few years and left the day I qualified and could show my dad a piece of paper that I qualified in, something he regarded as useful, and left, really to do something quite imprecise, which I got a copy of the Ethereum yellow paper from my flatmate with whose brother I would then go on to start Aztec. Not that I knew it at the time. I knew that there was something very important and rather magical about this ephemeral new world computer and this amazing, beautiful blueprint being written by Gav Wood and Vitalik Buterin.
00:02:22.180 - 00:03:29.488, Speaker A: I think it took us a while to try to work out how to harness this thing. And the sort of the first idea had really been the obvious thing was go and replace some core function of global finance. So really the first thought was, can we do duration transformation, maturity transformation, which obviously is now all over the news as of the past couple of weeks. But there was a known problem back then, actually Credit Suisse Washington, the poster child for financial risk. At that point, it was really about credit control, but just sort of there was this thought, well, maybe we can bring what is essentially a shadow banking industry, which is an industry where most of the actual long term finance sits off balance sheet is not directly visible on the balance sheets of banks, and can we bring it into the open using this great globally checkable computer? So the first step was, and the wrong assumption that I made was, no one is going to be mad enough to want to leverage their eth. This would be a crazy thing to want to do. So no one's going to do that.
00:03:29.488 - 00:04:09.922, Speaker A: So how are we going to get an expression of leverage on chain? Oh, well, let's go and get good quality credit, stuff that's yield bearing, bring that on chain, and then we can start to do this balance sheet management, transforming deposits that are not kind of smart pieces of capital, but are quite widely accessible, and use that to sort of finance an economically productive capital, like loans and bonds. So that was the thought. And so the first thing we started doing was trying to tokenize pieces of good quality credit and put them on chain. First problem. And by this point, I had partnered up with Zack Williamson, who was my co founder at Aztec.
00:04:10.026 - 00:04:10.236, Speaker B: The.
00:04:10.258 - 00:04:31.820, Speaker A: The first obvious problem was that anyone who was going to be indebted via the blockchain did not want all of their information publicly visible. So this is actually how quite quickly we went from addressing a sort of debt instantiation problem on chain to a privacy problem. And that's really how Aztec was born, was. It was the first response to how do we do on chain privacy.
00:04:35.240 - 00:05:04.050, Speaker C: Interesting. Very interesting. Yeah, I maybe. So, taking a little bit of a step back, before we get into all the different kind of technical aspects, could you maybe highlight how maybe a brief like history or touching upon zero knowledge or zero knowledge proofs, what got you interested in them, and then how they're ultimately being used in blockchains today?
00:05:04.910 - 00:05:53.284, Speaker A: Yeah, sure. So, weirdly, I actually heard about zero knowledge proofs before I heard about Ethereum. It was in a London pub with a guy called Tom Littleton, who then worked at Google X, and had obviously come across these sort of. These magical things. And they are. They're so magical, in fact, that when I think it was very much like the door to Diagon Alley, I was a muggle and looking at where this door reputedly was, and I refused to see it. And he said, there's this thing where you can take a computation and you've done the computation and you can prove it to someone, but you don't need to give them the whole computation, you can just make this claim and through this magic, they can be totally convinced that you did it, and you never need to show them any of what we would normally regard as the evidence.
00:05:53.284 - 00:06:18.306, Speaker A: And this seemed totally. And I obviously thought, well, he's two pints of beer to the good, and he obviously doesn't know what he's talking about. And this is completely mad. And then I. It wasn't until joining this entrepreneur first accelerator with Sac that we then downloaded the zcash. I would say zcash. I'm still trying to say zcash, but I think I'll accept a feat on this.
00:06:18.306 - 00:07:21.052, Speaker A: The zcash explainers on the Pinocchio protocol, which was the thing that described the protocol that first gave privacy to Zcashenhe, the pioneers in using zero knowledge proofs on blockchains was zcash, and it was for privacy, not for scaling. As we were reading these notes now, actually turned out those notes have been written by Ariel Gabazon, who later became chief scientist of Aztec. Not that he would have given us a second look back then. We were all amateurs. I mean, Zack was quickly becoming an expert from a standing start, but, yeah, maybe just to go a little bit back into that, the history of ZKP's, their first use on chain was for privacy, but they were actually originally a scaling solution for mathematics in general. And for this, you have to go right way back to, arguably, to the kind of seventies and eighties. So probably quite a few of you, some of your viewers, will have heard of a thing called the four color theorem.
00:07:21.052 - 00:07:41.822, Speaker A: So this idea, if I take any map of the world or any even theoretical map, it doesn't need to be a map in the real world. There's this amazing fact that I can color the map with four colors so that no two adjoining countries share the same color. So I can always do it in four colors. Three colors. It's not true. Five colors is definitely true. And it was very easy to prove.
00:07:41.822 - 00:08:21.400, Speaker A: And four colors, no one could prove it, but people kind of knew it was true. And there was fake proofs going around from as early as 1890 something. And then in the 1970s, these scandinavian mathematicians did manage to prove it, but the transcript ran to 500 pages, and you had to feed all these configurations through this mainframe computer. And so mathematicians said, well, we've got a bit of a crisis here. This thing appears to be true, or people are saying it's true, but we can't prove it. We have no ability to scale ourselves. We can't check a proof that runs to 500 pages and then look at all of the nuts and bolts in this main, in this computer and check whether it was correctly done.
00:08:21.400 - 00:09:26.662, Speaker A: So this is when the MIT mathematicians in the 1980s started wondering, is it possible to make a moral trade off? So if we're prepared to do not just the work, to do the computation that we're claiming, like we can color all maps in four colors, but much, much, much more work, is it possible to hand someone a proof that will allow them to check very cheaply, very quickly that you've done the computation that you said you had without having to go through the 500 pages of the proof or whatever else it is, or check the computer? Right. And so that's really where. So the Z case, or what we should actually call succinct proofs. So this is the ability to do lots of work to prove you've done a computation and have the world verify it very succinctly, that's where this work started. The fact it found its way onto blockchains in the form of privacy seems to be something of historical anomaly. But of course, it did show up in scaling much later. And this is actually what now underpins a lot of L2 technologies.
00:09:26.726 - 00:09:52.520, Speaker B: So, yeah, that's a fascinating overview. Before getting into some of the weeds, can you give a little bit of context on what we as an industry have accomplished in the last few years? We've had a number of different experiments in these spaces. What do you think we've really moved forward and what have we yet to accomplish? And what are the key bottlenecks for preventing further development?
00:09:52.680 - 00:10:45.194, Speaker A: Yeah, so what we have accomplished, I mean, but for blockchain, but for this industry, we would be a lot, lot further back than we are. It has been breathtaking the way that. What was the role, the mandate of the university faculty, the college faculty has transferred into private enterprise is absolutely exceptional. So let's go back to the 20 1314 time when these proofs have been used for privacy. So these protocols, one thing that was typical of these protocols is they required what's known as a trusted setup. I don't know how many of your viewers, I don't know how many of your viewers saw the aztec ignition ceremony, but there was actually a more famous ceremony called powers of tower. Anyway, with the aztec one, we had this big globe, and you could see this transcript flying around the world.
00:10:45.194 - 00:11:38.802, Speaker A: Really what that was was a way to create a mathematical shorthand that made the proofs more succinct. I can explain a little bit more about that, but you're basically creating high dimensional space inside of a mathematical point. Now, these trusted setup ceremonies, which are very, very good for creating succinctness and making sure that the proof is very, very short, which is good for scaling, because it means that everyone can check the next batch of transactions very cheaply. But they were typified by that setup ceremony, encoding the thing that you wanted to prove into the ceremony. So what it meant was the setup ceremony was only good for one type of computation. So these were not, in other words, universal. So supposing, for example, I wanted to do a particular type of ethereum transaction, I could just do the setup ceremony for that particular transaction.
00:11:38.802 - 00:12:20.480, Speaker A: But this is no good for scaling L2s, because every time someone wants to do a different transaction, you have to go and do the trusted setup again. So one thing that happened and was a kind of big breakthrough was, I think it was early 2019, when Sonic was released. So this was the first kind of universal snark that meant that there was one trusted setup for every potential computation. So that was kind of breakthrough number one. Then there was this thing called Planck, which was in late 2019, which actually came out of aztec. It was the work of Seth Williamson and Ariel Gabison. And this just created an order of magnitude improvement on sonic.
00:12:20.480 - 00:13:13.488, Speaker A: And then maybe the other thing to really shout about the last 18 months has been this breathless increase in what we call lookup arguments. Now, these look up arguments are very like the way that microchips were sped up in the early evolution of hardware. There were things called lookup tables, which were ways of avoiding having to have lots and lots of gates inside your microchip. And instead, for operations that would recur a lot, you'd have these very specialized gates, which would look up and check whether a value was inside a table. So this actually got imported into Planck quite early on. And we've now had this, I think there were six or seven strict improvements just in 2022 alone in the power of lookup arguments. And this has actually been really, really important.
00:13:13.488 - 00:14:35.210, Speaker A: And one of the reasons it's important ties back to the original design of Ethereum. So when Vitalik and Gav Wood created the original Ethereum virtual machine, they encoded for a certain type of number, a certain type of integer, 256 bits. Doesn't exactly matter what that means, but that is particularly bad at fitting inside snarks, which are the things, as it turns out, are really important to scale these blockchain networks. So what these lookup arguments are able to do is to translate between this binary world, which is how classical computing works, and how a lot of classical computing security is created and the world of snarks where everything happens in big sort of prime fields which just are not compatible at all. And these lookup arguments have managed to make it much easier to absorb these classical primitives of computing inside snark world. So I've given this little potted history in front of you as snarks. If you had Ellie Ben Sassen on here, he'd be telling all about starks, which actually are very important, by the way, for one big reason, which is they are quantum proof, right? So they only resolve to hashes, they make very minimal assumptions about their security.
00:14:35.210 - 00:14:50.410, Speaker A: And so as quantum computers start to become more powerful, stocks are going to have this big advantage in the sense that they are going to be protected against quantum computers, whereas snarks in their current format are not.
00:14:51.190 - 00:15:21.960, Speaker C: And I always try to like wrap that back, like the engineering side of things, back to the user standpoint. And maybe in context of succinct proofs in the blockchain world, how, I mean, some of those advancements that you listed today, how has that ultimately affected the blockchain industry? Either through privacy or I through scaling as kind of the main, one of the kind of the main use cases today.
00:15:22.300 - 00:16:15.232, Speaker A: Right? So, okay, so these techniques broadly, what they've made it possible to do is to prove bigger and bigger and bigger and bigger computations more and more efficiently. What that means is over time you can batch more and more and more transactions for the user and batch them into a single proof. And all those transactions are checkable very cheaply by an Ethereum node. Now the reason this trade off is really important and really worth making, you know, going from sort of raw computations to a prover that has to do loads of work and a verifier that has to do a tiny amount of work to check the transactions is because, you know, what do we want out of Ethereum? We want it to remain decentralized. So we want it still. We want Ethereum still to be digestible by the slowest node on the network. So if you're running a slow node, you should still be able to check that.
00:16:15.232 - 00:17:12.559, Speaker A: Ethereum is kind of true, has internal consistency, right? So what the zero knowledge snark or the succinct proof does is it allows a single aggregator, so we're going to talk about provers later in the program, allows a single aggregator to kind of batch together whole transactions and create a proof that proves those transactions are correct. And that might be, let's say 100,000 transactions at some point, right? And it produced a tiny proof that is as cheap to check as just one of the original transactions. This means now all those Ethereum nodes can still participate in the network, can still check that it is internally correct. What we've done is we've not really got away from the computational cost of processing these transactions. We've avoided all 100,000 plus Ethereum nodes, each having to check all the transactions. And we put the burden onto one sort of quote, unquote prover, so that those 100,000 verifiers can all know the network is correct. So we have made, in other words, an advance.
00:17:12.559 - 00:17:30.379, Speaker A: And that's really why the improvement in zkps is so important, because we now need to get the absolute cost of that prover work done by one person. We need that to go down and down and down and down to run more and more complex programs, to allow the Ethereum virtual machine to be more expressive. So we have more products.
00:17:31.900 - 00:18:57.690, Speaker C: Perfect. Maybe I'll even back up just one more time, and then we can start talking about different high level scaling solutions for layer ones. I think as a whole, for the industry, really what we're trying to do is have a shared ledger run on a arbitrary amount of computers that the community agrees upon on a certain fixed cost, whether that's an Ethereum node or a Solana node, or whatever kind of node requirements those may be, have enough replication so that it is sufficiently decentralized. And now the conversation has kind of shifted to the point to where how can we increase the capacity of the overall network while remaining kind of decentralized. And now we've kind of come to the point and the blockchain world where rollups has kind of entered center stage. There are different types of roll ups, but for this podcast specifically, we're going to be focused on succinct proofs or zero knowledge proofs for the podcast. So just adding a little bit more context on the industry kind of where we are, I think the viewers will be a little kind of up to date, but just want to add that color for anybody that's not as up to date with the industry.
00:18:57.690 - 00:19:18.340, Speaker C: So on your high level thoughts for layer one, what role do you think they play in the ecosystem more broadly? And then we'll kind of touch upon L2s and how you think they'll be utilized going forward in the industry as well?
00:19:18.720 - 00:20:04.392, Speaker A: Yes, I mean, I think probably most people sort of understand and agree to what is a layer one. It's just, it's an attestational thing. It's almost an evangelical thing. It's, you know, it's like the sort of, I think one of Essex investors, Charlie Somhurst, often analogizes the growth of the layer ones to sort of being like the various schisms and creation of early, early sort of religions of two, 3000 years. And all these things are basically attestational. You believe something more because there is more capital behind it and it appears to be more secure and harder to subvert its record of history. In this particular, so far this has all been economic history.
00:20:04.392 - 00:20:50.842, Speaker A: We don't really have anything sort of on chain or very few things on chain that do not have sort of mostly economic content. And so yeah, the layer one you need to know is going to be able to stay fueled, is going to have enough of a security fee to be able to pay for lots of capital to stand around and say this is the record of economic events. That's really what the layer one is doing. It's come attached typically, particularly from Ethereum onwards with this instruction set, basically a digital computer that determines how all of those transactions are recorded. I think the interesting thing that's now going to start to happen with the L2s. The L2 s are now offering all of these different microchips to the engineers. They can go and build all kinds of different programs.
00:20:50.842 - 00:21:43.320, Speaker A: The interesting question is going to be to watch is the original Ethereum virtual machine, is it going to be a kind of a relic? Are we going to, when we look at the original Ethereum virtual machine instead of ten years at layer one, is it going to be like going and looking for the Titanic on the sea floor? Is it still going to be an active use or is its role really just going to be to verify the other blockchains that it is now spawning and giving security to? Right, these L2s, by definition they're always servant to a layer one that says this is the record of economic history and rubber stamps all of the updates that are happening in these L2s. So it's very important to know that Ethereum and other layer ones are well secured in the long run and have really good models to make sure that people will keep on posting security capital against them.
00:21:44.380 - 00:22:04.960, Speaker C: And I think one thing the industry is also not very good at is ultimately defining security or even decentralization. In your words, how do you define security and then how do you define decentralization? And then we can kind of continue on with L2s and the different scaling or properties that they ultimately provide.
00:22:05.360 - 00:23:16.360, Speaker A: I guess you could define security as the sort of the cost of subverting or reversing or posting transactions that are not internally correct. Right. Is a reasonable sort of good guess for how securities measures. The trouble is, most definitions will lead you then into this sort of bitcoin versus ether debate, right? And obviously, since several months ago, the way that Ethereum and bitcoin underwrite their security is now totally different. So bitcoin proof of work and ethereum proof of stake, and there is this kind of recursive truth, I think Vitalik calls it weak subjectivity that is sort of baked into the structure of a proof of stake that makes bitcoin. People say, well, bitcoin is if you give me a quote, unquote fake version of bitcoin, or you give me a version of bitcoin and then it turns out there's another version of bitcoin with more work on it that's still internally consistent. You, without going out and looking at anyone else's current record of that state, can just determine which of those is the quote unquote truer.
00:23:16.360 - 00:23:29.230, Speaker A: And this doesn't quite work in the same way for proof of stake. So I've given you a definition, but also know that there are some things outside the model that people, bitcoin versus ether people will debate on ad infinitum.
00:23:29.810 - 00:23:38.910, Speaker C: And then on the decentralization front, how do you view that? Because I think that can also be different from just like generalized security of the network.
00:23:39.810 - 00:24:43.466, Speaker A: Yeah. So, I mean, there's, what ethereum is doing is breaking different roles into different pieces. And so for example, what does the ZK proof? You could say that for the same, for an increased level of transaction throughput, it improves decentralization because more people can check the state of the network now because it costs a lot less computational power to know that a particular update is true. Right. So, but of course, what it also does is it puts into the hands of some more centralized people the power to potentially order transactions because it's, you know, it's harder, for example, to do sequencing because there are now more transactions in the network, or it's harder to do proving, because the proving is the thing that proves that the sequence, the sequencing has been done in a way that is allowed by the logical rules of the system. And both of those are now going to be increasingly very high intensity. And so it means fewer people can do it.
00:24:43.466 - 00:25:00.990, Speaker A: So it's kind of what it's done is it's made. So read access verified, read access cheaper, but write access has a higher threshold of computational power required, and therefore, you could say, could end up, unless you're very careful being a centralizing force.
00:25:02.170 - 00:25:40.296, Speaker C: Gotcha. Interesting. Okay, cool. And then I think on a high level, I mean, so all these networks kind of have a certain amount of throughput. That throughput for majority of networks is relatively small. And what we have started to utilize these different roll ups and kind of the different varieties of them is to be able to batch transactions, whether an arbitrary amount of transactions, into one transaction. And then that single transaction is posted on the base layer of whether that's ethereum or whatever network.
00:25:40.296 - 00:26:25.990, Speaker C: And that ultimately, as you're saying, increases transaction throughput and allows the individual nodes on the network to only verify that one transaction, then all these hundreds of thousands or transactions that the roll up ultimately put together to submit to the network. So from that point of view, maybe touch upon briefly how we ultimately ended up today on kind of. I think there's primarily two dominant forms of roll ups that exist, optimistic roll ups or fraud proofs and validity proofs. Could you talk about each one of those kind of their pros and cons? And then we'll kind of get more into the zero knowledge flavor, which is where you've optimized for or specialized in.
00:26:26.450 - 00:27:25.744, Speaker A: Sure. Yeah. I mean, so briefly, there's, as you say, there's kind of this optimistic version of scaling where the idea is that transactions will be batched and put on chain in a centralized way. I say in a centralized way. There'll be someone at any one moment who's batching these transactions and putting them on chain. And then the world is then turned into a sort of a watchtower on that blockchain to sort of sift through everything that's been done and check whether there was an internal error. So now this is predicated on a model where the people who are normally securing the network, they post some collateral, they put some money up, and if they have approved a transaction that actually is internally inconsistent in some way, should not be allowed by the logic of the network, they will then be punished and slashed.
00:27:25.744 - 00:28:14.030, Speaker A: Now, the really hard thing about this is we don't really have a good model for knowing how incentivized people will be given a certain amount of incentive to go and check that network. Right? There's not exactly, it's much more a bounty program than it is a kind of a dependable fee structure. And so the hard thing is, it's very hard to reason about the security of optimistic networks, but they do allow you to go to the races, really quickly. So they have some great advantages, basically, for just bootstrapping the system and getting it up and running. The kind of the more formal version of doing this is what we call this, ok, roll ups. This is where you actually create proofs. So at the point of which a state update is given and some transactions are batched, it is accompanied by one of these really expensive to create proofs.
00:28:14.030 - 00:28:50.420, Speaker A: And then the network is able to verify at the point of finality, it says, oh yeah, we see your update. We see that Logan's balance has gone up by such and such, and Eric's balance gone down by such and such, and Tom's balance, etcetera. And we also see a proof that this was internally consistent. So great, we can verify it immediately. And so what this means is you don't require this, what we call sort of unbonding or sort of slashing period where the world has to just sort of make sure that all this stuff made sense. You can just know it's true or you know it's internally consistent and you can rely upon it straight away. But of course you have a much higher cost of creating that block in the first place.
00:28:51.480 - 00:29:26.910, Speaker C: Perfect. Those are high level, two different ways of batching these protocols, or batching these transactions into the base layer protocol. Could you also go into a little bit on the proverb and sequencers and just touch upon how each of those work? And then I would love to dive into one like the different virtual machines, and then also the data availability layers. I think that would be an interesting topic.
00:29:29.330 - 00:29:51.996, Speaker A: First of all, proving versus sequencing. So these kind of. It's kind of quite surprising when people first talk about the difference between proving and sequencing. It sounds like it ought to be part of the same function. And it kind of, in its original idea, you know, would have been. But they've been separated out for reasons which I'll explain in a second. So let's suppose the three of us are running our L2 networks.
00:29:51.996 - 00:30:16.210, Speaker A: This means we've all put up some stakes so that we can participate as computational aggregators. We're taking people's transactions and we're sort of batching them together. There's two things in that process that we need to do. So let's suppose it's my turn to create the next block out of the three of us. So I'm going to look into what we call the mempool. This is just transactions that people have sent out. They want them to be executed, but they haven't yet.
00:30:16.210 - 00:31:12.728, Speaker A: And I've got to do two things one is line the transactions up in a row and work out what is the resulting state of Ethereum. As a result of that, what's happened to all the users balances? And the second is, remember, this is not an optimistic roll up, it's a k roll up. I've then got to create this really expensive proof, which is much more expensive than executing those original transactions, this huge proof, which is going to create this tiny certificate, which then Ethereum can cheaply validate. Those are the two components of the proof. The idea really is that the prover work, whilst a lot more work. It takes a lot more computational resources to create that ZK proof, but there's a lot less bad stuff that you can do in your role as a prover. And one reason is you can't do much to muck around with the order of the transactions.
00:31:12.728 - 00:31:47.720, Speaker A: The only thing that you can do is decide not to propagate the proof in the first place. Right, but it's in the sequencing and the ordering the transactions that I'm sure your viewers have heard lots about MeV, but that's actually where MeV can be executed. And that's why decentralizing the sequencer is so important, because you can really muck around with people's economics. You can put a big trade in on uniswap, move the price, have another transaction that executes at a totally different price as a consequence. And so that's why sequencing is such an important topic for decentralization roll ups, maybe.
00:31:47.760 - 00:32:16.680, Speaker C: On the topic of decentralizing sequencers, how do you, I mean, with MeV being such an important topic, how do you see decentralization ultimately playing out and kind of the different roles or different ways to decentralize sequencers? And then at what point do you think it is decentralized enough from like a L2 standpoint, this is such a hard question.
00:32:17.580 - 00:33:14.810, Speaker A: I'm still very much on the fence about how big a future. There are a lot of great intellects out there who say MeV is going to be persistently this enormous sort of, you know, sub economy of crypto. I'm slightly in two minds, but I can't work out whether this looks like the afterglow of the big Bang and over time background radiation just dissipates, or whether this is going to be something that's sort of persistent. I think also, by the way, people talk a lot about L2s, and they also talk a lot about layer threes. The more that we actually silo computations into, you know, we've got an aave application over here, and we've got some other application here and some other application here. If we have more of these sort of economic silos where transactions are being batched locally around an application, I don't know how big MEV is as a consequence of that. It might be, actually that the size of MEV drops.
00:33:14.810 - 00:33:37.570, Speaker A: So that's kind of one thing to say. I don't even know how big the MEV is, is it's something that's just really exciting now. Like the early days of hedge funds, for example, which actually hedge funds are much harder to operate now than they were 10, 15, 20 years ago when the big arb craves were available. Actually, by the way, I've just lost my train of thought.
00:33:38.350 - 00:34:18.912, Speaker C: In general, how decentralized do you think sequencers should be? What are the different ways? I think one of the big benefits of having a single sequencer was the ability to not arrive at consensus and kind of remove the consensus overhead from the base layer. And so I'm curious, once you start adding more sequencers or trying to decentralize sequencers, one, like how does that, what do you think is enough decentralization? And then two, if you do start decentralizing sequencers, do you run into the same problems that you do kind of at the base layer, right?
00:34:18.936 - 00:34:43.820, Speaker A: I mean, I imagine you can only measure failures of decentralization extension. In other words, watch the network. Has something bad happened? Fix it. Modeling is incredibly difficult. I mean, look, there are certainly two decentralized settings for sequencing. One is L2 by L2. Each one just looks after the decentralization of its own sequencer.
00:34:43.820 - 00:35:20.700, Speaker A: That might end up being fine. It might also be an advantage because there might be some environments where, for example, here's a good example of where a sequencer can't do that much. That's bad. Suppose you're in a sort of decentralized social media application, right? The ordering of messages is important, but it's not. If my message appears on the, on the site slightly after yours. There might be issues around provenance and ordering of messages, but it's not calamitous. On the other hand, if we're talking about Uniswap, sequencing matters a great deal.
00:35:20.700 - 00:36:20.088, Speaker A: So it might be that different L2s, if they're hosting different types of applications, will care more or less about decentralization of sequencing, and might care actually more about volume and getting lots of transactions through than they do about decentralization sequencer. There are obviously some benefits to what we call shared sequencing. I don't know if this would be a familiar concept, but this is basically the model where inside a given layer one, all L2s tap into the same L2 sequencer. Now this is quite useful because first of all, it allows L2 s not to get their hands dirty. They actually don't need to worry about building this sequencing decentralization tech, which is an advantageous. It also means that the layer one, as you alluded to Logan, gets to essentially re centralize, or at least by its own definition, the sequencing of all transactions across its own L2s. Right.
00:36:20.088 - 00:36:59.950, Speaker A: This is quite good for its quote unquote value capture number one. And it also allows potentially some synchrony between the L2. So obviously one cost that we all pay with L2 is I've got some concept of state over here and whole load of applications and some concept over here, another one over here. And there is some balkanization, splitting out of economic activity between all of these separate networks. So shared sequencing may be a way to kind of bind those economic events back together so that an application on roll up a kind of has some concept of what's going on on roll up b and vice versa.
00:37:01.970 - 00:37:41.580, Speaker C: That makes sense, I think. Interesting. So maybe if I were to summarize all this again, I mean, there's a certain amount of throughput on Ethereum, I think. What are blocks today? Between like 80 and 100 kb, which is not very much data. I think with EIP 4844, the blocks become. I may get this wrong, but I think it's between one and two megabytes on average, every 12 seconds. And then when full Ethereum roadmap is executed, there'll be like 1.3
00:37:41.580 - 00:38:14.740, Speaker C: megabytes of data, ultimately per second or equivalent. And ultimately what we're trying to do here is increase the throughput of these networks by batching these transactions to the base layer. And the proofs are aggregate, the sequencers are aggregating it. And we're trying to now get into the decentralization of L2s, because right now, in most cases, they're only a single sequencer.
00:38:15.650 - 00:38:44.110, Speaker A: Right. And so actually this starts to get into the territory now of what we call data availability. Sorry. There are two things you care about. One is how many transactions can I check to correct and put in order? And that kind of thing. Right? And then the other is, quote, very, very roughly and hand wavingly, how securely are they stored? Right? So it's a bit like what's going on your computer. So if you've got the process a bit that's actually doing the number crunching, and then you've got the stuff that's actually storing your important information.
00:38:44.110 - 00:40:22.226, Speaker A: And so this is where there's a lot of experimentation going on, realizing that what all these roll ups are doing, zero knowledge proofs, they're making execution generally a lot cheaper for a given level of decentralization. So checking that these transactions are internally correct and putting them in order. But the default L2 model puts all the information on chain, so you're paying for those hundreds of thousands of nodes all to store the same stuff. Now, again, let's go back to decentralized social media. Is it really important for decentralized social media that every terrible tweet I send out is stored by 140,000 nodes? Probably that's not a great use of their capacity. And so again, this sort of, this notion of modularity on the data availability side is hopefully at some point the application writer of the future is going to be able to say, how quickly do I need my transactions to get to Mainnet? Because this is going to tell you, how many tiers of roll ups can I afford to wait before my transaction goes through? And the other thing the application writer should also have control of is how much redundant storage is protecting the information from my users, right? I mean, if it's kind of a few dozen or a few hundred or a few thousand nodes, that sounds pretty good. As long as it's a decentralized network, probably the information is not going to be lost.
00:40:22.226 - 00:40:41.680, Speaker A: But I'm not really in a mood to pay for the huge data storage offering that Ethereum is giving me at layer one. And so these are the two kind of essentially axes on which people are prepared to loosen assumptions so that networks reflect what are the actual security requirements of the application.
00:40:43.340 - 00:41:41.738, Speaker C: Maybe talking slightly on the data availability layers, Ethereum on a high level is kind of transitioning to becoming more of a data availability layer and over time increasing the amount of data that the network can ultimately propagate to all these nodes. But now there is another topic like volitions and these separate data availability layers that these roll ups can settle to. Can you speak to those briefly? Because I think now with Ethereum and right now the data being expensive, and as more people kind of ultimately use Ethereum, over time, that data will still be very hot commodity. And now we're exploring like these volitions and other data availability layers, can you talk briefly like how those work?
00:41:41.874 - 00:42:47.200, Speaker A: The distinction between roll up, validium, volition, which are all quite technical sounding words, but they really come back to what I was just, what we were just covering before about the difference between transaction execution and then the storage of the resultant information, right. So the actual computer processing to check the update and then the storage of that information. So the default roll up just says everything gets stored and executed everywhere, right. So the standard roll up was just, we're not going to undermine any of these security assumptions that's been made on layer one, and we're going to paste all the information there and we're just going to use some trick to get up the rate of the number of transactions we can get through the system in any one go. Okay, so that's kind of the default roll up. That's very much the most non modular expression of a roll up that you can find. Then there came this concept of a validium.
00:42:47.200 - 00:43:43.130, Speaker A: So validium was, I think this expression was first used by Starkware. I might be wrong, but I think it was starkware which said, okay, so you're still gonna have the same guarantees over the internal logic. Like was all the stuff correct? That bit is all going to be checked by layer one. Okay, so there's no reduction in assumptions there, but we really can't afford for all of the data to go to layer one because it's getting really expensive because of all the good work we did in scaling in the first place. Right. It's now the data that's dominating and not, not the execution. So this concept of validity meant that there would be a sort of a somewhat decentralized, but more proprietary group of, or sort of a more centralized group of storage providers who on command, will send the storage, the information about the roll up that is relevant to the next block will send it back so that the next block can be produced.
00:43:43.130 - 00:44:43.490, Speaker A: That then became this idea of volition, which also connects to modern what we're seeing is l three designs, layer three designs, which is where really the application needs to be able to determine how expensive is it to store stuff and how expensive is it to do processing. The example I offered before, which is the decentralized social media app, cares somewhat about ordering and cares not very much about storage as long as there are pretty good guarantees around that storage. So the ability to sort of decide what kind of data availability guarantee you are willing to pay for. If you're doing transactions that are all $100,000 plus transactions, you can pay for all the data availability that anyone can. But if you're just posting stuff to lens, you obviously cannot afford to make the same level of payment and therefore you don't want to pay for the same level of guarantee.
00:44:44.510 - 00:45:25.774, Speaker C: Yeah. Yeah. Interesting. Yeah. No, I mean, at the end of the day, I mean, it all gets fairly technical, but I think really what we're trying to achieve in blockchains more holistically is how do we increase kind of user adoption and I think, or add kind of the privacy component, going back to like the beginning of the podcast, which we spoke about and kind of how you got pulled in initially. And so the industry is like, very funny today because everybody's trying these different solutions. On one hand, you have like the monolithic kind of like hyper scaled layer ones, like the Solana approach.
00:45:25.774 - 00:46:07.370, Speaker C: Ethereum is taking the l two roadmap. You have cosmos with their zones and subnets or avalanche with the subnets. And so everybody's kind of taking these different approaches. And your point of view, I mean, Ethereum now is talking about these layer threes, and in some cases, I've heard love layer four s. Where do you think the end state is of all this? Do you think we're just going to continue to try a bunch of these different scaling solutions and see what works? Or how do you think about it, one, from the technical point of view, and then how do you think about it as an actual user of these networks and actually using these applications? Where do you think it ends up?
00:46:07.910 - 00:47:00.506, Speaker A: Right? So, well, first of all, where do I think is the strategic advantage between all these layer ones? For me, what Ethereum is doing is running an accelerator for digital microchips. It is offering the world so many different choices with so many different security assumptions. You've got optimism, you've got arbitrum. Okay? These are all doing some EVM stuff, got scroll doing EVM stuff. But with CK guarantees, you've got, you know, Risc zero offering Risc five architecture. And then the good thing about that is, you know, web two engineers can come in and they have a language that's familiar to them, right? It's basically we have now almost a sort of a digital semiconductor industry. And a lot of it, not all of it, but a lot of it is happening on Ethereum.
00:47:00.506 - 00:47:53.730, Speaker A: And that means that Ethereum can run multiple experiments, work out what gets adoption with the engineer, who, by the way, and this has been the weird confusion over the past few years, is something that I've kind of really struggled to understand, is I don't really understand why the end user cares about the L2. This should be so far out of sight, so deeply buried down through the layers of infrastructure. Most users don't care about which chip is inside their computer or the arrangement of the circuitry, they don't care about that. But the weird thing here is that somehow the L2 is reached through to the user. But really the focus, the entire focus of these L2s has to be on the engineer, because it's going to be the engineer that's going to determine their future success. So that's why I really like the Ethereum model. The Ethereum model is now having made this final, large, enormous change.
00:47:53.730 - 00:48:27.142, Speaker A: Or there's actually this, as you say, there's one to go, 4844. But I think these will become less and less frequent. This layer one is now carrying so much capital on it, it really can't afford to do too many kind of mid air rebuildings of the aircraft. And it's actually just got to say, okay, this is something like the final architecture. And the innovation travels up further and further and further through the tiers of infrastructure until the infrastructure is speaking directly to the engineer. I don't know when that should be, but possibly it's L2. Very likely it's layer three.
00:48:27.142 - 00:49:03.196, Speaker A: Layer three is basically where you're operating on L2 that is posting all this data back to layer one. And the layer three says, no, sorry, that's way too expensive. So we're going to loosen some further assumptions on the security that we're offering you. But I have to say, I do really like this experimental platform that ethereum has kind of accidentally built. It wasn't always supposed to be like this. Lots of people remember the old sharding models of Ethereum. It's kind of locked its way into this grand experiment.
00:49:03.196 - 00:49:06.760, Speaker A: But I really like that for its future proofing.
00:49:07.540 - 00:49:53.618, Speaker C: Yeah, it is. It's very interesting how Ethereum ultimately got here and I ultimately fascinated by Ethereum, the l two space and even these other ecosystems, just all of them taking different approaches and seeing what works on that front. I mean, we haven't talked too much about Aztec, specifically what you originally kind of got pulled into the industry, into. Bye. Building an encrypted L2. So more privacy focused. Can you talk a little bit more about that and kind of the vision there? And then even more so, I think they just had an announcement that they were going to sunset at least part of the application.
00:49:53.618 - 00:49:55.410, Speaker C: And could you speak to that as well?
00:49:55.530 - 00:50:30.484, Speaker A: Yes, also I've got to be very careful because I left the company actually two years ago. Can't quite believe it's been so long. So. So, yeah, I mean, obviously it's in some sense it's a sad moment to see Aztec connect sunsetted, but actually, this comes back to something I was just saying before, which is these L2s, their main job is going to be to speak to the engineer and get the engineer to build wonderful applications for the user. And the engineer has got to win the user and not the L2. So that's kind of the first thing. And I guess looking at their announcement, this announcement really is in line with that.
00:50:30.484 - 00:51:08.146, Speaker A: They're building this thing noir, which is this amazing rust like privacy language. And I can totally see that that's where all their focus now needs to be. I think also, Aztec, I guess, had to sort of make it through the realms of investment funding. And to some extent, that meant that you needed to interact directly with the end user. Right. You needed to sort of show some traction. But it's been the fact that the research and development cycles for companies like Aztec have been so long and longer because they had to build it all from scratch.
00:51:08.146 - 00:51:49.240, Speaker A: They were building a city in the sand. They had to build all these foundations that just looked completely crazy and impossible. And to give credit to the Aztec team, they did a lot of this under their own steam, including the creation of Planck. Right. So, first of all, you know, privacy, why is privacy going to be really important? It's actually probably better than anything a matter for practical personal security. We are broadcasting a lot of information for all this transparency gain that we get by transacting on chain, that transparency is part of the security, but also part of the lack of security for the individual users. It's a pretty important thing.
00:51:49.240 - 00:52:21.314, Speaker A: One interesting feature of privacy that I guess it seems that this is what Aztec is kind of navigating. Is it to some extent limits the expression in what you can do in terms of building smart contracts. I always use the illustrative example. It's kind of boring, but it's also the clearest example. You go and try and build makerdao completely in the dark. Everything is hidden. So the collateral is hidden, the debt is hidden, the amount of Dai, all this stuff is hidden.
00:52:21.314 - 00:53:17.598, Speaker A: Right. So you end up with some interesting consequences on the financial stability of that system. One is how do you make sure the system is in positive equity? So a little bit like Credit Suisse to talk topically, if all of your securities are held to market securities, how do you tell whether the equity is underwater here in the aztec system? It's all completely hidden. How do you tell a ZK makerdao is in good financial health? Well, you need the people who are borrowing from the system to create a load of proofs every few seconds or minutes to show they still have enough collateral to pay off the debt. Right. And then what happens if they stop producing those proofs? You need someone to go in and recover the collateral. How? Because the only person who knows the collateral that was inside the system was the borrower in the first place, and they are the credit risk.
00:53:17.598 - 00:54:28.300, Speaker A: So you can see that for that reason, there's a load of pieces of core financial infrastructure that you can build in daylight using sort of a classic public blockchain. But the moment it goes private, it's kind of harder. So privacy has this interesting feature where it can probably do quite a lot for bilateral transactions. And there's probably a lot of applications actually in gaming where, for example, you actually can't have a fun game unless you can hide some of the traits around the characters or entities or whatever that you're using to do your bidding in that game. So I think if I had to sort of bet, I think where aztec and noir are likely to find massive adoption is probably going to be in gaming some elements of DeFi, but it's very interesting because they're addressing L2 in a totally different way to everyone else. I guess that's probably all I can and should say about Aztec, but yes, sad to see Aztec Connect go, but I think the developer platform is just going to make that no more than a distant memory.
00:54:28.800 - 00:55:12.290, Speaker C: Going back to what you said initially, these L2 ecosystems being built for engineers, what would you say to, I guess now it's kind of like a competition for engineers to build in that specific ecosystem, a war for the developers. What would you say to an engineer to build an Aztec? Or optimism or arbitrum? Or like, what is your specific thing to build one specific ecosystem versus any of the other ecosystems? Or an l three or l four? What's your sales pitch to them to build in a specific ecosystem?
00:55:12.410 - 00:56:19.164, Speaker A: Look, if security is not going to matter to you all that early and what you want to build is some sort of non financial application, or a financial application that isn't going to accidentally find that it's sitting on billions of dollars of assets quickly, you probably can accept low levels of security. So that's one trade off you might be thinking about is how can I just get fast transactions? I get confirmed times quite quickly. I'm less worried about doing the fine calculus of security. So DeSOC might be sort of very, very decentralized social media, a very good example of that. There are some contracts where, for example, there is already good precedent, good burn in for EVM contracts like the aaves, the maker Daos, where these are major pieces of capital creating infrastructure. They're creating the money, essentially, that's going to fuel the rest of these systems. Obviously, it is a huge advantage that complex systems like that are able to, without any rewrite of code, go onto a ZKVM.
00:56:19.164 - 00:57:06.380, Speaker A: So in that instance, you probably have very little choice but to select a secure ZKVM that requires no rewrite of your contract code. So you might advise, are they maker that they should make a beeline for those destinations? And then there might be people who say, well, actually, I've only got a background in rust. Okay, well, in that case, go and try out this RISC V architecture. Because actually, if that's what you know and that's the code you're familiar with and happy with, go and try an environment where you actually recognize the smart contract interface. So I guess, you know, there's no one fit for any one engineer. And of course, actually for gaming, by the way. Yeah, you probably do want some access to private stage.
00:57:06.380 - 00:57:38.144, Speaker A: You probably want to be able to make some proof that you have a power level that's over seven, but not tell people exactly what the power level is. Or if you're doing identity systems range proofs, proving that your date of birth is predates a certain date that makes you over 18, but not giving away what your actual date of birth is, and private data. So I think it really depends on the context of the engineer. So that's not a particularly, particularly unambiguous answer, but it really depends on the team.
00:57:38.192 - 00:58:03.360, Speaker C: And maybe real briefly with arbitrum announcing that they're going to do an airdrop, maybe not commenting on that specifically, but what are your thoughts on tokens in L2s? And like, what role do tokens or the incentives of these ecosystems having their own native. Not currency, but, yeah, own incentives, I would say.
00:58:04.100 - 00:59:29.542, Speaker A: So I have particular kind of token allergens that flare up once every four years, and it's definitely happened this past cycle. Look, here's the really hard thing about L2s, is that in the long run, what is their game? Their game is going to be how can I attract the maximum amount of validator capital to secure my network for a given level of feed that I can generate? If you think about this and its correspondence with the sovereign bond markets, if the. If some country with its own currency borrows in its native currency, it pays a really high rate of interest on the debt versus if it issues that bond in dollars because it has to pay the investor, the rational investor, for the increased volatility in the instrument, because they're in this currency, they don't know, versus the dollar, which they do know. I think a similar phenomenon in the very long run is going to allocate to both layer ones and L2s, right? Which is, and let's take the particular instance of the L2 here. The L2 might through a mechanism like Eigen layer. By the way, I have no stake in Eigen layer, so this is, I'm a huge fan of team. I think what they've done is it makes a great deal of sense.
00:59:29.542 - 01:01:05.138, Speaker A: And through a system like Eigen layer, there might be an option to post the token, the native token of the l two validated capital, and also to post ether. And these both are doing the role of securing the network, right? But the interesting thing is the ether is probably going to be the higher liquidity and lower volatility denominating asset for that stake, right? What does that mean? It's going to create a two speed system, because the ETh denominated stake might only require a 2% return on capital over the year, but the native token might require four or five or six or seven or 8% or more, because it doesn't have the rich liquidity. And relative, these things are all relative, but the relative price stability of ether. And therefore, I wonder whether in the long run we're actually all going to graduate towards a model in which basically in any l two on ether, on ethereum, you have to post ether because it is the cheaper form of capital. In other words, the network for the same level security fee gets to attract way more capital than it does in its own native token. That's an interesting. The flip side though, which actually was made clear to me the other day by someone I was speaking to on this subject, and they're saying, yeah, here's the problem, is that currently the cost of operating the network is simply too high, particularly for ZK rollups, to be able to, not to have to reward investors with very, very, very high early returns.
01:01:05.138 - 01:01:51.530, Speaker A: And therefore, possibly we're in this interesting situation where to get a L2 to maturity, you need the token with the incredibly high potential capital return that the token offers. And yet the long run punchline is everything gets staked in ether and the token has no function in the network. This is a very weird paradox, because it means that to get to market, to win against your competitors, you have to promise the token, and yet to be competitive with your competitors in the long run, you have to take validated capital in ETH. So what does that mean for the lifecycle of the token? I think that's a question that everyone issuing a token does need to ask themselves. Are you doing it for the short run or are you doing it for the long run?
01:01:52.790 - 01:02:36.330, Speaker C: Lots to think about there. Yeah, definitely will be interesting to see how the tokenomics of these different L2s evolve. Will be very fascinating to see. They definitely have kind of their, at least initially, their own incentives in mind, which can kind of pervert some of the original thought process of joining with layer one. But very interesting. Cool. Well, and maybe just kind of wrapping up the podcast, speaking with some of the work that you're doing today with geometry and kind of what you're looking out for there.
01:02:36.630 - 01:03:14.848, Speaker A: Yeah, sure. So, geometry, as I said before, we're a research house and also an investor. We see invest very early stage in companies that have a very high concentration of R and D requirements. They need to be leveraging research in some way. We actually do quite a lot of that research internally at geometry as well. We're incredibly lucky to have this extraordinary team who've come from all corners of the ZK world, from sea cash, from the Ethereum foundation, various other places, semaphore. We've been doing quite a lot recently.
01:03:14.848 - 01:03:43.606, Speaker A: I say, this is the royal we. I can just about read through this stuff. I certainly can't originate it. So I'm not responsible for any of this stuff that Kobe Gerken and his amazing team do. But recently, some stuff that's come out of geometry is on a thing called folding schemes. These are also quite technical subjects, quite dry in certain ways, but all comes back to how do you continue to hyper enrich these encrypted substances with more and more and more and more information to make ZK proofs more efficient. Right.
01:03:43.606 - 01:04:18.696, Speaker A: So it's all about the enrichment of encrypted numbers and putting more and more structure into them. And folding is one technique that's kind of come about over the last year, year and a half that geometry has contributed quite a lot to. We've done some stuff, actually with privacy. So not the kind of general purpose privacy that Aztec does, but things like a library that allows you to kind of privately shuffle cards. So you could use that, for example, for a poker game. And generally, everything that geometry does is sort of open sourced. It's for the community, and we always want it to stay that way.
01:04:18.696 - 01:04:46.360, Speaker A: And it's allowed us to also to do really fun things like do collaborative research with our portfolio companies. And one's a sort of microchip company or semiconductor company making ZK proofs faster at the hardware layer. One is scroll who are doing ZKVM, and it allows for a very sort of rich and interesting intellectual life. So it's a really fun way to run something that's basically trying not to look like a venture fund as much as humanly possible.
01:04:47.380 - 01:05:23.438, Speaker C: Wonderful, wonderful. No, I definitely appreciate the nuance discussion and spicy kind of take on tokens at the end, but no, I definitely appreciate all your thoughts. I think you were named in 2021, the 45th most notable person in crypto, and so I hope that ultimately continues to rise. And what you're doing ultimately on the fund aspect, helping the different teams, helping on the research side. But no, I really appreciate the conversation and all.
01:05:23.574 - 01:05:42.200, Speaker A: I will say there are one or two other people on that list who I'm nothing. I'm not sure whether that list dignifies me or not, but you should take a look at that 2021 list again and see some of the other people on that list. I don't know. I don't know what it really meant. But yeah, it was quite fun.
01:05:43.580 - 01:06:06.740, Speaker B: And Tom, just to close out for builders who are researchers who are listening to this and looking for where is the white space or where should they be spending more time? Where are you looking to? If you could wave a wand and direct more attention into any, into any sort of subspace or field that you're paying attention to, where would you want them to spend more time?
01:06:07.320 - 01:07:42.186, Speaker A: So I say this is someone who last played lemmings in 1990 something, 89, and I'm not a gamer, but I do think that there is a very high likelihood that mature DeFi is actually not going to find its final expression in options on chain or futures on chain or more and more complex packages, speculative packages, and instead is going to find maturity in the very same way, by the way, that original options markets and other synthetic markets sprang up, which was in response to demand from people who are farming to protect against their crop production or aircraft, people building sort of airlines and wanting to hedge against fuel costs. At the end of the day, Defi is most useful. The speculative uses kind of should be a sort of a side application, right? People taking the other side of a really useful trade. But the really useful trades are all in hedging. So I wonder whether DeFi is going to find its product market fit most emphatically in on chain gaming where there is production of on chain commodities. There are interesting macroeconomic trades that can be run. You know, probably amongst all of us, there are sort of frustrated would be governors of central banks, finance ministers.
01:07:42.186 - 01:08:29.980, Speaker A: This is stuff, actually, that you can replicate on chain. And you could see a world in which sort of macroeconomic warfare is the expression that I'd like to use for this, essentially risk, but with risk the board game, but with real financial consequences and long term financial strategies. It's probably in that context that I think DeFi will find its final fit. So therefore, my sense is people would be using their time well. By looking at on chain gaming, I think we're increasingly going to see the application of inference models as vessels for economic retention. So basically like operating like companies or stables of horses in the real world. Right.
01:08:29.980 - 01:09:01.340, Speaker A: And I think we will also find in there very interesting applications of everything from amms to borrow lend protocols, servicing the real needs of users, and of course, synthetic options, servicing the needs of real users who need to make their financial way in an on chain gaming environment. And so I really do think that's where we should be staring and probably rather less at just sort of replicating financial privileges we've already seen in the banking system and copied and pasted on chain.
01:09:04.200 - 01:09:13.720, Speaker C: Makes sense. Cool. Well, thank you again, Tom. I really appreciate your time sharing your knowledge with us and the broader community. So appreciate it.
01:09:13.840 - 01:09:15.490, Speaker A: Thanks, Logan. Thanks, Eriche.
