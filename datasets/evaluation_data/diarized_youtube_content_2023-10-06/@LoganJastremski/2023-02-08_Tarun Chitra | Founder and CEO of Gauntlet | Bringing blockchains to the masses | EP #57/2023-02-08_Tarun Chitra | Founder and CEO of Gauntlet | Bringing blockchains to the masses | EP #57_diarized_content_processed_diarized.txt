00:00:00.960 - 00:00:12.046, Speaker A: Well, thank you so much for joining me on the podcast, Jeroen. Super excited to have you here. Big, big fan of what you have done, and so really honored to get the chance to speak with you.
00:00:12.238 - 00:00:14.662, Speaker B: Hey, happy to be here. Excited to chat.
00:00:14.806 - 00:00:33.950, Speaker A: Thank you. I would love. So I start each podcast with a little bit about everybody's background, their unique story, and falling down the rabbit hole. I feel like it's kind of endless once you peer in the hole. And so I would love to hear a little bit more about your journey and just getting into crypto more broadly.
00:00:34.810 - 00:01:13.170, Speaker B: Yeah, for sure. So I think in 2010 and eleven, I worked at a company that was building asics application specific integrated circuits. That's like hardware, custom hardware. Not at all for crypto, not at all for mining. It was actually for doing simulations of proteins. But at that time, we put out this chip order for $25 million and the supplier ghosted us. And there were not a lot of people buying chips at that time.
00:01:13.170 - 00:01:58.878, Speaker B: So it was very, very weird. And it became this question of, who is this person? Who are these people who are buying up custom chip space? It's very different right now. I think. I'm sure many of your listeners heard of the semiconductor shortage and all this type of stuff, and part of that has to do with the fact that demand for semiconductors has gone up because of lidar, self driving car chips, whatever. There's a ton. There's way more custom silicon than there was back then, even though it's only ten years ago or so. But so we were just like, what is this? And then we found out eventually they, like our supplier, came back, we'll give you a 10% discount.
00:01:58.878 - 00:03:10.030, Speaker B: And we were like, fuck you. You have to tell us why you disappeared for six months. And that sort of is what led me to learn about bitcoin. And then I mined a bunch, sold it all in the bottom in 2013, and so don't take my mining investment advice. But then I kept in touch with the academic literature on crypto, partially because I worked in this company that did a lot of distributed systems work, and very few people that I worked with really believed that bitcoin actually worked. And then once it was sort of like, clear in practice, it was very hard to take down. It took a while before sort of the math was able to be made to really describe under what conditions does it work, how well does it work, how big can the network get? All these kinds of properties which are very non trivial in a lot of ways.
00:03:10.030 - 00:04:40.100, Speaker B: And that sort of was when I started reading all these papers and trying to convince my coworkers, and a lot of them were like, ah, this still doesn't seem like it makes sense. Like, just seems like bad database. And then after that, I went into high frequency trading, still working on some hardware simulation type of stuff. And I was reading a lot of proof of stake papers, like the Algorand paper in 2016. And one question I'd always had, Washington, how do people actually measure risk in these systems in a way that's meaningful when the market price of these assets are changing? A proof of stake system is only as really as secure to some extent as how much the total assets staked are worth. One question you might have is then what happens if someone could manipulate that price or make some type of derivative asset that causes the amount staked to go down? And I think at that time, in proof of stake land, we were in a much more optimistic world where it was like, just like, the cryptography will work and you won't have to worry about the financial aspect. But that's kind of led me down this rabbit hole of writing simulations to stress test these networks the way you stress test trading strategies and would find sometimes some weird things.
00:04:40.100 - 00:05:27.290, Speaker B: And I showed them to people who were building different layer ones at that time, and, you know, I think a lot of them were like, hey, do you want to work for us? And I was like, not really. I found people who raised money for layer ones in 2017 sketchy in a particular way. Maybe I was the idiot, but I just like, I just really, it was like hard to not find them, just like a little bit off putting in some way. And so that just kind of led me to be like, hey, no, ill, like maybe consult or something. And that sort of consulting thing kind of led to me starting gauntlet. So thats sort of the long form version of this beautiful.
00:05:29.110 - 00:06:01.450, Speaker A: Super interesting. Its funny how long it takes people to kind of wrap their heads around the blockchain industry. I mean, even you were saying kind of proving that, like, in a math sense, is that how, like I would say more holistically, you try to ideate? I guess even with gauntlet is like, from like the point of view of like, math, is that like how you kind of approach different topics? Can I prove this in a mathematical sense?
00:06:02.800 - 00:07:07.810, Speaker B: Yeah. So, I mean, for me, it was more like, there has been a lot of work on distributed systems since the eighties, sort of a lot of mathematical theory, a lot of reasons for why these things work. And one thing I always found kind of weird about the cryptocurrency space at that time was people would be very rigorous about the math and the crypto side, like how the signature scheme works, but then the distributed system side, I guess everyone inherited this thing from Satoshi, where it's fine to be wrong with a bunch of basic math assumptions and probability assumptions that are made there. Extreme informality about that stuff. It was very weird. If you read one of those bitcoin for dummies books, they'll spend two chapters teaching you what an elliptic curve is and how ECDSA works, but they'll spend zero time telling you what's the probability of. How do you compute the probability of the block doesn't get finalized.
00:07:07.810 - 00:07:37.100, Speaker B: They'll just be like, oh yeah, it's six blocks. Why six? I don't know. I'm just going to tell you it's six. And that aspect of it I always found misleading because it is sort of untrue. It's very hard to believe some of those claims. I think the engineers working on things knew the realities of when and when it doesn't work. And they had really good intuition, but they had no way of formalizing it.
00:07:37.100 - 00:09:11.250, Speaker B: And for me, coming from more of a mathematical background prior to working in hardware, to me it was a little bit, there's a bit of smoke and mirrors. But then once people actually figured out how to formalize it, it started to make me realize that there's something very deep here that existing decentralized systems and distributed systems did not really have. And kind of figuring out how to harness that in some new way would be really interesting. And so that's sort of my background into it was more. More that, and I think it's like to be into crypto in 2011, 2012 was either you were like, I'm just going to make a quick buck, or you were like a true believer in the philosophy and had no clue how the system worked or why the code worked. Or like, you know, if you think about bitcoin itself, it had a bunch of cv's, it had a bunch of like, bad hard forks at that time. All this stuff that you, if you saw on the network now, people would be like, oh, it's like a fake blockchain, right? So I think there's also this fact that, like, yes, people who are maybe more risk averse, like me, like, needed to see some sort of proof of that form.
00:09:11.250 - 00:09:27.340, Speaker B: People who are engineers maybe only needed to see that it ran in practice. But I do think that sometimes there's a bit of sleight of hand there because well, maybe it's running in practice, but it's only really running on, like, eight nodes. And you don't. You didn't, you know, you didn't.
00:09:28.960 - 00:09:42.464, Speaker A: True. No. It's very interesting, I would add, of one more third group, people that were buying bitcoin for Silk Road. Definitely a big group for sure.
00:09:42.632 - 00:09:51.020, Speaker B: That's fair. I didn't mean to trivialize the Swiss and the Swiss Switzerland economy.
00:09:53.790 - 00:10:52.332, Speaker A: Super interesting, though. No, I always find it very unique in how people approach the space because there's multiple different avenues, and people can kind of come at it from different angles and then ultimately see the promise and then never leave. So it's fascinating to me. So now you're very busy in your day to day leading gauntlet, and also one of the, you and Robert on the venture sides with robot ventures. Could you maybe just give a brief, like, high level overview of what Gauntlet is for those that are uneducated? And what purpose are you trying to serve into the market on creating the models and the incentive management behind Gauntlet, and then definitely want to talk about robot ventures as well.
00:10:52.516 - 00:12:06.380, Speaker B: Yeah. So I think one thing that I'd realized as I started really running simulations of proof of stake networks and starting to analyze them more as trading strategies and less as cryptographic insurance funds, because in some sense, they are a form of a trading strategy, I realized it's basically impossible to have completely static parameters. Like, I set the inflation rate once forever, and it works. Or I set the fees once forever, and it works in the sense that if demand, you had these demand shocks, you would suddenly have to change your gas prices. Or if you had these price shocks, you would suddenly need to change your inflation schedule. And there was sort of no way around that. And so one thing I realized was like, hey, when we're running these simulations, part of the reason, the thing that people were interested in in 2018 2019 was like, hey, they wanted, like, a value for their parameter, right? They wanted the inflation rate, what the issuance rate should be, or they wanted, like, a gas fear.
00:12:06.380 - 00:13:16.862, Speaker B: They wanted to show that, like, some particular mechanism that would, like, provide some lower volatility. But as we were doing all of this research, and you can think of it as, like, we were doing audits effectively, like, you know, we were. We were running these simulations against the real code. And the difference between, say, like, a normal security audit and sort of these things are more on the economic side, is the normal security audit looks for things where under some conditions, your code says one plus one equals three, one plus one equals three is very bad because that means I can put two coins in and get three coins out, and I can keep doing that until I drain a contract. There's many other forms of these, but I think that's always the highest, simplest example of how to explain that the economic things are much more behavioral based. For instance, in the Uniswap contract, which is formally verified, the formal verification will say something like, hey, if you put in a non zero amount of trade, like, I put in five eth, then I have to get out a non zero amount of USDC. I can't just put money in.
00:13:16.862 - 00:14:09.282, Speaker B: And then the contract says, nope, not giving you any money out. Right. On the other hand, that doesn't tell you anything about any guarantees about how much you're getting. So, like, for instance, in sort of like classical minor extractable value terms, if a user sends in a trade for five ETH, and a bunch of people who are liquidity providers see it and then send transactions ahead of it to remove their liquidity, well, I may have thought I was getting $5,000, but afterwards it turns out I'm getting much less. And so there's sort of this aspect of expected behavior which is different than absolute worst case behavior. There's sort of security. Odds are looking at absolute worst case behavior.
00:14:09.282 - 00:14:57.490, Speaker B: Economic stuff is looking at everything in between worst case and average, and you kind of have to look through all of those. And so we were doing this initially more as an auditing type of thing, and then at then we started working with DeFi protocols. First customer of that form was compound. That was when we realized, actually there's this huge universe of parameters that need to be continuously optimized. I think if there's anything I've learned in the last five years of being in this industry full time is that security has gone from being like, hey, here's a one shot thing. Hey, we analyzed it safe to, it's always continuous. You look at something like the Nomad Bridge hack, which was a deployment issue.
00:14:57.490 - 00:15:49.760, Speaker B: Actually, the smart contracts barely changed. You could say they didn't change, but the way they deployed it to the chain initialized things incorrectly and caused something to go wrong. There's this notion that security in blockchains is necessarily much more continuous and requires a lot more monitoring than security elsewhere. And I think in some ways, the economic side is more obvious that that's true, because, hey, the market has a regime change, and you need to suddenly be more conservative with the amount of leverage your protocol is providing. Makes a lot of sense. It doesn't make sense to say, hey, here's a bank that's going to offer an interest rate, to borrow at an interest rate that they'll give you, and they're never going to change it. It's going to be that forever, right? That's just not realistic in a lot of ways.
00:15:49.760 - 00:16:54.026, Speaker B: But even the auditing side has the same problem in the sense that deployments change, other contracts interacting with you change, and that changes some of the assumptions that are being made. Long story short, we started to realize, hey, people actually need this monitoring of their economic parameters, like their margin requirements, their fees, their interest rates, and you need to optimize those to trade off two things. How much risk your protocol is holding, like a DeFi protocol or a lending protocol, and how much revenue it's generating. Because the revenue acts as your insurance fund to some extent. If there's sort of an attack and the risk is obviously like, well, if everyone's borrowing and then defaulting, then, like, the protocol is not safe for lenders who are pulling their capital in there. The latter is actually quite important for risk too, but in an indirect way. So the simplest example of this that I'll give is like the mango markets attack, which took place in November or October, sorry.
00:16:54.026 - 00:18:04.826, Speaker B: And in the mega markets attack, there's sort of this implied weird sort of property of the protocol that I think they hadn't been optimizing parameters. They kind of kept the same parameters since when they launched, but when they launched, it was this huge bull market, and so they were like, hey, you can borrow how much ever you want against mango. Now, the problem is, the mango protocol was not diversifying its treasury sufficiently quickly, so it was mainly denominated in mango. And so what that means is that if we look at the treasury as sort of an insurance fund, treasury is denominated in mango. So if someone puts mango as collateral and borrows stablecoins, the protocol is actually extra long mango, like in a sort of super linear way, like the protocol is owning the mango collateral of the borrower. They're also owning mango collateral in the insurance fund that's supposed to be allocated to that borrower. And so if someone manipulates the price of mango, they're actually manipulating it more than one x for that user because they're manipulating the share of the insurance fund.
00:18:04.826 - 00:18:59.026, Speaker B: Now, if the insurance fund had more USDC, then the manipulation omega wouldn't impact the net position of the protocol. And so then that extra convexity was sort of, you can take advantage of that, and then the insurance fund can't cover anything at some point. So the reason I bring this up is there's sort of a sense in which these protocols need to actually be watching these things, because in times of high growth, it might make sense to actually be collecting a lot of revenue. But then when the market shifts, you actually really do need to change how you're doing this. So that's sort of the function capabilities we serve mainly for lending protocols right now, but also perpetual protocols. And we're also building this protocol for doing this insurance fund type of thing called era at a very high level. That's sort of what we do is there's a lot of stuff there.
00:18:59.026 - 00:19:00.098, Speaker B: So I apologize.
00:19:00.194 - 00:19:38.040, Speaker A: No, no, it was good. Fascinating. Just listening to it all. It's amazing how complex, I guess, things can be that are not extremely obvious on the surface. And so building software and models to highlight those is definitely welcome in the space. And something that has been happening, I guess, in trend five for a long time. But porting now those, I guess porting those functionality over into the crypto ecosystem.
00:19:38.040 - 00:19:46.120, Speaker A: I know DeFi is the main focus today. Are you also going to be focused on gaming or any other branches in the future?
00:19:46.940 - 00:20:52.800, Speaker B: Yeah, we also do incentive optimization, which is sort of like, how much should you pay in token rewards for different types of users. And we work right now, one of our customers is immutable, which is this L2 for gaming, for optimizing how their incentives work. So at a very high level, you could view what we do is like, we take on chain data, we take your code, we make simulations, and we'll give you some notion of optimized parameters as given new data comes in, we take in the data, run these simulations and give you parameters. So anything that fits that mold where there's economic value to those parameters and you really, really care about being careful with them, that's what we'll do. So whether that is gaming directly, whether that's indirectly through the NFT exchanges parameters, we'll definitely do it. I think the hard part is gaming right now is like very. It's like a little too far from.
00:20:52.800 - 00:21:10.080, Speaker B: Yeah, yeah. But like, it doesn't seem like it's kind of like stuck per se, but I don't know, what's your opinion? Where do you see blockchain gaming? Because, like, I personally, I don't play video games. It was very hard for me.
00:21:10.420 - 00:22:00.164, Speaker A: I don't play video games either. So I wouldn't be the best person to ask on that question. I'm more. I like the infrastructure stuff. And so to me, this is extremely fascinating on just, yeah, the different vulnerabilities and just being able to highlight those. But yeah, gaming, to me, I am fascinated just on the onboarding sense. I mean, even with Electric Capital's report that like 80 plus percent of people, their first time on chain interaction was through an NFT and even looking at like popular dune analytics dashboards showing how few people actually interact with, like, smart contracts, to me, the reason why I'm fascinated on Orlando, at least like, interested on the gaming side, is just one unique game could bring in the entire user base of web3.
00:22:00.164 - 00:22:24.620, Speaker A: But to your point, it is still very early in the gaming front, and building a good game does take a lot of time. I think possibly to start will be a relatively kind of simplest, simplistic game. Probably not like a AAA game, but something that is more like among friends, Orlando. Just like pretty easy, but just kind of catchy. But we'll see.
00:22:25.120 - 00:23:03.330, Speaker B: Yeah, I mean, I think the, the concept of AAAA game has always been something I find kind of weird because in some sense, if you were, if we look at like, gaming things that were good investments in the past, a lot of them are things that don't look like they were going to be AAA games, but they like, accidentally did, or like they were like mobile games, which like, every game developer shits on. Right. So there's kind of this like, weird thing where I suspect it's just not going to look like the modality that people have for like, this notion of like League of Legends or AAA or whatever.
00:23:03.370 - 00:23:03.990, Speaker A: True.
00:23:06.170 - 00:23:31.608, Speaker B: Which I think somehow, like, crypto investors have not really leaned into. They seem to be wanting to invest in these games that are like team from Riot games, whatever. But I kind of feel like it's going to be. It's more likely to be like the Dark Forest type of thing to me. That's my guess. And I don't play games, so I'm a bad assessor of this than it is. Let's try to make League of Legends.
00:23:31.608 - 00:23:34.328, Speaker B: Let's shove a token into League of Legends somehow.
00:23:34.464 - 00:24:04.440, Speaker A: Yeah, well, there's definitely a lot of falling kind of in the venture space and just following narratives. So it wouldn't surprise me that that's also happening in gaming. But on that front, you and Robert obviously running robot ventures, I would love to hear a little bit more about how you one like, kind of manage your time between Kotlin and robot ventures. But what are the things that you really look for on the venture side and things that excite you?
00:24:05.860 - 00:25:20.156, Speaker B: Yeah, so I mean, I, I would say that, like, 95% of my time is on Gauntlet. So it's like, mainly not robot ventures. But the nice thing is there's a lot of synergy in the sense that I'm talking to people when they're kind of, like, early, early starting out and trying to walk through what they're working on. And a lot of that actually is the same as talking through things with people who we work with, who are our customers. And so I was already kind of doing this, in fact, the person who sort of tried to convince me to start investing, I think, in 2020 or 2019, late 2019 was like, hey, have you heard of this protocol? What do you think? And I was like, you're the 15th vc to ask me today. And then they were like, oh, what would, how would, how could you be the first to ask you today? I was like, well, I should be the investor then, right? And so that's sort of, that's sort of how it started. And so a lot of it comes down to like, hey, I'm already, like, kind of reading all these papers.
00:25:20.156 - 00:26:01.080, Speaker B: I'm writing a lot of research. People are making protocols based off research that we do at Gauntlet. And so it kind of was a nice synergy in a lot of ways. And so it just kind of started from that and then kind of bloomed into us doing a lot of investments in different parts of the world. But, yeah, I would say my approach to it is I view everything from the purely technical or financial lens. I have no ability to assess nfts or games.
00:26:02.220 - 00:26:41.820, Speaker A: Nice. I mean, I try to approach, I mean, my background is product technical, and so I try to approach it from the technical side as well. And I'm curious. I mean, just looking at some of the portfolio companies that you have on the website, you're kind of, I mean, obviously very big in the ethereum ecosystem, but also have some different infrastructure or different investments in the Solana ecosystem and different kind of bridges. Is there a community or ecosystem that you're, like, particularly excited about? Are you generally kind of agnostic to the technical underpinnings that teams decide to build upon?
00:26:45.040 - 00:28:12.790, Speaker B: I think that the philosophy, I guess, that we have is a, it's probably too early in this industry to be hyper worried about, like, everything always being competitive in the sense that probably everyone needs to grow at this point for the industry to make it, as opposed to, like, one winner take all type of thing. I don't really, I've never, I've never under, like, I refused to use Twitter until basically the pandemic, I, like, barely touched it. And part of the reason was, like, I just couldn't stand, like, reading crypto Twitter. Like, people just like, yeah, especially in the bear market. Like, last time was just, like, very annoying where everyone was like, a cheerleader for something and just like, I couldn't really. I was not that. And my view is like, hey, look, whatever solutions work from a pure technical standpoint or from a pure engineering standpoint or even a pure product standpoint, although I think the latter thing, if it doesn't have the former in crypto, tends to get exploited, is that it doesn't totally matter exactly what you're starting with, provided there's a baseline level of engineering or technical prowess.
00:28:12.790 - 00:28:49.520, Speaker B: I think there have definitely been a lot of projects that I would say faked it till they made it eventually inherited security properties that made them much more stable. I think Solana is a great example of this. I remember the week they launched. There's this bug in their consensus protocol that another layer, one helped them figure out and fix, which was near protocol. And I thought that was kind of interesting. I was like, would that have happened a year later? Probably not. So it's like.
00:28:49.520 - 00:30:22.592, Speaker B: But the idea of the spirit of, like, hey, people didn't really, we're not quite as competitive and we're helping each other was certainly much more prevalent in 2019. And I just generally think, I'm not sure the precise choice you make makes all the difference. Now, of course, in a world where all the assets are coming from five people who all maybe will go to jail, then maybe that's not exactly the ideal framework to build on. You know, as long as people are committed to the things they're decisions they've chose, I think it's fine. I think the hard part, and this is the type of thing that I think we spent a lot of time trying to tease out of people, is whether if, like, the particular choices they made, technology wise, product wise, et cetera, go south in that, like, hey, it turns out to not work for your application, or, hey, you can't, like, get everything to where, whether you sort of are the person who just kind of quits or whether you're willing to be flexible and changing kind of what you're working on. And I think that type of attitude is much more successful in crypto where, like, you could be the most successful app, and then all of a sudden, let's say the base protocol you're on makes a gas price change, and all of a sudden you're like, the least successful one. I mean, it's hypothetical, but it's very.
00:30:22.592 - 00:30:54.200, Speaker B: Such things do happen in Ethereum. There's a thing called gas token a long time ago, which was sort of a rebate on GA, and the moment that left a bunch of different applications suddenly saw a huge drop off. So my point is, there's a sense in which you're always subject to the market forces and so are you willing to change for them or are you going to just be like, okay, well I guess AI seems easier now, so I'm going to switch to that because.
00:30:54.860 - 00:30:56.120, Speaker A: It should be the trend.
00:30:56.540 - 00:31:04.204, Speaker B: Yeah. And so I think that like from a sociological standpoint, that's something I really try to tease out at people.
00:31:04.372 - 00:31:21.220, Speaker A: So more betting on the founders than their particular, the founders kind of technical ability, regardless of where they initially decide to choose to build upon, also combined with the fact of obviously what product they're building.
00:31:21.800 - 00:32:43.174, Speaker B: Yeah. So I mean, I think I would segment it into like things that are extremely technical, like things on the ZK side. It's much, much more actually about the, I mean there's certainly a lot that the, you know, the person, you know, how they are kind of sociology, like sort of psychologically in that sense does matter. But I think the, in those in the ZK world you actually do like, because there's so much technical risk inherent to the product you're building. Because you're sort of building something that's like very zero or 100, right? Like either it's a platform and it works and people can use it and build a ton of other stuff on top of it, or it's just too clunky and no one's ever going to use it. There's really not a middle outcome, I think, for a lot of CK stuff because it's already a hard technology to explain to people, let alone the hard technology for an average software engineer to be like, hey, I'm going to write a circuit in circum to then, hey, I'm going to productionize this and be able to monitor it, spin up 500 instances of it. Like you kind of have to de risk each of those steps in that pipeline and you have to de risk that.
00:32:43.174 - 00:33:08.400, Speaker B: You have to be very cognizant of exactly your plan to do that. And I think for extremely technical things in MeV and ZK, I think there is a higher weighting on the idea and execution, whereas I think for things that maybe you could pivot from or could change your product direction on, then it's much more on the psychology of the founder.
00:33:08.520 - 00:34:10.590, Speaker A: That makes sense. Maybe transitioning a little bit, but staying on the topic of the core infrastructure you have, I mean, obviously with Gauntlet evaluated quite a few of these protocols, I've seen you at the celestia events and moderating talks there. I feel like you have a very full picture view of the entire ecosystem. Why do you? I guess my question here being why do you think there is not more adoption on chain? When I look at doing analytics, OpenseA and Uniswap have less than 5 million active addresses ever, respectively on each. But obviously both of them do tremendous amounts of volume. What do you think we need to do to get from the low millions and active addresses to hundreds of millions and billions? Is it better applications or is it better infrastructure? I'm curious what you think.
00:34:13.970 - 00:35:06.350, Speaker B: I don't know if it's necessarily better infrastructure. One of the most used applications at some point was actually usT. And I will give the Cosmos SDK a lot of credit for standing up through the blow up. I don't know if any blockchain client has ever had to handle as much traffic live as it did when UST was blowing up, period. I was looking at some of the bandwidth numbers node operators were saying were claiming, and it was like unreal. It was actually insane. And the gigabits, yeah, it was like some of them were claiming that they were getting terabytes worth of spam added up over like hundreds to thousands of nodes, something like that.
00:35:06.350 - 00:36:06.350, Speaker B: It was like, because there was tons of people trying to get their money out all at the same time. And then there were a ton of people who were short Ust who were also trying to like knock down the network so that people couldn't get them. So it was like a very, it was a very interesting, like game theory type of thing. But Ust actually did have quite a bit of usage. And I'm not going to say that that was a good thing necessarily, but I'm going to try to point out that I think they did focus on some application use cases and convincing people that that type of stablecoin is the thing to look for. So I guess I would say, yeah, I think it's application driven, not infrastructure. I actually think there's a lot of infrastructure that, if anything like the infrastructure side has been extremely invested in, perhaps overinvested minus ZK, because I actually think that's quite important.
00:36:06.350 - 00:37:11.446, Speaker B: And the real question is, what application could people use? The unfortunate part of it is it's not even just that you have to have a good application you have to have some sort of good ux that these new users are going to be comfortable with. I just am not so sure how wallets will ever kind of come to parity with like, systems that are centralized at least, you know, in a sort of a sense in which it's like a random person on the street. I could like give them a flyer and a QR code and say, download this app and they would be onboarded in the same way, like every delivery company. Like, I can't walk outside on the street without like five people giving me a little thing of like, scan this QR code and get $5 free off your next grocery, right? Yeah, I don't see how that happens.
00:37:11.478 - 00:38:01.590, Speaker A: With wallets, do you think? I mean, in that sense, I mean, in 2020, I think compound was really kind of one of the biggest pushes for the bull market. And then afterwards we saw the innovation and cool things happening in Defi summer and having the benefits of everything being on a single shard and being composable with the UX problem and L2s and different app chains or breaking apart or segmenting state. Do you think that is going to be more problematic than large single shard networks that are now coming online, like Swe or Slana or even Aptos?
00:38:03.170 - 00:39:18.386, Speaker B: Yeah, yeah, yeah. I think that just as we have in normal cloud computing, you do end up having a separation between the performance dominated nodes and the kind of split up, purpose built ones. So I'm not so sure there's such a clear dichotomy where one is going to win out. I think it's going to be just certain applications do better on one and certain applications do better on the other. But I will say, I think the sharded stuff, robustness wise, is a lot better. So applications that need to be a lot more robust in the sense of like, the users are unwilling to tolerate, like certain types of deviations from the expected protocol will probably go there, and then things where everyone only cares about responsiveness and latency will go in the other direction and there will just be some trade off between them. Now, again, this gets back to the part that I don't understand games.
00:39:18.386 - 00:40:08.640, Speaker B: I don't really understand games that really want to be a low latency, but also be on a blockchain. Because I think a lot of the blockchain native games that have worked like Dark Forest, actually very asynchronous interactions, and that's why they're kind of easy to use relative to trying to do something like Star Atlas, which I feel like is just really hard problem. So other than trading, it is hard to come up with these low latency use cases, at least in my mind. Maybe this idea of a game on its own shard makes sense, but a game on a low latency shared machine just seems super, super hard to ensure the UX for people.
00:40:09.700 - 00:40:11.720, Speaker A: In what aspect?
00:40:15.070 - 00:41:09.598, Speaker B: If there's traders who are doing mev to try to constantly be at the front of the block, then you're going to just observe lots of jitter as the user in the game because you actually have no guarantees. Your transactions are just worthless relative to the trading transactions. They're always going to be willing to pay more than you, but you have to share the same space. It's as if you went on a subway and there was a notion of like first class in the subway and you know, like. But like the first class. And then the demand for the subway car didn't make sense, but yet, you know, somehow the first class was always full and always empty and the back was always full and cramped and it didn't like quite optimize distribution correctly. Because I do kind of think the problem with games is like the users don't want to pay much per interaction.
00:41:09.598 - 00:41:27.470, Speaker B: The cost per interaction has to be down, whereas the cost per interaction for traders can be sometimes arbitrarily high. And having those two economic models shoved into one thing seems sometimes a little hard for me to imagine working.
00:41:27.630 - 00:42:01.430, Speaker A: I do want to talk about MeV, but maybe before that. What are your thoughts on. It's been interesting following Solana's architecture. I think they've done a bunch of unique things. But I'm curious to get your thoughts on fees per contract and trying to isolate those individual hotspots instead of doing global fee markets. Do you think that will have a big impact on the network or other chains will adopt this? Or what do you think the pros or cons of adopting something like this are?
00:42:01.810 - 00:42:55.170, Speaker B: Yeah, so yeah, as a disclaimer, disclosure, disclosure, I wrote a paper on this with some people on these kind of dynamic fee markets, and under what circumstances you can actually optimize them in a blockchain. There's sort of two threads of this. One is the Solana version, which kind of has this hot and cold spot type of fee modulation. The other is sort of the Ethereum version, which is multi dimensional. EIP 1559, which is like storage will have its own base fee and its own sort of adjustment, versus compute will have its own adjustment. And maybe different types of opcodes will have their own types of adjustments. In some sense they're two similar things.
00:42:55.170 - 00:44:05.300, Speaker B: It's just the level of aggregation at which they're doing the fee adjustment is different. So in the Solana case, they're choosing the base unit of the thing that's having a dynamic fee as being a contract, whereas in eth it's more close to the function call and opcodes themselves versus a particular contract. I think there's clearly something like this that will work. It's just not clear what aggregation level is the correct one. I think the other interesting thing is you do open up a lot of DDoS vectors via these dynamic fees where you can cause the fees to go up in a particular contract to lower its quality of service so that only the highest value users can use it. Not saying that's necessarily a bad thing, right? Like for Uniswap that's fine, because like, well, the LP's are making more revenue in some sense. So it's like, it's not like so bad in some ways, but for a game, again, it's like, it's not clear that for a game you really don't want that.
00:44:05.300 - 00:45:33.660, Speaker B: You don't want like the person who's like, you know, there's a reason, like all of these like game companies have these huge fraud teams to try to like kick out all these bots that are like causing people to not be able to get items when they are supposed to and stuff. So I think there is sort of this interesting question of like should a. Should it be regulated at the contract level or at the opcode level? And not to sound overly philosophical, but this is sort of a communism versus capitalism debate in the sense that, like one is like, do we tax you at the level of like, raw materials? Do we tax like the wood and the oil and the raw imports, but then after that you do whatever you want, or do we tax you at the level of like, property? Like you own a property that's consuming resources, we're taxing the property a fixed percentage, and there's quite a big difference between them. Because let's suppose I'm an oil refinery, then it's possible that being taxed on oil is actually worse for me than being taxed as a property. On the other hand, if I'm a household, being taxed on oil probably doesn't really matter because I buy it indirectly through and I don't feel it directly, but the property tax does matter. And so there's sort of a sense in which the choice of how you do this favors certain types of protocols. And makes others worse.
00:45:33.660 - 00:46:59.940, Speaker B: And so it really does boil down to how much do you want the state, the layer one to own to control this, versus how much do you want the individual apps to control it. And I think there's definitely, like, there's going to be dynamic fees like this, right? Like, I think EIP 1559 proves that dynamic fees are better user ux in a lot of ways. And yeah, it's just going to boil down to what aggregation level makes the most sense. I think the Solana model makes sense in the following sense. If the MEV from a particular contract can be distributed in some ways, partially to the app itself, rather than strictly to validators, then there's a sense in which, like, okay, if I'm a hotspot and I'm causing a bunch of congestion, but at least the application gets some of the value from that, not just the validators, then there's a sense in which I'm now incentivized to do some sort of demand smoothing versus, like, otherwise, if I'm an app and I'm a hotspot, I'm just like, oh, whatever, I'll let it stay hot and not try to ameliorate it in some ways to get more fees. So there's some trade off between how much goes to the validators and how much goes. Yeah.
00:46:59.940 - 00:47:49.766, Speaker B: On the other hand, the EIP 50 to 59 type of thing gives nothing to the app, ever. The only redistribution it can do MEV wise is to stakers. I think you already see that difference philosophically between Ethan Solana in that in Solana's MeV auctions on Jito, people are bidding effectively on a per function call basis. It's like I'm bidding in an auction for swapping on uniswap or swapping on manga or something. Whereas in ETh, you're bidding on the whole block. I mean, theoretically you're bidding on, like, sequences of transactions, but in the current MeV boost model, you are bidding on the whole block. And there's again, this aggregation disaggregation problem.
00:47:49.766 - 00:48:22.056, Speaker B: Like, is it better to have many auctions for, like, every possible function call, which is hard to bid in, but maybe it's, it's because it's more granular, people are more close to their true value, or is it better to throw everyone into one thing and you're bidding on the same thing? And these aggregation versus disaggregation types of problems are fundamentally, in my mind, the difference between socialist systems and capitalist systems. And we're kind of like we're sort of like the different countries are choosing.
00:48:22.168 - 00:48:28.850, Speaker A: Their models and the capitalist system is per contract or on the block level?
00:48:30.390 - 00:48:46.610, Speaker B: Yeah, I think it's. Yeah, it's a good question. I think the per contract thing is a little closer to like pure capitalism, where like, the raw inputs are sort of priced differently for each contractor.
00:48:46.950 - 00:49:12.174, Speaker A: Interesting. I don't know. Yeah, I find all this fascinating. It's a deep reptile. It goes very deep. Maybe one last question on just infrastructure, and then I want to talk about mev, because I know you're an expert in that field. I'm very curious.
00:49:12.174 - 00:49:52.180, Speaker A: On Ethereum long term, what do you think will be sufficient enough throughput? I know once full sharding is implemented, or not full sharding, but more data availability at the base layer, it will be 1.3 megabytes per second that the l two s will have to consume. Do you think 1.3 megabytes at the base layer is going to be enough throughput for Ethereum's demand and other chains will consume excess demand? Or do you think Ethereum will continue to add more throughput if it's needed?
00:49:54.280 - 00:50:13.496, Speaker B: This is a great question that I don't have a particularly strong answer on. Not because I don't have beliefs in what direction it'll go in, but because having seen the politics around this change since 2018, quite dramatically, I don't actually.
00:50:13.528 - 00:50:15.048, Speaker A: Just on Ethereum roadmap.
00:50:15.184 - 00:51:34.540, Speaker B: On Ethereum's roadmap, with regard to sharding, I think the idea that rollup started taking off kind of took off a lot of pressure from the Ethereum team to shard immediately, and then they haven't even decided on the slashing rules for sharding the like. Clients don't quite make. There's a lot of stuff that's so open that I hesitate to make any prediction about this. From that, from the, like, sociological standpoint, from the technical standpoint, I think the reason you're doing things like EIP 4844, which is like you can write blobs that cost live in a different economic zone, but you can't do compute on them. And that's for the L2, other than the fraud proofs, is sort of a sign that throughput will go up, but it's going to actually be harder to measure because the blobs should count too, but they might be much larger, but they might not have any compute done on them. So it's sort of, we are moving to a world where data availability has tiers, where there's highest data availability, like I need to write on the chain, I need the swap on main net. There's sort of medium data availability, which is like the blob for an l two.
00:51:34.540 - 00:52:41.814, Speaker B: Then there's the lowest data availability, which in the cloud world would be the archive tape node type of thing, which is maybe data availability elsewhere that I only have a proof for. That's the Celestia or Eigenva model of the world. If we look at cloud computing, it's pretty clear they have the same things of very hot data storage medium and then cold, which is archival. And I think we are kind of converging to that world such that the concept of throughput will somehow depend on how you want to average the three different versions. I think at the hottest level it might not increase that much from the current speed, but if I start including the blob bandwidth and the DA bandwidth, if there's a lot of usage that could go up dramatically if you add those in. So that would be my view in that sense, and that it will go up, but not in ways that you might think, not in the ways that the one number might say. You might need three numbers.
00:52:41.982 - 00:52:50.170, Speaker A: I like breaking each of those out. That makes a lot of sense. Do you think more throughputs always better or there are downsides in terms of storage?
00:52:51.030 - 00:53:48.010, Speaker B: There's downsides in terms of UX for developers. Now I have to think about where I'm storing something and I have to think about how the access works. There is a sense in which developer friendliness goes down. The more you force people to separate different types of compute and storage. And that in turn leads to user UX being worse because it's like the user is now implicitly managed. And I remember in the early days of, I don't know, I hate the phrase web two, but whatever web two, like 2009 or 2010, there'd be so many apps you'd go on and you'd log in and then it would just take forever. And if you ever looked at the JavaScript logs or the iOS logs, you would just see it's trying to hit Amazon s three and waiting on the storage thing to give you back an answer.
00:53:48.010 - 00:54:24.420, Speaker B: And then of course people started making like caching things and CDN like things. But my point is like, I think we're just kind of doing the same thing here. It's just like slightly different because some things require cryptographic proofs, some things require cryptographic proofs only on fraud, and some things require very, very infrequent proofs. And that aspect doesn't exist in the web two world. You just kind of like trusted the person you trusted. AWS will give you the right thing every time. You never really had to verify that they did.
00:54:25.840 - 00:55:08.146, Speaker A: Yeah. Interesting. No, I think about this a lot and trying. I mean, I wish as an industry as a whole we could come up with more like standardized metrics to kind of judge one, like decentralization, because I think that's always a hot topic, but too, just like different scalability levels. And always I like throughput just because it's at least on a high level. I think one of the more basic metrics that at least you're able to do a more apples to apples comparison, but it does get a little bit more nuanced as you, I guess, break apart what data you're submitting and different what data is included in that throughput.
00:55:08.298 - 00:56:33.270, Speaker B: I mean, the thing is, if you're starting to separate compute throughput versus data throughput, which almost every computer architecture in history, I worked on a lot of non von Neumann computers which look very different than x 86 in your box right now that you're using. But even then, there are very few things that force the compute and data throughput to actually be pipelined to be the same. And I think, yeah, even like the current intel chips are not really truly von Neumann architecture, like the L1 cache is split into data cache and a compute construction cache. And so there's a sense in which all of these optimizations that we've discovered over time have boiled down to actually separating compute and data and burstily having throughput for one or the other, and then matching those as best as possible. That's a very coarse view of it. And I'm sure there's a bunch of hardware people who would kill me for saying that, but I think they still would probably agree with the direction of that. And I think in some sense that's the same thing we're converging to in blockchains, except we have this notion of how much value is there, perennial unit latency.
00:56:33.270 - 00:57:40.864, Speaker B: The data that needs to be there all the time has to also be high value. And the data that doesn't need to be there, but can be cryptographically verified infrequently, is the lowest value. And so now we're deciding how much we want to pipeline throughput based on the value we're assigning to each of those components. And I think the Solana version of the world is let the market contract, decide what that value is, and then the cosmos version of the world is like, I decide my own values, and if you want to accept them, then you can cross in from IBC. Then the ETH version is a little more like the normal computer, maybe a little more authoritarian in some regards in terms of choosing these particular prices. But the ETH version actually looks the most like a normal computer in that, like, the roll ups are like your GPU or a sound card or something that has its own, or NIC has its own memory and execution, but it still has. It's still getting bussed by the main thing.
00:57:40.992 - 00:57:42.380, Speaker A: I like that analogy.
00:57:43.000 - 00:57:59.108, Speaker B: And so I think Ethereum is definitely looking like a computer, Solana is looking more like an ASIC, and Cosmos is just like. I don't know, it's very hard. I don't think there's a hardware analogy because you don't design hardware the way cosmos is built.
00:57:59.304 - 00:58:32.922, Speaker A: That's funny. I like those analogies, though. They paint a pretty clear picture, I think. Super interesting. So maybe on the topic of MeV, and I love maybe just to start with, order books versus AMA's or amms. And I think initially just amms became the standard because throughput and latency was just very high on these earlier chains. But now people are trying to build order books on chain as well.
00:58:32.922 - 00:58:44.390, Speaker A: What is kind of your thoughts around the differences between the two? And do you believe one will kind of be dominant over the other over the long term, or they can both, like, coexist?
00:58:46.090 - 01:00:00.910, Speaker B: Yeah, I mean, look, I have an invested stake in amms in the sense of, like, I've written all of the early academic papers on them, so I have spent a lot of time thinking about them. And when I first came from trading and I saw Uniswap in 2018, I was like, how are people not getting constantly picked off? Why would anyone do this? And then once there was $15 million, it's like, there's got to be some reason that this works. And then that's sort of how the mathematic stuff came out. But I think there's a sense in which people have to remember that not all traders are purely highly rational profit seeking. One thing that's worth pointing out about Uniswap LP is, I think in a lot of these markout versus long term growth threads that we always see on Twitter is, sorry. And markout is like, normally in algorithmic trading, for instance, a markout is the way you measure your performance over a fixed time horizon. So it's like, hey, here's how much I started with, here's how much I ended.
01:00:00.910 - 01:00:33.830, Speaker B: What's the percentage I got to in amms, most of the math is actually done over no fixed time scale. So you look at how the portfolio value grows over time, and then you look at a rate of growth, but you don't fix it to a time window. And those two are statistically extremely different. Of course, Twitter threads have no nuance of that form, so people are just like, ah, look, this thing is bad. So unfortunately, there's a lot of detail into exactly what assumptions are being made. Exactly what. You're taking an average overdose.
01:00:33.830 - 01:00:38.794, Speaker B: And those two things aren't the same. But whatever. Ignoring the fights.
01:00:38.882 - 01:00:40.310, Speaker A: That's why podcasts reports.
01:00:42.170 - 01:01:29.960, Speaker B: Ignoring the fights. One really important thing for growth in defi summer was that daos, who are passive capital and large owners of tokens, could provide liquidity for people to buy their governance tokenization, to vote in their dao. And those daos are not economically rational in the sense that they're not trying to monetize their token and make money off of it by being an LP. They actually want to be a passive investor. They just want exposure to the beta of this thing. And they don't care about optimizing exactly the fees they earn in the same way that people who buy ETF's behave. They're like, okay, I want exposure to the tech sector, but do I care that I'm getting the exact optimal portfolio? Not exactly.
01:01:29.960 - 01:02:47.672, Speaker B: I just kind of am like, I want roughly that. And these passive capital allocators who are not necessarily purely profit maximizing, oftentimes serve as the liquidity of last resort. When markets are collapsing, like these dows are not like, when the market's crashing, a Dow is not like trying to remove its liquidity from the us pool. In general, I think there's a place for liquidity that's sourced from people like that. And especially when you're bootstrapping a token where, yeah, the people who have the token are rich in that token, they actually are fine losing money in that token's terms, because they're actually just trying to get anyone to use it or have access to it, which is very different than what's my exact markout. I think that part is always missed, is that the bootstrapping, this is like a bootstrapping cost, and it's actually a kind of cheap customer acquisition cost in some sense. It's basis points versus like, I don't know how much the delivery companies raised to give away, but it's like clearly billions of dollars versus like, oh, I'm giving you basis points of loss in my token, that's a very cheap customer acquisition in a lot of ways.
01:02:47.672 - 01:03:19.510, Speaker B: So from that standpoint, amms are just easier to bootstrap with than an order book because, like, a Dao cannot be an active participant. It can barely vote on one decision. How is it going to, like, constantly be, you know, and so I think there's this. There's a world where passive capital is important now. Does that mean that, hey, we shouldn't, like, improve the designs? Absolutely not. I mean, there's a reason I've written, like, 20 papers is like, there is a way to improve things. There's a sense in which there's this trade off between passive and active participants.
01:03:19.510 - 01:03:36.310, Speaker B: But I think, yeah, the analysis of, like, the pure return piece, I think is somewhat misleading because it misses this fact that, like, a lot of the biggest LP's are actually these passive providers who can't adjust their liquidity.
01:03:37.730 - 01:03:58.118, Speaker A: It makes a lot of sense, and, yeah, it's an interesting point of view. So essentially, long daos and kind of passive lp'ing into amms, there's definitely, if.
01:03:58.134 - 01:04:50.824, Speaker B: You look at finance over the last ten years, other than the 2019 to 2022 retail finance boom, people were just moving all their money into ETF's and not buying individual stocks. There was a huge. So there's always going to be this pendulum swing between way more passive, way more active. But the beauty in Defi and crypto is the Daos themselves have a lot of incentive to be these passive providers in a way that's very different than the normal market, where if I'm like, I don't know, square, I'm not going to take my square stock and go market, make it myself. I'm going to go sell it to a broker. And obviously a broker is charging me a fee, so I'm losing money. Technically, it's almost much more efficient to just have an amm.
01:04:50.824 - 01:04:54.020, Speaker B: I have some bounded notion of loss, and that's good enough.
01:04:54.880 - 01:05:23.150, Speaker A: Yeah, that makes sense. Fascinating. I guess. Like, on the topic of order books on chain, what are your thoughts surrounding those? I mean, obviously you've spent a good amount of time on the Amm side on the order books. Do you think just with the distributed nature, they're always going to be slightly less efficient than a kind of centralized order book? But what are your thoughts there?
01:05:24.530 - 01:06:02.680, Speaker B: Yeah, I mean, I think the thing about order books is, like, they work best when you have products where people need a lot of fidelity over the sizing versus price. Right. Like an amm in some sense is like, you have no fidelity over sizing versus price. That's just as fixed, like, effectively the kind of rough ratio you're getting. And an order book is like something where you really want to have more granular control over that, and there's a computational complexity difference between them. An order book requires many more updates to stay synchronized. AMM requires one, but obviously splits the revenue differently.
01:06:02.680 - 01:06:41.850, Speaker B: And so I think the hard part in blockchain is that the number of updates costs you more. And so there is a tendency to optimize to reduce gas costs over time. And then if that's true, then you'll tend to use the things that, like, have fewer updates, even if they have less fidelity. And that's sort of like an interesting trade off. That's like the opposite of the trade off. Say you make an AI in AI, you're not ever trying to reduce these kind of computational complexity. You're trying to just throw more hardware, throw a larger model, always, especially in 2023.
01:06:41.850 - 01:07:24.240, Speaker B: And that's sort of a. A very different. Like the approach of an order book is a little more like the approach of AI, where you're like, you want to allow for a lot of complexity and expression. And I think for things like perps, that does matter. For things like spot trading, it's not clear that it always matters. Like, it definitely does sometimes. And for things like NFT order book, like constructs, like pseudoswap, I'm not 100% sure you totally need to do that, but I think there is definitely a need for it.
01:07:24.240 - 01:07:37.840, Speaker B: I don't think it'll ever be as good as the centralized version, but I also don't think that. I think people now understand the centralized version can rob you. So it's like that default risk wasn't added into your latency calculation.
01:07:40.430 - 01:08:19.300, Speaker A: Interesting. Now, it's all definitely thought provoking. Yeah, it is interesting just thinking about the different trade offs on maybe that front. Obviously, you've been through a lot of bull markets and bear markets. Now, was there anything outside of. I think FTX obviously caught a lot of people off guard, but is there anything like this bull market that uniquely surprised you, or do you feel like it was all kind of plays out similar to past cycles? I'm curious to see your thoughts from this last giant run up.
01:08:22.800 - 01:09:08.530, Speaker B: I think every crypto market has. The bull market has the following where there's some real innovation that causes things to go up, and then there's kind of this plateau, and then it becomes like scammer central, and then it crashes. And in 2017, I feel like, it was definitely all the proof of stake chains. If we look at the top performing icos that still exists and are used, it's like Polkadot, Cosmos, Tezos, Ethlend, which is Aave. You can't say that those scammed you. They did raise the money and they did build a working blockchain. And there are some users.
01:09:08.530 - 01:09:59.324, Speaker B: In the case of Aave, they're real users. In the case of Cosmos and Polkadot, there's some users. I guess Tezos NFTs actually do have a lot of users. But my point is, they are real investments, and those were all, for the most part, earlier in the cycle. But I think it was like once people saw, oh, those investments went up, then it was a lot easier to have the fraudulent things. I think the difference this time is that the fraudulence was systematic in Solana, Luna and Avax in some way, which is unfortunate for those ecosystems, and that it will be hard to remove that association. That being said, I think the other difference that was crazy was definitely NFTs.
01:09:59.324 - 01:10:48.336, Speaker B: Because NFTs, to some extent, people were always, there's always this meme of people like, oh, self custody is too hard. No one's going to do self custody, which I think for high enough value that's probably true, but for low value transactions, I don't know. I don't totally think that's really true. And I think NFTs were interesting because they were sort of the biggest event that caused people to download their own wallets. I remember around the time that Safemoon came out, there were just people giving out TikTok videos of how to install metamask and then change your RPC to BNB chain. Crazy. I was like, I was like, there really are these TikToks.
01:10:48.336 - 01:11:19.350, Speaker B: People are like, no one ever did that before for installing wallets. And so clearly there was something that struck a nerve. And I agree with you, that type of stuff will probably get more users. I just don't know what that will. That sort of would. The NFT boom would actually be my positive, in spite of me not liking them or understanding them. I think there was this huge positive of people got used to using soft custody wallets.
01:11:19.350 - 01:12:02.628, Speaker B: That was quite different. That was not really true. Even during the 2017 boom, I mean, there were certainly some ICOs you had to participate on chain in, like, say, the status ICO, which was famously like the first form of MeV that ever existed, or that was publicly became an issue where this minor front run, everyone inserted their own transactions, and then people started shaming them. And then everyone stopped doing Mev for like a year. But the, I think people weren't doing that many on chain transactions. The wallet sucked. You basically had to run node.
01:12:02.628 - 01:12:06.240, Speaker B: You couldn't. The, like, clients were not that great.
01:12:06.910 - 01:12:17.358, Speaker A: Dai came out in like December of 2017, so almost at the picotop, but other than that, was it tether existed. There wasn't very many tether existed.
01:12:17.414 - 01:12:25.030, Speaker B: So tether was on a side chain on bitcoin where it posted proofs every once in a while.
01:12:25.150 - 01:12:26.766, Speaker A: I remember it was sketchy.
01:12:26.958 - 01:12:49.470, Speaker B: Yeah. And then they eventually moved to ETH, and one of the funniest things about that era was like, all the bitcoin maxis were like shitting on ETH and like being pro tethered and then like, at the same time, like, and including the people who make tether, and then at the same time they're moving all the tether supply to like ETh and Tron. So it's kind of like kind of ironic.
01:12:49.590 - 01:13:10.480, Speaker A: Yeah, crazy times. Yeah, definitely wild past couple of years, I guess. Wrapping up the podcast. Is there anything in like 2023 that you're uniquely excited either on like the infrastructure or application front that you're the either looking to invest in or just excited to watch the progress in?
01:13:11.100 - 01:14:17.590, Speaker B: Yeah, I mean, I think the things that I'm most excited about, and obviously the disclosures investor in all of these is axiom, which is a way of taking historical data on the blockchain and basically getting validators to generate, give you the answer. Like they can give you a SQL query over the blockchain, give you the answer, and then give you a ZKP that they computed it correctly. So it allows sort of smart contracts to introspect to their own state. So right now, if you think about protocols, they sort of have to have this time invariant property. Like the contract isn't really allowed to look at its historical state, very much like unless it stores all its historical state, and obviously that's expensive. So contracts really tend to look only at the right now and maybe a little bit in the past state, but usually you don't really need all the state from the past. You want some summary queries, but those queries will change depending on the user's demands.
01:14:17.590 - 01:15:21.340, Speaker B: Right now, what do you do for that? Well, a, you go to people like ganlod for governance, where we do these queries, optimize them and submit a proposal, or you go to Dune and you do a query post hoc, like you go to Nansen. Those are all different ways of doing that, but the contract itself can't do that anywhere in any chain. And one interesting thing is, validators could offer that as a service. Obviously, I think the graph and stuff have tried to do it, but they've never been able to do it because it's actually extremely expensive to do this. However, there are certain things you can actually do ZKps of efficiently that would allow a contract to introspect itself. And that sort of changes the design space of some of these mechanisms, because you can now use the history of how users use your protocol to change the parameters of the protocol in some sense. The second thing is, I guess who you've had on this podcast Eigen layer.
01:15:21.340 - 01:16:44.970, Speaker B: Personally, I'm not that interested in the data availability use case for Agnar, not that it's not a valid one. I think it's just like, I think there's a lot more interesting things you can do with it that no one else has thought about. The data availability thing is, there's a lot of people trying to do it, whatever, but there's a lot of more interesting things you can get validators to offer as services when they're restaking, for instance, generating zero knowledge proofs for axiom, or providing oracle updates or guarantees on MEV redistribution. So one of the hardest part about we talked a little bit earlier about redistributing MEV is, well, why are the validators incentivized to do that? In some sense, they could cheat off chain. They could just be like, oh, send me your order flow off chain and I will execute it, and we'll skip the flashbots auction, we'll do an off chain agreement that's separate, and we'll avoid redistributing it. Then you and I split the revenue from the MEV, and then we don't have to share it with other stakers. Now of course that is hard ux wise, order flow wise, there's all this other stuff that makes it more difficult to do, but in theory it's still possible.
01:16:44.970 - 01:18:11.900, Speaker B: One way of trying to get around that for redistribution is to use something like Eigen layer, force people to restake, and that way they're committing to a staking pool. Arguably something like jitto on fauna does something similar. But the thing is, they can't change the consensus rules to enforce the slashing and Eigen layer sort of gets around that in some ways, and I think we'll see a lot of things that are using restaking to force covenants so that if validators are providing these services that they can't cheat because they actually have the biggest incentive to cheat in a lot of these circumstances. And so I'm just saying, like, to me, those are much more interesting applications and they actually like, there's just data availability I feel like is great. But I do think it's going to be a little bit like storage and not crypto blockchain storage, but more like Dropbox or box in that it's certainly a necessary business. People are going to really want it, but it will kind of eventually have margins that erode financially. It's actually hard for me to imagine it being extremely sustainable revenue for a network, which is also why people are fine outsourcing it.
01:18:11.900 - 01:18:43.870, Speaker B: But I do think that these other kind of things that you can do with restaking are very sustainable businesses. You basically can guarantee properties of these MeV auctions that you couldn't otherwise because they're technically outside of consensus. And once you have that, then you can. To me that seems like a source of revenue that people have not really tapped into.
01:18:46.370 - 01:19:12.800, Speaker A: I like your framing and your idea. I haven't thought of that too much. And so it is interesting. I think when people think of Eiglit, they think just about the, again, data availability, as you said. And so figuring out the restaking and how to use it in different, unique ways, that would be exciting as well, got me thinking. Interesting. Well, cool.
01:19:12.800 - 01:19:39.232, Speaker A: I really appreciate your time, I appreciate the chat. I love your unique insights and just how you think about some of these things differently than other people's. I think obviously that's made you and the Gantlen team extremely successful and great early investor as well. So definitely appreciate you sharing all your insights and appreciate you coming on the podcast and sharing all your wisdom.
01:19:39.416 - 01:19:40.400, Speaker B: Thanks for having me.
01:19:40.480 - 01:19:40.800, Speaker A: Thank you.
