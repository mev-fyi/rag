00:00:06.120 - 00:00:06.488, Speaker A: Cool.
00:00:06.534 - 00:00:25.006, Speaker B: Thank you, David. I really, truly am honored to be here and excited to chat with you. Really looking forward to this podcast. I flew in this morning and honestly just could not contain my excitement. So thank you. Super. Thank you very much.
00:00:25.118 - 00:00:27.700, Speaker A: Yeah, thanks, Logan. Great to have you here.
00:00:27.820 - 00:00:28.640, Speaker B: Wonderful.
00:00:28.940 - 00:00:30.060, Speaker A: Back in the Bay area.
00:00:30.180 - 00:01:03.340, Speaker B: Back in the bay. No, it is funny. I was driving down the highway and I was like, man, it feels funny being back, but no, California has a special place in my heart. And all the technology and wonderful builders and researchers such as yourself, it is a special place. So excited to be back. I think maybe a great place to start is if you could just do a little bit of background on yourself, like who you are, how you kind of got to this point in the world and what you've been working on recently.
00:01:04.120 - 00:01:37.340, Speaker A: Sure. Yeah. So I have been a faculty member at Berkeley for 18 years before I moved to Stanford a few years ago. So I've been at Stanford about seven or eight years at this point. So when I was at Berkeley. So I'm a researcher, so I do research for a living, and my first research area is in an area called wireless communication. So in those days, when I started doing research on wireless communication, a cell phone is a big deal.
00:01:37.340 - 00:02:23.238, Speaker A: Only like a selected set of individuals in the world have cell phones. So our mission at that point was how to make sure that as many people can have cell phones as possible. And so we work very hard on the technology. And one limiting factor of the technology is that the bandwidth, the communication bandwidth is very, very low, so it can support very few people and very limited applications. So a lot of my research is focused on how to improve the efficiency of using the very limited communication spectrum. And that led to sort of contributions in third generation, fourth generation, fifth generation technology. That was my early life.
00:02:23.238 - 00:02:48.810, Speaker A: Yeah, that's amazing. So I work with companies like Qualcomm. So I do academic research in university Berkeley, and then I spend time at companies like Qualcomm to transfer the technology. And you may know that Qualcomm is kind of the company that is the underpinning of the third generation, fourth generation solar technology. So I was lucky at the right place at the right time.
00:02:49.870 - 00:03:16.630, Speaker B: That's wonderful. I think just being able to not only get your hands nitty gritty on the research side, but actually see it come to fruition is very special. Oftentimes I think people have to choose one or the other. Being able to actually go very technical on the research side, but then able to apply that and then see that come to fruition must be special.
00:03:17.090 - 00:03:59.480, Speaker A: Yeah. And I think that's often a bit of a luck, because the research area that is chosen, in that case, wireless communication, happens to be sort of at the early stage of growth, and wireless communication is basic infrastructure research, and infrastructure research could have a significant impact only when you start rolling out the infrastructure. When you go from the first cell phone to the first 1 million to the first 10 million, that's when sort of the research has the most impact. And that's why I was lucky enough to be able to transfer the technology very pretty quickly to actual systems. And so that, you know, every time you use a cell phone, you're using my technology.
00:03:59.640 - 00:04:00.384, Speaker B: So wonderful.
00:04:00.432 - 00:04:01.270, Speaker A: Pretty proud of that.
00:04:01.360 - 00:04:09.034, Speaker B: That is. That is amazing. And was that specifically the research that you did at Berkeley, or was that also why you were here at Stanford?
00:04:09.202 - 00:04:09.906, Speaker A: At Berkeley.
00:04:09.978 - 00:04:30.950, Speaker B: Okay. And then, so you were working at Qualcomm, working on bandwidth, and then ultimately ended up here at Stanford, and you were a professor. What would you kind of say, like, have been your main courses and then kind of like your main day to day roles here at Stanford?
00:04:31.730 - 00:04:59.150, Speaker A: Yeah. So when I came to Stanford from Berkeley, I thought, hey, this is a new environment, new place. I should also work on a new research area. So wireless communication at that point was getting towards maturity. Now everyone has a cell phone or more than one cell phone. So I feel that looking for a new research area would be cool. So I have worked on, one area I've worked on for a few years is computational genomics.
00:04:59.150 - 00:06:05.334, Speaker A: And so you may ask, so what is the connection between computational genomics and wireless communication? So the underpinning of wireless communication that I worked on is actually a subject called information theory. Information theory studies. How was the fastest way of propagating information and wireless communications? Obviously, user of the application. However, computation, genomics, I realized one thing is, one problem that I was looking at is genomic assembly. So in genomic assembly, you take data, so called read data that is fragmented through a sequencing machine and assemble it to get the genome. And so I start asking a very natural question, which nobody has asked, though, which is, well, do you have enough information, when do you have enough information in your data to actually reconstruct the original genome? And so that was sort of my segue into computational genomics, because that was one thing I worked on for a few years. Yeah.
00:06:05.334 - 00:06:31.696, Speaker A: And then, so, in general, I'm kind of looking for interesting new area to sort of apply kind of tools and also the method of thinking, first principle thinking of information theory. And so I'm not really very tied to a particular domain. However, when I do work on a domain, I do like to spend enough time for it to make an impact. So that was my goal.
00:06:31.808 - 00:07:11.830, Speaker B: That's awesome. Yeah, I think, I mean, if I was not in the crypto industry, I think ultimately getting more into the genome and DNA sequencing, I think, is another super fascinating field. So I think it's not only impressive that you've gone super deep in one of these fields, but have been able to do it across multiple fields that are right now kind of really at the forefront of research, but also coming to fruition, which is very cool as well. You're doing the DNA or genome research, and then how did you get involved in blockchains?
00:07:13.010 - 00:07:16.104, Speaker A: So that was a 2018. Okay.
00:07:16.152 - 00:07:16.688, Speaker B: Okay.
00:07:16.784 - 00:07:54.460, Speaker A: As you remember, there's something happened in 2018. Bitcoin was almost 20k in the beginning of 2018. And a friend of mine, actually a former student of mine, called me up and said, hey, David, I have a former student, my former student's former student, he wants to start a blockchain project. Okay. Those days, everybody wants to start a blockchain project. But he feel that he needed some researchers to do research in blockchain. He feels that this requires research.
00:07:54.460 - 00:08:28.228, Speaker A: And so he said, hey, David, are you interested in exploring this area? So at that point in time, I don't know anything about blockchain. I have hardly ever heard of Nakamoto. And so I said, okay, why don't we take a look? So I start reading Nakamoto's white paper. It's only nine pages, but here's the advice to your audience. Everyone should read it. The nine pages are actually amazing in the sense that every page, every section contains a new idea. That is amazing.
00:08:28.228 - 00:09:15.952, Speaker A: So after reading that paper, I got hooked into the area and I sort of put aside my work in genomics and start working blockchain. So I spent a year on Lee from Stanford doing research learning, because I'm going to an entirely new area. So I feel I need the time to sort of put aside my regular duties as an academic to learn about the area and do research in the area. So that was 2018. So that's how I got started in blockchain. And then after one year on leave, I came back to Stanford and I decided to devote myself on blockchain research. Because one thing I observed is that just like wireless communication, blockchain infrastructure is at an early stage, I think.
00:09:15.952 - 00:09:44.372, Speaker A: Early stage, I think we talked just before we started the podcast about the scaling problem. About right now, blockchain is really at a small scale. And so this is like cell phones, only a million users, a few million users in the late nineties. In the late nineties. So I feel that there is an opportunity for basic research in blockchains to have a big impact on how the infrastructure will shape up in the next few years.
00:09:44.476 - 00:10:21.210, Speaker B: Yeah, I totally agree. And I'm very much looking forward to onboarding the next 100 million users and watching the technology progress. But maybe to just double click on kind of your research side and then something that you also mentioned on the first principles or kind of how you approach each of these spaces that you've studied in the past and your first principles methodology, could you touch upon that? And then how you applied that research method to blockchain and crypto?
00:10:21.710 - 00:10:29.222, Speaker A: Yeah. So first principle research, maybe I can say a few words of what that really means.
00:10:29.286 - 00:10:30.286, Speaker B: Yeah, that'd be excellent.
00:10:30.398 - 00:11:07.630, Speaker A: And so the best thing is to give an example of what that is. And the best example I can think of is information theory that I mentioned earlier. So, in 1948, well, we're going way back then, 1948, there was this one paper written, it's like Nakamoto's white paper, about 70 years earlier. And that paper sort of describes what is the optimal way of doing communication from point a to point b. Okay. It's a paper written by a guy named Claude Shannon. He was, at that point, about 30 years old.
00:11:07.630 - 00:12:05.094, Speaker A: He started working on this ten years ago. So he was in his early twenties. And what he did was the following. He said, you know, whoa, there's television, there is telegram, telegraph. There are many existing communication system at that point already. But then he said, well, if I had to start from a clean slate, how would I design the optimal communication system? So he's trying to take his mind away from the constraints imposed on by the existing system, on how people think about communication at that point design, start from a clean slate and say, okay, my goal is just to get information from point a to point b. First question, well, how do you describe information? How do you quantify information? And from there on, he designed this principle called information theory, and he described how to design, theoretically, an optimal communication system.
00:12:05.094 - 00:12:18.420, Speaker A: So I think that's the best example that I can think of, of first principle thinking, that is removing yourself from the constraint given by existing system and start from a clean slate. So that was the main inspiration. Yes.
00:12:19.880 - 00:13:06.384, Speaker B: And then. Yeah, no, I think personally, just working at Tesla and kind of seeing other brilliant people use the kind of first principles methodology. I've always admired people that are able to do it, not just because they can do it, just because it's not always easy. But I think the most interesting thing about first principles is that. That it highlights or puts spotlight on things that were not commonly seen before. And so those things that are brought into the light are normally unique and can have profound changes. And so I just think it's a cool way to think about things.
00:13:06.384 - 00:13:07.660, Speaker B: It's hard to do, though.
00:13:08.520 - 00:13:36.832, Speaker A: Yeah, it's hard to do. And also pretty brave, because when Shannon proposed his theory, people, the engineers of those days, look at him and say, what is this guy talking about? This is completely out of this world. This guy's science fiction. And it took 70 years, but nowadays, all communication systems, whether it's wireless, underwater, optics, long range communication, everything is based on his principles.
00:13:37.006 - 00:13:37.548, Speaker B: Wow.
00:13:37.644 - 00:13:40.476, Speaker A: So that's the impact of first principle thinking.
00:13:40.548 - 00:13:54.880, Speaker B: Yep. That's amazing. And so how kind of did you apply that methodology to, like, your research when learning about blockchain and crypto and maybe even, like, more broadly and how you applied it to your other areas of research?
00:13:55.700 - 00:14:31.870, Speaker A: Yeah. So I could give you two examples. So, the first example is my first project in blockchains. So, you remember I said I spent one year doing research. To me, the best way of learning something is to actually do something. So I decided to think about the following. Is that, okay, if we look at bitcoin, first thing I observe about bitcoin, which is really interesting, is that its throughput is only about five to seven transactions per second.
00:14:31.870 - 00:14:55.858, Speaker A: Now, if you convert that into bits per second, okay, that's about 20k bits per second. Now, 20k bits per second is like a modem. I don't know. You are pretty young. But in those days, in the, I think in the eighties or 1980s, those were roughly the speed of a telephone modem. That's how people communicate those days.
00:14:55.914 - 00:14:56.946, Speaker B: 56K modems.
00:14:56.978 - 00:15:21.428, Speaker A: Yes. You can remember the Dao tone that you get when you start communication. I said this weird, though, because this is now in, you know, 2020, 2019. At that point, how come? And we're communicating at tens or hundreds of megabits per second. So why are we limited by this? What's going on here? Why are we only. Why can't we process 707,000. 70,000 transactions per second?
00:15:21.524 - 00:15:22.200, Speaker B: Yeah.
00:15:22.620 - 00:15:47.606, Speaker A: So I start saying we remove sort of the existing bitcoin protocol, and we have just a constraint on communication speed, a constraint on propagation speed. What is the best sort of version of bitcoin that one can design that meets those limits? So that was my first project, Prism. And so we designed such a protocol. Yes.
00:15:47.718 - 00:16:27.504, Speaker B: That's awesome. And I think the data propagation is something that most people do not understand about blockchains. The amount of data that you can propagate in a given second, I think is a very unique constraint. But as you mentioned, something that has grown over 1000 x from the early nineties to today and maybe talk a little bit more about Prism and how as you're going through that process, what you learned and what you're able to come up with, like a better version of, say, bitcoin.
00:16:27.632 - 00:16:58.620, Speaker A: Yeah. So in Prism, we notice that bitcoin has two limitations, really. One is this low throughput limitation that I mentioned, seven, five to seven transactions per second. But the other limitation is how long it takes to confirm a bitcoin transaction, which is, there's a classical number called six deep. You wait until a block is six deep in the blockchain. In the bitcoin blockchain before you confirm. Six deep is 1 hour.
00:16:58.620 - 00:17:33.970, Speaker A: But in fact, from a Nakamoto actually did a calculation on how reliable a certain confirmation rule is as a function of how deep it is. Six deep would only give you like 1% probability of deconfirmation, which is pretty high for a secure system. So really, to get a very secure system, you need hours of confirmation. So bitcoin is slow in two respects. One, the throughput and two, the confirmation delay. And people often mix up these two things. I just want to separate out a little bit more.
00:17:33.970 - 00:17:55.244, Speaker A: So we realized what happened is that in bitcoin, the protocol is seemingly very simple. You just mine on the longest chain. That's it. The longest chain protocol. That's it. So there's one activity, mining. And everybody do the same thing.
00:17:55.244 - 00:18:22.330, Speaker A: Mine on the longest chain. So there is no like, stage one, you do this, stage two, you do this. Stage three, you do this. Very simple protocol. But actually, if you open the hood, it's actually a very complicated protocol in some sense, because when you actually mine a block, there are actually two things you're doing. One is you're adding transactions. You're trying to add transactions onto the ledger.
00:18:22.330 - 00:19:00.808, Speaker A: Two is you're actually trying to vote on the previous transactions that are in the chain already. So you're actually doing two things, although you seem to be doing only one thing. And if you look at very classical consensus protocol, you will see that this voting and proposing is actually totally separate activity. Okay, so. And then we realize. So in some sense, to speed up the confirmation, you gotta improve on the voting process to speed up the throughput, you have to improve the proposing process. Okay.
00:19:00.808 - 00:19:14.082, Speaker A: So by decoupling these two activity, we can, our prism was able to scale up both the voting process and both the proposing process. Okay, so shall I go a little bit more detail?
00:19:14.146 - 00:19:27.498, Speaker B: Yeah, of course. Okay. I think by being able to decouple, both of them allows these unique properties. And I think that goes back to like the first principles of being able to articulate and then kind of separate the different components.
00:19:27.674 - 00:19:57.270, Speaker A: Yeah. So if you look at like classical consensus protocol, typically what happens is that everybody votes at the same time. So that's how they get fast confirmation. Like a lot of people vote at the same time. In bitcoin, you get one vote every ten minutes. So that's very slow. So you basically need six votes to get some, any reasonable level of certainty, and better to get 60 votes, which is 600 minutes, which is 10 hours.
00:19:57.270 - 00:20:38.564, Speaker A: So our idea was we can improve the voting speed by instead of having one chain, we have 100 or 1000 parallel chain. So that you can imagine all these chains are simultaneously voting on a block being proposed. So we separate out the proposing activity, which is in one chain, and then the voting is happening supported by 1000 chains so that you can simultaneously get many votes and that speed up the confirmation. So that was our way of improving the confirmation latency.
00:20:38.652 - 00:20:44.044, Speaker B: Gotcha. And that was on the confirmation side. And then on the throughput side.
00:20:44.092 - 00:21:34.570, Speaker A: Yeah, on the throughput side. So what we realized, one thing is that the reason why bitcoin cannot support high throughput is because if you want high throughput, you basically need bigger blocks. Okay. Either you need bigger blocks or you need to propose, you need to mine blocks more frequently. So those are the two possibilities. But in both possibility, okay, what happens is that you will increase the amount of forking in the bitcoin protocol. For example, if you propose very frequently, you mine very frequently, then before you get to propose your block, other people will be generating new blocks already, and they will be all mining on the same parent.
00:21:34.570 - 00:22:05.120, Speaker A: So that's how forking occurs. And forking will decrease the security of bitcoin. And so what we realized is that we cannot increase the size of the block. We could not increase the mining rate, so we have to keep the same. However, what we could do is to separate out the proposing activity from the transaction carrying ability. So again, it's another decoupling. And so we generate a third type of block now.
00:22:05.120 - 00:22:39.448, Speaker A: So first of all, we have only one block in bitcoin. Then we separate out into proposing transactions and voting blocks. And then a third, we take out the transactions and put them into transaction blocks and have a separate mining procedure for those. And then the proposer is like a leader. Once they get proposed, all they're doing is they're having references, hashes to a bunch of transaction blocks, and that signifies that you want to include the content of those transaction blocks into the ledger.
00:22:39.544 - 00:22:40.460, Speaker B: That makes sense.
00:22:40.990 - 00:22:56.302, Speaker A: And actually, this principle of decoupling is now sort of used in different forms in different consensus protocols. We're just talking about aptos, for example. In fact, that protocol also uses this decoupling idea.
00:22:56.446 - 00:23:31.800, Speaker B: Yeah, no, it's very interesting, as we continue to progress down the research rabbit hole and also getting some of these more advanced consensus algorithms into production, how they continue to change. And it's interesting, they're kind of, at least for the moment, going back to the first principles and taking things apart and speeding those each up. It's very interesting. So did you ever, in Prism, did you ever build out a working protocol in code or GitHub?
00:23:32.740 - 00:23:44.378, Speaker A: Yeah, so we built a prototype. Oh, prism, yes. We could get up to in this prototype, about 40, 50,000 transactions per second.
00:23:44.434 - 00:23:46.950, Speaker B: Wow. And how many nodes was that?
00:23:48.610 - 00:23:58.922, Speaker A: We've ran it on about 100 nodes. Aws nodes. We built a prototype, and I think there are some projects which are trying to take that to a production level.
00:23:58.986 - 00:24:00.130, Speaker B: Wow. Right now, that's amazing.
00:24:00.210 - 00:24:06.282, Speaker A: That's a lot of throughput. It's open source, so I know there's some projects working on that definitely included.
00:24:06.306 - 00:24:32.062, Speaker B: In the show notes. And then I think you mentioned one other thing. There was recently, I think you and a group of your students was awarded a grant with to work with the Ethereum foundation. Could you maybe go into kind of what you've been doing with the Ethereum foundation, what you've been helping them with, and kind of like some of the results of those findings?
00:24:32.166 - 00:25:08.152, Speaker A: Yeah, I think there's an interesting story there. So that project started after I came back, when I started research lab at Stanford to do blockchains. So that was like, in 2020. Yeah. So at that time, the Ethereum foundation published a paper, and the title of the protocol that they were proposing is called Gaspar. Gasparenty, which stands for ghost plus Casper. And this was the protocol for the beacon chain.
00:25:08.152 - 00:25:44.110, Speaker A: The beacon chain protocol. And so we were quite interested in this protocol because we've never seen a protocol which looks like this. Okay. This is not your standard bitcoin long exchange protocol, nor is it the standard so called Bison tan forktorin BFT protocol. It's kind of strange because it's not very often you have new protocols. And so we did an analysis on that protocol, and we were able to find some attacks on that protocol. Okay? So we knew it was not secure, at least not secure according to your basic security models.
00:25:44.110 - 00:26:19.960, Speaker A: So there's a sort of general understanding in the consensus community that there are some models for which a protocol should be analyzed for security, standard models. So we apply those standard models, and we find that this protocol is not secure, and we find some attacks, and we publish a paper in one of the top security conferences. At the same time, we contacted Ethereum foundation. We said, hey, you know, your protocol has some issues here. You know, I think that's the standard thing. When you find some attacks, you should tell the owner of the protocol. And so we start collaborating and say, hey, okay, good, you find some attacks.
00:26:19.960 - 00:26:59.082, Speaker A: How do we improve the protocol? How do we remove these attacks? And so that's how we started collaborating. But, you know, the so what we did was the following. We said, okay, your protocol is not secure. Now maybe we should design a more secure protocol. So there are two directions. One is we can try to tune the protocol, but then we took a sort of different path. We said, okay, instead of trying to improve your protocol by tuning parameters, by changing this little thing here, let's stand back and say, okay, let's apply first principle thinking.
00:26:59.082 - 00:28:03.928, Speaker A: Again, it's like, okay, why do you have such a complicated protocol? What is the goal that you're trying to accomplish that earlier protocols cannot accomplish? Okay. And we figure out that they are trying to achieve two goals. One is called finality, and one is called dynamic availability. And then we realized that actually, there's a theorem somewhere which says that both of these goals simultaneously cannot be achieved by any protocol. Okay, that's one thing, but it was never mentioned in their Gazpr paper. And so we decide to go back and say, okay, so, given that we cannot simultaneously achieve these two objectives, how can we? What's the next best thing? And the next best thing we came up with is a formulation whereby you could create sort of a ledger, which is dynamically variable, which means always growing. And that's what Ethereum wants.
00:28:03.928 - 00:28:16.426, Speaker A: Because they want. Because Ethereum 1.0 is always growing. So they want to keep that. But at the same time, you want to say that a prefix of this ledger is very certain. It's very final.
00:28:16.538 - 00:28:17.114, Speaker B: Yeah.
00:28:17.242 - 00:28:18.682, Speaker A: And that's how Casper do it.
00:28:18.746 - 00:28:31.634, Speaker B: And maybe just for everybody to follow along. Could you go a little bit more in depth on the data availability side. And also just speak a little bit more on the finality side.
00:28:31.682 - 00:28:44.890, Speaker A: Yeah. So it's dynamic availability. So there are many availability. It's quite confusing. I get confused sometimes too. Data availability is another kind of different kind of worm. That's another story.
00:28:44.890 - 00:29:05.102, Speaker A: Dynamic availability means the following. It means that. Think about bitcoin. Okay. Bitcoin started off with very few miners, in fact, Nakamoto himself or themselves. But now it grows to like 200 exahash per second. Okay.
00:29:05.102 - 00:29:53.006, Speaker A: So is a huge range of participation. And it drops sometimes when the market crash, it drops a bit, but yet the chain keeps on growing no matter what. Now, this is not the case with many classical Byzantine for torrent consensus protocol. In those protocols, when the participation is reduced, the chain just halt, doesn't grow anymore. Okay. But those protocol has another property called finality, which means that even if the network partition you can still preserve what is confirmed will not get deconfirmed. So in some sense, existing byzantine for tolerance protocol is like very conservative.
00:29:53.006 - 00:30:23.190, Speaker A: It's like, okay, I want to make sure that there is sufficient people in the room so that whatever we vote on, we have a quorum and nobody will say that, hey, wait a minute. I wasn't there when this decision was made. I don't believe in this decision. We'll make another set of decisions. That won't happen. Okay, but you can see that that's completely contradictory to dynamic availability because there you want to say that no matter how many people in the room, you still continue making decision. So there's no way you can achieve both at the same time.
00:30:23.190 - 00:30:23.774, Speaker A: Basically.
00:30:23.902 - 00:30:24.770, Speaker B: Gotcha.
00:30:25.350 - 00:31:07.190, Speaker A: And however, what you can achieve is to say that, okay, maybe for decisions that are not very important, I don't need a quorum. I just make decision based on whoever is in the room. Okay, that could be a transaction like buying a coffee. However, for important decisions, maybe very high, high value transactions, then I have to wait for enough people in the room and I make those decisions. So those decisions will be further falling behind. The advantage of dynamic variable is that if you buy coffee, maybe you shouldn't have to wait for everybody to show up because I can move forward. And for the important transactions, I'll wait until more.
00:31:07.190 - 00:31:20.952, Speaker A: So the important transactions are lagging behind the less important transactions. So that's the formulation we came up with. And we were able to design protocol which gets the optimal security meeting that objective.
00:31:21.056 - 00:31:36.320, Speaker B: Wonderful. And so you gave that feedback to the ethereum foundation and ultimately, one, they fixed the issues that you were able to articulate and then two were able to incorporate some of the feedback that Yuru gave.
00:31:36.440 - 00:32:09.170, Speaker A: Yeah. So when we came up with this protocol and we talked to uthere foundation, and then we realized that actually, well, they said, fine, good, you achieved these two objectives. But actually, we want to achieve something else more. Is it more? Okay, what is that? Well, something happened during that year is that there is a concept called Mev. Okay. I'm sure your audience, many of your audience, will be familiar with this concept, Mev, which stands for. What does it stand for? Minor extractable value.
00:32:09.170 - 00:32:59.468, Speaker A: Okay? Which basically means that you can try to reorder. So miners try to reorder transactions to maximize the value. And so that was a concept that was never really in consensus, because consensus is really worried about sort of the long term behavior, the short term reorg. And the ordering is not of concern, because one thing strange about consensus is that they care about everybody agree on the ordering. But what that ordering is, is not of consideration in the basic formulation. Okay, it's kind of interesting formulation, but as long as that ordering is stable, they're happy. But this minor extractable value is really about local sort of reorganization of blocks.
00:32:59.468 - 00:33:53.744, Speaker A: And so ethereum foundation at that point, got very concerned about this, because in Ethereum 1.0, there was a lot of re Auger blocks to exploit to extract minor mev. So they said, hey, we actually have something else. We didn't worry about this meV. So now then we start adding a new formulation, new constraint on this problem, which is, how do you design a protocol which satisfies the dynamic availability and the finality, the goals that they were interested in? And in addition, you also want this reorg resistance, which is resistance to MeV. And so that became sort of our main research project working with them. And now we have a protocol that achieves all three objectives, and hopefully this will be deployed in some future version of Ethereum.
00:33:53.872 - 00:33:56.620, Speaker B: And what is that new protocol called?
00:33:56.930 - 00:34:00.290, Speaker A: There's no name yet. We're still writing the paper right now.
00:34:00.370 - 00:34:01.114, Speaker B: Very interesting.
00:34:01.162 - 00:34:03.138, Speaker A: This is like work in progress right now.
00:34:03.234 - 00:34:36.730, Speaker B: Very cool. So I guess maybe like a spicy take. What is your. So, Gasper is going to be going live relatively soon for Ethereum two, and it's going through its different test nets, but hopefully it will be live sometime this year. Since you've done such a deep dive on the research side and have such a background, deep background on the consensus, what is your, I'd say, like, holistic point of view on the algorithm as it stands today?
00:34:40.110 - 00:35:20.802, Speaker A: Yeah, I mean, the protocol is quite complex right now. So it's a bit more complex for my taste, I should say. And so our goal is, in some sense, come up with a simpler protocol that achieves the same objective. So the thing is that you can do research, but there's also deployment constraint. The merge is happening soon. You can't delay the merge for two years to get this research incorporated. So our hope is that in a, you know, every few months, there could be an opportunity to sort of upgrade the consensus, and we're hoping to get into the roadmap in the next one or two years.
00:35:20.802 - 00:35:34.562, Speaker A: That's the hope. Now, the proposal we have, however, is not that far away from the existing protocol. It's not like, completely throw away what you have. So I think we have a decent chance of getting into the roadmap of Euthereum.
00:35:34.626 - 00:35:35.170, Speaker B: That'd be wonderful.
00:35:35.210 - 00:35:37.170, Speaker A: So that's what we are hoping for.
00:35:37.290 - 00:36:33.690, Speaker B: That's awesome. I always find it interesting, just people's journey and how they ultimately arrived. Your story so far is very interesting of you working, doing the research at Berkeley, working at Qualcomm, being early on the broadband and broadband side of things, and kind of bringing that to the masses, then ultimately coming to Stanford, getting into genomics, and then ultimately kind of pivoting again into blockchain and crypto and going down the rabbit hole in there. But now you kind of have started something new and recently raised a good amount of money to bring this vision to life of Babylon. Could you speak to what Babylon is and what your goal or objective is with it?
00:36:33.810 - 00:37:41.586, Speaker A: Yeah. So a lot of the work in layer one has been on how to build a good protocol, a standalone protocol. Now, in our work with Ethereum foundation, one thing we observed, one thing we observed was to design good protocol, you not necessarily have to design everything from scratch. You can take good protocols that solve certain problems, have certain features, and compose them together in a smart way to achieve more features. And that's essentially our solution to the problem of getting finality and dynamic availability. So it's composing existing, off the shelf bison time for torrent protocol and a Nakamoto style protocol together to form a secure protocol. Okay, so that work sort of gives me an inspiration, which is, hey, you know, designing protocol is not about, like, everything every time.
00:37:41.586 - 00:38:06.652, Speaker A: Start from first principle, you can put together various pieces, but if you take a further step, if you take one more step, then the natural question is, hey, can you take not only an existing protocol design, but an existing blockchain and compose it with your blockchain to get some additional benefits. Okay.
00:38:06.716 - 00:38:07.404, Speaker B: Yep.
00:38:07.572 - 00:38:58.690, Speaker A: So that led us to this work, Babylon, which is, suppose you start with a proof of stake blockchain. Okay. And suppose there are some security limitations associated. These chains, which I can go to a little bit later. If you compose it with the most secure blockchain in the world, which is bitcoin, then how can I compose it together to give security benefit to the proof of stake chain? And so that was the high level sort of thinking that started from a research phase. But to progress from research to sort of a project, a real project, there has to be use cases. And so we discovered two use cases.
00:38:58.690 - 00:39:20.410, Speaker A: And those two use cases are kind of a puzzling problem. So the first one is the following. We realized one thing about proof of stake blockchain. Okay, so one. So proof of stake blockchain has sort of two. When people think of proof of stake blockchain, they usually think of two good things about it.
00:39:20.920 - 00:39:25.816, Speaker B: One is green, no less energy usage.
00:39:25.888 - 00:39:53.150, Speaker A: Less energy usage. And two is fast. Right? Bitcoin is slow. It takes, I just mentioned, hours to confirm proof of stake blockchain. Seconds, tens of seconds at most to confirm. Bang, bang, bang. But then we realized that there's one thing about proof of stake blockchain that's very slow, and that is the so called unbonding.
00:39:53.150 - 00:40:10.538, Speaker A: So proof of stake blockchain is about first you stick, you put your coins and that get locked up. And in many blockchains you can be locked up for three or four weeks. You can't take your money out.
00:40:10.714 - 00:40:15.474, Speaker B: And could you explain why coins are locked up for such a long period of time?
00:40:15.522 - 00:40:49.380, Speaker A: Yeah, so that's where I'm going next. But so first I want to lay out the fact that proof of stake is fast, supposedly, but it's very slow in terms of liquidity. It's very fast and confirm it, but very slow in liquidity. And so this is actually not specific to one blockchain. You can observe this in many proof of stake blockchain. For example, cosmos have a 21 day lockup period. Proof of stake Ethereum.
00:40:49.380 - 00:41:29.026, Speaker A: It's not finalized yet, but it's of the order of two to three weeks also. And avalanche, I believe, is three weeks. So all these are very long time. And so the question is why? What's going on here? And it turns out there's a problem called long range attack on proof of stake protocol. It's very specific to proof of stake protocol. And the attack goes like this. You stake your money, okay? And then you unbond, you take your money out but proof of stake blockchain is all about using your stake to participate.
00:41:29.026 - 00:41:36.270, Speaker A: The fact that you've unbonded your stake can actually be still used to build an alternate chain.
00:41:37.820 - 00:41:38.412, Speaker B: Yeah.
00:41:38.516 - 00:42:16.540, Speaker A: Nothing prevents you from doing that. So that means that if you allow very fast unbonding, then a lot of money can be taken out and then they can very quickly build an alternate chain. Now, the reason why you take 21 days is to hope that socially, in other words, humans establish an understanding that the correct blockchain has been growing for 21 days. So kind of like if you go on telegram and people will tell you, hey, this is actually a blockchain. It's been around 21 days. Not the new one. That new one was a fake chain.
00:42:16.540 - 00:42:28.380, Speaker A: So the 21 days is really to allow humans to work jointly with the blockchain to get the final consensus.
00:42:28.690 - 00:42:29.538, Speaker B: That makes sense.
00:42:29.634 - 00:43:05.390, Speaker A: Okay, now, you never see that in bitcoin. In bitcoin there's no such unbonding and there are no humans involved. So that is sort of a limitation very specific to a proof of stake blockchain. Okay, so is that clear enough? Explanation the problem. And so that's why 21 days. This problem is very well recognized and that's why this long and bonding period is kind of built in to most of the proof of stake blockchain. Gotcha in the world now.
00:43:05.390 - 00:43:52.860, Speaker A: So the real problem here, why you take 21 days is that it is basically saying that during this long time, they don't expect everybody to be around every minute. You see, this problem doesn't occur if everybody running this blockchain or every client in the blockchain is actually actively observing the system. Okay, so this long time has allow you to take a break and then come back again. Okay. As long as you come back within 21 days, you're still fine. Okay. So what is missing really is someone to observe the system continuously.
00:43:52.860 - 00:44:16.660, Speaker A: And a proof of stake blockchain by itself has no such person. So that's why it requires a society social consensus. So our idea of Babylon is that we can use bitcoin, which is always on, always running, always keeping track of time of events. It can be the constant observer.
00:44:17.730 - 00:44:18.602, Speaker B: That makes sense.
00:44:18.706 - 00:44:23.666, Speaker A: And that was the first use case of bitcoin.
00:44:23.858 - 00:44:47.550, Speaker B: Interesting. That was kind of the light bulb moment for Babylon and its creation. Yeah, interesting. Very cool. And I think as we were talking earlier, there's one other interesting part that you aim to solve or bring to light with Babylon. Maybe go a little bit deeper into that second.
00:44:47.630 - 00:45:23.000, Speaker A: Yeah. So there's a second use case of Babylon. And the second use case of Babylon is the following. So we talked about a little bit, we touched a little bit about the scalability issue. Right now there are actually, you can think of sort of what does scaling blockchain really means. It means that you want to handle as an aggregate a very high throughput. And one reason why sort of a single platform is very hard to do that is because it's like one set of validators all doing the same thing.
00:45:23.000 - 00:46:05.820, Speaker A: And there's limitation due to computation, storage, memory, etcetera. So one way of scaling blockchain is actually for different instead of having many applications use the same platform, like Ethereum, is to have each application to build their own blockchain. So you have an NFT application or DeFi application or gaming application, you build your own blockchain. And so all these can coexist. They're running parallel, and maybe there's some bridging between them to communicate, but at the heart is parallel processing. So that's another approach. That's one approach to scaling blockchain.
00:46:06.910 - 00:46:12.470, Speaker B: And is that with the Babylon execution environment just natively parallel?
00:46:12.590 - 00:46:26.022, Speaker A: So, no. So I guess I'm making general comments. So to be more concrete, for example, Cosmos, Cosmos is an ecosystem which is designed for different applications to build application specific blockchains.
00:46:26.046 - 00:46:26.630, Speaker B: Yes.
00:46:26.790 - 00:47:17.270, Speaker A: Okay. So one problem of this though is that now the security is kind of fragmented because different blockchain by its nature is small, small token values, more number of validators. And so the security will not be as good as, say, Ethereum. Okay, and so what Babylon does is to say that, okay, let's keep your autonomicity of this blockchain separate, but we can protect by providing security on top of all these blockchains, so that when they are in good situation, they're running normally, but in case there's something bad happening, then they can always turn to bitcoin to provide an additional security benefit.
00:47:17.570 - 00:47:18.630, Speaker B: That makes sense.
00:47:18.970 - 00:47:34.302, Speaker A: So that's sort of the second use case of having, using Babylon, which is using bitcoin to provide additional security benefit. And so this is in some sense you can think of as a soft way of doing security sharing.
00:47:34.406 - 00:47:35.022, Speaker B: Interesting.
00:47:35.086 - 00:47:38.750, Speaker A: As opposed to Ethereum, which is kind of a heavy way because everybody's using the same platform.
00:47:38.830 - 00:47:54.038, Speaker B: Yeah, I know bitcoin is relatively limited in the amount of smart contracts that you can do on that. Is it relatively complex to interact with it?
00:47:54.134 - 00:47:55.966, Speaker A: No, it's my contract needed in our technology.
00:47:56.078 - 00:47:56.710, Speaker B: Oh, really?
00:47:56.830 - 00:48:33.342, Speaker A: Yeah, we basically, what we need is to send very. So bitcoin not only have smart contract natively on bitcoin. It also has very low throughput, as we mentioned earlier. So you cannot put huge amount of data onto bitcoin. So our technology is basically compress all this thing we have to put in, in two transactions every say ten minutes or every hour depending very low frequency, very small number of transactions.
00:48:33.446 - 00:48:34.102, Speaker B: Gotcha.
00:48:34.206 - 00:48:45.810, Speaker A: And we're basically putting those, what we call checkpoints and we can interpret those checkpoints in an intelligent manner to provide the security benefits that I mentioned earlier. That's it.
00:48:47.630 - 00:49:03.608, Speaker B: So it's kind of like a L2 in some sense. I mean, not like just in how you're compressing the data or infrequently transacting back down to the layer one to try to optimize space and throughput.
00:49:03.744 - 00:49:11.824, Speaker A: Yeah. So in a way is. So L2 is of course one way of thinking about L2 is rollups, right?
00:49:11.872 - 00:49:12.568, Speaker B: Yep.
00:49:12.744 - 00:49:36.070, Speaker A: So rollups do not have their own consensus engine. They rely entirely on, say, ethereum to do the consensus. In our design, the proof of stake protocol, the blockchain is still autonomous. So they're still running the consensus, but they're getting some help from bitcoin in a light way.
00:49:36.230 - 00:49:36.870, Speaker B: Gotcha.
00:49:36.950 - 00:49:51.980, Speaker A: Not an intrusive way. So in typical, running your own consensus validators is doing the job only when it comes to either emergency or very expensive transactions. Then you turn the bitcoin to get additional security.
00:49:52.520 - 00:50:03.416, Speaker B: That makes sense. Very cool. Now I'm excited to have you bring that to life. Yes. How far along are you in the process? Do you have a testnet?
00:50:03.608 - 00:50:15.582, Speaker A: No, not yet. We're building the testnet right now. We're hoping to have an mvp in the about two months. And then. And then we build it up to a testnet by the end of the year.
00:50:15.646 - 00:50:15.966, Speaker B: Wonderful.
00:50:15.998 - 00:50:17.638, Speaker A: So that's what we're shooting for right now.
00:50:17.774 - 00:50:23.890, Speaker B: That's amazing. I'm excited for that. What type of like builders or applications do you wish to see on Babylon?
00:50:24.310 - 00:50:32.286, Speaker A: Ah, so the. So the Babylon is like a middleman.
00:50:32.438 - 00:50:33.062, Speaker B: Okay.
00:50:33.166 - 00:50:59.106, Speaker A: So it is on the one side getting security from bitcoin. On the other side it is allowing proof of stake blockchains to subscribe to Babylon, to send information to Babylon to get the security. So Babylon is like a platform whose customers are proof of stake chains. But we are using the bitcoin as a source of security.
00:50:59.218 - 00:51:02.380, Speaker B: Gotcha. So it can work on any proof of stake chain?
00:51:02.490 - 00:51:21.900, Speaker A: Yes, any proof of stake chain. But our current focus is on Cosmos chains because that's a natural ecosystem. Because as I talked about. We have application specific, relatively small proof of stake chain using Babylon to get bitcoin security. So that's the architecture that we are working on right now.
00:51:22.880 - 00:51:37.812, Speaker B: That's awesome. I'm very interested to see it come to light and see how people use it. Which proof of stake chains use it? How many cosmo chains get on board? Because. Yeah, that bootstrapping of proof of stake networks can be a challenge.
00:51:37.876 - 00:51:38.520, Speaker A: Yes.
00:51:40.620 - 00:51:41.612, Speaker B: Very interesting.
00:51:41.716 - 00:51:42.044, Speaker A: Cool.
00:51:42.092 - 00:52:36.892, Speaker B: Well, I think we're approaching an hour. Kind of hit a majority of the topics that we initially discussed. Maybe just going back to the scaling topic and based off, like, a lot of your research that you did in your earlier years, you mentioned to increase throughput, you have to have larger blocks or faster block times. Do you kind of think, like, that data throughput is the largest limitation to blockchains? Or do you think it could be like the execution environments or consensus algorithms? I guess like an overview. What do you think it will take the blockchain and crypto community to get to hundreds of millions of users? Loaded question. Loaded question.
00:52:37.076 - 00:52:41.556, Speaker A: You promised me in the beginning that there would no, you can pass.
00:52:41.628 - 00:52:43.120, Speaker B: You can pass. You can pass.
00:52:46.660 - 00:53:40.916, Speaker A: I mean, I really feel that. I really like, in some sense, the cosmos design in the sense that or this general concept of application specific blockchain. So I see that as sort of one really good, because you are scaling both execution, communication, storage all by just having relatively independent blockchain, but building bridges between them. So that's the cosmos design, right? So you have chains, and then there's this thing called IBC inter blockchain communication protocol to allow them to communicate each other. Because at the end of the day, not everybody have to interact with everyone at all, every day, every second of the day. So there's some natural decoupling between the communication. And so these application specific blockchain, I think, captures that pretty well.
00:53:40.916 - 00:53:56.338, Speaker A: So there's a natural scaling here. And so I kind of think that solving the lack of security sharing can sort of push that vision further.
00:53:56.434 - 00:54:08.950, Speaker B: That makes sense, designing the blockchains for their specific applications and making sure those blockchains are secure for everybody to be able to interact with them.
00:54:09.290 - 00:54:09.930, Speaker A: Yeah.
00:54:10.050 - 00:54:33.676, Speaker B: Nice. I really appreciate the conversation, David. I was really looking forward to it, and it did not disappoint. It's cool to just hear your background, your history, all the research that you've done, your first principles analysis to all the areas, especially new and uncharted areas. So I really appreciate your time, and thank you so much.
00:54:33.828 - 00:54:36.716, Speaker A: Great. Thanks so much, Logan. It was so fun talking to you.
00:54:36.828 - 00:54:37.780, Speaker B: Wonderful. Thanks, Dave.
