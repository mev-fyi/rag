00:00:00.280 - 00:00:14.450, Speaker A: Fast forwarding in the future, what problems or core engineering challenges will continue to be addressed even like ten years in the future? Or what would you, if you could wave a magic wand, what engineering problems would you want to solve at scale?
00:00:15.190 - 00:01:21.490, Speaker B: Yeah, so I think that as we scale the number of roll ups and we want to maintain composability between applications, then I think like a fundamental problem is, even in the shared sequencer architecture, fast propagation of state across different nodes that are interacting in the system. I mean, for composability, you need to have that data propagate very fast. And so the challenge of doing so in a decentralized environment, even if the number, the decentralization of actors participating in shared sequencing, that is ultimately driving the progress forward is a smaller number of actors in this set of people participating in ethereum validation, or even hotshot finality gadget DA systems, etcetera. This fundamental challenge of being able to quickly propagate information across a distributed set of actors is going to be an important problem to solve.
00:01:25.070 - 00:01:42.010, Speaker A: Awesome, Ben, thank you so much for joining me. Really look forward to this conversation and congrats on the recent announcement for the raise. I know in this market it's no small feat. And so excited to see what you and the espresso team continue to build.
00:01:42.710 - 00:01:50.792, Speaker B: Thanks, Logan. Yeah, it's really a pleasure to join you on the podcast here. I know we've been trying to do this for some time. So happy to finally get on here.
00:01:50.936 - 00:02:24.660, Speaker A: Likewise. Well, I want to just kind of jump right into it, and I would love for you to maybe paint a little bit more of a broad picture before we jump into what espresso is doing on the more technical merits and really highlight where you think Espresso's role is going to be and the blockchain system more broadly. And if that question is maybe a little bit too specific, we can talk about the difference, what different blockchain architectures are trying to optimize for.
00:02:26.400 - 00:04:19.620, Speaker B: Great. That's a really great question. So let me try to sort of approach this in the way that I explain the scaling issue to people who know nothing about blockchains, which I, you know, get asked about often. And I realize, whoa, actually there's a ton of context here because people coming in don't necessarily understand, like why do blockchains have scaling bottlenecks in the first place? And it's sort of important to understand that before understanding what role something like espresso plays at all. So the scaling issues of blockchains largely come from artificial constraints that are introduced for trying to achieve various principles around decentralization. And of course, there are different philosophies around what level of decentralization is important and how that ends up leading to different goals that we have, such as anti monopolization, easier to use systems, systems that get many, many people to be able to participate in and share state, etcetera. Without going into any of those, of those in detail, I think we can look at the scaling problem through the lens of how many people are you trying to get to participate in a blockchain system? And what are their operational requirements? And so, one view of this, which is sort of the view that Ethereum has taken, at least implicitly, in their roll up centric roadmap, is that the number of actors that are driving the system forward doesn't necessarily need to be that large.
00:04:19.620 - 00:05:23.200, Speaker B: And they can be fairly sophisticated actors. I mean, any of these roll ups, their sequencers are extraordinarily sophisticated, especially ZK rollups. I mean, they're using extraordinary amount of computational power to generate ZK proofs. It is permissionless in the sense that anyone can join that. But the operational requirements of running a roll up are extremely high, and those are the actors that are actually driving the system forward. And then on the other hand, you have the number of nodes participating that are providing verification and lending to security, or after the fact integrity of what happened. And so, Ethereum's perspective is to create a, whether it was intentional or sort of unintentional, but certainly implicit in the role of scaling view is that you can have a bifurcation between the nodes that are driving the system forward and the nodes that are participating in verification.
00:05:23.200 - 00:06:45.658, Speaker B: You know, other blockchain system architectures may take a similar approach, but maybe a more top down approach of saying like, okay, we can actually select the actors, we can select us, we can sample a small number of actors to participate in driving forward transactions. We can, in an organized way, sort of shard the work of generating some form of proof. That becomes helpful advice to what people generally call light clients, which are nodes that are not driving the system forward, but just lending to verification. And yet other architectures might say, you know what? There's no reason to have a bifurcation between the nodes that are verifying and the nodes that are driving the system forward. The main purpose of this system is to get a distributed fault tolerance, but to have strong state synchronization and to be geo distributed, and just to achieve as high performance as we can within that sort of minimally distributed architecture. And we can have the same nodes participate in verification and driving the system forward. Where espresso comes in is, and I haven't thought about it in great detail beyond the Ethereum ecosystem.
00:06:45.658 - 00:08:31.410, Speaker B: But where espresso comes in in the Ethereum ecosystem is from the fact that because Ethereum is scaling in this, not this top down, but sort of bottom up in organic way, where anyone can come in and build one of these roll ups, that is this, again, this sophisticated today runs as some sophisticated entity that has a centralized sequencer and is driving for transaction processing in a way that can be then verified easily by the rest of the ethereum network. Sort of bottom up approach results in a lot of fragmentation of transaction processing, and it erodes what I think is one of the core value propositions of web3 systems, which is the ability to get many applications on the same system and some form of state synchronization between them. The ability of a user to say, hey, I have dollars, I want to swap them to euros and then buy this NFT. But I don't want to do these things asynchronously because, well, what if I don't end up buying the NFT? Then I just spent money on swapping my dollars for euros and paid the slippage costs and paid the bridge costs, and I don't want to do that. So, and of course there are more sophisticated examples with flash loans, et cetera, that we don't need to get into. But anyways, that's what I see is what differentiates web3 from web two. And so what espresso provides is within this sort of organic bottom up scaling ecosystem of different roll ups, a marketplace through which rollups can sell sequencing rights to sophisticated actors that are bidding for the right to propose transactions for multiple rollups in order to enable these more synchronous interactions.
00:08:32.590 - 00:08:57.520, Speaker A: Yeah, there's a lot to dive in there, I think maybe starting with kind of the artificial kind of node limitations, how do you see those kind of playing out over time with L2s or these different roll up ecosystems playing with some of those constraints?
00:09:03.940 - 00:09:54.676, Speaker B: Yeah. So L2s within Ethereum, if we're looking at L2s within Ethereum, I think that again, this is not necessarily a conscious choice of the roll up scaling roadmap, but naturally roll ups are this roll up centric roadmap and L2s in general are reducing the number of parties that are of participating in driving progress forward. It's permissionless to join in that you can join the system as a sophisticated actor. You can run your own roll up. Create your own roll up. But it's preserving the same decentralization that Ethereum has by reducing the amount of work that ethereum does. But then Ethereum is not really participating in driving transaction processing forward.
00:09:54.676 - 00:11:04.160, Speaker B: It's just acting as sort of a settlement layer, as people are calling it. And I think that then when you look at L2s, that some L2s may actually prefer to have their own centralized sequencer, which is like the extreme version of this bifurcation between number of nodes driving progress forward versus number of nodes participating in verification. Like a centralized sequencer is one node, and then you have roll ups that may want to decentralize their sequencers, so to say. But even so, in order to preserve the performance, the number of nodes that even in a decentralized sequence or architecture, the number of nodes that are actually driving progress forward is going to be a more sophisticated set of actors. So if people are, for example, joining the espresso network as a form of sequencer decentralization, espresso is designed in a way that's sort of in line with this roll up centric roadmap, where there can be asymmetry between number of nodes participating in verification and data availability versus the number of nodes that are actually driving progress forward.
00:11:05.220 - 00:11:25.498, Speaker A: Yeah, it is an interesting roadmap. I'm very excited to see how it plays out long term and how many different roll ups ultimately come about. Do you think in maybe this future end state, are there going to be millions of rollups, hundreds of roll ups? What's kind of your.
00:11:25.594 - 00:12:03.930, Speaker B: That's a really good question. Yeah, well, it's hard to say with a straight face there's going to be a million roll ups. I've heard 100 million from someone and I'm like, there are only something like 3000 applications on Ethereum today. So let's first ask how many applications there will be, because I don't think there will be more roll ups than applications. That just wouldn't make any sense. Right. So, because every roll up is an application in the end of the day, and in fact, I view rollups as synonymous with applications, there are just some roll ups that are super applications which are basically general purpose vms.
00:12:03.930 - 00:13:23.810, Speaker B: So we have special purpose vms which are down to an application like, I don't know, uniswap versus a very general purpose VM, like arbitrum, optimism, Zksync, Starkware, etcetera. I think that we are probably going to see only a handful of general purpose vms that are competing with each other for developers on maybe different features or aspects of these different general purpose vms. I can't imagine there being 3000 general purpose Vmsheen. I mean there's a power law here, and any developer, in any developer tooling space, there are a number of top tools that end up bubbling to the top and they're competitive. And we don't have 3000 equally competitive tools that developers choose from. Nobody has the bandwidth to choose between 3000 options. But I do see every application potentially, or most applications developing their own rollups.
00:13:23.810 - 00:14:39.500, Speaker B: And we're seeing that today where an application would always rather, especially through the accessibility of using rollup as a service, services that make this very easy with three clicks, you would always rather run your own roll up or chain because of all the benefits that it affords you economically. And in terms of making your gas price isolated from the gas prices and congestion of other applications. The one caveat is synchronization with other applications and sharing state with other applications. So there's this tension between run your own chain versus run in the same virtual machine as another application for the purpose of interacting with it. Solutions like the ones that espresso are developing and I think what's also just naturally happening within this roll up ecosystem of achieving interoperability without being in the same vm are going to also make it easier for applications to have their own roll ups because it's lowering the downside of being in your own environment.
00:14:40.850 - 00:15:28.150, Speaker A: So a couple, just to reiterate, a couple like general purpose chains kind of totally aligned there and then for specific applications, whether they want to customize the stack, have their own execution environment, get to a sufficient amount of scale, do their own custom roll up. It comes with that trade off of you can kind of customize your stack a little bit bored, but it's a little bit more sovereignty, a little bit harder to do data synchronization and composability with other applications. But that's where Espresso ultimately comes in, and espresso and comes in with shared sequencing to really enable that.
00:15:29.850 - 00:15:30.630, Speaker B: Yeah.
00:15:31.650 - 00:15:47.120, Speaker A: And maybe like if we were to go down like specifically with Espresso, can you talk a little bit more from a technical standpoint about how you're doing the shared sequencing between all these L2s?
00:15:48.620 - 00:17:28.200, Speaker B: Yes. So one thing to start out with is the fact that Espresso doesn't, it's not a shared sequencer that sort of forces all roll ups that integrate with Espresso to use a common sequencer, which is what you might think if you first hear about Espresso or you first hear about a shared sequencer. So it's actually what I like to call more of a system for ad hoc shared sequencing, whereby organically roll ups can end up being sequenced by the same party. But this is done through a market mechanism that we introduce. But maybe before going to the market mechanism, let's just look at a shared sequencer itself and what that enables. So a shared sequencer for a given time slot is what I call a single node or protocol that is in charge of proposing the blocks or transactions for two different chains, roll ups, whatever you want to call them at the same time within those time slots. And when you look at most blockchain architectures, the leader, like whether we're looking at Solana's protocol or Cosmos protocol or Ethereum protocol, all these protocols, they tend to have some kind of leader that for a given time slot is proposing the set of transactions that will be appended to different applications within the system.
00:17:28.200 - 00:18:56.612, Speaker B: And because there's one party doing this, that party has the ability to enforce whatever atomic constraints that it wants across what it does on these different applications at the same time. Now, one of the differences between just sharing a sequencer and actually being in the same execution environment is the ability of users to sort of encode these atomic constraints. So with a shared sequence or architecture, but different execution environment, there's no way within the execution environment to specify these constraints. But it's something that the proposer that is simultaneously building the blocks for these different roll ups, chains, whatever at the same time can promise to users. And we can use other mechanisms for enforcing those promises, economic bonding, etcetera. There's also a line of work now within, particularly within the ZK rollup space, of ways that we can allow different execution environments to actually specify cross chain dependencies that we could have a transaction on one roll up. Say you can only process me if something or another happens on a different roll up.
00:18:56.612 - 00:19:41.980, Speaker B: And this has to ultimately be verified in the ZK proof that updates my state. And so shared sequencing also helps with this, because if there's one party that you need to have a, some party now simultaneously build two blocks for the different roll ups that actually satisfies these constraints. Specifying the constraints is a non deterministic thing we can say. Like if these constraints are satisfied, then I get processed, but you actually need a party to then go ahead and make sure and coordinate the block construction across these two different roll ups in a way that ends up satisfying the constraints. So that is what a shared sequencer enables. It is sort of an essential piece of infrastructure for enabling more synchronous interactions between independent execution environments.
00:19:43.920 - 00:20:37.520, Speaker A: So maybe if I could re articulate just for the audience and myself, you're going to have these independent L2s that are kind of siloed, individualized. They have their sovereignty within them. But what shared sequencing allows is for the user of a particular l two to do shared sequencing if they elect with another layer or two. And that shared sequencing doesn't have to happen for all transactions. If you want, you can just keep transaction isolated to that specific L2. But if you want to, you can do that across multiple L2s or a variety of different L2s to bring back that composability or synchronization of different applications across them.
00:20:38.300 - 00:21:43.908, Speaker B: Yeah, that was a good summary of what I was saying with one caveat, which is that a user can't really decide whether shared sequencing happens or not. This is more of a roll up decision. Two rollups in a given time slot could decide to be sequenced together, which means that there is a common party, a common actor, system, component, server, whatever you want to call it, that is actually in charge of appending the next set of transactions to these two different roll ups at the same time. And that party is then in turn able to make promises to end users about atomic conditions across their transactions. So a user may rely on such a party for cross chain liquidity. It may rely on such a party to enforce that one transaction on one roll up succeeds only if their transaction on a different roll up succeeds, or that only one of two transactions will succeed. So they tried to do two things on two different rollups, and they want to promise that only one of the two will actually succeed.
00:21:43.908 - 00:22:02.640, Speaker B: I'm trying to buy, I don't know, a product in two different systems, but I only want them to go through. So that is the only caveat that users can't really drive that decision. It really has to be the roll up set for a given time slot, opt into being sequenced by a common party.
00:22:03.180 - 00:22:27.140, Speaker A: So both l two s opt in, or there's a variety of l two s that can opt in. And then is it at the application level that they said, hey, this application, now that we have our time slot to do shared sequencing, can you please do the shared sequencing across the different L2s? Or is it strictly only at the L2 to opt in and only that L2?
00:22:29.040 - 00:23:38.010, Speaker B: In principle, this can actually be add an application layer. We thought about this a little bit. The reason why we are focusing more on doing this just at the L2 level is it can get really complicated if you have a shared sequencer for certain applications, but then distinct sequencers for the overall l two, because it brings into question what the abilities of these different sequencers are in terms of what they can promise over the pre and post state. If you have complete autonomy over the next set of updates to the role of VMDez, you can make any kind of promise over the state, the resulting state that you want. But if what you do could be affected by what a different sequencer does. So you're sequencing for like Uniswap, but there's a different sequencer that's actually sequencing for accounts, then the Uniswap transactions are dependent on the balances and accounts. And that makes it more difficult for you to promise some kind of end state.
00:23:38.010 - 00:24:23.714, Speaker B: But it is interesting and maybe that something that will naturally arise from what we're doing, because what I didn't really get into is how we're enabling roll ups to opt into shared sequencing. It's through this market mechanism which allows them to essentially auction off their sequencing rights. Because of the way that we're generating this, it may organically lead to applications auctioning off individual sequencing rights and not just roll ups, but that is not something that we are actively designing around at this stage. We're trying to solve it first at a L2 level before going to the application level.
00:24:23.882 - 00:25:02.480, Speaker A: That makes sense, one level higher before going deeper. Once these say a variety of L2s opt in, are they now using espresso for the shared sequencing indefinitely until they kind of reopt out of those surfaces. So every transaction going forward has the possibility to be atomic, if that can work deterministically, or if they want to interact with other blockchain applications shared state, they can do so.
00:25:03.500 - 00:26:29.030, Speaker B: So what happens when a rollup integrates with Espresso is that they are integrating into participating in this marketplace so that if they sell the writes to sequence for a given time slot, that sale is final and can't be reneged. But that also means that a roll up will not necessarily be sequenced by a shared sequencer. In every time slot, rollups can set what we call reservation price, and that price needs to be cleared in order for their sequencing rights to be sold to a third party. So you can end up in a situation where some blocks are being sequenced by the rollups default sequencer or sequencing protocol if it wants to provide a default consensus algorithm or something like that, and then other time slots are being sequenced by third parties that aren't native to that roll up. It remains to be seen how this marketplace pans out. And maybe we'll see all, maybe cross domain interactions are so valuable that we'll see really all roll ups just always being sequenced by third parties. Maybe we'll see a mixture, maybe this will look different for different types of roll ups.
00:26:30.250 - 00:27:02.800, Speaker A: Yeah, no, it is interesting. I mean, when you enable markets, it's often surprising what comes about in these permissionless settings. I think throughout the years I've continually been surprised what works and what doesn't work, but always keeps us entertained in terms of sequencing multiple different L2s. Can you dive into that a little bit deeper from a technical perspective, how you do consensus among them?
00:27:03.220 - 00:27:30.842, Speaker B: Yes. Yes. Good. So I guess today let's first look at what a sequencer does for rollup. A centralized sequencer. So a centralized sequencer is playing a few different roles. In a temporary window, it's proposing the next list of transactions.
00:27:30.842 - 00:28:25.614, Speaker B: It's providing what we would call pre confirmation or pre finality on these transactions. If you trust the centralized sequencer, you actually get finality from the centralized sequencer. The centralized sequencer has the ability to, if it's honest, ensure that any of the transactions that it's confirming will eventually appear on Ethereum. They'll eventually be reflected in the state of the roll up contract on Ethereum, or if we're on different layer one. When I say Ethereum, I'm just talking about a layer one in a roll up architecture. The sequencer is also providing a temporary data availability guarantee, which is to say that until these transactions are actually posted to Ethereum, it preserves the data. And the sequencers don't post transactions to Ethereum all at once as they come in, because that will erode their ability to compress data.
00:28:25.614 - 00:29:14.310, Speaker B: So they wait until they collect a buffer of transactions and then they compress it and they post. So in that interim window, they're also acting as a, as a trusted centralized DA system. So you need to preserve those properties. When you get two different roll ups to share a third party sequencer. Of course, the layer one, or ethereum, ultimately ends up providing data availability and shared consensus for what happened. But in that interim time window, before Ethereum is reflected on Ethereum, you do need to have some form of shared consensus, shared DA, over what is being proposed by this shared sequencer. And that's something that espresso provides through a protocol called hotshot.
00:29:14.310 - 00:29:46.760, Speaker B: Hotshot is this extremely high throughput, optimistically responsive consensus protocol that operates quite differently from Ethereum, operates on much faster timescales, but ultimately can have theoretically as many nodes as Ethereum participating in keeping it honest. And we actually plan to try to engage l one validators to participate through restaking mechanisms in the protocol.
00:29:48.580 - 00:30:32.600, Speaker A: I know there's a variety of different consensus designs. I think avalanche has done some really cool things there with probabilistic consensus bullshark or narwhal. Some things that Solana doing as well is interesting in terms of optimizing for latency. Is there a particular thing that espresso is optimizing for? Is it to allow higher node count? Is it trying to reduce the latency for data synchronization? Or is it just trying to be more broadly general purpose consensus layer among all the L2 s?
00:30:34.100 - 00:32:20.808, Speaker B: It is definitely optimizing for low latency and high throughput. And it is adopting what I say is the sort of implicit principle of this roll up centric architecture, which is that you can have a sort of bifurcation between two types of nodes in the system, those that are contributing to its security and its integrity, and those that are actually driving the progress forward. This is also reflected in the principles of what people call optimistic responsiveness, which means that if everything is good and the network is well connected and, and the most powerful nodes in the system are alive and well, then the system can make progress at web two speeds, whereas you also have the guarantee that in the worst case, yes, performance may not be guaranteed, but security safety is guaranteed and even liveness is guaranteed. It's just that you don't necessarily make progress at blitz speeds. And our experience is that these systems, 99% of the time are participating, are performing well, and that the situations in which you have a lot of nodes going down are sort of far and few between. And you want to have a guarantee that when nodes go down that you maintain security, maintain liveness as well. But a lot of these blockchain architectures, like Ethereum itself, are sort of not optimizing for these 99% best case scenario.
00:32:20.808 - 00:33:17.430, Speaker B: They're optimizing for worst case scenario, and that ends up compromising performance in the best case. So hotshot is actually built off of the hot stuff system. So it has many similar principles to like Narwhal and other systems that are, you know, also coming from that, you know, a common ancestry of work. It, it is leveraging basically like a high performance content delivery network that is backed up by a very, very large set of participating actors, that plays very well with this ability of optimistic, really responsive protocols to again perform very well when everyone's well connected and still remain robust when the system is not as well connected.
00:33:18.330 - 00:33:27.306, Speaker A: And are those so generally limited by the one third and two thirds kind of like key thresholds?
00:33:27.498 - 00:34:14.400, Speaker B: Yes, yes, I so certainly for safety and also for liveness, you need to have two thirds of the node online to participate in signing things to make progress. And the key thing though, is that two thirds of the nodes could be on the optimistic path, connected through a content delivery network that is run on a smaller, powerful set of nodes. And if that goes down, you can still communicate over these gossip protocols, but you're not communicating over gossip protocols. In the best case, you're communicating through a more high performance architecture that allows you to make progress at very high speeds.
00:34:15.100 - 00:34:27.438, Speaker A: Gotcha. Having larger hardware requirements or beefier nodes with faster Internet connections generally in the most optimistic scenario, or at least having.
00:34:27.494 - 00:34:44.130, Speaker B: Higher requirements on the nodes that are facilitating fast interactions, where weaker nodes can participate in contributing a signature, but they just need to upload or download a piece of information from a very high power node.
00:34:44.430 - 00:35:44.650, Speaker A: Okay, interesting. Some of the more feedback that I've heard from the market, kind of along the lines is some of the nice properties that L2s have was kind of that centralization aspect for increased performance. But when you add back sequencing to that overall sequencing, to do the cross kind of chart or cross l two communication, you're reintroducing some of those original bottlenecks, so to speak, with original layer ones. Do you feel like espresso is optimizing that in a different use case than layer ones? Or do you feel like some of the issues around scaling consensus and doing block proposers will most likely be similar to layer one architectures today?
00:35:46.350 - 00:36:35.782, Speaker B: Yeah, well, maybe, and sorry, this isn't the most direct answer to your question, but I want to slightly challenge, I guess, the premise of the question. Sure. So in an architecture with a shared sequencer, the shared sequencer is what we call the node that is actually proposing for the two rollups at the same time. And what it proposes ends up getting persisted and, you know, and finalized by a, some kind of consensus algorithm, right. Which would like to be higher performance, higher, lower latency, higher throughput than ethereum, and then this is ultimately settled on ethereum. Right. And so if you compare that to what rollups today are doing with centralized sequencers, it's actually quite similar.
00:36:35.782 - 00:37:46.570, Speaker B: It's just that the two roles of proposing and then persisting the data and maintaining finality are done by this one node. But from a user's perspective, if you trust the proposer. So if you trust the shared sequencer in the shared sequencing world, then you can get a 100 millisecond confirmation from this shared sequencer, and you can trust that it's going to end up sending its data to whatever DA layer, finale layer that's being shared. You don't actually need to wait for confirmation from the consensus algorithm. And that's very similar to the choice of a user to trust a centralized sequencer instead of waiting for Ethereum confirmation. So what you get in this other world is sort of now like a three layered system where like first you get the proposal, and if you trust the proposal, then, you know, you can optimistically, you know, move on with your life. If you want to wait a few more seconds, then you wait for confirmation from the shared finality gadget and bridge providers, liquidity providers may want to do so, for example, because they have a lot of, you know, a lot to lose on the line.
00:37:46.570 - 00:37:55.010, Speaker B: And then if you really don't trust anyone and you're wearing a tin hat, then you wait for Ethereum to finalize everything.
00:37:55.670 - 00:38:09.430, Speaker A: Okay. Makes a lot of sense. So if you trust like the pre conformations today with L2, you should continue to trust the pre confirmations with L2 and espresso. Because.
00:38:11.490 - 00:38:37.150, Speaker B: As an end user, yeah, as an end user, I would say so as an application on the different roll ups, maybe you want to wait for a shared finality because you don't trust the, like you trust the sequencer for your own ecosystem today, but you don't necessarily trust a shared sequencer, then maybe you will want to wait a few more seconds for confirmation from this shared consensus protocol.
00:38:38.090 - 00:39:29.470, Speaker A: So maybe fast forwarding like ten years, do you feel like getting, bringing these blockchain ecosystems? Maybe if we don't have hundreds of millions of L2s, but 10,000 L2s just for applications growing across these various ecosystems, and some general purpose L2s that have also ultimately survived and thrived. Do you feel like the key bottleneck will be making sure that latency across all these different ecosystems is fairly low in kind of the consensus design or just fast forwarding in the future, what problems or core engineering challenges will continue to be addressed even like ten years in the future? Or what would you, if you could wave the magic wand, what engineering problems would you want to solve at scale?
00:39:32.510 - 00:41:21.930, Speaker B: Yeah, so I think that as we scale the number of roll ups and we want to maintain composability between applications, then I think like a fundamental problem is, even in the shared sequencer architecture, fast propagation of state across different nodes that are interacting in the system. I mean, for composability, you need to have that data propagate very fast. And so the challenge doing so in a decentralized environment, even if the number, the decentralization of actors participating in shared sequencing, that is ultimately driving the progress forward is a smaller number of actors in this set of people participating in ethereum validation, or even hotshot finality gadget DA systems, etcetera. This fundamental challenge of being able to quickly propagate information across a distributed set of actors is going to be an important problem to solve. And you know, I don't have a concrete suggestion around that, I think, but I think that is a problem that the whole web3 industry is going to face, or for fundamental reasons. And sort of let me bring in something that you mentioned in the last question as well, which is, aren't you reintroducing these same issues when you need to have shared consensus among different roll ups? So the answer to that is that the type of the shared finality across different roll ups or shared DA across different roll ups doesn't require. It doesn't require sharing state.
00:41:21.930 - 00:42:22.068, Speaker B: And that is sort of the fundamental difference between the hotshot finality gadget and DA system that the shared sequencing architecture of espresso is leveraging, does not have to do the same things that ethereum or Solana have to do. It is leveraging the fact that you do not have to have all nodes in the system receive the data. You can have razor coded shards of data that are being distributed, and things are being agreed upon in a way that prevents a cocation data. Things are being agreed upon in a way that prevents data from being lost as long as a certain threshold of actors remain uncorrupted. But it does not require the state to be received by all nodes at the same time. And that is fundamentally why it's able to scale in a way that these other systems weren't able to scale. But when it comes back to what are the shared sequencers who are driving system progress forward? What do they need to do? Yes, they need to have access to shared state.
00:42:22.068 - 00:42:30.080, Speaker B: So that becomes a fundamental problem among the smaller set of actors, albeit, but a fundamental problem that all these different ecosystems need to address.
00:42:30.660 - 00:42:49.426, Speaker A: Yeah, the data synchronization problem is a little difficult. Was that kind of some of the thought process around Espresso Da? Or what was the thoughts leading behind the engineering decision to integrate that further.
00:42:49.498 - 00:43:29.238, Speaker B: Within espresso so espresso Da does follow this philosophy that you can have a content delivery network that is run by a smaller set of actors that is very performant and facilitates easy, quick access to data. So it does solve the data synchronization problem in a more centralized way, but with a decentralized guarantee of security. So the way that espresso DA works, we call it siromisu, because it has these three different layers. So you have, the CDN is sort of like the first layer. Information goes to the CDN. Everyone can download from the CDN. That's your web two architecture.
00:43:29.238 - 00:44:16.570, Speaker B: And then apart from that, we have a randomly selected set of nodes that receive the data as well. And you can go to that set of nodes to get the data to you. And if both of those layers fail, then everyone participating in the DA systems, this could be the 12,000 or, I don't know, 100,000 nodes that are participating in systems like Ethereum. They all get a tiny slice, a razor coated piece of the data. So none of those nodes actually have the full picture of the data. And if the first two layers of tiramisu fail, well, it's going to take a while to recover the data, but it will be recoverable. And I, as you know, at that engineer, I sort of, I really like this philosophy for building systems.
00:44:16.570 - 00:44:47.312, Speaker B: I'm not going to sort of enforce that philosophy on everyone else, but I think that that is a pretty reasonable philosophy. And that is implicitly what we have in a roll up centric roadmap like Ethereum. Right? We implicitly have sort of these different layers of, within our system where, you know, you have smaller set of actors that you're trusting optimistically for performance, and then the ultimate finality and security of something decentralized like Ethereum makes sense.
00:44:47.416 - 00:45:51.770, Speaker A: Optimize for high performance, but have fail back if needed, for worst case scenario, that the data can still be recovered and the system can move forward. Yeah, it is elegant from an engineering design. And maybe taking a step back with Espresso, where do you view kind of the longer term roadmap in terms of end users? Do you think these are individuals or engineers that want to go out, build their own L2, and then connect to other L2s? And as this kind of more modular thesis becomes more popular over time, more and more l two s will want to integrate into a system like espresso. And those type of engineers are the ones that you're really attracting and going after to help accelerate the espresso future as well.
00:45:53.870 - 00:47:07.798, Speaker B: Those are the types of engineers that we are targeting. What I like to point out is that ultimately the end end users, the users of applications and roll ups, all they really want to do is a secure, convenient, frictionless means of interacting, whether it's sending money to somebody or trading on decentralized markets, or buying nfts, playing games, etcetera. They just want a frictionless way of interacting that optimizes their utility. And this is going to be easiest across applications and L2s that are maximally interconnected. So there is going to be pressure from end end users to try to balance the sovereign gene independence that you want from being your own chain, but also being interconnected. And I think espresso is sort of an elegant solution to that. Often marketplaces are the best way of preserving sovereignty at the same time as enabling sort of economic efficiency.
00:47:07.798 - 00:49:20.496, Speaker B: And of course, we're also going to the biggest l two s today and saying, well look, this is a marketplace like you can come into the system with your own default sequencer, and if you don't, you can set your own reserve price, right? So you can say, okay, I'm only going to sell my sequencing rights if it's above a certain amount. So you do preserve a lot of your, the default is that you continue running as you are today. And if the system doesn't help you, then it doesn't help you. One of the things that I think the sort of bigger l two s today are more concerned about is, well, if we're integrating with this marketplace, then what dependencies does that create for us? Like if the marketplace goes down, like let's say we sold our sequencing rights, then does that end up causing delay for us? And what does the exit path look like? Do we have to do a hard fork and upgrade our roll up and just turn off espresso? Or is there a smoother transition? So that's something that we're really focusing on today is how do we make that? Let's say the espresso marketplace goes down for, as you know, a minute or even 30 seconds, then is there a mechanism for roll ups to sort of regain seamlessly control during that interim period and then come back into the marketplace? We also don't want to create a system where, okay, we run, and then if we go down like everyone has to hard fork and exit, they're not going to come back. So it's a strong incentive for us to address that sort of problem too. I think there's natural, naturally going to be a little bit more skepticism from the biggest l two s to adopt this, versus the sort of the more forward thinking, sort of competitive minded l two s that are looking at coming into the system, playing from behind. Like, what can they do to maximize for users? I think for those types of roll ups, espresso is like an obvious thing for the biggest l two s today.
00:49:20.496 - 00:49:41.880, Speaker B: It's not as obvious a thing because they are already ahead of the game and they already have a big, you know, user base. But on the other hand, we are, you know, partnering with off chain labs to bring espresso to the, to the arbitrum ecosystem, where we're partnering with Polygon to bring things to the polygon CDK ecosystem. So we are seeing this vision penetrate there as well.
00:49:42.620 - 00:50:19.738, Speaker A: Do you think? Kind of the users ultimately will demand these shared sequencing, as you kind of mentioned, just because the types of applications or apps that they'll want to interact with will most likely live on different shards or different l two s, and it'll kind of be a. It's up to the engineers ultimately to decide whether they want to integrate or not. But it may come as a user demand, because the types of things that users want to do will be across these different l two ecosystems where espresso really shines.
00:50:19.924 - 00:50:20.646, Speaker B: Yeah.
00:50:20.798 - 00:50:41.610, Speaker A: Yes, it would be very interesting. I mean, if so, do you feel like some of the l two s today just because they are already have some type of market share and dominance? They don't want to, are a little bit slow and are dragging their feet in terms of doing shared sequencing?
00:50:44.030 - 00:51:27.210, Speaker B: Well, everyone's talking about shared sequencing. Some of these ecosystems are looking at doing it within their own ecosystem. So optimism has talked about the super chain, Zika sync has talked about the hyperchain. Something that I like to point out to these different ecosystems is that espresso is complementary to that. So you can have the entire super chain participate in a marketplace where the super chain as a whole can also sell its sequencing rights to third parties. That enables interactions across different ecosystems. But my experience is that the larger ecosystems are thinking more within their own ecosystem than across ecosystems, naturally and smaller, newer roll ups are sort of facing the reality that they need to have interactions with other roll ups.
00:51:27.210 - 00:51:37.370, Speaker B: My hope is that people can come to see espresso as a solution for both of these types of actors and one that ultimately will also bridge the gap between them.
00:51:38.030 - 00:51:53.568, Speaker A: What do you feel like today is the largest misunderstanding about Espresso and what you either are doing from the product perspective or the technical perspective? What do you feel like you see on Twitter blog posts. What are people just generally wrong about?
00:51:53.744 - 00:53:01.150, Speaker B: Right? So the number one thing is that espresso is stealing sovereignty and revenue capture from roll ups, and that people view it. And I think this is our fault for calling it a shared sequencer initially, because they view it as something that replaces the, you take the sequencer, which is what's generating all the revenue for ROHps today, and you give it up to this, to espresso. Like, why the hell would you do that? And this is a misconception for a couple different reasons. So, first of all, the majority, vast majority of revenue that's being generated by sequencers today is actually coming from the gas fees inside of the roll up. And that doesn't get touched today with ascent flood sequencer architecture. You aren't really, you talk about sequencer revenue, you're not really differentiating between the execution revenue that comes from within the role of itself, the gas fees that are being paid from the profit that's being made. Just to have the sequencing right, just to have the ability to determine which transactions get in and in what order, or meV, so to say.
00:53:01.150 - 00:53:39.930, Speaker B: So we, as a shared, even if we were a single shared sequencer, we wouldn't touch the execution fees that are being generated by rollups today. It only pertains to meV. And we started calling it a marketplace. We've always intended to create a sort of a revenue distribution mechanism. And the way you end up doing that is essentially by running an auction or lottery for the shared sequencer. Right? Decentralized consensus protocols don't have one sequencer. They elect a rotating set of sequencers.
00:53:39.930 - 00:54:27.748, Speaker B: If you randomly rotate, then it's hard to know what like the MEV is, because you just randomly selected a party and you don't have that protocol, isn't really aware of the value of being that party. Whereas if you elect this party through an auction mechanism or some kind of lottery mechanism, then the protocol is aware of that value and can redistribute it. Now, with roll ups joining, the system can actually set their own reserve prices above which, and only above which, some shared sequencer will end up acquiring a sequencing rights. You can just view this equally as a marketplace. And calling it a marketplace is what we've started doing recently to sort of combat this number one misconception, that espresso is just like taking away sequencing revenue from rollups. It's not at all doing that. It's creating a surplus value that they can flow back to roll ups.
00:54:27.748 - 00:55:42.720, Speaker B: And it's preserving the amount that they can make on their own today. I'd say the other misconception is there's been this like extremely sort of like very technical, sort of like low level, like losing the forest for the trees debate about whether shared sequencers enable atomic inclusion or atomic execution. And you know, yes, there are differences between what a shared sequencer enables, between fundamentally distinct execution environments, and what a shared execution environment achieves. A. But saying that a shared sequencer only enables atomic inclusion, which gives you no guarantees over atomic execution, is sort of missing the point, which is that if you have one party that's able to build the next set of blocks for two different roll ups, that party can do a lot because that party has the ability for itself to enforce atomic execution. And in turn it can make that promise to end users. It can make it through an economic bond, collateral that it puts up, it can make it through just a trust relationship with end users.
00:55:42.720 - 00:56:26.988, Speaker B: It can actually act as a liquidity provider on its own. So you can think of this shared party as a bridge provider itself. You send it money on one roll up, it gives you money on the other. The bridge provider is the one that's taking the risk. And if it's now controlling both rollups simultaneously, it doesn't take the risk. There are other things that this shared party can facilitate in terms of coordinated block building in combination with things like polygons, ag layer, or other designs within ZK roll up architectures where transactions can again specify conditions that need to be satisfied that depend on the states of other roll ups. And yeah, they can specify them, but who's going to actually end up making sure that they're satisfied? Well, a shared coordinator, and that's a shared SQL.
00:56:26.988 - 00:56:38.520, Speaker B: So there's a whole focus on, oh, shared sequencers are just shared inclusion or shared execution. Like misses the whole point about what comes from having a single party be able to coordinate the block construction across two different roll ups.
00:56:39.420 - 00:57:10.100, Speaker A: I definitely appreciate you diving into that. And I feel like they'll probably do an entire episode on just misconceptions of when someone, the market rights are auctioned off for a kind of set sequence. Is that only, is that like per block? Like how are blocks being built? Or like, is there a block time? All the, do you have, like writes for four blocks, etcetera, etcetera.
00:57:10.640 - 00:57:43.670, Speaker B: Fantastic. Fantastic question. It's time slot based, and the reason for that is that nothing. All rollouts have a fixed block time, and they may not have the same block time. So ethereum is a system where there's a direct translation between blocks and time because each block is 12 seconds. But hotshot, for example, it's a protocol, is not optimistically responsive. Protocols that don't have a built in concept of how long it takes to produce a block do not have a block time.
00:57:43.670 - 00:58:14.710, Speaker B: They are responsive to how fast the network behaves. And you could have 100 blocks within a second or one block within a second, depending on what the network does. And certainly within ecosystem where you're trying to achieve some form of shared sequencing across many different roll ups that have, may have, some may have no fixed block time, some may have fixed block times. Even if they have fixed block times, they may be different durations. You do not want to auction off per block. You want to auction off per time slot, because that's the only thing that you can synchronize.
00:58:16.290 - 00:58:31.670, Speaker A: And that time slot is just like, hey, today, March 25, at 01:00 p.m. eastern standard time, I'm going to auction off shared sequencing for two distinct roll ups.
00:58:32.170 - 00:59:10.680, Speaker B: So within the Ethereum ecosystem, we're going to use the Ethereum clock. So Ethereum is a great clock. It has a, you know, every 12 seconds it produces a block, and it functions as sort of a weekly synchronized clock across all different roll ups. I think it's going to be a very interesting question, what we do when we look to expand beyond the Ethereum ecosystem. Today we are focused on the Ethereum ecosystem, but there's no fundamental reason why espresso has to be limited to the Ethereum ecosystem. And I think that's a wonderful and challenging engineering question, but also a very important question for us, as we look to expand beyond Ethereum, what's going to function as a shared clock?
00:59:11.560 - 00:59:23.980, Speaker A: Yeah, getting data synchronization is hard. Getting a clock to kind of unify those ecosystems at the same point in time is also a difficult problem.
00:59:28.280 - 00:59:28.752, Speaker B: Cool.
00:59:28.816 - 01:00:03.360, Speaker A: Well, Ben, really appreciate you for coming on the podcast. And as we kind of wrap up, maybe, is there any things in particular that I know we touched upon? A lot, a lot of it more technical, probably the most podcasts, but is there anything in particular that you want guests to come away with and say, hey, this is why you should be excited about espresso, this is why you should be excited about the modular kind of roadmap, and this is the opportunities that we are presenting kind of for the future?
01:00:05.220 - 01:01:22.446, Speaker B: Yes. I think the main thing is coming back to sort of the fundamental principles of what differentiates web3 from web two, and sort of really questioning and understanding, you know, with all these scaling solutions. Are we sort of missing the point or not? And so I, you know, I believe that the main difference between web3 and web two is creating a single environment or collection of individual single environments that can engender participation from so many different applications that were not developed in any coordinated way, did not do any conscious integrations, and yet they're able to interact so seamlessly. That is what makes web3 different from l two. And you could say that decentralization enabled that, because fundamentally, to get so many applications developed in this uncoordinated way to interact with each other, you need to have a common root of trust. And building a decentralized system is the best way to create the root of trust. But it's very easy to lose that in looking at scaling performance.
01:01:22.446 - 01:02:11.710, Speaker B: And I think that's to some degree what has happened within the ethereum ecosystem, especially because it's been this bottom up approach, and espresso is trying to create infrastructure that addresses that. It's creating infrastructure that allows these different scaling ecosystems to preserve their independence and sovereignty, but to regain what makes web3 web3. And we believe the best way to do that is through introducing the shared sequencing marketplace. There could be other approaches, but if you sort of agree with the principles of what I'm saying, then I think that you should check out what we're doing, because we've put a lot of thought into this, coming at this from a first principles approach, and maybe that's something that will resonate with you too.
01:02:12.290 - 01:02:56.188, Speaker A: 100% bottoms up approach ultimately allows the market to decide what will win long term. And that experimentation and that process is very important to enable that discoverability. So again, thank you so much for coming on, Ben, I really appreciate you going in more depth on the technical side and the product side about what you guys are truly enabling espresso and really pushing the entire space forward, because I think what I'm personally excited about is getting to the point where we can focus again on products. And kind of finishing up that infrastructure side is really that last step in my mind to pushing the industry forward. So thank you again.
01:02:56.364 - 01:02:58.580, Speaker B: Thanks, Logan. It was really fun coming on the podcast.
