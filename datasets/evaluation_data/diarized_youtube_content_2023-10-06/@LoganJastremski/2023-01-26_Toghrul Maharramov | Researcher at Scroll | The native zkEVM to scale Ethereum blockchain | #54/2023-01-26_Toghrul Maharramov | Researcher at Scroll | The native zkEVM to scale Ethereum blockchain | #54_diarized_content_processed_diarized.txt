00:00:00.240 - 00:00:27.610, Speaker A: Really appreciate you coming on. You and I have gotten into a couple Twitter debates, and I thought it would just be awesome to have you on the podcast, chat about all your thoughts, what you're building at scroll, and more broadly, the ethereum ecosystem. I think you're a very good proponent for ETh and everything that the community is doing and someone that actually understands the small nuances of the tech, which I think is rare. And so excited to have you here.
00:00:29.690 - 00:00:47.650, Speaker B: Thanks for having me. That's how I typically get on podcasts. I have a few fights with people on Twitter, and then it ends up with somebody saying, oh, yeah, why can't you just come on our podcast and we'll chat about it there? No, it's a. Yeah, thanks for having me again, excited to talk about stuff.
00:00:47.690 - 00:01:18.470, Speaker A: It's a perfect way to kind of spur the conversation. And I think in the past I've been a little bit apprehensive to go back and forth, but I think you do it very eloquently and sometimes with a little frosty, but it's appreciated. Before we jump into the nitty gritty of things and talk about the overview of Ethan, what they want to do, the Ethereum roadmap to scaling, I would love just to learn a little bit about your background and how you got into the blockchain and crypto industry.
00:01:20.340 - 00:02:17.068, Speaker B: Yeah, of course I got into crypto. I have a computer science background. I was always interested in how hardware works, how you make the code, run the cpu to understand what exactly to run, et cetera. I remember when I was 1314, I used to take my dad's PC apart and try to overcloak it and do random shit with the hardware that I didn't own, which my parents weren't happy about at the time. But yeah, so, yeah, I was doing a degree in computer science and my flatmate was into crypto. So it was, I think, late 2016. Yeah, around late 2016, when tokens just started on Ethereum and he was trading and, like, going into the first icos and stuff.
00:02:17.068 - 00:03:15.676, Speaker B: And I was familiar with bitcoin at that time, but I didn't really look into it. I just heard, oh, yeah, it reached $1,000 in 2014 and then crashed. And I was like, yeah, whatever, people will forget about it at some point. Never looked into it. So, yeah, I saw him doing that and I got interested, like, what's the whole hype about it? And I started looking into ETH and trying to understand how it works from trying to understand what's the fuss about smart contracts, et cetera, because I was already familiar from my degree with distributed systems and what they are. And so it was more about understanding the application of that. And once I looked at it, I found the whole idea of programmable, censorship resistant applications that run basically, magically run without any single party being capable of stopping or interfering with how they're executed.
00:03:15.676 - 00:04:08.898, Speaker B: Really exciting and also important, especially considering how the majority of the world doesn't live in a society where you can express your votes freely or you can interact with the Internet freely, et cetera. So I thought, just from the perspective of censorship resistance, blockchains are very important and especially smart contracts. And that's how I got into it. And then I worked on a few different projects. I mostly worked on consensus protocols and did a bit of work on privacy preserving stuff. And then I think in, but I was always following Ethereum and all the progress that was going on in Ethereum. And I think in 2019, I stumbled across Vitalik's post on roll ups.
00:04:08.898 - 00:04:35.600, Speaker B: And that's how I first heard about roll ups. And. Yeah, and I just started reading, reading about them from there and eventually got roll up pilled and God became a roll up maxi. No, actually I'm not a maxi, but yeah, I became sort of addicted with roll ups and started writing about them on Twitter. Yeah. And then I got a job at scroll and now I'm here.
00:04:35.680 - 00:05:33.890, Speaker A: Amazing. It's a very cool journey. I'm always kind of fascinated because I feel like once you start to learn about blockchain and crypto and have a decent level understanding, you really can't go back. And it's kind of like the only thing that your mind thinks about, and it's kind of all consuming. So definitely cool to hear your story. And so you said that roll ups were kind of like censorship resistance were like the key things that really got you interested on the Ethereum side. Could you talk about maybe, how do we start this and framing it on Ethereum? You touched upon it briefly, but maybe could you go slightly deeper into what specifically made you super excited about the Ethereum and what they were doing? Was it like the smart contracts? Was it just like the general ethos of decentralization?
00:05:38.430 - 00:06:45.388, Speaker B: I think, I don't remember. It was quite a long time ago, but I think originally it was probably the idea of smart contracts. And at the time, the smart contracts were very basic. There was the DAO, which unfortunately panned out the way it did, and tokens and some basic contracts that weren't really anywhere near as complex as the stuff that we use on the daily basis nowadays. So I think that's what initially got me hooked. And then the whole ethos, the community values the idea of just the open mindedness because at the time, obviously, it's not like that anymore because there are a few communities that are open minded, but at the time it was mainly bitcoin, ethereum, and then a few forks of different protocols, mainly bitcoin. And I never really bought into bitcoin because if you, in my opinion, if you don't have general purpose execution capabilities, your protocol is basically useless.
00:06:45.388 - 00:07:15.840, Speaker B: I don't buy into the whole store of value, meme, etcetera. For me, it's just useless. Actually, I've never used bitcoin in my life. I think I got, like, sent bitcoin once and that's it. So I'm probably one of the few people in crypto who basically never interacted with bitcoin. Yeah, and all the other protocols were basically either cheap forks of Ethereum or some forks of bitcoin. And there was nothing that interesting in the market at that time.
00:07:15.840 - 00:07:23.942, Speaker B: So, yeah, Ethereum was a natural place to go if you were open minded, liked smart contracts, and liked the idea of decentralization.
00:07:24.126 - 00:08:09.354, Speaker A: Definitely. I mean, I got into blockchain and crypto in 2017 and trying to figure out, like, everything that was going on at that time. It was all icos and definitely fell down the Ethereum rabbit hole and stayed with it. I think in the 2018 and 2019 standpoint, it was super hard because there actually wasn't that many actual applications that you could really use. It got really exciting, though, starting in 2020 again. Once DeFi Summer took off with composability, I was really like, it got me really invigorated about Ethereum again. But where I personally kind of started to become less favorable in Ethereum was when we started seeing some of the scalability issues with, like, gas costs.
00:08:09.354 - 00:08:26.880, Speaker A: So could you talk about, I guess, like high level, like, how has your journey been with Ethereum? Kind of been with it early, kind of seeing it get that traction and then going through, like, the ups and downs and kind of how you think about scalability today, and then we'll go into, like, some of the more technical details.
00:08:28.340 - 00:09:28.964, Speaker B: Yeah, of course. I think in terms of gas, if we go back to the fees, obviously, as everybody else, it went from like, oh, yeah, let's just send 100 transactions and it doesn't matter. It cost like three cents per transaction to at some point, I remember I got an airdrop. I don't remember what token exactly was airdropped. And I spent like $400 selling it on Uniswap, something like that, something ridiculous. So, yeah, in terms of that, I think at some point, as everybody, as most of the people, it got to a point where I was like, is it worth it? Or are all the values worth the fees being paid to? But then I took a step back and I thought about it and it was like, it's probably a temporary issue. So at the time, the roadmap wasn't really clear.
00:09:28.964 - 00:10:47.672, Speaker B: I think in 2020 we still, yeah, roll up centric roadmap came out later in 2020. So we still had the three phase sharding model with beacon chain, phase zero, then phase one being data availability and phase two being execution sharding. And I'll be honest, I was like, at some point I just, because I spent a lot of time studying sharding and at some point I just came to a realization that sharding isn't really a good option in a sense that if you want to have shared security, sharding and cosmos ecosystem works perfectly fine because every single chain is sovereignty. So they are essentially independent from one another. But in the context of Ethereum, I just thought that, I was never convinced by it. I just thought that the complexities are not worth it and the security trade offs that happened there. And so, yeah, so roll ups for me were like an eye opener that you could essentially have sharding, but without all the complexities that come with sharding to the base layer, because you essentially outsource all the complexity to roll ups.
00:10:47.672 - 00:11:15.230, Speaker B: And then the only complexity that is added to the base layer is the more complex smart contracts that are verifying those batches from the roll ups. Just one thing to add, when I'm talking about sharding, I'm talking about execution sharding data. Sharding is less complex and also leads to less problems as a result. So, yeah, perfect.
00:11:15.530 - 00:12:02.770, Speaker A: Appreciate the background context and definitely want to get into all the nitty gritty on L2s and the different roll up solutions. But before that, I think when you and I, or when I follow along with your Twitter threads, the biggest thing that I kind of see as a recurring thing that ultimately ends up being like the crux of debates is kind of nodes and how big either full node should be or light clients, all that like physical hardware and how that kind of ultimately manifests itself into the network. So I'd love to start the conversation there. What are your thoughts around, like, full nodes, thoughts on, like, hardware specifications, and then we'll jump into light clients.
00:12:05.050 - 00:12:34.620, Speaker B: Yeah, of course. Let's go there. It's the most fun subject to debate about, because essentially most of the other things. I have a feeling that a lot of other things Solana and Ethereum communities are in agreement with. It's mostly the full nodes and nodes where our visions a bit diverge. Yeah. From my perspective, I think that we discussed it with Anatolia last week.
00:12:34.620 - 00:13:26.300, Speaker B: I think the goal of the protocol should be to maximize the number of full nodes, because the more full nodes you have, the bigger the probability that at least one of them is honest and is going to catch some invalid state transition or something invalid that the majority is doing. It doesn't have to be like millions. I don't think it's realistic. I think once you get to hundreds of thousands of full nodes, you get into problems with data propagation, all the hops. The cost of propagating data is just not worth it anymore. But assuming that we're within a certain acceptable range, I think you should maximize that. Within that range, you should maximize the number of full nodes.
00:13:26.300 - 00:14:13.298, Speaker B: And in terms of the hardware requirements, again, my opinion is that the lower the hardware requirements, the better. Obviously, Solana also has that ethos, where within a certain limit, you would like to minimize the hardware requirements. But for Solana, the user demand is more important. So the throughput demand is more important than the hardware environment. So if, let's say, the demand, all of a sudden spikes, instead of limiting the supply, Solana would just increase the hardware requirements to cope with that demand. Whereas with ethereum, the ethos is a bit different. So regardless of how high the demand is, the supply will always be relatively limited.
00:14:13.298 - 00:14:53.760, Speaker B: It can obviously change. It's changed a few times throughout the years, but still quite limited relative to the demand. And that allows Ethereum to be quite accessible in terms of running full node. So essentially, I used to run a full node on a raspberry PI four, which is what with the SSD is like $150. So the cost of running that is relatively negligible. And I think that an average household would be able to do that on their PC or raspberry PI. They don't have to, but the point is that you should always have an option if you want to.
00:14:54.340 - 00:15:29.340, Speaker A: That makes sense in terms. I guess one of the big things that I also, that I think is a little misconstrued on the Ethereum side is the number of full nodes. I think, a lot. I don't know why it ultimately got to be this way, but essentially the number of staking nodes was kind of determined or used as the full validator count, but those aren't actually representative full nodes. Do you know how many full nodes there are on the Ethereum side today? Because I look at different websites and I see conflicting numbers, I find anywhere between like 6010 thousand.
00:15:31.000 - 00:16:11.512, Speaker B: Yeah, I would say, I mean if you, obviously if you look at the, as you said, if you look at different websites, the numbers would be different. But my rough guesstimate would be between eight and 12,000, I would say, because not all nodes are reach, are reachable at all time. And so also I find with a lot of websites there are some weird bugs. For example, I think RPC nodes constantly shows that 33% of the nodes are behind and syncing, which is unrealistic. I would say my guesstimate would be between 8012 thousand nodes.
00:16:11.576 - 00:17:11.710, Speaker A: That makes sense. Then on the second point, definitely an agreeance on the hardware standpoint, ultimately you do want to keep it as low as possible. I think one of the hard things that I now see in the industry today is like what is, in terms like what is acceptable and where do we kind of like draw the line. So I'm curious, like in your point of view, I mean, obviously, I mean, everybody wants the hardware to be cheap as possible and kind of have the network be as inclusive as possible. But I think what's been interesting to me is as Ethereum has grown, they have upgraded some of the properties or increased gas limits, ultimately increasing the amount of compute needed to run a node. So in your mind, I'm curious how you think about where the line should be drawn in terms of how much a hardware cost should be and then at what point in time is it acceptable to upgrade hardware?
00:17:16.520 - 00:18:24.092, Speaker B: It's difficult to quantify, but I would say a good benchmark for me, with the way IoT hardware and stuff, the efficiency of the hardware increased in the last decade. I would say a good benchmark for me would be a raspberry PI four. I wouldn't go any lower than that because it's unviable. Like if Raspberry PI Four can run at any PC or laptop, any relatively modern SBC can run it. And I wouldn't really downgrade to mobile phone level of hardware because at that point I can't see any single user running a full node and spending like data on propagating and repropagating data, and that wouldn't be viable. I think a good minimum bound is like the minimum stationary hardware that you can run on. So with laptops.
00:18:24.092 - 00:18:46.454, Speaker B: It's unrealistic that you would go on a plane, connect to the Wifi on a plane, and sync your node while being on a plane. It's more like, yeah, you have a PC at home, or you have a raspberry PI at home. You install the client there and just run it there. So, yeah, that would be my rough benchmark for how low the hardware should be.
00:18:46.542 - 00:19:24.190, Speaker A: From a community aspect. From a community aspect, I guess, like, how do we kind of find common ground, I would say, like, across ecosystems to, I would say define like, what is acceptable. Because this is like, what I would love to do for the industry is really try to find like common grounds and objective ways that we can measure these. And so I'm curious, like, as an industry, and like, can we find like a measurement or an objective? And I think full nodes is a great kind of proxy to that, but they can, the cost difference can be different. But I'm curious if you have any thoughts.
00:19:27.450 - 00:20:16.200, Speaker B: I think it would be difficult to come to like an objective measurement. Just be, firstly, it's, there are just too many variables and different protocols have different philosophies and different ethos. So different communities. And so you would have, let's say, Solana community saying, oh yeah, the most important is to be capable of handling user demand and say that, oh yeah, Ethereum fails because the fees are too high. And the Ethereum community would be like, oh yeah, if I can't run a full node at home, then there's no point in using that protocol. I don't think some tried. So I've seen, there was a paper published by Cordano Iohk researchers a few months ago about trying to define what decentralization is in a more formal way.
00:20:16.200 - 00:20:41.710, Speaker B: But yeah, but it's always, it doesn't matter whom you read and whom you talk to, it's always going to be even within Ethereum. I think if you put like 20 people in a room from major people, major influencers, or major researchers from the Ethereum community, none of them will agree on what the definition of decentralization is.
00:20:41.830 - 00:21:43.800, Speaker A: Yeah, it is a hard topic. I mean, maybe let's jump onto the decentralization topic and then jump back and then we can talk with lite clients. This is another thing that I would love to get your take on it. My point of view on decentralization is similar to yours and the fact that the Mac, we should really try to get a large number of full nodes, because that actually does have a physical representation of recoverability of the network. Then the second thing being is what is the Nakamoto coefficient? Because in proof of stake networks, it also can have a effect on the network in terms of stake distribution or censoring transactions. So the number of independent parties that would need to collude to obtain majority of stake wake to either sensor transactions or call the network. But I'm curious your thoughts and how you kind of view it as well.
00:21:45.740 - 00:22:56.480, Speaker B: I think Nakamoto coefficient is one of the proxies of how you can measure decentralization, but I don't think it's like, because sometimes people present it as like the most important one. And taking a step back, even if we assume that, like, it's the most important metric of how you would measure decentralization, how do we define Nakamoto coefficient? Does it measure liveliness failures or correctness failures? Or. Because, let's say for ethereum, do you classify liveness failure as a protocol failure? Because, sorry, finalization. So essentially in Gaspar, you can have more than 33% of the nodes that are dishonest, but the protocol will continue to produce blocks. So the liveness will be preserved, but the blocks won't be finalized. So there's a probability that like a certain chain will be reorgan at some point. So do we consider that to be the correct way of the, of.
00:22:56.480 - 00:23:28.498, Speaker B: So does Ethereum still, is still, is it, is Ethereum still functioning when that happened, or is it, has it failed when that happens? So it's, it's a bit of a. There's no concrete definition of what no commodore coefficient is, and therefore it's difficult to put a number on it. And on top of that, there's also questions. For example, with Lido, just do you count Lido as one entity or do you count Lido as multiple entities?
00:23:28.554 - 00:23:29.322, Speaker A: This is a good question.
00:23:29.386 - 00:23:30.266, Speaker B: Don't control them.
00:23:30.338 - 00:23:31.350, Speaker A: What do you think?
00:23:31.690 - 00:23:55.476, Speaker B: Yeah, I personally wouldn't count it as one entity. So, for example, the same with. I'm not sure if that's still the case, but I remember some time before Solana foundation delegated quite a bit of Solana tokens to users for users to run validated nodes. Right. So do you count those as being Solana foundation or do you count those as being the users?
00:23:55.508 - 00:23:58.360, Speaker A: I think it's just because it's the Solana foundation stake.
00:23:59.900 - 00:24:31.430, Speaker B: Yeah. So you're essentially relying on their promise to actually delegate that stake, because it's difficult to verify unless you get speak to every single validator that actually got the delegated tokens. And I think it's the same with Lido. It's just that you have to, to a certain degree, trust that they're actually doing it. But I think it would be fair to assume that it's actually distributed.
00:24:32.420 - 00:25:35.660, Speaker A: Yeah, I mean, I've seen a couple metrics of trying to break out. Even if you didn't count Lido as a single entity, what that would look like. And I think I saw, and I could be totally wrong on this, it was something around like ten or twelve independent parties that make up like majority of like the lido pools. So when you did that and then also added Coinbase, which was like the other entity, to like the majority or to 33% stake weight, it is definitely trending in the right direction, but it isn't as high as other Nakamoto coefficients in the industry. And so I do think the reason why I like the Nakamoto coefficient just because it is something that is objectorable that you can do unbiased comparison across blockchains. But I definitely agree on the edges. There's things that you mentioned that could definitely affect and ultimately are a little bit more nuanced.
00:25:37.960 - 00:26:05.580, Speaker B: Just one thing to add. I think it's fine to use Nakama coefficient as a proxy, but we should agree on what Nakamoto coefficient is because again, so for Ethereum, I would say 51% is the Nakamoto coefficient, whereas for something like Solana it will be 67%. So I, how do we compare the two?
00:26:06.080 - 00:26:11.460, Speaker A: And is that still true in the proof of stake transition?
00:26:12.920 - 00:26:54.028, Speaker B: Yeah, because so the only, so if the colluding parties control, let's say, 40% of the stake on Ethereum, the only thing that will happen to is the blocks will take time to finalize. So they won't be finalized after twelve minutes. They'll be finalized after the stake of those colluding nodes decay slightly through leakage to go back to below 33%, and therefore the protocol still continues to produce blocks. So whether we consider that to be a fully functioning network, or whether we consider that to be some sort of a failure makes sense.
00:26:54.124 - 00:27:25.250, Speaker A: Interesting. Yeah, definitely lots of small nuances. And I think those small nuances are actually a lot of like, the crux of debates that kind of end up happening on Twitter. But in terms of maybe staying on the decentralization a little bit front, kind of wrapping that up and then going back to like, clients, and then I do want to get into like the Ethereum scaling roadmap on economic security. How do you view that in terms of like security for the network as well?
00:27:31.590 - 00:28:35.020, Speaker B: I think people present it as a bigger thing than it actually is. So let's put it this way. If the majority of the network is corrupt, and let's say they're the portion of the network that they control, let's say the stake or the hashrate is valued, I don't know, for example like 10 billion, they still won't be able to produce an invalid block because any honest full node will reject it. So the economic security is only helpful to protect against censorship attacks and also to protect against, for example, so lite clients rely on economic security to function. So the higher the economic security the better. But they don't protect against correctness failures, because if that happens, any honest node will just reject the block.
00:28:35.680 - 00:29:08.950, Speaker A: Makes sense. I think I agree with that as well. So let's jump to like clients because you mentioned them. What is your point of view on clients compared to full nodes and say that you had, I'm curious, if you had larger full nodes, would it be acceptable in your point of view to just run like a large majority of light clients that are able to double check full nodes?
00:29:12.140 - 00:30:35.770, Speaker B: I think live clients are fantastic and they're the most viable solution for the vast majority of the users going forward. I think in a few years hopefully we'll have the wallets on your mobile phone, et cetera, running some form of like client that can do at least some verification versus just trusting some centralized RPC. And yeah, so they drive a lot of innovation in terms of being able to interact with blockchains without trusting some centralized entities or some middleman that basically acts as a layer connecting you to the chain. But at the same time they are not a substitute of full nodes because like clients rely on full nodes for their security. So essentially you can only maximize the network resilience by having as many full nodes as possible. Obviously I'm not talking about stuff like Celestia because for DA it's a bit different, but for execution chains, they still depend on the resilience of the network and so the number of full nodes.
00:30:37.970 - 00:31:24.946, Speaker A: Yeah, I'm definitely excited to watch the continued like client revolution and how they'll continue to be utilized in the future. I'm definitely in agreement. I think they're super helpful. So maybe kind of getting into the meat bones of Ethereum now and learning a little bit more about the roadmap. I'd love to start maybe with like some of the recent or upcoming upgrades, maybe starting with 4844 proto Dank sharding, and then kind of get into some of the roadmap with dank sharding and your thoughts around each, the whole idea of data availability.
00:31:25.058 - 00:32:39.640, Speaker B: Sharding is important for Ethereum because of rollups. So the core notion of rollups and how they differentiate from, let's say, plasma or validiums is that they posted enough data on chain to allow the user, in case of some failure, to be able to recover the state and be able to transact or interact with the roll up without trusting anybody. And that's the main crux of why rollups are secure the way they are, and cold data and Ethereum. So by cold data, I mean publishing data inside a transaction is quite expensive at the moment, because all the call data can be interpreted by the EVM, so it has to be inputted into the EVM, and it's quite expensive. And also, the other reason why is because Ethereum tries to limit the block sizes, because in the worst case scenario, the blocks, if, if, let's say, the entire block, is just filled with bytes, that the block sizes can balloon and therefore can have negative impact on the performance of the network. And that's how dank sharding was born. Dank Sharding is a solution created.
00:32:39.640 - 00:33:32.076, Speaker B: It's a sharding model created by a researcher, Ethereum foundation, called Dankground Feist. That's how you got the name, actually, a lot of people think that he named it after himself. It wasn't, it was Vitalik who named it after him. So Dankra didn't actually do that. So the core idea of dank sharding is that there's a tight coupling between blob transactions or data blocks, whatever you want to call it, and the execution block. So the way the current blocks are, in a sense that because we or the Ethereum accept that, there's some level of centralization going on because of mev. So even right now, Ethereum has an informal proposal builder separation.
00:33:32.076 - 00:35:02.960, Speaker B: When there are a few large entities that are building the blocks, and then the validators in the network just accept the highest bidding block and then propagate it as their own. And because it's already quite expensive to run a builder node, the whole premise of product dank sharding is that it exploits the fact that those nodes are quite beefy, because dank sharding is based on a polynomial commitment scheme called KCG, which is quite expensive to compute, and therefore it would be unviable to run on, let's say, your raspberry PI. Technically, I could run a validator node on a Raspberry PI, and there's no way a raspberry PI could compute. I don't remember the exact number of KCG commitments per block, but like, quite a significant number of commitments per block in like acceptable time. And therefore, essentially the computation of KCG commitments is outsourced to the builder nodes. They do all the work, and then they separate the block into like slices called blob transactions. Or if you want to think about it and classic sharding, every blob transaction is essentially block a separate data block, and then propagate them to different nodes.
00:35:02.960 - 00:35:14.980, Speaker B: Let me, I think I probably need to return and explain why KCG commitments are important, because people are going to think like, what the hell is a KZG commitment?
00:35:15.020 - 00:35:15.880, Speaker A: Go for it.
00:35:17.270 - 00:35:38.214, Speaker B: So essentially, the data inside the blob transaction, or the data availability block, whatever you want to call it, is inaccessible to the. Oh, just a second. I actually forgot to plug the laptop and. Sorry about that.
00:35:38.302 - 00:35:38.934, Speaker A: No worries.
00:35:39.062 - 00:35:42.502, Speaker B: Sorry that I. You will have to do a few edits here.
00:35:42.566 - 00:35:43.046, Speaker A: Oh, good.
00:35:43.118 - 00:35:57.240, Speaker B: Yeah, I think it's charging now. Oh, luckily it was at 1% shipped. Glad I saw it in time. Yeah. Let me remember what you commit. KCG commitment.
00:35:57.320 - 00:35:57.664, Speaker A: Yep.
00:35:57.712 - 00:36:58.610, Speaker B: Oh yeah. So the data inside the blob transactions is not directly accessible to the EVM environment. So the execution environment, because that would require for every node to have that data, which defeats the point of sharding. So the data is disseminated and split between different nodes in the network. And what KCG commitment allows you to do is that data in a block transaction to be committed to this one small commitment, which is a few hundred bytes in size. And then anybody who's trying to like, let's say it's a roll up that published that data via blob transaction, any interaction within the execution environment. So within VM can prove the opening of that KZG commitment.
00:36:58.610 - 00:37:49.584, Speaker B: So let's say I need certain part of the commitment, not the entire thing, but like a few hundred bytes within that commitment, I can prove that I know how to, what the original data was that was put in that commitment for that hundred bytes, without revealing all the data that was in that commitment. And that allows you to basically support that data within the execution environment without having every node store all the data corresponding to that, to that, to the, to the stuff being executed inside the EVM. And have I explained it in a relatively accessible, I believe so way, or.
00:37:49.632 - 00:37:54.620, Speaker A: Is it so, I mean, okay, because each node is doing a fraction of the work.
00:37:56.810 - 00:38:09.330, Speaker B: A fraction of the storage. So the work is done is essentially the same for every validator node. The only thing is, they only store a portion of the data per blocker, the entire block.
00:38:09.450 - 00:38:10.310, Speaker A: Gotcha.
00:38:10.770 - 00:38:45.584, Speaker B: And proto dank sharding came about because dank sharding is quite, is probably the most complex change to ethereum that ever happened after proof of stake transition. And so it was obvious that it'll take quite a bit of time to materialize. And proto dank sharding is essentially the same thing as dank sharding, but without the sharding part. So the commitments are, paradoxically. I know. So the commitments are computed still. So everything is done in the same way.
00:38:45.584 - 00:39:08.590, Speaker B: But the only thing is, the data is not split between nodes. All the nodes receive all the data. But the downside of that is that you can only propagate so much data. So per block, you're limited to two megabytes versus dank sharding, which I think is limited to 32 megabytes of data.
00:39:09.810 - 00:39:55.630, Speaker A: Yeah. So I think, I mean, I think that's my big takeaway here. And I'm curious on your opinion. By increasing the amount of data that ethereum can ultimately propagate and that can run on kind of generalized hardware, as we talked about earlier, that is relatively low in hardware specs, the end goal is really increasing how much data you can propagate at the base layer while keeping the hardware specs relatively low. And the ethereum ethos standpoint. And with sharding or proto dank sharding, with 4844, you're doing. I think it was, on average, one, or the target was 1 each block with the upper bound being two.
00:39:55.630 - 00:39:58.034, Speaker A: Or maybe it's two on average.
00:39:58.082 - 00:40:16.606, Speaker B: Yes. So each blob is 128, target is eight blobs per block, and then the maximum allowed is 16. So, yeah, 1 maximum amount is two megabytes. And prototype sharding, in Dax sharding, it's 128 into 56, respectively.
00:40:16.718 - 00:40:39.766, Speaker A: Gotcha. A little over 1 mb on average for proto dank sharding. And then dank sharding will be 32 megabytes for each block. And those block times are every 12 seconds, which average out, I think, to like 1.3 megabytes per second with think charting. So my.
00:40:39.918 - 00:40:47.124, Speaker B: I guess, like, I think the target is 1.3. The maximum was like 2.6, something like that.
00:40:47.212 - 00:40:51.800, Speaker A: Okay. Yeah, I know Ethereum gives a little bit of buffer room in the blocks.
00:40:52.100 - 00:40:52.880, Speaker B: Yeah.
00:40:53.220 - 00:41:20.550, Speaker A: So my question being, like, I'm always interested in trying to figure out how much data we'll actually need. Do you think at the end of the day and today. Maybe we should go back a little bit and like preface this with like, how much data is being propagated today? I think I was looking prior, Ethereum's like average blocks are like 100 kb today.
00:41:22.130 - 00:41:25.530, Speaker B: Yeah, should be roughly like that hundred maximum 200.
00:41:25.570 - 00:42:09.510, Speaker A: Yeah, this, I mean, 4844 is doing at least ten x more data throughput and then the full dank sharding is much, much more. So do you think though, like this data increase in capacity, even with full dank charting, like 1.3 megabytes per second will be enough with even like the L2s and the kind of data, compute data and compute compression? Because this is like my end question that I always come back to. And if it's not enough, like, does Ethereum either increase like the hardware requirements that we talked about earlier, or do you like continue to add like shards?
00:42:14.890 - 00:42:35.070, Speaker B: Let me go step by step here. So I think if you apply compression and with CK roll ups, if you post the state divs, it's likely that you would probably be able to fit maybe like 40, 50,000 transactions per second.
00:42:35.930 - 00:42:45.550, Speaker A: I think of the talent, he did the math and averaged with full sharding and full ZK rollups, it was around like 80 or 100,000 tps.
00:42:47.290 - 00:43:43.150, Speaker B: Yeah, I think that's a very optimistic number. Realistically, it's going to be a bit less because not every transaction is identical. And also it's hard to predict. That's one of the problems that we're facing when trying to model how much data is going to be consumed is that the user behavior is likely going to change once people transition to roll ups, because it's cheaper to transact with roll ups, and therefore you don't need to think about what you do that much, because the difference between paying, I don't know, $510 per transaction, a few cents per transaction is drastic. And like, for the majority of the current users, paying a few cents per transaction is negligible. It doesn't matter, obviously. Ideally, once everything matures, it's likely going to drop below $0.01,
00:43:43.150 - 00:44:27.700, Speaker B: drastically below $0.01. But I think in the beginning, the current pricing, I think is around three, four transaction, and therefore the behavior is going to drastically change. So it's hard to predict how much data is going to be consumed per transaction. But based on the current estimates on the current usage, let's say if we just replicate the current network usage to roll ups, I would say 50,000 would be a good approximation. Upper bound? Yeah, I think Vitalik is probably right. It's around 100k, but I think realistically the upper bound will be in and around 50k.
00:44:27.890 - 00:44:48.260, Speaker A: That makes sense. When do you think? I mean, I think you're following kind of the Ethereum roadmap much closer than I. I know that they plan on getting out 4844 sometime this year, but I'm curious if you have any understanding of when the Ethereum community plans to ship. Like full sharding.
00:44:51.480 - 00:45:32.416, Speaker B: Yeah, 4844 or for the dank sharding. Depends on how you prefer calling it should be shipped something this year. I think it's going to be so work withdrawals are probably going to ship in March April. And so Ethereum typically have has two hard forks per year. So I would say Q three, Q four this year would be a good approximation. Unless something goes horribly wrong. That would probably be the time where prototype sharding is shipped and full tank sharding, I would say, at least in the most optimistic case, a year from that gotcha.
00:45:32.416 - 00:45:34.580, Speaker B: So I would say late 2024.
00:45:35.720 - 00:46:05.530, Speaker A: That makes sense. Interesting. So I mean, in some of our Twitter debates, I always come back to just saying like, increase the base layer throughput. And you're like, no, it's not that simple. So I'd love to learn some of your thoughts around, I mean, if say that 500,000 is not sufficient, what are the better routes to actually increasing data throughput or more transactions per second to meet user demand?
00:46:09.550 - 00:47:37.056, Speaker B: So I think by the time the demand is enough for us to think about whether 50,000 DP's is enough or no, I think we're a few years away from that being even possible. I would say that just one thing to preface before I go on to say what I was about to say is that while I think that hardware should be limited, I don't think it should be fixed. So I think we should update the hardware. As the availability of hardware efficiency of the hardware available on the market improves. So let's say once every two years it's fine to maybe increase the hardware requirements by 20 30%, because overall the accessibility of the hardware scales and therefore of more efficient hardware, and therefore I don't think it's an issue. So from time to time, I don't think it would be a problem to increase the throughput capacity of Ethereum or let's say increase the gas throughput of Ethereum as the hardware increases. But also in terms of state growth, which is the main bottleneck of Ethereum at the moment.
00:47:37.056 - 00:48:20.420, Speaker B: There are a few things that can be done in order to combat that problem. There is still quite a bit of headroom where you can optimize. So I think you can probably squeeze out quite a bit more performance that is currently available. It's just that because performance wasn't really a priority until now or until rollups proliferate, there was no real effort to optimize things. It was more about let's get everything secure, let's get everything correct. And then once everything is running correctly and we're sure that everything is correct, we can optimize from there.
00:48:21.360 - 00:48:37.660, Speaker A: That makes sense. So as hardware becomes increasingly better or becomes more efficient over time, just kind of swap out the whole hardware for new hardware to take advantage of those same efficiencies while price stays relatively stagnant?
00:48:39.180 - 00:49:13.674, Speaker B: Yeah, I would say so let's say in a few years, the hardware that is currently considered high end is probably going to be considered low end, and so more people are going to be running that hardware versus the same hardware being run now. And therefore it makes sense for you to increase the hardware requirements because the majority of people have access to what was considered to be high end hardware just a few years ago.
00:49:13.842 - 00:49:34.750, Speaker A: That makes sense. And I guess maybe this may lead us to our next topics with L2s. But is there outside of, say, just increasing, like the hardware requirements, are there any other kind of point of views that you think are acceptable to increasing the base layer throughput so that you can do more transactions?
00:49:36.180 - 00:49:43.156, Speaker B: You mean by doing something on the l two s or by optimizing by.
00:49:43.228 - 00:49:57.920, Speaker A: Like, I mean like overall, like outside of like hardware, or just increasing like the raw hardware in your mind, is there any other ways to increase, like base layer throughput that would allow the network to kind of still function?
00:49:59.700 - 00:51:04.940, Speaker B: Yeah, I think you could apply some form of compression compression on data, and then once data availability sampling comes around, you can probably shard the data to even more. So let's say instead of splitting the block between having each node custody like one 10th of the block, you can have each node custody one 20th of the block. And that's why also, that's another reason why for Ethereum, especially, the number of full nodes matter, because the more nodes you have in the network, the more you can essentially split the custody of the data, obviously to a certain degree, because the more you split it, the lower the number of the nodes that have a certain part of the block, and therefore the probability of retrieving it decreases. But yeah, those, I think, are the two main approaches. There are probably a few more that I haven't thought of at the moment, but yeah.
00:51:05.520 - 00:51:31.210, Speaker A: Okay, interesting. Let's transition to L2s. I think this is a topic that you have spent a lot of time thinking about, and so would love to learn your thoughts and how you're thinking about the market. So maybe talk about optimistic roll ups, your thoughts there, and then kind of compare and contrast those to what's going on, even at scroll and the zero knowledge space.
00:51:33.430 - 00:52:36.348, Speaker B: I think I'm a bit biased here towards the ZK part, but. Yeah, but I'll happily talk about both. So just a short explanation of what an optimistic roll up is. So an optimistic roll up, it does the execution of chain commits the data to on chain and some state and optimistically assumes that it's correct, unless a certain node challenges that state commitment within a certain period of time, let's say a few days. And in terms of challenging, anybody who's running an l two full node is capable of challenging as long as they submit a bond. Because otherwise, if you don't submit a bond, there's a problem of. Because essentially I can grief the challenge, the person who submitted the original commitment by saying, oh yeah, he's actually wrong, but in reality that the person who submitted the commitment is right.
00:52:36.348 - 00:52:59.390, Speaker B: So you need some form of bond or a security deposit to protect against those kinds of attacks. Yeah. And essentially what it boils down to, the security of an optimistic roll up, is that we need at least one full node capable of detecting an invalid commitment and challenging it.
00:53:00.770 - 00:53:07.310, Speaker A: Comparison to zero knowledge from optimistic roll ups and kind of like the general trade offs there.
00:53:08.650 - 00:54:19.702, Speaker B: Yeah. So the main assumption, the only assumption that an optimistic roll up makes is that there is an honest full node roll up, full node that can detect an invalid state commitment and challenge it in time. But the benefit of optimistic rollups at the moment, obviously, once the ZK stuff becomes more optimized and more mature, it's not going to be a benefit anymore, is that an optimistic roll up can essentially run anything at the moment in an efficient manner. So for example, optimism, the basis of optimism, the challenge mechanism, the execution environment that it uses, is mips. And you can compile any execution environment into MIPs and just have a valid fraud proof. So currently they compile geth, which is one of the Ethereum clients, but you can essentially compile anything. Solana, CoSM, WaSM, whatever you feel like you want to compile, you can compile into it.
00:54:19.702 - 00:54:56.952, Speaker B: And the same is true with arbitram. Arbitrum doesn't use mips, they use WASm instead. But the concept is the same. You can compile anything into WaSm, any execution environment, and have essentially a fully functional roll up. You still need to add a few things, but the core will be functional. Whereas with a lot of ZK robots, including scroll at the moment, you have to custom build the circuits for every single execution environment because it's theoretically possible to do the same thing. Just create a circuit for mips and then compile anything into mips.
00:54:56.952 - 00:55:54.824, Speaker B: But the overhead will be so massive that it wouldn't be viable to compute proofs. It'll take like, probably hours or even days to compute approved for such a, such a setup. So instead, what all of us do are essentially, essentially custom build a circuit targeting a specific execution environment. So in case of scroll, it's EVM. So every single opcode is represented by a custom opcode, by a custom circuit, sub circuitous. Every single, let's say, read write has its own circuit or a sub circuit, et cetera, et cetera. Whereas if we were to take an optimistic approach, you would just compile it, compile geth into mips, and then just prove it and approve the execution of mips in the circuit.
00:55:54.824 - 00:56:26.234, Speaker B: So, yeah, that's the downside. Let me explain the ZK bar, because I went onto the downside without actually explaining what ZK rollups are. ZK rollups, instead of using the optimistic, it's called. I don't think I mentioned it's called fraud proofs. Or there's another term that people like to use nowadays because. No, no, no. So optimism uses a different term.
00:56:26.234 - 00:57:31.110, Speaker B: They don't use fraud proofs anymore because they think it has, like, bad connotation. Yeah, but I forgot the term anyways. Yeah, so Zk rollups use zero knowledge proofs in order to prove the correctness instead of using fraud proofs. And zero knowledge proofs are a family of cryptographic protocols that were originally constructed as systems, as a privacy preserving cryptographic systems. But modern zero knowledge proofs have this nice property which is called succinctness, which means that you can prove to verify the correctness of execution in sub linear time. Meaning that, I don't know if, let's say it takes you five minutes to execute it. The cost of the time to verify that execution versus doing execution is much smaller, let's say, for modern zero knowledge process, a few milliseconds at most.
00:57:31.110 - 00:58:11.646, Speaker B: And this allows you to aggregate a lot of transactions off chain, prove them off chain, and then put them on chain and cheaply verify them on chain. And we refer to those proofs as validity proofs. Instead of zero knowledge proofs because they prove the correctness or validity of the execution instead of actually being zero knowledge or privacy preserving. That's one of the things that confuses people. A lot of people whom I talk to actually think that zero knowledge roll ups are privacy preserving. They're not there. They just use zero knowledge proofs because of that nice property of succinctness.
00:58:11.646 - 00:58:49.680, Speaker B: So essentially, the security of a zero knowledge roll up boils down to the correctness of the proof system that is being used. So let's say you use graph 16, which is one of the relatively modern proof systems. Assuming that it's correct and it's not broken, every time the proof is submitted on chain, you're guaranteed that it's going to be correct. And so as long as you trust the maps and you trust the proof system you used is correct, your funds will always be secure.
00:58:51.260 - 00:59:16.660, Speaker A: I guess. I mean, the big thing that you mentioned there is the hardware aspect needing to kind of be tailored to a single execution environment. What will it take to get that proof time down to generate the proof? Like to say much smaller time, and then make it a little bit more general purpose for multiple different virtual machines.
00:59:18.520 - 01:00:01.780, Speaker B: Hardware acceleration. So either proving inside GPU's or AsIcs, I guess probably the main approach, aside from optimizing the proof systems themselves, which bear in mind that these whole things that are being built right now. So scroll, CQVM, StarQuest, Car, were considered to be impossible like four years ago. So the fact that we currently have those system live on Mainnet in some cases is a miracle to some degree, because I remember looking at even basic execution, like 2017, 2018, and thinking, no, man, it's going to take another ten years for this to be viable.
01:00:04.040 - 01:00:53.280, Speaker A: It's amazing what the blockchain space has ultimately pushed to get some of these technologies to become, as you said, much more mainstream in a relatively short amount of time, where people ultimately didn't know how long it would take or even if it was possible. So it's been amazing to watch. On one thing, though, that you mentioned, I mean, just kind of having asics for the hardware standpoint or even on the optimistic side, one of the big criticisms of l two s as they stand today has been kind of around sequencers and sequencer decentralization. Can you talk about maybe on both fronts, on the optimistic side and the zero knowledge side, your point of view on decentralization around the sequencers and kind of creating these proofs?
01:00:54.680 - 01:01:48.800, Speaker B: Yeah, just one thing to preface before I go into it. So I stick to quite a stringent definition of strict definition of l two. And according to my definition, there are no live l two s. At the moment, the only l two is fuel labs, which has like $8 tvl is barely used. So, yeah, I would say while, let's say optimism, arbitram, starknet, etcetera strive to be l two s and rollups. In their current form, they're not, because a roll up has to only rely on the state correctness enforcement mechanism, which is either the fraud proof or the validity proof. And if you add any other assumptions on top of it, it's no longer a roll up.
01:01:48.800 - 01:02:00.236, Speaker B: And so they're kind of, I call them, a lot of people find it a bit insulting, but I call them glorified side chains with a promise of being roll ups.
01:02:00.388 - 01:02:02.780, Speaker A: Okay, I like it.
01:02:02.820 - 01:03:19.350, Speaker B: So, yeah, in terms of sequencer decentralization, another thing to add. So most of the zero knowledge roll ups are going to split the roles of sequencers and the nodes that compute the proofs. So you have sequencers that compute the blocks and submit the batches, and then the provers who compute the zero knowledge proofs for those batches and then submit the batches, the ability proofs on chain. And so those are two different problems. But sticking to sequencers, I think I said in one of the panels, like a few months ago, that paradoxically, it's more difficult to decentralize an l two rather than an l one, because with an l one, you just plop a consensus on top of it, have a bunch of nodes participate in it, and call it quits. Whereas with optimistic roll ups, sorry, with any roll up, you need to optimize and minimize the overhead that you introduce. Because if you just add another consensus on top of it, it adds unnecessary overhead.
01:03:19.350 - 01:04:19.180, Speaker B: And basically just, you're already paying Ethereum for the consensus. So essentially you're spending money on another consensus on top of that consensus. And because roll up security doesn't rely on sequencer decentralization, even with a centralized sequencer, the worst that they can do is temporarily censor a transaction. So it's a UX issue rather than a security issue. But ideally, of course, you would want to have decentralized sequencers, decentralized progress, decentralized everything. And I'm of an opinion that if you don't have that, then the UX is going to suffer, you're gonna have problems at some point. And so, yeah, so essentially it's this cat and mouse game where you want to have the best UX, but at the same time, you want to have the least overhead possible.
01:04:19.180 - 01:05:23.684, Speaker B: And I think for a lot of us, we're gonna accept a bigger overhead, but improve the UX, because I spoke about it with Anatoly. So in a hypothetical example where Solana becomes and Ethereum ZK roll up, let's say this in an alternative reality that is the case, Solana, you can, if you're a user interacting with it, you can use Solana's security properties for, let's say, some economic guarantees until the state and the volatility proof are committed to Ethereum. Then you can use Ethereum security. So essentially the model can be described as side chain security until it's secured by the underlying base layer. In that case, it's the security of the base layer. You can wait, you can wait for a bit. And as proof systems improve, as everything improves, the wait time is going to be shorter.
01:05:23.684 - 01:05:55.330, Speaker B: I think in a few years you're going to have wait times of a few seconds. Hopefully I'm not going to commit to that or promise anything, but yeah, hopefully that's going to be a case. But if you don't want to, you can essentially use the security, the economic security of the roll up to promise you some temporary finality guarantees. I wouldn't call it finality, but economic guarantees that your transaction is going to be included.
01:05:55.910 - 01:06:45.530, Speaker A: That makes sense. Yeah, I definitely understand that. I think I've always been fascinated with L2s, because once you start trying to decentralize the sequencers and start adding consensus, you get that overhead, which is unfortunate, but at that point it starts to look, like you said, kind of a separate blockchain or a side chain, if you will. So I'm curious, like how these do you think it will ultimately, I mean, beef and kind of sidechains that are kind of their own consensus, have their own kind of full node ecosystem, and then ultimately occasionally settle back down to an ethereum or to another layer one, just to get those decentralization properties.
01:06:47.590 - 01:08:16.720, Speaker B: So in my vision, you would have roll ups that would have really fast off chain confirmation. So let's say a few seconds, and then assuming that proof systems are optimized enough, once every, let's say a minute or 30 seconds, they would secure it with the base layer. So it's not going to be like once every hour or once every 2 hours, but it's going to be like once every block or once every two blocks. Technically you don't even need. So that's one of the interesting things about if you commit transaction data rather oh, I, sorry, I keep going on these tangents, but it's important to add that there are two data availability modes for rollups. One is you commit transaction data, compress transaction data, and another mode is you commit state differences instead of, instead of transaction data. So state divs are essentially, let's say you have a merkle tree which stores all the state, and you say instead of committing the transaction and having the nodes executed and then updating that state, you say, oh, by the way, this transaction resulted in this slot in the tree being changed from this value to this value, which for the vast majority of transactions is smaller.
01:08:16.720 - 01:09:22.806, Speaker B: But the downside is that you need to trust some external party that they store the transaction history because it's not unchained. So it's a trade off. But the advantage of posting the transaction data instead of state devs for ZK roll ups. Also note that state divs are only possible with ZK rollups, not with optimistic roll ups. Because with optimistic roll ups you need a way to reconstruct the state tree in order to trust that it was correctly committed, whereas with via execution, whereas with ZK rops, you can just do it by trusting the loyalty proof. But yes, but the advantage of having transaction data is that you don't need to wait for the finalization of the challenge mechanism or let's say the validity proof being submitted in order to finalize your transaction off chain. Because as long as you run an l two full node and the data is finalized on chain, you can verify that the data leads to correct state commitment.
01:09:22.806 - 01:09:51.870, Speaker B: So you don't have to wait for the bridge or the l one to be convinced that that state commitment is actually correct. And therefore you can have finality that spans Ethereum block. So essentially, if, let's say at some point Ethereum has single slot finality where every 12 seconds the block is finalized, you can have the same for l two s, l two s settling on l one.
01:09:53.330 - 01:10:15.350, Speaker A: I see. So that L2 kind of, with its distributed kind of node set and consensus, would ultimately have its own finalization. And because the L2 it has finalized in that ecosystem, you'll count that as final. Even though it's going to ultimately settle on to a layer one.
01:10:17.690 - 01:11:13.818, Speaker B: It'S technically already settled on an l one. It's just that l one doesn't know that it's settled on an l one because the l one doesn't run an l two node. So as long as the data is committed and you can verify that this date commitment, so you can compute the state commitment from that data. You don't need to wait for the l one to be convinced that the data commitment and the resultant state is correct. You just know that, yeah, at some point somebody will commit the validity proof or the challenge window will expire and this will be finalized by the l one. So it's a bit confusing because we've been having this debate amongst those different projects how to define finality for roll ups, because once the data is committed it's essentially finalized. But the l one doesn't know that it's finalized.
01:11:13.818 - 01:11:26.760, Speaker B: So do you view the finality from the perspective of an l two node, or do you view the finality from the perspective of the bridge on the l one that is verifying the commitments?
01:11:27.700 - 01:11:54.316, Speaker A: I guess in my point of view, if going back to our earlier comments on full nodes and economic security in some sense, or high Nakamoto coefficient, if the main purpose of the L2 is to inherit those features, to me it makes sense to have finalization once it's truly using the properties of the layer one, but it's still using.
01:11:54.508 - 01:12:05.000, Speaker B: The only difference is that you as an l two node are assured that it's finalized. It's just that the l one doesn't know until the validity proof comes in or the challenge.
01:12:05.460 - 01:12:25.000, Speaker A: That's my point of view is that once it's actually, I mean, yes, it will up in there eventually, but because it does not know or that transition has existed as of yet, then it would only be kind of final final once it's actually like hit the layer one.
01:12:27.500 - 01:13:18.860, Speaker B: So I agree with you. So I have those two visions. So I use two definitions. So one, which is a bit confusing to any outsider, I use off chain finality to define once the data is finalized, and then I roll up finality once the l one is convinced that the data is finalized. Because for you as an l two user, if you run an l two node, you don't have to wait. The only people who have to wait are the people who either run lite clients or the people who, and don't want to have some economic guarantees, or the people who are withdrawing from the l two to an l one because the withdrawals can only be processed after the l one is convinced.
01:13:20.880 - 01:13:51.860, Speaker A: Yeah, yeah, it is interesting, I'm curious what the industry is going to kind of think about this, and as kind of l two has become more prolific, more dominant in the ethereum ecosystem. But I guess going back to the consensus and the consensus overhead and just like general decentralization on the L2s and running L2 full nodes. What is your point of view for like a sufficient or fully decentralized L2?
01:13:55.120 - 01:14:46.540, Speaker B: From the perspective of security, you can minimize the number of full nodes and it's going to be as secure. From the perspective of censorship, resistance and economic guarantee, you should try to maximize it, but within reason. You don't have to have the same number as l one nodes. You can have like, let's say an order of magnitude less, as long as there's enough to plausibly guarantee that your transaction is not going to be temporarily censored. Or like, it's easy for an attacker to take over and let's say take a controlling stake and temporarily basically mislead you by promising that they're included your transaction into an l one batch, but not actually include.
01:14:47.560 - 01:14:56.420, Speaker A: So if ethereum in that example had like 100,000 full nodes, like a thousand kind of full nodes on the L2 would be sufficient?
01:14:57.520 - 01:14:59.326, Speaker B: Yeah, I think so. Yeah.
01:14:59.398 - 01:15:53.590, Speaker A: Makes sense. Cool. Well, I feel like we've almost been going an hour and a half, or a little bit shy of that typically. Maybe we can touch upon Mev super quick, and then I feel like you and I could talk about this forever, but I want to try to keep it hard, cap it a little bit on the time stuff. So let's go to mev, talk about that briefly, and then maybe do an episode too, because I've been having a lot of fun and love hearing kind of your point of view on the ETH roadmap and how things are going to play out. So on Mev, what is, I mean, you said earlier in the podcast that it was a little bit of a kind of naturally centralizing force. What is your point of view on cross chain Mev and just enter a chain Mev and kind of some of the differences between those.
01:15:56.300 - 01:16:53.818, Speaker B: For Ethereum, I think going forward that the more we move towards the l two s being the main focal point from the perspective of the users, because my vision is that in a few years, most of the users are rarely, if ever, going to touch Ethereum itself. Most of the transactions are going to happen on the layers above. So l two s, l three s, and then I. Ethereum is mainly going to be used for security as well as composability between roll ups. So let's say I want to move from scroll to optimism. I'll bridge through Ethereum. But then it's also, I think for the vast majority of users, you wouldn't even need to do that, as long as you're not moving like, I don't know, $100 million, you could bridge through, let's say, connext or some other bridges that connect roll ups directly instead of going through Del one.
01:16:53.818 - 01:18:02.000, Speaker B: And there are also, there's also like a lot of research being done on how you can bridge faster without adding additional trust assumptions through, like using third party bridges. And there are some interesting results there. Yeah. In that scenario, MeV becomes a bit of a different beat to what it currently is, because on top of having MeV centered in every single roll up and then on Ethereum, if we assume that there's some use of Ethereum as well, there's also going to be a lot of MeV between roll ups and between, let's say, a roll up and the base layer, for example, arbitrage. So let's say a uniswap pool on one chain is drained versus the other. It incentivizes people to move their liquidity around. And that creates an interesting new class of problems, I would say, in terms of MeV and how you would solve it and optimize it, because you need to find the fastest way to bridge between different chains.
01:18:02.000 - 01:18:36.080, Speaker B: And then. Yeah, because right now it's in terms of how MEV works. Right now it's relatively straightforward. You see a transaction, you either sandwich it or front run it, or you see an arbitrage opportunity and you arbitrage it. And that's basically it. Whereas if you see an opportunity on one chain, but you have funds on the other chain, you basically need to find the most optimal way to move and to exploit it as fast as possible.
01:18:36.480 - 01:19:05.110, Speaker A: Yeah, I'm super curious about this as well, and ultimately where it will end up. I'm curious about roll up to roll up to communication and how that end up working, it does seem like it's going to be a bigger issue as we have different roll ups and different kind of ecosystems. And then, as you said, the liquidity moving from one to another kind of. Yeah, it'll be interesting to watch.
01:19:06.890 - 01:20:15.120, Speaker B: Yeah, it's definitely an open ended question. I think it's from technical perspective, it's not an insurmountable problem. It's just a problem of do we actually standardize the way bridges function, or everybody has their own way of functioning, and then basically the developers figure out a way to fix the communication between the bridges and people are attempting. I had a few conversations about, like, standardizing bridges or like doing an EIP on like a bridge spec. But I considering that the effort involved in optimizing every bridge design, I think it sounds likely that you'll get some standardized API and therefore it'll be more responsible responsibility of like the cross chain bridges to optimize and basically support every single API, or like the smart contract functionalities of the bridge.
01:20:16.020 - 01:21:07.790, Speaker A: Yeah, that makes sense. Interesting, very interesting. Well, thank you again for coming on the podcast. I feel like, I mean, we've touched upon so many topics, kind of what people view as decentralization, kind of different ways to try to quantify it, what you can quantify, what you can't quantify, different discussions around full nodes like clients roll ups, ZK optimistic, Ethereum's improvement proposals in terms of throughput with proto dank sharding and full dank sharding. Mev, we touched upon a lot, and I think it's amazing that you have kind of the wealth of knowledge and are able to be such a strong proponent for the community. So really appreciate you coming on the podcast and sharing all these thoughts.
01:21:09.130 - 01:21:59.210, Speaker B: Thanks for having me. It was a lot of fun. I enjoy having talking about tech rather than arguing about stupid thing and stupid things on Twitter. It's more exciting and I think the way Twitter works, it's more optimized to make a short comment and then not explain your viewpoint properly, and then people just attacking it or you attacking somebody else rather than having a proper discussion. So I always like this kind of a setup where you can actually have a long form discussion and basically talk out all the nuances and the differences in terms of viewpoints.
01:21:59.670 - 01:22:16.530, Speaker A: I fully agree, and that was the goal of the podcast, be able to talk long form with smart people such as yourself. So thank you again. I appreciate your work and look forward to continuing talking and discussing on Twitter as well. But yeah, thank you for coming on the podcast.
01:22:17.750 - 01:22:18.990, Speaker B: Thanks a lot for having me.
