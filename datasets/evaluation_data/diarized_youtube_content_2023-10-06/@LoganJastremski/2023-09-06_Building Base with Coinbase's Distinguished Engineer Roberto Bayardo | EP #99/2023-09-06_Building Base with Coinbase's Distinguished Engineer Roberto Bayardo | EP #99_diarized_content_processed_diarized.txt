00:00:00.160 - 00:00:46.256, Speaker A: We also just got to know a lot of the folks working on Ethereum, and they're just some of the brightest people in the space. They really think through the details really clicked with the optimism team, and the one thing I think that I really liked that they were doing is that they're focusing on the foundations. A lot of teams were focusing on something very specific, like Arbitrum is about its fraud proofs, Polygon. ZK is really about their ZK. Optimism was about creating somewhat appropriately modularized platform. We felt we could play in that world kind of as a peer, not really as kind of like a second tier customer. I think optimism, despite maybe being a little bit behind on the fraud proof side, has an architecture which we believe will actually leap rock some of these other chains.
00:00:46.256 - 00:01:03.820, Speaker A: I think there'll be places for other blockchains. I'm not an ethereum, Maxi, by any means. I admire a lot of the work going on in sort of the second 3rd gen blockchains, I guess, the Aptos, the Solanos, the sweeties, and so on. And now some of the hyper optimized evms. And that stuff is super cool, too.
00:01:06.500 - 00:01:44.150, Speaker B: Roberto, thank you so much for coming on the podcast. As we were talking earlier, I just really appreciative of you and what the Coinbase team have ultimately done for the entire industry, not only with base, but pushing for crypto and crypto adoption. It's truly been amazing to watch, as an outsider for how much the team has done to push forward the entire industry on the legal front, or even from setting up the exchange to allowing people to interact with crypto. So just wanted to start off the podcast and say thank you to you and the team for what you've done.
00:01:44.270 - 00:02:14.030, Speaker A: Yeah. Well, first of all, thanks for having me. I've been eager to talk about base and a podcast for some time, and so this is actually my first podcast that I do get to do that. But, yeah, Stan with crypto. Right. Our leadership team has really been adamant about standing up for the industry and, you know, more than just, you know, because of a bottom line thing, because they really believe in the mission and the principles behind it. And so that's been inspiring for me to see.
00:02:14.030 - 00:02:30.300, Speaker A: I can't say that I'm the one that's driving that personally, but I'm totally on board for them, you know, to try and see that we have avenues to continue to grow this business, which I think has a lot of potential benefits for society.
00:02:30.640 - 00:02:45.768, Speaker B: Yeah, no, I think you and I kind of nerd out on the technical details, because ultimately what we really want to enable is getting this technology in the hands of hundreds of millions and billions of people. And we've really just kind of started that process.
00:02:45.904 - 00:03:13.654, Speaker A: We are really early. Yeah, that's the amazing thing. And we're really early. The amazing thing is we're this early, yet the numbers in terms of dollar signs anyway, is pretty huge. So when think about the potential, there's some problems we need to solve to bring the masses on chain. I'm fully convinced we'll get there. We have a lot of smart people in this space.
00:03:13.654 - 00:03:15.662, Speaker A: That's one thing I've really been impressed by.
00:03:15.726 - 00:03:30.024, Speaker B: Likewise, to that point. This being your first public facing appearance to the crypto crowd, could you dive a little bit into your background? What got you excited about the crypto industry?
00:03:30.152 - 00:03:57.706, Speaker A: Yeah, yeah. It's been a long road for me to kind of become fully immersed in it. We were talking about this earlier today. I actually got involved in what I would call decentralized and trustless computing 1.0, pre blockchain era kind of peer to peer computing. When Napster came along, I thought that was like the coolest thing in the world. You could go and download music from just random people on the Internet and you could chat with them.
00:03:57.706 - 00:04:43.344, Speaker A: So it was peer to peer, not just network wise, but person to person as well. So there's a community aspect as well, but there's flaws in those protocols and they tried to fix them, but ultimately there was no proper incentive mechanism. There was no good reason to just leave your software running, right? There was only downsize the record companies coming in after you or something like that, or using bandwidth that you might have to end up paying for or getting disconnected by your cable Internet connection. So the incentive mechanisms weren't there and that kind of died down for a while. But nevertheless, I found it really inspiring. And so of course, when the bitcoin paper came around, I didn't discover it until maybe two years after it was written. But when I read it, I was just blown away.
00:04:43.344 - 00:05:39.330, Speaker A: Okay, this solving the right problem, right. It provides an incentive called mining, in which you can generate new coins and that can incentivize people to run nodes and collect transaction fees and creating just an entirely new form of money. I didn't understand it necessarily from the economic perspective, but I realized pretty quickly that it was solving a very interesting technical problem. So yeah, that's kind of what really got me, I think started in this space, though. By the time that happened, I was working at Google, which really doesn't have any interest in that kind of thing. So it had been a hobby of mine for some time while I was doing kind of other things. But ultimately I think the interest kept building and I think I also ran out of interesting things to do at Google, then just decided to jump in the space full time.
00:05:39.330 - 00:05:50.002, Speaker A: I've been at Coinbase for almost two years now. Doing this full time to figure out ways of bringing Coinbase users on chain has kind of been the, been the theme of my work since I've joined.
00:05:50.146 - 00:06:13.802, Speaker B: Fascinating. Well, let's just dive into it then. On some of the technical fronts. I think the industry as a whole would love to learn a little bit more about your thought process on building base. On top of Ethereum choosing the optimism tech stack for base. Could you share any color around some of the thoughts there?
00:06:13.866 - 00:07:08.968, Speaker A: Yeah, I'd love to. Naturally, it's a variety of factors that sort of led us where we are, but we really did a pretty detailed investigation before we concluded, before we made these decisions and how we've ended up when we're here. And in fact, the way I got to know Jesse was both of us were interested in building things on chain, getting users on chain. How do we do that? Well, initial thought was maybe we'll just partner, right? Maybe we'll just figure out a chain on which we can build on. And so what's the best thing we can possibly do? So we met with a bunch of different folks. We met with various l two s, we met with various Alt l one s, I guess you could say, and incredible talent out there. But ultimately I think what we boiled it down to is there was a few factors that we really wanted to weigh heavily.
00:07:08.968 - 00:08:07.140, Speaker A: One, Washington in just utmost consideration for security, really strong team and a team that we thought we could work with and platform, tools, ecosystem, community, these things all came into play. Look, Ethereum is really dominant along a lot of those dimensions, but it fails on cost. But at that time, the l two vision was starting to play out, promising much lower fees. That's what we concluded very quickly, that, look, we can jump into this l two ecosystem, whether it's through partnering with an existing one or building our own. I think that decision came a little bit later. But that decision to bet on the EVM, Ethereum, specifically l two s, yeah, that was made about a year, year and a half ago, a little bit before the whole base thing kicked off.
00:08:08.120 - 00:08:35.612, Speaker B: And maybe just diving a little bit deeper into that technical choice. You mentioned security and also the developer ecosystem. I think no doubt Ethereum has amazing developer community, very robust tools for developers. Can you dive a little bit deeper into the security aspect? What specifically about Ethereum kind of contrarize versus other ecosystems?
00:08:35.716 - 00:09:13.430, Speaker A: Yeah, well, I mean, some of it's quite simply the Lindy effect. Right. It's been around for a long time. It's been battle tested. But we also just got to know a lot of the folks working on Ethereum, and they're just some of the brightest people in the space. They really think through the details. I mean, if you look at things like EIP 1559, how they just redid the entire fee mechanism in a way that they had some ideas of how it will work out, and then it basically ended up almost exactly as predicted, definitely smoothed the volatility transaction throughput.
00:09:13.430 - 00:09:30.208, Speaker A: You could just tell the way that they were operating, that they really were thoughtful and thinking through carefully, in particular with that switch to the. And so I think that view, when we witnessed that, we were also quite impressed even before the merge happened.
00:09:30.224 - 00:09:30.352, Speaker B: Right.
00:09:30.376 - 00:10:09.422, Speaker A: We're kind of seeing the process towards that. Yeah. And so just, you read lots of Vitalik's writings as well, the detailed writings he's provided on their consensus mechanism and how it works and why those decisions were made very clear, very thoughtfully laid out. Yes, there's been a lot of really innovative work and consensus mechanisms, but some of it's not as well tested, battle tested. And so those gave us just a little bit more concern. It may be that some of those may play out and turn out to be wonderful tools, but on the other hand, I think if that turns out to be the case, it wouldn't surprise me if Ethereum would eventually adopt those ideas as well.
00:10:09.526 - 00:10:19.866, Speaker B: Yeah, it does. I think one of the beautiful things about the industry is everybody's kind of looking at each other and being like, all right, what are they doing? And adapting best practices?
00:10:20.038 - 00:10:47.552, Speaker A: Yeah, there's so many interesting trade offs in the space. And I think the other thing, the other aspect of Ethereum that we really liked was they really have this commitment to maintain a low barrier to entry. They really want to make it to where almost anyone can run a node because they find that they feel that's incredibly important for decentralization. I like it, too. I have a bunch of computers in my water heater closet. My wife thinks I'm nuts. I run Ethereum validator on it.
00:10:47.552 - 00:11:10.300, Speaker A: I'm actually running a base node because we're doing some work on a rust version of op rust. And so we're testing it, just sync things at home. Just any chain that I can run myself, I feel better. It feels like, okay, this is decentralized. If I can do it on a commodity PC, then a lot of other people can do it as well.
00:11:11.160 - 00:11:45.720, Speaker B: On the hardware aspect, it has been interesting. Different ecosystems really playing around with the hardware aspect. Some like ethereum purposely kind of keeping the hardware requirements smaller, other ecosystems increasing the hardware requirements. Do you feel like having hardware requirements low necessarily allows more nodes to run? Or I guess, how important do you feel like the actual cost of the hardware should be in kind of some of these ecosystems?
00:11:45.800 - 00:12:33.196, Speaker A: Yeah, it's a fairly nuanced question, actually more nuanced than I think most people realize. And interestingly, Vitalik just gave a talk. I was at the science and engineering of consensus workshop, I guess sort of a pre workshop to science and blockchain. And Vitalik gave kind of a remote talk where he touched on some of this stuff, right. Talking about scalability and in particular, and it's related to this question of hardware requirements, because you can jack up transactions per second with higher hardware, higher quality hardware, for example, there's other ways you can do it as well. I think some people have just taken Ethereum and said, we'll have bigger blocks, we'll have shorter block times, but there's secondary externalities to doing that that then come into play. And I think they're not as well understood.
00:12:33.196 - 00:13:06.782, Speaker A: And the Ethereum folks are very paranoid about that. They're very conservative. And again, because of our emphasis on security, that resonates with us. And so given that conservativeness, given that they're not eager to crank up block size and so on, that fits very neatly in with, well, you may as well make it work on commodity hardware while you're at it as well. So all these things kind of have really interesting interplays and fit together in this way. And that kind of defines ethereum in the way it is. Pluses and minuses.
00:13:06.782 - 00:13:35.190, Speaker A: The minus is the TPS is a little bit low. The scaling roadmap is a little bit longer, quite a bit longer in some cases. But we convinced ourselves that the roadmap is sound and it'll be able to reach the levels that we think we'll need for a user base of Coinbase's size. I think I'm rambling a little bit. Please feel free to stop me. That said, I think there'll be places for other blockchains. I'm not an ethereum, Maxi, by any means.
00:13:35.190 - 00:14:01.814, Speaker A: I admire a lot of the work going on in the second 3rd gen blockchains. I guess the aptoses the Solanos, the sweats and so on. And now some of the hyper optimized evms. That stuff's super cool too. Again, that technology, like you said, people are watching each other. Pieces of it will be interchanged between different parts. But yeah, I think a community as a whole, I think we're going to be solving this problem, not just sort of any one team.
00:14:01.942 - 00:14:31.024, Speaker B: Yeah, yeah, no, it's. It is fascinating, as you said, I think there's numerous different trade offs for different ecosystems. And I'm always been kind of fascinated just watching each of them and kind of what they're optimizing for. It is kind of interesting, but maybe definitely appreciate the additional color and kind of choosing the Ethereum stack. Can you maybe also expand upon why optimism and the optimism technical stack?
00:14:31.072 - 00:14:32.944, Speaker A: Yeah, and why and l two, right?
00:14:32.992 - 00:14:33.272, Speaker B: Yes.
00:14:33.336 - 00:15:16.010, Speaker A: So obviously fees on Ethereum l one right now are way too expensive for the average user, certainly the average Coinbase user. So l two is a natural way of reducing those fees while still remaining in the ecosystem. But in a way that doesn't compromise on certain things. We could have done the normal thing of, okay, let's just create a ethereum fork, which again jacks up the block times or whatever, but we didn't want to compromise on those security aspects. And as most of your listeners are probably aware of, the vision of l two s is to be able to scale in a way where you do inherit that security of the base layer. Now, of course, we're not there yet. There's a lot of work to do.
00:15:16.010 - 00:15:44.414, Speaker A: We have this stage zero, stage one, stage two categorization that Vitalik has laid out. We're at stage zero right now. Not going to try to pretend otherwise, but you have to skate to where the puck is going. We looked at the vision, we found it was sound. Then at that point we bought into the l two vision. We've been bought in the ethereum l two scaling vision. What is the right platform to build on at that point? Frankly, there's a lot of really good ones.
00:15:44.414 - 00:16:23.570, Speaker A: There's a lot of options, really different, difficult choice. And we went back and forth a few times, but ultimately we really clicked with the optimism team. And the one thing I think that I really liked that they were doing was that they're focusing on the foundations. A lot of teams were focusing on something very specific, like arbitrum is about its fraud proofs, Polygon. ZK is really about their ZK. And wonderful work on their Zkhdem kevm. Optimism was about creating somewhat appropriately, modularized platform on which they've developed into this super chain vision.
00:16:23.570 - 00:16:51.262, Speaker A: We felt we could play in that world as a peer, not really as a second tier customer, that maybe we'll be a layer three going into a L2 or something like that. I think optimism, despite maybe being a little bit behind on the fraud proof side, has an architecture which we believe will actually leapfrog some of these other chains, perhaps a not too distant future. We'll see some really interesting developments going on there, though.
00:16:51.366 - 00:17:01.078, Speaker B: Is that leapfrog, do you think will be enabled by focusing on the foundation having those modular components that can eventually be upgraded over time?
00:17:01.214 - 00:17:32.274, Speaker A: Yeah, that and just the community aspect as well. They've got this public goods funding model and they do these rfps. They've done these rfps for fault proving development. I think they've accepted two of them. One of them is from risk zero, who just gave. I was at the Rust Ethereum thing on Saturday and they gave a wonderful update on the work that they're doing. I think ZK prover is right around, maybe not right around the corner, but it'll be 2024 for sure.
00:17:32.274 - 00:17:48.580, Speaker A: There'll be something ready that I think we can deploy on the op stack because it's got a modular proving architecture. People associate optimism with broad proofs and optimistic roll ups, but frankly, it's far outgrown that narrow label.
00:17:48.880 - 00:18:15.130, Speaker B: Yeah, it is interesting, I guess maybe expanding upon that optimism vision, or maybe the vision that you think will ultimately play out. Do you see optimism? Are these super chains being hundreds of chains, thousands of chains? Maybe just add some color. And in this end state of l two s, how are these different kinds of landscapes working out? Are they talking with each other?
00:18:15.290 - 00:18:57.116, Speaker A: That's a great question. Right now it's not hundreds of chains, because if you want to do a true l two, you're still posting everything to the l one, which that is your bottleneck. But what l two s are giving us now, they're giving us execution starting, and they're giving us state sharing. So by putting things on an l two, the l one does not necessarily have to maintain as much state as it does before, doesn't have to execute as many transactions as it does before, but it still has to store all the data. But there's solutions to that as well. I've been working on with Ethereum Foundation, EIP 4844 was involved in some early prototyping there. That is a small step in that direction.
00:18:57.116 - 00:19:22.940, Speaker A: Ultimately, though, what we need is data sharding as well. We've got execution sharding. We've got state sharding, effectively with ltd twos. We do not have data sharding. That is the next step. Everyone's heard the term dank sharding by now. That is sort of the next scalability, I think breakthrough or whatever you want to call it, where, I mean the big thing about that is now if I'm running a validator, I don't have to download everything.
00:19:22.940 - 00:20:01.276, Speaker A: I only have to download a fraction of this blob data that is introduced by EIP 4844, allowing greater scalability. That's going to get you up to, I don't know, maybe dozens of chains. I haven't done all the math. At least the way dank sharding is currently defined. We can get up to thousands though, with alternate data availability layers, again compromising someone on security. But by the time we're there, I'm sure there's going to be other breakthroughs. There's so many different dimensions of scaling, we can get into more of them, right?
00:20:01.348 - 00:20:02.012, Speaker B: I would love to.
00:20:02.076 - 00:20:18.732, Speaker A: So l two scaling is mostly about horizontal scaling, at least. I define it as horizontal scaling. There's vertical scaling techniques as well and I think they can come to play in the Ethereum world. And then there's off chain scaling. Right. This is the magic of zero knowledge proofs. There's going to be some amazing stuff there as well.
00:20:18.796 - 00:20:33.638, Speaker B: Yeah, I do agree. One thing that you said that caught my mind was really the execution charting and the data sharding. When you refer to execution charting, is that just having multiple different L2s?
00:20:33.734 - 00:20:53.524, Speaker A: Yeah, exactly. And that's why you want more than one L2. There shouldn't just be one L2 because then it's going to be executing every transaction and that's naturally going to be a bottleneck. It's going to mean if you want to play in that world, if you want to download and recreate the chain, it's going to be expensive. Yeah. So there's a lot of benefits to horizontal scaling. Right.
00:20:53.524 - 00:21:20.010, Speaker A: And the management of state and storage and syncing and so on. Some downsides as well, obviously. Right. What if you want to do things across chain? That becomes a lot harder. But this comes back to the super chain thing, right? The super chain mission anyway is to make this more seamless. Interoperability across l two s that do build on that platform. A lot of work to be done there.
00:21:20.010 - 00:21:25.026, Speaker A: I'm not claiming they've solved all those problems yet, though. But that's the vision perfect.
00:21:25.098 - 00:21:46.174, Speaker B: Then the second thing that you mentioned was the data sharding that will ultimately be implemented with think sharding. Kind of a stopgap to that is 4844. You mentioned you were working on that ahead of time, or with the Ethereum foundation, maybe expand on 4844.
00:21:46.222 - 00:22:07.198, Speaker A: Yeah, I think that's worth talking a bit. It's very poorly understood for some reason, for what is actually a very simple thing. Maybe not very simple, but it is a fairly simple thing. I think people have maybe gotten the impression that 4844 is the ultimate scaling solution for Ethereum. It's not. It's a step towards that. It's a small step towards that.
00:22:07.198 - 00:22:45.790, Speaker A: What it does is. So again, the way l two s work, they have to have data. They have transactions. Those transactions need to be available for anyone to be able to validate the chain, compute the state and the way they work now, and to make sure it's available at the security of the l one is they post it to the l one. As long as the l one exists, that data is there. You can grab it, you can run the transactions, you can prove that it's being done correctly. What 4844 does is it says that data doesn't need to be around forever, it just needs to be around long enough, at least on the l one, to where you can validate the proposals that are written to the l one.
00:22:45.790 - 00:23:22.700, Speaker A: This is getting a little techno. I don't think I want to dive down too much there. But the bottom line is, yeah, so it introduces a new data type. So batches of transactions right now are written in something called call data, which is one of the cheaper kinds of data in Ethereum, 16 gas per non zero byte for those of you who are technically inclined to. But it's persisted forever. And so naturally you don't want to lower that cost of gas because it has a real impact on people who are unknowns. Now, blob data is new kind of data that EIP 484 introduces, literally just raw blobs of bytes.
00:23:22.700 - 00:23:47.060, Speaker A: Ethereum l one doesn't care what they are. In fact, Ethereum has consensus layer and an execution layer. The execution layer doesn't even have to deal with blobsen. That is handled entirely by the consensus layer. Execution layer just needs to know their hashes. And so the consensus layer, they get these blobs, they collect them, but after some amount of time, they can just throw it away. So you can handle that much more cheaply.
00:23:47.060 - 00:24:17.856, Speaker A: Eap for it, for the initial parameterization is very conservative. They're going to do a maximum six blobs per block. Each blob is 128 kb, but a target of three blobs for block. So it'll be about 0.375 extra data, which can be provided basically at almost no additional extra cost because it's so cheap for these consensus clients to maintain. And so, yeah, you get more data availability storage. Your l two costs go down.
00:24:17.856 - 00:24:25.070, Speaker A: Well, either your l two costs go down or you get more transactions on the total sum of l two s, depending on how it works out in the marketplace. Right?
00:24:25.160 - 00:24:25.666, Speaker B: Yeah.
00:24:25.778 - 00:24:28.410, Speaker A: Did that make sense? Sorry, that was a lot of words.
00:24:28.450 - 00:25:27.880, Speaker B: No, I mean, I think it makes, I nerd out about all this stuff a lot, so I'm in the details, but maybe for me I can just re articulate it for the listeners. Ultimately, the data availability is kind of the final bottleneck in scaling these blockchains, because the l two s have to post the data route to the l one to inherit the security. With 484, you're separating the two data types. One's less persistent than the other, and then you're increasing the amounts of data for specifically l two data blobs. That will ultimately result in cheaper data for l two s to settle to. And it's starting off a little bit conservative, but over time you can expand the amount of data that the blobs will hold. And then this is the first stopgap to ultimately dank charting.
00:25:27.880 - 00:25:33.960, Speaker B: And dank charting, I think it will have 16 megabyte blocks.
00:25:35.020 - 00:26:20.514, Speaker A: I've forgotten the exact numbers for dank sharding, and my guess is they're going to change by the time it comes along. Because, look, if you read the original 4844 proposal, it was supposed to be 16 blobs, maximum of 1 mb each, with a target of eight. If I remember correctly, it was much more aggressive, but people became a little more concerned about like, can the p two p layer handle this? So, got dialed back. Dialed back to two blobs, target four maximum for a little while, and it's still a chance that it will actually go out that way. I hope it doesn't. I think three six is perfectly valid, but I think a little more vetting still has to be done there. But, yeah, I think the goal is just to make sure the p two p layer, there's no unanticipated consequences there.
00:26:20.642 - 00:26:33.310, Speaker B: And on the peer to peer layer, are you referring to specifically the amount of bandwidth that individual nodes have, and if you increase it too much, the nodes won't have enough bandwidth to continue?
00:26:33.650 - 00:27:00.378, Speaker A: Precisely. Yeah, but the reason I think three six is very conservative, is 0.375 megabytes. If you look, if you had a block full of just l two call data, that's about a full megabyte. So you're already dealing with this problem somewhat pretty effectively. That's not a huge increase in my opinion, so I think it'll work out, but it's good to be conservative. You can always increase those numbers in the next hard fork.
00:27:00.378 - 00:27:31.400, Speaker A: I know ethereum wants to have a faster hard fork cadence. It's been a little struggle to get it there. Yeah, there might be a more narrow, I guess, one where they could do that if it turned out to be a problem. Right now, with the existing demand, we're doing okay. I think l two fees are reasonable, at least for the types of applications we're seeing right now. Stuff could spike, though, and that could maybe force a more quick reaction.
00:27:32.540 - 00:28:01.580, Speaker B: Yeah, I'm super fascinated. Just on the data front, I mean, as you mentioned, l two is trying to solve that execution component. Then you have the data increases coming on ethereum's base layer with 4844, and then ding sharding. Can we maybe go a little bit more into the sharding of data on ding sharding and how ultimately you see that playing out in future, what that ultimately enables for ethereum down the line?
00:28:01.660 - 00:29:08.730, Speaker A: Yeah, I think the important thing is it simply means that if I run a validator node, I don't have to receive and store all that data immediately. We can handle much larger amounts of data without my node being inundated with all of it and me having to store it and serve all of it. I think that's really just the fundamental important part of it that's enabled by what's called erasure coding. That was actually a big complexity in 4844, is they wanted to get in an erasure coding scheme in that initial version so that it would very easily be able to migrate into that dang, sharding world. And so they use something called KZG commitments for encoding that blob data, which gets even more esoteric cryptography. It does two things. It allows you to do magic commitments to the data and useful in ZK and so on, but it also has this erasure coding property where you can split up the data, spray it across a bunch of nodes, and then you can prove that that data is available without downloading it.
00:29:08.730 - 00:29:11.790, Speaker A: That's another really critical aspect of dank sharding.
00:29:13.010 - 00:29:17.570, Speaker B: Is that ultimately through data availability sampling, or is that just exactly.
00:29:17.610 - 00:29:18.658, Speaker A: Yeah, okay. Yeah.
00:29:18.714 - 00:30:10.836, Speaker B: Interesting. Yeah, it is fascinating. I think I mean, as we are kind of setting up to really, I think what I really want to get to the heart of is kind of the core building blocks of scaling blockchain architecture. One is the execution side, and that's being done horizontally through these L2s like base. And then also on Ethereum, the increasing amount of data with 4844 for increased data availability and then increase ultimately sharding, which will be the second component to allowing more throughput for Ethereum. And then ultimately the goal of that is to enable either higher TPS or I always say, a bigger sand block for engineers to play it.
00:30:10.948 - 00:31:18.648, Speaker A: Yeah, it's interesting. A lot of the L2s right now could potentially crank up throughput considerably, especially once the data availability problems are solved a little more. But then again, I think again these, what I call the secondary externalities of data growth come into play. If you look at optimism and arbitrum, they've both been parameterized to give you about roughly two x the throughput of layer one. Again, pretty conservative, but keep an eye on it, see what happens to state growth, make sure the implementations can keep up. And then, yeah, I think we'll have, there's other parts of the vision, stateless ethereum for example, or there's other ideas that may come into play eventually where you nodes don't have to maintain everything, they just have to maintain the accounts that are relevant for the apps that they support and so on. So there's a lot of wild stuff like that that I think eventually we're going to have to incorporate in order to achieve these grandiose numbers that Jesse loves to talk about.
00:31:18.648 - 00:31:26.758, Speaker A: A million developers and a billion users on chain. Yeah, we think it's doable, but I think all this kind of stuff has to come into play.
00:31:26.904 - 00:32:02.864, Speaker B: And maybe to that point, I guess maybe on your estimates, are there any targets that you think we need to get to, whether on the data availability side, whether in megabytes or gigabytes, and then is it more so just continuing to add different L2s for continued or higher amount of execution to take advantage of that increased throughput? Are there numbers in your head that you've been doing to ultimately get to say, 100 million users?
00:32:02.992 - 00:32:45.322, Speaker A: Yeah, I have not done that detail math, just very loose back of the envelope thing. I'm just focusing on what are the next steps to get us there. But I think another thing we haven't spoken about yet is there's stopgaps. We're talking about a vision where that this scalable Ethereum world, if everything's a true l two, or it settled the l one, fully provable and fully challengable. Not every blockchain needs that level of security. This is where the whole app chain thing comes in, which I'm sure you've heard a ton about. Layer three s can provide that app chain thing.
00:32:45.322 - 00:33:12.530, Speaker A: Then there's all databuilded layers I can layer. I know you've had Sriram on your podcast before. They're already offering one. It's what I understand, which you can start tapping right now. And through their restaking stuff, it's actually secured by Ethereum. Maybe not the full brunt of Ethereum, but a pretty good chunk of it. I think these kinds of mechanisms will come into play in the scaling roadmap as well, and that's going to be an important part of getting to a billion users for sure.
00:33:13.790 - 00:33:44.170, Speaker B: Maybe on that different data availabilities or volitions, I guess in coinbases or bases vision. Is it continuing to allow flexibility for different settlement layers, whether that's Ethereum or another data availability committee or eigen layer, just so users have a little bit of more preference for what security they would like or how cheap ultimately they would like their transactions.
00:33:44.250 - 00:34:29.286, Speaker A: Yeah, right now we're not pursuing that personally. What we want to do is we want to build base to be this maximally secure chain with reasonable costs, maybe I wouldn't call it low cost, but at least reasonable cost. That's our goal right now for base because we want it to be a seamless transition from our centralized exchange into the on chain world. We don't want there to be a step function decrease in security, for example, when you get there, we don't have to solve all the problems. Once you're on chain, you can move on to one of these other. You can maybe bridge over to an l three or an Alt L two. That does offer maybe even lower prices and higher TPS or whatever else you really need for whatever particular application you're doing.
00:34:29.286 - 00:34:37.262, Speaker A: But yeah, I think we're probably. Again, we're so early. Our team is really small, by the way. I don't think people realize how many.
00:34:37.286 - 00:34:40.902, Speaker B: People are working on the base team before launch.
00:34:40.966 - 00:35:21.378, Speaker A: It was maybe half a dozen of us. We've added a few people since then. I think we're up to about a dozen people right now. To give credit, there's a lot of other contributors from around the company that have chipped in in their spare time and that have kind of been on loan temporarily. So we've gotten a lot of help from others in the company, but the core team has been pretty tiny. And so, yeah, we have a bit more of strategic thinking to do, but this is our first priorities are that really safe environment for our users and usability is also an important thing, which we haven't even talked about yet. It's not as interesting technically, I guess, but making everything work together really nicely.
00:35:21.378 - 00:35:36.350, Speaker A: We've got Coinbase wallet, we've got this dapp marketplace in Coinbase, just giving users options of how they can use on chain apps in a way that's safe and secure and that doesn't confuse the hell out of them.
00:35:37.610 - 00:35:58.294, Speaker B: I think really simplifying the experience is also key. Just outside of the technical aspects, obviously, we have to technically be able to support the amount of users that we're really looking to get to on the scale side, but we also just have to make it easy. And it's not an easy problem to solve.
00:35:58.422 - 00:36:25.946, Speaker A: No, I mean, like, I had a. I wanted to, like I was at the Rust Ethereum thing and they had an NFt mint on Zora, right? And I couldn't get it to work right. It's like, you know, I've been doing this for years and I couldn't get it to work right. So, yeah, we have a lot of work to do on. On making these systems a little more reliable and robust and so on. And sometimes it's not the blockchain, it's just there was a glitch in wallet or maybe it didn't know the Zora network. I don't know exactly what happened.
00:36:25.946 - 00:36:30.634, Speaker A: I filed a bug report. But yeah, this kind of thing is really common right now. It needs to get a lot better.
00:36:30.762 - 00:36:54.588, Speaker B: 100% in terms of, I would say another kind of popular discussion within the crypto community around L2s has really been about single sequencers or having multiple sequencers. Can you kind of expand upon the basis team's thoughts around the conversation that's being had today?
00:36:54.724 - 00:37:22.048, Speaker A: Yeah. So credible decentralization is something we absolutely want to, want to provide. We do have elements of decentralization now, even with single sequencer. The challenge with a single sequencer is often misunderstood. Simply having a single sequencer doesn't mean you can do whatever you want. We cannot forge transaction signatures. The optimism protocol is defined by signed transactions that are posted.
00:37:22.048 - 00:38:00.470, Speaker A: The l one. We can't post anything fake there, but with a single sequencer you can do other things that are. We just have to give you our word that we're not going to take advantage of. There's this law of change thing, I don't know if you saw that came out of, I don't even remember this is in there. But one of the things we've promised is that we're not going to do fancy transaction ordering things for me, right? You send us a transaction, we're going to put them in a block, we're going to sort them by priority fee, boom. But when we have a single sequencer, you're trusting us to do that. So decentralized in sequencer is definitely of interest, though.
00:38:00.470 - 00:38:33.422, Speaker A: It's a, we want to make sure the security issues are those decentralization issues are a higher priority for us. So validity proofs, fraud proofs, making sure our upgrade keys are sufficiently decentralized. Coinbase does not hold all the upgrade keys. Right now it's between us and optimism. We want a larger party to be able to do that. Things like that are the things we're immediately focusing on. But we are talking the decentralized sequencer providers as well and figuring out how we will ultimately leverage that as well.
00:38:33.422 - 00:38:43.838, Speaker A: And unfortunately, I missed the decentralized sequencer espresso talk at the workshop, which got shifted in the later session. But I'm going to have to go back and watch that video because that looks really interesting.
00:38:44.014 - 00:39:40.622, Speaker B: Yeah. And that the Stanford Blockchain conference I really believe is one of the highest signal conversations that I've been to. And so there's lots of amazing talks. But I appreciate you coming here to do the podcast and chat about base. The whole decentralized sequencer thing is interesting. I think theres one conversation around the real time censorship resistance of the blockchains. Can you maybe expand upon that? Do you feel like base? Not particularly base, but l two is where do you feel like the end state is for them? Do you feel like in one sense you're removing consensus and getting a lot of the performance properties by having a SQL sequencer, but then kind of come into some issues with potentially being less decentralized.
00:39:40.622 - 00:39:48.606, Speaker B: How do you view the end state of l two s? Do you think most of them will be decentralized? Do you think there'll be a single sequencer?
00:39:48.758 - 00:40:14.250, Speaker A: I think it'll be a mix again for a single sequencer. I think the biggest challenge right now for an op architecture is liveness. Our single sequencer goes offline, the chain grinds to a halt. That's a problem. Decentralized sequencers can fix that. There's other ways of fixing that as well, something called permissionless proposals that have been batted around. Maybe people will go that route as well.
00:40:14.250 - 00:40:36.326, Speaker A: But I think these companies like Espresso are going to make it really easy to do decentralized sequencers. So, yeah, why not adopt that? Right. But does that bring me back into the equation? I don't know. There's a lot of things to think through there, but we're keeping a very close eye on it. But there's other l two architectures. You mentioned censorship, by the way. I'd love to say a little bit about censorship.
00:40:36.326 - 00:41:09.080, Speaker A: You can't censor base because optimism has forced inclusion from the l one. So if you ever have, if the sequencer is censoring you, which it can do, not that we're going to do that, but it could. Theoretically, you can always force your transactions, the l one, obviously at a higher cost, but at that point, if you're being censored, you're probably just going to send a transaction that gets your money out of there and you'll be on your merry way. That's another thing, I think, that sometimes people don't understand about centralized sequences. It's not going to lock your money up forever.
00:41:11.020 - 00:41:19.060, Speaker B: There's a lot of technical nuance in all these discussions, and really there's a variety of so much.
00:41:19.140 - 00:41:39.324, Speaker A: And even that censorship resistancing, and that doesn't apply to all l two s. There's some l two s that post. Instead of batches of transactions, they post state diffs. And that makes. That gives you a trade off on censorship resistance. The forced inclusion becomes a lot harder. And as far as I understand it, it's not supported right now by those kinds of roll ups that do that.
00:41:39.324 - 00:41:50.408, Speaker A: The advantage of state diffs, obviously, is much less data posted a l one, so you'll get some lower fees. So, yeah, all these crazy trade offs, I mean, it's. Keeping track of them is just challenging.
00:41:50.464 - 00:42:14.780, Speaker B: It is challenging. And I guess on that point, do you feel like ultimately the consumer should understand these trade offs, or do you feel like eventually the industry kind of standardizes on a couple key technology stacks and those are kind of more commonly understood?
00:42:16.700 - 00:42:46.520, Speaker A: Ideally, no, they shouldn't have to worry about that. And that's again why we're so interested in scaling Ethereum in a way that has the fewest compromises possible, because then you don't have to explain that to users. But it's inevitable that you're going to have to make some compromises to achieve the scale we want to make. So it's going to leak through somehow. I'm hoping that wallets can make that as seamless as possible. It's going to be a lot of collaboration, I think, between the protocol designers and between the interface and UI and wallet providers.
00:42:46.580 - 00:43:22.854, Speaker B: So yeah, and maybe, yeah, it is interesting. There's, on the engineering side just so many different kind of designs that you could choose. It is fascinating, but I think one thing that the team should really be proud of is just how quickly and how much adoption base has really gotten. Just looking at some of the metrics, I think it's one of the fastest growing L2s within all ecosystems. How does that feel just being on a smaller team, having that core engineering team?
00:43:22.902 - 00:43:57.798, Speaker A: Yeah, it's great. I think we were optimistic just simply because people see we have a lot of users and they want to build apps for those users and if we make it easy for them and get them to base, we want to be agnostic. We don't want to make base to be the only chain we support. Obviously we have sends and receives to arbitrum and optimism and all these other l two s as well. But I think people see that base is associated with Coinbase naturally, it might be an easier place for users to onboard. And so I think that's driven a lot of interest in developers. But also kudos to our biz dev team.
00:43:57.798 - 00:44:30.040, Speaker A: They've struck up partnerships with lots of creators, lots of big brands, some really cool NFT mints, and it's got a really interesting, unique vibe. And that's really been different than a lot of other chains because I think a lot of other chains, the way they've incentivized usage is through token airdrops. Not airdrops, but just token incentives in general. That's not something we're going to do. There's not going to be a base token. Sorry, I don't mind. Many times I have to tell people that everyone continues to ask that we're not going to do a base token.
00:44:30.040 - 00:44:50.978, Speaker A: ETH is the gas token. We're going to stick with that. We're going to incentivize usage in other ways. We think the best way to do that, the stickiest way to do that is through bringing on the best developers and the best applications. And so that's been the strategy. We've had some quick wins with Friendtech. This was somewhat unanticipated.
00:44:50.978 - 00:45:01.310, Speaker A: We didn't really expect that to take off the way it did. Yeah, let's just hope we get a continued string of those kinds of things and momentum continues to grow.
00:45:01.610 - 00:45:38.956, Speaker B: Yeah. Maybe expanding on some of friendtech and how it had that virality and really getting people, I think not only the crypto community, but expanding upon the crypto community. I think that was really the vision behind base and where we want to expand crypto too is getting not only crypto people, but mass adoption. And I think this product was really one of the first that I've seen bring outside adoption in.
00:45:39.068 - 00:46:19.192, Speaker A: Yeah, it really did a nice job of integrating social, you know, social kind of social network kind of elements with crypto. Right. I think that was the unique aspect of that system. I think social and blockchains are a really good match and no one's quite figured out the perfect way to do that. I think friendtech has an interesting angle on that. I hope it's sustainable. I'm not convinced yet that it is, but I think, yeah, I think they've got some really interesting ideas and if they can keep building on it and keep executing it, they added images recently which is, I think that was a great move and there's a great team behind it.
00:46:19.192 - 00:46:43.406, Speaker A: So I'm optimistic that they'll continue to poke at this problem. Another one that I really like is warpcast. They're moving the optimism chain. They've got a really interesting model as well. But yeah, the idea of being able to own your own data and have different ways of funding and incentivizing this stuff is really appealing to me.
00:46:43.598 - 00:47:29.804, Speaker B: Yeah, I've just been impressed by the adoption that base has gotten for really being live for a short period of time. Kudos again to the team. I think as you guys continue to get more adoption, how do you see either base changing over time with either increased compute or 4844? Do you feel like if, I guess more projects come into the base ecosystem, does that have any effects towards the scalability of it?
00:47:29.892 - 00:48:16.336, Speaker A: Yeah, it's funny you mentioned that. We did this pre mortem and our biggest fears were no one would use it or too many people would use it, the prices would spike and people like, oh, base Coinbase created yet another expensive blockchain. We seem to be in the sweet spot, but it could easily go in every other direction. Direction. So, yeah, so we've had a strong concern about how we're going to handle scalability and that's one of the reasons we got involved in EIP 4844 very early, because we saw that way back then as a key concern. If we dump all our users on chain, clearly bad things are going to happen with prices. And 444 was the nearest term solution to at least giving you a little more headroom on that.
00:48:16.336 - 00:48:32.834, Speaker A: And so we're going to keep doing that. Right. I think any scalability initiative for Ethereum, we're going to continue to try and support the best way we can. Yeah. And so, yeah, scalability, decentralization, we already talked about that as another area we want to keep pushing towards.
00:48:32.922 - 00:48:52.556, Speaker B: And do you feel like that will particularly be around like, Ethereum just needs more data to continue to kind of support Coinbase's large user base? Or do you also, how do you think about it in terms of like the execution environment and kind of scaling that as well?
00:48:52.668 - 00:49:45.090, Speaker A: Yeah, so there's a guy on my team who's very interested in going beyond this factor of two throughput of the Ethereum L1. So if you did have more data availability to be able to do that, would the system be able to handle it? And I think that's something we need to figure out. I think it could. There's some sort of existence proofs with polygon, POS and BSc, though they've not had been without their own problems. Right. So, yeah, so I think we're going to be looking at that as well. How can we scale up the existing implementations? We're working on op ref rust implementation that will is looking at, it's based on the Aragon data model and can provide an archival node with under two terabytes, for example.
00:49:45.090 - 00:50:13.672, Speaker A: So these things you can hack on just the details and improve scalability in that way. Right. That solves kind of, again, the syncing problem problem and the state management problem, things like that. So, yeah, all these kinds of things are important. We have to figure out how to best spend our time though, right? Yeah, but that's what I like about the super chain stuff. I think as we get more super chains participating in it, it's a broader community contributing to the stack and working all together on all these problems.
00:50:13.816 - 00:50:57.466, Speaker B: Yeah, it is fascinating and I, I'm super interested just kind of how it plays out. I think very much, I would say just an advocate for getting more people on chain. And I think it really, I was very young in the early Internet days, but if I could imagine it, I would really feel like this, because when I look at, say, dune analytics and see really how limited applications are on chain, I just really want the hands, this technology in the hands of hundreds of millions of people. And I'm very happy that we're just exploring all the different trade offs to ultimately get there.
00:50:57.538 - 00:51:17.620, Speaker A: Yeah, it's interesting you mentioned the early Internet days, because I was around then and there was a lot of skepticism from some prominent academics that this is never going to scale. Obviously they were very wrong. Right. And you hear the same skepticism about blockchains. Oh, it'll never scale. It's a slow database. Right.
00:51:17.620 - 00:51:35.340, Speaker A: People are really good at figuring out solutions to scaling. So I do think this is a harder problem than scaling the Internet was. But I also think we have much better tools and much more sophisticated scientific knowledge than we did back then. So I'm pretty confident we'll get there.
00:51:35.730 - 00:52:03.858, Speaker B: I think perhaps another debate within the L2 ecosystem, and maybe not as much so, but kind of the difference between optimistic roll ups and succinct rollups with validity proofs. How are you kind of thinking about them? I know optimistic is choosing the fraud proofs, but longer term, how are you guys kind of thinking about it and evaluating different tradeoffs?
00:52:03.874 - 00:52:44.262, Speaker A: Yeah, actually it's not true that optimism is choosing. Optimism is developing a fraud proof called canon. But again, this is this modular architecture that I had mentioned. They've also funded risk zero to develop a ZK prover, and they're taking a very interesting different approach than some of the other ZK chains in the space where they're developing a prover. I think they call it a level zero prover. I forget what. But basically, instead of doing a Zke VMdez, they've created a ZK VM that can prove execution traces over RISC instruction set.
00:52:44.262 - 00:53:08.440, Speaker A: So when you can do that, as long as you can compile your client down to a RISC instruction set, you can prove any computation that that client can do. And this is, by the way, one of the reasons we're doing op ref, because we want to be able to generate very as concise as possible execution traces in a bare metal language like Rust will allow you to do that more effectively.
00:53:08.560 - 00:53:11.376, Speaker B: It seems like Rust is becoming more and more popular.
00:53:11.528 - 00:53:47.208, Speaker A: Yeah, I mean, look, go is a fantastic language, and it's my go to language for most of my development. But when you're doing bare metal work, the previous choices were c and c. Trust me, I've programmed in c a lot. I actually kind of like it. But it has this fatal flaw of memory errors. Right. And Rust is figure out a way to provide that bare layer style programming without that enormous what has led to the vast majority of security problems in today's software.
00:53:47.208 - 00:54:06.920, Speaker A: So, yeah, I've been pretty impressed. I've only been programming it, by the way, since Georgios from paradigm basically set up a meeting with us and said, guys, we're going to do this. Op wrath, who's in? I'm not sure I have time. It's like, come on, you got to do it. I don't really know Rust. I wanted to learn it. It's like, it's not that bad.
00:54:06.920 - 00:54:11.636, Speaker A: So I got involved in that. Yeah, now I'm really liking it.
00:54:11.788 - 00:55:12.058, Speaker B: Fascinating. No, it's interesting. Just watching the continued development, I think maybe taking a step back. We've talked about a lot, I would say, really about one, starting with coinbases thought process around choosing the ethereum stack and the ethereum virtual machine, talking about scaling execution with sharding architecture of different L2s, talking about data availability with 4844, and then ultimately ding charting. As you scale the throughput of the base layer on Ethereum, you can also scale execution or have kind of more roll ups to take advantage of the additional throughput. Talked about decentralized sequencers versus centralized sequencers, touched upon fraud versus succinct proofs. We've talked about quite a bit.
00:55:12.058 - 00:55:29.646, Speaker B: Is there anything, I would say, maybe that you feel like the industry needs to focus more on, or is a core problem, or if you could have a team of engineers focus on a specific area to push the industry forward, is there anything that you would have them uniquely focus on?
00:55:29.718 - 00:56:17.936, Speaker A: Yeah, I've been more and more bullish about ZK tech snarks in particular, and not just for rollups. Yes, they're very important for rollups, but it's still a fairly poorly understood technology. And I do worry about rollups that are secured purely by ZK validity proofs, by the way. So this relies on a property called soundness, which means you shouldn't be able to prove anything that's not true. Right. It's really easy to get that wrong. Anyone who's taken like an initial introductory course to creating these circuits in circum, for example, always has these bugs where, yes, it seems like it's right, but there's an input that you didn't think of and it passes the constraints and it goes right through.
00:56:17.936 - 00:57:16.156, Speaker A: And this is a fatal flaw in validity proofs, right? If you can post a a proof that establishes the validity of invalid state route, well, you just drain the bridge. I think there's a lot of work to be done in securing and really proving correctness, I think, of these circuits behind ZK rollups, but beyond that, off chain computation, you can't do everything on a blockchain. ZKML is the big buzzword now. I think rightfully so. Machine learning models are so powerful and can do so much, you don't want to execute them in a smart contract, especially when you have a billion parameters, right? Imagine the amount of gas that's going to take. But you have a few models that you've sort of certified offline and you've committed to and posted those on chain. You can do the computation offline, generate validity proof for that computation, post it along with the output, and then chain can use it with a very low cost.
00:57:16.156 - 00:57:46.310, Speaker A: So that I think is going to be, that's going to evolve in all kinds of crazy directions. Beyond that, I think ZK tech will allow us to bootstrap blockchains much more quickly as well. Again, this comes back down to the syncing time and state management. And do I really have to download everything or can I download a small proof that says this has been computed properly? There's just so much stuff there. I wish I understood it better. I'm really engrossing myself right now into how all this stuff works. I think I'm starting to get it.
00:57:46.310 - 00:57:49.474, Speaker A: But yeah, it's involved.
00:57:49.602 - 00:57:51.270, Speaker B: It's a deep rabbit hole for sure.
00:57:51.610 - 00:58:01.958, Speaker A: But that worries me because I think there's so few people, people that understand it. When more people start understanding it, I think that's when you're going to see some of the flaws, I think that maybe had been swept under the rug a little bit.
00:58:02.014 - 00:58:24.502, Speaker B: So, yeah, from a technical standpoint, do you feel like the hardest part about scaling blockchains is like execution? Or are you worried more about data availability or the storage that comes along with it, or even consensus algorithms? I know at Stanford a lot of the talks is around just the consensus and the different design of consensus.
00:58:24.566 - 00:58:50.994, Speaker A: Yeah. One of the themes of Vitalik's talk today is consensus is not related to scalability. He says there's two. I think he pointed out that there's two aspects of scalability that often get conflated. One is TPS and one is latency, time to finality and things like that. So consensus is about that latter one. Consensus is about time to finality for the most part, because we have blocks.
00:58:50.994 - 00:59:08.260, Speaker A: Even if you have a low, slow consensus algorithm, you can jam a bunch of crap into a block and have extremely high tps. You can get highly scalable blockchains with slow consensus algorithms. So I think I may have forgotten the question, but.
00:59:10.360 - 00:59:18.112, Speaker B: Consensus is definitely important. Data availability is important, execution is important, and then ultimately we have to store that data.
00:59:18.216 - 00:59:19.728, Speaker A: Yeah. Which is the most important?
00:59:19.784 - 00:59:20.568, Speaker B: Which one is the.
00:59:20.664 - 00:59:48.990, Speaker A: They're all important. Again, consensus, if you want to have low latency, is definitely important. Horizontal scalability of consensus is important as well. A lot of these BFT algorithms don't actually scale that well with lots of validators. There's some innovations there, and with avalanche consensus being probabilistic that are very interesting. Randomness in general tends to make things better. So I'm intrigued by that.
00:59:48.990 - 01:00:30.342, Speaker A: It's a little tougher to reason about, but it seems like it has some real promise. So I think there's some advances being made there that could potentially be useful if we really want to continue to allow huge validator sets with low time to finality. But yeah, I think it's all a problem. I think data availability though is the biggest one. It's easy to scale execution, you just create more blockchains. But if you really want to do it with that security guarantee of the l one, that data availability problem becomes the real hard part. Yeah, and we've never seen a blockchain that has done this kind of selective gossip where you only have to store parts of it.
01:00:30.342 - 01:00:49.300, Speaker A: So I think there's going to be a lot of p two p layer changes that we need to, that we're going to be fighting with for a bit before we get all that stuff to work. But yeah, I think that is, that is, and I think you've heard that in the, on Twitter as well, a lot of talk about data availability. I mean, it's just a really hard problem to crack.
01:00:49.420 - 01:01:08.748, Speaker B: Yeah. And do you feel like the concern with cranking up just data availability is how we ultimately store it, or is it more around the requirements if you do crank up the data availability committee on the node side for how much bandwidth they need?
01:01:08.844 - 01:01:22.962, Speaker A: Yeah, exactly. It's bandwidth. Right. Not storage is cheap. Right. But bandwidth is the hard part and that's why I mentioned the p two p layer and being able to get the data to where it needs to go in a way that it's not being broadcasted to everyone. Right.
01:01:22.962 - 01:01:26.538, Speaker A: So that is definitely a challenge.
01:01:26.634 - 01:02:24.956, Speaker B: Yeah, it is fascinating, I guess, in terms of maybe coming back to wrapping up the podcast, we've been talking for approximately an hour. I feel like I could nerd out for another hour, but kind of coming to an end. I think. Obviously you and your team have put a tremendous amount of technical work into not only scaling base, but I would say the broader industry. And with that comes more novel applications and really unlocking different things that engineers can build. And I think that has only been reiterated by the success that you guys have already gotten. I guess my question being what things are you uniquely excited for to see built on base and what type of applications would you like to see built in the crypto industry?
01:02:25.108 - 01:02:53.080, Speaker A: Yeah, I mean, I have to say I love Defi, I love finance, I love the idea of creating a new financial system. That's the obvious thing, though. Stablecoin is already the killer app of blockchains. I mean, having access to stable currencies anywhere in the world. People in the US don't appreciate that, unfortunately. But I think it's a really big deal. But we need to broaden beyond that to really, I think, achieve the interest of the masses.
01:02:53.080 - 01:03:32.646, Speaker A: And so that's where I'm fascinated by friendtech and Warpcast and some of these social applications. But I don't know, I'm just kind of eager to see how it develops. I'm not a product guy, you shouldn't be asking me what are the next killer products that are going to be developed on it, but I just think the nature of blockchains and the fact that they do have this permissionless, composable model that anyone in the world can just tap whatever contract you put on there to achieve amazing things, magical stuff should happen if we continue providing the right tools and the scalability and bringing the users to them.
01:03:32.828 - 01:03:56.634, Speaker B: Yeah. The composability to me was always kind of the magic sauce of blockchain architecture, really breaking down the siloed walls that our old legacy systems really had. And then you can kind of natively interact with things all in a kind of single ecosystem. Being able to touch things without those silos was really amazing innovation.
01:03:56.682 - 01:04:24.450, Speaker A: Yeah. And I think that's one of the reasons I got really interested in Defi, because it really exemplified it. Right. We create an amm, someone creates a lending pool and flash loans come around, and then you can just sort of, these things get chained together in just all kinds of crazy, wacky ways. I mean, sometimes with detrimental effects, but the fact that this can be done in a way that, without having to ask anyone. Right. You just build it according to the rules of the contract and the protocol, and magic happens.
01:04:24.500 - 01:04:40.890, Speaker B: So it is amazing. Yeah. Well, maybe we can just wrap it up there. Maybe before is anything that you feel like we should kind of touch upon that we didn't talk about or go any deeper on any technical side.
01:04:41.230 - 01:04:48.290, Speaker A: We've covered a lot of ground. Nothing, nothing immediately comes to mind. So I'm sure probably when we're done, I'll be like, damn, I should have talked about that.
01:04:49.030 - 01:05:22.784, Speaker B: We'll get some follow up questions on Twitter, but I'm sure. But Roberto, thank you so much for coming on the podcast. Really appreciate you taking the time from Stanford blockchain to speak with me and also just share more with the community. I think everybody is excited about what you and the base team are building, and really, I'm just truly appreciative of you sharing your thoughts and helping the industry grow, not only with base, but even with Coinbase and what you guys are pushing forward to expand the industry. So thank you.
01:05:22.832 - 01:05:31.804, Speaker A: Absolutely. That's what it's all about in the industry, growing it for everybody. But thanks so much for having me. Glad I had this opportunity as well. It's been a lot of fun.
01:05:31.892 - 01:05:32.740, Speaker B: Appreciate it. Thank you.
