00:00:00.120 - 00:00:11.766, Speaker A: From Celestia's perspective, like one of the most important parts of a blockchain, one of the reasons the whole point of a blockchain is that end users can efficiently verify the state of the chain.
00:00:11.838 - 00:00:22.534, Speaker B: How you personally or the celestial ecosystem views decentralization, because I think that's one thing that's hard in the industry for everybody to agree upon.
00:00:22.622 - 00:00:41.410, Speaker A: I think there's two main areas of decentralization on layer one itself. The first area is decentralizing block production, making sure there's a diverse range of block producers. And the reason why you need that is for sensorship resistance. The second part of decentralization I would arguably argue, potentially even more important, which is this idea of decentralizing block verification.
00:00:41.490 - 00:01:21.414, Speaker B: I really do think you guys have pushed the space and you specifically have pushed the space forward on light clients and really opened the eyes of the industry, I think, to what are possible with them. Today I'm joined by Mustafa, one of the co founders of Celestia. They're truly pioneers in the space on light clients. Mustafa has put out quite a few papers, deep technical papers about blockchain and the entire industry, and they're building really one of the high throughput, kind of modular tech stacks. And so I think this is going to be a fabulous conversation and really appreciate you joining me on the podcast, Mustafa.
00:01:21.582 - 00:01:23.850, Speaker A: Yeah, thanks for having me here. I'm looking forward to it.
00:01:24.470 - 00:01:42.630, Speaker B: Perfect, perfect. Well, I'm excited to kick it off and maybe we can just jump right into things on a high level. I mean, could you just kind of explain a little bit about celestias vision, kind of what is the end goal and how you see kind of blockchains kind of ending up?
00:01:43.930 - 00:02:38.040, Speaker A: Sure. So the kind of general idea of celestia is to kind of really take a look at blockchain architecture and to kind of strip back the core components of a blockchain and to basically create a very simple layer, one that only does the core functions that layer one needs to do for other people to build on top of it. Celestia was originally called lazy Ledger and the reason for that was because it's basically a very lazy layer one. It only does the core things that layer one needs to do, and those are consensus and data availability. Celestial doesn't have any smart contract platform. It doesn't have any execution environment for users to build applications on top. Instead, developers are expected to deploy their own execution environments on top of celestia in the form of roll up chains.
00:02:38.040 - 00:02:56.460, Speaker A: And so Celestia is basically a very simple layer one that's optimized to data availability and consensus to allow roll up chain developers to build scalable roll ups on top of a scalable DA. And that's last year.
00:02:57.200 - 00:03:26.630, Speaker B: Perfect. At like what point in time did you realize like kind of separating these stacks was the appropriate way to go? And for me personally, it also took me a little bit of time to learn like how data availability work. You've put out a lot of great papers on this. Could you explain, I mean, maybe to start out with like what data availability is, why it kind of needs to be increasing, and then how over time your views change on kind of the modular stack?
00:03:28.410 - 00:04:43.580, Speaker A: Yeah, sure. So at first it's not really obvious why blockchains needed availability, but I guess I can take a short of explaining why. So you can imagine like take bitcoin for example, if you imagine a version of bitcoin, let's take bitcoin and let's say what is the most simple version of bitcoin we can create in the sense like how can we simplify the bitcoin layer one as much as possible and push as many as much tasks as possible to the end user or to the end node rather than the validators doing all those tasks. What you get is you can create a version of bitcoin where you can actually be allowed to publish invalid transactions onto the chain. So like imagine a better bitcoin where like you can double spend, and if there's two transactions that are double spending the same coins, they're actually published on the chain. So you might think, well, how can that be secure? Well, it's completely secure if you have an end user protocol rule where the end users simply ignore the those invalid transactions. And it turns out that in order to have double spend resistance, all you need is two things.
00:04:43.580 - 00:05:27.890, Speaker A: You need ordering. You need to know what the order the transactions were published in so that you know which transactions came first. And secondly, you need to know the complete set of possible transactions that spent a specific specific coin or a specific balance from an account. Because in order to know which transaction came first, you need to know what all the transactions are. And knowing what all those transactions are is basically data availability. You need to make sure that the transactions were published in the first place. And so that's kind of like a very basic conceptual reason why data availability is important in theory.
00:05:27.890 - 00:06:29.326, Speaker A: But in practice, if you look at modern scaling solutions like rollups, they need data availability as well, because fraud proofs need to prove fraud, and ZK roll ups need data availability. So that users can recompute the state of the chain. And so because data availability is kind of like a central component to these modernization scaling solutions, that's why we've kind of decided to build a scalable DA layer. But interestingly, the idea of lazy ledger came about before optimistic roll ups. Originally the kind of like application, the application model in Celestia or laser ledger wasn't based on roll ups. It was just based on kind of like what I would describe as like a pessimistic roll up that didn't even have fraud or ZK proofs. But then when rollups came around and roll ups started becoming more popular, everything clicks into place.
00:06:29.326 - 00:06:42.970, Speaker A: And this idea of a modular stack kind of really made a lot of sense, where you can have data availability layer at the bottom of the stack and then you can build an execution environment on top of that data availability layer using a roll up.
00:06:44.270 - 00:07:49.180, Speaker B: That makes sense. So maybe just to summarize, for the non technical audience, I mean, ultimately the end state, as you observed, and even before fraud proofs came online or before prod proofs came online, you were kind of researching this and how to scale the data availability layer. So the base layer, because whether it's a zero knowledge roll up or a fraud proof roll up, both of those ultimately have to settle back down to the layer one and consume block space. You were thinking about how to scale the base layer blockchain with more data to ultimately process more transactions. And then how did you start realizing at the end of the day, the separation of execution was really the path forward and in your point of view, where the space needed to go.
00:07:50.120 - 00:08:34.800, Speaker A: Yeah. So I think the way I kind of came to realize that is by kind of looking at the evolution of the blockchain stack and comparing it also to how web two evolved. So, like, if you look at before Ethereum, we had bitcoin and we also had other chains like Namecoin or Litecoin. And it was basically like you had to, like bitcoin was a blockchain for only one application, which is like the cryptocurrency application or like sound money. But at the time, if you wanted to create another application, you had to create your own chain. And, you know, like Ethereum didn't exist. And the problem that Ethereum solved is a problem that you had to create your own chain every time you want to deploy a new application.
00:08:34.800 - 00:09:40.560, Speaker A: And Ethereum solved that by having introducing this world computer model where every chain can share or every application can share the same chain by having a general purpose smart contract platform. But then, obviously, that has scalability issues. And the reason for that is because it just doesn't scale if you want to run the same application on the same computer, a world computer is inherently not scalable. And then that's where systems like Cosmos and Polkadot came into play. So, Cosmos, for example, the Cosmos vision was like, instead of having a shared smart contract environment where everyone deploys their own contract, why don't we make it very easy for people to create their own layer one chains? And by providing this cosmos SDK and tenement software, people can create their own chains. And instead of having to share the same chain, you can create your own chain with its own resources, and those chains can interoperate using IBC. So Celestia is really the marriage of those two visions.
00:09:40.560 - 00:10:42.870, Speaker A: Whereas ethereum, it has shared security, but it doesn't necessarily have scale, because you have to share the same execution environment. And whereas cosmos had scale in the sense that you can create your own layer one chain with its own resources, but it didn't have shared security. And the roll ups really kind of, like, kind of, like, married the two visions in some way, because you can now have roll up chains as application specific chains, but they can now share security. And the idea of Celestia was to. The original idea of Celestia was like, what if we kind of, like, took the vision of allowing anyone to create their own chain, but gave those chains shared security by having these lots of application specific chains? And what if we created a layer one chain that was solely optimized for only one function, which is data availability and shared security, which, at the time, no one was building.
00:10:43.770 - 00:11:56.000, Speaker B: Yeah, I think Celestia has really been a pioneer in pushing the space forward on the data availability front and, like, clients, very head of the game in both regards. I think so one thing you mentioned was the shared security and how important that is. And I think, as you mentioned, with the cosmos ecosystem, I think the upper bound, the Cosmos hub, is kind of limited to, like, I think, 175 or almost 200 full nodes, and then each Cosmos zone, or even, like, in the avalanche kind of subnets, they kind of have to spin up their kind of own security, which definitely can be a drawback. And that shared security model really makes it easier to spin up new execution environments, as you say. And maybe, like, I would also, I really loved your talk with Anatolia at the Celestia summit, kind of talking about, ultimately Celestia versus Lana and kind of the different kind of nuances there. And if I maybe summarize and please correct me if I'm wrong. A lot of it kind of just boils down to the cost of hardware per nodes.
00:11:56.000 - 00:12:13.660, Speaker B: Could you maybe talk a little bit more on why you don't think, or why you do not believe a single layer one can scale kind of on a monolithic chain? Is it purely in your point of view, the data availability, or is it the execution environment itself?
00:12:14.920 - 00:13:38.310, Speaker A: I mean, there's several elements to it. So from my perspective and from celestial's perspective, like one of the most important parts of a blockchain, one of the reasons the whole point of a blockchain is that end users can efficiently verify the state of the chain. Like the fact that, for example, what makes bitcoin interesting is not just that you have a decentralized set of miners and block producers, it's also that people can run full nodes so that they don't have to trust the miners, to be honest, to not steal their money or not to violate the monetary policy of the chain. The only thing they have to trust the miners for is not for safety, but for liveness and censorship resistance. And that's a very important part of the kind of like, blockchain security model. But the kind of like, thinking behind Solana, or like one of the, one of the problem, one of the current problems of Solana is that there's extremely high full node resource requirements for end users to verify the chain. And the decentralization story of Solana very much revolves around this idea of having a decentralized set of values, which is good, but that's a very different threat model to bitcoin and ethereum, where you don't even need to trust the validators for safety in the first place.
00:13:38.310 - 00:14:08.590, Speaker A: With Celestia, one of the things that we really trying to optimize is to make sure that users are actually first class citizens of the network by having light nodes that are trust minimized. That's one of the things that data availability sampling enables. It enables users to verify the data availability of the chain without having to have the same resource requirements as a full node that's downloaded the entire chain.
00:14:09.570 - 00:14:40.910, Speaker B: Yeah, the trust minimization with the lite clients. Again, I think you guys were much further ahead here in terms of other ecosystems, including Solana. In terms of, I mean, just going back to the specific question, is it the execution environment or is it the data availability? Do you think that the monolithic chains are just going to have a harder time scaling long term, or is it just that hardware costs just become increasingly high?
00:14:42.370 - 00:15:29.580, Speaker A: So I think there's several elements to it. So the question to me is like, well, in theory you could have like in theory you could have like a monolithic layer, one chain with like fraud proofs, for example. And that could enable trust, minimize, like clients. But one of the issues there is that you will need constantly more and more expensive nodes to generate those fraud proofs. If you want to participate in generating these fraud proofs, you will need to run a full node to generate these fraud proofs. And the cost of running a full node becomes more and more expensive. That's one of the scaling challenges of having everything on a single chain.
00:15:29.580 - 00:16:12.980, Speaker A: Whereas like if you shard the state into multiple chains, then you're basically, you're allowing like you're saying that full nodes can, you can be a full node of only a specific shard and then you need less resource requirements to participate in the security of the network. So that's inherently more scalable. But roll ups are basically like sharding, except that anyone can kind of, kind of create a shard. Like if you look at the history of rollups, it kind of like went on the back off. And from the Ethereum perspective, for example, Ethereum dropped execution sharding in favor of roll ups because they basically achieved the same, similar, a similar thing to sharding, except in a much more simple way.
00:16:13.920 - 00:17:03.330, Speaker B: Yeah, yeah, no, it does make a lot of sense. I think maybe I always say like, I think kind of in a comical sense, a lot of these kind of nuances ultimately kind of boil down to like full nodes and like the cost to run full nodes. And I definitely understand Celestia's perspective of making sure that everybody is treated equally, maybe diving a little bit deeper into like how you personally or the Celestia ecosystem views decentralization. Because I think that's one thing that's a hard in the industry for everybody to agree upon. And so I always love to ask guests, like, what is your specific view on decentralization of slesia itself between full nodes, like clients? How do you view it all?
00:17:04.350 - 00:18:09.940, Speaker A: Yeah, I think there's several elements to centralization. I think if you're talking about purely from a sense of a layer one chain, like purely layer one, and there's other layers we can talk about, like layer zero, which is the social consensus layer. But let's talk about layer one for now. I think there's two main areas of decentralization on layer one itself. The first area is decentralizing block production, like making sure there's a diverse range of block producers and the reason why you need that is for sensor persistence. And in general, from what we've seen historically, there's only so far you can go no chain. As far as I can tell, no major chain, as far as I can tell, has had Nakamoto coefficient of more than 50, which means you don't need more than 50 validators to do a 51% attack on the chain or two thirds majority attack on the chain.
00:18:09.940 - 00:19:55.340, Speaker A: So even though chains like Ethereum or even Solana have tried to support this idea of having hundreds or thousands of values, in practice the power distributes to a few values. And you can help get around that a little bit by enabling a very decentralized token distribution to make sure there's a wide range of stakers that can delegate to different values. But that's where we go to the second part of decentralization, which is also, I would argue, potentially even more important, which is this idea of decentralizing block verification. And what do we mean by decentralizing block verification? Is this idea that even if the validators are malicious, they should not be able to do any safety violations? And by safety violations, I mean they should not be able to add invalid transactions to the chain, like steal people's money or print money out of thin air or anything like that. With block verification, ideally end users, and that might include like exchanges or merchants or businesses, or even like normal people running a wallet, ideally should be able to have some safety assurances about the state of the chain and verify that the validators have not added any invalid transactions to the chain. And to achieve, to achieve block verification, you need trust minimized, like clients. Or you need to make sure that block size is not so high that it's too expensive to run a full node.
00:19:56.540 - 00:19:57.320, Speaker B: Yeah.
00:19:58.220 - 00:20:17.160, Speaker A: And then finally, I would say there's also layer zero decentralization, which is this idea of having a decentralized social consensus layer. So it's like no one can arbitrarily add unfavorable changes to the protocol rules, for example.
00:20:19.500 - 00:21:01.294, Speaker B: No, each of those make a lot of sense. Yeah, I do think it's interesting. I mean, all change, as you mentioned, or a majority of them still have relatively small Nakamoto coefficients for that, like one third stake weight to start censoring transactions. And I feel like that's almost a social problem, as well as how do you get validators to also move their stake to prevent some of that so light clients. Yeah, like clients are super interesting as well and definitely want to dive deeper into those. A little bit of down the podcast. But I think Celestia is unique in its approach.
00:21:01.294 - 00:21:28.900, Speaker B: I mean, you can, in my point of view, and again, correct me if I'm wrong, can kind of like put it into two buckets for like decentralization and security, one being like the core base layer of Celestia itself, and kind of measuring decentralization from Celestia and the data availability layer. And then another component would be the execution environments or the L2s building on top of celestia. Could you maybe dive a little bit deeper into each of those and how you think about it?
00:21:29.880 - 00:21:35.060, Speaker A: You mean specifically the decentralization of each of those, or the scalability of them?
00:21:35.440 - 00:21:44.712, Speaker B: The decentralization. I want to get into scale too, but right now just kind of like articulating the decentralization aspect because I do think it is important.
00:21:44.776 - 00:22:43.940, Speaker A: Yeah. So I think, as I mentioned before, the question is, what are we trying to achieve of decentralization? Because like decentralization is not a goal in itself. It's like a, it's like a tool to achieve a goal. Like what is that goal? That goal, in my view, is number one, censorship resistance, and number two, resistance against safety violations from the validator set or from the executors of a specific chain. So from the perspective of the data availability layer, what that looks like is going back to the two points I mentioned earlier, block production decentralization and block verification decentralization. From the perspective of the data availability layer, you need block production decentralization to make sure that the DA layer is censorship resistance. So it's like not censoring specific roller blocks or censoring specific transactions.
00:22:44.240 - 00:22:46.036, Speaker B: And these are full nodes, correct?
00:22:46.208 - 00:22:46.960, Speaker A: Sorry.
00:22:47.420 - 00:22:48.920, Speaker B: And these are full nodes.
00:22:49.420 - 00:23:43.326, Speaker A: These are like consensus participating full nodes. So these are like validators that you need. You need a distributed set of validators. And ultimately to do that, you need a distributed set of staking participants or like a distributed set of token holders that can delegate their stake. But it's a question like the goal in that, in my opinion, is not to necessarily make it so that you have as many validators as possible, because there's a trade off between having too many validators as part of the BFD protocol and having a slow BFD protocol with slow finality. So the question is like, what is like the, what is a good threshold or minimum? What is a good motive coefficient? You have to achieve your base layer of sensory resistance. Like that's the first kind of element of decentralization.
00:23:43.326 - 00:24:41.216, Speaker A: And as you know, like you use ten limit consensus, which you usually limit to 100 or 200 values. And then secondly, the other important element of decentralization is, as I mentioned, is resistance against safety violations. And from the perspective of the data adverse layer, a safety violation is if the validators try to withhold data or they try to commit to a block without publishing the data behind that block. And to kind of like prevent that, we make it, we focus on trust minimized light nodes that use data availability sampling to ensure that it's very cheap for end users to, to verify that safety violations have not occurred on the chain. So that's what it looks like from the DA layer, from the execution layer. Sorry, go ahead.
00:24:41.248 - 00:25:14.120, Speaker B: Maybe just touching on the DA layer. Specifically, you mentioned like once you have like a sufficient or some threshold of full nodes on the base layer, that will probably be sufficient. What do you think? I mean, it's interesting now, just like kind of, there's so many people with so many different points of view. What do you think is like a good number to get to? Some people think it's like ten, some people think it's 100, some people think it's 1000 or 10,000. How do you view it?
00:25:14.740 - 00:25:58.980, Speaker A: I mean, it's really hard to say. I don't know if you have enough data points about that because blockchains are only about like ten years old. But you know, it's really hard to say. But as I said before, no chain has, no major chain has had like a nakromatic coefficient of more like more than 50 or so. But even then, kind of like censorship attacks have been extremely rare, even with chains with low, low nakrometric coefficients. Like so, like for example, bitcoin at certain points has had an extremely low computer coefficient where you only need to crop like four or five binding pools to censor transactions. But even then, censorship hasn't occurred.
00:25:58.980 - 00:27:02.480, Speaker A: But I'm not going to say that four or five is good enough because we simply don't have enough data points. But I would say on the order of magnitude. It's also not just about the number of ballots, it's also about making sure that kind of like geographically distributed among a sufficient number of jurisdictions and also making sure that there's an underlying decentralized token holder base that can redelegate their stake to other validators if they try to censor. But I would say on the order of tens or hundreds of boundaries seems sufficient to me. Once you get into the range of thousands of, that have two thirds of stake, that seems to me like overkill at this point of time because once you start to get into the range of thousands of validators, then you're really starting to trade off the performance of the BFD protocol with a number of values for potentially very diminishing returns.
00:27:03.340 - 00:27:44.400, Speaker B: Agreed? Yeah, I definitely think full agreeance with you. I think Nakamoto kind of coefficient and that real time censorship resistance is the thing to optimize for. And I think in some cases the industry is over optimized for large number of full nodes at the cost of performance. And I think we both agree we need lots of performance to actually scale the networks. And then I apologize, I cut you off about the execution environments, but how do you view those as well? And then I would love to jump into light clients because I really do think you guys have push this space forward on the light clients front and I want to give a good amount of time to talk about all the work that you've done there.
00:27:45.060 - 00:28:33.790, Speaker A: Yeah, sure. And I should just clarify on the last point when you mentioned full nodes. I'm specifically talking about consensus participating full nodes like what actual values. But in terms of decentralization of the execution environment, we expect the developers to use roll ups to develop execution environments. So the question is like what does roll up decentralization look like? And going back to the two points I mentioned, number one, decentralization of block verification to make sure that there sensitivity resistance. And number two, decentralization of block verification to make sure that there's resistance against safety violations. Like what does that look like on a roll up? So first of all, let's talk about censorship resistance on rollup.
00:28:33.790 - 00:29:00.694, Speaker A: Now the interesting thing about rollup is that you can actually have a sensory persistent roll up, even if that roll up has one block producer or like one sequencer. For a roll up, to achieve the end goal of censorship resistance, you don't necessarily need to have decentralization of block production, even though that helps. And the reason for that is because.
00:29:00.862 - 00:29:05.782, Speaker B: Sorry, sorry sir, I was going to jump in, but you beat me to the point.
00:29:05.926 - 00:30:19.630, Speaker A: Yeah, so the reason for that is because rollups can inherit the consensus and the ordering of the underlying data availability layer that they post their data to. And one of the ways they can do that is like let's assume that a roll up has a single sequencer and that signal sequencer starts censoring transactions. Then what you can implement is a protocol where users can force the sequencer to include transactions by having some kind of on chain inbox on the DA layer that users can submit transactions to that inbox such that the next roll up block is only valid if the sequencer includes the transactions posted to that inbox, and for example, that's what arbitram does. And that's a very interesting and potentially powerful property of rollups. The idea that you don't need this massive set of sequencers, you technically, you can achieve censorship resistance with only one sequencer. That's a significant reduction in the hardware costs to operate a roll up chain. And secondly, the other decentralization property which I mentioned, which is decentralization, decentralization block verification.
00:30:19.630 - 00:30:44.730, Speaker A: There's several ways that roll ups can achieve this. Usually they achieve it through fraud proofs or ZK proofs, like a user can run or a smart contract can run, a like client that verifies Ford or ZK proofs of that roll up instead of having to download every transaction in that roll up and verify it. And usually that's needed for bridges between rollups and l one s or roll ups and roll ups.
00:30:46.190 - 00:31:37.360, Speaker B: Yeah. Makes a lot of sense. So just to recap for less technical people, ultimately, because the L2 settle to kind of a secure layer one, it doesn't necessarily matter as much if there is a single sequencer because they can always kind of hit the eject button and go back to the layer one. And then on the second point of decentralization, with zero knowledge or optimistic, ultimately, or with the light clients, you can kind of subsample a small set of data, which ultimately allows you to do a small fraction of the work compared to the large full nodes on the base layer. Correct?
00:31:37.400 - 00:31:41.952, Speaker A: Yeah. Well, by self sampling, you mean like four Zk proofs, I guess.
00:31:42.136 - 00:31:43.248, Speaker B: Correct. Yeah.
00:31:43.424 - 00:31:44.220, Speaker A: Yep.
00:31:44.840 - 00:32:16.470, Speaker B: Okay, cool. No, makes a lot of sense and appreciate giving lots of clarity there. I always kind of like to start there, but as I said, like, I really do think you guys have pushed the space and you specifically have pushed the space forward on like, clients and really open the eyes of the industry, I think, to what are possible with them. Can you maybe just do a quick deep dive of what are lite clients, why they're so important, and ultimately how they help scaling so much.
00:32:16.930 - 00:33:06.120, Speaker A: Sure. So if anyone's ever used a metamask wallet, which I'm sure most of you have, you probably have heard that, how does metamask interact with the Ethereum network? Your metamask wallet isn't connecting directly to the Ethereum network. Instead, it's using a kind of like trusted third party. And that's usually infuria or alchemy. And that trusted third, that trusted third party, you're relying on them to tell you the correct information about the state of the Ethereum chain, and you're also relying on them to relay your transactions. And to me, that's very antithetical to what web3 should be about. In web3, in my opinion, that users should be first class citizens of the network, because the whole point of web3 is trust minimization.
00:33:06.120 - 00:34:15.872, Speaker A: The whole thing that differentiates web two and web3 is that in web3, you're not interacting with a centralized database. You're interacting with a decentralized network, and you don't have to trust a third party to tell you what the correct state of that decentralized network is. But the problem is that typically, traditionally, in order to interact with the chain and to know what the state of a chain is, to know what your balance is, to know what transactions have occurred, you would typically have to run what's called a full node. And a full node is basically a node that downloads every single block in that chain and has to check every transaction is valid and read every transaction to check that the chain is correct and to tell you to verify information about what the state of that chain is and to get your balances and so on and so forth. But you also have something called a lite client. And a like client is, you know, exists in bitcoin and ethereum. But a traditional, like client is kind of like a full node, except that it doesn't verify every transaction in the chain.
00:34:15.872 - 00:34:59.413, Speaker A: Instead, a lite client only downloads what's called a block header. Like, it only downloads the metadata or a small fraction of the data of every single block in that chain. And instead of verifying that every transaction is correct, instead, it just assumes that every transaction is correct. And the reason why it can make that assumption is because it trusts that the validator set or the miners of that chain are honest. And that's inherently a weaker threat model than a full node, because full nodes don't trust the miners or violators of a chain, to be honest. They verify for themselves. They don't trust.
00:34:59.413 - 00:36:05.890, Speaker A: They verify. But, like, clients or traditional lite clients don't do that. They just assume that the validators are honest. And so, like, if the validators arbitrarily started changing the protocol rules of that chain, like, against everyone's wishes, like, let's say they wanted to print a bunch of more money, or they wanted to change the inflation schedule or something like that, they could do that without, like, clients noticing. And so that's why instead of having traditional, like, clients, we can have what's called trust. Minimized like clients and a trust minimized like client is kind of like halfway between a traditional like client and a full node. A traditional trust minimized like client is better than a traditional like client because it can use technologies like fraud or ZK proofs or data availability sampling to get very high assurances about the correctness of every transaction in that chain without them having to manually download and check every transaction in that chain.
00:36:05.890 - 00:36:15.370, Speaker A: So like trust minimize like clients, they have almost the same level of security guarantees as a full node, but with way less resource requirements.
00:36:17.150 - 00:37:30.840, Speaker B: Yeah, no, it's super interesting. And I mean, again like you and the team were pioneers and kind of creating these like clients. And I think I'm assuming now at least majority of teams that I talked to today are going or at least attempting to implement these trust minimize like clients in some form or fashion. And so it has to feel great to see your research spread across the industry on the so maybe touching a little bit upon, or if I could re articulate it a little bit, these trust minimized light clients have fraud or validity proofs where they're able to get higher assurances than just strictly a light client that only downloads the header information. And because with a fraud or zero knowledge proof, these assurances are much higher than it would just be for the header specifically. But you also mentioned data availability sampling and how the light clients themselves can do data availability sampling, and I think that's an important element as well. Could you talk a little bit about data availability sampling and how that ties into these trust minimized lite clients?
00:37:31.660 - 00:38:47.410, Speaker A: Sure. So having fraud or ZK proofs to verify the state of the chain, that's not sufficient to have a trust minimized like client. You also need to make sure that a light client can get assurances that all data in the chain was actually published without them having to download that data themselves. And the reason why that's important is, as I mentioned, for fraud proofs, no one can generate a fraud proof if the data wasn't published. If there's no evidence, if the evidence for transactions have been published, you can't generate, you can't prove that something bad happened. And for ZK proof, it's needed because, well, for ZK proof, state availability is needed because just because you can prove that using ZK proofs that the next state commitment or the next state of the chain was valid, it might be the case that the sequencer of the next block has actually committed to a valid state, but they haven't actually published or told everyone what the actual transactions are that led to that state. So, like, people will know that the block is correct, but they don't know what's actually inside that block.
00:38:47.410 - 00:39:42.890, Speaker A: And so they don't know what their balances are, and they can't build on that block because they don't know how to build on it. And so that's why data availability is important for fraud or ZK roll ups. But what data availability sampling allows is allows light nodes to verify that all the data in that chain was published without having to download the data themselves to check that it was published. And the way that data availability sampling works is, in a very layman's or nutshell way of explaining it, is that you basically sample random data, or random chunks in that block. And by sampling random tracks from that block after a certain number of samples, you can get an almost 100% guarantee that the entire block was published by only sampling, let's say 1% or 2% of that block.
00:39:45.390 - 00:40:25.420, Speaker B: Yeah, it's super fascinating what data availability sampling ultimately allows you to do, having many of these light clients download a particular subset of the data to have these stronger guarantees, and ultimately, with the end goal being these, like, clients almost being as strong in the security guarantees as the full nodes, it is all fascinating. And how many years or what kind of like, ultimately led you to this innovation that you've studied so much on the trust minimum clients?
00:40:26.060 - 00:41:02.900, Speaker A: Yeah, I think one of the things that made me really interested in lite clients is there was this scaling debate in bitcoin back in 2013 to 2016. This was before ethereum. There was a point in bitcoin's history where for the first time ever, there was a 1 mb block size limit, and that was starting to get full. And this was like during like the bull market, if I. Of like 2013, if I remember correctly. But these blocks were starting to get full and the transaction fees were starting to get really high. Like, they were like $50 a transaction.
00:41:02.900 - 00:42:13.240, Speaker A: And that was causing kind of like this crisis in bitcoin where merchants stopped accepting bitcoin. So, like, the whole narrative around bitcoin at that point was like, it was supposed to be peer to peer cash, but it was like $50 a transaction, and then merchants stopped accepting it because it was too expensive and no one was using it for payments. And so there was this kind of like, divide in the bitcoin community where like a portion of the community wanted to increase the block size limit from 1 something higher, whereas the other portion of the bitcoin community did not want to do that. And they wanted to pursue scaling techniques like lightning network instead. And the reason why, like that, that portion of the bitcoin network did not want to increase the block size image is because they were concerned that if you increase the block size limit, that will increase the costs of running a full node, that will become more and more expensive to run a full node. And if it becomes more and more expensive to run a full node, you will have to trust the miners more and more to not misbehave. Bitcoin has had a very contentious relationship with miners.
00:42:13.240 - 00:43:02.960, Speaker A: The fact that only a few mining pools, you know, controlled a large chunk of the bitcoin hash rate, you know, you really didn't want trust, you didn't want to trust the miners for safety. And so to me, like, well, I was asking, well, how can we increase the block size without increasing the resource requirements for end users to verify the chain? And interestingly, in the bitcoin paper itself, like Satoshi mentioned, I. A very early ideas, a very early idea, like a very early version of fraud proofs that he called alerts. Alerts, was this idea like a full node could alert other nodes or other light clients that a certain block was invalid and force them to have to redownload that block to check that it was correct?
00:43:03.380 - 00:43:03.836, Speaker B: Interesting.
00:43:03.868 - 00:44:04.680, Speaker A: But that was a very primitive version of Ford proofs that wasn't scalable or DOS resistance. And so then there was some further innovation on this idea of compact fraud proofs, which are like fraud proofs that are small, that you can send them to a light node and they can easily verify them even if they have high resource requirements. But at the time, compact fraud proofs were a thing that the bitcoin community was very interested in. But I, there was this data availability problem, because compact fault proofs could only work if you can have data availability proofs for, like, clients. And that was a much harder problem to solve than compact fault proofs. And that's kind of like what led me down this rabbit hole of trying to figure out how to make trust minimized light nodes, and made me realize that data availability is kind of like the core, one of the core like functions of bitcoin, of a blockchain.
00:44:05.340 - 00:44:41.914, Speaker B: Yeah, fascinating. That's a cool story. Yeah. It took me much, probably longer to realize how important data availability was, or just kind of raw throughput of these nodes and bandwidth constraints than you. And so I'm definitely envious of your earlier learnings, maybe on that topic. Specifically, I know you and I have gone back and forth a little bit on Twitter on throughput. And John, I think comically said Celestia's initial throughput was going to be like 1.4
00:44:41.914 - 00:45:24.830, Speaker B: megabytes. But as you mentioned, it's just a parameter that's tunable. One thing that I've really tried to do, kind of as just like a more apples to apples comparison, has been evaluating these different blockchains by their data availability layers just to have some common ground. I know Celestia is still working towards Mainnet and it is primarily ties, but I guess maybe where do you believe you're going to start? And then ultimately where do you think it's going to end up? Or is it just a number that's going to be ever increasing as kind of demand increases as well?
00:45:25.690 - 00:45:57.740, Speaker A: Sure. So, yeah, as I mentioned, it's important to realize there's a difference between throughput and scalability. Like through ports, you can just say throughput is how many transactions per second you can do or like how many data per second you can make available. But scalability is through port divided by the cost for end users to verify that chain. Like, it's very easy in theory to increase throughput by just increasing the block size. Like you can say that bitcoin is extremely scalable. All you have to do is increase the block size.
00:45:57.740 - 00:46:55.950, Speaker A: But you can increase the block size if you increase the requirements, the resource requirements for running a full node. And that's fine, but that's not the same thing as scaling. And that's why we say in Celestia, celestia scales with number of like clients, because the more like clients you have, the more like clients are sampling blocks, the bigger the blocks you can securely have without violating this property of decentralizing block verification. So as I mentioned on Twitter, ultimately the block size is a governance parameter, and the community has to kind of like make a decision about what is the maximum block size we can have without having a sufficient number of light clients. And after that, the community will probably only want to increase the block size after knowing that there's a sufficient number of light nodes that are sampling the chain, if that makes sense.
00:46:56.450 - 00:47:24.960, Speaker B: Yeah, no, it definitely does. I guess on the base layer, does that kind of grow linearly on the amount of bandwidth that is needed for the nodes? And then as it gets higher and higher, you add these trust, minimize light clients. And with more light clients coming online, each of them have to do a smaller fraction of the work, allowing you to scale the base layer even more.
00:47:26.260 - 00:47:39.720, Speaker A: So our goal is light nodes roughly have to do an equal amount of work regardless of how big the block size is, increasing the block size.
00:47:41.460 - 00:47:41.748, Speaker B: Would.
00:47:41.764 - 00:48:32.150, Speaker A: Just basically mean that we need to increase the number of light nodes. But that means each light node will do roughly the same amount of work. But what that does also mean is that to increase the log size, we also have to increase the resource requirements for running a validator node. So let's say if it's 100 megabits and you want to increase the ten x, it needs to be now one gigabit, for example. But that's, that's why it kind of goes back into, into this idea of like block decentralization of block production versus decentralization of block verification. And that's why we place a heavy, a very heavy emphasis on decentralization of block verification. Because even if these validators have very high resource requirements, you don't have to trust them for safety because we have these trust minimized light nodes.
00:48:32.890 - 00:49:00.710, Speaker B: Yeah, no, it definitely makes sense to me, and I mean on more layman's terms, ultimately, the more kind of trust minimized light clients that you add to the network, the more scalability or higher resource requirements you can have at the base layer because you're still, in your words, getting that end user vacation, which is ultimately kind of the kind of main premise of blockchains more holistically.
00:49:01.330 - 00:49:02.270, Speaker A: Exactly.
00:49:02.650 - 00:50:09.870, Speaker B: Yeah, makes sense. It was interesting, I was reading Vitalik's blog posts, I forget what it was called, the endgame. And he was kind of talking about ethereum and slacia and some of these more high throughput chains. And I think the key thing there, I think you're actually, I don't know either one of the co authors, but I think definitely cited on his work on light clients. But how majority of these blockchains will all end up in a similar end state in the sense of the resources on the base layer continue to increase and these trust minimize like clients ultimately allow these end user verification. If that world ultimately ends up happening, what do you think? Like are going to be like the main kind of differentiators of different chains? Is it the execution environments? Is it how they do consensus? I'm curious, or if you don't believe that's like the end goal or end state, please feel free to share. Anything else?
00:50:10.410 - 00:51:17.030, Speaker A: Yeah, I mean, it's a good question. I do think that's the end state and that's why I've mentioned, I've been mentioning why block decentralization of block verification is extremely important because if we do end up in this end state where you have centralized block production. It then becomes extremely important to have decentralized block verification so that we don't have to trust decentralized block producers for safety. They can't arbitrarily change the rules of the chain and so on and so forth. But in terms of what differentiates what will be the differentiator of different chains, I think one of the things that we truly believe is, and one of the reasons why we believe in a modular ecosystem and we have messages like we believe in modularism and not maximalism is because we don't really believe that there's ever going to be a one size fits all solution. If you look at web two, does every website use the same hosting provider? Does every website use the same technology stack? Of course not. Different applications use different technology stacks.
00:51:17.030 - 00:51:50.048, Speaker A: I don't really believe in the idea of having a single world computer or single settlement layer. It sounds nice in theory, but in practice I don't think that's what the market wants. I think in practice there's going to be a range of, there's going to be a stack and different components in that stack. It's going to be a modular stack where you can swap out different components depending on what the application wants to maximize and what kind of trade offs is trying to make. Because ultimately different stacks have different trade offs, definitely.
00:51:50.104 - 00:51:57.224, Speaker B: Do you think that will be like 510 or 110,000? I guess, going back similar to question.
00:51:57.272 - 00:52:01.200, Speaker A: To the nodes, I mean, how many, what? How many chains?
00:52:02.060 - 00:52:29.900, Speaker B: Well, I mean, whether it's like either different layer ones or either different L2s on Celestia, do you end up that being consolidated into kind of relatively few players or kind of few core execution environments that kind of take majority of the market share, or do you kind of envision it being thousands that are doing slightly different things tailored for specific applications?
00:52:30.680 - 00:53:29.570, Speaker A: Well, I mean, I actually see a potential future where there's millions of application specific rollup chains. I think that's a very realistic feature that we're heading. If you look at the Internet today. Yes, we have decentralized webp platforms like Facebook or Twitter. And so it's like for web3, do we believe, like, I think, like for web3, I feel like believing that everyone will use the same few l one s or like use the same few chains and not have these application specific chains. It's kind of like leaving, like everyone will use the same centralized platforms like, you know, Twitter or Facebook. I think in practice you are going to have like, just like in web two, you have like all these independent blogs blog sites and all these independent, you know, shops using Shopify and so on and so forth, I think there'll be more of that, because the whole point of web3 is that it's supposed to be more decentralized.
00:53:29.570 - 00:54:23.648, Speaker A: So I do see a future where there's millions of application specific roll up chains, and many of them will have shared security. For example, in the future. If you wanted to create a DAO or a Dex or a game or anything like that, why would you like, you're probably, there's a strong chance that instead of deploying as a smart contract on a shared layer, one smart contract platform, you could potentially deploy it as a roll up chain. And because one of the goals of Celestia and a modular stack is to make deploying new roll up chains as easy as deploying smart contracts. And if you look at the evolution of the web, that's kind of like where we are today. No one these days uses a shared hosting provider like Bluehost or geocities or dreamhost. Instead, people use virtual machines.
00:54:23.648 - 00:54:45.610, Speaker A: You can spin off a virtual machine in seconds and have your own execution environment. So the web now consists of these millions or even billions of these virtual machines that are all talking to each other. And I see roll ups as being very similar or the same as virtual machines, because roll ups are effectively like virtual blockchains that user use the same online shared security layer.
00:54:46.750 - 00:55:09.610, Speaker B: Do you think ultimately, in that world of vision, are these different execution environments, or you ultimately see again like a couple execution environments, whether that's parallelization fuel SVM, a move virtual machine, or EVM virtual machine, or do you think people will kind of tailor the virtual seams?
00:55:12.910 - 00:55:48.570, Speaker A: I think there will definitely be multiple dominant execution environments. I don't see one specific execution environment succeeding. I think the effects, I think people saying everyone will just use the EVM because that has the biggest developer base. I don't think that will actually play out. If you look at Web two, there's new programming languages every five years, entire, you know, web two stack, the most popular web two stack frameworks are completely different. You know, like Golang didn't exist like a decade or two ago. Rust is now suddenly the new big thing and so on and so forth.
00:55:48.570 - 00:56:27.120, Speaker A: Whereas like 15 years ago was Python, like every five years, there's always new innovations in programming language, programming languages and execution environments, and that it's constantly, constantly an evolving landscape. I think that's one of the benefits of having a modular stack that you're not going to and not coupling a smart contract environment with your layer one, because what's best is constantly evolving. So by having a modular stack, you can easily, very easily iterate on experimenting with new execution environments by deploying new roll ups instead of having to deploy new layer one chains.
00:56:28.310 - 00:57:25.020, Speaker B: Definitely, sure. Security definitely makes that a lot easier. And yeah, definitely much easier to spin up a new virtual machine if something comes out with something super innovative. And it's much easier to adopt there than rewrite your entire tech stack and have to change the virtual machine on the millions of different roll ups. How do you see composability playing out in that state? Are applications ultimately sector specific? You see some very large roll ups and liquidity is moved to that specific roll up, or do you feel like composability is not something super needed? I'm curious what you think how it will play out.
00:57:25.880 - 00:58:10.148, Speaker A: Yeah, I think composability and interoperability are definitely important, but I just don't see. I just think it's very unlikely that we will have a single synchronous chain where every smart contract operates and have synchronous interoperability with every contract. Because we see in practice today, if you look at the web3 landscape today, there isn't a single synchronous chain that everyone uses and has smart contracts that interoperate with each other. Instead, you have this cluster of different ecosystems with their own state machines. You have Ethereum, and then you have polygon, Solana, avalanche, and so on and so forth. And you have bridges between them. And Ethereum has realized that.
00:58:10.148 - 00:59:12.690, Speaker A: And now you have these roll up chains that are effectively their own chains that have bridges between them. And the way that I kind of like, think about bridges is I kind of categorize bridges into kind of trust minimized and trusted. And both types of bridges are okay depending on your use case, by the way, I see the kind of like ecosystem evolving is we have is by having these shared security zones, or what I call clusters. Like, you won't have, you won't have, you will have like the same one big shared security zone. We'll have like multiple shared security zones. And what I mean by that is like, if you look at like, for example, like Ethereum, you can classify Ethereum and all of its roll ups as one shared security zone in the sense, like, all the roll ups on Ethereum can interoperate with each other in a trust minimized way by sharing security. And then if you want to exit Ethereum and go to Polygon, you have to use this trusted bridge, and you have to go across these shared security boundaries.
00:59:12.690 - 00:59:38.580, Speaker A: And I think that's okay. And that's how I see web3 evolving. We will have shared security zones that interoperate with each other and assets will be floating across these shared security boundaries. But it's important to have the shared security zones in the first place because we don't want to end up in a world where if you have millions of chains, each chain is in their own shared security zone because that's not shared security anymore in the first place.
00:59:40.240 - 00:59:45.820, Speaker B: Yeah. And that's kind of like your main issue with Cosmos and kind of the avalanche ecosystem.
00:59:46.640 - 00:59:47.660, Speaker A: Yeah, for sure.
00:59:48.480 - 01:00:12.990, Speaker B: Makes sense. And then in that kind of shared security world it definitely helps with the asynchronous composability. But with synchronous composability, do you think, again some apps will co locate or I'm curious, just nailing you down on some of your thoughts on the composability aspects?
01:00:14.250 - 01:00:44.830, Speaker A: Well, you'll still have synchronous composability within rollups, for example, or across, potentially even across roll ups if those roll ups share the same aggregator. So there's been some interesting discussions recently about role about having synchronous compatibility across roll ups. And that's possible if those roll ups have a shared aggregator. And that shared aggregator could actually be a decentralized aggregator network as well.
01:00:45.650 - 01:00:47.010, Speaker B: Would that be a layer three?
01:00:47.130 - 01:00:47.830, Speaker A: Sorry?
01:00:48.250 - 01:00:49.790, Speaker B: Would that be a layer three?
01:00:50.370 - 01:01:33.740, Speaker A: Well, well, this idea of layering starts to become a bit fuzzy at that point. It's not really clear to layer three or layer 2.5 or 2.1 at that point, but it's like, it's like a shared aggregator network. But yeah, I mean, I think the question is like is it realistic to assume that in the future, is it realistic to build towards the world where every web3 application has synchronous composability? No, in my opinion, no. And like we will, we will have synchronous composability zones, but between those zones you will have asynchronous composability. And that's like, that is to me, in my view, that's clearly what the market wants.
01:01:33.740 - 01:01:46.380, Speaker A: That's the market today. The web3 ecosystem as a whole has asynchronous composability. And within specific chains or specific roll up chains you have synchronous composability.
01:01:47.950 - 01:02:34.304, Speaker B: Yeah. Your vision definitely makes sense. I'm super curious to see ultimately where long term things kind of end up. And I appreciate the different chains trying different things. It's a very deep rabbit hole. I think you've obviously spent a lot of time and thinking about it, and the market is, I mean, right now the bear market is probably my favorite, just because we can kind of do some of these long for conversations and it's less marketing and pr, but lots of furious battles being fought on Twitter at the moment in terms of these different virtual machines. We kind of touched upon it.
01:02:34.304 - 01:02:57.290, Speaker B: Do you see any? I mean, I, in my mind, I kind of think of like two main buckets of virtual machines, one being like the parallel execution and one being the serial threaded. Is that how you think about them as well? And if so, where do you think those ultimately progress long term?
01:02:58.790 - 01:03:34.470, Speaker A: Yeah, I mean, I definitely think like parallel execution environments definitely, and have a lot of advantages over execution environments that don't have parallelization. I think there's just like a, it's just objective, technically speaking, it's like objectively better. And I don't see any advantages of having a non parallelizable execution environment. And that's like, I guess like that's one of the advantages of the EVM. And that's why innovating on new execution environments enabled by modular stack where people can innovate without having to launch any layer one, is very important, do you think?
01:03:34.510 - 01:03:47.450, Speaker B: I mean, that majority of virtual machines kind of going forward or outside of the ethereum virtual machine will be paralyzable long term.
01:03:47.910 - 01:03:58.530, Speaker A: That definitely seems to be where it's heading. All the new execution environments move and sue and fuel are all paralyzed.
01:03:59.190 - 01:04:36.230, Speaker B: Yeah, I definitely agree. Yeah, they definitely make a lot more sense. Definitely applaud Ethereum for the invention of smart contracts and kind of pushing forward from bitcoin. Massive undertaking in that. And I think we've learned a lot since Ethereum and what the Ethereum researchers have ultimately put out and kind of now the next step of virtual machines is definitely parallelization. So no, super interesting, I guess like maybe kind of wrapping up the podcast win Mainnet.
01:04:37.850 - 01:04:44.790, Speaker A: So we're planning, so we released incentivized Testnet and we're very close to Mainnet. It's planned for q two or q three.
01:04:45.570 - 01:04:56.210, Speaker B: Awesome. Wonderful. I know a lot of people have been super excited about it, and I wish you the best success on the testnets.
01:04:56.670 - 01:05:09.010, Speaker A: Thank you. And if anyone wants to run a node, you can go on our docs@sladeshi.org and you can go there and run a trust, minimize that client to see what it's like.
01:05:09.590 - 01:05:43.730, Speaker B: Oh yeah, that's awesome. And maybe last thing, I think now that there is a multiple kind of L2s. There's multiple different layer ones, there's avalanche subnets, there's cosmos zones, all these different ecosystems to build upon. What would your pitch be to the engineers or builders watching this podcast? To build on top of Celestia and l two, or just contribute to the Celestia ecosystem.
01:05:44.670 - 01:06:12.938, Speaker A: So I guess my pitch would be, and my pitch wouldn't be about building on Celestia specifically. It will be building in a modular ecosystem. Celestia is just one stack in that ecosystem. But my pitch would be. Over the past ten years, we've been stuck in this endless loop of new layer ones. Every bull market we have this new cycle of new layer ones. They get filled up and then don't scale or they fail to get traction.
01:06:12.938 - 01:06:50.280, Speaker A: Next cycle we have these new layer ones that promise incremental improvements, and clearly that's not sustainable. Do we really want to live in a world where every cycle we have these new layer ones and then you redeploy these layer ones? I would say that's very restricting for developers. And if you have more freedom, build on a modular stack, build a roll up chain on the modular stack, use whatever execution environment you want. Like build whatever you want instead of having to be restricted to specific layer one ecosystems.
01:06:51.780 - 01:07:40.520, Speaker B: Yeah. I'm personally very much looking forward to all the different kind of engineering point of views that bring in more users and more developers. And so if engineers can experiment and on celestia and different virtual machines and users love it, I'm all for it, but definitely really appreciate your time. Mustafa. I think on Twitter it's easy to go back and forth and throw haymakers, but it's fun to get into the long form, nitty gritty of things. Last thing, besides Celestia itself, are there any specific things that you're excited for for the rest of kind of 2023 that you're looking forward to or you just think are interesting to kind of follow along with?
01:07:41.300 - 01:08:46.340, Speaker A: Yeah, I think one of the things I'm definitely kind of watching or looking out for this year is this emergence of roll up as a service providers. This idea that you can create your own chain, you can upload the code to some roll up as a service provider, and within seconds that roll up as a service provider will automatically deploy as a roll up on some DLA and run the first sequencer for you. And you don't have to trust that service at all because the roll up is trust minimized and inherits censorship resistance from the base layer and has 400 ck proofs. And it's going to be very interesting to see what the world looks like out of that, in a world where deploying a new chain in a decentralized way truly becomes as easy as deploying a new smart contract. So developers will no longer be limited to the kind of like restrictions of smart contract platforms. They can now just deploy a new chain as easy as deploying new smart contract. And that's definitely one of the things that I'm kind of like, looking to see how that evolves.
01:08:46.960 - 01:09:07.940, Speaker B: Definitely. Hopefully lots of innovations to get spurred out of that. So again, thank you so much, Mustafa. I appreciate your time. Appreciate you coming on the podcast. Appreciate you pushing the space forward, appreciate all the research you've done on light clients, and look forward to seeing what you and the Celestia team continue to build. Thanks.
01:09:07.980 - 01:09:08.840, Speaker A: It's been fun.
01:09:09.460 - 01:09:10.420, Speaker B: Awesome. Thank you.
