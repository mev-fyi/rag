00:00:00.560 - 00:00:08.750, Speaker A: I have seen multiple people try parallelization in the EVM past, and so could you explain why from your perspective that hasn't worked?
00:00:08.830 - 00:00:43.980, Speaker B: You know, with any change that looks obvious in retrospect, people are always like, well, why didn't someone just do this before? It's a matter of execution and focus. And I think from what I've seen, and maybe I've missed some other efforts, but a lot of the parallelization efforts are like shorter term experiments. Like, you know, we're going to try this for a weekend and then post about it on Twitter. And we got like a two x beat up. It was great. It needs to be done with a longer term goal in mind, which is to accelerate all parts of the pipeline and deliver much higher performance. And I think that that just requires a focused team.
00:00:46.640 - 00:01:12.180, Speaker A: Super excited for today's podcast. Today I'm joined Keone. He's building Monad that paralyze EVM new layer one. And I'm also joined with Omar. Maybe before we dive a little bit deep into like all the technical stack, Omar, could you give a brief intro? Gonna have Omar do be co hosting the podcast with me as well.
00:01:14.360 - 00:01:41.830, Speaker C: Kyunee, it's very nice to have you, and we're a fan of what you're building. A very brief background. Myself long time ago, applied maths, then turned that somehow into magic Internet money and trading coins on the Internet with cartoon characters, that turned into venture capital. And today I have the distinction of being the head of investments at Meta Labs, the team that brought you Zksync and a lot of other wonderful things.
00:01:43.010 - 00:01:46.150, Speaker B: Awesome. Yeah. Good to be with both of you guys today.
00:01:47.020 - 00:02:05.840, Speaker A: Perfect. Well, really appreciate it. I think this is going to be a super fun podcast. Before diving into specifically what you're building at Monad, could you just also give a brief background about your history, how you ultimately got to building up until the point of Monad?
00:02:06.300 - 00:02:55.250, Speaker B: Yeah, sure thing. Just to give a quick intro, I'm keone. I started my career in 2011 in the high frequency trading space, spent two years at a company called Getgo, and then moved over to jump trading in 2013 and spent eight years there. It was a really great experience. Learned a lot about building really high performance trading systems. I was on a team of about ten people serving as quant and team lead there. So did a lot of machine learning stuff and a lot of just building high performance trading systems to go and compete in major centralized venues in traditional finance such as the CME, Eurox, exchange, other exchanges like that.
00:02:55.250 - 00:04:07.800, Speaker B: And then I spent a little bit of time, like seven or eight months in the crypto division of JMP, mostly working on Solana Defi, along with my co founder James, who was my teammate starting in 2014. We've been working together ever since then. Worked together on the HFT systems for a number of years, and then worked together on Solana Defi. JMP is a really great company. They're very thoughtful about investing in technology at a very early stage, have always been an early mover in investing in different technologies as an automated trader, and then got involved in crypto quite early, I think in 2015 or 16. They started a crypto group there in 2015 or 2016, and have just been involved in the space as traders and researchers and developers for a number of years. For James and me, we joined the Jump crypto team in 2021 and started working on some DeFi projects there.
00:04:07.800 - 00:04:12.476, Speaker B: I think you're muted.
00:04:12.628 - 00:04:58.080, Speaker A: Yep. Amazing. I guess, on that front, I mean, now, I mean, working in high frequency trading, working also in the Solana ecosystem, what were some of, like, the, I mean, big learning lessons there and then ultimately, I mean, I guess, maybe backing up. I mean, were there any specific things that you learned in high frequency trading that you kind of either saw parallels into, like, the blockchain ecosystem after that, I guess, like, what kind of working inside the Solana ecosystem made you more interested in, like, building out the EVM and the EVM tech stack?
00:04:58.640 - 00:05:53.160, Speaker B: Yeah. High frequency trading is really just all about efficiency. The public perception of it is that it's really focused on latency wars and really optimizing microwave towers and cables and things like that, optimizing your software. All of that is true, but all of it's really done in the pursuit of just making more competitive markets. And the result is that in traditional finance, the markets are extremely, extremely tight. Like, if you look at the ES, which is the S and P 500 future contract, it's a futures contract that's, I believe, 50 times the. Yeah, 50 times the S and p 500 index value.
00:05:53.160 - 00:07:12.020, Speaker B: So it has a notional value of like $250,000, and the minimum increment is $12.50. And in reality, people are so, yeah, when people make a trade, they're paying like $6 in spread on something that's $250,000. It's just very, very small. And then in practice, the high frequency traders are competing for edges that are much, much smaller than that on their order of cents. So its just really a very optimized environment, which ultimately gives all investors, retail investors, institutional investors, et cetera, a really good low cost trading experience which allows them to transfer risk more efficiently. And its just the result of this really competitive war going on between different automated players to make markets as tightly as possible. And when we started getting involved more on the DeFi side, and we looked at the state of the world with respect to automated market makers and the typical slippage that anyone interacting with an AMM would encounter, we were just really surprised by how much multiple orders of magnitude larger slippage.
00:07:12.020 - 00:08:33.663, Speaker B: The slippage for someone trading in that case that I mentioned before, like trading the ES fractions of a basis point, like a 10th of a basis point or less, and then you go to DeFi and it's very common for people to experience 1% slippage or 2% slippage. That's very normal. So we just realized that there is still a big gap between DeFi and centralized finance also. Another thing is that people frequently compare the experience of trading on DeFi versus the experience of trading on binance. But then binance itself is still several tiers below what the really advanced centralized exchanges and traditional finance are in terms of the number of basis points of slippage that one would typically pay really a big gap. And the gap gets narrowed by making it much more efficient for participants to be able to go compete and be able to compete on making the absolute tightest markets possible, which frequently requires frequent updates because the prices are changing all the time, and you're reacting to supply and demand and reacting to other exchanges. So as market makers need to be able to frequently update their quotes.
00:08:33.663 - 00:09:02.119, Speaker B: And right now, the high cost of updating each one of those quotes means that as a result, they end up with just really widespreads, or we end up using systems like Amms to quote markets in Defi. So anyway, that's a really long winded way of saying that we just see that there are kind of a lot of different optimizations that were needed in order to help decentralized finance take over the world. And it really started at the base layer.
00:09:03.990 - 00:09:49.814, Speaker A: Definitely. I definitely want to get into more of throughput and being able to set those bids and ask on the order book aside, what will it take to actually get there. But before, I think you obviously got to see pretty up and close with Solana, the Solana defi system. Solana was also, I believe, the first virtual machine that was also parallelizable. It has since kind of, or one of the first. Many have kind of followed in suit the Ethereum kind of in its early iterations, or still as today, single threaded. It's just a little bit more simple of a virtual machine.
00:09:49.814 - 00:10:14.400, Speaker A: But I think the big innovation that you have unlocked at Monad is the parallelization of the virtual machine. Can you talk? Maybe just briefly, because I know numerous times in the past there's been multiple different attempts at trying to paralyze the virtual machine. Why have those failed? And I guess, how are you doing it differently that you are able to paralyze the Ethereum virtual machine?
00:10:16.260 - 00:10:53.810, Speaker B: We do optimistic parallelization, so we're running transactions in parallel and always ensuring that we get the same outcome as if we had run those transactions serially. So just to be clear, there is still linearity in blocks in monad. There is still linearity of transactions within that block. But then when we go to execute, we run transactions in parallel and roll back transactions that have their predicate actually affected by another transaction that was running in parallel as well.
00:10:55.510 - 00:11:21.522, Speaker C: So maybe get into the dependency algorithm that you ultimately use in order to sort of make that assessment the idea of, and maybe to explain to the listener the idea, the separation between sort of optimistic execution and then having a formal ordering and guarantees before you go into execution and the trade offs there, right?
00:11:21.706 - 00:12:31.362, Speaker B: Yeah, I guess it's kind of like a big design decision for any blockchain about the format of transactions. And basically, to what extent is there a prespecification of all dependencies for a given transaction? So, like for an example, Solana has stateless programs. All of the state lives in external accounts, and those accounts have to get passed into the function calls. So effectively the dependencies are specified ahead of time. With EIP, I believe it's 29, 30. There's the idea of introducing access lists like a list of, again, like accounts that are going to be touched by a given transaction, which then could potentially be used by Ethereum to pre fetch dependencies. But yeah, I think basically the state of the world is that wallets are quite slow to adapt to changes in transaction format.
00:12:31.362 - 00:12:56.310, Speaker B: The reality of the world is that most users interact with the blockchain using metamask, basically in order to preserve compatibility with the gateway that the end user is actually accessing the blockchain through, which is metamask. We elected to not use EIP 2930 and just handle everything on the back end on our side.
00:12:58.010 - 00:13:30.650, Speaker C: If that's the case, could an Ethereum client basically, instead of saying, okay, let's start from scratch, because I'm going to inherit the same data structures, I'm going to inherit the same infrastructure, I'm going to use optimistic parallelism here, I'm not going to have true explicit state dependencies upfront. Could I just implement this at the client level, or do I need a sort of new network to do this?
00:13:31.510 - 00:13:35.730, Speaker B: Correct. You could implement this particular thing at the client level.
00:13:36.550 - 00:13:43.490, Speaker C: And so just from your experience, what has stopped the community from moving towards that in the past?
00:13:44.750 - 00:14:51.476, Speaker B: I think the biggest thing is that there are multiple bottlenecks. A huge bottleneck is state access for the transactions that are being done. And this is because, you know, even with all the state living on SSD, pulling data from the SSD is still, I believe, like in the ten to 20 microsecond range. And these are, you know, there's just a lot of accesses from disk, and then on top of that, the actual state is being stored in typically a high level key value store like LevelDB or RocksDB, LMDB, MDBX, sorry, they're all just kind of forks of LevelDB. So there's not really optimization on the state access front, which is another area where we've made significant improvements in order to be able to access state really fast. I mean, I think at the end of the day, you know, we have a giant list of transactions. Ethereum is the, what do they call it, the world computer.
00:14:51.476 - 00:15:28.234, Speaker B: The idea is that the entire world's transactions are all going to be processed against a single node, and then we'll replicate the node to have many copies. So fundamentally, the constraint is really how efficiently you can access all the state that all of those transactions are depending on. And some blockchains kind of get around this. Bye. Really raising the RaM requirements, because ram is much more efficient to access than SSD. And then they just try to keep a lot of active state in ram. But we don't agree with that philosophy either.
00:15:28.234 - 00:15:49.418, Speaker B: In any event, I guess, to go back to your question, yes, ethereum clients could implement this specific optimization, but there are a bunch of other ones that, all taken together, alleviate all of the bottlenecks in the pipe and then allow us to pack a lot more computation into a given amount of time. And I could talk about some of the other optimizations as well, but I.
00:15:49.434 - 00:16:22.280, Speaker A: Think I would love to learn. I mean, I have seen multiple people try parallelization in the EVM past, and so could you explain why from your perspective that hasn't worked, and then maybe also list like the high level optimizations, making the EVM paralyzed being one of them? But the other things that you're currently mentioning, what would you highlight as the key differentiators in your tech stack from previous attempts?
00:16:22.740 - 00:17:35.380, Speaker B: Right? Yeah, I would say that with any change that looks obvious in retrospect, people are always like, well, why didn't someone just do this before people tried it but didn't? I think it's a matter of execution and focus. And I think from what I've seen, and maybe I've missed some other efforts, but a lot of the parallelization efforts are shorter term experiments. We're going to try this for a weekend and then post about it on Twitter. And we got a two x speed up. That's great, but it needs to be done with a longer term goal in mind, which is to accelerate all parts of the pipeline and deliver much higher performance. And I think that that just requires a focused team with, you know, they'll quit their jobs from other things and are just super focused on doing that. But yeah, I don't feel that what we're doing is like something that's insane or totally inconceivable.
00:17:35.380 - 00:17:39.280, Speaker B: It's more just about focus and execution.
00:17:39.940 - 00:18:05.770, Speaker C: Well, let's get into a little bit more of those optimizations, the one that's maybe non trivial for garden variety ethereum client to run. And then perhaps that could lead us into an interesting topic, which is consensus. And I know you, you guys have your own sort of flavor of consensus and it takes bits and pieces of what works, but I'm curious how you sort of put it all together.
00:18:07.230 - 00:19:10.654, Speaker B: Yeah, I think the, you know, one of the really significant changes is what we call deferred execution. So really just the idea that actually, sorry to take a step back. With existing, most existing blockchains that are leader based, the process is the leader selects a list of transactions from the mempool that's going to be the official list of transactions, executes all of those transactions, determines the Merkle route that results from that new state, and then publishes a block with all those transactions and the new Merkle route. And then the other validating nodes, like the other ones participating in consensus, go execute those transactions as well. Check that the Merkle route matches and then vote yes or no. And that entire sequence is really what completes the production of a block. And of course I glossed over the details of how consensus works and the multiple rounds of voting and so on.
00:19:10.654 - 00:20:00.782, Speaker B: But the point is that to produce a block, you have to execute twice, like the leader executes, and then the validators execute and you have to have consensus happening. And the thing about consensus is that if you want your nodes to be decentralized, that means that they're potentially on opposite sides of the world. And the round trip time around the world is literally hundreds of milliseconds. So consensus ends up taking up most of that. If it's a 1 second block time, most of that 1 second is spent on consensus. So execution is actually, the budget for execution is actually quite limited because it has to fit into that sequence. Now with Monad, we actually move execution out of that hot path.
00:20:00.782 - 00:20:57.550, Speaker B: So the nodes just come to consensus about the official ordering of transactions. They're validating that all the transactions have valid signatures, they're validating that the transactions have sufficient gas in their account to pay for, to be paid for. But then the execution happens in a slightly deferred fashion, like basically happening in parallel to the next round of consensus happening for the next block. So what that means is that execution is a little bit delayed. Oh, and then, by the way, we just do some other things to ensure that the nodes are still in consistent state, do other things to make sure that the API or the interface from the user's perspective is still very similar to Ethereum. But yeah, the execution is just happening in a slightly delayed fashion. And I want to just say one other thing about that and then kind of pause for a second.
00:20:57.550 - 00:21:39.650, Speaker B: But the idea of having execution be a little bit delayed. Having the nodes have a slightly delayed view of the state of the world sounds scary, but in reality we always have a delayed state of the world. Like when you open your metamask, it's going and querying a node, and then that node is responding. But then in the time that it's responded, more things could have happened on the blockchain. So I think inherently we always live with the delayed view of the true state of the world. But the true state is actually just determined at the time of consensus, because that defines the official ordering of transactions, which officially defines the state. And then execution is just revealing that state.
00:21:40.630 - 00:21:53.600, Speaker C: Well, let's, let's get into the bits about ordering, but let me just ask one clarification here. So do you have two rounds of consensus, one on execution and one on ordering?
00:21:53.940 - 00:21:58.000, Speaker B: No, we have one round of consensus for ordering.
00:21:58.500 - 00:22:05.160, Speaker C: Okay, so let's talk about ordering then. So is there a canonical way in which transactions are ordered?
00:22:06.140 - 00:22:15.040, Speaker B: They're ordered using the priority gas auction method. So just, you know, ordered from highest to lowest priority fee.
00:22:16.820 - 00:22:19.692, Speaker C: But that's up to ultimately up to the block proposer.
00:22:19.796 - 00:22:20.440, Speaker B: Correct.
00:22:21.100 - 00:22:37.480, Speaker C: Okay, so this immediately opens up the age old question of now age old, I suppose, mev. Let's talk a little about MeV, conditional on the fact that you now have delayed execution.
00:22:40.630 - 00:23:32.866, Speaker B: Yeah. So we see the question of how transactions get ordered as being totally orthogonal to everything else that happens. Something chooses the order and then the nodes communicate about that order, decide it, and then execute it. In version zero of Monad, it's a priority gas auction. In future versions, it's possible that will integrate with something like flashbots or maybe flashbots itself to allow ordering to take into account bundles. And basically I think maybe what you're getting at is that with only priority gas auction, certain competitions between searchers just get settled by spam. That's definitely quite important on the horizon.
00:23:32.866 - 00:24:14.840, Speaker B: But most other blockchains other than Ethereum, basically just use priority gas auction at this point with a sufficiently high gas or minimum cost, so that while there is spam, it's similar to the postal service. Postal service carries a lot of spam and it's fine. People just pay for every piece of junk mail that they want to send. And our focus is really on making sure that we can execute super efficiently so that we can carry a lot of transactions. And then over time we'll evolve with respect to weeding out spam through a variety of different strategies.
00:24:17.820 - 00:24:48.600, Speaker A: Maybe ultimately I'm curious, like just from your background, being in high frequency trading, being at jump, it feels like you could have started a new layer one or L2 with any kind of new virtual machine. Why specifically did you choose Ethereum or the Ethereum virtual machine? Was it just the existing infrastructure and tooling, or was it just tapping into that developer base?
00:24:49.300 - 00:25:52.752, Speaker B: I think with any technology it should exist to solve a very clear problem. And I think for us the clear problem is that developers are limited in what they can build. Like most developers are EVM developers, they're using solidity, they're using their tool set. It's like the JavaScript of crypto. 97% of all tvl is Evm Tvl. So it has the mind share. There are a lot of developers, but then they're just facing severe problems in terms of cost of gas, the low plentifulness of transactions, and then they're doing a lot, jumping through a lot of hoops, like optimizing tiny amounts of gas in order to save for end users, and in the process, perhaps even sometimes like having worse security practices from a smart contract perspective, because they're omitting defensive assertions or omitting extra checks that would cost more gas.
00:25:52.752 - 00:26:35.280, Speaker B: We just don't want there to be that trade off. A developer should never omit a defensive assertion because it's going to cost a little bit more gas. So our view is really that we need to go where the developers are and solve real problems for them. And then at a later point in time, like in subsequent versions of Monad, we'll make additional improvements to the EVM in the form of new precompiles, perhaps some new opcodes, new vm behaviors enabled by runtime flags. That'll help developers in the other dimension, which is making it easier for them to build high performance applications.
00:26:38.820 - 00:27:02.440, Speaker C: There's this interesting paradigm that we see people move back and forth between basically security and gas costs, these very clear security practices that we developed over time. And every once in a while somebody comes up with a clever and obfuscated way of getting around it. To think that they sort of reduced costs and all you've done is a rude Goldberg machine to introduce reentrancy.
00:27:02.480 - 00:27:02.704, Speaker A: Right.
00:27:02.752 - 00:27:25.300, Speaker C: If you look at just the graveyard of exploits, they basically are one big form of reentrancy, at least some of the more astonishing ones. And so I always found that amusing. So I admire this idea that there are certain trade offs that need not be made at all, and instead they just need to be table stakes.
00:27:28.880 - 00:28:15.330, Speaker B: Yeah, I think P. Cavor Satio's list of reentrancy attacks on GitHub is a really, really good one to look through. I think it's good inspiration for all of us as developers, whether it's smart contract developers or system developers, you really got to make it. Yeah. So that, because a lot of them are quite convoluted, and as you said, a lot of them really just result from the fact that later on down the call stack you can reenter the same function that had been visited before. And that's like a pretty unexpected, hard to think about thing. So, yeah, that's one of the things that we're planning on introducing.
00:28:15.330 - 00:28:25.290, Speaker B: Perhaps in this version that we'll release at the end of the year, or perhaps like in the version after that. But definitely thinking about a lot of improvements like that.
00:28:26.230 - 00:29:13.730, Speaker A: One thing that I've really loved recently has kind of been your more informative Twitter posts from Monad, and even, I think, your personal account, kind of highlighting the different data availability and the limited throughput that Ethereum has today, and kind of running the math on what that actually provides to roll ups at the end of the day. And I think very few people actually kind of run that math. But could you kind of walk us through some of those numbers that you've provided on Twitter and ultimately how the Monad team is ultimately thinking about increasing the base layer throughput such that developers don't have to worry about these more expensive gas costs.
00:29:14.670 - 00:29:58.674, Speaker B: Yeah, definitely. I think our team's mentality overall is just that. Yeah. Like, for any design that we're considering, we just have to run the numbers and use common sense. And I know it's hard for like, everyday users because they're just so many, they're inundated with so many different ads for different projects, and it's hard to cut through the noise. But I think at the end of the day, the roll ups are. So I guess I have two responses.
00:29:58.674 - 00:31:36.068, Speaker B: And one is, what are rollups really good for? What is the fundamental constraint of any system? And then the other one is like, the specifics of sort of the math of EIP 4844 and the expected throughput of rollups right now after EIP 4844, et cetera. I think I maybe want to talk about the first one, just for a second. You see a lot of debates on Twitter, and Tolly has kind of had this, taken this stance as well, which is that at the end of the day, it's really just about decentralized data storage and how efficiently your network can propagate information about transactions through a decentralized network, which can then the individual nodes can then go execute using whatever runtime they have, in Solana's case, the sea level runtime, etcetera. But the constraint is really just how much data you can propagate through the network. And so I think people who have very fantastical beliefs about what roleups will be able to achieve aren't, at the end of the day, the constraint is really the data availability, the amount of throughput you can have to propagate a bunch of transactions through a decentralized datastore. Now, if this datastore is not decentralized, then you can probably get a lot more throughput. But then that really subverts the actual purpose of what we're all doing here.
00:31:36.068 - 00:32:14.770, Speaker B: So I think some of the frustration that you see in some of his posts is just that people apply a fantastical standard to the possibility of roll ups while not looking at what existing systems Solana can do. Right now, I don't want to put words in his mouth, so I think I'll probably just go back and look at Twitter to see what he said. But it's like, if there is really a magical thing that can deliver all this data availability, then Solana will happily use it. And so it's really about the efficiency of propagation of data through the network.
00:32:18.670 - 00:33:10.020, Speaker A: Yeah, I mean, I definitely agree with that. I think ultimately, it took me personally a while to learn about kind of these core bottlenecks, and as you mentioned, data availability being one of them, or kind of the key thing that really unlocks scalability to the masses. And ultimately today that's very limited and less than typically in the megabytes per seconds of data. I think eventually it'll get to gigabytes and then probably further pushed upon that. But so in that point of view, with L2s and even layer threes, is it your point of view that it should be done all on kind of a single layer and that these additional layers are not particularly useful for scaling? Or how do you view them? Like, directly?
00:33:10.400 - 00:34:09.106, Speaker B: Right. So I think to address the question that you were asking before about basically like, run me through the numbers of what systems will look like right now and what will look like in the future, I'll just try to do that really quickly because I think that informs your question just now as well. Right now, the gas limit on Ethereum is a target of 15 million gas per block, and a block is every 12 seconds. So that's about a million gas per second. And then additionally, the cost per byte of call data is 16 bytes. Sorry, 16 gas per byte. So if you take the roughly 1 mb/second sorry, I'm just messing up numbers all over the place.
00:34:09.106 - 00:34:52.920, Speaker B: Okay, starting again. So 15 million gas, 12 seconds. So that's about a million gas per second and then 16 gas per byte. So if you divide a million gas by 16 gas, you get, what is it, about 60 kb/second is that right? Did I do that right? That's like a million divided by 16. Yeah. 62,060 2000 bytes. So 62, then the average transaction on Ethereum right now is about 250 bytes.
00:34:52.920 - 00:35:29.860, Speaker B: So if you do 62,500 divided by 250, you get about 250 transactions per second. And that's assuming that all of the. There's assuming a couple of things. So one is that it's assuming that the entire block is like, all the gas in the block is being used for transaction data, which is not. Probably not going to be true. And then it's also assuming that there's no compression on the transactions, which is also not necessarily true, or definitely not true, actually. So let's fix that second one.
00:35:29.860 - 00:35:48.520, Speaker B: So right now, transaction about 250 bytes on Ethereum main net, but you can actually compress those transactions by fact, currently seeing compression of about two x, which then increases the throughput by about two x. So we go from 250 to about 500.
00:35:49.100 - 00:35:52.800, Speaker A: And when you say compression. Are you specifically talking about l two s?
00:35:53.820 - 00:36:26.506, Speaker B: Yeah, so I'm talking about the. Actually, yeah, that's a good point. So one of the benefits of l two s and then the posting of call data back to l one, is that that transaction data doesn't really need to be directly used. Most of the time it's really just there as a record so that transaction data could be compressed. And in practice that's what we see roll ups like optimism or arbitrum do and getting about a two x compression factor.
00:36:26.698 - 00:36:33.714, Speaker C: Well, it depends obviously what you publish to the l one ultimately as well. Call data versus the actual differentials, right?
00:36:33.762 - 00:37:12.220, Speaker B: Yeah. So that's kind of the other asterisk to my math is that with ZK rollups, with certain designs, I guess, such as with Zksync era, there's the potential, or there is the behavior of posting the differences to all of the state values like all of the slots, rather than posting the full transaction data. And the proof can still validate against that data. If that is smaller, then that additionally allows for more throughput.
00:37:14.360 - 00:38:01.810, Speaker A: I guess at the end of the day, maybe we did the math showed that with ethereum as it stands today, the data availability and the number of transaction throughput is relatively small. It is going to be increasing over time with 4844 and then ultimately the full. It always confuses me. Is it ding charting or full charting roadmap. But the numbers are 1.3 megabytes per second of available throughput. Is it your opinion, just to clarify, on L2s, that I guess, what is your stance on L2s and layer threes? And then what is the approach that you specifically at Monad are taking to ultimately get more scalability?
00:38:04.630 - 00:38:51.646, Speaker B: Yeah, I think the, so my view is that in existing systems right now, we kind of know what the throughput will be. And then there is additional improvements that come from, like you mentioned, EIP 4844, that one in particular adds another 250 kb/second sorry, 250 block. So about another 20 throughput. So I think previously we said there's like 60 and then you're adding another 20. So then it's like a 30% increase. So we go from 500 ish to 605,700 ish. But I think the.
00:38:51.646 - 00:40:06.772, Speaker B: Yeah, at the end of the day, the blockchain kind of has two different things that it needs to do. And one is have distributed consensus about an official record of transactions. And then the other thing that needs to get done is actually just the execution of those transactions. And in Monad we're just focusing on optimizing both and having a really efficient system for executing a bunch of transactions and making the appropriate state transitions. And we also do that by just making improvements at the algorithmic level so that then all nodes that are running the system can direct, like, anyone can run that kind of node and directly just query that node for like the, you know, the values of whatever account they want to know about or any contract, any state variable there. I think there are a lot of other systems that are being developed that have certain advantages as well. I think the main advantage of a roll up is that you can move computation off chain, so it enables the potential for much more complex computation to happen.
00:40:06.772 - 00:40:56.050, Speaker B: Like if you imagine inverting matrices or just doing a bunch of updating neural nets or just something complicated, it is kind of inefficient for that to be run on every single node. So there is the potential that it could just be run in this smaller set of nodes. And then that other environment just is kind of like a side environment to do a lot of computation. And then the bridging is still clear whenever users need to transfer assets back and forth from that specialized environment. But with Monad, we're just really focused on improving execution for all the nodes and then having as efficient as possible of a decentralized consensus method for propagating that transaction data across all the nodes so that we can deliver the best performance.
00:40:57.170 - 00:41:06.562, Speaker C: So on consensys, you do end up using just out of the box tendermint, or I know you have some improvements on that as well.
00:41:06.746 - 00:41:29.050, Speaker B: Yeah, we use the tendermint algorithm right now. It's like the tendermint paper, but not the Cosmos SDK implementation. That one has a lot of inefficiencies, like places where one component is talking another one by polling it, and that introduces latency. There's inefficient infrastructures. Yeah.
00:41:29.210 - 00:41:41.110, Speaker C: Well, actually, let's talk. This is the question that Logan loves to ask, but never ends up asking. Overhead costs, messaging between nodes. So is there any improvement there as well?
00:41:42.250 - 00:42:29.120, Speaker B: Yeah, we have some improvements coming from making block proposals. Just refer to transaction ids instead of including the full transaction data. I believe some people refer to this as like shared mempool. So, you know, like the idea that we propagate the transactions through the mempool. We have a lot of improvements there as well, improving the P two p network for propagating transactions through the mempool. But then when all of your nodes actually have records of those transactions already, then you can just communicate a proposal as a reference to those transaction ids.
00:42:31.660 - 00:43:07.890, Speaker C: And then is there a small subset of nodes that you can query? So there's this idea of like a sparse node or a node that doesn't have to, because we're already saying something about dependencies. I'm not necessarily interested in all states, just a subset of state that sort of sits here in the corner and doesn't interact with the rest of state. So is there like a distinct class of nodes that you can use for, let's say, for querying or for quicker partial synchrony with the sort of the canonical chain and so on?
00:43:09.350 - 00:43:14.410, Speaker B: We don't have something like that. That's an interesting idea, but haven't done anything like that.
00:43:17.700 - 00:43:57.370, Speaker A: So I guess at the end of the day, I want to ultimately relate some of this back to the user standpoint. We have gotten fairly technical even on the developer side of things. I guess what you would really like to have happen is, I guess by creating Monad, being able to kind of unlock the EVM experience on the developer side, that allows developers to build interesting apps, have high throughput, have parallelization, and being able to take advantage of all the tooling that currently exists.
00:43:59.830 - 00:44:53.340, Speaker B: Yeah, I think there is sort of like a common standard that's been developed with EVM, really has a strong network effect. Like a lot of the applied cryptography research is being done in the context of EVM as well. We talk about developers and libraries, but there's also the researchers to take into account. The goal is really just to unblock them, like you said, and remove considerations like gas optimization, that is, trading security for gas potentially, and also in the longer term make improvements to the EVM, extend and embrace the EVM, introduce new features that help developers build more powerful apps.
00:44:54.640 - 00:45:14.174, Speaker A: Yeah, interesting. I love it. I guess on terms of data throughput initially, because we were talking about that quite a substantially. Is there any expectations that you will have once Monad comes online or that eventually that you're targeting in terms of.
00:45:14.222 - 00:45:16.050, Speaker B: The throughput through the network?
00:45:16.470 - 00:45:17.250, Speaker A: Correct.
00:45:17.910 - 00:45:20.450, Speaker C: And how it translates ultimately to TPS?
00:45:21.510 - 00:46:17.080, Speaker B: Yeah, I think ultimately the network throughput is really going to be the driver of the terminal TPS. For a given chain, there will be probably many chains like, there will be many monad instances, each with its own connected set of validators and a bunch of apps maybe that are related to each other, that are all, in that instance, our goal and our focus is like, yeah, right now our expectation is 10,000 tps for typical transaction size and complexity. As ethereum main net right now, but I think in the long term it'll go up as we're able to make additional improvements to the p two p layer and the messaging propagation.
00:46:17.780 - 00:46:22.280, Speaker C: These different instances, are they related to each other in any way?
00:46:23.420 - 00:46:59.264, Speaker B: I was just referring to the fact that that's my mental model for what the end state for blockchains is. A bunch of different instances. The way that there's polygon supernets or avalanche subnets, they're just separate instances of shared global state run generally by separate validators. We see that as part of our long term roadmap, but I think it's too early to speculate on how many such instances there will be right now.
00:46:59.432 - 00:47:26.350, Speaker C: But certainly the idea is how they ultimately inherit something interesting or something useful from the base chain, from the main instance. In the case of hyperchains or l three s or whatever, let's say you just have this recursive ladder of verifiers where you're just doing l three submit, you know, ln plus one submits to ln and so on, and then you get down to the base layer. Is this something that you're considering, or are you. Are we basically talking about separate chains?
00:47:27.370 - 00:47:36.070, Speaker B: I think it's just too early to say right now. We have to focus on the first Monad before we. But I think the things you said are. They make sense.
00:47:39.330 - 00:48:10.900, Speaker A: Interesting. Interesting. No, it's. I find it fascinating. I think all these different blockchains are taking kind of slightly different approach to me. It seems fairly obvious in my point of view that parallelization on the virtual machine side will be kind of the path forward, and so definitely excited that you're pushing the space forward and the ethereum space forward with that. I guess in long term, like, it's kind of like a competition in a sense, for developers.
00:48:10.900 - 00:48:20.160, Speaker A: Do you feel like Monad is competing directly with Ethereum or is it just trying to grow the pie overall in the space?
00:48:21.300 - 00:49:55.630, Speaker B: No, we're not competing with Ethereum. I think there really needs to be a lot of growth of developers in order for all of crypto to grow. We're just focused on, I guess, what you would say, like high frequency or high transaction count, low value per transaction types of applications, which I think is pretty orthogonal to where I personally feel Ethereum will end up converging, which is like valuable nfts are probably going to be minted on Ethereum because that's where they have the greatest Lindy effect. I think it's just opening up a different kind of dimension for developers to focus and ultimately, if we want to have applications that have a million daily active users or 5 million daily active users, they're going to need a back end that can do a million or 5 million times 20 or 100 transactions per user per day, which then ends up becoming like 20 million 100 million transactions per day, which translates to like, say for 20 million transactions per day, that's 200 tps. For 100 million, that's 1000 transactions per second. We're just going to need systems that can process at least 10,000 transactions per second in order to be able to support a single app like that.
00:49:56.970 - 00:50:08.802, Speaker A: Yeah, I guess to the engineers watching, what is the pitch of building on Monad versus some other high throughput chains in the space, such as Solana or.
00:50:08.946 - 00:50:27.302, Speaker C: SWE Aptos, and maybe say something about compatibility equivalence. I know you said at some point you're going to introduce different opcodes, so at some point you're just going to branch off. But what about out of the box version one of Monad, right?
00:50:27.406 - 00:51:22.080, Speaker B: Yeah, I think it's for those developers, it's having no vendor lock in. They can build for EVM, which is a standard, and just go where users and usages dictate and where the transaction fees make sense for the users. I mentioned that we will make improvements to the EVM. Our Northstar is always going to be backward compatibility with Ethereum. These are more optional features that people can opt into. Of course, if they find them really valuable, and those are essential to building the kinds of apps that are trying to support 10 million users, and they have to do that on Monad, then it's going to be fine anyway. But if they're building a more generic thing, they don't use these features, then of course they could just deploy them in other environments as well.
00:51:24.860 - 00:51:50.044, Speaker C: I think there's a future in which a lot of the improvements that are done, especially the ones that we talked about at the beginning of the podcast, the ones that are relatively simple to implement on the client side, I think it ends up being implemented. There's no reason, in the same way that when you see something that's so obvious that somebody ends up actually building it, there's no reason not to adopt it. So I think there's certainly a positive sum game being played.
00:51:50.212 - 00:52:13.180, Speaker B: Yeah, we definitely intend to push changes back to Ethereum as Eips, although we don't know if the community will accept them or not. That's another game. But I think having another environment where it's already implemented and where developers are taking advantage of that feature, that's a really, you know, it actually makes it a lot easier to push forward improvements to Ethereum.
00:52:14.080 - 00:52:47.820, Speaker C: Well, speaking of, this is a question I always like to ask, and it's more on the, it's more on the negative as opposed to the positive, like what you would subtract if you had a magic wand, what would you remove from Ethereum today? And by magic wand I mean you wouldn't introduce anything that would basically break consensus or, or produce some sort of crazy runtime error just to anchor this. The answer I usually get is the wretched ERC 20. This is basically the one I always get. I'm curious, your take.
00:52:49.040 - 00:53:19.610, Speaker B: Yeah, I mean, it's definitely, it would be nice to have a canonical token standard without the possibility of partial burn on transfer or all these weird things that then I introduce edge cases for apps that are on top of them. But I think if I had to choose something else, I would probably say self destruct, because I think it just introduces a lot of weird state dependency issues.
00:53:21.190 - 00:53:27.856, Speaker C: I think that's been deprecated, or at least nobody uses. I haven't seen a lot of instance.
00:53:27.998 - 00:53:28.484, Speaker B: Interesting.
00:53:28.532 - 00:53:29.988, Speaker C: Yeah, maybe we have to check.
00:53:30.124 - 00:53:33.600, Speaker B: Okay, yeah, I'll have to check. Maybe my wish has already come true then.
00:53:35.900 - 00:54:13.680, Speaker A: The other thing I was a little bit curious about is, I guess over time, the other thing that a lot of blockchains today at least debate upon is kind of the number of full nodes that I guess is sufficient to say you're decentralized and then also kind of what hardware or what is the appropriate level of cost for a specific hardware. Could you share a little bit more on your thoughts around, like, what you're ultimately trying to achieve on like the full node sides and then what nodes from the hard, what are the nodes running? From the hardware perspective.
00:54:15.540 - 00:54:48.440, Speaker B: Our expectations are actually pretty similar to ethereum's. It's like eight core cpu, 16 gigs of ram, two terabyte ssD. I think that that one, like Ethereum in previous times has opted for a smaller ssD, but two terabytes is like dollar 250 right now, so it's pretty reasonable. And then 100 megabits per second of upload and download network bandwidth.
00:54:50.530 - 00:55:00.138, Speaker A: I guess the big thing that you differentiate there from a core ethereum, and this goes back to increasing throughput, is just the bandwidth speeds on upload and download.
00:55:00.314 - 00:55:01.030, Speaker B: Right.
00:55:02.650 - 00:55:46.360, Speaker A: Makes a lot of sense, if anything. Now I've kind of become a high throughput maxi just because I really like data availability and being able to post a lot to things on chain, whether it's an l two. Or just doing a lot of reads and writes. Maybe getting back to the original question of high frequency trading and being able to do mini bids and ask on chain for different order books. What's the throughput expectations either in terms of TPS or just bandwidth required, do you think, to bring those bid and ask spreads to, I would say as close as, to comparable as like high frequency trading as possible?
00:55:50.660 - 00:57:03.422, Speaker B: So, yeah, I guess there are sort of two answers to that question, and one is related to the cost of each update. We think that the cost of each update needs to be a cent or less, which is also true in Solana, like for CRM or now Openbook. The more expensive it is, the less that market makers are going to update their quotes, which then means that they're just going to have to quote a little bit wider so that they don't, the fair value will wiggle a little bit more without them making changes. But yeah, the numbers are kind of like at a 10th of a cent, then if you send 50 million orders a day, it's. Wait, sorry, did I get that right? 50 million orders per day? Yeah, it's $50,000 per day. 50 million orders per day is kind of like the. Let's see.
00:57:03.422 - 00:57:47.296, Speaker B: So let me back up a little bit. So in centralized exchanges, like the most liquid ones with the most number of participants, you could see like 10 million orders per day. But of course, it's like the number of orders is elastic. That's because you charge $0.00 per order. So if you charge a 10th of a cent instead, then maybe like that 10 million would become 1 million or something like that without too much of an effect on the liquidity. So, yeah, we sort of think that like $50,000 per day worth of fees being expended by major participants to support like 100 ish markets makes sense.
00:57:47.296 - 00:58:25.370, Speaker B: That's $50,000 divided by 100. It's $500 per day per market. So that's not too bad. So that's, one answer is like a 10th of a cent. But then I think the other thing is actually just like, if you're trying to build an exchange, then you need to have like real natural users. Like for any exchange, there's kind of, in our HFT days, we would talk about like hfts versus naturals, and naturals are just there to like transfer risk, to like buy AMC stock or sell bitcoin or whatever you really need.
00:58:25.450 - 00:58:27.970, Speaker C: Is that what you call, is that what you call a noise trader?
00:58:28.090 - 00:59:20.790, Speaker B: Naturals, that's what I would call, yeah, I guess different people have different names, but just natural participants. So at the end of the day, if you want to build a really good exchange, you need to have a lot of. It is about user onboarding and user experience. When I reflect on the Solana and serum, serum actually did offer that in the sense that the transaction fees are less than a cent per transaction. But the problem was really just user acquisition. It's a little bit of a chicken and egg problem. You need a lot of liquidity so that the spreads are tight, so that users have a good experience, and then you need that liquidity to then beget a lot of users that are taking advantage of that, and you got a good virtuous cycle.
00:59:20.790 - 00:59:55.790, Speaker B: I think one of the problems with serum is that the CRM token, there's a lot of stuff in the news now about how Alameda actually just owned all the serum. That's not good. I would say that a successful on chain limit order book launch would involve kind of like redoing the tokenomics, like having the distribution be such that that token can be used to incentivize market makers and real participants to go take advantage of this, and then that can bootstrap into a successful ecosystem.
00:59:56.450 - 01:00:47.392, Speaker C: But to be fair, all major centralized exchanges, forget crypto for a second. And they also require significant bootstrapping. But they do this with off chain agreements. Basically, they do this with requiring people to be there, and there are certain legal contracts in place, and there's a certain amount of liquidity that you need to provide. And the concept of rebates, the idea of LP tokens is not new. There are lots of structures in place that you need to bootstrap. However, the interesting bit is the market makers don't usually have equity in the exchange, which is something that's so counterintuitive, because in crypto, and you do any sort of liquidity rewards or liquidity mining, you're basically giving someone something, I don't know what it is that you're giving them, but something that resembles some sort of pseudo equity at the actual exchange.
01:00:47.392 - 01:00:55.940, Speaker C: So there's something funny about the bootstrapping model of on chain order books that I think is different. So what do you think about that specifically, given your background?
01:00:56.480 - 01:01:26.786, Speaker B: Yeah, I think the ability to give the. These participants scan in the game is actually, it's overall positive. Like, it makes it more feasible to bootstrap an exchange than it is in traditional finance. But that is just. So if you're asking like, why aren't there a ton of like, successful dexs. Given that possibility, the answer is twofold. One is that it's really, really hard to launch a centralized exchange.
01:01:26.786 - 01:02:40.848, Speaker B: Like really, really, really hard. There's hundreds of failed exchanges and a jump we would make markets on exchanges sometimes that ultimately ended up failing. They just weren't able to get enough taker flow, enough natural flow to really get off the ground. So you're going from, I don't know, really low odds of success to higher chance of success when you have, you're armed with the ability to incentivize the market maker or the participants that are making continuous two sided markets. But I think the other factor is just that you need a confluence of, you need the stars to line up, you need lots of users, you need market makers, cheap fees. From the perspective of Monad, what we're bringing is low cost EVM compatibility, which we think will make it easier for users to use and easier for developers to build. The developers building that exchange to build and potentially easier for market makers to interface as well, because it's also EVM.
01:02:40.848 - 01:02:54.200, Speaker B: So I think it's like part of the reason it hasn't happened is like, it's hard. And then the other part is that you do need all these prerequisites, and we're trying to help line up all those prerequisites.
01:02:55.260 - 01:03:18.260, Speaker C: If there's one piece about user experience that's missing today, even from centralized exchanges. So let me ask you the question in two ways. One, let's talk about dexs, and the other, let's talk about centralized exchanges. What is missing fundamentally on, I guess I don't want to name any centralized exchanges today because everybody seems to be in hot water, but you know what I mean.
01:03:19.520 - 01:03:42.202, Speaker B: Well, I think, yeah, the killer features are capital efficiency, aka margin. And then I guess specifically, if you want to get advanced, it's portfolio margin. It's like being able to go long one coin or one perpetual, and then short the other one using the p.
01:03:42.226 - 01:03:44.442, Speaker C: And l of the entire account.
01:03:44.586 - 01:04:36.360, Speaker B: Right. And I think that's just quite complicated because I believe that even with centralized exchanges in traditional finance, when they allow margin, usually the risk calculations are, you know, it's like someone is using a spreadsheet to compute historical correlations between Microsoft and Google and then saying like, okay, I guess it's okay for this person to be long one and short the other, and we'll give them credit for 60% of their. It's not very precise either. And then it's just tricky in crypto when a lot of the assets have much higher volatilities and you could have random things like mobile coin on FTX happen where the thing goes up by ten x. So I think it's just.
01:04:36.780 - 01:04:41.770, Speaker C: So wait, you're saying that VAR doesn't work? Is that what you're saying?
01:04:43.190 - 01:05:31.860, Speaker B: Yeah, I would say that it's probably better to have something where there's more, a real time observation of the available liquidity so that then margining can be done based on the real time state of the book. So that then we know if we have to stop out this person by closing out their short position, this is how much of an impact it'll have on the market. But that's a very tricky problem because then that could then trigger further liquidations which further move the price. It's all very. I don't know, I guess what I'm saying is it's all stitched together in CeFi and Tradfi as well. Anyway, that would be the dream. That would be the dream would be to have portfolio margining.
01:05:31.860 - 01:05:38.330, Speaker B: But even just more generally, allowing people to take leverage is really the killer feature.
01:05:39.110 - 01:06:00.850, Speaker A: Maybe to ask the. Just because of time asking kind of like the final question for 2023 on a high level, I mean, maybe two questions, when do you plan to kind of go live? Or how's kind of the testnet progressing? And then for the remaining of the year, what specifically are you looking forward to?
01:06:03.240 - 01:06:52.090, Speaker B: Testnet is progressing well. We're just as a team working super hard to ship the test net as soon as we can. Currently we think in a couple of months. We're targeting end of year, like December for Mainnet, but we'd like to allow for enough time to have like six months of test net. So it might be a little bit after that. And then for me, I'm, and for our whole team, like just excited about partnering with builders who have ambitious plans to build real apps that provide real utility for normal non crypto natives in addition to crypto natives, and just excited about the sort of like the open design space coming from much cheaper gas.
01:06:54.160 - 01:07:13.500, Speaker A: Definitely agree. Well, thank you so much. I really appreciate you joining us. I hope you had as much fun as we did, kind of going in the technical deep dive and excited for what is to come. I too am very excited for what engineers can ultimately create once gas is much cheaper.
01:07:15.040 - 01:07:16.800, Speaker B: Awesome. Yeah. Thanks so much for having me.
